URL: http://www.cs.orst.edu/~reddyc/pubs/ml96-old.ps
Refering-URL: http://www.cs.orst.edu/~reddyc/pubs.html
Root-URL: 
Email: freddyc,tadepallig@cs.orst.edu  silvana@inf.ucv.cl  
Title: Theory-guided Empirical Speedup Learning of Goal Decomposition Rules  
Author: Chandra Reddy, Prasad Tadepalli Silvana Roncagliolo 
Address: Corvallis, OR-97330. USA  Valparaiso, Chile.  
Affiliation: Department of Computer Science Oregon State Univ.,  Escuela de Ingenieria Informatica Universidad Catolica de  
Abstract: Speedup learning is the study of improving the problem-solving performance with experience and from outside guidance. We describe here a system that successfully combines the best features of Explanation-based learning and empirical learning to learn goal decomposition rules from examples of successful problem solving and membership queries. We demonstrate that our system can efficiently learn effective decomposition rules in three different domains. Our results suggest that theory-guided empirical learning can overcome the problems of purely explanation-based learning and purely empirical learning, and be an effective speedup learning method.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackerman, P., & Kanfer, R. </author> <year> (1993). </year> <title> Kanfer-Ackerman Air Traffic Control Task c fl CD-ROM Database, Data-Collection Program, and Playback Program. </title> <institution> Dept. of Psychology, Univ. of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: Literals are in function-free first-order calculus representation, and are positive. The d-rules for lowest-level subgoals each has a single observable action in the place of subgoals. The following is an example of a d-rule from air-traffic control domain <ref> (Ackerman & Kanfer, 1993) </ref>. goal: (land-plane ?p) condition: ((at ?plane ?pat) (free-cursor) (wind-axis ?dir) (can-land-short ?p) (short-runway ?rway) (runway-dir ?rway ?dir) (free ?loc) (runway-loc ?rway ?loc) ) subgoals: ((move-cursor ?pat) (select ?pat ?p) (move-cursor ?loc) (short-runway-deposit ?p ?rway)) This example can be translated as follows: To land a plane ?p on <p> Each data point is a mean of 5 runs. The errorbars show one standard deviation on either side of the mean for these runs. Air-Traffic Control (ATC) domain. This domain is a simplified version of Kanfer-Ackerman air-traffic control task <ref> (Ackerman & Kanfer, 1993) </ref>. The main task is landing a plane from any configuration. The task has a queue of incoming planes, holding patterns and runways. The planes are accepted into the holding patterns, and then are landed on the runways.
Reference: <author> Angluin, D. </author> <year> (1988). </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 319-342. </pages>
Reference-contexts: Learning monotone DNF formulas from random examples in the sense of Valiant (1984), is no easier than learning arbitrary DNF formulas. However, there is a simple algorithm to learn them from positive examples and membership queries <ref> (Angluin, 1988) </ref>. A membership query asks if a specific example is in the target concept, and the teacher answers yes or no. The algorithm works as follows: let L be the positive literals of a positive example.
Reference: <author> Cohen, W. </author> <year> (1995a). </year> <title> Pac-learning non-recursive prolog clauses. </title> <journal> Artificial Intelligence, </journal> <volume> 79 (1), </volume> <pages> 1-38. </pages>
Reference-contexts: Unfortunately, however, determinacy of clauses is a strong assumption that does not hold in our domains. We are exploring weaker assumptions than determinacy which can be used to characterize the conditions under which our algorithm can be shown to PAC-learn a class of d-rules (e.g., see <ref> (Cohen, 1995a) </ref>). Another ILP system, FOIL, starts from the most general hypothesis and progressively specializes it by restricting the hypothesis from covering negative examples (Quinlan, 1990).
Reference: <author> Cohen, W. </author> <year> (1995b). </year> <title> Pac-learning recursive logic programs: efficient algorithms. Jl. of Art. </title> <journal> Int. Res., </journal> <volume> 2, </volume> <pages> 500-539. </pages>
Reference-contexts: The class of determinate conjunctive clauses is proved to be PAC-learnable from examples and the input-output information of the literals when the arity of the literals and the depth of the dependency chain of the variables in the clause are constant <ref> (Cohen, 1995b) </ref>. This result can be extended to show that first-order monotone DNF concepts are learnable under similar conditions from examples and membership queries. Unfortunately, however, determinacy of clauses is a strong assumption that does not hold in our domains.
Reference: <author> DeJong, G., & Mooney, R. </author> <year> (1986). </year> <title> Explantion-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 145-176. </pages>
Reference: <author> Dietterich, T., London, B., Clarkson, K., & Dromey, G. </author> <year> (1982). </year> <title> Learning and inductive inference. </title> <editor> In Cohen, P., & Feigebaum, E. (Eds.), </editor> <booktitle> The Handbook of AI, </booktitle> <volume> Vol. 3. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: If there are no other terms left, the example is made a new disjunct t n+1 of the hypothesis. Note that we are using single-representation trick which means that the language of the target concepts and the examples is the same <ref> (Dietterich, London, Clarkson, & Dromey, 1982) </ref>. We call this algorithm as First-order MDA (FMDA). To make the generalization procedure (While loop in FMDA) clearer, consider a simple example. Let the target concept C be holding-level (1, ?loc).
Reference: <author> Fikes, R., Hart, P., & Nilsson, N. </author> <year> (1972). </year> <title> Learning and executing generalized robot plans. </title> <journal> Artificial Intelligence, </journal> <volume> 3, </volume> <pages> 251-288. </pages>
Reference-contexts: For each domain, the input to the system is its domain theory and a teacher. The domain theory comprises the operators and the definitional axioms. The operators are specified in STRIPS style | i.e., with parameters, explicit preconditions, an add list and a delete list for each operator <ref> (Fikes, Hart, & Nilsson, 1972) </ref>. Axioms are definitions of utility functions (e.g., +, &gt;, etc.), and of abstract or higher-level terms such as toLeftOf, above, etc. The teacher provides training examples and answers the learner's queries. <p> There are no multiple d-rules for the same goal in this domain. ExEL takes at most 3 training problems for 100% performance on test problems. STRIPS World (SW). This is a minor variation of the STRIPS-world domain <ref> (Fikes et al., 1972) </ref>; the configuration of the rooms in this domain is a grid, whereas in the standard STRIPS world it could be of any shape. The domain consists of rooms which are connected to each other by doors. The doors can be open or closed.
Reference: <author> Haussler, D. </author> <year> (1989). </year> <title> Learning conjunctive concepts on structural domains. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 7-40. </pages>
Reference-contexts: In the propositional case, the MSG of a set of examples can be found easily by intersecting the sets of literals of all the examples. Unfortunately, finding an existential conjunctive concept that is consistent with a set of positive examples is an NP-hard problem <ref> (Haussler, 1989) </ref>. Haussler solves this problem by letting the learner ask "subset queries" (Haussler, 1989). A subset query is answered yes if the hypothesis is a subset of the target and no otherwise. <p> Unfortunately, finding an existential conjunctive concept that is consistent with a set of positive examples is an NP-hard problem <ref> (Haussler, 1989) </ref>. Haussler solves this problem by letting the learner ask "subset queries" (Haussler, 1989). A subset query is answered yes if the hypothesis is a subset of the target and no otherwise. Haussler's algorithm learns first-order conjunctive concepts incrementally by considering all matches between the variables of the current hypothesis and the objects of the new example.
Reference: <author> Korf, R. </author> <year> (1987). </year> <title> Planning as search: A quantitative approach. </title> <journal> Artificial Intelligence, </journal> <volume> 33, </volume> <pages> 65-88. </pages>
Reference-contexts: D-rules decompose a goal into a sequence of subgoals. Planning using subgoals has the complexity O (b D max ) where D max is the distance between the farthest successive subgoals, and b is the branching factor (the number of applicable operators) <ref> (Korf, 1987) </ref>. Without goal decomposition, the complexity is exponential in the distance between the start state and the goal, which is typically higher than D max .
Reference: <author> Leckie, C., & Zukerman, I. </author> <year> (1993). </year> <title> An inductive approach to learning search control rules for planning. </title> <booktitle> In Proceedings of the 13th IJCAI, </booktitle> <pages> pp. 1100-1105. </pages>
Reference-contexts: Another ILP system, FOIL, starts from the most general hypothesis and progressively specializes it by restricting the hypothesis from covering negative examples (Quinlan, 1990). DOLPHIN (Zelle & Mooney, 1993) and Grasshopper <ref> (Leckie & Zukerman, 1993) </ref> are two systems that use FOIL for learning control knowledge for a problem solver. FOIL is difficult to adapt to our work because it needs negative examples.
Reference: <author> Marsella, S. </author> <year> (1993). </year> <title> Planning under the restriction of Hierarchical Partial Orders. </title> <type> Ph.D. thesis, </type> <institution> Rutgers University, NJ. </institution>
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Learning Search Control Knowledge. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA. </address>
Reference-contexts: Unfortunately, however, EBL does not always lead to efficient problem solving. Many EBL systems suffer from the "utility problem" identified by Minton as the problem of excessive cost of finding and using the appropriate learned knowledge in problem solving <ref> (Minton, 1988) </ref>. One of the sources of the utility problem is the specificity of the explanations in EBL. To learn macro-operators, for example, the system has to explain why a particular operator sequence led to a goal. <p> D-rules In this section, we compare d-rules with macro operators and Prodigy's control rules, the two most prominent methods for representing control knowledge. The macro operators method <ref> (Minton, 1988) </ref> decreases the distance to a goal, but increases the number of operators the planner has to consider at each decision point. That is, this method can decrease the depth of the search tree, but increases the branching factor of its nodes. <p> Moreover, learning macro operators in recursive domains is difficult because of the Generalization-to-N problem, which results in overly specific rules in such domains (Shavlik, 1990; Subra-manian & Feldman, 1990). The control rules select, reject or order applicable domain operators in each state <ref> (Minton, 1988) </ref>. These rules help in reducing the number of operators considered at each point, but do not affect the distance to the goal. D-rules decompose a goal into a sequence of subgoals.
Reference: <author> Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 47-80. </pages>
Reference: <author> Mitchell, T., Utgoff, P., & Banerji, R. </author> <year> (1983). </year> <title> Learning by experimentation: Acquiring and refining problem-solving heuristics. </title> <editor> In Michalski, R., & et al. (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach, </booktitle> <volume> Vol. 1. </volume> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One approach to this problem is to empirically learn heuristics or control knowledge from examples of successful and unsuccessful problem solving. For example, this is the approach adopted to learn heuristics for symbolic integration in LEX <ref> (Mitchell, Utgoff, & Banerji, 1983) </ref>. The purely empirical approach of LEX and other similar systems does not exploit the knowledge of the goals and the operators of the domain (i.e., "the domain theory") that the problem solver already has.
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1990). </year> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <pages> pp. 368-381. </pages> <month> Ohmsha/Springer-Verlag. </month>
Reference-contexts: GOLEM, an ILP system, circumvents this problem by restricting the target class to be "determinate," which means that there is a unique binding to all the free variables of a literal in a clause given the bindings of all the previous literals <ref> (Muggleton & Feng, 1990) </ref>. The class of determinate conjunctive clauses is proved to be PAC-learnable from examples and the input-output information of the literals when the arity of the literals and the depth of the dependency chain of the variables in the clause are constant (Cohen, 1995b).
Reference: <author> Natarajan, B. </author> <year> (1989). </year> <title> On learning from exercises. </title> <booktitle> In Proceedings of Computational Learning Theory Conference. </booktitle> <address> Santa Cruz, CA. </address>
Reference-contexts: Exercises help the learner bootstrap its knowledge by using what is learned in simpler subproblems to solve more difficult problems and learn from them. It has been shown that exercises are just as powerful to learn from as positive examples in some cases <ref> (Natarajan, 1989) </ref>. We might also be able to eliminate queries in our system by trying to solve a set of exercises. If an exercise cannot be solved by a hypothesized d-rule, we might consider it incorrect, and try another hypothesis.
Reference: <author> Natarajan, B. </author> <year> (1991). </year> <title> Probably approximate learning of sets and functions. </title> <journal> SIAM Journal of Computing, </journal> <volume> 20 (2), </volume> <pages> 328-351. </pages>
Reference-contexts: A concept class is learnable from positive examples, with one-sided error if there is a polynomial-time algorithm that computes the MSG of a set of positive examples with respect to that concept class <ref> (Natarajan, 1991) </ref>. In the propositional case, the MSG of a set of examples can be found easily by intersecting the sets of literals of all the examples. Unfortunately, finding an existential conjunctive concept that is consistent with a set of positive examples is an NP-hard problem (Haussler, 1989).
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions of from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference-contexts: Another ILP system, FOIL, starts from the most general hypothesis and progressively specializes it by restricting the hypothesis from covering negative examples <ref> (Quinlan, 1990) </ref>. DOLPHIN (Zelle & Mooney, 1993) and Grasshopper (Leckie & Zukerman, 1993) are two systems that use FOIL for learning control knowledge for a problem solver. FOIL is difficult to adapt to our work because it needs negative examples.
Reference: <author> Ruby, D., & Kibler, D. </author> <year> (1991). </year> <title> Learning subgoal sequences for planning. </title> <booktitle> In Proceedings of AAAI-91. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: The SteppingStones approach by Ruby and Kibler also learns subgoal sequences, which are similar to d-rules <ref> (Ruby & Kibler, 1991) </ref>. However, like in EBL, their system learns by generalizing single examples.
Reference: <author> Shavlik, J. </author> <year> (1990). </year> <title> Acquiring recursive and iterative concepts in explanation-based learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 39-70. </pages>
Reference: <author> Sterling, L., & Shapiro, E. </author> <year> (1986). </year> <title> The Art of Prolog. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Fortunately, the answer to this question is no, because we use a Horn-clause theory with negation-as-failure to introduce these abstract literals. As a consequence of the existence of unique minimal models for Horn-clause theories <ref> (Sterling & Shapiro, 1986) </ref>, every subset query may still be replaced with a single membership query, by using an instantiated version of the hypothesis. To illustrate this algorithm, consider our running example to be the first training example.
Reference: <author> Subramanian, D., & Feldman, R. </author> <year> (1990). </year> <title> The utility of EBL in recursive domain theories. </title> <booktitle> In Proceedings of AAAI-90 Menlo Park, </booktitle> <address> CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Zelle, J., & Mooney, R. </author> <year> (1993). </year> <title> Combining FOIL and EBG to speedup logic programs. </title> <booktitle> In Proceedings of the 13th IJCAI, </booktitle> <pages> pp. 1106-1111. </pages>
Reference-contexts: Another ILP system, FOIL, starts from the most general hypothesis and progressively specializes it by restricting the hypothesis from covering negative examples (Quinlan, 1990). DOLPHIN <ref> (Zelle & Mooney, 1993) </ref> and Grasshopper (Leckie & Zukerman, 1993) are two systems that use FOIL for learning control knowledge for a problem solver. FOIL is difficult to adapt to our work because it needs negative examples.
References-found: 23

