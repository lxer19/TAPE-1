URL: http://www.cs.cornell.edu/Info/People/coleman/PAPERS/CS97_1649.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/coleman/papers.html
Root-URL: 
Title: EXTERIOR NEWTON METHOD FOR CONVEX QUADRATIC PROGRAMMING  
Author: THOMAS F. COLEMANy AND JIANGUO LIUz 
Keyword: Key words: Convex quadratic programming, exterior methods, Newton methods, dual problems.  
Note: AN  
Abstract: CORNELL COMPUTER SCIENCE TECNICAL REPORT: 97-1649 Abstract. We propose an exterior Newton method for convex quadratic programming (QP) problems. This method is based on a dual formulation: a sequence of points is generated which monotonically decreases the dual objective function. We show that the generated sequence converges globally and quadratically to the solution (if the QP is feasible). Measures for detecting infeasibility are provided. The major computation in each iteration is to solve a KKT-like system. Therefore, given an effective symmetric sparse linear solver, the proposed method is suitable for large sparse problems. Preliminary numerical results are presented. 1. Introduction. We consider convex quadratic programming (QP) problem of the form: 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, J. E. Hopcroft and J. D. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1974. </year>
Reference-contexts: Therefore (3.26) follows from (3.16). Lemma 3.5. For each iteration, O (nlog n) comparisons and O (n) arithmetic operations are required to calculate ff opt . Proof. First, we can sort the vector r to get fi and p in O (nlog n) comparisons (see, e.g., Page 271 of <ref> [1] </ref>). <p> The matrix A was generated as follows: given m = l 2 and n &gt; m, we partition the unit square <ref> [0; 1] </ref> fi [0; 1] into an (l + 2) fi (l + 2) evenly spaced grid, and obtain m = l 2 interior nodes (mesh points). For each node there is a first order spline basis function (the pyramid function) i (1 i m). <p> The matrix A was generated as follows: given m = l 2 and n &gt; m, we partition the unit square <ref> [0; 1] </ref> fi [0; 1] into an (l + 2) fi (l + 2) evenly spaced grid, and obtain m = l 2 interior nodes (mesh points). For each node there is a first order spline basis function (the pyramid function) i (1 i m).
Reference: [2] <author> T. J. Carpenter and D. F. Shanno, </author> <title> An interior point method for quadratic programs based on conjugate projected gradients, </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 (1993), </volume> <pages> pp. 5-28. </pages>
Reference-contexts: In addition, we try to improve the convergence results. Specifically, we prove global convergence without assuming strict complementarity, and quadratic (instead of superlinear) convergence is established in this paper. There are numeours studies on solving convex QP. For example, one may see <ref> [2] </ref>, [10], [11], [13], [14], [15], [16], [17], [21], and [22]. More references may be found in [4] and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian.
Reference: [3] <author> T. F. Coleman and A. R. Conn, </author> <title> Second order conditions for an exact penalty function, </title> <journal> Mathematical Programming, </journal> <volume> 19 (1980), </volume> <pages> pp. 178-185. </pages>
Reference-contexts: Since H &gt; 0 and the Schur complement of H 1 in H is a zero matrix, by Theorem 1 in [12], the matrix H 0. So the piecewise quadratic function f defined in (1.1) is convex. Therefore, by (1.2), (2.3), Corollary 1 in <ref> [3] </ref>, and the linear independence of fe i : i 2 A fl g, (y fl ; w fl ) is a solution to (2.1). Now assume (y fl ; w fl ) is a solution to (2.1). Then by Corollary 1 in [3], there exits a vector fl , 1 <p> Therefore, by (1.2), (2.3), Corollary 1 in <ref> [3] </ref>, and the linear independence of fe i : i 2 A fl g, (y fl ; w fl ) is a solution to (2.1). Now assume (y fl ; w fl ) is a solution to (2.1). Then by Corollary 1 in [3], there exits a vector fl , 1 fl i 1 for every i 2 A fl , such that " AH 1 (y fl c + A T w fl ) b = i2A fl c sign (y fl i ) e i # X fl " 0 :(2.4) Define
Reference: [4] <author> T. F. Coleman and L. A. Hulbert, </author> <title> A globally and superlinearly convergent algorithm for convex quadratic programming with simple bounds, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3 (1993), </volume> <pages> 298-321. </pages>
Reference-contexts: Hence techniques for unconstrained minimization can be adapted. Moreover, the major computation in each iteration is to solve a KKT-like system. Therefore, applicability to large sparse problems depends on the ability to effectively solve large sparse symmetric linear systems. In <ref> [4] </ref>, a new exterior method is proposed to solve convex QP with simple bounds. <p> Global and superlinear convergence results are established and the potential of such methods is demonstrated by the results of numerical experiments. Our method is an extension of the exterior method proposed in <ref> [4] </ref>. However, this extension is by no means trivial. For example, feasibility is not an issue for simple bounds but is important for problem (1.1). In addition, we try to improve the convergence results. <p> There are numeours studies on solving convex QP. For example, one may see [2], [10], [11], [13], [14], [15], [16], [17], [21], and [22]. More references may be found in <ref> [4] </ref> and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian.
Reference: [5] <author> J. E. Dennis, Jr. and R. B. Schnabel, </author> <title> numerical methods for unconstrained optimization and nonlinear equations, </title> <publisher> (Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983). </year>
Reference-contexts: To show quadratic convergence, we cite a standard result (see, e.g., <ref> [5] </ref>), used subsequently. Theorem 5.2. Let D &lt; l be an open convex set. Let v fl 2 D, F : &lt; l ! &lt; l , F (v fl ) = 0, rF (v fl ) be nonsingular, and rF be Lipschitz continuous at v fl in D.
Reference: [6] <author> R. Fletcher, </author> <title> Practical Methods of Optimization, Second Edition, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Measures for detecting infeasibility are provided. Exterior methods have the advantage that no feasible starting point is required. They are especially suitable for problems where feasibility is uncertain. Many of the well-known penalty function methods for nonlinear programming are exterior methods (see, e.g., <ref> [6] </ref>) and [7]). However, in most cases determining an appropriate penalty parameter in each iteration can be difficult. Our proposed method is penalty-parameter-free, and is based on a dual formulation of problem (1.1): a sequence of points is generated which monotonically decrease the dual objective function.
Reference: [7] <author> P. E. Gill and W. Murray and M. H. Wright, </author> <title> Practical Optimization, </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: Measures for detecting infeasibility are provided. Exterior methods have the advantage that no feasible starting point is required. They are especially suitable for problems where feasibility is uncertain. Many of the well-known penalty function methods for nonlinear programming are exterior methods (see, e.g., [6]) and <ref> [7] </ref>). However, in most cases determining an appropriate penalty parameter in each iteration can be difficult. Our proposed method is penalty-parameter-free, and is based on a dual formulation of problem (1.1): a sequence of points is generated which monotonically decrease the dual objective function.
Reference: [8] <author> P. E. Gill and W. Murray and M. H. Wright, </author> <title> Numerical Linear Algebra and Optimization, </title> <journal> Vol. </journal> <volume> 1, </volume> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1991. </year> <month> 24 </month>
Reference-contexts: be shown that = ; (the empty set) if and only if ~ := f~x 2 &lt; 2n : ~ A~x = ~ b; ~x 0 g = ;; where ~ A = A 0 # " 2e 2 &lt; m+n : By Farkas' Lemma (see, e.g., Lemma 7.7.2 in <ref> [8] </ref>), if ~ = ;, then there exist w 2 &lt; m and ~w 2 &lt; n such that b T w + k A T w + ~wk 1 + k ~wk 1 &lt; 0: Thus by (2.6), along the ray (c tA T ; tw) (t 2 &lt;), we
Reference: [9] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> second edition, </address> <year> 1989. </year>
Reference-contexts: Then H is positive definite. Proof. Let D Y := jY j 1 2 D 2 . Clearly, D Y &gt; 0 whenever &gt; 0 and y i 6= 0 for all 1 i n. By the Sherman-Morrison-Woodbury formula (see, e.g., <ref> [9] </ref>), we have (H 1 + D 2 Y ) 1 = H HD Y (I +D Y HD Y ) 1 D Y H.
Reference: [10] <author> D. Goldfarb, </author> <title> Extensions of Newton's method and simplex methods for solving quadratic programs, In Numerical Methods for nonlinear Optimization, </title> <editor> F. A. Lootsman, eds., </editor> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1972. </year>
Reference-contexts: In addition, we try to improve the convergence results. Specifically, we prove global convergence without assuming strict complementarity, and quadratic (instead of superlinear) convergence is established in this paper. There are numeours studies on solving convex QP. For example, one may see [2], <ref> [10] </ref>, [11], [13], [14], [15], [16], [17], [21], and [22]. More references may be found in [4] and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian.
Reference: [11] <author> D. Goldfarb and S. Liu, </author> <title> An O(n 3 L) primal interior point algorithm for convex quadratic programming, </title> <journal> Mathematical Programming, </journal> <volume> 49 (1991), </volume> <pages> pp. 325-340. </pages>
Reference-contexts: In addition, we try to improve the convergence results. Specifically, we prove global convergence without assuming strict complementarity, and quadratic (instead of superlinear) convergence is established in this paper. There are numeours studies on solving convex QP. For example, one may see [2], [10], <ref> [11] </ref>, [13], [14], [15], [16], [17], [21], and [22]. More references may be found in [4] and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian.
Reference: [12] <author> E. V. Haynsworth, </author> <title> Determination of the inertia of a partitioned Hermitian matrix, Linear Algebra and its Applications, </title> <booktitle> 1 (1968), </booktitle> <pages> pp. 73-81. </pages>
Reference-contexts: Since H &gt; 0 and the Schur complement of H 1 in H is a zero matrix, by Theorem 1 in <ref> [12] </ref>, the matrix H 0. So the piecewise quadratic function f defined in (1.1) is convex. Therefore, by (1.2), (2.3), Corollary 1 in [3], and the linear independence of fe i : i 2 A fl g, (y fl ; w fl ) is a solution to (2.1). <p> Since A has full row rank, we see that the Schur complement of H 1 +D 2 Y AH 1 A T AH 1 (H 1 + D 2 = AD Y (I + D Y HD Y ) D Y A &gt; 0: Therefore, by Corollary 1 in <ref> [12] </ref>, H &gt; 0. Corollary 3.2. Suppose &gt; 0. Then at any point (y; w) satisfying y i 6= 0 for every 1 i n, the solution s of (3.6) is a descent direction for f . <p> Therefore, the equation G fl z = 0 can be reduced to 2 4 I c fl fiI c fl (H 1 A T ) T 3 5 6 z 1 z 2 7 Using Corollary 1 in <ref> [12] </ref> and the fact that rank (A T I fl ) = m, we see that the coefficient matrix in (5.3) is positive definite. Consequently (z 1 I c fl ; z 2 ) = 0.
Reference: [13] <author> D. Hertog and C. Roos and T. Terlaky, </author> <title> A polynomial method of weighted centers for convex quadratic programming, </title> <journal> J. Inform. Optim. Sci., </journal> <volume> 12 (1991), </volume> <pages> pp. 187-205. </pages>
Reference-contexts: In addition, we try to improve the convergence results. Specifically, we prove global convergence without assuming strict complementarity, and quadratic (instead of superlinear) convergence is established in this paper. There are numeours studies on solving convex QP. For example, one may see [2], [10], [11], <ref> [13] </ref>, [14], [15], [16], [17], [21], and [22]. More references may be found in [4] and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian.
Reference: [14] <author> W. Li and J. Swetits, </author> <title> A new algorithm for solving strictly convex quadratic programs, </title> <note> SIAM Journal on Optimization, to appear. </note>
Reference-contexts: In addition, we try to improve the convergence results. Specifically, we prove global convergence without assuming strict complementarity, and quadratic (instead of superlinear) convergence is established in this paper. There are numeours studies on solving convex QP. For example, one may see [2], [10], [11], [13], <ref> [14] </ref>, [15], [16], [17], [21], and [22]. More references may be found in [4] and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian.
Reference: [15] <author> S. Mehrotra and J. Sun, </author> <title> An algorithm for convex quadratic programming that requires O(n 3:5 L) arithmetic operations, </title> <journal> Math. Oper. Res., </journal> <note> 15 (1990), </note> <author> pp.342-363. [16] , K. Madsen and H. Nielsen and M. Pinar, </author> <title> A new finite continuation algorithm for bound constrained quadratic programming Tech. </title> <type> Report, </type> <institution> IMM-REP-1995-22, Institute of Mathematical Modelling, Technical University of Denmark, </institution> <year> 1995. </year>
Reference-contexts: In addition, we try to improve the convergence results. Specifically, we prove global convergence without assuming strict complementarity, and quadratic (instead of superlinear) convergence is established in this paper. There are numeours studies on solving convex QP. For example, one may see [2], [10], [11], [13], [14], <ref> [15] </ref>, [16], [17], [21], and [22]. More references may be found in [4] and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian.
Reference: [17] <author> R. Monteiro and I. Adler, </author> <title> Interior path-following primal-dual algorithms, part II: Convex quadratic programming, </title> <journal> Mathematical Programming, </journal> <volume> 44 (1989), </volume> <pages> pp. 43-66. </pages>
Reference-contexts: In addition, we try to improve the convergence results. Specifically, we prove global convergence without assuming strict complementarity, and quadratic (instead of superlinear) convergence is established in this paper. There are numeours studies on solving convex QP. For example, one may see [2], [10], [11], [13], [14], [15], [16], <ref> [17] </ref>, [21], and [22]. More references may be found in [4] and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian.
Reference: [18] <author> J. J. More and D. Sorensen, </author> <title> Computing a trust region step, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 4 (1983), </volume> <pages> pp. 553-572. </pages>
Reference-contexts: Since (1.1) has a unique solution, we must have x = ^x. If jx i j &gt; 1 for some 1 i n and if fx k g does not converge to x, then by Lemma 4.10 in <ref> [18] </ref>, there exists a subsequence fx k l g which converges to x and kx k l +1 x k l k * for some fixed * &gt; 0 (for all k l ).
Reference: [19] <author> J. J. More and G. Toraldo, </author> <title> Algorithms for bound constrained quadratic programming problems, </title> <journal> Numerische Mathematik, </journal> <volume> 55 (1989), </volume> <pages> pp. 377-400. </pages>
Reference-contexts: Numerical Experiments Preliminary numerical tests were conducted on 4 sets of problems using Algorithm Exterior-Newton which is described in Section 3.1. For each test, the More-Toraldo <ref> [19] </ref> QP generator was adapted to generate the quadratic coefficient matrix H. The matrix A was generated in different ways which will be specified later. The test results are reported in Tables 1, 2, 3, and 4, respectively. The numbers in each table are the numbers of iterations.
Reference: [20] <author> D. Sorensen, </author> <title> Trust region methods for unconstrained optimization, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 19 (1982), </volume> <pages> pp. 409-426. </pages>
Reference-contexts: Our idea is to compute search directions which reflect a smooth transition from the solution of (3.5) to the "modified" Newton direction, i.e., the solution of (3.4). Since a solution to (3.5) satisfies the following linear system (see, e.g., <ref> [20] </ref>): " 0 0 s = rf for some 0; we compute a search direction by solving the following equation: H s = rf;(3.6) where H = H + jY j 1 D 0 # The parameter = (y; w) is calculated by = kF (y 0 ;w 0 )k +
Reference: [21] <author> P. Wolfe, </author> <title> The simplex method for quadratic programming, </title> <journal> Econometrica, </journal> <volume> 27 (1959), </volume> <pages> pp. 382-398. </pages>
Reference-contexts: Specifically, we prove global convergence without assuming strict complementarity, and quadratic (instead of superlinear) convergence is established in this paper. There are numeours studies on solving convex QP. For example, one may see [2], [10], [11], [13], [14], [15], [16], [17], <ref> [21] </ref>, and [22]. More references may be found in [4] and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian.
Reference: [22] <author> Y. Ye, </author> <title> Interior point algorithms for quadratic programming, In Recent Developments in Mathematical Programming, </title> <editor> S. Kumar, eds., </editor> <publisher> Gordon & Beach Scientific Publishers, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: Specifically, we prove global convergence without assuming strict complementarity, and quadratic (instead of superlinear) convergence is established in this paper. There are numeours studies on solving convex QP. For example, one may see [2], [10], [11], [13], [14], [15], [16], [17], [21], and <ref> [22] </ref>. More references may be found in [4] and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian. <p> There are numeours studies on solving convex QP. For example, one may see [2], [10], [11], [13], [14], [15], [16], [17], [21], and <ref> [22] </ref>. More references may be found in [4] and [22]. Given a function f : &lt; n ! &lt;, we use rf to denote the gradient of f and r 2 f to denote the Hessian.
References-found: 21

