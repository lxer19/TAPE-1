URL: http://www.cs.umn.edu/Research/Agassiz/Paper/huang.mascots98.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: huangj@cs.umn.edu  lilja@ece.umn.edu  
Title: An Efficient Strategy for Developing a Simulator for a Novel Concurrent Multithreaded Processor Architecture  
Author: Jian Huang David J. Lilja 
Keyword: execution-driven simulation, processor design, multithreading, superthreading, system modeling.  
Date: December 11, 1997  
Address: Minneapolis, MN 55455 U.S.A.  Minneapolis, MN 55455 U.S.A.  
Affiliation: Department of Computer Science and Engineering University of Minnesota  Department of Electrical and Computer Engineering University of Minnesota  
Abstract: In developing a simulator for a new processor architecture, it often is not clear whether it is more efficient to write a new simulator from scratch or whether it is better to try and modify an existing simulator. Writing a new simulator has the advantages of needing to understand only the new processor architecture itself and being the "owner" of the final code. However, it also forces the processor architect to develop or adapt all of the related software tools, such as compilers, assemblers, and so forth. On the other hand, modifying an existing simulator and related tools can be time-consuming and error-prone since the simulator writer must learn the details of someone else's code which is, typically, not well documented. We describe the SImulator for Multithreaded Computer Architectures (SIMCA) that was developed with the primary goal of obtaining a functional simulator as quickly as possible to begin evaluating the superthreaded architecture being developed at the University of Minnesota [2, 3]. The performance of the simulator itself was important, but secondary. We achieved our goal using a technique we call process-pipelining that exploits the unique features of this new architecture to hide the details of the underlying simulator, which is based on the SimpleScalar tool set [4]. This approach allowed us to quickly produce a functional simulator whose performance was only a factor of 3.8 to 4.9 times slower than the base SimpleScalar simulator. Because of the modularity of this approach, some simple optimizations allowed us to improve its performance by a factor of two with little effort. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. M. </author> <title> Amdahl. "Validity of the single processor approach to achieving large scale computing capabilities,". </title> <booktitle> Proc. AFIPS 1967 Spring Joint Computer Conf., </booktitle> <address> Atlantic City, N.J., </address> <pages> 483-485. </pages>
Reference-contexts: Furthermore, each thread processing unit is capable of issuing several instructions per cycle. If the average effective instruction issue rate is about two, there will be a load or store instruction every two to three cycles. Thus, using a straight-forward application of Amdahl's Law <ref> [1] </ref> we find that the serialization of accesses to the memory system limits the maximum possible speedup for this simulator to no more than a factor of two to five times. 7 Conclusion The primary goal in developing this simulator was to provide as quickly as possible a platform on which
Reference: [2] <author> J. Tsai, P.-C. Yew. </author> <title> "The Superthreaded Architecture: Thread Pipelining with Run-Time Data Dependence Checking and Control Speculation". </title> <booktitle> In the Proceedings of Int'l Conf. on Parallel Architecture and Compiler Techniques (PACT'96), </booktitle> <year> 1996. </year>
Reference-contexts: In this paper, we describe our strategy for developing a simulator for the superthreaded processor architecture <ref> [2, 3] </ref> that takes advantage of the structure of this proposed architecture to hide many of the nasty details of the the selected tool set. In particular, we developed the SImulator for Multithreaded Computer Architectures (SIMCA) on top of the SimpleScalar Tool set. <p> Sections 3 and 4 then describe the details of the simulator, while Section 5 evaluates its performance. Section 6 summarizes and concludes the paper. 2 2 Background: The Superthreaded Architecture Similar to other concurrent multiple-threaded architectures, the superthreaded architecture <ref> [2, 3] </ref> exploits task-level parallelism using multiple threads of control. The superthreaded processor typically consists of a number of thread processing units with shared instruction and data caches, as shown in Figure 1.
Reference: [3] <author> Jenn-Yuan Tsai, Zhenzhen Jiang, Zhiyuan Li, David J. Lilja, Xin Wang, Pen-Chung Yew, Bixia Zheng, Stephen J. Schwinn, and Robert Glamm. </author> <title> "Integrating Parallelizing Compilation Technology and Processor Architecture for Cost-Effective Concurrent Multithreading". </title> <journal> Journal of Information Science and Engineering, </journal> <note> Special Issue on Compiler Techniques for High-Performance Computing, </note> <month> March </month> <year> 1998. </year>
Reference-contexts: In this paper, we describe our strategy for developing a simulator for the superthreaded processor architecture <ref> [2, 3] </ref> that takes advantage of the structure of this proposed architecture to hide many of the nasty details of the the selected tool set. In particular, we developed the SImulator for Multithreaded Computer Architectures (SIMCA) on top of the SimpleScalar Tool set. <p> Sections 3 and 4 then describe the details of the simulator, while Section 5 evaluates its performance. Section 6 summarizes and concludes the paper. 2 2 Background: The Superthreaded Architecture Similar to other concurrent multiple-threaded architectures, the superthreaded architecture <ref> [2, 3] </ref> exploits task-level parallelism using multiple threads of control. The superthreaded processor typically consists of a number of thread processing units with shared instruction and data caches, as shown in Figure 1.
Reference: [4] <author> D. Burger, T. Austin and S. Bennett. </author> <title> Evaluating Future Microprocessors: the Simple Scalar Tool Set. </title> <address> http://www.cs.wisc.edu/~ mscalar/simplescalar.html </address>
Reference-contexts: Since these features can substantially increase the complexity of a processor, the study of these novel architectures is based more and more on detailed simulations. Previous research [9] has shown that execution-driven simulation tools, such as the SimpleScalar tool set <ref> [4] </ref>, FastMIPS [7], MINT [5], and CacheMire [6], provide better evaluations for these new designs than traditional trace-driven simulations. Unfortunately, these simulators can be quite complex to develop in themselves. <p> After the head thread exits the loop, it performs its write-back operation and executes a wait wb done instruction before initiating the next portion of the program. 3 Organization of SIMCA The SimpleScalar tool set <ref> [4] </ref> comes with its own versions of gcc, gas, and gld, the GNU implementations of a C compiler, an assembler, and a loader. The tool kit also comes with its own versions of the standard UNIX libraries. We call these SimpleScalar modified tools ss-gcc, ss-gas, and ss-gld.
Reference: [5] <author> J. E. Veenstra and R. J. Fowler. </author> <title> MINT Tutorial and User Manual. </title> <type> Technical Report 452, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> June </month> <year> 1993. </year> <month> 15 </month>
Reference-contexts: Since these features can substantially increase the complexity of a processor, the study of these novel architectures is based more and more on detailed simulations. Previous research [9] has shown that execution-driven simulation tools, such as the SimpleScalar tool set [4], FastMIPS [7], MINT <ref> [5] </ref>, and CacheMire [6], provide better evaluations for these new designs than traditional trace-driven simulations. Unfortunately, these simulators can be quite complex to develop in themselves.
Reference: [6] <author> M. Brorsson, F. Dahlgren, H. Nilsson, and P. Stenstrom. </author> <title> "The CacheMire Test Bench A Flexible and Effective Approach for Simulation of Multiprocessors". </title> <booktitle> In the Proceedings of 26th Annual Simulation Symposium, </booktitle> <pages> pp. 41-49, </pages> <month> March, </month> <year> 1993. </year>
Reference-contexts: Since these features can substantially increase the complexity of a processor, the study of these novel architectures is based more and more on detailed simulations. Previous research [9] has shown that execution-driven simulation tools, such as the SimpleScalar tool set [4], FastMIPS [7], MINT [5], and CacheMire <ref> [6] </ref>, provide better evaluations for these new designs than traditional trace-driven simulations. Unfortunately, these simulators can be quite complex to develop in themselves.
Reference: [7] <author> FAST-MIPS v5.0. </author> <note> http://www-mount.ee.umn.edu/mcerg/fast-mips/. </note>
Reference-contexts: Since these features can substantially increase the complexity of a processor, the study of these novel architectures is based more and more on detailed simulations. Previous research [9] has shown that execution-driven simulation tools, such as the SimpleScalar tool set [4], FastMIPS <ref> [7] </ref>, MINT [5], and CacheMire [6], provide better evaluations for these new designs than traditional trace-driven simulations. Unfortunately, these simulators can be quite complex to develop in themselves.
Reference: [8] <author> J. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: A quantitative Approach, Second Edition, 1996. </title> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: For many application programs, it is not unusual for one out of every five or six instructions to be a load or store <ref> [8] </ref>. Furthermore, each thread processing unit is capable of issuing several instructions per cycle. If the average effective instruction issue rate is about two, there will be a load or store instruction every two to three cycles.
Reference: [9] <author> R. C.Covington, S. Madala, V. Mehta, J. R. Jump, and J. b. Sinclair. </author> <title> "The Rice Parallel Processing Testbed". </title> <booktitle> In ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <year> 1988. </year> <month> 16 </month>
Reference-contexts: Since these features can substantially increase the complexity of a processor, the study of these novel architectures is based more and more on detailed simulations. Previous research <ref> [9] </ref> has shown that execution-driven simulation tools, such as the SimpleScalar tool set [4], FastMIPS [7], MINT [5], and CacheMire [6], provide better evaluations for these new designs than traditional trace-driven simulations. Unfortunately, these simulators can be quite complex to develop in themselves.
References-found: 9

