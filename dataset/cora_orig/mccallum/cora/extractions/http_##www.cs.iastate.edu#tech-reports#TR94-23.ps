URL: http://www.cs.iastate.edu/tech-reports/TR94-23.ps
Refering-URL: http://www.cs.iastate.edu/tech-reports/catalog.html
Root-URL: http://www.cs.iastate.edu
Title: Parametric Micro-level Performance Models for Parallel Computing TR94-23  
Author: Youngtae Kim, Mark Fienup, Jeffrey C. Clary Suresh C. Kothari 
Address: 226 Atanasoff Ames, IA 50011  
Affiliation: Iowa State University of Science and Technology Department of Computer Science  
Date: December 5, 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Bronson, E. C., and Casavant, T. L. </author> <title> Experimental Application-Driven Architecture Analysis of an SIMD/MIND Parallel Processing system. </title> <journal> IEEE Trans. Parallel Distrib. Systems 1, </journal> <month> 2 (Apr. </month> <year> 1990), </year> <pages> 195-205. </pages>
Reference-contexts: As concrete illustrations, we present models for matrix multiplication, LU decomposition, and fast Fourier transform (FFT), all implemented on a 2-D processor array. These algorithms are of considerable interest in practice; individually, they have been used as examples in many empirical and theoretical studies <ref> [19, 3, 1, 16, 6] </ref>. Together, the algorithms represent varying degrees of computation, communication, and memory requirements, and serve well as test cases. PM models are validated, and their utility is demonstrated in a case study on Mas-Par MP-1 and MP-2. <p> Shift the B matrix to North M 2 P (T load + mT Xt + T store ) + dM 2 =LeP T Xs Note : For the shift communication on MasPar, xnet <ref> [1] </ref> is used. * Computation time f comp = P M 3 (T mult + T add ) * Communication time f comm = 2M 2 P mT Xt + 2dM 2 =LeP T Xs * Memory access time f mem = P [2M 3 T load + M 2 (3T
Reference: [2] <author> Carmona, E. A., and Rice, M. D. </author> <title> Modeling the Serial and Parallel Fractions of a Parallel Algorithm. </title> <journal> J. Parallel Distrib. Comput. </journal> <volume> 13 (1991), </volume> <pages> 286-298. </pages>
Reference-contexts: The papers [19, 3] discuss shortcomings of earlier theoretical research, and propose new models called BSP and LogP for parallel computation. An important aspect of both models is the incorporation of communication parameters which were ignored in earlier theoretical research. The studies <ref> [2, 7, 8, 18] </ref> address several pragmatic issues and provide insights into important attributes of parallel performance. A good introduction to performance and scalability of parallel systems is provided in recent books [10, 12]. This paper is about parametric micro-level (PM) performance models for parallel computation. <p> However, in parallel computing, efficiency is commonly defined as the speedup divided by the number of processors. The isoefficiency analysis [12, 13] is based on this definition. It has been argued in <ref> [2] </ref> that instead of relying on time as a measure of work, efficiency should be defined by using unit counts based on the size of an indivisible task as the measure of work. The ratio of work accomplished (wa) to the work expended (we) is proposed in [2] as the alternative <p> been argued in <ref> [2] </ref> that instead of relying on time as a measure of work, efficiency should be defined by using unit counts based on the size of an indivisible task as the measure of work. The ratio of work accomplished (wa) to the work expended (we) is proposed in [2] as the alternative definition of efficiency. Following these ideas, consider a normalized FLOP as the unit of work. There are some objections to using FLOP as a unit of work in general [8]. <p> Ef f (N ) = f comp (N) f comp (N)+f comm (N)+f misc (N)+f mem (N) fi LB f (N ) Interestingly, for examples provided in <ref> [2] </ref>, the commonly used definition and the alternate definition of efficiency both led to the same results. The following observation may explain why it is so.
Reference: [3] <author> Culler, D., Karp, R., Patterson, D., Sahay, A., Schauser, K. E., Santos, E., Sub-ramonian, R., and Eicken, T. V. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> Proc. 4th ACM SIGPLAN symp. Principles and Practices of Parallel Programming. </booktitle> <year> 1993, </year> <pages> pp. 1-12. </pages>
Reference-contexts: 1 Introduction How to model parallel computation has been an important topic of research in high-performance computing. Performance models have been extensively investigated through theoretical and empirical studies. One important issue is how to make models realistic. The papers <ref> [19, 3] </ref> discuss shortcomings of earlier theoretical research, and propose new models called BSP and LogP for parallel computation. An important aspect of both models is the incorporation of communication parameters which were ignored in earlier theoretical research. <p> A good introduction to performance and scalability of parallel systems is provided in recent books [10, 12]. This paper is about parametric micro-level (PM) performance models for parallel computation. While BSP and LogP models <ref> [19, 3] </ref> focus on what is a realistic abstraction for modeling parallel performance, our emphasis is on pragmatic models to accurately predict and analyze execution times. <p> Models need to be designed with a set of parameters applicable to a wide class of parallel algorithms and architectures. Specifics enter into the picture when parameter values have to be determined. There is an example in <ref> [3] </ref> where two implementations of FFT are considered. The experimental results show a dramatic difference in communication costs of those two implementations. If a model is to predict the difference, it is inevitable that details of the implementation of algorithm have to be considered. <p> As concrete illustrations, we present models for matrix multiplication, LU decomposition, and fast Fourier transform (FFT), all implemented on a 2-D processor array. These algorithms are of considerable interest in practice; individually, they have been used as examples in many empirical and theoretical studies <ref> [19, 3, 1, 16, 6] </ref>. Together, the algorithms represent varying degrees of computation, communication, and memory requirements, and serve well as test cases. PM models are validated, and their utility is demonstrated in a case study on Mas-Par MP-1 and MP-2.
Reference: [4] <author> Fienup, M. A., and Kothari, S. C. </author> <title> Implementations of Fast Fourier Transform on the MasPar MP-1 and MP-2. </title> <type> Tech. Rep. TR 93-01, </type> <institution> Department of Computer Science, Iowa State University, </institution> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: The formulas are complex, but the advantage is that the performance predictions are very accurate. The three algorithms used in the study are well-known. The LU decomposition is described in [5]. The details of the FFT algorithm can be found in <ref> [4] </ref>. Cannon's parallel algorithm is described in [12]. The LU decomposition uses a 2-D scattered data layout for the coefficient matrix, and it includes partial pivoting. Different communication patterns are used by the three algorithms.
Reference: [5] <author> Fox, G. C., Johnson, M. A., Lyzenga, G. A., Otto, S. W., Salmon, J. K., and Walker, D. W. </author> <title> Solving Problems on concurrent Processors Vol. 1, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: These formulas are carefully derived by examining the parallel algorithm to capture all its essential details. The formulas are complex, but the advantage is that the performance predictions are very accurate. The three algorithms used in the study are well-known. The LU decomposition is described in <ref> [5] </ref>. The details of the FFT algorithm can be found in [4]. Cannon's parallel algorithm is described in [12]. The LU decomposition uses a 2-D scattered data layout for the coefficient matrix, and it includes partial pivoting. Different communication patterns are used by the three algorithms.
Reference: [6] <author> Gupta, A., and Kumar, V. </author> <title> The Scalability of FFT on Parallel Computers. </title> <journal> IEEE Trans. Parallel Distrib. Systems 4 (Aug. </journal> <year> 1993), </year> <pages> 922-932. </pages>
Reference-contexts: As concrete illustrations, we present models for matrix multiplication, LU decomposition, and fast Fourier transform (FFT), all implemented on a 2-D processor array. These algorithms are of considerable interest in practice; individually, they have been used as examples in many empirical and theoretical studies <ref> [19, 3, 1, 16, 6] </ref>. Together, the algorithms represent varying degrees of computation, communication, and memory requirements, and serve well as test cases. PM models are validated, and their utility is demonstrated in a case study on Mas-Par MP-1 and MP-2.
Reference: [7] <author> Gustafson, J. L., and Snell, Q. O. HINT: </author> <title> A New Way To Measure Computer Performance. </title> <type> Tech. Rep. </type> <institution> IS-5109, Ames Laboratory, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: The papers [19, 3] discuss shortcomings of earlier theoretical research, and propose new models called BSP and LogP for parallel computation. An important aspect of both models is the incorporation of communication parameters which were ignored in earlier theoretical research. The studies <ref> [2, 7, 8, 18] </ref> address several pragmatic issues and provide insights into important attributes of parallel performance. A good introduction to performance and scalability of parallel systems is provided in recent books [10, 12]. This paper is about parametric micro-level (PM) performance models for parallel computation.
Reference: [8] <author> Gustafson, J. L. </author> <title> The Consequences of Fixed Time Performance Measurement. </title> <booktitle> Proc. Twenty-fifth Hawaii Internat. Conf. System Sciences Vol.3. </booktitle> <year> 1992, </year> <pages> pp. 113-124. </pages>
Reference-contexts: The papers [19, 3] discuss shortcomings of earlier theoretical research, and propose new models called BSP and LogP for parallel computation. An important aspect of both models is the incorporation of communication parameters which were ignored in earlier theoretical research. The studies <ref> [2, 7, 8, 18] </ref> address several pragmatic issues and provide insights into important attributes of parallel performance. A good introduction to performance and scalability of parallel systems is provided in recent books [10, 12]. This paper is about parametric micro-level (PM) performance models for parallel computation. <p> To develop such models, we adopt a micro-level approach which incorporates precise details of interprocessor communication, memory operations, miscellaneous overheads due to auxiliary instructions, and effects of communication and computation schedules. Execution times can be predicted by fitting timing curves to experimental data, as discussed in <ref> [8] </ref>. The basic approach is to determine an algebraic expression for the fitting formula by analysis of algorithm and then determine the coefficients by experiments. This approach is closely aligned with our goals; it can accurately predict execution times. <p> The execution profiles can be used to view the performance in different ways. Other metrics such as speedup, efficiency, and MFLOPS are defined on basis of execution profiles. It is well known that performance metrics can provide different and sometimes misleading views of performance <ref> [8, 9] </ref>. We correlate various performance metrics to provide coherent views of parallel performance. PM models are appropriate for a large class of data parallel numerical algorithms, described later in the paper. <p> In our case study, single precision arithmetic is used, and the messages are four bytes each. The communication cost formulas in Table 1 are simplified in accordance with [15] to show the cost on MasPar when the message size is four bytes. Examples are cited in <ref> [8] </ref> to point out that simple overhead-type operations should not be neglected, no matter how trivial they may seem. PM models consider miscellaneous overheads arising from auxiliary instructions to implement loops in the machine language, register moves, etc. A regression formula is used to predict the miscellaneous overhead time. <p> The ratio of work accomplished (wa) to the work expended (we) is proposed in [2] as the alternative definition of efficiency. Following these ideas, consider a normalized FLOP as the unit of work. There are some objections to using FLOP as a unit of work in general <ref> [8] </ref>. In our case, however, we are considering numerical algorithms and taking into account memory and other operations separately. Another objection is that operation count is an imperfect measure of computational work since it does not standardize across computers [8]. <p> objections to using FLOP as a unit of work in general <ref> [8] </ref>. In our case, however, we are considering numerical algorithms and taking into account memory and other operations separately. Another objection is that operation count is an imperfect measure of computational work since it does not standardize across computers [8]. We agree and address this point later in the context of comparing two machines. With a normalized FLOP as the unit of work, wa is proportional to MFLOPS and we is proportional to peak M F LOP S.
Reference: [9] <author> Hennessy, J. L., and Patterson, D. A. </author> <title> Computer Architecture A Quantitative Approach, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: The execution profiles can be used to view the performance in different ways. Other metrics such as speedup, efficiency, and MFLOPS are defined on basis of execution profiles. It is well known that performance metrics can provide different and sometimes misleading views of performance <ref> [8, 9] </ref>. We correlate various performance metrics to provide coherent views of parallel performance. PM models are appropriate for a large class of data parallel numerical algorithms, described later in the paper.
Reference: [10] <author> Hwang, K. </author> <title> Advanced Computer Architecture Parallelism, Scalability, Programmability, </title> <publisher> McGraw-Hill, </publisher> <address> NY, </address> <year> 1993. </year> <month> 40 </month>
Reference-contexts: The studies [2, 7, 8, 18] address several pragmatic issues and provide insights into important attributes of parallel performance. A good introduction to performance and scalability of parallel systems is provided in recent books <ref> [10, 12] </ref>. This paper is about parametric micro-level (PM) performance models for parallel computation. While BSP and LogP models [19, 3] focus on what is a realistic abstraction for modeling parallel performance, our emphasis is on pragmatic models to accurately predict and analyze execution times.
Reference: [11] <author> Karonis, N. T. </author> <title> Timing Parallel Programs That Use Message Passing. </title> <journal> J. Parallel Distrib. Comput. </journal> <volume> 14 (1992), </volume> <pages> 29-36. </pages>
Reference-contexts: A systematic development of experimental procedures is an important and complex topic by itself. For example, a timing procedure suitable for programs that use message passing is described in <ref> [11] </ref>. To do complete justice to it is beyond the scope of this paper. 2.3 Scope and Applicability PM models are applicable to a class of numerical algorithms described as follows. First, the work done by the algorithm is characterizable as a set of floating point operations.
Reference: [12] <author> Kumar, V., Grama, A. Y., Gupta, A., and Karypis, G. </author> <title> Introduction to Parallel Computing, Design and Analysis of Algorithms, </title> <address> Benjamin/Cummings, CA, </address> <year> 1994. </year>
Reference-contexts: The studies [2, 7, 8, 18] address several pragmatic issues and provide insights into important attributes of parallel performance. A good introduction to performance and scalability of parallel systems is provided in recent books <ref> [10, 12] </ref>. This paper is about parametric micro-level (PM) performance models for parallel computation. While BSP and LogP models [19, 3] focus on what is a realistic abstraction for modeling parallel performance, our emphasis is on pragmatic models to accurately predict and analyze execution times. <p> The formulas are complex, but the advantage is that the performance predictions are very accurate. The three algorithms used in the study are well-known. The LU decomposition is described in [5]. The details of the FFT algorithm can be found in [4]. Cannon's parallel algorithm is described in <ref> [12] </ref>. The LU decomposition uses a 2-D scattered data layout for the coefficient matrix, and it includes partial pivoting. Different communication patterns are used by the three algorithms. <p> However, in parallel computing, efficiency is commonly defined as the speedup divided by the number of processors. The isoefficiency analysis <ref> [12, 13] </ref> is based on this definition. It has been argued in [2] that instead of relying on time as a measure of work, efficiency should be defined by using unit counts based on the size of an indivisible task as the measure of work.
Reference: [13] <author> Kumar, V., and Gupta, A. </author> <title> Analyzing Scalability of Parallel Algorithms and Architectures. </title> <type> Tech. Rep. </type> <institution> TR91-18, Computer Science Department, University of Minnesota, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: However, in parallel computing, efficiency is commonly defined as the speedup divided by the number of processors. The isoefficiency analysis <ref> [12, 13] </ref> is based on this definition. It has been argued in [2] that instead of relying on time as a measure of work, efficiency should be defined by using unit counts based on the size of an indivisible task as the measure of work.
Reference: [14] <author> Lam, M. S. </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines. </title> <booktitle> Proc. ACM SIGPLAN Conf. Prog. Lang. Design and Implementation. </booktitle> <year> 1988, </year> <pages> pp. 318-328. </pages>
Reference-contexts: This technique has been previously studied <ref> [14, 17] </ref> for VLIW and other architectures. The technique is commonly used on RISC workstations. On MasPar, we had to apply the technique by hand to source level programs to change the order of operations in successive iterations of a loop so that data could be prefetched.
Reference: [15] <institution> MasPar Assembly Language Reference Manual, MasPar Com puter Corporation, </institution> <address> Sunnyvale, CA, </address> <year> 1990. </year>
Reference-contexts: The pipelining is done at the bit level for each message. In our case study, single precision arithmetic is used, and the messages are four bytes each. The communication cost formulas in Table 1 are simplified in accordance with <ref> [15] </ref> to show the cost on MasPar when the message size is four bytes. Examples are cited in [8] to point out that simple overhead-type operations should not be neglected, no matter how trivial they may seem.
Reference: [16] <author> Ponnusamy, R., Thakur, R., Choudhary, A., Velamakanni, K., Bozkus, Z., and Fox, G. </author> <title> Experimental Performance Evaluation of the CM-5. </title> <journal> J. Parallel Distrib. Comput. </journal> <volume> 19 (1993), </volume> <pages> 192-202. </pages>
Reference-contexts: As concrete illustrations, we present models for matrix multiplication, LU decomposition, and fast Fourier transform (FFT), all implemented on a 2-D processor array. These algorithms are of considerable interest in practice; individually, they have been used as examples in many empirical and theoretical studies <ref> [19, 3, 1, 16, 6] </ref>. Together, the algorithms represent varying degrees of computation, communication, and memory requirements, and serve well as test cases. PM models are validated, and their utility is demonstrated in a case study on Mas-Par MP-1 and MP-2.
Reference: [17] <author> Rau, B. R., Lee, M., Tirumalai, P. P., and Schlansker, M. S. </author> <title> Register Allocation for Software Pipelined Loops. </title> <booktitle> Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: This technique has been previously studied <ref> [14, 17] </ref> for VLIW and other architectures. The technique is commonly used on RISC workstations. On MasPar, we had to apply the technique by hand to source level programs to change the order of operations in successive iterations of a loop so that data could be prefetched.
Reference: [18] <author> Sun, X. H., and Rao, V. N. </author> <title> Scalable Problems and Memory-bounded Speedup. </title> <journal> SIAM J. Scientific and Statistical Computing 11 (May 1990), </journal> <pages> 838-858. </pages>
Reference-contexts: The papers [19, 3] discuss shortcomings of earlier theoretical research, and propose new models called BSP and LogP for parallel computation. An important aspect of both models is the incorporation of communication parameters which were ignored in earlier theoretical research. The studies <ref> [2, 7, 8, 18] </ref> address several pragmatic issues and provide insights into important attributes of parallel performance. A good introduction to performance and scalability of parallel systems is provided in recent books [10, 12]. This paper is about parametric micro-level (PM) performance models for parallel computation.
Reference: [19] <author> Valiant, L. G. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Comm. ACM 33, </journal> <volume> 8(Aug. </volume> <year> 1990), </year> <pages> 103-111. 41 </pages>
Reference-contexts: 1 Introduction How to model parallel computation has been an important topic of research in high-performance computing. Performance models have been extensively investigated through theoretical and empirical studies. One important issue is how to make models realistic. The papers <ref> [19, 3] </ref> discuss shortcomings of earlier theoretical research, and propose new models called BSP and LogP for parallel computation. An important aspect of both models is the incorporation of communication parameters which were ignored in earlier theoretical research. <p> A good introduction to performance and scalability of parallel systems is provided in recent books [10, 12]. This paper is about parametric micro-level (PM) performance models for parallel computation. While BSP and LogP models <ref> [19, 3] </ref> focus on what is a realistic abstraction for modeling parallel performance, our emphasis is on pragmatic models to accurately predict and analyze execution times. <p> As concrete illustrations, we present models for matrix multiplication, LU decomposition, and fast Fourier transform (FFT), all implemented on a 2-D processor array. These algorithms are of considerable interest in practice; individually, they have been used as examples in many empirical and theoretical studies <ref> [19, 3, 1, 16, 6] </ref>. Together, the algorithms represent varying degrees of computation, communication, and memory requirements, and serve well as test cases. PM models are validated, and their utility is demonstrated in a case study on Mas-Par MP-1 and MP-2. <p> The same program is executed by all processors, but different data is processed. Within 10 each step, some processors in a MIMD machine may finish their computations earlier and remain partly idle till the next synchronization point. The concept of tight synchronization is inherent in the BSP model <ref> [19] </ref>. The BSP model considers an algorithm as a sequence of supersteps. Each superstep combines computation and communication. Many of the numerical algorithms from scientific and engineering applications fall in the category to which PM models can be applied.
References-found: 19

