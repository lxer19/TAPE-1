URL: http://www.iscs.nus.sg/~plong/papers/apple.ps
Refering-URL: 
Root-URL: 
Title: Apple Tasting and Nearly One-Sided Learning  
Author: David P. Helmbold Nicholas Littlestone Philip M. Long 
Address: U.C. Santa Cruz 4 Independence Way Klosterwiesgasse 32/2 Santa Cruz, CA 95064 Princeton, NJ 08540 A-8010 Graz, Austria  
Affiliation: Computer Science NEC Research Institute IGI, Technische Universitaet Graz  
Abstract: In the standard on-line model the learning algorithm tries to minimize the total number of mistakes made in a series of trials. On each trial the learner sees an instance, either accepts or rejects that instance, and then is told the appropriate response. We define a natural variant of this model ("apple tasting") where the learner gets feedback only when the instance is accepted. We use two transformations to relate the apple tasting model to an enhanced standard model where false acceptances are counted separately from false rejections. We present a strategy for trading between false acceptances and false rejections in the standard model. From one perspective this strategy is exactly optimal, including constants. We apply our results to obtain a good general purpose apple tasting algorithm as well as nearly optimal apple tasting algorithms for a variety of standard classes, such as conjunctions and disjunctions of n boolean variables. We also present and analyze a simpler transformation useful when the instances are drawn at random rather than selected by an adversary. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: We call each acceptance of a bad apple and each rejection of a good apple a mistake and attempt to keep the number of mistakes small. This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory <ref> [1] </ref> [4] [3] [5] [9] [10] [11] [12] [13] [14] [15]. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance. <p> A function g is consistent with a sample if for each (x; ) in the sample for which 6= fl, g (x) = . We denote the set of samples by S. A learning algorithm A is a computable function from S fi X fi <ref> [0; 1] </ref> 1 to f0; 1g. That is, given a sample, an element x of X and a source of numbers in [0; 1], A outputs an element of f0; 1g, which might be interpreted as a prediction of f (x). <p> We denote the set of samples by S. A learning algorithm A is a computable function from S fi X fi <ref> [0; 1] </ref> 1 to f0; 1g. That is, given a sample, an element x of X and a source of numbers in [0; 1], A outputs an element of f0; 1g, which might be interpreted as a prediction of f (x). Elements of the infinite sequence that is the final argument of A will be generated one by one independently at random as needed by A. <p> Elements of the infinite sequence that is the final argument of A will be generated one by one independently at random as needed by A. We are particularly interested in the random source R which generates numbers by sampling from the uniform distribution on <ref> [0; 1] </ref>. We assume that R takes unit time to generate each random number. <p> On the tth trial: * B receives x t and passes it on to A. * A predicts ~ t . * B calls R obtaining r 2 <ref> [0; 1] </ref> and predicts t = 1 if ~ t = 1 or ~ t = 0 and r M =T 0 otherwise. * If t = 0, then, loosely speaking, A acts as if this trial never happened, i.e., A's future predictions will be functions only of the subsequence of
Reference: [2] <author> J. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer Verlag, </publisher> <year> 1980. </year>
Reference-contexts: This model contains a trade-off, sometimes referred to as the exploration-exploitation trade-off, that is absent in the standard on-line model. We refer to the model that we consider here as the apple tasting model. This can be formalized using the language of decision theory (c.f., <ref> [2] </ref>) as follows.
Reference: [3] <author> A. Blum. </author> <title> Learning boolean functions in an infinite attribute space. </title> <booktitle> Proceedings of the 22nd ACM Symposium on the Theory of Computation, </booktitle> <year> 1990. </year>
Reference-contexts: We call each acceptance of a bad apple and each rejection of a good apple a mistake and attempt to keep the number of mistakes small. This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory [1] [4] <ref> [3] </ref> [5] [9] [10] [11] [12] [13] [14] [15]. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance.
Reference: [4] <author> A. Blum. </author> <title> Separating PAC and mistake-bound learn ing models over the boolean domain. </title> <booktitle> Proceedings of the 31st Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1990. </year>
Reference-contexts: We call each acceptance of a bad apple and each rejection of a good apple a mistake and attempt to keep the number of mistakes small. This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory [1] <ref> [4] </ref> [3] [5] [9] [10] [11] [12] [13] [14] [15]. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance.
Reference: [5] <author> A. Blum, L. Hellerstein, and N. Littlestone. </author> <title> Learn ing in the presence of finitely many or infinitely many irrelevant attributes. </title> <booktitle> The 1991 Workshop on Computational Learning Theory, </booktitle> <year> 1991. </year>
Reference-contexts: This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory [1] [4] [3] <ref> [5] </ref> [9] [10] [11] [12] [13] [14] [15]. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance.
Reference: [6] <author> W. Feller. </author> <title> An Introduction to Probability and its Ap plications, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> third edition, </address> <year> 1968. </year>
Reference-contexts: Note that jS 0;0;1 j is at most the number of failures before jS 0;1;1 j successes in independent Bernoulli trials with probability p M =T of success, so (see, e.g. <ref> [6] </ref>), E (jS 0;0;1 j) jS 0;1;1 j T jS 0;1;1 j T M M : Combining this with (2) and (1) yields the desired result. 2 This trivially yields the following Corollary, in which we remove the restriction that T M , and record an apple tasting bound in terms
Reference: [7] <author> D. Haussler, N. Littlestone, and M. Warmuth. </author> <title> Pre dicting f0; 1g functions on randomly drawn points. </title> <type> Technical report, </type> <institution> University of California at Santa Cruz, </institution> <year> 1990. </year>
Reference-contexts: Certain other concept classes can be learned in the standard model by algorithms that make no false-negative mistakes. These (stan-dard model) algorithms determine the unique largest f 2 F consistent with the examples and expect to make a number O (d ln T ) mistakes (see <ref> [16, 9, 7] </ref>). We can easily take full advantage of this fact, obtaining algorithms with bounds that grow as ln T rather than p T . Taking full advantage of the possible trade-offs between false negatives and false positives remains an open problem. <p> This hypothesis is then used to predict on the rest of the examples, ignoring any additional feedback. Standard doubling techniques can be used when T is unknown. We will make use of the 1-inclusion graph learning strategy given by Haussler, Littlestone, and Warmuth <ref> [7] </ref>. This algorithm gives a mapping B from sequences of examples to hypotheses such that the expected error of the hypothesis is bounded as described in the following lemma. Lemma 14 ([7]) There is a computable mapping B from sequences of elements in X fi f0; 1g to functions from X <p> Certain target classes can be effectively learned in the standard on-line model by algorithms that make no false-negative mistakes <ref> [16, 9, 7] </ref>. As these algorithms are always correct when they predict 0, they can be used without modification as apple tasting algorithms. For such a target class F, RALC (F ; T ) is O (d ln T ).
Reference: [8] <author> D. Helmbold, N. Littlestone, and P. M. </author> <title> Long. On-line learning with linear loss constraints. </title> <note> In preparation, </note> <year> 1992. </year>
Reference-contexts: Surprisingly, this lower bound exactly matches the upper bound given by our generic algorithms. These results on trading off false-positives and false-negatives are of independent interest, in addition to their apple tasting application. We are currently working on generalizing these results <ref> [8] </ref>. By converting the generic algorithms, we obtain a general result showing that there is an apple tasting algorithm for any concept class F which, for any * &gt; 0, expects to make O p ln T mistakes on any sequence of T trials (Theorem 13). <p> The following was proved <ref> [8] </ref> using techniques like those used to prove a similar theorem which doesn't differentiate between positive and negative mistakes [11].
Reference: [9] <author> D. Helmbold, R. Sloan, and M. Warmuth. </author> <title> Learn ing nested differences of intersection-closed concept classes. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 165-196, </pages> <year> 1990. </year>
Reference-contexts: This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory [1] [4] [3] [5] <ref> [9] </ref> [10] [11] [12] [13] [14] [15]. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance. <p> Certain other concept classes can be learned in the standard model by algorithms that make no false-negative mistakes. These (stan-dard model) algorithms determine the unique largest f 2 F consistent with the examples and expect to make a number O (d ln T ) mistakes (see <ref> [16, 9, 7] </ref>). We can easily take full advantage of this fact, obtaining algorithms with bounds that grow as ln T rather than p T . Taking full advantage of the possible trade-offs between false negatives and false positives remains an open problem. <p> Certain target classes can be effectively learned in the standard on-line model by algorithms that make no false-negative mistakes <ref> [16, 9, 7] </ref>. As these algorithms are always correct when they predict 0, they can be used without modification as apple tasting algorithms. For such a target class F, RALC (F ; T ) is O (d ln T ).
Reference: [10] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant at tributes abound: a new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory [1] [4] [3] [5] [9] <ref> [10] </ref> [11] [12] [13] [14] [15]. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance. <p> Thus, for conjunctions, the upper bound of O (n + p T n) obtained by applying our transformation to the standard algorithms for learning conjunctions <ref> [17, 10] </ref> is somewhat comforting when T is moderately small. For the most effective application of these transformation results, one must understand how algorithms for the standard model can trade off false negatives and false positives.
Reference: [11] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, </type> <institution> UC Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory [1] [4] [3] [5] [9] [10] <ref> [11] </ref> [12] [13] [14] [15]. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance. <p> of the instance. (Equivalently, in this paper, we will sometimes speak of the learner accepting or rejecting the instance.) In the standard on-line model, the learner is assumed to receive a label indicating the correct classification of the instance at the end of each trial (this label may be corrupted) <ref> [11, 12] </ref>. By contrast, here we consider a model in which the learner only receives information about the correct classification if the learner chooses the accept action. <p> One of the interesting features of the apple tasting model is the dependence of the bounds on T , the number of trials. In the standard model, algorithms can be forced to make all of their mistakes on the first trials <ref> [11] </ref>. The p T factors in our bounds indicate that this is not the case in the apple tasting model. <p> The following was proved [8] using techniques like those used to prove a similar theorem which doesn't differentiate between positive and negative mistakes <ref> [11] </ref>. Lemma 3 Let X be a finite set, F be a class of functions from X to f0; 1g, and M ; M + be nonneg-ative integers such that for any algorithm A, either L (A; F ) M or L + (A; F) M + .
Reference: [12] <author> N. Littlestone and M. Warmuth. </author> <title> The weighted major ity algorithm. </title> <booktitle> Proceedings of the 30th Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1989. </year>
Reference-contexts: This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory [1] [4] [3] [5] [9] [10] [11] <ref> [12] </ref> [13] [14] [15]. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance. <p> of the instance. (Equivalently, in this paper, we will sometimes speak of the learner accepting or rejecting the instance.) In the standard on-line model, the learner is assumed to receive a label indicating the correct classification of the instance at the end of each trial (this label may be corrupted) <ref> [11, 12] </ref>. By contrast, here we consider a model in which the learner only receives information about the correct classification if the learner chooses the accept action.
Reference: [13] <author> W. Maass. </author> <title> On-line learning with an oblivious envi ronment and the power of randomization. </title> <booktitle> The 1991 Workshop on Computational Learning Theory, </booktitle> <pages> pages 167-175, </pages> <year> 1991. </year>
Reference-contexts: This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory [1] [4] [3] [5] [9] [10] [11] [12] <ref> [13] </ref> [14] [15]. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance.
Reference: [14] <author> W. Maass and G. Turan. </author> <title> On the complexity of learn ing from counterexamples. </title> <booktitle> Proceedings of the 30th Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1989. </year>
Reference-contexts: This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory [1] [4] [3] [5] [9] [10] [11] [12] [13] <ref> [14] </ref> [15]. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance. <p> performance for an algorithm A learning a function from class F of functions from X to f0; 1g on a sequence of T inputs by AL (A; F; T ) = sup AL (A; ; f ): Finally, we define the apple tasting learning complexity (analogous to standard learning complexity <ref> [14] </ref>) of F on sequences of T trials as ALC (F; T ) = inf AL (A; F; T ): Note that the above definition allows a different algorithm for each T , and therefore, loosely speaking, we might view our apple tasting algorithms as "know-ing" T .
Reference: [15] <author> W. Maass and G. Turan. </author> <title> On the complexity of learn ing from counterexamples and membership queries. </title> <booktitle> Proceedings of the 31st Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1990. </year>
Reference-contexts: This is similar to the on-line learning task that has been considered by a variety of researchers in computational learning theory [1] [4] [3] [5] [9] [10] [11] [12] [13] [14] <ref> [15] </ref>. As in that task, learning can be thought of as proceeding in a sequence of trials. In each trial, first the learner observes an object, situation, or event|we call the observation an instance.
Reference: [16] <author> B. Natarajan. </author> <title> Probably approximate learning of sets and functions. </title> <journal> Siam J. Comput., </journal> <volume> 10(2) </volume> <pages> 328-351, </pages> <year> 1991. </year>
Reference-contexts: Certain other concept classes can be learned in the standard model by algorithms that make no false-negative mistakes. These (stan-dard model) algorithms determine the unique largest f 2 F consistent with the examples and expect to make a number O (d ln T ) mistakes (see <ref> [16, 9, 7] </ref>). We can easily take full advantage of this fact, obtaining algorithms with bounds that grow as ln T rather than p T . Taking full advantage of the possible trade-offs between false negatives and false positives remains an open problem. <p> Certain target classes can be effectively learned in the standard on-line model by algorithms that make no false-negative mistakes <ref> [16, 9, 7] </ref>. As these algorithms are always correct when they predict 0, they can be used without modification as apple tasting algorithms. For such a target class F, RALC (F ; T ) is O (d ln T ).
Reference: [17] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communica tions of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: Thus, for conjunctions, the upper bound of O (n + p T n) obtained by applying our transformation to the standard algorithms for learning conjunctions <ref> [17, 10] </ref> is somewhat comforting when T is moderately small. For the most effective application of these transformation results, one must understand how algorithms for the standard model can trade off false negatives and false positives.
Reference: [18] <author> V. Vapnik and A. Chervonenkis. </author> <title> On the uniform con vergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: Formally, for a set X, a class F of f0; 1g-valued functions defined on X, and a probability distribution D on X, we define RALC (F; T ) to be A sup E 2D T (AL (A; ; f )) : The VC-dimension <ref> [18] </ref> of F is defined to be the size of the largest set S = fs 1 ; :::; s k g X for which f (f (s 1 ); :::; f (s k )) : f 2 F g = f0; 1g jSj .
References-found: 18

