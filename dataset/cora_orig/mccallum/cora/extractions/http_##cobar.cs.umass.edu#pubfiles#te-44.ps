URL: http://cobar.cs.umass.edu/pubfiles/te-44.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Email: soderlan@cs.washington.edu  fdfisher lehnertg@cs.umass.edu  
Title: Automatically Learned vs. Hand-crafted Text Analysis Rules  
Author: Stephen Soderland David Fisher, Wendy Lehnert 
Address: Seattle, WA 98195-2350  Amherst, MA 01003-4610  
Affiliation: Dept. Computer Science Engineering University of Washington  Dept. Computer Science University of Massachusetts  
Abstract: As vast quantities of on-line text become available, there is an increasing need for systems that automatically analyze the conceptual content of natural language text. Systems that operate on narrowly defined domains show promise, but require a different set of domain-specific rules for each application. This paper describes CRYSTAL, a system that learns text analysis rules automatically from examples. Rules induced by CRYSTAL achieve performance approaching that of hand-crafted rules. CRYSTAL has a particularly efficient learning algorithm that is not improved by more extensive search. This offers a practical alternative to time-consuming manual knowl edge engineering for each new domain.
Abstract-found: 1
Intro-found: 1
Reference: [ Clark and Niblett 1989 ] <author> Clark, P. and Niblett, T. </author> <title> The CN2 Induction Algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: This is repeated until all positive instances have been covered or have been selected as seed. This machine learning methodology is called a covering algorithm [ Michalski 1983 ] <ref> [ Clark and Niblett 1989 ] </ref> . An efficient search control for generalizing concept definitions is vital for CRYSTAL because of the expressive representation of its rules.
Reference: [ Fisher et al. 1995 ] <author> Fisher, D., Soderland, S., McCarthy, J., Feng, F., Lehnert, W. </author> <title> Description of the UMass System as Used for MUC-6. </title> <booktitle> In Proceedings of the Sixth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 221-236, </pages> <year> 1995. </year>
Reference: [ Huffman 1996 ] <author> Huffman, S. </author> <title> Learning Information Extraction Patterns from Examples. Connectionist, Statistical, and Symbolic approaches to Learning for Natural Language Processing. </title> <publisher> Springer, </publisher> <pages> 246-260, </pages> <year> 1996. </year>
Reference-contexts: A typical initial definition has dozens of constraints on the terms and semantic classes found in an instance, resulting in an extremely large space of possible generalizations. Previous systems [ Riloff 1993 ] [ Kim and Moldovan 1992 ] <ref> [ Huffman 1996 ] </ref> that learn text analysis rules avoid this problem by restricting the rule representation or by restricting representation of stored training instances [ Krupka 1995 ] .
Reference: [ Kim and Moldovan 1992 ] <author> Kim, J. and Moldovan, D. PALKA: </author> <title> A System for Linguistic Knowledge Acquisition. </title> <type> Technical Report PKPL 92-8, </type> <institution> USC Department of Electrical Engineering Systems, </institution> <year> 1992. </year>
Reference-contexts: A typical initial definition has dozens of constraints on the terms and semantic classes found in an instance, resulting in an extremely large space of possible generalizations. Previous systems [ Riloff 1993 ] <ref> [ Kim and Moldovan 1992 ] </ref> [ Huffman 1996 ] that learn text analysis rules avoid this problem by restricting the rule representation or by restricting representation of stored training instances [ Krupka 1995 ] .
Reference: [ Krupka 1995 ] <author> Krupka, G. </author> <title> Description of the SRA System as Used for MUC-6. </title> <booktitle> In Proceedings of the Sixth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 221-236, </pages> <year> 1995. </year>
Reference-contexts: Previous systems [ Riloff 1993 ] [ Kim and Moldovan 1992 ] [ Huffman 1996 ] that learn text analysis rules avoid this problem by restricting the rule representation or by restricting representation of stored training instances <ref> [ Krupka 1995 ] </ref> . In each of these systems, the text analysis rules (or stored instances) require an anchor word, typically the verb, but allow no other term constraints. Sentence elements that contain information to be extracted have semantic class constraints, but other sentence elements are ignored.
Reference: [ Michalski 1983 ] <author> Michalski, R. S. </author> <title> A Theory and Methodology of Inductive Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 111-161, </pages> <year> 1983. </year>
Reference-contexts: This is repeated until all positive instances have been covered or have been selected as seed. This machine learning methodology is called a covering algorithm <ref> [ Michalski 1983 ] </ref> [ Clark and Niblett 1989 ] . An efficient search control for generalizing concept definitions is vital for CRYSTAL because of the expressive representation of its rules.
Reference: [ MUC-4 1992 ] <editor> Proceedings of the Fourth Message Understanding Conference, </editor> <publisher> Morgan Kauf-mann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Even a totally useless system that extracts nothing has accuracy of 99%. As a point of comparison, the best system performance for participants in the ARPA-sponsored Message Understanding Conferences has been recall and precision between 50% and 60% <ref> [ MUC-4 1992 ] </ref> [ MUC-5 1993 ] [ MUC-6 1995 ] .
Reference: [ MUC-5 1993 ] <editor> Proceedings of the Fifth Message Understanding Conference, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Even a totally useless system that extracts nothing has accuracy of 99%. As a point of comparison, the best system performance for participants in the ARPA-sponsored Message Understanding Conferences has been recall and precision between 50% and 60% [ MUC-4 1992 ] <ref> [ MUC-5 1993 ] </ref> [ MUC-6 1995 ] .
Reference: [ MUC-6 1995 ] <editor> Proceedings of the Sixth Message Understanding Conference, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: The ARPA-sponsored Sixth Message Understanding Conference <ref> [ MUC-6 1995 ] </ref> used such a "Management Succession" domain. This domain is illustrated by Figure 1. Input Text: Who's News: Topologix Inc. Donald E. Martella, formerly vice president, operations, was named president and chief executive officer of this maker of parallel processing subsystems. <p> Even a totally useless system that extracts nothing has accuracy of 99%. As a point of comparison, the best system performance for participants in the ARPA-sponsored Message Understanding Conferences has been recall and precision between 50% and 60% [ MUC-4 1992 ] [ MUC-5 1993 ] <ref> [ MUC-6 1995 ] </ref> .
Reference: [ Quinlan and Cameron-Jones 1995 ] <author> Quinlan, J.R. and Cameron-Jones, </author> <title> R.M. Oversearching and Layered Search in Emperical Learning. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1019-1024, </pages> <year> 1995. </year>
Reference-contexts: 7 Most of the changes in recall and precision are statistically significant, but none of the changes in average recall and precision. 1, 2, 5, and 10 Why should precision go down as recall goes up? This is a case of a well known phenomenon in machine learning called overfitting <ref> [ Quinlan and Cameron-Jones 1995 ] </ref> . A machine learning algorithm may create a concept description that fits accidental characteristics of the training. Increased search for an optimal generalization turns out to increase the likelihood of finding rules that over-fit the training data.
Reference: [ Riloff 1993 ] <author> Riloff, E. </author> <title> Automatically Constructing a Dictionary for Information Extraction Tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 811-816, </pages> <year> 1993. </year>
Reference-contexts: A typical initial definition has dozens of constraints on the terms and semantic classes found in an instance, resulting in an extremely large space of possible generalizations. Previous systems <ref> [ Riloff 1993 ] </ref> [ Kim and Moldovan 1992 ] [ Huffman 1996 ] that learn text analysis rules avoid this problem by restricting the rule representation or by restricting representation of stored training instances [ Krupka 1995 ] .
Reference: [ Soderland et al. 1995 ] <author> Soderland, S., Fisher, D., Asel-tine, J., Lehnert, W. </author> <title> CRYSTAL: Inducing a Conceptual Dictionary. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1314-1321, </pages> <year> 1995. </year>
Reference-contexts: An attractive alternative is to use machine learning to acquire the necessary rules. This paper describes CRYSTAL, a system that learns domain-specific text analysis rules from training examples. An earlier implementation of CRYSTAL was presented in <ref> [ Soderland et al. 1995 ] </ref> . A fuller treatment may be found in [ Soderland 1996 ] .
Reference: [ Soderland 1996 ] <author> Soderland, S. </author> <title> Learning Text Analysis Rules for Domain-specific Natural Language Processing. </title> <type> Ph.D. thesis, </type> <institution> technical report UM-CS-1996-087 University of Massachusetts, Amherst, </institution> <year> 1996. </year>
Reference-contexts: This paper describes CRYSTAL, a system that learns domain-specific text analysis rules from training examples. An earlier implementation of CRYSTAL was presented in [ Soderland et al. 1995 ] . A fuller treatment may be found in <ref> [ Soderland 1996 ] </ref> . With CRYSTAL, a domain expert's responsibility is to define the target concepts for a domain and to create training data by marking each reference to the target concept in a set of representative texts. This does not require any background in linguistics or computer science.
References-found: 13

