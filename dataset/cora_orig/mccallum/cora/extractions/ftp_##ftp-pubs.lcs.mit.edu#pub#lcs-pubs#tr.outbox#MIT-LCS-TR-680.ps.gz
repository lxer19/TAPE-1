URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-680.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr600.html
Root-URL: 
Title: A Theory of Clock Synchronization  
Author: by Boaz Patt Nancy A. Lynch Cecil H. Green 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  All rights reserved. Author  Certified by  Professor Of Computer Science and Engineering Thesis Supervisor Certified by Baruch Awerbuch Associate Professor  Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Date: October 1994  October 17, 1994  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1994.  Department of Electrical Engineering and Computer Science  of Computer Science, Johns Hopkins University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. E. Abate, E. W. Butterline, R. A. Carley, P. Greendyk, A. M. Montenegro, C. D. Near, S. H. Richman, and G. P. Zampelli. </author> <title> AT&T's new approach to the synchronization of telecommunication networks. </title> <journal> IEEE Communication Magazine, </journal> <pages> pages 35-45, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: We remark that much previous work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., <ref> [26, 6, 24, 28, 1, 15] </ref>) and theoretical viewpoint (e.g., [16, 19, 7, 13, 33, 3], surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system. <p> It is straightforward to verify that the local clock functions thus defined are continuous. Also, since % &gt; 0 and since P satisfies the standard bounds mapping, we get that the local clock functions are and monotonically increasing. Therefore, local time 0 v is invertible (at least) on <ref> [T s v ; 1] </ref>, where T s v is the local time of the first point in P that occurs at v (if it exists). We denote the inverse function by by local time 1 This concludes the definition of the local clock functions. <p> Therefore, there is a step of the send module in P whose local time is local time ( C 1 ), which implies that local time 1 v is defined over <ref> [local time ej CSA ( C 1 ); 1] </ref>. This, in turn, implies that Eq. (3.1) is well defined. Finally, note that by real-time blindness, the basic component of the state of a CSA is fixed throughout a trajectory, and therefore Eq. (3.2) is not ambiguous. <p> Then clearly in x we have [ext L v ; ext U v ] = <ref> [1; 1] </ref>, and since x;v does not contain the source point, we also have d x;v (sp; p x;v ) = d x;v (p x;v ; sp) = 1, and we are done. Assume for the rest of the proof that x occurs after the first action of v.
Reference: [2] <author> M. Ajtai, J. Aspnes, C. Dwork, and O. Waarts. </author> <title> A theory of competitive analysis for distributed algorithms. </title> <booktitle> In 35th Annual Symposium on Foundations of Computer Science, </booktitle> <address> Santa Fe, New Mexico, </address> <month> Oct. </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Approaches similar to local competitiveness were used in the past. For example, see the "best effort" algorithm of Fischer and Michael [9] for database management. (It may be interesting to note that the algorithm in [9] uses synchronized clocks.) Some other work was done by Ajtai et al. <ref> [2] </ref>, after our preliminary paper was published [29]. Loosely speaking, in [2] they consider a shared memory system, where an execution is a sequence of processor accesses to the shared memory. The order by which processors take steps is given by an arbitrary schedule. <p> For example, see the "best effort" algorithm of Fischer and Michael [9] for database management. (It may be interesting to note that the algorithm in [9] uses synchronized clocks.) Some other work was done by Ajtai et al. <ref> [2] </ref>, after our preliminary paper was published [29]. Loosely speaking, in [2] they consider a shared memory system, where an execution is a sequence of processor accesses to the shared memory. The order by which processors take steps is given by an arbitrary schedule. <p> The order by which processors take steps is given by an arbitrary schedule. A task is defined as a predicate over the output values, and a task is said to be completed when this predicate is satisfied. In the formulation of <ref> [2] </ref>, the competitive factor of an algorithm is the maximum, over all schedules, of the total number of steps taken by the algorithm until the task is completed, divided by the minimal number of steps required by any correct algorithm to complete the task, under the same schedule. <p> Hence the quantity of interest for us is a target function defined over the output values, whereas in <ref> [2] </ref>, the output values are of no interest (provided they are correct), and the implicit target function is the number of steps required to produce the output. Nevertheless, the local competitiveness approach is not widely accepted. <p> We argued that this approach can be of independent interest as a method for evaluating distributed optimization tasks. We compared the concept of local competitiveness with the approach of <ref> [2] </ref>, and we discussed some of its advantages and disadvantages. 77 Chapter 5 The Basic Result The starting point for this chapter is the following problem: given two points in an execution of a clock synchronization system, find the tightest bounds on the real time that elapses between their occurrence. <p> Moses and Bloom [27] look at the problem of clock synchronization from the knowledge theoretic perspective. They study the case of drift-free clocks, and their main result can be viewed as a special case of one of our characterization theorems. Ajtai et al. <ref> [2] </ref> present an approach for the analysis of distributed algorithms which is closely related to our notion of local competitiveness. Let us conclude with some interesting problems that this thesis leaves unsolved.
Reference: [3] <author> H. Attiya, A. Herzberg, and S. Rajsbaum. </author> <title> Optimal clock synchronization under different delay assumptions. </title> <note> SIAM J. Comput., 1994. Accepted for publication. A preliminary version appeared in Proceedings of the 12th Annual ACM Symposium on Principles of Distributed Computing, </note> <year> 1993. </year>
Reference-contexts: work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., [26, 6, 24, 28, 1, 15]) and theoretical viewpoint (e.g., <ref> [16, 19, 7, 13, 33, 3] </ref>, surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system. The large number of variants is justified by the wide spectrum of applications. <p> The main idea in the analysis of [13] is to formulate the problem as a linear program; solving this program, they find the worst case scenario, and an algorithm is presented so that optimal tightness is guaranteed in this case. In <ref> [3] </ref>, Attiya et al. observe that the algorithm of [13] always gives the best worst-case tightness, even if the actual execution happens to be more favorable for synchronization than the worst possible. This observation motivates them to generalize the results of [13]; specifically, in [3] they present an algorithm which gives <p> In <ref> [3] </ref>, Attiya et al. observe that the algorithm of [13] always gives the best worst-case tightness, even if the actual execution happens to be more favorable for synchronization than the worst possible. This observation motivates them to generalize the results of [13]; specifically, in [3] they present an algorithm which gives optimal tightness for each specific execution of their system. The focus in all the above papers [19, 13, 3] is on obtaining bounds in a centralized off-line fashion. Typically, the algorithms can be viewed as consisting of two stages. <p> This observation motivates them to generalize the results of [13]; specifically, in [3] they present an algorithm which gives optimal tightness for each specific execution of their system. The focus in all the above papers <ref> [19, 13, 3] </ref> is on obtaining bounds in a centralized off-line fashion. Typically, the algorithms can be viewed as consisting of two stages. In the first stage, timing information is gathered at the processors by sending messages over the links. <p> (Technically, since our definition of executions contains also the real time of occurrence of events, only a local view of the execution, which contains just local times of occurrence, is available for computation.) We remark that our model can be viewed as a distributed version of the model considered in <ref> [3] </ref>. To evaluate the quality of a synchronization algorithm, we define in Chapter 4 a new measure, which may be of independent interest in its own right. Intuitively, our approach is 13 a combination of the execution-specific approach of [3], the competitive analysis approach [32, 23], and the causality partial order <p> be viewed as a distributed version of the model considered in <ref> [3] </ref>. To evaluate the quality of a synchronization algorithm, we define in Chapter 4 a new measure, which may be of independent interest in its own right. Intuitively, our approach is 13 a combination of the execution-specific approach of [3], the competitive analysis approach [32, 23], and the causality partial order of Lamport [16]. <p> Using synchronization graphs, we obtain a lower bound on the achievable internal tightness of synchronization. Our lower bound generalizes known lower bounds for drift-free clocks <ref> [19, 13, 3] </ref> to the case of drifting clocks. Moreover, our derivation is relatively simple and intuitive. In Chapter 8, we show a somewhat surprising result regarding the space complexity of optimal synchronization algorithms. <p> The main idea in the system definition in this chapter (first introduced by Attiya et al. <ref> [3] </ref>) is to partition the system into two: an active part (called environment) that generates messages and delivers them, and a passive part, played by the clock synchronization algorithm, whose role is to interpret the resulting communication patterns. <p> This is a disadvantage if the environment is not necessarily adversarial, as may be the case for clock synchronization systems. Another approach, developed by Attiya et al. <ref> [3] </ref>, is that the input for a synchronization algorithm is not only the system specification, but also the actual execution, or more precisely, the view of the execution. 1 In this approach, an algorithm is called optimal if it 1 Recall that views consist of the events and their local times <p> The latter approach is more attractive since an optimal algorithm in this sense has a stronger guarantee of output quality than the guarantee made by an optimal algorithm in the former sense. Both approaches of <ref> [3] </ref> and [13], however, suffer from an important disadvantage, which is that the algorithms they consider are centralized and off-line. More specifically, the algorithms are based on the implicit assumption that all input has been gathered and it is available at a single processor for computation. <p> This is clearly a drawback, since the output of clock synchronization algorithms typically needs to be available all the time, i.e., on-line. For example, in the approach of <ref> [3] </ref>, the input is a view of the execution, which contain certain messages. Notice that this view can be made available at a single processor only if more messages are sent, in which case the view is necessarily extended. <p> Thus an output considered optimal for a view may not be optimal when that view is extended to enable computation. The approach we present in this chapter can be viewed as a combination of the optimality notion of <ref> [3] </ref> with the well-known concept of competitive analysis of on-line algorithms [32, 23], using Lamport's causality relation [16]. More specifically, in competitive analysis the quality of the output produced by an on-line algorithm is evaluated at each point with respect to the input known at that point. <p> The local competitiveness of an algorithm is the maximal ratio between the tightness it produces at any point, and the best possible tightness for the given local view at that point. The concept of local competitiveness can be viewed as a combination of the per-execution evaluation approach of <ref> [3] </ref>, competitive analysis [32, 23], and the causality partial order [16]. We argued that this approach can be of independent interest as a method for evaluating distributed optimization tasks. <p> An algorithm for internal synchronization is required to provide bounds on the length of this real time interval, and the smallest difference in an execution is the internal tightness of that execution. The task of internal synchronization has been the target of considerable research (see, e.g., <ref> [19, 7, 13, 3] </ref> and the survey [31]). However, to the best of our knowledge, the only known non-trivial lower bounds for internal tightness were for the case of drift-free clocks. <p> Again, their lower bound can be viewed as showing that the worst possible scenario under the given constraints is bounded by the maximal cycle mean in the corresponding synchronization graph. Attiya, Herzberg and Rajsbaum <ref> [3] </ref> refined the results of [13] to hold for each execution of the system, rather than for the worst possible executions. Theorem 7.1 generalizes the result of [3] to the case of bounded-drift clocks. <p> Attiya, Herzberg and Rajsbaum <ref> [3] </ref> refined the results of [13] to hold for each execution of the system, rather than for the worst possible executions. Theorem 7.1 generalizes the result of [3] to the case of bounded-drift clocks. Our result generalizes the previous bounds also to the case where the latency bounds may be different for each individual message. 118 Summary In this chapter we discussed the internal clock synchronization problem. Formally, based on the definition of [13].
Reference: [4] <author> D. Bertsekas and R. Gallager. </author> <title> Data Networks. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: Intuitively, our algorithm computes distances in the graph of superpoints. Since arc weights in the graph of superpoints may only decrease, we use (two independent versions of) the distributed Bellman-Ford algorithm for single-source shortest paths computation <ref> [4] </ref>.
Reference: [5] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press/McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: A synchronization graph is then constructed from the new view and its standard bounds mapping, and the distances from the current point to the source point and from the source point to the current point are computed, using any single-source shortest paths algorithm for general graphs (see, e.g., <ref> [5] </ref>). Using these distances, the output variables are updated according to Eqs. (6.2, 6.3).
Reference: [6] <author> F. Cristian. </author> <title> Probabilistic clock synchronization. </title> <journal> Distributed Computing, </journal> <volume> 3 </volume> <pages> 146-158, </pages> <year> 1989. </year>
Reference-contexts: We remark that much previous work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., <ref> [26, 6, 24, 28, 1, 15] </ref>) and theoretical viewpoint (e.g., [16, 19, 7, 13, 33, 3], surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system. <p> Usually, loosely coupled systems use external synchronization algorithms, and tightly coupled systems use internal synchronization. One important protocol for external synchronization is NTP [25, 26], used over the Internet. Another prominent technique in practice is "probabilistic clock synchronization" proposed by Cristian <ref> [6] </ref>. In this approach, the transmission time of messages is assumed to adhere to some probability distribution, and the transmission times of different messages are assumed to be independent. <p> In a multiple round-trip scenario, however, the output of our algorithm will be usually better. 113 Chapter 7 Internal Synchronization In this chapter we prove a lower bound on the tightness of another variant of clock synchronization, called internal clock synchronization <ref> [6] </ref>. The goal of internal synchronization is that all processors generate a "tick," called fire below, such that all fire steps occur in the smallest possible interval of real time.
Reference: [7] <author> D. Dolev, J. Y. Halpern, and R. </author> <title> Strong. On the possiblity and impossibility of achieving clock synchronization. </title> <journal> J. Comp. and Syst. Sci., </journal> <volume> 32(2) </volume> <pages> 230-250, </pages> <year> 1986. </year>
Reference-contexts: work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., [26, 6, 24, 28, 1, 15]) and theoretical viewpoint (e.g., <ref> [16, 19, 7, 13, 33, 3] </ref>, surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system. The large number of variants is justified by the wide spectrum of applications. <p> numbers ext L or ext U is finite, the output according to the (T ; ") formulation is the same as for the case where both ext L and ext U are infinite. 4.1.2 Definition of Internal Synchronization We use a variant of the elegant definition of Dolev et al. <ref> [7] </ref> and Halpern et al. [13], which we formulate as follows. (A discussion of the definition is given in Chapter 7.) 72 An internal synchronization system is a clock synchronization system, such that each CSA module has a special internal action called fire v , where v is the site of <p> An algorithm for internal synchronization is required to provide bounds on the length of this real time interval, and the smallest difference in an execution is the internal tightness of that execution. The task of internal synchronization has been the target of considerable research (see, e.g., <ref> [19, 7, 13, 3] </ref> and the survey [31]). However, to the best of our knowledge, the only known non-trivial lower bounds for internal tightness were for the case of drift-free clocks. <p> This requirement alone is not sufficient, since it allows for the trivial solution where all clock variables always have the same fixed value (say, 0). Dolev et al. discuss this issue in depth <ref> [7] </ref>. In [19], this difficulty is avoided as follows. Each processor v is assumed to have a special output variable denoted CORR v ; the tightness is measured as the maximal difference between the values of local time v + CORR v , over all processors v.
Reference: [8] <author> D. Dolev, R. Reischuk, and R. </author> <title> Strong. Observable clock synchronization. </title> <booktitle> In Proceedings of the 13th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1994. </year> <month> 143 </month>
Reference-contexts: It may be interesting to note that after our preliminary paper [29] was published, a few papers which have considerable overlap with our results have appeared. Specifically, Dolev et al. <ref> [8] </ref> have defined the notion of observable clock synchronization which is closely related to our notion of optimal clock synchronization. Their analysis is for the special case where the communication is done over a broadcast channel.
Reference: [9] <author> M. J. Fischer and A. Michael. </author> <title> Sacrificing serializability to attain high availability of data in an unreliable network. </title> <booktitle> In Proc. ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, </booktitle> <pages> pages 70-75, </pages> <year> 1982. </year>
Reference-contexts: The local competitiveness definition can be generalized using any positive valued target function that measures the quality of the output. Approaches similar to local competitiveness were used in the past. For example, see the "best effort" algorithm of Fischer and Michael <ref> [9] </ref> for database management. (It may be interesting to note that the algorithm in [9] uses synchronized clocks.) Some other work was done by Ajtai et al. [2], after our preliminary paper was published [29]. <p> Approaches similar to local competitiveness were used in the past. For example, see the "best effort" algorithm of Fischer and Michael <ref> [9] </ref> for database management. (It may be interesting to note that the algorithm in [9] uses synchronized clocks.) Some other work was done by Ajtai et al. [2], after our preliminary paper was published [29]. Loosely speaking, in [2] they consider a shared memory system, where an execution is a sequence of processor accesses to the shared memory.
Reference: [10] <author> R. Gawlick, R. Segala, J. Stgaard-Andersen, and N. Lynch. </author> <title> Liveness in timed and un-timed systems. </title> <type> Technical Report MIT/LCS/TR-587, </type> <institution> MIT Lab. for Computer Science, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: an inverse of N , and maps the "&lt;" relation to a "!" relation. 2.1 Definition of Mixed Automata Our first step is to give a definition of trajectories (adapted from [20]), which have turned out to be a key concept in the formal analysis of real-time systems (see, e.g., <ref> [10, 21] </ref>). Intuitively, a trajectory for a given interval will be used to describe an "evolution" of a nondeterministic system when only time passes through that interval of time. The definition below is stated in general terms; the specialization for our purposes is done later.
Reference: [11] <author> J. D. Guyton and M. F. Schwartz. </author> <title> Experiences with a survey tool for dicovering Network Time Protocol servers. </title> <type> Techical Report CU-CS-704-94, </type> <institution> University of Colorado, Boulder, </institution> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: We hope that despite its weaknesses, this thesis can be used to improve synchronization in many cases. This may lead to a slightly more convenient world, and it can perhaps be translated into financial profit (for example, Merrill Lynch is using NTP to synchronize their worldwide network <ref> [11] </ref>). It may be interesting to note that after our preliminary paper [29] was published, a few papers which have considerable overlap with our results have appeared.
Reference: [12] <author> J. Halpern and I. Suzuki. </author> <title> Clock synchronization and the power of broadcasting. </title> <booktitle> In Proc. of Allerton Conference, </booktitle> <pages> pages 588-597, </pages> <year> 1990. </year>
Reference-contexts: Another instance of relative time constraints is where a set of events is known to occur within a time interval of known length. (Halpern and Suzuki <ref> [12] </ref> make this assumption for the set of receive events of a broadcast message.) Formally, we have a set Q of events, such that for any pair p i ; p j 2 Q we know that now (p i ) now (p j ) a ; and the reduction to
Reference: [13] <author> J. Y. Halpern, N. Megiddo, and A. A. Munshi. </author> <title> Optimal precision in the presence of uncertainty. </title> <journal> Journal of Complexity, </journal> <volume> 1 </volume> <pages> 170-196, </pages> <year> 1985. </year>
Reference-contexts: work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., [26, 6, 24, 28, 1, 15]) and theoretical viewpoint (e.g., <ref> [16, 19, 7, 13, 33, 3] </ref>, surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system. The large number of variants is justified by the wide spectrum of applications. <p> Square brackets are used to denote intervals, including the case of infinite intervals. 11 that gives optimal tightness in the worst possible scenario allowable by the system speci-fications. Halpern et al. <ref> [13] </ref> generalized the results of [19] to networks whose underlying topology is arbitrary, and whose message latency bounds may be different for each link. The main idea in the analysis of [13] is to formulate the problem as a linear program; solving this program, they find the worst case scenario, and <p> Halpern et al. <ref> [13] </ref> generalized the results of [19] to networks whose underlying topology is arbitrary, and whose message latency bounds may be different for each link. The main idea in the analysis of [13] is to formulate the problem as a linear program; solving this program, they find the worst case scenario, and an algorithm is presented so that optimal tightness is guaranteed in this case. In [3], Attiya et al. observe that the algorithm of [13] always gives the best worst-case tightness, even <p> The main idea in the analysis of <ref> [13] </ref> is to formulate the problem as a linear program; solving this program, they find the worst case scenario, and an algorithm is presented so that optimal tightness is guaranteed in this case. In [3], Attiya et al. observe that the algorithm of [13] always gives the best worst-case tightness, even if the actual execution happens to be more favorable for synchronization than the worst possible. This observation motivates them to generalize the results of [13]; specifically, in [3] they present an algorithm which gives optimal tightness for each specific execution of their system. <p> In [3], Attiya et al. observe that the algorithm of <ref> [13] </ref> always gives the best worst-case tightness, even if the actual execution happens to be more favorable for synchronization than the worst possible. This observation motivates them to generalize the results of [13]; specifically, in [3] they present an algorithm which gives optimal tightness for each specific execution of their system. The focus in all the above papers [19, 13, 3] is on obtaining bounds in a centralized off-line fashion. Typically, the algorithms can be viewed as consisting of two stages. <p> This observation motivates them to generalize the results of [13]; specifically, in [3] they present an algorithm which gives optimal tightness for each specific execution of their system. The focus in all the above papers <ref> [19, 13, 3] </ref> is on obtaining bounds in a centralized off-line fashion. Typically, the algorithms can be viewed as consisting of two stages. In the first stage, timing information is gathered at the processors by sending messages over the links. <p> An algorithm is called optimal if it is locally 1-competitive. 1.3.2 A General Theory The heart of this thesis is a new analysis of clock synchronization problems. Intuitively, we show that even though clock synchronization problems can be formulated as linear programs <ref> [13] </ref>, fortunately they have a much simpler structure, namely distances in a certain graph. More specifically, in Chapter 5 we introduce a new concept, which we call synchronization graphs. Synchronization graphs are weighted, directed graphs derived from system specifications and local views of executions. <p> Using synchronization graphs, we obtain a lower bound on the achievable internal tightness of synchronization. Our lower bound generalizes known lower bounds for drift-free clocks <ref> [19, 13, 3] </ref> to the case of drifting clocks. Moreover, our derivation is relatively simple and intuitive. In Chapter 8, we show a somewhat surprising result regarding the space complexity of optimal synchronization algorithms. <p> However, it is not clear a priori what is the input for synchronization algorithms. One classical answer for this question is that the input is the system specification. A typical example for this approach is the paper by Halpern et al. <ref> [13] </ref>, where designing a synchronization algorithm is viewed as a "game against nature:" an algorithm is called optimal if it produces the best output under the worst-case scenario allowable by the system specification. <p> This approach has the appealing property of robustness, but it may give rise to algorithms that produce the best worst-case result always, even if the actual execution does not happen to be the worst possible (the algorithm given in <ref> [13] </ref> has this property). This is a disadvantage if the environment is not necessarily adversarial, as may be the case for clock synchronization systems. <p> The latter approach is more attractive since an optimal algorithm in this sense has a stronger guarantee of output quality than the guarantee made by an optimal algorithm in the former sense. Both approaches of [3] and <ref> [13] </ref>, however, suffer from an important disadvantage, which is that the algorithms they consider are centralized and off-line. More specifically, the algorithms are based on the implicit assumption that all input has been gathered and it is available at a single processor for computation. <p> U is finite, the output according to the (T ; ") formulation is the same as for the case where both ext L and ext U are infinite. 4.1.2 Definition of Internal Synchronization We use a variant of the elegant definition of Dolev et al. [7] and Halpern et al. <ref> [13] </ref>, which we formulate as follows. (A discussion of the definition is given in Chapter 7.) 72 An internal synchronization system is a clock synchronization system, such that each CSA module has a special internal action called fire v , where v is the site of the module. <p> An algorithm for internal synchronization is required to provide bounds on the length of this real time interval, and the smallest difference in an execution is the internal tightness of that execution. The task of internal synchronization has been the target of considerable research (see, e.g., <ref> [19, 7, 13, 3] </ref> and the survey [31]). However, to the best of our knowledge, the only known non-trivial lower bounds for internal tightness were for the case of drift-free clocks. <p> The tightness is defined to be the maximal difference between the local time v + CORR v values, measured only when the algorithm is in a final state. In <ref> [13] </ref>, the difficulty of problem definition is solved differently: each processor is required to flip a special internal bit during the execution of the algorithm; the tightness is defined to be the maximal difference in real time between two remote bit flips. <p> It can be shown that for these graphs, the maximum cycle mean is (H L)(n 1)=n, which is the lower bound proved in [19]. Halpern, Megiddo and Munshi <ref> [13] </ref> extended the result of [19] to the case where the underlying graph of the system is not complete, and the latency bounds for each link may be different (i.e., there are different H and L for each link). <p> Again, their lower bound can be viewed as showing that the worst possible scenario under the given constraints is bounded by the maximal cycle mean in the corresponding synchronization graph. Attiya, Herzberg and Rajsbaum [3] refined the results of <ref> [13] </ref> to hold for each execution of the system, rather than for the worst possible executions. Theorem 7.1 generalizes the result of [3] to the case of bounded-drift clocks. <p> Our result generalizes the previous bounds also to the case where the latency bounds may be different for each individual message. 118 Summary In this chapter we discussed the internal clock synchronization problem. Formally, based on the definition of <ref> [13] </ref>. Using synchronization graphs, we presented a new lower bound for internal synchronization for system over systems with drifting clocks.
Reference: [14] <author> R. M. Karp. </author> <title> A characterization of the minimum cycle mean in a digraph. </title> <journal> Discrete Mathematics, </journal> <volume> 23 </volume> <pages> 309-311, </pages> <year> 1978. </year>
Reference-contexts: That is, mcm (G) = max fw ()=jj : is a directed cycle of Gg. We remark that the maximum cycle mean can be computed in polynomial time <ref> [14] </ref>. To analyze internal synchronization systems, the definition of patterns and views is extended so that the fire steps are points with the usual attributes (i.e., processor of occurrence, local time of occurrence, and for patters, real time of occurrence). We extend the standard bounds mapping too, using Def. 3.11.
Reference: [15] <author> H. Kopetz and W. Ochsenreiter. </author> <title> Clock synchronization in distributed real-time systems. </title> <journal> IEEE Trans. Comm., </journal> <volume> 36(8) </volume> <pages> 933-939, </pages> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: We remark that much previous work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., <ref> [26, 6, 24, 28, 1, 15] </ref>) and theoretical viewpoint (e.g., [16, 19, 7, 13, 33, 3], surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system.
Reference: [16] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Comm. ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., [26, 6, 24, 28, 1, 15]) and theoretical viewpoint (e.g., <ref> [16, 19, 7, 13, 33, 3] </ref>, surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system. The large number of variants is justified by the wide spectrum of applications. <p> Intuitively, our approach is 13 a combination of the execution-specific approach of [3], the competitive analysis approach [32, 23], and the causality partial order of Lamport <ref> [16] </ref>. Loosely speaking, we call a clock synchronization algorithm locally K-competitive if the tightness of its output at any point at any execution is at most K times the best possible tightness among all correct algorithms, given the local view at that point. <p> We also believe that the discovery of synchronization graphs is an important contribution to the research of timing-based systems. In some sense, synchronization graphs can be viewed as the extension of Lamport's graphs <ref> [16] </ref>, used to describe executions of completely asynchronous systems, to the case where processors have clocks. <p> The graph structure is essentially the one described by Lamport <ref> [16] </ref>. Let us recall the following standard graph-theoretic definitions. Definition 3.4 Let G = (V; E) be a directed graph. <p> Similarly, we can speak about the view of a pattern. 54 3. The reachability relation in views and patterns of executions is essentially the "hap-pened before" relation described by Lamport <ref> [16] </ref>: a point p is reachable from a point q in the graph of a view of an executions if and only if q "happened before" p. Introducing null points into views and patterns. <p> of environments contain information that can be used by CSAs for computation, while the real time information in patterns is available only for analysis. * a local view at a point p is the restriction of the view to all the points that "happened before" p (as defined by Lamport <ref> [16] </ref>). <p> The approach we present in this chapter can be viewed as a combination of the optimality notion of [3] with the well-known concept of competitive analysis of on-line algorithms [32, 23], using Lamport's causality relation <ref> [16] </ref>. More specifically, in competitive analysis the quality of the output produced by an on-line algorithm is evaluated at each point with respect to the input known at that point. In the centralized on-line setting, all past input is known, and the future input is unknown. <p> The concept of local competitiveness can be viewed as a combination of the per-execution evaluation approach of [3], competitive analysis [32, 23], and the causality partial order <ref> [16] </ref>. We argued that this approach can be of independent interest as a method for evaluating distributed optimization tasks. <p> Using Theorem 3.2, we also derive a corollary for local views (Theorem 5.8). Philosophically, synchronization graphs can be viewed as an extension of the graphs used by Lamport to describe executions of completely asynchronous systems <ref> [16] </ref>. Lamport's graphs are unweighted, and the main property of interest regarding a pair of points is whether one is reachable from the other. Reachability expresses the fact that in all possible executions which have that graph, one point occurs before the other. <p> On the theoretical side, we believe that synchronization graphs may prove a useful tool in the analysis of timing-based systems. In a sense, synchronization graphs can be viewed as a weighted version of Lamport's graphs <ref> [16] </ref>: Lamport used his unweighted graphs to describe executions of completely asynchronous systems; synchronization graphs are weighted, and can be used to describe executions of systems where processors have clocks. Let us review the main weaknesses of synchronization graphs.
Reference: [17] <author> L. Lamport. </author> <title> The mutual exclusion problem. Part I: A theory of interprocess communication. </title> <journal> J. ACM, </journal> <volume> 33(2) </volume> <pages> 313-326, </pages> <year> 1986. </year>
Reference-contexts: Internal synchronization: We do not know of a good technique for on-line distributed internal synchronization other than the naive use of external synchronization algo rithms. Conceivably, synchronization graphs can be used to this end. 140 Appendix A Time-Space Diagrams In this appendix we present Time-Space Diagrams <ref> [17] </ref>.
Reference: [18] <author> B. Liskov. </author> <title> Practical uses of synchronized clocks in distributed systems. </title> <journal> Distributed Computing, </journal> <volume> 6 </volume> <pages> 211-219, </pages> <year> 1993. </year> <booktitle> Invited talk at the 9th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1990. </year>
Reference-contexts: From the theoretical perspective, having synchronized clocks enables one to use distributed algorithms that proceed in rounds, thus considerably simplifying their design and analysis. For an excellent discussion of the importance of clock synchronization, see Liskov's keynote address at the 9th PODC <ref> [18] </ref>. The basic difficulty in clock synchronization is that timing information tends to deteriorate over the temporal and spatial axes.
Reference: [19] <author> J. Lundelius and N. Lynch. </author> <title> An upper and lower bound for clock synchronization. Information and Computation, </title> <address> 62(2-3):190-204, </address> <year> 1984. </year> <month> 144 </month>
Reference-contexts: work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., [26, 6, 24, 28, 1, 15]) and theoretical viewpoint (e.g., <ref> [16, 19, 7, 13, 33, 3] </ref>, surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system. The large number of variants is justified by the wide spectrum of applications. <p> One of the popular variants studied theoretically is internal synchronization in the case where all clocks in the system are assumed to run exactly at the rate of real time (we call such clocks drift-free hereafter). Lundelius and Lynch <ref> [19] </ref> consider the case in which there is a communication link between each pair of processors, and message latency bounds are identical for all links in the system. <p> Square brackets are used to denote intervals, including the case of infinite intervals. 11 that gives optimal tightness in the worst possible scenario allowable by the system speci-fications. Halpern et al. [13] generalized the results of <ref> [19] </ref> to networks whose underlying topology is arbitrary, and whose message latency bounds may be different for each link. <p> This observation motivates them to generalize the results of [13]; specifically, in [3] they present an algorithm which gives optimal tightness for each specific execution of their system. The focus in all the above papers <ref> [19, 13, 3] </ref> is on obtaining bounds in a centralized off-line fashion. Typically, the algorithms can be viewed as consisting of two stages. In the first stage, timing information is gathered at the processors by sending messages over the links. <p> Using synchronization graphs, we obtain a lower bound on the achievable internal tightness of synchronization. Our lower bound generalizes known lower bounds for drift-free clocks <ref> [19, 13, 3] </ref> to the case of drifting clocks. Moreover, our derivation is relatively simple and intuitive. In Chapter 8, we show a somewhat surprising result regarding the space complexity of optimal synchronization algorithms. <p> An algorithm for internal synchronization is required to provide bounds on the length of this real time interval, and the smallest difference in an execution is the internal tightness of that execution. The task of internal synchronization has been the target of considerable research (see, e.g., <ref> [19, 7, 13, 3] </ref> and the survey [31]). However, to the best of our knowledge, the only known non-trivial lower bounds for internal tightness were for the case of drift-free clocks. <p> This requirement alone is not sufficient, since it allows for the trivial solution where all clock variables always have the same fixed value (say, 0). Dolev et al. discuss this issue in depth [7]. In <ref> [19] </ref>, this difficulty is avoided as follows. Each processor v is assumed to have a special output variable denoted CORR v ; the tightness is measured as the maximal difference between the values of local time v + CORR v , over all processors v. <p> To rule out the trivial solution of setting CORR v = local time v , in <ref> [19] </ref> the executions of synchronization algorithms are required to be finite, i.e., at some point the algorithm enters a terminating state, after which the CORR variable is fixed. <p> Since was an arbitrary cycle in , we conclude that tightness v (e) mcm (), as desired. 117 Theorem 7.1 coincides with known results for the special case of systems with drift-free clocks. For example, Lundelius and Lynch <ref> [19] </ref> considered a system of n processors, where the underlying communication graph is complete, and the latency bounds of all messages are finite and identical (say upper bound H and lower bound L). <p> It can be shown that for these graphs, the maximum cycle mean is (H L)(n 1)=n, which is the lower bound proved in <ref> [19] </ref>. Halpern, Megiddo and Munshi [13] extended the result of [19] to the case where the underlying graph of the system is not complete, and the latency bounds for each link may be different (i.e., there are different H and L for each link). <p> It can be shown that for these graphs, the maximum cycle mean is (H L)(n 1)=n, which is the lower bound proved in <ref> [19] </ref>. Halpern, Megiddo and Munshi [13] extended the result of [19] to the case where the underlying graph of the system is not complete, and the latency bounds for each link may be different (i.e., there are different H and L for each link).
Reference: [20] <author> N. Lynch. </author> <title> Simulation techniques for proving properties of real-time systems. </title> <booktitle> In Rex Workshop '93, Lecture Notes in Computer Science, Mook, </booktitle> <address> the Netherlands, </address> <year> 1994. </year> <note> Springer-Verlag. To appear. </note>
Reference-contexts: In the remainder of this section, we give a more detailed overview of the thesis. 1.3.1 The Setting Based on the model of timed input/output automata of Lynch and Vaandrager <ref> [20] </ref>, we define in Chapter 2 a new formal model, called mixed automata. This model enables us to describe systems with local clocks. Using the formalism of mixed automata, we define in Chapter 3 the environment we consider. Intuitively, the main assumptions expressed by our definitions are the following. <p> The mixed automaton model is based on the timed I/O automata model of Lynch and Vaandrager <ref> [22, 20] </ref>, abbreviated TIOA henceforth. <p> The trajectory ! is an inverse of N , and maps the "&lt;" relation to a "!" relation. 2.1 Definition of Mixed Automata Our first step is to give a definition of trajectories (adapted from <ref> [20] </ref>), which have turned out to be a key concept in the formal analysis of real-time systems (see, e.g., [10, 21]). Intuitively, a trajectory for a given interval will be used to describe an "evolution" of a nondeterministic system when only time passes through that interval of time. <p> In addition to the now at-tribute of states which represents real time (as in the TIOA model <ref> [20] </ref>), a state of a mixed automaton may also have local times attributes, for each local clock. The locations of clocks are represented by special objects called sites. Formally, we have the following definition. <p> When we talk about more than a single automaton, we use subscripts to denote the context. For example, local time A;v denotes the local time function of automaton A at site v. We remark that timed I/O automata, as defined in <ref> [20] </ref>, are a special case of mixed automata, where the site set is empty. 3 Example: the sender automaton. Let us illustrate the concept of a mixed automaton with a toy example, which we shall return to later. <p> We remark that the definition of executions of mixed automata we give here is a straightforward extension of the definition of timed executions in <ref> [20] </ref>. We shall use the following notations (cf. Definition 2.1 and Figure 2-1). Notation 2.8 Let I be a (possibly infinite) interval of R + , and let A be a mixed automaton. <p> The mixed automaton model is based on the timed I/O automata model of Lynch and Vaandrager <ref> [22, 20] </ref>. Our model formalizes the notion of a system with local clocks. We defined the basic notions of executions and their timed traces, which roughly are the sequences of input and output events in executions.
Reference: [21] <author> N. Lynch and N. Shavit. </author> <title> Timing-based mutual exclusion. </title> <booktitle> In Proceedings of the 13th IEEE Real-Time Systems Symposium, </booktitle> <address> Phoenix, Arizona, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: an inverse of N , and maps the "&lt;" relation to a "!" relation. 2.1 Definition of Mixed Automata Our first step is to give a definition of trajectories (adapted from [20]), which have turned out to be a key concept in the formal analysis of real-time systems (see, e.g., <ref> [10, 21] </ref>). Intuitively, a trajectory for a given interval will be used to describe an "evolution" of a nondeterministic system when only time passes through that interval of time. The definition below is stated in general terms; the specialization for our purposes is done later.
Reference: [22] <author> N. Lynch and F. Vaandrager. </author> <title> Forward and backward simulations Part I: </title> <journal> Untimed systems. </journal> <note> Submitted for publication. Also, MIT/LCS/TM-486. </note>
Reference-contexts: The mixed automaton model is based on the timed I/O automata model of Lynch and Vaandrager <ref> [22, 20] </ref>, abbreviated TIOA henceforth. <p> The mixed automaton model is based on the timed I/O automata model of Lynch and Vaandrager <ref> [22, 20] </ref>. Our model formalizes the notion of a system with local clocks. We defined the basic notions of executions and their timed traces, which roughly are the sequences of input and output events in executions.
Reference: [23] <author> M. Manasse, L. McGeoch, and D. Sleator. </author> <title> Competitive algorithms or on-line problems. </title> <booktitle> In Proceedings of the 20th Annual ACM Symposium on Theory of Computing. ACM SIGACT, ACM, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: To evaluate the quality of a synchronization algorithm, we define in Chapter 4 a new measure, which may be of independent interest in its own right. Intuitively, our approach is 13 a combination of the execution-specific approach of [3], the competitive analysis approach <ref> [32, 23] </ref>, and the causality partial order of Lamport [16]. <p> Thus an output considered optimal for a view may not be optimal when that view is extended to enable computation. The approach we present in this chapter can be viewed as a combination of the optimality notion of [3] with the well-known concept of competitive analysis of on-line algorithms <ref> [32, 23] </ref>, using Lamport's causality relation [16]. More specifically, in competitive analysis the quality of the output produced by an on-line algorithm is evaluated at each point with respect to the input known at that point. <p> If no message is sent by the send module, 75 then the optimal algorithm may be trivial since the best possible output is trivial. This example points to a deeper problem in system design (shared also by the classical competitiveness model of <ref> [32, 23] </ref>): the question is to determine what is the input for the algorithm, and what is under the control of the algorithm. The reader should note, however, that a locally competitive algorithm must do well on all cases. <p> The concept of local competitiveness can be viewed as a combination of the per-execution evaluation approach of [3], competitive analysis <ref> [32, 23] </ref>, and the causality partial order [16]. We argued that this approach can be of independent interest as a method for evaluating distributed optimization tasks.
Reference: [24] <author> K. Marzullo and S. Owicki. </author> <title> Maintaining the time in a distributed system. </title> <booktitle> In Proceedings of the 2nd Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 44-54, </pages> <year> 1983. </year>
Reference-contexts: We remark that much previous work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., <ref> [26, 6, 24, 28, 1, 15] </ref>) and theoretical viewpoint (e.g., [16, 19, 7, 13, 33, 3], surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system.
Reference: [25] <author> D. L. Mills. </author> <title> Internet time synchronization: the Network Time Protocol. </title> <journal> IEEE Trans. Comm., </journal> <volume> 39(10) </volume> <pages> 1482-1493, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Only then can each processor adjust its clock. Practical work is typically more focused on on-line distributed algorithms. Usually, loosely coupled systems use external synchronization algorithms, and tightly coupled systems use internal synchronization. One important protocol for external synchronization is NTP <ref> [25, 26] </ref>, used over the Internet. Another prominent technique in practice is "probabilistic clock synchronization" proposed by Cristian [6]. In this approach, the transmission time of messages is assumed to adhere to some probability distribution, and the transmission times of different messages are assumed to be independent.
Reference: [26] <author> D. L. Mills. </author> <title> The Network Time Protocol (version 3): Specification, implementation and analysis. </title> <type> RFC 1305 RFC 1305, </type> <institution> Network Working Group, University of Delaware, </institution> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: We remark that much previous work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., <ref> [26, 6, 24, 28, 1, 15] </ref>) and theoretical viewpoint (e.g., [16, 19, 7, 13, 33, 3], surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system. <p> Only then can each processor adjust its clock. Practical work is typically more focused on on-line distributed algorithms. Usually, loosely coupled systems use external synchronization algorithms, and tightly coupled systems use internal synchronization. One important protocol for external synchronization is NTP <ref> [25, 26] </ref>, used over the Internet. Another prominent technique in practice is "probabilistic clock synchronization" proposed by Cristian [6]. In this approach, the transmission time of messages is assumed to adhere to some probability distribution, and the transmission times of different messages are assumed to be independent. <p> Sharpening the bounds may only result in tighter synchronization. 3.1.5 Example: the Simplified Network Time Protocol (SNTP) In this section we give a concrete example of a clock synchronization system. Our example is based on NTP (Network Time Protocol), the clock synchronization algorithm used over the Internet <ref> [26] </ref>). We present a simplified version of an NTP system, which we call below SNTP. In SNTP, we have only two processors, s and v, connected by a bidirectional communication link. Both processors have drift-free clocks. <p> The latter algorithm compares favorably to the so-called round-trip technique, used by many practical algorithms. In the last section of this chapter, we present the main ideas in the round-trip technique, based on NTP (Network Time Protocol, the external synchronization protocol used over the Internet <ref> [26] </ref>). 1 We also explain why our technique is superior to the one used in NTP. This chapter is organized as follows. In Section 6.1 we recall the definition of external synchronization, and make a few preliminary observations. <p> of Theorem 6.3. 6.4 The Round-Trip Technique It may be interesting at this point to compare our analysis and algorithms with the common clock synchronization technique known as "round-trip probes." For concreteness, we take the external synchronization system NTP (Network Time Protocol, the clock synchronization algorithm used over the Internet <ref> [26] </ref>) as our prime source for this technique. We consider here a simplified variant of NTP, called SNTP, that was introduced in Section 3.1.5. In the SNTP system, we have only two processors with drift-free clocks, connected by perfect asynchronous links. <p> For example, it seems reasonable to assume that our techniques can be implemented over the Internet, thus improving on the current version of NTP <ref> [26] </ref>. In addition, by implementing our methods with bounded space, one can get algorithms which are optimal with respect to a part of the execution (e.g., an algorithm that guarantees that its output is the best possible output for the last day).
Reference: [27] <author> Y. Moses and B. Bloom. </author> <title> Knowledge, timed precedence and clocks. </title> <booktitle> In Proceedings of the 13th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1994. </year>
Reference-contexts: Specifically, Dolev et al. [8] have defined the notion of observable clock synchronization which is closely related to our notion of optimal clock synchronization. Their analysis is for the special case where the communication is done over a broadcast channel. Moses and Bloom <ref> [27] </ref> look at the problem of clock synchronization from the knowledge theoretic perspective. They study the case of drift-free clocks, and their main result can be viewed as a special case of one of our characterization theorems.
Reference: [28] <author> Y. Ofek. </author> <title> Generating a fault tolerant global clock using high-speed control signals for the MetaNet architecture. </title> <journal> IEEE Trans. Comm., </journal> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: We remark that much previous work was done for fault-tolerant clock synchronization, which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., <ref> [26, 6, 24, 28, 1, 15] </ref>) and theoretical viewpoint (e.g., [16, 19, 7, 13, 33, 3], surveys [31, 30] and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system.
Reference: [29] <author> B. Patt-Shamir and S. Rajsbaum. </author> <title> A theory of clock synchronization. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <address> Montreal , Canada, </address> <pages> pages 810-819, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: For example, see the "best effort" algorithm of Fischer and Michael [9] for database management. (It may be interesting to note that the algorithm in [9] uses synchronized clocks.) Some other work was done by Ajtai et al. [2], after our preliminary paper was published <ref> [29] </ref>. Loosely speaking, in [2] they consider a shared memory system, where an execution is a sequence of processor accesses to the shared memory. The order by which processors take steps is given by an arbitrary schedule. <p> This may lead to a slightly more convenient world, and it can perhaps be translated into financial profit (for example, Merrill Lynch is using NTP to synchronize their worldwide network [11]). It may be interesting to note that after our preliminary paper <ref> [29] </ref> was published, a few papers which have considerable overlap with our results have appeared. Specifically, Dolev et al. [8] have defined the notion of observable clock synchronization which is closely related to our notion of optimal clock synchronization.
Reference: [30] <author> F. B. Schneider. </author> <title> Understanding protocols for Byzantine clock synchronization. </title> <type> Research Report 87-859, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> Aug. </month> <year> 1987. </year> <month> 145 </month>
Reference-contexts: which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., [26, 6, 24, 28, 1, 15]) and theoretical viewpoint (e.g., [16, 19, 7, 13, 33, 3], surveys <ref> [31, 30] </ref> and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system. The large number of variants is justified by the wide spectrum of applications.
Reference: [31] <author> B. Simons, J. L. Welch, and N. Lynch. </author> <title> An overview of clock synchronization. </title> <type> Research Report RC 6505 (63306), </type> <institution> IBM, </institution> <year> 1988. </year>
Reference-contexts: which is beyond the scope of this thesis. 1.2 Previous Work Different variants of the clock synchronization problem have been the target of a vast amount of research from both practical viewpoint (e.g., [26, 6, 24, 28, 1, 15]) and theoretical viewpoint (e.g., [16, 19, 7, 13, 33, 3], surveys <ref> [31, 30] </ref> and references therein); the exact definition of the problem depends both on the intended use of the clocks and on the specific underlying system. The large number of variants is justified by the wide spectrum of applications. <p> The task of internal synchronization has been the target of considerable research (see, e.g., [19, 7, 13, 3] and the survey <ref> [31] </ref>). However, to the best of our knowledge, the only known non-trivial lower bounds for internal tightness were for the case of drift-free clocks. In this chapter, based on synchronization graphs, we give a lower bound for the internal tightness in a synchronization system with bounded-drift clocks.
Reference: [32] <author> D. D. Sleator and R. E. Tarjan. </author> <title> Amortized efficiency of list update and paging rules. </title> <journal> Comm. ACM, </journal> <volume> 28(2) </volume> <pages> 202-208, </pages> <year> 1985. </year>
Reference-contexts: To evaluate the quality of a synchronization algorithm, we define in Chapter 4 a new measure, which may be of independent interest in its own right. Intuitively, our approach is 13 a combination of the execution-specific approach of [3], the competitive analysis approach <ref> [32, 23] </ref>, and the causality partial order of Lamport [16]. <p> Thus an output considered optimal for a view may not be optimal when that view is extended to enable computation. The approach we present in this chapter can be viewed as a combination of the optimality notion of [3] with the well-known concept of competitive analysis of on-line algorithms <ref> [32, 23] </ref>, using Lamport's causality relation [16]. More specifically, in competitive analysis the quality of the output produced by an on-line algorithm is evaluated at each point with respect to the input known at that point. <p> If no message is sent by the send module, 75 then the optimal algorithm may be trivial since the best possible output is trivial. This example points to a deeper problem in system design (shared also by the classical competitiveness model of <ref> [32, 23] </ref>): the question is to determine what is the input for the algorithm, and what is under the control of the algorithm. The reader should note, however, that a locally competitive algorithm must do well on all cases. <p> The concept of local competitiveness can be viewed as a combination of the per-execution evaluation approach of [3], competitive analysis <ref> [32, 23] </ref>, and the causality partial order [16]. We argued that this approach can be of independent interest as a method for evaluating distributed optimization tasks.

References-found: 32

