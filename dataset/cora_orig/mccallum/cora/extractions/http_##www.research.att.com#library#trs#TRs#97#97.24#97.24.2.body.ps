URL: http://www.research.att.com/library/trs/TRs/97/97.24/97.24.2.body.ps
Refering-URL: http://www.research.att.com/library/trs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Rate of Change and other Metrics: a Live Study of the World Wide Web  
Author: Fred Douglis Anja Feldmann Balachander Krishnamurthy Jeffrey Mogul 
Date: December 1997  
Affiliation: AT&T Labs Research  Digital Equipment Corporation Western Research Laboratory AT&T Labs Research  
Pubnum: Technical Report #97.24.2  
Abstract: We tested the validity of these assumptions, and their dependence on characteristics of Web resources, including access rate, age at time of reference, content type, resource size, and Internet top-level domain. We also measured the rate at which resources change, and the prevalence of duplicate copies in the Web. We quantified the potential benefit of a shared proxy-caching server in a large environment by using traces that were collected at the Internet connection points for two large corporations, representing significant numbers of references. Only 22% of the fl This is an extended version of a paper that appeared in the USENIX Symposium on Internetworking Technologies and Systems (USITS), December, 1997. Copyright to this work is retained by the authors. Permission is granted for the noncommercial reproduction of the complete work for educational or research purposes. y Email: douglis@research.att.com. z Email: anja@research.att.com. x Email: bala@research.att.com. Email: mogul@pa.dec.com. resources referenced in either of the traces we analyzed were accessed more than once, but about half of the references were to those multiply-referenced resources. Of this half, 13-26% were to a resource that had been modified since the previous traced reference to it. We found that the content type and rate of access have a strong influence on these metrics, the top-level domain has a moderate influence, and size has little effect. In addition, we studied other aspects of the rate of change, including semantic differences such as the insertion or deletion of anchors, phone numbers, and email addresses. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Martin F. Arlitt and Carey L. Williamson. </author> <title> Web server workload characterization: The search for invariants (extended version). </title> <type> Technical Report DISCUS Working Paper 96-3, </type> <institution> Dept. of Computer Science, University of Saskatchewan, </institution> <month> March </month> <year> 1996. </year> <note> Available as ftp://ftp.cs.usask.- ca/pub/discus/paper.96-3.ps.Z. </note>
Reference-contexts: Arlitt and Williamson used server logs from several sites to analyze document types and sizes, frequency of reference, inter-reference times, aborted connections, and other metrics <ref> [1] </ref>. Here we considered many of the same issues, from the perspective of a collection of clients rather than a relatively small number of servers. Kroeger et al. [14] studied the potential for caching to reduce latency, using simulations based on traces of request and response headers.
Reference: [2] <author> Gaurav Banga, Fred Douglis, and Michael Rabi-novich. </author> <title> Optimistic deltas for WWW latency reduction. </title> <booktitle> In Proceedings of 1997 USENIX Technical Conference, </booktitle> <pages> pages 289-303, </pages> <address> Anaheim, CA, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: Other resources, though apparently cachable, may change frequently. When a resource does change, the extent of the change can affect the performance of systems that use delta encodings to propagate only the changes, rather than full copies of the updated re Rate of Change ... sources <ref> [2, 11, 15] </ref>. The nature of the change is also relevant to systems that notify users when changes to a page have been detected (e.g., AIDE [7] or URL-minder [16]): one would like to have a metric of how "interesting" a change is. <p> Like us, they used logs from proxy-caching servers as well as tcpdump, but they examined headers only. They noted that dynamic documents that are presently uncacheable could be used to transmit the differences between versions. This idea was developed in more detail in WebExpress [11] and "optimistic deltas" <ref> [2] </ref>. A later study by Mogul et al. [15] quantified the potential benefits of delta encoding and compression, using the same traces we used in this paper.
Reference: [3] <author> A. Bestavros. </author> <title> Speculative data dissemination and service to reduce server load, network traffic and service time in distributed information systems. </title> <booktitle> In Proceedings of the 12th International Conference on Data Engineering, </booktitle> <pages> pages 180-189, </pages> <address> New Orleans, </address> <month> February </month> <year> 1996. </year> <note> Also available as http://www.cs.bu.edu/ best/res/papers/icde96.ps. </note>
Reference-contexts: Educational sites serve resources that are noticeably older than other TLDs. Note that 17% of responses in the AT&T trace had no Host header; currently, these fall into the "other" category, and show some periodicity at an interval of 1 day. Previously, Bestavros <ref> [3] </ref> and Gwertzmann and Seltzer [10] found that more popular resources changed less often than others. As described next in Section 3.4, we found that when a resource changed, its frequency of change was greater the more often it was accessed.
Reference: [4] <author> Tim Bray. </author> <title> Measuring the Web. </title> <booktitle> In Proceedings of the Fifth International World Wide Web Conference, </booktitle> <pages> pages 993-1005, </pages> <address> Paris, France, </address> <month> May </month> <year> 1996. </year> <note> Also available as http://www5conf.- inria.fr/fich html/papers/P9/Overview.html. </note>
Reference-contexts: One example of an interesting change is the insertion of a new anchor (hyperlink) to another page. A number of recent studies have attempted to characterize the World Wide Web in terms of content (e.g., <ref> [4, 21] </ref>), performance (e.g., [19]), or caching behavior (e.g., [10]). These studies generally use one of two approaches to collect data, either "crawling" (traversing a static Web topology), or analyzing proxy or server logs. Data collected using a crawler does not reflect the dynamic rates of accesses to Web resources. <p> Here we address other aspects of the rate of change. When possible, we consider how the metric is affected by variables such as frequency of access, content type, resource size, site, or top-level domain (TLD) 1 . We answer ques 1 We use Bray's classification of TLDs <ref> [4] </ref>, such as educational, commercial, government, regional, and so on. tions such as: * How frequently are resources reaccessed? The frequency of reaccess is essential to the utility of caching and delta encoding. * What fraction of requests access a resource that has changed since the previous request to the same <p> Resources are clustered by size. Here, there is not a large distinction between sizes, although the smallest resources tend to be somewhat older than others. This is unsurprising since there are many small images that are essen tially static. d. Resources are clustered by TLD, using Bray's categorization <ref> [4] </ref>. This clustering reduced the 20,400 host addresses to 13,300 distinct sites (such as a campus, or the high-order 16 bits of an IP address for numeric addresses). Educational sites serve resources that are noticeably older than other TLDs. <p> Woodruff, et al [21] used the Web crawler for the Inktomi [12] search engine to categorize resources based on such attributes as size, tags, and file extensions. For HTML documents, they found a mean document size of 4.4 Kbytes. Bray <ref> [4] </ref> similarly used the Open Text Index [17] to analyze a large set of resources. He found an mean resource size of 6.5 Kbytes and a median of 2 Kbytes. Bray's study primarily focussed on the relationships between resources, e.g. the number of inbound and outbound links.
Reference: [5] <author> Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. </author> <title> Syntactic clustering of the web. </title> <booktitle> In Proceedings of the Sixth International World Wide Web Conference, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1997. </year> <note> Available as http://www6.ntt-labs.com/HyperNews/get/PAPER205.html. </note>
Reference-contexts: Gribble and Brewer [9] studied traces from a large collection of clients at U.C. Berkeley, gathered via a packet-sniffer like the one used for our AT&T trace. They examined a largely different set of metrics, such as access rates, locality of reference, and service response times. Broder, et al. <ref> [5] </ref> analyze the syntactic similarity of files, using a web-crawler to create "sketches" of all accessible resources on the Web. These sketches can be used to find resources that are substantially similar.
Reference: [6] <author> Carlos R. Cunha, Azer Bestavros, and Mark E. Crovella. </author> <title> Characteristics of WWW client-based traces. </title> <type> Technical Report BU-CS-95-010, </type> <institution> Computer Science Department, Boston University, </institution> <month> July </month> <year> 1996. </year> <note> Also available as http://www.cs.bu.edu/techre-ports/95-010-www-client-traces.ps.Z. </note>
Reference-contexts: Several studies have considered dynamic accesses, though they have not considered the frequency or extent of modifications. Cunha et al. <ref> [6] </ref> instrumented NCSA Mosaic to gather client-based traces. Those traces were then used to consider document type distributions, resource popularity, and caching policies. Williams et al. [20] studied logs from several environments to evaluate policies governing the removal of documents from a cache.
Reference: [7] <author> Fred Douglis, Thomas Ball, Yih-Farn Chen, and Eleftherios Koutsofios. </author> <title> The AT&T Internet Difference Engine: Tracking and viewing changes on the web. </title> <booktitle> World Wide Web, </booktitle> <pages> pages 27-44, </pages> <month> January </month> <year> 1998. </year>
Reference-contexts: The nature of the change is also relevant to systems that notify users when changes to a page have been detected (e.g., AIDE <ref> [7] </ref> or URL-minder [16]): one would like to have a metric of how "interesting" a change is. One example of an interesting change is the insertion of a new anchor (hyperlink) to another page. <p> In addition to the rate-based analysis, we performed semantic comparisons to multiple versions of textual documents and found that some entities such as telephone numbers are remarkably stable across versions. Semantic comparisons may prove useful in conjunction with notification tools <ref> [7, 16] </ref> as well as search engines, directories, and other Web services. We are collecting a significantly larger trace dataset, to verify our conclusions here. We also intend to perform an extended semantic-difference study to locate minor changes in otherwise-identical web pages.
Reference: [8] <author> R. Fielding, J. Gettys, J. Mogul, H. Frystyk, T. Berners-Lee, et al. </author> <note> RFC 2068: Hypertext transfer protocol | HTTP/1.1, January 1997. </note>
Reference-contexts: For those modified responses with a last-modified timestamp, the time between two differing timestamps indicates a lower bound on the rate of change: the 2 Note that this use of age differs from the HTTP/1.1 terminology, where a response Age header indicates how long a response has been cached <ref> [8] </ref>. resource might have changed more than once between the observed accesses. If a modified response lacks a last-modified timestamp, then we assume that it changed at the time the response was generated. Again, the resource might have changed more than once between the observed accesses.
Reference: [9] <author> Steven D. Gribble and Eric A. Brewer. </author> <title> System design issues for internet middleware services: Deductions from a large client trace. </title> <booktitle> In Proceedings of the Symposium on Internetworking Systems and Technologies, </booktitle> <pages> pages 207-218. </pages> <publisher> USENIX, </publisher> <month> December </month> <year> 1997. </year>
Reference-contexts: They found that even an infinite-size proxy cache could eliminate at most 26% of the latency in their traces, largely because of the same factors we observed: many URLs are accessed only once, and many are modified too often for caching to be effective. Gribble and Brewer <ref> [9] </ref> studied traces from a large collection of clients at U.C. Berkeley, gathered via a packet-sniffer like the one used for our AT&T trace. They examined a largely different set of metrics, such as access rates, locality of reference, and service response times.
Reference: [10] <author> James Gwertzman and Margo Seltzer. </author> <title> World-Wide Web cache consistency. </title> <booktitle> In Proceedings of 1996 USENIX Technical Conference, </booktitle> <pages> pages 141-151, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: One example of an interesting change is the insertion of a new anchor (hyperlink) to another page. A number of recent studies have attempted to characterize the World Wide Web in terms of content (e.g., [4, 21]), performance (e.g., [19]), or caching behavior (e.g., <ref> [10] </ref>). These studies generally use one of two approaches to collect data, either "crawling" (traversing a static Web topology), or analyzing proxy or server logs. Data collected using a crawler does not reflect the dynamic rates of accesses to Web resources. <p> useful than a scheme that can take advantage of delta en codings. * How "old" are resources when accessed, i.e., what is the difference between the reference time and the last-modified time? The age of resources can be an important consideration in determining when to expire a possibly stale copy <ref> [10] </ref>. * For those references yielding explicit modification timestamps, how much time elapses between modifications to the same resource, and how do the modification rate and access rate of a resource interact? If a cache can detect modifications at regular intervals, it can use that information to improve data consistency. * <p> Educational sites serve resources that are noticeably older than other TLDs. Note that 17% of responses in the AT&T trace had no Host header; currently, these fall into the "other" category, and show some periodicity at an interval of 1 day. Previously, Bestavros [3] and Gwertzmann and Seltzer <ref> [10] </ref> found that more popular resources changed less often than others. As described next in Section 3.4, we found that when a resource changed, its frequency of change was greater the more often it was accessed.
Reference: [11] <author> Barron C. Housel and David B. Lindquist. WebEx-press: </author> <title> A system for optimizing Web browsing in a wireless environment. </title> <booktitle> In Proceedings of the Second Annual International Conference on Mobile Computing and Networking, </booktitle> <pages> pages 108-116, </pages> <address> Rye, New York, </address> <month> November </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: Other resources, though apparently cachable, may change frequently. When a resource does change, the extent of the change can affect the performance of systems that use delta encodings to propagate only the changes, rather than full copies of the updated re Rate of Change ... sources <ref> [2, 11, 15] </ref>. The nature of the change is also relevant to systems that notify users when changes to a page have been detected (e.g., AIDE [7] or URL-minder [16]): one would like to have a metric of how "interesting" a change is. <p> Like us, they used logs from proxy-caching servers as well as tcpdump, but they examined headers only. They noted that dynamic documents that are presently uncacheable could be used to transmit the differences between versions. This idea was developed in more detail in WebExpress <ref> [11] </ref> and "optimistic deltas" [2]. A later study by Mogul et al. [15] quantified the potential benefits of delta encoding and compression, using the same traces we used in this paper.
Reference: [12] <author> Inktomi. </author> <note> http://inktomi.berkeley.edu, January 1997. </note>
Reference-contexts: Viles and French [19] studied the availability and latency of a set of servers that were found through web-crawling, primarily to ascertain when machines were accessible and how long it took to contact them. Woodruff, et al [21] used the Web crawler for the Inktomi <ref> [12] </ref> search engine to categorize resources based on such attributes as size, tags, and file extensions. For HTML documents, they found a mean document size of 4.4 Kbytes. Bray [4] similarly used the Open Text Index [17] to analyze a large set of resources.
Reference: [13] <author> Guy Jacobson, Balachander Krishnamurthy, and Divesh Srivastava. Grink: </author> <title> To grok and link. </title> <type> Technical Memorandum, </type> <institution> AT&T Labs-Research, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: For example, telephone numbers (in various guises) are one class of pattern. Across instances of a Web page, changes in telephone numbers may be of interest. The manner in which we recognize semantically interesting patterns, referred to as grinking (for "grok and link"), is part of another project <ref> [13] </ref>; we concentrate here on the rate of change of semantically interesting items. Using the AT&T 5 trace, we looked for the following classes of patterns: HREFs (hyperlinks), IMGs (image references), email addresses, telephone numbers, and URL strings that occur in the body of a Web page. <p> Because each of these forms can occur in many different ways, we probably did not recognize every occurrence. For example, a preliminary study <ref> [13] </ref> found over twenty different variations of North American telephone number syntax. More importantly, we cannot always assert a string matching one of these patterns is indeed a telephone 5 We did not perform an analysis of semantic differences for the Digital trace. number.
Reference: [14] <author> Thomas M. Kroeger, Darrell D. E. Long, and Jef-frey C. Mogul. </author> <title> Exploring the bounds of web latency reduction from caching and prefetching. </title> <booktitle> In Proceedings of the Symposium on Internetworking Systems and Technologies, </booktitle> <pages> pages 13-22. </pages> <publisher> USENIX, </publisher> <month> December </month> <year> 1997. </year>
Reference-contexts: Here we considered many of the same issues, from the perspective of a collection of clients rather than a relatively small number of servers. Kroeger et al. <ref> [14] </ref> studied the potential for caching to reduce latency, using simulations based on traces of request and response headers.
Reference: [15] <author> Jeffrey Mogul, Fred Douglis, Anja Feldmann, and Balachander Krishnamurthy. </author> <booktitle> Potential benefits of delta-encoding and data compression for HTTP. In Proceedings of ACM SIGCOMM'97 Conference, </booktitle> <pages> pages 181-194, </pages> <month> September </month> <year> 1997. </year> <note> An extended version appears as Digital Equipment Corporation Western Research Lab TR 97/4, July, 1997, available as http://www.research.digital .com/wrl/techreports/abstracts/97.4.html. </note>
Reference-contexts: Other resources, though apparently cachable, may change frequently. When a resource does change, the extent of the change can affect the performance of systems that use delta encodings to propagate only the changes, rather than full copies of the updated re Rate of Change ... sources <ref> [2, 11, 15] </ref>. The nature of the change is also relevant to systems that notify users when changes to a page have been detected (e.g., AIDE [7] or URL-minder [16]): one would like to have a metric of how "interesting" a change is. <p> The other trace was obtained over 2 days at the primary Internet proxy for Digital Equipment Corporation, and amounts to 9 Gbytes of data. The traces used in our study have been described elsewhere <ref> [15] </ref> and are discussed in greater detail in Section 2. Our trace collection and analysis were motivated by several questions. A primary issue was the potential benefit of delta encoding and/or compression to reduce bandwidth requirements, a study of which was presented separately [15]. <p> in our study have been described elsewhere <ref> [15] </ref> and are discussed in greater detail in Section 2. Our trace collection and analysis were motivated by several questions. A primary issue was the potential benefit of delta encoding and/or compression to reduce bandwidth requirements, a study of which was presented separately [15]. Here we address other aspects of the rate of change. When possible, we consider how the metric is affected by variables such as frequency of access, content type, resource size, site, or top-level domain (TLD) 1 . <p> In terms of bytes transferred, GIFs contributed a relatively low amount of traffic for the number of accesses or resources, while all other content types contributed a greater share. (See <ref> [15] </ref> for additional statistics about content types.) 4 Rate of Change ... 3.2 Change Ratio Content type Accesses Resources % by count % by bytes % by count % by bytes image/gif 57 36 48 18 text/html 20 21 24 33 image/jpeg 12 24 16 28 application/octet-stream 8 13 9 13 <p> The response-body size differences between our two traces is due to the omission of certain content types from the Digital trace; these content-types show a larger mean, and a larger variance, 19 Rate of Change ... than the included types <ref> [15] </ref>. Several studies have considered dynamic accesses, though they have not considered the frequency or extent of modifications. Cunha et al. [6] instrumented NCSA Mosaic to gather client-based traces. Those traces were then used to consider document type distributions, resource popularity, and caching policies. <p> They noted that dynamic documents that are presently uncacheable could be used to transmit the differences between versions. This idea was developed in more detail in WebExpress [11] and "optimistic deltas" [2]. A later study by Mogul et al. <ref> [15] </ref> quantified the potential benefits of delta encoding and compression, using the same traces we used in this paper. Arlitt and Williamson used server logs from several sites to analyze document types and sizes, frequency of reference, inter-reference times, aborted connections, and other metrics [1]. <p> These sketches can be used to find resources that are substantially similar. Such an approach might be an efficient way to find near-duplicates to which our work on semantic differences (and our previous work on delta encod ing <ref> [15] </ref>) is best applied. 5 Conclusions and Future Work We have used live traces of two large corporate communities to evaluate the rate and nature of change of Web resources.
Reference: [16] <author> Url-minder. </author> <note> http://www.netmind.com/URL-mind-er/URL-minder.html, December 1996. </note>
Reference-contexts: The nature of the change is also relevant to systems that notify users when changes to a page have been detected (e.g., AIDE [7] or URL-minder <ref> [16] </ref>): one would like to have a metric of how "interesting" a change is. One example of an interesting change is the insertion of a new anchor (hyperlink) to another page. <p> In addition to the rate-based analysis, we performed semantic comparisons to multiple versions of textual documents and found that some entities such as telephone numbers are remarkably stable across versions. Semantic comparisons may prove useful in conjunction with notification tools <ref> [7, 16] </ref> as well as search engines, directories, and other Web services. We are collecting a significantly larger trace dataset, to verify our conclusions here. We also intend to perform an extended semantic-difference study to locate minor changes in otherwise-identical web pages.
Reference: [17] <author> Opentext. </author> <note> http://www.opentext.com, 1997. </note>
Reference-contexts: Woodruff, et al [21] used the Web crawler for the Inktomi [12] search engine to categorize resources based on such attributes as size, tags, and file extensions. For HTML documents, they found a mean document size of 4.4 Kbytes. Bray [4] similarly used the Open Text Index <ref> [17] </ref> to analyze a large set of resources. He found an mean resource size of 6.5 Kbytes and a median of 2 Kbytes. Bray's study primarily focussed on the relationships between resources, e.g. the number of inbound and outbound links.
Reference: [18] <author> Arthur van Hoff, John Giannandrea, Mark Hapner, Steve Carter, and Milo Medin. </author> <title> The http distribution and replication protocol. </title> <note> W3C Note, available as http://www.w3.org/TR/NOTE-drp-19970825.html, August 1997. </note>
Reference-contexts: The rate of duplication may be important to the success of protocols such as the proposed "HTTP Distribution and Replication Protocol" (DRP) <ref> [18] </ref>, which would use content signatures, such as an MD5 checksum, to determine whether the content of a resource instance is cached under a different URL. * Can we detect and exploit changes in semantically distinguishable elements of HTML documents, including syntactically marked elements such as anchors and other interresource references
Reference: [19] <author> Charles L. Viles and James C. </author> <title> French. Availability and latency of World Wide Web information servers. </title> <journal> Computing Systems, </journal> <volume> 8(1) </volume> <pages> 61-91, </pages> <month> Winter </month> <year> 1995. </year>
Reference-contexts: One example of an interesting change is the insertion of a new anchor (hyperlink) to another page. A number of recent studies have attempted to characterize the World Wide Web in terms of content (e.g., [4, 21]), performance (e.g., <ref> [19] </ref>), or caching behavior (e.g., [10]). These studies generally use one of two approaches to collect data, either "crawling" (traversing a static Web topology), or analyzing proxy or server logs. Data collected using a crawler does not reflect the dynamic rates of accesses to Web resources. <p> In the Digital trace, 62% of status-200 responses included last-modified times. Only 16075 (4.3%) of responses contained cookies. 4 Related work One can gather data from a number of sources, both static (based on crawling) and dynamic (based on user accesses). Viles and French <ref> [19] </ref> studied the availability and latency of a set of servers that were found through web-crawling, primarily to ascertain when machines were accessible and how long it took to contact them.
Reference: [20] <author> S. Williams, M. Abrams, C. R. Standridge, G. Ab-dulla, and E. A. Fox. </author> <title> Removal policies in network caches for World-Wide Web documents. </title> <booktitle> In 21 Rate of Change ... REFERENCES Proceedings of ACM SIGCOMM'96 Conference, </booktitle> <pages> pages 293-305, </pages> <month> August </month> <year> 1996. </year> <note> Also available as http://ei.cs.vt.edu/~succeed/96WAASF1/. </note>
Reference-contexts: Several studies have considered dynamic accesses, though they have not considered the frequency or extent of modifications. Cunha et al. [6] instrumented NCSA Mosaic to gather client-based traces. Those traces were then used to consider document type distributions, resource popularity, and caching policies. Williams et al. <ref> [20] </ref> studied logs from several environments to evaluate policies governing the removal of documents from a cache. Like us, they used logs from proxy-caching servers as well as tcpdump, but they examined headers only.
Reference: [21] <author> Allison Woodruff, Paul M. Aoki, Eric Brewer, Paul Gauthier, and Lawrence A. Rowe. </author> <booktitle> An investigation of documents from the WWW. In Proceedings of the Fifth International WWW Conference, </booktitle> <pages> pages 963-979, </pages> <address> Paris, France, </address> <month> May </month> <year> 1996. </year> <note> Also available as http://www5conf.- inria.fr/fich html/papers/P7/Overview.html. 22 </note>
Reference-contexts: One example of an interesting change is the insertion of a new anchor (hyperlink) to another page. A number of recent studies have attempted to characterize the World Wide Web in terms of content (e.g., <ref> [4, 21] </ref>), performance (e.g., [19]), or caching behavior (e.g., [10]). These studies generally use one of two approaches to collect data, either "crawling" (traversing a static Web topology), or analyzing proxy or server logs. Data collected using a crawler does not reflect the dynamic rates of accesses to Web resources. <p> Viles and French [19] studied the availability and latency of a set of servers that were found through web-crawling, primarily to ascertain when machines were accessible and how long it took to contact them. Woodruff, et al <ref> [21] </ref> used the Web crawler for the Inktomi [12] search engine to categorize resources based on such attributes as size, tags, and file extensions. For HTML documents, they found a mean document size of 4.4 Kbytes.
References-found: 21

