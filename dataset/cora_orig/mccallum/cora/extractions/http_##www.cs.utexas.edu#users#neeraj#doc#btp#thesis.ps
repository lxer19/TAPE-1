URL: http://www.cs.utexas.edu/users/neeraj/doc/btp/thesis.ps
Refering-URL: http://www.cs.utexas.edu/users/neeraj/home.html
Root-URL: http://www.cs.utexas.edu
Title: Parallel Search for Neural Network  Under the guidance of  
Author: Neeraj Garg Dr Subhashis Banerjee 
Degree: A thesis submitted in partial fulfilment of the requirements for the degree of Bachelor of Technology in Computer Science and Engineering by  
Date: May 1997  
Address: Delhi  
Affiliation: Department of Computer Science and Engineering Indian Institute of Technology,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> John Hertz, Andrew Krogh, Richard G. </author> <title> Palmer Introduction to the Theory of Neural Computation Addison Wesley, </title> <year> 1992 </year>
Reference: [2] <author> R. Bechofer, Hayter, Ajit. C. Tamhane. </author> <title> Designing Experiments for selecting the largest normal mean when the variance are known and unequal. In Journal of Statistical Planning and Inference Vol 28, </title> <publisher> Elsevier, North Holland, </publisher> <year> 1991. </year>
Reference-contexts: 1 + ffi fl i=1 i k i 5 (5.1) where ffi i =Best Population Mean of population i ffi =Mean of ffi i N = Total Number of Samples to be allocated k =Standard Deviation of Population k n k = samples allocated to population k Bechofer et al. <ref> [2] </ref> have considered the problem of selecting the largest normal mean when the variances are known and unequal. Instead of distributing a fixed number of observations across different populations, they have tried to minimize the number of observations that are required to predict with a pre specified level of confidence.
Reference: [3] <author> Akiko N. Aizawa and Benjamin W. </author> <title> Wah A Sequential Sampling Procedure for Genetic Algorithms Computers and Mathematics with Applications vol. </title> <journal> 27, </journal> <volume> no. 9/10, </volume> <month> May </month> <year> 1994, </year> <pages> pp. 77-82. </pages> <publisher> Pergamon Press, Ltd., </publisher> <address> Tarrytown, NY, </address>
Reference-contexts: BAYESIAN APPROACH 21 network is small for small time periods. In case of Order Statistics the probability that the actual best population is selected is maximized. 5.2 Bayesian Approach In an alternative approach in based on Bayesian framework, described by Aizawa and Wah in <ref> [3] </ref>, an error function, associated with selection of a population, is defined and minimized. We expect that the error function will be such that * For the real best population it is zero. * For other populations, it is proportional to difference in actual means. <p> Hence their Aizawa and Wah <ref> [3] </ref> results become applicable to our problem at least in a heuristic sense.
Reference: [4] <author> Patrick Henry Winston Genetic Algorithms, </author> <title> Chapter 25, </title> <journal> Artificial Intelligence Addison Wesley, </journal> <year> 1992 </year>
Reference: [5] <author> Pankaj Mehra and Benjamin. W. </author> <booktitle> Wah . Artificial Neural Networks : Concepts and Theory IEEE Computer Society Press, </booktitle> <year> 1992. </year>
Reference: [6] <author> D. E. Rumelhart, G. E. Hinton and J.L.McClellad. </author> <booktitle> Parallel Distributed Processing : Explorations in the Micro-structure of Cognition, </booktitle> <volume> Vol 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference: [7] <author> G. E. Hinton. </author> <title> Connectionist Learning Procedure. </title> <booktitle> In Artificial Intelligence, </booktitle> <address> Vol40, </address> <year> 1989, </year> <pages> pages 185-234. </pages>
Reference: [8] <author> Tong and Witzell. </author> <title> Allocation of Observations for selecting the best Normal population. In Design of Experiments: Ranking and Selection Procedures </title> . 
Reference-contexts: Which means that we shall take more samples from better populations. 19 5.1. ORDER STATISTIC 20 However, poor populations can't be completely neglected, particularly if they have high error of prediction, in which case they can overtake the current best. Tong and Witzell <ref> [8] </ref> have discussed the special case in which all the populations follow Gaussian distribution. Since we are dealing with populations which come from error of a given neural network, they are likely to follow Gaussian Distribution. Tong and Witzell [8] have tried to take the samples in such a way that <p> Tong and Witzell <ref> [8] </ref> have discussed the special case in which all the populations follow Gaussian distribution. Since we are dealing with populations which come from error of a given neural network, they are likely to follow Gaussian Distribution. Tong and Witzell [8] have tried to take the samples in such a way that the Probability that the population having maximum sample mean has real maximum mean, is maximized. Following rigorous mathematical analysis they have been able to get optimal results for two population problem.
Reference: [9] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, </author> <title> George Karypis Introduction to Parallel Computing :Design And Analysis of Algorithms Benjamin/Cummings, </title> <year> 1994. </year>
Reference: [10] <author> Elaine Rich and Kevin Knight Connectionist Models, </author> <type> Chapter 18, </type> <institution> Artificial Intelligence Tata Mc Graw Hill, </institution> <address> New Delhi, </address> <year> 1993 </year>
Reference: [11] <author> Darkell Whitley, Stephen Dominic, Rajshri Das, Charles W. </author> <title> Anderson Genetic Reinforcement Learning for Neurocontrol Problems Machine Learning, </title> <booktitle> Vol 13 Nov-Dec, </booktitle> <year> 1993 </year>
Reference-contexts: Previously researchers have tried using Genetic Algorithm to train feed forward networks. For example Montana and Davis [12] tried to describe the whole network as a string by encoding every weight in eight bits genes and then applying genetic operators. Whitley et al. <ref> [11] </ref> encoded every weight as a real number, and treated the real number itself as a gene. However, all these efforts were on using genetic algorithm instead of back propagation while we propose to use them in conjunction. 6.4 Final Algorithm This section describes how the three algorithms 1.
Reference: [12] <author> B. Montana and L. </author> <booktitle> Davis Training Feed Forward Networks using Genetic Algorithms Proceedings of the 1989 International Joint Conf. on AI, </booktitle> <pages> pp 762-767 </pages>
Reference-contexts: In case diversity is not maintained, only the hidden units that model easier topology will be chosen and consequently we will have bad performance. Previously researchers have tried using Genetic Algorithm to train feed forward networks. For example Montana and Davis <ref> [12] </ref> tried to describe the whole network as a string by encoding every weight in eight bits genes and then applying genetic operators. Whitley et al. [11] encoded every weight as a real number, and treated the real number itself as a gene.
Reference: [13] <institution> Simon Haykins Neural Networks: A Comprehensive Foundations Mac Millan College Publishing Company, </institution> <note> 1994 53 BIBLIOGRAPHY 54 </note>
Reference: [14] <institution> Institute for Parallel and Distributed High Performance Computing SNNS User Manual, </institution> <note> Version 4.1 report 6/95, </note> <institution> University of Stuttgart, </institution> <year> 1995 </year>
Reference: [15] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Mancheck, </author> <title> Vaidy Sunderam PVM: Parallel Virtual Machine </title>
References-found: 15

