URL: ftp://ftp.eecs.umich.edu/people/durfee/aaai95fss.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/durfee/vita.html
Root-URL: http://www.cs.umich.edu
Email: durfee@umich.edu  
Title: Rational Agents, Limited Knowledge, and Nash Equilibria (Extended Abstract)  
Author: Edmund H. Durfee 
Note: This work was supported, in part, by the National Science Foundation under PYI award IRI-9158473.  
Date: Introduction  
Address: Ann Arbor, MI 48109-2110,  
Affiliation: EECS Dept., University of Michigan  
Abstract: It is not surprising that research into multiagent systems has been concerned with questions about what an agent is and how it should behave, since individual agents are the building blocks of multiagent systems. What has been more surprising, however, is the confusion as to what is being studied (and often designed) in multiagent systems. Some researchers have claimed to give the individual agent preeminence, such that a multiagent system is basically a collection of separate individuals that interact in some way (through a shared environment, most often). And yet, such researchers typically look for (and even design protocols or conventions to achieve) systemwide properties for the collection of agents - properties such as whether the agent society is stable, efficient, fair, and so on. Other researchers have started with requirements for the overall multiagent system, and have worked backwards to design individual agents that have no choice but to act in a way that they collectively achieve system goals. Yet these researchers have had to tackle issues in the design of single agents that contribute to overall goals based on local beliefs and decisions. Thus, whereas some researchers have taken dogmatic stances as to whether system behavior emerges from individual agents (sometimes called a multiagent systems perspective) or whether agent behavior is dictated by overall system properties (sometimes called a cooperative problem solving perspective), in reality the relationships between the individuals and collective are bidirectional. Thus, while acknowledging differences in research agendas can be useful, indiscriminate bifurcation of the field can obscure useful synergies (Durfee and Rosenschein, 1994). A similar tension between taking the perspective of agents or of systems has existed in the game theory community. A traditional emphasis of game theory has been on the analysis of equilibrium solutions in games. Characteristics of the players of a game - typically, that the players are rational in the sense that they will act so as to maximize their expected payoffs - are assumed. A variety of equilibrium solution concepts (Nash equilibria, correlated equilibria, etc.) have emerged as representing, in some sense, the rational outcome of the game when played by rational agents. But the same differences in perspective that exist in the multiagent systems community also exist in the game theory community, as to whether system behavior depends on agent behavior, or whether agent behavior is dictated by the system (game) being played. A recent thread of research in the game theory community has been directed toward questions of how the solutions to games depend not only on properties of the strategic situations that the players face, but also on properties of the players themselves - what they know, their abilities to sense the world and reason about it, and so on. For example, Aumann and Brandenburger (1995) have recently addressed the question of what agents need to know about themselves and others to be assured of collectively achieving a Nash Equilibrium solution. To answer this question, they draw on machinery that is outside of the agents - a model of the epistemic states of all the agents - to prove that, under particular epistemic conditions, it must be the case that rational agents will be in a Nash Equilibrium. However, in their development, they explicitly do not provide prescriptive mechanisms for the agents to use in determining their actions from their knowledge. Moreover, they do not address the question of what rational agents should be doing when the epistemic conditions are not met, either to establish them or to act rationally despite being in less well-defined epistemic states. The purpose of this paper, therefore, is to briefly address these questions. I begin with a very brief summary of the results of Aumann and Brandenburger, which lead us to a point where we can talk about what agents should do under certain conditions, but not about how agents can determine what they should do. I then summarize a method that, while inspired to some extent by game-theoretic notions, is grounded in the endeavor of artificial intelligence to build artificial agents that can take purposeful, rational action given what they know and what they can do. I demonstrate that, in some 2-agent cases, the epistemic states that Aumann and Brandenburger claim lead to Nash Equilibria also lead agents following our method into Nash Equilibria. I extend the states that they consider, moreover, into asymmetric cases, and prove that 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aumann, R. and Brandenberger, A. </author> <year> 1995. </year> <title> Epistemic Conditions for Nash Equilibrium. </title> <note> Econometrica (to appear). </note>
Reference: <author> Binmore, K. </author> <year> 1990. </year> <booktitle> Essays on the Foundations of Game Theory. </booktitle> <publisher> Basil Blackwell, Oxford. </publisher>
Reference: <author> Durfee, E. H., and Rosenschein, J.S. </author> <year> 1994. </year> <title> Distributed Problem Solving and Multi-Agent Systems: Comparisons and Examples. </title> <booktitle> Proceedings of the Thirteenth International Distributed Artificial Intelligence Workshop , pages 94-104, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Thus, while acknowledging differences in research agendas can be useful, indiscriminate bifurcation of the field can obscure useful synergies <ref> (Durfee and Rosenschein, 1994) </ref>. A similar tension between taking the perspective of agents or of systems has existed in the game theory community. A traditional emphasis of game theory has been on the analysis of equilibrium solutions in games.
Reference: <author> Durfee, E. H. </author> <year> 1995. </year> <title> Blissful Ignorance: Knowing Just Enough to Coordinate Well. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems , pages 406-413, </booktitle> <address> San Francisco, CA, </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The Recursive Modeling Method (RMM) draws on game theory for its inspiration, but its motivations are those of artificial intelligence: to provide reasoning mechanisms for computational agents (players in the game) that make rational decisions in multiagent situations <ref> (Gmytrasiewicz & Durfee, 1995) </ref>. Rationality, for RMM, is of the decision-theoretic kind: an agent should make a decision that maximizes its expected utility, given everything that it knows about the situation (including the other agents). <p> Of course, the nesting of knowledge can be deeper, leading to branchy, tall trees which take longer to construct and solve. Strategies for keeping these costs in check are discussed elsewhere <ref> (Durfee, 1995) </ref>. For the time being, moreover, assume that the trees of nested beliefs must be finite; infinite trees would require the establishment of common knowledge, which is impractical in realistic situations (Halpern & Moses, 1984). Communication to Establish the Epistemic Conditions Now lets put the ideas together.
Reference: <author> Gmytrasiewicz, P.J., and Durfee, E.H. </author> <year> 1993. </year> <title> Toward a Theory of Honesty and Trust Among Communicating Autonomous Agents. Group Decision and Negotiation 2 </title> <type> 237-258. </type>
Reference-contexts: Moreover, if it is the case that R1 has correct model (s) of R2s payoffs and R2s rationality, then if R1 can transmit an intention that it can rationally live up to (as above), the two agents must be in a Nash equilibrium (assuming the message is received and believed <ref> (Gmytrasiewicz and Durfee, 1993) </ref>).
Reference: <author> Gmytrasiewicz, P.J., and Durfee, E. H. </author> <year> 1995. </year> <title> A Rigorous, Operational Formalization of Recursive Modeling. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> pages 125-132, </pages> <address> San Francisco, CA. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The Recursive Modeling Method (RMM) draws on game theory for its inspiration, but its motivations are those of artificial intelligence: to provide reasoning mechanisms for computational agents (players in the game) that make rational decisions in multiagent situations <ref> (Gmytrasiewicz & Durfee, 1995) </ref>. Rationality, for RMM, is of the decision-theoretic kind: an agent should make a decision that maximizes its expected utility, given everything that it knows about the situation (including the other agents).
Reference: <author> Halpern, J.Y., and Moses, Y., </author> <year> 1984. </year> <title> Knowledge and Common Knowledge in a Distributed Environment. </title> <booktitle> In Proceedings of the Third ACM Conference on Principles of Distributed Computing. </booktitle>
Reference-contexts: Strategies for keeping these costs in check are discussed elsewhere (Durfee, 1995). For the time being, moreover, assume that the trees of nested beliefs must be finite; infinite trees would require the establishment of common knowledge, which is impractical in realistic situations <ref> (Halpern & Moses, 1984) </ref>. Communication to Establish the Epistemic Conditions Now lets put the ideas together. RMM provides tools for using nested models of knowledge about agents decisionmaking situations.
References-found: 7

