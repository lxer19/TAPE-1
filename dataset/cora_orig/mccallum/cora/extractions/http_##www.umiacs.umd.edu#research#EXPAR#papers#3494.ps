URL: http://www.umiacs.umd.edu/research/EXPAR/papers/3494.ps
Refering-URL: http://www.umiacs.umd.edu/research/EXPAR/papers/3494.html
Root-URL: 
Email: dbader@eng.umd.edu  joseph@umiacs.umd.edu  
Title: Practical Parallel Algorithms for Dynamic Data Redistribution, Median Finding, and Selection (Preliminary Draft)  
Author: David A. Bader Joseph JaJa 
Keyword: Parallel Algorithms, Communication Primitives, Median Finding, Selection, Load Balancing, Data Redistribution, Parallel Performance.  
Date: July 14, 1995  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies, and Department of Electrical Engineering, University of Maryland,  
Abstract: A common statistical problem is that of finding the median element in a set of data. This paper presents a fast and portable parallel algorithm for finding the median given a set of elements distributed across a parallel machine. In fact, our algorithm solves the general selection problem that requires the determination of the element of rank i, for an arbitrarily given integer i. Practical algorithms needed by our selection algorithm for the dynamic redistribution of data are also discussed. Our general framework is a single-address space, distributed memory programming model that is enhanced by a set of communication primitives. We use efficient techniques for distributing, coalescing, and load balancing data as well as efficient combinations of task and data parallelism. The algorithms have been coded in Split-C and run on a variety of platforms, including the Thinking Machines CM-5, IBM SP-1 and SP-2, Cray Research T3D, Meiko Scientific CS-2, Intel Paragon, and workstation clusters. Our experimental results illustrate the scalability and efficiency of our algorithms across different platforms and improve upon all the related experimental results known to the authors. More efficient implementations of the communication primitives will likely result in even faster execution times. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: +, fi, min, max, etc.) and a shared input array A [0 : p 1] on a p processor partition, distributed with one element per processor, the PREFIX Communication Library Primitive coalesces the data such that each processor k contains a single element P S [k] = A [0] A <ref> [1] </ref> : : : A [k]. Parallel computers can handle this efficiently when the element P S [k] is assumed to reside on processor k [10], and Split-C implements this as a primitive library function. An analysis for this operation on the BDM model is given in [4]. <p> Thus, each processor k contains P S [i] = A [0] A <ref> [1] </ref> : : : A [i], for all 0 i p 1.
Reference: [2] <author> R.H. Arpaci, D.E. Culler, A. Krishnamurthy, S.G. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <publisher> In ACM Press, </publisher> <editor> editor, </editor> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 320-331, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Note that when p does not divide n evenly, the last processor will receive less than q elements. We refer to this as Method A. for 8 processors and 63 elements, with an arbitrary initial distribution of N = <ref> [10; 3; 2; 20; 0; 14; 6; 8] </ref>. Here, q j = l 8 = 8, for 0 j 6, while q 7 = 7, since P 7 receives the remainder of elements when p does not divide the total number of elements evenly. <p> The Meiko CS-2 Computing Facility was acquired through NSF CISE Infrastructure Grant number CDA-9218202, with support from the College of Engineering and the UCSB Office of Research, for research in parallel computing. Arvind Krishnamurthy provided additional help with his port of Split-C to the Cray Research T3D <ref> [2] </ref>. The Jet Propulsion Lab/Caltech 256-node Cray T3D Supercomputer used in this investigation was provided by funding from the NASA Offices of Mission to Planet Earth, Aeronautics, and Space Science.
Reference: [3] <author> D. A. Bader and J. JaJa. </author> <title> Parallel Algorithms for Image Histogramming and Connected Components with an Experimental Study. </title> <institution> Technical Report CS-TR-3384 and UMIACS-TR-94-133, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> December </month> <year> 1994. </year> <booktitle> To be presented at the Fifth ACM SIGPLAN Symposium of Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: No processor can send or receive more than one word at a time. We present several useful communication primitives in <ref> [3] </ref> and [4] for the transpose (also known as "index" or "all-to-all personalized" communication) and the broadcast data movements. <p> This primitive is also known as the index operation ([8], [11]). The BDM algorithm and analysis for the TRANSPOSE data movement is given in <ref> [3] </ref> and is similar to that of the LogP model [17]. <p> The analysis of this BCAST algorithm is simple. Since this algorithm just performs two TRANS POSE Communication Primitives, the complexities of the BCAST Primitive are ( T comm (n; p) 2 p ; T comp (n; p) = O (q): (4) See <ref> [3] </ref> and [4] for algorithmic details, performance analysis, and empirical results for these com munication primitives. 3.6 Communication Primitive: PREFIX (A [0 : p 1]; ) Given an associative operator (e.g. +, fi, min, max, etc.) and a shared input array A [0 : p 1] on a p processor partition, <p> Note that when p does not divide n evenly, the last processor will receive less than q elements. We refer to this as Method A. for 8 processors and 63 elements, with an arbitrary initial distribution of N = <ref> [10; 3; 2; 20; 0; 14; 6; 8] </ref>. Here, q j = l 8 = 8, for 0 j 6, while q 7 = 7, since P 7 receives the remainder of elements when p does not divide the total number of elements evenly. <p> In particular, for fixed p and a specific machine, we expect the total execution time to increase linearly with m. The results shown in Figure 4 confirm this latter observation. 1 We sample a mean zero, s.d. one, Gaussian curve at the center of p intervals equally spaced along <ref> [3; 3] </ref>. The sample values are normalized to sum to n by multiplying each by n sum of the p samples .
Reference: [4] <author> D. A. Bader, J. JaJa, D. Harwood, and L.S. Davis. </author> <title> Parallel Algorithms for Image Enhancement and Segmentation by Region Growing with an Experimental Study. </title> <institution> Technical Report CS-TR-3449 and UMIACS-TR-95-44, Institute for Advanced Computer Studies (UMIACS), University of Maryland, College Park, MD, </institution> <month> May </month> <year> 1995. </year> <note> Submitted to Journal of Supercomputing. </note>
Reference-contexts: No processor can send or receive more than one word at a time. We present several useful communication primitives in [3] and <ref> [4] </ref> for the transpose (also known as "index" or "all-to-all personalized" communication) and the broadcast data movements. <p> The analysis of this BCAST algorithm is simple. Since this algorithm just performs two TRANS POSE Communication Primitives, the complexities of the BCAST Primitive are ( T comm (n; p) 2 p ; T comp (n; p) = O (q): (4) See [3] and <ref> [4] </ref> for algorithmic details, performance analysis, and empirical results for these com munication primitives. 3.6 Communication Primitive: PREFIX (A [0 : p 1]; ) Given an associative operator (e.g. +, fi, min, max, etc.) and a shared input array A [0 : p 1] on a p processor partition, distributed with <p> Parallel computers can handle this efficiently when the element P S [k] is assumed to reside on processor k [10], and Split-C implements this as a primitive library function. An analysis for this operation on the BDM model is given in <ref> [4] </ref>. <p> Each processor holds values [jq; (j + 1)q 1]; * R: Random. Each processor holds uniformly random values in the range [0; 2 31 1]. The last two input sets correspond to an intermediate problem set from a computer vision algorithm for segmenting images <ref> [4] </ref>. Set L512 (derived from band 5 of a 512 fi 512 Landsat TM image) contains a total of 2 18 elements, which is the same size as the input sets ending with tag 8 on a 32 processor machine.
Reference: [5] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fatoohi, S. Fineberg, P. Frederickson, T. Lasinski, R. Schreiber, H. Simon, V. Venkatakrishnan, and S. Weeratunga. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> Numerical Aerodynamic Simulation Facility, NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Also, in every case, Method B outperforms Method A. We benchmark our selection algorithm in Table I. The input for this problem, taken from the NAS Parallel Benchmark for Integer Sorting <ref> [5] </ref>, is 2 23 integers in the range [0; 2 19 ), spread out evenly across the processors.
Reference: [6] <author> D.H. Bailey, E. Barszcz, L. Dagum, and H.D. Simon. </author> <title> NAS Parallel Benchmark Results 10-94. Report NAS-94-001, Numerical Aerodynamic Simulation Facility, </title> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Note that when p does not divide n evenly, the last processor will receive less than q elements. We refer to this as Method A. for 8 processors and 63 elements, with an arbitrary initial distribution of N = <ref> [10; 3; 2; 20; 0; 14; 6; 8] </ref>. Here, q j = l 8 = 8, for 0 j 6, while q 7 = 7, since P 7 receives the remainder of elements when p does not divide the total number of elements evenly. <p> As shown in Table II, our high-level selection algorithm beats the 17 linear scale log scale fastest sorting results for the NAS input that are known to the authors. Note that the algorithm in <ref> [6] </ref> is machine-specific and does not actually result in a sorted list. elements by approximately one-half during each successive iteration. <p> 4 7.05 8 3.55 32 0.929 128 0.275 Meiko CS-2 16 3.03 32 1.55 32 2.77 Table I: Execution Times for the High-Level BDM Selection (in seconds) on the NAS IS input set Researchers Time (in seconds) Notes Bader & JaJa 2.77 BDM Selection Dusseau [21] 7.67 Radix Sort TMC <ref> [6] </ref> 4.31 Ranking without permuting the data Table II: Execution Time for Selection on a 32-processor CM-5 on the NAS IS input set content of this paper does not necessarily reflect the position or the policy of the government and no official endorsement should be inferred.
Reference: [7] <author> C.F. Baillie and P.D. Coddington. </author> <title> Cluster Identification Algorithms for Spin Models Sequential and Parallel. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(2) </volume> <pages> 129-144, </pages> <year> 1991. </year>
Reference-contexts: all processors have a uniform workload is an essential primitive to many irregular problems, such as computational adaptive graph (grid) problems ([30], [19], [13]) including finite element calculations, molecular dynamics [24], particle dynamics [18], plasma particle-in-cell [20], raytraced volume rendering [22], region growing and computer vision [33], and statistical physics <ref> [7] </ref>, and, as we will show, the selection problem. The running time of 6 TRANSPOSE BROADCAST these parallel algorithms is categorized by the maximum running time of any of the p processors' subproblems.
Reference: [8] <author> V. Bala, J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.-T. Ho, S. Kipnis, and M. Snir. </author> <title> CCL: A Portable and Tunable Collective Communication Library for Scalable Parallel Computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6 </volume> <pages> 154-164, </pages> <year> 1995. </year>
Reference-contexts: Also, when reading or writing more than one element, bulk data transports are provided with corresponding bulk read and bulk write primitives. The first hierarchy of collective communication primitives are similar to those for the IBM POWERparallel machines <ref> [8] </ref>, the Cray MPP systems [15], standard message passing [29], and communication libraries for shared memory languages on distributed memory machines, such as Split-C [16], and include the following: bcast, reduce, combine, scatter, gather, concat, transpose, and prefix. A higher level primitive redist is described later for dynamic data redistribution. <p> Note that when p does not divide n evenly, the last processor will receive less than q elements. We refer to this as Method A. for 8 processors and 63 elements, with an arbitrary initial distribution of N = <ref> [10; 3; 2; 20; 0; 14; 6; 8] </ref>. Here, q j = l 8 = 8, for 0 j 6, while q 7 = 7, since P 7 receives the remainder of elements when p does not divide the total number of elements evenly.
Reference: [9] <author> P. Berthome, A. Ferreira, B.M. Maggs, S. Perennes, </author> <title> and C.G. Plaxton. Sorting-Based Selection Algorithms for Hypercubic Networks. </title> <booktitle> In Proceedings of the 7th International Parallel Processing Symposium, </booktitle> <pages> pages 89-95, </pages> <address> Newport Beach, CA, April 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [10] <author> G.E. Blelloch. </author> <title> Prefix sums and their applications. </title> <type> Technical Report CMU-CS-90-190, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> November </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: Parallel computers can handle this efficiently when the element P S [k] is assumed to reside on processor k <ref> [10] </ref>, and Split-C implements this as a primitive library function. An analysis for this operation on the BDM model is given in [4]. <p> Note that when p does not divide n evenly, the last processor will receive less than q elements. We refer to this as Method A. for 8 processors and 63 elements, with an arbitrary initial distribution of N = <ref> [10; 3; 2; 20; 0; 14; 6; 8] </ref>. Here, q j = l 8 = 8, for 0 j 6, while q 7 = 7, since P 7 receives the remainder of elements when p does not divide the total number of elements evenly.
Reference: [11] <author> J. Bruck, C.-T. Ho, S. Kipnis, and D. Weathersby. </author> <title> Efficient Algorithms for All-to-All Commu--nications in Multi-Port Message-Passing Systems. </title> <booktitle> In 6th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <volume> volume 6, </volume> <pages> pages 298-309, </pages> <address> Cape May, NJ, June 1994. </address> <publisher> ACM Press. </publisher>
Reference-contexts: This primitive is also known as the index operation ([8], <ref> [11] </ref>). The BDM algorithm and analysis for the TRANSPOSE data movement is given in [3] and is similar to that of the LogP model [17]. <p> However, parallel machine vendors, realizing the importance of fast primitives ([8], <ref> [11] </ref>, [29], and [15]), provide their own library calls which benefit from knowledge of and access to lower level machine specifics and optimizations. Communication primitives are considered to be a black box, where the implementation is unimportant from the user's perspective, as long as the primitives produce the correct results.
Reference: [12] <author> W.W. Carlson and J.M. Draper. </author> <title> AC for the T3D. </title> <type> Technical Report SRC-TR-95-141, </type> <institution> Supercomputing Research Center, Bowie, MD, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: We also acknowledge William Carlson and Jesse Draper from the Center for Computing Science (formerly Supercomputing Research Center) for writing the parallel compiler AC (version 2.6) <ref> [12] </ref> on which the T3D port of Split-C has been based. We would like to acknowledge the use of a 16-node IBM SP-2-TN2, which was provided by an IBM Shared University award and an NSF Research Infrastructure Initiative Grant No. CDA9401151.
Reference: [13] <author> A. Choudhary, G. Fox, S. Ranka, S. Hiranandani, K. Kennedy, C. Koelbel, and J. Saltz. </author> <title> Software Support for Irregular and Loosely Synchronous Problems. </title> <journal> International Journal of Computing Systems in Engineering, </journal> <pages> 3(1-4), </pages> <year> 1992. </year>
Reference-contexts: execution time is similar, and making use of a vendor's library can improve performance. 4 Dynamic Redistribution of Data The technique of dynamically redistributing data such that all processors have a uniform workload is an essential primitive to many irregular problems, such as computational adaptive graph (grid) problems ([30], [19], <ref> [13] </ref>) including finite element calculations, molecular dynamics [24], particle dynamics [18], plasma particle-in-cell [20], raytraced volume rendering [22], region growing and computer vision [33], and statistical physics [7], and, as we will show, the selection problem.
Reference: [14] <author> T.H. Cormen, C.E. Leiserson, and R.L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1990. </year>
Reference-contexts: Note that when p does not divide n evenly, the last processor will receive less than q elements. We refer to this as Method A. for 8 processors and 63 elements, with an arbitrary initial distribution of N = <ref> [10; 3; 2; 20; 0; 14; 6; 8] </ref>. Here, q j = l 8 = 8, for 0 j 6, while q 7 = 7, since P 7 receives the remainder of elements when p does not divide the total number of elements evenly.
Reference: [15] <institution> Cray Research, Inc. </institution> <note> SHMEM Technical Note for C, October 1994. Revision 2.3. </note>
Reference-contexts: Also, when reading or writing more than one element, bulk data transports are provided with corresponding bulk read and bulk write primitives. The first hierarchy of collective communication primitives are similar to those for the IBM POWERparallel machines [8], the Cray MPP systems <ref> [15] </ref>, standard message passing [29], and communication libraries for shared memory languages on distributed memory machines, such as Split-C [16], and include the following: bcast, reduce, combine, scatter, gather, concat, transpose, and prefix. A higher level primitive redist is described later for dynamic data redistribution. <p> However, parallel machine vendors, realizing the importance of fast primitives ([8], [11], [29], and <ref> [15] </ref>), provide their own library calls which benefit from knowledge of and access to lower level machine specifics and optimizations. Communication primitives are considered to be a black box, where the implementation is unimportant from the user's perspective, as long as the primitives produce the correct results.
Reference: [16] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, S. Luna, T. von Eicken, and K. Yelick. </author> <title> Introduction to Split-C. </title> <institution> Computer Science Division - EECS, University of Cali-fornia, Berkeley, </institution> <note> version 1.0 edition, </note> <month> March 6, </month> <year> 1994. </year>
Reference-contexts: The first hierarchy of collective communication primitives are similar to those for the IBM POWERparallel machines [8], the Cray MPP systems [15], standard message passing [29], and communication libraries for shared memory languages on distributed memory machines, such as Split-C <ref> [16] </ref>, and include the following: bcast, reduce, combine, scatter, gather, concat, transpose, and prefix. A higher level primitive redist is described later for dynamic data redistribution. Note that shared arrays are held in distributed memory across a set of processors.
Reference: [17] <author> D.E. Culler, R.M. Karp, D.A. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: This primitive is also known as the index operation ([8], [11]). The BDM algorithm and analysis for the TRANSPOSE data movement is given in [3] and is similar to that of the LogP model <ref> [17] </ref>. This TRANSPOSE communication algorithm has the following complexity: ( T comm (n; p) t + p ; T comp (n; p) = O (q): (3) 3.5 Communication Primitive: BCAST (A [r][x : x + q 1]) Another useful data movement primitive is BCAST broadcasting primitive.
Reference: [18] <author> L. Dagum. </author> <title> Three-Dimensional Direct Particle Simulation on the Connection Machine. </title> <type> RNR Technical Report RNR-91-022, </type> <institution> NASA Ames, NAS Division, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: library can improve performance. 4 Dynamic Redistribution of Data The technique of dynamically redistributing data such that all processors have a uniform workload is an essential primitive to many irregular problems, such as computational adaptive graph (grid) problems ([30], [19], [13]) including finite element calculations, molecular dynamics [24], particle dynamics <ref> [18] </ref>, plasma particle-in-cell [20], raytraced volume rendering [22], region growing and computer vision [33], and statistical physics [7], and, as we will show, the selection problem. The running time of 6 TRANSPOSE BROADCAST these parallel algorithms is categorized by the maximum running time of any of the p processors' subproblems.
Reference: [19] <author> J. De Keyser and D. Roose. </author> <title> Load Balacing Data Parallel Programs on Distributed Memory Computers. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1199-1219, </pages> <year> 1993. </year>
Reference-contexts: primitives, execution time is similar, and making use of a vendor's library can improve performance. 4 Dynamic Redistribution of Data The technique of dynamically redistributing data such that all processors have a uniform workload is an essential primitive to many irregular problems, such as computational adaptive graph (grid) problems ([30], <ref> [19] </ref>, [13]) including finite element calculations, molecular dynamics [24], particle dynamics [18], plasma particle-in-cell [20], raytraced volume rendering [22], region growing and computer vision [33], and statistical physics [7], and, as we will show, the selection problem.
Reference: [20] <author> K. Dincer. </author> <title> Particle-in-cell simulation codes in High Performance Fortran. </title> <type> Report SCCS-663, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, Syracuse, </institution> <address> NY, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: performance. 4 Dynamic Redistribution of Data The technique of dynamically redistributing data such that all processors have a uniform workload is an essential primitive to many irregular problems, such as computational adaptive graph (grid) problems ([30], [19], [13]) including finite element calculations, molecular dynamics [24], particle dynamics [18], plasma particle-in-cell <ref> [20] </ref>, raytraced volume rendering [22], region growing and computer vision [33], and statistical physics [7], and, as we will show, the selection problem. The running time of 6 TRANSPOSE BROADCAST these parallel algorithms is categorized by the maximum running time of any of the p processors' subproblems. <p> Note that when p does not divide n evenly, the last processor will receive less than q elements. We refer to this as Method A. for 8 processors and 63 elements, with an arbitrary initial distribution of N = <ref> [10; 3; 2; 20; 0; 14; 6; 8] </ref>. Here, q j = l 8 = 8, for 0 j 6, while q 7 = 7, since P 7 receives the remainder of elements when p does not divide the total number of elements evenly.
Reference: [21] <author> A.C. Dusseau. </author> <title> Modeling Parallel Sorts with LogP on the CM-5. </title> <type> Technical Report UCB//CSD-94-829, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1994. </year>
Reference-contexts: 1.98 32 0.571 Cray T3D 4 7.05 8 3.55 32 0.929 128 0.275 Meiko CS-2 16 3.03 32 1.55 32 2.77 Table I: Execution Times for the High-Level BDM Selection (in seconds) on the NAS IS input set Researchers Time (in seconds) Notes Bader & JaJa 2.77 BDM Selection Dusseau <ref> [21] </ref> 7.67 Radix Sort TMC [6] 4.31 Ranking without permuting the data Table II: Execution Time for Selection on a 32-processor CM-5 on the NAS IS input set content of this paper does not necessarily reflect the position or the policy of the government and no official endorsement should be inferred.
Reference: [22] <author> S. Goil and S. Ranka. </author> <title> Dynamic Load Balancing for Raytraced Volume Rendering on Distributed Memory Machines. </title> <type> Report SCCS-693, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, Syracuse, </institution> <address> NY, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: of Data The technique of dynamically redistributing data such that all processors have a uniform workload is an essential primitive to many irregular problems, such as computational adaptive graph (grid) problems ([30], [19], [13]) including finite element calculations, molecular dynamics [24], particle dynamics [18], plasma particle-in-cell [20], raytraced volume rendering <ref> [22] </ref>, region growing and computer vision [33], and statistical physics [7], and, as we will show, the selection problem. The running time of 6 TRANSPOSE BROADCAST these parallel algorithms is categorized by the maximum running time of any of the p processors' subproblems.
Reference: [23] <author> E. Hao, P.D. MacLenzie, and Q.F. Stout. </author> <title> Selection on the Reconfigurable Mesh. </title> <booktitle> In Proceedings of the 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 38-45, </pages> <address> McLean, VA, October 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: A more general problem is that of selection; namely, we have to find the element of rank i, for a given parameter i, 1 i n. Parallel sorting trivially solves the selection problem, but sorting is known to be computationally harder than selection. Previous parallel algorithms for selection ([9], <ref> [23] </ref>, [31], [25]) and data redistribution ([28], [34]) tend to be network dependent or assume the PRAM model, and thus, are not efficient or portable to current parallel machines. In this paper, we present algorithms that are shown to be scalable and efficient across a number of different platforms.
Reference: [24] <author> Y.-S. Hwang, R. Das, J. Saltz, B. Brooks, and M. Hodoscek. </author> <title> Parallelizing Molecular Dynamics Programs for Distributed Memory Machines: An Application of the CHAOS Runtime Support 21 Library. </title> <institution> Technical Report CS-TR-3374 and UMIACS-TR-94-125, Department of Computer Sci--ence and UMIACS, Univ. of Maryland, </institution> <year> 1994. </year>
Reference-contexts: of a vendor's library can improve performance. 4 Dynamic Redistribution of Data The technique of dynamically redistributing data such that all processors have a uniform workload is an essential primitive to many irregular problems, such as computational adaptive graph (grid) problems ([30], [19], [13]) including finite element calculations, molecular dynamics <ref> [24] </ref>, particle dynamics [18], plasma particle-in-cell [20], raytraced volume rendering [22], region growing and computer vision [33], and statistical physics [7], and, as we will show, the selection problem.
Reference: [25] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Parallel sorting trivially solves the selection problem, but sorting is known to be computationally harder than selection. Previous parallel algorithms for selection ([9], [23], [31], <ref> [25] </ref>) and data redistribution ([28], [34]) tend to be network dependent or assume the PRAM model, and thus, are not efficient or portable to current parallel machines. In this paper, we present algorithms that are shown to be scalable and efficient across a number of different platforms. <p> Note that the median finding algorithm is a special case of the selection problem where i is equal to d n 2 e. The output is the element from A with rank i. The parallel selection algorithm is motivated by similar sequential ([14], [32]) and parallel ([1], <ref> [25] </ref>) algorithms. We use recursion, where at each stage, a "good" element from the collection is chosen to split the input into two partitions, one consisting of all elements less than or equal to the splitter and the second consisting of the remaining elements.
Reference: [26] <author> J. JaJa and K.W. Ryu. </author> <title> The Block Distributed Memory Model. </title> <type> Technical Report CS-TR-3207, </type> <institution> Computer Science Department, University of Maryland, College Park, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Our algorithms will be described as shared-memory programs that make use of these primitives and do not make any assumptions of how these primitives are actually implemented. In our analysis, we use the BDM model and the results of <ref> [26] </ref> and [27]. The basic data transport is a read or write operation. The remote read and write typically have both blocking and non-blocking versions. Also, when reading or writing more than one element, bulk data transports are provided with corresponding bulk read and bulk write primitives. <p> An efficient BDM algorithm is given ([3], <ref> [26] </ref>) which takes q elements (q p) on a single processor and broadcasts them to the other p 1 processors using just two TRANSPOSE Communication Primitives. <p> This algorithm is identical in complexity to Eq. (2). On the other hand, this problem can be solved using k-ary balanced tree algorithm <ref> [26] </ref>, in which case the communication would be T comm (n; p) 2 (2t log k p + p). For larger q, a more efficient algorithm to broadcast the q elements from a single processor to p processors is based on the TRANSPOSE primitive. <p> We perform the TRANSPOSE (A) primitive, thus, giving every processor q p elements. Each processor then locally rearranges the data so that an additional 4 TRANSPOSE data movement will result in each processor holding a copy of all the q elements in its column of A <ref> [26] </ref>. The analysis of this BCAST algorithm is simple.
Reference: [27] <author> J.F. JaJa and K.W. Ryu. </author> <title> The Block Distributed Memory Model for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <pages> pages 752-756, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> (Extended Abstract). </note>
Reference-contexts: A parallel selection algorithm is described and analyzed in Section 5, together with experimental results on a number of platforms. 2 The Block Distributed Memory Model We use the Block Distributed Memory (BDM) Model ([26], <ref> [27] </ref>) as a computation model for developing and analyzing our parallel algorithms on distributed memory machines. This model allows the design of algorithms using a single address space and does not assume any particular interconnection topology. <p> Our algorithms will be described as shared-memory programs that make use of these primitives and do not make any assumptions of how these primitives are actually implemented. In our analysis, we use the BDM model and the results of [26] and <ref> [27] </ref>. The basic data transport is a read or write operation. The remote read and write typically have both blocking and non-blocking versions. Also, when reading or writing more than one element, bulk data transports are provided with corresponding bulk read and bulk write primitives. <p> We present two methods for the dynamic redistribution of data which remap the data such that no processor contains more than the average number of data elements. The first method is similar to a method presented in ([26], <ref> [27] </ref>), and only a brief sketch will be given.
Reference: [28] <author> K. Mehrotra, S. Ranka, and J.-C. Wang. </author> <title> A Probabilistic Analysis of a Locality Maintaining Load Balancing Algorithm. </title> <booktitle> In Proceedings of the 7th International Parallel Processing Symposium, </booktitle> <pages> pages 369-373, </pages> <address> Newport Beach, CA, April 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: For a large class of irregular problems such that data are distributed with a certain class of distributions, it has been shown that the distribution is typically closer to the first scenario, (N [i] m; 8i) <ref> [28] </ref>. 4.2 Dynamic Data Redistribution: Method B A more efficient dynamic data redistribution algorithm, here referred to as Method B, makes use of the fact that a processor initially filled with at least q elements should not need to receive any more elements, but instead, should send its excess to other
Reference: [29] <author> Message Passing Interface Forum. </author> <title> A Message Passing Interface Standard. </title> <type> Technical Report CS-94-230, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Also, when reading or writing more than one element, bulk data transports are provided with corresponding bulk read and bulk write primitives. The first hierarchy of collective communication primitives are similar to those for the IBM POWERparallel machines [8], the Cray MPP systems [15], standard message passing <ref> [29] </ref>, and communication libraries for shared memory languages on distributed memory machines, such as Split-C [16], and include the following: bcast, reduce, combine, scatter, gather, concat, transpose, and prefix. A higher level primitive redist is described later for dynamic data redistribution. <p> However, parallel machine vendors, realizing the importance of fast primitives ([8], [11], <ref> [29] </ref>, and [15]), provide their own library calls which benefit from knowledge of and access to lower level machine specifics and optimizations. Communication primitives are considered to be a black box, where the implementation is unimportant from the user's perspective, as long as the primitives produce the correct results.
Reference: [30] <author> C.-W. Ou and S. Ranka. </author> <title> Parallel Remapping Algorithms for Adaptive Problems. </title> <booktitle> In Proceedings of the 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 367-374, </pages> <address> McLean, VA, February 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [31] <author> R. Sarnath and X. </author> <title> He. Efficient parallel algorithms for selection and searching on sorted matrices. </title> <booktitle> In Proceedings of the 6th International Parallel Processing Symposium, </booktitle> <pages> pages 108-111, </pages> <address> Beverly Hills, CA, March 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Parallel sorting trivially solves the selection problem, but sorting is known to be computationally harder than selection. Previous parallel algorithms for selection ([9], [23], <ref> [31] </ref>, [25]) and data redistribution ([28], [34]) tend to be network dependent or assume the PRAM model, and thus, are not efficient or portable to current parallel machines. In this paper, we present algorithms that are shown to be scalable and efficient across a number of different platforms.
Reference: [32] <author> R. Sedgewick. </author> <title> Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: Note that the median finding algorithm is a special case of the selection problem where i is equal to d n 2 e. The output is the element from A with rank i. The parallel selection algorithm is motivated by similar sequential ([14], <ref> [32] </ref>) and parallel ([1], [25]) algorithms. We use recursion, where at each stage, a "good" element from the collection is chosen to split the input into two partitions, one consisting of all elements less than or equal to the splitter and the second consisting of the remaining elements.
Reference: [33] <author> C. Weems, E. Riseman, A. Hanson, and A. Rosenfeld. </author> <title> The DARPA Image Understanding Benchmark for Parallel Computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 </volume> <pages> 1-24, </pages> <year> 1991. </year>
Reference-contexts: redistributing data such that all processors have a uniform workload is an essential primitive to many irregular problems, such as computational adaptive graph (grid) problems ([30], [19], [13]) including finite element calculations, molecular dynamics [24], particle dynamics [18], plasma particle-in-cell [20], raytraced volume rendering [22], region growing and computer vision <ref> [33] </ref>, and statistical physics [7], and, as we will show, the selection problem. The running time of 6 TRANSPOSE BROADCAST these parallel algorithms is categorized by the maximum running time of any of the p processors' subproblems.
Reference: [34] <author> J. Woo and S. Sahni. </author> <title> Load Balancing on a Hypercube. </title> <booktitle> In Proceedings of the 5th International Parallel Processing Symposium, </booktitle> <pages> pages 525-530, </pages> <address> Anaheim, CA, April 1991. </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 22 </pages>
Reference-contexts: Parallel sorting trivially solves the selection problem, but sorting is known to be computationally harder than selection. Previous parallel algorithms for selection ([9], [23], [31], [25]) and data redistribution ([28], <ref> [34] </ref>) tend to be network dependent or assume the PRAM model, and thus, are not efficient or portable to current parallel machines. In this paper, we present algorithms that are shown to be scalable and efficient across a number of different platforms. The organization of this paper is as follows.
References-found: 34

