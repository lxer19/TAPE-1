URL: http://www.cs.indiana.edu/l/www/pub/gasser/morphology95.ps.Z
Refering-URL: http://www.cs.indiana.edu/l/www/pub/gasser/
Root-URL: http://www.cs.indiana.edu
Email: gasser@indiana.edu  
Title: Transfer in a Connectionist Model of the Acquisition of Morphology  
Author: Michael Gasser 
Note: Expanded version of paper presented at the Morphology Workshop, Nijmegen, June 13, 1995.  
Affiliation: Computer Science and Linguistics Departments, Cognitive Science Program Indiana University  
Abstract: The morphological systems of natural languages are replete with examples of the same devices used for multiple purposes: (1) the same type of morphological process (for example, suffixation for both noun case and verb tense) and (2) identical morphemes (for example, the same suffix for English noun plural and possessive). These sorts of similarity would be expected to convey advantages on language learners in the form of transfer from one morphological category to another. Connectionist models of morphology acquisition have been faulted for their supposed inability to represent phonological similarity across morphological categories and hence to facilitate transfer. This paper describes a connectionist model of the acquisition of morphology which is shown to exhibit transfer of this type. The model treats the morphology acquisition problem as one of learning to map forms onto meanings and vice versa. As the network learns these mappings, it makes phonological generalizations which are embedded in connection weights. Since these weights are shared by different morphological categories, transfer is enabled. In a set of experiments with artificial stimuli, networks were trained first on one morphological task (e.g., tense) and then on a second (e.g., number). It is shown that in the context of suffixation, prefixation, and template rules, the second task is facilitated when the second category either makes use of the same forms or the same general process type (e.g., prefixation) as the first. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cottrell, G. W. & Plunkett, K. </author> <year> (1991). </year> <title> Learning the past tense in a recurrent network: acquiring the mapping from meaning to sounds. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 13, </volume> <pages> 328-333. </pages>
Reference: <author> Daugherty, K. & Seidenberg, M. </author> <year> (1992). </year> <title> Rules or connections? the past tense revisited. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 14, </volume> <pages> 259-264. </pages>
Reference: <author> Gasser, M. </author> <year> (1992). </year> <title> Learning distributed syllable representations. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 14, </volume> <pages> 396-401. </pages>
Reference-contexts: In the perception module, phonology is learned as a side-effect as the system is trained to recognize words. Phonological knowledge takes the form of the weights on the connections from the input (phonetic) layer of processing units to the recurrent hidden layer of units. I have shown elsewhere <ref> (Gasser, 1992) </ref> that the patterns of activation appearing on this hidden layer embody generalizations about the phonological structure found in the input forms and can provide a basis for learning in the production module of the system.
Reference: <author> Gasser, M. </author> <year> (1994a). </year> <title> Acquiring receptive morphology: a connectionist model. </title> <booktitle> Annual Meeting of the Association for Computational Linguistics, </booktitle> <volume> 32, </volume> <pages> 279-286. </pages>
Reference-contexts: For each morpheme, the network's response is taken to be the morpheme which its output is closest to. Performance is evaluated separately for each morphological category, that is, for the root and each inflection in a word. I have demonstrated elsewhere <ref> (Gasser, 1994a) </ref> that the perception component has the capacity to learn prefixation, suffixation, circumfixation, infixation, deletion, mutation, and template rules. 7 I have also shown that performance is always superior with a version of the model in which root and inflection recognition are handled by separate hidden-layer modules (Gasser, 1994b). <p> In this paper, all experiments make use of this modular version 7 Reduplication and metathesis are not accommodated by the simple segment-based model; these would require a hierarchical version of the network which has not yet been implemented <ref> (Gasser, 1994a) </ref>. 9 of the perception component. For production, I will describe only the training of the syllable-to-phone module. The inputs to this network are the distributed syllable representations which appear on the hidden layer of the corresponding perception network following training.
Reference: <author> Gasser, M. </author> <year> (1994b). </year> <title> Modularity in a connectionist model of morphology acquisition. </title> <booktitle> Proceedings of the International Conference on Computational Linguistics, </booktitle> <volume> 15, </volume> <pages> 214-220. </pages>
Reference-contexts: demonstrated elsewhere (Gasser, 1994a) that the perception component has the capacity to learn prefixation, suffixation, circumfixation, infixation, deletion, mutation, and template rules. 7 I have also shown that performance is always superior with a version of the model in which root and inflection recognition are handled by separate hidden-layer modules <ref> (Gasser, 1994b) </ref>. In the modular version, shown in Figure 2 the input (phone) layer is connected to both hidden-layer groups of units.
Reference: <author> MacWhinney, B. & Leinbach, J. </author> <year> (1991). </year> <title> Implementations are not conceptualization: revising the verb learning model. </title> <journal> Cognition, </journal> <volume> 40, </volume> <pages> 121-157. </pages>
Reference: <author> Pinker, S. & Prince, A. </author> <year> (1988). </year> <title> On language and connectionism: analysis of a parallel distributed processing model of language acquisition. </title> <journal> Cognition, </journal> <volume> 28, </volume> <pages> 73-193. </pages>
Reference-contexts: the Rumelhart and McClelland model of the acquisition of the English past tense (Rumelhart & McClel-land, 1986), Pinker and Prince fault the model on these grounds, for what they call "morphological localism": the English past tense forms are learned in a network which is dedicated to this morphological task alone <ref> (Pinker & Prince, 1988) </ref>. In the Rumelhart and McClelland model, as in most of the succeeding connectionist models of morphology acquisition (Daugherty & Seidenberg, 1992; MacWhinney & Leinbach, 1991; Plunkett & Marchman, 1991), morphology learning consists in learning to map a stem onto an affixed form.
Reference: <author> Plunkett, K. & Marchman, V. </author> <year> (1991). </year> <title> U-shaped learning and frequency effects in a multi-layered perceptron: implications for child language acquisition. </title> <journal> Cognition, </journal> <volume> 38, </volume> <pages> 1-60. </pages>
Reference: <author> Rumelhart, D. E. & McClelland, J. L. </author> <year> (1986). </year> <title> On learning the past tense of English verbs. </title> <editor> In McClelland, J. L. & Rumelhart, D. E. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Volume 2, </volume> <pages> pp. 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: If there is complete modularity between the parts of the system dedicated to the different tasks, then no generalization is possible. In their critique of the Rumelhart and McClelland model of the acquisition of the English past tense <ref> (Rumelhart & McClel-land, 1986) </ref>, Pinker and Prince fault the model on these grounds, for what they call "morphological localism": the English past tense forms are learned in a network which is dedicated to this morphological task alone (Pinker & Prince, 1988). <p> For each input phone the hidden layer and output layer of units are activated in turn. The network's output is compared to the target pattern, an error is calculated, and the network's weights are adjusted accordingly with the familiar back-propagation learning algorithm <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>. For purposes of evaluating the performance of the perception module of the network, the output of the module is examined following the presentation of the word-final boundary pattern.
Reference: <author> Rumelhart, D. E., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Volume 1, </volume> <pages> pp. 318-364. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <month> 21 </month>
Reference-contexts: If there is complete modularity between the parts of the system dedicated to the different tasks, then no generalization is possible. In their critique of the Rumelhart and McClelland model of the acquisition of the English past tense <ref> (Rumelhart & McClel-land, 1986) </ref>, Pinker and Prince fault the model on these grounds, for what they call "morphological localism": the English past tense forms are learned in a network which is dedicated to this morphological task alone (Pinker & Prince, 1988). <p> For each input phone the hidden layer and output layer of units are activated in turn. The network's output is compared to the target pattern, an error is calculated, and the network's weights are adjusted accordingly with the familiar back-propagation learning algorithm <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>. For purposes of evaluating the performance of the perception module of the network, the output of the module is examined following the presentation of the word-final boundary pattern.
References-found: 10

