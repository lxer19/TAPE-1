URL: http://www.ececs.uc.edu/~sweaver/postscript/ja15July.ps
Refering-URL: http://www.ececs.uc.edu/~sweaver/
Root-URL: 
Email: Email: scott.weaver@uc.edu  
Phone: 3  
Title: An Analytical Framework for Local Feedforward Networks  
Author: Scott Weaver ; Leemon Baird Marios Polycarpou 
Date: September 1996; Revised: June 1997.  
Note: 2 Wright-Patterson Air Force Base WL/AACF 2241 Avionics  IEEE Transactions on Neural Networks Submitted:  
Address: 45221-0030  WPAFB, Ohio 45433-7318  5000 Forbes Avenue  Pittsburgh, Pennsylvania 15213-3891  
Affiliation: 1 Department of Electrical and Computer Engineering University of Cincinnati Cincinnati, Ohio  Circle  Computer Science Department  Carnegie Mellon University  
Abstract: Interference in neural networks occurs when learning in one area of the input space causes unlearning in another area. Networks that are less susceptible to interference are referred to as spatially local networks. To obtain a better understanding of these properties, a theoretical framework, consisting of a measure of interference and a measure of network localization, is developed. These measures incorporate not only the network weights and architecture but also the learning algorithm. Using this framework to analyze sigmoidal, multi-layer perceptron (MLP) networks that employ the back-propagation learning algorithm on the quadratic cost function, we address a familiar misconception that single-hidden-layer, sigmoidal networks are inherently non-local by demonstrating that given a sufficiently large number of adjustable weights, single-hidden-layer, sigmoidal MLPs exist that are arbitrarily local and retain the ability to approximate any continuous function on a compact domain. fl Partially supported under Task 2312 R1 by the United States Air Force Office of Scientific Research.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Sofge and D. White, </author> <title> "Applied learning: optimal control for manufacturing," in Handbook of Intelligent Control Neural, Fuzzy, and Adaptive Approaches (D. </title> <editor> White and D. Sofge, eds.), </editor> <address> (New York, NY), </address> <pages> pp. 259-281, </pages> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of localization <ref> [1, 2, 3, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the localization properties of a learning system. <p> The general problem of interference has been uncovered in various forms by researchers in many areas <ref> [5, 1] </ref>. For example, consider a dynamical system after it settles into a desired trajectory (where only a small portion of the input space is reached). Suppose that without noise, a network function approximator learns the system dynamics, reduces the approximation error, and then ceases learning. <p> remains active and continually memorizes the system dynamics along the trajectory (because the error never goes to zero) even though there is no need to do so. "Global Network Collapse" results, as the other areas of the input space (those areas not on the trajectory) gradually unlearn due to interference <ref> [1] </ref>. Another variant of the interference problem is in the classification literature: "Catastrophic Interference" occurs when the training of a new pattern causes the unlearning of originally trained patterns [5]. <p> = i=1 (8) where w = [a 1 a 8 b 1 b 8 c 1 c 8 ] T , and a i = 1, b i = 1, c i = i=7, for i 2 f0; 7g: The centers are equally spaced along the input domain X = <ref> [0; 1] </ref>. <p> With an appropriate choice of w k and N k , we show the following two statements: 1. I f k ;w k ;H (x; x 0 ) is bounded for all k 2 N; x; x 0 2 <ref> [0; 1] </ref> n . 2. The limit of I f k ;w k ;H (x; x 0 ) as k approaches infinity is zero almost everywhere. <p> by 1 + n, we see interference has an upper bound that is not a function of x or k, that is, I f k ;w k ;H (x; x 0 ) ^xkz k (x; w k )k 2 B 1 = B (41) for all x; x 0 2 <ref> [0; 1] </ref> n . In similar fashion one can show interference is bounded below by B. Now we show that lim k!1 I f k ;w k ;H (x; x 0 ) = 0 almost everywhere. <p> At this point we have met the conditions of the Lebesgue Dominated Convergence Theorem : I f k ;w k ;H (x; x 0 ) is a sequence of integrable functions on <ref> [0; 1] </ref> n fi [0; 1] n . Because there exists a bound B &gt; 0 such that jI f k ;w k ;H (x; x 0 )j B for all k 2 N; x; x 0 2 [0; 1] n and (45) is an integrable function, we see lim E <p> At this point we have met the conditions of the Lebesgue Dominated Convergence Theorem : I f k ;w k ;H (x; x 0 ) is a sequence of integrable functions on <ref> [0; 1] </ref> n fi [0; 1] n . Because there exists a bound B &gt; 0 such that jI f k ;w k ;H (x; x 0 )j B for all k 2 N; x; x 0 2 [0; 1] n and (45) is an integrable function, we see lim E [I f k ;w <p> ;H (x; x 0 ) is a sequence of integrable functions on <ref> [0; 1] </ref> n fi [0; 1] n . Because there exists a bound B &gt; 0 such that jI f k ;w k ;H (x; x 0 )j B for all k 2 N; x; x 0 2 [0; 1] n and (45) is an integrable function, we see lim E [I f k ;w k ;H (x; x 0 ) 2 ] = E [ lim I f k ;w k ;H (x; x 0 ) 2 ] = 0 (46) for X = [0; 1] n . <p> x 0 2 <ref> [0; 1] </ref> n and (45) is an integrable function, we see lim E [I f k ;w k ;H (x; x 0 ) 2 ] = E [ lim I f k ;w k ;H (x; x 0 ) 2 ] = 0 (46) for X = [0; 1] n . Equation (46) implies that there exists a k such that E [I f k ;w k ;H (x; x 0 ) 2 ] &lt; * for arbitrary * &gt; 0 and hence L f k ;w k ;H;X can be made arbitrarily large.
Reference: [2] <author> J. H. Friedman, </author> <title> "Local learning based on recursive covering." </title> <journal> submitted to The Annals of Statistics, </journal> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of localization <ref> [1, 2, 3, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the localization properties of a learning system.
Reference: [3] <author> J. Sjoberg, Q. Zhang, L. Ljung, A. Benveniste, B. Deylon, P.-Y. Glorennec, H. Hjalmarsson, and A. Juditsky, </author> <title> "Nonlinear black-box modeling in system identification: A unified overview," </title> <journal> Automatica, </journal> <volume> vol. 31, </volume> <pages> pp. 1691-1724, </pages> <year> 1995. </year>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of localization <ref> [1, 2, 3, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the localization properties of a learning system. <p> The functions shown in Figure 5 (b) and 5 (c), however, vanish rapidly at positive and negative infinity and therefore exhibit local properties (see Sjoberg et al. <ref> [3] </ref>).
Reference: [4] <author> W. Baker and J. Farrell, </author> <title> "An introduction to connectionist learning control systems," in Handbook of Intelligent Control Neural, Fuzzy, and Adaptive Approaches (D. </title> <editor> White and D. Sofge, eds.), </editor> <address> (New York, NY), </address> <pages> pp. 35-63, </pages> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of localization <ref> [1, 2, 3, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the localization properties of a learning system. <p> The network architecture and weights help determine the interference that occurs and hence play an important role in determining network localization. 1 Baker and Farrell <ref> [4] </ref> use the term "coverage" for this condition. 11 0 -1 1 partial of f wrt a (a) 0 x partial of f wrt b (b) 0 x partial of f wrt c (c) P N with respect to an amplitude a i , inverted width b i , and center
Reference: [5] <author> R. </author> <title> French, "Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference," </title> <booktitle> in Proceedings of the 16th Annual Cognitive Science Society Conference, </booktitle> <volume> vol. 5, </volume> <pages> pp. 207-220, </pages> <year> 1994. </year>
Reference-contexts: The general problem of interference has been uncovered in various forms by researchers in many areas <ref> [5, 1] </ref>. For example, consider a dynamical system after it settles into a desired trajectory (where only a small portion of the input space is reached). Suppose that without noise, a network function approximator learns the system dynamics, reduces the approximation error, and then ceases learning. <p> Another variant of the interference problem is in the classification literature: "Catastrophic Interference" occurs when the training of a new pattern causes the unlearning of originally trained patterns <ref> [5] </ref>. These and other interference problems may appear different when embedded in their particular applications but the root of these problems is the same; learning tends to interfere with previous learning elsewhere in the input space.
Reference: [6] <author> A. G. Barto, </author> <title> "Connectionist learning for control," in Neural Networks for Control (I. </title> <editor> W. Thomas Miller, R. S. Sutton, and P. J. </editor> <title> Werbos, </title> <type> eds.), </type> <institution> (Massachusetts Institute of Technology, </institution> <address> MA), </address> <pages> pp. 5-58, </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Although local networks, in general, lessen the problem of interference, there are trade-offs to consider (see Barto <ref> [6] </ref> for a nice summary). For example, look-up tables can be thought of as the most 2 local of approximation structures because there is a one-to-one relationship between a point in the in-put space and an adjustable parameter.
Reference: [7] <author> R. E. Bellman, </author> <title> Adaptive control processes: A guided tour. </title> <publisher> Princeton University, </publisher> <address> NJ: </address> <publisher> Princeton University Press, </publisher> <year> 1961. </year>
Reference-contexts: However, look-up tables are obviously inappropriate when the dimension of the problem grows large because the curse of dimensionality <ref> [7] </ref> causes memory requirements to become prohibitive; furthermore, look-up tables provide no generalization of untrained points. Finding the correct balance between avoiding interference problems, reducing memory requirements, and enhancing generalization, that is, finding a balance between local versus non-local networks, is a key problem in network learning.
Reference: [8] <author> M. A. Cohen and S. Grossberg, </author> <title> "Absolute stability of global pattern formation and parallel memory storage by competitive neural networks," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 5, </volume> <pages> pp. 815-826, </pages> <year> 1983. </year> <month> 22 </month>
Reference-contexts: Finding the correct balance between avoiding interference problems, reducing memory requirements, and enhancing generalization, that is, finding a balance between local versus non-local networks, is a key problem in network learning. The trade-offs involved in local learning systems are closely related to the well-known stability-plasticity dilemma <ref> [8] </ref>, namely how to design a learning system that is "plastic" enough to learn new patterns, and yet is stable enough to remember old learned patterns.
Reference: [9] <author> G. A. Carpenter and S. Grossberg, </author> <title> "A massively parallel architecture for a self-organizing neural pattern recognition machine," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 37, </volume> <pages> pp. 54-115, </pages> <year> 1987. </year>
Reference-contexts: The trade-offs involved in local learning systems are closely related to the well-known stability-plasticity dilemma [8], namely how to design a learning system that is "plastic" enough to learn new patterns, and yet is stable enough to remember old learned patterns. Carpenter and Grossberg <ref> [9] </ref> developed an architectural solution to this question using their adaptive resonance theory (ART), which overcomes the stability-plasticity dilemma by adapting the stored pattern of a category only when the input is sufficiently similar to it. This paper develops analytical tools necessary to measure the localization properties of a network.
Reference: [10] <author> K.-I. Funahashi, </author> <title> "On the approximate realization of continuous mappings by neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 183-192, </pages> <year> 1989. </year>
Reference-contexts: This leads us to a new question, "Does there exist a single-hidden-layer MLP network that is arbitrarily local and approximates any smooth function arbitrarily closely in a compact set?" The following theorem combines the universal-localization theorem of Section 3 and well-known, universal-approximation results of single-hidden-layer sigmoidal MLPs. (See, for example, <ref> [10, 11, 12] </ref>). Theorem 2 Let X be a compact subset of R n , H = er w h (x; w), and g fl (x) be a real valued continuous function on X . <p> This proof is a combination of the proof of Theorem 1 and a universal-approximation theorem in <ref> [10] </ref>, which says that given a real-valued continuous function g fl (x) on X , then for arbitrary * &gt; 0 there exist a number of nodes N , and weights w, such that g (x; w) = P N c i j=1 b ij x j ) 1 satisfies max
Reference: [11] <author> G. Cybenko, </author> <title> "Approximation by superpositions of a sigmoidal function," Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 303-314, </pages> <year> 1989. </year>
Reference-contexts: This leads us to a new question, "Does there exist a single-hidden-layer MLP network that is arbitrarily local and approximates any smooth function arbitrarily closely in a compact set?" The following theorem combines the universal-localization theorem of Section 3 and well-known, universal-approximation results of single-hidden-layer sigmoidal MLPs. (See, for example, <ref> [10, 11, 12] </ref>). Theorem 2 Let X be a compact subset of R n , H = er w h (x; w), and g fl (x) be a real valued continuous function on X .
Reference: [12] <author> K. Hornik, M. Stinchcombe, and H. White, </author> <title> "Multilayer feedforward networks are universal ap-proximators," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 359-366, </pages> <year> 1989. </year>
Reference-contexts: This leads us to a new question, "Does there exist a single-hidden-layer MLP network that is arbitrarily local and approximates any smooth function arbitrarily closely in a compact set?" The following theorem combines the universal-localization theorem of Section 3 and well-known, universal-approximation results of single-hidden-layer sigmoidal MLPs. (See, for example, <ref> [10, 11, 12] </ref>). Theorem 2 Let X be a compact subset of R n , H = er w h (x; w), and g fl (x) be a real valued continuous function on X .
Reference: [13] <author> R. Bartle, </author> <title> The Elements of Real Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1976. </year> <month> 23 </month>
Reference-contexts: To show kz k ()k is bounded from above we use the ratio test (Bartle <ref> [13] </ref> p. 296) and see that lim g (i + 1) = lim e 2 e i + 1 ! 4 therefore P 1 i=0 g (i) is convergent and less then some value B 2 and therefore kz k ()k 2 &lt; 2B 2 .
References-found: 13

