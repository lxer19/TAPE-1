URL: file://ftp.cc.gatech.edu/pub/ai/students/carlos/papers/dual-control.ps.gz
Refering-URL: http://www.cc.gatech.edu/aimosaic/robot-lab/publications/learning.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fcarlos ashwing@cc.gatech.edu  
Title: A New Heuristic Approach for Dual Control  
Author: Juan C. Santamara and Ashwin Ram 
Affiliation: Georgia Institute of Technology  
Date: July 1997.  
Address: Providence, Rhode Island,  Atlanta, GA 30332-0280  
Note: To appear in the Proceedings of the AAAI-97 Workshop on On-line Search,  
Abstract: Autonomous agents engaged in a continuous interaction with an incompletely known environment face the problem of dual control [Fel'dbaum 1965]. Simply stated, actions are necessary not only for studying the environment, but also for making progress on the task. In other words, actions must bear a "dual" character: They must be investigators to some degree, but also directors to some degree. Because the number of variables involved in the solution of the dual control problem increases with the number of decision stages, the exact solution of the dual control problem is computationally intractable except for a few special cases. This paper provides an overview of dual control theory and proposes a heuristic approach towards obtaining a near-optimal dual control method that can be implemented. The proposed algorithm selects control actions taking into account the information contained in past observations as well as the possible information that future observations may reveal. In short, the algorithm anticipates the fact that future learning is possible and selects the control actions accordingly. The algorithm uses memory-based methods to associate long-term benefit estimates to belief states and actions, and selects the actions to execute next according to such estimates. The algorithm uses the outcome of every experience to progressively refine the long-term benefit estimates so that it can make better, improved decisions as it progresses. The algorithm is tested on a classical simulation problem. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkeson, C. G. </author> <year> 1993. </year> <title> Local trajectory optimizers. </title> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle>
Reference: <author> Bar-Shalom, Y. </author> <year> 1990. </year> <title> Stochastic dynamic programming: Caution and probing. </title> <journal> IEEE Transactions on Automatic Control AC-26(5):216-224. </journal>
Reference: <author> Bellman, R. </author> <year> 1957. </year> <title> Dynamic Programming. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference: <author> Bertsekas, D. P. </author> <year> 1995. </year> <title> Dynamic Programming and Optimal Control, </title> <booktitle> volume 1. </booktitle> <address> Belmont, MA: </address> <publisher> Athena Scientific. </publisher>
Reference: <author> Cassandra, A. R.; Kaelbling, L. P.; and Littman, M. L. </author> <year> 1994. </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> 1023-1028. </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> DeGroot, M. </author> <year> 1970. </year> <title> Optimal Statistical Decisions. </title> <address> New York, NY: </address> <publisher> McGraw Hill. </publisher>
Reference: <author> Fel'dbaum, A. A. </author> <year> 1965. </year> <title> Optimal Control Systems. </title> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Jaakkola, T.; Singh, S. P.; and Jordan, M. I. </author> <year> 1995. </year> <title> Reinforcement learning algorithm for partially observable markov decision problems. </title> <editor> In Tesauro, G.; Touretzky, D.; and Leen, T., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Lin, L. J. </author> <year> 1992. </year> <title> Self-improving reactive agents based on reinforcement learning. </title> <booktitle> Machine Learning 8(3-4):293-321. </booktitle>
Reference: <author> Meier, L. </author> <year> 1965. </year> <title> Combined optimal control and estimation. </title> <booktitle> In Proceedings of the 3rd Annual Allerton Conference on Circuit and System Theory. </booktitle>
Reference: <author> Narendra, K. S., and Annaswamy, A. M. </author> <year> 1989. </year> <title> Stable Adaptive Systems. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference: <author> Parr, R., and Russell, S. </author> <year> 1995. </year> <title> Approximationg optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1088-1094. </pages> <address> Montreal, Quebec, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ram, A., and Leake, D. B. </author> <year> 1995. </year> <title> Learning, goals, and learning goals. </title> <editor> In Ram, A., and Leake, D. B., eds., </editor> <booktitle> Goal-Driven Learning. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <address> chapter 1. </address>
Reference: <author> Ross, S. M. </author> <year> 1993. </year> <title> Introduction to Probability Models. </title> <address> Boston, MA: </address> <publisher> Academic Press. </publisher>
Reference: <author> Rummery, G. A., and Niranjan, M. </author> <year> 1994. </year> <title> On-line q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFEG/TR66, </type> <institution> Cambridge University Department. </institution>
Reference: <author> Santamara, J. C.; Sutton, R. S.; and Ram, A. </author> <year> 1996. </year> <title> Experiments with reinforcement learning in problems with continuous state and actions spaces. </title> <type> Technical Report UM-CS-1996-088, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Singh, S. P., and Sutton, R. S. </author> <year> 1996. </year> <title> Reinforcement learning with replacing eligibility traces. </title> <booktitle> Machine Learning 22 </booktitle> <pages> 123-158. </pages>
Reference: <author> Stengel, R. F. </author> <year> 1994. </year> <title> Optimal Control and Estimation. </title> <address> Mineola, NY: </address> <publisher> Dover Publications. </publisher>
Reference: <author> Sutton, R. S. </author> <year> 1988. </year> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> 1990. </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Machine Learning: Proceedings of the Seventh International Conference, </booktitle> <pages> 216-224. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. </author> <year> 1996. </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle>
Reference: <author> Tsitsiklis, J. N., and Van Roy, B. </author> <year> 1996. </year> <title> Analysis of temporal-difference learning with function approximation. </title> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle>
Reference: <author> Watkins, C. J. C. H. </author> <year> 1989. </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. Dissertation, </type> <institution> Univeristy of Cambridge, Eng-land. </institution>
References-found: 23

