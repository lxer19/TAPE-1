URL: http://www.cs.umn.edu/Users/dept/users/kumar/sparse-2D-trisolve.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: A High Performance Two Dimensional Scalable Parallel Algorithm for Solving Sparse Triangular Systems  
Author: Mahesh V. Joshi Anshul Gupta George Karypis Vipin Kumar 
Abstract: Solving a system of equations of the form T x = y, where T is a sparse triangular matrix, is required after the factorization phase in the direct methods of solving systems of linear equations. A few parallel formulations have been proposed recently. The common belief in parallelizing this problem is that the parallel formulation utilizing a two dimensional distribution of T is unscalable. In this paper, we propose the first known efficient scalable parallel algorithm which uses a two dimensional block cyclic distribution of T . The algorithm is shown to be applicable to dense as well as sparse triangular solvers. Since most of the known highly scalable algorithms employed in the factorization phase yield a two dimensional distribution of T , our algorithm avoids the redistribution cost incurred by the one dimensional algorithms. We present the parallel runtime and scalability analyses of the proposed two dimensional algorithm. The dense triangular solver is shown to be scalable. The sparse triangular solver is shown to be at least as scalable as the dense solver. We also show that it is optimal for one class of sparse systems. The experimental results of the sparse triangular solver show that it has good speedup characteristics and yields high performance for a variety of sparse systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anshul Gupta, George Karypis and Vipin Ku-mar, </author> <title> Highly Scalable Parallel Algorithms for Sparse Matrix Factorization, </title> <type> Technical Report 94-63, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year>
Reference-contexts: Most of the research in the domain of direct methods has been concentrated on developing fast and scalable algorithms for factorization, since it is compu-tationally the most expensive phase of a direct solver. Recently some very highly scalable parallel algorithms have been proposed for sparse matrix factorizations <ref> [1] </ref>, which require the matrix to be distributed among processors using a two dimensional mapping. This results in a two dimensional distribution of the triangular factor matrices used by the triangular solvers to get the final solution. <p> Our triangular solver uses the same distribution of the factor matrix, T , as used by the highly scalable algorithm employed in the numerical factorization phase <ref> [1] </ref>. Thus, there is no cost incurred in redistributing the data. Distribution of T is described in the next paragraph. We assume that the supernodal tree is binary in the top log p levels 1 , where p is the number of pro-cessors used to solve the problem. <p> The processor assigned to a given element of a supernodal matrix is determined by the row and column indices of the element and a bitmask determined by the depth of the supern-ode in the supernodal tree <ref> [1] </ref>. This method achieves a two dimensional block cyclic distribution of the su-pernodal matrix among the logical grid of processors in the subcube as shown in Figure 3.
Reference: [2] <author> Anshul Gupta, Fred Gustavson, Mahesh Joshi, George Karypis and Vipin Kumar, </author> <title> Design and Implementation of a Scalable Parallel Direct Solver for Sparse Symmetric Positive Definite Systems, </title> <booktitle> Proc. Eighth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Min-neapolis, </address> <year> 1997. </year>
Reference-contexts: This results in a two dimensional distribution of the triangular factor matrices used by the triangular solvers to get the final solution. With the emergence of powerful parallel computers and the need of solving very large problems, completely parallel direct solvers are rapidly emerging <ref> [2, 4] </ref>. There is a need to find efficient and scalable parallel algorithms for triangular solvers, so that they do not form a performance bottleneck when the direct solver is used to solve large problems on large number of processors. <p> This makes a strong case for the utilization of our algorithm in a parallel direct solver. We have implemented a sparse triangular solver based on the proposed algorithm and it is a part of the recently announced high performance direct solver <ref> [2] </ref>. The experimental results presented in the section 5 show the good speedup characteristics and a high performance on a variety of sparse matrices. <p> Then the process of dense trapezoidal forward elimination as defined in Section 2 is applied. The parallel multifrontal algorithm was developed keeping in mind that our sparse triangular solver is a part of a parallel direct solver described in <ref> [2] </ref>. Our triangular solver uses the same distribution of the factor matrix, T , as used by the highly scalable algorithm employed in the numerical factorization phase [1]. Thus, there is no cost incurred in redistributing the data. Distribution of T is described in the next paragraph.
Reference: [3] <author> Anshul Gupta and Vipin Kumar, </author> <title> Parallel Algorithms for Forward Elimination and Backward Substitution in Direct Solution of Sparse Linear Systems, </title> <booktitle> Proc. </booktitle> <address> Supercomputing'95, San Diego, </address> <year> 1995. </year>
Reference-contexts: There is a need to find efficient and scalable parallel algorithms for triangular solvers, so that they do not form a performance bottleneck when the direct solver is used to solve large problems on large number of processors. A few attempts have been made recently to formulate such algorithms <ref> [3, 4, 6] </ref>. Most of the attempts until now, rely on redistributing the factor matrix from a two dimensional mapping to a one dimensional mapping. This is because it was believed till now, that the parallel formulations based on two dimensional mapping are unscalable [3]. <p> Most of the attempts until now, rely on redistributing the factor matrix from a two dimensional mapping to a one dimensional mapping. This is because it was believed till now, that the parallel formulations based on two dimensional mapping are unscalable <ref> [3] </ref>. But, as we show in this paper, even a simple two dimensional block cyclic mapping of data can be utilized by our parallel algorithm to achieve as much scalability as achieved by the algorithms based on one dimensional mapping. <p> With a nested-dissection based ordering scheme, the number of nodes, n, at each level is ff p N=2 l for the 2-D problems and ff (N=2 l ) 2=3 for the 3-D problems, where ff is a small constant <ref> [3] </ref>. The row dimension of a supernodal matrix, m, can be assumed to be a constant times n, for a balanced supernodal tree. <p> This shows that our sparse formulation is optimally scalable for the class of 3-D problems. The comparison of the isoefficiency expressions to those of the solver described in <ref> [3] </ref> shows that our two dimensional parallel formulation is as scalable as the one dimensional formulation. 5 Performance Results and Discussion We have implemented our sparse triangular solver as part of a direct solver.
Reference: [4] <author> Michael T. Heath and Padma Raghavan, </author> <title> LA-PACK Working Note 62: Distributed Solution of Sparse Linear Systems , Technical Report UT-CS-93-201, </title> <institution> University of Tennessee, Knoxville, TN, </institution> <year> 1993. </year>
Reference-contexts: This results in a two dimensional distribution of the triangular factor matrices used by the triangular solvers to get the final solution. With the emergence of powerful parallel computers and the need of solving very large problems, completely parallel direct solvers are rapidly emerging <ref> [2, 4] </ref>. There is a need to find efficient and scalable parallel algorithms for triangular solvers, so that they do not form a performance bottleneck when the direct solver is used to solve large problems on large number of processors. <p> There is a need to find efficient and scalable parallel algorithms for triangular solvers, so that they do not form a performance bottleneck when the direct solver is used to solve large problems on large number of processors. A few attempts have been made recently to formulate such algorithms <ref> [3, 4, 6] </ref>. Most of the attempts until now, rely on redistributing the factor matrix from a two dimensional mapping to a one dimensional mapping. This is because it was believed till now, that the parallel formulations based on two dimensional mapping are unscalable [3].
Reference: [5] <author> Mahesh V. Joshi, Anshul Gupta, George Karypis, and Vipin Kumar, </author> <title> Two Dimensional Scalable Parallel Algorithms for Solution of Triangular Systems, </title> <type> Technical Report TR 97-024, </type> <institution> Department of Computer Science, University of Min-nesota, Minneapolis, MN, </institution> <year> 1997. </year>
Reference-contexts: An analysis similar to that of dense case, shows that the the isoefficiency function for the 2-D problems is O (p 2 = log p) and for the 3-D problems, it is O (p 2 ). Refer to <ref> [5] </ref> for the detailed derivation of the expressions above. The parallel formulation is thus, more scalable than the corresponding dense formulation for the class of 2-D problems and it is as scalable as the dense formulation for the class of 3-D problems. <p> But, after a threshold on b, the distribution gets closer to the block distribution and processors start idling. This threshold depends on the characteristics of the problem and the parallel machine. The detailed results exhibiting these trends can be found in <ref> [5] </ref>. For most of the matrices we tested, we found that a blocksize of 64 gives better performance, on an IBM SP2 machine.
Reference: [6] <author> Chunguang Sun, </author> <title> Efficient Parallel Solutions of Large Sparse SPD Systems on Distributed Memory Multiprocessors , Technical Report 92-102, </title> <institution> Cornell University, Theory Center, </institution> <year> 1992. </year> <title> Matrix Application Domain N jT j bcsstk15 Structural Engineering 3948 474921 bcsstk30 Structural Engineering 28924 4293227 copter2 3D Finite Element Methods 55476 10218255 hsct2 High Speed Commercial Transport 88404 18744255 144pf3D 3D Finite Element Methods 144649 48898387 </title>
Reference-contexts: There is a need to find efficient and scalable parallel algorithms for triangular solvers, so that they do not form a performance bottleneck when the direct solver is used to solve large problems on large number of processors. A few attempts have been made recently to formulate such algorithms <ref> [3, 4, 6] </ref>. Most of the attempts until now, rely on redistributing the factor matrix from a two dimensional mapping to a one dimensional mapping. This is because it was believed till now, that the parallel formulations based on two dimensional mapping are unscalable [3]. <p> In this section, we propose a parallel multifrontal algorithm for solving such systems. We first give the description of the serial multi the factorization process. (b) Supernodal Tree for this matrix. frontal algorithm for forward elimination which closely follows the algorithm described in <ref> [6] </ref>. The algorithm is guided by the structure of elimination tree of T . We refer to a collection of consecutive nodes in an elimination tree each with only one child, as a supernode. The nodes in a supernode are collapsed to form the su-pernodal tree.
References-found: 6

