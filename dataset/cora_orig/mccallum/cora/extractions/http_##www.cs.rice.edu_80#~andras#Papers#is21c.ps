URL: http://www.cs.rice.edu:80/~andras/Papers/is21c.ps
Refering-URL: http://www.cs.rice.edu:80/~andras/Baocr/kornai.html
Root-URL: 
Phone: 2  
Title: An HMM-Based Legal Amount Field OCR System for Checks  
Author: Andras Kornai K.M. Mohiuddin Scott D. Connell 
Note: Proc Systems, Man and Cybernetics 1995 pp 2800-2805, final version  
Address: 650 Harry Road, San Jose, CA 95120  East Lansing, MI 48824  
Affiliation: 1 IBM Almaden Research Center  Department of Computer Science Michigan State University  
Abstract: The system described in this paper applies Hidden Markov technology to the task of recognizing the handwritten legal amount on personal checks. We argue that the most significant source of error in handwriting recognition is the segmentation process. In traditional handwriting OCR systems, recognition is performed at the character level, using the output of an independent segmentation step. Using a fixed stepsize series of vertical slices from the image, the HMM system described in this paper avoids taking segmentation decisions early in the recognition process. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.K. Baker, </author> <title> "Stochastic modeling for automatic speech understanding," </title> <editor> Reprinted in A. Waibel and Kai-Fu Lee (eds) Readings in Speech recognition, </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1990. </year> <pages> pp. 297-307 </pages>
Reference-contexts: For example, Table 4. above contains no full covariance results for dimension 35 or higher because the training set simply does not contain a sufficient number of data points. Mixture models, a standard tool in speech recognition <ref> [1] </ref>, offer a way out of this problem. In the following Table 5, the numbers in boldface show the points where the increased number of mixtures begins to provide better results than full covariances (where the latter is available).
Reference: [2] <author> E.J. Bellegarda, J.R. Bellegarda, D. Nahamoo and K.S. Nathan, </author> <title> "A Probabilistic Framework for the Recognition of On-line Handwriting," </title> <booktitle> Proc. 3rd International Workshop on Frontiers in Handwriting Recognition, </booktitle> <address> Buffalo, NY, </address> <pages> pp. 225-234, </pages> <month> May </month> <year> 1993 </year>
Reference-contexts: The CDF/72 set was obtained by omitting 16 features corresponding to the four corners of the image. To further reduce the dimensionality for the HMM stages (as well as for other algorithms), principal component analysis (PCA, see [8]) was performed. Using the IBM Hawthorne on-line recognition system (see <ref> [2] </ref> [3] [13]) we projected CDF and other feature vectors onto the space spanned by the eigenvectors corresponding to the d largest eigenvalues of the overall covariance matrix. set/size feat/dim after PCA % correct ETL/1000 CDF/72 27 96.12 ETL/1000 CDF/88 27 96.92 NIST/1000 CDF/72 27 88.00 NIST/2000 CDF/72 27 88.00 NIST/1000
Reference: [3] <author> J.R. Bellegarda, D. Nahamoo, K.S. Nathan and E.J. Bellegarda, </author> <title> "Supervised Hidden Markov Modeling for On-line Handwriting Recognition," </title> <booktitle> Proc. 1994 ICASSP, Adelaide, South Australia, </booktitle> <volume> Vol 5, </volume> <pages> pp 149-152, </pages> <month> April </month> <year> 1994 </year>
Reference-contexts: The CDF/72 set was obtained by omitting 16 features corresponding to the four corners of the image. To further reduce the dimensionality for the HMM stages (as well as for other algorithms), principal component analysis (PCA, see [8]) was performed. Using the IBM Hawthorne on-line recognition system (see [2] <ref> [3] </ref> [13]) we projected CDF and other feature vectors onto the space spanned by the eigenvectors corresponding to the d largest eigenvalues of the overall covariance matrix. set/size feat/dim after PCA % correct ETL/1000 CDF/72 27 96.12 ETL/1000 CDF/88 27 96.92 NIST/1000 CDF/72 27 88.00 NIST/2000 CDF/72 27 88.00 NIST/1000 CDF/88
Reference: [4] <author> Richard G. Casey: </author> <title> Moment Normalization of Handprinted Characters. </title> <journal> IBM Journal of Research and Development 1970, </journal> <pages> 548-553 </pages>
Reference-contexts: Traditional (as opposed to Hidden Markov) systems would also employ moment normalization <ref> [4] </ref> or similar steps which somewhat blur the line between preprocessing and feature extraction. We currently perform only one such step, slant detection and normalization. The dominant near-vertical direction of the writing is found by a modified Hough-transform, and again normalization is performed passively.
Reference: [5] <author> A. J. Elms, </author> <title> "A connected character recognizer using Level Building of HMMs," </title> <booktitle> Proc. 12th IAPR International Conference on Pattern Recognition, </booktitle> <pages> pp. 439-441, </pages> <year> 1994. </year>
Reference-contexts: 12 76.06 NIST/1000 CO/12 16 79.12 NIST/2000 CO/12 16 79.81 Table 3: Reco rates for multi-state HMM In the literature, several methods of feature extraction using sliding windows have been proposed: computing the blackness of individual pixels within the window [11], FFT estimation of the magnitude spectrum for vertical lines <ref> [5] </ref>, and tracking the vertical displacement of black lines.
Reference: [6] <author> S.A. Euler, B.-H. Juang, C.-H. Lee, and F.K. Soong. </author> <title> "Statistical segmentation and word modeling techniques in isolated word recognition," </title> <booktitle> In ICASSP-90, </booktitle> <pages> pages 745-748, </pages> <address> Albuquerque, </address> <year> 1990. </year>
Reference-contexts: 79.00 80.50 35/1000 78.56 79.44 78.88 79.88 40/1000 76.94 79.19 79.56 79.06 27/2000 1 90.12 91.31 91.62 91.93 Table 5: The impact of more mixtures Since in general increasing the number of mixtures has beneficial effects both in single state and in multi-state models, the possibility of using tied mixtures <ref> [6] </ref> was also investigated. In one set of experiments, the 10*16 LO10 features were first reduced to 27 dimensions, and the resulting data file, containing altogether 32,000 feature vectors, were subjected to unsupervised clustering into a set of n 27-dimensional gaussians.
Reference: [7] <author> V.N. Gupta, M. Lennig, and P. Mermelstein, </author> <title> "Integration of acoustic information in a large vocabulary word recognizer," </title> <booktitle> Proc. ICASSP 1987 </booktitle>
Reference-contexts: In on-going experiments global height normalization (adjusting h only once per legal amount field) is replaced by a local height normalization process which places height, together with displacement from the baseline, in a separate codebook <ref> [7] </ref>. So far, the advantages of this technique have been demonstrated only on isolated character (NIST) data, where it provides consistent, but modest improvements in recognition rate its effects on actual check data remain to be determined.
Reference: [8] <author> H. Hotelling, </author> <title> "Analysis of a complex of statistical variables into principal components," </title> <booktitle> JEP 24 </booktitle> <pages> 417-41, 498-520, </pages> <year> 1933. </year>
Reference-contexts: The CDF/72 set was obtained by omitting 16 features corresponding to the four corners of the image. To further reduce the dimensionality for the HMM stages (as well as for other algorithms), principal component analysis (PCA, see <ref> [8] </ref>) was performed.
Reference: [9] <author> A. Kaltenmeyer, T. Caesar, J.M. Gloger and E. Mandler, </author> <title> "Sophisticated Topology of HMMs for Cursive Script Recognition," </title> <booktitle> Proc. 2nd ICDAR 1993, </booktitle> <pages> 139-142. </pages>
Reference-contexts: Windowing in the vertical direction (see <ref> [9] </ref>) i.e. averaging blackness over 12 or 24 horizontal stripes, referred to as the CO/12 and CO/24 features, yields a 12 (24) dimensional feature vector for each horizontal step.
Reference: [10] <author> T. Kohonen, J. Kangas, J. Laaksonen, K. Torkkola. </author> <title> "LVQ PAK: A program package for the correct application of Learning Vector Quantization algorithms," </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <pages> pages I 725-730, </pages> <address> Baltimore, </address> <month> June </month> <year> 1992. </year> <note> IEEE. </note>
Reference-contexts: So far, the advantages of this technique have been demonstrated only on isolated character (NIST) data, where it provides consistent, but modest improvements in recognition rate its effects on actual check data remain to be determined. We baselined against two standard techniques, Learning Vector Quantization <ref> [10] </ref> and Multi-Layer Perceptrons [12] to two data sets that were extensively used in later tests.
Reference: [11] <author> S.-S. Kuo, O. Agazzi, </author> <title> "Visual Keyword Recognition Using Hidden Markov Models," </title> <booktitle> Proc. CVPR, </booktitle> <pages> pp. 329-334, </pages> <year> 1993. </year>
Reference-contexts: ETL/1000 CO/24 12 92.24 NIST/1000 CO/12 12 73.38 NIST/2000 CO/12 12 76.06 NIST/1000 CO/12 16 79.12 NIST/2000 CO/12 16 79.81 Table 3: Reco rates for multi-state HMM In the literature, several methods of feature extraction using sliding windows have been proposed: computing the blackness of individual pixels within the window <ref> [11] </ref>, FFT estimation of the magnitude spectrum for vertical lines [5], and tracking the vertical displacement of black lines.
Reference: [12] <author> K.M. Mohiuddin, J. Mao, </author> <title> "A Comparative Study of Different Classifiers for Handprinted Character Recognition," </title> <booktitle> Pattern Recognition in Practice IV, </booktitle> <pages> pp. 437-448, </pages> <year> 1994. </year>
Reference-contexts: So far, the advantages of this technique have been demonstrated only on isolated character (NIST) data, where it provides consistent, but modest improvements in recognition rate its effects on actual check data remain to be determined. We baselined against two standard techniques, Learning Vector Quantization [10] and Multi-Layer Perceptrons <ref> [12] </ref> to two data sets that were extensively used in later tests.
Reference: [13] <author> K.S. Nathan, J.R. Bellegarda, D. Nahamoo and E.J. Bellegarda, </author> <title> "On-line Handwriting Recognition Using Continuous Parameter Hidden Markov Models," </title> <booktitle> Proc. 1993 ICASSP, </booktitle> <address> Minneapolis, MN, </address> <pages> pp. 121-124, </pages> <month> April </month> <year> 1993 </year>
Reference-contexts: To further reduce the dimensionality for the HMM stages (as well as for other algorithms), principal component analysis (PCA, see [8]) was performed. Using the IBM Hawthorne on-line recognition system (see [2] [3] <ref> [13] </ref>) we projected CDF and other feature vectors onto the space spanned by the eigenvectors corresponding to the d largest eigenvalues of the overall covariance matrix. set/size feat/dim after PCA % correct ETL/1000 CDF/72 27 96.12 ETL/1000 CDF/88 27 96.92 NIST/1000 CDF/72 27 88.00 NIST/2000 CDF/72 27 88.00 NIST/1000 CDF/88 23
Reference: [14] <author> K.M. Sayre, </author> <title> "Machine Recognition of Handwritten Words: A Project Report," </title> <journal> Pattern Recognition, </journal> <volume> Vol. 5, pp.213-228, </volume> <year> 1973. </year>
Reference-contexts: Within a single line two segmentation tasks are relevant: segmentation of the line into words and segmentation of the words into characters. Given that character-level segmentation by the usual method <ref> [14] </ref> of searching for extrema in the continuous curves that make up cursive words is known to be highly sensitive to image quality, we concentrated on the word-level segmentation problem.
Reference: [15] <author> H. Takahashi, </author> <title> "A Neural Net OCR Using Geometrical and Zonal-pattern Features," </title> <booktitle> Proc. 1st ICDAR, </booktitle> <pages> pp. 821-828, </pages> <year> 1991. </year>
Reference-contexts: One feature set used in the experiments is the 88 Contour Direction Features (see <ref> [15] </ref>) we will refer to this as the CDF/88 set. The CDF/72 set was obtained by omitting 16 features corresponding to the four corners of the image. To further reduce the dimensionality for the HMM stages (as well as for other algorithms), principal component analysis (PCA, see [8]) was performed.
References-found: 15

