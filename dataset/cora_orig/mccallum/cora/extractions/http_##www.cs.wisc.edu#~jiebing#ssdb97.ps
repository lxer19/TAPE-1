URL: http://www.cs.wisc.edu/~jiebing/ssdb97.ps
Refering-URL: http://www.cs.wisc.edu/~jiebing/jiebing.html
Root-URL: 
Email: -jiebing, dewitt@cs.wisc.edu  
Title: Query Pre-Execution and Batching in Paradise: A Two-Pronged Approach to the Efficient Processing of Queries
Author: JieBing Yu David J. DeWitt 
Note: 1 This work is supported by NASA under contracts #USRA-5555-17, #NAGW-3895, and #NAGW-4229, ARPA through ARPA Order number 018 monitored by the U.S. Army Research Laboratory under contract DAAB07-92-C-Q508, IBM, Intel,  
Address: Wisconsin Madison  
Affiliation: Department of Computer Sciences University of  Sun Microsystems, Microsoft, and Legato.  
Abstract: The focus of the Paradise project [1,2] is to design and implement a scalable database system capable of storing and processing massive data sets such as those produced by NASAs EOSDIS project. This paper describes extensions to Paradise to handle the execution of queries involving collections of satellite images stored on tertiary storage. Several modifications were made to Paradise in order to make the execution of such queries both transparent to the user and efficient. First, the Paradise storage engine (the SHORE storage manager) was extended to support tertiary storage using a log-structured organization for tape volumes. Second, the Paradise query processing engine was modified to incorporate a number of novel mechanisms including query pre-execution, object abstraction, cache-conscious tape scheduling, and query batching. A performance evaluation on a working prototype demonstrates that, together, these techniques can provide a dramatic improvement over more traditional approaches to the management of data stored on tape. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. DeWitt, N. Kabra, J. Luo, J. Patel and J. Yu. </author> <title> Client Server Paradise, </title> <booktitle> Proc. of the 20 th VLDB Conference, </booktitle> <address> San-tiago, Chile, </address> <year> 1994. </year>
Reference-contexts: This approach is fine if images are always accessed in their entirety. However, processing of only pieces of images is fairly common [10]. As a solution, Paradise uses tiling <ref> [1, 2] </ref> to partition each image into multiple tiles, with each tile stored as a separate object on tape. Thus, only those tiles that are actually touched by a query need to be read from tape.
Reference: [2] <author> J. Patel, J. Yu., et al. </author> <title> Building a Scalable Geo-Spatial DBMS: Technology, Implementation and Evaluation, </title> <booktitle> Proc. of the 1997 SIGMOD Conference, </booktitle> <month> May, </month> <year> 1997. </year>
Reference-contexts: This approach is fine if images are always accessed in their entirety. However, processing of only pieces of images is fairly common [10]. As a solution, Paradise uses tiling <ref> [1, 2] </ref> to partition each image into multiple tiles, with each tile stored as a separate object on tape. Thus, only those tiles that are actually touched by a query need to be read from tape.
Reference: [3] <author> B. Kobler and J. Berbert, </author> <title> NASA Earth Observing System Data Information System (EOSDIS), </title> <booktitle> Digest of Papers: 11 th IEEE Symposium on Mass Storage Systems, </booktitle> <address> Los Alamitos, </address> <year> 1991. </year>
Reference-contexts: When fully deployed, these satellites will have an aggregate data rate of about 2 megabytes a second. While this rate is, in itself, not that impressive, it adds up to a couple of terabytes a day and 10 petabytes over the 10 year lifetime of the satellites <ref> [3] </ref>. Given todays mass storage technology, the data will almost certainly be stored on tape. The latest tape technology offers media that is both very dense and reliable, as well as drives with reasonable transfer rates. For example, Quantums DLT-7000 drive has a transfer rate of approximately 5.0 MB/sec (compressed).
Reference: [4] <author> Quantum Corporation, </author> <title> DLT-4000 Product Manual, </title> <year> 1995. </year>
Reference-contexts: For example, Quantums DLT-7000 drive has a transfer rate of approximately 5.0 MB/sec (compressed). The cartridges for this drive have a capacity of 70 GB (compressed), a shelf life of 10 years, and are rated for 500,000 passes <ref> [4] </ref>. However, since tertiary storage systems are much better suited for sequential access, their use as the primary medium for database storage is limited. Efficiently processing data on tape presents a number of challenges [5].
Reference: [5] <author> M. Carey, L. Haas and M. Livny. </author> <title> Tapes Hold data, Too: Challenges of Tuples on Tertiary Store, </title> <booktitle> Proc. of the 1993 SIGMOD Conference, </booktitle> <month> May, </month> <year> 1993. </year>
Reference-contexts: However, since tertiary storage systems are much better suited for sequential access, their use as the primary medium for database storage is limited. Efficiently processing data on tape presents a number of challenges <ref> [5] </ref>. While the cost/capacity gap [5] between tapes and disks has narrowed, there is still a factor of 3.5 in density between the best commodity tape technology (35 GB uncompressed) and the best commodity disk technology (10 GB uncompressed) and a factor of 7 in total cost ($2,000 for a 10 <p> However, since tertiary storage systems are much better suited for sequential access, their use as the primary medium for database storage is limited. Efficiently processing data on tape presents a number of challenges <ref> [5] </ref>. While the cost/capacity gap [5] between tapes and disks has narrowed, there is still a factor of 3.5 in density between the best commodity tape technology (35 GB uncompressed) and the best commodity disk technology (10 GB uncompressed) and a factor of 7 in total cost ($2,000 for a 10 GB disk and $14,000 for
Reference: [6] <author> J. Gray. MOX, </author> <title> GOX and SCANS: The Way to Measure an Archive, </title> <month> June </month> <year> 1994. </year>
Reference-contexts: There are two different approaches for handling tape-based data sets in database systems. The first is to use a Hierarchical Storage Manager (HSM) such as the one marketed by EMASS <ref> [6] </ref> to store large objects externally. Such systems almost always operate at the granularity of a file. That is, a whole file is the unit of migration from tertiary storage (i.e. tape) to secondary storage (disk) or memory. <p> In particular, seek operations on tape are almost four orders of magnitude slower than seeks on disk. Thus, a much larger block size is required <ref> [6] </ref>. Our implementation makes it possible to configure the tape block size when the tape volume is being formatted. In a separate study [29], we examine the effect of different tape block sizes for a variety of operations on raster satellite images stored on a Quantum DLT 4000 tape drive.
Reference: [7] <author> R. Herring and L. Tefend. </author> <title> Volume Serving and Media Management in a Networked, Distributed Client/Server Environment, </title> <booktitle> Proc. 3 rd Goddard Conf. Mass Storage Systems and Technologies, </booktitle> <institution> NASA Con. </institution> <note> Publication 3262, </note> <year> 1993. </year>
Reference-contexts: In effect, it corresponds to the approach in which the database storage engine treats tertiary storage as just another layer in the storage hierarchy. The Whole configuration is intended to simulate the approach of using an external tertiary storage manager (such as EMASS <ref> [7] </ref>) to migrate tertiary data on a per-file basis. In the Whole configuration each raster image is stored as a separate file on tape.
Reference: [8] <author> M. Olson. </author> <title> Extending the POSTGRES Database System to Manage Tertiary Storage, </title> <type> Masters Thesis, </type> <institution> EECS, University of California at Berkeley, </institution> <month> May, </month> <year> 1992. </year>
Reference-contexts: Highlights approach is closely integrated with LFS and treats tertiary storage primarily as a backing store. LTS has a more flexible design whose objective is to provide a general-purpose block-oriented tertiary storage manager. Extensions to Postgres to manage data on optical juke box are described in <ref> [8] </ref>. Our design for Paradises tertiary storage volume manager borrows a number of techniques from LTS, but focuses on the use of tape devices instead of optical devices. A multilayered caching and migration architecture to manage persistent objects on tertiary storage is proposed in [17].
Reference: [9] <author> S. Sarawagi. </author> <title> Query Processing in Tertiary Memory databases, </title> <booktitle> Proc. of the 19 th VLDB Conference, </booktitle> <address> Switzerland, </address> <month> September, </month> <year> 1995. </year>
Reference-contexts: Integrated Approach The most comprehensive system-level approach for integrating tertiary storage into a general database management system is proposed in <ref> [9] </ref>. A novel technique of breaking relations on tertiary storage into smaller segments (which are the units of migration from tertiary to secondary storage) is used to allow the migration of these segments to be scheduled optimally.
Reference: [10] <author> M. Stonebraker, J. Frew, K. Gardels, and J. Meredith. </author> <title> The SEQUOIA 20000 Storage Benchmark, </title> <booktitle> Proc. of the 1993 SIGMOD Conference, </booktitle> <month> May, </month> <year> 1993. </year>
Reference-contexts: In addition, with increasingly powerful object-relational features of systems such as Illustra (Postgres) and Paradise, complicated tasks like analyzing clipped portions of interest on a large number of satellite images can be performed as a single query <ref> [10] </ref>. In this paper, we describe the extensions that were made to Paradise [1,2] to handle query processing on image data sets stored on magnetic tape. Unfortunately, it is not just as simple as adding support for tape-based storage volumes. <p> This approach is fine if images are always accessed in their entirety. However, processing of only pieces of images is fairly common <ref> [10] </ref>. As a solution, Paradise uses tiling [1, 2] to partition each image into multiple tiles, with each tile stored as a separate object on tape. Thus, only those tiles that are actually touched by a query need to be read from tape. <p> Second, the benchmark must incorporate a variety of access patterns in order to be realistic. In order to study the effectiveness of the techniques described above, we developed a scaled-down benchmark that we call the Tertiary Storage Mini-Benchmark. This benchmark uses regional data from the Sequoia 2000 benchmark <ref> [10] </ref>.
Reference: [11] <author> J. Gray, et. al. </author> <title> The Sequoia Alternative Architecture Study for EOSDIS, </title> <publisher> NASA, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Paradise also uses query batching to make query processing on tape efficient. Query batching is a variant of traditional tape-based batch processing from the 1970s and what Gray terms a data pump <ref> [11] </ref>.
Reference: [12] <author> E. Shekita, M. Carey, </author> <title> A Performance Evaluation of Pointer-Based Joins, </title> <booktitle> Proc. Of the 1990 SIGMOD Conference, </booktitle> <month> May, </month> <year> 1990. </year>
Reference-contexts: While using techniques from executing pointer joins <ref> [12] </ref> or assembling complex objects [13] to reorder object accesses may help reduce the number of random accesses, in a multiuser environment, even if each query is executed using its best plan, the aggregate effect can still result in a large number of random tape accesses.
Reference: [13] <author> T. Keller, G. Graefe, D. Maier, </author> <title> Efficient Assembly of Complex Objects, </title> <booktitle> Proc. of the 1991 SIGMOD Conference, </booktitle> <month> May, </month> <year> 1991. </year>
Reference-contexts: While using techniques from executing pointer joins [12] or assembling complex objects <ref> [13] </ref> to reorder object accesses may help reduce the number of random accesses, in a multiuser environment, even if each query is executed using its best plan, the aggregate effect can still result in a large number of random tape accesses.
Reference: [14] <author> J. Kohl, C. Staelin and M. Stonebraker. Highlight: </author> <title> Using a LogStructured Files System for Tertiary Storage management, </title> <booktitle> Proc. Winter USENIX 1993, </booktitle> <pages> pages 435-447, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Section 4 describes the design and implementation of query pre-execution and query batching inside Paradise. Section 5 contains a performance evaluation of these techniques. Our conclusions and future research directions are contained in Section 6. 2. Related Work Tertiary Storage Management The focus of the Highlight <ref> [14] </ref> and LTS [15] projects is the application of logstructured file system techniques [16] to the management of tertiary storage. Highlight integrates LFS with tertiary storage by allowing the automatic migration of LFS file segments (containing user data, index nodes, and directory files) between secondary and tertiary storage.
Reference: [15] <author> D. Ford and J. Myllymaki. </author> <title> A LogStructured Organization for Tertiary Storage, </title> <booktitle> Proc. of the 12 th Conference on Data Engineering. </booktitle>
Reference-contexts: Section 4 describes the design and implementation of query pre-execution and query batching inside Paradise. Section 5 contains a performance evaluation of these techniques. Our conclusions and future research directions are contained in Section 6. 2. Related Work Tertiary Storage Management The focus of the Highlight [14] and LTS <ref> [15] </ref> projects is the application of logstructured file system techniques [16] to the management of tertiary storage. Highlight integrates LFS with tertiary storage by allowing the automatic migration of LFS file segments (containing user data, index nodes, and directory files) between secondary and tertiary storage.
Reference: [16] <author> M. Rosenblum and J. Ousterhout. </author> <title> The Design and Implementation of a LogStructured File System, </title> <journal> ACM Trans. Computer System, </journal> <volume> 10(4) </volume> <pages> 26-52, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Section 5 contains a performance evaluation of these techniques. Our conclusions and future research directions are contained in Section 6. 2. Related Work Tertiary Storage Management The focus of the Highlight [14] and LTS [15] projects is the application of logstructured file system techniques <ref> [16] </ref> to the management of tertiary storage. Highlight integrates LFS with tertiary storage by allowing the automatic migration of LFS file segments (containing user data, index nodes, and directory files) between secondary and tertiary storage. <p> For this set of tests, we determined that the optimal tape block size was between 64 and 256 Kbytes. Since tapes are (unfortunately) an append-only media, a logstructured organization <ref> [16] </ref> is used to handle updates to tape blocks with dirty tape blocks being appended at the current tail of the tape. A mapping table is used to maintain the correspondence between logical and physical tape blocks.
Reference: [17] <author> R.Grossman, D. Hanley and X. Qin,. </author> <title> Caching and Migration for Multilevel Persistent Object Stores, </title> <booktitle> Proceedings of the 14 th IEEE Computer Society Mass Storage System Symposium, </booktitle> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Our design for Paradises tertiary storage volume manager borrows a number of techniques from LTS, but focuses on the use of tape devices instead of optical devices. A multilayered caching and migration architecture to manage persistent objects on tertiary storage is proposed in <ref> [17] </ref>. Their preliminary results demonstrate that sequential access to tape segments benefits from the multilevel caching while random accesses may cause excessive overhead.
Reference: [18] <author> J. Li and C. Orji. </author> <title> I/O Optimization Policies in Tape-Based Tertiary Systems, </title> <type> Tech Report, </type> <institution> Florida International University. </institution>
Reference-contexts: Tape Scheduling The very high access latency associated with magnetic tape devices has prompted a number of researchers to explore alternative ways of minimizing the number of random tape I/Os. <ref> [18] </ref> and [19] extend various disk I/O scheduling algorithms to the problem of tape I/O scheduling. [18] models access behaviors for helical scan tapes (e.g. 8mm tapes) and investigates both tape scheduling and cache replacement policies. <p> Tape Scheduling The very high access latency associated with magnetic tape devices has prompted a number of researchers to explore alternative ways of minimizing the number of random tape I/Os. <ref> [18] </ref> and [19] extend various disk I/O scheduling algorithms to the problem of tape I/O scheduling. [18] models access behaviors for helical scan tapes (e.g. 8mm tapes) and investigates both tape scheduling and cache replacement policies.
Reference: [19] <author> B. Hillyer and A. Silberschatz. </author> <title> Random I/O Scheduling in Online Tertiary Storage Systems, </title> <booktitle> Proc. of the 1996 SIGMOD Conference, </booktitle> <month> May, </month> <year> 1995. </year>
Reference-contexts: Tape Scheduling The very high access latency associated with magnetic tape devices has prompted a number of researchers to explore alternative ways of minimizing the number of random tape I/Os. [18] and <ref> [19] </ref> extend various disk I/O scheduling algorithms to the problem of tape I/O scheduling. [18] models access behaviors for helical scan tapes (e.g. 8mm tapes) and investigates both tape scheduling and cache replacement policies. <p> Their results demonstrate that it is very important to consider the position of the tape head when attempting to obtain an optimal schedule for a batch of tape accesses. <ref> [19] </ref> models the behavior of accesses to serpentine tapes (e.g. DLT tapes), and compares different scheduling algorithms designed to optimize random I/Os on a DLT drive. Both studies show that careful scheduling of tape accesses can have a significant impact on performance. <p> The use of multiple chunks for each query allows chunks from concurrent queries to be easily merged together by interleaving accesses in a multiuser environment. The reordering function can be more than just an address sorting function. For example, more sophisticated functions like the one proposed in <ref> [19] </ref> can be used to deal with tapes with nonlinear access characteristics (e.g. DLT tapes).
Reference: [20] <author> L.T. Chen, D. Rotem. </author> <title> Optimizing Storage of Objects on Mass Storage Systems with Robotic Devices, Algorithms for Data Structures, </title> <booktitle> Spring V, </booktitle> <year> 1994. </year>
Reference-contexts: DLT tapes), and compares different scheduling algorithms designed to optimize random I/Os on a DLT drive. Both studies show that careful scheduling of tape accesses can have a significant impact on performance. Data Placement on Tapes <ref> [20] </ref> and [21] investigate the optimal placement of data on tape in order to minimize random tape I/Os. These algorithms assume a known and fixed access pattern for the tertiary tape blocks.
Reference: [21] <author> L.T. Chen, R, Drach, M. Keating, S. Louis, D. Rotem and A. Shoshan. </author> <title> Efficient Origination and Access of MultiDimensional Datasets on Tertiary Systems, </title> <journal> Information Systems Journal. </journal> <month> April, </month> <year> 1995. </year>
Reference-contexts: DLT tapes), and compares different scheduling algorithms designed to optimize random I/Os on a DLT drive. Both studies show that careful scheduling of tape accesses can have a significant impact on performance. Data Placement on Tapes [20] and <ref> [21] </ref> investigate the optimal placement of data on tape in order to minimize random tape I/Os. These algorithms assume a known and fixed access pattern for the tertiary tape blocks.
Reference: [22] <author> S. Sarawagi and M. Stonebraker. </author> <title> Single Query Optimization for Tertiary Memory, </title> <booktitle> Proc. of the 1994 SIGMOD Conference, </booktitle> <month> May, </month> <year> 1994. </year>
Reference-contexts: In addition, collecting the access patterns and reorganizing data on tapes over time may be a difficult task to accomplish in an online system. Tertiary Storage Query Processing <ref> [22] </ref> and [23, 24] propose special techniques to optimize the execution of single join operations for relations stored on tape.
Reference: [23] <author> J. Myllymaki and M. Livny. </author> <title> Joins on Tapes: Synchronizing Disk and Tape Join Access, </title> <booktitle> Proc. of the 1995 SIGMETRICS Conference, </booktitle> <address> Ottawa, Canada. </address>
Reference-contexts: In addition, collecting the access patterns and reorganizing data on tapes over time may be a difficult task to accomplish in an online system. Tertiary Storage Query Processing [22] and <ref> [23, 24] </ref> propose special techniques to optimize the execution of single join operations for relations stored on tape. <p> Careful selection of the processing block size and the ordering of block accesses is demonstrated to reduce execution time by about a factor of 10. [24] exploits the use of I/O parallelism between disk and tape devices during joins. <ref> [23] </ref> also identifies a number of system factors that have a direct impact on query processing with a focus on single relational operations. User-Managed Tertiary Storage The first attempt to integrate tertiary storage into a database system appeared in [25].
Reference: [24] <author> J. Myllymaki and M. Livny. </author> <title> Efficient Buffering for Concurrent Disk and Tape I/O, </title> <booktitle> Proceedings of Performance '96 - The International Conference on Performance Theory, Measurement and Evaluation of Computer and Communication Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: In addition, collecting the access patterns and reorganizing data on tapes over time may be a difficult task to accomplish in an online system. Tertiary Storage Query Processing [22] and <ref> [23, 24] </ref> propose special techniques to optimize the execution of single join operations for relations stored on tape. <p> Tertiary Storage Query Processing [22] and [23, 24] propose special techniques to optimize the execution of single join operations for relations stored on tape. Careful selection of the processing block size and the ordering of block accesses is demonstrated to reduce execution time by about a factor of 10. <ref> [24] </ref> exploits the use of I/O parallelism between disk and tape devices during joins. [23] also identifies a number of system factors that have a direct impact on query processing with a focus on single relational operations.
Reference: [25] <author> M. Stonebraker. </author> <title> Managing Persistent Objects in a MultiLevel Store, </title> <booktitle> Proc. of the 1991 SIGMOD Conference, </booktitle> <month> May, </month> <year> 1991. </year>
Reference-contexts: User-Managed Tertiary Storage The first attempt to integrate tertiary storage into a database system appeared in <ref> [25] </ref>. A three-level storage hierarchy was proposed to be under the direct control of a database management system with tertiary storage at the bottom layer. Data could be elevated from tertiary storage to secondary storage via user-level commands.
Reference: [26] <author> J. Fine, T. Anderson, K. Dahlin, J. Frew, M. Olson, D. Patterson. </author> <title> Abstract: A Latency-Hiding Technique for High Capacity Mass-Storage Systems," Sequoia 2000 Project Report 92/11, </title> <institution> University of California, Berkeley, </institution> <year> 1992. </year>
Reference-contexts: A three-level storage hierarchy was proposed to be under the direct control of a database management system with tertiary storage at the bottom layer. Data could be elevated from tertiary storage to secondary storage via user-level commands. Another user-level approach is described in <ref> [26] </ref>, in which the concept of a user-defined abstract is proposed to reduce the number of accesses that have to be made to tertiary storage. <p> This approach requires that the OIDs for the tiles be stored as part of the images metadata. We term the set of OIDs corresponding to the taperesident tiles a system-level object abstraction. This differs from the user-level abstraction proposed by <ref> [26] </ref> in that the tiling process is handled automatically by Paradise. Figure 6 illustrates one such representation for a raster image. In this example, the body of the image is partitioned into 4 tiles stored on tape, while its metadata containing the tile OIDs are stored on disk.
Reference: [27] <author> S. Sarawagi and M. Stonebraker. </author> <title> Reordering Query Execution in Tertiary Memory Databases, </title> <booktitle> Proc. of the 20 th VLDB Conference, </booktitle> <address> India, </address> <month> September, </month> <year> 1996. </year>
Reference-contexts: These mini-queries are then scheduled at runtime according to the availability of the involved segments on disk and memory. A set of priority-based algorithms are used to fetch the desired segments from tertiary storage on demand and to replace segments on the cache disk. Follow-up work in <ref> [27] </ref> details a framework for dynamically reordering query execution by modifying query plans based on the availability of data segments. The difference between this approach and ours is that our emphasis is on optimizing tape accesses at the bottom layer of the execution engine, leaving the original query plan unchanged. <p> Not only is this strategy simpler but also it provides more opportunities for optimizing executions under multiuser environment. However, it appears fruitful to consider combining the two approaches using query pre-execution as mechanism to resolve <ref> [27] </ref> accesses to satellite images and using schedule nodes [27] in our query plans to handle data dependencies between operators in the query tree. 3. System Architecture Paradise is an object-relational database system whose primary focus is on the efficient management and processing of large, spatial and multimedia data sets. <p> Not only is this strategy simpler but also it provides more opportunities for optimizing executions under multiuser environment. However, it appears fruitful to consider combining the two approaches using query pre-execution as mechanism to resolve <ref> [27] </ref> accesses to satellite images and using schedule nodes [27] in our query plans to handle data dependencies between operators in the query tree. 3. System Architecture Paradise is an object-relational database system whose primary focus is on the efficient management and processing of large, spatial and multimedia data sets. <p> While this idea sounds impractical, we will demonstrate in Section 5 that it works extremely well for taperesident sets of satellite images. In the general case, a mechanism such as proposed in <ref> [27] </ref> for inserting schedule nodes in the query plan will be needed to resolve data dependencies between operators in the query tree. In order to support the query pre-execution phase, special mechanisms were added to Paradises query execution engine to monitor the processing of the system-level object abstractions.
Reference: [28] <author> M. Carey, D. DeWitt, M. Franklin, N. Hall, M. McAu-liffe, J. Naughton, D. Schuh, M. Solomon, C. Tan, O. Tsa-talos, S. White and M. Zwilling. </author> <title> Shoring up Persistent Objects, </title> <booktitle> Proc. of the 1994 SIGMOD Conference, </booktitle> <month> May, </month> <year> 1994. </year>
Reference-contexts: System Architecture Paradise is an object-relational database system whose primary focus is on the efficient management and processing of large, spatial and multimedia data sets. The structure of the Paradise server process is shown in Figure 2. The SHORE storage manager <ref> [28] </ref> is used as the underlying persistent object manager. Support for tertiary storage in Paradise began by the extending SHORE.
Reference: [29] <author> J. Yu and D. J. DeWitt. </author> <title> Processing Satellite Images on Tertiary Storage: A Study of the Impact of Tile Size on Performance, </title> <booktitle> 5 th NASA Goddard Conference on Mass Storage Systems and Technologies, </booktitle> <month> September, </month> <year> 1996. </year>
Reference-contexts: In particular, seek operations on tape are almost four orders of magnitude slower than seeks on disk. Thus, a much larger block size is required [6]. Our implementation makes it possible to configure the tape block size when the tape volume is being formatted. In a separate study <ref> [29] </ref>, we examine the effect of different tape block sizes for a variety of operations on raster satellite images stored on a Quantum DLT 4000 tape drive. For this set of tests, we determined that the optimal tape block size was between 64 and 256 Kbytes.
References-found: 29

