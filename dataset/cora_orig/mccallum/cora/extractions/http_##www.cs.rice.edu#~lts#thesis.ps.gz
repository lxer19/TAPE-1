URL: http://www.cs.rice.edu/~lts/thesis.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~lts/resume.html
Root-URL: 
Title: Value-Driven Redundancy Elimination  
Author: by Loren Taylor Simpson Keith D. Cooper, Linda Torczon, Faculty Fellow 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Approved, Thesis Committee:  Associate Professor, Chair  Ken Kennedy, Noah Harding Professor  Sarita Adve, Assistant Professor  
Date: April, 1996  
Address: Houston, Texas  
Affiliation: RICE UNIVERSITY  Department of Computer Science Rice University  Department of Computer Science Rice University  Department of Computer Science Rice University  Department of Electrical and Computer Engineering Rice University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1974. </year>
Reference-contexts: It does not accomplish constant folding or algebraic simplification, and it does not remove redundant computations. Redundant computations are removed by a separate algorithm. We partition the values in the routine into congruence classes using a variation of the algorithm by Hopcroft for minimizing a finite-state machine <ref> [1] </ref>. We use the term partition to refer to a set of congruence classes such that each value (SSA name) in the routine is in exactly one class. The partitioning of values is accomplished by starting with an initial partition and iteratively refining the partition until it stabilizes. <p> The lookup function 44 for all SSA names i VN [i] &gt; changed FALSE for all blocks b in reverse postorder for all definitions x in b expr hVN [x <ref> [1] </ref>]; x:op; VN [x [2]]i temp lookup (expr; x) if VN [x] 6= temp changed TRUE VN [x] temp Remove all entries from the hash table while changed searches a hash table for the expression hVN [x [1]]; x:op; VN [x [2]]i. <p> b in reverse postorder for all definitions x in b expr hVN [x <ref> [1] </ref>]; x:op; VN [x [2]]i temp lookup (expr; x) if VN [x] 6= temp changed TRUE VN [x] temp Remove all entries from the hash table while changed searches a hash table for the expression hVN [x [1]]; x:op; VN [x [2]]i. If the expression is found, it returns the name of the expression. Otherwise, it adds the expression to the table with name hxi. The RPO algorithm computes a sequence of equivalence relations, ~ = i , that partition the set of SSA names.
Reference: [2] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: provide the consistent naming that we require. 59 H H H H Hj z x + y B 2 z x + y B 3 H H H H Hj if (: : : ) B 1 B 4 Before After at the point where they appear in the routine <ref> [2] </ref>. This approach uses data- flow analysis to determine the set of expressions available along all paths from the start of the routine. Notice that the calculation of z in Figure 5.1 will be removed because it is in the AVAIL set. <p> The final step is to move b 2 to the next non-empty entry in the Blocks array. 5.2 AVAIL-Based Removal The classical approach to redundancy elimination is to remove computations in the set of available expressions (AVAIL) at the point where they appear in the routine <ref> [2] </ref>. This approach uses data-flow analysis to determine the set of expressions available along all paths from the start of the routine. An expression is available if it is computed along all paths from the beginning of the routine.
Reference: [3] <author> Frances E. Allen, John Cocke, and Ken Kennedy. </author> <title> Reduction of operator strength. </title> <editor> In Steven S. Muchnick and Neil D. Jones, editors, </editor> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: The classic example replaces certain multiplication operations inside a loop with equivalent addition operations. This case arises routinely in loop-based array address calculations, and many other operations can be reduced in this manner. Allen, Cocke, and Kennedy provide a detailed catalog of such reductions <ref> [3] </ref>. Strength reduction has been an important transformation for two principal reasons. First, multiplying integers has usually taken longer than adding them. This made strength reduction profitable; the amount of improvement varied with the relative costs of addition and multiplication. <p> This effect may be especially pronounced in code that has been automatically blocked to improve locality [46, 9]. This chapter presents a new algorithm for performing strength reduction. It produces results similar to those of Allen, Cocke, and Kennedy's classic algorithm <ref> [3] </ref>. <p> Allen, Cocke, and Kennedy describe a variety of other candidate types <ref> [3] </ref>. These are straightforward extensions to the technique. If this operation is a candidate for reduction, the Replace function described in the next section is invoked immediately. Since this function transforms x into an induction variable, x is labeled as an induction variable with the same header block as i. <p> We rely on the copy coalescing phase of a Chaitin-style graph coloring register allocator to accomplish this task [11, 7]. A.3 Previous Work Reduction of operator strength has a long history in the literature. The classic method is presented in a paper by Allen, Cocke, and Kennedy <ref> [3] </ref>. It, in turn, builds on earlier work by Cocke and Kennedy [15, 28].
Reference: [4] <author> Bowen Alpern, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Conference Record of the Fifteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-11, </pages> <address> San Diego, California, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: The unified hash table algorithm is almost global, but it can fail to discover redundancies where values flow through a back edge in the CFG. 27 Chapter 3 Value Partitioning Alpern, Wegman, and Zadeck describe a global approach to value numbering that we call value partitioning <ref> [4] </ref>. In this chapter, we extend this algorithm to handle commutative operations and to eliminate redundant store operations. The algorithm operates on the static single assignment (SSA) form of the routine [18]. In contrast to hash-based approaches, this technique partitions values into congruence classes. <p> This chapter will present two techniques for removing code from a routine: Dominator-Based Removal The technique suggested by Alpern, Wegman, and Zadeck is to remove computations that are dominated by another definition of the same value number <ref> [4] </ref>. Figure 5.1 shows an example routine that we can improve with this method. Since the computation of z in block B 1 dominates the computation in block B 4 , the second computation can be removed.
Reference: [5] <author> Preston Briggs and Keith D. Cooper. </author> <title> Effective partial redundancy elimination. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(6) </volume> <pages> 159-170, </pages> <month> June </month> <year> 1994. </year> <booktitle> Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: Therefore, if all members of a class are touched, they will be returned to their original class. 5. The entry in lookup [x 0 ] indicates class number 5 and points to the node for x 0 in the members list of classes <ref> [5] </ref>. If x 0 is touched, the node will be removed from that list and appended to the members list of intersections [5]. <p> The entry in lookup [x 0 ] indicates class number 5 and points to the node for x 0 in the members list of classes <ref> [5] </ref>. If x 0 is touched, the node will be removed from that list and appended to the members list of intersections [5]. Once all the uses in position p have been touched, we can determine which classes must be split by iterating through each class s with a non-empty list at intersections [s]. To accomplish this, we must carefully maintain the set of classes to be split. <p> We must be careful to keep track of which classes have a non-empty entry in touched once or touched twice, just as we do for the intersections array. 5. The first time A is touched, it will be removed from the members list of classes <ref> [5] </ref> and appended to the members list of touched once [5]. The second time A is touched, it will be removed from the touched once [5] and appended to touched twice [5]. <p> The first time A is touched, it will be removed from the members list of classes <ref> [5] </ref> and appended to the members list of touched once [5]. The second time A is touched, it will be removed from the touched once [5] and appended to touched twice [5]. <p> The first time A is touched, it will be removed from the members list of classes <ref> [5] </ref> and appended to the members list of touched once [5]. The second time A is touched, it will be removed from the touched once [5] and appended to touched twice [5]. <p> The first time A is touched, it will be removed from the members list of classes <ref> [5] </ref> and appended to the members list of touched once [5]. The second time A is touched, it will be removed from the touched once [5] and appended to touched twice [5]. <p> The ability of the compiler to perform code motion is influenced heavily by the "shape" of the input program. Briggs and Cooper showed that global reassociation followed by value partitioning will transform code into a form that makes PRE more effective <ref> [5] </ref>. Further improvements are still possible. <p> The entries marked with a "ffi" represent optimizations that existed prior to the beginning of this research; entries marked with a "*" represent contributions of this research. To achieve accurate comparisons, we varied only the type of redundancy elimination performed. Routines were optimized with the sequence of global reassociation <ref> [5] </ref>, 93 Single Extended Dominator AVAIL Lazy Value-driven Basic Basic Based Based Code Code Blocks Blocks Removal Removal Motion Motion Hash-based ffi ffi * * * * Partitioning ffi * ffi * SCC * * * * Table 9.1 Tests performed redundancy elimination (different in each test), global constant propagation [45], <p> Our reference point will be the combination of value partitioning and lazy code motion as presented in 1994 by Briggs and Cooper <ref> [5] </ref>. We compare this combination with SCC-based value numbering and value-driven code motion. The results are shown in Figures 9.22 and 9.23. 9.6 Compile Times We compared the time required by each of the redundancy elimination techniques for some of the larger routines in the test suite. <p> In the future, we may see microprocessors where an integer multiply and an integer add both take a single cycle. On such a machine, strength reduction will still have a role to play. In combination with algebraic reassociation <ref> [16, 40, 5] </ref>, strength reduction may let the compiler use fewer induction variables in a loop, lowering both the operation count inside the loop and the demand for registers. This effect may be especially pronounced in code that has been automatically blocked to improve locality [46, 9]. <p> Chapters 2 through 7 describe the algorithms for redundancy elimination which combines loop invariant code motion and common subexpression elimination. We perform global reassociation and global renaming prior to redundancy elimination <ref> [5] </ref>. Because our algorithms for redundancy elimination will not move conditionally executed code out of loops, it is possible that some loop invariant code will remain inside a loop.
Reference: [6] <author> Preston Briggs, Keith D. Cooper, and Linda Torczon. </author> <title> Rematerialization. </title> <journal> SIGPLAN Notices, </journal> <volume> 27(7) </volume> <pages> 311-321, </pages> <month> July </month> <year> 1992. </year> <booktitle> Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: Q Q e 1 Q Q HP + Before Expression inserted + Q Q e 1 Q Q Q Qs + e 1 Move expressions forward 85 ? e 1 " ! # e 1 e 1 " ! e 1 ? Before After technique of Briggs, Cooper, and Torczon <ref> [6] </ref>. The key difference is where the code to compute the constant is inserted.
Reference: [7] <author> Preston Briggs, Keith D. Cooper, and Linda Torczon. </author> <title> Improvements to graph coloring register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(3) </volume> <pages> 428-455, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Another advantage of this approach is that it does not depend on the algorithm used for redundancy elimination. The idea is to insert instructions that reduce the register pressure but are cheaper than the load and store operations that will be inserted during register allocation <ref> [11, 7] </ref>. Further, these instructions will be placed on a more global basis than the spill code. In some sense, this transformation will undo the adverse effects of redundancy elimination. This transformation will operate as follows: 1. Perform value numbering to identify expressions with the same value. 2. <p> For example, the copy into t3 0 in Figure A.7 can be eliminated if the load into t4 0 uses the value of osr 6 directly. We rely on the copy coalescing phase of a Chaitin-style graph coloring register allocator to accomplish this task <ref> [11, 7] </ref>. A.3 Previous Work Reduction of operator strength has a long history in the literature. The classic method is presented in a paper by Allen, Cocke, and Kennedy [3]. It, in turn, builds on earlier work by Cocke and Kennedy [15, 28].
Reference: [8] <author> Jiazhen Cai and Robert Paige. </author> <title> "Look Ma, no hashing, and no arrays neither". </title> <booktitle> In Conference Record of the Eighteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 143-154, </pages> <address> Orlando, Florida, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: The Allen, Cocke, and Kennedy-style techniques, including ours, include loop invariant values as region constants. Paige has looked at reducing a number of set operators [36, 35] and using multiset discrimination as an alternative to hashing to avoid its worst case behavior <ref> [8] </ref>. Sites looked at the related issue of minimizing the number of loop induction variables [41]. Markstein, Markstein, and Zadeck, in a chapter for a forthcoming ACM Press Book, present an algorithm that combines strength reduction, expression reassociation, and code motion. Our work has treated these issues separately.
Reference: [9] <author> David Callahan, Steve Carr, and Ken Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <journal> SIGPLAN Notices, </journal> <volume> 25(6) </volume> <pages> 53-65, </pages> <month> June </month> <year> 1990. </year> <booktitle> Proceedings 137 of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: This effect may be especially pronounced in code that has been automatically blocked to improve locality <ref> [46, 9] </ref>. This chapter presents a new algorithm for performing strength reduction. It produces results similar to those of Allen, Cocke, and Kennedy's classic algorithm [3].
Reference: [10] <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 252-262, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: However, certain transformations applied by the compiler can introduce them. In our experiments, the operator strength reduction pass can introduce them even though it is fairly careful not to introduce redundancies. Other optimizations such as loop unrolling and transformations intended to improve cache performance can introduce them <ref> [10] </ref>. If any of these transformations are performed before redundancy elimination, then the compiler writer might give more weight to SCC-based value numbering. For the code motion phase, the choice is between lazy code motion and value- driven code motion.
Reference: [11] <author> Gregory J. Chaitin, Marc A. Auslander, Ashok K. Chandra, John Cocke, Martin E. Hopkins, and Peter W. Markstein. </author> <title> Register allocation via coloring. </title> <journal> Computer Languages, </journal> <volume> 6 </volume> <pages> 47-57, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Another advantage of this approach is that it does not depend on the algorithm used for redundancy elimination. The idea is to insert instructions that reduce the register pressure but are cheaper than the load and store operations that will be inserted during register allocation <ref> [11, 7] </ref>. Further, these instructions will be placed on a more global basis than the spill code. In some sense, this transformation will undo the adverse effects of redundancy elimination. This transformation will operate as follows: 1. Perform value numbering to identify expressions with the same value. 2. <p> For example, the copy into t3 0 in Figure A.7 can be eliminated if the load into t4 0 uses the value of osr 6 directly. We rely on the copy coalescing phase of a Chaitin-style graph coloring register allocator to accomplish this task <ref> [11, 7] </ref>. A.3 Previous Work Reduction of operator strength has a long history in the literature. The classic method is presented in a paper by Allen, Cocke, and Kennedy [3]. It, in turn, builds on earlier work by Cocke and Kennedy [15, 28].
Reference: [12] <author> David R. Chase. </author> <title> Brief survey of optimizations. </title> <type> Unpublished paper, </type> <year> 1987. </year>
Reference-contexts: Linear function test replacement is a separate pass for each loop. Chase extended the Allen, Cocke, and Kennedy method to reduce more additions <ref> [12] </ref>. A second family of techniques has grown up around the literature of data-flow analysis [19, 26, 20, 30]. These methods use the careful code placement calculations developed for code motion to perform strength reduction.
Reference: [13] <author> Cliff Click. </author> <title> Combining Analyses, Combining Optimizations. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1995. </year>
Reference-contexts: Such an algorithm would combine the ability to perform constant folding and algebraic simplification with the ability to make optimistic assumptions and later disprove them. Click presents an extension to value partitioning that includes constant folding, algebraic simplification, and unreachable code elimination <ref> [13] </ref>. He presents two versions of the algorithm.
Reference: [14] <author> John Cocke. </author> <title> Global common subexpression elimination. </title> <journal> SIGPLAN Notices, </journal> <volume> 5(7) </volume> <pages> 20-24, </pages> <month> July </month> <year> 1970. </year> <booktitle> Proceedings of a Symposium on Compiler Optimization. </booktitle>
Reference-contexts: Properties of the value numbered SSA form let us simplify the formulation of AVAIL. The traditional data-flow equations deal with the formal identity of lexical names, while our equations deal with identical values <ref> [14] </ref>. This is a very important distinction. We need not consider the killed set for a block because no values are redefined in SSA form, and value numbering preserves this property. Consider the code fragment in Figure 5.5.
Reference: [15] <author> John Cocke and Ken Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> <month> 20(11), November </month> <year> 1977. </year>
Reference-contexts: A.3 Previous Work Reduction of operator strength has a long history in the literature. The classic method is presented in a paper by Allen, Cocke, and Kennedy [3]. It, in turn, builds on earlier work by Cocke and Kennedy <ref> [15, 28] </ref>. These algorithms transform one loop at a time, 135 working outward through each loop nest, making passes to generate def-use chains, find loops and insert landing pads, find region constants and induction variables, and to perform the actual reduction and instruction replacement.
Reference: [16] <author> John Cocke and Peter Markstein. </author> <title> Measurement of program improvement algo-rithms. </title> <booktitle> In Proceedings of Information Processing 80. </booktitle> <publisher> North Holland Publishing Company, </publisher> <year> 1980. </year>
Reference-contexts: In the future, we may see microprocessors where an integer multiply and an integer add both take a single cycle. On such a machine, strength reduction will still have a role to play. In combination with algebraic reassociation <ref> [16, 40, 5] </ref>, strength reduction may let the compiler use fewer induction variables in a loop, lowering both the operation count inside the loop and the demand for registers. This effect may be especially pronounced in code that has been automatically blocked to improve locality [46, 9].
Reference: [17] <author> John Cocke and Jacob T. Schwartz. </author> <title> Programming languages and their compilers: Preliminary notes. </title> <type> Technical report, </type> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference-contexts: It uses value numbers to find redundant computations and remove them. 4. It discovers constant values, evaluates expressions whose operands are constants, and propagates them through the code. Cocke and Schwartz describe a local technique that uses hashing to discover redundant computations and fold constants <ref> [17] </ref>. We believe the technique was originally invented by Balke at Computer Sciences Corporation [24]. Each unique value is identified by its value number. Two computations in a basic block have the same value number if they are provably equal.
Reference: [18] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The nodes in the CFG represent basic blocks and the edges correspond to possible control flow from one basic block directly to another. 1.2 Static Single Assignment Form Many of the optimizations presented here are based on static single assignment (SSA) form <ref> [18] </ref>. The basic idea used in constructing SSA form is to give unique names to the targets of all assignments in the routine, and then to overwrite uses of the assignments with the new names. A complication arises in routines with more than one basic block. <p> adds a fair amount of overhead and complication to the algorithm, but it does not change its asymptotic complexity. 2.1 Static Single Assignment Form Many of the difficulties encountered during value numbering of extended basic blocks can be overcome by constructing the static single assignment (SSA) form of the routine <ref> [18] </ref>. The critical property of SSA that we require is the naming discipline that it imposes on the code. Each SSA name is assigned a value by exactly one operation in a routine; therefore, no name is ever reassigned, and no expression ever becomes inaccessible. <p> The compiler can use this replacement scheme over a limited region of code in blocks dominated by the operation and in parameters to -nodes in the dominance frontier of the operation <ref> [18] </ref>. 1 In both cases, control must flow through the block where the first evaluation occurred (defining the SSA name's value). The -nodes require special treatment. Before the compiler can analyze the - nodes in a block, it must have previously assigned value numbers to all of the inputs. <p> However, it is possible to incorporate value numbering into the SSA construction process. There is a great deal of similarity between the value numbering process and the renaming process during SSA construction <ref> [18, section 5.2] </ref>. The renaming process can be modified as follows to accomplish renaming and value numbering simultaneously: 17 * For each name in the original program, a stack is maintained which contains subscripts used to replace uses of that name. <p> In this chapter, we extend this algorithm to handle commutative operations and to eliminate redundant store operations. The algorithm operates on the static single assignment (SSA) form of the routine <ref> [18] </ref>. In contrast to hash-based approaches, this technique partitions values into congruence classes. Two values are congruent if they are 1. Computed by the same opcode, and 2. Each of the corresponding operands are congruent. <p> ffi * * * * Partitioning ffi * ffi * SCC * * * * Table 9.1 Tests performed redundancy elimination (different in each test), global constant propagation [45], operator strength reduction (see Appendix A), redundancy elimination 18 , global constant propagation 18 , global peephole optimization, dead code elimination <ref> [18, Section 7.1] </ref>, copy coalescing, and a pass to eliminate empty basic blocks. 9.3 Raw Instruction Counts Figures 9.2 through 9.7 present ILOC instruction counts for each combination in Table 9.1. Table 9.2 presents a guide for these figures. <p> This chapter presents a new algorithm for performing strength reduction. It produces results similar to those of Allen, Cocke, and Kennedy's classic algorithm [3]. By assuming some specific prior optimizations and operating on the SSA form of the procedure <ref> [18] </ref>, we have derived a method that (1) is simple to understand and to implement, (2) relies on the dominator tree which must be computed during SSA 122 do i = 1, 100 enddo sum 0:0 L: t1 i 1 t3 t2 + a t4 load t3 sum sum + t4 <p> Since we consider compile-time constants to be region 124 constants, we require some form of constant propagation to identify as many constants as possible. We use Wegman and Zadeck's sparse conditional constant algorithm [44]. We construct the pruned SSA form of the program <ref> [18] </ref>. In the program's SSA graph, each node represents an operation or a -node, and edges flow from uses to definitions. The SSA graph can be built from the resulting program by adding the use-definition chains, which can be represented as a lookup table indexed by SSA names. <p> The SSA graph in Figure A.7 contains a great deal of dead code. This is because many of the use-definition edges in the original SSA graph have been changed, resulting in "orphaned" nodes. We rely on a separate pass of dead code elimination to remove these instructions <ref> [18, Section 7.1] </ref>. Many of the copies introduced during strength reduction can be eliminated. For example, the copy into t3 0 in Figure A.7 can be eliminated if the load into t4 0 uses the value of osr 6 directly.
Reference: [19] <author> Dhananjay M. Dhamdhere. </author> <title> On algorithms for operator strength reduction. </title> <journal> Communications of the ACM, </journal> <pages> pages 311-312, </pages> <month> May </month> <year> 1979. </year> <month> 138 </month>
Reference-contexts: Linear function test replacement is a separate pass for each loop. Chase extended the Allen, Cocke, and Kennedy method to reduce more additions [12]. A second family of techniques has grown up around the literature of data-flow analysis <ref> [19, 26, 20, 30] </ref>. These methods use the careful code placement calculations developed for code motion to perform strength reduction. These methods avoid the control-flow analysis used in the Allen, Cocke, and Kennedy methods; our algorithm uses properties of SSA for the same purpose.
Reference: [20] <author> Dhananjay M. Dhamdhere. </author> <title> A new algorithm for composite hoisting and strength reduction. </title> <journal> International Journal of Computer Mathematics, </journal> <pages> pages 1-14, </pages> <year> 1989. </year>
Reference-contexts: Linear function test replacement is a separate pass for each loop. Chase extended the Allen, Cocke, and Kennedy method to reduce more additions [12]. A second family of techniques has grown up around the literature of data-flow analysis <ref> [19, 26, 20, 30] </ref>. These methods use the careful code placement calculations developed for code motion to perform strength reduction. These methods avoid the control-flow analysis used in the Allen, Cocke, and Kennedy methods; our algorithm uses properties of SSA for the same purpose.
Reference: [21] <author> Karl-Heinz Drechsler and Manfred P. Stadel. </author> <title> A solution to a problem with Morel and Renvoise's "Global optimization by suppression of partial redundan-cies". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4):635640, </volume> <month> October </month> <year> 1988. </year>
Reference-contexts: set of locations where each computation will produce the same value and to select the ones that are expected to be least frequently executed. 6.1 Partial Redundancy Elimination Partial redundancy elimination (PRE) is an optimization introduced by Morel and Renvoise that combines common subexpression elimination with loop invariant code motion <ref> [34, 21] </ref>. Partially redundant computations are redundant along some, but not necessarily all, execution paths. In general, PRE moves code upward in the routine to the earliest point where the computation would produce that same value without lengthening any path through the program.
Reference: [22] <author> Karl-Heinz Drechsler and Manfred P. Stadel. </author> <title> A variation of Knoop, Ruthing, and Steffen's "lazy code motion". </title> <journal> SIGPLAN Notices, </journal> <volume> 28(5) </volume> <pages> 29-38, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Drechsler and Stadel present a variation of this technique that they claim is more practical <ref> [22] </ref>. The data-flow equations for this framework are shown in Figures 6.3 and 6.4. One advantage that Drechsler and Stadel's framework has over Knoop et al. is that it never inserts instructions in the middle of a block.
Reference: [23] <author> George E. Forsythe, Michael A. Malcolm, and Cleve B. Moler. </author> <title> Computer Methods for Mathematical Computations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1977. </year>
Reference-contexts: Therefore, we feel that our experiments are useful in measuring the effectiveness of machine-independent optimizations. 9.2 Tests Performed In this experiment, we optimized over 50 routines from the Spec benchmark suite and from Forsythe, Malcolm, and Moler's book on numerical methods <ref> [23] </ref>. We refer to the latter as the FMM benchmark. Each routine was optimized in several different ways by varying the type of value numbering and code removal/motion. Table 9.1 shows the possible combinations.
Reference: [24] <author> Particia C. Goldberg. </author> <title> A comparison of certain optimization techniques. </title> <editor> In Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers, </booktitle> <pages> pages 31-50. </pages> <address> PrenticeHall, </address> <year> 1972. </year>
Reference-contexts: It discovers constant values, evaluates expressions whose operands are constants, and propagates them through the code. Cocke and Schwartz describe a local technique that uses hashing to discover redundant computations and fold constants [17]. We believe the technique was originally invented by Balke at Computer Sciences Corporation <ref> [24] </ref>. Each unique value is identified by its value number. Two computations in a basic block have the same value number if they are provably equal. In the literature, this technique and its derivatives are called "value numbering." The algorithm is relatively simple. In practice, it is very fast.
Reference: [25] <author> Matthew S. Hecht. </author> <title> Flow Analysis of Computer Programs. </title> <booktitle> Programming Languages Series. </booktitle> <publisher> Elsevier North-Holland, Inc., </publisher> <address> 52 Vanderbilt Avenue, New York, NY 10017, </address> <year> 1977. </year>
Reference-contexts: If XY and X 6= Y , then X strictly dominates Y (X Y ). The immediate dominator of Y (idom (Y )) is the closest strict dominator of Y <ref> [25] </ref>. In the routine's dominator tree, the parent of each node is its immediate dominator. Notice that all nodes that dominate a node X are ancestors of X in the dominator tree.
Reference: [26] <author> J.R. Issac and Dhananjay M. Dhamdhere. </author> <title> A composite algorithm for strength reduction and code movement. </title> <journal> International Journal of Computer and Information Sciences, </journal> <pages> pages 243-273, </pages> <year> 1980. </year>
Reference-contexts: Linear function test replacement is a separate pass for each loop. Chase extended the Allen, Cocke, and Kennedy method to reduce more additions [12]. A second family of techniques has grown up around the literature of data-flow analysis <ref> [19, 26, 20, 30] </ref>. These methods use the careful code placement calculations developed for code motion to perform strength reduction. These methods avoid the control-flow analysis used in the Allen, Cocke, and Kennedy methods; our algorithm uses properties of SSA for the same purpose.
Reference: [27] <author> John B. Kam and Jeffrey D. Ullman. </author> <title> Global data flow analysis and iterative algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 23(1) </volume> <pages> 158-171, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: Kam and Ullman showed that the iterative data-flow analysis used for a large class of data-flow frameworks requires D (CFG) passes over the CFG <ref> [27] </ref>. This number can be as large as O (B) where B is the number of blocks in the CFG, but it is believed that, in practice, this number is bounded by a small constant [32]. We expect that for most programs D (CFG) = D (SSA).
Reference: [28] <author> Ken Kennedy. </author> <title> Reduction in strength using hashed temporaries. </title> <type> SETL Newsletter 102, </type> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <month> March </month> <year> 1973. </year>
Reference-contexts: Apply Insert an instruction to apply an opcode to two operands and return the SSA name of the result. Simplifications such as constant folding are performed if possible. The replacement process is supported by a hash table that tracks the results of reduction <ref> [28] </ref>. This prevents us from performing the same reduction twice and causes the recursion in Reduce to terminate. <p> A.3 Previous Work Reduction of operator strength has a long history in the literature. The classic method is presented in a paper by Allen, Cocke, and Kennedy [3]. It, in turn, builds on earlier work by Cocke and Kennedy <ref> [15, 28] </ref>. These algorithms transform one loop at a time, 135 working outward through each loop nest, making passes to generate def-use chains, find loops and insert landing pads, find region constants and induction variables, and to perform the actual reduction and instruction replacement.
Reference: [29] <author> Jens Knoop, Oliver Ruthing, and Bernhard Steffen. </author> <title> Lazy code motion. </title> <journal> SIGPLAN Notices, </journal> <volume> 27(7) </volume> <pages> 224-234, </pages> <month> July </month> <year> 1992. </year> <booktitle> Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: This will shorten the path through B 3 and leave the length of the path through B 2 unchanged. 6.2 Lazy Code Motion Knoop, Ruthing, and Steffen describe a descendant of PRE, called lazy code motion (LCM) <ref> [29, 31] </ref>. Drechsler and Stadel present a variation of this technique that they claim is more practical [22]. The data-flow equations for this framework are shown in Figures 6.3 and 6.4. <p> It has been argued that eliminating redundancies will always increase register pressure <ref> [29, 31] </ref>. However, it is also possible that eliminating redundancies can reduce register pressure. The example in Figure 8.1 demonstrates how this can happen. When the second computation of x is removed, the lifetime of the first computation of x is lengthened.
Reference: [30] <author> Jens Knoop, Oliver Ruthing, and Bernhard Steffen. </author> <title> Lazy strength reduction. </title> <journal> Journal of Programming Languages, </journal> <volume> 1(1) </volume> <pages> 71-91, </pages> <year> 1993. </year> <month> 139 </month>
Reference-contexts: Linear function test replacement is a separate pass for each loop. Chase extended the Allen, Cocke, and Kennedy method to reduce more additions [12]. A second family of techniques has grown up around the literature of data-flow analysis <ref> [19, 26, 20, 30] </ref>. These methods use the careful code placement calculations developed for code motion to perform strength reduction. These methods avoid the control-flow analysis used in the Allen, Cocke, and Kennedy methods; our algorithm uses properties of SSA for the same purpose.
Reference: [31] <author> Jens Knoop, Oliver Ruthing, and Bernhard Steffen. </author> <title> Optimal code motion: </title> <journal> Theory and practice. ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(4) </volume> <pages> 1117-1155, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: This will shorten the path through B 3 and leave the length of the path through B 2 unchanged. 6.2 Lazy Code Motion Knoop, Ruthing, and Steffen describe a descendant of PRE, called lazy code motion (LCM) <ref> [29, 31] </ref>. Drechsler and Stadel present a variation of this technique that they claim is more practical [22]. The data-flow equations for this framework are shown in Figures 6.3 and 6.4. <p> It has been argued that eliminating redundancies will always increase register pressure <ref> [29, 31] </ref>. However, it is also possible that eliminating redundancies can reduce register pressure. The example in Figure 8.1 demonstrates how this can happen. When the second computation of x is removed, the lifetime of the first computation of x is lengthened.
Reference: [32] <author> Donald E. Knuth. </author> <title> An empirical study of Fortran programs. </title> <journal> Software Practice and Experience, </journal> <volume> 1 </volume> <pages> 105-133, </pages> <year> 1971. </year>
Reference-contexts: The loop connectedness of a graph is the maximum number of back edges in any acyclic path. This number can be as large as O (N ); Knuth showed that, for control-flow graphs of real Fortran programs, it is bounded, in practice, by three <ref> [32] </ref>. We are concerned with the loop-connectedness of the SSA graph; we also expect it to be small. In our test suite, the maximum number of iterations required by the SCC algorithm is four. We will first present a simplified version, called the RPO algorithm, that is easier to understand. <p> This number can be as large as O (B) where B is the number of blocks in the CFG, but it is believed that, in practice, this number is bounded by a small constant <ref> [32] </ref>. We expect that for most programs D (CFG) = D (SSA). However, the program in Figure 4.5 is an example where this is not true. The back edges are shown with bold arrows. Notice that D (CFG) = 2, but D (SSA) = 6.
Reference: [33] <author> Thomas Lengauer and Robert Endre Tarjan. </author> <title> A fast algorithm for finding domina-tors in a flowgraph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 121-141, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: In a flow graph, if node X appears on every path from the 12 start node to node Y , then X dominates Y (XY ) <ref> [33] </ref>. If XY and X 6= Y , then X strictly dominates Y (X Y ). The immediate dominator of Y (idom (Y )) is the closest strict dominator of Y [25]. In the routine's dominator tree, the parent of each node is its immediate dominator. <p> We avoid the need to build the loop tree by using the dominator tree that was constructed during the conversion to SSA form <ref> [33] </ref>. We take advantage of two key properties of SSA form to identify region constants and induction variables. 1.
Reference: [34] <author> Etienne Morel and Claude Renvoise. </author> <title> Global optimization by suppression of par-tial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: set of locations where each computation will produce the same value and to select the ones that are expected to be least frequently executed. 6.1 Partial Redundancy Elimination Partial redundancy elimination (PRE) is an optimization introduced by Morel and Renvoise that combines common subexpression elimination with loop invariant code motion <ref> [34, 21] </ref>. Partially redundant computations are redundant along some, but not necessarily all, execution paths. In general, PRE moves code upward in the routine to the earliest point where the computation would produce that same value without lengthening any path through the program.
Reference: [35] <author> Robert Paige and Shaye Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Their principal limitation is that they work from a simpler notion of a region constant|only literal constants can be found. The Allen, Cocke, and Kennedy-style techniques, including ours, include loop invariant values as region constants. Paige has looked at reducing a number of set operators <ref> [36, 35] </ref> and using multiset discrimination as an alternative to hashing to avoid its worst case behavior [8]. Sites looked at the related issue of minimizing the number of loop induction variables [41].
Reference: [36] <author> Robert Paige and Jacob T. Schwartz. </author> <title> Reduction in strength of high level oper-ations. </title> <booktitle> In Conference Record of the Fourth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 58-71, </pages> <address> Los Angeles, California, </address> <month> January </month> <year> 1977. </year>
Reference-contexts: Their principal limitation is that they work from a simpler notion of a region constant|only literal constants can be found. The Allen, Cocke, and Kennedy-style techniques, including ours, include loop invariant values as region constants. Paige has looked at reducing a number of set operators <ref> [36, 35] </ref> and using multiset discrimination as an alternative to hashing to avoid its worst case behavior [8]. Sites looked at the related issue of minimizing the number of loop induction variables [41].
Reference: [37] <author> Gordon D. Plotkin. </author> <title> Call-by-name, call-by-value, and the -calculus. </title> <journal> Theoretical Computer Science, </journal> <volume> 1 </volume> <pages> 125-159, </pages> <year> 1975. </year>
Reference-contexts: There is no guarantee that the resulting code cannot be improved. In fact, it is possible that the optimizer will make the program worse. Therefore, many tradeoffs must be considered when designing an optimizer. The fundamental responsibility of the optimizer is to preserve observational equivalence <ref> [37] </ref>. In other words, the optimized program must produce the same results as the unoptimized program for all possible inputs. A typical optimizer is organized as a sequence of passes (or optimizations). Each pass tries to either improve the running time of the program or decrease its space requirements.
Reference: [38] <author> Lori L. Pollock. </author> <title> An Approach to Incremental Compilation of Optimized Code. </title> <type> PhD thesis, </type> <institution> University of Pittsburgh, </institution> <year> 1986. </year>
Reference-contexts: One optimization may require code in a certain "shape". One optimization may discover "facts" that another needs. One optimization may introduce opportunities or destroy opportunities for another optimization <ref> [38] </ref>. 1.4 Classification of Optimizations When discussing a particular optimization, it is often helpful to compare it to other optimizations. To do this effectively, we must understand which optimizations are good candidates for comparison. The first classification is based on the assumptions made about the target machine.
Reference: [39] <author> John H. Reif. </author> <title> Symbolic programming analysis in almost linear time. </title> <booktitle> In Conference Record of the Fifth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 76-83, </pages> <address> Tucson, Arizona, </address> <month> January </month> <year> 1978. </year>
Reference-contexts: This name is called the -node's original name. In practice, to save space and time, -functions are placed at only certain join points and for only certain names. Specifically, a -function is placed at the birthpoint of each value the earliest location where the joined value exists <ref> [39] </ref>. The -functions provide a single definition for a name that had more than one definition (on different control-flow paths) in the original routine.
Reference: [40] <author> Vatsa Santhanam. </author> <title> Register reassociation in PA-RISC compilers. </title> <journal> HewlettPackard Journal, </journal> <pages> pages 33-38, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In the future, we may see microprocessors where an integer multiply and an integer add both take a single cycle. On such a machine, strength reduction will still have a role to play. In combination with algebraic reassociation <ref> [16, 40, 5] </ref>, strength reduction may let the compiler use fewer induction variables in a loop, lowering both the operation count inside the loop and the demand for registers. This effect may be especially pronounced in code that has been automatically blocked to improve locality [46, 9].
Reference: [41] <author> Richard L. </author> <title> Sites. The compilation of loop induction expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 50-57, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Paige has looked at reducing a number of set operators [36, 35] and using multiset discrimination as an alternative to hashing to avoid its worst case behavior [8]. Sites looked at the related issue of minimizing the number of loop induction variables <ref> [41] </ref>. Markstein, Markstein, and Zadeck, in a chapter for a forthcoming ACM Press Book, present an algorithm that combines strength reduction, expression reassociation, and code motion. Our work has treated these issues separately. A.4 Summary This chapter presents a simple and elegant algorithm for performing reduction of operator strength.
Reference: [42] <author> Robert E. Tarjan. </author> <title> Depth first search and linear graph algorithms. </title> <journal> SIAM Journal on Computing, </journal> <volume> 1(2) </volume> <pages> 146-160, </pages> <month> June </month> <year> 1972. </year> <month> 140 </month>
Reference-contexts: This observation led us to an improved algorithm, called SCC-based value numbering because it concentrates on the strongly connected components of the SSA graph. The algorithm works in conjunction with Tarjan's depth-first algorithm for finding SCCs, shown in Figure 4.6 <ref> [42] </ref>. Tarjan's algorithm uses a stack to determine which nodes are in the same SCC; nodes not contained in any cycle are popped singly, while all the nodes in the same SCC are popped together. <p> The idea of finding induction variables as SCCs of the SSA graph is due to Wolfe [47]. To discover the SCCs, we will use Tarjan's algorithm based on depth-first search, shown in Figure A.4 <ref> [42] </ref>. It uses a stack to determine which nodes are in the same SCC; nodes not contained in any cycle are popped singly, while all the nodes in the same SCC are popped together.
Reference: [43] <author> Robert Endre Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year>
Reference-contexts: Figure A.3 shows the SSA graph for the example program in Figure A.1. A.1.2 Finding Region Constants and Induction Variables Previous strength reduction algorithms have been centered around loops, or regions, inside a procedure. These are detected using Tarjan's flow-graph reducibility algorithm <ref> [43] </ref>. Given a region r, a node in the SSA graph is a region constant with respect to r if its value does not change inside r.
Reference: [44] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with condi-tional branches. </title> <booktitle> In Conference Record of the Twelfth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 291-299, </pages> <address> New Orleans, Louisiana, </address> <month> January </month> <year> 1985. </year>
Reference-contexts: Since we consider compile-time constants to be region 124 constants, we require some form of constant propagation to identify as many constants as possible. We use Wegman and Zadeck's sparse conditional constant algorithm <ref> [44] </ref>. We construct the pruned SSA form of the program [18]. In the program's SSA graph, each node represents an operation or a -node, and edges flow from uses to definitions.
Reference: [45] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with condi-tional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: We do this by associating a value from the constant propagation lattice (f&gt;; ?g [ Z) with each SSA name <ref> [45] </ref>. This framework will discover 48 at least as many congruences as hash-based value numbering or value partitioning. Under this extended framework, an element can fall D (SSA) + 1 times with respect to the value numbering lattice and twice with respect to the constant propagation lattice. <p> [5], 93 Single Extended Dominator AVAIL Lazy Value-driven Basic Basic Based Based Code Code Blocks Blocks Removal Removal Motion Motion Hash-based ffi ffi * * * * Partitioning ffi * ffi * SCC * * * * Table 9.1 Tests performed redundancy elimination (different in each test), global constant propagation <ref> [45] </ref>, operator strength reduction (see Appendix A), redundancy elimination 18 , global constant propagation 18 , global peephole optimization, dead code elimination [18, Section 7.1], copy coalescing, and a pass to eliminate empty basic blocks. 9.3 Raw Instruction Counts Figures 9.2 through 9.7 present ILOC instruction counts for each combination in
Reference: [46] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <journal> SIGPLAN Notices, </journal> <volume> 26(6) </volume> <pages> 30-44, </pages> <month> June </month> <year> 1991. </year> <booktitle> Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: This effect may be especially pronounced in code that has been automatically blocked to improve locality <ref> [46, 9] </ref>. This chapter presents a new algorithm for performing strength reduction. It produces results similar to those of Allen, Cocke, and Kennedy's classic algorithm [3].
Reference: [47] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <journal> SIGPLAN Notices, </journal> <volume> 27(7) </volume> <pages> 162-174, </pages> <month> July </month> <year> 1992. </year> <booktitle> Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: In Figure A.3, the SCC containing sum 1 and sum 2 does not represent an induction variable because t4 0 is not a region constant. The idea of finding induction variables as SCCs of the SSA graph is due to Wolfe <ref> [47] </ref>. To discover the SCCs, we will use Tarjan's algorithm based on depth-first search, shown in Figure A.4 [42].
References-found: 47

