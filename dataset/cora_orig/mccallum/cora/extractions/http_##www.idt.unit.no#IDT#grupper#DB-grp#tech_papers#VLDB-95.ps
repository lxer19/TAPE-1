URL: http://www.idt.unit.no/IDT/grupper/DB-grp/tech_papers/VLDB-95.ps
Refering-URL: http://www.idt.unit.no/IDT/grupper/DB-grp/tech_papers/tech_papers.html
Root-URL: 
Title: A Temporal Foundation of Video Databases  
Author: Rune Hjelsvold, Roger Midtstraum, and Olav Sandst-a 
Address: Trondheim, Norway  
Affiliation: Norwegian Institute of Technology  
Abstract: Audio and video data represent streams of data with inherent temporal properties. In this paper we consider a video database as a collection of partial ordered sets where temporal relationships exist between elements from the same video stream. Video production introduces dependencies between different time coordinate systems. In this paper we give a formal definition of the contents of a video database. We also define mapped video object sets and operations on such sets that can be used for querying the temporal properties of video data and that can be used for mapping video objects between different time coordinate systems. At last, we illustrate how the proposed foundation can be used in searching and browsing.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.F. Allen. </author> <title> Maintaining Knowledge about Temporal Intervals. </title> <journal> Communications of the ACM, </journal> <month> November </month> <year> 1983. </year>
Reference-contexts: t Y = Z where 8z 2 Z (9x 2 X (z:Interval Within x:Interval)^ 6 9y 2 Y (y:Interval Intersects z:Interval)^ 6 9v 2 Z (v Intersects z ^ v 6= z))g Filter Operations Allen has shown that there are 13 (disjunct) relationships that can exist between two temporal intervals <ref> [1] </ref> that can be evaluated by comparing start and end times for the intervals [13]. Other relationships, such as Intersects can also be defined.
Reference: [2] <institution> Apple Computer, Inc. </institution> <month> QuickTime. </month> <title> Inside Macintosh. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1993. </year>
Reference-contexts: AV Clips are the building blocks in a video stream composition. Each AV Clip represents an interval of an audio/video recording (stored media segment) and all AV Clips in a video stream share a common time-line in a way similar to the approach used in QuickTime <ref> [2] </ref>. This means that we assume that stored segment time values are bound to video stream time coordinate systems during production.
Reference: [3] <author> P. Atzeni and V. De Antonellis. </author> <title> Relational Database Theory. </title> <publisher> The Ben-jamin/Cummings Publishing Company, Inc., </publisher> <year> 1993. </year>
Reference-contexts: The rest of this subsection and the next subsection will explain the use of the mapped video object set. Set and Relational Operations Set theoretic operations such as intersection, union, and difference are defined in the normal way. Relational operations such as selection and projection <ref> [3] </ref> are also defined on mapped video object sets: Selection: P (A) = fa j a 2 A ^ P (a) g Projection: Y (A) = fa [Y ] j a 2 Ag where P is a boolean predicate defined over attributes of the elements of A.
Reference: [4] <author> C. Breiteneder, S. Gibbs, and D. Tsichritzis. </author> <title> Modelling of Audio/Video Data. </title> <booktitle> In Proceedings of the 11th International Conference on the Entity-Relationship Approach, </booktitle> <address> Karlsruhe, Germany, </address> <month> October 7-9 </month> <year> 1992. </year>
Reference-contexts: Much of the research on video databases is concerned with the problems of guaranteeing constant frame rates and audio/video synchronization. Video information is, however, more complex than just a stream of frames. As noted by Gibbs et al. <ref> [4] </ref>, one piece of video may be temporally derived from another piece of video - e.g., when a sequence from a video recording is used in a video document. <p> In this paper, however, we are concerned with the time systems defined by video streams and stored media segments and the relation between these time systems. Digital audio and video represent data with a discrete time system <ref> [4] </ref> - e.g., the unit of time in PAL video is 1/25th of a second while the unit of time in CD audio is 1/44100th of a second. Contrary to real time, there is no total ordering of time in a video database.
Reference: [5] <author> J. Clifford and A. Crocker. </author> <title> The Historical Relational Data Model (HRDM) Revisited. In A.U. </title> <editor> Tansel et al., editors, </editor> <title> Temporal Databases: Theory, Design, and Implementation, chapter 1. </title> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1993. </year>
Reference-contexts: The temporal set operations defined in this section are different from the ones defined by Clifford and Crocker <ref> [5] </ref>, in which case only merge-compatible tuples - i.e., tuples where attributes other than the time attributes have pairwise equal values are merged. The reason for defining the operations this way will become more apparent when discussing querying in Section 6.
Reference: [6] <author> A. Duda, R. Weiss, and D.K. Gifford. </author> <title> Content-Based Access to Algebraic Video. </title> <booktitle> In Proceedings of the International Conference on Multimedia Computing and Systems, </booktitle> <address> Boston, MA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: We then avoid unnesting of an arbitrary number of levels during replay and querying. In addition, this allows the distinctions that we have made between primary, basic and secondary contexts. The opposite choice has been taken in OVID [17] and by Duda et al. <ref> [6] </ref> to give the authors of video/multimedia documents greater flexibility in reusing composite components and in handling document dependencies.
Reference: [7] <author> J.C. Ellis. </author> <title> A History of Film. </title> <publisher> Prentice Hall, 3rd edition, </publisher> <year> 1990. </year>
Reference-contexts: During a series of experiments and studies in the last half of the 1920s, Soviet film makers <ref> [7] </ref> found that the meaning of a piece of film was heavily influenced by the surrounding parts (i.e., its context). Sergei M. <p> Sergei M. Eisenstein, for instance, showed "the fact that two film pieces of any kind, placed together, inevitably combine into a new concept, a new quality, arising out of the juxtaposition" <ref> [7] </ref>. Eisenstein illustrates this by a small example: "For example, take a grave, juxtaposed with a woman in mourning weeping beside it, and scarcely anybody will fail to jump to the conclusion: a widow." [7] The Soviet film makers identified three main steps in film creation time considerations play the major <p> placed together, inevitably combine into a new concept, a new quality, arising out of the juxtaposition" <ref> [7] </ref>. Eisenstein illustrates this by a small example: "For example, take a grave, juxtaposed with a woman in mourning weeping beside it, and scarcely anybody will fail to jump to the conclusion: a widow." [7] The Soviet film makers identified three main steps in film creation time considerations play the major role in two of these steps: Pudovkin offers a sort of formula: Film creation equals (1) what is shown in the shots, (2) the order in which they appear, and (3) how long each <p> three main steps in film creation time considerations play the major role in two of these steps: Pudovkin offers a sort of formula: Film creation equals (1) what is shown in the shots, (2) the order in which they appear, and (3) how long each is held on the screen. <ref> [7] </ref> It is, therefore, necessary to establish a context - i.e., adding pieces of video for the user to interpret a single piece of video correctly.
Reference: [8] <author> R. Hjelsvold. </author> <title> Sharing and Reuse of Video Information. </title> <booktitle> In Proceedings of the ACM Multimedia'94 Conference Workshop on Multimedia Database Management Systems, </booktitle> <address> San Francisco, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: When digital video is managed by a computer one can create virtual video documents [15, 14] and, thus, avoid making copies of video and meta-data to save storage space and preserve database consistency <ref> [8] </ref>. Since video is a complex data type we have argued [11] that a video database should provide a generic data model which captures the most important properties of video data such as video document composition, video document structuring and contents indexing.
Reference: [9] <author> R. Hjelsvold. </author> <title> Video Information Contents and Architecture. </title> <booktitle> In Proceedings of the 4th International Conference on Extending Database Technology, </booktitle> <address> Cambridge, UK, </address> <month> March 28-31 </month> <year> 1994. </year>
Reference-contexts: The main goal of our research is to develop database support for modelling, searching and browsing of video data. 1 Some typical frame rates are 24 fps (cinematic film), 25 fps (PAL the European video standard), and 30 fps (NTSC the American/Japanese video standard). 1 As discussed in <ref> [9, 10] </ref>, a video database should support applications and users in sharing video information. In a shared environment the same piece of video may be used in several video documents. <p> As shown in Figure 2 this relation can be established via an annotation object that identifies the stream interval of interest and that is linked to an element of a real-world model. * Video Document Structure: Video documents can as other more traditional documents have a certain structure <ref> [9] </ref>. This structure can be represented by a set of structural components where each component identifies a stream interval. 3.1 Media Streams and Video Time Systems As other data, video data can be related to real time - e.g., date and time for production (recording and editing).
Reference: [10] <author> R. Hjelsvold and R. Midtstraum. </author> <title> Modelling and Querying Video Data. </title> <booktitle> In Proceedings of the 20th VLDB Conference, </booktitle> <address> Santiago, Chile, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: The main goal of our research is to develop database support for modelling, searching and browsing of video data. 1 Some typical frame rates are 24 fps (cinematic film), 25 fps (PAL the European video standard), and 30 fps (NTSC the American/Japanese video standard). 1 As discussed in <ref> [9, 10] </ref>, a video database should support applications and users in sharing video information. In a shared environment the same piece of video may be used in several video documents. <p> A more thorough discussion of application domain models related to video databases can be found in <ref> [10] </ref> and in [18] The space limitations do not allow a discussion of every aspect of querying in our model.
Reference: [11] <author> R. Hjelsvold and R. Midtstraum. </author> <title> Databases for Video Information Sharing. </title> <booktitle> In Proceedings of the IS&T/SPIE Symposium on Electronic Imaging Science and Technology, Conference on Storage and Retrieval for Image and Video Databases III, </booktitle> <address> San Jose, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: When digital video is managed by a computer one can create virtual video documents [15, 14] and, thus, avoid making copies of video and meta-data to save storage space and preserve database consistency [8]. Since video is a complex data type we have argued <ref> [11] </ref> that a video database should provide a generic data model which captures the most important properties of video data such as video document composition, video document structuring and contents indexing. It should also provide a set of well-defined operations that can be used in video data browsing and querying. <p> These documents constitute the secondary context seen from one specific document's perspective. 3 Video Data A video database will store different types of information as shown in the data model presented in <ref> [11] </ref>. In this paper we will define a video database as a collection of sets where the elements of each set are elements of the same type. We are using an object-oriented approach so each element is identified by a unique object identity. <p> We have proposed a video document structure that allows the user to organize the contents of a video document in a hierarchical structure (inspired by film theory [16]) of shots, scenes, sequences, and compound units <ref> [11] </ref>. One important aspect of structural components is that they may represent contexts for interpreting video data at various levels of granularity - e.g., a shot represents a context of finer granularity than a sequence. <p> To demonstrate and to evaluate our research we are developing a video database framework called VideoSTAR (Video STorage And Retrieval) <ref> [11] </ref>. The research reported in this paper has been the basis for developing a video query algebra interface to VideoSTAR [12] which is now running on our prototype system. As a further activity we will evaluate the VideoSTAR framework by implementing real video databases on top of the framework.
Reference: [12] <author> R. Hjelsvold, R. Midtstraum, and O. Sandst-a. </author> <title> Searching and Browsing a Shared Video Database. To be presented at the First Internation Workshop on Multimedia Database Management Systems, to be held in Blue Mountain Lake, </title> <address> NY, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Handling contexts is an important feature of a shared video base since the interpretation of video data is depending strongly on its context. In a different paper <ref> [12] </ref> we have proposed explicit handling of contexts and defined three different contexts for a piece of video. The primary context is the context established in one specific video document as a result of the montage. <p> To demonstrate and to evaluate our research we are developing a video database framework called VideoSTAR (Video STorage And Retrieval) [11]. The research reported in this paper has been the basis for developing a video query algebra interface to VideoSTAR <ref> [12] </ref> which is now running on our prototype system. As a further activity we will evaluate the VideoSTAR framework by implementing real video databases on top of the framework. A part of this evaluation is to evaluate the video query algebra and the expressiveness of the underlying foundation.
Reference: [13] <author> T.Y.C. Leung and R.R. Muntz. </author> <title> Stream Processing: Temporal Query Processing and Optimization. In A.U. </title> <editor> Tansel et al., editors, </editor> <title> Temporal Databases: Theory, Design, and Implementation, chapter 14. </title> <publisher> The Ben-jamin/Cummings Publishing Company, Inc., </publisher> <year> 1993. </year>
Reference-contexts: 6 9y 2 Y (y:Interval Intersects z:Interval)^ 6 9v 2 Z (v Intersects z ^ v 6= z))g Filter Operations Allen has shown that there are 13 (disjunct) relationships that can exist between two temporal intervals [1] that can be evaluated by comparing start and end times for the intervals <ref> [13] </ref>. Other relationships, such as Intersects can also be defined. Because a video database contains several different time coordinate systems, we have to add the condition that two stream intervals have to be from the same stream before a time interval relationship can exist.
Reference: [14] <author> T.D.C. Little et al. </author> <title> A Digital On-Demand Video Service Supporting Content-Based Queries. </title> <booktitle> In Proceedings of ACM Multimedia 93, </booktitle> <address> Anaheim, USA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The traditional way to cope with this type of information sharing is to make a separate copy of the video for each document. When digital video is managed by a computer one can create virtual video documents <ref> [15, 14] </ref> and, thus, avoid making copies of video and meta-data to save storage space and preserve database consistency [8].
Reference: [15] <author> W.E. Mackay and G. Davenport. </author> <title> Virtual Video Editing In Interactive Multimedia Applications. </title> <journal> Communications of the ACM, </journal> <volume> 32(7), </volume> <year> 1989. </year>
Reference-contexts: The traditional way to cope with this type of information sharing is to make a separate copy of the video for each document. When digital video is managed by a computer one can create virtual video documents <ref> [15, 14] </ref> and, thus, avoid making copies of video and meta-data to save storage space and preserve database consistency [8].
Reference: [16] <author> J. Monaco. </author> <title> How to Read a Film. The Art, Technology, Language, History and Theory of Film and Media. </title> <publisher> Oxford University Press, </publisher> <year> 1981. </year>
Reference-contexts: We have proposed a video document structure that allows the user to organize the contents of a video document in a hierarchical structure (inspired by film theory <ref> [16] </ref>) of shots, scenes, sequences, and compound units [11]. One important aspect of structural components is that they may represent contexts for interpreting video data at various levels of granularity - e.g., a shot represents a context of finer granularity than a sequence.
Reference: [17] <author> E. Oomoto and K. Tanaka. OVID: </author> <title> Design and Implementation of a Video-Object Database System. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(4), </volume> <year> 1993. </year>
Reference-contexts: We then avoid unnesting of an arbitrary number of levels during replay and querying. In addition, this allows the distinctions that we have made between primary, basic and secondary contexts. The opposite choice has been taken in OVID <ref> [17] </ref> and by Duda et al. [6] to give the authors of video/multimedia documents greater flexibility in reusing composite components and in handling document dependencies.
Reference: [18] <author> L.A. Rowe, J.S. Boreczky, </author> <title> and C.A. Eads. Indexes for User Access to Large Video Databases. </title> <booktitle> In Proceedings of the IS&T/SPIE Symposium on Electronic Imaging Science and Technology, Conference on Storage and Retrieval for Image and Video Databases II, </booktitle> <address> San Jose, CA, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: context will be valid for a piece of video independent from any primary context into which it may appear - e.g., information about the location for recording and information about persons or objects shown in the frames. (This type of information has been called sensory indexes by Rowe et al. <ref> [18] </ref>.) When retrieving information regarding a specific video document it may also be useful to consider other documents into which the same piece of video has been used. <p> * Different types of annotations may have a different number of attributes but annotations of all types identify a stream interval where the annota tion is valid. * The stream interval may refer to a stored media segment for sensory content indexing or a video stream for topic content indexing <ref> [18] </ref>. 3.6 Structure In a video database there are two main types of meta-data related to a video stream, content indexes and video document structure. <p> A more thorough discussion of application domain models related to video databases can be found in [10] and in <ref> [18] </ref> The space limitations do not allow a discussion of every aspect of querying in our model.
Reference: [19] <author> T.G.A. Smith. </author> <title> If You Could See What I Mean... Descriptions of Video in an Anthropologist's Notebook. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <year> 1992. </year>
Reference-contexts: This way of indexing arbitrary pieces of video contents has been inspired by T.G.A. Smith <ref> [19] </ref>.
Reference: [20] <author> A.U. Tansel et al. </author> <title> Temporal Databases Theory, Design and Implementation. </title> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Conventional database systems were designed to maintain the most recent values and the change of the database values over time is not explicitly maintained. On the other hand, temporal databases are addressing the needs to maintain past, present, and future data <ref> [20] </ref>. Video data are inherently temporal although not in the traditional sense; the contents of a video screen are dynamically changing when video data are displayed to the user.
References-found: 20

