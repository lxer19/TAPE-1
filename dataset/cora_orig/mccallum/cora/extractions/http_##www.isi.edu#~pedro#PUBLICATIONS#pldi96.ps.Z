URL: http://www.isi.edu/~pedro/PUBLICATIONS/pldi96.ps.Z
Refering-URL: http://www.isi.edu/~pedro/PUBLICATIONS/pldi96.html
Root-URL: http://www.isi.edu
Email: (martin@cs.ucsb.edu)  (pedro@cs.ucsb.edu)  
Title: Commutativity Analysis: A New Analysis Framework for Parallelizing Compilers  
Author: Martin C. Rinard Pedro C. Diniz 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: This paper presents a new analysis technique, commutativity analysis, for automatically parallelizing computations that manipulate dynamic, pointer-based data structures. Commutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e. generate the same final result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code. We have implemented a prototype compilation system that uses commutativity analysis as its primary analysis framework. We have used this system to automatically parallelize two complete scientific computations: the Barnes-Hut N-body solver and the Water code. This paper presents performance results for the generated parallel code running on the Stanford DASH machine. These results provide encouraging evidence that commutativity analysis can serve as the basis for a successful parallelizing compiler. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: In the right context this approach works well researchers have successfully used data dependence analysis to parallelize computations that manipulate dense arrays using affine access functions <ref> [1, 27, 11, 15] </ref>. But data dependence analysis is, by itself, inadequate for computations that manipulate dynamic, pointer-based data structures.
Reference: [2] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic program parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: But data dependence analysis is, by itself, inadequate for computations that manipulate dynamic, pointer-based data structures. Its limitations include a need to perform complicated analysis to extract global properties of the data structure topology and an inherent inability to parallelize computations that manipulate graphs <ref> [2] </ref>. fl The first author is supported in part by an Alfred P. Sloan Research Fellowship. The second author is sponsored by the PRAXIS XXI program administrated by JNICT Junta Nacional de Investigac ao Cientifica e Tecnologica from Portugal, and holds a Fulbright travel grant.
Reference: [3] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446-449, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: We have implemented a run-time system that provides this functionality. It currently runs on the Stanford DASH multiprocessor [22] and on multiprocessors from Silicon Graphics. We have used the compilation system to automatically paral-lelize two complete scientific applications: the Barnes-Hut N-body solver <ref> [3] </ref> and the Water code [39]. The Barnes-Hut is representative of our target class of dynamic computations: it performs well because it uses a pointer-based data structure (a space subdivision tree) to organize the computation. <p> We have used this compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body solver <ref> [3] </ref> and the Water [34] code. 2 Explicitly parallel versions of the applications are available in the SPLASH [34] and SPLASH-2 [39] benchmark suites.
Reference: [4] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum, and J. Martin. </author> <title> The Perfect Club benchmarks: Effective performance evaluation of supercomputers. </title> <type> ICASE Report 827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: We believe that this approach accurately reflects how parallelizing compilers in general will be used in practice. We expect that programmers may have to tune their programs to the capabilities of the compiler to get good performance. The experience of other researchers supports this hypothesis <ref> [4, 5] </ref>. For our two applications it was relatively straightforward to produce code that the compiler could successfully analyze. Almost all of the translation effort was devoted to expressing the computation in a clean object-based style with classes, objects and methods instead of structures and procedures.
Reference: [5] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: We believe that this approach accurately reflects how parallelizing compilers in general will be used in practice. We expect that programmers may have to tune their programs to the capabilities of the compiler to get good performance. The experience of other researchers supports this hypothesis <ref> [4, 5] </ref>. For our two applications it was relatively straightforward to produce code that the compiler could successfully analyze. Almost all of the translation effort was devoted to expressing the computation in a clean object-based style with classes, objects and methods instead of structures and procedures.
Reference: [6] <author> W. Blume and R. Eigenmann. </author> <title> Symbolic range propagation. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <pages> pages 357-363, </pages> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: We have also developed rules for conditional and array expressions [30]. In the worst case the expression manipulation algorithms may take exponential running time. Like other researchers applying similar expression manipulation techniques in other analysis contexts <ref> [6] </ref>, we have not observed this behavior in practice. Finally, it is undecidable in general to determine if two expressions always denote the same value [18].
Reference: [7] <author> F. Bodin, P. Beckman, D. Gannon, J. Gotwals, S. Narayana, S. Srinivas, and Beata Winnicka. Sage++: </author> <title> An object-oriented toolkit and class library for building Fortran and C++ structuring tools. </title> <booktitle> In Proceedings of the Object-Oriented Numerics Conference, </booktitle> <year> 1984. </year>
Reference-contexts: We use Sage++ <ref> [7] </ref> as a front end. The analysis phase consists of approximately 14,000 lines of C++ code, with approximately 1,800 devoted to interfacing with Sage++. The generated code contains calls to a run-time library that provides the basic concurrency management and synchronization functionality.
Reference: [8] <author> D. Callahan. </author> <title> Recognizing and parallelizing bounded recurrences. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: In the long run we believe parallelizing compilers will incorporate both commutativity analysis and data dependence analysis, using each when it is appropriate. 8.2 Reductions Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [14, 13, 24, 8] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [9] <author> M. Carlisle and A. Rogers. </author> <title> Software caching and computation migration in Olden. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Given the dynamic nature of our target application set, the compiler would have to rely on dynamic techniques such as replication and task migration to optimize the locality of the generated computation <ref> [9, 29] </ref>. 7.4 Pointer Analysis The algorithms in Section 4 perform a data usage analysis at the granularity of the type system. An obvious alternative is to use pointer analysis [12, 38] to identify the regions of memory that each operation may access.
Reference: [10] <author> D. Chase, M. Wegman, and F. Zadek. </author> <title> Analysis of pointers and structures. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: We also discuss reduction analysis research. 8.1 Data Dependence Analysis Research on automatically parallelizing serial computations that manipulate pointer-based data structures has focused on techniques that precisely represent the run-time topology of the heap <ref> [16, 21, 10, 25] </ref>. The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures [16, 21].
Reference: [11] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect benchmark programs. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: In the right context this approach works well researchers have successfully used data dependence analysis to parallelize computations that manipulate dense arrays using affine access functions <ref> [1, 27, 11, 15] </ref>. But data dependence analysis is, by itself, inadequate for computations that manipulate dynamic, pointer-based data structures.
Reference: [12] <author> M. Emami, R. Ghiya, and L. Hendren. </author> <title> Context-sensitive interpro-cedural points-to analysis in the presence of function pointers. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: An obvious alternative is to use pointer analysis <ref> [12, 38] </ref> to identify the regions of memory that each operation may access. A major advantage of this approach for non type-safe languages like C and C++ is that it would allow the compiler to analyze programs that may violate their type declarations.
Reference: [13] <author> A. Fisher and A. Ghuloum. </author> <title> Parallelizing complex scans and reductions. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: In the long run we believe parallelizing compilers will incorporate both commutativity analysis and data dependence analysis, using each when it is appropriate. 8.2 Reductions Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [14, 13, 24, 8] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [14] <author> A. Ghuloum and A. Fisher. </author> <title> Flattening and parallelizing irregular, recurrent loop nests. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: In the long run we believe parallelizing compilers will incorporate both commutativity analysis and data dependence analysis, using each when it is appropriate. 8.2 Reductions Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [14, 13, 24, 8] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [15] <author> M.W. Hall, S.P. Amarasinghe, B.R. Murphy, S. Liao, and M.S. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural paralleliz-ing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: In the right context this approach works well researchers have successfully used data dependence analysis to parallelize computations that manipulate dense arrays using affine access functions <ref> [1, 27, 11, 15] </ref>. But data dependence analysis is, by itself, inadequate for computations that manipulate dynamic, pointer-based data structures. <p> Number Processors of Bodies Serial 1 2 4 8 16 32 8192 65.0 63.4 31.9 15.8 8.8 5.3 3.6 Table 3: Execution Times for Barnes-Hut (seconds) Barnes-Hut (8192 bodies) Barnes-Hut (16384 bodies) We start our analysis of the performance with the parallelism coverage <ref> [15] </ref>, which measures the amount of time that the serial computation spends in parallelized sections. To obtain good parallel performance, the compiler must parallelize a substantial part of the computation. By Amdahl's law any remaining serial sections of the computation impose an absolute limit on the parallel performance. <p> The generated program computes the reduction in parallel. Researchers have recently generalized the basic reduction recognition algorithms to recognize reductions of arrays instead of scalars. The reported results indicate that this optimization is crucial for obtaining good performance for the measured set of applications <ref> [15] </ref>. There are interesting connections between reduction analysis and commutativity analysis. Many (but not all) of the computations that commutativity analysis is designed to handle can be viewed as performing multiple reductions concurrently across a large data structure.
Reference: [16] <author> L. Hendren, J. Hummel, and A. Nicolau. </author> <title> Abstractions for recursive pointer data structures: Improving the analysis and transformation of imperative programs. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: We also discuss reduction analysis research. 8.1 Data Dependence Analysis Research on automatically parallelizing serial computations that manipulate pointer-based data structures has focused on techniques that precisely represent the run-time topology of the heap <ref> [16, 21, 10, 25] </ref>. The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures [16, 21]. <p> The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures <ref> [16, 21] </ref>. It must therefore analyze the code that builds the data structures and propagate the results of this analysis through the program to the section that uses the data. A limitation of these techniques is an inherent inability to parallelize computations that manipulate graphs.
Reference: [17] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The key question is how well the generated code would perform on such a platform. Message-passing machines have traditionally suffered from much higher communication costs than shared-memory machines. Compilation research for message-passing machines has therefore emphasized the development of data and computation placement algorithms that minimize communication <ref> [17] </ref>.
Reference: [18] <author> O. Ibarra, P. Diniz, and M. Rinard. </author> <title> On the complexity of commutativity analysis. </title> <booktitle> In Proceedings of the 2nd Annual International Computing and Combinatorics Conference, </booktitle> <address> Hong Kong, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Like other researchers applying similar expression manipulation techniques in other analysis contexts [6], we have not observed this behavior in practice. Finally, it is undecidable in general to determine if two expressions always denote the same value <ref> [18] </ref>. We therefore focus on developing algorithms that work well for the cases that occur in practice. 5 Code Generation If the analysis marks a method as parallel, the compiler generates two versions of the method: a serial version and a parallel version.
Reference: [19] <author> R. Kemmerer and S. Eckmann. UNISEX: </author> <title> a UNIx-based Symbolic EXecutor for pascal. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 15(5) </volume> <pages> 439-458, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: A compiler switch that disables the exploitation of commutativity and associativity for operators such as + will allow the programmer to prevent the compiler from performing transformations that may change the order in which the parallel program combines the summands. 3.4 Symbolic Execution The compiler uses symbolic execution <ref> [19] </ref> to extract the expressions that denote the new values of instance variables and the mul-tiset of invoked operations. Symbolic execution simply executes the methods, computing with expressions instead of values.
Reference: [20] <author> Butler W. Lampson and David D. Redell. </author> <title> Experience with processes and monitors in Mesa. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 105-117, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: Even though traditional compilers have not exploited commuting operations, these operations play an important role in other areas of parallel computing. Explicitly parallel programs, for example, often use locks, monitors and critical regions to ensure that operations execute atomically <ref> [20, 35] </ref>. For the program to execute correctly, the programmer must ensure that all of the atomic operations commute.
Reference: [21] <author> J. Larus and P. Hilfinger. </author> <title> Detecting conflicts between structure accesses. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: We also discuss reduction analysis research. 8.1 Data Dependence Analysis Research on automatically parallelizing serial computations that manipulate pointer-based data structures has focused on techniques that precisely represent the run-time topology of the heap <ref> [16, 21, 10, 25] </ref>. The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures [16, 21]. <p> The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures <ref> [16, 21] </ref>. It must therefore analyze the code that builds the data structures and propagate the results of this analysis through the program to the section that uses the data. A limitation of these techniques is an inherent inability to parallelize computations that manipulate graphs.
Reference: [22] <author> D. Lenoski. </author> <title> The Design and Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: The dynamic nature of our target application set means that the compiler must rely on a run-time system to provide basic task management functionality such as synchronization and dynamic load balancing. We have implemented a run-time system that provides this functionality. It currently runs on the Stanford DASH multiprocessor <ref> [22] </ref> and on multiprocessors from Silicon Graphics. We have used the compilation system to automatically paral-lelize two complete scientific applications: the Barnes-Hut N-body solver [3] and the Water code [39]. <p> This section presents performance results for both the automatically parallelized and explicitly parallel versions on a 32 processor Stanford DASH machine <ref> [22] </ref> running a modified version of the IRIX 5.2 operating system.
Reference: [23] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy task creation: a technique for increasing the granularity of parallel programs. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 185-197, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The parallel visit operation executes the recursive calls concurrently using the spawn construct, which creates a new task for each operation. A straightforward application of lazy task creation techniques <ref> [23] </ref> can increase the granularity of the resulting parallel computation. The compiler also augments each graph node with a mutual exclusion lock mutex.
Reference: [24] <author> S. Pinter and R. Pinter. </author> <title> Program optimization and parallelization using idioms. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: In the long run we believe parallelizing compilers will incorporate both commutativity analysis and data dependence analysis, using each when it is appropriate. 8.2 Reductions Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [14, 13, 24, 8] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [25] <author> J. Plevyak, V. Karamcheti, and A. Chien. </author> <title> Analysis of dynamic structures for efficient parallel execution. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: We also discuss reduction analysis research. 8.1 Data Dependence Analysis Research on automatically parallelizing serial computations that manipulate pointer-based data structures has focused on techniques that precisely represent the run-time topology of the heap <ref> [16, 21, 10, 25] </ref>. The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures [16, 21].
Reference: [26] <author> C. Polychronopoulos and D. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: If a for loop contains nothing but invocations of parallel versions of methods, the compiler generates parallel loop code instead of code that serially spawns each invoked operation. The generated code can then apply standard parallel loop execution techniques; it currently uses guided self-scheduling <ref> [26] </ref>. 5.2 Suppressing Excess Concurrency In practice parallel execution inevitably generates overhead in the form of synchronization and task management overhead. If the compiler exploits too much concurrency, the resulting overhead may overwhelm the performance benefits of parallel execution.
Reference: [27] <author> W. Pugh and D. Wonnacott. </author> <title> Eliminating false data dependences using the Omega test. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In the right context this approach works well researchers have successfully used data dependence analysis to parallelize computations that manipulate dense arrays using affine access functions <ref> [1, 27, 11, 15] </ref>. But data dependence analysis is, by itself, inadequate for computations that manipulate dynamic, pointer-based data structures.
Reference: [28] <author> M. Rinard. </author> <title> The Design, Implementation and Evaluation of Jade, a Portable, Implicitly Parallel Programming Language. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: It is clearly feasible, however, to generate code for message-passing machines. The basic required functionality is a software layer that uses message-passing primitives to implement the abstraction of a single shared object store <ref> [28, 32] </ref>. The key question is how well the generated code would perform on such a platform. Message-passing machines have traditionally suffered from much higher communication costs than shared-memory machines. Compilation research for message-passing machines has therefore emphasized the development of data and computation placement algorithms that minimize communication [17].
Reference: [29] <author> M. Rinard. </author> <title> Communication optimizations for parallel computing using data access information. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Given the dynamic nature of our target application set, the compiler would have to rely on dynamic techniques such as replication and task migration to optimize the locality of the generated computation <ref> [9, 29] </ref>. 7.4 Pointer Analysis The algorithms in Section 4 perform a data usage analysis at the granularity of the type system. An obvious alternative is to use pointer analysis [12, 38] to identify the regions of memory that each operation may access.
Reference: [30] <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A technique for automatically parallelizing pointer-based computations. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <address> Honolulu, HI, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: It then sorts the operands according to an arbitrary order on expressions. This sort facilitates the eventual expression comparison by making it easier to identify isomorphic subexpressions. We have also developed rules for conditional and array expressions <ref> [30] </ref>. In the worst case the expression manipulation algorithms may take exponential running time. Like other researchers applying similar expression manipulation techniques in other analysis contexts [6], we have not observed this behavior in practice.
Reference: [31] <author> J. K. Salmon. </author> <title> Parallel Hierarchical N-body Methods. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Although it is considered to be an important, widely studied computation, all previously existing parallel versions were parallelized by hand using low-level, explicitly parallel programming systems <ref> [31, 33] </ref>. We are aware of no other compiler that is capable of automatically parallelizing this computation. The space subdivision tree organizes the data as follows.
Reference: [32] <author> D. Scales and M. S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: For the program to execute correctly, the programmer must ensure that all of the atomic operations commute. Four of the six parallel applications in the SPLASH benchmark suite [34] and three of the four parallel applications described in <ref> [32] </ref> rely on commuting operations to expose the concurrency and generate correct parallel execution. This experience suggests that compilers will be unable to parallelize a wide range of computations unless they go beyond data dependence analysis to recognize and exploit commuting operations. <p> It is clearly feasible, however, to generate code for message-passing machines. The basic required functionality is a software layer that uses message-passing primitives to implement the abstraction of a single shared object store <ref> [28, 32] </ref>. The key question is how well the generated code would perform on such a platform. Message-passing machines have traditionally suffered from much higher communication costs than shared-memory machines. Compilation research for message-passing machines has therefore emphasized the development of data and computation placement algorithms that minimize communication [17].
Reference: [33] <author> J. Singh. </author> <title> Parallel Hierarchical N-body Methods and their Implications for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Although it is considered to be an important, widely studied computation, all previously existing parallel versions were parallelized by hand using low-level, explicitly parallel programming systems <ref> [31, 33] </ref>. We are aware of no other compiler that is capable of automatically parallelizing this computation. The space subdivision tree organizes the data as follows. <p> As part of the translation we eliminated several computations that dealt with parallel execution. For example, the parallel version used costzones partitioning to schedule the force computation phase <ref> [33] </ref>; the serial version eliminated the costzones code and the associated data structures. We also split a loop in the force computation phase into three loops. <p> The largest contribution to the performance difference is that the explicitly parallel version builds the space subdivision tree in parallel, while the automatically parallelized version builds the tree serially. The explicitly parallel version also uses an application-specific scheduling algorithm called costzones partitioning in the force computation phase <ref> [33] </ref>. This algorithm provides better locality than the guided self-scheduling algorithm in the automatically parallelized version.
Reference: [34] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Explicitly parallel programs, for example, often use locks, monitors and critical regions to ensure that operations execute atomically [20, 35]. For the program to execute correctly, the programmer must ensure that all of the atomic operations commute. Four of the six parallel applications in the SPLASH benchmark suite <ref> [34] </ref> and three of the four parallel applications described in [32] rely on commuting operations to expose the concurrency and generate correct parallel execution. <p> We have used this compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body solver [3] and the Water <ref> [34] </ref> code. 2 Explicitly parallel versions of the applications are available in the SPLASH [34] and SPLASH-2 [39] benchmark suites. <p> We have used this compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body solver [3] and the Water <ref> [34] </ref> code. 2 Explicitly parallel versions of the applications are available in the SPLASH [34] and SPLASH-2 [39] benchmark suites. This section presents performance results for both the automatically parallelized and explicitly parallel versions on a 32 processor Stanford DASH machine [22] running a modified version of the IRIX 5.2 operating system.
Reference: [35] <author> G. Steele. </author> <title> Making asynchronous parallelism safe for the world. </title> <booktitle> In Proceedings of the Seventeenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 218-231, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: Even though traditional compilers have not exploited commuting operations, these operations play an important role in other areas of parallel computing. Explicitly parallel programs, for example, often use locks, monitors and critical regions to ensure that operations execute atomically <ref> [20, 35] </ref>. For the program to execute correctly, the programmer must ensure that all of the atomic operations commute.
Reference: [36] <author> C. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 144-155, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: A standard problem with traditional parallelizing compilers, for example, has been the difficulty of successfully amortizing the barrier synchronization overhead at each parallel loop <ref> [36] </ref>. Our prototype compiler introduces four sources of overhead when it generates parallel code: * Loop Overhead: The overhead generated by the execution of a parallel loop.
Reference: [37] <author> W. Weihl. </author> <title> Commutativity-based concurrency control for abstract data types. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1488-1505, </pages> <month> Decem-ber </month> <year> 1988. </year>
Reference-contexts: Although we designed commutativity analysis to parallelize serial programs, it may also benefit other areas of computer science. For example, commuting operations allow computations on the persistent data in object-oriented databases to execute in parallel. Transaction processing systems can exploit commuting operations to use more efficient locking algorithms <ref> [37] </ref>. Commuting operations make protocols from distributed systems easier to implement efficiently; the corresponding reduction in the size of the associated state space may make it easier to verify the correctness of the protocol. In all of these cases the system relies on commuting operations for its correct operation.
Reference: [38] <author> R. Wilson and M. Lam. </author> <title> Efficient context-sensitive pointer analysis for C programs. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Program Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: An obvious alternative is to use pointer analysis <ref> [12, 38] </ref> to identify the regions of memory that each operation may access. A major advantage of this approach for non type-safe languages like C and C++ is that it would allow the compiler to analyze programs that may violate their type declarations.
Reference: [39] <author> S. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: We have implemented a run-time system that provides this functionality. It currently runs on the Stanford DASH multiprocessor [22] and on multiprocessors from Silicon Graphics. We have used the compilation system to automatically paral-lelize two complete scientific applications: the Barnes-Hut N-body solver [3] and the Water code <ref> [39] </ref>. The Barnes-Hut is representative of our target class of dynamic computations: it performs well because it uses a pointer-based data structure (a space subdivision tree) to organize the computation. The Water code is a more traditional scientific computation that organizes its data as arrays of objects representing water molecules. <p> We have used this compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body solver [3] and the Water [34] code. 2 Explicitly parallel versions of the applications are available in the SPLASH [34] and SPLASH-2 <ref> [39] </ref> benchmark suites. This section presents performance results for both the automatically parallelized and explicitly parallel versions on a 32 processor Stanford DASH machine [22] running a modified version of the IRIX 5.2 operating system.
References-found: 39

