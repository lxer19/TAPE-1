URL: http://www.cs.tamu.edu/faculty/bhuyan/papers/mcmc.ps
Refering-URL: http://www.cs.tamu.edu/faculty/bhuyan/
Root-URL: http://www.cs.tamu.edu
Email: E-mail: bhuyan,ninan@cs.tamu.edu  
Title: ANALYSIS OF INTERCONNECTION NETWORKS FOR CACHE COHERENT MULTIPROCESSORS WITH SCIENTIFIC APPLICATIONS  
Author: Laxmi N. Bhuyan and Nan Ni 
Address: College Station, TX 77845  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: Interconnection networks, such as, shared bus and multistage interconnection networks (MINs) are very suitable for the design of shared memory multiprocessors. The existing analytical models of these networks are based on unrealistic synthetic workload for simplicity of the analyses. Also, they consider the networks in isolation without incorporating other architectural details of a multiprocessor. In this paper we present queueing network models for these interconnection networks for multiprocessors with private cache memories. The multiprocessors operate with hardware cache coherence protocols and synchronization techniques. The models are based on single and multiple closed classes of customers. The input parameters for the queueing network are derived from an execution-driven simulation of some scientific applications, These parameters include average instruction execution times, memory read and write request rates etc., and are different for different applications. The queueing network model is solved using simple mean value analysis (MVA) algorithms. The output parameters, like response time, are compared with those obtained from the simulator to show that the proposed queueing models are accurate. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L.Barroso and M.Dubois, </author> <title> "Performance Evaluation of the slotted ring multiprocessor", </title> <journal> IEEE Transaction on Computers, </journal> <month> July </month> <year> 1995, </year> <pages> pp. 878-890 </pages>
Reference-contexts: A plethora of research has been reported on the queueing network modeling of these interconnection networks [6, 10, 12]. However such models for cache based multiprocessors are rather limited <ref> [1, 8, 11] </ref>. In [8], the effect of cache coherence problem was neglected, and a uniform memory distribution was assumed. <p> A detailed analysis of the cache coherence protocol was done in [11], but the paper assumes a synthetic workload where a memory request is randomly and uniformly distributed to all the memory modules. In <ref> [1] </ref>, an approximate analysis is reported for multiprocessors with slotted ring network where execution-driven simulation is used to measure the input parameters. In this paper, we develop detailed queueing network models for shared-bus and MIN using mean value analysis (MVA) algorithm [7].
Reference: [2] <author> L.N.Bhuyan, Q.Yang and D.P.Agrawal, </author> <title> "Performance of multiprocessor interconnection networks", </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1989, </year> <pages> pp. 25-37 </pages>
Reference-contexts: Interconnection networks, such as shared-bus and multistage interconnection network (MIN) are very suitable for the design of shared memory multiprocessors <ref> [2] </ref>. The simplicity and low cost of shared-bus system have made it attractive. However, shared-bus architecture is not suitable for scalable multiprocessors. When the interconnection network becomes more complex, the memory access time increases and severely degrades system performance. <p> An N fi N MIN connects N processors to N memory modules. For N a power of 2, it employs log 2 N stages of 2 fi 2 switches with N=2 switches per stage <ref> [2] </ref>. Each switch has two inputs and two outputs shown in Figure 7 for an 8 fi 8 MIN. The main advantage of these networks is their cost effectiveness. They allow a rich subset of one-to-one and simultaneous mappings of processors to memory modules. <p> Depending on the interconnection between two stages of switches, many MINs can be defined, but their bandwidth remains the same. Performance of these MINs for different switching techniques has been presented in various papers. For example, probabilistic analysis of a synchronous circuit-switch MIN was presented in <ref> [2] </ref>, queueing analysis of a synchronous packet-switch MIN was presented in [12] and the same for an asynchronous packet switched MIN was presented in [6].
Reference: [3] <editor> E.A.Brewer, C.N.Dellarocas et al., "PROTEUS: </editor> <title> a high performance parallel architecture simulator", </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Inst. of Technology, Sept.1991 </institution>
Reference-contexts: The input and output measurements are obtained from an execution-driven multiprocessor simulator (we fl This research has been supported in past by NSF grants MIP-9301959 and 9522740. call Extended Proteus or ExProteus) that is a modified version of Proteus <ref> [3] </ref>. We have incorpo-rated the detailed operations of the interconnection network into the simulator that was missing in the original Proteus. Our bus-based multiprocessors use the write once cache coherence protocol [5] whereas the MIN-based system uses directory cache protocol [4] for maintaining cache coherency among the shared blocks. <p> The bus-memory service time is fixed. The effect of such assumptions is examined in the next section by comparing the analytical results with those obtained from the execution driven simulation that does not have these assumptions. 2.1 APPLICATIONS AND RESULTS Our ExProteus simulator is based on Proteus <ref> [3] </ref>, developed at MIT. We have done extensive modification of the simulator to adapt to our situation. We selected four numerical applications as workload.
Reference: [4] <author> L.M.Censier and P.Feautrier, </author> " <title> A new solution to cache coherence problems in multicache systems", </title> <journal> IEEE Transactions on Computer, </journal> <volume> Vol.27, </volume> <month> Dec. </month> <year> 1978, </year> <pages> pp. 1112-1118 </pages>
Reference-contexts: We have incorpo-rated the detailed operations of the interconnection network into the simulator that was missing in the original Proteus. Our bus-based multiprocessors use the write once cache coherence protocol [5] whereas the MIN-based system uses directory cache protocol <ref> [4] </ref> for maintaining cache coherency among the shared blocks. A plethora of research has been reported on the queueing network modeling of these interconnection networks [6, 10, 12]. However such models for cache based multiprocessors are rather limited [1, 8, 11]. <p> Unlike bus, multistage interconnection networks don't have a convenient snooping mechanism. In this case, the cache-coherence problem with interconnection network is solved using full map directory based scheme <ref> [4] </ref>. In a directory-based scheme, a directory of cache block states is maintained at the memory controller. A processor sends a request to the memory controller when either there is a cache miss or it is a write request and the local cache block state is valid.
Reference: [5] <author> J.R.Goodman, </author> <title> "Using cache memory to reduce processor-memory traffic," </title> <booktitle> in Proc. 10th International Symposium on Computer Architecture, </booktitle> <year> 1983, </year> <pages> pp. 124-132 </pages>
Reference-contexts: We have incorpo-rated the detailed operations of the interconnection network into the simulator that was missing in the original Proteus. Our bus-based multiprocessors use the write once cache coherence protocol <ref> [5] </ref> whereas the MIN-based system uses directory cache protocol [4] for maintaining cache coherency among the shared blocks. A plethora of research has been reported on the queueing network modeling of these interconnection networks [6, 10, 12]. However such models for cache based multiprocessors are rather limited [1, 8, 11]. <p> A processor holds the bus until the completion of the request. While waiting for the service, the processor is idle, because we assume sequential consistency in this paper. The cache protocol we use in this paper is the write once protocol <ref> [5] </ref>, in which each shared block can be in one of five states at any time.
Reference: [6] <author> H.Jiang, L.N.Bhuyan and J.K.Muppala, "MVAMIN: </author> <title> Mean value analysis algorithms for multistage interconnection networks", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol.12, </volume> <year> 1991, </year> <pages> pp. 189-201 </pages>
Reference-contexts: When the interconnection network becomes more complex, the memory access time increases and severely degrades system performance. The MIN shows a good promise for use because the bandwidth remains the same across all the stages, making them very appropriate for large scale multiprocessors <ref> [6, 8] </ref>. In this paper, we present queueing network models for these shared memory multiprocessors with private caches to analyze the performance of each of these interconnection networks. <p> Our bus-based multiprocessors use the write once cache coherence protocol [5] whereas the MIN-based system uses directory cache protocol [4] for maintaining cache coherency among the shared blocks. A plethora of research has been reported on the queueing network modeling of these interconnection networks <ref> [6, 10, 12] </ref>. However such models for cache based multiprocessors are rather limited [1, 8, 11]. In [8], the effect of cache coherence problem was neglected, and a uniform memory distribution was assumed. <p> For example, probabilistic analysis of a synchronous circuit-switch MIN was presented in [2], queueing analysis of a synchronous packet-switch MIN was presented in [12] and the same for an asynchronous packet switched MIN was presented in <ref> [6] </ref>. Our analysis is similar to [6], where buffers are provided at the input of each output link of a switch. and packets are transmitted on an FCFS basis asynchronously one at a time. <p> For example, probabilistic analysis of a synchronous circuit-switch MIN was presented in [2], queueing analysis of a synchronous packet-switch MIN was presented in [12] and the same for an asynchronous packet switched MIN was presented in <ref> [6] </ref>. Our analysis is similar to [6], where buffers are provided at the input of each output link of a switch. and packets are transmitted on an FCFS basis asynchronously one at a time. We assume infinite queue length because it is known that the performance reaches peak only after a small buffer size [12]. <p> The queue Q (p; m; i), visited by a request from processor p to memory m at stage i, in a N fi N network can be obtained by the following formula <ref> [6] </ref>: Q (p; m; i) = (p=2) 2 i+1 mod N + m=( 2 i+1 ) for 0 i n 1 Q (p; m; i) = (m=2) 2 in+1 mod N + p=( 2 in+1 ) for n i 2n 1 From the above equation, it is seen that a memory <p> The same is true in the backward network that carries the memory responses back. Thus a MIN has to be modeled as a multiple class queueing network to distinguish the visits by different processors to different switches <ref> [6, 10] </ref>. We have N terminals for processors, 2N log 2 N queueing centers for switches and N queueing centers for N memory modules. There are N classes of customers with one customer in each class. Thinking time for a class i customer is Z i .
Reference: [7] <author> E.D.Lazowska, et al., </author> <title> "Quantitative system performance|computer system analysis using queueing network model," </title> <publisher> Prentice-Hall, </publisher> <year> 1984 </year>
Reference-contexts: In [1], an approximate analysis is reported for multiprocessors with slotted ring network where execution-driven simulation is used to measure the input parameters. In this paper, we develop detailed queueing network models for shared-bus and MIN using mean value analysis (MVA) algorithm <ref> [7] </ref>. The shared-bus and multiple-bus networks operate in a circuit switching mode while the MIN uses packet switching. We intentionally chose two different switching techniques to provide a better understanding of the analysis. <p> Customers in the queueing center are served on first-in first-out (FIFO) basis. The average service demand of the queueing center is BM latency, which is an input to the simulator. Before applying the MVA algorithm, let us briefly review main points of the algorithm <ref> [7] </ref>. The input parameters of terminal type closed single class MVA algorithm queueing network model are service demands, population of the system and thinking time. Suppose there are K service centers and N customers. <p> Exact MVA is no longer practical to solve such a model, therefore, approximate MVA is applied for the analysis <ref> [7] </ref>. Simulation results and analytical results are shown in Figure 9 and Figure 10. They match really well except for the FWA. 4 CONCLUSION In this paper, we evaluated the performance of cache based shared memory multiprocessors with shared-bus and multistage interconnection networks.
Reference: [8] <author> J.H.Patel, </author> " <title> Analysis of multiprocessors with private cache memories", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol.30, </volume> <month> Oct. </month> <year> 1981, </year> <pages> pp. 771-780 </pages>
Reference-contexts: When the interconnection network becomes more complex, the memory access time increases and severely degrades system performance. The MIN shows a good promise for use because the bandwidth remains the same across all the stages, making them very appropriate for large scale multiprocessors <ref> [6, 8] </ref>. In this paper, we present queueing network models for these shared memory multiprocessors with private caches to analyze the performance of each of these interconnection networks. <p> A plethora of research has been reported on the queueing network modeling of these interconnection networks [6, 10, 12]. However such models for cache based multiprocessors are rather limited <ref> [1, 8, 11] </ref>. In [8], the effect of cache coherence problem was neglected, and a uniform memory distribution was assumed. <p> A plethora of research has been reported on the queueing network modeling of these interconnection networks [6, 10, 12]. However such models for cache based multiprocessors are rather limited [1, 8, 11]. In <ref> [8] </ref>, the effect of cache coherence problem was neglected, and a uniform memory distribution was assumed. A detailed analysis of the cache coherence protocol was done in [11], but the paper assumes a synthetic workload where a memory request is randomly and uniformly distributed to all the memory modules.
Reference: [9] <author> J.P.Singh, W.D.Weber and A.Gupta, </author> <title> "SPLASH: Standford parallel applications for shared-memory,", </title> <journal> ACM SIGARCH Computer Architecture News, Vol.20, No.1, </journal> <volume> Mar.1992, </volume> <pages> pp. 319-327 </pages>
Reference-contexts: In addition, the characteristics of the applications heavily impact the system performance. In this paper, the parameters related to applications are derived from execution-driven simulation of some scientific applications, namely MATMUL, FFT, FWA and MP3D <ref> [9] </ref>. The parameters we use include average computation (thinking) time between two memory requests, and fractions of memory access (read/write), which give a more realistic picture of the whole system than simple assumption regarding the behavior of the applications. <p> We selected four numerical applications as workload. These applications are multiplication of two 2D matrix (MATMUL), Cooley-Tukey fast Fourier transform (FFT), Floyd-Warshall's all-pair-shortest-path algorithm (FWA) and simulation of rarified flows over objects in a wind tunnel (MP3D), from the SPLASH benchmark suite <ref> [9] </ref>. The matrix multiplication was done between two 256fi256 double precision matrices. The basic data structures are three shared two-dimensional arrays: two input arrays and one output array. The simulations of Cooley-Tukey FFT algorithm was done on an input of 2 14 points. <p> MP3D is a three-dimensional particle simulator used in rarefied fluid flow simulation. We take 3000 molecules with the default geometry provided with SPLASH <ref> [9] </ref>. The simulation was done for 50 time steps.
Reference: [10] <institution> D.Towsley,"Approximate models of multiple-bus multiprocessor systems", </institution> <note> IEEE Transactions on Computer, Vol.35, No.3, </note> <year> 1986, </year> <pages> pp. 220-228 </pages>
Reference-contexts: Our bus-based multiprocessors use the write once cache coherence protocol [5] whereas the MIN-based system uses directory cache protocol [4] for maintaining cache coherency among the shared blocks. A plethora of research has been reported on the queueing network modeling of these interconnection networks <ref> [6, 10, 12] </ref>. However such models for cache based multiprocessors are rather limited [1, 8, 11]. In [8], the effect of cache coherence problem was neglected, and a uniform memory distribution was assumed. <p> The same is true in the backward network that carries the memory responses back. Thus a MIN has to be modeled as a multiple class queueing network to distinguish the visits by different processors to different switches <ref> [6, 10] </ref>. We have N terminals for processors, 2N log 2 N queueing centers for switches and N queueing centers for N memory modules. There are N classes of customers with one customer in each class. Thinking time for a class i customer is Z i .
Reference: [11] <author> Q.Yang, L.N.Bhuyan and B.C.Liu, </author> <title> "Analysis and comparison of cache coherence protocols for a packet siwitched multiprocessor", </title> <journal> IEEE Transactions on Computer, </journal> <month> August, </month> <year> 1989, </year> <pages> pp. 1143-1153 </pages>
Reference-contexts: A plethora of research has been reported on the queueing network modeling of these interconnection networks [6, 10, 12]. However such models for cache based multiprocessors are rather limited <ref> [1, 8, 11] </ref>. In [8], the effect of cache coherence problem was neglected, and a uniform memory distribution was assumed. <p> However such models for cache based multiprocessors are rather limited [1, 8, 11]. In [8], the effect of cache coherence problem was neglected, and a uniform memory distribution was assumed. A detailed analysis of the cache coherence protocol was done in <ref> [11] </ref>, but the paper assumes a synthetic workload where a memory request is randomly and uniformly distributed to all the memory modules. In [1], an approximate analysis is reported for multiprocessors with slotted ring network where execution-driven simulation is used to measure the input parameters.
Reference: [12] <author> H.Yoon, K.Y.Lee and M.T.Liu, </author> <title> "Performance analysis of multibuffer packet-switching network in multiprocessor systems", </title> <journal> IEEE Transactions on Computer, </journal> <month> March, </month> <year> 1990 </year>
Reference-contexts: Our bus-based multiprocessors use the write once cache coherence protocol [5] whereas the MIN-based system uses directory cache protocol [4] for maintaining cache coherency among the shared blocks. A plethora of research has been reported on the queueing network modeling of these interconnection networks <ref> [6, 10, 12] </ref>. However such models for cache based multiprocessors are rather limited [1, 8, 11]. In [8], the effect of cache coherence problem was neglected, and a uniform memory distribution was assumed. <p> Performance of these MINs for different switching techniques has been presented in various papers. For example, probabilistic analysis of a synchronous circuit-switch MIN was presented in [2], queueing analysis of a synchronous packet-switch MIN was presented in <ref> [12] </ref> and the same for an asynchronous packet switched MIN was presented in [6]. Our analysis is similar to [6], where buffers are provided at the input of each output link of a switch. and packets are transmitted on an FCFS basis asynchronously one at a time. <p> We assume infinite queue length because it is known that the performance reaches peak only after a small buffer size <ref> [12] </ref>. Separate networks are assumed for sending requests from processors to memory modules (forward) and for sending replies from memory modules to processors (backward). A closed queueing network model for a 4fi4 MIN-based multiprocessor is shown in Figure 8. The processors with their caches are represented as delay centers.
References-found: 12

