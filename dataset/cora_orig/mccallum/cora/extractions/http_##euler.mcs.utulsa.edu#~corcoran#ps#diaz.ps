URL: http://euler.mcs.utulsa.edu/~corcoran/ps/diaz.ps
Refering-URL: http://euler.mcs.utulsa.edu/~corcoran/pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: corcoran@penguin.mcs.utulsa.edu  
Phone: (918) 631-2228  
Title: An Overview of Parallelism in Genetic Algorithms  
Author: Arthur L. Corcoran 
Date: August 18, 1993  
Address: 600 South College Avenue Tulsa, OK 74104-3189  
Affiliation: Department of Mathematical and Computer Sciences The University of Tulsa  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> F. Baiardi, D. Ciuffolini, A. M. Lomartire, D. Montanari, and G. Pesce. </author> <title> Nested hybrid genetic algorithms for system configuration and program mapping in massively parallel systems. </title> <note> In Forrest [16]. </note>
Reference-contexts: Each PE performed a local search in parallel. The local search repeatedly tried to improve its portion of the TSP path. They presented a lot of theory and their experimental results showed marginal improvement over other techniques in a much shorter time. Baiardi et al. <ref> [1] </ref> developed a GA-based programming tool to solve the program mapping problem, that is, mapping tasks to processors in an optimal manner. They used two nested GAs. The outer GA searches for an optimal configuration of the architecture represented by a system graph.
Reference: [2] <author> S. Baluja. </author> <title> Structure and performance of fine-grain parallelism in genetic search. </title> <note> In Forrest [16]. </note>
Reference-contexts: They also tested the cellular GA described above. They ranked the cellular GA between I-pCHC and ESGA. They concluded that the results for the cellular model were consistently good. Baluja <ref> [2] </ref> compared the performance of three cellular GAs and one island GA on a wide range of problems, including DeJong's [11] test suite. Two of the cellular GAs used a ring topology. In one the neighborhood was defined as the closest four neighbors on the left and right.
Reference: [3] <editor> R. K. Belew and L. B. Booker, editors. </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <address> San Mateo, California, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [4] <author> D. E. Brown, C. L. Huntley, and A. R. Spillane. </author> <title> A parallel genetic heuristic for the quadratic assignment problem. </title> <note> In Schaffer [44]. </note>
Reference-contexts: Each iteration of the inner and outer GAs was allocated to processors as they became available. They obtained optimal mappings with their model for several simple problems. 2.3 Global Models on the Intel Hypercube Brown et al. <ref> [4] </ref> developed a hybrid GA for solving the quadratic assignment problem. Their parallel heuristic, called SAGA, combines simulated annealing (SA) with a order-based GA. SAGA uses the GA for global search and applies SA to each chromosome for local optimization or fine tuning.
Reference: [5] <author> R.-J. Chen, R. R. Meyer, and J. Yackel. </author> <title> A genetic algorithm for diversity minimization and its parallel implementation. </title> <note> In Forrest [16]. </note>
Reference-contexts: They discovered that high reproduction and mutation rates performed the best while crossover rate made virtually no difference. While their algorithm performed quite well on this intractable problem, they had several ideas for future work which they thought would improve the algorithm even more. Chen et al. <ref> [5] </ref> used a CM-5 for a GA on the diversity minimization problem. This is a combinatorial problem that arises in the design of parallel databases. They presented some theoretical results related to the lower bound for these problems. Their GA used a two level heuristic for the fitness function.
Reference: [6] <author> J. P. Cohoon, S. U. Hegde, W. N. Martin, and D. S. Richards. </author> <title> Punctuated equilibria: A parallel genetic algorithm. </title> <note> In Grefenstette [23]. </note>
Reference-contexts: Each genetic population PE can work independently of the others and the control PEs are used to coordinate their efforts. They tested AGA on DeJong's [11] test suite and found near linear speedup. 3.4 Other Island Models Cohoon et al. <ref> [6] </ref> simulated an island model in their implementation for the optimal linear arrangement problem (OLA). Their model had independent GAs running in parallel. After a fixed number of generations, a set of chromosomes was migrated to the immediate neighbors of each processor.
Reference: [7] <author> J. P. Cohoon, W. N. Martin, and D. S. Richards. </author> <title> A multi-population genetic algorithm for solving the K-partition problem on hypercubes. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: Tanese [50] obtained better solution quality than a serial GA when he used Walsh polynomials for the objective function. These results were obtained by tuning the migration rate and migration interval. Cohoon et al. <ref> [7] </ref> used an Intel i860 Hypercube on the K-partition problem. Their island model executed independent GAs for a fixed number of generations, then copied randomly selected subsets of each subpopulation to its neighbors.
Reference: [8] <author> R. J. Collins and D. R. Jefferson. </author> <title> Selection in massively parallel genetic algorithms. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: Neighbors were selected randomly. Experimental results on a 12 node transputer organized in a torus topology found the GA to be superior to a branch and bound heuristic in both solution quality and time. 4.2 Cellular Models on the Connection Machine Collins and Jefferson <ref> [8] </ref> developed a cellular GA for the graph partitioning problem on a 16K processor CM-2 connection machine. Their study compared two panmictic selection algorithms and two cellular ones.
Reference: [9] <author> Y. Davidor. </author> <title> A naturally occuring niche and species phenomenon: The model and first results. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: The second parent was selected based on the fitness distribution in the neighborhood. Other than this selection technique, the cellular GA operated much as the island GA. It was not clear from her results whether the island or cellular GA was better. Davidor <ref> [9] </ref> generalized the neighborhood model used by others and called it the `ECO Genetic Algorithm'. He describes ECO as the latest attempt at controlling convergence in a GA, tracing the history of suggested mechanisms from preselection to crowding to sharing to cellular population models.
Reference: [10] <author> Y. Davidor, T. Yamada, and R. Nakano. </author> <title> The ECOlogical framework II: Improving GA performance at virtually zero cost. </title> <note> In Forrest [16]. </note>
Reference-contexts: He provides an argument for the natural emergence of niche and species in the ECO model. Using a custom function, he tested the ECO model in a simulation. He provided interesting figures to support his argument. Davidor et al. <ref> [10] </ref> tested ECO on a job shop scheduling problem (JSS) in a simulation. He found that ECO provided better solutions to a serial GA with virtually no overhead. Gordon et al. [20] explored the implementation of various GAs in parallel using the language Sisal.
Reference: [11] <author> K. A. De Jong. </author> <title> An Analysis of the Behavior of a Class of Genetic Adaptive Systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1975. </year> <month> 17 </month>
Reference-contexts: This will be discussed more in later sections. Gordon and Whitley [19] implemented a wide range of GAs in parallel and compared the results on a wide range of test problems, including DeJong's <ref> [11] </ref> test suite. Of the four global model parallel GAs tested, parallel Genitor was the best. Parallel CHC was not far behind. Elitist SGA was next followed closely by SGA. Island and Cellular GAs were also tested and those results will be discussed in later sections. <p> Each generation, a NGA would send its best individual to its neighboring NGAs and receive the neighbor's best individuals in return. The new members would replace the worst individuals in the population. They used four of DeJong's <ref> [11] </ref> testbed functions in their experiments. They were able to confirm existing theory by Goldberg [17] about the optimal population size for the four functions. They noted a trend for improved solutions as population size increased, yet with a small cost in communications overhead. <p> That is, in the same number of total function evaluations, the island model parallel GA consistently outperformed the serial GA in terms of solution quality. Muhlenbein and Born [38] used a collection of continuous functions in their experiments, including DeJong's <ref> [11] </ref> test suite. They ran their model on a 64 node Megaframe Hyperclus-ter. Their island model had an independent GA running on each processor, with a ladder like connection between neighbors. Each crossover and mutation was performed at the neighborhood level, and each GA performed local hill climbing. <p> However, they only tested nonoverlapping neighborhoods with a mechanism for migration, so technically this is an island model. Their implementation placed an independent neighborhood on each of the nodes of their 16 node transputer. They tested their HSDGA on ring, torus, and lattice migration structures using DeJong's <ref> [11] </ref> test suite. They found that the ring topology was by far the best. Next were the torus and lattice, respectively. They did not compare HSDGA with a serial GA (other than a one processor HSDGA), but found the speedup to be nearly linear. <p> Genetic population PEs contain the population, evaluation PEs evaluate the fitness of individual chromosomes, and control PEs provide the brains for the AGA. Each genetic population PE can work independently of the others and the control PEs are used to coordinate their efforts. They tested AGA on DeJong's <ref> [11] </ref> test suite and found near linear speedup. 3.4 Other Island Models Cohoon et al. [6] simulated an island model in their implementation for the optimal linear arrangement problem (OLA). Their model had independent GAs running in parallel. <p> They qualified these results by noting they are only valid for an idealized parallel machine. Gordon and Whitley [19] implemented a wide range of GAs in parallel and compared the results on a wide range of test problems, including DeJong's <ref> [11] </ref> test suite. Recall from the results from the previous section that of the four global model parallel GAs tested, they were ranked from best to worst as follows: parallel Genitor, Parallel CHC (pCHC), Elitist SGA (ESGA), and SGA. <p> It was even able to find a better solution for the QAP than the best known published solution. Manderick and Spiessens [31] implemented a cellular GA on a Symbolics workstation using DeJong's <ref> [11] </ref> test suite and a Walsh function. Their GA was called FG. Results were mixed, with FG outperforming a serial GA at times, and at times the serial GA the winner. Spiessens and Manderick [47] tested FG on the DAP, a massively parallel computer with a mesh topology. <p> It was much better than the island model. Gordon and Whitley [19] implemented a wide range of GAs in parallel and compared the results on a wide range of test problems, including DeJong's <ref> [11] </ref> test suite. Recall from the results from the previous sections that they ranked the performance of these from best to worst as follows: I-Genitor, Genitor, I-ESGA, pCHC, I-pCHC, ESGA, SGA, and I-SGA. They also tested the cellular GA described above. They ranked the cellular GA between I-pCHC and ESGA. <p> They ranked the cellular GA between I-pCHC and ESGA. They concluded that the results for the cellular model were consistently good. Baluja [2] compared the performance of three cellular GAs and one island GA on a wide range of problems, including DeJong's <ref> [11] </ref> test suite. Two of the cellular GAs used a ring topology. In one the neighborhood was defined as the closest four neighbors on the left and right.
Reference: [12] <author> P. S. de Souza and S. N. Talukdar. </author> <title> Genetic algorithms in asynchronous teams. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: In essence, he claims that several independent GAs run in parallel will on average find the optimal solution quicker than a single GA run only once. 2.5 Other Global Models De Souza and Talukdar <ref> [12] </ref> introduced the idea of an A-team, or Asynchronous team, as a simple and flexible way to combine different algorithms so they can interact synergistically. Their model used an unsupervised group of encapsulated algorithms with a consensus building capability.
Reference: [13] <author> M. Dorigo and E. Sirtori. </author> <title> ALECSYS: A parallel laboratory for learning classifier systems. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: In addition to successfully optimizing both criteria on a real worl problem, they were able to reduce the time required to find the solution. Dorigo and Sirtori <ref> [13] </ref> produced a parallel classifier system in response to Robertson's work [42] on the connection machine. Their system, ALECSYS, used two levels of parallelism. A `low level' parallelism was used within a single classifier and a `high level' parallelism was used which allowed the classifiers to work together.
Reference: [14] <author> M. Elketroussi and D. Fan. </author> <title> GADELO: A multi-population genetic algorithm based on dynamic exploration of local optima. </title> <note> In Forrest [16]. </note>
Reference-contexts: Of the four global model parallel GAs tested, parallel Genitor was the best. Parallel CHC was not far behind. Elitist SGA was next followed closely by SGA. Island and Cellular GAs were also tested and those results will be discussed in later sections. Elketroussi and Fan <ref> [14] </ref> implemented a multi-population GA called GADELO for optimizing multimodal functions. Several independent populations are evolved in parallel. Periodically, the population with the highest variability is split into two subpopulations. The variability is defined as the distance between the two best chromosomes within a population. <p> Both forked GAs can be run concurrently. This is like an island model with no migration. Experimental results on an FM Sound parameter identification problem and a travelling salesman problem found the fGA to outperform the regular GA in terms of solution quality. Elketroussi and Fan <ref> [14] </ref> implemented a multi-population GA called GADELO for optimizing multimodal functions. Several independent populations are evolved in parallel. Their work is summarized under the global model. However, note that their model can also be considered an island model without migration.
Reference: [15] <author> J. M. Fitzpatrick and J. J. Grefenstette. </author> <title> Genetic algorithms in noisy environments. </title> <journal> Machine Learning, </journal> <volume> 3(2) </volume> <pages> 101-120, </pages> <year> 1988. </year>
Reference-contexts: Goldberg [18] provides a more gentle treatment. An alternative derivation was provided by Fitzpatrick and Grefenstette <ref> [15] </ref>. Further development of the theory was presented by Grefenstette and Baker [24]. To understand implicit parallelism, one must first understand the idea of a schema. Holland [25] first introduced the schema as a type of similarity template with which different genetic strings could be compared.
Reference: [16] <editor> S. Forrest, editor. </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <address> San Mateo, California, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [17] <author> D. E. Goldberg. </author> <title> Optimal initial population size for binary-coded genetic algorithms. </title> <type> Technical Report TCGA-85001, </type> <institution> University of Alabama, </institution> <year> 1985. </year>
Reference-contexts: The new members would replace the worst individuals in the population. They used four of DeJong's [11] testbed functions in their experiments. They were able to confirm existing theory by Goldberg <ref> [17] </ref> about the optimal population size for the four functions. They noted a trend for improved solutions as population size increased, yet with a small cost in communications overhead. They also noted a slightly higher probability of premature convergence over a serial GA.
Reference: [18] <author> D. E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: The following is a brief overview of the idea of implicit parallelism, the underlying theory needed to understand it, and its implications for GAs. 12 The idea was first introduced by Holland [25], who gave a rigorous mathematical argument to support it. Goldberg <ref> [18] </ref> provides a more gentle treatment. An alternative derivation was provided by Fitzpatrick and Grefenstette [15]. Further development of the theory was presented by Grefenstette and Baker [24]. To understand implicit parallelism, one must first understand the idea of a schema. <p> That is, if the chromosome is encoded using the binary alphabet, f0; 1g, the schema is defined by the augmented alphabet, f0; 1; flg. Schemata allow similarities to be expressed in a concise and powerful way. The following examples provided by Goldberg <ref> [18] </ref> illustrates this for chromosomes of length 5: schema fl0000 matches two strings, f10000; 00000g schema fl111fl matches f01110; 01111; 11110; 11111g While the genetic algorithm does not directly manipulate schemata, its behavior is governed by the schema theorem. <p> That is, while n structures are processed each generation, n 3 schemata are being operated upon. Implicit parallelism is at work with no need for special bookkeeping or memory other than the population itself. Below is an overview of Goldberg's <ref> [18] </ref> rederivation of Holland's estimate. Consider a population of n binary strings of length l. Also, consider the set of schemata that survive crossover and mutation with a probability greater than p s , a constant.
Reference: [19] <author> V. S. Gordon and D. Whitley. </author> <title> Serial and parallel genetic algorithms as function optimizers. </title> <note> In Forrest [16]. </note>
Reference-contexts: One of the well known serial GAs implemented with Sisal was SGA. They compared the parallelism in their parallel SGA with that found in an island model and a cellular model. This will be discussed more in later sections. Gordon and Whitley <ref> [19] </ref> implemented a wide range of GAs in parallel and compared the results on a wide range of test problems, including DeJong's [11] test suite. Of the four global model parallel GAs tested, parallel Genitor was the best. Parallel CHC was not far behind. <p> They also found that the critical path was independent of the population size. They qualified these results by noting they are only valid for an idealized parallel machine. Gordon and Whitley <ref> [19] </ref> implemented a wide range of GAs in parallel and compared the results on a wide range of test problems, including DeJong's [11] test suite. <p> They found the cellular GA to have a higher parallelism than any other model, yet it had a longer critical path than the parallel SGA (global model) and thus ran slower. It was much better than the island model. Gordon and Whitley <ref> [19] </ref> implemented a wide range of GAs in parallel and compared the results on a wide range of test problems, including DeJong's [11] test suite.
Reference: [20] <author> V. S. Gordon, D. Whitley, and A. P. W. Bohm. </author> <title> Dataflow parallelism in genetic algorithms. </title> <note> In Manner and Manderick [32]. </note>
Reference-contexts: Periodically, genetic crossover is performed between any two solutions. Experimental results were presented but no comparisons were made with other techniques. Gordon et al. <ref> [20] </ref> explored the implementation of various GAs in parallel using the language Sisal. Sisal is a machine independent parallel algorithm design language which runs on a wide variety of architectures, including the Cray 2, Sequent, Alliant, nCUBE 2 and Manchester Dataflow Machine. <p> The migration model also had a better rate of improvement as more processors were added. For the TSP, it was found that higher migration rates resulted in better solution quality with less total effort. Gordon et al. <ref> [20] </ref> explored the implementation of various GAs in parallel using the language Sisal. They compared the parallelism in their parallel SGA with that found in an island model and a cellular model. The cellular model will be discussed more in the next section. <p> He provided interesting figures to support his argument. Davidor et al. [10] tested ECO on a job shop scheduling problem (JSS) in a simulation. He found that ECO provided better solutions to a serial GA with virtually no overhead. Gordon et al. <ref> [20] </ref> explored the implementation of various GAs in parallel using the language Sisal. They compared the parallelism in their parallel SGA with that found in an island model and a cellular model. The island model is discussed above.
Reference: [21] <author> M. Gorges-Schleuter. </author> <title> ASPARAGOS: An asynchronous parallel genetic optimization strategy. </title> <note> In Schaffer [44]. </note>
Reference-contexts: As the food production was based on consumption levels, the best performance was found when both farmers and nomads were present. 4.4 Other Cellular Models Gorges-Schleuter <ref> [21] </ref> developed an asynchronous parallel GA called ASPARAGOS. A description of ASPARAGOS is found in Muhlenbein's work above under other architectures. She tested the performance of ASPARAGOS on the travelling salesman problem (TSP).
Reference: [22] <author> M. Gorges-Schleuter. </author> <title> Comparison of local mating strategies in massively parallel genetic algorithms. </title> <note> In Manner and Manderick [32]. </note>
Reference-contexts: They ranked the performance of these from best to worst as follows: I-Genitor, I-ESGA, I-pCHC, and I-SGA. When combined with the global models the ranking was: I-Genitor, Genitor, I-ESGA, pCHC, I-pCHC, ESGA, SGA, and I-SGA. They concluded that the results for the island model were good. Gorges-Schleuter <ref> [22] </ref> explored both island models and cellular models. Her work on cellular models will be summarized below. Her island model used migration only between local neighborhoods. <p> A description of ASPARAGOS is found in Muhlenbein's work above under other architectures. She tested the performance of ASPARAGOS on the travelling salesman problem (TSP). Running ASPARAGOS on a simulator, she found it had better results than simulated annealing on the TSP, especially for large problems. Gorges-Schleuter <ref> [22] </ref> explored both island models and cellular models. Her work on island models is summarized above. Her cellular model used a unique selection technique. The center individual of a `circular' neighborhood 11 was selected as one parent. The second parent was selected based on the fitness distribution in the neighborhood.
Reference: [23] <editor> J. J. Grefenstette, editor. </editor> <booktitle> Genetic Algorithms and Their Applications: Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <address> Hillsdale, New Jersey, 1987. </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: [24] <author> J. J. Grefenstette. </author> <title> How genetic algorithms work: A critical look at implicit parallelism. </title> <note> In Schaffer [44]. </note>
Reference-contexts: Goldberg [18] provides a more gentle treatment. An alternative derivation was provided by Fitzpatrick and Grefenstette [15]. Further development of the theory was presented by Grefenstette and Baker <ref> [24] </ref>. To understand implicit parallelism, one must first understand the idea of a schema. Holland [25] first introduced the schema as a type of similarity template with which different genetic strings could be compared. <p> Consequently, short, highly fit schemata are allocated trials at an exponentially increasing rate while less fit schemata rapidly decline in number. This is what Holland termed implicit parallelism. 5.1 The Argument for Implicit Parallelism Grefenstette and Baker <ref> [24] </ref> showed implicit parallelism to be true using the following argument. Let the target sampling rate tsr (x; t) define the expected number of offspring to be generated from an individual x at time t. <p> Grefenstette and Baker <ref> [24] </ref> generalize theorem 1 as follows. Let u be a monotonic fitness function if the following condition holds: u (x i ) u (x j ) iff fff (x i ) fff (x j ) (4) where ff is defined as before.
Reference: [25] <author> J. H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> The University of Michigan Press, </publisher> <address> Ann Arbor, Michigan, </address> <year> 1975. </year>
Reference-contexts: The following is a brief overview of the idea of implicit parallelism, the underlying theory needed to understand it, and its implications for GAs. 12 The idea was first introduced by Holland <ref> [25] </ref>, who gave a rigorous mathematical argument to support it. Goldberg [18] provides a more gentle treatment. An alternative derivation was provided by Fitzpatrick and Grefenstette [15]. Further development of the theory was presented by Grefenstette and Baker [24]. <p> Goldberg [18] provides a more gentle treatment. An alternative derivation was provided by Fitzpatrick and Grefenstette [15]. Further development of the theory was presented by Grefenstette and Baker [24]. To understand implicit parallelism, one must first understand the idea of a schema. Holland <ref> [25] </ref> first introduced the schema as a type of similarity template with which different genetic strings could be compared. The schema includes the alphabet used to encode the chromosome and adds a wild card or `don't care' symbol, which is arbitrarily denoted with `fl'. <p> Note that this statement is independent of the particular representatives of the two schemata at time t. 5.2 How Many Schemata are Usefully Processed? Holland <ref> [25] </ref> has estimated that O (n 3 ) schemata are processed usefully in a genetic algorithm. That is, while n structures are processed each generation, n 3 schemata are being operated upon. Implicit parallelism is at work with no need for special bookkeeping or memory other than the population itself.
Reference: [26] <author> P. Husbands and F. Mill. </author> <title> Simulated co-evolution as the mechanism for emergent planning and scheduling. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: The GA was distributed two chromosomes per node among the 64 nodes of the CM. The GA had very good results for problems up to 10 9 variables. They found that their results changed drastically for different random seeds. 2.2 Global Models on the INMOS Transputer Husbands and Mill <ref> [26] </ref> used a transputer based parallel machine (500 transputers) to implement a co-evolving species model which simultaneously optimized manufacturing plans and schedules. In addition to successfully optimizing both criteria on a real worl problem, they were able to reduce the time required to find the solution.
Reference: [27] <author> P. Jog and D. Van Gucht. </author> <title> Parallelisation of probabalistic sequential search algorithms. </title> <note> In Grefenstette [23]. </note>
Reference-contexts: They gave no details comparing the number of generations for the optimal to be discovered. Later work performed on an Intel i860 Hypercube is summarized above. Jog and Van Gucht <ref> [27] </ref> studied an island model with a local improvement operator on the classical occupancy problem (COP) and the travelling salesman problem (TSP). For the COP they found that using migration yielded solutions faster than no migration.
Reference: [28] <author> H. Kitano, S. F. Smith, and T. Higuchi. </author> <title> GA-1: A parallel associative memory processor for rule learning with genetic algorithms. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: Using a 9 transputer system, they found 200 classifiers per processor to have the 2 best results. They gave time results for their parallel algorithm but never compared it with other methods. Kitano et al. <ref> [28] </ref> used transputers for a GA-based learning system. The hardware was the IXM2 parallel associative memory machine which enables high performance processing through the use of 64 T800 transputers and associative memories. The associative memory was used for bit-vector matching. <p> Experimental results showed their untuned PGA to have substantially better results than tuned serial GAs in both speed and intelligence. 3.2 Island Models on the INMOS Transputer Kitano et al. <ref> [28] </ref> used transputers for a GA-based learning system. Their work is summarized above under global models. In addition to allowing global models, their system allowed mating as found in the island models. This will be discussed in more detail under cellular models. <p> Selection 9 and mating were restricted to local neighborhoods. For a large problem instance, their experimental results showed the GA to be superior to the heuristics. Their GA was also able to find a previously unknown minimal solution. Kitano et al. <ref> [28] </ref> used transputers for a GA-based learning system. Their work is summarized above under global models. In addition to allowing global and island models, their system allowed mating as found in the cellular models. The population of their GA was spread over the processors in a hierarchy.
Reference: [29] <author> C. Kosak, J. Marks, and S. Shieber. </author> <title> A parallel genetic algorithm for network-diagram layout. </title> <editor> In Belew and Booker [3]. </editor> <volume> 18 </volume>
Reference-contexts: Messages between the classifiers were processed serially via broadcast to the entire population. He concluded that his GA used communication heavily, yet he showed that the algorithm was fully parallelizable and that the speedup obtained was independent of the population size used. Kosak et al. <ref> [29] </ref> used a 4096 node CM for a GA on a generalized network-diagram layout problem. Their algorithm randomly scattered genetic material between processors, so communication was used heavily. Nonetheless, they were able to process over 100 generations in less than a second.
Reference: [30] <author> S. W. Mahfoud and D. E. Goldberg. </author> <title> A genetic algorithm for parallel simulated annealing. </title> <note> In Manner and Manderick [32]. </note>
Reference-contexts: In another example they showed how a GA with a low accuracy algorithm could be paired with a high accuracy heuristic to reduce the work necessary to obtain a solution. Mahfoud and Goldberg <ref> [30] </ref> introduce a new model which combines GAs and parallel simulated annealing. This model, termed parallel recombinative simulated annealing (PRSA), has several copies of a simulated annealing algorithm running in parallel with mutation used as a neighborhood operator. Periodically, genetic crossover is performed between any two solutions.
Reference: [31] <author> B. Manderick and P. Spiessens. </author> <title> Fine-grained parallel genetic algorithms. </title> <note> In Schaffer [44]. </note>
Reference-contexts: Experimental results on a 64 processor Megaframe Hypercluster on the quadratic assignment problem and travelling salesman problem showed ASPARAGOS outperformed other techniques. It was even able to find a better solution for the QAP than the best known published solution. Manderick and Spiessens <ref> [31] </ref> implemented a cellular GA on a Symbolics workstation using DeJong's [11] test suite and a Walsh function. Their GA was called FG. Results were mixed, with FG outperforming a serial GA at times, and at times the serial GA the winner.
Reference: [32] <editor> R. Manner and B. Manderick, editors. </editor> <title> Parallel Problem Solving from Nature, 2. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1992. </year>
Reference: [33] <author> N. Mansour and G. C. Fox. </author> <title> A hybrid genetic algorithm for task allocation in multicomputers. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: Experimental results on a 32 node iPSC/2 compared SAGA to CRAFT, a heuristic algorithm. SAGA produced higher quality solutions than CRAFT in all cases. For moderately sized problems CRAFT ran considerably faster than SAGA, but for large problems SAGA was a clear winner. Mansour and Fox <ref> [33] </ref> developed a hybrid GA for solving the task allocation problem. They combined local hill climbing with the global search provided by the GA. They also tuned their parameters to prevent premature convergence.
Reference: [34] <author> T. Maruyama, T. Hirose, and A. Konagaya. </author> <title> A fine-grained parallel genetic algorithm for distributed parallel systems. </title> <note> In Forrest [16]. </note>
Reference-contexts: For less than 32 processors, block was the quickest, followed by indexed and then interleaved. They explained that scalability of the different strategies as well as the compatibility among solutions in each processor were responsible for these differences. 2.4 Global Models on Other Architectures Maruyama et al. <ref> [35, 34] </ref> developed an asynchronous parallel GA (APGA) which they tested on a 15 processor Sequent Symmetry. Synchronizations were eliminated by applying the genetic operators to an individual in a processor and to the youngest ancestors in other processors. <p> The APGA had nearly identical results as the SPGA, and was about twice as fast. They never compared the APGA to the heuristics, but their figures showed the APGA had better results in a shorter time. These results are restated in Maruyama et al. <ref> [34] </ref>. In addition to the graph partitioning problem, they had similar results using the well known f 1 and f 7 test functions. They also ran tests on a network of 6 Sun workstations. Shonkwiler [46] presented IIP, a universal method for parallelizing GAs. <p> Each processor would try to optimize its portion of the BM. Periodically, parts of the system graph would be migrated to other processors. He was able to find optimal solutions in experimental results for both ring and torus topologies. 3.3 Island Models on Other Architectures Maruyama et al. <ref> [35, 34] </ref> developed an asynchronous parallel GA (APGA) which they tested on a 15 processor Sequent Symmetry. Their work is summarized above under global models. Because of the lack of synchronization, this could also be considered an island model. Note, however, that reproduction is still panmictic. <p> FG was tested on an `ugly' deceptive problem. They found that local tournament selection was better than local proportionate selection and that small neighborhood sizes were best. They did not compare FG with any other technique and acknowledged their tests were limited. Maruyama et al. <ref> [35, 34] </ref> developed an asynchronous parallel GA (APGA) which they tested on a 15 processor Sequent Symmetry. Their work is summarized above under global and island models. While the algorithm is panmictic, it does resemble a cellular GA in the sense that a localized neighborhood is used for mating.
Reference: [35] <author> T. Maruyama, A. Konagaya, and K. Konishi. </author> <title> An asynchronous fine-grained parallel genetic algorithm. </title> <note> In Manner and Manderick [32]. </note>
Reference-contexts: For less than 32 processors, block was the quickest, followed by indexed and then interleaved. They explained that scalability of the different strategies as well as the compatibility among solutions in each processor were responsible for these differences. 2.4 Global Models on Other Architectures Maruyama et al. <ref> [35, 34] </ref> developed an asynchronous parallel GA (APGA) which they tested on a 15 processor Sequent Symmetry. Synchronizations were eliminated by applying the genetic operators to an individual in a processor and to the youngest ancestors in other processors. <p> Synchronizations were eliminated by applying the genetic operators to an individual in a processor and to the youngest ancestors in other processors. The values of the youngest ancestors are broadcast among the processors to maintain diversity. In Maruyama et al. <ref> [35] </ref>, this model was tested on a graph partitioning problem. The GA was compared with a synchronous parallel GA (SPGA) and two heuristics. The APGA had nearly identical results as the SPGA, and was about twice as fast. <p> Each processor would try to optimize its portion of the BM. Periodically, parts of the system graph would be migrated to other processors. He was able to find optimal solutions in experimental results for both ring and torus topologies. 3.3 Island Models on Other Architectures Maruyama et al. <ref> [35, 34] </ref> developed an asynchronous parallel GA (APGA) which they tested on a 15 processor Sequent Symmetry. Their work is summarized above under global models. Because of the lack of synchronization, this could also be considered an island model. Note, however, that reproduction is still panmictic. <p> FG was tested on an `ugly' deceptive problem. They found that local tournament selection was better than local proportionate selection and that small neighborhood sizes were best. They did not compare FG with any other technique and acknowledged their tests were limited. Maruyama et al. <ref> [35, 34] </ref> developed an asynchronous parallel GA (APGA) which they tested on a 15 processor Sequent Symmetry. Their work is summarized above under global and island models. While the algorithm is panmictic, it does resemble a cellular GA in the sense that a localized neighborhood is used for mating.
Reference: [36] <author> L. D. Merkle and G. B. Lamont. </author> <title> Comparison of parallel messy genetic algorithm data distribution strategies. </title> <note> In Forrest [16]. </note>
Reference-contexts: They showed that their efforts to reduce premature convergence resulted in much better solution quality. The GA without hill climbing was much slower than HGATA. Merkle and Lamont <ref> [36] </ref> looked at speeding up Messy GAs. Messy GAs use a partial enumeration of the search space to ensure a good sampling rate for a GA. The most expensive part of a Messy GA is called the Primordial Phase.
Reference: [37] <author> H. Muhlenbein. </author> <title> Parallel genetic algorithms, population genetics and combinatorial optimization. </title> <note> In Schaffer [44]. </note>
Reference-contexts: He also found that cooperative organisms were able to provide a measurable improvement in performance. 4.3 Cellular Models on Other Architectures Muhlenbein <ref> [37] </ref> developed an asynchronous parallel GA called ASPARAGOS. ASPARAGOS performs local selection in neighborhoods on a two dimensional grid. The neighborhood of 10 an individual is composed of its left neighbor, right neighbor, bottom neighbor, and bottom right neighbor. Recombination is performed through an interesting process called voting recombination.
Reference: [38] <author> H. Muhlenbein, M. Schomisch, and J. Born. </author> <title> The parallel genetic algorithm as function optimizer. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: That is, in the same number of total function evaluations, the island model parallel GA consistently outperformed the serial GA in terms of solution quality. Muhlenbein and Born <ref> [38] </ref> used a collection of continuous functions in their experiments, including DeJong's [11] test suite. They ran their model on a 64 node Megaframe Hyperclus-ter. Their island model had an independent GA running on each processor, with a ladder like connection between neighbors.
Reference: [39] <author> M. Munetomo, Y. Takai, and Y. Sato. </author> <title> An efficient migration scheme for subpopulation-based asynchronously parallel genetic algorithms. </title> <note> In Forrest [16]. </note>
Reference-contexts: Elketroussi and Fan [14] implemented a multi-population GA called GADELO for optimizing multimodal functions. Several independent populations are evolved in parallel. Their work is summarized under the global model. However, note that their model can also be considered an island model without migration. Munetomo et al. <ref> [39] </ref> studied different migration schemes in the island model. Their model varied the migration rate from an island in relation to the standard deviation of the fitnesses of the individuals in that island's population. Migration only occurred as the standard deviation dropped below a fixed value.
Reference: [40] <author> C. C. Pettey and M. R. Leuze. </author> <title> A theoretical investigation of a parallel genetic algorithm. </title> <note> In Schaffer [44]. </note>
Reference-contexts: They noted a trend for improved solutions as population size increased, yet with a small cost in communications overhead. They also noted a slightly higher probability of premature convergence over a serial GA. Pettey and Leuze <ref> [40] </ref> generalize their model so that communication can involve any NGA. They provide some theoretical analysis, and experimental results were similar to the previous work. Tanese's [49] parallel GA also exchanged the best individuals among neighboring processors.
Reference: [41] <author> C. C. Pettey, M. R. Leuze, and J. J. Grefenstette. </author> <title> A parallel genetic algorithm. </title> <note> In Grefenstette [23]. </note>
Reference-contexts: Each GA is usually started with a different random seed. Periodically, individuals are transmitted between the islands in a process called migration. The island model strategy eliminates the global synchronization that is required in the panmictic approach. 5 3.1 Island Models on Various Hypercubes Pettey et al. <ref> [41] </ref> used an Intel iPSC Hypercube to implement their version of the island model. Their parallel GA consisted of a group of identical `nodal' GAs (NGAs), distributed one per node.
Reference: [42] <author> G. G. Robertson. </author> <title> Parallel implementation of genetic algorithms in a classifier system. </title> <note> In Grefenstette [23]. </note>
Reference-contexts: In this section, a summary is provided of several recent pa-pers involving global parallel genetic algorithms. They are organized below based on the architecture used. 2.1 Global Models on the Connection Machine Robertson <ref> [42] </ref> used the connection machine (CM) to implement a parallel GA in a classifier system. His classifier system followed the Michigan Approach, where each chromosome represents a single classifier (or rule). The distribution of the classifiers was one per processor. <p> In addition to successfully optimizing both criteria on a real worl problem, they were able to reduce the time required to find the solution. Dorigo and Sirtori [13] produced a parallel classifier system in response to Robertson's work <ref> [42] </ref> on the connection machine. Their system, ALECSYS, used two levels of parallelism. A `low level' parallelism was used within a single classifier and a `high level' parallelism was used which allowed the classifiers to work together.
Reference: [43] <author> A. V. Sannier and E. D. Goodman. </author> <title> Genetic learning procedures in distributed environments. </title> <note> In Grefenstette [23]. </note>
Reference-contexts: While the algorithm is panmictic, it does resemble a cellular GA in the sense that a localized neighborhood is used for mating. The reason it is panmictic is that this localized neighborhood is drawn from the global population. Sannier and Goodman <ref> [43] </ref> developed a cellular GA on a Prime 550 in which each chromosome represents a living system. Each of these living systems is capable of detecting internal and external environmental conditions as well as reacting to these conditions.
Reference: [44] <editor> J. D. Schaffer, editor. </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <address> San Mateo, California, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [45] <author> F. Seredynski. </author> <title> Mapping by migrating boltzmann machine. </title> <note> In Manner and Manderick [32]. </note>
Reference-contexts: They found that the ring topology was by far the best. Next were the torus and lattice, respectively. They did not compare HSDGA with a serial GA (other than a one processor HSDGA), but found the speedup to be nearly linear. Seredynski <ref> [45] </ref> developed an island model for the task allocation problem. However, instead of using a parallel GA, he used a parallel Boltzmann Machine (BM). He wrote the model using Occam2 and implemented it on a 128 node transputer.
Reference: [46] <author> R. Shonkwiler. </author> <title> Parallel genetic algorithms. </title> <note> In Forrest [16]. </note>
Reference-contexts: These results are restated in Maruyama et al. [34]. In addition to the graph partitioning problem, they had similar results using the well known f 1 and f 7 test functions. They also ran tests on a network of 6 Sun workstations. Shonkwiler <ref> [46] </ref> presented IIP, a universal method for parallelizing GAs. IIP, or independent and identical processing, is simply the running of independent, identical GAs using a different random seed in each processor. He gives a detailed theoretical analysis of IIP and concludes that typically the IIP model will give superlinear speedup. <p> Their work is summarized above under global models. Because of the lack of synchronization, this could also be considered an island model. Note, however, that reproduction is still panmictic. Shonkwiler <ref> [46] </ref> presented IIP, a universal method for parallelizing GAs. IIP, or independent and identical processing, is simply the running of independent, identical GAs using a different random seed in each processor. His work is summarized above under global models. IIP could be considered an island model with no migration.
Reference: [47] <author> P. Spiessens and B. Manderick. </author> <title> A massively parallel genetic algorithm implementation and first analysis. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: Manderick and Spiessens [31] implemented a cellular GA on a Symbolics workstation using DeJong's [11] test suite and a Walsh function. Their GA was called FG. Results were mixed, with FG outperforming a serial GA at times, and at times the serial GA the winner. Spiessens and Manderick <ref> [47] </ref> tested FG on the DAP, a massively parallel computer with a mesh topology. FG was tested on an `ugly' deceptive problem. They found that local tournament selection was better than local proportionate selection and that small neighborhood sizes were best.
Reference: [48] <author> H. Tamaki and Y. Nishikawa. </author> <title> A paralleled genetic algorithm based on a neighborhood model and its application to the jobshop scheduling. </title> <note> In Manner and Manderick [32]. </note>
Reference-contexts: The cellular model is apparent when most of the selection and mating is centered in localized neighborhoods. When taken as a whole, their system provides the capability to develop large scale, real time, GA-based rule learning systems. Tamaki and Nishikawa <ref> [48] </ref> developed a parallel GA based on a cellular model and applied it to the job shop scheduling problem (JSS). They defined the neighborhood of an individual as the set of individuals within hamming distance one, that is, the immediately adjacent individuals. Neighbors were selected randomly.
Reference: [49] <author> R. Tanese. </author> <title> Parallel genetic algorithm for a hypercube. </title> <booktitle> In Grefenstette [23]. </booktitle> <pages> 19 </pages>
Reference-contexts: They also noted a slightly higher probability of premature convergence over a serial GA. Pettey and Leuze [40] generalize their model so that communication can involve any NGA. They provide some theoretical analysis, and experimental results were similar to the previous work. Tanese's <ref> [49] </ref> parallel GA also exchanged the best individuals among neighboring processors. His algorithm made the migration rate and the number of individuals transferred adjustable parameters. He also made the choice of neighbor vary over time.
Reference: [50] <author> R. Tanese. </author> <title> Distributed genetic algorithms. </title> <note> In Schaffer [44]. </note>
Reference-contexts: He also made the choice of neighbor vary over time. Experimental results on a 64 node nCUBE/six with a custom designed, multimodal objective function found near linear speedup with roughly the same solution quality as a serial GA. Tanese <ref> [50] </ref> obtained better solution quality than a serial GA when he used Walsh polynomials for the objective function. These results were obtained by tuning the migration rate and migration interval. Cohoon et al. [7] used an Intel i860 Hypercube on the K-partition problem.
Reference: [51] <author> K. Thearling. </author> <title> Putting artificial life to work. </title> <note> In Manner and Manderick [32]. </note>
Reference-contexts: The cellular ones also had a higher inbreeding coefficient. The cellular models were able to find the optimum solution for several problems much more quickly than the panmictic strategies. Thearling <ref> [51] </ref> developed genetically optimized artificial life forms which could swarm over corrupted bit map images and correct any errors found. Two forms of parallelism were used. The first form was the parallel evaluation of each organism's performance or fitness for each generation.
Reference: [52] <author> S. Tsutsui and Y. Fujimoto. </author> <title> Forking genetic algorithm with blocking and shrinking modes (fGA). </title> <note> In Forrest [16]. </note>
Reference-contexts: She found that low migration rates led 8 to more variability among the islands, while large migration rates led to behavior more like the global model. She also found that the torus topology had better variability than the ring topology. Tsutsui and Fujimoto <ref> [52] </ref> developed a new island model which they called the forking GA (fGA). Their GA had a forking mechanism which was developed to handle both cases for the best solution: local optimal or global optimal.
Reference: [53] <author> M. G. A. Verhoeven, E. H. L. Aarts, E. van de Sluis, and R. J. M. Vaessens. </author> <title> Parallel local search and the travelling salesman problem. </title> <note> In Manner and Manderick [32]. </note>
Reference-contexts: Thus, given a rule or partial rule, their system was able to find a match extremely fast, as their experimental results showed. Their work will be discussed in more detail under the island and cellular model sections. Verhoeven et al. <ref> [53] </ref> distributed a travelling salesman problem over networks of 50 and 400 T800 transputers. Each PE performed a local search in parallel. The local search repeatedly tried to improve its portion of the TSP path.
Reference: [54] <author> H.-M. Voigt, I. Santiba~nez-Koref, and J. Born. </author> <title> Hierarchically structured distributed genetic algorithms. </title> <note> In Manner and Manderick [32]. </note>
Reference-contexts: Their work is summarized above under global models. In addition to allowing global models, their system allowed mating as found in the island models. This will be discussed in more detail under cellular models. Voigt et al. <ref> [54] </ref> called their algorithm a Hierarchically Structured Distributed Genetic Algorithm (HSDGA). HSDGA limited mating to local neighborhoods which are hierarchically 6 connected to form an `evolution surface'. Each local neighborhood or environment is isolated from the others except that certain members may belong to more than one neighborhood.
Reference: [55] <author> G. von Laszewski. </author> <title> Intelligent structural operators for the k-way graph partitioning problem. </title> <note> In Belew and Booker [3]. </note>
Reference-contexts: In this case, new species are formed at the boundaries between emergent structures. The cellular model also avoids the problems associated with the panmictic approach, such as synchronization for example. 4.1 Cellular Models on the INMOS Transputer Von Laszewski <ref> [55] </ref> applied a cellular GA to the k-way graph partitioning problem. They compared their GA with a round robin and a divide and conquer heuristic. Their GA was implemented on a 64 node transputer with one chromosome per processor. Selection 9 and mating were restricted to local neighborhoods.
Reference: [56] <author> D. Whitley. </author> <title> Cellular genetic algorithms. </title> <note> In Forrest [16]. </note>
Reference-contexts: Cellular models (a term used by Whitley <ref> [56] </ref>) similarly arose from the desire to exploit the fine grained, massively parallel architectures. These machines consist of a huge number of simple processors typically connected in a ring or torus topology.
Reference: [57] <author> B. P. Zeigler and J. Kim. </author> <title> Asynchronous genetic algorithms on parallel computers. </title> <note> In Forrest [16]. </note>
Reference-contexts: IIP, or independent and identical processing, is simply the running of independent, identical GAs using a different random seed in each processor. His work is summarized above under global models. IIP could be considered an island model with no migration. Zeigler and Kim <ref> [57] </ref> developed an asynchronous genetic algorithm (AGA) on a Motorola MPC using an Object Oriented model. Under their model, each processor is categorized as one of the following types: genetic population PE, evaluation PE, or control PE.
References-found: 57

