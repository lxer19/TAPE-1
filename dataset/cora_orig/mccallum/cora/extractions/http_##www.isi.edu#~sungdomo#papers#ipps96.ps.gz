URL: http://www.isi.edu/~sungdomo/papers/ipps96.ps.gz
Refering-URL: http://www.isi.edu/~sungdomo/publications.html
Root-URL: http://www.isi.edu
Title: The Combined Effectiveness of Unimodular Transformations, Tiling, and Software Prefetching  
Author: Rafael H. Saavedra, Weihua Mao, Daeyeon Park, Jacqueline Chame, and Sungdo Moon 
Address: Los Angeles, California 90089-0781  
Affiliation: Computer Science Department University of Southern California  
Abstract: Unimodular transformations, tiling, and software prefetching are loop optimizations known to be effective in increasing parallelism, reducing cache miss rates, and eliminating processor stall time. Although these optimizations individually are quite effective, there is the expectation that even better improvements can be obtained by combining them together. In this paper we show that indeed this is the case when unimodular transformations are combined with either tiling or software prefetching. However, our results also show that although combining tiling with pre-fetching tends to improve the performance of tiling alone, it is also the case that in some situations tiling can degrade the cache performance of software prefetching. The reasons for this unexpected behavior are three fold: 1) tiling introduces interference misses inside the localized space which are difficult to characterize with current techniques based on locality analysis; 2) prefetch predicates are computed using only estimates on the amount of capacity misses, so the latency induced by cache interference is not completely covered; and 3) tiling limits the maximum amount of latency that can be masked with prefetching. 
Abstract-found: 1
Intro-found: 1
Reference: [AbuS78] <author> Abu-Sufah, W., </author> <title> Improving the Performance of Virtual Memory Computers, </title> <type> Ph.D. thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> Nov. </month> <year> 1978. </year>
Reference-contexts: In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Adve90] <author> Adve, S.V. and Hill, </author> <title> M.D., ``Weak Ordering ANew Definition'', </title> <booktitle> The 17th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Seattle, Washington, </address> <month> May 28-31 </month> <year> 1991, </year> <pages> pp. 2-14. </pages>
Reference-contexts: Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches [Tang76, Cens78, YenW85], relaxed memory models <ref> [Dubo88, Sche88, Adve90, Ghar90] </ref>, data prefetching [LeeR87, Port89, Mowr91], and multithreading [Smit78, Hals88, Agar90] are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Agar90] <author> Agarwal, A., Lim B.-H., Kranz, D., and Kubiatowicz, J., </author> <month> ``APRIL: </month> <title> A Processor Architecture for Multiprocessing'', </title> <booktitle> The 17th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Seattle, Washington, </address> <month> May 28-31 </month> <year> 1991, </year> <pages> pp. 104-114. </pages>
Reference-contexts: Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches [Tang76, Cens78, YenW85], relaxed memory models [Dubo88, Sche88, Adve90, Ghar90], data prefetching [LeeR87, Port89, Mowr91], and multithreading <ref> [Smit78, Hals88, Agar90] </ref> are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Alle83] <author> Allen, J.R., </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations, </title> <type> Ph.D. thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Alle87] <author> Allen, J.R., </author> <title> ``Automatic Translation of FORTRAN Programs to Vector Form'', </title> <journal> ACM Trans. on Prog. Langs. and Sys., Vol.9, </journal> <volume> No.4, </volume> <year> 1987, </year> <pages> pp. 491-542. </pages>
Reference-contexts: In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Ande90] <author> Anderson, E., Bai, Z., Bischof, C., Demmel, J., Dongarra, J.J., DuCroz, J., Greenbaum, A., Hammarling, S., McKenney, A., and Sorensen, D., </author> <title> ``LAPACK: A Portable Linear Algebra Library for High-Performance Computers'', </title> <booktitle> Proc. of Supercomputing'90, </booktitle> <publisher> IEEE Press, </publisher> <year> 1990, </year> <pages> pp. 1-10. </pages>
Reference-contexts: In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates <ref> [Ande90, Dong90] </ref>. Compiler based loop optimizations have been extensively studied by many people [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91]. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Bane88] <author> Banerjee, U., </author> <title> Dependence Analysis for Supercomputers, </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Bane91] <author> Banerjee, U., </author> <title> ``Unimodular Transformations of Double Loops'', </title> <editor> in A. Nicolau, D. Gelernter, T. Gross, and D. Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <publisher> MIT press, </publisher> <year> 1991, </year> <pages> pp. 192-219. </pages>
Reference-contexts: Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms. For example, it is known that without some amount of loop restructuring, such as unimodular transformations (loop interchange, loop reversal, and loop skewing) <ref> [Bane91, Wolf91] </ref>, and tiling [Wolf87, Wolf89, LamM91], scientific applications perform poorly in cache-based machines [Ferr91, Tema93]. <p> In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Call90] <author> Callahan, D., Carr, S., and Kennedy, K., </author> <title> ``Improving Register Allocation for Subscripted Variables'', </title> <booktitle> Proc. of the ACM SIGPLAN'90 Conf. on Prog. Lang. Design and Implementation, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Cens78] <author> Censier, M. and Feautier, P. </author> <title> ``A New Solution to Coherence Problems in Multicache Systems'', </title> <journal> IEEE Trans. on Comp., Vol.27, </journal> <volume> No.12, </volume> <month> December </month> <year> 1978, </year> <pages> pp. 1112-1118. </pages>
Reference-contexts: 1. Introduction Making an effective use of a multiprocessor's memory hierarchy is one of the most active research areas in parallel computing. Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches <ref> [Tang76, Cens78, YenW85] </ref>, relaxed memory models [Dubo88, Sche88, Adve90, Ghar90], data prefetching [LeeR87, Port89, Mowr91], and multithreading [Smit78, Hals88, Agar90] are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Dong90] <author> Dongarra, J.J., Duff, I.S., Sorensen, D.C., and van der Vorst, H.A., </author> <title> Solving Linear Systems on Vector and Shared-Memory Computers, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1990. </year>
Reference-contexts: In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates <ref> [Ande90, Dong90] </ref>. Compiler based loop optimizations have been extensively studied by many people [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91]. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Dubo88] <author> Dubois, M., Scheurich, C., and Briggs, </author> <title> F.A., ``Synchronization, Coherence, and Event Ordering in Multiprocessors'', </title> <journal> IEEE Computer, Vol.21 No.2, </journal> <month> February </month> <year> 1988, </year> <pages> pp. 9-21. </pages>
Reference-contexts: Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches [Tang76, Cens78, YenW85], relaxed memory models <ref> [Dubo88, Sche88, Adve90, Ghar90] </ref>, data prefetching [LeeR87, Port89, Mowr91], and multithreading [Smit78, Hals88, Agar90] are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Ferr91] <author> Ferrante, J., Sarkar, V., and Thrash, W., </author> <title> ``On Estimating and Enhancing Cache Effectiveness'', </title> <booktitle> Proc. of the Fourth Workshop on Lang. and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: For example, it is known that without some amount of loop restructuring, such as unimodular transformations (loop interchange, loop reversal, and loop skewing) [Bane91, Wolf91], and tiling [Wolf87, Wolf89, LamM91], scientific applications perform poorly in cache-based machines <ref> [Ferr91, Tema93] </ref>.
Reference: [Ghar90] <author> Gharachorloo, K., Lenoski, D., Laudon, J., Gibbons, P., Gupta, A., and Hennessy, J.L., </author> <title> ``Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors'', </title> <booktitle> The 17th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Seattle, </address> <month> May 28-31 </month> <year> 1991, </year> <pages> pp. 15-26. </pages>
Reference-contexts: Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches [Tang76, Cens78, YenW85], relaxed memory models <ref> [Dubo88, Sche88, Adve90, Ghar90] </ref>, data prefetching [LeeR87, Port89, Mowr91], and multithreading [Smit78, Hals88, Agar90] are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Hals88] <author> Halstead, R.H., Jr., and Fujita, T., ``MASA: </author> <title> A Mul-tithreaded Processor Architecture for Parallel Symbolic Computing'', </title> <booktitle> Proc. of the 15th Annual Int. Conf. on Comp. Arch., Hono-lulu, Hawaii, </booktitle> <month> June </month> <year> 1988, </year> <pages> pp. 443-451. </pages>
Reference-contexts: Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches [Tang76, Cens78, YenW85], relaxed memory models [Dubo88, Sche88, Adve90, Ghar90], data prefetching [LeeR87, Port89, Mowr91], and multithreading <ref> [Smit78, Hals88, Agar90] </ref> are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Irig88a] <author> Irigoin, F. and Troilet, R., </author> <title> ``Dependence Approximation and Global Parallel Code Generation for Nested Loops'', </title> <booktitle> Int. Workshop on Par. and Dist. </booktitle> <address> Algs., </address> <month> October </month> <year> 1988. </year>
Reference-contexts: In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Irig88b] <author> Irigoin, F. and Troilet, R., </author> <title> ``Supernode Partitioning'', Procs. </title> <booktitle> of the 15th Annual ACM Symp. on Principles of Prog. </booktitle> <address> Langs., </address> <month> January </month> <year> 1988, </year> <pages> pp. 319-329. </pages>
Reference-contexts: In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [LamM91] <author> Lam, M.S., Rothberg, E.E., and Wolf, </author> <title> M.E., ``The Cache Performance and Optimization of Blocked Algorithms'', </title> <booktitle> Proc. Fourth Int. Conf. on Arch. Support for Prog. Lang. and Oper. Sys., </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms. For example, it is known that without some amount of loop restructuring, such as unimodular transformations (loop interchange, loop reversal, and loop skewing) [Bane91, Wolf91], and tiling <ref> [Wolf87, Wolf89, LamM91] </ref>, scientific applications perform poorly in cache-based machines [Ferr91, Tema93]. <p> Interference with tiling, however, is a function of the ratio between the tile size and cache size. Moreover, the optimal fixed tile increases as the square root of the cache size (B = (C /2) 1/2 ; see <ref> [LamM91] </ref> for a detailed analysis on the cache behavior of tiling). <p> This creates self-interference, whose magnitude depends on the particular values of the original matrix size, tile size, and cache parameters <ref> [LamM91] </ref>. ignored, the cache behavior of prefetching alone is still better than that of prefetching and tiling. <p> We are currently studying how this could be done. The last alternative consists of copying tiles into contiguous regions of the address space. Lam et. al. have shown that by doing this a significant amount of self- and cross-interference misses can be eliminated <ref> [LamM91] </ref> 2 . The only drawback of this approach is the overhead involved in copying tiles. We have performed some simulations which show that when tiling and pre-fetching are used in combination with copy optimization the overhead of copying tiles is sufficiently large to offset the decrease in stall time. <p> With respect to the second approach, we have obtained some preliminary results using copy optimization as formulated in <ref> [LamM91] </ref> which indicate that although this optimization reduces the amount of stall time, nevertheless the overhead involved in copying the tiles tends to eliminate most of the improvements.
Reference: [LeeR87] <author> Lee, R.L, Yew, P,-C., and Lawrie, D.H., </author> <title> ``Data Pre-fetching in Shared Memory Multiprocessors'', </title> <booktitle> Proc. Int. Conf. Par. Proc., </booktitle> <month> August </month> <year> 1987, </year> <pages> pp. 28-31. </pages>
Reference-contexts: Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches [Tang76, Cens78, YenW85], relaxed memory models [Dubo88, Sche88, Adve90, Ghar90], data prefetching <ref> [LeeR87, Port89, Mowr91] </ref>, and multithreading [Smit78, Hals88, Agar90] are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Mowr91] <author> Mowry, T. and Gupta, A., </author> <title> ``Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors'', </title> <journal> Journal of Par. and Dist. Comp., Vol.12, No.2, </journal> <month> June </month> <year> 1991, </year> <pages> pp. 87-106. </pages>
Reference-contexts: Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches [Tang76, Cens78, YenW85], relaxed memory models [Dubo88, Sche88, Adve90, Ghar90], data prefetching <ref> [LeeR87, Port89, Mowr91] </ref>, and multithreading [Smit78, Hals88, Agar90] are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Mowr92] <author> Mowry, T.C., Lam, M.S., and Gupta, A., </author> <title> ``Design and Evaluation of a Compiler Algorithm for Prefetching'', </title> <booktitle> Fifth Int. Conf. on Arch. Support for Prog. Lang. and Oper. Sys., </booktitle> <address> Boston Massachusetts, </address> <month> October 12-15 </month> <year> 1992, </year> <pages> pp. 62-73. </pages>
Reference-contexts: Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality. Furthermore, the same framework has been adapted by Mowry in the form of software prefetching to take advantage of data prefetch-ing hardware <ref> [Mowr92] </ref>. Tiling eliminates the latency suffered by loop nests by directly improving cache behavior, and software prefetching hides latency by prefetching data ahead of time. Several people, including Wolf and Mowry, have proposed combining these two techniques to improve performance even more. <p> With respect to the first, Mowry has found that the overhead of indiscriminate prefetching is too high to justify the marginal improvements derived from it <ref> [Mowr92] </ref>. Furthermore, our simulations show that even when the prefetch overhead is hhhhhhhhhhhhhhhhhhhhh 1 The size of X's footprint loaded by the k and j loops is less than the cache size, but the region covered the same footprint (interval of address space) is larger than the cache size.
Reference: [Port89] <author> Porterfield, A., </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Algorithms, </title> <type> Ph.D. thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches [Tang76, Cens78, YenW85], relaxed memory models [Dubo88, Sche88, Adve90, Ghar90], data prefetching <ref> [LeeR87, Port89, Mowr91] </ref>, and multithreading [Smit78, Hals88, Agar90] are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms. <p> In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Saav94] <author> Saavedra, R.H., Mao, W., Park, D., and Chame, J., </author> <title> ``The Combined Effectiveness of Unimodular Transformations, Tiling, and Software Prefetching'', </title> <institution> Tech Rept USC-CS-94-596, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Additional results obtained using a detailed execution-driven simulator, which unfortunately for lack of space could not be included in this paper, can be found in <ref> [Saav94] </ref>. <p> This behavior is not present in B-TP because interference is mostly the result of tiling and not prefetching (see <ref> [Saav94] </ref> for a detailed explanation). Therefore, increasing the prefetch distance has a small effect on the cache miss ratio. set associativity. As expected increasing the associativity results in a lower miss rate. <p> For example, reference X, on both O-OT and O-TP, suffers from temporal self-interference which is carried by loops k and j (see <ref> [Saav94] </ref> for a detailed explanation). This interference destroys some of the temporal locality carried by loop j 1 .
Reference: [Sche88] <author> Scheurich, C. and Dubois, M., </author> <title> ``Concurrent Miss Resolution in Multiprocessor Caches'', </title> <booktitle> Proc. of the 1988 Int. Conf. on Par. Proc., </booktitle> <address> Vol.I, University Park, Pennsylvania, </address> <month> August </month> <year> 1988, </year> <pages> pp. 118-125. </pages>
Reference-contexts: Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches [Tang76, Cens78, YenW85], relaxed memory models <ref> [Dubo88, Sche88, Adve90, Ghar90] </ref>, data prefetching [LeeR87, Port89, Mowr91], and multithreading [Smit78, Hals88, Agar90] are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Smit78] <author> Smith, B.J., </author> <title> ``A Pipelined, Shared Resource MIMD Computer'', </title> <booktitle> 1978 Int. Conf. on Parallel Proc., </booktitle> <year> 1978, </year> <pages> pp. 6-8. </pages>
Reference-contexts: Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches [Tang76, Cens78, YenW85], relaxed memory models [Dubo88, Sche88, Adve90, Ghar90], data prefetching [LeeR87, Port89, Mowr91], and multithreading <ref> [Smit78, Hals88, Agar90] </ref> are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Tang76] <author> Tang, C.K., </author> <title> ``Cache Design in a Tightly Coupled Multiprocessor System'', </title> <booktitle> AFIPS Conf. Proc., </booktitle> <address> New York, New York, </address> <month> June </month> <year> 1976, </year> <pages> pp. 749-753. </pages>
Reference-contexts: 1. Introduction Making an effective use of a multiprocessor's memory hierarchy is one of the most active research areas in parallel computing. Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches <ref> [Tang76, Cens78, YenW85] </ref>, relaxed memory models [Dubo88, Sche88, Adve90, Ghar90], data prefetching [LeeR87, Port89, Mowr91], and multithreading [Smit78, Hals88, Agar90] are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
Reference: [Tema93] <author> Temam, O., Granston, E, and Jalby, W., </author> <title> ``To Copy or Not to Copy'', </title> <address> Supercomputing'93, Portland, Oregon, </address> <year> 1993, </year> <pages> pp. 410-419. </pages>
Reference-contexts: For example, it is known that without some amount of loop restructuring, such as unimodular transformations (loop interchange, loop reversal, and loop skewing) [Bane91, Wolf91], and tiling [Wolf87, Wolf89, LamM91], scientific applications perform poorly in cache-based machines <ref> [Ferr91, Tema93] </ref>.
Reference: [Wolf82] <author> Wolfe, M.J., </author> <title> Optimizing Supercompilers for Supercomputers, </title> <type> Ph.D. thesis, </type> <institution> University of Illinois, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [Wolf87] <author> Wolfe, M.J., </author> <title> ``Iteration Space Tiling for Memory Hierarchies'', </title> <booktitle> Proc. of the 3rd SIAM Conf. on Parallel Processing for Scientific Computing, </booktitle> <year> 1987, </year> <pages> pp. 357-361. </pages>
Reference-contexts: Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms. For example, it is known that without some amount of loop restructuring, such as unimodular transformations (loop interchange, loop reversal, and loop skewing) [Bane91, Wolf91], and tiling <ref> [Wolf87, Wolf89, LamM91] </ref>, scientific applications perform poorly in cache-based machines [Ferr91, Tema93].
Reference: [Wolf89] <author> Wolfe, M.J., </author> <title> ``More Iteration Space Tiling'', </title> <address> Supercomputing'89, </address> <year> 1989. </year>
Reference-contexts: Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms. For example, it is known that without some amount of loop restructuring, such as unimodular transformations (loop interchange, loop reversal, and loop skewing) [Bane91, Wolf91], and tiling <ref> [Wolf87, Wolf89, LamM91] </ref>, scientific applications perform poorly in cache-based machines [Ferr91, Tema93].
Reference: [Wolf91] <author> Wolf, M. and Lam, M., </author> <title> ``A Loop Transformation Theory and an Algorithm to Maximize Parallelism'', </title> <journal> IEEE Trans. on Par. and Dist. Sys., Vol.2, </journal> <volume> No.4, </volume> <month> October </month> <year> 1991, </year> <pages> pp. 452-471. </pages>
Reference-contexts: Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms. For example, it is known that without some amount of loop restructuring, such as unimodular transformations (loop interchange, loop reversal, and loop skewing) <ref> [Bane91, Wolf91] </ref>, and tiling [Wolf87, Wolf89, LamM91], scientific applications perform poorly in cache-based machines [Ferr91, Tema93]. <p> In an effort to avoid these problems, many linear algebra libraries have been reimplemented using unimodular transformations and tiling in order to reduce the pernicious effects of the high miss rates [Ande90, Dong90]. Compiler based loop optimizations have been extensively studied by many people <ref> [AbuS78, Alle83, Alle87, Bane88, Bane91, Call90, Irig88a, Irig88b, Port89, Wolf82, Wolf91] </ref>. Recently Wolf and Lam have proposed a comprehensive framework in which tiling and unimodular loop transformations can be combined to exploit parallelism and improve data locality.
Reference: [YenW85] <author> Yen, W.C., Yen, D.W., and Fu, K.-S., </author> <title> ``Data Coherence Problems in a Multicache System'', </title> <journal> IEEE Trans. on Computers, Vol.C-34, </journal> <volume> No.1, </volume> <month> January </month> <year> 1985, </year> <pages> pp. 56-65. </pages>
Reference-contexts: 1. Introduction Making an effective use of a multiprocessor's memory hierarchy is one of the most active research areas in parallel computing. Many hardware and software mechanisms have been proposed to eliminate and/or hide the large memory latencies required in interprocessor communication. Of the hardware mechanisms, coherent caches <ref> [Tang76, Cens78, YenW85] </ref>, relaxed memory models [Dubo88, Sche88, Adve90, Ghar90], data prefetching [LeeR87, Port89, Mowr91], and multithreading [Smit78, Hals88, Agar90] are considered the most promising. Equally important are program transformations which make it possible to exploit the benefits offered by the hardware mechanisms.
References-found: 32

