URL: http://www.cs.dartmouth.edu/~jasonliu/courses/sim188/biblio/drs94.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~jasonliu/courses/sim188/notes-15.html
Root-URL: http://www.cs.dartmouth.edu
Email: Email: mdd@cs.princeton.edu  
Title: FAST: A Functional Algorithm Simulation Testbed  
Author: Marios D. Dikaiakos Anne Rogers Kenneth Steiglitz 
Address: Princeton, NJ 08544-2087  
Affiliation: Department of Computer Science, Princeton University  
Abstract: In this paper we extend the practical range of simulations of parallel executions by what we call "functional algorithm simulation," that is, simulation without actually performing most of the numerical computations involved. We achieve this by introducing a new approach for generating and collecting communication and computation characteristics for a class of parallel scientific algorithms. We describe FAST (Fast Algorithm Simulation Testbed), a prototype system that we developed to implement and test our approach. FAST overcomes some of the difficulties imposed by the very high complexity of interesting scientific algorithms, collects profile information representative of the algorithms rather than the underlying mapping strategies and hardware-design choices, and allows a performance assessment of parallel machines with various sizes and different interconnection schemes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Josh Barnes and Piet Hut. </author> <title> A hierarchical O(N log N) force-calculation algorithm. </title> <journal> Nature, </journal> <volume> 324 </volume> <pages> 446-449, </pages> <month> De-cember </month> <year> 1986. </year>
Reference-contexts: Another underlying assumption is that during the parallel execution, the initialization phase takes an insignificant portion of the overall processing time. Both assumptions are valid for many important scientific applications <ref> [1, 7, 8] </ref>. 3 The Functional Algorithm Simula tion Testbed To test the Functional Algorithm Simulation method we developed a prototype system called Functional Algorithm Simulation Testbed (FAST). <p> Table 1 shows a typical subset of the Intermediate Representation as generated by FAST for SIMPLE, a CFD code,. It describes calculations and message exchanges taking place at the points of the two-dimensional grid employed by SIMPLE. Grid-point [0, 2] RECEIVE from [0, 0]: 8 bytes RECEIVE from <ref> [0, 1] </ref>: 96 bytes RECEIVE from [0, 3]: 96 bytes RECEIVE from [1, 2]: 96 bytes JcbR comp: 257.96 sec DAV comp: 445.04 sec ENPR comp: 188.84 sec HEAT1 comp: 427.68 sec SEND to [0, 3]: 48 bytes Table 1: Fraction of the Intermediate Representation for SIMPLE. <p> It describes calculations and message exchanges taking place at the points of the two-dimensional grid employed by SIMPLE. Grid-point [0, 2] RECEIVE from [0, 0]: 8 bytes RECEIVE from [0, 1]: 96 bytes RECEIVE from [0, 3]: 96 bytes RECEIVE from <ref> [1, 2] </ref>: 96 bytes JcbR comp: 257.96 sec DAV comp: 445.04 sec ENPR comp: 188.84 sec HEAT1 comp: 427.68 sec SEND to [0, 3]: 48 bytes Table 1: Fraction of the Intermediate Representation for SIMPLE.
Reference: [2] <author> D. Culler, R. Karp, D. Patterson, et al. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM Sigplan Simposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: However, simulation of massively parallel machines (with hundreds or thousands of processors) is practically impossible, unless performed in a distributed fashion. Theoretical models of parallel computation such as the various flavors of PRAM, BSP [11], and LogP <ref> [2] </ref> provide the algorithm designers with a convenient platform for formally expressing new algorithms and studying theoretically their performance. However, they rely on unrealistic assumptions. <p> Table 1 shows a typical subset of the Intermediate Representation as generated by FAST for SIMPLE, a CFD code,. It describes calculations and message exchanges taking place at the points of the two-dimensional grid employed by SIMPLE. Grid-point <ref> [0, 2] </ref> RECEIVE from [0, 0]: 8 bytes RECEIVE from [0, 1]: 96 bytes RECEIVE from [0, 3]: 96 bytes RECEIVE from [1, 2]: 96 bytes JcbR comp: 257.96 sec DAV comp: 445.04 sec ENPR comp: 188.84 sec HEAT1 comp: 427.68 sec SEND to [0, 3]: 48 bytes Table 1: Fraction <p> It describes calculations and message exchanges taking place at the points of the two-dimensional grid employed by SIMPLE. Grid-point [0, 2] RECEIVE from [0, 0]: 8 bytes RECEIVE from [0, 1]: 96 bytes RECEIVE from [0, 3]: 96 bytes RECEIVE from <ref> [1, 2] </ref>: 96 bytes JcbR comp: 257.96 sec DAV comp: 445.04 sec ENPR comp: 188.84 sec HEAT1 comp: 427.68 sec SEND to [0, 3]: 48 bytes Table 1: Fraction of the Intermediate Representation for SIMPLE.
Reference: [3] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural Requirements of Parallel Scientific Applications with Explicit Communication. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Knowledge of these characteristics is valuable for the design of parallel algorithms and the understanding of parallel architectures. A straightforward approach for collecting information of this kind is to profile programs running on parallel machines <ref> [3] </ref>. Unfortunately, monitoring of real implementations is hindered by time considerations and the high-cost/low-availability of massively parallel computers. It also lacks the desirable flexibility for performing comparative studies and scalability analyses easily [5]. Simulators of parallel workloads are used to overcome some of the shortcomings of profiling. <p> It describes calculations and message exchanges taking place at the points of the two-dimensional grid employed by SIMPLE. Grid-point [0, 2] RECEIVE from [0, 0]: 8 bytes RECEIVE from [0, 1]: 96 bytes RECEIVE from <ref> [0, 3] </ref>: 96 bytes RECEIVE from [1, 2]: 96 bytes JcbR comp: 257.96 sec DAV comp: 445.04 sec ENPR comp: 188.84 sec HEAT1 comp: 427.68 sec SEND to [0, 3]: 48 bytes Table 1: Fraction of the Intermediate Representation for SIMPLE. <p> Grid-point [0, 2] RECEIVE from [0, 0]: 8 bytes RECEIVE from [0, 1]: 96 bytes RECEIVE from <ref> [0, 3] </ref>: 96 bytes RECEIVE from [1, 2]: 96 bytes JcbR comp: 257.96 sec DAV comp: 445.04 sec ENPR comp: 188.84 sec HEAT1 comp: 427.68 sec SEND to [0, 3]: 48 bytes Table 1: Fraction of the Intermediate Representation for SIMPLE. In a future version of FAST, instead of having the user manually writing the code that generates the Intermediate Representation, we could replace FAST's front-end with the front-end of a parallel programming language like JADE.
Reference: [4] <author> M. Dikaiakos. </author> <title> Functional algorithm simulation. </title> <type> Technical Report PhD Thesis (under preparation), </type> <institution> Department of Computer Science, Princeton University, </institution> <year> 1993. </year>
Reference-contexts: This mapping problem is also NP-complete [10]. To map the task-clusters onto processors we have implemented and incorporated in our system a number of heuristics similar to those included in the clustering stage [10]. More information about the clustering and mapping stages are presented in <ref> [4] </ref>. The clustering and mapping stages can be bypassed if the user of FAST chooses to have the front-end par tition the problem-instance at hand into a number of blocks equal to the number of available processors. <p> The y-axis for the phase-diagram is drawn on the right part of the plot. These represent estimates derived by a FAST simulation. More detailed results about SIMPLE, and further case-studies performed with FAST can be found in <ref> [4, 5] </ref>. 5 Conclusions In this paper we introduced the Functional Algorithm Simulation Method, a new methodology for generating and collecting computation and communication characteristics for a class of important scientific applications.
Reference: [5] <author> M. Dikaiakos, A. Rogers, and K. Steiglitz. </author> <title> Functional algorithm simulation: Implementation and experiments. </title> <type> Technical Report TR-429-93, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: A straightforward approach for collecting information of this kind is to profile programs running on parallel machines [3]. Unfortunately, monitoring of real implementations is hindered by time considerations and the high-cost/low-availability of massively parallel computers. It also lacks the desirable flexibility for performing comparative studies and scalability analyses easily <ref> [5] </ref>. Simulators of parallel workloads are used to overcome some of the shortcomings of profiling. Simulation can be trace-driven or direct [6]. <p> The parallel execution graph is subsequently passed through clustering, a stage seeking to minimize the communication overhead of the parallel execution, without sacrificing parallelism [10]. Clustering has been proven to be NP-Complete. A number of different heuristics have been implemented in FAST to allow experimentation with different choices <ref> [5] </ref>. After clustering, FAST performs a mapping of the clustered parallel-execution graph onto a message-passing architecture. The number of processors is provided by the user of the system. This mapping problem is also NP-complete [10]. <p> The y-axis for the phase-diagram is drawn on the right part of the plot. These represent estimates derived by a FAST simulation. More detailed results about SIMPLE, and further case-studies performed with FAST can be found in <ref> [4, 5] </ref>. 5 Conclusions In this paper we introduced the Functional Algorithm Simulation Method, a new methodology for generating and collecting computation and communication characteristics for a class of important scientific applications.
Reference: [6] <author> S. Goldschmidt and J. Hennessy. </author> <title> The Accuracy of Trace-Driven Simulations of Multiprocessors. </title> <booktitle> In 1993 ACM Sigmetrics, </booktitle> <pages> pages 146-157, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: It also lacks the desirable flexibility for performing comparative studies and scalability analyses easily [5]. Simulators of parallel workloads are used to overcome some of the shortcomings of profiling. Simulation can be trace-driven or direct <ref> [6] </ref>. Drawbacks of the trace-driven approach are: first, the large space and time requirements for generating and storing the trace-set and, second, its inaccuracy which arises when the trace has been generated in a multiprocessor different than the one simulated [6]. <p> Simulation can be trace-driven or direct <ref> [6] </ref>. Drawbacks of the trace-driven approach are: first, the large space and time requirements for generating and storing the trace-set and, second, its inaccuracy which arises when the trace has been generated in a multiprocessor different than the one simulated [6]. Direct simulation does not use traces but incurs a significant slowdown with respect to uniprocessor execution. Therefore, exact simulation (trace driven or direct) represents a useful alternative to profiling when considering systems of small to medium size.
Reference: [7] <author> L. Greengard and V. Rokhlin. </author> <title> A fast algorithm for particle simulation. </title> <journal> Journal of Computational Physics, </journal> <volume> 73 </volume> <pages> 325-348, </pages> <year> 1987. </year>
Reference-contexts: Another underlying assumption is that during the parallel execution, the initialization phase takes an insignificant portion of the overall processing time. Both assumptions are valid for many important scientific applications <ref> [1, 7, 8] </ref>. 3 The Functional Algorithm Simula tion Testbed To test the Functional Algorithm Simulation method we developed a prototype system called Functional Algorithm Simulation Testbed (FAST).
Reference: [8] <author> J. Lee, C. Lin, and L. Snyder. </author> <title> Programming SIMPLE for Parallel Portability. </title> <booktitle> In Preliminary Proceedings of the 4th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1992. </year>
Reference-contexts: Another underlying assumption is that during the parallel execution, the initialization phase takes an insignificant portion of the overall processing time. Both assumptions are valid for many important scientific applications <ref> [1, 7, 8] </ref>. 3 The Functional Algorithm Simula tion Testbed To test the Functional Algorithm Simulation method we developed a prototype system called Functional Algorithm Simulation Testbed (FAST). <p> It is a simplified ver-sion of the routine which calculates temperature and heat variables in the SIMPLE application <ref> [8] </ref>: for j=0 to siz_Y do D [i,j]=s [i,j]+R [i,j]+R [i,j-1]*(1-A [i,j]) V [i,j]=(R [i,j-1]*V [i,j-1]+ endfor endfor We notice that in every grid-point (i,j), the computation above requires the values R [i,j-1] and V [i,j-1] from the neighboring point (i,j-1), as well as values of "local" variables, that is, s <p> the same number of processors and sparser topologies. 4 Validation with SIMPLE and results In order to assess the validity of the Functional Algorithm Simulation method and the accuracy of FAST we study the SIMPLE kernel, for which there exist a number of published performance measurements by different research groups <ref> [9, 8] </ref>. The SIMPLE computation simulates the hydrodynamics of a pressurized fluid inside a spherical cell by solving a number of equations on a two-dimensional grid for a number of successive time-steps. We compare measurements of parallel time and speedup with the respective figures reported by our system. <p> The SIMPLE computation simulates the hydrodynamics of a pressurized fluid inside a spherical cell by solving a number of equations on a two-dimensional grid for a number of successive time-steps. We compare measurements of parallel time and speedup with the respective figures reported by our system. Data reported in <ref> [9, 8] </ref> have been collected from implementations of SIMPLE on the iPSC/2 multiprocessor. Thus, FAST is provided with the hardware characteristics of Intel's iPSC/2. plots labeled "Measured running times" and "Projected running times" present data from [9].
Reference: [9] <author> A. Rogers. </author> <title> Compiling for Locality of Reference. </title> <type> Technical Report PhD Thesis 91-1195, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1991. </year>
Reference-contexts: the same number of processors and sparser topologies. 4 Validation with SIMPLE and results In order to assess the validity of the Functional Algorithm Simulation method and the accuracy of FAST we study the SIMPLE kernel, for which there exist a number of published performance measurements by different research groups <ref> [9, 8] </ref>. The SIMPLE computation simulates the hydrodynamics of a pressurized fluid inside a spherical cell by solving a number of equations on a two-dimensional grid for a number of successive time-steps. We compare measurements of parallel time and speedup with the respective figures reported by our system. <p> The SIMPLE computation simulates the hydrodynamics of a pressurized fluid inside a spherical cell by solving a number of equations on a two-dimensional grid for a number of successive time-steps. We compare measurements of parallel time and speedup with the respective figures reported by our system. Data reported in <ref> [9, 8] </ref> have been collected from implementations of SIMPLE on the iPSC/2 multiprocessor. Thus, FAST is provided with the hardware characteristics of Intel's iPSC/2. plots labeled "Measured running times" and "Projected running times" present data from [9]. <p> Data reported in [9, 8] have been collected from implementations of SIMPLE on the iPSC/2 multiprocessor. Thus, FAST is provided with the hardware characteristics of Intel's iPSC/2. plots labeled "Measured running times" and "Projected running times" present data from <ref> [9] </ref>. The "Measured running times" correspond to a SIMPLE-implementation with Pingali and Rogers' paralleliz-ing compiler. The "Projected running times" were extracted from a model of a hand-written implemen on a 64-processor hypercube (PT = 232988 s). tation of SIMPLE. <p> The "Projected running times" were extracted from a model of a hand-written implemen on a 64-processor hypercube (PT = 232988 s). tation of SIMPLE. This model takes into consideration the extra overhead representative of the functional language used in <ref> [9] </ref> (e.g. time spent for array allocations), as well as times spent for floating-point computations and message transmissions.
Reference: [10] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: In that way the same parallel code could be used for functional algorithm simulation and actual execution. In the second phase of the front-end a simple parser transforms the Intermediate Representation into a weighted task-flow graph which follows the Macro-Dataflow model of computation <ref> [10] </ref>. Task-nodes in the graph contain a number of Intermediate Representation primitives. Their "boundaries" are defined by Send and Receive primitives occurring in the IR. The tasks start executing upon receipt of all incoming data and continue to completion without interruption. Upon completion their results are forwarded to adjacent nodes. <p> The parallel execution graph is subsequently passed through clustering, a stage seeking to minimize the communication overhead of the parallel execution, without sacrificing parallelism <ref> [10] </ref>. Clustering has been proven to be NP-Complete. A number of different heuristics have been implemented in FAST to allow experimentation with different choices [5]. After clustering, FAST performs a mapping of the clustered parallel-execution graph onto a message-passing architecture. <p> After clustering, FAST performs a mapping of the clustered parallel-execution graph onto a message-passing architecture. The number of processors is provided by the user of the system. This mapping problem is also NP-complete <ref> [10] </ref>. To map the task-clusters onto processors we have implemented and incorporated in our system a number of heuristics similar to those included in the clustering stage [10]. More information about the clustering and mapping stages are presented in [4]. <p> The number of processors is provided by the user of the system. This mapping problem is also NP-complete <ref> [10] </ref>. To map the task-clusters onto processors we have implemented and incorporated in our system a number of heuristics similar to those included in the clustering stage [10]. More information about the clustering and mapping stages are presented in [4]. The clustering and mapping stages can be bypassed if the user of FAST chooses to have the front-end par tition the problem-instance at hand into a number of blocks equal to the number of available processors.
Reference: [11] <author> L. G. Valiant. </author> <title> A Bridging Model for parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: However, simulation of massively parallel machines (with hundreds or thousands of processors) is practically impossible, unless performed in a distributed fashion. Theoretical models of parallel computation such as the various flavors of PRAM, BSP <ref> [11] </ref>, and LogP [2] provide the algorithm designers with a convenient platform for formally expressing new algorithms and studying theoretically their performance. However, they rely on unrealistic assumptions.
References-found: 11

