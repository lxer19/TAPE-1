URL: http://polaris.cs.uiuc.edu/reports/1507.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Email: -paek@cs.uiuc.edu  padua@cs.uiuc.edu  
Title: Compiling for Scalable Multiprocessors with Polaris  
Author: YUNHEUNG PAEK and DAVID A. PADUA 
Keyword: Parallelizing Compiler, Multiprocessors, Communication  
Address: 1304 West Springfield Avenue, Urbana, IL 61801, USA  1304 West Springfield Avenue, Urbana, IL 61801, USA  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign,  Department of Computer Science University of Illinois at Urbana-Champaign,  
Note: Parallel Processing Letters c World Scientific Publishing Company  Received (received date) Revised (revised date) Communicated by (Name of Editor)  
Abstract: Due to the complexity of programming scalable multiprocessors with physically distributed memories, it is onerous to manually generate parallel code for these machines. As a consequence, there has been much research on the development of compiler techniques to simplify programming, to increase reliability, and to reduce development costs. For code generation, a compiler applies a number of transformations in areas such as data privatization, data copying and replication, synchronization, and data and work distribution. In this paper, we discuss our recent work on the development and implementation of a few compiler techniques for some of these transformations. We use Polaris, a parallelizing Fortran restructurer developed at Illinois, as the infrastructure to implement our algorithms. The paper includes experimental results obtained by applying our techniques to several benchmark codes. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> P. Banerjee, J. A. Chandy, M. Gupta, E. W. Hodges IV, J. G. Holm, A. Lain, D. J. Palermo, S. Ramaswamy, and E. Su. </author> <title> The PARADIGM Compiler for Distributed-Memory Multicomputers. </title> <journal> IEEE Computer, </journal> <volume> 28(10) </volume> <pages> 37-47, </pages> <month> October </month> <year> 1995. </year>
Reference: 2. <author> R. Barua, D. Kranz, and A. Agarwal. </author> <title> Communication-Minimal Partitioning of Parallel Loops and Data Arrays for Cache-Coherent Distributed-Memory Multiprocessors. </title> <booktitle> In Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, New York, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Although there are some technical problems in the current implementation of CRAFT, we hope to be able to pipeline the data blocks to hide the latency. Data distribution and work distribution <ref> [2] </ref> are additional important issues in this matter. Other strategies to reduce the increased communication overhead include traditional loop transformation techniques, such as loop interchange, loop fusion, and loop distribution.
Reference: 3. <author> D. Bernstein, M. Breternitz, A. Gheith, and B. Mendelson. </author> <title> Solutions and Debugging for Data Consistency in Multiprocessors with Noncoherent Caches. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 23(1) </volume> <pages> 83-103, </pages> <year> 1995. </year> <month> 11 </month>
Reference-contexts: In fact, we have found in this work that multiprocessors with noncoherent caches have important advantages over their cache coherent counterparts. For example, non-cache coherent machines are easier to scale and are more economical <ref> [3] </ref>. To optimize communication costs in multiprocessors, it is often necessary for software to have explicit control over data movement. On cache coherent machines, controlling data movement can be cumbersome unless the machine includes mechanisms to override the hardware cache controller.
Reference: 4. <author> W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoeflinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, W. Pottenger, L. Rauchwerger, and P. Tu. </author> <title> Parallel Programming with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: The target language of our translator is CRAFT [6] augmented with libraries that provide single-sided communication primitives. Our work extends the parallelization techniques implemented in the Polaris restructurer <ref> [4] </ref>, which was developed by the authors and other researchers at Illinois. This paper is organized as follows. In Section 2, we briefly introduce the Polaris restructurer. In Section 3, we discuss shared-memory programming in the T3D. In Section 4, we discuss several basic and advanced transformation techniques. <p> In fact, on an extensive collection of programs gathered from the Perfect Benchmarks, SPEC, and other sources, Polaris substantially outperforms the native parallelizer of the SGI <ref> [4] </ref>. However, as discussed below, for non-cache coherent machines a simple code generation algorithm is not sufficient to achieve reasonable performance.
Reference: 5. <author> B. Chapman, P. Mehrota, and H. Zima. </author> <title> Programming in Vienna Fortran 90. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 51-59, </pages> <month> Fall </month> <year> 1992. </year>
Reference: 6. <author> Cray Research Inc. </author> <title> CRAY MPP Fortran Reference Manual, </title> <year> 1993. </year>
Reference-contexts: In this work, we profit from the fact that the Cray T3D supports fast single-sided communication in the form of PUT/GET primitives. The target language of our translator is CRAFT <ref> [6] </ref> augmented with libraries that provide single-sided communication primitives. Our work extends the parallelization techniques implemented in the Polaris restructurer [4], which was developed by the authors and other researchers at Illinois. This paper is organized as follows. In Section 2, we briefly introduce the Polaris restructurer.
Reference: 7. <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year>
Reference: 8. <author> J. Grout. </author> <title> Inline Expansion for the Polaris Research Compiler. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> May </month> <year> 1995. </year>
Reference: 9. <author> M. Hall, J. Anderson, S. Amarasinghe, B. Murphy, S. Liao, E. Bugnion, and M. Lam. </author> <title> Maimizing Multiprocessor Performance with the SUIF Compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 84-89, </pages> <month> December </month> <year> 1996. </year>
Reference: 10. <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: we discuss further how to deal with these issues to improve performance based on data flow analyses. 6 subroutine foo cdir$ private (procedure private variables) cdir$ private (loop private variables) enddo Fig. 2: Declaration of privatizable variables in Polaris 4.5 Compatibility Problems MPP Fortran extensions, such as CRAFT and HPF <ref> [10] </ref>, help the user to attain high performance through distribution of data while maintaining compatibility with conventional Fortran 77 or Fortran 90. It is very difficult to achieve total compatibility, however. CRAFT, therefore, diverts from several conventional language features in the name of performance.
Reference: 11. <author> S. Hiranandani, Kennedy K, and C. Tseng. </author> <title> Evaluating Compiler Optimizations for Fortran D. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 27-45, </pages> <year> 1994. </year>
Reference-contexts: Non-cache coherent machines, by contrast, allow the programmer or the compiler to have explicit and direct control over communications through explicit data movement operations. Having explicit communication control results in other advantages [12], such as a substantial reduction in communication costs from prefetching, data pipelining, and aggregation <ref> [11] </ref>. In this work, we profit from the fact that the Cray T3D supports fast single-sided communication in the form of PUT/GET primitives. The target language of our translator is CRAFT [6] augmented with libraries that provide single-sided communication primitives. <p> Therefore, we have found that we need to apply other techniques to improve our copying scheme in the future. These techniques would be based on inter-loop data region analysis. We may further reduce the overhead by using optimization techniques similar 10 to those used in message passing models <ref> [11] </ref>. For instance, we found that several small data blocks often can be merged into a larger block to reduce the number of transmitted data blocks.
Reference: 12. <author> R. Larus. </author> <title> Compiling for shared-memory and message-passing computer. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <year> 1996. </year>
Reference-contexts: Non-cache coherent machines, by contrast, allow the programmer or the compiler to have explicit and direct control over communications through explicit data movement operations. Having explicit communication control results in other advantages <ref> [12] </ref>, such as a substantial reduction in communication costs from prefetching, data pipelining, and aggregation [11]. In this work, we profit from the fact that the Cray T3D supports fast single-sided communication in the form of PUT/GET primitives.
Reference: 13. <author> S. Lundstorm. </author> <title> A Controllable MIMD Architecture. </title> <booktitle> Proceedings of the 1980 International Conference of Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <year> 1980. </year>
Reference-contexts: discuss the techniques we use to overcome these two main performance degradation factors and present some performance numbers. 7 5.1 Shared Data Copying Scheme To deal with the cache bypassing penalty, we developed the shared data copying (also called copy-in/copy-out) scheme that was applied for earlier hierarchical global memory systems <ref> [13] </ref>. In the scheme, shared memory is used as a repository of values for private memory, as shown in Figure 3. Before a parallel loop starts, the processors copy all that is used in the loop from shared memory into private memory.
Reference: 14. <author> J. Nielocha, R. Harrison, and R. Littlefield. </author> <title> Global Arrays: A Portable Shared-Memory Programming Model for Distributed Memory Computers. </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <pages> pages 340-349, </pages> <month> November </month> <year> 1994. </year>
Reference: 15. <institution> Oak Ridge National Laboratory, Oak Ridge, TN. </institution> <note> PVM 3 User's Guide and Reference Manual. </note>
Reference-contexts: To achieve this goal, it is necessary to deal with data distribution, data movement, and other issues. 3 Shared-memory Programming in the T3D Like most other distributed-memory machines, the Cray T3D supports the message-passing programming models through libraries such as MPI and PVM <ref> [15] </ref>. However, unlike the true message-passing machines, the T3D also has several powerful hardware features to efficiently support shared-memory programming [16] on top of physically distributed memory structures. The 3-D torus interconnection network provides high-throughput and low-latency remote memory access.
Reference: 16. <author> W. Oed. </author> <title> The Cray Reseach Massively Parallel Processor System CRAY T3D. </title> <institution> Cray Research Inc., </institution> <year> 1993. </year>
Reference-contexts: However, unlike the true message-passing machines, the T3D also has several powerful hardware features to efficiently support shared-memory programming <ref> [16] </ref> on top of physically distributed memory structures. The 3-D torus interconnection network provides high-throughput and low-latency remote memory access. The remote access is only five to six times slower than the local access.
Reference: 17. <author> Y. Paek and D. Padua. </author> <title> Automatic Parallelization for Non-cache Coherent Multiprocessors. </title> <booktitle> In Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, New York, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Once the first phase is complete, several advanced techniques are applied in the second phase to improve code quality. In this section we discuss the basic transformations. An earlier version of the material in this section was presented in <ref> [17] </ref>. 4.1 Shared Subprograms A shared subprogram is a subroutine or a function whose execution requires all processors to participate. A shared subprogram contains a parallel construct (e.g., a doall), an I/O statement with private variables, a call to other shared subprograms, or any other statements marked with parallel assertions. <p> The barriers used to control the flow of execution of masters and slaves are illustrated in the following code. Due to the T3D's special hardware for global barrier synchronization, the total overhead incurred by barriers has little impact on the performance, according to our experiments <ref> [17] </ref>. Thus, in this paper, we do not discuss any techniques to reduce barrier overhead. 4.4 Data Declaration In CRAFT, a variable must be declared as either shared or private. <p> We have developed translation algorithms to overcome all these restrictions when we translate Fortran to CRAFT. The algorithms include linearization, renaming, data replication, array reshaping and procedure cloning. Details of these algorithms and other compatibility problems are given in <ref> [17] </ref>. 5 Data Copying The basic transformations try only to generate a correct parallel program and to apply a few optimizations. Thus, they do not do anything with cache exploitation and data distribution. To obtain reasonable performance, we have to deal with these issues. <p> To obtain reasonable performance, we have to deal with these issues. Although the causes of performance degradation of parallel programs in the T3D vary depending on the applications, our analyses of several programs from the Perfect and SPEC benchmarks <ref> [17] </ref> show that there are two primary factors that prevent the achievement of maximum theoretical performance. The first factor is that shared data is not cached in the T3D, which results in loss of performance whenever the computation uses shared data repetitively. We call this the cache bypassing penalty. <p> In <ref> [17] </ref>, we showed that the hand-coded version of this scheme works well in most cases, and that it works particularly well when the data distribution requirements of a program are dynamic. <p> The preliminary performance that we reported in <ref> [17] </ref> was unsatisfactory. But, the shared data copying scheme removed the most important performance degradation factors. The experiments presented in Section 5 revealed that this scheme is effective on these applications. However, it also revealed that there is much room for improvement.
Reference: 18. <author> C. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Academic Publishers, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: We have obtained good speedups for these machines by applying in the backend a relatively simple code generation algorithm that tries to distribute as evenly as possible the parallel work across the processors <ref> [18] </ref>. In fact, on an extensive collection of programs gathered from the Perfect Benchmarks, SPEC, and other sources, Polaris substantially outperforms the native parallelizer of the SGI [4]. However, as discussed below, for non-cache coherent machines a simple code generation algorithm is not sufficient to achieve reasonable performance.
Reference: 19. <author> W. Pottenger. </author> <title> Induction Variable Substitution and Reduction Recognition in the Polaris Parallelizing Compiler. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> December </month> <year> 1994. </year>
Reference: 20. <author> P. Tu. </author> <title> Automatic Array Privatization and Demand-Driven Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Based on simple inter-procedural data flow analysis, we identify as procedure private the variables that are not used in parallel regions. Also, the privatizer, one of the frontend passes in Polaris, identifies loop private data <ref> [20] </ref>. Both classes of private variables have to be declared as private in CRAFT, as shown in Figure 2. Shared arrays must be explicitly distributed by the software. For simplicity, our current implementation automatically applies block distribution to all dimensions of all shared arrays. <p> The removal of these anti and output dependences is attributed to the copy-in/copy-out operations <ref> [20] </ref> using PUT/GET primitives; and, the primitives, in turn, are essential to the shared data copying scheme. Loop fusion has been found useful to remove most of the communication overhead for PUT/GET operations in TOMCATV. These additional loop transformation and parallelization techniques are very important for the data copying scheme. <p> We will pursue research to extend our work to more general classes of applications. The data copying scheme requires an accurate data access region analysis. We currently use the access region information generated by Polaris' frontend passes <ref> [20] </ref>. However, we need a more accurate data access region analysis to improve our opportunity to optimize the communication incurred by the copying scheme. We do not yet have clear solutions for data and work partitions, which are among the most important issues for scalable multiprocessor machines.
References-found: 20

