URL: ftp://ftp.cs.toronto.edu/pub/kbms/cml-palo92.ps.Z
Refering-URL: http://www.cs.utoronto.ca/~juris/myp.html
Root-URL: 
Email: greiner@learning.siemens.com  juris@cs.toronto.edu  
Title: A Statistical Approach to Solving the EBL Utility Problem  
Author: Russell Greiner Igor Jurisica 
Address: Princeton, NJ 08540  Toronto, Ontario M5S 1A4, Canada  
Affiliation: Siemens Corporate Research  Department of Computer Science University of Toronto  Department of Computer Science.  
Date: July 12-16, 1992  
Note: Proceedings of Tenth National Conference on Artificial Intelligence, San Jose, CA,  Supported by a University of Toronto Open Fellowship and a Research Assistantship from the  
Abstract: Many "learning from experience" systems use information extracted from problem solving experiences to modify a performance element PE, forming a new element PE 0 that can solve these and similar problems more efficiently. However, as transformations that improve performance on one set of problems can degrade performance on other sets, the new PE 0 is not always better than the original PE; this depends on the distribution of problems. We therefore seek the performance element whose expected performance, over this distribution, is optimal. Unfortunately, the actual distribution, which is needed to determine which element is optimal, is usually not known. Moreover, the task of finding the optimal element, even knowing the distribution, is intractable for most interesting spaces of elements. This paper presents a method, palo, that side-steps these problems by using a set of samples to estimate the unknown distribution, and by using a set of transformations to hill-climb to a local optimum. This process is based on a mathematically rigorous form of utility analysis: in particular, it uses statistical techniques to determine whether the result of a proposed transformation will be better than the original system. We also present an efficient way of implementing this learning system in the context of a general class of performance elements, and include empirical evidence that this approach can work effectively. fl Much of this work was performed at the University of Toronto, where it was supported by the Institute for Robotics and Intelligent Systems and by an operating grant from the National Science and Engineering Research Council of Canada. We also gratefully acknowledge receiving many helpful comments from William Cohen, Dave Mitchell, Dale Schuurmans and the anonymous referees. 
Abstract-found: 1
Intro-found: 1
Reference: [BD88a] <institution> M.Boddy and T.Dean Solving time dependent planning problems. </institution> <type> Technical Report, </type> <institution> Brown University, </institution> <year> 1988. </year>
Reference-contexts: In fact, if we ignore the Equation 2 part of palo's code, we have, in effect, an anytime algorithm <ref> [BD88a, BD88b] </ref>, that simply returns better and better elements over time. Of course, there are advantages to knowing when we have reached a local optimum: First, we can then switch off the learning part and thereafter simply run this (probably locally) optimal performance element.
Reference: [BMSJ78] <author> B. Buchanan, T. Mitchell, R. Smith, and C. Johnson, Jr. </author> <title> Models of learning systems. </title> <booktitle> In Encyclopedia of Computer Science and Technology, </booktitle> <volume> volume 11. </volume> <publisher> Dekker, </publisher> <year> 1978. </year>
Reference-contexts: We then focus on an instantiation of this general palo algorithm that can solve a learning problem that provably cannot be algorithmically solved in a stronger sense, as well as empirical data that demonstrates palo's effectiveness. In more detail <ref> [BMSJ78] </ref>: A performance element PE is a program that attempts to solve the given problems.
Reference: [Bol85] <author> B. Bollobas. </author> <title> Random Graphs. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: of nodes (each corresponding to a proposition; e.g., the node N 0 corresponds to "GoodCar ()" and N 2 corresponds to the empty disjunction), and A N fi N is a set of arcs, each corresponding either to the use of a rule (e.g., the a 1 arc 7 See <ref> [Bol85, p. 12] </ref>. N.b., these inequalities holds for essentially arbitrary distributions, not just normal distributions, subject only to the minor constraint that the sequence f i g has a finite second moment.
Reference: [CG91] <author> W. Cohen and R. Greiner. </author> <title> Probabilistic hill climbing. </title> <booktitle> In Proceedings of CLNL-91, </booktitle> <address> Berke-ley, </address> <month> September, </month> <year> 1991. </year>
Reference-contexts: By contrast, the number of possible macros depends on the number of rules in the system, which grows as more rules are added. This involves certain changes to the palo algorithm; see <ref> [CG91] </ref>. mations to a performance element is (likely to be) an improvement.
Reference: [Che52] <author> H. Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sums of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23 </volume> <pages> 493-507, </pages> <year> 1952. </year>
Reference-contexts: This average tends to the population mean, as n ! 1; i.e., = lim n;1 Y n . Chernoff bounds <ref> [Che52] </ref> describe the probable rate of convergence: the probability that "Y n is more than + fl" goes to 0 exponentially fast as n increases; and, for a fixed n, exponentially as fl increases.
Reference: [CM81] <author> W. Clocksin and C. Mellish. </author> <title> Programming in Prolog. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year> <title> Learning: Utility and Bias 247 </title>
Reference-contexts: Subsection 4.3 presents some empirical results that demonstrate that a system that uses these approximations can be effective. We choose the PE G class of performance elements as it corresponds to many standard problem solvers (including Prolog <ref> [CM81] </ref>; see also [GN87]); and the T RO class of transformations of strategies, as it corresponds to many EBL systems and moreover, the task of finding the global optimality strategy is NP-hard [Gre91]. 4.1 Graph Based PEs This subsection uses a particularly simple performance element PE 0 to illustrate the class
Reference: [Coh90] <author> W. Cohen. </author> <title> Using distribution-free learning theory to analyze chunking. </title> <booktitle> In Proceedings of CSCSI-90, </booktitle> <year> 1990. </year>
Reference-contexts: Most of the formal models, however, either deal with learning problems that are far harder than the problems actually attempted by real learning systems (e.g., [GL89, Gre91]) or model only relatively narrow classes of learning algorithms (e.g., <ref> [NT88, Coh90] </ref>.) By contrast, our model is very general and directly relevant to many systems. There are also a great number of existing LfE systems, and considerable experimental work on the utility problem.
Reference: [BD88b] <institution> M.Boddy and T.Dean An analysis of time dependent planning. </institution> <note> In Proceedings of AAAI-88, </note> <year> 1988. </year>
Reference-contexts: In fact, if we ignore the Equation 2 part of palo's code, we have, in effect, an anytime algorithm <ref> [BD88a, BD88b] </ref>, that simply returns better and better elements over time. Of course, there are advantages to knowing when we have reached a local optimum: First, we can then switch off the learning part and thereafter simply run this (probably locally) optimal performance element.
Reference: [DeJ88] <author> G. DeJong. </author> <booktitle> AAAI workshop on Explanation-Based Learning. Sponsored by AAAI, </booktitle> <year> 1988. </year>
Reference-contexts: 1 Introduction Problem solving is inherently combinatorially expensive [Nil80]. There are, of course, many methods designed to side-step this problem. One collection of techniques is based on the observation that many problems occur repeatedly; this has led to a number of "learning from experience" (or "LfE") systems <ref> [DeJ88, MCK + 89, LNR87] </ref> that each use information gleaned from one set of problem solving experiences to modify the underlying problem solver, forming a new one capable of solving similar problems more efficiently. <p> Unfortunately, the task of finding the globally optimal PE is intractable for most interesting cases [Gre91]. In contrast, most other previous LfE research <ref> [DeJ88, MCK + 89, LNR87] </ref> has focused on experimental techniques for incrementally modifying a problem solver, producing a series of performance elements PE 0 ; . . . ; PE m where each PE i+1 is a modification of PE i (e.g., PE i+1 might contain one new macro-rule not in
Reference: [GD91] <author> J. Gratch and G. DeJong. </author> <title> A hybrid approach to guarateed effective control strategies. </title> <booktitle> In Proceedings of IWML-91, </booktitle> <year> 1991. </year>
Reference-contexts: The correct action for the learner to take for such initial PEs is simply to leave them unmodified | i.e., not to learn. The work reported in <ref> [GD91, GD92] </ref> is perhaps the most similar to ours, in that their system also uses statistical technique to guarantee that the learned control strategy will be an improvement, based on a utility analysis.
Reference: [GD92] <author> J. Gratch and G. DeJong. COMPOSER: </author> <title> A probabilistic solution to the utility problem in speedup learning. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: We are now experimenting with variants of palo 0 that are less conservative in their estimates, in the hope that they will be correspondingly less sample-hungry. (See also <ref> [GD92] </ref>.) Finally, while this paper has focused on but a single set of proposed transformations T RO , there are many other transformation sets T that can also be used to find an efficient satisficing system; e.g., [Gre92a] discusses a set of transformations that correspond to operator compositions. 13 The "palo-style" <p> The correct action for the learner to take for such initial PEs is simply to leave them unmodified | i.e., not to learn. The work reported in <ref> [GD91, GD92] </ref> is perhaps the most similar to ours, in that their system also uses statistical technique to guarantee that the learned control strategy will be an improvement, based on a utility analysis.
Reference: [GE91] <author> R. Greiner and C. Elkan. </author> <title> Measuring and improving the effectiveness of representations. </title> <booktitle> In IJCAI-91, </booktitle> <year> 1991. </year>
Reference-contexts: There are obvious extensions to this cost model that can incorporate different utility values for answers of different "qualities"; see <ref> [GE91] </ref>. from some already-visited node down to a retrieval; e.g., fi 1 h ha 1 a 2 i; ha 3 a 4 a 5 i; ha 6 a 7 i i. <p> a set of transformations that correspond to operator compositions. 13 The "palo-style" approach is not restricted to speed-up learning; it can also be used to build learning systems that can find performance elements that are nearly optimal in terms of other measures, including accuracy [Gre92d] or categoricity [Gre92b]; see also <ref> [GE91, Gre92c] </ref>. 5 Conclusion Comparison with other relevant research: There are many other research projects | both theoretical and empirical | that also address the task of using a set of examples to produce a more efficient performance element.
Reference: [GJ92] <author> R. Greiner and I. Jurisica. </author> <title> EBL systems that (almost) always im prove performance. </title> <type> Technical Report, </type> <institution> Siemens Corporate Research, </institution> <year> 1992. </year>
Reference-contexts: With probability at least 1 ffi, palo will terminate. It then returns an element 4 These samples may be produced by the user, who is simply asking questions relevant to one of his tasks. 5 This proof, and others, appear in the expanded version of this paper <ref> [GJ92] </ref>. <p> Approximating [PE i ; PE 0 ; S]: The palo algorithm requires values of [PE i ; t j (PE i ); S] for each t j 2 9 To simplify the presentation, this article will only consider depth-first strategies; <ref> [GJ92] </ref> extends this to deal with arbitrary strategies. T . <p> L (q) U (q) 1 0 0 3 f ( 2 ) f ( 2 ) This table only bounds the value of [ PE A ; PE B ; q ] for a single sample. 10 The value of [ PE A ; PE B ; S ] 10 <ref> [GJ92] </ref> shows how to obtain slightly tighter bounds, Learning: Utility and Bias 245 will be between L (PE A ; PE B ; S) def P U (PE A ; PE B ; S) = q2S U (q). <p> Second, if we are not happy with the performance of that element, a palo-variant can then jump to different performance element in another part of the space, and begin hill climbing from there, possibly using a form of simulated annealing approach [RMt86]. The extended paper <ref> [GJ92] </ref> presents other experimental data, based on other probability distributions, reduction graphs, parameter values, and so forth. In general, palo 0 's performance is similar to the above description: For each setting, palo 0 climbs appropriately, requiring successively more samples for each step.
Reference: [GL89] <author> R. Greiner and J. Likuski. </author> <title> Incorporating redundant learned rules: A preliminary formal analysis of EBL. </title> <booktitle> In IJCAI-89, </booktitle> <year> 1989. </year>
Reference-contexts: We often view this PE 0 as a modified version of PE. macro-rules, re-ordering the rules, adding censors to ex-isting rules, and so on. Our previous papers have presented algorithms that find the PE 0 whose performance is optimal <ref> [GL89, Gre91] </ref> or nearly-optimal [OG90, GO91], where performance is measured as the expected cost of a PE over some fixed distribution of problems. Unfortunately, the task of finding the globally optimal PE is intractable for most interesting cases [Gre91]. <p> Most of the formal models, however, either deal with learning problems that are far harder than the problems actually attempted by real learning systems (e.g., <ref> [GL89, Gre91] </ref>) or model only relatively narrow classes of learning algorithms (e.g., [NT88, Coh90].) By contrast, our model is very general and directly relevant to many systems. There are also a great number of existing LfE systems, and considerable experimental work on the utility problem.
Reference: [GN87] <author> M. Genesereth and N. Nilsson. </author> <booktitle> Logical Foundations of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, CA, </address> <year> 1987. </year>
Reference-contexts: Subsection 4.3 presents some empirical results that demonstrate that a system that uses these approximations can be effective. We choose the PE G class of performance elements as it corresponds to many standard problem solvers (including Prolog [CM81]; see also <ref> [GN87] </ref>); and the T RO class of transformations of strategies, as it corresponds to many EBL systems and moreover, the task of finding the global optimality strategy is NP-hard [Gre91]. 4.1 Graph Based PEs This subsection uses a particularly simple performance element PE 0 to illustrate the class PE G ,
Reference: [GO91] <author> R. Greiner and P. Orponen. </author> <title> Probably approximately optimal derivation strategies. </title> <editor> In J.A. Allen, R. Fikes, and E. Sandewall, editors, </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Conference, </booktitle> <address> San Mateo, CA, April 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We often view this PE 0 as a modified version of PE. macro-rules, re-ordering the rules, adding censors to ex-isting rules, and so on. Our previous papers have presented algorithms that find the PE 0 whose performance is optimal [GL89, Gre91] or nearly-optimal <ref> [OG90, GO91] </ref>, where performance is measured as the expected cost of a PE over some fixed distribution of problems. Unfortunately, the task of finding the globally optimal PE is intractable for most interesting cases [Gre91]. <p> We define the expected cost of a strategy as the weighted sum of the costs of its paths, each weighted by the probability that we will need to pursue this path, i.e., that none of the prior paths succeeded <ref> [Smi89, GO91] </ref>. (Of course, the cost of a path is the sum of the cost of its arcs.) While the models of performance elements and cost presented above are sufficient for the rest of this article, they appear quite limited. <p> We would also define S to be a set of subsets of N , where the query processor would have to reach each member of some s 2 S for the derivation to succeed. This extension leads to additional complica tions in specifying strategies; see <ref> [GO91, Appendix A] </ref>.
Reference: [Gol79] <author> A. Goldberg. </author> <title> An average case complexity analysis of the satisfiability problem. </title> <booktitle> In Proceedings of the 4th Workshop on Automated Deduction, </booktitle> <year> 1979 </year>
Reference-contexts: This Pr [ ] reflects the distribution of problems our PE is actually addressing; n.b., it is not likely to be a uniform distribution over all possible problems <ref> [Gol79] </ref>, nor will it necessarily correspond to any particular collection of "benchmark challenge problems" [Kel87].
Reference: [Gre91] <author> R. Greiner. </author> <title> Finding the optimal derivation strategy in a redundant knowledge base. </title> <journal> Artificial Intelligence, </journal> <volume> 50(1) </volume> <pages> 95-116, </pages> <year> 1991. </year>
Reference-contexts: Unfortunately, a modification that improves the problem solver's performance for one set of problems can degrade its performance for other problems <ref> [Min88b, Gre91] </ref>; hence, many of these modifications will in fact lower the system's overall performance. <p> We often view this PE 0 as a modified version of PE. macro-rules, re-ordering the rules, adding censors to ex-isting rules, and so on. Our previous papers have presented algorithms that find the PE 0 whose performance is optimal <ref> [GL89, Gre91] </ref> or nearly-optimal [OG90, GO91], where performance is measured as the expected cost of a PE over some fixed distribution of problems. Unfortunately, the task of finding the globally optimal PE is intractable for most interesting cases [Gre91]. <p> Unfortunately, the task of finding the globally optimal PE is intractable for most interesting cases <ref> [Gre91] </ref>. <p> choose the PE G class of performance elements as it corresponds to many standard problem solvers (including Prolog [CM81]; see also [GN87]); and the T RO class of transformations of strategies, as it corresponds to many EBL systems and moreover, the task of finding the global optimality strategy is NP-hard <ref> [Gre91] </ref>. 4.1 Graph Based PEs This subsection uses a particularly simple performance element PE 0 to illustrate the class PE G , whose elements each correspond to a finite graph whose arcs have fixed costs. <p> Most of the formal models, however, either deal with learning problems that are far harder than the problems actually attempted by real learning systems (e.g., <ref> [GL89, Gre91] </ref>) or model only relatively narrow classes of learning algorithms (e.g., [NT88, Coh90].) By contrast, our model is very general and directly relevant to many systems. There are also a great number of existing LfE systems, and considerable experimental work on the utility problem.
Reference: [Gre92a] <author> R. Greiner. </author> <title> Effective operator composition. </title> <type> Technical Report, </type> <institution> Siemens Corporate Research, </institution> <year> 1992. </year>
Reference-contexts: estimates, in the hope that they will be correspondingly less sample-hungry. (See also [GD92].) Finally, while this paper has focused on but a single set of proposed transformations T RO , there are many other transformation sets T that can also be used to find an efficient satisficing system; e.g., <ref> [Gre92a] </ref> discusses a set of transformations that correspond to operator compositions. 13 The "palo-style" approach is not restricted to speed-up learning; it can also be used to build learning systems that can find performance elements that are nearly optimal in terms of other measures, including accuracy [Gre92d] or categoricity [Gre92b]; see
Reference: [Gre92b] <author> R. Greiner. </author> <title> Learning near optimal horn approximations. In Procededings of Knowledge Assimilation Symposiium, </title> <publisher> Stanford, </publisher> <year> 1992. </year>
Reference-contexts: e.g., [Gre92a] discusses a set of transformations that correspond to operator compositions. 13 The "palo-style" approach is not restricted to speed-up learning; it can also be used to build learning systems that can find performance elements that are nearly optimal in terms of other measures, including accuracy [Gre92d] or categoricity <ref> [Gre92b] </ref>; see also [GE91, Gre92c]. 5 Conclusion Comparison with other relevant research: There are many other research projects | both theoretical and empirical | that also address the task of using a set of examples to produce a more efficient performance element.
Reference: [Gre92c] <author> R. Greiner. </author> <title> Probabilistic hill-climbing: Theory and applications. </title> <booktitle> In Proceedings of CSCSI-92, </booktitle> <year> 1992. </year>
Reference-contexts: a set of transformations that correspond to operator compositions. 13 The "palo-style" approach is not restricted to speed-up learning; it can also be used to build learning systems that can find performance elements that are nearly optimal in terms of other measures, including accuracy [Gre92d] or categoricity [Gre92b]; see also <ref> [GE91, Gre92c] </ref>. 5 Conclusion Comparison with other relevant research: There are many other research projects | both theoretical and empirical | that also address the task of using a set of examples to produce a more efficient performance element.
Reference: [Gre92d] <author> R. Greiner. </author> <title> Producing more accurate representational systems. </title> <type> Technical Report, </type> <institution> Siemens Corporate Research, </institution> <year> 1992. </year>
Reference-contexts: efficient satisficing system; e.g., [Gre92a] discusses a set of transformations that correspond to operator compositions. 13 The "palo-style" approach is not restricted to speed-up learning; it can also be used to build learning systems that can find performance elements that are nearly optimal in terms of other measures, including accuracy <ref> [Gre92d] </ref> or categoricity [Gre92b]; see also [GE91, Gre92c]. 5 Conclusion Comparison with other relevant research: There are many other research projects | both theoretical and empirical | that also address the task of using a set of examples to produce a more efficient performance element.
Reference: [Kel87] <author> R. Keller. </author> <title> Defining operationality for explanation-based learning. </title> <booktitle> In AAAI-87, </booktitle> <year> 1987. </year>
Reference-contexts: This Pr [ ] reflects the distribution of problems our PE is actually addressing; n.b., it is not likely to be a uniform distribution over all possible problems [Gol79], nor will it necessarily correspond to any particular collection of "benchmark challenge problems" <ref> [Kel87] </ref>. We can then define the expected cost of a performance element: C [ PE ] = E [ c (PE; q) ] = q2Q Our underlying challenge is to find the performance element whose expected cost is minimal.
Reference: [LNR87] <author> J. Laird, A. Newell, and P. Rosenbloom. </author> <title> SOAR: An architecture of general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33(3), </volume> <year> 1987. </year>
Reference-contexts: 1 Introduction Problem solving is inherently combinatorially expensive [Nil80]. There are, of course, many methods designed to side-step this problem. One collection of techniques is based on the observation that many problems occur repeatedly; this has led to a number of "learning from experience" (or "LfE") systems <ref> [DeJ88, MCK + 89, LNR87] </ref> that each use information gleaned from one set of problem solving experiences to modify the underlying problem solver, forming a new one capable of solving similar problems more efficiently. <p> Typical LfEs traverse the space of possible PEs using transformations that modify a given PE by adding 1 "EBL" abbreviates "Explanation-Based Learning". 2 These two components, PE and LfE, may correspond the same bundle of code; cf., SOAR <ref> [LNR87] </ref>. We often view this PE 0 as a modified version of PE. macro-rules, re-ordering the rules, adding censors to ex-isting rules, and so on. <p> Unfortunately, the task of finding the globally optimal PE is intractable for most interesting cases [Gre91]. In contrast, most other previous LfE research <ref> [DeJ88, MCK + 89, LNR87] </ref> has focused on experimental techniques for incrementally modifying a problem solver, producing a series of performance elements PE 0 ; . . . ; PE m where each PE i+1 is a modification of PE i (e.g., PE i+1 might contain one new macro-rule not in
Reference: [MCK + 89] <author> S. Minton, J. Carbonell, C. Knoblock, D. Kuokka, O. Etzioni, and Y. Gil. </author> <title> Explanation-based learning: A problem solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40(1-3):63-119, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Problem solving is inherently combinatorially expensive [Nil80]. There are, of course, many methods designed to side-step this problem. One collection of techniques is based on the observation that many problems occur repeatedly; this has led to a number of "learning from experience" (or "LfE") systems <ref> [DeJ88, MCK + 89, LNR87] </ref> that each use information gleaned from one set of problem solving experiences to modify the underlying problem solver, forming a new one capable of solving similar problems more efficiently. <p> Unfortunately, the task of finding the globally optimal PE is intractable for most interesting cases [Gre91]. In contrast, most other previous LfE research <ref> [DeJ88, MCK + 89, LNR87] </ref> has focused on experimental techniques for incrementally modifying a problem solver, producing a series of performance elements PE 0 ; . . . ; PE m where each PE i+1 is a modification of PE i (e.g., PE i+1 might contain one new macro-rule not in
Reference: [Min88a] <author> S. Minton. </author> <title> Learning Search Control Knowledge: An Explanation-Based Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Hingham, MA, </address> <year> 1988. </year>
Reference: [Min88b] <author> S. Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> In AAAI-88, </booktitle> <year> 1988. </year>
Reference-contexts: Unfortunately, a modification that improves the problem solver's performance for one set of problems can degrade its performance for other problems <ref> [Min88b, Gre91] </ref>; hence, many of these modifications will in fact lower the system's overall performance. <p> Unfortunately, a modification that improves the problem solver's performance for one set of problems can degrade its performance for other problems [Min88b, Gre91]; hence, many of these modifications will in fact lower the system's overall performance. This paper addresses this problem (often called the "EBL 1 utility problem" <ref> [Min88b, SER91] </ref>) by using a statistical technique to determine whether the result of a proposed modification will, with provably high confidence, be better than the original system.
Reference: [Mit82] <author> T. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2) </volume> <pages> 203-26, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: A learning element LfE uses information gleaned from these problem solving experience (s) to transform PE into a new performance element, PE 0 . 2 Just as concept learning can be characterized as a search through a space of possible concept descriptions <ref> [Mit82] </ref>, so the LfE system can be viewed as searching through a space of possible PEs, seeking a new performance element PE 0 whose performance is superior to that of the original PE.
Reference: [Nil80] <author> N. Nilsson. </author> <booktitle> Principles of Artifical Intelligence. </booktitle> <publisher> Tioga Press, </publisher> <address> Palo Alto, </address> <year> 1980. </year>
Reference-contexts: 1 Introduction Problem solving is inherently combinatorially expensive <ref> [Nil80] </ref>. There are, of course, many methods designed to side-step this problem.
Reference: [NT88] <author> B. Natarajan and P. Tadepalli. </author> <title> Two frameworks for learning. </title> <booktitle> In Proceedings of IML-88, </booktitle> <year> 1988. </year>
Reference-contexts: Most of the formal models, however, either deal with learning problems that are far harder than the problems actually attempted by real learning systems (e.g., [GL89, Gre91]) or model only relatively narrow classes of learning algorithms (e.g., <ref> [NT88, Coh90] </ref>.) By contrast, our model is very general and directly relevant to many systems. There are also a great number of existing LfE systems, and considerable experimental work on the utility problem.
Reference: [OG90] <author> P. Orponen and R. Greiner. </author> <title> On the sample complexity of finding good search strategies. </title> <booktitle> In Proceedings of COLT-90, </booktitle> <year> 1990. </year>
Reference-contexts: We often view this PE 0 as a modified version of PE. macro-rules, re-ordering the rules, adding censors to ex-isting rules, and so on. Our previous papers have presented algorithms that find the PE 0 whose performance is optimal [GL89, Gre91] or nearly-optimal <ref> [OG90, GO91] </ref>, where performance is measured as the expected cost of a PE over some fixed distribution of problems. Unfortunately, the task of finding the globally optimal PE is intractable for most interesting cases [Gre91]. <p> See <ref> [OG90] </ref>. <p> Our model can handle these situations by simply allowing any arc (not just retrieval arcs) to be a probabilistic experiment. <ref> [OG90] </ref> elaborates on this description, and discusses some of the additional com plications that arise.
Reference: [RMt86] <editor> D. Rumelhart, J. McClelland and the PDP Research Group, editors. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Mi-crostructure of Cognition, volume 1: Foundations. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Second, if we are not happy with the performance of that element, a palo-variant can then jump to different performance element in another part of the space, and begin hill climbing from there, possibly using a form of simulated annealing approach <ref> [RMt86] </ref>. The extended paper [GJ92] presents other experimental data, based on other probability distributions, reduction graphs, parameter values, and so forth. In general, palo 0 's performance is similar to the above description: For each setting, palo 0 climbs appropriately, requiring successively more samples for each step.
Reference: [SER91] <author> A. Segre, C. Elkan, and A. Russell. </author> <title> A critical look at experimental evaluations of EBL. </title> <journal> Machine Learning Journal, </journal> <volume> 6(2), </volume> <year> 1991. </year>
Reference-contexts: Unfortunately, a modification that improves the problem solver's performance for one set of problems can degrade its performance for other problems [Min88b, Gre91]; hence, many of these modifications will in fact lower the system's overall performance. This paper addresses this problem (often called the "EBL 1 utility problem" <ref> [Min88b, SER91] </ref>) by using a statistical technique to determine whether the result of a proposed modification will, with provably high confidence, be better than the original system.
Reference: [SK75] <author> H. Simon and J. Kadane. </author> <title> Optimal problem-solving search: All-or-none solutions. </title> <journal> Artificial Intelligence, </journal> <volume> 6 </volume> <pages> 235-247, </pages> <year> 1975. </year>
Reference: [Smi89] <author> D. Smith. </author> <title> Controlling backward inference. </title> <journal> Artificial Intelligence, </journal> <volume> 39(2) </volume> <pages> 145-208, </pages> <month> June </month> <year> 1989. </year> <title> Learning: Utility and Bias 248 </title>
Reference-contexts: We define the expected cost of a strategy as the weighted sum of the costs of its paths, each weighted by the probability that we will need to pursue this path, i.e., that none of the prior paths succeeded <ref> [Smi89, GO91] </ref>. (Of course, the cost of a path is the sum of the cost of its arcs.) While the models of performance elements and cost presented above are sufficient for the rest of this article, they appear quite limited. <p> query asked ) = 0:01 P ( Red () in Fact Set j GoodCar () query asked ) = 0:2 P ( Gold () in Fact Set j GoodCar () query asked ) = 0:8 Given these values, it is easy to compute the expected costs of the various strategies <ref> [Smi89] </ref>: C [ fi hcrgi ] = based on information that is available from PE A 's computation. 11 We decided against using a simple blocks-world example as it would be more complicated to describe, but would be no more meaningful as we would still have to make up a distribution
References-found: 35

