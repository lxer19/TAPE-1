URL: http://www.cs.colorado.edu/~suvas/papers/sc97/paper/paper.ps
Refering-URL: http://www.cs.colorado.edu/~suvas/papers/sc97/paper/index.htm
Root-URL: http://www.cs.colorado.edu
Email: suvas@cs.colorado.edu  grunwald@cs.colorado.edu  
Title: Loop Re-ordering and Pre-fetching at Run-time  
Author: Suvas Vajracharya Dirk Grunwald 
Keyword: Run-time systems, data locality, temporal locality, loop transformations, dependence-driven, systolic arrays, coarse-grain dataflow.  
Address: Boulder, CO, U.S.A.  Boulder, CO, U.S.A.  
Affiliation: Department of Computer Science University of Colorado  Department of Computer Science University of Colorado  
Abstract: The order in which loop iterations are executed can have a large impact on the number of cache misses that an applications takes. A new loop order that preserves the semantics of the old order but has a better cache data re-use, improves the performance of that application. Several compiler techniques exist to transform loops such that the order of iterations reduces cache misses. This paper introduces a run-time method to determine the order based on a dependence-driven execution. In a dependence-driven execution, an execution traverses the iteration space by following the dependence arcs between the iterations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 19(6):233 246, </volume> <month> June </month> <year> 1985. </year>
Reference-contexts: The set of distance vectors makes up the data dependences that determine the allowable re-ordering transformation. Based on these dependences, optimizing compilers may make the following transformations to improve locality: * Loop Interchange: Loop Interchange <ref> [18, 1] </ref> swaps an inner loop with an outer loop.
Reference: [2] <author> Jennifer M. Anderson, Lance M. Berc, Jeffrey Dean, Sanjay Ghemawat, Monika Henzinger, Shun-Tak Leung, Richard L. Sites, Mark Vandevoorde, Carl Waldspurger, and William Weihl. </author> <title> Continuous profiling: Where have all the cycles gone? In SOSP (To appear in), </title> <month> October </month> <year> 1996. </year>
Reference-contexts: The cache penalties for the two caches are 15 cycles and 60 cycles respectively. All programs were compiled with DEC C++ (cxx) compiler with -O5 option. To determine where the cycles (cache miss, branch mis-predict, useful computation, etc.) were spent, we used the Digital Continuous Profiling Infrastructure (DCPI) <ref> [2] </ref> available on the Alpha platforms. DCPI runs on the background with low overhead (slowdown of 1-3%) and unobtrusively generates profile data for the applications running on the machines by sampling hardware performance counters available on the Alphas.
Reference: [3] <author> Robert Babb. </author> <title> Parallel processing with large-grain data flow techniques. </title> <journal> IEEE Computer, </journal> <volume> 17(7) </volume> <pages> 55-61, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: In this paper, we present a hybrid compile-time/run-time method to re-order loop iterations using a dependence-driven execution model that is loosely based on the concept of systolic arrays [9, 11] and coarse grain dataflow <ref> [3] </ref>. In a dependence-driven execution, the system enables a block of iterations when the dependence constraints on those iterations are satisfied. The immediate execution of newly enabled iterations produces a depth-first traversal of the iteration space which improves data re-use.
Reference: [4] <author> David F. Bacon, Susan L. Graham, and Oliver J. Sharp. </author> <title> Compiler transformations for high-performance computing. </title> <type> Technical Report CSD-93-781, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1993. </year>
Reference-contexts: While the generated code is more complex than the original, the new code has better locality and parallelism. A survey of compiler transformations for high performance computing can be found in <ref> [4, 17] </ref>. 3 Dependence Driven Execution Given the transformed iteration space, compilers must generate code that describes the traversal over the entire iteration space. This early commitment to a specific order limits flexibility.
Reference: [5] <author> Forest Baskett. </author> <title> Keynote address. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Despite rapid increases in CPU performance, the primary obstacles to achieving higher performance in current processor organizations remain control and data hazards. An estimate <ref> [5] </ref> shows that the performance of single-chip microprocessors are improving at a rate of 80% annually, while DRAM speeds are improving at a rate of only 5-10% in that same amount time [5] [8]. <p> An estimate <ref> [5] </ref> shows that the performance of single-chip microprocessors are improving at a rate of 80% annually, while DRAM speeds are improving at a rate of only 5-10% in that same amount time [5] [8]. The growing inability of the memory systems to keep up with the processors increases the importance of cache data re-use to reduce traffic to main memory and pre-fetching mechanisms to hide memory access latencies.
Reference: [6] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared memory parallel computing. </title> <journal> ACM. Trans on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: By putting together and specializing these objects, the user specializes the system to create a "software systolic array" for the application at hand. This object-oriented model is based on AWESIME [7] and the Chores <ref> [6] </ref> run-time systems. The following is a list of objects in Dude: * Data Descriptor: Data Descriptors describe a subsection of the data space. For example, a matrix can be divided into sub-matrices with each sub-matrix being defined by a data descriptor.
Reference: [7] <author> Dirk Grunwald. </author> <title> A users guide to awesime: An object oriented parallel programming and simulation system. </title> <type> Technical Report CU-CS-552-91, </type> <institution> University of Colorado, Boulder, </institution> <year> 1991. </year>
Reference-contexts: By putting together and specializing these objects, the user specializes the system to create a "software systolic array" for the application at hand. This object-oriented model is based on AWESIME <ref> [7] </ref> and the Chores [6] run-time systems. The following is a list of objects in Dude: * Data Descriptor: Data Descriptors describe a subsection of the data space. For example, a matrix can be divided into sub-matrices with each sub-matrix being defined by a data descriptor.
Reference: [8] <author> J.L. Hennessy and D.A. Patterson. </author> <title> Computer Architecture: a Quantitative Approach. </title> <address> Morgan-Kaufman, </address> <year> 1990. </year>
Reference-contexts: An estimate [5] shows that the performance of single-chip microprocessors are improving at a rate of 80% annually, while DRAM speeds are improving at a rate of only 5-10% in that same amount time [5] <ref> [8] </ref>. The growing inability of the memory systems to keep up with the processors increases the importance of cache data re-use to reduce traffic to main memory and pre-fetching mechanisms to hide memory access latencies.
Reference: [9] <author> H.T. Kung and C.E. Leiserson. </author> <title> Systolic arrays (for vlsi). In Sparse Matrix Proc.(Society for Industrial and Applied Mathematics, </title> <booktitle> 1979), </booktitle> <pages> pages 256-282, </pages> <year> 1978. </year>
Reference-contexts: In this paper, we present a hybrid compile-time/run-time method to re-order loop iterations using a dependence-driven execution model that is loosely based on the concept of systolic arrays <ref> [9, 11] </ref> and coarse grain dataflow [3]. In a dependence-driven execution, the system enables a block of iterations when the dependence constraints on those iterations are satisfied. The immediate execution of newly enabled iterations produces a depth-first traversal of the iteration space which improves data re-use.
Reference: [10] <author> S.Y Kung. </author> <title> Wavefront array processors. </title> <booktitle> In Systolic Signal Processing Systems, </booktitle> <pages> pages 97-160, </pages> <year> 1987. </year>
Reference-contexts: However, because of the temporal locality that this model offers, we can expect a performance improvement even on a uniprocessor. Unlike the computation in systolic arrays, the computations in Dude are not synchronized by a global clock (and in that sense, our model is closer to wavefront arrays <ref> [10] </ref>).
Reference: [11] <author> S.Y Kung. </author> <title> VLSI Array Processors. Systems Sciences. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: In this paper, we present a hybrid compile-time/run-time method to re-order loop iterations using a dependence-driven execution model that is loosely based on the concept of systolic arrays <ref> [9, 11] </ref> and coarse grain dataflow [3]. In a dependence-driven execution, the system enables a block of iterations when the dependence constraints on those iterations are satisfied. The immediate execution of newly enabled iterations produces a depth-first traversal of the iteration space which improves data re-use.
Reference: [12] <author> M.S. Lam, E.E. Rothberg, and M.E. Wolf. </author> <title> The cache performance and optimization of blocked algorithm. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Optimizing compilers will apply this transformation to improve memory locality if the interchange reduces the array access stride. * Blocking (or Tiling): Blocking <ref> [19, 12, 16] </ref> takes advantage of applications with spatial locality by traversing a rectangle of iteration space at a time.
Reference: [13] <author> S. Levialdi. </author> <title> On shrinking binary picture patterns. </title> <journal> Communications of the ACM, </journal> <volume> 15(1) </volume> <pages> 7-10, </pages> <month> January </month> <year> 1972. </year>
Reference-contexts: Further analysis of the skewing code using DCPI also revealed that this method suffered from resource conflict stalls due to all the integer arithmetic in the subscript expression required by the compiler-transformed code (see right side of figure 1). 4.3 Component Labeling Levialdi's algorithm <ref> [13] </ref> for component labeling is used in image processing to detect connected components of a picture. It involves a series of phases, each phase consisting of changing a 1-pixel to a 0-pixel if its upper, left, and upper-left neighbors are 0-pixels.
Reference: [14] <author> Hanan Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: In a dependence-driven execution, the memory locality of the entire execution of the nested loop is sensitive to the order in which the initially unconstrained Iterates are loaded. For applications which have block memory access patterns such as this one, the system loads the Iterates in Morton order <ref> [14] </ref>. Now the computation begins. After the initial Iterates have been loaded, the system scheduler pops off a (Red) Iterate from the system LIFO queue and applies the main operator to the data described by the descriptor for that Iterate.
Reference: [15] <author> Suvas Vajracharya and Dirk Grunwald. </author> <title> Dependence-driven run-time system. </title> <booktitle> In Proceedings of Language and Compilers for Parallel Computing, </booktitle> <pages> pages 168-176, </pages> <year> 1996. </year>
Reference-contexts: This imbalance suggests that computational overhead for logic to avoid cache misses may not be significant if this logic can reduce traffic to memory, or hide the latency of memory operations. Elsewhere <ref> [15] </ref>, we discussed the parallelism and scalability of a dependence-driven execution on a multiprocessor.
Reference: [16] <author> Michael Edward Wolf. </author> <title> Improving locality and parallelism in nested loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Optimizing compilers will apply this transformation to improve memory locality if the interchange reduces the array access stride. * Blocking (or Tiling): Blocking <ref> [19, 12, 16] </ref> takes advantage of applications with spatial locality by traversing a rectangle of iteration space at a time.
Reference: [17] <author> Michael Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: While the generated code is more complex than the original, the new code has better locality and parallelism. A survey of compiler transformations for high performance computing can be found in <ref> [4, 17] </ref>. 3 Dependence Driven Execution Given the transformed iteration space, compilers must generate code that describes the traversal over the entire iteration space. This early commitment to a specific order limits flexibility.
Reference: [18] <author> M.J. Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <type> PhD thesis, </type> <institution> Univ. Illinois, Urbana, </institution> <month> April </month> <year> 1987. </year> <type> Rep. 329. </type>
Reference-contexts: The set of distance vectors makes up the data dependences that determine the allowable re-ordering transformation. Based on these dependences, optimizing compilers may make the following transformations to improve locality: * Loop Interchange: Loop Interchange <ref> [18, 1] </ref> swaps an inner loop with an outer loop. <p> In some cases, skewing <ref> [18] </ref> can be applied to enable blocking transformation. Skewing traverses the iteration space diagonally in waves. From the transformed iteration space, the compiler generates code in the form of new loops. As an example, consider the hyperbolic 1D PDE.
Reference: [19] <author> M.J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proc. of Supercomputing 89, </booktitle> <pages> pages 655-664, </pages> <month> Nov </month> <year> 1989. </year>
Reference-contexts: Optimizing compilers will apply this transformation to improve memory locality if the interchange reduces the array access stride. * Blocking (or Tiling): Blocking <ref> [19, 12, 16] </ref> takes advantage of applications with spatial locality by traversing a rectangle of iteration space at a time.
References-found: 19

