URL: http://ai.fri.uni-lj.si/papers/kononenko95-ijcai.ps
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Title: On Biases in Estimating Multi-Valued Attributes  
Author: Igor Kononenko 
Address: Trzaska 25, SI-61001 Ljubljana Slovenia  
Affiliation: University of Ljubljana, Faculty of electr. eng. and computer sc.  
Abstract: We analyse the biases of eleven measures for estimating the quality of the multi-valued attributes. The values of information gain, J-measure, gini-index, and relevance tend to linearly increase with the number of values of an attribute. The values of gain-ratio, distance measure, Relief , and the weight of evidence decrease for informative attributes and increase for irrelevant attributes. The bias of the statistic tests based on the chi-square distribution is similar but these functions are not able to discriminate among the attributes of different quality. We also introduce a new function based on the MDL principle whose value slightly decreases with the increasing number of attribute's values.
Abstract-found: 1
Intro-found: 1
Reference: [ Baim, 1988 ] <author> P.W. Baim. </author> <title> A method for attribute selection in inductive learning systems. </title> <journal> IEEE Trans. on PAMI, </journal> <volume> 10 </volume> <pages> 888-896, </pages> <year> 1988. </year>
Reference-contexts: We adopted and extended their scenario in order to verify results of methods tested by White and Liu and to test also some other well known measures: gini-index [Breiman et al., 1984], J -measure [Smyth and Goodman, 1990], the weight of evidence [Michie, 1989], and relevance <ref> [Baim, 1988] </ref>. Besides, we developed and tested also one new selection measure based on the minimum description length (MDL) principle and a measure derived from the algorithm RELIEF [Kira and Rendell, 1992]. In the following we describe all selection measures, the experimental scenario and results.
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: We adopted and extended their scenario in order to verify results of methods tested by White and Liu and to test also some other well known measures: gini-index <ref> [Breiman et al., 1984] </ref>, J -measure [Smyth and Goodman, 1990], the weight of evidence [Michie, 1989], and relevance [Baim, 1988]. Besides, we developed and tested also one new selection measure based on the minimum description length (MDL) principle and a measure derived from the algorithm RELIEF [Kira and Rendell, 1992].
Reference: [ Kira and Rendell, 1992 ] <author> K. Kira and L. Rendell. </author> <title> A practical approach to feature selection. </title> <booktitle> Proc. Intern. Conf. on Machine Learning (Aberdeen, </booktitle> <editor> July 1992) D.Sleeman and P.Edwards (eds.), </editor> <publisher> Morgan Kaufmann, pp.249-256. </publisher>
Reference-contexts: Besides, we developed and tested also one new selection measure based on the minimum description length (MDL) principle and a measure derived from the algorithm RELIEF <ref> [Kira and Rendell, 1992] </ref>. In the following we describe all selection measures, the experimental scenario and results. We analyse the (dis)advantages of various selection measures. 2 Selection measures In this section we briefly describe all selection measures and develop a new one based on the MDL principle. <p> Among all the measures only RELIEF (together with the search for the nearest instances) is non-myopic in the sense that it is able to appropriately deal with strongly dependent attributes. Besides, RELIEF can also efficiently estimate continuous attributes <ref> [Kira and Rendell, 1992] </ref>. The extensions introduced in the algorithm RELIEFF [Kononenko, 1994] enable it to efficiently deal with noisy data, missing values, and multi-class problems. All these important features, together with the relatively acceptable bias described in this paper, make RELIEFF a promising measure.
Reference: [ Kononenko, 1994 ] <author> I. Kononenko. </author> <title> Estimating attributes: Analysis and extensions of RELIEF. </title> <booktitle> Proc. European Conf. on Machine Learning (Catania, </booktitle> <month> April </month> <year> 1994), </year> <editor> L. De Raedt and F.Bergadano (eds.), </editor> <publisher> Springer Verlag, pp.171-182. </publisher>
Reference-contexts: Among all the measures only RELIEF (together with the search for the nearest instances) is non-myopic in the sense that it is able to appropriately deal with strongly dependent attributes. Besides, RELIEF can also efficiently estimate continuous attributes [Kira and Rendell, 1992]. The extensions introduced in the algorithm RELIEFF <ref> [Kononenko, 1994] </ref> enable it to efficiently deal with noisy data, missing values, and multi-class problems. All these important features, together with the relatively acceptable bias described in this paper, make RELIEFF a promising measure.
Reference: [ Li and Vitanyi, 1993 ] <author> M. Li and P. Vitanyi. </author> <title> An introduction to Kolmogorov Complexity and its applications, </title> <publisher> Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: ... .. .. .. ... .. .. .. ... .. .. .. ... .. .. .. ... .. .. ... .. .. .. ... .. .. .. ... .. .. .. ... .. .. .. ... .. .. .. ... . 2.5 MDL According to the minimal description length principle <ref> [Rissanen, 1983; Li and Vitanyi, 1993] </ref> the problem of selecting the best attribute can be stated as the problem of selecting the most compressive attribute. Let us have the following transmission problem.
Reference: [ Mantaras, 1989 ] <author> R.L. Mantaras. </author> <title> ID3 Revisited: A distance based criterion for attribute selection. </title> <booktitle> Proc. Int. Symp. Methodologies for Intelligent Systems, </booktitle> <address> Charlotte, North Carolina, U.S.A., </address> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: One possible approach to this problem in top down induction of decision trees is the construction of binary decision trees. The other approach is to introduce a kind of normalization into the selection criterion, such as gain-ratio [Quinlan, 1986] and distance measure <ref> [Mantaras, 1989] </ref>. Recently, White and Liu [1994] showed that, even with normalization, information based heuristics still tend to overestimate the attributes with more values. Their experiments indicated that 2 and G statistics are superior estimation techniques to information gain, gain ratio, and distance measure.
Reference: [ Michie, 1989 ] <author> D. Michie. </author> <title> Personal Models of Rationality. </title> <journal> J. of Statistical Planning and Inference, Special Issue on Foundations and Philosophy of Probability and Statistics, </journal> <volume> 21, </volume> <year> 1989. </year>
Reference-contexts: We adopted and extended their scenario in order to verify results of methods tested by White and Liu and to test also some other well known measures: gini-index [Breiman et al., 1984], J -measure [Smyth and Goodman, 1990], the weight of evidence <ref> [Michie, 1989] </ref>, and relevance [Baim, 1988]. Besides, we developed and tested also one new selection measure based on the minimum description length (MDL) principle and a measure derived from the algorithm RELIEF [Kira and Rendell, 1992]. In the following we describe all selection measures, the experimental scenario and results. <p> the information content of the rule which is appropriate for selecting a single attribute-value for rule generation: J j = p :j i p ijj A straightforward generalization gives the attribute selection measure: J = j Another selection measure related to information theory is the average absolute weight of evidence <ref> [Michie, 1989] </ref>. It is based on plausibility which is an alternative to entropy from the information theory. Let odds = p=(1 p).
Reference: [ Press et al., 1988 ] <author> W.H. Press, S.A. Teukolsky, W.T. Vettering, </author> <title> B.P. Flannery. Numerical recipes in C: </title> <booktitle> The art of scientific computing, </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference: [ Quinlan, 1986 ] <author> R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: However, various heuristics tend to overestimate the multi-valued attributes. One possible approach to this problem in top down induction of decision trees is the construction of binary decision trees. The other approach is to introduce a kind of normalization into the selection criterion, such as gain-ratio <ref> [Quinlan, 1986] </ref> and distance measure [Mantaras, 1989]. Recently, White and Liu [1994] showed that, even with normalization, information based heuristics still tend to overestimate the attributes with more values. Their experiments indicated that 2 and G statistics are superior estimation techniques to information gain, gain ratio, and distance measure.
Reference: [ Rissanen, 1983 ] <author> J.R.Rissanen J. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11 </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: ... .. .. .. ... .. .. .. ... .. .. .. ... .. .. .. ... .. .. ... .. .. .. ... .. .. .. ... .. .. .. ... .. .. .. ... .. .. .. ... . 2.5 MDL According to the minimal description length principle <ref> [Rissanen, 1983; Li and Vitanyi, 1993] </ref> the problem of selecting the best attribute can be stated as the problem of selecting the most compressive attribute. Let us have the following transmission problem.
Reference: [ Smyth and Goodman, 1990 ] <author> P. Smyth and R.M. Good-man. </author> <title> Rule induction using information theory. </title> <editor> In. G.Piatetsky-Shapiro and W.Frawley (eds.) </editor> <title> Knowledge Discovery in Databases, </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: We adopted and extended their scenario in order to verify results of methods tested by White and Liu and to test also some other well known measures: gini-index [Breiman et al., 1984], J -measure <ref> [Smyth and Goodman, 1990] </ref>, the weight of evidence [Michie, 1989], and relevance [Baim, 1988]. Besides, we developed and tested also one new selection measure based on the minimum description length (MDL) principle and a measure derived from the algorithm RELIEF [Kira and Rendell, 1992].
Reference: [ White and Liu, 1994 ] <author> A.P. White and W.Z. Liu. </author> <title> Bias in information-based measures in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 321-329, </pages> <year> 1994. </year>
Reference-contexts: the object's class: Gain = H C + H A H CA = H C H CjA (1) In order to avoid the overestimation of the multivalued attributes Quinlan [1986] introduced the gain-ratio: GainR = Gain H A Mantaras [1989] defined a distance measure D that can be rewritten as <ref> [White and Liu, 1994] </ref>: 1 D = Gain H CA Smyth and Goodman [1990] introduced the J-measure for estimating the information content of the rule which is appropriate for selecting a single attribute-value for rule generation: J j = p :j i p ijj A straightforward generalization gives the attribute selection <p> Press et al. [1988] give the algorithms for evaluating the above formula. We have two statistics that are well approximated by the chi-square distribu tion with (V 1)(C 1) degrees of freedom <ref> [White and Liu, 1994] </ref>, 2 and G: X X (e ij n ij ) 2 ; e ij = n :: and G = 2n :: Gain log e 2; e = 2:7182::: (12) 2 5 10 20 40 0.0 0.4 Gain [bit] C = 2 .... .............. ................ .............. ...............
References-found: 12

