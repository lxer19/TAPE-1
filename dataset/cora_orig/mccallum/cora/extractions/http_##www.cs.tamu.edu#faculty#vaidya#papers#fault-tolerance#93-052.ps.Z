URL: http://www.cs.tamu.edu/faculty/vaidya/papers/fault-tolerance/93-052.ps.Z
Refering-URL: http://www.cs.tamu.edu/faculty/vaidya/Vaidya-ftc.html
Root-URL: http://www.cs.tamu.edu
Email: E-mail: vaidya@cs.tamu.edu  
Title: Distributed Recovery Units: An Approach for Hybrid and Adaptive Distributed Recovery  
Author: Nitin H. Vaidya 
Address: College Station, TX 77843-3112  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: Technical Report 93-052 November 1993 Abstract Traditionally, distributed recovery schemes have been designed for systems consisting of multiple recovery units. Each recovery unit (RU) resides on a single processor and it can fail and recover as a whole. This report introduces the "distributed recovery unit (DRU)" abstraction as an approach for design of "hybrid" and "adaptive" recovery schemes for distributed systems. The distributed system is viewed as a collection of DRUs, each DRU consisting of one or more RUs. This report presents a new recovery scheme based on the DRU abstraction. The proposed approach combines coordinated checkpointing with independent checkpointing and optimistic message logging to obtain a recovery scheme that can effectively trade the overhead during failure-free operation with the overhead during recovery. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. M. Chandy and L. Lamport, </author> <title> "Distributed snapshots: Determining global states in distributed systems," </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> vol. 3, </volume> <pages> pp. 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: 1 Introduction A distributed system is a collection of processes that communicate by sending messages. A number of failure recovery schemes have been designed to provide fault tolerance in distributed systems. Most of these recovery schemes can be divided into two categories: (a) coordinated checkpointing <ref> [1, 7, 14] </ref> and (b) message logging and independent checkpointing [5, 6, 12]. Coordinated checkpointing schemes require the distributed system to periodically record a consistent state on the stable storage. When a failure occurs, the system rolls back to the most recently recorded consistent state. <p> We choose the local recovery scheme to be the coordinated checkpointing scheme by Chandy and Lamport <ref> [1] </ref> and the global recovery scheme to be an optimistic message logging scheme by Johnson and Zwaenepoel [5]. The optimistic logging scheme by Strom and Yemini [12] is also an excellent choice for the global recovery scheme. <p> The following describes each of them. 3.3 Failure-free operation Checkpointing: The procedure for checkpointing is based on the chosen local recovery scheme, namely, the coordinated checkpointing scheme by Chandy amd Lamport <ref> [1] </ref>. When a DRU-leader wants to initiate a checkpoint, it increments its own checkpoint number (CN), takes a checkpoint and sends a marker message to each process in its DRU.
Reference: [2] <author> E. N. Elnozahy and W. Zwaenepoel, "Manetho: </author> <title> Transparent rollback-recovery with low overhead, limited rollback, and fast output commit," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. 41, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: The example in the previous section does not require any modifications to the local and global recovery schemes, while the new recovery schemes presented in this report require some simple modifications. 2.3 Motivation In the literature, a large number of distributed recovery schemes have been proposed (e.g. <ref> [2, 5, 6, 12] </ref>). None of the recovery schemes have been shown to be suitable for all applications. There are three parameters that may be used to evaluate the performance of a recovery scheme: 1. Overhead during normal (failure-free) operation. 2. Overhead during recovery after failure. 3.
Reference: [3] <author> J. Goldberg, I. Goldberg, and T. F. Lawrence, </author> <title> "Adaptive fault tolerance," </title> <booktitle> in IEEE Workshop on Advances in Parallel and Distributed Systems, </booktitle> <pages> pp. 127-132, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: To be able to adapt successfully, however, it is necessary to develop appropriate decision-making mechanisms (e.g. heuristics). Such heuristics have been developed and are a subject of current research [13]. It should be noted that recently Goldberg et al. <ref> [3] </ref> have also advocated the use of adaptive fault tolerance mechanisms. To illustrate the general approach outlined above, we first present a recovery scheme assuming that the membership of processes to the DRUs does not change over time (Section 3).
Reference: [4] <author> D. B. Johnson, </author> <title> "Efficient transparent optimistic rollback recovery for distributed application programs," </title> <booktitle> in Symposium on Reliable Distributed Systems, </booktitle> <pages> pp. 86-95, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: This capability is useful in automatically "fine-tuning" the recovery scheme if requirements of an application change over time. The research presented here is related to previous work by Sistla and Welch [11], Lowry et al. [8] and Johnson <ref> [4] </ref>. Section 5 presents a comparison of the past work with the research presented here. Note that the work presented here generalizes an approach presented in [13]. The proof of correctness of the algorithms presented in this report has been omitted here. <p> The distributed recovery units abstraction is used as a means for achieving this goal. The goal in [8] is to build large distributed systems such that each partition can use its recovery scheme without any modifications. Johnson <ref> [4] </ref> proposed an output-driven optimistic message logging and checkpointing scheme, in which recording of needed recovery information on stable storage is driven by the need to commit output to the outside world. Johnson allows each process to individually choose between messages logging and checkpointing, or only checkpointing.
Reference: [5] <author> D. B. Johnson and W. Zwaenepoel, </author> <title> "Recovery in distributed systems using optimistic message logging and checkpointing," </title> <journal> Journal of Algorithms, </journal> <volume> vol. 11, </volume> <pages> pp. 462-491, </pages> <month> Septem-ber </month> <year> 1990. </year>
Reference-contexts: A number of failure recovery schemes have been designed to provide fault tolerance in distributed systems. Most of these recovery schemes can be divided into two categories: (a) coordinated checkpointing [1, 7, 14] and (b) message logging and independent checkpointing <ref> [5, 6, 12] </ref>. Coordinated checkpointing schemes require the distributed system to periodically record a consistent state on the stable storage. When a failure occurs, the system rolls back to the most recently recorded consistent state. <p> The example in the previous section does not require any modifications to the local and global recovery schemes, while the new recovery schemes presented in this report require some simple modifications. 2.3 Motivation In the literature, a large number of distributed recovery schemes have been proposed (e.g. <ref> [2, 5, 6, 12] </ref>). None of the recovery schemes have been shown to be suitable for all applications. There are three parameters that may be used to evaluate the performance of a recovery scheme: 1. Overhead during normal (failure-free) operation. 2. Overhead during recovery after failure. 3. <p> We choose the local recovery scheme to be the coordinated checkpointing scheme by Chandy and Lamport [1] and the global recovery scheme to be an optimistic message logging scheme by Johnson and Zwaenepoel <ref> [5] </ref>. The optimistic logging scheme by Strom and Yemini [12] is also an excellent choice for the global recovery scheme. Our goal here is to demonstrate the utility of our approach in designing new and useful recovery schemes. We have chosen the scheme from [5] as an example for its ease <p> logging scheme by Johnson and Zwaenepoel <ref> [5] </ref>. The optimistic logging scheme by Strom and Yemini [12] is also an excellent choice for the global recovery scheme. Our goal here is to demonstrate the utility of our approach in designing new and useful recovery schemes. We have chosen the scheme from [5] as an example for its ease of understanding. The basic idea here is to implement a DRU using a (modified) coordinated check-pointing scheme such that each DRU would behave analogous to a deterministic process, as expected by the chosen global recovery scheme. <p> Each interval, called a state interval is initiated by the receipt of a message. Each state interval of a process can be identified by a unique state interval index <ref> [5] </ref>, which is incremented by one each time a message is received by the process. Each message is tagged by its send sequence number (SSN), the sender process identifier (sender-id), and identifier of the DRU (DRU-id) to which the sender process belongs. <p> The state interval index is incremented each time the process receives a message. Each message is tagged with the current state interval index of the sender process. * A dependency vector <ref> [5] </ref> which records the largest index of any state interval of each other process on which this process directly depends. * An order information buffer (in volatile memory) for intra-DRU messages received by the process. <p> In other words, if checkpoint CN1 of process P is obsolete, then checkpoint CN1 of each process in its DRU (including process Q) is also obsolete. To be able to recover from failures, it is necessary to first determine a recoverable system state. Johnson and Zwaenepoel <ref> [5] </ref> have presented algorithm FIND REC to determine the "maximum recoverable system state" provided a list of "stable" state intervals is available. We use algorithm FIND REC as well, except that the definition of "stable" state intervals in [5] needs to be modified to suit our implementation using DRUs. <p> Johnson and Zwaenepoel <ref> [5] </ref> have presented algorithm FIND REC to determine the "maximum recoverable system state" provided a list of "stable" state intervals is available. We use algorithm FIND REC as well, except that the definition of "stable" state intervals in [5] needs to be modified to suit our implementation using DRUs. To define stable state intervals, we first need to define an effective checkpoint [5]. <p> We use algorithm FIND REC as well, except that the definition of "stable" state intervals in <ref> [5] </ref> needs to be modified to suit our implementation using DRUs. To define stable state intervals, we first need to define an effective checkpoint [5]. Definition 1 The effective checkpoint for a state interval of some process P is the checkpoint on stable storage for process P with the largest state interval index * such that * [5]. Assuming the above meaning of *, we present a definition of stable state intervals. <p> To define stable state intervals, we first need to define an effective checkpoint <ref> [5] </ref>. Definition 1 The effective checkpoint for a state interval of some process P is the checkpoint on stable storage for process P with the largest state interval index * such that * [5]. Assuming the above meaning of *, we present a definition of stable state intervals. The definition is recursive due to condition 2 below. Definition 2 A state interval of process P is stable iff for * &lt; ff following is true: 1. <p> Dependency vectors are updated as described earlier to add the pseudo-dependencies. 4. Procedure is stable in Figure 2 is used to determine which state intervals are stable. Subsequently, algorithm FIND REC from <ref> [5] </ref> is performed at the stable storage to determine the maximum recoverable system state. For each process, algorithm FIND REC determines the state interval of that process that is a part of the maximum recoverable system state.
Reference: [6] <author> T. Juang and S. Venkatesan, </author> <title> "Crash recovery with little overhead," </title> <booktitle> in International Conf. Distributed Computing Systems, </booktitle> <pages> pp. 454-461, </pages> <year> 1991. </year> <month> 23 </month>
Reference-contexts: A number of failure recovery schemes have been designed to provide fault tolerance in distributed systems. Most of these recovery schemes can be divided into two categories: (a) coordinated checkpointing [1, 7, 14] and (b) message logging and independent checkpointing <ref> [5, 6, 12] </ref>. Coordinated checkpointing schemes require the distributed system to periodically record a consistent state on the stable storage. When a failure occurs, the system rolls back to the most recently recorded consistent state. <p> The example in the previous section does not require any modifications to the local and global recovery schemes, while the new recovery schemes presented in this report require some simple modifications. 2.3 Motivation In the literature, a large number of distributed recovery schemes have been proposed (e.g. <ref> [2, 5, 6, 12] </ref>). None of the recovery schemes have been shown to be suitable for all applications. There are three parameters that may be used to evaluate the performance of a recovery scheme: 1. Overhead during normal (failure-free) operation. 2. Overhead during recovery after failure. 3.
Reference: [7] <author> R. Koo and S. Toueg, </author> <title> "Checkpointing and rollback-recovery for distributed systems," </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> vol. 13, </volume> <pages> pp. 23-31, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: 1 Introduction A distributed system is a collection of processes that communicate by sending messages. A number of failure recovery schemes have been designed to provide fault tolerance in distributed systems. Most of these recovery schemes can be divided into two categories: (a) coordinated checkpointing <ref> [1, 7, 14] </ref> and (b) message logging and independent checkpointing [5, 6, 12]. Coordinated checkpointing schemes require the distributed system to periodically record a consistent state on the stable storage. When a failure occurs, the system rolls back to the most recently recorded consistent state.
Reference: [8] <author> A. Lowry, J. R. Russell, and A. P. Goldberg, </author> <title> "Optimistic failure recovery for very large networks," </title> <booktitle> in Symposium on Reliable Distributed Systems, </booktitle> <pages> pp. 66-75, </pages> <year> 1991. </year>
Reference-contexts: This capability is useful in automatically "fine-tuning" the recovery scheme if requirements of an application change over time. The research presented here is related to previous work by Sistla and Welch [11], Lowry et al. <ref> [8] </ref> and Johnson [4]. Section 5 presents a comparison of the past work with the research presented here. Note that the work presented here generalizes an approach presented in [13]. The proof of correctness of the algorithms presented in this report has been omitted here. <p> The messages passing between the clusters are treated as input-output messages. As shown in Section 2, this scheme can be obtained using our approach as well. However, in general, our approach does not require messages between DRUs to be treated as input-output. Lowry et al. <ref> [8] </ref> suggested an optimistic scheme for systems partitioned into clusters. This is an improvement over Sistla and Welch [11] where all messages across the clusters are treated pessimistically. <p> Lowry et al. design interfaces between clusters that allow (i) each cluster to use different recovery schemes and (ii) the messages between two clusters to be logged optimistically. Our work differs from <ref> [8] </ref> in its goals and the approach used. Our goal is to design a single recovery scheme for a distributed system that can adapt to the requirements of an application. The distributed recovery units abstraction is used as a means for achieving this goal. The goal in [8] is to build <p> work differs from <ref> [8] </ref> in its goals and the approach used. Our goal is to design a single recovery scheme for a distributed system that can adapt to the requirements of an application. The distributed recovery units abstraction is used as a means for achieving this goal. The goal in [8] is to build large distributed systems such that each partition can use its recovery scheme without any modifications.
Reference: [9] <author> D. L. Russell, </author> <title> "State restoration in systems of communicating processes," </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> vol. 6, </volume> <pages> pp. 183-194, </pages> <month> March </month> <year> 1980. </year>
Reference-contexts: The recovery scheme for this system requires that (i) each process takes a checkpoint just before sending a message to another process, and (ii) each process logs a received message before using it. This systems can recover from a failure without the domino-effect <ref> [9] </ref>. 3 Now consider a system consisting of multiple DRUs illustrated in Figure 1 (b).
Reference: [10] <author> R. D. Schlichting and F. B. Schneider, </author> <title> "Fail-stop processors: An approach to designing fault-tolerant computing systems," </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> vol. 1, </volume> <pages> pp. 222-238, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: Each process is assumed to be deterministic, i.e., the final state of a process depends only on its initial state and on the content and order of messages it receives. Each process is assumed to be fail-stop <ref> [10] </ref>.
Reference: [11] <author> A. P. Sistla and J. L. Welch, </author> <title> "Efficient distributed recovery using message logging," </title> <booktitle> in Proc. ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pp. 223-238, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: This capability is useful in automatically "fine-tuning" the recovery scheme if requirements of an application change over time. The research presented here is related to previous work by Sistla and Welch <ref> [11] </ref>, Lowry et al. [8] and Johnson [4]. Section 5 presents a comparison of the past work with the research presented here. Note that the work presented here generalizes an approach presented in [13]. The proof of correctness of the algorithms presented in this report has been omitted here. <p> (ii) the general approach for design of recovery schemes based on the DRU abstraction, and (iii) the motivation for the proposed approach. 2.1 A Simple Example The simple example in this section leads to the design of a recovery scheme identical to the partitioning approach proposed by Sistla and Welch <ref> [11] </ref>. Consider a system consisting of multiple processes illustrated in Figure 1 (a). The recovery scheme for this system requires that (i) each process takes a checkpoint just before sending a message to another process, and (ii) each process logs a received message before using it. <p> A message sent by a process while in a recoverable state is committable. Thus, above recovery scheme requires that a message from one DRU to another DRU is not delivered unless it is committable. This scheme is identical to the partitioning approach proposed by Sistla and Welch <ref> [11] </ref> for large distributed system. However, our approach differs from [11] in that the DRU abstraction also facilitates design of recovery schemes that do not require messages between DRUs to be committable before they are delivered. <p> Thus, above recovery scheme requires that a message from one DRU to another DRU is not delivered unless it is committable. This scheme is identical to the partitioning approach proposed by Sistla and Welch <ref> [11] </ref> for large distributed system. However, our approach differs from [11] in that the DRU abstraction also facilitates design of recovery schemes that do not require messages between DRUs to be committable before they are delivered. <p> In other words, if a DRU-merge (DRU-fork) was in progress when a failure occurred, the protocol must be re-initiated after recovery. 21 5 Comparison with other relevant schemes Sistla and Welch <ref> [11] </ref> propose an approach that partitions a system into multiple clusters. The messages passing between the clusters are treated as input-output messages. As shown in Section 2, this scheme can be obtained using our approach as well. <p> However, in general, our approach does not require messages between DRUs to be treated as input-output. Lowry et al. [8] suggested an optimistic scheme for systems partitioned into clusters. This is an improvement over Sistla and Welch <ref> [11] </ref> where all messages across the clusters are treated pessimistically. Lowry et al. design interfaces between clusters that allow (i) each cluster to use different recovery schemes and (ii) the messages between two clusters to be logged optimistically. Our work differs from [8] in its goals and the approach used.
Reference: [12] <author> R. E. Strom and S. A. Yemini, </author> <title> "Optimistic recovery in distributed systems," </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> vol. 3, </volume> <pages> pp. 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: A number of failure recovery schemes have been designed to provide fault tolerance in distributed systems. Most of these recovery schemes can be divided into two categories: (a) coordinated checkpointing [1, 7, 14] and (b) message logging and independent checkpointing <ref> [5, 6, 12] </ref>. Coordinated checkpointing schemes require the distributed system to periodically record a consistent state on the stable storage. When a failure occurs, the system rolls back to the most recently recorded consistent state. <p> Each recovery unit can fail and recover as a whole. Each recovery unit is often simply a process or a single machine. Many recovery schemes often assume that each recovery unit is deterministic. The typical underlying assumption is that the recovery unit resides on a single processor <ref> [12] </ref>. This report presents a new approach that extends the concept of a recovery unit to a distributed environment and shows utility of the new approach in designing new recovery schemes. For brevity, we use the term "process" to mean "recovery unit". <p> The example in the previous section does not require any modifications to the local and global recovery schemes, while the new recovery schemes presented in this report require some simple modifications. 2.3 Motivation In the literature, a large number of distributed recovery schemes have been proposed (e.g. <ref> [2, 5, 6, 12] </ref>). None of the recovery schemes have been shown to be suitable for all applications. There are three parameters that may be used to evaluate the performance of a recovery scheme: 1. Overhead during normal (failure-free) operation. 2. Overhead during recovery after failure. 3. <p> We choose the local recovery scheme to be the coordinated checkpointing scheme by Chandy and Lamport [1] and the global recovery scheme to be an optimistic message logging scheme by Johnson and Zwaenepoel [5]. The optimistic logging scheme by Strom and Yemini <ref> [12] </ref> is also an excellent choice for the global recovery scheme. Our goal here is to demonstrate the utility of our approach in designing new and useful recovery schemes. We have chosen the scheme from [5] as an example for its ease of understanding. <p> The following additional data structures are maintained by each process. * A version-number. The version number is initialized to one. It may sometimes be incremented during a DRU-fork or DRU-merge operation. Note that the version-number is different from incarnation numbers used in many algorithms <ref> [12] </ref>. * A DRU-id-list, containing DRU identifiers of all DRUs that this process has been in. (As time progresses, it is possible to delete older entries in the DRU-id-list.
Reference: [13] <author> N. H. Vaidya, </author> <title> "Dynamic cluster-based recovery: Pessimistic and optimistic schemes (preliminary version)," </title> <type> Tech. Rep. 93-027, </type> <institution> Comp. Sc. Dept., Texas A&M Univ., </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The research presented here is related to previous work by Sistla and Welch [11], Lowry et al. [8] and Johnson [4]. Section 5 presents a comparison of the past work with the research presented here. Note that the work presented here generalizes an approach presented in <ref> [13] </ref>. The proof of correctness of the algorithms presented in this report has been omitted here. <p> Alternately, the function of a DRU-leader can be performed by one of the user processes in the DRU, or the DRU-leader functionality can be distributed to all processes within a DRU <ref> [13] </ref>. The choice made for implementing DRU-leader function affects the recovery scheme. <p> The distributed recovery unit abstraction provides a mechanism to introduce adaptive behavior into the recovery schemes. To be able to adapt successfully, however, it is necessary to develop appropriate decision-making mechanisms (e.g. heuristics). Such heuristics have been developed and are a subject of current research <ref> [13] </ref>. It should be noted that recently Goldberg et al. [3] have also advocated the use of adaptive fault tolerance mechanisms. To illustrate the general approach outlined above, we first present a recovery scheme assuming that the membership of processes to the DRUs does not change over time (Section 3). <p> Many different mechanisms can be used for maintaining unique DRU identifiers. This section presents one such mechanism and <ref> [13] </ref> presents another. 16 Identifiers of the processes are assumed to form a linear order. Given this assumption, identifier of a DRU is given by a tuple (proc-id , vers-num) where proc-id is the largest identifier of any process in that DRU and vers-num is the version-number of that process.
Reference: [14] <author> Y. Wang and W. K. Fuchs, </author> <title> "Lazy checkpoint coordination for bounding rollback propagation," </title> <booktitle> in Symposium on Reliable Distroibuted Systems, </booktitle> <pages> pp. 78-85, </pages> <month> October </month> <year> 1993. </year> <month> 24 </month>
Reference-contexts: 1 Introduction A distributed system is a collection of processes that communicate by sending messages. A number of failure recovery schemes have been designed to provide fault tolerance in distributed systems. Most of these recovery schemes can be divided into two categories: (a) coordinated checkpointing <ref> [1, 7, 14] </ref> and (b) message logging and independent checkpointing [5, 6, 12]. Coordinated checkpointing schemes require the distributed system to periodically record a consistent state on the stable storage. When a failure occurs, the system rolls back to the most recently recorded consistent state.
References-found: 14

