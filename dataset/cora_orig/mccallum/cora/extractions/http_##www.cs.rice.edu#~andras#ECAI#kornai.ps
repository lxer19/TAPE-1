URL: http://www.cs.rice.edu/~andras/ECAI/kornai.ps
Refering-URL: http://www.cs.rice.edu/~andras/ECAI/kornai.html
Root-URL: 
Email: kornai@almaden.ibm.com  
Title: Vectorized Finite State Automata  
Author: Andras Kornai 
Address: 650 Harry Road, San Jose, CA 95120  
Affiliation: IBM Almaden Research Center  
Abstract: We present a technique of finite state parsing based on vectorization and describe the application of this technique to a well-known problem of natural language processing, that of extracting relational information from English text. We define Vectorized Finite State Automata, the theoretical model behind the applied system, and discuss their significance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Steven Bird, </author> <title> Computational Phonology, </title> <publisher> Cambridge Univer sity Press (1995) </publisher>
Reference-contexts: G 0 (1) provides a reasonable model of the unary features (privative oppositions) used in phonological theory, and G 0 (0) is perhaps a good model of the monotonic features sometimes found in computational phonology <ref> [1] </ref>, because the failure symbol can only be turned on once in the course of a derivation. In a phonological setting we would also find gradual oppositions such as vowel height, which are best modelled as chains (fully ordered sets) of ground values.
Reference: [2] <author> Garrett Birkhoff and Thomas Bartee, </author> <title> Modern applied algebra McGraw-Hill, </title> <address> New York (1970) </address>
Reference-contexts: Is it really necessary to move away from the boolean restriction? Outside the domain of names, all partial orders that had some practical use seem to lead to a notion of (co)unification that satisfies the usual distributivity axioms, meaning that Stone's theorem (see e.g. <ref> [2] </ref>) guarantees that a feature decomposition will exist. So for the most part fixed groups of RVG flags can be used for encoding multi-valued features with very little overhead.
Reference: [3] <author> Glenn David Blank, </author> <title> `A finite and real-time processor for natu ral language', </title> <booktitle> Communications of the ACM 32(10) 1174-1189 (1989) </booktitle>
Reference-contexts: Section 2 presents an overview of NewsMonitor, a system extracting relational information from English text, with particular emphasis on the VFSA pattern matching engine around which NewsMonitor is built. Section 3 provides the formal definition of VFSA, discusses their properties, and compares them to Register Vector Grammars (RVGs) <ref> [3] </ref>. The theoretical implications of the work are discussed in Section 4. Broadly speaking, there are three ways vectorization can enter the standard setup for finite state language modeling. <p> the same feature analysis, and by overloading the flags the clarity and purpose of this analysis would be lost. 3.3 VFSA and RVGs Though VFSA are equivalent in generative capacity to FSA, their closest conceptual relatives are not the standard finite automata but the Register Vectors Grammars (RVGs) introduced in <ref> [3] </ref> by Blank, who discusses how a number of important syntactic phenomena including wh-movement and limited self-embedding can be analyzed by keeping partial results around. The central computational mechanism of VFSA, namely the use of asymmetric (overwriting) operations guided by template matching is already present in [3] (and possibly already in <p> Grammars (RVGs) introduced in <ref> [3] </ref> by Blank, who discusses how a number of important syntactic phenomena including wh-movement and limited self-embedding can be analyzed by keeping partial results around. The central computational mechanism of VFSA, namely the use of asymmetric (overwriting) operations guided by template matching is already present in [3] (and possibly already in the unpublished work of Kunst cited there). We did not follow Blank in his use of explicit side-effecting (called actions there), and we use X features for lexical categories. <p> A single register encourages a local view and a temporal metaphor of updating the state, while the simultaneous use of several vectors encourages a global view and the spatial metaphor of spreading information across states. For RVGs, the kind of macro expansion system described in <ref> [3] </ref> makes the best sense as rule compiler, since the goal there is to collapse more complex state changes into a single unit. In contrast, the pattern matcher used in NewsMonitor employs awk/sed style rules for manipulating the state space. Vectorized Finite State Automata 40 A.
Reference: [4] <author> Colin Cherry, Morris Halle, and Roman Jakobson, </author> <title> `Toward the logical description of languages in their phonemic aspect', </title> <booktitle> Language 29 34-46 (1953) </booktitle>
Reference: [5] <author> Noam Chomsky and Morris Halle, </author> <title> The Sound Pattern of En glish, </title> <publisher> Harper & Row, </publisher> <address> New York (1968) </address>
Reference: [6] <author> Ray S. Jackendoff, </author> <title> X Syntax: A Study of Phrase Structure, </title> <publisher> MIT Press, </publisher> <address> Cambridge MA 1977 </address>
Reference-contexts: results are shipped out to the relational database by inspecting the status (and associated confidences) of the Person, Company, and Title flags, and locating the left and right boundary markers associated with these. * The system encourages a highly lexical style of analysis, with ` X features' of lexical categories <ref> [6] </ref> applied throughout. Over 70 categories are used, and the majority of these are highly specific to the task. For example, the lexical flag "company class" would be set for entries such as Incorporated or Limited but not for Telecom.
Reference: [7] <author> Paul F. Jacobs and Lisa F. Rau, `SCISOR: </author> <title> extracting in formation from on-line news', </title> <booktitle> Communications of the ACM 33(10) 88-97 (1990) </booktitle>
Reference-contexts: To put these figures in perspective, note that major system such as RelationalText never progressed beyond 60% precision at 30% recall. GE's SCISOR <ref> [7] </ref> had at the time 80-90% combined recall and precision, a result that has not been significantly improved upon in the past five years by any system performing detail parsing.
Reference: [8] <author> Laszlo Kalman and Andras Kornai, </author> <title> `Pattern matching: a finite-state approach to parsing and generation', </title> <institution> ms, Institute of Linguistics, Hungarian Academy of Sciences (1985) </institution>
Reference: [9] <author> Ronald M. Kaplan and Martin Kay, </author> <title> `Regular models of phonological rule systems', </title> <note> Computational Linguistics 20(3) 331-378 (1994) </note>
Reference-contexts: The theoretical implications of the work are discussed in Section 4. Broadly speaking, there are three ways vectorization can enter the standard setup for finite state language modeling. First, the alphabet itself can be composed of n-tuples, a conceptualization particularly useful for n-ary regular relations and n-way finite automata <ref> [9] </ref>. Second, the alphabet symbols in a single dimension can be thought of as vectors composed of (binary) features, as is commonly done in Prague-style and in generative phonology [4],[5]. Third, the state space itself can be conceptualized as a vector space, as in RVGs.
Reference: [10] <author> Andras Kornai, </author> <title> `Natural Languages and the Chomsky Hier archy', </title> <editor> in: M. King (ed): </editor> <booktitle> Proceedings of the 2nd European Conference of the Association for Computational Linguistics 1-7 (1985) </booktitle>
Reference-contexts: English sentences, as demonstrated by the RVG work, or even binary arithmetic, as the example of Section 1 shows, but information extraction is a particularly challenging domain where, as the high performance of NewsMonitor shows, the techniques appropriate for sentential syntax are neither sufficient nor necessary, In earlier work ([8], <ref> [10] </ref>) we argued that the storage device required for keeping the dependency information between the spoken/parsed and the yet unspoken/unparsed parts of a sentence need not be structured as a stack of potentially unbounded depth, but can in fact be limited to hold only a few data structures of the size
Reference: [11] <author> Andras Kornai, </author> <title> `The generative power of feature geometry', </title> <note> Annals of Mathematics and Artificial Intelligence 8 37-46 (1993) </note>
Reference-contexts: valued) functions called flags or features. (The term `flag' is more common in syntax and computer science, the term `feature' is more common in phonology here we use them interchangeably.) In phonology, the goal of such an embedding or feature decomposition is to achieve a high degree of notational compactness <ref> [11] </ref>. Here we show that notational compactness is more than an abstract goal, inasmuch as feature decomposition, properly implemented, can lead to appreciable savings in memory requirements.
Reference: [12] <author> Andras Kornai and Zsolt Tuza, `Narrowness, </author> <title> pathwidth, and their application in natural language processing', </title> <note> Discrete Applied Mathematics 36 87-92 (1992) </note>
Reference-contexts: At the sentence level, it is always possible to update the array of vectors in a single left-to-right pass, storing only a handful of vectors (in a fixed set of registers) at any given time <ref> [12] </ref>. According to the measure of complexity proposed here, this means constant dimensionality i.e. real-time operation. At the text level, the matcher requires dimensionality linear in the input length, indicating performance linear in the length of the input text. This is what we actually observe.
Reference: [13] <author> David McDonald, </author> <title> `Recovering relational information from text', </title> <note> Brattle Software Technical Report (1989) </note>
Reference-contexts: In addition to the Dow Jones ticker tape and other real-time indicators of financial performance, the system also contained a batch component called RelationalText <ref> [13] </ref>, which analyzed the Wall Street Journal for relational information deemed highly significant for portfolio managers: who is where? and Vectorized Finite State Automata 37 A.
Reference: [14] <author> Harold L. Somers, </author> <title> Valency and Case in Computational Lin guistics, Edinburgh University Press, 1987 Vectorized Finite State Automata 41 A. </title> <publisher> Kornai </publisher>
Reference-contexts: As the example in Section 1 shows, the VFSA architecture does not make it impossible to perform arithmetic operations on numerical quantities, but it clearly discourages such efforts. * The system encourages a case grammar or valence-style analysis <ref> [14] </ref>, with separate flags for each slot that can be filled.
References-found: 14

