URL: http://www.cs.monash.edu.au/~korb/paper.ps.Z
Refering-URL: http://www.cs.monash.edu.au/~korb/
Root-URL: 
Email: fjrn,korbg@cs.monash.edu.au  
Title: The MML Evolution of Classification Graphs  
Author: Julian Neil and Kevin B. Korb 
Keyword: Machine learning, MML induction, classification learning, MML genetic algorithms, concept formation, decision trees, C4.5.  
Date: January 15, 1996  
Address: Clayton, Victoria 3168 Australia  
Affiliation: Department of Computer Science Monash University  
Abstract: Minimum encoding induction (MML and MDL) is well developed theoretically and is currently being employed in two central areas of investigation in machine learning|namely, classification learning and the learning of causal networks. MML and MDL offer important tools for the evaluation of models, but offer little direct help in the problem of how to conduct the search through the model space. Here we combine MML with one of the more powerful search techniques available to machine learning: genetic algorithms (GAs). We develop a genetic algorithm to search the space of classification (decision) graphs using an MML-based fitness criterion and establish its effectiveness across a range of test cases from the UC Irvine machine learning archive. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W.L. Buntine. </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> University of Sydney, </institution> <year> 1991. </year>
Reference-contexts: More recently, Oliver has been using sets of classification trees (what he calls "fanning" trees) to make classification predictions based upon the weighted predictions of multiple trees ([20], following Buntine <ref> [1] </ref>). It will be no surprise to Bayesians that the prediction based upon multiple models (giving an expected value) improves upon the performance of the predictions of any one classification tree.
Reference: [2] <author> W.L. Buntine. </author> <title> Learning classification trees. </title> <journal> Statistics and Computing, </journal> <volume> 2 </volume> <pages> 63-73, </pages> <year> 1992. </year>
Reference-contexts: Most iterative approaches to generating classification trees also have a pruning phase, where a grown tree is selectively pruned back to find some balance between tree size and predictive performance <ref> [25, 2, 19, 20] </ref>. Although node mutation performs some pruning automatically (by occasionally turning split nodes into leaves), introducing more intelligent 18 pruning in a genetic operator might keep the search from exploring highly unfit areas.
Reference: [3] <author> G.J. Chaitin. </author> <title> On the length of programs for computing finite sequences. </title> <journal> Journal of the ACM, </journal> <volume> 13 </volume> <pages> 547-549, </pages> <year> 1966. </year>
Reference-contexts: errors together with whatever structure truly underlies the data obtained, there typically lies a minimum point which MML induction aims at finding: the hypothesis which reports as much of the underlying structure as can be inferred from the available data, without also reporting phantoms arising from noise in the data <ref> [29, 3] </ref>.
Reference: [4] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-283, </pages> <year> 1988. </year>
Reference-contexts: Rules that are allocated no training items are discarded, leaving a set of rules for the next generation. The reported results are favorable when compared to two other classification induction algorithms, CN2 <ref> [4] </ref> and NewID3.
Reference: [5] <author> K. De Jong. </author> <title> Genetic algorithms: A 10 year perspective. </title> <editor> In J.J. Grefenstette, editor, </editor> <booktitle> Proceedings of the First International Conference on Genetic Algorithms, </booktitle> <pages> pages 169-177, </pages> <address> Hillsdale, NJ, 1985. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: We have found it more appropriate to adopt a more complex representation than the standard bit string, and to design meaningful operators to act directly on that representation. This idea has continued to reappear as evolutionary algorithms are applied to more complex problems <ref> [5, 14, 16] </ref>. Initially, we attempted to design a linear encoding for classification graphs by traversing the graph in a particular manner (e.g., depth first) and encoding each node with its properties, including connectivity, in order.
Reference: [6] <author> D.L. Dowe and N. Krusel. </author> <title> Decision tree models of bushfire activity. </title> <journal> AI Applications, </journal> <volume> 8(3) </volume> <pages> 71-72, </pages> <year> 1994. </year>
Reference-contexts: They have also been applied to a wide variety of practical problems in the assessment of models, such as bushfire prediction <ref> [6] </ref> and the classification of seals into species [31], and have recently been employed in the important problem of learning causal/Bayesian network structures [15].
Reference: [7] <author> M.P. Georgeff and C.S. Wallace. </author> <title> A general selection criterion for inductive inference. </title> <booktitle> In European Conf. on Artificial Intelligence, </booktitle> <pages> pages 473-482, </pages> <address> New York, NY, 1984. </address> <publisher> Elsevier Science Publishers. </publisher>
Reference-contexts: the operators available to modify graphs. 3 1.3 The MML Induction Principle Wallace's Minimum Message Length induction principle asserts (a) that the best explanation of a given set of data is that which is the shortest and (b) that the best hypothesis is that which participates in the best explanation <ref> [7] </ref>.
Reference: [8] <author> D.E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Most of the theory behind genetic algorithms is based on the study of simple genetic algorithms with linear genetic representations of fixed lengths <ref> [10, 8] </ref>. To enforce such a scheme for more complex structures requires the use of specialized genetic operators, designed specifically to overcome the limitations of linear representations. Furthermore, many problems require variable length chromosomes.
Reference: [9] <author> P.G. Greene and S.F. Smith. </author> <title> Competition-based induction of decision models from examples. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 229-257, </pages> <year> 1993. </year>
Reference-contexts: One effort in that direction, however, is Greene and Smith's COGIN (Coverage-based Genetic Induction) <ref> [9] </ref>. COGIN uses a population of fixed-length rules, where each rule gives a classification for some Boolean combination of attribute values.
Reference: [10] <author> J.H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1975. </year>
Reference-contexts: The ff coefficient is also altered during the search to minimize the message length. In this paper, all message length calculations were made with ff = 1. 1.4 Genetic Algorithms Genetic Algorithms (GAs) are a search technique modeled on biological evolution <ref> [10] </ref>. They use a simplified model of genetics and reproduction to generate a (typically) large and various population of individual chromosomes which encode solutions to a target problem. Selection pressure is applied based upon a fitness score reporting how well chromosomes do in solving the target problem. <p> Mutation randomly alters a gene in the child. 5 A property of genetic algorithms much emphasized by John Holland is their ability to direct the search by keeping around clusters of genes which have proved fruitful through succeeding generations in "schemata " (see his discussion of the schema theorem, <ref> [10] </ref>). This can be thought of as a way of usefully biasing the search toward regions of the search space which represent the most beneficial schemata. This (briefly) describes the usual GA in applications during the 1980s and 1990s. <p> Most of the theory behind genetic algorithms is based on the study of simple genetic algorithms with linear genetic representations of fixed lengths <ref> [10, 8] </ref>. To enforce such a scheme for more complex structures requires the use of specialized genetic operators, designed specifically to overcome the limitations of linear representations. Furthermore, many problems require variable length chromosomes.
Reference: [11] <author> C. Howson and P. Urbach. </author> <title> Scientific Reasoning: The Bayesian Approach. Open Court, </title> <address> La Salle, IL, </address> <note> second edition, </note> <year> 1993. </year>
Reference-contexts: That is, it is a simple consequence of the rule of Bayesian conditional-ization <ref> [11] </ref> that the posterior probability of h given e is proportional to I (hje). It is both interesting and important that in constructing a message which minimizes encoding length there is a trade-off between the complexity of hypotheses and how well the hypothesis recapitulates the data.
Reference: [12] <author> D. Hume. </author> <title> A Treatise of Human Nature (1739-40). </title> <publisher> Oxford University Press, </publisher> <year> 1978. </year>
Reference: [13] <author> L. </author> <type> Hunter. </type> <institution> Monash University Computer Science Seminar, </institution> <year> 1995. </year>
Reference-contexts: If any three adjacent bits are on, the binary classification is positive <ref> [13] </ref>. In analysing test results we plotted the mean, maximum and minimum message lengths against generation number (Figure 6). The standard deviation in the message length at each generation provides a crude measure of diversity in the population.
Reference: [14] <editor> J. Koza. </editor> <booktitle> Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: We have found it more appropriate to adopt a more complex representation than the standard bit string, and to design meaningful operators to act directly on that representation. This idea has continued to reappear as evolutionary algorithms are applied to more complex problems <ref> [5, 14, 16] </ref>. Initially, we attempted to design a linear encoding for classification graphs by traversing the graph in a particular manner (e.g., depth first) and encoding each node with its properties, including connectivity, in order.
Reference: [15] <author> W. Lam and F. Bacchus. </author> <title> Learning Bayesian belief networks. </title> <journal> Computational Intelligence, </journal> <volume> 10 </volume> <pages> 269-696, </pages> <year> 1994. </year>
Reference-contexts: They have also been applied to a wide variety of practical problems in the assessment of models, such as bushfire prediction [6] and the classification of seals into species [31], and have recently been employed in the important problem of learning causal/Bayesian network structures <ref> [15] </ref>.
Reference: [16] <author> Z. Michalewicz. </author> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <note> second edition, 1994. 6 Our thanks for the assistance of Jon Oliver in the preparation of this paper. 19 </note>
Reference-contexts: We have found it more appropriate to adopt a more complex representation than the standard bit string, and to design meaningful operators to act directly on that representation. This idea has continued to reappear as evolutionary algorithms are applied to more complex problems <ref> [5, 14, 16] </ref>. Initially, we attempted to design a linear encoding for classification graphs by traversing the graph in a particular manner (e.g., depth first) and encoding each node with its properties, including connectivity, in order.
Reference: [17] <author> J.J. Oliver. </author> <title> Decision graphs | an extension of decision trees. </title> <booktitle> In 4t Int. Conf. Artificial Intelligence and Statistics, </booktitle> <year> 1993. </year>
Reference-contexts: Furthermore, these methods have been applied successfully in the most common test domain for machine learning methods, namely in concept formation and classification learning, with MML being used to learn classification trees (decision trees) [33] and their generalization to classification graphs <ref> [17] </ref>, which are specialized directed acyclic graphs. 1 Although MML is quite effective in this evaluative role, the assessment of hypotheses given some data, it is mute on the question of how one should best generate hypotheses deemed worth assessing. <p> In an example from Oliver <ref> [17] </ref> in Figure 1 the tree structure forces the subtree representing the disjunct (C ^ D) to be duplicated. <p> Early work on generating classification graphs constrained the topology to a fixed form [28]. Oliver and Wallace <ref> [17] </ref> extended Wallace's [33] work on classification trees by applying the Minimum Message Length Principle (MMLP, described below) [31, 32] to the problem of growing classification graphs, using greedy search on the operators available to modify graphs. 3 1.3 The MML Induction Principle Wallace's Minimum Message Length induction principle asserts (a) <p> of the complexity of the model and overfitting has to be avoided by such means as pruning back branches after a tree has been completed [24]. 1.3.1 Using MML to Grow Classification Graphs In order to evaluate classification graphs, Oliver and Wallace invented an MML coding scheme for such graphs <ref> [17] </ref>. The first part of his message transmits the graph itself. This is done by traversing the graph node by node, ensuring that all parents of a node are transmitted before the node is itself sent.
Reference: [18] <author> J.J. Oliver and R.A. Baxter. </author> <title> MML and Bayesianism: Similarities and differences. </title> <type> Technical Report Technical Report 94/206, </type> <institution> Computer Science, Monash University, </institution> <year> 1994. </year>
Reference-contexts: of information-theoretic measures to evaluate models of data by ranking them according to their posterior probabilities is well-grounded theoretically in the Minimum Message Length (MML) induction method of Wallace [32] and in the Minimum Description Length (MDL) method of Rissanen [27], and are legitimately viewed as variations on Bayesian induction <ref> [18] </ref>. They have also been applied to a wide variety of practical problems in the assessment of models, such as bushfire prediction [6] and the classification of seals into species [31], and have recently been employed in the important problem of learning causal/Bayesian network structures [15].
Reference: [19] <author> J.J. Oliver and D.J. </author> <title> Hand. On pruning and averaging decision trees. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Workshop, </booktitle> <pages> pages 430-437, </pages> <year> 1995. </year>
Reference-contexts: Most iterative approaches to generating classification trees also have a pruning phase, where a grown tree is selectively pruned back to find some balance between tree size and predictive performance <ref> [25, 2, 19, 20] </ref>. Although node mutation performs some pruning automatically (by occasionally turning split nodes into leaves), introducing more intelligent 18 pruning in a genetic operator might keep the search from exploring highly unfit areas.
Reference: [20] <author> J.J. Oliver and D.J. </author> <title> Hand. Averaging over decision trees. </title> <journal> Journal of Classification, </journal> <note> To appear in 1996. An extended version is available as Technical Report TR 5-94, </note> <institution> Dept. of Statistics, Open University, Walton Hall, Milton Keynes, MK7 6AA, UK. </institution>
Reference-contexts: But, most obviously, using multiple graphs selected from the best in the population to produce predictions weighted by their posterior probability (i.e., Oliver and Hand's "fanning" procedure) will almost certainly improve predictive performance <ref> [20] </ref>. Most iterative approaches to generating classification trees also have a pruning phase, where a grown tree is selectively pruned back to find some balance between tree size and predictive performance [25, 2, 19, 20]. <p> Most iterative approaches to generating classification trees also have a pruning phase, where a grown tree is selectively pruned back to find some balance between tree size and predictive performance <ref> [25, 2, 19, 20] </ref>. Although node mutation performs some pruning automatically (by occasionally turning split nodes into leaves), introducing more intelligent 18 pruning in a genetic operator might keep the search from exploring highly unfit areas.
Reference: [21] <author> J.J. Oliver and C.S. Wallace. </author> <title> Inferring decision graphs. </title> <booktitle> In Proceedings of Workshop 8 - Evaluating and Changing Representation in Machine Learning IJCAI-91, </booktitle> <year> 1991. </year>
Reference-contexts: Oliver and Wallace's program to use MML to learn classification graphs employs standard greedy search with two-step lookahead to avoid local minima in the space of graphs. 2 As Oliver and Wallace reported <ref> [21] </ref>, this technique compares favorably with what has become virtually a benchmarking standard in classification learning, Quinlan's C4.5 [24]. More recently, Oliver has been using sets of classification trees (what he calls "fanning" trees) to make classification predictions based upon the weighted predictions of multiple trees ([20], following Buntine [1]). <p> In an example from Oliver [17] in Figure 1 the tree structure forces the subtree representing the disjunct (C ^ D) to be duplicated. Since all 2 Oliver and Wallace <ref> [21] </ref> describe an algorithm with an arbitrary number of lookahead steps, but Oliver has privately communicated that he normally does not use more than two step lookahead. 2 the information-theoretic tree learning algorithms (e.g., ID3, C4.5 and MML) rely upon mea-suring the efficient encodings, inefficiencies in the representation lead to inefficiencies <p> As such the number of candidate attributes will be reduced by one, making it less expensive to specify the split attribute. (See <ref> [21, 33] </ref> for a full description of message length calculation.) To transmit the second part of the message, the entire training set is categorized using the graph. Each leaf node is then associated with the number of instances in each class. <p> M is the number of classes. n c are the number of objects of class c in the category. ff is the coefficient of a symmetric Beta prior. To grow classification graphs, Oliver and Wallace <ref> [21] </ref> repeatedly generate a list of possible modifications to the original graph, using the operators: convert a leaf into a split node using any available attribute; join a pair of leaves (i.e., replace with a single leaf having multiple parents). <p> Each individual is represented as a complete graph rather than a single rule contributing to the model and the GA is used to search the space of possible classification graphs for that with the minimum MML cost <ref> [21] </ref>. Here we describe the encoding and design choices we have made and some of the difficulties which we had to overcome. 2.1 Genotype Choosing a chromosomal structure poses difficulties for learning grpahical representations. <p> One was a significant loss of variation in the population. Another is a technical point concerning the calculation of MML cost. Prior to the inclusion of the redundancy operator, the message length calculation we used necessarily differed from that in Oliver and Wallace <ref> [21] </ref> (as described in 1.3), because they did not consider redundant splitting attributes to be among the candidates available when communicating a node.
Reference: [22] <author> D. Orvosh and L. Davis. </author> <title> Shall we repair? Genetic algorithms, combinatorial optimization, and feasibilty constraints. </title> <editor> In S. Forrest, editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <address> Los Altos, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Orvosh and Davis <ref> [22] </ref> advise that a 5% replacement rate of damaged chromosomes by repaired chromosomes improves performance more than any other rate. Further investigation is needed to determine how best to handle redundancy correction.
Reference: [23] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: In x3 we describe our experimental results, comparing them with Oliver's classification graphs and Quinlan's C4.5. In x4 we conclude with some likely future directions for research in this area. 1.1 Classification Trees Classification trees, beginning with Quinlan's ID3 <ref> [23] </ref> and now with his C4.5, have largely replaced version space methods (e.g., AQ11) as the dominant learning technique in classification learning and concept formation.
Reference: [24] <author> J.R. Quinlan. C4.5: </author> <title> Programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Calif., </address> <year> 1993. </year>
Reference-contexts: and Wallace's program to use MML to learn classification graphs employs standard greedy search with two-step lookahead to avoid local minima in the space of graphs. 2 As Oliver and Wallace reported [21], this technique compares favorably with what has become virtually a benchmarking standard in classification learning, Quinlan's C4.5 <ref> [24] </ref>. More recently, Oliver has been using sets of classification trees (what he calls "fanning" trees) to make classification predictions based upon the weighted predictions of multiple trees ([20], following Buntine [1]). <p> In ID3 or C4.5, however, no direct account is taken of the complexity of the model and overfitting has to be avoided by such means as pruning back branches after a tree has been completed <ref> [24] </ref>. 1.3.1 Using MML to Grow Classification Graphs In order to evaluate classification graphs, Oliver and Wallace invented an MML coding scheme for such graphs [17]. The first part of his message transmits the graph itself.
Reference: [25] <author> J.R. Quinlan and R.L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: So the value of a tree is proportional the inverse of the dollar cost of using it over a set of examples. To grow trees, Turney uses a top down approach similar to that used by Quinlan and Rivest <ref> [25] </ref> and Wallace and Patrick [33]. <p> Most iterative approaches to generating classification trees also have a pruning phase, where a grown tree is selectively pruned back to find some balance between tree size and predictive performance <ref> [25, 2, 19, 20] </ref>. Although node mutation performs some pruning automatically (by occasionally turning split nodes into leaves), introducing more intelligent 18 pruning in a genetic operator might keep the search from exploring highly unfit areas.
Reference: [26] <author> H. Raiffa. </author> <title> Decision Analysis. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1970. </year>
Reference-contexts: the space 1 It is standard in the machine learning literature to label these trees and graphs decision trees and graphs, however we prefer to use "classification tree/graph" since we believe it is more descriptive and less likely to conflict with the prior use of "decision tree" in decision analysis <ref> [26] </ref>. 1 of possible hypotheses is a crucial one. Our approach is to draw upon a set of techniques which have been proven effective in a large variety of search problems when provided with a reliable evaluation mechanism|namely, the simulation of evolution via genetic algorithms.
Reference: [27] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11 </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction The use of information-theoretic measures to evaluate models of data by ranking them according to their posterior probabilities is well-grounded theoretically in the Minimum Message Length (MML) induction method of Wallace [32] and in the Minimum Description Length (MDL) method of Rissanen <ref> [27] </ref>, and are legitimately viewed as variations on Bayesian induction [18].
Reference: [28] <author> R. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: Classification graphs provide an elegant solution to this problem by allowing nodes to have more than one parent, in "joins." This allows for the efficient representation of both conjunction and disjunction (See Figure 2). Early work on generating classification graphs constrained the topology to a fixed form <ref> [28] </ref>.
Reference: [29] <author> R. Solomonoff. </author> <title> A formal theory of inductive inference i, ii. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22 and 224-254, </pages> <year> 1964. </year>
Reference-contexts: errors together with whatever structure truly underlies the data obtained, there typically lies a minimum point which MML induction aims at finding: the hypothesis which reports as much of the underlying structure as can be inferred from the available data, without also reporting phantoms arising from noise in the data <ref> [29, 3] </ref>.
Reference: [30] <author> P.D. Turney. </author> <title> Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 369-409, </pages> <year> 1995. </year>
Reference-contexts: difference between this approach and ours is that individual chromosomes are not expected to classify the entire set of training instances, but only small clusters of related instances. 1.4.2 Inexpensive Classification with Expensive Tests (ICET) Turney investigates the use of GAs in the problem of cost-sensitive learning of classification trees <ref> [30] </ref>. Candidate splits are assessed both upon the standard information gain of the split and upon the cost of performing the measurement associated with the attribute at that node together with a cost associated with misclassifying examples at the node. <p> To grow trees, Turney uses a top down approach similar to that used by Quinlan and Rivest [25] and Wallace and Patrick [33]. At each growth stage, split attributes are considered for each leaf in the tree, and the one that maximizes the Information Cost Function <ref> [30] </ref> is used: ICF i = 2 ffiI i 1 (C i + 1) ! 0 ! 1 (2) where ffiI i is the information gain that attribute i provides at the current stage of tree growth, C i is the cost associated with the attribute i, and ! is a
Reference: [31] <author> C.S. Wallace and D.M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: They have also been applied to a wide variety of practical problems in the assessment of models, such as bushfire prediction [6] and the classification of seals into species <ref> [31] </ref>, and have recently been employed in the important problem of learning causal/Bayesian network structures [15]. <p> Early work on generating classification graphs constrained the topology to a fixed form [28]. Oliver and Wallace [17] extended Wallace's [33] work on classification trees by applying the Minimum Message Length Principle (MMLP, described below) <ref> [31, 32] </ref> to the problem of growing classification graphs, using greedy search on the operators available to modify graphs. 3 1.3 The MML Induction Principle Wallace's Minimum Message Length induction principle asserts (a) that the best explanation of a given set of data is that which is the shortest and (b)
Reference: [32] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 49 </volume> <pages> 240-252, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction The use of information-theoretic measures to evaluate models of data by ranking them according to their posterior probabilities is well-grounded theoretically in the Minimum Message Length (MML) induction method of Wallace <ref> [32] </ref> and in the Minimum Description Length (MDL) method of Rissanen [27], and are legitimately viewed as variations on Bayesian induction [18]. <p> Early work on generating classification graphs constrained the topology to a fixed form [28]. Oliver and Wallace [17] extended Wallace's [33] work on classification trees by applying the Minimum Message Length Principle (MMLP, described below) <ref> [31, 32] </ref> to the problem of growing classification graphs, using greedy search on the operators available to modify graphs. 3 1.3 The MML Induction Principle Wallace's Minimum Message Length induction principle asserts (a) that the best explanation of a given set of data is that which is the shortest and (b)
Reference: [33] <author> C.S. Wallace and J.D. Patrick. </author> <title> Coding decision trees. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 7-22, </pages> <year> 1993. </year> <month> 20 </month>
Reference-contexts: Furthermore, these methods have been applied successfully in the most common test domain for machine learning methods, namely in concept formation and classification learning, with MML being used to learn classification trees (decision trees) <ref> [33] </ref> and their generalization to classification graphs [17], which are specialized directed acyclic graphs. 1 Although MML is quite effective in this evaluative role, the assessment of hypotheses given some data, it is mute on the question of how one should best generate hypotheses deemed worth assessing. <p> Early work on generating classification graphs constrained the topology to a fixed form [28]. Oliver and Wallace [17] extended Wallace's <ref> [33] </ref> work on classification trees by applying the Minimum Message Length Principle (MMLP, described below) [31, 32] to the problem of growing classification graphs, using greedy search on the operators available to modify graphs. 3 1.3 The MML Induction Principle Wallace's Minimum Message Length induction principle asserts (a) that the best <p> As such the number of candidate attributes will be reduced by one, making it less expensive to specify the split attribute. (See <ref> [21, 33] </ref> for a full description of message length calculation.) To transmit the second part of the message, the entire training set is categorized using the graph. Each leaf node is then associated with the number of instances in each class. <p> Each leaf node is then associated with the number of instances in each class. Assuming a symmetric Beta prior distribution of class membership in each leaf, the category can be coded with a message of length <ref> [33] </ref>: log 2 Pr (ejh) = log 2 4 j=0 3 " M1 Y n c 1 Y (i + ff) (1) N is the number of items in the category. <p> So the value of a tree is proportional the inverse of the dollar cost of using it over a set of examples. To grow trees, Turney uses a top down approach similar to that used by Quinlan and Rivest [25] and Wallace and Patrick <ref> [33] </ref>.
References-found: 33

