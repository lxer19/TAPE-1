URL: http://www.cs.wisc.edu/~finton/poster.ps
Refering-URL: http://www.cs.wisc.edu/~finton/djfpubs.html
Root-URL: 
Email: finton@cs.wisc.edu  
Title: Reinforcement learning for multi-step problems  
Author: David J. Finton and Yu Hen Hu 
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: In reinforcement learning for multi-step problems, the sparse nature of the feedback aggravates the difficulty of learning to perform. This paper explores the use of a reinforcement learning architecture, leading to a discussion of reinforcement learning in terms of feature abstraction, credit-assignment, and temporal-difference learning. Issues discussed include: the conditioning of the reinforcement signal, requirements for feature abstraction, the effect of feature representation on credit-assignment, and structural and temporal credit-assignment in terms of on-line tree search. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, C. W. </author> <year> (1986). </year> <title> Learning and problem solving with multilayer connectionist systems. </title> <type> Ph.D. thesis, </type> <institution> University of Massachusetts, Amherst, Massachusetts. </institution>
Reference: [2] <author> Anderson, C. W. </author> <year> (1987). </year> <title> Strategy learning with multilayer connectionist representations. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning (Ann Arbor), </booktitle> <pages> pp. 103-114. </pages>
Reference-contexts: Barto, Sutton, and Anderson [3] took this approach in designing a system of two neuron-like units which control movement of a cart in order to keep an attached pole balanced vertically. Anderson <ref> [2] </ref> extended this approach to a system with a back-propagation action subnetwork coupled to an evaluation sub-network which learns to predict the reinforcement signal; he applied this system to both the pole-balancing task and the Towers of Hanoi problem. <p> Hence, Tesauro's Neurogammon system included a few simple pre-computed features in its feature detection. Barto, Sutton, and Anderson's [3] pole-balancing system had the input categories built into the system; the system then learned a heuristic estimate of success for each category. Anderson's extension <ref> [2] </ref> of this work used back-propagation to learn feature detectors on-line; this is a step in the right direction, although the features required by the task turned out to be simple (as shown by the flatness of the features in Figure 5 of his paper), even though the temporal credit-assignment problem
Reference: [3] <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-13, </volume> <pages> 834-846, </pages> <address> Boston, MA. </address>
Reference-contexts: One outgrowth of this research has been learning architectures which incorporate two networks, where one network learns to predict the reinforcement signal, and the second network learns the task, given regular feedback signals from the first network. Barto, Sutton, and Anderson <ref> [3] </ref> took this approach in designing a system of two neuron-like units which control movement of a cart in order to keep an attached pole balanced vertically. <p> As Tesauro [20] notes, without including some simple, precoded features, the learning system's order of computation can be quite high, requiring a possibly intractable number of examples. Hence, Tesauro's Neurogammon system included a few simple pre-computed features in its feature detection. Barto, Sutton, and Anderson's <ref> [3] </ref> pole-balancing system had the input categories built into the system; the system then learned a heuristic estimate of success for each category.
Reference: [4] <author> Feldman, J. A., & Ballard, D. H. </author> <year> (1982). </year> <title> Connectionist models and their properties. </title> <journal> Cognitive Science, </journal> <volume> 6, </volume> <pages> 205-254. </pages>
Reference-contexts: A GENERAL REINFORCEMENT LEARNING ARCHITECTURE The general plan of the learning architecture is shown in Figure 1. The learning architecture is a two-layer, feed-forward, connectionist network, in which a layer of receptor nodes feeds into a winner-take-all layer <ref> [4] </ref> of effector nodes. The receptors are activated by input links which carry activation directly from the environment. The effectors trigger responses which may change the environment; there is one effector node for each possible output action or conclusion.
Reference: [5] <author> Grossberg, S. </author> <year> (1987). </year> <title> Competitive learning: from interactive activation to adaptive resonance. </title> <booktitle> Cognitive Science 11, </booktitle> <pages> 23-63. </pages>
Reference-contexts: It may also be true that the combination of features and performance function exacerbates the dips and peaks in the error function, making gradient-descent less effective at the task of abstracting feature detectors. Bottom-up techniques such as Hebbian methods, competitive learning [7][10][14] <ref> [5] </ref> can discover regularities in the inputs. But these processes must somehow be guided toward features which are at the right level of categorization, and are relevant to the performance task. We feel that the top-down feedback gives the necessary information for such guidance.
Reference: [6] <author> Hinton, G. E., McClelland, J. L., & Rumelhart, D. E. </author> <year> (1986). </year> <title> Distributed Representations. </title> <editor> In D. E. Rumelhart & J. L. McClelland, (Eds.), </editor> <booktitle> Parallel Distributed Processing, 1, (Chapter 3). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: In the general case, this activation pattern is a distributed representation; that is, each context pattern involves many nodes, and a single node will participate in the activation patterns for many context patterns <ref> [6] </ref> . The correct strategy will be to update the weights of all links going from the currently active receptor nodes (which denote the current context) to the effector node (which triggers the action).
Reference: [7] <author> Holdaway, R. M. </author> <year> (1989). </year> <title> Enhancing supervised learning algorithms via self-organization. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (Washington, DC), II, </booktitle> <pages> 523-529. </pages>
Reference-contexts: Holdaway <ref> [7] </ref> used a Kohonen-style self-organizing front-end as the feature representation for his back-propagation trained network. We feel that this general idea of using a bottom-up self-organization process is crucial to on-line feature abstraction. In our experiments, top-down feedback was too weak for adequate feature abstraction.
Reference: [8] <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell, (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, 2, (Chapter 20). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [9] <author> Holland, J. H., Holyoak, K. J., Nisbett, R. E., & Thagard, P. R. </author> <year> (1986). </year> <title> Induction: Processes of Inference, Learning, and Discovery. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: [10] <author> Kohonen, T. </author> <year> (1982). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43, </volume> <pages> 59-69. </pages>
Reference: [11] <author> Lippmann, R. </author> <year> (1987). </year> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 4(2), </volume> <pages> 4-22. </pages>
Reference: [12] <author> Michie, D., & Chambers, R.A. </author> <year> (1968). </year> <title> BOXES: An experiment in adaptive control. </title> <editor> In E. Dale & D. Michie, (Eds.), </editor> <booktitle> Machine Intelligence 2. </booktitle> <publisher> Edinburgh: Oliver and Boyd. </publisher>
Reference: [13] <author> Robinson, T., & Fallside, F. </author> <year> (1989). </year> <title> Dynamic reinforcement driven error propagation networks with application to game playing. </title> <booktitle> Proceedings of the Eleventh Annual Conference of the Cognitive Science Society (Ann Arbor). </booktitle>
Reference-contexts: Another two-network approach is the reinforcement learning system of Robinson and Fallside <ref> [13] </ref> , which learns to play tic-tac-toe. The system played against an opponent which either completed a line of two to win a game, or else moved randomly if no such win moves were available.
Reference: [14] <author> Rumelhart, D.E., & Zipser, D. </author> <year> (1986). </year> <title> Feature discovery by competitive learning. </title> <editor> In D. E. Rumelhart & J. L. McClelland, (Eds.), </editor> <booktitle> Parallel Distributed Processing, 1, (Chapter 5). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: [15] <author> Samuel, A. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. IBM J. </title> <booktitle> of Research and Development 3, </booktitle> <pages> 210-229. </pages>
Reference: [16] <author> Schmidhuber, J. </author> <year> (1989). </year> <title> A local learning algorithm for dynamic feedforward and recurrent networks. </title> <journal> Connection Science, </journal> <volume> 1(4), </volume> <pages> 403-412. </pages>
Reference-contexts: John Holland's Bucket Brigade algorithm [8][9] may also be described in terms of temporal-difference learning, since the evaluation of its classifiers (if-then rules) depends on the evaluation of the classifiers they enable to fire. Ju .. rgen Schmidhuber's Neural Bucket Brigade <ref> [16] </ref> is a neural net version of the Bucket = DRAFT COPY : September 1992 = - -- Brigade algorithm, with link weights updated according to the weights of their successors. Schmidhuber's approach is more faithful to the Bucket brigade algorithm than the architecture presented in this paper. <p> But in a reinforcement learning system there is no teacher to insure that the feedback is not too critical or too lenient, or scaled improperly. Some systems, such as the Schmidhuber's Neural Bucket Brigade <ref> [16] </ref> , do not address the quality of the reinforcement signal; this makes them brittle with respect to the quality of the reinforcement, especially as the performance tasks become more difficult and take longer to learn.
Reference: [17] <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal credit assignment in reinforcement learning. </title> <type> Ph.D. thesis, </type> <institution> University of Massachusetts, Amherst, Massachusetts. </institution> = <note> DRAFT COPY : September 1992 </note> = - -- 
Reference-contexts: Thus, credit-assignment is divided into two sub-problems: the temporal credit-assignment problem (determining the responsibility of particular actions for the end result), and the structural credit-assignment problem (assigning credit or blame to the individual mechanisms responsible for selection of a particular action) <ref> [17] </ref> Because the reinforcement signal is a scalar rather than a vector, structural credit-assignment is more difficult. <p> RELATED WORK Much of the work done in reinforcement learning has involved some form of temporal-difference learning. The decomposition of credit-assignment into its structural and temporal components is due to Richard Sutton <ref> [17] </ref> , who also introduced the formal study of temporal-difference learning [18] . Temporal difference learning takes the difference between temporally-successive predictions of an outcome as the error signal for learning, instead of comparing the predicted outcome with the actual outcome. <p> We are also investigating the use of a variant of this idea, using (1.0 - Rt) * R, where Rt is a time-averaged trace of the past values of R. This reinforcement comparison scheme was one of many extensively studied by Richard Sutton. <ref> [17] </ref> The hand-coded network experiment also showed that an additional hidden layer was not needed. Previous versions of the model included an additional hidden layer of nodes before the effectors.
Reference: [18] <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1), </volume> <pages> 9-44. </pages>
Reference-contexts: RELATED WORK Much of the work done in reinforcement learning has involved some form of temporal-difference learning. The decomposition of credit-assignment into its structural and temporal components is due to Richard Sutton [17] , who also introduced the formal study of temporal-difference learning <ref> [18] </ref> . Temporal difference learning takes the difference between temporally-successive predictions of an outcome as the error signal for learning, instead of comparing the predicted outcome with the actual outcome. Sutton shows that this can result in increased learning power.
Reference: [19] <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (Austin). </booktitle>
Reference: [20] <author> Tesauro, G., & Sejnowski, T. J. </author> <year> (1989). </year> <title> A parallel network that learns to play backgammon. </title> <booktitle> Artificial Intelligence 39, </booktitle> <pages> 357-390. </pages>
Reference-contexts: Currently, most connectionist systems which learn to perform have used pre-computed input representations. Most of this work has been devoted to the study of credit-assignment, so using hard-wired feature detectors was a reasonable simplification. As Tesauro <ref> [20] </ref> notes, without including some simple, precoded features, the learning system's order of computation can be quite high, requiring a possibly intractable number of examples. Hence, Tesauro's Neurogammon system included a few simple pre-computed features in its feature detection.
Reference: [21] <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Ph.D. thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address> = <note> DRAFT COPY : September 1992 </note> = - -- 
Reference-contexts: In contrast, the system presented here only uses winner-take-all to force the output layer to produce a single response at a time. Temporal difference credit schemes may also be described in terms of Watkins' Q-learning <ref> [21] </ref> , which applies dynamic programming to learning tasks which are Markov decision processes. The structural credit-assignment scheme we use is very similar to 1-step Q-learning, except that our weight update is multiplied by the activation flowing into the link during the previous time step. 5.
References-found: 21

