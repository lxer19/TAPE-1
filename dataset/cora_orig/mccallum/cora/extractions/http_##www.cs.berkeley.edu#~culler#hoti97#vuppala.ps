URL: http://www.cs.berkeley.edu/~culler/hoti97/vuppala.ps
Refering-URL: http://www.cs.berkeley.edu/~culler/hoti97/
Root-URL: 
Abstract: Conventional routers are unable to scale to accommodate the explosive growth of the Internet traffic due to their architectural constraints. Their architecture does not allow for scalability by the addition of more processing nodes or faster bus technology. There has been an emergence of very high speed, economical and scalable SAN/LAN interconnects providing bandwidths of gigabits per second. Design of an IP router composed of off-the-shelf processors interconnected by such SAN/LAN switches is proposed. Implementation of a prototype and its perform ance are described.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> R. Horst, et al, </editor> <booktitle> Performance Modeling of ServerNet Topologies, Proc of the International Parallel Processing Symposium, </booktitle> <pages> pp. 518-523, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: The architecture of todays routers does not allow for scalability, e.g., by the addition of more processing nodes or faster bus technology. At the same time there has been an emergence of very high speed, economical and scalable SAN/LAN interconnects, such as Myrinet [2] and ServerNet <ref> [1] </ref>. These SAN/LAN interconnects provide bandwidths of giga-bits per second. For example, a Myrinet link composed of a full duplex pair of channels provides a bandwidth of 1.28 Gbps. These switches can be incrementally ex panded to provide higher aggregate throughput and thus scalability.
Reference: [2] <author> Nanette J. Boden et al, </author> <title> Myrinet - A Gigabit-per-second Local-area Network, </title> <booktitle> IEEE Micro, </booktitle> <month> Feb </month> <year> 1995. </year>
Reference-contexts: The architecture of todays routers does not allow for scalability, e.g., by the addition of more processing nodes or faster bus technology. At the same time there has been an emergence of very high speed, economical and scalable SAN/LAN interconnects, such as Myrinet <ref> [2] </ref> and ServerNet [1]. These SAN/LAN interconnects provide bandwidths of giga-bits per second. For example, a Myrinet link composed of a full duplex pair of channels provides a bandwidth of 1.28 Gbps. These switches can be incrementally ex panded to provide higher aggregate throughput and thus scalability.
Reference: [3] <author> Peter Newman et al, </author> <title> Flow Labelled IP: A con-nectionless approach to ATM, </title> <booktitle> Proceedings of IEEE Infocom, </booktitle> <month> March </month> <year> 1996. </year>
Reference: [4] <author> Cisco Systems, </author> <title> Scaling the Internet with Tag Switching, </title> <note> White Paper, http://www.cisco.com </note>
Reference-contexts: Section 8 outlines the work in progress. 2. Related Work Realizing the bottleneck introduced by IP routers, different solutions have been proposed. As mentioned in the last section, the problem lies in the router architecture and in the complexity of IP routing. IP-switching [3][8], Tagswitching <ref> [4] </ref> and Cell Switch Router (CSR) [9] attempt to solve the latter, i.e., the complexity of IP routing, by routing packets at layer-2. Multigi-gabit routers such as Pluris Incs Massively Parallel Router (MPR) [5] and Ascend Incs GRF IP Switch [10] are some of the solution to the architectural issues. <p> The entries in a TIB generally correspond to the routing table entries at that tag switch. IP-switching and Tagswitching do not completely eliminate the need for IP routing. With both the approaches, certain nodes will always be heavily dependent on the IP layer. Tagswitches with aggregate routes <ref> [4] </ref> and IP-switches on the Internet backbone [5] are some such examples. A MPR router is composed of a large number of single board processing nodes and a proprietary Self-Healing Butterfly interconnect. The processing nodes share the routing load and each has a copy of the forwarding table.
Reference: [5] <author> Pluris Inc, </author> <title> Pluris Massively Parallel Routing, </title> <note> White Paper, http://www.pluris.com </note>
Reference-contexts: IP-switching [3][8], Tagswitching [4] and Cell Switch Router (CSR) [9] attempt to solve the latter, i.e., the complexity of IP routing, by routing packets at layer-2. Multigi-gabit routers such as Pluris Incs Massively Parallel Router (MPR) <ref> [5] </ref> and Ascend Incs GRF IP Switch [10] are some of the solution to the architectural issues. IP-switching dynamically chooses between IP routing and ATM switching, depending on the characteristics of the network traffic. <p> IP-switching and Tagswitching do not completely eliminate the need for IP routing. With both the approaches, certain nodes will always be heavily dependent on the IP layer. Tagswitches with aggregate routes [4] and IP-switches on the Internet backbone <ref> [5] </ref> are some such examples. A MPR router is composed of a large number of single board processing nodes and a proprietary Self-Healing Butterfly interconnect. The processing nodes share the routing load and each has a copy of the forwarding table.
Reference: [6] <author> S. Bradner, </author> <title> Benchmarking Terminology, </title> <type> RFC 1242. </type>
Reference-contexts: Only 10Mb/s Ethernet ports were used for the benchmarks. Null (port 9) UDP packets were used for all the tests. 7.1 Throughput Throughput is defined as the maximum rate at which none of the offered frames is dropped by the device <ref> [6] </ref>. Throughput for various test configurations and the theoretical frame rates for 10Mb/s Ethernet [7] are shown in figure. It is seen that the egress stage of the router does not have any impact on the throughput of the router. <p> 1518 bytes [7] were getting fragmented and hence 1514 byte sized frames were used. 7.2 Frame Loss Rate Frame loss rate is defined as the percentage of frames that should have been forwarded by a network device under steady state load that were not forwarded due to lack of resources <ref> [6] </ref>. It was measured for frame sizes of 64, 128, 256, 512, 1024, 1280 and 1514 bytes. <p> loss was noticed for all the test configurations. 7.3 Latency Latency for store and forward devices is the time interval starting when the last bit of the input frame reaches the input port and the ending when the first bit of the output frame is seen on the output port <ref> [6] </ref>. The measurements shown in Figure 7 do not show the latency as defined above. Instead it shows the difference between the time the frame was transmitted and then received by the Tester. This includes the transmission times on the media.
Reference: [7] <author> S. Bradner and J. McQuaid, </author> <title> Benchmarking Methodology, RFC 1944. </title>
Reference-contexts: Null (port 9) UDP packets were used for all the tests. 7.1 Throughput Throughput is defined as the maximum rate at which none of the offered frames is dropped by the device [6]. Throughput for various test configurations and the theoretical frame rates for 10Mb/s Ethernet <ref> [7] </ref> are shown in figure. It is seen that the egress stage of the router does not have any impact on the throughput of the router. Note that the Tester could not generate frames at the theoretical frame rate for smaller frame sizes. Frames of size 1518 bytes [7] were getting <p> 10Mb/s Ethernet <ref> [7] </ref> are shown in figure. It is seen that the egress stage of the router does not have any impact on the throughput of the router. Note that the Tester could not generate frames at the theoretical frame rate for smaller frame sizes. Frames of size 1518 bytes [7] were getting fragmented and hence 1514 byte sized frames were used. 7.2 Frame Loss Rate Frame loss rate is defined as the percentage of frames that should have been forwarded by a network device under steady state load that were not forwarded due to lack of resources [6].
Reference: [8] <author> Peter Newman et al, </author> <title> IP Switching and Gigabit Routers, </title> <journal> IEEE Communications Magazine, </journal> <month> Jan </month> <year> 1997. </year>
Reference: [9] <author> Y. Katsube et al, </author> <title> Toshibas Router Architecture Extension for ATM : Overview, RFC 2098 </title>
Reference-contexts: Related Work Realizing the bottleneck introduced by IP routers, different solutions have been proposed. As mentioned in the last section, the problem lies in the router architecture and in the complexity of IP routing. IP-switching [3][8], Tagswitching [4] and Cell Switch Router (CSR) <ref> [9] </ref> attempt to solve the latter, i.e., the complexity of IP routing, by routing packets at layer-2. Multigi-gabit routers such as Pluris Incs Massively Parallel Router (MPR) [5] and Ascend Incs GRF IP Switch [10] are some of the solution to the architectural issues.
Reference: [10] <institution> Ascend Inc., </institution> <note> GRF IP Switch, http://www.ascend.com </note>
Reference-contexts: IP-switching [3][8], Tagswitching [4] and Cell Switch Router (CSR) [9] attempt to solve the latter, i.e., the complexity of IP routing, by routing packets at layer-2. Multigi-gabit routers such as Pluris Incs Massively Parallel Router (MPR) [5] and Ascend Incs GRF IP Switch <ref> [10] </ref> are some of the solution to the architectural issues. IP-switching dynamically chooses between IP routing and ATM switching, depending on the characteristics of the network traffic. It identifies a flow in IP traffic and assigns it a virtual circuit in the ATM hardware, thus bypassing the IP layer.
References-found: 10

