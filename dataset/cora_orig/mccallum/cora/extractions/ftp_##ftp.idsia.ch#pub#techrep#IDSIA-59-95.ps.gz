URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-59-95.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Title: ENVIRONMENT-INDEPENDENT REINFORCEMENT ACCELERATION  difference between time and space is that you can't reuse time.  
Author: Jurgen Schmidhuber Merrick Furst 
Address: Corso Elvezia 36 CH-6900-Lugano, Switzerland  
Affiliation: IDSIA,  
Note: Write-up of an invited talk at Hongkong University of Science and Technology  The biggest  
Pubnum: Technical Note IDSIA-59-95  
Email: juergen@idsia.ch  
Web: http://www.idsia.ch  
Date: (May 29, 1995)  June 21, 1995  
Abstract: A reinforcement learning system with limited computational resources interacts with an unrestricted, unknown environment. Its goal is to maximize cumulative reward, to be obtained throughout its limited, unknown lifetime. System policy is an arbitrary modifiable algorithm mapping environmental inputs and internal states to outputs and new internal states. The problem is: in realistic, unknown environments, each policy modification process (PMP) occurring during system life may have unpredictable influence on environmental states, rewards and PMPs at any later time. Existing reinforcement learning algorithms cannot properly deal with this. Neither can naive exhaustive search among all policy candidates | not even in case of very small search spaces. In fact, a reasonable way of measuring performance improvements in such general (but typical) situations is missing. I define such a measure based on the novel "reinforcement acceleration criterion" (RAC). At a given time, RAC is satisfied if the beginning of each completed PMP that computed a currently valid policy modification has been followed by long-term acceleration of average reinforcement intake (the computation time for later PMPs is taken into account). I present a method called "environment-independent reinforcement acceleration" (EIRA) which is guaranteed to achieve RAC. EIRA does neither care whether the system's policy allows for changing itself, nor whether there are multiple, interacting learning systems. Consequences are: (1) a sound theoretical framework for "meta-learning" (because the success of a PMP recursively depends on the success of all later PMPs, for which it is setting the stage). (2) A sound theoretical framework for multi-agent learning. The principles have been implemented (1) in a single system using an assembler-like programming language to modify its own policy, and (2) a system consisting of multiple agents, where each agent is in fact just a connection in a fully recurrent reinforcement learning neural net. A by-product of this research is a general reinforcement learning algorithm for such nets. Preliminary experiments illustrate the theory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto. </author> <title> Connectionist approaches for control. </title> <type> Technical Report COINS 89-89, </type> <institution> University of Massachusetts, </institution> <address> Amherst MA 01003, </address> <year> 1989. </year>
Reference-contexts: In particular, PMP i may affect the environmental conditions for PMP k ; k &gt; i. This is something not properly taken into account by existing algorithms for adaptive control and reinforcement learning (see, e.g., <ref> [4, 1, 13, 14] </ref>), and not even by naive, inefficient, but supposedly infallible exhaustive search among all policy candidates, as will be seen next.
Reference: [2] <author> M. Boddy and T. L. Dean. </author> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 245-285, </pages> <year> 1994. </year>
Reference-contexts: The environment is unrestricted | it may be non-Markovian and/or "changing", and even non-computable: in what follows, it won't even be necessary to introduce a formal model of the environment. Different system actions may require different amounts of execution time (like in scenarios studied in <ref> [8, 2] </ref>, and in references given therein). Occasionally the environment provides real-valued "reinforcement". The sum of all reinforcements obtained between "system birth" (at time 0) and time t &gt; 0 is denoted by R (t).
Reference: [3] <author> S. Heil. </author> <type> Diploma thesis, </type> <institution> 1995. Fakultat fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: The content of this work cell, however, does not have essential limits, and may point to any address in storage. See Heil <ref> [3] </ref>, however, for an alternative implementation without double indexed addressing. Lifelong sequence of instruction cycles. If the instruction and its arguments pass a syntax check, the instruction is executed. This may result in modifications of IP and/or environment and/or storage. <p> Outlook. The few experiments conducted so far were designed to illustrate basic principles of incremental self-improvement based on EIRA. They were based on low-level, assembler-like instructions (making even apparently simple tasks difficult | additional experiments with such low-level instructions can be found in <ref> [3] </ref> and in [10]).
Reference: [4] <author> P. R. Kumar and P. Varaiya. </author> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice Hall, </publisher> <year> 1986. </year>
Reference-contexts: In particular, PMP i may affect the environmental conditions for PMP k ; k &gt; i. This is something not properly taken into account by existing algorithms for adaptive control and reinforcement learning (see, e.g., <ref> [4, 1, 13, 14] </ref>), and not even by naive, inefficient, but supposedly infallible exhaustive search among all policy candidates, as will be seen next.
Reference: [5] <author> L. A. Levin. </author> <title> Laws of information (nongrowth) and aspects of the foundation of probability theory. </title> <journal> Problems of Information Transmission, </journal> <volume> 10(3) </volume> <pages> 206-210, </pages> <year> 1974. </year>
Reference-contexts: However, there is no reason why certain instructions should not be complex, time consuming programs, such as neural net learning algorithms, statistic classifiers, Q-learning [13], universal search <ref> [5] </ref>, etc. 4 EIRA for Recurrent Nets: Multi-Agent Perspective Now we solve the non-Markovian maze task from section 3 with multiple, very simple, non-self-modifying, learning agents. Each agent is in fact just a connection (whose current weight represents its current policy) in a fully recurrent neural net.
Reference: [6] <author> M. Li and P. M. B. Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: This lack is shared by almost all other reinforcement learning approaches, though. Note, however, that unlike previous, less general systems, the novel system in principle can exploit almost arbitrary environmental regularities <ref> [6] </ref> (if there are any) for speeding up performance improvement, simply because it can run almost arbitrary learning algorithms (due to the general nature of its programming language).
Reference: [7] <author> Mark B. </author> <title> Ring. Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas 78712, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: EIRA automatically takes care of this, thus recursively encouraging "learning how to learn how to learn ...". This represents an essential difference to previous approaches to "continual" learning, see <ref> [7] </ref>. Improvement speed? Due to the generality of the approach, no reasonable statements can be made about improvement speed, which indeed highly depends on the nature of the environment and the choice of initial primitive instructions. This lack is shared by almost all other reinforcement learning approaches, though.
Reference: [8] <author> S. Russell and E. </author> <title> Wefald. </title> <booktitle> Principles of metareasoning. Artificial Intelligence, </booktitle> <volume> 49 </volume> <pages> 361-395, </pages> <year> 1991. </year>
Reference-contexts: The environment is unrestricted | it may be non-Markovian and/or "changing", and even non-computable: in what follows, it won't even be necessary to introduce a formal model of the environment. Different system actions may require different amounts of execution time (like in scenarios studied in <ref> [8, 2] </ref>, and in references given therein). Occasionally the environment provides real-valued "reinforcement". The sum of all reinforcements obtained between "system birth" (at time 0) and time t &gt; 0 is denoted by R (t).
Reference: [9] <author> J. H. Schmidhuber. </author> <title> A self-referential weight matrix. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 446-451. </pages> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: In the end, no weight stack had more than 7 entries (each followed by faster reinforcement intake than all the previous ones). Ongoing work. It is intended to replace the random weight modification process by a process that may be strongly influenced by the current weights themselves. Following <ref> [9] </ref>, the idea is to use activation patterns across special output units to address and modify the network's own weights.
Reference: [10] <author> J. H. Schmidhuber. </author> <title> On learning how to learn learning strategies. </title> <type> Technical Report FKI-198-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen, </institution> <month> November </month> <year> 1994. </year> <note> Revised January 1995. </note>
Reference-contexts: In this application, the environment of each connection's policy (represented by its current weight) continually changes, because the policies of all the other connections keep changing. 4 2 EIRA for Incremental Self-Improvement Outline. The system in this section (see <ref> [10] </ref> for details) implements the ideas from section 1, in particular those in paragraph (**) on "incremental self-improvement". To improve/speed up its own (initially very dumb, highly random) learning strategy, the system policy makes use of an assembler-like programming language suitable to modify the policy itself. <p> For instance, the semantics of J mpleq (a1; a2; a3) are If c c a1 &lt; c c a2 then IP c a3 . Some problem-specific instructions, such as move-robot-one-step-north (), allow for manipulating processes in the environment. See a detailed list of instructions in <ref> [10] </ref>. Instruction probabilities / Current policy. For each program cell and for every possible instruction and instruction argument, there is a probability value P ij ; where i is the index of a program cell, and j 2 I. <p> The latter were accompanied by a steady increase of stack entries corresponding to probability modifications computed by useful SMS. More details in <ref> [10] </ref>. Final system state. All in all, there were nearly 0:5 fl 10 6 SMS during system life. <p> Still, without storage cells, the system could not achieve its dramatic performance improvement. It actually learned to make use of the changing policy environment. Evidence of "learning how to learn"? Many useful SMS directly computed valid modifications of probabilities of future SMS ("learning to learn"). More details in <ref> [10] </ref>. Stability of probability modifications. With the experiments conducted so far, the top level hardly ever countermanded probability modifications other than the 10 most recent valid ones. <p> Outlook. The few experiments conducted so far were designed to illustrate basic principles of incremental self-improvement based on EIRA. They were based on low-level, assembler-like instructions (making even apparently simple tasks difficult | additional experiments with such low-level instructions can be found in [3] and in <ref> [10] </ref>).
Reference: [11] <author> J. H. Schmidhuber. </author> <title> Discovering solutions with low Kolmogorov complexity and high generalization capability. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Due to the generality of the approach, however, no reasonable statements can be made about improvement speed, which indeed highly depends on the nature of the environment and the choice of initial primitive instructions. Storage / Instructions. The assembler-like programming language is partly inspired by the one in <ref> [11] </ref>. There is a finite amount of addressable storage broken into two groups: work cells and program cells. Processes in the external environment occasionally write inputs (integer values) into certain work cells.
Reference: [12] <author> R. S. Sutton. </author> <title> Integrated modeling and control based on reinforcement learning and dynamic programming. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 471-478. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: The remainder of this paper mainly serves to illustrate the theory. In section 2, I will exemplify the ideas in paragraph (**) on incremental self-improvement and describe a particular, concrete, working system that implements them. Section 3 will then apply this system to a variant of Sutton's maze task <ref> [12] </ref>. One difference to Sutton's original task is that the policy environment continually changes because of actions generated by the system itself. Section 4 will then exemplify the ideas in paragraph (*) on multi-agent learning and also describe a particular, concrete, working system that implements them. <p> Instead, this section's purpose is to illustrate typical aspects of the system's basic (bias independent) mode of operation. Non-Markovian variant of Sutton's Markovian maze task, with changing policy environment. There is a two-dimensional grid with obstacles (taken from Sutton <ref> [12] </ref>). Initially, 8 an "animat" is placed on the "start field". In addition to 17 general instructions (for arithmetic operations, conditional jumps, self-modifications etc.), there are four problem-specific instructions with obvious meaning: one-step-north (), one-step-south (), one-step-east (), one-step-west ().
Reference: [13] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: In particular, PMP i may affect the environmental conditions for PMP k ; k &gt; i. This is something not properly taken into account by existing algorithms for adaptive control and reinforcement learning (see, e.g., <ref> [4, 1, 13, 14] </ref>), and not even by naive, inefficient, but supposedly infallible exhaustive search among all policy candidates, as will be seen next. <p> However, there is no reason why certain instructions should not be complex, time consuming programs, such as neural net learning algorithms, statistic classifiers, Q-learning <ref> [13] </ref>, universal search [5], etc. 4 EIRA for Recurrent Nets: Multi-Agent Perspective Now we solve the non-Markovian maze task from section 3 with multiple, very simple, non-self-modifying, learning agents. Each agent is in fact just a connection (whose current weight represents its current policy) in a fully recurrent neural net.
Reference: [14] <author> R. J. Williams. </author> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256, </pages> <year> 1992. </year> <month> 12 </month>
Reference-contexts: In particular, PMP i may affect the environmental conditions for PMP k ; k &gt; i. This is something not properly taken into account by existing algorithms for adaptive control and reinforcement learning (see, e.g., <ref> [4, 1, 13, 14] </ref>), and not even by naive, inefficient, but supposedly infallible exhaustive search among all policy candidates, as will be seen next.
References-found: 14

