URL: http://www.cs.wustl.edu/cs/techreports/1997/wucs-97-38.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: milind@dworkin.wustl.edu JXCHEN@us.oracle.com dw@arl.wustl.edu  guru@arl.wustl.edu  
Phone: +1 314 935 4203 +1 415 506-8617 +1 314 935 8563  +1 314 935 7534  
Title: Enhancements to 4.4 BSD UNIX for Efficient Networked Multimedia in Project MARS  
Author: Milind M. Buddhikot Xin Jane Chen Dakang Wu Guru M. Parulkar 
Abstract: Cluster based architectures that employ high performance inexpensive Personal Computers (pcs) interconnected by high speed commodity interconnect have been recognized as a cost-effective way of building high performance scalable Multimedia-On-Demand (mod)storage servers [4, 5, 7, 9]. Typically, the pcs in these architectures run operating systems such as unix that have traditionally been optimized for interactive computing. They do not provide fast disk-to-network data paths and guaranteed cpu and storage access. This paper reports enhancements to the 4.4 bsd unix system carried out to rectify these limitations in the context of our Project Massively-parallel And Real-time Storage (MARS) [7]. We have proposed and implemented the following enhancements to a 4.4 bsd compliant public domain NetBSD unix operating system: (1) A new kernel buffer management system called Multimedia M-buf (mmbuf) which shortens the data path from a storage device to network interface, (2) priority queueing within the scsi driver to differentiate between real-time and non-real-time streams, and (3) integration of these new os services with a cpu scheduling mechanism called Real Time Upcall [22] and a software disk striping driver called Concatenated Disk (ccd). These enhancements collectively provide quality of service guarantee and high throughput to multimedia stream connections. Our experimental results demonstrate throughput improvements and QOS guarantees on the data path from the disk to network in a MOD server. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, T.E., et.al., </author> <title> "Scheduler Activations: Effective Kernel Support for User Level Management of Parallelism," </title> <journal> ACM Transactions on Computer Systems, </journal> <year> 1992, </year> <pages> pp. 53-79. </pages>
Reference-contexts: The rtu facility has already been demonstrated to be useful for high performance user level protocol implementations. A more detailed discussion of the implementation and related work in this area such as Scheduler Activations <ref> [1] </ref>, Processor Capacity Reserves [20], Q-threads [16] etc. can be found in [22, 24]. 4.6 Streams api We have designed an api consisting of a new set of system calls that allow applications to access mm-bufs and real-time guarantees from the scsi driver for network destined disk retrievals.
Reference: [2] <author> Bobrow, D., G., "Tenex, </author> <title> A Paged Time Sharing System for pdp-10,", </title> <journal> Communications of ACM, </journal> <volume> Vol. 15, No. 3, </volume> <pages> pp. 135-143, </pages> <month> Mar. </month> <year> 1972. </year>
Reference-contexts: level application can effectively make use of such real-time service guarantees from the disk driver only if the cpu scheduler provides qos guarantees in the form of periodic execution. 1.1 Research Contributions We would like to note that the shortcomings discussed above have been well known to be performance bottlenecks <ref> [2, 12, 13, 21, 26] </ref>. However, none of the earlier solutions are complete and research efforts such as [3, 4, 15, 18, 25, 28] are underway to rectify them. (Please see Section 6). Our work shares some common ideas and objectives with these research efforts. <p> In the following, however, we try to strike a balance between recent active projects and research widely cited in literature. The idea of minimizing data copy to achieve higher performance is well known and has been reported in early operating systems such as Tenex <ref> [2] </ref> and Accent [26]. The Container Shipping system [21], the dash ipc [29] and fbufs [12] have addressed the problem of minimizing physical data movement across protection domains in an os by employing virtual memory re-mapping techniques.
Reference: [3] <author> Brustoloni, J., C., and Steenkiste, P. </author> <title> "Evaluation of Data Passing and Scheduling Avoidance," </title> <booktitle> Proceedings of NOSSDAV97, </booktitle> <address> St. Louis, MO, </address> <pages> pp. 101-111, </pages> <month> May 19-21, </month> <year> 1997. </year>
Reference-contexts: However, none of the earlier solutions are complete and research efforts such as <ref> [3, 4, 15, 18, 25, 28] </ref> are underway to rectify them. (Please see Section 6). Our work shares some common ideas and objectives with these research efforts. This paper describes our innovative ideas and details the software infrastructure we have developed to implement these ideas. <p> However, none of these projects report design of the zero-copy i/o between disks and networks. A more recent paper by Brustoloni et al. <ref> [3] </ref> proposes new copy avoidance techniques called emulated share and emulated copy which do not require any changes to i/o api as required by some of the above mentioned techniques (including ours).
Reference: [4] <editor> Bolosky, W., et al., </editor> <booktitle> "The Tiger Video Fileserver," Proceedings of NOSSDAV96, </booktitle> <pages> pp. 97-104, </pages> <address> Zushi, Japan, </address> <month> Apr. </month> <pages> 23-26, </pages> <year> 1996. </year>
Reference-contexts: However, none of the earlier solutions are complete and research efforts such as <ref> [3, 4, 15, 18, 25, 28] </ref> are underway to rectify them. (Please see Section 6). Our work shares some common ideas and objectives with these research efforts. This paper describes our innovative ideas and details the software infrastructure we have developed to implement these ideas. <p> Our work is entirely based on bsd class of operating systems (NetBSD, FreeBSD, OpenBSD, 4.4 BSD). Our project mars, of which the work reported in this paper is a part, has several similarities with the Tiger File system project at Microsoft <ref> [4] </ref>. Tiger is a distributed, fault-tolerant, real-time file server that uses a distributed storage architecture consisting of cubs pcs controlled by a control manager. It stripes constant-bit-rate (cbr) data such as video, and audio over cubs in fixed length data units.
Reference: [5] <author> Buddhikot, M., Parulkar, G., M., and Cox, Jerome, Jr., </author> <title> "Design of a Large Scale Multimedia Server," Journal of Computer Networks and ISDN Systems, </title> <publisher> Elsevier (North Holland), </publisher> <pages> pp. 504-524, </pages> <month> Dec </month> <year> 1994. </year>
Reference: [6] <author> Buddhikot, M., and Parulkar, G., M., </author> <title> "Efficient Data Layout, Scheduling and Playout Control in MARS," </title> <journal> ACM/Springer Multimedia Systems Journal, pp. </journal> <volume> 199-211, Volume 5, Number 3, </volume> <year> 1997. </year>
Reference: [7] <author> Buddhikot, M., Parulkar, G., and Gopalakrish-nan, R., </author> <title> "Scalable Multimedia-On-Demand via World-Wide-Web (WWW) with QOS Guarantees," </title> <booktitle> Proceedings of Sixth International Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <address> NOSSDAV96, Zushi, Japan, </address> <month> April 23-26, </month> <year> 1996. </year>
Reference-contexts: The resulting benefits are minimizing expensive context switches, efficient con-currency control, efficient dispatching of upcalls, and elimination of the need for concurrency control between rtus [22]. Several examples of the effectiveness of rtus in providing excellent qos guarantees for me dia processing and user-level-protocol processing have been reported in <ref> [7, 22] </ref>. The rtu facility has already been demonstrated to be useful for high performance user level protocol implementations.
Reference: [8] <author> Buddhikot, M., Wu, D., Jane, X., and Parulkar, G., </author> <title> "Project MARS: Experimental Scalable and High performance Multimedia-On-Demand Services and Servers," </title> <institution> Washington University, Department of Computer Scince, </institution> <note> Technical report (in preparation). </note>
Reference-contexts: We used these system calls in our experiments described in the next section. The design and implementation of these system calls are out of the scope of this report and can be found in <ref> [8] </ref>. 5 Performance Evaluation In this section, we will describe the experiments carried out to characterize the performance benefits of our solutions. We have successfully implemented the mmbuf system, priority queueing in the scsi driver and the new stream api (system calls) in the latest release of NetBSD. <p> These enhancements have also been integrated with the ccd driver, the rtu mechanism and a locally developed driver for the atm interface from Efficient Networks [10]. Also, experimental prototypes of a single node as well as a distributed multi-node mars video server using these enhancements are completed <ref> [8] </ref>. In all the experiments described here, we used a 200MHz Pentium PC with 128 mb ram, eni atm interface, and an Adaptec dual scsi ahc-3940, running the enhanced NetBSD 1.2G kernel. We connected two 9 gb Seagate baracudda scsi disks to the controller. <p> However, in our design, the frame level info is completely independent of the inode information about the data file and can be potentially stored on different storage and/or file system <ref> [8] </ref>. Unlike symphony, we follow design advocated in [31] and keep our file system in the kernel. Also, we support an efficient zero-copy data path for network destined storage retrievals.
Reference: [9] <author> Bernhardt, C., and Biersack, E., </author> <title> "A Scalable Video Server: </title> <booktitle> Architecture, Design and Implementation," In Proceedings of the Real-time Systems Conference, </booktitle> <pages> pp. 63-72, </pages> <address> Paris, France, </address> <month> Jan. </month> <year> 1995. </year>
Reference: [10] <author> Cranor, C., </author> <title> "BSD ATM," Release Notes, </title> <institution> Wash--ington University in St. Louis, </institution> <month> Jul 3, </month> <year> 1996. </year>
Reference-contexts: These enhancements have also been integrated with the ccd driver, the rtu mechanism and a locally developed driver for the atm interface from Efficient Networks <ref> [10] </ref>. Also, experimental prototypes of a single node as well as a distributed multi-node mars video server using these enhancements are completed [8].
Reference: [11] <author> Cranor, C., and Parulkar, G, </author> <title> "Design of Universal Continuous Media I/O," </title> <booktitle> Proceedings of NOSS-DAV95, </booktitle> <pages> pp 83-86, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: A novel feature of these calls is that they allow aggregation of multiple read/send requests for same or different active streams into a single system call, much like a supercall <ref> [11] </ref>. Such aggregation significantly minimizes system call overheads especially under heavy loads.
Reference: [12] <author> Druschel, P., and Peterson, L., "Fbufs: </author> <title> A high-bandwidth cross domain transfer facility," </title> <booktitle> Pro ceedings of 14 th SOSP, </booktitle> <pages> pp. 1892-202, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: level application can effectively make use of such real-time service guarantees from the disk driver only if the cpu scheduler provides qos guarantees in the form of periodic execution. 1.1 Research Contributions We would like to note that the shortcomings discussed above have been well known to be performance bottlenecks <ref> [2, 12, 13, 21, 26] </ref>. However, none of the earlier solutions are complete and research efforts such as [3, 4, 15, 18, 25, 28] are underway to rectify them. (Please see Section 6). Our work shares some common ideas and objectives with these research efforts. <p> The idea of minimizing data copy to achieve higher performance is well known and has been reported in early operating systems such as Tenex [2] and Accent [26]. The Container Shipping system [21], the dash ipc [29] and fbufs <ref> [12] </ref> have addressed the problem of minimizing physical data movement across protection domains in an os by employing virtual memory re-mapping techniques. However, none of these projects report design of the zero-copy i/o between disks and networks.
Reference: [13] <author> Fall, K., and Pasquale, J., </author> <title> "Exploiting In-Kernel Data Paths to Improve I/O Throughput and CPU Availability", </title> <booktitle> Proceedings of the USENIX Winter Technical Conference, </booktitle> <address> San Diego, Cali-fornia, </address> <month> January </month> <year> 1993, </year> <pages> pp. 327-333. </pages>
Reference-contexts: level application can effectively make use of such real-time service guarantees from the disk driver only if the cpu scheduler provides qos guarantees in the form of periodic execution. 1.1 Research Contributions We would like to note that the shortcomings discussed above have been well known to be performance bottlenecks <ref> [2, 12, 13, 21, 26] </ref>. However, none of the earlier solutions are complete and research efforts such as [3, 4, 15, 18, 25, 28] are underway to rectify them. (Please see Section 6). Our work shares some common ideas and objectives with these research efforts. <p> We believe that the Genie framework can support a zero copy data path between a file system and network protocol stack. However, currently no such design has been reported. Kevin Fall et al.'s <ref> [13, 14] </ref> work on providing in-kernel data paths to improve i/o throughput and cpu availability has goals very similar to ours, namely, minimizing data copies and supporting asynchronous and concurrent i/o operations. They have designed and implemented a mechanism called Splice in Ul-tirix 4.2 operating system to meet these goals.
Reference: [14] <author> Fall, K., and Pasquale, J., </author> <title> "Improving Continuous-Media Playback Performance With In-Kernel Data Paths", </title> <booktitle> Proceedings of the IEEE International Conference on Multimedia Computing and Systems (ICMCS), </booktitle> <address> Boston, MA, </address> <month> June </month> <year> 1994, </year> <pages> pp. 100-109 </pages>
Reference-contexts: We believe that the Genie framework can support a zero copy data path between a file system and network protocol stack. However, currently no such design has been reported. Kevin Fall et al.'s <ref> [13, 14] </ref> work on providing in-kernel data paths to improve i/o throughput and cpu availability has goals very similar to ours, namely, minimizing data copies and supporting asynchronous and concurrent i/o operations. They have designed and implemented a mechanism called Splice in Ul-tirix 4.2 operating system to meet these goals.
Reference: [15] <author> Goyal, P., Guo, X., Vin, H.M., </author> <title> "A Hierarchical CPU Scheduler for Multimedia Operating Systems," </title> <booktitle> 2nd Symposium on Operating Systems Design and Implementation (OSDI96), Oct. </booktitle> <volume> 96, </volume> <pages> pp. 107-121. </pages>
Reference-contexts: However, none of the earlier solutions are complete and research efforts such as <ref> [3, 4, 15, 18, 25, 28] </ref> are underway to rectify them. (Please see Section 6). Our work shares some common ideas and objectives with these research efforts. This paper describes our innovative ideas and details the software infrastructure we have developed to implement these ideas.
Reference: [16] <author> Kawachiya, K., Tokuda, H., "Q-Thread: </author> <title> A New Execution Model for Dynamic QOS Control of Continuous-Media Processing," </title> <address> NOSSDAV 96, Japan, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: The rtu facility has already been demonstrated to be useful for high performance user level protocol implementations. A more detailed discussion of the implementation and related work in this area such as Scheduler Activations [1], Processor Capacity Reserves [20], Q-threads <ref> [16] </ref> etc. can be found in [22, 24]. 4.6 Streams api We have designed an api consisting of a new set of system calls that allow applications to access mm-bufs and real-time guarantees from the scsi driver for network destined disk retrievals.
Reference: [17] <author> Kleiman, S., </author> <title> "Design of vnode interface," </title> <booktitle> Proceedings of the USENIX Symposium, </booktitle> <year> 1986. </year>
Reference: [18] <author> Khanna, S., et. al., </author> <title> "Real-time Scheduling in SunOS5.0," </title> <booktitle> USENIX, Winter 1992, </booktitle> <address> pp.375-390. </address>
Reference-contexts: However, none of the earlier solutions are complete and research efforts such as <ref> [3, 4, 15, 18, 25, 28] </ref> are underway to rectify them. (Please see Section 6). Our work shares some common ideas and objectives with these research efforts. This paper describes our innovative ideas and details the software infrastructure we have developed to implement these ideas.
Reference: [19] <editor> McKusik, M., et al. </editor> <booktitle> "The Design and Implementation of the 4.4 BSD Operating System," </booktitle> <publisher> Addi-son Wesley, </publisher> <year> 1996. </year>
Reference: [20] <author> Mercer, C.W., Savage, S., Tokuda, H., </author> <title> "Processor Capacity Reserves: Operating System Support for Multimedia Applicati ons," </title> <booktitle> IEEE Intl. Conf. on Multimedia Computing and Systems, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: The rtu facility has already been demonstrated to be useful for high performance user level protocol implementations. A more detailed discussion of the implementation and related work in this area such as Scheduler Activations [1], Processor Capacity Reserves <ref> [20] </ref>, Q-threads [16] etc. can be found in [22, 24]. 4.6 Streams api We have designed an api consisting of a new set of system calls that allow applications to access mm-bufs and real-time guarantees from the scsi driver for network destined disk retrievals.
Reference: [21] <author> Pasquale, Joseph, Anderson, Eric, and Muller, P. Keith, </author> <title> "Container Shipping: operating system support for i/o intensive applications," </title> <journal> IEEE Computer Magazine, </journal> <volume> 27 (3): </volume> <pages> 84-93, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: level application can effectively make use of such real-time service guarantees from the disk driver only if the cpu scheduler provides qos guarantees in the form of periodic execution. 1.1 Research Contributions We would like to note that the shortcomings discussed above have been well known to be performance bottlenecks <ref> [2, 12, 13, 21, 26] </ref>. However, none of the earlier solutions are complete and research efforts such as [3, 4, 15, 18, 25, 28] are underway to rectify them. (Please see Section 6). Our work shares some common ideas and objectives with these research efforts. <p> The idea of minimizing data copy to achieve higher performance is well known and has been reported in early operating systems such as Tenex [2] and Accent [26]. The Container Shipping system <ref> [21] </ref>, the dash ipc [29] and fbufs [12] have addressed the problem of minimizing physical data movement across protection domains in an os by employing virtual memory re-mapping techniques. However, none of these projects report design of the zero-copy i/o between disks and networks.
Reference: [22] <author> Gopal, R., </author> <title> "Efficient Quality of Service Support in Computer Operating systems for High Speed Networking and Multimedia," </title> <type> Doctoral Dissertation, </type> <institution> Washington University in St. Louis, </institution> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: management system called Multimedia M-buf (mmbuf) which shortens the data path from a storage device to network output device, (2) priority queueing within the scsi driver to differentiate between real-time and non-real-time streams, and (3) integration of these new os services with a cpu scheduling mechanism called Real Time Upcall <ref> [22] </ref> and a software disk striping driver called Concatenated Disk (ccd). We have created a new system call api for applications to access these services. We have implemented these ideas in NetBSD 1.2G and demonstrated clear performance improvements. <p> We believe that though ccd is sub-optimal, it represents a very simple and cost-effective way to build small disk arrays. 4.5 Periodic cpu access: Real Time Up call (RTU) Real Time Upcalls is a novel mechanism designed and implemented within our research group at Wash-ington University <ref> [22] </ref> to provide guaranteed cpu access to user level and kernel level periodic tasks. rtus are an alternative to real-time periodic threads and have advantages such as low implementation complexity, portability, and efficiency. Figure 9 illustrates the basic concept behind rtus. <p> Figure 9 illustrates the basic concept behind rtus. An rtu is essentially a function in a user program that is invoked periodically in real-time to perform certain activity <ref> [22] </ref>. Various examples of such activities are protocol processing such as tcp, udp, multimedia and bulk data processing, and periodic data retrievals from storage systems. <p> It uses a variant of the Rate Monotonic (rm) scheduling policy. The main feature of this policy is that there is no asynchronous preemption. The resulting benefits are minimizing expensive context switches, efficient con-currency control, efficient dispatching of upcalls, and elimination of the need for concurrency control between rtus <ref> [22] </ref>. Several examples of the effectiveness of rtus in providing excellent qos guarantees for me dia processing and user-level-protocol processing have been reported in [7, 22]. The rtu facility has already been demonstrated to be useful for high performance user level protocol implementations. <p> The resulting benefits are minimizing expensive context switches, efficient con-currency control, efficient dispatching of upcalls, and elimination of the need for concurrency control between rtus [22]. Several examples of the effectiveness of rtus in providing excellent qos guarantees for me dia processing and user-level-protocol processing have been reported in <ref> [7, 22] </ref>. The rtu facility has already been demonstrated to be useful for high performance user level protocol implementations. <p> The rtu facility has already been demonstrated to be useful for high performance user level protocol implementations. A more detailed discussion of the implementation and related work in this area such as Scheduler Activations [1], Processor Capacity Reserves [20], Q-threads [16] etc. can be found in <ref> [22, 24] </ref>. 4.6 Streams api We have designed an api consisting of a new set of system calls that allow applications to access mm-bufs and real-time guarantees from the scsi driver for network destined disk retrievals. <p> Clearly, these measurements indicate that our os enhancements provide qos guarantees and significant improvements in throughput on the data-path from the disks to the network interface. The research contributions described in this paper combined with new cpu scheduling mechanism such as rtus <ref> [22] </ref> makes 4.4 bsd unix a strong candidate for a true multimedia operating system. Also, note that, since, our work is based on a public-domain operating system, all the os extensions are freely available (via web or ftp) to interested parties.
Reference: [23] <author> Gopalakrishnan, R., Parulkar, G.M., </author> <title> "A Framework for QoS Guarantees for Multimedia Applications within an End-system," </title> <booktitle> Swiss German Computer Science Society Conf., </booktitle> <year> 1995. </year>
Reference: [24] <author> Gopalakrishnan, R., Parulkar, G.M., </author> <title> "A Real-time Upcall Facility for Protocol Processing with QOS Guarantees," </title> <booktitle> (Poster) ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Copper Mountain, Colorado, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The rtu facility has already been demonstrated to be useful for high performance user level protocol implementations. A more detailed discussion of the implementation and related work in this area such as Scheduler Activations [1], Processor Capacity Reserves [20], Q-threads [16] etc. can be found in <ref> [22, 24] </ref>. 4.6 Streams api We have designed an api consisting of a new set of system calls that allow applications to access mm-bufs and real-time guarantees from the scsi driver for network destined disk retrievals.
Reference: [25] <author> Reddy, A., L., and Wyllie, </author> <title> J."Disk Scheduling Algorithms for Multimedia Operating Systems," </title> <booktitle> Proceedings of ACM Multimedia'93, </booktitle> <address> Anaheim, CA, </address> <pages> pp. 225-234, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: However, none of the earlier solutions are complete and research efforts such as <ref> [3, 4, 15, 18, 25, 28] </ref> are underway to rectify them. (Please see Section 6). Our work shares some common ideas and objectives with these research efforts. This paper describes our innovative ideas and details the software infrastructure we have developed to implement these ideas. <p> The low level disk driver always processes request from the tt work queue in fifo fashion and therefore, the work queue must be sorted using an appropriate disk scheduling algorithm, such as the bsd elevator algorithm or newer scheduling algorithms <ref> [25, 28, 32] </ref> to minimize seek and rotation latencies in each round. 4.3 Implementation of drr with two pri ority queues We describe a prototype implementation of drr fair queueing in the current bsd unix. <p> Some of the key similarities and differences of our work from this project are as follows: like symphony, our work also employs differentiation of disk retrievals into multiple priority classes and provides hooks for implementing suitable disk scheduling policies such as scan-edf, cscan,, symphony disk scheduler, or Grouped Sweep Scheduling <ref> [25, 28, 32] </ref> and associated admission control algorithms. Similar to Symphony, our meta information is two level: the frame level meta info and the traditional unix inode information.
Reference: [26] <author> Rashid, R., and Robertson, G., </author> <title> "Accent: </title>
Reference-contexts: level application can effectively make use of such real-time service guarantees from the disk driver only if the cpu scheduler provides qos guarantees in the form of periodic execution. 1.1 Research Contributions We would like to note that the shortcomings discussed above have been well known to be performance bottlenecks <ref> [2, 12, 13, 21, 26] </ref>. However, none of the earlier solutions are complete and research efforts such as [3, 4, 15, 18, 25, 28] are underway to rectify them. (Please see Section 6). Our work shares some common ideas and objectives with these research efforts. <p> In the following, however, we try to strike a balance between recent active projects and research widely cited in literature. The idea of minimizing data copy to achieve higher performance is well known and has been reported in early operating systems such as Tenex [2] and Accent <ref> [26] </ref>. The Container Shipping system [21], the dash ipc [29] and fbufs [12] have addressed the problem of minimizing physical data movement across protection domains in an os by employing virtual memory re-mapping techniques. However, none of these projects report design of the zero-copy i/o between disks and networks.
References-found: 26

