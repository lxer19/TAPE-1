URL: http://www.cs.berkeley.edu/~tea/hb.ps
Refering-URL: http://www.cs.washington.edu/homes/tom/
Root-URL: 
Title: Thread Management for Shared-Memory Multiprocessors  
Author: Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy 
Keyword: Index Terms thread, multiprocessor, operating system, parallel programming, performance  
Address: Seattle WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Threads, or "lightweight processes," have become a common and necessary component of new languages and operating systems. Threads allow the programmer or compiler to express, create, and control parallel activities, contributing to the structure and performance of programs. In this article, we discuss the many alternatives that present themselves when designing a support system for threads on a shared-memory multiprocessor. These alternatives influence the ease, granularity, and performance of parallel programming. We conclude with a brief survey of three contemporary thread management systems (Windows NT, Presto, and Multilisp), using them to illustrate the issues raised in this article. 
Abstract-found: 1
Intro-found: 1
Reference: [Anderson et al. 89] <author> Anderson, T. E., Lazowska, E. D., and Levy, H. M. </author> <title> The Performance Implications of Thread Management Alternatives for Shared Memory Multiprocessors. </title> <booktitle> In 1989 ACM SIGMETRICS and Performance '89 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 49-60, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: As the cost of thread operations begins to approach that of a few procedure calls, several issues become performance-critical that, for slower operations, would merely be second-order effects. Simplicity in the thread system's implementation is crucial to performance <ref> [Anderson et al. 89] </ref>. There is a performance advantage to building multiple thread systems, each tuned for a single type of application.
Reference: [Bershad et al. 88] <author> Bershad, B., Lazowska, E., and Levy, H. </author> <title> PRESTO: A System for Object-Oriented Parallel Programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In the next section, we describe the functionality found in thread management systems. Section 3 discusses a number of thread design issues. In Section 4, we survey three systems for shared-memory multiprocessors, Windows NT [Custer 93], Presto <ref> [Bershad et al. 88] </ref>, and Multilisp [Halstead 85], focusing our attention on how they have addressed the issues raised in this article. 2 Thread Management Concepts 2.1 Address Spaces, Threads, and Multiprocessing An address space is the set of memory locations that can be generated and accessed directly by a program.
Reference: [Custer 93] <author> Custer, H. </author> <title> Inside Windows NT. </title> <publisher> Microsoft Press, </publisher> <year> 1993. </year>
Reference-contexts: In the next section, we describe the functionality found in thread management systems. Section 3 discusses a number of thread design issues. In Section 4, we survey three systems for shared-memory multiprocessors, Windows NT <ref> [Custer 93] </ref>, Presto [Bershad et al. 88], and Multilisp [Halstead 85], focusing our attention on how they have addressed the issues raised in this article. 2 Thread Management Concepts 2.1 Address Spaces, Threads, and Multiprocessing An address space is the set of memory locations that can be generated and accessed directly
Reference: [Dijkstra 68] <author> Dijkstra, E. W. </author> <title> Cooperating Sequential Processes. </title> <booktitle> In Programming Languages, </booktitle> <pages> pages 43-112. </pages> <publisher> Academic Press, </publisher> <year> 1968. </year>
Reference-contexts: Concurrent programming has a long history. The operation of programs that must handle real-world concurrency (e.g., operating systems, database systems, and network file servers) can be complex and difficult to understand. Dijkstra <ref> [Dijkstra 68] </ref> and Hoare [Hoare 74, Hoare 78] showed that these programs can be simplified when structured as cooperating sequential threads that communicate at discrete points within the program.
Reference: [Enc 86] <institution> Encore Computer Corporation. </institution> <note> UMAX 4.2 Programmer's Reference Manual, </note> <year> 1986. </year>
Reference-contexts: Some UNIX systems allow memory regions to be set up as shared between processes; any data in the shared region can be accessed by more than one process without having to send a message by way of the operating system. The Sequent Symmetry's DYNIX [Seq 88] and Encore's UMAX <ref> [Enc 86] </ref> are operating systems that provide support for multiprocessing based on shared memory between UNIX processes.
Reference: [Halstead 85] <author> Halstead, R. </author> <title> Multilisp: A Language for Concurrent Symbolic Computation. </title> <journal> ACM Transaction on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: In the next section, we describe the functionality found in thread management systems. Section 3 discusses a number of thread design issues. In Section 4, we survey three systems for shared-memory multiprocessors, Windows NT [Custer 93], Presto [Bershad et al. 88], and Multilisp <ref> [Halstead 85] </ref>, focusing our attention on how they have addressed the issues raised in this article. 2 Thread Management Concepts 2.1 Address Spaces, Threads, and Multiprocessing An address space is the set of memory locations that can be generated and accessed directly by a program.
Reference: [Hoare 74] <author> Hoare, C. A. R. </author> <title> Monitors: An Operating System Structuring Concept. </title> <journal> Communications of the ACM, </journal> <volume> 17(10) </volume> <pages> 549-557, </pages> <month> October </month> <year> 1974. </year>
Reference-contexts: Concurrent programming has a long history. The operation of programs that must handle real-world concurrency (e.g., operating systems, database systems, and network file servers) can be complex and difficult to understand. Dijkstra [Dijkstra 68] and Hoare <ref> [Hoare 74, Hoare 78] </ref> showed that these programs can be simplified when structured as cooperating sequential threads that communicate at discrete points within the program.
Reference: [Hoare 78] <author> Hoare, C. A. R. </author> <title> Communicating Sequential Processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-677, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: Concurrent programming has a long history. The operation of programs that must handle real-world concurrency (e.g., operating systems, database systems, and network file servers) can be complex and difficult to understand. Dijkstra [Dijkstra 68] and Hoare <ref> [Hoare 74, Hoare 78] </ref> showed that these programs can be simplified when structured as cooperating sequential threads that communicate at discrete points within the program.
Reference: [Redell et al. 80] <author> Redell, D. D., Dalal, Y. K., Horsley, T. R., Lauer, H. C., Lynch, W. C., McJones, P. R., Murray, H. G., and Purcell, S. C. </author> <title> Pilot: An Operating System for a Personal Computer. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 81-92, </pages> <month> February </month> <year> 1980. </year> <month> 18 </month>
Reference-contexts: Separate address spaces are not needed on dedicated systems to protect against malicious users; software errors can crash the 3 system but at least are localized to one user, one machine. Even single-user systems can have concurrency, however. More sophisticated systems, such as Xerox's Pilot <ref> [Redell et al. 80] </ref>, provide only one address space per machine, but support multiple threads within that single address space. Because any thread can access any memory location, Pilot provides a compiler with strong type-checking to decrease the likelihood that one thread will corrupt the data structures of another.
Reference: [Seq 88] <institution> Sequent Computer Systems, Inc. </institution> <type> Symmetry Technical Summary, </type> <year> 1988. </year>
Reference-contexts: Some UNIX systems allow memory regions to be set up as shared between processes; any data in the shared region can be accessed by more than one process without having to send a message by way of the operating system. The Sequent Symmetry's DYNIX <ref> [Seq 88] </ref> and Encore's UMAX [Enc 86] are operating systems that provide support for multiprocessing based on shared memory between UNIX processes.
Reference: [Tevanian et al. 87] <author> Tevanian, A., Rashid, R. F., Golub, D. B., Black, D. L., Cooper, E., and Young, M. W. </author> <title> Mach Threads and the Unix Kernel: The Battle for Control. </title> <booktitle> In Proceedings of the 1987 USENIX Summer Conference, </booktitle> <pages> pages 185-197, </pages> <year> 1987. </year> <month> 19 </month>
Reference-contexts: The Sequent Symmetry's DYNIX [Seq 88] and Encore's UMAX [Enc 86] are operating systems that provide support for multiprocessing based on shared memory between UNIX processes. More sophisticated operating systems for shared-memory multiprocessors, such as Microsoft's Windows NT and Carnegie Mellon University's Mach operating system <ref> [Tevanian et al. 87] </ref> support multiple address spaces and multiple threads within each address space. Threads in the same address space communicate directly with one another using shared memory; threads communicate across address space boundaries using messages.
References-found: 11

