URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93368.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: fgagan, als, saltzg@cs.umd.edu  
Title: An Integrated Runtime and Compile-time Approach for Parallelizing Structured and Block Structured Applications  
Author: Gagan Agrawal, Alan Sussman Joel Saltz 
Address: College Park, MD 20742  College Park, MD 20742  
Affiliation: Department of Computer Science, University of Maryland,  UMIACS and Dept. of Computer Sc., University of Maryland,  
Abstract: Scientific and engineering applications often involve structured meshes. These meshes may be nested (for multigrid codes) and/or irregularly coupled (called multiblock or irregularly coupled regular mesh problems). In this paper, we present a combined runtime and compile-time approach for parallelizing these applications on distributed memory parallel machines in an efficient and machine-independent fashion. We have designed and implemented a runtime library which can be used to port these applications on distributed memory machines. The library is currently implemented on several different systems. To further ease the task of application programmers, we have developed methods for integrating this runtime library with compilers for HPF-like parallel programming languages. We discuss how we have integrated this runtime library with the Fortran 90D compiler being developed at Syracuse University. We present experimental results to demonstrate the efficacy of our approach. We have experimented with a multiblock Navier-Stokes solver template and a multigrid code. Our experimental results show that our primitives have low runtime communication overheads. Further, the compiler parallelized codes perform within 20% of the code parallelized by manually inserting calls to the runtime library.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Christopher A. Atwood. </author> <title> Selected computations of transonic cavity flows. </title> <booktitle> In Proceedings of the 1993 ASME Fluids Engineering Conference, Forum on Computational Aero- and Hydro-Acoustics, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. Multiblock applications are used in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts <ref> [1, 8, 31] </ref>, large scale climate modeling [14], reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23].
Reference: [2] <author> M.J. Berger and P. Colella. </author> <title> Local adaptive mesh refinement for shock hydrodynamics. </title> <journal> Journal of Computational Physics, </journal> <volume> 82 </volume> <pages> 67-84, </pages> <year> 1989. </year>
Reference-contexts: Boundary updates require communication between blocks, which is restricted to moving regular array sections (possibly including non-unit strides). Multiblock grids are frequently used for modeling geometrically complex objects which cannot be easily 2 modeled using a single regular mesh <ref> [2, 3, 24, 29, 36] </ref>. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid.
Reference: [3] <author> M.J. Berger and J. Oliger. </author> <title> Adaptive mesh refinement for hyperbolic partial differential equations. </title> <journal> Journal of Computational Physics, </journal> <volume> 53 </volume> <pages> 484-512, </pages> <year> 1984. </year>
Reference-contexts: Boundary updates require communication between blocks, which is restricted to moving regular array sections (possibly including non-unit strides). Multiblock grids are frequently used for modeling geometrically complex objects which cannot be easily 2 modeled using a single regular mesh <ref> [2, 3, 24, 29, 36] </ref>. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid.
Reference: [4] <author> Zeki Bozkus, Alok Choudhary, Geoffrey Fox, Tomasz Haupt, Sanjay Ranka, and Min-You Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <note> Submitted to the Journal of Parallel and Distributed Computing, </note> <month> March </month> <year> 1993. </year>
Reference-contexts: We present methods by which the compilers for HPF style parallel programming languages can automatically generate calls to our runtime primitives. We discuss how we have integrated our runtime primitives with the Fortran 90D compiler being developed at Syracuse University <ref> [4] </ref>. While the design of our runtime system was initially motivated by multigrid and multiblock applications, our primitives can also be used in many cases for parallelizing computations with regular data access patterns. We present experimental results to demonstrate the efficacy of our approach. <p> Since, in typical multiblock and multigrid applications, the number of blocks and their respective sizes is not known until runtime, the distribution of blocks onto processors is done at runtime. The distributed array descriptors (DAD) <ref> [4] </ref> for the arrays representing these blocks are, therefore, generated at runtime. Distributed array descriptors contain information about the portions of the arrays residing on each processor, and are used at runtime for performing communication and distributing loops iterations. <p> The irregular communication arising from use of indirection arrays can be handled using the Parti primitives for irregular problems [10], which have also been integrated with compilers for HPF-style languages (including the Rice University Fortran 77D compiler [18] and the Syracuse University Fortran 90D compiler <ref> [4] </ref>). F90D and HPF also provide a number of intrinsic functions (such as reduction, spread, etc.). We assume that if a computation can be done using these intrinsics, it is either written this way by the programmer or is recognized by the compiler in an early phase of the compilation. <p> If the offsets are not compile- time constants, then we use a regular section move to handle communication. This situation can also be handled by shifts into a temporary array <ref> [4] </ref>. The fourth condition says that along dimension j a loop variable does not appear in either the left hand side or the right hand side index and the loop invariant scalars are different.
Reference: [5] <author> W. Briggs. </author> <title> A Multigird Tutorial. </title> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: One class of scientific and engineering applications involves structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) [9]. Multigrid is a common technique for accelerating the solution of partial-differential equations <ref> [5, 30] </ref>. Multigrid codes employ a number of meshes at different levels of resolution. The restriction and prolongation operations for shifting between different multigrid levels require moving regular array sections [19] with non-unit strides. In multiblock problems, the data is divided into several interacting regions (called blocks or subdomains).
Reference: [6] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Recently there have been major efforts in developing programming language and compiler support for distributed memory machines. Based on the initial works of projects like Fortran D [13, 22] and Vienna Fortran <ref> [6, 41] </ref> , the High Performance Fortran Forum has recently proposed the first version of High Performance 1 This work was supported by ARPA under contract No. NAG-1-1485, by NSF under grant No. ASC 9213821 and by ONR under contract No. SC 292-1-22913. <p> Reducing communication costs is crucial in achieving good performance on applications [20, 21]. While current systems like the Fortran D project [22] and the Vienna Fortran Compilation system <ref> [6] </ref> have implemented a number of optimizations for reducing communication costs (like message blocking, collective communication, message coalescing and aggregation), these optimizations have been developed only in the context of regular problems (i.e. in code having only regular data access patterns). <p> The interpolation required during the prolongation step in multigrid codes also involves interaction among the neighboring array elements. Such communication is handled by allocation of extra space at the beginning and end of each array dimension on each processor. These extra elements are called overlap, or ghost, cells <ref> [6, 16, 27] </ref>. Depending upon the data access pattern in a loop, the required data is copied from other processors and is stored in the overlap cells. In our runtime system, communication is performed in two phases. <p> The communication primitives include a procedure Overlap Cell Fill Sched, which computes a schedule that is used to direct the filling of overlap cells along a given dimension of a distributed array. Communication for filling in the overlap cells has been implemented in other systems for regular computations <ref> [6, 16, 27] </ref>, so we will not be discussing the details here. The primitive Regular Section Copy Sched carries out the preprocessing required for performing the regular section moves.
Reference: [7] <author> Siddhartha Chatterjee, John R. Gilbert, Fred J.E. Long, Robert Schreiber, and Shang-Hua Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 149-158, </pages> <month> May </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 28, No. 7. </volume> <pages> 27 </pages>
Reference-contexts: In our current system, this analysis is always done at runtime. However, if the distributions of source and destination distributeed arrays and description of source and destination regular sections are available at compile-time, then this analysis can be done at compile-time as well. In separate works, Chatterjee et al <ref> [7] </ref>, Stichnoth [37] and Gupta et al [17] have developed compile-time methods for analyzing and generating communication associated with HPF's forall statements and/or F90 style array expressions.
Reference: [8] <author> Kalpana Chawla and William R. Van Dalsem. </author> <title> Numerical simulation of a powered-lift landing. In Pro--ceedings of the 72nd Fluid Dynamics Panel Meeting and Symposium on Computational and Experimental Assessment of Jets in Cross Flow, </title> <address> Winchester, UK. AGARD, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. Multiblock applications are used in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts <ref> [1, 8, 31] </ref>, large scale climate modeling [14], reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23].
Reference: [9] <author> A. Choudhary, G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, S. Ranka, and J. Saltz. </author> <title> Software support for irregular and loosely synchronous problems. </title> <booktitle> Computing Systems in Engineering, 3(1-4):43-52, 1992. Papers presented at the Symposium on High-Performance Computing for Flight Vehicles, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: One class of scientific and engineering applications involves structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) <ref> [9] </ref>. Multigrid is a common technique for accelerating the solution of partial-differential equations [5, 30]. Multigrid codes employ a number of meshes at different levels of resolution. The restriction and prolongation operations for shifting between different multigrid levels require moving regular array sections [19] with non-unit strides.
Reference: [10] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems data copy reuse and runtime partitioning. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Languages, Compilers and Runtime Environments for Distributed Memory Machines, </booktitle> <pages> pages 185-220. </pages> <publisher> Elsevier, </publisher> <year> 1992. </year>
Reference-contexts: Special effort is required in developing compiler and runtime support for applications that do not necessarily have regular data access patterns. Our group has already developed compiler embedded runtime support for completely irregular computations (i.e. codes in which distributed arrays are accessed based on indirection arrays) <ref> [10, 11, 18] </ref>. One class of scientific and engineering applications involves structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) [9]. Multigrid is a common technique for accelerating the solution of partial-differential equations [5, 30]. <p> We do not consider applications in which indirection arrays may need to be analyzed to identify communication patterns. The irregular communication arising from use of indirection arrays can be handled using the Parti primitives for irregular problems <ref> [10] </ref>, which have also been integrated with compilers for HPF-style languages (including the Rice University Fortran 77D compiler [18] and the Syracuse University Fortran 90D compiler [4]). F90D and HPF also provide a number of intrinsic functions (such as reduction, spread, etc.).
Reference: [11] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <institution> Technical Report CS-TR-3076 and UMIACS-TR-93-42, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> May </month> <year> 1993. </year> <note> To appear in LCPC '93. </note>
Reference-contexts: Special effort is required in developing compiler and runtime support for applications that do not necessarily have regular data access patterns. Our group has already developed compiler embedded runtime support for completely irregular computations (i.e. codes in which distributed arrays are accessed based on indirection arrays) <ref> [10, 11, 18] </ref>. One class of scientific and engineering applications involves structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) [9]. Multigrid is a common technique for accelerating the solution of partial-differential equations [5, 30].
Reference: [12] <author> D. Loveman (Ed.). </author> <title> Draft High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: NAS1-19480 and NAS1-18605 while the authors Sussman and Saltz were in residence at the Institute for Computer Applications in Science and Engineering (ICASE), NASA Langley Research Center, Hampton, VA 23681. The authors assume all responsibility for the contents of the paper. 1 Fortran (HPF) <ref> [12] </ref>. HPF allows programmer to specify the layout of distributed data and specify parallelism through operations on array sections and through parallel loops. Proposed HPF compilers are being designed to produce Single Program Multiple Data (SPMD) Fortran 77 (F77) code with message passing and/or runtime communication primitives. <p> Regular section move primitives can be used for handling the communication required when a distribution of an array is changed (using the redistributed statement of HPF <ref> [12] </ref>. They can also be used to handle the communication required for filling ghost cells when the data distribution is cyclic or block-cyclic. The primitives can also be used for handling communication in forall loops and array expressions in many regular applications, especially when strides are involved.
Reference: [13] <author> Geoffrey Fox, Seema Hiranandani, Ken Kennedy, Charles Koelbel, Uli Kremer, Chau-Wen Tseng, and Min-You Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report CRPC-TR90079, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Current distributed memory machines have good communication bandwidths, but they also have high startup latencies which often result in high communication overheads. Recently there have been major efforts in developing programming language and compiler support for distributed memory machines. Based on the initial works of projects like Fortran D <ref> [13, 22] </ref> and Vienna Fortran [6, 41] , the High Performance Fortran Forum has recently proposed the first version of High Performance 1 This work was supported by ARPA under contract No. NAG-1-1485, by NSF under grant No. ASC 9213821 and by ONR under contract No. SC 292-1-22913.
Reference: [14] <institution> Survey of principal investigators of grand challenge applications: Workshop on grand challenge applications and software technology, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Multiblock applications are used in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts [1, 8, 31], large scale climate modeling <ref> [14] </ref>, reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23]. In this paper we present a combined runtime and compile-time approach for parallelizing this general class of applications on distributed memory machines. <p> Multiblock applications are used in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts [1, 8, 31], large scale climate modeling <ref> [14] </ref>, reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23]. In this paper we present a combined runtime and compile-time approach for parallelizing this general class of applications on distributed memory machines. <p> Multiblock applications are used in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts [1, 8, 31], large scale climate modeling <ref> [14] </ref>, reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23]. In this paper we present a combined runtime and compile-time approach for parallelizing this general class of applications on distributed memory machines. <p> applications are used in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts [1, 8, 31], large scale climate modeling <ref> [14] </ref>, reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23]. In this paper we present a combined runtime and compile-time approach for parallelizing this general class of applications on distributed memory machines.
Reference: [15] <author> Al Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: We view these primitives as forming a portion of a portable, compiler independent, runtime support library. This library is currently implemented on the Intel iPSC/860, the Thinking Machines' CM-5 and the PVM message passing environment for network of workstations <ref> [15] </ref>. The design of the library is architecture independent and therefore it can be easily ported on any distributed memory parallel machine or any environment which supports message passing (e.g. Express). The library primitives can currently be invoked from Fortran or C programs.
Reference: [16] <author> Michael Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The interpolation required during the prolongation step in multigrid codes also involves interaction among the neighboring array elements. Such communication is handled by allocation of extra space at the beginning and end of each array dimension on each processor. These extra elements are called overlap, or ghost, cells <ref> [6, 16, 27] </ref>. Depending upon the data access pattern in a loop, the required data is copied from other processors and is stored in the overlap cells. In our runtime system, communication is performed in two phases. <p> The communication primitives include a procedure Overlap Cell Fill Sched, which computes a schedule that is used to direct the filling of overlap cells along a given dimension of a distributed array. Communication for filling in the overlap cells has been implemented in other systems for regular computations <ref> [6, 16, 27] </ref>, so we will not be discussing the details here. The primitive Regular Section Copy Sched carries out the preprocessing required for performing the regular section moves.
Reference: [17] <author> S.K.S. Gupta, S.D. Kaushik, S. Mufti, S. Sharma, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed memory machines. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: However, if the distributions of source and destination distributeed arrays and description of source and destination regular sections are available at compile-time, then this analysis can be done at compile-time as well. In separate works, Chatterjee et al [7], Stichnoth [37] and Gupta et al <ref> [17] </ref> have developed compile-time methods for analyzing and generating communication associated with HPF's forall statements and/or F90 style array expressions.
Reference: [18] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Special effort is required in developing compiler and runtime support for applications that do not necessarily have regular data access patterns. Our group has already developed compiler embedded runtime support for completely irregular computations (i.e. codes in which distributed arrays are accessed based on indirection arrays) <ref> [10, 11, 18] </ref>. One class of scientific and engineering applications involves structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) [9]. Multigrid is a common technique for accelerating the solution of partial-differential equations [5, 30]. <p> The irregular communication arising from use of indirection arrays can be handled using the Parti primitives for irregular problems [10], which have also been integrated with compilers for HPF-style languages (including the Rice University Fortran 77D compiler <ref> [18] </ref> and the Syracuse University Fortran 90D compiler [4]). F90D and HPF also provide a number of intrinsic functions (such as reduction, spread, etc.).
Reference: [19] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Multigrid is a common technique for accelerating the solution of partial-differential equations [5, 30]. Multigrid codes employ a number of meshes at different levels of resolution. The restriction and prolongation operations for shifting between different multigrid levels require moving regular array sections <ref> [19] </ref> with non-unit strides. In multiblock problems, the data is divided into several interacting regions (called blocks or subdomains). There are computational phases in which regular computation is performed on each block independently.
Reference: [20] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the Sixth International Conference on Supercomputing. </booktitle> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: HPF offers the promise of significantly easing the task of programming distributed memory machines and making programs independent of a single machine architecture. Reducing communication costs is crucial in achieving good performance on applications <ref> [20, 21] </ref>.
Reference: [21] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings Supercomputing '91, </booktitle> <pages> pages 86-100. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1991. </year>
Reference-contexts: HPF offers the promise of significantly easing the task of programming distributed memory machines and making programs independent of a single machine architecture. Reducing communication costs is crucial in achieving good performance on applications <ref> [20, 21] </ref>.
Reference: [22] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year> <month> 28 </month>
Reference-contexts: Current distributed memory machines have good communication bandwidths, but they also have high startup latencies which often result in high communication overheads. Recently there have been major efforts in developing programming language and compiler support for distributed memory machines. Based on the initial works of projects like Fortran D <ref> [13, 22] </ref> and Vienna Fortran [6, 41] , the High Performance Fortran Forum has recently proposed the first version of High Performance 1 This work was supported by ARPA under contract No. NAG-1-1485, by NSF under grant No. ASC 9213821 and by ONR under contract No. SC 292-1-22913. <p> HPF offers the promise of significantly easing the task of programming distributed memory machines and making programs independent of a single machine architecture. Reducing communication costs is crucial in achieving good performance on applications [20, 21]. While current systems like the Fortran D project <ref> [22] </ref> and the Vienna Fortran Compilation system [6] have implemented a number of optimizations for reducing communication costs (like message blocking, collective communication, message coalescing and aggregation), these optimizations have been developed only in the context of regular problems (i.e. in code having only regular data access patterns).
Reference: [23] <author> J.R.G.Townshend, C.O.Justice, W. Li, C.Gurney, and J.McManus. </author> <title> Global land cover classification by remote sensing:present capabilities and future possibilities. </title> <booktitle> Remote Sensing of Environment, </booktitle> <volume> 35 </volume> <pages> 243-256, </pages> <year> 1991. </year>
Reference-contexts: air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts [1, 8, 31], large scale climate modeling [14], reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics <ref> [23] </ref>. In this paper we present a combined runtime and compile-time approach for parallelizing this general class of applications on distributed memory machines. We present runtime support that we have designed and implemented for parallelizing these applications on distributed memory machines in an efficient, convenient and machine independent manner.
Reference: [24] <author> Scott R. Kohn and Scott B. Baden. </author> <title> An implementation of the LPAR parallel programming model for scientific computations. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 759-766. </pages> <publisher> SIAM, </publisher> <month> March </month> <year> 1993. </year>
Reference-contexts: Boundary updates require communication between blocks, which is restricted to moving regular array sections (possibly including non-unit strides). Multiblock grids are frequently used for modeling geometrically complex objects which cannot be easily 2 modeled using a single regular mesh <ref> [2, 3, 24, 29, 36] </ref>. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. <p> We discuss the optimizations that we have used to achieve this performance. Several other researchers have also developed runtime libraries or programming environments for multiblock applications. Baden <ref> [24] </ref> has developed a Lattice Programming Model (LPAR). This system, however, achieves only coarse grained parallelism since a single block can only be assigned to one processor. Quinlan [26] has developed P++, a set of C++ libraries for grid applications.
Reference: [25] <author> George Lake. </author> <type> Personal communication. </type>
Reference-contexts: In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. Multiblock applications are used in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation <ref> [25, 38] </ref>, simulation of high performance aircrafts [1, 8, 31], large scale climate modeling [14], reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23].
Reference: [26] <author> Max Lemke and Daniel Quinlan. </author> <title> P++, a C++ virtual shared grids based programming environment for architecture-independent development of structured grid applications. </title> <type> Technical Report 611, </type> <institution> GMD, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Several other researchers have also developed runtime libraries or programming environments for multiblock applications. Baden [24] has developed a Lattice Programming Model (LPAR). This system, however, achieves only coarse grained parallelism since a single block can only be assigned to one processor. Quinlan <ref> [26] </ref> has developed P++, a set of C++ libraries for grid applications. While this library provides a convenient interface, the libraries do not optimize communication overheads. Our library, on the contrast, reduces communication costs by using message aggregation. The rest of this paper is organized as follows.
Reference: [27] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The interpolation required during the prolongation step in multigrid codes also involves interaction among the neighboring array elements. Such communication is handled by allocation of extra space at the beginning and end of each array dimension on each processor. These extra elements are called overlap, or ghost, cells <ref> [6, 16, 27] </ref>. Depending upon the data access pattern in a loop, the required data is copied from other processors and is stored in the overlap cells. In our runtime system, communication is performed in two phases. <p> The communication primitives include a procedure Overlap Cell Fill Sched, which computes a schedule that is used to direct the filling of overlap cells along a given dimension of a distributed array. Communication for filling in the overlap cells has been implemented in other systems for regular computations <ref> [6, 16, 27] </ref>, so we will not be discussing the details here. The primitive Regular Section Copy Sched carries out the preprocessing required for performing the regular section moves.
Reference: [28] <author> R. Mathur, L.K. Peters, and R.D. </author> <title> Saylor. Sub-grid representation of emission source clusters in regional air quality modeling. Atmospheric Environment, </title> <address> 26A:3219-3238, </address> <year> 1992. </year>
Reference-contexts: In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. Multiblock applications are used in important grand-challenge applications like air quality modeling <ref> [28, 32] </ref>, computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts [1, 8, 31], large scale climate modeling [14], reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23].
Reference: [29] <author> S. McCormick. </author> <title> Multilevel Adaptive Methods for Partial Differential Equations. </title> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: Boundary updates require communication between blocks, which is restricted to moving regular array sections (possibly including non-unit strides). Multiblock grids are frequently used for modeling geometrically complex objects which cannot be easily 2 modeled using a single regular mesh <ref> [2, 3, 24, 29, 36] </ref>. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid.
Reference: [30] <author> S. McCormick. </author> <title> Multilevel Projection Methods for Partial Differential Equations. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: One class of scientific and engineering applications involves structured meshes. These meshes may be nested (as in multigrid codes) or may be irregularly coupled (called Multiblock or Irregularly Coupled Regular Mesh Problems) [9]. Multigrid is a common technique for accelerating the solution of partial-differential equations <ref> [5, 30] </ref>. Multigrid codes employ a number of meshes at different levels of resolution. The restriction and prolongation operations for shifting between different multigrid levels require moving regular array sections [19] with non-unit strides. In multiblock problems, the data is divided into several interacting regions (called blocks or subdomains).
Reference: [31] <author> R. Meakin. </author> <title> Moving body overset grid methods for complete aircraft tiltrotor simulations, </title> <booktitle> AIAA-93-3350. In Proceedings of the 11th AIAA Computational Fluid Dynamics Conference, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. Multiblock applications are used in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts <ref> [1, 8, 31] </ref>, large scale climate modeling [14], reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23].
Reference: [32] <author> Odman M.T. and A.G. Russell. </author> <title> A multiscale finite element pollutant transport scheme for urban and regional modeling. Atmospheric Environment, </title> <address> 25A:2385-2398, </address> <year> 1991. </year>
Reference-contexts: In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. Multiblock applications are used in important grand-challenge applications like air quality modeling <ref> [28, 32] </ref>, computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts [1, 8, 31], large scale climate modeling [14], reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23].
Reference: [33] <author> Richard Muntz. </author> <type> Personal communication. </type>
Reference-contexts: in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation [25, 38], simulation of high performance aircrafts [1, 8, 31], large scale climate modeling [14], reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases <ref> [33] </ref>, and land cover dynamics [23]. In this paper we present a combined runtime and compile-time approach for parallelizing this general class of applications on distributed memory machines.
Reference: [34] <author> Naomi H. Naik and John Van Rosendale. </author> <title> The improved robustness of multigrid elliptic solvers based on multiple semicoarsened grids. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 30(1) </volume> <pages> 215-229, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Accordingly, each grid may be distributed over the entire set of processors, or some grids may have to be distributed over parts of the processor space. A particular form of multigrid technique is the semi-coarsening multigrid technique <ref> [34] </ref>. Semi-coarsening multigrid works as follows. Starting from the finest grid, coarser grids are generated by coarsening by different factors along different dimensions. There may be many grids, having the same number of mesh points, but obtained by different coarsening factor along each dimensions (i.e. they may have different shapes).
Reference: [35] <author> Andrea Overman and John Van Rosendale. </author> <title> Mapping robust parallel multigrid algorithms to scalable memory architectures. </title> <booktitle> To appear in Proceedings of 1993 Copper Mountain Conference on Multigrid Methods, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: We present experimental results to demonstrate the efficacy of our approach. We have experimented with one multiblock application [40] and one multigrid code <ref> [35] </ref>. We have measured the runtime overheads of our primitives. We have compared the performance of compiler parallelized multiblock and multigrid templates with those of the hand parallelized (i.e. parallelized by inserting calls to the runtime library by hand) versions. <p> As we have discussed in the previous sections, each of these blocks needs to be distributed onto a portion of the processor space. Similarly, in multigrid codes, communication overheads can typically be reduced by distributing each coarse grid over a part of the processor space <ref> [35] </ref>. The current version of HPF does not provide any convenient mechanism for distributing arrays (or templates) onto a part of the processor space. We therefore need additional functionality for conveniently distributing arrays onto part of the processor space. <p> The template, which was designed to include portions of the entire code that are representative of the major computation and communication patterns of the original code, consists of nearly 2,000 lines of F77 code. We have also worked with a semi-coarsening multigrid code <ref> [35] </ref>. This has nearly 2,500 lines of F77 code. <p> This technique still has this overhead as compared to a hand parallelized version. To study the exact costs of each of these factors, we present a more detailed experiment in Section 5.4. 5.3 Multigrid Code We have also experimented with a semi-coarsening multigrid code developed by Rosendale and Overman <ref> [35] </ref>. This has nearly 2,500 lines of F77 code. We discussed the semi-coarsening multigrid technique and the mapping policy used in parallelizing such an application earlier in Section 2. We rewrote this code using forall loops and including the distribution directives and then parallelized it using our compiler. <p> We rewrote this code using forall loops and including the distribution directives and then parallelized it using our compiler. This code had also been parallelized by inserting the calls to the library routines manually <ref> [35] </ref>. In Figure 10, we show the performance comparison of these two parallel versions run on Intel iPSC/860. The results are for a 32x32x32 grid, using a coarsening factor of 4 along each dimension.
Reference: [36] <author> J.J. Quirk. </author> <title> An Adaptive Grid Algorithm for Computational Shock Hydrodynamics. </title> <type> PhD thesis, </type> <institution> Cranfield Institute of Technology, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: Boundary updates require communication between blocks, which is restricted to moving regular array sections (possibly including non-unit strides). Multiblock grids are frequently used for modeling geometrically complex objects which cannot be easily 2 modeled using a single regular mesh <ref> [2, 3, 24, 29, 36] </ref>. In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid.
Reference: [37] <author> James M. Stichnoth. </author> <title> Efficient compilation of array statements for private memory multicomputers. </title> <type> Technical Report CMU-CS-93-109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: However, if the distributions of source and destination distributeed arrays and description of source and destination regular sections are available at compile-time, then this analysis can be done at compile-time as well. In separate works, Chatterjee et al [7], Stichnoth <ref> [37] </ref> and Gupta et al [17] have developed compile-time methods for analyzing and generating communication associated with HPF's forall statements and/or F90 style array expressions.
Reference: [38] <author> J.M. Stone and M.L. Norman. Zeus-2d: </author> <title> A radiation magnetohydrodynamics code for astrophysical flows in two space dimensions: I. the hydrodynamic algorithms and tests. </title> <journal> The Astrophysical Journal Supplements, </journal> <volume> 80(753), </volume> <year> 1992. </year>
Reference-contexts: In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. Multiblock applications are used in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics [40], structure and galaxy formation <ref> [25, 38] </ref>, simulation of high performance aircrafts [1, 8, 31], large scale climate modeling [14], reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23].
Reference: [39] <author> Alan Sussman and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives. </title> <institution> Technical Report CS-TR-3070 and UMIACS-TR-93-36, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> May </month> <year> 1993. </year> <month> 29 </month>
Reference-contexts: We discuss the nature of computation and communication in multiblock and multigrid codes and also describe how the library primitives facilitate parallelization of these applications. The set of runtime routines that we have developed is called the Multiblock Parti library <ref> [39] </ref>. In summary, these primitives allow an application programmer or a compiler to 3 * Lay out distributed data in a flexible way, to enable good load balancing and minimize interprocessor communication, * Give high level specifications for performing data movement, and * Distribute the computation across the processors. <p> Similarly, the interpolation required during the prolongation step also involves interaction among neighboring elements. Also, support for distributing loop iterations and transforming global distributed array 5 references to local references is required. 2.2 Multiblock Parti Primitives We now discuss the design of our runtime library <ref> [39] </ref>. Since, in typical multiblock and multigrid applications, the number of blocks and their respective sizes is not known until runtime, the distribution of blocks onto processors is done at runtime. The distributed array descriptors (DAD) [4] for the arrays representing these blocks are, therefore, generated at runtime. <p> Distributed array descriptors contain information about the portions of the arrays residing on each processor, and are used at runtime for performing communication and distributing loops iterations. We will not discuss the details of the primitives which allow the user to specify data distribution. For more details, see <ref> [39] </ref>. As we discussed previously, two types of communication are required in both multiblock and multigrid applications. Inter-block communication is required because of boundary conditions between blocks (in multi-block codes) and restrictions and prolongations between grids at different levels of resolution (in multigrid codes).
Reference: [40] <author> V.N. Vatsa, M.D. Sanetrik, and E.B. Parlette. </author> <title> Development of a flexible and efficient multigrid-based multiblock flow solver; AIAA-93-0677. </title> <booktitle> In Proceedings of the 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: In Figure 1, we show how the area around an aircraft wing has been modeled with a multiblock grid. Multiblock applications are used in important grand-challenge applications like air quality modeling [28, 32], computational fluid dynamics <ref> [40] </ref>, structure and galaxy formation [25, 38], simulation of high performance aircrafts [1, 8, 31], large scale climate modeling [14], reservoir modeling for porous media [14], simulation of propulsion systems [14], computational combustion dynamics [14], geophysical databases [33], and land cover dynamics [23]. <p> We present experimental results to demonstrate the efficacy of our approach. We have experimented with one multiblock application <ref> [40] </ref> and one multigrid code [35]. We have measured the runtime overheads of our primitives. We have compared the performance of compiler parallelized multiblock and multigrid templates with those of the hand parallelized (i.e. parallelized by inserting calls to the runtime library by hand) versions. <p> We have parallelized a template from a multiblock computation fluid dynamics application that solves the thin-layer Navier-Stokes equations over a 3D surface (multiblock TLNS3D), using our prototype Fortran 90D compiler. The multiblock TLNS3D code we are working with was developed by Vatsa et al. <ref> [40] </ref> at NASA Langley Research Center, and consists of nearly 18,000 lines of Fortran 77 code. The template, which was designed to include portions of the entire code that are representative of the major computation and communication patterns of the original code, consists of nearly 2,000 lines of F77 code.
Reference: [41] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran | a language specification, version 1.1. </title> <type> Interim Report 21, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> March </month> <year> 1992. </year> <month> 30 </month>
Reference-contexts: Recently there have been major efforts in developing programming language and compiler support for distributed memory machines. Based on the initial works of projects like Fortran D [13, 22] and Vienna Fortran <ref> [6, 41] </ref> , the High Performance Fortran Forum has recently proposed the first version of High Performance 1 This work was supported by ARPA under contract No. NAG-1-1485, by NSF under grant No. ASC 9213821 and by ONR under contract No. SC 292-1-22913. <p> Once a processor subspace has been declared, arrays or templates can be distributed onto it. For example, !HPF$ TEMPLATE T (100,100) !HPF$ DISTRIBUTE T (BLOCK,BLOCK) ONTO P1 Similar functionality is available in Vienna Fortran <ref> [41] </ref>, where a distribution can be mapped to a processor reference.
References-found: 41

