URL: http://www.ri.cmu.edu/afs/cs/project/nectar-io/ftp/ParityLogging/TR93-200.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/danner/www/danner.html
Root-URL: 
Title: Page -1 of  
Author: Stodolsky, Mark Holland, William V. Courtright II, and Garth A. Gibson 
Keyword: Efficient Small Writes  
Date: October 20, 1993  
Note: Submitted to Transactions on Computer Systems  Daniel  CMU-CS-93-200  
Address: Pittsburgh, Pennsylvania 15213-3890  
Affiliation: School of Computer Science Carnegie Mellon University  
Pubnum: 66  
Abstract: Parity encoded redundant disk arrays provide highly reliable, cost effective secondary storage with high performance for reads and large writes. Their performance on small writes, however, is much worse than mirrored disks the traditional, highly reliable, but expensive organization for secondary storage. Unfortunately, small writes are a substantial portion of the I/O workload of many important, demanding applications such as on-line transaction processing. This paper presents parity logging, a novel solution to the small write problem for redundant disk arrays. Parity logging applies journalling techniques to substantially reduce the cost of small writes. We provide a detailed analysis of parity logging and competing schemes mirroring, oating storage, and RAID level 5 and verify these models by simulation. Parity logging provides performance competitive with mirroring, the best of the alternative single failure tolerating disk array organizations. However, its overhead is close to the minimum offered by RAID level 5. Finally, parity logging can exploit data caching much more effectively than all three alternative approaches. A Redundant Disk Array Architecture for This research was supported by the ARPA, Information Science and Technology Office, under the title Research on Parallel Computing, ARPA Order No. 7330, the National Science Foundation under contract NSF ECD-8907068, and by IBM and NCR graduate fellowships. Work furnished in connection with this research is provided under prime contract MDA972-90-C-0035 issued by ARPA/CMO to CMU. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. 
Abstract-found: 1
Intro-found: 1
Reference: [ATC90] <author> Array Technology Corporation. </author> <title> Product Description, RAID+ Series Model RX, Revision 1.0, Array Technology Corporation, </title> <year> 1990. </year>
Reference-contexts: Nonbinary codes can achieve much lower check information space overhead in a multiple failure tolerating array. In particular, a variant of a Reed-Solomon code called Parity has been used in disk array products to provide double failure tolerance with only two check information disks <ref> [ATC90] </ref>.
Reference: [Bhide92] <author> Bhide, A., and Dias, D. </author> <title> RAID Architectures for OLTP. </title> <institution> Computer Science Research Report RC 17879, IBM Corporation, </institution> <year> 1992. </year>
Reference-contexts: Section 10. Related Work Bhide and Dias <ref> [Bhide92] </ref> have independently developed a scheme similar to parity logging. Their LRAID-X4 organization maintains separate parity and parity-update log disks, and periodically applies the logged updates to the parity disk.
Reference: [Bitton88] <author> Bitton, D., and Gray, J. </author> <title> Disk Shadowing. </title> <booktitle> In Proceedings of the 14th Conference on Very Large Data Bases (1988), </booktitle> <pages> 331-338. </pages>
Reference-contexts: Reintegration, therefore, requires disks to sequentially transfer cylinders, and the remaining disks to transfer cylinders. If the disks arms are in random positions, it can be shown <ref> [Bitton88] </ref> that the expected longest seek of these parallel transfers is of the disk. <p> The updated check information is then rewritten, all log copies are truncated, and the logging cycle starts again. Mirroring and oating data and parity also extend to multiple failure tolerance in straightforward manner. Mirroring becomes -copy shadowing <ref> [Bitton88] </ref>. Floating data and parity becomes oating data and check, requiring oated read-rotate-write accesses per blind write. Relative to these other schemes, parity logging has better performance because of its lower nonpreread overhead.
Reference: [Cao93] <author> Cao, P., Lim, S. B., Venkataraman, S., and Wilkes, J. </author> <title> The TickerTAIP Parallel RAID Architecture. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture (May 1993). IEEE, </booktitle> <address> San Diego, </address> <pages> 52-63. </pages>
Reference-contexts: 1. Our failure model treats disk and controller failures as independent. If concurrent controller and disk failures must be survived, controller state must be partitioned and replicated <ref> [Schulze89, Gibson92, Cao93] </ref>. 2. Notice that we make no attempt to reduce the cost of the preread and overwrite of the target data block. Addi tional savings are possible if data writes and deferred and optimally scheduled [Solworth90, Orji93]. D T TVD D TVD TVD Page 5 of 66 2.1.
Reference: [Chen90] <author> Chen, P. M., and Patterson, D. A. </author> <title> Maximizing Performance in a Striped Disk Array. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture (1990), </booktitle> <pages> 322-331. </pages>
Reference-contexts: While this assumption approximately holds for many OLTP workloads, other work-loads exhibit substantial locality. In particular, consider the extreme situation in which all user I/O is concentrated within one region. Choosing an appropriate data stripe unit <ref> [Chen90] </ref> will balance the user I/O across the actuators that contain data for this region; however, log and data traffic are partitioned over non-overlapping disks.
Reference: [Gibson89] <author> Gibson, G., Hellerstein, L., Karp, R. M., Katz, R. H., and Patterson, D. A. </author> <title> Coding Page 32 of 66 Techniques for Handling Failures in Large Disk Arrays. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III) (1989). ACM, </booktitle> <pages> 123-132. </pages>
Reference-contexts: This paper is not concerned with the choice of codes that might be used for failure tolerance, except to note that the best of these codes all have one property important to small random write performance <ref> [Gibson89] </ref>: each small write updates disks disks containing check information (generalized parity) and the disk containing the users data. This check maintenance work, which scales up with the number of failures tolerated, is exactly the work that parity logging is designed to handle more efficiently.
Reference: [Gibson92] <author> Gibson, G. </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: 1. Our failure model treats disk and controller failures as independent. If concurrent controller and disk failures must be survived, controller state must be partitioned and replicated <ref> [Schulze89, Gibson92, Cao93] </ref>. 2. Notice that we make no attempt to reduce the cost of the preread and overwrite of the target data block. Addi tional savings are possible if data writes and deferred and optimally scheduled [Solworth90, Orji93]. D T TVD D TVD TVD Page 5 of 66 2.1. <p> Section 9. Multiple failure tolerating arrays A significant advantage of parity logging is its efficient extension to multiple failure tolerating arrays. Multiple failure tolerance provides much longer mean time to data loss and greater tolerance for bad blocks discovered during reconstruction <ref> [Gibson92] </ref>. Using codes more powerful than parity, RAID level 5 and its variants can all be extended to tolerate concurrent failures. Figure 28 gives an example of one of the more easily understood double failure tolerant disk array organizations. <p> If, instead, generalized parity (check information) is computed as a multiple-bit symbol, dependent on a multiple-bit symbol from each of a subset of the data disks, then the code is a non-binary code <ref> [Macwilliams77, Gibson92] </ref>. Nonbinary codes can achieve much lower check information space overhead in a multiple failure tolerating array. In particular, a variant of a Reed-Solomon code called Parity has been used in disk array products to provide double failure tolerance with only two check information disks [ATC90].
Reference: [Gibson93] <author> Gibson, G., and Patterson, D. </author> <title> Designing Disk Arrays for High Data Reliability. </title> <journal> Journal of Parallel and Distributed Computing (January, </journal> <year> 1993), </year> <pages> 4-27. </pages>
Reference-contexts: Loss ( ) and a reliability function , the probability of surviving a lifetime of , of where represents the number of redundancy groups, the number of drives per group, the mean time for a disk to fail, and is the mean time required to repair a single failed disk <ref> [Gibson93] </ref>.
Reference: [Geist87] <author> Giest, R. M., and Daniel, S. </author> <title> A Continuum of Disk Scheduling Algorithms. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5, 1 (1987), </volume> <pages> 77-92. </pages>
Reference: [Gray90] <author> Gray, J., Horst, B., and Walker, M. </author> <title> Parity Striping of Disc Arrays: Low-Cost Reliable Storage with Acceptable Throughput. </title> <booktitle> In Proceedings of the 16th International Conference on Very Large Databases (VLDB) (1990), </booktitle> <pages> 148-161. </pages>
Reference: [Hewlett-Packard93] <author> Hewlett-Packard Company, </author> <title> HP C2247 3.5-inch Disk Drive Product Brief, </title> <publisher> 5091-2788E. Hewlett-Packard Company, </publisher> <year> 1993. </year>
Reference-contexts: Consequently, it is reasonable to assume that the number of data units per track may not decrease even as database account record sizes grow with new value-added features. Additionally, the large capacity of the outer cylinders in zone bit recording <ref> [Seagate92, Hewlett-Packard93] </ref> disks offers the potential to increase the efficiency of sequential transfers in parity logging by locating the log and parity in this area. 5.2. <p> For modern disks, a of 300,000 hours is not unreasonable <ref> [Hewlett-Packard93] </ref> and repair times of 1 hour are easily achieved with an on-line spare T load T cpu T write + + = T cpu 2 S 1 2 3I N 2+( ) R M-+[ ] 2C P N C L L+[ ] 2RT T 1-( ) H M+ +( )+
Reference: [Holland92] <author> Holland, M., and Gibson, G. </author> <title> Parity Declustering for Continuous Operation in Redundant Disk Arrays. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V) (1992). ACM, </booktitle> <pages> 23-35. </pages>
Reference-contexts: Application of data compression to the parity log should be very profitable. A detailed comparison of the log structured filesystem [Rosenblum91], which completely avoids small writes, and parity logging should be undertaken. The interaction of parity logging and parity declustering <ref> [Holland92] </ref> merits particular exploration. Parity declustering provides high performance during degraded-mode and reconstruction while parity logging provides high performance during fault-free operation. The combination of the two should provide a particularly cost effective system for OLTP environments.
Reference: [Holland93] <author> Holland, M., Gibson, G. A., and Siewiorek, D. </author> <title> Fast, On-line Recovery in Redundant Disk Arrays, </title> <booktitle> Proceedings of the International Symposium of Fault Tolerant Computing (1993), </booktitle> <pages> 422-431. </pages>
Reference-contexts: t amw DTC L ( ) MTTDL R t ( ) t MTTDL 2N 1-( ) MTTF disk MTTR disk MTTF disk 2 GN N 1-( ) MTTR disk = R t ( ) exp t MTTDL-( )= G N MTTF disk MTTR disk MTTF disk Page 22 of 66 <ref> [Holland93] </ref>. As shown in figure 20, both parity logging and mirroring offer extremely reliable storage. Section 8.
Reference: [IBM0661] <author> IBM Corporation, </author> <title> IBM 0661 Disk Drive Product Description, Model 370, First Edition, Low End Storage Products, </title> <type> 504/114-2. </type> <institution> IBM Corporation, </institution> <year> 1989. </year>
Reference: [Jones91] <author> J. Jones, J. Jr., and Liu, T. </author> <title> RAID: A Technology Poised for Explosive Growth. Mont-gomery Securities Industry Report. Montgomery Securities, </title> <address> San Francisco, </address> <year> 1991. </year>
Reference: [Lee91] <author> Lee, E., and Katz, R. </author> <title> Performance Consequences of Parity Placement in Disk Arrays. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV) (1991). ACM, </booktitle> <pages> 190-199. </pages> <note> Page 33 of 66 </note>
Reference-contexts: Parity logging was simulated with a single track of log buffer per region for several different degrees of log striping. The RAIDSIM package, a disk array simulator derived from the Sprite operating system disk array driver <ref> [Ousterhout88, Lee91] </ref>, was extended with implementations of parity logging and oating data and parity. In each simulation, a request stream was generated by 66 user processes (i.e, three per disk). Each process requests a small write from a disk selected at random, then waits for acknowledgment from the disk array.
Reference: [MacWilliams77] <author> MacWilliams, F. J., and Sloane, N. J. A. </author> <title> The Theory of Error-Correcting Codes, </title> <publisher> North-Holland Mathematical Library, Volume 16, Elsevier Science Publishing Company, </publisher> <address> New York, NY, </address> <year> 1977. </year>
Reference-contexts: If, instead, generalized parity (check information) is computed as a multiple-bit symbol, dependent on a multiple-bit symbol from each of a subset of the data disks, then the code is a non-binary code <ref> [Macwilliams77, Gibson92] </ref>. Nonbinary codes can achieve much lower check information space overhead in a multiple failure tolerating array. In particular, a variant of a Reed-Solomon code called Parity has been used in disk array products to provide double failure tolerance with only two check information disks [ATC90].
Reference: [Menon92] <author> Menon, J., and Kasson, J. </author> <title> Methods for Improved Update Performance of Disk Arrays. </title> <booktitle> In Proceedings of the Hawaii International Conference on System Sciences (1992), </booktitle> <pages> 74-83. </pages>
Reference-contexts: Section 4. Analytical Models for Alternative Schemes Few other authors have addressed the problem of high performance yet reliable disk storage for small write workloads. The most notable of these is oating data and parity <ref> [Menon92] </ref>. <p> These can be combined into two read-rotate-write accesses, each of which takes disk seconds for a total disk busy time of . Again, no long term controller storage is required. The oating data and parity modification to RAID level 5 was proposed by Menon and Kas-son <ref> [Menon92] </ref>. In its most aggressive variant, this technique organizes data and parity into cylinders that contain either data only or parity only. <p> When a disk block is written, a new location is chosen in a manner that minimizes the disk arm time devoted to the write, and a new physical-to-logical mapping is established. We have described one such scheme, oat-ing data and parity <ref> [Menon92] </ref>, in this paper. An extreme example of this approach is the log structure filesystem (LFS), in which all data is written in a segmented log, and segments are periodically reclaimed by garbage collection [Rosenblum91]. This approach converts all writes into long sequential transfers, greatly enhancing write throughput.
Reference: [NCR93] <author> NCR Corporation, </author> <title> NCR ADP 93-02 Disk Array Controller Manual, </title> <institution> MS-0025, NCR Corporation, </institution> <year> 1993. </year>
Reference-contexts: As long as parallel reads on different sublogs proceeded at nearly the same rate, this algorithm will not consume much extra buffer space. If buffer exhaustion occurs, the algorithm can simply serialize. Parity is computed with a similar approach in the ADP-93-02 disk array controller <ref> [NCR93] </ref>. In summary, parity logging buffers parity updates until they can be written to a log efficiently. It then further delays their reintegration into the a redundant disk arrays parity until the size of the log makes a complete revision of the parity efficient.
Reference: [Orji93] <author> Orji, C. U., and Solworth, J. A. </author> <title> Write-only disk caches. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data (May 1993). ACM, </booktitle> <pages> 307-316. </pages>
Reference-contexts: Notice that we make no attempt to reduce the cost of the preread and overwrite of the target data block. Addi tional savings are possible if data writes and deferred and optimally scheduled <ref> [Solworth90, Orji93] </ref>. D T TVD D TVD TVD Page 5 of 66 2.1. Partitioning the Log Into Regions As stated, however, this scheme is completely impractical: an entire disks capacity of random access memory is required to hold the parity during the application of the parity updates. <p> Write-twice is typically combined with one of the write buffering techniques to improve the efficiency of the second write. This technique has been pursued most fully for mirroring <ref> [Solworth91, Orji93] </ref>. tracksr Tfewtrackseek Page 30 of 66 The oating location technique improves the efficiency of writes by eliminating the static association of logical disk blocks and fixed locations in the disk array. <p> In doubly distorted mirrors, write buffering, write-twice and oating location are all combined to provide high write throughput while maintaining data sequentiality <ref> [Orji93] </ref>. However, all oating location techniques require substantial host or controller storage for mapping information and buffered data. Section 11. Concluding Remarks This paper presents a novel solution to the small write problem in redundant disk arrays based on a distributed (and possibly replicated) log.
Reference: [Ousterhout88] <author> J. Ousterhout, et. al. </author> <title> The Sprite Network Operating System. </title> <booktitle> IEEE Computer (February 1988). IEEE, </booktitle> <pages> 23-36. </pages>
Reference-contexts: Parity logging was simulated with a single track of log buffer per region for several different degrees of log striping. The RAIDSIM package, a disk array simulator derived from the Sprite operating system disk array driver <ref> [Ousterhout88, Lee91] </ref>, was extended with implementations of parity logging and oating data and parity. In each simulation, a request stream was generated by 66 user processes (i.e, three per disk). Each process requests a small write from a disk selected at random, then waits for acknowledgment from the disk array.
Reference: [Patterson88] <author> Patterson, D., Gibson, G., and Katz, R. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> In Proceedings of the ACM SIGMOD Conference (1988). ACM, </booktitle> <pages> 109-116. </pages>
Reference-contexts: This large write optimization avoids the preread of data and parity associated with small writes, improving write performance by about a factor of four <ref> [Patterson88] </ref>. This optimization can not be applied directly to parity logging disk arrays as we have described them so far because there may exist outstanding (not yet reintegrated) logged updates for a particular parity unit at the time when a large write overwrites that parity unit.
Reference: [Rosenblum91] <author> Rosenblum, R. and Ousterhout, J. </author> <title> The Design and Implementation of a Log-Structured File System. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles (1991). ACM, </booktitle> <pages> 1-15. </pages>
Reference-contexts: Write buffering delays users write requests in a large disk or file cache to achieve deep queues, which can then be scheduled to substantially reduce seek and rotational positioning overheads <ref> [Seltzer90, Solworth90, Rosenblum91, Polyzois93] </ref>. Fault tolerance is at risk in these systems unless fault-tolerant caches are used (nonvolatility is a minimum requirement). The write-twice approach attempts to reduce the latency of writes without sacrificing the durability advantages of disk storage. <p> We have described one such scheme, oat-ing data and parity [Menon92], in this paper. An extreme example of this approach is the log structure filesystem (LFS), in which all data is written in a segmented log, and segments are periodically reclaimed by garbage collection <ref> [Rosenblum91] </ref>. This approach converts all writes into long sequential transfers, greatly enhancing write throughput. However, because logically nearby blocks may not be physically nearby, the performance of LFS in read intensive workloads may be degraded. <p> More dynamic assignment of controller memory should allow higher performance to be achieved or a substantial reduction in the amount of memory required. Application of data compression to the parity log should be very profitable. A detailed comparison of the log structured filesystem <ref> [Rosenblum91] </ref>, which completely avoids small writes, and parity logging should be undertaken. The interaction of parity logging and parity declustering [Holland92] merits particular exploration. Parity declustering provides high performance during degraded-mode and reconstruction while parity logging provides high performance during fault-free operation.
Reference: [Polyzois93] <author> Polyzios, C. A., Bhide, and A., Dias, D. M. </author> <title> Disk Mirroring with Alternating Deferred Updates. </title> <booktitle> In Proceedings of the 19th International Conference on Very Large Databases (VLDB) (1993), </booktitle> <pages> 604-617. </pages>
Reference-contexts: Write buffering delays users write requests in a large disk or file cache to achieve deep queues, which can then be scheduled to substantially reduce seek and rotational positioning overheads <ref> [Seltzer90, Solworth90, Rosenblum91, Polyzois93] </ref>. Fault tolerance is at risk in these systems unless fault-tolerant caches are used (nonvolatility is a minimum requirement). The write-twice approach attempts to reduce the latency of writes without sacrificing the durability advantages of disk storage.
Reference: [Ramakrishnan92] <author> Ramakrishnan, K. K., et. al. </author> <title> Analysis of File I/O Traces in Commercial Computing Environments. </title> <booktitle> In Performance Evaluation Review (SIGMETRICS). 20, 1 (1992). ACM, </booktitle> <pages> 78-90. </pages>
Reference-contexts: Performance in More General Workloads Up to this point, the results in this section have applied only to workloads whose accesses are 100% small random writes. This section examines a mixed workload, defined in figure 26, modeled on statistics taken from an airline reservation system <ref> [Ramakrishnan92] </ref>. With this more general workload, the results of the earlier sections are modified by two important effects: reads and medium to large writes. The issues encountered in extending oating data and parity to handle variable sized access are complex and it is neglected from this section.
Reference: [Schulze89] <author> Schulze, M. E., Gibson. G. A., Katz, R. H., and Patterson, D. A. </author> <booktitle> How Reliable is a RAID? In Proceedings of the 1989 IEEE Computer Society International Conference (COMP-CON 89) (1989). IEEE, </booktitle> <pages> 118-123. </pages>
Reference-contexts: 1. Our failure model treats disk and controller failures as independent. If concurrent controller and disk failures must be survived, controller state must be partitioned and replicated <ref> [Schulze89, Gibson92, Cao93] </ref>. 2. Notice that we make no attempt to reduce the cost of the preread and overwrite of the target data block. Addi tional savings are possible if data writes and deferred and optimally scheduled [Solworth90, Orji93]. D T TVD D TVD TVD Page 5 of 66 2.1.
Reference: [Seagate92] <author> Seagate Corporation, </author> <title> Seagate ST3600N/ND Family SCSI-2 Product Manual, </title> <type> Volume 1, </type> <institution> 77738477-A. Seagate Corporation, </institution> <year> 1992. </year> <note> Page 34 of 66 </note>
Reference-contexts: Consequently, it is reasonable to assume that the number of data units per track may not decrease even as database account record sizes grow with new value-added features. Additionally, the large capacity of the outer cylinders in zone bit recording <ref> [Seagate92, Hewlett-Packard93] </ref> disks offers the potential to increase the efficiency of sequential transfers in parity logging by locating the log and parity in this area. 5.2.
Reference: [Solworth90] <author> Solworth, J. A. and Orji, C. U. </author> <title> Distorted Mirrors. </title> <booktitle> In Proceedings of the ACM SIG-MOD Conference (1990). ACM, </booktitle> <pages> 123-132. </pages>
Reference-contexts: Notice that we make no attempt to reduce the cost of the preread and overwrite of the target data block. Addi tional savings are possible if data writes and deferred and optimally scheduled <ref> [Solworth90, Orji93] </ref>. D T TVD D TVD TVD Page 5 of 66 2.1. Partitioning the Log Into Regions As stated, however, this scheme is completely impractical: an entire disks capacity of random access memory is required to hold the parity during the application of the parity updates. <p> Write buffering delays users write requests in a large disk or file cache to achieve deep queues, which can then be scheduled to substantially reduce seek and rotational positioning overheads <ref> [Seltzer90, Solworth90, Rosenblum91, Polyzois93] </ref>. Fault tolerance is at risk in these systems unless fault-tolerant caches are used (nonvolatility is a minimum requirement). The write-twice approach attempts to reduce the latency of writes without sacrificing the durability advantages of disk storage.
Reference: [Solworth91] <author> Solworth, J. A. and Orji, C. U. </author> <title> Write-only disk caches. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems (Dec. 1991). IEEE, </booktitle> <pages> 10-17. </pages>
Reference-contexts: Write-twice is typically combined with one of the write buffering techniques to improve the efficiency of the second write. This technique has been pursued most fully for mirroring <ref> [Solworth91, Orji93] </ref>. tracksr Tfewtrackseek Page 30 of 66 The oating location technique improves the efficiency of writes by eliminating the static association of logical disk blocks and fixed locations in the disk array. <p> This approach converts all writes into long sequential transfers, greatly enhancing write throughput. However, because logically nearby blocks may not be physically nearby, the performance of LFS in read intensive workloads may be degraded. The distorted mirror approach <ref> [Solworth91] </ref> uses the 100% storage overhead of mirroring to avoid this problem: half of each mirror writes data in fixed locations, while the other half is used for oating storage, achieving higher write throughput while maintaining data sequentiality.
Reference: [Salem86] <author> Salem, K., and Garcia-Molina, H. </author> <title> Disk Striping. </title> <booktitle> In Proceedings of the 2nd IEEE International Conference on Data Engineering (1986). IEEE, </booktitle> <year> 1986. </year>
Reference-contexts: This may be rewritten as 4. This single access could be separated into two accesses each taking S+R+2R/D for a total of 2S+(2+4/D)R. For most modern disks S is about twice R, so the single access is more efficient. 5. Disks that support zero-latency writes <ref> [Salem86] </ref> can eliminate the initial rotational positioning delay. If only a single track is buffered (K=1) this can reduce the I/O time by 26% in drives such as the IBM 0661 (which does not support this feature).

References-found: 30

