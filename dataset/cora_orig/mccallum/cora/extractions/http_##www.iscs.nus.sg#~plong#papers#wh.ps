URL: http://www.iscs.nus.sg/~plong/papers/wh.ps
Refering-URL: 
Root-URL: 
Title: Worst-case Quadratic Loss Bounds for Prediction Using Linear Functions and Gradient Descent  
Author: Nicolo Cesa-Bianchi, Philip M. Long, and Manfred K. Warmuth 
Keyword: prediction, Widrow-Hoff algorithm, gradient descent, adaptive linear filter theory, smoothing, inner product spaces, computational learning theory, on-line learning, linear systems, worst-case loss bounds.  
Abstract: In this paper we study the performance of gradient descent when applied to the problem of on-line linear prediction in arbitrary inner product spaces. We prove worst-case bounds on the sum of the squared prediction errors under various assumptions concerning the amount of a priori information about the sequence to predict. The algorithms we use are variants and extensions of on-line gradient descent. Whereas our algorithms always predict using linear functions as hypotheses, none of our results requires the data to be linearly related. In fact, the bounds proved on the total prediction loss are typically expressed as a function of the total loss of the best fixed linear predictor with bounded norm. All the upper bounds are tight to within constants. Matching lower bounds are provided in some cases. Finally, we apply our results to the problem of on-line prediction for classes of smooth functions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference: [2] <author> A. Barron. </author> <title> Universal approximation bounds for superpositions of a sigmoidal function. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(3) </volume> <pages> 930-945, </pages> <year> 1993. </year>
Reference: [3] <author> N. Cesa-Bianchi, Y. Freund, D.P. Helmbold, D. Haussler, R.E. Schapire, and M.K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> Proceedings of the 25th ACM Symposium on the Theory of Computation, </booktitle> <year> 1993. </year>
Reference-contexts: In many cases we can even bound the additional loss of the algorithm over the above infimum similarly to the additional loss bounds of <ref> [3] </ref> obtained in a simpler setting. <p> In the next section, we show that techniques from <ref> [3] </ref> may also be applied to obtain a L Y=X (s) + O (Y p bound on the total loss (unnormalized) when bounds X on jjx t jj and Y on jy t j are known for all t. <p> It is an open problem whether, instead of using adversarial arguments as we do here, our lower bounds can already be obtained when the examples are randomly and independently drawn from a natural distribution. For more simple functions this was done in <ref> [3] </ref>: the lower bounds there are with respect to uniform distributions and the upper bounds which essentially meet the lower bounds are proven for the worst-case as done in this paper.
Reference: [4] <author> A. Dawid. </author> <title> Statistical theory: The prequential approach. </title> <journal> Journal of the Royal Statistical Society (Series A), </journal> <pages> pages 278-292, </pages> <year> 1984. </year>
Reference: [5] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: The derivation is based on previous derivations used in the proof of convergence of the on-line gradient descent algorithm (see, e.g. <ref> [5] </ref>). Lemma IV.1: Choose x; ^ w 1 ; w 2 X ; y 2 R; &gt; 0. Let ^y = ( ^ w 1 ; x) and ^ w 2 = ^ w 1 + (y ^y)x.
Reference: [6] <author> V. Faber and J. Mycielski. </author> <title> Applications of learning theorems. </title> <journal> Fundamenta Informaticae, </journal> <volume> 15(2) </volume> <pages> 145-167, </pages> <year> 1991. </year>
Reference-contexts: Our bounds are worst-case in the sense that they hold for all sequences of pairs (x t ; y t ). (In some cases we assume the norm of the x t 's is bounded by a second parameter.) Faber and Mycielski <ref> [6] </ref> noted that a natural class of smooth functions of a single real variable can be defined using inner products as above. The same class of smooth functions, as well as linear functions in R n , has been heavily studied in statistics [9] (however, with probabilistic assumptions). <p> We call this algorithm GD (defined below). In this paper we analyze the behavior of GD in the case in which there isn't necessarily a w for which for all t, y t = (w; x t ). Faber and Mycielski <ref> [6] </ref> also studied this case, but their algorithms made use of side information which, in this paper, we assume is not available. <p> Our general results are summarized in Table I. We may apply our general bounds to a class of smooth functions of a single real variable, in the manner used by Faber and Mycielski <ref> [6] </ref> in the case that there is a perfect smooth function. The smoothness of a function is measured by the 2-norm of its derivative. <p> 2 du W , and t (f (x t ) y t ) 2 E, then the special case of the general GD algorithm applied to this problem has a sum of squared errors bounded by inf " t # p A bound of W 2 X was proved by <ref> [6] </ref> in the case when E = 0. It is surprising that the time required for the algorithm we describe for this problem to make its tth prediction ^y t is O (t) in the uniform cost model provided that all past examples and predictions are saved. <p> In Theorem V.2 we extend our result to apply to classes of smooth functions of n &gt; 1 real variables studied by Faber and Mycielski <ref> [6] </ref> in the absence of noise. We further show that upper bound (6), even viewed as bound on the excess of the algorithm's total loss over the loss of the best function of "size" at most W , is optimal, constants included. <p> A.1 Normalized loss If we run algorithm GD with learning rate set in each trial t to fi jjx t jj 2 , we can then prove a variant of Theorem IV.1 for a different notion of loss (previously studied by Faber and Mycielski <ref> [6] </ref>) which we call normalized loss. The normalized loss incurred by an algorithm predicting ^y t on a trial (x t ; y t ) is defined by (^y t y t ) 2 jjx t jj 2 . <p> A. Smooth functions of a single variable We begin with a class of smooth functions of a single real variable that was studied by Faber and Mycielski <ref> [6] </ref> in a similar context, except using the assumption that there was a function f in the class such that y t = f (x t ) for all t. <p> This algorithm is illustrated in Figure 5. B. Smooth functions of several variables Theorem V.1 can be generalized to higher dimensions as follows. The analogous generalization in the absence of noise was carried out in <ref> [6] </ref>. The domain X is R n + .
Reference: [7] <author> M. Feder, N. Merhav, and M. Gutman. </author> <title> Universal prediction of individual sequences. </title> <journal> IEEE transactions of information theory, </journal> <volume> 38 </volume> <pages> 1258-1270, </pages> <year> 1992. </year>
Reference: [8] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1990. </year>
Reference-contexts: We define the norm jjAjj of a matrix A by jjAjj 2 = sup jjvjj 2 =1 jjAvjj 2 : This is the norm induced by the Euclidean norm for vectors in R n (see <ref> [8] </ref>.) Notice that jjAvjj 2 jjAjj 2 jjvjj 2 (Cauchy Schwartz inequality). We will make use of the following well-known facts. Fact VI.1 ([11]) For any real matrix A, jjAjj 2 = p where max is the largest eigenvalue of A T A.
Reference: [9] <author> W. Hardle. </author> <title> Smoothing Techniques. </title> <publisher> Springer Verlag, </publisher> <year> 1991. </year>
Reference-contexts: The same class of smooth functions, as well as linear functions in R n , has been heavily studied in statistics <ref> [9] </ref> (however, with probabilistic assumptions). Thus, general results for learning classes of functions defined by arbitrary inner product spaces can be applied in a variety of circumstances. <p> Similar function classes have also often been studied in nonparametric statistics (see, e.g. <ref> [9] </ref>) using probabilistic assumptions on the generation of the x t 's. Let R + be the set of nonnegative reals.
Reference: [10] <author> S.S. Haykin. </author> <title> Adaptive Filter Theory. </title> <publisher> Prentice Hall, </publisher> <address> 2nd edition, </address> <year> 1991. </year>
Reference-contexts: A more substantive difference is that they assumed x 1 = x 2 = x 3 :::. The algorithm GD for the special case of linear functions in R n is a central building block in area of signal processing (see e.g. [19], [25], [26], [29], <ref> [10] </ref>) where it is usually called Least-Mean-Square (LMS) algorithm. Therefore, there is an extensive literature studying the convergence properties of this algorithm (see, e.g. [29], [10]). All this research, however, is based on probabilistic assumptions on the generation of the x t 's. <p> the special case of linear functions in R n is a central building block in area of signal processing (see e.g. [19], [25], [26], [29], <ref> [10] </ref>) where it is usually called Least-Mean-Square (LMS) algorithm. Therefore, there is an extensive literature studying the convergence properties of this algorithm (see, e.g. [29], [10]). All this research, however, is based on probabilistic assumptions on the generation of the x t 's.
Reference: [11] <author> R.A. Horn and C.R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference: [12] <author> S. Hui and S.H. Zak. </author> <title> Robust stability analysis of adaptation algorithms for single perceptron. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(2) </volume> <pages> 325-328, </pages> <year> 1991. </year>
Reference-contexts: Faber and Mycielski [6] also studied this case, but their algorithms made use of side information which, in this paper, we assume is not available. Hui and Zak <ref> [12] </ref> also studied the robustness of GD in the presence of noise in a similar setting, however they modelled observation noise, assuming that there was a w such that for all t y t = (w; x t ), but that the learner's observation of y t was corrupted with noise.
Reference: [13] <author> S. Kaczmarz. </author> <title> Angenaherte Auflosung von systemen linearer gle-ichungen. </title> <journal> Bull. Acad. Polon. Sci. Lett. A, </journal> <volume> 35 </volume> <pages> 355-357, </pages> <year> 1937. </year>
Reference-contexts: This paper shows that the algorithm GD can analyzed even without probabilistic assumptions. 2 Even though in the neural network community this algorithm is usually credited to Widrow and Hoff [28], a similar algorithm for the iterative solution of a system of linear equations was previously developed by Kaczmarz <ref> [13] </ref>. Gradient descent is an algorithm design technique which has achieved considerable practical success in more complicated hypothesis spaces, in particular multilayer neural networks. Despite this success, there appears not to be a principled method for tuning the learning rate.
Reference: [14] <author> D. Kimber and P.M. </author> <title> Long. The learning complexity of smooth functions of a single variable. </title> <booktitle> The 1992 Workshop on Computational Learning Theory, </booktitle> <pages> pages 153-159, </pages> <year> 1992. </year>
Reference-contexts: In the case E = 0, however, there is an algorithm with an optimal bound on P t (^y t y t ) 2 which computes its tth prediction in O (log t) time <ref> [14] </ref>, raising the hope that there might be a similarly efficient robust algorithm. In Theorem V.2 we extend our result to apply to classes of smooth functions of n &gt; 1 real variables studied by Faber and Mycielski [6] in the absence of noise.
Reference: [15] <author> J. Kivinen and M. K. Warmuth. </author> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <type> Report UCSC-CRL-94-16, </type> <institution> University of California, Santa Cruz, </institution> <month> June </month> <year> 1994. </year> <note> An extended abstract appeared in Proceedings of STOC 95. </note>
Reference-contexts: Then, P m inf f2LIN W;n t=1 (f (x t )y t ) 2 fl p It has been shown recently <ref> [15] </ref> that even on very simple probabilistic artificial data, the above tunings and worst-case loss bounds are close to optimal.
Reference: [16] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference: [17] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, Technical Report UCSC-CRL-89-11, </type> <institution> University of California Santa Cruz, </institution> <year> 1989. </year>
Reference: [18] <author> N. Littlestone, </author> <title> P.M. Long, and M.K. Warmuth. On-line learning of linear functions. </title> <booktitle> Proceedings of the 23rd ACM Symposium on the Theory of Computation, </booktitle> <pages> pages 465-475, </pages> <year> 1991. </year>
Reference-contexts: We further show that upper bound (6), even viewed as bound on the excess of the algorithm's total loss over the loss of the best function of "size" at most W , is optimal, constants included. Littlestone, Long and, Warmuth <ref> [18] </ref> proved bounds for another algorithm for learning linear functions in R n , in which the x t 's were measured using the infinity norm, and the w's were measured using 1-norm. <p> However, the algorithm of <ref> [18] </ref> does not appear to generalize to arbitrary inner product spaces as did the GD algorithm, and therefore those techniques do not appear to be as widely applicable.
Reference: [19] <author> R.W. </author> <title> Lucky. Techniques of adaptive equalization of digital communication systems. </title> <journal> Bell System Technical Journal, </journal> <volume> 45 </volume> <pages> 255-286, </pages> <year> 1966. </year>
Reference-contexts: A more substantive difference is that they assumed x 1 = x 2 = x 3 :::. The algorithm GD for the special case of linear functions in R n is a central building block in area of signal processing (see e.g. <ref> [19] </ref>, [25], [26], [29], [10]) where it is usually called Least-Mean-Square (LMS) algorithm. Therefore, there is an extensive literature studying the convergence properties of this algorithm (see, e.g. [29], [10]). All this research, however, is based on probabilistic assumptions on the generation of the x t 's.
Reference: [20] <author> D.G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: If min = 0, then the bound holds vacuously. As sume then min &gt; 0. Via an application of the Kantorovich inequality to the square matrix A T A (see e.g. <ref> [20] </ref>) it can be shown that R ( ^ x t+1 ) 1 ( min + max ) 2 R ( ^ x t ): (22) Therefore, we get 4 min max By summing up over all iterations t we obtain 4 min max 1 X R ( ^ x t
Reference: [21] <author> N. Littlestone and M.K. Warmuth. </author> <title> The weighted majority algorithm. </title> <type> Technical Report UCSC-CRL-91-28, </type> <institution> UC Santa Cruz, </institution> <month> October </month> <year> 1991. </year> <note> A preliminary version appeared in the Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, </note> <month> October 89, </month> <pages> pages 256-261. </pages>
Reference: [22] <author> N. Merhav and M. Feder. </author> <title> Universal sequential learning and decision from individual data sequences. </title> <booktitle> The 1992 Workshop on Computational Learning Theory, </booktitle> <pages> pages 413-427, </pages> <year> 1992. </year>
Reference: [23] <author> J. Mycielski and S. Swierczkowski. </author> <title> General learning theorems. </title> <type> Unpublished, </type> <year> 1991. </year>
Reference-contexts: When X is an arbitrary real vector space, and therefore its elements may not be uniquely represented by finite tuples of reals, the GD algorithm is a natural generalization of on-line gradient descent 3 and may viewed as follows <ref> [23] </ref>. 4 After each trial t, there is a set S t of elements w of X for which (w; x t ) = y t .
Reference: [24] <author> J. Mycielski. </author> <title> A learning algorithm for linear operators. </title> <journal> Proceedings of the American Mathematical Society, </journal> <volume> 103(2) </volume> <pages> 547-550, </pages> <year> 1988. </year>
Reference-contexts: Mycielski <ref> [24] </ref> had already treated the special case of linear functions in R n . The algorithm they analyzed for this "noise-free" case was a generalization of the on-line gradient descent algorithm 2 to arbitrary inner product spaces. We call this algorithm GD (defined below).
Reference: [25] <author> M.M. Sondhi. </author> <title> An adaptive echo canceller. </title> <journal> Bell System Technical Journal, </journal> <volume> 46 </volume> <pages> 497-511, </pages> <year> 1967. </year>
Reference-contexts: A more substantive difference is that they assumed x 1 = x 2 = x 3 :::. The algorithm GD for the special case of linear functions in R n is a central building block in area of signal processing (see e.g. [19], <ref> [25] </ref>, [26], [29], [10]) where it is usually called Least-Mean-Square (LMS) algorithm. Therefore, there is an extensive literature studying the convergence properties of this algorithm (see, e.g. [29], [10]). All this research, however, is based on probabilistic assumptions on the generation of the x t 's.
Reference: [26] <author> M.M. Sondhi and D.A. Berkley. </author> <title> Silencing echoes in the telephone network. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 68 </volume> <pages> 948-963, </pages> <year> 1980. </year>
Reference-contexts: A more substantive difference is that they assumed x 1 = x 2 = x 3 :::. The algorithm GD for the special case of linear functions in R n is a central building block in area of signal processing (see e.g. [19], [25], <ref> [26] </ref>, [29], [10]) where it is usually called Least-Mean-Square (LMS) algorithm. Therefore, there is an extensive literature studying the convergence properties of this algorithm (see, e.g. [29], [10]). All this research, however, is based on probabilistic assumptions on the generation of the x t 's.
Reference: [27] <author> V. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the 3nd Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [28] <author> B. Widrow and M.E. Hoff. </author> <title> Adaptive switching circuits. </title> <booktitle> 1960 IRE WESCON Conv. Record, </booktitle> <pages> pages 96-104, </pages> <year> 1960. </year>
Reference-contexts: All this research, however, is based on probabilistic assumptions on the generation of the x t 's. This paper shows that the algorithm GD can analyzed even without probabilistic assumptions. 2 Even though in the neural network community this algorithm is usually credited to Widrow and Hoff <ref> [28] </ref>, a similar algorithm for the iterative solution of a system of linear equations was previously developed by Kaczmarz [13]. Gradient descent is an algorithm design technique which has achieved considerable practical success in more complicated hypothesis spaces, in particular multilayer neural networks.
Reference: [29] <author> B. Widrow and S.D. Stearns. </author> <title> Adaptive signal processing. </title> <publisher> Pren-tice Hall, </publisher> <year> 1985. </year>
Reference-contexts: A more substantive difference is that they assumed x 1 = x 2 = x 3 :::. The algorithm GD for the special case of linear functions in R n is a central building block in area of signal processing (see e.g. [19], [25], [26], <ref> [29] </ref>, [10]) where it is usually called Least-Mean-Square (LMS) algorithm. Therefore, there is an extensive literature studying the convergence properties of this algorithm (see, e.g. [29], [10]). All this research, however, is based on probabilistic assumptions on the generation of the x t 's. <p> for the special case of linear functions in R n is a central building block in area of signal processing (see e.g. [19], [25], [26], <ref> [29] </ref>, [10]) where it is usually called Least-Mean-Square (LMS) algorithm. Therefore, there is an extensive literature studying the convergence properties of this algorithm (see, e.g. [29], [10]). All this research, however, is based on probabilistic assumptions on the generation of the x t 's.
Reference: [30] <author> N. Young. </author> <title> An introduction to Hilbert space. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: The last requirement can be dropped essentially without affecting the definition (see e.g. <ref> [30, page 25] </ref>). For x 2 X , the norm of x, denoted by jjxjj, is defined by jjxjj = p 5 Algorithm GD. Input: 0. * Choose X 's zero vector as initial hypothesis ^ w 1 . * On each trial t: 1. <p> Update the current hypothesis ^ w t according to the rule ^ w t+1 = ^ w t + (y t ^y t )x t : Fig. 1. Pseudo-code for algorithm GD. (See Theorems IV.1, IV.2, IV.3, and Corollary IV.1.) (These definitions are taken from <ref> [30] </ref>.) An example of an inner product is the dot product in R n . <p> Let L 2 (R + ) be the space of (measurable) functions g from R + to R for which R 1 0 g (u) 2 du is finite. L 2 (R + ) is well known to be an inner product space (see, e.g. <ref> [30] </ref>), with the inner product defined by (g 1 ; g 2 ) = 0 Algorithm A SMO . Input: E; W; X 0. * On each trial t: 1. Get x t 2 [0; X] from the environment. 2. <p> Let L 2 (R n + ) be the space of (measurable) functions g from R n + to R for which Z 1 : : : 0 is finite. Again, it is well known (see e.g. <ref> [30] </ref>), that L 2 (R n + ) has an inner product defined by (g 1 ; g 2 ) = 0 Z 1 g 1 (x)g 2 (x) dx n : : : dx 1 : Now apply algorithm GD to this particular inner product space, L 2 (R n <p> This theorem further has obvious consequences concerning the finite dimension case 6 An orthonormal basis can be found under quite general conditions. See e.g. <ref> [30] </ref> for details. 13 when the "noise level" E is not too large relative to the number n of variables as well as X and Y . Theorem VII.2: Let hX d i d2N be any sequence of inner product spaces such that X d is a d-dimensional vector space.
References-found: 30

