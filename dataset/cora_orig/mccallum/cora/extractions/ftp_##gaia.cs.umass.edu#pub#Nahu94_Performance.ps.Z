URL: ftp://gaia.cs.umass.edu/pub/Nahu94:Performance.ps.Z
Refering-URL: http://www.cs.umass.edu/~nahum/home.html
Root-URL: 
Title: Performance Issues in Parallelized Network Protocols  
Author: Erich M. Nahum, David J. Yates, James F. Kurose, and Don Towsley 
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Date: November 1994.  
Note: Appears in "Proceedings of the First Symposium on Operating Systems Design and Implementation," Usenix Association,  
Abstract: This paper provides an experimental performance study of packet-level parallelism on a contemporary shared-memory multiprocessor. We examine several unexplored areas in packet-level parallelism and investigate how various protocol structuring and implementation techniques can affect performance. We study TCP/IP and UDP/IP protocol stacks, implemented with a parallel version of the x-kernel running in user space on Silicon Graphics multiprocessors. Our results show that only limited packet-level parallelism can be achieved within a single connection under TCP, but that using multiple connections can improve available parallelism. We also demonstrate that packet ordering plays a key role in determining single-connection TCP performance, that careful use of locks is a necessity, and that selective exploitation of caching can improve throughput. We also describe experiments that compare parallel protocol performance on two generations of a parallel machine and show how computer architectural trends can influence performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brian Allison. </author> <title> DEC 7000/10000 Model 600 AXP multiprocessor server. </title> <booktitle> In Proceedings IEEE COMPCON, </booktitle> <pages> pages 456-464, </pages> <address> San Francisco CA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Erich Nahum was supported by an ARPA Research Assistantship in Parallel Processing. David Yates is the recipient of a Motorola Codex University Partnership in Research Grant. The authors can be reached at -nahum, yates, kurose, towsley-@cs.umass.edu. shown by recent vendor introductions <ref> [1, 8, 9] </ref>. There is thus an opportunity to exploit the potential of parallelism in network protocol processing, and this has become a growing area of research. The approach we study here is that of packet-level parallelism, sometimes referred to as thread-per-packet or processor-per-message parallelism.
Reference: [2] <author> Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Since caching has been shown to be effective in data structure manipulations <ref> [2, 7] </ref>, we decided to evaluate the use of simple per-thread resource caches in the message tool. Whenever a thread requires a new MNode (the message tool's internal data representation), it first checks a local cache, which can be done without locking.
Reference: [3] <author> Thomas E. Anderson, Henry M. Levy, Brian N. Bershad, and Edward D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <year> 1991. </year>
Reference-contexts: This is only one architectural comparison, with different generations of both the MIPS architecture and multiprocessor interconnects. Still, it suggests that network protocol processing speed may not be improving as fast as application performance, which agrees with the operating system trends shown in <ref> [3, 24] </ref>. We plan to investigate this further. 8 Conclusions and Future Work We briefly summarize our findings as follows: * Preserving order pays.
Reference: [4] <author> Mats Bjorkman. </author> <title> The xx-Kernel: an execution environment for parallel execution of communication protocols. </title> <institution> Dept. of Computer Science, Uppsala University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: In Section 8 we summarize our results and conclude. 2 Experimental Environment As stated earlier, our environment is based on a parallelized x-kernel, and as such, it is similar in several respects to the platform described by Bjorkman and Gunningberg at the Swedish Institute of Computer Science (SICS) <ref> [4, 5] </ref>. Our platform was, for the most part, developed independently, and for a different type of machine. The exception is the SICS MP TCP code, which we used to guide the design of our parallel TCP, as described in Section 5.1.
Reference: [5] <author> Mats Bjorkman and Per Gunningberg. </author> <title> Locking effects in multiprocessor implementations of protocols. </title> <booktitle> In ACM SIG-COMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 74-83, </pages> <address> San Francisco, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Originally proposed by Hutchinson and Peterson in the x-kernel [14], this approach distributes packets across processors, achieving speedup both with multiple connections and within a single connection. Packets can be processed on any processor, maximizing flexibility and utilization. Other systems using this approach include <ref> [5, 11] </ref>. Several other approaches to parallelism have also been proposed and are briefly described here; more detailed surveys can be found in [5, 11]. In layered parallelism, protocols are assigned to specific processors, and messages passed between layers through interprocess communication. <p> Packets can be processed on any processor, maximizing flexibility and utilization. Other systems using this approach include <ref> [5, 11] </ref>. Several other approaches to parallelism have also been proposed and are briefly described here; more detailed surveys can be found in [5, 11]. In layered parallelism, protocols are assigned to specific processors, and messages passed between layers through interprocess communication. Parallelism gains can be achieved mainly through pipelining effects. An example is found in [10]. Connection-level parallelism associates connections with a single processor or thread, achieving speedup with multiple connections. <p> In Section 8 we summarize our results and conclude. 2 Experimental Environment As stated earlier, our environment is based on a parallelized x-kernel, and as such, it is similar in several respects to the platform described by Bjorkman and Gunningberg at the Swedish Institute of Computer Science (SICS) <ref> [4, 5] </ref>. Our platform was, for the most part, developed independently, and for a different type of machine. The exception is the SICS MP TCP code, which we used to guide the design of our parallel TCP, as described in Section 5.1. <p> The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This is similar to the approaches taken in <ref> [5, 11, 21, 28] </ref>. The drivers act as senders or receivers, producing or consuming packets as fast as possible, to simulate the behavior of a simplex data transfer over an error-free network. <p> To check variance, we ran one 8-processor test 400 times, and observed that the data fit a normal bell-curve distribution. Throughput graphs include 90 percent confidence intervals. The figures show that, as Bjorkman and Gunningberg discovered <ref> [5] </ref>, UDP send-side performance scales well with larger numbers of processors. In our discussion, scalability means the first derivative of speedup as the last processor is added to the experiment. Note that a test can demonstrate high speedup to a point but exhibit poor scalability.
Reference: [6] <editor> Dave Borman. NTCP: </editor> <title> A proposal for the next generation of TCP and UDP. </title> <booktitle> In Submission to the End2End-Interest mailing list, </booktitle> <pages> pages 1-37, </pages> <address> Eagan, MN, </address> <year> 1993. </year> <note> Cray Research. End2End archives available via FTP at ftp.isi.edu. </note>
Reference-contexts: This turns out to be important for the high bandwidths generated by our experiments, and we note that 32-bit flow control information is used in both 4.4 BSD with large windows [16] and in the next-generation TCP proposals <ref> [6, 30] </ref>. Due to the semantics of TCP, the protocol consequently has a great deal of per-connection state, which must be locked to provide consistency and semantic correctness. For example, each connection has a retransmission queue, a reassembly queue, and various windows for both the send and receive sides.
Reference: [7] <author> Peter Druschel and Larry L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 189-202, </pages> <address> Asheville, NC, </address> <month> Dec </month> <year> 1993. </year>
Reference-contexts: Since caching has been shown to be effective in data structure manipulations <ref> [2, 7] </ref>, we decided to evaluate the use of simple per-thread resource caches in the message tool. Whenever a thread requires a new MNode (the message tool's internal data representation), it first checks a local cache, which can be done without locking.
Reference: [8] <editor> Michel Cekleov et. al. </editor> <booktitle> SPARCCenter 2000:Multiprocessing for the 90's! In Proceedings IEEE COMPCON, </booktitle> <pages> pages 345-353, </pages> <address> San Francisco CA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Erich Nahum was supported by an ARPA Research Assistantship in Parallel Processing. David Yates is the recipient of a Motorola Codex University Partnership in Research Grant. The authors can be reached at -nahum, yates, kurose, towsley-@cs.umass.edu. shown by recent vendor introductions <ref> [1, 8, 9] </ref>. There is thus an opportunity to exploit the potential of parallelism in network protocol processing, and this has become a growing area of research. The approach we study here is that of packet-level parallelism, sometimes referred to as thread-per-packet or processor-per-message parallelism.
Reference: [9] <author> Mile Galles and Eric Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <type> Technical report, </type> <institution> Silicon Graphics Inc., Mt. View, </institution> <address> CA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Erich Nahum was supported by an ARPA Research Assistantship in Parallel Processing. David Yates is the recipient of a Motorola Codex University Partnership in Research Grant. The authors can be reached at -nahum, yates, kurose, towsley-@cs.umass.edu. shown by recent vendor introductions <ref> [1, 8, 9] </ref>. There is thus an opportunity to exploit the potential of parallelism in network protocol processing, and this has become a growing area of research. The approach we study here is that of packet-level parallelism, sometimes referred to as thread-per-packet or processor-per-message parallelism. <p> Without more detailed information, we cannot assert any explanations for the behavior. We do note though, that the Power Series performs locking using a separate dedicated synchronization bus, similar to the Sequent. The Challenge, however, uses memory to synchronize, relying on the coherency protocol and the load-linked/store-conditional instructions <ref> [9] </ref>. Given that Bjorkman and Gunningberg did not observe the receive-side drop for their UDP receive side tests on the Sequent, we suspect that the difference in synchronization may be the cause of the anomaly. We are pursuing further studies along these dimensions.
Reference: [10] <author> Dario Giarrizzo, Matthias Kaiserswerth, Thomas Wicki, and Robin C. Williamson. </author> <title> High-speed parallel protocol implementation. </title> <booktitle> First IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <pages> pages 165-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In layered parallelism, protocols are assigned to specific processors, and messages passed between layers through interprocess communication. Parallelism gains can be achieved mainly through pipelining effects. An example is found in <ref> [10] </ref>. Connection-level parallelism associates connections with a single processor or thread, achieving speedup with multiple connections. Multiprocessor STREAMS most closely matches this model [26, 27]. Functional parallelism decomposes functions within a single protocol and assigns them to processing elements. Examples include [19, 23, 25].
Reference: [11] <author> Murray W. Goldberg, Gerald W. Neufeld, and Mabo R. Ito. </author> <title> A parallel approach to OSI connection-oriented protocols. </title> <booktitle> Third IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <pages> pages 219-232, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Originally proposed by Hutchinson and Peterson in the x-kernel [14], this approach distributes packets across processors, achieving speedup both with multiple connections and within a single connection. Packets can be processed on any processor, maximizing flexibility and utilization. Other systems using this approach include <ref> [5, 11] </ref>. Several other approaches to parallelism have also been proposed and are briefly described here; more detailed surveys can be found in [5, 11]. In layered parallelism, protocols are assigned to specific processors, and messages passed between layers through interprocess communication. <p> Packets can be processed on any processor, maximizing flexibility and utilization. Other systems using this approach include <ref> [5, 11] </ref>. Several other approaches to parallelism have also been proposed and are briefly described here; more detailed surveys can be found in [5, 11]. In layered parallelism, protocols are assigned to specific processors, and messages passed between layers through interprocess communication. Parallelism gains can be achieved mainly through pipelining effects. An example is found in [10]. Connection-level parallelism associates connections with a single processor or thread, achieving speedup with multiple connections. <p> The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This is similar to the approaches taken in <ref> [5, 11, 21, 28] </ref>. The drivers act as senders or receivers, producing or consuming packets as fast as possible, to simulate the behavior of a simplex data transfer over an error-free network. <p> The trends agree somewhat with those shown in <ref> [11] </ref>, which showed better speedup with larger data units. However, their tests included presentation-layer conversion, which is much more compute-bound and data-intensive than checksumming. Although the SGI documentation gives the aggregate bus bandwidth as 1.2 gigabytes/sec, we wished to see the read bandwidth limitations imposed by checksumming. <p> In this example, the application is our test code, which simply counts packets that arrive. The application's critical section itself is small, a lock-increment-unlock sequence; the performance is lost preserving the order. We are not the first to observe this problem <ref> [11, 13] </ref>, but to our knowledge, previous work has not provided adequate solutions. For example, in [11], Goldberg et. al. use a ticketing scheme similar to ours, but assign tickets to packets at the driver for use in re-ordering at the application. <p> The application's critical section itself is small, a lock-increment-unlock sequence; the performance is lost preserving the order. We are not the first to observe this problem [11, 13], but to our knowledge, previous work has not provided adequate solutions. For example, in <ref> [11] </ref>, Goldberg et. al. use a ticketing scheme similar to ours, but assign tickets to packets at the driver for use in re-ordering at the application. However, this assumes a one-to-one correspondence between arriving packets and application data units.
Reference: [12] <author> Maurice Herlihy. </author> <title> A methodology for implementing highly concurrent data objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(5) </volume> <pages> 6-16, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: LL and SC allow programmers to produce lock free primitives <ref> [12] </ref>. A simple example of this is atomic increment, which replaces a lock-increment-unlock sequence. We tried this for two reasons. First, the x-kernel's message tool relies on the notion that reference counts are atomically manipulated, and so the primitives map perfectly with the existing code.
Reference: [13] <author> Norman C. Hutchinson. </author> <title> Protocols versus parallelism. </title> <booktitle> In Proceedings from the x-Kernel Workshop, </booktitle> <address> Tucson, AZ, Novem-ber 1992. </address> <institution> University of Arizona. </institution>
Reference-contexts: In this example, the application is our test code, which simply counts packets that arrive. The application's critical section itself is small, a lock-increment-unlock sequence; the performance is lost preserving the order. We are not the first to observe this problem <ref> [11, 13] </ref>, but to our knowledge, previous work has not provided adequate solutions. For example, in [11], Goldberg et. al. use a ticketing scheme similar to ours, but assign tickets to packets at the driver for use in re-ordering at the application.
Reference: [14] <author> Norman C. Hutchinson and Larry L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> Jan-uary </month> <year> 1991. </year>
Reference-contexts: The approach we study here is that of packet-level parallelism, sometimes referred to as thread-per-packet or processor-per-message parallelism. Originally proposed by Hutchinson and Peterson in the x-kernel <ref> [14] </ref>, this approach distributes packets across processors, achieving speedup both with multiple connections and within a single connection. Packets can be processed on any processor, maximizing flexibility and utilization. Other systems using this approach include [5, 11].
Reference: [15] <author> Van Jacobson. </author> <title> Efficient protocol implementation. </title> <booktitle> In ACM SIGCOMM 1990 Tutorial Notes, </booktitle> <address> Philadelphia, PA, </address> <month> Septem-ber </month> <year> 1990. </year>
Reference-contexts: In addition to adding header prediction, this involved updating the congestion control and timer mechanisms, as well as reordering code in the send side to test for the most frequent scenarios first <ref> [15] </ref>. The one change we made to the base Net/2 structure was to use 32-bit flow-control windows, rather than the 16-bit windows defined by the TCP specification.
Reference: [16] <author> Van Jacobson, Robert Braden, and Dave Borman. </author> <title> TCP extensions for high performance. </title> <booktitle> In Network Information Center RFC 1323, </booktitle> <pages> pages 1-37, </pages> <address> Menlo Park, CA, </address> <month> May </month> <year> 1992. </year> <note> SRI International. </note>
Reference-contexts: This turns out to be important for the high bandwidths generated by our experiments, and we note that 32-bit flow control information is used in both 4.4 BSD with large windows <ref> [16] </ref> and in the next-generation TCP proposals [6, 30]. Due to the semantics of TCP, the protocol consequently has a great deal of per-connection state, which must be locked to provide consistency and semantic correctness.
Reference: [17] <author> Jonathan Kay and Joseph Pasquale. </author> <title> The importance of non-data touching processing overheads in TCP/IP. </title> <booktitle> In SIG-COMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 259-269, </pages> <address> San Francisco, CA, </address> <month> September </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Our expectations were that relative speedup would be greater when processing larger packets with checksumming, since checksumming occurs outside of locked regions and thus constant per-packet costs would constitute a smaller fraction of the processing time <ref> [17] </ref>. Figures 3, 5, 7, and 9 show that, in general, tests with larger packets have better speedup than those with smaller ones, and experiments for a particular packet size with checksumming have better speedup than those without, although the differences are not as pronounced as we had expected.
Reference: [18] <author> Jonathan Kay and Joseph Pasquale. </author> <title> Measurement, analysis, and improvement of UDP/IP throughput for the DECStation 5000. </title> <booktitle> In USENIX Winter 1993 Technical Conference, </booktitle> <pages> pages 249-258, </pages> <address> San Diego, CA, </address> <year> 1993. </year>
Reference-contexts: We thus wished to examine the extent to which checksumming made a difference in protocol speedup and throughput. The checksum code used in our studies was the fastest available portable algorithm that we were aware of, which was from UCSD <ref> [18] </ref>. 2.3 In-Memory Drivers Since our platform runs in user space, accessing the FDDI adaptor involves crossing the IRIX socket layer, which is prohibitively expensive. Normally, in a user-space implementation of the x-kernel, a simulated device driver is configured below the media access control layer (in this case, FDDI).
Reference: [19] <author> Odysseas G. Koufopavlou and Martina Zitterbart. </author> <title> Parallel TCP for high performance communication subsystems. </title> <booktitle> In Proceedings of the IEEE Global Telecommunications Conference (GLOBECOM), </booktitle> <pages> pages 1395-1399, </pages> <year> 1992. </year>
Reference-contexts: An example is found in [10]. Connection-level parallelism associates connections with a single processor or thread, achieving speedup with multiple connections. Multiprocessor STREAMS most closely matches this model [26, 27]. Functional parallelism decomposes functions within a single protocol and assigns them to processing elements. Examples include <ref> [19, 23, 25] </ref>. The relative merits of one approach over the others depends on many factors, including the host architecture, the number of connections, whether the implementation is in hardware or software, the thread scheduling policies employed , and the cost of primitives such as locking and context switching.
Reference: [20] <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference: [21] <author> Bert Lindgren, Bobby Krupczak, Mostafa Ammar, and Karsten Schwan. </author> <title> An architecture and toolkit for parallel and configurable protocols. </title> <booktitle> In Proceedings of the International Conferenceon Network Protocols, </booktitle> <address> San Francisco, CA, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This is similar to the approaches taken in <ref> [5, 11, 21, 28] </ref>. The drivers act as senders or receivers, producing or consuming packets as fast as possible, to simulate the behavior of a simplex data transfer over an error-free network. <p> Using either a connectionless protocol such as UDP or a connection-oriented protocol such as TCP with multiple connections, an application must be able to handle out-of order delivery. Lindgren et. al. <ref> [21] </ref> make a related argument that the parallel application must be tied closely to the parallel communication system. To illustrate the benefits of using multiple connections, we ran send-side and receive-side experiments of TCP-1 with MCS locks, without ticketing, using 4KB packets with and without checksumming.
Reference: [22] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: To preserve the original ordering, we implemented FIFO queueing using the MCS locks by Mellor-Crummey and Scott <ref> [22] </ref>. Their locking algorithm requires atomic swap and compare-and-swap functions, which we implemented using short R4000 assembler routines. checksumming on. The top curve in the figure is from the modified TCP where packets are assumed to be in order, a potential upper bound.
Reference: [23] <author> Arun N. Netravali, W. D. Roome, and K. Sabnani. </author> <title> Design and implementation of a high-speed transport protocol. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(11) </volume> <pages> 2010-2024, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: An example is found in [10]. Connection-level parallelism associates connections with a single processor or thread, achieving speedup with multiple connections. Multiprocessor STREAMS most closely matches this model [26, 27]. Functional parallelism decomposes functions within a single protocol and assigns them to processing elements. Examples include <ref> [19, 23, 25] </ref>. The relative merits of one approach over the others depends on many factors, including the host architecture, the number of connections, whether the implementation is in hardware or software, the thread scheduling policies employed , and the cost of primitives such as locking and context switching.
Reference: [24] <author> John Ousterhout. </author> <booktitle> Why aren't operating systems getting faster as fast as hardware? In Proceedings of the Summer USENIX Conference, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This is only one architectural comparison, with different generations of both the MIPS architecture and multiprocessor interconnects. Still, it suggests that network protocol processing speed may not be improving as fast as application performance, which agrees with the operating system trends shown in <ref> [3, 24] </ref>. We plan to investigate this further. 8 Conclusions and Future Work We briefly summarize our findings as follows: * Preserving order pays.
Reference: [25] <author> Tom F. La Porta and Mischa Schwartz. </author> <title> A high-speed protocol parallel implementation: Design and analysis. </title> <booktitle> Fourth IFIP TC6.1/WG6.4 International Conference on High Performance Networking, </booktitle> <pages> pages 135-150, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: An example is found in [10]. Connection-level parallelism associates connections with a single processor or thread, achieving speedup with multiple connections. Multiprocessor STREAMS most closely matches this model [26, 27]. Functional parallelism decomposes functions within a single protocol and assigns them to processing elements. Examples include <ref> [19, 23, 25] </ref>. The relative merits of one approach over the others depends on many factors, including the host architecture, the number of connections, whether the implementation is in hardware or software, the thread scheduling policies employed , and the cost of primitives such as locking and context switching.
Reference: [26] <author> David Presotto. </author> <title> Multiprocessor streams for Plan 9. </title> <booktitle> In UKUUG, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Parallelism gains can be achieved mainly through pipelining effects. An example is found in [10]. Connection-level parallelism associates connections with a single processor or thread, achieving speedup with multiple connections. Multiprocessor STREAMS most closely matches this model <ref> [26, 27] </ref>. Functional parallelism decomposes functions within a single protocol and assigns them to processing elements. Examples include [19, 23, 25].
Reference: [27] <author> Sunil Saxena, J. Kent Peacock, Fred Yang, Vijaya Verma, and Mohan Krishnan. </author> <title> Pitfalls in multithreading SVR4 STREAMS and other weightless processes. </title> <booktitle> In Winter 1993 USENIX Technical Conference, </booktitle> <pages> pages 85-96, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Parallelism gains can be achieved mainly through pipelining effects. An example is found in [10]. Connection-level parallelism associates connections with a single processor or thread, achieving speedup with multiple connections. Multiprocessor STREAMS most closely matches this model <ref> [26, 27] </ref>. Functional parallelism decomposes functions within a single protocol and assigns them to processing elements. Examples include [19, 23, 25].
Reference: [28] <author> Douglas C. Schmidt and Tatsuya Suda. </author> <title> Measuring the impact of alternative parallel process architectures on communication subsystem performance. </title> <booktitle> Fourth IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: The relative merits of one approach over the others depends on many factors, including the host architecture, the number of connections, whether the implementation is in hardware or software, the thread scheduling policies employed , and the cost of primitives such as locking and context switching. Schmidt and Suda <ref> [28] </ref> show that packet-level parallelism and connection-level parallelism generally perform better than layer parallelism on a shared-memory multiprocessor, due to the context-switching overhead when crossing layers using layer parallelism. This paper provides an experimental performance study of packet-level parallelism using TCP/IP and UDP/IP protocol stacks. <p> The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This is similar to the approaches taken in <ref> [5, 11, 21, 28] </ref>. The drivers act as senders or receivers, producing or consuming packets as fast as possible, to simulate the behavior of a simplex data transfer over an error-free network.
Reference: [29] <author> Michael D. Smith. </author> <title> Tracing with Pixie. </title> <type> Technical report, </type> <institution> Center for Integrated Systems, Stanford University, </institution> <address> Stan-ford, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The TCP numbers here are from our baseline TCP, TCP-1, further described in section 5.1. Our results show that TCP does not scale nearly as well as UDP, in either the send or receive case. Locking state is the culprit here. For example, profiling with Pixie <ref> [29] </ref> shows that in an 8-processor receive-side test, 90 percent of the time is spent waiting to acquire the TCP connection state lock; on the send side, the amount is 85 percent. Several unusual points warrant mentioning.
Reference: [30] <author> Robert Ullman. TP/IX: </author> <title> The next internet. In Network Information Center RFC 1475, </title> <address> Menlo Park, CA, </address> <month> June </month> <year> 1993. </year> <note> SRI International. </note>
Reference-contexts: This turns out to be important for the high bandwidths generated by our experiments, and we note that 32-bit flow control information is used in both 4.4 BSD with large windows [16] and in the next-generation TCP proposals <ref> [6, 30] </ref>. Due to the semantics of TCP, the protocol consequently has a great deal of per-connection state, which must be locked to provide consistency and semantic correctness. For example, each connection has a retransmission queue, a reassembly queue, and various windows for both the send and receive sides.
Reference: [31] <author> George Varghese and Tony Lauck. </author> <title> Hashed and hierarchical timing wheels: Data structures for the efficient implementation of a timer facility. </title> <booktitle> In The Proceedings of the 11th Symposium on Operating System Principles, </booktitle> <month> November </month> <year> 1987. </year>
Reference-contexts: To handle this recursion, counting locks are used, so that if a thread already owns the lock, it simply increments a count and proceeds. Similarly, an unlock decrements the count, and the lock is released when the count reaches zero. The event manager uses a timing wheel <ref> [31] </ref> to manage events which are to occur in the future. The wheel is essentially another chained-bucket hash table, where the hashing function is based on the time that the event is scheduled to run.
References-found: 31

