URL: ftp://ftp.cs.dartmouth.edu/TR/TR98-339.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR98-339/
Root-URL: http://www.cs.dartmouth.edu
Title: Avoiding Conflicts Dynamically in Direct Mapped Caches with Minimal Hardware Support  
Author: Peter DeSantis Advisor: Thomas Cormen 
Degree: Senior Honors Thesis  
Date: June, 1998  
Address: Hanover, New Hampshire  TR# PCS-TR98-339  
Affiliation: Dartmouth College  Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [DEC93] <author> "ATOM: </author> <title> Reference Manual", </title> <address> Digitial Equiptment Corperation, </address> <month> December </month> <year> 1993. </year> <title> [DEC95] "ATOM: User Manual" Digital Equiptment Corperation, </title> <month> June </month> <year> 1995. </year>
Reference-contexts: The object code was instrumented with ATOM. Every memory reference was instrumented by ATOM, 12 System Cache Assumptions Cache Size 256K Block Size 1K Indexing Virtual Table 3: Parameters of simulated system cache. resulting in a call to a user-defined function. [S94] <ref> [DEC93] </ref> [DEC95] This function both traced the actual system cache and performed the appropriate sampling routine. Table 3 shows the parameters of the simulated system cache. These parameters follow the earlier work of [BCL1].
Reference: [BCL1] <author> Bershad, B. N., Chen, J. B., Lee, Dennis, and Romer, T. H. </author> <title> "Avoiding Cache Misses Dynamically in Large Direct-Mapped Caches." </title> <booktitle> Proc. 6th Interna 48 tional Conference on Architectural Support Programming Languages and Oper--ating Systems, </booktitle> <pages> pages 158-170. </pages> <publisher> ACM, </publisher> <year> 1994. </year>
Reference-contexts: Remapping the page simulates associativity in the direct-mapped cache. The new page will be recolored, assigned to a different cache location, and 2 the data will no longer be contending for the same cache block thus eliminating future conflicts. <ref> [BCL1] </ref> For such a recoloring strategy to be effective, pages must be recolored only when the cost of recoloring is less than the cost of the averted future cache misses. To identify good recoloring candidates, a distinction must be made between various kinds of cache misses. <p> After this initial the work, the authors looked at ways to monitor the cache without the addition of costly dedicated hardware. 2.1 The CML The Cache Miss Lookaside (CML) buffer attempted to provide a low overhead solution to identifying candidate pages for remapping. <ref> [BCL1] </ref> The device exploits the extra time available on a cache miss. On every cache miss, the address of the page that missed is sent to the CML. The CML has two sections of register pairs: HOT and LRU. Each register pair contains a page address and a miss count. <p> Table 3 shows the parameters of the simulated system cache. These parameters follow the earlier work of <ref> [BCL1] </ref>. After the trace completed, the predicted cache results were compared to the targeted conflicts in the simulated system cache. 5 Testing Sampling Accuracy 5.1 Experiment 1 The first set of experiments attempts to correlate the Prediction Cache with the actual simulated system cache. <p> The length of the sampling sessions was estimated empirically. The cost of remapping pages and of the cache miss were adapted from <ref> [BCL1] </ref> and [BCL2]. Experimental Results Benchmark Number of Sampling Sessions Number of Pages Remapped Cache Misses Avoided Tomcatv 21,406 2,841 1,325,170,360 Swim 2,214 230 30,422,749 Fpppp 3 4 1,785,218 Table 5: Ewpirical results from final experiment. the Tomcatv benchmark, sampling accounts for about 75% of the total time.
Reference: [BCL2] <author> Bershad, B. N., Chen, J. B., Lee, Dennis, and Romer, T. H. </author> <title> "Dynamic Page Mapping Policies for Cache Conflict Resolution on Standard Hardware." </title> <booktitle> Proceedings of the First Symposium on Operating Systems Design and Implementation, Usenix, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: In no case does the CML perform any better than the 2-way associative cache. 2.2 Identify Cache Conflicts on Standard Hardware In later work, the ideas of the CML were applied to standard hardware by using the system's TLB. <ref> [BCL2] </ref> By monitoring the TLB, either through page protection mechanisms or through the system's software-controlled TLB, the operating system attempts to locate pages that should be recolored. Many different software policies, divided into active and periodic policies, were tested. <p> The length of the sampling sessions was estimated empirically. The cost of remapping pages and of the cache miss were adapted from [BCL1] and <ref> [BCL2] </ref>. Experimental Results Benchmark Number of Sampling Sessions Number of Pages Remapped Cache Misses Avoided Tomcatv 21,406 2,841 1,325,170,360 Swim 2,214 230 30,422,749 Fpppp 3 4 1,785,218 Table 5: Ewpirical results from final experiment. the Tomcatv benchmark, sampling accounts for about 75% of the total time.
Reference: [H96] <author> Hennessy, John, and Patterson, David. </author> <title> Computer Architecture A Quantitative Approach, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Fransisco, California, </address> <year> 1996. </year>
Reference-contexts: After x has been loaded into the cache, a conflict miss occurs if m or more distinct references to cache blocks in x's associativity set not containing x have occurred since the last reference to x. <ref> [H96] </ref> Using this distinction, the pathological case of the direct-mapped cache can be 3 discussed. In a direct-mapped cache, the associativity is 1; that is, each piece of data can be mapped to exactly one cache block.
Reference: [H88] <author> Hill, Mark D. </author> <title> "A Case For Direct Mapped Caches'." </title> <publisher> IEEE, </publisher> <month> December </month> <year> 1988. </year>
Reference-contexts: The advantages of parallel address translation are not significant when cache size gets large (straightforward parallel address translation requires cache size not to exceed page size times associativity). Additionally, in many modern cache architectures, virtual addressing is used, making parallel address translation unnecessary. <ref> [H88] </ref> The former two disadvantages create the most compelling case against direct-mapped cache architecture. Many solutions to these problems have been investigated recently.
Reference: [S95] <author> Sites, Richard L. </author> <title> "Method and Apparatus for Cache Miss Reduction by Simulating Cache Associativity.", </title> <institution> United States Patent 5,442,571. </institution> <month> August, </month> <year> 1995. </year>
Reference-contexts: The simplest implementation would be a latch that saved the conflicting page numbers on every cache miss. Such a system was patented and eventually implemented by DEC. <ref> [S95] </ref> The critical timing of such an approach is well within the time available during a cache miss. Also, the cost of such a device is insignificant when compared to the CML or the associativity hardware on a higher associativity cache.
Reference: [S94] <author> Srivastava, Amitabh, and Eustace, Alan. </author> <title> "ATOM: A System for Building Customized Program Analysis Tools." </title> <booktitle> SIGPLAN `94 Conference on Programming Language Design and Implementation., </booktitle> <year> 1994. </year> <month> 49 </month>
Reference-contexts: All simulations were done on DEC Alpha workstations. The object code was instrumented with ATOM. Every memory reference was instrumented by ATOM, 12 System Cache Assumptions Cache Size 256K Block Size 1K Indexing Virtual Table 3: Parameters of simulated system cache. resulting in a call to a user-defined function. <ref> [S94] </ref> [DEC93] [DEC95] This function both traced the actual system cache and performed the appropriate sampling routine. Table 3 shows the parameters of the simulated system cache. These parameters follow the earlier work of [BCL1].
References-found: 7

