URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/93/tr1164.ps.Z
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/tech_reports/tech_reports.html
Root-URL: 
Title: Cache Profiling and the SPEC Benchmarks: A Case Study  
Author: Alvin R. Lebeck David A. Wood 
Keyword: Performance, Cache Memories, Cache Profile, Program Behavior, Performance Tuning  
Note: To appear IEEE Computer  
Address: 1210 West Dayton Street Madison, WI. 53706  
Affiliation: Computer Sciences Department University of Wisconsin Madison  
Email: alvy@cs.wisc.edu  
Phone: (608) 262-6617  
Date: June 1994  
Abstract: As VLSI technology improvements continue to widen the gap between processor and main memory cycle times, cache performance becomes increasingly important to overall system performance. Cache memories help alleviate the cycle time disparity, but only for programs that exhibit sufficient spatial and temporal locality. Programs with unruly access patterns spend much of their time transferring data to and from the cache. To fully exploit the performance potential of fast processors, programmers must explicitly consider cache behavior, restructuring their codes to increase locality. As these fast processors proliferate, techniques for improving cache performance must move beyond the supercomputer and multiprocessor communities and into the mainstream of computing. In this paper, we examine some of the techniques that programmers can use to improve cache performance. We show how to use CPROF, a cache profiler, to identify cache performance bottlenecks and gain insight into their origin. This insight helps programmers understand which of the well-known program transformations are likely to improve cache performance. Using CPROF and a "cookbook" of simple transformations, we show how to tune the cache performance of six of the SPEC92 benchmarks. By restructuring the source code, we greatly improve cache behavior and achieve execution time speedups ranging from 1.02 to 3.46. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Callahan, K. Kennedy, and A. Porterfield, </author> <title> Analyzing and Visualizing Performance of Memory Hierarchies, Instrumentation for Visualization, </title> <publisher> ACM Press, </publisher> <year> (1990). </year>
Reference-contexts: Consider the simple example of nested loops where the outer-loop iterates L times and the inner-loop sequentially accesses an array of N 4-byte integers. - 4 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh a) Cache b) Small Array c) Large Array A [0] A [2] A [4] A <ref> [1] </ref> . . . . . A [8] A [10] . . Sequentially accessing an array that fits in cache (Figure 1b) should produce M cache misses, where M is the number of cache blocks required to hold the array. <p> There are a number of cache and memory system profilers that differ in the level of detail they present to a programmer. High-level tools, such as MTOOL [2], identify procedures or basic blocks that incur large memory overheads. Other cache profilers, such as PFC-Sim <ref> [1] </ref> and CPROF, identify cache misses at the source line level, allowing much more detailed analysis. Of course this extra detail does not come for free; MTOOL runs much faster than profilers requiring address tracing and full cache simulation. <p> | | | | | | | | | | | | | | Speedup btrix cholesky compress dnasa7 eqntott gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key [6] key [7] i + 1 val [0] val <ref> [1] </ref> val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the key and value arrays (Figure 5a) resulted in as <p> Speedup btrix cholesky compress dnasa7 eqntott gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key [6] key [7] i + 1 val [0] val <ref> [1] </ref> val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the key and value arrays (Figure 5a) resulted in as many as two cache misses for each successful hash table probe. <p> cholesky compress dnasa7 eqntott gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key [6] key [7] i + 1 val [0] val <ref> [1] </ref> val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the key and value arrays (Figure 5a) resulted in as many as two cache misses for each successful hash table probe.
Reference: 2. <author> A. J. Goldberg and J. Hennessy, </author> <title> Performance Debugging Shared Memory Multiprocessor Programs with MTOOL, </title> <booktitle> Proceedings Supercomputing '91, </booktitle> <pages> pp. </pages> <month> 481-490 (November </month> <year> 1991). </year>
Reference-contexts: Consider the simple example of nested loops where the outer-loop iterates L times and the inner-loop sequentially accesses an array of N 4-byte integers. - 4 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh a) Cache b) Small Array c) Large Array A [0] A <ref> [2] </ref> A [4] A [1] . . . . . A [8] A [10] . . Sequentially accessing an array that fits in cache (Figure 1b) should produce M cache misses, where M is the number of cache blocks required to hold the array. <p> There are a number of cache and memory system profilers that differ in the level of detail they present to a programmer. High-level tools, such as MTOOL <ref> [2] </ref>, identify procedures or basic blocks that incur large memory overheads. Other cache profilers, such as PFC-Sim [1] and CPROF, identify cache misses at the source line level, allowing much more detailed analysis. <p> | | | | | | | | | | | | Speedup btrix cholesky compress dnasa7 eqntott gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key [6] key [7] i + 1 val [0] val [1] val <ref> [2] </ref> val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the key and value arrays (Figure 5a) resulted in as many as
Reference: 3. <author> S. L. Graham, P. B. Kessler, and M. K. McKusick, </author> <title> An Execution Profiler for Modular Programs, </title> <journal> Software Practice & Experience 13 pp. </journal> <month> 671-685 </month> <year> (1983). </year>
Reference-contexts: Although asymptotic analysis is effective for certain algorithms, analyzing large complex programs is very difficult. Instead, programmers often rely on an execution-time profile to isolate problematic code sections, and then apply asymptotic analysis only on those sections. Unfortunately, traditional execution-time profiling tools, e.g., - 2 - gprof <ref> [3] </ref>, are generally insufficient to identify cache performance problems. For the example above, an execution--time profile would identify the procedure or source lines as a bottleneck, but the programmer could easily conclude that the floating-point operations were responsible. <p> | | | | | | | | | | Speedup btrix cholesky compress dnasa7 eqntott gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key [6] key [7] i + 1 val [0] val [1] val [2] val <ref> [3] </ref> j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the key and value arrays (Figure 5a) resulted in as many as two cache
Reference: 4. <author> A. Gupta, M. Martonosi, and T. Anderson, MemSpy: </author> <title> Analyzing Memory System Bottlenecks in Programs, Performance Evaluation Review 20(1) pp. </title> <month> 1-12 (June </month> <year> 1992). </year>
Reference-contexts: Consider the simple example of nested loops where the outer-loop iterates L times and the inner-loop sequentially accesses an array of N 4-byte integers. - 4 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh a) Cache b) Small Array c) Large Array A [0] A [2] A <ref> [4] </ref> A [1] . . . . . A [8] A [10] . . Sequentially accessing an array that fits in cache (Figure 1b) should produce M cache misses, where M is the number of cache blocks required to hold the array. <p> allocators (e.g., some versions of malloc ()) return cache-block aligned memory. - 8 - /* old declaration of a twelve */ /* byte structure */ struct ex_struct - int val1,val2,val3; /* new declaration of structure */ /* padded to 16-byte block size */ struct ex_struct - int val1,val2,val3; char pad <ref> [4] </ref>; -; Example 3. <p> However full simulation also permits a profiler to identify which data structures are responsible for cache misses and to determine the type of miss, features provided by both MemSpy <ref> [4] </ref> and CPROF. MemSpy [4] is very similar to CPROF, the difference being the granularity at which source code is annotated and the miss type classification. MemSpy annotates source code at the procedure level and provides only two miss types for uniprocessors: compulsory and replacement. <p> However full simulation also permits a profiler to identify which data structures are responsible for cache misses and to determine the type of miss, features provided by both MemSpy <ref> [4] </ref> and CPROF. MemSpy [4] is very similar to CPROF, the difference being the granularity at which source code is annotated and the miss type classification. MemSpy annotates source code at the procedure level and provides only two miss types for uniprocessors: compulsory and replacement. <p> - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh | | | | | | | | | | | | | | | | | | | | | | | | | Speedup btrix cholesky compress dnasa7 eqntott gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key <ref> [4] </ref> key [5] key [6] key [7] i + 1 val [0] val [1] val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial
Reference: 5. <author> M. D. Hill and A. J. </author> <title> Smith , Evaluating Associativity in CPU Caches, </title> <journal> IEEE Transactions on Computers 38(12) pp. </journal> <month> 1612-1630 (December </month> <year> 1989). </year>
Reference-contexts: To select appropriate program transformations, a programmer must first obtain insight into the cause of poor cache behavior. One approach to understanding the cause of cache misses, is to classify each miss into one of three - 5 - disjoint types <ref> [5] </ref>: compulsory, capacity, conflict. 1 A compulsory miss is caused by referencing a previously unrefer--enced cache block. In the small array example above (see Figure 1b), all misses are compulsory. <p> | | | | | | | | | | | | | | | | | | | | | | | | | Speedup btrix cholesky compress dnasa7 eqntott gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key <ref> [5] </ref> key [6] key [7] i + 1 val [0] val [1] val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy
Reference: 6. <author> R. E. Kessler and Mark D. Hill, </author> <title> Page Placement Algorithms for Large Real-Index Caches, </title> <journal> ACM Trans. on Computer Systems 10(4) pp. </journal> <month> 338-359 (November </month> <year> 1992). </year>
Reference-contexts: However, many operating systems use page coloring to minimize this effect, thus reducing the performance difference between virtual-indexed and real-indexed caches <ref> [6] </ref>. Techniques for Improving Cache Behavior The analysis techniques described in the previous section can help a programmer understand the cause of cache misses. In this section, we present a cookbook of simple program transformations that can help eliminate some of the misses. <p> | | | | | | | | | | | | | | | | | | | | | | | Speedup btrix cholesky compress dnasa7 eqntott gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key <ref> [6] </ref> key [7] i + 1 val [0] val [1] val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the <p> dnasa7 eqntott gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key <ref> [6] </ref> key [7] i + 1 val [0] val [1] val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the key and value arrays (Figure 5a) resulted in as many as two cache misses for each successful hash table probe. <p> gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key <ref> [6] </ref> key [7] i + 1 val [0] val [1] val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the key and value arrays (Figure 5a) resulted in as many as two cache misses for each successful hash table probe.
Reference: 7. <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf, </author> <title> The Cache Performance and Optimizations of Blocked Algorithms, </title> <booktitle> Proceedings ASPLOS IV, </booktitle> <pages> pp. </pages> <month> 63-74 (April </month> <year> 1991). </year>
Reference-contexts: Someday compilers may automate this analysis and transform the code to reduce the miss frequency; recent research has produced promising results for restricted problem domains <ref> [7, 10] </ref>. However, for general codes using current commercial compilers, the programmer must manually analyze the programs and perform transformations by hand. To select appropriate program transformations, a programmer must first obtain insight into the cause of poor cache behavior. <p> The first three techniques change the allocation of data structures, whereas loop interchange modifies the order that data structures are referenced. Capacity misses can be eliminated by program transformations that reuse data before it is displaced from the cache, such as loop fusion, blocking <ref> [7, 10] </ref>, structure and array packing, and loop interchange. In the following sections we present examples of each of these techniques, except loop interchange which was dis - 7 - cussed in the introduction. <p> | | | | | | | | | | | | | | | | | | | | | Speedup btrix cholesky compress dnasa7 eqntott gmtry mxm tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key [6] key <ref> [7] </ref> i + 1 val [0] val [1] val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the key and <p> tomcatv spice vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key [6] key <ref> [7] </ref> i + 1 val [0] val [1] val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the key and value arrays (Figure 5a) resulted in as many as two cache misses for each successful hash table probe. <p> vpenta xlisp 1.56 2.04 2.53 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 17 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Cache Set key [4] key [5] key [6] key <ref> [7] </ref> i + 1 val [0] val [1] val [2] val [3] j i + 2 key [0] val [0] key [1] val [1] key [6] val [6] key [7] val [7] i + 3 a.) Original Cache Mapping b.) New Cache Mapping The initial allocation strategy for the key and value arrays (Figure 5a) resulted in as many as two cache misses for each successful hash table probe. <p> Statically transposing the array (effectively performing loop interchange but with much simpler code modification) results in speedups of 1.32 on the DECstation 5000/240, 1.16 on the 5000/125 and 1.13 on the 5000/200. Blocking can also be applied to cholesky <ref> [7] </ref>, but we chose to apply a much simpler transformation. btrix btrix is a tri-diagonal solver. CPROF shows that most of the misses are again capacity misses that occur in two nested loops.
Reference: 8. <author> SPEC Newsletter, </author> , <title> Standard Performance Evaluation Corporation, </title> <address> Fairfax, VA (December 1991). SPEC c/o NCGA, Suite 200, 2722 Merilee Drive, Fairfax, VA 22031 </address>
Reference-contexts: Unfortunately, caches only work well for programs that exhibit sufficient locality. Other programs have reference patterns that caches cannot exploit, and spend much of their execution time transferring data between main memory and cache. For example, the SPEC92 <ref> [8] </ref> benchmark tomcatv spends 53% of its time waiting for memory on a DECstation 5000/125. Fortunately, for many programs small changes in the source code can radically alter their memory reference pattern, greatly improving cache performance. Consider the well-known example of traversing a two-dimensional FORTRAN array. <p> A <ref> [8] </ref> A [10] . . Sequentially accessing an array that fits in cache (Figure 1b) should produce M cache misses, where M is the number of cache blocks required to hold the array.
Reference: 9. <author> D. N. Pnevmatikatos and M. D. Hill, </author> <title> Cache Performance of the Integer SPEC Benchmarks on a RISC, </title> <journal> ACM SIGARCH Computer Architecture News 18(2) pp. </journal> <month> 53-68 (June </month> <year> 1990). </year>
Reference-contexts: Execution-time profiling shows that eqntott spends 95% of its time in the quick-sort routine <ref> [9] </ref>. CPROF further reveals that most of this time is spent moving the sort keys from memory into the cache; over 90% of the misses are generated in one comparison routine. <p> Explicit padding increases the allocated size from the 12 bytes required by C language semantics to 16 bytes; a 33% increase in storage. This increase could adversely affect virtual memory performance for larger programs, although this was not a problem with this input <ref> [9] </ref>. tomcatv tomcatv is a FORTRAN 77 mesh generation program that uses seven two-dimensional data arrays, each of which requires approximately 0.5 M-Byte.
Reference: 10. <author> A. Porterfield, </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications, </title> <type> Ph. D. Thesis, </type> <institution> Dept of Computer Science, Rice University, </institution> <year> (1989). </year>
Reference-contexts: A [8] A <ref> [10] </ref> . . Sequentially accessing an array that fits in cache (Figure 1b) should produce M cache misses, where M is the number of cache blocks required to hold the array. <p> Someday compilers may automate this analysis and transform the code to reduce the miss frequency; recent research has produced promising results for restricted problem domains <ref> [7, 10] </ref>. However, for general codes using current commercial compilers, the programmer must manually analyze the programs and perform transformations by hand. To select appropriate program transformations, a programmer must first obtain insight into the cause of poor cache behavior. <p> In the small array example above (see Figure 1b), all misses are compulsory. Eliminating a compulsory miss requires prefetching the data, either by an explicit prefetch operation <ref> [10] </ref> or by placing more data items in a single cache block. For example, if the integers in our example require only 2 bytes rather than 4, we can cut the misses in half by changing the declaration. <p> The first three techniques change the allocation of data structures, whereas loop interchange modifies the order that data structures are referenced. Capacity misses can be eliminated by program transformations that reuse data before it is displaced from the cache, such as loop fusion, blocking <ref> [7, 10] </ref>, structure and array packing, and loop interchange. In the following sections we present examples of each of these techniques, except loop interchange which was dis - 7 - cussed in the introduction.
Reference: 11. <author> A. J. Smith, </author> <title> Cache Memories, </title> <journal> ACM Computing Surveys 14(3) pp. </journal> <month> 473-530 (Sept. </month> <year> 1982). </year>
Reference-contexts: If a block can reside in any location in the cache (A =C/B) we call it a fully-associative cache; if it can reside in exactly one location (A =1) we call it direct-mapped; if it can reside in exactly A locations, we call it A-way set-associative. (Smith's survey <ref> [11] </ref> provides a more detailed description of cache design.) With these three parameters, a programmer can analyze the first-order cache behavior for simple algorithms.
Reference: 12. <author> B. Zorn and P. N. Hilfinger, </author> <title> A Memory Allocation Profiler for C and Lisp, </title> <booktitle> Proceedings of the Summer 1988 USENIX Conference, </booktitle> <month> (June </month> <year> 1988). </year> <month> - 27 </month> - 
Reference-contexts: CPROF annotates both static and dynamic data structures. Dynamically allocated structures are labeled by concatenating the procedure names on the call stack at the point of allocation <ref> [12] </ref>. An appended counter value allows unique identification of all dynamically allocated structures. The text window is used to view individual source files, where each line is annotated with the corresponding number of cache misses.
References-found: 12

