URL: http://www.cse.psu.edu/~zha/papers/lowrank.ps
Refering-URL: http://www.cse.psu.edu/~zha/papers.html
Root-URL: http://www.cse.psu.edu
Title: LOW RANK MATRIX APPROXIMATION USING THE LANCZOS BIDIAGONALIZATION PROCESS WITH APPLICATIONS  
Author: HORST D. SIMON AND HONGYUAN ZHA 
Abstract: Low rank approximation of large and/or sparse matrices is important in many applications. We show that good low rank matrix approximations can be directly obtained from the Lanczos bidiagonalization process without computing singular value decomposition. We also demonstrate that a so-called one-sided reorthogonalization process can be used to maintain adequate level of orthogonality among the Lanczos vectors and produce accurate low rank approximations. This technique reduces the computational cost of the Lanczos bidiagonalization process. We illustrate the efficiency and applicability of our algorithm using numerical examples from several applications areas. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H.C. Andrews and B.R. Hunt. </author> <title> Digital Image estoration, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1977. </year>
Reference: [2] <author> H.C. Andrews and C.L. Patterson. </author> <title> The singular value decomposition (SVD) image coding. </title> <journal> IEEE Transactions on Communications, </journal> <volume> COM-24:425-432, </volume> <year> 1976. </year>
Reference-contexts: If the range of the quantities to be plotted is too large, we will plot them in log-scale. Example 1. A detailed description of using singular value decomposition for single 2-D image compression/coding is given in <ref> [2] </ref>. We have tested Algorithm One-sided on many 2-D image arrays which are digitized images of everyday-life photos. The results are very similar and therefore we only present one test result.
Reference: [3] <author> M. Berry. </author> <title> Large Scale Singular Value Computations. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 6 </volume> <pages> 13-49, </pages> <year> 1992. </year>
Reference-contexts: In this paper, we explore one possible avenue of applying the Lanczos bidiagonalization process for finding approximations of A j . Lanczos bidiagonalization process has been used for computing a few dominant singular triplets (singular values and the corresponding left and right singular vectors) of large sparse matrices <ref> [3, 5, 8] </ref>. We will show that in many cases of interest good approximations can be directly obtained from the Lanczos bidiagonalization process without computing any singular value decomposition. <p> The Lanczos bidiagonalization process. Bidiagonalization of a general rectangular matrix using orthogonal transformations such as Householder transformations and Givens rotations was first proposed in [7]. It was later adapted to solving large sparse least squares problems [18] and to finding a few dominant singular triplets of large sparse matrices <ref> [3, 5, 8] </ref>. <p> The singular values of B k are then used as approximations of the singular values of A and the singular vectors of B k are combined with the left and right Lanczos vectors fU k g and fV k g to form approximations of the singular vectors of A <ref> [5, 3] </ref>. <p> Notice that the part of computation for the SVD of B k and the combination of the singular vectors and Lanczos vectors have to be done after the Lanczos bidiagonalization process. In <ref> [3] </ref> the computation of the SVD of B k along on a Cray-2S accounts for 12% to 34% of the total CPU time for a 5831fi1033 matrix with k = 100 depending on whether A T A or the 2-cyclic matrix [0 A; A' 0] is used.
Reference: [4] <institution> Cornell SMART System, ftp://ftp.cs.cornell.edu/pub/smart. </institution>
Reference-contexts: As a concrete example, we show various timings for a term-document matrix of size 4322 fi 11429 with 224918 nonzeros generated from the document collection NPL <ref> [4] </ref>. Let the SVD of B k be B k = U B k B k ) T . Then U k U B k and V k V B k give the left and right singular vectors of A, respectively. <p> We have tested five classes of matrices and compared the low rank approximations computed by Algorithm one-sided with those computed by the SVD. * 2-D images. * Ill-conditioned test matrices from Regularization Tools [21]. * Large sparse test matrices from SVDPACK [25] and document collections <ref> [4] </ref>. * Several general rectangular matrix from Matrix Market [14]. 5 Sometimes a second orthogonalization is needed to achieve orthonormality within working preci sion [19, Section 6.9]. <p> In this example we tested two data collections: a 3681 fi 1033 term-document matrix from data collection consisting of abstracts in biomedicine with 30 queries, and a 4322 fi 11529 term-document matrix from the NPL collection with 93 queries <ref> [4] </ref>. We used 11-point average precision, a standard measure for comparing information retrieval systems [12]. For k = 10; 20; : : : ; 150, two sequences are plotted in the left of and the precision for A 110 and J 110 are 65:50% and 63:21%, respectively.
Reference: [5] <author> J. Cullum, R. A. Willoughby and M. </author> <title> Lake. A Lanczos Algorithm for Computing Singular Values and Vectors of Large Matrices. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 4 </volume> <pages> 197-215, </pages> <year> 1983. </year>
Reference-contexts: In this paper, we explore one possible avenue of applying the Lanczos bidiagonalization process for finding approximations of A j . Lanczos bidiagonalization process has been used for computing a few dominant singular triplets (singular values and the corresponding left and right singular vectors) of large sparse matrices <ref> [3, 5, 8] </ref>. We will show that in many cases of interest good approximations can be directly obtained from the Lanczos bidiagonalization process without computing any singular value decomposition. <p> The Lanczos bidiagonalization process. Bidiagonalization of a general rectangular matrix using orthogonal transformations such as Householder transformations and Givens rotations was first proposed in [7]. It was later adapted to solving large sparse least squares problems [18] and to finding a few dominant singular triplets of large sparse matrices <ref> [3, 5, 8] </ref>. <p> no reorthogonalization is incorporated in the proposed algorithm LSQR [18]. 1 For computing a few dominant singular triplets, one approach is to completely ignore the issue of loss of orthogonality during the Lanczos bidiagonalization process and later on to identify those spurious singular values thus generated from the true ones <ref> [5] </ref>. We will not pursue this approach since spurious singular values will cause considerable complication in forming approximations of A j discussed in the previous section. We opt to use the approach that will maintain certain level of orthogonality among the Lanczos vectors [19, 22, 23, 24]. <p> In Section 4 we will discuss two other more efficient reorthogonalization schemes using the coupled two term recurrence. Now we briefly describe the Lanczos bidiagonalization process presented in <ref> [7, 18, 5] </ref>. <p> Remark. There is another version of the Lanczos bidiagonalization recurrence <ref> [7, 5] </ref>, ff 1 v 1 = b; fi 1 u 1 = Av 1 ; fi i+1 u i+1 = Av i+1 ff i+1 v i : For A with more columns than rows, this version is usually better than (2.2) because the chances of introducing spurious zero singular values <p> fi 1 u 1 = Av 1 ; fi i+1 u i+1 = Av i+1 ff i+1 v i : For A with more columns than rows, this version is usually better than (2.2) because the chances of introducing spurious zero singular values arising from m 6= n is reduced <ref> [7, 5] </ref>. <p> The singular values of B k are then used as approximations of the singular values of A and the singular vectors of B k are combined with the left and right Lanczos vectors fU k g and fV k g to form approximations of the singular vectors of A <ref> [5, 3] </ref>.
Reference: [6] <author> P. Geladi and H. Grahn. </author> <title> Multivariate image analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: In some applications such as compression of multiple-spectral and hyper-spectral image cubes <ref> [6, 13] </ref>, principal component analysis for face recognition and image databases [27, 16, 26, 20], each column of the matrix A represents a single image acquired at a specific wavelength (channel) or a facial image of a particular individual: the columns of the 2-D image array is stacked into a single
Reference: [7] <author> G. H. Golub and W. Kahan. </author> <title> Calculating the Singular Values and Pseudo-Inverse of a Matrix. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 2 </volume> <pages> 205-224, </pages> <year> 1965. </year>
Reference-contexts: Section 6 concludes the paper and points out some directions for future investigation. 2. The Lanczos bidiagonalization process. Bidiagonalization of a general rectangular matrix using orthogonal transformations such as Householder transformations and Givens rotations was first proposed in <ref> [7] </ref>. It was later adapted to solving large sparse least squares problems [18] and to finding a few dominant singular triplets of large sparse matrices [3, 5, 8]. <p> In Section 4 we will discuss two other more efficient reorthogonalization schemes using the coupled two term recurrence. Now we briefly describe the Lanczos bidiagonalization process presented in <ref> [7, 18, 5] </ref>. <p> Remark. There is another version of the Lanczos bidiagonalization recurrence <ref> [7, 5] </ref>, ff 1 v 1 = b; fi 1 u 1 = Av 1 ; fi i+1 u i+1 = Av i+1 ff i+1 v i : For A with more columns than rows, this version is usually better than (2.2) because the chances of introducing spurious zero singular values <p> fi 1 u 1 = Av 1 ; fi i+1 u i+1 = Av i+1 ff i+1 v i : For A with more columns than rows, this version is usually better than (2.2) because the chances of introducing spurious zero singular values arising from m 6= n is reduced <ref> [7, 5] </ref>.
Reference: [8] <author> G. H. Golub, F. Luk and M. Overton. </author> <title> A Block Lanczos Method for Computing the Singular Values and Corresponding Singular Vectors of a Matrix. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7 </volume> <pages> 149-169, </pages> <year> 1981. </year>
Reference-contexts: In this paper, we explore one possible avenue of applying the Lanczos bidiagonalization process for finding approximations of A j . Lanczos bidiagonalization process has been used for computing a few dominant singular triplets (singular values and the corresponding left and right singular vectors) of large sparse matrices <ref> [3, 5, 8] </ref>. We will show that in many cases of interest good approximations can be directly obtained from the Lanczos bidiagonalization process without computing any singular value decomposition. <p> The Lanczos bidiagonalization process. Bidiagonalization of a general rectangular matrix using orthogonal transformations such as Householder transformations and Givens rotations was first proposed in [7]. It was later adapted to solving large sparse least squares problems [18] and to finding a few dominant singular triplets of large sparse matrices <ref> [3, 5, 8] </ref>.
Reference: [9] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: Often A is a general rectangular matrix and sometimes either m AE n or m o n. The theory of singular value decomposition (SVD) provides the following characterization of the best rank-j approximation of A in terms of the Frobenius norm k k F <ref> [9] </ref>. Theorem 1.1. Let the singular value decomposition of A 2 R mfin be A = P Q T with = diag (oe 1 ; : : : ; oe min (m;n) ), oe 1 : : : oe min (m;n) , and P and Q orthogonal.
Reference: [10] <author> P. C. Hansen. </author> <title> Truncated SVD Solutions to Discrete Ill-Posed Problems with Ill-Determined Numerical Rank. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 503-518, </pages> <year> 1990. </year>
Reference-contexts: Example 2. Truncated SVD is a very useful tool for solving ill-posed problems. The first step involved is to find a low rank approximation A k of the original matrix A computed from the SVD of A <ref> [10, 11] </ref>. This step can be replaced by using J k = U k B k V T k instead, and this will be especially efficient when the matrix A is large and sparse. The MATLAB regularization tools developed by P.C.
Reference: [11] <author> P. C. Hansen. </author> <title> Rgularization Tools: A Matlab package for analysis and solution of discrete ill-posed problems, </title> <booktitle> Numerical Algorithms, </booktitle> <volume> 6 </volume> <pages> 1-35, </pages> <year> 1994. </year>
Reference-contexts: Hansen's Regularization toolbox <ref> [21, 11] </ref>. The first matrix is phillips (100), a square matrix of dimension 100. Its singular values are plotted in Figure 5 in Section 5. <p> Example 2. Truncated SVD is a very useful tool for solving ill-posed problems. The first step involved is to find a low rank approximation A k of the original matrix A computed from the SVD of A <ref> [10, 11] </ref>. This step can be replaced by using J k = U k B k V T k instead, and this will be especially efficient when the matrix A is large and sparse. The MATLAB regularization tools developed by P.C.
Reference: [12] <author> D. Harman. </author> <note> TREC-3 Conference Report. NIST Special Publication 500-225, </note> <year> 1995. </year>
Reference-contexts: We used 11-point average precision, a standard measure for comparing information retrieval systems <ref> [12] </ref>. For k = 10; 20; : : : ; 150, two sequences are plotted in the left of and the precision for A 110 and J 110 are 65:50% and 63:21%, respectively.
Reference: [13] <author> C. Lee and D.A. Landgrebe. </author> <title> Analyzing high dimensional multispectral data. </title> <journal> IEEE Transactions on Geoscience and Remote Sensing, </journal> <volume> 31 </volume> <pages> 792-800, </pages> <year> 1993. </year>
Reference-contexts: In some applications such as compression of multiple-spectral and hyper-spectral image cubes <ref> [6, 13] </ref>, principal component analysis for face recognition and image databases [27, 16, 26, 20], each column of the matrix A represents a single image acquired at a specific wavelength (channel) or a facial image of a particular individual: the columns of the 2-D image array is stacked into a single
Reference: [14] <author> Matrix Market. </author> <note> http://math.nist.gov/MatrixMarket/. </note>
Reference-contexts: classes of matrices and compared the low rank approximations computed by Algorithm one-sided with those computed by the SVD. * 2-D images. * Ill-conditioned test matrices from Regularization Tools [21]. * Large sparse test matrices from SVDPACK [25] and document collections [4]. * Several general rectangular matrix from Matrix Market <ref> [14] </ref>. 5 Sometimes a second orthogonalization is needed to achieve orthonormality within working preci sion [19, Section 6.9]. <p> The precision for A 550 and J 550 are 23:23% and 21:42%, respectively. Example 4. Matrix Market contains several general rectangular matrices. Of special interests to us is the set LSQ which comes from linear least squares problems in surveying <ref> [14] </ref>. This set contains four matrices all of them are in Harwell-Boeing format. We first convert them into MATLAB's .mat format. The matrix illc1033.mat is of dimension 1033fi320, it is an interesting matrix because it has several clusters of singular values which are very close to each other.
Reference: [15] <author> MultiSpec. </author> <note> http://dynamo.ecn.purdue.edu/ biehl/MultiSpec/documentation.html. </note>
Reference-contexts: Example 5. This test matrix is obtained by converting a 220-band image cube taken from the homepage of MultiSpec, a software package for analyzing multispectral and hyperspectral image data developed at Purdue University <ref> [15] </ref>. The data values are proportional to radiance units.
Reference: [16] <author> A. O'Toole, H. Abdi, K.A. Deffenbacher and D. Valentin. </author> <title> Low-dimensional representation of faces in higher dimensions of the face space. </title> <journal> Journal of American Optical Society, </journal> <volume> 10 </volume> <pages> 405-411, </pages> <year> 1993. </year>
Reference-contexts: In some applications such as compression of multiple-spectral and hyper-spectral image cubes [6, 13], principal component analysis for face recognition and image databases <ref> [27, 16, 26, 20] </ref>, each column of the matrix A represents a single image acquired at a specific wavelength (channel) or a facial image of a particular individual: the columns of the 2-D image array is stacked into a single long vector. 3 For a 512 fi 512 2-D image the
Reference: [17] <author> C. C. Paige. </author> <title> Error analysis of the Lanczos algorithm for tridiagonalizing a symmetric matrix. </title> <journal> Journal of Institute of Mathematics and Applications, </journal> <volume> 18: </volume> <pages> 341-349, </pages> <year> 1976. </year>
Reference-contexts: In another word, we may simply apply (2.2) to A T . Therefore in what follows we will deal exclusively with recurrence (2.2). When the need arises we will simply apply recurrence (2.2) to A T . Following the error analysis in <ref> [17] </ref>, it is straightforward to show that in finite precision arithmetic, Equations (2.1) and (2.2) become fi 1 u 1 = b; ff 1 v 1 = A T u 1 + g 1 ; ff i+1 v i+1 = A T u i+1 fi i+1 v i g i+1 ;
Reference: [18] <author> C. C. Paige and M. A. Saunders. </author> <title> LSQR: an Algorithm for Sparse Linear Equations and Sparse Least Squares. </title> <journal> ACM Transaction on Mathematical Software, </journal> <volume> 8 </volume> <pages> 43-71, </pages> <year> 1982. </year>
Reference-contexts: The Lanczos bidiagonalization process. Bidiagonalization of a general rectangular matrix using orthogonal transformations such as Householder transformations and Givens rotations was first proposed in [7]. It was later adapted to solving large sparse least squares problems <ref> [18] </ref> and to finding a few dominant singular triplets of large sparse matrices [3, 5, 8]. For solving least squares problems the orthogonality of the left and right Lanczos vectors is usually not a concern and therefore no reorthogonalization is incorporated in the proposed algorithm LSQR [18]. 1 For computing a <p> sparse least squares problems <ref> [18] </ref> and to finding a few dominant singular triplets of large sparse matrices [3, 5, 8]. For solving least squares problems the orthogonality of the left and right Lanczos vectors is usually not a concern and therefore no reorthogonalization is incorporated in the proposed algorithm LSQR [18]. 1 For computing a few dominant singular triplets, one approach is to completely ignore the issue of loss of orthogonality during the Lanczos bidiagonalization process and later on to identify those spurious singular values thus generated from the true ones [5]. <p> In Section 4 we will discuss two other more efficient reorthogonalization schemes using the coupled two term recurrence. Now we briefly describe the Lanczos bidiagonalization process presented in <ref> [7, 18, 5] </ref>.
Reference: [19] <author> B.N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1980. </year>
Reference-contexts: We will not pursue this approach since spurious singular values will cause considerable complication in forming approximations of A j discussed in the previous section. We opt to use the approach that will maintain certain level of orthogonality among the Lanczos vectors <ref> [19, 22, 23, 24] </ref>. Even within this approach there exist several variations depending on how reorthogonalization is implemented. <p> In this section we will assess the error of using J k as an approximation of A. We will also discuss ways to compute ! k recursively in finite precision arithmetic. Many a priori error bounds have been derived for the Ritz values/vectors computed by the Lanczos tridiagonalization process <ref> [19] </ref>. It turns out that our problem of estimating ! k a priori is rather straightforward. It all boils down to how well a singular vector can be approximated from a Krylov subspace. <p> It all boils down to how well a singular vector can be approximated from a Krylov subspace. To proceed we need a result concerning the approximation of an eigenvector of a symmetric matrix from a Krylov subspace <ref> [19, Section 12.4] </ref>. Lemma 3.1. Let C 2 R nfin be symmetric and f an arbitrary vector. <p> For those dominant singular values that have converged, the corresponding fi ik will be small <ref> [19, Section 13.6] </ref>. <p> This is in sharp contrast to the case in tridiagonalizing a symmetric matrix <ref> [19, Equation (13.4.4)] </ref>. However, the assumption is usually not a severe restriction especially when semi-reorthogonality is maintained (cf. Section 4). LOW RANK MATRIX APPROXIMATION 8 I V T k V k , then columns of these matrices, and finally each individual elements of these matrices. <p> As is in the Lanczos tridiagonalization process, maintaining orthogonality of both fU k g and fV k g to full machine precision is not necessary. What is needed is the so-called semiorthogonality among the left and right Lanczos vectors <ref> [19, 22] </ref>, i.e., carrying out reorthogonalization so that j (U k ) = O ( p p * M ) are maintained throughout the bidiagonalization process. <p> The recurrence (4.10) is used to monitor the level of orthogonality of the left and right Lanczos vectors. Since the f i and g k represent local rounding errors and are therefore not known, they are replaced by terms that simulate the rounding error process. Following <ref> [19, 22] </ref>, we replace (4.10) by the following, * l = * M m; * r = * M n ! i+1;k = (ff k ffi i;k + fi k ffi i;k1 ff i ! ik + * l )=fi i+1 (4.15) where k = 1; : : : ; i <p> computed by the SVD. * 2-D images. * Ill-conditioned test matrices from Regularization Tools [21]. * Large sparse test matrices from SVDPACK [25] and document collections [4]. * Several general rectangular matrix from Matrix Market [14]. 5 Sometimes a second orthogonalization is needed to achieve orthonormality within working preci sion <ref> [19, Section 6.9] </ref>.
Reference: [20] <author> A. Petland, R.W. Picard and S. Sclaroff. Photobook: </author> <title> content-based manipulation of image databases. </title> <journal> International Journal of computer vision, </journal> <volume> 18 </volume> <pages> 233-254, </pages> <year> 1996. </year>
Reference-contexts: In some applications such as compression of multiple-spectral and hyper-spectral image cubes [6, 13], principal component analysis for face recognition and image databases <ref> [27, 16, 26, 20] </ref>, each column of the matrix A represents a single image acquired at a specific wavelength (channel) or a facial image of a particular individual: the columns of the 2-D image array is stacked into a single long vector. 3 For a 512 fi 512 2-D image the
Reference: [21] <institution> Regularization Tools. ftp://ftp.mathworks.com/pub/contrib/v4/linalg/regtools/. </institution>
Reference-contexts: Hansen's Regularization toolbox <ref> [21, 11] </ref>. The first matrix is phillips (100), a square matrix of dimension 100. Its singular values are plotted in Figure 5 in Section 5. <p> We have tested five classes of matrices and compared the low rank approximations computed by Algorithm one-sided with those computed by the SVD. * 2-D images. * Ill-conditioned test matrices from Regularization Tools <ref> [21] </ref>. * Large sparse test matrices from SVDPACK [25] and document collections [4]. * Several general rectangular matrix from Matrix Market [14]. 5 Sometimes a second orthogonalization is needed to achieve orthonormality within working preci sion [19, Section 6.9]. <p> The MATLAB regularization tools developed by P.C. Hansen contain eleven m-files for generating test matrices which are very ill-conditioned for testing regularization algorithms <ref> [21] </ref>. We have tested all the eleven classes of matrices with dimension 100 fi 100. 6 The singular value spectrum of the test matrices of some of the classes are rather similar and therefore we did not repeat their results here. The five classes selected are * foxgood 6 In [21] <p> <ref> [21] </ref>. We have tested all the eleven classes of matrices with dimension 100 fi 100. 6 The singular value spectrum of the test matrices of some of the classes are rather similar and therefore we did not repeat their results here. The five classes selected are * foxgood 6 In [21] the dimension of the test matrices is an input argument and therefore can be set at will. All the test matrices we used are of dimension 100 fi 100 except parallax (100) which is 100 fi 23 since only 23 observations are available.
Reference: [22] <author> H. D. Simon. </author> <title> The Lanczos algorithm for solving symmetric linear systems. </title> <type> Ph.D. Dissertation, </type> <institution> Dept. of Mathematics, University of California, Berkeley, </institution> <year> 1982. </year>
Reference-contexts: We will not pursue this approach since spurious singular values will cause considerable complication in forming approximations of A j discussed in the previous section. We opt to use the approach that will maintain certain level of orthogonality among the Lanczos vectors <ref> [19, 22, 23, 24] </ref>. Even within this approach there exist several variations depending on how reorthogonalization is implemented. <p> above equations can be written as U k+1 (fi 1 e 1 ) = b; A T U k+1 = V k+1 B T (2.2) 1 Maintaining certain level of orthogonality among the Lanczos vectors will accelerate the conver gence at the expense of more computational cost and storage requirement <ref> [22, Section 4] </ref>. <p> The result can be considered as extension of similar result for monitoring the loss of orthogonality in the Lanczos tridiagonalization process for symmetric matrices <ref> [22] </ref>. Proposition 4.3. <p> As is in the Lanczos tridiagonalization process, maintaining orthogonality of both fU k g and fV k g to full machine precision is not necessary. What is needed is the so-called semiorthogonality among the left and right Lanczos vectors <ref> [19, 22] </ref>, i.e., carrying out reorthogonalization so that j (U k ) = O ( p p * M ) are maintained throughout the bidiagonalization process. <p> The recurrence (4.10) is used to monitor the level of orthogonality of the left and right Lanczos vectors. Since the f i and g k represent local rounding errors and are therefore not known, they are replaced by terms that simulate the rounding error process. Following <ref> [19, 22] </ref>, we replace (4.10) by the following, * l = * M m; * r = * M n ! i+1;k = (ff k ffi i;k + fi k ffi i;k1 ff i ! ik + * l )=fi i+1 (4.15) where k = 1; : : : ; i
Reference: [23] <author> H. D. Simon. </author> <title> Analysis for the Symmetric Lanczos Algorithm with Reorthogonalization. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 61 </volume> <pages> 101-131, </pages> <year> 1984. </year>
Reference-contexts: We will not pursue this approach since spurious singular values will cause considerable complication in forming approximations of A j discussed in the previous section. We opt to use the approach that will maintain certain level of orthogonality among the Lanczos vectors <ref> [19, 22, 23, 24] </ref>. Even within this approach there exist several variations depending on how reorthogonalization is implemented.
Reference: [24] <author> H. D. Simon. </author> <title> The Lanczos Algorithm with Partial Reorthogonalization, </title> <journal> Mathematics of Computation, </journal> <volume> 42 </volume> <pages> 115-142, </pages> <year> 1984. </year> <title> LOW RANK MATRIX APPROXIMATION 24 </title>
Reference-contexts: We will not pursue this approach since spurious singular values will cause considerable complication in forming approximations of A j discussed in the previous section. We opt to use the approach that will maintain certain level of orthogonality among the Lanczos vectors <ref> [19, 22, 23, 24] </ref>. Even within this approach there exist several variations depending on how reorthogonalization is implemented.
Reference: [25] <author> SVDPACK, </author> <note> http://www.netlib.org/svdpack/index.html. </note>
Reference-contexts: Even within this approach there exist several variations depending on how reorthogonalization is implemented. For example in SVDPACK, a state-of-the-art software package for computing dominant singular triplets of large sparse matrices <ref> [25] </ref>, implementations of Lanczos tridiagonalization process applied to either A T A or the 2-cyclic matrix [0 A; A' 0] with partial re-orthogonalization are provided. Interesting enough, for the coupled two term recurrence that will be detailed in a moment, only a block version with total reorthogonalization is implemented. <p> We first use a test matrix from SVDPACK to illustrate the relation between the levels of orthogonality among columns of U k+1 and V k . The matrix is a term-document matrix from an information retrieval application by Apple Computer Inc. <ref> [25] </ref>. It is sparse and of dimension 3206 fi 44. Its singular values are plotted in Figure 9 in Section 5. We first apply the Lanczos bidiagonalization process to A T . <p> We have tested five classes of matrices and compared the low rank approximations computed by Algorithm one-sided with those computed by the SVD. * 2-D images. * Ill-conditioned test matrices from Regularization Tools [21]. * Large sparse test matrices from SVDPACK <ref> [25] </ref> and document collections [4]. * Several general rectangular matrix from Matrix Market [14]. 5 Sometimes a second orthogonalization is needed to achieve orthonormality within working preci sion [19, Section 6.9]. <p> Another thing we noticed is that when ! k = kA J k k F falls around the level of eps, the monotonicity of f! k g no longer holds, see Figure 4 and Figure 6. Example 3. Three test matrices are included in SVDPACK <ref> [25] </ref>. All of them are in Harwell-Boeing format. We used a utility routine that converts a Harwell-Boeing format to MATLAB's .mat format.
Reference: [26] <author> N.E. Troje and T. Vetter. </author> <title> Representation of human faces. </title> <type> Technical Report No. 41, </type> <institution> Max-Planck-Institute fur biologische Kybernetik, Tubingen, Germany, </institution> <year> 1996. </year>
Reference-contexts: In some applications such as compression of multiple-spectral and hyper-spectral image cubes [6, 13], principal component analysis for face recognition and image databases <ref> [27, 16, 26, 20] </ref>, each column of the matrix A represents a single image acquired at a specific wavelength (channel) or a facial image of a particular individual: the columns of the 2-D image array is stacked into a single long vector. 3 For a 512 fi 512 2-D image the
Reference: [27] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3 </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: In some applications such as compression of multiple-spectral and hyper-spectral image cubes [6, 13], principal component analysis for face recognition and image databases <ref> [27, 16, 26, 20] </ref>, each column of the matrix A represents a single image acquired at a specific wavelength (channel) or a facial image of a particular individual: the columns of the 2-D image array is stacked into a single long vector. 3 For a 512 fi 512 2-D image the <p> In early remote sensing satellite facilities such as Landsat Thematic Mapper, the number of channels is 7 while now channels are numbered in the several hundreds upto 1024. The number of face images used in an image database ranges from several hundred to several thousand <ref> [27] </ref>. Therefore in those applications the matrix A is very skinny, i.e., m AE n. To facilitate the discussion, we will say that those Lanczos vectors with smaller dimension belong to the short space while those with larger dimension belong to the long space.
Reference: [28] <author> G. Xu and T. Kailath. </author> <title> Fast Estimation of Principal Eigenspace Using Lanczos Algorithm. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 15 </volume> <pages> 974-994, </pages> <year> 1994. </year>
Reference-contexts: If we assume that the spectrum of AA T has one cluster [oe 2 n ; oe 2 j+1 ] which is well separated from the rest of the spectrum, a better estimate of tan 6 (p j ; K m ) can be obtained using the techniques in <ref> [28] </ref>. Now we examine the effect of loss of orthogonality among the columns of U k and V k has on the accuracy of J k and give a posterior estimate of kA J k k F .
References-found: 28

