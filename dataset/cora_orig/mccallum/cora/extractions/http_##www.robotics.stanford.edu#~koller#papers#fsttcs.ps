URL: http://www.robotics.stanford.edu/~koller/papers/fsttcs.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/fsttcs.html
Root-URL: http://www.robotics.stanford.edu
Phone: 2  3  4  
Title: Generating Degrees of Belief from Statistical Information: An Overview  
Author: Fahiem Bacchus Adam J. Grove Joseph Y. Halpern Daphne Koller Adam Grove, Joseph Halpern, and Daphne Koller 
Note: The work of Fahiem Bacchus was supported by NSERC under their operating grants program and by IRIS. The work of  was sponsored in part by the Air Force Office of Scientific Research (AFSC), under Contract F49620-91-C-0080. During this work, Grove was at Stanford University, and was supported by an IBM Graduate Fellowship. The United States Government is authorized to reproduce and distribute reprints for governmental purposes.  
Address: Waterloo, Waterloo, Canada, N2L 3G1  4 Independence Way, Princeton, NJ 08540  San Jose, CA 95120  Stanford, CA 94305  
Affiliation: 1 Dept. of Computer Science, University of  NEC Research Institute,  IBM Almaden Research Center,  Dept. of Computer Science, Stanford University,  
Abstract: Consider an agent (or expert system) with a knowledge base KB that includes statistical information (such as "90% of patients with jaundice have hepatitis"), first-order information ("all patients with hepatitis have jaundice"), and default information ("patients with jaundice typically have a fever"). A doctor with such a KB may want to assign a degree of belief to an assertion ' such as "Eric has hepatitis". Since the actions the doctor takes may depend crucially on this degree of belief, we would like to specify a mechanism by which she can use her knowledge base to assign a degree of belief to ' in a principled manner. We have been investigating a number of techniques for doing so; in this paper we give an overview of one of them. The method, which we call the random worlds method, is a natural one: For any given domain size N , we consider the fraction of models satisfying ' among models of size N satisfying KB . If we do not know the domain size N , but know that it is large, we can approximate the degree of belief in ' given KB by taking the limit of this fraction as N goes to infinity. As we show, this approach has many desirable features. In particular, in many cases that arise in practice, the answers we get using this method provably match heuristic assumptions made in many standard AI systems. Consider an agent (or expert system) with a knowledge base KB that includes statistical information (such as "90% of patients with jaundice have hepatitis"), first-order information ("all patients with hepatitis have jaundice"), and default information ("patients with jaundice typically have a fever"). A doctor with such a KB may want to assign a degree of belief to an assertion ' such as "Eric has hepatitis". Since the actions the doctor takes may depend crucially on this degree of belief, we would like to specify a mechanism by which she can use her knowledge base to assign degrees of belief to ' in a principled manner. We have 
Abstract-found: 1
Intro-found: 1
Reference: [Bac90] <author> F. Bacchus. </author> <title> Representing and Reasoning with Probabilistic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: This suggests that the language we intend to use is richer than first-order logic. This is indeed the case. We define a statistical language L , which is a variant of a language designed by Bacchus <ref> [Bac90] </ref>. L augments standard first-order logic with a form of statistical quantifier. For a formula (x), the term jj (x)jj x is a proportion expression. It will be interpreted as a rational number between 0 and 1 that represents the proportion of domain elements satisfying (x). <p> Finally, any rational number is also considered to be a proportion expression, and the set of proportion expressions is closed under addition and multiplication. One important difference between our syntax and that of <ref> [Bac90] </ref> is the use of approximate equality to compare proportion expressions. It is not hard to see that exact comparisons are sometimes inappropriate. Consider a statement such as "90% of birds fly".
Reference: [BGHK92] <author> F. Bacchus, A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> From statistics to belief. </title> <booktitle> In Proc. National Conference on Artificial Intelligence (AAAI '92), </booktitle> <pages> pages 602-608, </pages> <year> 1992. </year>
Reference-contexts: One might also argue that an interval valued degree of belief, [0; 0:6], is appropriate. We are currently investigating other methods for assigning degrees of belief, also based on the principle of indifference, that address these issues. (See <ref> [BGHK92] </ref> for some discussion.) Given all the nice properties of the random-worlds method, it is reasonable to ask how hard it is to compute degrees of belief.
Reference: [BGHK93a] <author> F. Bacchus, A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Generating degrees of belief from statistical information. </title> <note> In preparation, </note> <year> 1993. </year>
Reference-contexts: Intuitively, we use t to interpret approximate equality statements. Thus, (W; t) j= i 0 if the values of and 0 are within t i of each other. For further details of the semantics see <ref> [BGHK93b, BGHK93a] </ref>. Given N and t , we define #worlds t N () to be the number of worlds W over the domain f1; : : : ; N g such that (W; t ) j= . <p> We briefly describe a few of these properties here; a more thorough discussion can be found in <ref> [BGHK93b, BGHK93a] </ref>. First, suppose that given a knowledge base KB , we believe an assertion ' with probability 1. We can interpret this as saying that ' is a default conclusion from KB . Suppose we write KB j ' if Pr w 1 ('jKB ) = 1. <p> Other results in this spirit are also proved in <ref> [BGHK93b, BGHK93a] </ref>. For example, it is shown that we can often ignore seemingly irrelevant information. In particular, even if we do not have exactly the "right" reference class, we can often use the smallest reference class that is applicable.
Reference: [BGHK93b] <author> F. Bacchus, A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Statistical foundations for default reasoning. </title> <booktitle> In Proc. Thirteenth International Joint Conference on Artificial Intelligence (IJCAI '93), </booktitle> <year> 1993. </year>
Reference-contexts: Intuitively, we use t to interpret approximate equality statements. Thus, (W; t) j= i 0 if the values of and 0 are within t i of each other. For further details of the semantics see <ref> [BGHK93b, BGHK93a] </ref>. Given N and t , we define #worlds t N () to be the number of worlds W over the domain f1; : : : ; N g such that (W; t ) j= . <p> We briefly describe a few of these properties here; a more thorough discussion can be found in <ref> [BGHK93b, BGHK93a] </ref>. First, suppose that given a knowledge base KB , we believe an assertion ' with probability 1. We can interpret this as saying that ' is a default conclusion from KB . Suppose we write KB j ' if Pr w 1 ('jKB ) = 1. <p> Other results in this spirit are also proved in <ref> [BGHK93b, BGHK93a] </ref>. For example, it is shown that we can often ignore seemingly irrelevant information. In particular, even if we do not have exactly the "right" reference class, we can often use the smallest reference class that is applicable. <p> These include: It generalizes reasoning paradigms such as probabilistic logic [Nil86], *- semantics [Pea89], and maximum entropy [Jay78]. It provably satisfies many well-known reasoning heuristics, such as preference for specific information, indifference to irrelevant information, and Demp-ster's rule for combining evidence. (See <ref> [BGHK93b] </ref> for more details of these properties, and their relevance to default reasoning.) There is a computational technique for computing degrees of belief with unary KB 's (using maximum entropy). It provides a rich framework, that can handle quantitative, qualitative, and default information.
Reference: [Car50] <author> R. Carnap. </author> <title> Logical Foundations of Probability. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1950. </year>
Reference-contexts: Typically, however, N is known to be large. We therefore approximate the degree of belief for the true, but unknown, N , by computing the value of this degree of belief as N grows large. We note that this method is related to earlier work of [Joh32] and Carnap <ref> [Car50, Car52] </ref>. We said earlier that we expect the knowledge base to contain statistical information and default rules, as well as first-order facts. This suggests that the language we intend to use is richer than first-order logic. This is indeed the case.
Reference: [Car52] <author> R. Carnap. </author> <title> The Continuum of Inductive Methods. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1952. </year>
Reference-contexts: Typically, however, N is known to be large. We therefore approximate the degree of belief for the true, but unknown, N , by computing the value of this degree of belief as N grows large. We note that this method is related to earlier work of [Joh32] and Carnap <ref> [Car50, Car52] </ref>. We said earlier that we expect the knowledge base to contain statistical information and default rules, as well as first-order facts. This suggests that the language we intend to use is richer than first-order logic. This is indeed the case.
Reference: [GHK92a] <author> A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Asymptotic conditional probabilities for first-order logic. </title> <booktitle> In Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 294-305, </pages> <year> 1992. </year>
Reference-contexts: As is shown in <ref> [GHK92a] </ref>, all questions regarding degrees of belief are undecidable in general, even if the KB is purely first-order (and so does not include any statistical information). However, if we assume that KB mentions only unary predicates and constants, then the situation becomes much better.
Reference: [GHK92b] <author> A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Random worlds and maximum entropy. </title> <booktitle> In Proc. 7th IEEE Symp. on Logic in Computer Science, </booktitle> <pages> pages 22-33, </pages> <year> 1992. </year>
Reference-contexts: It is clear that we do not mean that exactly 90% of all birds fly. Among other things, this would imply that the number of birds is a multiple of ten, which is surely not an intended implication. We therefore use the approach described in <ref> [GHK92b, KH92] </ref>, and compare proportion expressions using (instead of = and ) one of an infinite family of connectives i and i , for i = 1; 2; 3 : : : ("i-approximately equal" or "i-approximately less than or equal"). <p> However, if we assume that KB mentions only unary predicates and constants, then the situation becomes much better. Indeed, as shown in <ref> [GHK92b] </ref>, we can typically compute degrees of belief using a maximum entropy computation. Interestingly, it seems that all connection to maximum entropy is lost once our knowledge base contains even a single binary predicate. While the restriction to unary predicates may seem severe, it is not so unreasonable in practice.
Reference: [Hac75] <author> I. Hacking. </author> <title> The Emergence of Probability. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1975. </year>
Reference-contexts: This principle states that all possibilities should be given equal probability, and was regarded as one of the basic principles of probability theory by the earliest workers on probability theory, such as Bernoulli and Laplace <ref> [Hac75] </ref>. We use this idea to assign equal degrees of belief to all basic "situations" consistent with the known facts. The question is, what is a situation? In many applications, including the one of most interest to us, it makes sense to consider finite domains only.
Reference: [Jay78] <author> E. T. Jaynes. </author> <title> Where do we stand on maximum entropy? In R. </title> <editor> D. Levine and M. Tribus, editors, </editor> <booktitle> The Maximum Entropy Formalism, </booktitle> <pages> pages 15-118. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1978. </year>
Reference-contexts: To summarize, we believe that the random-worlds method is a principled| and very powerful|approach for assigning degrees of belief with a number of attractive features. These include: It generalizes reasoning paradigms such as probabilistic logic [Nil86], *- semantics [Pea89], and maximum entropy <ref> [Jay78] </ref>.
Reference: [Joh32] <author> W. E. Johnson. </author> <title> Probability: The deductive and inductive problems. </title> <journal> Mind, </journal> <volume> 41(164) </volume> <pages> 409-423, </pages> <year> 1932. </year>
Reference-contexts: Typically, however, N is known to be large. We therefore approximate the degree of belief for the true, but unknown, N , by computing the value of this degree of belief as N grows large. We note that this method is related to earlier work of <ref> [Joh32] </ref> and Carnap [Car50, Car52]. We said earlier that we expect the knowledge base to contain statistical information and default rules, as well as first-order facts. This suggests that the language we intend to use is richer than first-order logic. This is indeed the case.
Reference: [Key21] <author> J. M. </author> <title> Keynes. A Treatise on Probability. </title> <publisher> Macmillan, </publisher> <address> London, </address> <year> 1921. </year>
Reference-contexts: Our method, which we call the random worlds method, can be viewed as a particular realization of the principle of insufficient reason [Kri86] or the principle of indifference <ref> [Key21] </ref>. This principle states that all possibilities should be given equal probability, and was regarded as one of the basic principles of probability theory by the earliest workers on probability theory, such as Bernoulli and Laplace [Hac75].
Reference: [KH92] <author> D. Koller and J. Y. Halpern. </author> <title> A logic for approximate reasoning. </title> <editor> In B. Nebel, C. Rich, and W. Swartout, editors, </editor> <booktitle> Proc. Third International Conference on Principles of Knowledge Representation and Reasoning (KR '92), </booktitle> <pages> pages 153-164, </pages> <year> 1992. </year>
Reference-contexts: It is clear that we do not mean that exactly 90% of all birds fly. Among other things, this would imply that the number of birds is a multiple of ten, which is surely not an intended implication. We therefore use the approach described in <ref> [GHK92b, KH92] </ref>, and compare proportion expressions using (instead of = and ) one of an infinite family of connectives i and i , for i = 1; 2; 3 : : : ("i-approximately equal" or "i-approximately less than or equal").
Reference: [KLM90] <author> S. Kraus, D. Lehmann, and M. Magidor. </author> <title> Nonmonotonic reasoning, preferential models and cumulative logics. </title> <journal> Artificial Intelligence, </journal> <volume> 44 </volume> <pages> 167-207, </pages> <year> 1990. </year>
Reference-contexts: For example, if KB j= ' then KB j ', so we have degree of belief 1 in all the logical consequences of the knowledge base. In fact, j satisfies the properties postulated by Kraus, Lehmann, and Magidor <ref> [KLM90, Leh89] </ref> to be appropriate for a nonmonotonic consequence relation. As we saw above, the random-worlds method can reason from statistical information (which in the examples above express default rules) to conclusions about particular individuals. This is an important advantage of this framework.
Reference: [Kri86] <author> J. von Kries. </author> <title> Die Principien der Wahrscheinlichkeitsrechnung und Rational Expectation. </title> <type> Freiburg, 1886. </type>
Reference-contexts: In this paper we give an overview of one of them, and give pointers to the literature for the reader interested in further details. Our method, which we call the random worlds method, can be viewed as a particular realization of the principle of insufficient reason <ref> [Kri86] </ref> or the principle of indifference [Key21]. This principle states that all possibilities should be given equal probability, and was regarded as one of the basic principles of probability theory by the earliest workers on probability theory, such as Bernoulli and Laplace [Hac75].
Reference: [Leh89] <author> D. Lehmann. </author> <title> What does a conditional knowledge base entail? In Proc. </title> <booktitle> First International Conference on Principles of Knowledge Representation and Reasoning (KR '89), </booktitle> <pages> pages 212-222, </pages> <year> 1989. </year>
Reference-contexts: For example, if KB j= ' then KB j ', so we have degree of belief 1 in all the logical consequences of the knowledge base. In fact, j satisfies the properties postulated by Kraus, Lehmann, and Magidor <ref> [KLM90, Leh89] </ref> to be appropriate for a nonmonotonic consequence relation. As we saw above, the random-worlds method can reason from statistical information (which in the examples above express default rules) to conclusions about particular individuals. This is an important advantage of this framework.
Reference: [Nil86] <author> N. Nilsson. </author> <title> Probabilistic logic. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 71-87, </pages> <year> 1986. </year>
Reference-contexts: To summarize, we believe that the random-worlds method is a principled| and very powerful|approach for assigning degrees of belief with a number of attractive features. These include: It generalizes reasoning paradigms such as probabilistic logic <ref> [Nil86] </ref>, *- semantics [Pea89], and maximum entropy [Jay78].
Reference: [Pea89] <author> J. Pearl. </author> <title> Probabilistic semantics for nonmonotonic reasoning: A survey. </title> <editor> In R. J. Brachman, H. J. Levesque, and R. Reiter, editors, </editor> <booktitle> Proc. First International Conference on Principles of Knowledge Representation and Reasoning (KR '89), </booktitle> <pages> pages 505-516, </pages> <year> 1989. </year> <note> Reprinted in Readings in Uncertain Reasoning, </note> <editor> G. Shafer and J. Pearl (eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990, </year> <pages> pp. 699-710. </pages>
Reference-contexts: Our formalism gives us a straightforward way to represent such a default, by writing jjFly (x)jBird (x)jj x i 1 for some i. (This interpretation is closely related to other approaches applying probabilistic semantics to nonmonotonic logic; see Pearl <ref> [Pea89] </ref> for an overview. However, all these other approaches are essentially propositional in nature. <p> To summarize, we believe that the random-worlds method is a principled| and very powerful|approach for assigning degrees of belief with a number of attractive features. These include: It generalizes reasoning paradigms such as probabilistic logic [Nil86], *- semantics <ref> [Pea89] </ref>, and maximum entropy [Jay78].
Reference: [RC81] <author> R. Reiter and G. Criscuolo. </author> <title> On interacting defaults. </title> <booktitle> In Proc. Seventh International Joint Conference on Artificial Intelligence (IJCAI '81), </booktitle> <pages> pages 270-276, </pages> <year> 1981. </year>
Reference-contexts: For example, in the well-known Nixon Diamond problem <ref> [RC81] </ref>, we have statistics for the occurrence of pacifists in the class of Republicans, and very different statistics for the occurrence of pacifists within the class of Quakers.
Reference: [Sha76] <author> G. Shafer. </author> <title> A Mathematical Theory of Evidence. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1976. </year>
Reference-contexts: We can show that random worlds essentially treats the two pieces of statistical information as independent pieces of evidence; in fact, the degree of belief resulting obtained by random worlds is essentially equivalent to combining evidence using Dempster's rule of combination <ref> [Sha76] </ref>. We should point out that the random-worlds method does not give the uncontroversially most intuitive answer in every example. Indeed, it is unlikely any method could, since peoples' intuitions often disagree.
References-found: 20

