URL: http://www-psrg.lcs.mit.edu/ftpdir/rweiss/algebraic_video.IEEE-ICMCS-94.ps
Refering-URL: http://www.psrg.lcs.mit.edu/publications.html
Root-URL: 
Title: Content-Based Access to Algebraic Video  
Author: Ron Weiss Andrzej Duda David K. Gifford 
Affiliation: Programming Systems Research Group MIT Laboratory for Computer Science  
Abstract: Algebraic video uses a set of basic operations on video segments to create a desired video stream. The video algebra consists of operations for temporally and spatially combining video segments, and for attaching attributes to these segments. Video streams of interest can be discovered with video queries that describe desired attributes. Unlike previous approaches, algebraic video permits video expressions to be nested in arbitrarily deep hierarchies. It also permits video segments to inherit attributes by context. Experience with a prototype algebraic video system suggests that algebraic video can be used to create new video presentations from existing video libraries with query based discovery and algebraic combination of video segments of interest. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Adobe Systems Incorporated, Mountain View, CA. </institution> <note> Adobe Premiere User Guide, first edition, </note> <year> 1991. </year>
Reference-contexts: Our data model provides a full hierarchical organization of video footage that permits flexible browsing. Unlike simple stratification, the algebraic video model preserves the nested relationships between strata and allows to explore the context in which a stratum appears. Commercially available tools such as Adobe Premiere <ref> [1] </ref>, DiVA VideoShop [6] and MacroMind Director [13] allow the user to create movies using audio and video tracks, and also enables the user to specify special effects during video segment transitions. These commercial systems are based on two distinct paradigms: timelines and scripts. <p> 1 ) - (x 2 ; y 2 ) priority specifies that E 1 will be displayed with priority in the window defined by (x 1 ; y 1 ) as the bottom-left corner, and (x 2 ; y 2 ) as the right-top corner such that x i 2 <ref> [0; 1] </ref> and y i 2 [0; 1] audio audio E 1 channel f orce priority specifies that the audio of E 1 will be output to channel with priority. If f orce is true, override audio specifications of the component expressions. <p> y 2 ) priority specifies that E 1 will be displayed with priority in the window defined by (x 1 ; y 1 ) as the bottom-left corner, and (x 2 ; y 2 ) as the right-top corner such that x i 2 <ref> [0; 1] </ref> and y i 2 [0; 1] audio audio E 1 channel f orce priority specifies that the audio of E 1 will be output to channel with priority. If f orce is true, override audio specifications of the component expressions. <p> C 2 (0,0) - (1,1) 20 P 3 = window C 3 (.7,0) - (1,1) 30 (P 1 k P 3 ) ffi P 2 Concatenation Operations tem, the top-left (x 1 ; y 1 ) and bottom-right (x 2 ; y 2 ) corners, such that x i 2 <ref> [0; 1] </ref> and y i 2 [0; 1]. By default, a video expression is associated with a square that fits in the parent rectangle. Window priorities are used to resolve overlap conflicts of screen display. <p> P 3 = window C 3 (.7,0) - (1,1) 30 (P 1 k P 3 ) ffi P 2 Concatenation Operations tem, the top-left (x 1 ; y 1 ) and bottom-right (x 2 ; y 2 ) corners, such that x i 2 <ref> [0; 1] </ref> and y i 2 [0; 1]. By default, a video expression is associated with a square that fits in the parent rectangle. Window priorities are used to resolve overlap conflicts of screen display. The window operation establishes the video priority of the associated window region with the priority parameter.
Reference: [2] <author> T.G. Aguierre Smith and G. Davenport. </author> <title> The stratification system: A design environment for random access video. </title> <booktitle> In Proc. 3rd International Workshop on Network and Operating System Support for Digital Audio and Video., </booktitle> <address> La Jolla, CA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Video authoring and annotation tools provide facilities for composing and annotating complex video presentations. Davenport et al. <ref> [3, 2] </ref> implemented a video annotation system. It uses the concept of stratification to assign descriptions to video footage, where each stratum refers to a sequence of video frames. The strata may overlap or totally encompass each other. <p> Finally, the user can display the expression associated with a video expression. If the argument is not a node, the operation is simply the identity function. 8 3.3 Nested Stratification Hierarchical relations between the algebraic video nodes allow nested stratification. Davenport et al. <ref> [3, 2] </ref> defines a stratification mechanism, where textual descriptions called strata are associated with possibly overlapping portions of a linear video stream. In our data model, linear strata are just algebraic video nodes.
Reference: [3] <author> T.G. Aguierre Smith and N.C. Pincever. </author> <title> Parsing movies in context. </title> <booktitle> In Proc Summer 1991 Usenix Conference., </booktitle> <pages> pages 157-168, </pages> <address> Nashville, Tennessee, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Video authoring and annotation tools provide facilities for composing and annotating complex video presentations. Davenport et al. <ref> [3, 2] </ref> implemented a video annotation system. It uses the concept of stratification to assign descriptions to video footage, where each stratum refers to a sequence of video frames. The strata may overlap or totally encompass each other. <p> Finally, the user can display the expression associated with a video expression. If the argument is not a node, the operation is simply the identity function. 8 3.3 Nested Stratification Hierarchical relations between the algebraic video nodes allow nested stratification. Davenport et al. <ref> [3, 2] </ref> defines a stratification mechanism, where textual descriptions called strata are associated with possibly overlapping portions of a linear video stream. In our data model, linear strata are just algebraic video nodes.
Reference: [4] <author> A. S. Bruckman. </author> <title> Electronic scrapbook: Towards an intelligent home-video editing system. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Currently, around 2200 iconic primitives can be browsed. However, this user-friendly visual approach to annotation is limited by a fixed vocabulary. Also, it does not exploit textual data such as close-captioned text. Electronic Scrapbook is a system for home-video video annotation and editing <ref> [4] </ref>. The user can attach descriptions to video clips and use a modified form of case-based reasoning to edit and create personalized video stories. The user can query a database of video clips and also filter, sort, or remove overlapping segments from the results.
Reference: [5] <author> M. Davis. </author> <title> Media Streams: An iconic visual language for video annotation. </title> <booktitle> In Proc. IEEE Symposium on Visual Languages, </booktitle> <pages> pages 196-202, </pages> <address> Bergen, Norway, </address> <year> 1993. </year>
Reference-contexts: However, it does not support a fully functional free form annotation mechanism that enables subsequent content-based access. Media Streams is an iconic visual language that enables users to create multi-layered, iconic annotations of video content <ref> [5] </ref>. Icons denoting objects and actions are organized into cascading hierarchies from levels of generality to increasing levels of specificity. Additionally, icons are organized across multiple axes of descriptions such as objects, characters, relative positions, time or transitions.
Reference: [6] <institution> DiVA Corporation, </institution> <address> Cambridge, MA. DiVA VideoShop, </address> <year> 1991. </year>
Reference-contexts: Our data model provides a full hierarchical organization of video footage that permits flexible browsing. Unlike simple stratification, the algebraic video model preserves the nested relationships between strata and allows to explore the context in which a stratum appears. Commercially available tools such as Adobe Premiere [1], DiVA VideoShop <ref> [6] </ref> and MacroMind Director [13] allow the user to create movies using audio and video tracks, and also enables the user to specify special effects during video segment transitions. These commercial systems are based on two distinct paradigms: timelines and scripts.
Reference: [7] <author> E. Fiume, D. Tsichritzis, and L. Dami. </author> <title> A temporal scripting language for object-oriented animation. </title> <booktitle> In Proc. Eurographics 1987, </booktitle> <pages> pages 283-294, </pages> <address> Amsterdam, Netherlands, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: We are investigating other algebraic video composition operations. These include operations that will achieve overlay of video streams, synchronization on events, a general synchronization operator (similar to operators defined by Fiume et al. <ref> [7] </ref>), and non-determinism. 3.1.2 Output Characteristics Because multiple video streams can be scheduled to play at any specific time within one video expression, the playback may require multiple screen displays and audio outputs.
Reference: [8] <author> S. Gibbs, C. Breiteneder, and D. Tsichritzis. </author> <title> Audio/Video databases: An object-oriented approach. </title> <booktitle> In Proc. 9th IEEE Int. Data Engineering Conference, </booktitle> <pages> pages 381-390, </pages> <year> 1993. </year>
Reference-contexts: Swanberg et al. [16, 17] defines an architecture for parsing data semantics from the video stream. It uses a fixed set of video elements (including shot and episode), and is not suitable for free form modeling of the complex relations between video segments. Gibbs et al. <ref> [8] </ref> proposes an object-oriented approach to video databases. An audio/video database can be viewed as a collection of values (audio and video data) and activities (interconnectable components used to process values). Two abstraction mechanisms, temporal composition, and flow composition allow aggregation of values and activities.
Reference: [9] <author> D. K. Gifford, P. Jouvelot, M. A. Sheldon, and J. W. O'Toole. </author> <title> Semantic file systems. </title> <booktitle> In Thirteenth ACM Symposium on Operating Systems Principles. ACM, </booktitle> <month> October </month> <year> 1991. </year> <title> Available as Operating Systems Review Volume 25, Number 5. </title>
Reference-contexts: Our implementation is built on top of two existing subsystems: the VuSystem [18] and the Semantic File System <ref> [9] </ref>. The VuSystem provides an environment for recording, processing and playing video. A set of C++ classes manage basic functions such as synchronizing video streams, displaying in a window, and processing video streams. TCL [14] scripts control C++ classes and offer a programmable user interface that can be customized.
Reference: [10] <author> R. Hamakawa and J. Rekimoto. </author> <title> Object composition and playback models for handling multimedia data. </title> <booktitle> In Proc. First ACM International Conference on Multimedia., </booktitle> <pages> pages 273-281, </pages> <address> Ana-heim, CA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Multimedia authoring systems such as CMIFed [20] have rich structuring primitives for multimedia documents, but fail to address the structure of the video data itself. The video is still treated as unstructured linear stream. Hamakawa and Rekimoto <ref> [10] </ref> propose a multimedia authoring system that supports editing and reuse of multimedia data. Their system is based on a hierarchical and compositional model of multimedia objects. It allows the user to mark objects with a title at a certain point in time.
Reference: [11] <author> T.D.C Little et al. </author> <title> A digital on-demand video service supporting content-based queries. </title> <booktitle> In Proc. First ACM International Conference on Multimedia., </booktitle> <pages> pages 427-436, </pages> <address> Anaheim, California, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The system uses a 3 small, special-purpose taxonomy that can be used in descriptions, but does not exploit the logical structure of video. Content-based access systems provide facilities to discover video segments of interest. Little et al. <ref> [11] </ref> implemented a system that supports content-based retrieval of video footage. They define a specific data schema composed of movie, scene and actor relations with a fixed set of attributes. The system requires manual feature extraction, and then fits these features into the data schema.
Reference: [12] <author> W. E. Mackay and G. Davenport. </author> <title> Virtual video editing in interactive multimedia applications. </title> <journal> Communications of the ACM, </journal> <volume> 32(7), </volume> <month> July </month> <year> 1989. </year>
Reference-contexts: Also, the temporal composition mechanism is essentially equivalent to the timeline paradigm. 3 Design In general, video is composed of different story units such as shots, scenes and sequences arranged according to some logical structure <ref> [12] </ref>. Frames recorded sequentially form a shot. One or several related shots are combined in a scene and a series of related scenes forms a sequence. The logical structure is defined by a screenplay that organizes story units and provides detailed descriptions of scenes and sequences.
Reference: [13] <author> MacroMind. </author> <note> Director Version 2.0, </note> <year> 1990. </year>
Reference-contexts: Unlike simple stratification, the algebraic video model preserves the nested relationships between strata and allows to explore the context in which a stratum appears. Commercially available tools such as Adobe Premiere [1], DiVA VideoShop [6] and MacroMind Director <ref> [13] </ref> allow the user to create movies using audio and video tracks, and also enables the user to specify special effects during video segment transitions. These commercial systems are based on two distinct paradigms: timelines and scripts.
Reference: [14] <author> J.K. Ousterhout. </author> <title> An X11 toolkit based on the Tcl language. </title> <booktitle> In USENIX Association 1991 Winter Conference Proceedings, </booktitle> <pages> pages 105-115, </pages> <address> Dallas, TX, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: The VuSystem provides an environment for recording, processing and playing video. A set of C++ classes manage basic functions such as synchronizing video streams, displaying in a window, and processing video streams. TCL <ref> [14] </ref> scripts control C++ classes and offer a programmable user interface that can be customized. We use the VuSystem for managing raw video data and for its support for TCL programming.
Reference: [15] <author> R. Snodgrass. </author> <title> The temporal query language TQuel. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 12(2) </volume> <pages> 247-298, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: For example, the user can inspect the encompassing video segment by examining the parent nodes. Temporal queries are handled using temporal predicates defined similarly to the definitions by Snod-grass <ref> [15] </ref>. He defined a temporal predicate operator that takes time intervals as arguments and returns a Boolean value. The three temporal predicate operators are: precede, overlap and equal. A temporal predicate is an expression containing the temporal operators.
Reference: [16] <author> D. Swanberg, C.F. Chu, and R. Jain. </author> <title> Architecture of a multimedia information system for content-based retrieval. </title> <booktitle> In Proc. 3rd International Workshop on Network and Operating System Support for Digital Audio and Video., </booktitle> <address> La Jolla, CA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Second, the system is focused on retrieving previously stored information and is not suitable for users that need to create, edit and annotate a personally customized view of the video footage. Moreover, the browser does not support queries based on the temporal ordering of scenes. Swanberg et al. <ref> [16, 17] </ref> defines an architecture for parsing data semantics from the video stream. It uses a fixed set of video elements (including shot and episode), and is not suitable for free form modeling of the complex relations between video segments.
Reference: [17] <author> D. Swanberg, C.F. Chu, and R. Jain. </author> <title> Knowledge guided parsing in video datbases. </title> <booktitle> In IS&/SPIE's Symposium on Electronic Imaging: Science & Technology, </booktitle> <address> San Jose, CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Second, the system is focused on retrieving previously stored information and is not suitable for users that need to create, edit and annotate a personally customized view of the video footage. Moreover, the browser does not support queries based on the temporal ordering of scenes. Swanberg et al. <ref> [16, 17] </ref> defines an architecture for parsing data semantics from the video stream. It uses a fixed set of video elements (including shot and episode), and is not suitable for free form modeling of the complex relations between video segments.
Reference: [18] <author> D. K. Tennenhouse et al. </author> <title> A software-oriented approach to the design of media processing environments. </title> <booktitle> In Proc. IEEE International Conference on Multimedia Computing and Systems., </booktitle> <address> Boston, MA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Our implementation is built on top of two existing subsystems: the VuSystem <ref> [18] </ref> and the Semantic File System [9]. The VuSystem provides an environment for recording, processing and playing video. A set of C++ classes manage basic functions such as synchronizing video streams, displaying in a window, and processing video streams.
Reference: [19] <author> L. Teodosio and W. Bender. </author> <title> Salient video stills: Content and context preserved. </title> <booktitle> In Proc. First ACM International Conference on Multimedia., </booktitle> <pages> pages 39-46, </pages> <address> Anaheim, CA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: It allows textual, as well as non-textual descriptions such as key frames, icons, salient stills <ref> [19] </ref>, and image features like color, texture, and shape. The description operation associates content information with a video expression. The content description of an expression is not fixed by our model. However, for the purposes of this paper and our prototype implementation, a content is a boolean combination of attributes.
Reference: [20] <author> G. van Rossum et al. CMIFed: </author> <title> A presentation environment for portable hypermedia documents. </title> <booktitle> In Proc. First ACM International Conference on Multimedia., </booktitle> <pages> pages 183-188, </pages> <address> Anaheim, CA, </address> <month> August </month> <year> 1993. </year> <month> 12 </month>
Reference-contexts: The toolkits do not take advantage of this distinctive feature. Moreover, the toolkits lack methods for specifying the elaborate logical structure of video data and do not address content-based access. Our approach allows structured, multi-stream composition using video algebra operations and content-based access. Multimedia authoring systems such as CMIFed <ref> [20] </ref> have rich structuring primitives for multimedia documents, but fail to address the structure of the video data itself. The video is still treated as unstructured linear stream. Hamakawa and Rekimoto [10] propose a multimedia authoring system that supports editing and reuse of multimedia data.
References-found: 20

