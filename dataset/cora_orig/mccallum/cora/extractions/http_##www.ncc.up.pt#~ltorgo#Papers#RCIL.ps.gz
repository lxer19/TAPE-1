URL: http://www.ncc.up.pt/~ltorgo/Papers/RCIL.ps.gz
Refering-URL: http://www.ncc.up.pt/~ltorgo/Papers/list_pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email ltorgo@ciup1.ncc.up.pt  
Phone: 4100  Telf. (+351) 2 600 16 72 Ext. 115 Fax (+351) 2 600 3654  
Title: Rule Combination in Inductive Learning  
Author: Luis Torgo LIACC 
Address: 823 2.  PORTUGAL  
Affiliation: R.Campo Alegre,  PORTO  
Abstract: This paper describes the work on methods for combining rules obtained by machine learning systems. Three methods for obtaining the classification of examples with those rules are compared. The advantages and disadvantages of each method are discussed and the results obtained on three real world domains are commented. The methods compared are: selection of the best rule; PROSPECTOR-like probabilistic approximation for rule combination; and MYCIN-like approximation. Results show significant differences between methods indicating that the problemsolving strategy is important for accuracy oflearning systems. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Bergadano, F., Matwin,S., Michalski,R., Zhang,J. </author> : <title> "Measuring Quality of Concept Descriptions", </title> <booktitle> in EWSL88 - European Working Session on Learning, </booktitle> <publisher> Pitman, </publisher> <year> 1988. </year>
Reference-contexts: The weights are proportional to the value of consistency giving thus some degree of flexibility (see [12, 13] for more details). Our formula for the calculation of quality is a heuristic one. Many other possibilities exist for evaluating a composite effect of various rule properties (see for instance <ref> [1] </ref> for a function which also includes simplicity). Let us now come back to the best rule strategy. All rules applicable to a given example form a candidate set. After the candidate set has been formed, the rule with the highest quality value is chosen.
Reference: 2. <editor> Brazdil,P., Gams,M., Sian,S., Torgo,L., Van de Velde,W. </editor> : <booktitle> "Learning in Distributed Systems and Multi-Agent Environments", in Machine Learning - EWSL91 ,European Working Session on Learning, </booktitle> <editor> Kodratoff,Y. (Ed), </editor> <booktitle> Lecture Notes on Artificial Intelligence, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1991. </year>
Reference-contexts: These experiments seem to indicate that the best rule strategy can be a good strategy especially if we take in to account its simplicity when compared to other methods. 5 Relations to other work Recently several people have studied the effects of multiple sources of knowledge (see <ref> [2] </ref> for a survey). All approaches share the problem of conflict resolution which is one of the issues tackled by the two probabilistic approaches examined in this paper. Gams et al., [6] made several experiments with several knowledge bases when classifying new instances.
Reference: 3. <author> Brazdil,P., Torgo,L. </author> : <title> "Knowledge Acquisition via Knowledge Integration", in Current Trends in Knowledge Acquisition, </title> <publisher> IOS Press, </publisher> <year> 1990. </year>
Reference-contexts: When the number of knowledge bases increased the majority strategy was tbetter. These results seem to suggest that if flexible matching were introduced (which would increase the potential number of opinions) the probabilistic combination strategies examined in this paper might perform better. Brazdil and Torgo <ref> [3] </ref> used different learning algorithms to generate several knowledge bases which were combined into one using a kind of best quality strategy.
Reference: 4. <author> Shafer, G. </author> : <title> A Mathematical Theory of Evidence, </title> <publisher> Priceton University Press, Princeton, </publisher> <year> 1976. </year>
Reference-contexts: Some effort could be invested towards the use of a model which does not exhibit the limitations of conditional independence [8] that both certainty factors and degrees of sufficiency and necessity suffer from. Some experiments could be done with Dempster-Shaffer <ref> [4] </ref> theory of evidence and Pearl's belief networks [10]. 7 Conclusions The experiments carried out in this paper suggest that a simple and quite naive best rule strategy performs quite well in comparison with the two other more complex strategies tested.
Reference: 5. <author> Duda,R., Hart,P., Nilsson,N. </author> : <title> "Subjective Bayesian methods for rule-based inference systems", </title> <booktitle> in Proceedings of the AFIPS National Computer Conference, </booktitle> <volume> vol. 47, </volume> <pages> pp. 1075-1082. </pages>
Reference-contexts: Experiments were made on three real world domains. Their goal was to observe if different classification strategies could lead to different results. The following strategies were used : two well known expert systems approaches, MYCIN [11] certainty factors and PROSPECTOR 's <ref> [5] </ref> odds), together with the best rule strategy. The next section describes briefly the inductive system used in the experiments. <p> This assumption does not hold in general. Nevertheless, this approach has been widely used and achieved good practical results. 3.2 Using Degree of Sufficiency and Necessity ( PROSPECTOR ) PROSPECTOR <ref> [5] </ref> can be considered another successful expert system.
Reference: 6. <author> Gams,M., Bohanec,M., Cestnik,B. </author> : <title> "A Schema for Using Multiple Knowledge", </title> <institution> Josef Stefan Institute, </institution> <year> 1991. </year>
Reference-contexts: All approaches share the problem of conflict resolution which is one of the issues tackled by the two probabilistic approaches examined in this paper. Gams et al., <ref> [6] </ref> made several experiments with several knowledge bases when classifying new instances.
Reference: 7. <author> Heckerman,D. </author> <title> :"Probabilistic interpretation for MYCIN 's certainty factors", </title> <booktitle> in Uncertainty in Artificial Intelligence, </booktitle> <editor> Kanal,L. et al.(eds.), </editor> <publisher> North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: ) CF (H,E 2 ) (2) If CF (H,E1) and CF (H,E2) are both less than zero : CF (H,E 1 E 2 ) = CF (H,E 1 )+CF (H,E 2 )+CF (H,E 1 ) CF (H,E 2 ) (3) For the probabilistic definition of CF's we use the following <ref> [7] </ref> : CF (H,E) = l (H,E) l (H,E)-1 if 0 l (H,E) 1 where l (H,E) = P (E | H ) This formalisation is derived from a set of axioms [7] which imply that the rules must be conditionally independent given the hypothesis and its negation [9]. <p> (H,E 1 ) CF (H,E 2 ) (3) For the probabilistic definition of CF's we use the following <ref> [7] </ref> : CF (H,E) = l (H,E) l (H,E)-1 if 0 l (H,E) 1 where l (H,E) = P (E | H ) This formalisation is derived from a set of axioms [7] which imply that the rules must be conditionally independent given the hypothesis and its negation [9]. This assumption does not hold in general.
Reference: 8. <author> Kouba,Z. </author> : <title> "Data Analysis and Uncertainty Processing", </title> <booktitle> in Advanced Topics in Artificial Intelligence, </booktitle> <editor> Marik,V. et al. (eds.), </editor> <booktitle> Lectures Notes in Artificial Intelligence, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: The main extension should be to broaden the range of methods used to combine rules. Some effort could be invested towards the use of a model which does not exhibit the limitations of conditional independence <ref> [8] </ref> that both certainty factors and degrees of sufficiency and necessity suffer from.
Reference: 9. <author> Mntaras,R., </author> : <title> Approximate Reasoning Methods, </title> <publisher> Ellis Howood Limited, </publisher> <year> 1990. </year>
Reference-contexts: the following [7] : CF (H,E) = l (H,E) l (H,E)-1 if 0 l (H,E) 1 where l (H,E) = P (E | H ) This formalisation is derived from a set of axioms [7] which imply that the rules must be conditionally independent given the hypothesis and its negation <ref> [9] </ref>. This assumption does not hold in general. Nevertheless, this approach has been widely used and achieved good practical results. 3.2 Using Degree of Sufficiency and Necessity ( PROSPECTOR ) PROSPECTOR [5] can be considered another successful expert system.
Reference: 10. <author> Pearl, J. </author> : <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Some effort could be invested towards the use of a model which does not exhibit the limitations of conditional independence [8] that both certainty factors and degrees of sufficiency and necessity suffer from. Some experiments could be done with Dempster-Shaffer [4] theory of evidence and Pearl's belief networks <ref> [10] </ref>. 7 Conclusions The experiments carried out in this paper suggest that a simple and quite naive best rule strategy performs quite well in comparison with the two other more complex strategies tested.
Reference: 11. <author> Shortliffe,E., Buchanan,B., </author> <title> "A Model of Inexact Reasoning in Medicine", </title> <journal> in Mathematical Biosciences, </journal> <volume> 23, </volume> <year> 1975. </year>
Reference-contexts: Experiments were made on three real world domains. Their goal was to observe if different classification strategies could lead to different results. The following strategies were used : two well known expert systems approaches, MYCIN <ref> [11] </ref> certainty factors and PROSPECTOR 's [5] odds), together with the best rule strategy. The next section describes briefly the inductive system used in the experiments. <p> Several methodologies exist to solve these problems. In the following sections three different strategies are presented. Each strategy attempts to deal with the problem of uncertainty caused, for instance, by unknown attribute values or incomplete description of examples. 3.1 Using Certainty Factors (MYCIN) MYCIN <ref> [11] </ref> is one of the best known expert systems. MYCIN uses certainty factors (CF) as a way of modelling reasoning under uncertainty. A certainty factor is a number between -1 and 1 that represents the change in our belief on some hypothesis.
Reference: 12. <author> Torgo,L. </author> : <title> YAILS an incremental learning program, LIACC-ML Group, Internal report n 92.1, </title> <year> 1992. </year>
Reference-contexts: The weights are proportional to the value of consistency giving thus some degree of flexibility (see <ref> [12, 13] </ref> for more details). Our formula for the calculation of quality is a heuristic one. Many other possibilities exist for evaluating a composite effect of various rule properties (see for instance [1] for a function which also includes simplicity). Let us now come back to the best rule strategy.
Reference: 13. <author> Torgo,L. </author> : <title> "Controlled Redundancy in Incremtnal Rule Learning", </title> <booktitle> in this volume. </booktitle>
Reference-contexts: The weights are proportional to the value of consistency giving thus some degree of flexibility (see <ref> [12, 13] </ref> for more details). Our formula for the calculation of quality is a heuristic one. Many other possibilities exist for evaluating a composite effect of various rule properties (see for instance [1] for a function which also includes simplicity). Let us now come back to the best rule strategy.
References-found: 13

