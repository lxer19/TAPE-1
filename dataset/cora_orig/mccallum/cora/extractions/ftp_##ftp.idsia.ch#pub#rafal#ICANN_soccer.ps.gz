URL: ftp://ftp.idsia.ch/pub/rafal/ICANN_soccer.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00408.html
Root-URL: 
Email: e-mail: frafal, marco, juergeng@idsia.ch  
Title: On Learning Soccer Strategies  
Author: Rafa l Sa lustowicz, Marco Wiering, Jurgen Schmidhuber 
Date: 1997.  
Note: In W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, editors, Proceedings of the Seventh International Conference on Artificial Neural Networks (ICANN'97), volume 1327 of Lecture Notes in Computer Science, pages 769-774. Springer-Verlag Berlin Heidelberg,  
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Abstract: We use simulated soccer to study multiagent learning. Each team's players (agents) share action set and policy but may behave differently due to position-dependent inputs. All agents making up a team are rewarded or punished collectively in case of goals. We conduct simulations with varying team sizes, and compare two learning algorithms: TD-Q learning with linear neural networks (TD-Q) and Probabilistic Incremental Program Evolution (PIPE). TD-Q is based on evaluation functions (EFs) mapping input/action pairs to expected reward, while PIPE searches policy space directly. PIPE uses an adaptive probability distribution to synthesize programs that calculate action probabilities from current inputs. Our results show that TD-Q has difficulties to learn appropriate shared EFs. PIPE, however, does not depend on EFs and finds good policies faster and more reliably.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1996. </year>
Reference-contexts: Evaluation functions versus search through policy space. There are two rather obvious classes of candidate algorithms for multiagent reinforcement learning (RL). The first includes traditional single-agent RL algorithms based on adaptive evaluation functions (EFs) <ref> [1] </ref>. Usually online variants of dynamic programming and function approximators are combined to model EFs mapping input-action pairs to expected discounted future reward. Methods from the second class do not require EFs. Their policy space consists of complete algorithms defining agent behaviors, and they search policy space directly.
Reference: 2. <author> N. L. Cramer. </author> <title> A representation for the adaptive generation of simple sequential programs. </title> <editor> In J.J. Grefenstette, editor, </editor> <booktitle> Proceedings of an International Conference on Genetic Algorithms and Their Applications, </booktitle> <pages> pages 183-187, </pages> <address> Hillsdale NJ, 1985. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Methods from the second class do not require EFs. Their policy space consists of complete algorithms defining agent behaviors, and they search policy space directly. Well-known members of this class are Levin search [3], Genetic Programming, e.g. <ref> [2] </ref>, and Probabilistic Incremental Program Evolution [6]. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) [4] and Probabilistic Incremental Program Evolution (PIPE) [6].
Reference: 3. <author> L. A. Levin. </author> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266, </pages> <year> 1973. </year>
Reference-contexts: Methods from the second class do not require EFs. Their policy space consists of complete algorithms defining agent behaviors, and they search policy space directly. Well-known members of this class are Levin search <ref> [3] </ref>, Genetic Programming, e.g. [2], and Probabilistic Incremental Program Evolution [6]. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) [4] and Probabilistic Incremental Program Evolution (PIPE) [6].
Reference: 4. <author> L. J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Well-known members of this class are Levin search [3], Genetic Programming, e.g. [2], and Probabilistic Incremental Program Evolution [6]. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) <ref> [4] </ref> and Probabilistic Incremental Program Evolution (PIPE) [6]. TD-Q selects actions according to linear neural networks trained with the delta rule to map player inputs to evaluations of alternative actions. PIPE uses a probability distribution to synthesize programs that calculate action probabilities from inputs. <p> Finally PPT's probabilities are mutated to better explore the search space. All details can be found in [6]. 4 TD-Q Learning One of the most widely used EF-based approaches to reinforcement learning is TD-Q learning. We use Lin's successful TD () Q-variant <ref> [4] </ref>. For efficiency reasons our TD-Q version uses linear neural nets (nets with hidden units require too much simulation time).
Reference: 5. <author> M. L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learn-ing. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 157-163. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year>
Reference-contexts: There have been attempts to learn low-level cooperation tasks such as pass play. Published results on learning entire soccer strategies, however, have been limited to extremely reduced scenarios (e.g., two single opponent players in a 5 fi 4 grid world <ref> [5] </ref>). Our comparatively complex case study will involve simulations with continuous-valued inputs, simple physical laws to model ball bounces and friction, and up to 11 players (agents) on each team. Evaluation functions versus search through policy space.
Reference: 6. <author> R. P. Sa lustowicz and J. Schmidhuber. </author> <title> Probabilistic incremental program evolution. Evolutionary Computation, </title> <note> to appear, 1997. See ftp://ftp.idsia.ch/pub/rafal/- PIPE.ps.gz. </note>
Reference-contexts: Methods from the second class do not require EFs. Their policy space consists of complete algorithms defining agent behaviors, and they search policy space directly. Well-known members of this class are Levin search [3], Genetic Programming, e.g. [2], and Probabilistic Incremental Program Evolution <ref> [6] </ref>. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) [4] and Probabilistic Incremental Program Evolution (PIPE) [6]. <p> Well-known members of this class are Levin search [3], Genetic Programming, e.g. [2], and Probabilistic Incremental Program Evolution <ref> [6] </ref>. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) [4] and Probabilistic Incremental Program Evolution (PIPE) [6]. TD-Q selects actions according to linear neural networks trained with the delta rule to map player inputs to evaluations of alternative actions. PIPE uses a probability distribution to synthesize programs that calculate action probabilities from inputs. <p> ASET contains: go forward, turn to ball, turn to goal and shoot. Shots are noisy and noise makes long shots less precise than close passes. For a detailed description of the soccer simulator see [7]. 3 Probabilistic Incremental Program Evolution (PIPE) We use PIPE <ref> [6] </ref> to synthesize programs which, given player p's input vector i (p; t), select actions from ASET. Action Selection. Action selection depends on 5 variables: g 2 IR, A i 2 IR, 8i 2 ASET . <p> To evaluate a program we play one entire soccer game. PIPE then adapts PPT's probabilities so that the overall probability of creating the best program of the current population increases. Finally PPT's probabilities are mutated to better explore the search space. All details can be found in <ref> [6] </ref>. 4 TD-Q Learning One of the most widely used EF-based approaches to reinforcement learning is TD-Q learning. We use Lin's successful TD () Q-variant [4]. For efficiency reasons our TD-Q version uses linear neural nets (nets with hidden units require too much simulation time). <p> PIPE Set-up. Parameters for PIPE runs are: P T =0.8, " = 1, P el = 0, PS=10, lr=0.2, P M =0.1, mr=0.2, T R =0.3, T P =0.999999 (see <ref> [6] </ref> for details). During performance evaluations we test the current best-of-current-population program (except for the first evaluation where we test a random program). TD-Q Set-up. After a thorough parameter search we found the following best parameters for TD-Q runs: fl=0.99, Lr N =0.0001, =0.9, H max =100.
Reference: 7. <author> R. P. Sa lustowicz, M. A. Wiering, and J. Schmidhuber. </author> <title> Learning team strategies with multiple policy-sharing agents: A soccer case study. </title> <type> Technical Report IDSIA-29-97, </type> <institution> IDSIA, </institution> <year> 1997. </year> <note> See ftp://ftp.idsia.ch/pub/rafal/soccer.ps.gz. </note>
Reference-contexts: Actions. Players may execute actions from action set ASET. ASET contains: go forward, turn to ball, turn to goal and shoot. Shots are noisy and noise makes long shots less precise than close passes. For a detailed description of the soccer simulator see <ref> [7] </ref>. 3 Probabilistic Incremental Program Evolution (PIPE) We use PIPE [6] to synthesize programs which, given player p's input vector i (p; t), select actions from ASET. Action Selection. Action selection depends on 5 variables: g 2 IR, A i 2 IR, 8i 2 ASET . <p> Once all first entries have been processed we start processing the second entries, etc. The nets are trained using the delta-rule with learning rate Lr N . All details can be found in <ref> [7] </ref>. 5 Experiments We analyze TD-Q's and PIPE's behavior as we vary team size. We perform 10 independent runs for each combination of learning algorithm and team size. We play 3300 games of length t end = 5000 for both team sizes (1 and 11).
References-found: 7

