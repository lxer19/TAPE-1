URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/95.PLDI.Unifying_data_and_control_transformations.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/cierniak/research/papers-short.html
Root-URL: 
Email: fcierniak,weig@cs.rochester.edu  
Title: Unifying Data and Control Transformations for Distributed Shared-Memory Machines  
Author: Micha Cierniak Wei Li 
Note: This work was supported in part by an NSF Research Initiation Award (CCR-9409120) and ARPA contract F19628-94-C-0057. Appeared in the Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation  
Address: Rochester, NY 14627  
Affiliation: Department of Computer Science University of Rochester  
Abstract: We present a unified approach to locality optimization that employs both data and control transformations. Data transformations include changing the array layout in memory. Control transformations involve changing the execution order of programs. We have developed new techniques for compiler optimizations for distributed shared-memory machines, although the same techniques can be used for sequential machines with a memory hierarchy. Our compiler optimizations are based on an algebraic representation of data mappings and a new data locality model. We present a pure data transformation algorithm and an algorithm unifying data and control transformations. While there has been much work on control transformations, the opportunities for data transformations have been largely neglected. In fact, data transformations have the advantage of being applicable to programs that cannot be optimized with control transformations. The unified algorithm, which performs data and control transformations simultaneously, offers improvement over optimizations obtained by applying data and control transformations separately. The experimental results using a set of applications on a parallel machine show that the new optimizations improve performance significantly. These results are further analyzed using locality metrics with instrumentation and simulation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. M. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> Proceedings of ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: for sequentially iterated parallel loops, by Knobe et al. [19] for SIMD machines, by Li and Chen [20] for index domain alignment, by Ramanujam and Sadayap-pan [30] who find communication-free partitioning of arrays in fully parallel loops, by Gupta and Banerjee [16] with a constraint-based approach, by Anderson and Lam <ref> [1] </ref> on data alignment and parallelism, by Chatterjee, Gilbert, Schreiber and Teng [9] on array alignment and by Bau, Kodukula, Kotl-yar, Pingali and Stodghill [3] on a clean linear algebra solution to the alignment problem. However, the data mapping issues are sufficiently different on distributed shared memory machines.
Reference: [2] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proc. 5th Distributed Memory Comput. Conf., </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: While data transformations are new in compiling for distributed shared memory machines, automatic data distribution for distributed message passing machines has been investigated by Balasundaram and others <ref> [2] </ref>, by Hudak and Abramham [18] for sequentially iterated parallel loops, by Knobe et al. [19] for SIMD machines, by Li and Chen [20] for index domain alignment, by Ramanujam and Sadayap-pan [30] who find communication-free partitioning of arrays in fully parallel loops, by Gupta and Banerjee [16] with a constraint-based
Reference: [3] <author> D. Bau, I. Kodukula, V. Kotlyar, K. Pingali, and P. Stodghill. </author> <title> Solving alignment using elementary linear algebra. </title> <booktitle> Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1994. </year>
Reference-contexts: and Sadayap-pan [30] who find communication-free partitioning of arrays in fully parallel loops, by Gupta and Banerjee [16] with a constraint-based approach, by Anderson and Lam [1] on data alignment and parallelism, by Chatterjee, Gilbert, Schreiber and Teng [9] on array alignment and by Bau, Kodukula, Kotl-yar, Pingali and Stodghill <ref> [3] </ref> on a clean linear algebra solution to the alignment problem. However, the data mapping issues are sufficiently different on distributed shared memory machines.
Reference: [4] <author> R. Bianchini and T. J. LeBlanc. </author> <title> Software caching on cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the Fourth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 521-526, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: False sharing has been found to be a serious obstacle to high performance on distributed shared memory machines <ref> [12, 4, 6] </ref>. Previous work on compiler algorithms for cache locality has been on loop transformations, i.e., control transformations. Wolf and Lam [32] focus on loop tiling of the innermost loops as a means of achieving cache locality. They try all possible subsets of the loops in the loop nest. <p> Li [24] developed a new reuse model and algorithms that can optimize for not only dense matrix algorithms but also banded-matrix algorithms. The work by Eggers and Jeremiassen [12] and by Bian-chini and LeBlanc <ref> [4] </ref> showed that for some programs, program restructuring and data restructuring can eliminate or reduce false sharing so that performance can be improved. However, these transformation techniques are all performed by hand on specific application programs.
Reference: [5] <author> William J. Bolosky, Robert P. Fitzgerald, and Michael L. Scott. </author> <title> Simple but effective techniques for NUMA memory management. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: A large body of work now exists in the area of coherence protocols for large-scale multiprocessors. These protocols vary from hardware-only implementations [25] to software shared memory emulations for message-passing machines [27]. There are hybrid techniques that incorporate both migration and remote reference on NUMA machines <ref> [10, 5, 29] </ref>. To reduce the cost of data coherence and communication, data locality must be exploited. When locality is exploited by only run-time, kernel, or hardware-level policies that observe program behavior from below, false sharing becomes a major problem.
Reference: [6] <author> William J. Bolosky and Michael L. Scott. </author> <title> False sharing and its effect on shared memory performance. </title> <booktitle> In Proceedings of the Fourth Usenix Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 57-71, </pages> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year> <note> Also available as MSR-TR-93-1, </note> <institution> Microsoft Research Laboratory, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: False sharing has been found to be a serious obstacle to high performance on distributed shared memory machines <ref> [12, 4, 6] </ref>. Previous work on compiler algorithms for cache locality has been on loop transformations, i.e., control transformations. Wolf and Lam [32] focus on loop tiling of the innermost loops as a means of achieving cache locality. They try all possible subsets of the loops in the loop nest.
Reference: [7] <author> S. Carr, K. McKinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 252-262, </pages> <month> October </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: The techniques by Ferrante, Sarkar, and Thrash [14] estimate the number of distinct cache lines used by a given loop in a loop nest. Given this estimate, they compute the number of cache misses for a loop nest. Carr, McKinley and Tseng <ref> [7] </ref> developed a simple memory model that can be used for both perfectly and imperfectly nested loops. They used loop permutations, fusion and distribution for transformations. Gannon, Jalby and Gallivan [15] introduced the notion of uniformly generated data reuse, and proposed the notion of windows to capture data reuse. <p> We can see that they both have poor locality in this loop nest. 2 2.1 Control Transformations As discussed in Section 1, the problem of control transformations, i.e., loop transformations is relatively well understood. We will not describe details of any of the known approaches <ref> [32, 7, 15, 13, 24] </ref>. Different transformations may be used to improve data locality.
Reference: [8] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Imple--mentation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Development is continuing on distributed memory message-passing machines (e.g. the Intel Paragon and TMC CM-5) and network (cluster) of workstations, yet even for these machines there are efforts to implement a shared-memory programming model through runtime and kernel-level emulation of shared memory. Representative systems include Ivy [21], Munin <ref> [8] </ref> and others [27]. Most shared-memory machines, both hardware and software based, rely on data caching to exploit data locality and reduce communication. Cache coherence must be maintained for multiple copies of the same data on multiple processors.
Reference: [9] <author> S. Chatterjee, J. R. Gilbert, R. Schreiber, and S. Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> Proceedings of ACM Symposium on Principles of Programming Languages, </booktitle> <volume> 20, </volume> <year> 1993. </year>
Reference-contexts: machines, by Li and Chen [20] for index domain alignment, by Ramanujam and Sadayap-pan [30] who find communication-free partitioning of arrays in fully parallel loops, by Gupta and Banerjee [16] with a constraint-based approach, by Anderson and Lam [1] on data alignment and parallelism, by Chatterjee, Gilbert, Schreiber and Teng <ref> [9] </ref> on array alignment and by Bau, Kodukula, Kotl-yar, Pingali and Stodghill [3] on a clean linear algebra solution to the alignment problem. However, the data mapping issues are sufficiently different on distributed shared memory machines.
Reference: [10] <author> Alan L. Cox and Robert J. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <address> Litchfield Park, AZ, </address> <month> Decem-ber </month> <year> 1989. </year>
Reference-contexts: A large body of work now exists in the area of coherence protocols for large-scale multiprocessors. These protocols vary from hardware-only implementations [25] to software shared memory emulations for message-passing machines [27]. There are hybrid techniques that incorporate both migration and remote reference on NUMA machines <ref> [10, 5, 29] </ref>. To reduce the cost of data coherence and communication, data locality must be exploited. When locality is exploited by only run-time, kernel, or hardware-level policies that observe program behavior from below, false sharing becomes a major problem.
Reference: [11] <author> M. Dubois, J. Skeppstedt, L. Ricciulli, K. Ramamurthy, and P. Stenstrom. </author> <title> The detection and elimination of useless misses in multiprocessors. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 88-97, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: However, these transformation techniques are all performed by hand on specific application programs. Dubois et al. describe a hardware mechanism to eliminate misses due to false sharing, but at the expense of very large amounts of other communication <ref> [11] </ref>.
Reference: [12] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proc. 1991 International Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: False sharing has been found to be a serious obstacle to high performance on distributed shared memory machines <ref> [12, 4, 6] </ref>. Previous work on compiler algorithms for cache locality has been on loop transformations, i.e., control transformations. Wolf and Lam [32] focus on loop tiling of the innermost loops as a means of achieving cache locality. They try all possible subsets of the loops in the loop nest. <p> Li [24] developed a new reuse model and algorithms that can optimize for not only dense matrix algorithms but also banded-matrix algorithms. The work by Eggers and Jeremiassen <ref> [12] </ref> and by Bian-chini and LeBlanc [4] showed that for some programs, program restructuring and data restructuring can eliminate or reduce false sharing so that performance can be improved. However, these transformation techniques are all performed by hand on specific application programs.
Reference: [13] <author> C. Eisenbeis, W. Jalby, D. Windheiser, and F. Bodin. </author> <title> A strategy for array management in local memory. </title> <booktitle> In Proc. 3th Annual Workshop on Languages and Compilers, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: They used loop permutations, fusion and distribution for transformations. Gannon, Jalby and Gallivan [15] introduced the notion of uniformly generated data reuse, and proposed the notion of windows to capture data reuse. Eisen-beis, Jalby, Windheiser and Bodin <ref> [13] </ref> used the windows to develop a strategy to explicitly manage the data transfers to local memory. Porterfield [28] studied the problem of estimating the number of cache lines for uniprocessor machines, when the cache line size is 1. <p> We can see that they both have poor locality in this loop nest. 2 2.1 Control Transformations As discussed in Section 1, the problem of control transformations, i.e., loop transformations is relatively well understood. We will not describe details of any of the known approaches <ref> [32, 7, 15, 13, 24] </ref>. Different transformations may be used to improve data locality.
Reference: [14] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and exchange cache effectiveness. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: They try all possible subsets of the loops in the loop nest. The subset that can be brought into the innermost position, and has the best objective function from the reuse vector space model is chosen to be tiled. The techniques by Ferrante, Sarkar, and Thrash <ref> [14] </ref> estimate the number of distinct cache lines used by a given loop in a loop nest. Given this estimate, they compute the number of cache misses for a loop nest.
Reference: [15] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: Given this estimate, they compute the number of cache misses for a loop nest. Carr, McKinley and Tseng [7] developed a simple memory model that can be used for both perfectly and imperfectly nested loops. They used loop permutations, fusion and distribution for transformations. Gannon, Jalby and Gallivan <ref> [15] </ref> introduced the notion of uniformly generated data reuse, and proposed the notion of windows to capture data reuse. Eisen-beis, Jalby, Windheiser and Bodin [13] used the windows to develop a strategy to explicitly manage the data transfers to local memory. <p> We can see that they both have poor locality in this loop nest. 2 2.1 Control Transformations As discussed in Section 1, the problem of control transformations, i.e., loop transformations is relatively well understood. We will not describe details of any of the known approaches <ref> [32, 7, 15, 13, 24] </ref>. Different transformations may be used to improve data locality.
Reference: [16] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3 </volume> <pages> 179-193, </pages> <year> 1992. </year>
Reference-contexts: Balasundaram and others [2], by Hudak and Abramham [18] for sequentially iterated parallel loops, by Knobe et al. [19] for SIMD machines, by Li and Chen [20] for index domain alignment, by Ramanujam and Sadayap-pan [30] who find communication-free partitioning of arrays in fully parallel loops, by Gupta and Banerjee <ref> [16] </ref> with a constraint-based approach, by Anderson and Lam [1] on data alignment and parallelism, by Chatterjee, Gilbert, Schreiber and Teng [9] on array alignment and by Bau, Kodukula, Kotl-yar, Pingali and Stodghill [3] on a clean linear algebra solution to the alignment problem.
Reference: [17] <author> J. Hennessy and D. Patterson. </author> <booktitle> Computer Architecture: </booktitle>
Reference-contexts: To exploit the memory hierarchy, data reuse has to be maximized. To simplify our discussion, we will consider computers with large main memory and smaller, but faster cache memory. Cache hit ratio <ref> [17] </ref> is one metric for quantifying data reuse.
References-found: 17

