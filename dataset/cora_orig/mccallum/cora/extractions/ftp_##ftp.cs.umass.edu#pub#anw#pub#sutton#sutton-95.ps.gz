URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/sutton-95.ps.gz
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: sutton@gte.com  
Title: TD Models: Modeling the World at a Mixture of Time Scales  
Author: Richard S. Sutton 
Affiliation: Stow Research  
Note: As appeared in: Proceedings of the 12th Int. Conf. on Machine Learning, 531-539, Morgan Kaufmann, 1995.  
Abstract: Temporal-difference (TD) learning can be used not just to predict rewards, as is commonly done in reinforcement learning, but also to predict states, i.e., to learn a model of the world's dynamics. We present theory and algorithms for intermixing TD models of the world at different levels of temporal abstraction within a single structure. Such multi-scale TD models can be used in model-based reinforcement-learning architectures and dynamic programming methods in place of conventional Markov models. This enables planning at higher and varied levels of abstraction, and, as such, may prove useful in formulating methods for hierarchical or multi-level planning and reinforcement learning. In this paper we treat only the prediction problem|that of learning a model and value function for the case of fixed agent behavior. Within this context, we establish the theoretical foundations of multi-scale models and derive TD algorithms for learning them. Two small computational experiments are presented to test and illustrate the theory. This work is an extension and generalization of the work of Singh (1992), Dayan (1993), and Sutton & Pinette (1985).
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A.G., Bradtke, S.J., Singh, </author> <title> S.P. (1995) "Learning to act using real-time dynamic programming," </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Chrisman, L. </author> <title> (1994) "Reasoning about probabilistic actions at multiple levels of granularity," </title> <booktitle> AAAI Spring Symposium: Decision-Theoretic Planning, </booktitle> <publisher> Stanford University. </publisher>
Reference: <author> Dayan, P. </author> <title> (1993) "Improving generalization for temporal difference learning: </title> <booktitle> The successor representation," Neural Computation 5, </booktitle> <pages> 613-624. </pages>
Reference: <author> Dayan, P., Hinton, G.E. </author> <title> (1993) "Feudal reinforcement learning". </title> <editor> In C.L. Giles, S.J. Hanson, J.D. Cowan, Editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 5, </volume> <pages> 271-278. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dean, T., Kaelbling, L.P., Kirman, J., Nicholson, A. </author> <title> (in preparation) "Planning under time constraints in stochastic domains." </title> <editor> Dean, T., Lin, S.-H. </editor> <title> (in preparation) "Decomposition techniques for planning in stochastic domains." Drescher, G.L. (1991) Made Up Minds: A Constructivist Approach to Artificial Intelligence. </title> <publisher> MIT Press. </publisher>
Reference: <author> Fikes, R.E., Hart, P.E., Nilsson, N.J. </author> <title> (1972) "Learning and executing generalized robot plans," </title> <booktitle> Artificial Intelligence 3, </booktitle> <pages> 251-288. </pages>
Reference: <author> Hansen, E. </author> <title> (1994) "Cost-effective sensing during plan execution," </title> <booktitle> Proc. AAAI-94, </booktitle> <pages> 1029-1035. </pages>
Reference: <author> Kuipers, B.J. </author> <title> (1979) "Commonsense Knowledge of Space: Learning from Experience," </title> <booktitle> Proc. IJCAI-79, </booktitle> <pages> 499-501. </pages>
Reference: <author> Kaelbling, </author> <title> L.P. (1993) "Hierarchical learning in stochastic domains: Preliminary results," </title> <booktitle> Proc. of the Tenth Int. Conf. on Machine Learning, </booktitle> <pages> 167-173, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Korf, R.E. </author> <title> (1985) Learning to Solve Problems by Searching for Macro-Operators. </title> <address> Boston: </address> <publisher> Pitman Publishers. </publisher>
Reference: <author> Laird, J.E., Rosenbloom, P.S., Newell, A. </author> <title> (1986) "Chunking in SOAR: The anatomy of a general learning mechanism," </title> <booktitle> Machine Learning 1, </booktitle> <pages> 11-46. </pages>
Reference: <author> Lin, L.-J. </author> <title> (1993) Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution> <note> Technical Report CMU-CS-93-103. </note>
Reference: <author> Mataric, M.J. </author> <title> (1990) A Model for Distributed Mobile Robot Environment Learning and Navigation. </title> <type> MIT Masters thesis, </type> <institution> Electrical Engineering and Computer Science. </institution>
Reference: <author> Minton, S. </author> <title> (1988) Learning Search Control Knowledge: An Explanation-based Approach. </title> <publisher> Kluwer Academic. </publisher>
Reference: <author> Moore, A.W., Atkeson, </author> <title> C.G. (1993) "Prioritized sweeping: Reinforcement learning with less data and less real time," </title> <booktitle> Machine Learning 13, </booktitle> <pages> 103-130. </pages>
Reference: <author> Peng, J., Williams, </author> <title> R.J. (1993) "Efficient learning and planning within the Dyna framework," </title> <booktitle> Adaptive Behavior 1, </booktitle> <pages> 437-454. </pages>
Reference: <author> Ring, M. </author> <title> (1991) "Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies," </title> <booktitle> Proceedings of the Eighth International Conference on Machine Learning, </booktitle> <pages> 343-347, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sacerdoti, E.D. </author> <title> (1977) A Structure for Plans and Behavior. </title> <address> New York: </address> <publisher> Elsevier. </publisher>
Reference: <author> Schmidhuber, J. </author> <title> (1991) "Neural Sequence Chunkers." </title> <institution> Technische Universitat Munchen TR FKI-148-91. </institution>
Reference: <author> Singh, </author> <title> S.P. (1992) "Reinforcement learning with a hierarchy of abstract models," </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 202-207. </pages> <publisher> MIT/AAAI Press. </publisher>
Reference: <author> Singh, </author> <title> S.P. (1992) "Scaling reinforcement learning by learning variable temporal resolution models," </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> 406-415, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, </author> <title> R.S. (1988) "Learning to predict by the methods of temporal differences," </title> <booktitle> Machine Learning 3, </booktitle> <pages> 9-44. </pages>
Reference-contexts: However, temporal-difference (TD) methods offer great advantages in terms of incremental computation and, potentially, in learning rate if significant state information is available. In this section we derive a TD ()-like learning algorithm <ref> (Sutton, 1988) </ref> for fi-models. Our derivation follows that of TD () in Watkins (1989) and Jaakkola, Jordan & Singh (1994).
Reference: <author> Sutton, R. S. </author> <title> (1990) "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming," </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> 216-224. </pages>
Reference: <author> Sutton, R.S., Pinette, B. </author> <title> (1985) "The learning of world models by connectionist networks," </title> <booktitle> Proc. of the Seventh Annual Conf. of the Cognitive Science Society, </booktitle> <pages> 54-64. </pages>
Reference-contexts: undesirable, then this multi-scale model might be very useful for quickly computing the effect on received reward of taking or not taking the wall-following action. 11 A Hidden-State Example Although we have not emphasized it in this paper, multi-scale models are also useful in overcoming the problems of hidden state <ref> (e.g., see Sutton & Pinette, 1985) </ref>. Figure 4 shows one example. States 6 and 7 are confounded, both appearing to the learning agent as State 6.
Reference: <author> Tenenberg, J. Karlsson, J., & Whitehead, S. </author> <title> (1992) "Learning via task decomposition," </title> <booktitle> Proc. Second Int. Conf. on the Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Thrun, T., Schwartz, A. </author> <title> (1995) "Finding Structure in Reinforcement Learning," </title> <booktitle> in Advances in Neural Information Processing Systems, 7. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Watkins, C.J.C.H. </author> <title> (1989) Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University. </institution>
Reference: <author> Wixson, L.E. </author> <title> (1991) "Scaling reinforcement learning techniques via modularity," </title> <booktitle> Proc. Eighth Int. Conf. on Machine Learning, </booktitle> <pages> 368-372, </pages> <publisher> Morgan Kaufmann. </publisher>
References-found: 28

