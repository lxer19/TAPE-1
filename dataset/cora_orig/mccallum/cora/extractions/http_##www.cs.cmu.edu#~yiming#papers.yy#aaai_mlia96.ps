URL: http://www.cs.cmu.edu/~yiming/papers.yy/aaai_mlia96.ps
Refering-URL: http://www.cs.cmu.edu/~yiming/publications.html
Root-URL: 
Email: yang@mayo.edu  
Title: Sampling Strategies and Learning Efficiency in Text Categorization  
Author: Yiming Yang 
Address: Rochester, Minnesota 55905 USA  
Affiliation: Section of Medical Information Resources, Mayo Clinic/Foundation  
Abstract: This paper studies training set sampling strategies in the context of statistical learning for text categorization. It is argued sampling strategies favoring common categories is superior to uniform coverage or mistake-driven approaches, if performance is measured by globally assessed precision and recall. The hypothesis is empirically validated by examining the performance of a nearest neighbor classifier on training samples drawn from a pool of 235,401 training texts with 29,741 distinct categories. The learning curves of the classifier are analyzed with respect to the choice of training resources, the sampling methods, the size, vocabulary and category coverage of a sample, and the category distribution over the texts in the sample. A nearly-optimal categorization performance of the classifier is achieved using a relatively small training sample, showing that statistical learning can be successfully applied to very large text categorization problems with affordable computation. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Chute, C.; Yang, Y.; and Buntrock, J. </author> <year> 1994. </year> <title> An evaluation of computer-assisted clinical classification algorithms. </title> <booktitle> In 18th Ann Symp Comp Applic Med Care (SCAMC) JAMIA 1994;18(Symp.Suppl), </booktitle> <month> 162-6. </month> <title> Commission on Professional and Hospital Activities, </title> <address> Ann Arbor, MI. </address> <year> 1968. </year> <title> HICDA-2, Hospital Adaptation of ICDA, </title> <booktitle> 2nd Edition. </booktitle>
Reference-contexts: Automatic or semi-automatic text categorization tools can help reduce these costs and may also improve accuracy and consistency. Computer-based text categorization technologies include: * naive word-matching <ref> (Chute, Yang, & Buntrock 1994) </ref> which matches categories to text based on the shared words between the text and the names of cat egories; * thesaurus-based matching (Lindberg & Humphreys 1990) which uses lexical links (constructed manually or automatically in advance) to relate a given text to the names or descriptive <p> for example, decision tree methods (Quinlan 1986) (Lewis 1991), Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994), and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) <ref> (Yang & Chute 1994) </ref>. While empirical learning holds great potential for high accuracy text categorization, few practical systems have developed due to difficulties in scaling to large problems. The MEDLINE database, for example, uses about 17,000 subject categories (Nat 1993) to index its articles. <p> In contrast, nearest neighbor approaches and linear regression methods require less computation, and have been applied to relatively large categorization problems (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994) (Fuhr, Hartmanna, & et al. 1991) <ref> (Yang & Chute 1994) </ref>. Nevertheless, for large problems with tens of thousands of categories, the learning effectiveness and the computational tractability of empirical methods remains a largely unexplored area. Sampling strategies are important for both the effectiveness and the efficiency of statistical text categorization. <p> Also, nearest neighbor classification performs at least as well as least squares fit (Yang 1994) or rule-based approaches (Creecy et al. 1992). A production version of ExpNet was developed at the Mayo Clinic, and is in daily use as a search engine in computer-aided human coding of patient records <ref> (Chute, Yang, & Buntrock 1994) </ref>. The pool of training data consists of a subset of diagnoses from patient records at Mayo, and the definition phrases of the categories in HICDA-2 (Hospital Adaptation of ICDA, 2nd Edition)(Com 1968). About 2.4 million diagnoses (DXs) are coded each year using the HICDA-2 categories. <p> Similarly, an analysis of the correlation between accuracy improvement and computation cost helps evaluate the trade-off between categorization accuracy and learning efficiency. A test set was selected to evaluate sampling strategies. Five test sets were collected in our previous evaluation of different categorization methods <ref> (Chute, Yang, & Buntrock 1994) </ref>; none of them were from the training pool. Each set consists of about 1000 DXs arbitrarily chosen from the patient records at the time of that evaluation. <p> of needed instances, and using uncertainty sampling under the control of a globally optimal strategy; * a sampling strategy analysis for ExpNet on different domains, such as MEDLINE documents, the Reuters newswire collection, etc.; and * similar analyses for other statistical learning methods, e.g., the Linear Least Squares Fit mapping <ref> (Yang & Chute 1994) </ref>. Acknowledgement This work is supported at Mayo Foundation in part by NIH Research Grants LM05714 and LM05416.
Reference: <author> Creecy, R.; Masand, B.; Smith, S.; and Waltz, D. </author> <year> 1992. </year> <title> Trading mips and memory for knowledge engineering: classifying census returns on the connection machine. </title> <journal> Comm. ACM 35 </journal> <pages> 48-63. </pages>
Reference-contexts: Moreover, most empirical learning formalisms offer a context sensitive mapping from terms to categories, for example, decision tree methods (Quinlan 1986) (Lewis 1991), Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods <ref> (Creecy et al. 1992) </ref>, (Masand, Linoff, & Waltz 1992) (Yang 1994), and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994). While empirical learning holds great potential for high accuracy text categorization, few practical systems have developed due to difficulties in scaling to large problems. <p> Naive Bayesian methods assume term independence in category prediction, fundamentally sacrificing the strength of the original framework in handling the context sensitivity of term-to-category mapping. In contrast, nearest neighbor approaches and linear regression methods require less computation, and have been applied to relatively large categorization problems <ref> (Creecy et al. 1992) </ref>, (Masand, Linoff, & Waltz 1992) (Yang 1994) (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994). Nevertheless, for large problems with tens of thousands of categories, the learning effectiveness and the computational tractability of empirical methods remains a largely unexplored area. <p> A nearest neighbor classifier was selected for this study because of its relatively low computation cost and the relative ease with which it scales to large problems. Also, nearest neighbor classification performs at least as well as least squares fit (Yang 1994) or rule-based approaches <ref> (Creecy et al. 1992) </ref>. A production version of ExpNet was developed at the Mayo Clinic, and is in daily use as a search engine in computer-aided human coding of patient records (Chute, Yang, & Buntrock 1994).
Reference: <author> Fuhr, N.; Hartmanna, S.; and et al., G. L. </author> <year> 1991. </year> <title> Air/x a rule-based multistage indexing systems for large subject fields. In 606-623., </title> <editor> ed., </editor> <booktitle> Proceedings of RIAO'91. </booktitle>
Reference-contexts: sensitive mapping from terms to categories, for example, decision tree methods (Quinlan 1986) (Lewis 1991), Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994), and least-squares regression techniques <ref> (Fuhr, Hartmanna, & et al. 1991) </ref> (Yang & Chute 1994). While empirical learning holds great potential for high accuracy text categorization, few practical systems have developed due to difficulties in scaling to large problems. The MEDLINE database, for example, uses about 17,000 subject categories (Nat 1993) to index its articles. <p> In contrast, nearest neighbor approaches and linear regression methods require less computation, and have been applied to relatively large categorization problems (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994) <ref> (Fuhr, Hartmanna, & et al. 1991) </ref> (Yang & Chute 1994). Nevertheless, for large problems with tens of thousands of categories, the learning effectiveness and the computational tractability of empirical methods remains a largely unexplored area. Sampling strategies are important for both the effectiveness and the efficiency of statistical text categorization.
Reference: <author> Lewis, D. </author> <year> 1991. </year> <title> Evaluating text categorization. </title> <booktitle> In Proceedings of the Speech and Natural Language Workshop, Asilomar, </booktitle> <pages> 312-31. </pages> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Moreover, most empirical learning formalisms offer a context sensitive mapping from terms to categories, for example, decision tree methods (Quinlan 1986) <ref> (Lewis 1991) </ref>, Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994), and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994). <p> The largest number of categories ever tried in a decision tree or a neural network, however, is only a few hundreds or less (Schutze, Hull, & Pedersen 1995). Bayesian belief networks have a similar scaling-up difficulty, and a "naive" version <ref> (Lewis 1991) </ref> is often used for computational tractability. Naive Bayesian methods assume term independence in category prediction, fundamentally sacrificing the strength of the original framework in handling the context sensitivity of term-to-category mapping.
Reference: <author> Lewis, D. </author> <year> 1994. </year> <title> A sequential algorithm for training test classifiers. </title> <booktitle> In 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 94), </booktitle> <pages> 3-12. </pages>
Reference-contexts: Random sampling over instances, as another example, naturally favors common categories over rare categories, and hence is better than uniform sampling in a global sense. Uncertainty sampling <ref> (Lewis 1994) </ref> strategies are driven by the "failure" of a classifier. That is, instances that are not well classified by the learning system are added to the training sample. This results in the selection of instance belonging to rare categories that contribute little to global categorization performance.
Reference: <author> Lindberg, D., and Humphreys, B. </author> <year> 1990. </year> <title> The umls knowledge sources: Tools for building better user interfaces. </title> <booktitle> In Proceedings of the 14th Annual Symposium on Computer Applications in Medical Care (SCAMC 90), </booktitle> <pages> 121-125. </pages>
Reference-contexts: Computer-based text categorization technologies include: * naive word-matching (Chute, Yang, & Buntrock 1994) which matches categories to text based on the shared words between the text and the names of cat egories; * thesaurus-based matching <ref> (Lindberg & Humphreys 1990) </ref> which uses lexical links (constructed manually or automatically in advance) to relate a given text to the names or descriptive phrases of categories; and * empirical learning of term-category associations from a training set of texts and their categories as signed by humans.
Reference: <author> Masand, B.; Linoff, G.; and Waltz, D. </author> <year> 1992. </year> <title> Classi--fying news stories using memory based reasoning. </title> <booktitle> In 15th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 92), </booktitle> <pages> 59-64. </pages> <address> National Library of Medicine, Bethesda, MD. </address> <year> 1993. </year> <title> Medical Subject Headings (MeSH). </title>
Reference-contexts: Moreover, most empirical learning formalisms offer a context sensitive mapping from terms to categories, for example, decision tree methods (Quinlan 1986) (Lewis 1991), Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods (Creecy et al. 1992), <ref> (Masand, Linoff, & Waltz 1992) </ref> (Yang 1994), and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994). While empirical learning holds great potential for high accuracy text categorization, few practical systems have developed due to difficulties in scaling to large problems. <p> In contrast, nearest neighbor approaches and linear regression methods require less computation, and have been applied to relatively large categorization problems (Creecy et al. 1992), <ref> (Masand, Linoff, & Waltz 1992) </ref> (Yang 1994) (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994). Nevertheless, for large problems with tens of thousands of categories, the learning effectiveness and the computational tractability of empirical methods remains a largely unexplored area.
Reference: <author> Quinlan, J. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1(1) </booktitle> <pages> 81-106. </pages>
Reference-contexts: Empirical learning from categorized texts, on the other hand, fundamentally differs from word-matching because it is based on human relevance judgments, statistically capturing the semantic associations between terms and categories. Moreover, most empirical learning formalisms offer a context sensitive mapping from terms to categories, for example, decision tree methods <ref> (Quinlan 1986) </ref> (Lewis 1991), Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994), and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994).
Reference: <author> Salton, G. </author> <year> 1989. </year> <title> Automatic Text Processing: The Transformation, Analysis, </title> <booktitle> and Retrieval of Information by Computer. </booktitle> <address> Reading, Pennsylvania: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: For convenience, I use text to refer to either a DX or a category definition phrase. Texts in our system are represented using the conventional vector space model <ref> (Salton 1989) </ref>. That is, a text is represented as a vector whose dimensions are unique words in a diagnosis collection, and whose elements are word weights in this text. <p> A potential advantage of such a strategy is that it has better coverage of rare cases. Empirical Validation Performance Measures and Test Data Classifier effectiveness is measured by the conventional ten-point average precision (AVGP) <ref> (Salton 1989) </ref> of ranked categories per test instance. This is a global performance measure similar to correct classification rate.
Reference: <author> Schutze, H.; Hull, D.; and Pedersen, J. </author> <year> 1995. </year> <title> A comparison of classifiers and document representations for the routing problem. </title> <booktitle> In 18th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 95), </booktitle> <pages> 229-237. </pages>
Reference-contexts: Moreover, most empirical learning formalisms offer a context sensitive mapping from terms to categories, for example, decision tree methods (Quinlan 1986) (Lewis 1991), Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks <ref> (Schutze, Hull, & Pedersen 1995) </ref>, nearest neighbor classification methods (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994), and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994). <p> The Mayo Clinic, as another ex-ample, uses 29,741 categories (Com 1968) to code diagnoses. The largest number of categories ever tried in a decision tree or a neural network, however, is only a few hundreds or less <ref> (Schutze, Hull, & Pedersen 1995) </ref>. Bayesian belief networks have a similar scaling-up difficulty, and a "naive" version (Lewis 1991) is often used for computational tractability. Naive Bayesian methods assume term independence in category prediction, fundamentally sacrificing the strength of the original framework in handling the context sensitivity of term-to-category mapping.
Reference: <author> Tzeras, K., and Hartman, S. </author> <year> 1993. </year> <title> Automatic indexing based on bayesian inference networks. </title> <booktitle> In Proc 16th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 93), </booktitle> <pages> 22-34. </pages>
Reference-contexts: Moreover, most empirical learning formalisms offer a context sensitive mapping from terms to categories, for example, decision tree methods (Quinlan 1986) (Lewis 1991), Bayesian belief networks (non-naive ones) <ref> (Tzeras & Hartman 1993) </ref>, neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994), and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994).
Reference: <author> Yang, Y., and Chute, C. </author> <year> 1994. </year> <title> An example-based mapping method for text categorization and retrieval. </title> <journal> ACM Transaction on Information Systems (TOIS) 253-277. </journal>
Reference-contexts: Automatic or semi-automatic text categorization tools can help reduce these costs and may also improve accuracy and consistency. Computer-based text categorization technologies include: * naive word-matching <ref> (Chute, Yang, & Buntrock 1994) </ref> which matches categories to text based on the shared words between the text and the names of cat egories; * thesaurus-based matching (Lindberg & Humphreys 1990) which uses lexical links (constructed manually or automatically in advance) to relate a given text to the names or descriptive <p> empirical learning formalisms offer a context sensitive mapping from terms to categories, for example, decision tree methods (Quinlan 1986) (Lewis 1991), Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) <ref> (Yang 1994) </ref>, and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994). While empirical learning holds great potential for high accuracy text categorization, few practical systems have developed due to difficulties in scaling to large problems. <p> for example, decision tree methods (Quinlan 1986) (Lewis 1991), Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994), and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) <ref> (Yang & Chute 1994) </ref>. While empirical learning holds great potential for high accuracy text categorization, few practical systems have developed due to difficulties in scaling to large problems. The MEDLINE database, for example, uses about 17,000 subject categories (Nat 1993) to index its articles. <p> In contrast, nearest neighbor approaches and linear regression methods require less computation, and have been applied to relatively large categorization problems (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) <ref> (Yang 1994) </ref> (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994). Nevertheless, for large problems with tens of thousands of categories, the learning effectiveness and the computational tractability of empirical methods remains a largely unexplored area. <p> In contrast, nearest neighbor approaches and linear regression methods require less computation, and have been applied to relatively large categorization problems (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994) (Fuhr, Hartmanna, & et al. 1991) <ref> (Yang & Chute 1994) </ref>. Nevertheless, for large problems with tens of thousands of categories, the learning effectiveness and the computational tractability of empirical methods remains a largely unexplored area. Sampling strategies are important for both the effectiveness and the efficiency of statistical text categorization. <p> ExpNet does not require any off-line training, but does require an on-line search for the k-NN of each new text, which has a time complexity linear in the size of the training set (the number of training texts). See <ref> (Yang 1994) </ref> for additional details and for a discussion on the choice of the parameter k. A nearest neighbor classifier was selected for this study because of its relatively low computation cost and the relative ease with which it scales to large problems. <p> A nearest neighbor classifier was selected for this study because of its relatively low computation cost and the relative ease with which it scales to large problems. Also, nearest neighbor classification performs at least as well as least squares fit <ref> (Yang 1994) </ref> or rule-based approaches (Creecy et al. 1992). A production version of ExpNet was developed at the Mayo Clinic, and is in daily use as a search engine in computer-aided human coding of patient records (Chute, Yang, & Buntrock 1994). <p> Also, nearest neighbor classification performs at least as well as least squares fit (Yang 1994) or rule-based approaches (Creecy et al. 1992). A production version of ExpNet was developed at the Mayo Clinic, and is in daily use as a search engine in computer-aided human coding of patient records <ref> (Chute, Yang, & Buntrock 1994) </ref>. The pool of training data consists of a subset of diagnoses from patient records at Mayo, and the definition phrases of the categories in HICDA-2 (Hospital Adaptation of ICDA, 2nd Edition)(Com 1968). About 2.4 million diagnoses (DXs) are coded each year using the HICDA-2 categories. <p> ExpNet operates as an on-line category searcher. When a coder types in a new DX, ExpNet provides a ranked list of potential categories for the user to select. The on-line response time is proportional to the number of unique DXs in the training set <ref> (Yang 1994) </ref>. With a quarter million training DXs, the response time is about 1.5 seconds per new DX when running ExpNet on a SPARCstation 10. <p> Similarly, an analysis of the correlation between accuracy improvement and computation cost helps evaluate the trade-off between categorization accuracy and learning efficiency. A test set was selected to evaluate sampling strategies. Five test sets were collected in our previous evaluation of different categorization methods <ref> (Chute, Yang, & Buntrock 1994) </ref>; none of them were from the training pool. Each set consists of about 1000 DXs arbitrarily chosen from the patient records at the time of that evaluation. <p> of needed instances, and using uncertainty sampling under the control of a globally optimal strategy; * a sampling strategy analysis for ExpNet on different domains, such as MEDLINE documents, the Reuters newswire collection, etc.; and * similar analyses for other statistical learning methods, e.g., the Linear Least Squares Fit mapping <ref> (Yang & Chute 1994) </ref>. Acknowledgement This work is supported at Mayo Foundation in part by NIH Research Grants LM05714 and LM05416.
Reference: <author> Yang, Y. </author> <year> 1994. </year> <title> Expert network: Effective and efficient learning from human decisions in text categorization and retrieval. </title> <booktitle> In 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 94), </booktitle> <pages> 13-22. </pages>
Reference-contexts: Automatic or semi-automatic text categorization tools can help reduce these costs and may also improve accuracy and consistency. Computer-based text categorization technologies include: * naive word-matching <ref> (Chute, Yang, & Buntrock 1994) </ref> which matches categories to text based on the shared words between the text and the names of cat egories; * thesaurus-based matching (Lindberg & Humphreys 1990) which uses lexical links (constructed manually or automatically in advance) to relate a given text to the names or descriptive <p> empirical learning formalisms offer a context sensitive mapping from terms to categories, for example, decision tree methods (Quinlan 1986) (Lewis 1991), Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) <ref> (Yang 1994) </ref>, and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994). While empirical learning holds great potential for high accuracy text categorization, few practical systems have developed due to difficulties in scaling to large problems. <p> for example, decision tree methods (Quinlan 1986) (Lewis 1991), Bayesian belief networks (non-naive ones) (Tzeras & Hartman 1993), neural networks (Schutze, Hull, & Pedersen 1995), nearest neighbor classification methods (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994), and least-squares regression techniques (Fuhr, Hartmanna, & et al. 1991) <ref> (Yang & Chute 1994) </ref>. While empirical learning holds great potential for high accuracy text categorization, few practical systems have developed due to difficulties in scaling to large problems. The MEDLINE database, for example, uses about 17,000 subject categories (Nat 1993) to index its articles. <p> In contrast, nearest neighbor approaches and linear regression methods require less computation, and have been applied to relatively large categorization problems (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) <ref> (Yang 1994) </ref> (Fuhr, Hartmanna, & et al. 1991) (Yang & Chute 1994). Nevertheless, for large problems with tens of thousands of categories, the learning effectiveness and the computational tractability of empirical methods remains a largely unexplored area. <p> In contrast, nearest neighbor approaches and linear regression methods require less computation, and have been applied to relatively large categorization problems (Creecy et al. 1992), (Masand, Linoff, & Waltz 1992) (Yang 1994) (Fuhr, Hartmanna, & et al. 1991) <ref> (Yang & Chute 1994) </ref>. Nevertheless, for large problems with tens of thousands of categories, the learning effectiveness and the computational tractability of empirical methods remains a largely unexplored area. Sampling strategies are important for both the effectiveness and the efficiency of statistical text categorization. <p> ExpNet does not require any off-line training, but does require an on-line search for the k-NN of each new text, which has a time complexity linear in the size of the training set (the number of training texts). See <ref> (Yang 1994) </ref> for additional details and for a discussion on the choice of the parameter k. A nearest neighbor classifier was selected for this study because of its relatively low computation cost and the relative ease with which it scales to large problems. <p> A nearest neighbor classifier was selected for this study because of its relatively low computation cost and the relative ease with which it scales to large problems. Also, nearest neighbor classification performs at least as well as least squares fit <ref> (Yang 1994) </ref> or rule-based approaches (Creecy et al. 1992). A production version of ExpNet was developed at the Mayo Clinic, and is in daily use as a search engine in computer-aided human coding of patient records (Chute, Yang, & Buntrock 1994). <p> Also, nearest neighbor classification performs at least as well as least squares fit (Yang 1994) or rule-based approaches (Creecy et al. 1992). A production version of ExpNet was developed at the Mayo Clinic, and is in daily use as a search engine in computer-aided human coding of patient records <ref> (Chute, Yang, & Buntrock 1994) </ref>. The pool of training data consists of a subset of diagnoses from patient records at Mayo, and the definition phrases of the categories in HICDA-2 (Hospital Adaptation of ICDA, 2nd Edition)(Com 1968). About 2.4 million diagnoses (DXs) are coded each year using the HICDA-2 categories. <p> ExpNet operates as an on-line category searcher. When a coder types in a new DX, ExpNet provides a ranked list of potential categories for the user to select. The on-line response time is proportional to the number of unique DXs in the training set <ref> (Yang 1994) </ref>. With a quarter million training DXs, the response time is about 1.5 seconds per new DX when running ExpNet on a SPARCstation 10. <p> Similarly, an analysis of the correlation between accuracy improvement and computation cost helps evaluate the trade-off between categorization accuracy and learning efficiency. A test set was selected to evaluate sampling strategies. Five test sets were collected in our previous evaluation of different categorization methods <ref> (Chute, Yang, & Buntrock 1994) </ref>; none of them were from the training pool. Each set consists of about 1000 DXs arbitrarily chosen from the patient records at the time of that evaluation. <p> of needed instances, and using uncertainty sampling under the control of a globally optimal strategy; * a sampling strategy analysis for ExpNet on different domains, such as MEDLINE documents, the Reuters newswire collection, etc.; and * similar analyses for other statistical learning methods, e.g., the Linear Least Squares Fit mapping <ref> (Yang & Chute 1994) </ref>. Acknowledgement This work is supported at Mayo Foundation in part by NIH Research Grants LM05714 and LM05416.
References-found: 13

