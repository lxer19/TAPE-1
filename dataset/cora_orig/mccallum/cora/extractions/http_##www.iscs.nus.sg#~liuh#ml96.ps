URL: http://www.iscs.nus.sg/~liuh/ml96.ps
Refering-URL: http://www.ai.mit.edu/people/jude/research/afspaper.html
Root-URL: 
Email: fliuh,rudysg@iscs.nus.sg  
Title: A Probabilistic Approach to Feature Selection A Filter Solution  
Author: Huan Liu Rudy Setiono 
Address: Ridge, Singapore 119260  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Abstract: Feature selection can be defined as a problem of finding a minimum set of M relevant attributes that describes the dataset as well as the original N attributes do, where M N . After examining the problems with both the exhaustive and the heuristic approach to feature selection, this paper proposes a probabilistic approach. The theoretic analysis and the experimental study show that the proposed approach is simple to implement and guaranteed to find the optimal if resources permit. It is also fast in obtaining results and effective in selecting features that improve the performance of a learning algorithm. An on-site application involving huge datasets has been conducted independently. It proves the effectiveness and scalability of the proposed algorithm. Discussed also are various aspects and applications of this fea ture selection algorithm.
Abstract-found: 1
Intro-found: 1
Reference: [ Almuallim and Dietterich, 1994 ] <author> H. Almuallim and T.G. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, </journal> <volume> 69(1-2):279-305, </volume> <month> November </month> <year> 1994. </year> [ <editor> Brassard and Bratley, 1996 ] G. Brassard and P. Bratley. </editor> <booktitle> Fundamentals of Algorithms. </booktitle> <publisher> Prentice Hall, </publisher> <address> New Jersey, </address> <year> 1996. </year>
Reference-contexts: Feature selection has long been the focus of researchers of many fields pattern recognition, statistics, machine learning (see Section 2). Many methods have been proposed. In general, they can be classified into two categories: (1) the filter approach <ref> [ Almuallim and Dietterich, 1994; Kira and Rendell, 1992 ] </ref> , i.e., the feature selector is independent of a learning algorithm and serves as a filter to sieve the irrelevant and/or redundant attributes; and (2) the wrapper approach [ John et al., 1994 ] , i.e., the feature selector works as <p> See for dataset 1 (CorrAL) in Section 4 which is reproduced from [ John et al., 1994 ] . of M attributes. For example, Almuallim and Diet-terich's FOCUS algorithm <ref> [ Almuallim and Dietterich, 1994 ] </ref> starts with an empty feature set and carries out exhaustive search until it finds a minimal combination of features that are sufficient to construct a hypothesis consistent with a given set of examples. It works on binary, noise-free data. <p> It works on binary, noise-free data. As pointed out earlier, its time complexity is O (N M ). They proposed three heuristic algorithms to speed up the searching <ref> [ Almuallim and Dietterich, 1994 ] </ref> . There are many heuristic feature selection algorithms. The Relief algorithm [ Kira and Rendell, 1992 ] assigns a "relevance" weight to each feature, which is meant to denote the relevance of the feature to the target concept. <p> Another method of O (n) can be found in <ref> [ Almuallim and Dietterich, 1994 ] </ref> . 8 Using 10-fold cross validations to obtain accuracy is another factor that increases the time. loops). Another run of LVF on the other attributes will recognize the correct set of features.
Reference: [ Cheeseman et al., 1991 ] <author> P. Cheeseman, B. Kanefsky, and W.M. Taylor. </author> <title> Where the really hard problems are. </title> <booktitle> In Proceedings of IJCAI91, </booktitle> <pages> pages 331-337. </pages> <publisher> Mor-gan Kaufman, </publisher> <year> 1991. </year>
Reference-contexts: Our goal becomes clear, i.e., to have a reasonably fast algorithm that can find M relevant attributes with high probability. Based on the study in line with <ref> [ Cheeseman et al., 1991; Selman, 1995 ] </ref> , this work proposes a probabilistic approach, in particular, a Las Vegas algorithm, that makes probabilistic choices to help guide the search more quickly to find a correct set (or sets) of M attributes.
Reference: [ Devijver and Kittler, 1982 ] <author> P.A. Devijver and J. Kit-tler. </author> <title> Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice Hall International, </publisher> <year> 1982. </year>
Reference: [ John et al., 1994 ] <author> G.H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant feature and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kaufmann Publisher, </publisher> <year> 1994. </year>
Reference-contexts: general, they can be classified into two categories: (1) the filter approach [ Almuallim and Dietterich, 1994; Kira and Rendell, 1992 ] , i.e., the feature selector is independent of a learning algorithm and serves as a filter to sieve the irrelevant and/or redundant attributes; and (2) the wrapper approach <ref> [ John et al., 1994 ] </ref> , i.e., the feature selector works as a wrapper around a learning algorithm relying on which the relevant attributes are determined. <p> active research topic within statistics and pattern recognition [ Narendra and Fukunaga, 1977; Wyse et al., 1980; Devijver and Kittler, 1982 ] , but most work in this area has dealt with linear regression [ Langley, 1994 ] and is under assumptions that do not apply to most learning algorithms <ref> [ John et al., 1994 ] </ref> . Researchers pointed out that the most common assumption is monotonicity, that increasing the number of features can only improve the performance of a learning algorithm 3 . <p> See for dataset 1 (CorrAL) in Section 4 which is reproduced from <ref> [ John et al., 1994 ] </ref> . of M attributes. <p> In other words, redundant attributes help find an optimal solution faster. 4 Experimental Results Two types of datasets are chosen in experiments. One type is artificial data so that the relevant features are known before feature selection is conducted, which includes CorrAL <ref> [ John et al., 1994 ] </ref> , Monks1-3 [ Thrun et al., 1991 ] , and Parity5+5. The other type is real-world data including Credit, Vote, Labor, and Mushroom [ Quinlan, 1993; Murphy and Aha, 1994 ] . <p> The other type is real-world data including Credit, Vote, Labor, and Mushroom [ Quinlan, 1993; Murphy and Aha, 1994 ] . The choice of these datasets simplifies the comparison of this work with some published work. These datasets except Mushroom were used in <ref> [ John et al., 1994 ] </ref> in which comparisons with different methods were described. Nevertheless, the experiments here can alone demonstrate the effectiveness of LVF owing to the analysis given in the previous sections (1, 2 and 3). Artificial Data: 1. <p> were used in <ref> [ John et al., 1994 ] </ref> in which comparisons with different methods were described. Nevertheless, the experiments here can alone demonstrate the effectiveness of LVF owing to the analysis given in the previous sections (1, 2 and 3). Artificial Data: 1. CorrAL The data was designed in [ John et al., 1994 ] . There are six binary features, A 0 ; A 1 ; B 0 ; B 1 ; I; and C. Feature I is irrelevant, feature C is correlated to the class label 75% of the time. <p> X X X X X X X Vote 7 4 7 7 3 3 3 3 16 1 15 15 Credit 44 16 44 41 21 19 21 18 15 3 14 14 Labor X X X X X X X X X X X X The experimental results from <ref> [ John et al., 1994 ] </ref> are reproduced here in Table 3 for a reference purpose. See more details in the paper. <p> Before (Bf) means before feature selection, Forward (Fw) means forward stepwise selection, Backward (Bw) means backward stepwise selection, Relieve (Rl) is a modified version of Relief [ Kira and Rendell, 1992 ] , because of significant variance in the relevance rankings given by Relief <ref> [ John et al., 1994 ] </ref> . 4.3 Applying LVF to huge datasets Feature selection is particularly useful when datasets are huge since many learning algorithms may encounter difficulties. Feature selection can help reduce the dimensionality of the datasets so that more learning algorithms can be chosen to induce rules. <p> Acknowledgments: Thanks H.Y. Lee for the suggestions on an earlier version of this paper; H.L. Ong and A. Pang for providing the results on their applying LVF to huge datasets at Japan - Singapore AI Center; and Y.C. Chew for transforming the graphs in <ref> [ John et al., 1994 ] </ref> into tables. Thanks also go to the two anonymous referees for their valuable comments.
Reference: [ Kira and Rendell, 1992 ] <author> K. Kira and L.A. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 129-134. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Feature selection has long been the focus of researchers of many fields pattern recognition, statistics, machine learning (see Section 2). Many methods have been proposed. In general, they can be classified into two categories: (1) the filter approach <ref> [ Almuallim and Dietterich, 1994; Kira and Rendell, 1992 ] </ref> , i.e., the feature selector is independent of a learning algorithm and serves as a filter to sieve the irrelevant and/or redundant attributes; and (2) the wrapper approach [ John et al., 1994 ] , i.e., the feature selector works as <p> It works on binary, noise-free data. As pointed out earlier, its time complexity is O (N M ). They proposed three heuristic algorithms to speed up the searching [ Almuallim and Dietterich, 1994 ] . There are many heuristic feature selection algorithms. The Relief algorithm <ref> [ Kira and Rendell, 1992 ] </ref> assigns a "relevance" weight to each feature, which is meant to denote the relevance of the feature to the target concept. <p> Relief samples instances randomly from the training set and updates the relevance values based on the difference between the selected instance and the two nearest instances of the same and opposite classes. According to <ref> [ Kira and Rendell, 1992 ] </ref> , Relief assumes two-class classification problems and does not help with redundant features. If most of the given features are relevant to the concept, it would select most of them even though only a fraction are necessary for concept description. <p> See more details in the paper. Before (Bf) means before feature selection, Forward (Fw) means forward stepwise selection, Backward (Bw) means backward stepwise selection, Relieve (Rl) is a modified version of Relief <ref> [ Kira and Rendell, 1992 ] </ref> , because of significant variance in the relevance rankings given by Relief [ John et al., 1994 ] . 4.3 Applying LVF to huge datasets Feature selection is particularly useful when datasets are huge since many learning algorithms may encounter difficulties.
Reference: [ Langley, 1994 ] <author> P. Langley. </author> <title> Selection of relevant features in machine learning. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Relevance. </booktitle> <publisher> AAAI Press, </publisher> <year> 1994. </year> [ <editor> Liu and Setiono, 1995 ] H. Liu and R. </editor> <title> Setiono. Chi2:Feature selection and discretization of numeric attributes. </title> <booktitle> In Proceedings of the 7th IEEE International Conference on Tools with Artificial Intelligence, </booktitle> <year> 1995. </year> <note> http://www.iscs.nus.sg/ ~ liuh/tai95.ps </note>
Reference-contexts: it is not as general as the filter approach because (1) any learning algorithm is biased, choosing relevant attributes according to a particular learning algorithm is equivalent to changing the data to fit the learning algorithm, (2) the wrapper approach is restricted by the time complexity of the learning algorithm <ref> [ Langley, 1994 ] </ref> , and (3) when the dataset is too large, it may cause a problem in running some learning algorithms recall that one of the purposes of applying feature selection is to reduce the data. <p> further work. 2 Previous Approaches and Problems The problem of feature selection has long been an active research topic within statistics and pattern recognition [ Narendra and Fukunaga, 1977; Wyse et al., 1980; Devijver and Kittler, 1982 ] , but most work in this area has dealt with linear regression <ref> [ Langley, 1994 ] </ref> and is under assumptions that do not apply to most learning algorithms [ John et al., 1994 ] . Researchers pointed out that the most common assumption is monotonicity, that increasing the number of features can only improve the performance of a learning algorithm 3 .
Reference: [ Liu and Setiono, 1996 ] <author> H. Liu and R. Setiono. </author> <title> Feature selection and classification a probabilistic wrapper approach. </title> <booktitle> In Proceedings of the 9th International Conference on Industrial and Engineering Applications of AI and ES, </booktitle> <year> 1996. </year> <note> http://www.iscs.nus.sg/ ~ liuh/wrapper.ps </note>
Reference-contexts: Comparing to the wrapper approach, because the criterion is the predictive accuracy, for every newly generated set of features, the criterion must be tested. In other words, the number of criterion testing is significantly larger. Experiments <ref> [ Liu and Setiono, 1996 ] </ref> have confirmed that for the above datasets, the wrapper model normally spends a few hours 8 while the filter model usually takes several minutes on a dedicated SUN SPARC20 for each experiment of LVF on the publically available datasets.
Reference: [ Liu and Wen, 1993 ] <author> H. Liu and W.X. Wen. </author> <title> Concept learning through feature selection. </title> <booktitle> In Proceedings of First Australian and New Zealand Conference on Intelligent Information Systems, </booktitle> <year> 1993. </year>
Reference-contexts: This is prohibitive when N and/or M is large. In practice, heuristic methods are the way out of this exponential computation. Heuristic methods in general make use of low order (first or second) information 1 to approximately estimate the relevance of attributes. Although the heuristic methods work reasonably well <ref> [ Quinlan, 1993; Liu and Wen, 1993 ] </ref> , it is certain that they miss out the attributes with high order correlations, for exam ple, the parity problem.
Reference: [ Modrzejewski, 1993 ] <author> M. Modrzejewski. </author> <title> Feature selection using rough sets theory. </title> <editor> In P.B. Brazdil, editor, </editor> <booktitle> Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 213-226, </pages> <year> 1993. </year>
Reference: [ Murphy and Aha, 1994 ] <author> P.M. Murphy and D.W. Aha. </author> <title> UCI repository of machine learning databases. </title> <note> FTP from ics.uci.edu in the directory pub/machine-learning-databases, </note> <year> 1994. </year>
Reference-contexts: One type is artificial data so that the relevant features are known before feature selection is conducted, which includes CorrAL [ John et al., 1994 ] , Monks1-3 [ Thrun et al., 1991 ] , and Parity5+5. The other type is real-world data including Credit, Vote, Labor, and Mushroom <ref> [ Quinlan, 1993; Murphy and Aha, 1994 ] </ref> . The choice of these datasets simplifies the comparison of this work with some published work. These datasets except Mushroom were used in [ John et al., 1994 ] in which comparisons with different methods were described.
Reference: [ Narendra and Fukunaga, 1977 ] <author> P.M. Narendra and K. Fukunaga. </author> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Trans. on Computer, </journal> <volume> C-26(9):917-922, </volume> <month> September </month> <year> 1977. </year>
Reference: [ Pagallo and Haussler, 1990 ] <author> G. Pagallo and D. Haus-sler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: With the nominal and continuous attributes being mixed, Chi2 considers the latter only since nominal attributes cannot be further discretized. Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 [ Quinlan, 1986 ] , FRINGE <ref> [ Pagallo and Haussler, 1990 ] </ref> and C4.5 [ Quinlan, 1993 ] . The results in [ Al-muallim and Dietterich, 1994 ] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features.
Reference: [ Press et al., 1992 ] <author> W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: Also, the larger number of optima is, the more likely LVF will find M features, in presence of redundant attributes, according to the inconsistency criterion. With a good pseudo random number generator <ref> [ Press et al., 1992 ] </ref> , the selection of an optimal subset of M features can be considered non-replacement experi-ments.
Reference: [ Quinlan and Rivest, 1989 ] <author> J.R. Quinlan and R.L. Rivest. </author> <title> Inferring decision trees using the minimum description length princple. </title> <journal> Information and Computation, </journal> <volume> 80(3), </volume> <year> 1989. </year>
Reference-contexts: C4.5 [ Quinlan, 1993 ] is chosen here because (1) it works well on most data sets as reported by many researchers; and (2) it employs a heuristic to find simplest tree structures <ref> [ Quinlan, 1986; Quinlan and Rivest, 1989; Quinlan, 1995 ] </ref> . 10-fold cross validation is applied and the default settings of C4.5 are used in the experiment. For the experiments of "after feature selection", only the features shown the last column of Table 1 are used.
Reference: [ Quinlan, 1986 ] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: With the nominal and continuous attributes being mixed, Chi2 considers the latter only since nominal attributes cannot be further discretized. Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 <ref> [ Quinlan, 1986 ] </ref> , FRINGE [ Pagallo and Haussler, 1990 ] and C4.5 [ Quinlan, 1993 ] . The results in [ Al-muallim and Dietterich, 1994 ] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features. <p> C4.5 [ Quinlan, 1993 ] is chosen here because (1) it works well on most data sets as reported by many researchers; and (2) it employs a heuristic to find simplest tree structures <ref> [ Quinlan, 1986; Quinlan and Rivest, 1989; Quinlan, 1995 ] </ref> . 10-fold cross validation is applied and the default settings of C4.5 are used in the experiment. For the experiments of "after feature selection", only the features shown the last column of Table 1 are used.
Reference: [ Quinlan, 1993 ] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: This is prohibitive when N and/or M is large. In practice, heuristic methods are the way out of this exponential computation. Heuristic methods in general make use of low order (first or second) information 1 to approximately estimate the relevance of attributes. Although the heuristic methods work reasonably well <ref> [ Quinlan, 1993; Liu and Wen, 1993 ] </ref> , it is certain that they miss out the attributes with high order correlations, for exam ple, the parity problem. <p> With the nominal and continuous attributes being mixed, Chi2 considers the latter only since nominal attributes cannot be further discretized. Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 [ Quinlan, 1986 ] , FRINGE [ Pagallo and Haussler, 1990 ] and C4.5 <ref> [ Quinlan, 1993 ] </ref> . The results in [ Al-muallim and Dietterich, 1994 ] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features. <p> One type is artificial data so that the relevant features are known before feature selection is conducted, which includes CorrAL [ John et al., 1994 ] , Monks1-3 [ Thrun et al., 1991 ] , and Parity5+5. The other type is real-world data including Credit, Vote, Labor, and Mushroom <ref> [ Quinlan, 1993; Murphy and Aha, 1994 ] </ref> . The choice of these datasets simplifies the comparison of this work with some published work. These datasets except Mushroom were used in [ John et al., 1994 ] in which comparisons with different methods were described. <p> The data set consists of 16 features, 300 training instances and 135 test instances. 5. Credit (or CRX) The dataset contains instances for credit card applications. There are 15 features and a Boolean label. The dataset was divided by Quinlan <ref> [ Quinlan, 1993 ] </ref> into 490 training in stances and 200 test instances. 6. Labor The dataset contains instances for acceptable and unacceptable contracts. It is a small dataset with 16 features, a training set of 40 in stances, and a testing set of 17 instances. 7. <p> As mentioned above, it is clear for the artificial datasets whether the relevant features are chosen or not, but for the real-world datasets, indirect evaluation is necessary by checking a learning algorithm's performance before and after feature selection. C4.5 <ref> [ Quinlan, 1993 ] </ref> is chosen here because (1) it works well on most data sets as reported by many researchers; and (2) it employs a heuristic to find simplest tree structures [ Quinlan, 1986; Quinlan and Rivest, 1989; Quinlan, 1995 ] . 10-fold cross validation is applied and the default
Reference: [ Quinlan, 1995 ] <author> J.R. Quinlan. </author> <title> Mdl and categorical theories (continued). </title> <booktitle> In Proceedings of International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: C4.5 [ Quinlan, 1993 ] is chosen here because (1) it works well on most data sets as reported by many researchers; and (2) it employs a heuristic to find simplest tree structures <ref> [ Quinlan, 1986; Quinlan and Rivest, 1989; Quinlan, 1995 ] </ref> . 10-fold cross validation is applied and the default settings of C4.5 are used in the experiment. For the experiments of "after feature selection", only the features shown the last column of Table 1 are used.
Reference: [ Selman, 1995 ] <author> B. Selman. </author> <title> Stochastic search and phase transitions: AI meets physics. In C.S. </title> <editor> Mellish, editor, </editor> <booktitle> Proceedings of IJCAI95, </booktitle> <volume> volume 1, </volume> <pages> pages 998-1002, </pages> <year> 1995. </year>
Reference-contexts: Our goal becomes clear, i.e., to have a reasonably fast algorithm that can find M relevant attributes with high probability. Based on the study in line with <ref> [ Cheeseman et al., 1991; Selman, 1995 ] </ref> , this work proposes a probabilistic approach, in particular, a Las Vegas algorithm, that makes probabilistic choices to help guide the search more quickly to find a correct set (or sets) of M attributes.
Reference: [ Thrun et al., 1991 ] <editor> S.B. Thrun, et al. </editor> <title> The monk's problems: A performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: One type is artificial data so that the relevant features are known before feature selection is conducted, which includes CorrAL [ John et al., 1994 ] , Monks1-3 <ref> [ Thrun et al., 1991 ] </ref> , and Parity5+5. The other type is real-world data including Credit, Vote, Labor, and Mushroom [ Quinlan, 1993; Murphy and Aha, 1994 ] . The choice of these datasets simplifies the comparison of this work with some published work. <p> Both ID3 and C4.5 chose feature C as the root. This is an example of datasets in which if a feature like C is removed, a more accurate tree will result. 2. Monk1, Monk2, Monk3 The datasets were taken from <ref> [ Thrun et al., 1991 ] </ref> . They have six features. The training datasets provided were used for feature selection. Monk1 and Monk3 only need three features to describe the target concepts, but Monk2 requires all the six. The training data of Monk3 contains some noise.
Reference: [ Wyse et al., 1980 ] <author> N. Wyse, R. Dubes, and A.K. Jain. </author> <title> A critical evaluation of intrinsic dimensionality algorithms. In E.S. </title> <editor> Gelsema and Kanal L.N., editors, </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <pages> pages 415-425. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1980. </year>
References-found: 20

