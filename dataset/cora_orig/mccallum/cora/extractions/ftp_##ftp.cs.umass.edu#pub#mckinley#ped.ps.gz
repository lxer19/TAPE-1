URL: ftp://ftp.cs.umass.edu/pub/mckinley/ped.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Title: Interactive Parallel Programming Using the ParaScope Editor  
Author: Ken Kennedy Kathryn M c Kinley Chau-Wen Tseng 
Keyword: Parallel Programming, Parallelism Detection, Dependence Analysis, Transformations, Environments, Interactive, Editor  
Note: This research is supported by the National Science Foundation under grants CDA-8619893 and ASC-8518578, IBM, and the Cray Research Foundation.  
Date: May 24, 1994  
Address: Houston, TX 77251-1892  
Affiliation: Department of Computer Science, Rice University,  
Abstract: The ParaScope project is developing an integrated collection of tools to help scientific programmers implement correct and efficient parallel programs. The centerpiece of this collection is the ParaScope Editor, an intelligent interactive editor for parallel Fortran programs. The ParaScope Editor reveals to users potential hazards of a proposed parallelization in a program. It also provides a variety of powerful interactive program transformations that have been shown useful in converting programs to parallel form. In addition, the ParaScope Editor supports general user editing through a hybrid text and structure editing facility that incrementally analyzes the modified program for potential hazards. The ParaScope Editor is a new kind of program construction tool one that not only manages text, but also presents the user with information about the correctness of the parallel program under development. As such, it can support an exploratory programming style in which users get immediate feedback on their various strategies for parallelization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> The MIMDizer: </author> <title> A new parallelization tool. </title> <booktitle> The Spang Robinson Report on Supercom puting and Parallel Processing, </booktitle> <volume> 4(1) </volume> <pages> 2-6, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Ped is distinguished by its large collection of transformations, the expert guidance provided for each transformation, and the quality of its program analysis and user interface. Below we briefly describe Sigmacs [48], Pat [50], MIMDizer <ref> [1] </ref>, and Superb [57], placing emphasis on their unique features. Sigmacs is an interactive emacs-based programmable parallelizer in the Faust programming environment. It utilizes dependence information fetched from a project database maintained by the database server. Sigmacs displays dependences and provides some interactive program transformations.
Reference: [2] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: The success of automatic vectorization has led users to seek a similarly elegant software solution to the problem of programming parallel computers. A substantial amount of research has been conducted on whether sequential Fortran 77 programs can be automatically converted without user assistance to execute on shared-memory parallel machines <ref> [2, 3, 7, 8, 42, 49] </ref>. The results of this research have been both promising and disappointing. Although such systems can successfully parallelize many interesting programs, they have not established a level of success that will make it possible to avoid explicit parallel programming by the user.
Reference: [3] <author> F. Allen, M. Burke, P. Charles, J. Ferrante, W. Hsieh, and V. Sarkar. </author> <title> A framework for detecting useful parallelism. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: The success of automatic vectorization has led users to seek a similarly elegant software solution to the problem of programming parallel computers. A substantial amount of research has been conducted on whether sequential Fortran 77 programs can be automatically converted without user assistance to execute on shared-memory parallel machines <ref> [2, 3, 7, 8, 42, 49] </ref>. The results of this research have been both promising and disappointing. Although such systems can successfully parallelize many interesting programs, they have not established a level of success that will make it possible to avoid explicit parallel programming by the user.
Reference: [4] <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: It is used in preparation for loop fusion. * Loop fusion can increase the granularity of parallel regions by fusing two contiguous loops when dependences are not violated <ref> [4, 43] </ref>. * Statement interchange interchanges two adjacent independent statements. 5.2 Dependence Breaking Transformations The following transformations can be used to break specific dependences that inhibit parallelism. Often if a particular dependence can be eliminated, the safe application of other transformations is enabled. <p> It is useful for breaking dependences which arise on the first or last k iterations of the loop <ref> [4] </ref>. * Loop splitting, or index set splitting, separates the iteration space of one loop into two loops, where the user specifies at which iteration to split. <p> It improves the performance of the program by reducing the number of memory accesses required. * Unrolling decreases loop overhead and increases potential candidates for scalar re placement by unrolling the body of a loop <ref> [4, 38] </ref>. * Unroll and Jam increases the potential candidates for scalar replacement and pipelin ing by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops [15, 16, 38]. 5.4 Miscellaneous Transformations Finally Ped has a few miscellaneous transformations. * Sequential $ Parallel
Reference: [5] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Pro gram Transformations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: Dependences are also characterized by either being loop-carried or loop-independent <ref> [5, 9] </ref>. Consider the following: DO I = 2, N S 2 ... = A (I) ENDDO The dependence, S 1 ffiS 2 , is a loop-independent true dependence, and it exists regardless of the loop constructs surrounding it. <p> This prevents the loop from being run in parallel without explicit synchronization. When there are nested loops, the level of any carried dependence is the outermost loop on which it first arises <ref> [5, 9] </ref>. 3.2 Dependence Analysis A major strength of Ped is its ability to display dependence information and utilize it to guide structured transformations. Precise analysis of both control and data dependences in the program is thus very important.
Reference: [6] <author> J. R. Allen, D. Baumgartner, K. Kennedy, and A. Porterfield. </author> <title> PTOOL: A semi-automatic parallel programming assistant. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, August 1986. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: use of broad directives like this is unsound because of the danger that the user will discard real dependences with the false ones, leading to errors that are hard to detect. 2 To address this problem, we developed Ptool, an interactive browser which displays the dependences present in a program <ref> [6] </ref>. Within Ptool, the user selects a specific loop and is presented with what the analyzer believes are the dependences preventing the parallelization of that loop. The user may then confirm or delete these dependences based on their knowledge of the underlying algorithms of the program. <p> 2 Background Ped is being developed in the context of the ParaScope project [17], a parallel programming environment based on the confluence of three major research efforts at Rice University: IR n , the Rice Programming Environment [23]; PFC, a Parallel Fortran Converter [8]; and Ptool, a parallel programming assistant <ref> [6] </ref>. All of these are major contributors to the ideas behind the ParaScope Editor, so we begin with a short description of each. Figure 1 illustrates the evolution of ParaScope. 2.1 The IR n Programming Environment Ped enjoys many advantages because it is integrated into the IR n Programming Environment.
Reference: [7] <author> J. R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific pro grams for parallel execution. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Munich, Germany, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: The success of automatic vectorization has led users to seek a similarly elegant software solution to the problem of programming parallel computers. A substantial amount of research has been conducted on whether sequential Fortran 77 programs can be automatically converted without user assistance to execute on shared-memory parallel machines <ref> [2, 3, 7, 8, 42, 49] </ref>. The results of this research have been both promising and disappointing. Although such systems can successfully parallelize many interesting programs, they have not established a level of success that will make it possible to avoid explicit parallel programming by the user.
Reference: [8] <author> J. R. Allen and K. Kennedy. </author> <title> PFC: A program to convert Fortran to parallel form. </title> <editor> In K. Hwang, editor, </editor> <booktitle> Supercomputers: Design and Applications, </booktitle> <pages> pages 186-203. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Silver Spring, MD, </address> <year> 1984. </year>
Reference-contexts: The success of automatic vectorization has led users to seek a similarly elegant software solution to the problem of programming parallel computers. A substantial amount of research has been conducted on whether sequential Fortran 77 programs can be automatically converted without user assistance to execute on shared-memory parallel machines <ref> [2, 3, 7, 8, 42, 49] </ref>. The results of this research have been both promising and disappointing. Although such systems can successfully parallelize many interesting programs, they have not established a level of success that will make it possible to avoid explicit parallel programming by the user. <p> research, and Section 8 offers some conclusions. 2 Background Ped is being developed in the context of the ParaScope project [17], a parallel programming environment based on the confluence of three major research efforts at Rice University: IR n , the Rice Programming Environment [23]; PFC, a Parallel Fortran Converter <ref> [8] </ref>; and Ptool, a parallel programming assistant [6]. All of these are major contributors to the ideas behind the ParaScope Editor, so we begin with a short description of each.
Reference: [9] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> Octo-ber </month> <year> 1987. </year>
Reference-contexts: It is written in PL/1 and runs on an IBM mainframe. In recent years the project has focused on the more difficult problem of automatically parallelizing sequential code. PFC performs data dependence analysis <ref> [9, 13, 14, 56] </ref>, interprocedural side effect analysis [25] and interprocedural constant propagation [18]. More recently an implementation of regular section analysis [20, 32], which determines the subarrays affected by procedure calls, has been completed. <p> Dependences are also characterized by either being loop-carried or loop-independent <ref> [5, 9] </ref>. Consider the following: DO I = 2, N S 2 ... = A (I) ENDDO The dependence, S 1 ffiS 2 , is a loop-independent true dependence, and it exists regardless of the loop constructs surrounding it. <p> This prevents the loop from being run in parallel without explicit synchronization. When there are nested loops, the level of any carried dependence is the outermost loop on which it first arises <ref> [5, 9] </ref>. 3.2 Dependence Analysis A major strength of Ped is its ability to display dependence information and utilize it to guide structured transformations. Precise analysis of both control and data dependences in the program is thus very important. <p> A data dependence exists between these references only if the same location may be accessed 9 by both references. Dependence testing is the process of discovering and characterizing data dependences between array references. It is a difficult problem which has been the subject of extensive research <ref> [9, 13, 14, 56] </ref>. Conservative data dependence analysis requires that if a dependence cannot be disproven, it must be assumed to exist. False dependences result when conservative dependences do not actually exist. The most important objective of the dependence analyzer is to minimize false dependences through precise analysis. <p> When loop interchange is safe, it can be used to adjust the granularity of parallel loops <ref> [9, 38, 56] </ref>. 4 The details of Ped's implementation of several of these transformations appear in [38]. 19 * Loop skewing adjusts the iteration space of two perfectly nested loops by shifting the work per iteration in order to expose parallelism. <p> It breaks output and anti dependences which may be inhibiting parallelism [41]. * Array renaming, also known as node splitting [41], is used to break anti dependences by copying the source of an anti dependence into a newly introduced temporary array and renaming the sink to the new array <ref> [9] </ref>. Loop distribution may then be used to separate the copying statement into a separate loop, allowing both loops to be parallelized. * Loop peeling peels off the first or last k iterations of a loop as specified by the user. <p> For example, if do i = 20 1, 100 is split at 50, the following two loops result: do i = 1, 50 and do i = 51, 100. Loop splitting is useful in breaking crossing dependences, dependences that cross a specific iteration <ref> [9] </ref>. 5.3 Memory Optimizing Transformations The following transformations adjust a program's balance between computations and memory accesses to make better use of the memory hierarchy and functional pipelines.
Reference: [10] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: It is an improvement over dependence browsers because it supports incremental change while the user is reviewing potential problems with the proposed parallelization. Ped has also proven to be a useful basis for the development of several other advanced tools, including a compiler [34] and data decomposition tool <ref> [10, 11] </ref> for distributed-memory machines, as well an on-the-fly access anomaly detection system for shared-memory machines [35].
Reference: [11] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: It is an improvement over dependence browsers because it supports incremental change while the user is reviewing potential problems with the proposed parallelization. Ped has also proven to be a useful basis for the development of several other advanced tools, including a compiler [34] and data decomposition tool <ref> [10, 11] </ref> for distributed-memory machines, as well an on-the-fly access anomaly detection system for shared-memory machines [35].
Reference: [12] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: To improve the precision of interprocedural analysis, array access patterns can be summarized in terms of regular sections or data access descriptors. These abstractions describe subsections of arrays such as rows, columns, and rectangles that can be quickly intersected to determine whether dependences exist <ref> [12, 20, 32] </ref>. By distinguishing the portion of each array affected by a procedure, regular sections provide precise analysis of dependences for loops containing procedure calls. 3.4 Synchronization Analysis A dependence is preserved if synchronization guarantees that the endpoints of the dependence are always executed in the correct order.
Reference: [13] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year> <month> 29 </month>
Reference-contexts: It is written in PL/1 and runs on an IBM mainframe. In recent years the project has focused on the more difficult problem of automatically parallelizing sequential code. PFC performs data dependence analysis <ref> [9, 13, 14, 56] </ref>, interprocedural side effect analysis [25] and interprocedural constant propagation [18]. More recently an implementation of regular section analysis [20, 32], which determines the subarrays affected by procedure calls, has been completed. <p> A data dependence exists between these references only if the same location may be accessed 9 by both references. Dependence testing is the process of discovering and characterizing data dependences between array references. It is a difficult problem which has been the subject of extensive research <ref> [9, 13, 14, 56] </ref>. Conservative data dependence analysis requires that if a dependence cannot be disproven, it must be assumed to exist. False dependences result when conservative dependences do not actually exist. The most important objective of the dependence analyzer is to minimize false dependences through precise analysis.
Reference: [14] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: It is written in PL/1 and runs on an IBM mainframe. In recent years the project has focused on the more difficult problem of automatically parallelizing sequential code. PFC performs data dependence analysis <ref> [9, 13, 14, 56] </ref>, interprocedural side effect analysis [25] and interprocedural constant propagation [18]. More recently an implementation of regular section analysis [20, 32], which determines the subarrays affected by procedure calls, has been completed. <p> A data dependence exists between these references only if the same location may be accessed 9 by both references. Dependence testing is the process of discovering and characterizing data dependences between array references. It is a difficult problem which has been the subject of extensive research <ref> [9, 13, 14, 56] </ref>. Conservative data dependence analysis requires that if a dependence cannot be disproven, it must be assumed to exist. False dependences result when conservative dependences do not actually exist. The most important objective of the dependence analyzer is to minimize false dependences through precise analysis.
Reference: [15] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: order of the iterations is unchanged, but used in concert with loop interchange the iteration space may be tiled [55] to utilize memory bandwidth and cache more effectively [24]. * Scalar replacement takes array references with consistent dependences and replaces them with scalar temporaries that may be allocated into registers <ref> [15] </ref>. <p> overhead and increases potential candidates for scalar re placement by unrolling the body of a loop [4, 38]. * Unroll and Jam increases the potential candidates for scalar replacement and pipelin ing by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops <ref> [15, 16, 38] </ref>. 5.4 Miscellaneous Transformations Finally Ped has a few miscellaneous transformations. * Sequential $ Parallel converts a sequential DO loop into a parallel loop, and vice versa. * Statement addition adds an assignment statement. * Statement deletion deletes an assignment statement. 21 * Preserved dependence? indicates whether the current
Reference: [16] <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(4) </volume> <pages> 334-358, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: overhead and increases potential candidates for scalar re placement by unrolling the body of a loop [4, 38]. * Unroll and Jam increases the potential candidates for scalar replacement and pipelin ing by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops <ref> [15, 16, 38] </ref>. 5.4 Miscellaneous Transformations Finally Ped has a few miscellaneous transformations. * Sequential $ Parallel converts a sequential DO loop into a parallel loop, and vice versa. * Statement addition adds an assignment statement. * Statement deletion deletes an assignment statement. 21 * Preserved dependence? indicates whether the current
Reference: [17] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A par allel programming environment. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: A survey of the program transformations provided by Ped appears in Section 5. Issues involving interactive programming in Ped are discussed in Section 6. Section 7 summarizes related research, and Section 8 offers some conclusions. 2 Background Ped is being developed in the context of the ParaScope project <ref> [17] </ref>, a parallel programming environment based on the confluence of three major research efforts at Rice University: IR n , the Rice Programming Environment [23]; PFC, a Parallel Fortran Converter [8]; and Ptool, a parallel programming assistant [6].
Reference: [18] <author> D. Callahan, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propa gation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: It is written in PL/1 and runs on an IBM mainframe. In recent years the project has focused on the more difficult problem of automatically parallelizing sequential code. PFC performs data dependence analysis [9, 13, 14, 56], interprocedural side effect analysis [25] and interprocedural constant propagation <ref> [18] </ref>. More recently an implementation of regular section analysis [20, 32], which determines the subarrays affected by procedure calls, has been completed. This analysis significantly improves the precision of PFC's dependence graph, because arrays are no longer treated as single units across procedure calls. <p> In-terprocedural analysis is required so that worst case assumptions need not be made when calls are encountered. Interprocedural analysis provided in ParaScope discovers aliasing, side effects such as variable definitions and uses, and interprocedural constants <ref> [18, 25] </ref>. Unfortunately, improvements to dependence analysis are limited because arrays are treated as 10 monolithic objects, and it is not possible to determine whether two references to an array actually access the same memory location.
Reference: [19] <author> D. Callahan, J. Dongarra, and D. Levine. </author> <title> Vectorizing compilers: A test suite and results. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <address> Orlando, FL, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: PFC has had many successes. It was influential in the design of several commercial vector-ization systems [47], and it has successfully found near-optimal parallelism for a selected set of test cases <ref> [19] </ref>. However, it has not been successful enough to obviate the need for explicit parallel programming. In large complex loops, it tends to find many spurious race conditions, any one of which is sufficient to inhibit parallelization.
Reference: [20] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel pro gramming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: In recent years the project has focused on the more difficult problem of automatically parallelizing sequential code. PFC performs data dependence analysis [9, 13, 14, 56], interprocedural side effect analysis [25] and interprocedural constant propagation [18]. More recently an implementation of regular section analysis <ref> [20, 32] </ref>, which determines the subarrays affected by procedure calls, has been completed. This analysis significantly improves the precision of PFC's dependence graph, because arrays are no longer treated as single units across procedure calls. <p> To improve the precision of interprocedural analysis, array access patterns can be summarized in terms of regular sections or data access descriptors. These abstractions describe subsections of arrays such as rows, columns, and rectangles that can be quickly intersected to determine whether dependences exist <ref> [12, 20, 32] </ref>. By distinguishing the portion of each array affected by a procedure, regular sections provide precise analysis of dependences for loops containing procedure calls. 3.4 Synchronization Analysis A dependence is preserved if synchronization guarantees that the endpoints of the dependence are always executed in the correct order.
Reference: [21] <author> D. Callahan, K. Kennedy, and J. Subhlok. </author> <title> Analysis of event synchronization in a par allel programming tool. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Otherwise, there may exist race conditions. Establishing that the order specified by certain dependences will always be maintained has been proven Co-NP-hard. However, efficient techniques have been developed to identify dependences preserved in parallel loops by post and wait event synchronization <ref> [21, 22, 52] </ref>. Ped utilizes these techniques in a transformation that determines whether a particular dependence is preserved by event synchronization in a loop. Other forms of synchronization are not currently handled in Ped.
Reference: [22] <author> D. Callahan and J. Subhlok. </author> <title> Static analysis of low-level synchronization. </title> <booktitle> In Proceedings of the ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <address> Madison, WI, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Otherwise, there may exist race conditions. Establishing that the order specified by certain dependences will always be maintained has been proven Co-NP-hard. However, efficient techniques have been developed to identify dependences preserved in parallel loops by post and wait event synchronization <ref> [21, 22, 52] </ref>. Ped utilizes these techniques in a transformation that determines whether a particular dependence is preserved by event synchronization in a loop. Other forms of synchronization are not currently handled in Ped.
Reference: [23] <author> A. Carle, K. Cooper, R. Hood, L. Torczon K. Kennedy, and S. Warren. </author> <title> A practical environment for scientific programming. </title> <journal> Computer, </journal> 20(11) 75-89, November 1987. 
Reference-contexts: Section 7 summarizes related research, and Section 8 offers some conclusions. 2 Background Ped is being developed in the context of the ParaScope project [17], a parallel programming environment based on the confluence of three major research efforts at Rice University: IR n , the Rice Programming Environment <ref> [23] </ref>; PFC, a Parallel Fortran Converter [8]; and Ptool, a parallel programming assistant [6]. All of these are major contributors to the ideas behind the ParaScope Editor, so we begin with a short description of each.
Reference: [24] <author> S. Carr and K. Kennedy. </author> <title> Blocking linear algebra codes for memory hierarchies. </title> <booktitle> In Pro ceedings of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Chicago, IL, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Used alone the order of the iterations is unchanged, but used in concert with loop interchange the iteration space may be tiled [55] to utilize memory bandwidth and cache more effectively <ref> [24] </ref>. * Scalar replacement takes array references with consistent dependences and replaces them with scalar temporaries that may be allocated into registers [15].
Reference: [25] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the IR n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: It is written in PL/1 and runs on an IBM mainframe. In recent years the project has focused on the more difficult problem of automatically parallelizing sequential code. PFC performs data dependence analysis [9, 13, 14, 56], interprocedural side effect analysis <ref> [25] </ref> and interprocedural constant propagation [18]. More recently an implementation of regular section analysis [20, 32], which determines the subarrays affected by procedure calls, has been completed. This analysis significantly improves the precision of PFC's dependence graph, because arrays are no longer treated as single units across procedure calls. <p> In-terprocedural analysis is required so that worst case assumptions need not be made when calls are encountered. Interprocedural analysis provided in ParaScope discovers aliasing, side effects such as variable definitions and uses, and interprocedural constants <ref> [18, 25] </ref>. Unfortunately, improvements to dependence analysis are limited because arrays are treated as 10 monolithic objects, and it is not possible to determine whether two references to an array actually access the same memory location.
Reference: [26] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1989. </year> <month> 30 </month>
Reference-contexts: Scalar dataflow analysis constructs the control flow graph and postdominator tree for both structured and unstructured programs. Dominance frontiers are computed for each scalar variable and used to build the static single assignment (SSA) graph for each procedure <ref> [26] </ref>. A coarse dependence graph for arrays is constructed by connecting fDefsg with fDefs [ Usesg for array variables in each loop nest in the program. Symbolic analysis determines and compares the values of expressions in programs.
Reference: [27] <author> R. Cytron, J. Ferrante, and V. Sarkar. </author> <title> Experiences using control dependence in PTRAN. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: A control dependence, S 1 ffi c S 2 , indicates that the execution of S 1 directly determines if S 2 will be executed at all. The following formal definitions of control dependence and the post-dominance relation are taken from the literature <ref> [27, 28] </ref>. Def: x is post-dominated by y in G f if every path from x to stop contains y, where stop is the exit node of G f .
Reference: [28] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: More recently an implementation of regular section analysis [20, 32], which determines the subarrays affected by procedure calls, has been completed. This analysis significantly improves the precision of PFC's dependence graph, because arrays are no longer treated as single units across procedure calls. PFC also performs control dependence analysis <ref> [28] </ref>, which describes when the execution of one statement directly determines if another will execute. The analyses performed in PFC result in a statement dependence graph that specifies a 5 partial ordering on the statements in the program. <p> A control dependence, S 1 ffi c S 2 , indicates that the execution of S 1 directly determines if S 2 will be executed at all. The following formal definitions of control dependence and the post-dominance relation are taken from the literature <ref> [27, 28] </ref>. Def: x is post-dominated by y in G f if every path from x to stop contains y, where stop is the exit node of G f .
Reference: [29] <author> K. Fletcher, K. Kennedy, K. S. M c Kinley, and S. Warren. </author> <title> The ParaScope Editor: User interface goals. </title> <type> Technical Report TR90-113, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: The user then reanalyzes the program. In the current implementation, the parallel loop will remain parallel, but any deleted dependences will reappear and, as experience has shown, annoy users. As a result, a more sophisticated mechanism is planned <ref> [29] </ref>. In the future, edges that are deleted by users will be marked, rather than removed from the dependence graph. Additionally, the time, date, user, and an optional user-supplied explanation will be recorded with any assertions.
Reference: [30] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Fast yet exact tests are applied to simple separable subscripts. More powerful but expensive tests are held in reserve for the remaining subscripts. In most cases, results can be merged for an exact test <ref> [30] </ref>. Ped also characterizes all dependences by the flow of values with respect to the enclosing loops. This information is represented as a hybrid distance/direction vector, with one element per enclosing loop. Each element in the vector represents the distance or direction of the flow of values on that loop.
Reference: [31] <author> V. Guarna, D. Gannon, D. Jablonowski, A. Malony, and Y. Gaur. </author> <title> Faust: An integrated environment for parallel programming. </title> <journal> IEEE Software, </journal> <volume> 6(4) </volume> <pages> 20-27, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Sigmacs displays dependences and provides some interactive program transformations. Work is in progress to support automatic updating of dependence information after statement insertion and deletion. Faust can compute and display call and process graphs that may be animated dynamically at run-time <ref> [31] </ref>. Each node in a process graph represents a task or a process, which is a separate entity running in parallel. Faust also provides performance analysis and prediction tools for parallel programs. Pat can analyze programs containing general parallel constructs.
Reference: [32] <author> P. Havlak and K. Kennedy. </author> <title> Experience with interprocedural analysis of array side effects. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: In recent years the project has focused on the more difficult problem of automatically parallelizing sequential code. PFC performs data dependence analysis [9, 13, 14, 56], interprocedural side effect analysis [25] and interprocedural constant propagation [18]. More recently an implementation of regular section analysis <ref> [20, 32] </ref>, which determines the subarrays affected by procedure calls, has been completed. This analysis significantly improves the precision of PFC's dependence graph, because arrays are no longer treated as single units across procedure calls. <p> To improve the precision of interprocedural analysis, array access patterns can be summarized in terms of regular sections or data access descriptors. These abstractions describe subsections of arrays such as rows, columns, and rectangles that can be quickly intersected to determine whether dependences exist <ref> [12, 20, 32] </ref>. By distinguishing the portion of each array affected by a procedure, regular sections provide precise analysis of dependences for loops containing procedure calls. 3.4 Synchronization Analysis A dependence is preserved if synchronization guarantees that the endpoints of the dependence are always executed in the correct order.
Reference: [33] <author> L. Henderson, R. Hiromoto, O. Lubeck, and M. Simmons. </author> <title> On the use of diagnostic dependency-analysis tools in parallel programming: Experiences using PTOOL. </title> <journal> The Journal of Supercomputing, </journal> <volume> 4 </volume> <pages> 83-96, </pages> <year> 1990. </year>
Reference-contexts: The user may then confirm or delete these dependences based on their knowledge of the underlying algorithms of the program. Although Ptool is effective at helping users understand the parallelism available in a given Fortran program, it suffers because it is a browser rather than an editor <ref> [33] </ref>. When presented with dependences, the user frequently sees a transformation that can eliminate a collection of dependences, only to be frustrated because performing that transformation requires moving to an editor, making the change, and resubmitting the program for dependence analysis. <p> To assist users in determining if loops may be run in parallel, Ptool also classifies variables as shared or private. When examining large scientific programs, users frequently found an overwhelming num 6 ber of dependences in large loops, including spurious dependences due to imprecise dependence analysis <ref> [33] </ref>. To ameliorate this problem, a number of improvements were made in PFC's dependence analysis. In addition, a dependence filtering mechanism was incorporated in the Ptool browser which could answers complex queries about dependences based on their characteristics.
Reference: [34] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: It is an improvement over dependence browsers because it supports incremental change while the user is reviewing potential problems with the proposed parallelization. Ped has also proven to be a useful basis for the development of several other advanced tools, including a compiler <ref> [34] </ref> and data decomposition tool [10, 11] for distributed-memory machines, as well an on-the-fly access anomaly detection system for shared-memory machines [35].
Reference: [35] <author> R. Hood, K. Kennedy, and J. Mellor-Crummey. </author> <title> Parallel program debugging with on-the fly anomaly detection. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Ped has also proven to be a useful basis for the development of several other advanced tools, including a compiler [34] and data decomposition tool [10, 11] for distributed-memory machines, as well an on-the-fly access anomaly detection system for shared-memory machines <ref> [35] </ref>.
Reference: [36] <author> IBM. </author> <title> Parallel Fortran Language and Library Reference, </title> <booktitle> first edition, </booktitle> <month> February </month> <year> 1988. </year> <title> Document Number SC23-0431-0. </title>
Reference-contexts: It also permits arbitrary program changes through familiar editing operations. Ped supports several styles of parallel programming. It can be used to develop new parallel codes, convert sequential codes into parallel form, or analyze existing parallel programs. In particular, Ped currently accepts and generates Fortran 77, IBM parallel Fortran <ref> [36] </ref>, and parallel Fortran for the Sequent Symmetry [46]. The Parallel Computing Forum is developing PCF Fortran [44]. PCF Fortran defines a set of parallel extensions that a large number of manufacturers are 3 committed to accepting, obviating the current need to support numerous Fortran dialects.
Reference: [37] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Loop distribution with arbitrary control flow. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: It is used to separate statements which may be parallelized from those that must be executed sequentially <ref> [37, 38, 40] </ref>. The partitioning of the statements is tuned for vector or parallel hardware as specified by the user. * Loop interchange interchanges the headers of two perfectly nested loops, changing the order in which the iteration space is traversed.
Reference: [38] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaS cope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The dependences may then be displayed to the user, or analyzed to provide expert advice with respect to some transformation. The details of the dependence analysis techniques in Ped are described elsewhere <ref> [38] </ref>. 3.3 Interprocedural Analysis The presence of procedure calls complicates the process of detecting data dependences. In-terprocedural analysis is required so that worst case assumptions need not be made when calls are encountered. <p> It is used to separate statements which may be parallelized from those that must be executed sequentially <ref> [37, 38, 40] </ref>. The partitioning of the statements is tuned for vector or parallel hardware as specified by the user. * Loop interchange interchanges the headers of two perfectly nested loops, changing the order in which the iteration space is traversed. <p> When loop interchange is safe, it can be used to adjust the granularity of parallel loops <ref> [9, 38, 56] </ref>. 4 The details of Ped's implementation of several of these transformations appear in [38]. 19 * Loop skewing adjusts the iteration space of two perfectly nested loops by shifting the work per iteration in order to expose parallelism. <p> When loop interchange is safe, it can be used to adjust the granularity of parallel loops [9, 38, 56]. 4 The details of Ped's implementation of several of these transformations appear in <ref> [38] </ref>. 19 * Loop skewing adjusts the iteration space of two perfectly nested loops by shifting the work per iteration in order to expose parallelism. When possible, Ped computes and suggests the optimal skew degree. <p> When possible, Ped computes and suggests the optimal skew degree. Loop skewing may be used with loop interchange in Ped to perform the wavefront method <ref> [38, 54] </ref>. * Loop reversal reverses the order of execution of loop iterations. * Loop adjusting adjusts the upper and lower bounds of a loop by a constant. <p> It improves the performance of the program by reducing the number of memory accesses required. * Unrolling decreases loop overhead and increases potential candidates for scalar re placement by unrolling the body of a loop <ref> [4, 38] </ref>. * Unroll and Jam increases the potential candidates for scalar replacement and pipelin ing by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops [15, 16, 38]. 5.4 Miscellaneous Transformations Finally Ped has a few miscellaneous transformations. * Sequential $ Parallel <p> overhead and increases potential candidates for scalar re placement by unrolling the body of a loop [4, 38]. * Unroll and Jam increases the potential candidates for scalar replacement and pipelin ing by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops <ref> [15, 16, 38] </ref>. 5.4 Miscellaneous Transformations Finally Ped has a few miscellaneous transformations. * Sequential $ Parallel converts a sequential DO loop into a parallel loop, and vice versa. * Statement addition adds an assignment statement. * Statement deletion deletes an assignment statement. 21 * Preserved dependence? indicates whether the current <p> If there are no errors, dependence analysis is performed. Ped's analysis may be incremental when the scope of an edit is contained within a loop nest or is an insertion or deletion of a simple assignment statement. The details of incremental analysis after edits and transformations are discussed elsewhere <ref> [38] </ref>. The purpose of an edit may be error correction, new code development, or just to rearrange existing code. Unlike with transformations, where the correctness of pre-existing source is assumed, Ped does not know the intent of an edit.
Reference: [39] <author> U. Kremer, H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Advanced tools and techniques for automatic parallelization. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 387-393, </pages> <year> 1988. </year>
Reference-contexts: Superb also possesses a set of interactive program transformations designed to exploit data parallelism for distributed-memory machines. Algorithms are described for the incremental update of use-def and def-use chains following structured program transformations <ref> [39] </ref>. 8 Conclusions Programming for explicitly parallel machines is much more difficult than sequential programming. If we are to encourage scientists to use these machines, we will need to provide new tools that have a level of sophistication commensurate with the difficulty of the task.
Reference: [40] <author> D. Kuck. </author> <title> The Structure of Computers and Computations, Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1978. </year> <month> 31 </month>
Reference-contexts: A data dependence, S 1 ffiS 2 , indicates that S 1 and S 2 use or modify a common variable in a way that requires their execution order to be preserved. There are three types of data dependence <ref> [40] </ref>: * True (flow) dependence occurs when S 1 stores a variable S 2 later uses. * Anti dependence occurs when S 1 uses a variable that S 2 later stores. * Output dependence occurs when S 1 stores a variable that S 2 later stores. <p> It is used to separate statements which may be parallelized from those that must be executed sequentially <ref> [37, 38, 40] </ref>. The partitioning of the statements is tuned for vector or parallel hardware as specified by the user. * Loop interchange interchanges the headers of two perfectly nested loops, changing the order in which the iteration space is traversed.
Reference: [41] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe. </author> <title> Analysis and transformation of pro grams for parallel computation. </title> <booktitle> In Proceedings of COMPSAC 80, the 4th International Computer Software and Applications Conference, </booktitle> <pages> pages 709-715, </pages> <address> Chicago, IL, </address> <month> October </month> <year> 1980. </year>
Reference-contexts: Of course, if all the dependences carried on a loop are eliminated, the loop may then be run in parallel. * Scalar expansion makes a scalar variable into a one-dimensional array. It breaks output and anti dependences which may be inhibiting parallelism <ref> [41] </ref>. * Array renaming, also known as node splitting [41], is used to break anti dependences by copying the source of an anti dependence into a newly introduced temporary array and renaming the sink to the new array [9]. <p> It breaks output and anti dependences which may be inhibiting parallelism <ref> [41] </ref>. * Array renaming, also known as node splitting [41], is used to break anti dependences by copying the source of an anti dependence into a newly introduced temporary array and renaming the sink to the new array [9].
Reference: [42] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe. </author> <title> The structure of an advanced retar getable vectorizer. In Supercomputers: </title> <booktitle> Design and Applications, </booktitle> <pages> pages 163-178. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Silver Spring, MD, </address> <year> 1984. </year>
Reference-contexts: The success of automatic vectorization has led users to seek a similarly elegant software solution to the problem of programming parallel computers. A substantial amount of research has been conducted on whether sequential Fortran 77 programs can be automatically converted without user assistance to execute on shared-memory parallel machines <ref> [2, 3, 7, 8, 42, 49] </ref>. The results of this research have been both promising and disappointing. Although such systems can successfully parallelize many interesting programs, they have not established a level of success that will make it possible to avoid explicit parallel programming by the user.
Reference: [43] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: It is used in preparation for loop fusion. * Loop fusion can increase the granularity of parallel regions by fusing two contiguous loops when dependences are not violated <ref> [4, 43] </ref>. * Statement interchange interchanges two adjacent independent statements. 5.2 Dependence Breaking Transformations The following transformations can be used to break specific dependences that inhibit parallelism. Often if a particular dependence can be eliminated, the safe application of other transformations is enabled.
Reference: [44] <author> B. Leasure, </author> <title> editor. PCF Fortran: Language Definition, version 3.1. The Parallel Com puting Forum, </title> <address> Champaign, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: It can be used to develop new parallel codes, convert sequential codes into parallel form, or analyze existing parallel programs. In particular, Ped currently accepts and generates Fortran 77, IBM parallel Fortran [36], and parallel Fortran for the Sequent Symmetry [46]. The Parallel Computing Forum is developing PCF Fortran <ref> [44] </ref>. PCF Fortran defines a set of parallel extensions that a large number of manufacturers are 3 committed to accepting, obviating the current need to support numerous Fortran dialects. These extensions will be supported when they emerge. The remainder of this paper is organized as follows.
Reference: [45] <author> H. Levesque and J. Williamson. </author> <title> A Guidebook to Fortran on Supercomputers. </title> <publisher> Harcourt Brace Jovanovich, </publisher> <address> San Diego, CA, </address> <year> 1989. </year>
Reference-contexts: In these situations, the user is often able to solve the problem immediately when presented with the specific dependence in question. Unfortunately, in a completely automatic tool the user is never given this opportunity 1 . 1 Several automatic parallelization systems (for example, see <ref> [45] </ref>) provide a directive that instructs the compiler to ignore all dependences.
Reference: [46] <author> A. Osterhaug, </author> <title> editor. Guide to Parallel Programming on Sequent Computer Systems. </title> <publisher> Sequent Technical Publications, </publisher> <address> San Diego, CA, </address> <year> 1989. </year>
Reference-contexts: Ped supports several styles of parallel programming. It can be used to develop new parallel codes, convert sequential codes into parallel form, or analyze existing parallel programs. In particular, Ped currently accepts and generates Fortran 77, IBM parallel Fortran [36], and parallel Fortran for the Sequent Symmetry <ref> [46] </ref>. The Parallel Computing Forum is developing PCF Fortran [44]. PCF Fortran defines a set of parallel extensions that a large number of manufacturers are 3 committed to accepting, obviating the current need to support numerous Fortran dialects. These extensions will be supported when they emerge.
Reference: [47] <author> R. G. Scarborough and H. G. Kolsky. </author> <title> A vectorizing Fortran compiler. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 30(2) </volume> <pages> 163-171, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Also, separate iterations of a loop usually offer portions of computation that require similar execution times, and often provide enough computation to keep numerous processors occupied. PFC has had many successes. It was influential in the design of several commercial vector-ization systems <ref> [47] </ref>, and it has successfully found near-optimal parallelism for a selected set of test cases [19]. However, it has not been successful enough to obviate the need for explicit parallel programming.
Reference: [48] <author> B. Shei and D. Gannon. SIGMACS: </author> <title> A programmable programming environment. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Ped is distinguished by its large collection of transformations, the expert guidance provided for each transformation, and the quality of its program analysis and user interface. Below we briefly describe Sigmacs <ref> [48] </ref>, Pat [50], MIMDizer [1], and Superb [57], placing emphasis on their unique features. Sigmacs is an interactive emacs-based programmable parallelizer in the Faust programming environment. It utilizes dependence information fetched from a project database maintained by the database server. Sigmacs displays dependences and provides some interactive program transformations.
Reference: [49] <author> J. Singh and J. Hennessy. </author> <title> An empirical investigation of the effectiveness of and limi tations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The success of automatic vectorization has led users to seek a similarly elegant software solution to the problem of programming parallel computers. A substantial amount of research has been conducted on whether sequential Fortran 77 programs can be automatically converted without user assistance to execute on shared-memory parallel machines <ref> [2, 3, 7, 8, 42, 49] </ref>. The results of this research have been both promising and disappointing. Although such systems can successfully parallelize many interesting programs, they have not established a level of success that will make it possible to avoid explicit parallel programming by the user.
Reference: [50] <author> K. Smith and W. Appelbe. </author> <title> PAT an interactive Fortran parallelizing assistant tool. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Ped is distinguished by its large collection of transformations, the expert guidance provided for each transformation, and the quality of its program analysis and user interface. Below we briefly describe Sigmacs [48], Pat <ref> [50] </ref>, MIMDizer [1], and Superb [57], placing emphasis on their unique features. Sigmacs is an interactive emacs-based programmable parallelizer in the Faust programming environment. It utilizes dependence information fetched from a project database maintained by the database server. Sigmacs displays dependences and provides some interactive program transformations.
Reference: [51] <author> K. Smith, W. Appelbe, and K. Stirewalt. </author> <title> Incremental dependence analysis for inter active parallelization. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The user may also view the list of dependences for a given loop. However, Pat can only analyze programs where only one write occurs to each variable in a loop. Like Ped, incremental dependence analysis is used to update the dependence graph after structured transformations <ref> [51] </ref>. Rather than analyzing the effects of existing synchronization, Pat can instead insert synchronization to preserve specific dependences. Since Pat does not compute distance or direction vectors, loop reordering transformations such as loop interchange and skewing are not supported.
Reference: [52] <author> J. Subhlok. </author> <title> Analysis of Synchronization in a Parallel Programming Environment. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Otherwise, there may exist race conditions. Establishing that the order specified by certain dependences will always be maintained has been proven Co-NP-hard. However, efficient techniques have been developed to identify dependences preserved in parallel loops by post and wait event synchronization <ref> [21, 22, 52] </ref>. Ped utilizes these techniques in a transformation that determines whether a particular dependence is preserved by event synchronization in a loop. Other forms of synchronization are not currently handled in Ped.
Reference: [53] <author> M. Wegman and K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <type> Technical Report CS-89-36, </type> <institution> Dept. of Computer Science, Brown University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: When possible, this component eliminates or characterizes symbolic expressions used to determine loop bounds, loop steps, array subscript expressions, array dimensions, and control flow. Its main goal is to improve the precision of dependence testing. The SSA graph provides a framework for performing constant propagation <ref> [53] </ref>, auxiliary induction variable detection, expression folding, and other symbolic analysis techniques. Detecting data dependences in a program is complicated by array references, since it is difficult to determine whether two array references may ever access the same memory location. <p> statement. * Statement deletion deletes an assignment statement. 21 * Preserved dependence? indicates whether the current selected dependence is pre served by any post and wait event synchronization in the loop. * Constant replacement performs global constant propagation for each procedure in the program, using the sparse conditional constant algorithm <ref> [53] </ref>. Any variable found to have a constant value is replaced with that value, increasing the precision of subsequent dependence analysis. 5.5 Example The following example is intended to give the reader the flavor of this type of transformational system. Consider the first group of nested loops in Figure 4.
Reference: [54] <author> M. J. Wolfe. </author> <title> Loop skewing: The wavefront method revisited. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 279-293, </pages> <month> August </month> <year> 1986. </year> <month> 32 </month>
Reference-contexts: When possible, Ped computes and suggests the optimal skew degree. Loop skewing may be used with loop interchange in Ped to perform the wavefront method <ref> [38, 54] </ref>. * Loop reversal reverses the order of execution of loop iterations. * Loop adjusting adjusts the upper and lower bounds of a loop by a constant.
Reference: [55] <author> M. J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: If the minimum distance of the dependences in the loop is less than the step size, the resultant inner loop may be parallelized. Used alone the order of the iterations is unchanged, but used in concert with loop interchange the iteration space may be tiled <ref> [55] </ref> to utilize memory bandwidth and cache more effectively [24]. * Scalar replacement takes array references with consistent dependences and replaces them with scalar temporaries that may be allocated into registers [15].
Reference: [56] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: It is written in PL/1 and runs on an IBM mainframe. In recent years the project has focused on the more difficult problem of automatically parallelizing sequential code. PFC performs data dependence analysis <ref> [9, 13, 14, 56] </ref>, interprocedural side effect analysis [25] and interprocedural constant propagation [18]. More recently an implementation of regular section analysis [20, 32], which determines the subarrays affected by procedure calls, has been completed. <p> A data dependence exists between these references only if the same location may be accessed 9 by both references. Dependence testing is the process of discovering and characterizing data dependences between array references. It is a difficult problem which has been the subject of extensive research <ref> [9, 13, 14, 56] </ref>. Conservative data dependence analysis requires that if a dependence cannot be disproven, it must be assumed to exist. False dependences result when conservative dependences do not actually exist. The most important objective of the dependence analyzer is to minimize false dependences through precise analysis. <p> When loop interchange is safe, it can be used to adjust the granularity of parallel loops <ref> [9, 38, 56] </ref>. 4 The details of Ped's implementation of several of these transformations appear in [38]. 19 * Loop skewing adjusts the iteration space of two perfectly nested loops by shifting the work per iteration in order to expose parallelism.
Reference: [57] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 33 </month>
Reference-contexts: Ped is distinguished by its large collection of transformations, the expert guidance provided for each transformation, and the quality of its program analysis and user interface. Below we briefly describe Sigmacs [48], Pat [50], MIMDizer [1], and Superb <ref> [57] </ref>, placing emphasis on their unique features. Sigmacs is an interactive emacs-based programmable parallelizer in the Faust programming environment. It utilizes dependence information fetched from a project database maintained by the database server. Sigmacs displays dependences and provides some interactive program transformations.
References-found: 57

