URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/97.tr651.New_algorithms_for_fast_discovery_of_association_rules.ps.gz
Refering-URL: http://www.cs.indiana.edu/cstr/search/?Knowledge+discovery+MINK%3D2
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fzaki,srini,ogihara,lig@cs.rochester.edu  
Title: New Algorithms for Fast Discovery of Association Rules  
Author: Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mitsunori Ogihara, and Wei Li 
Note: supported this work. This work was supported in part by an NSF Research Initiation Award (CCR-9409120) and ARPA contract (F19628-94-C-0057).  
Date: July 1997  
Address: Rochester, Rochester NY 14627  Rochester, New York 14627  
Affiliation: Computer Science Department University of  The University of Rochester Computer Science Department  The University of Rochester Computer Science Department  
Pubnum: Technical Report 651  
Abstract: Association rule discovery has emerged as an important problem in knowledge discovery and data mining. The association mining task consists of identifying the frequent itemsets, and then forming conditional implication rules among them. In this paper we present efficient algorithms for the discovery of frequent itemsets, which forms the compute intensive phase of the task. The algorithms utilize the structural properties of frequent itemsets to facilitate fast discovery. The related database items are grouped together into clusters representing the potential maximal frequent itemsets in the database. Each cluster induces a sub-lattice of the itemset lattice. Efficient lattice traversal techniques are presented, which quickly identify all the true maximal frequent itemsets, and all their subsets if desired. We also present the effect of using different database layout schemes combined with the proposed clustering and traversal techniques. The proposed algorithms scan a (pre-processed) database only once, addressing the open question in association mining, whether all the rules can be efficiently extracted in a single database pass. We experimentally compare the new algorithms against the previous approaches, obtaining improvements of more than an order of magnitude for our test databases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> In IEEE Trans. on Knowledge and Data Engg., </journal> <pages> pages 5(6) 914-925, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Knowledge discovery and data mining (KDD) is an emerging field, whose goal is to make sense out of large amounts of collected data, by discovering hitherto unknown patterns. One of the central KDD tasks is the discovery of association rules <ref> [1] </ref>. The prototypical application is the analysis of supermarket sales or basket data [2, 5, 3]. Basket data consists of items bought by a customer along with the transaction date, time, price, etc. <p> The larger the value of k the more precise the clustering. For example, figure 2 shows the equivalence classes obtained for the instance where k = 2. Each equivalence class 4 is a potential maximal frequent itemset. For example, the class <ref> [1] </ref>, generates the maximal itemset 12345678. 2.2 Maximal Uniform Hypergraph Clique Clustering Let the set of items I denote the vertex set. <p> It can be seen immediately the the clique clustering is more accurate than equivalence class clustering. For example, while equivalence class clustering produced the potential maximal frequent itemset 12345678, the hypergraph clique clustering produces a more refined set f1235; 1258; 1278; 13456; 1568g for equivalence class <ref> [1] </ref>. Clique Generation The maximal cliques are discovered using an algorithm similar to the Bierstone's algorithm [19] for generating cliques. For a class [x], and y 2 [x], y is said to cover the subset of [x], given by cov (y) = [y] " [x]. <p> For each class C, we first identify its covering set, given as fy 2 Cjcov (y) 6= ;; and cov (y) 6 cov (z); for any z 2 C; z &lt; yg. For example, consider the class <ref> [1] </ref>, shown in figure 2. cov (2) =[2], since [2] [1]. Similarly, cov (y) =[y], for all y 2 [1]. The covering set of [1] is given by the set f2; 3; 5g. <p> For each class C, we first identify its covering set, given as fy 2 Cjcov (y) 6= ;; and cov (y) 6 cov (z); for any z 2 C; z &lt; yg. For example, consider the class <ref> [1] </ref>, shown in figure 2. cov (2) =[2], since [2] [1]. Similarly, cov (y) =[y], for all y 2 [1]. The covering set of [1] is given by the set f2; 3; 5g. The item 4 is not in the covering set since, cov (4) = f5; 6g is a subset of cov (3) = f4; 5; 6g. <p> For example, consider the class <ref> [1] </ref>, shown in figure 2. cov (2) =[2], since [2] [1]. Similarly, cov (y) =[y], for all y 2 [1]. The covering set of [1] is given by the set f2; 3; 5g. The item 4 is not in the covering set since, cov (4) = f5; 6g is a subset of cov (3) = f4; 5; 6g. Figure 3 shows the complete clique generation algorithm. <p> For example, consider the class <ref> [1] </ref>, shown in figure 2. cov (2) =[2], since [2] [1]. Similarly, cov (y) =[y], for all y 2 [1]. The covering set of [1] is given by the set f2; 3; 5g. The item 4 is not in the covering set since, cov (4) = f5; 6g is a subset of cov (3) = f4; 5; 6g. Figure 3 shows the complete clique generation algorithm. <p> It shows a particular instance of the clustering schemes which uses L 2 to generate the set of potential maximal itemsets. Let's assume that 7 for equivalence class <ref> [1] </ref>, there is only one potential maximal itemset, 123456, while 1235 and 13456 are "true" maximal frequent itemsets. The support of 2-itemsets in this class are also shown. <p> The support of 2-itemsets in this class are also shown. Like figure 1, the dashed circles represent the frequent sets, the bold circles the maximal such itemsets, and the boxes denote equivalence classes. The potential maximal itemset 123456 forms a lattice over the elements of equivalence class <ref> [1] </ref> = f12; 13; 14; 15; 16g. We need to traverse this lattice to determine the "true" frequent itemsets. <p> A pure bottom-up lattice traversal proceeds in a breadth-first manner generating frequent itemsets of length k, before generating itemsets of level k + 1, i.e., at each intermediate step we determine the border of frequent k-itemsets. For example, all pairs of elements of <ref> [1] </ref> are joined to produce new equivalence classes of frequent 3-itemsets, namely [12] = f3; 5g (producing the maximal itemset 1235), [13] = f4; 5; 6g, and [14] = f5; 6g. The next step yields the frequent class, [134] = f5; 6g ( producing the maximal itemset 13456).
Reference: [2] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In ACM SIGMOD Intl. Conf. Management of Data, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: One of the central KDD tasks is the discovery of association rules [1]. The prototypical application is the analysis of supermarket sales or basket data <ref> [2, 5, 3] </ref>. Basket data consists of items bought by a customer along with the transaction date, time, price, etc. The association rule discovery task identifies the group of items most often purchased along with another group of items. <p> Besides the retail sales example, association rules have been useful in predicting patterns in university enrollment, occurrences of words in text documents, census data, and so on. The task of mining association rules over basket data was first introduced in <ref> [2] </ref>, which can be formally stated as follows: Let I = fi 1 ; i 2 ; ; i m g be the set of database items. Each transaction, T , in the database, D, has a unique identifier, and contains a set of items, called an itemset. <p> See [26, 18] for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature <ref> [2, 21, 5, 16, 23, 17, 27, 3, 31] </ref>. Almost all the algorithms use the downward closure property of itemset support to prune the itemset lattice the property that all subsets of a frequent itemset must themselves be frequent. <p> A pass over the database is made at each level to find the frequent itemsets. The algorithms differ to the extent that they prune the search space using efficient candidate generation procedure. The first algorithm AIS <ref> [2] </ref> generates candidates on-the-fly. All frequent itemsets from the previous 1 transaction that are contained in the new transaction are extended with other items in that transaction. This results in to many unnecessary candidates. <p> This results in to many unnecessary candidates. The Apriori algorithm [21, 5, 3] which uses a better candidate generation procedure was shown to be superior to earlier approaches <ref> [2, 17, 16] </ref>. The DHP algorithm [23] collects approximate support of candidates in the previous pass for further pruning, however, this optimization may be detrimental to performance [4]. All these algorithms make multiple passes over the database, once for each iteration k. <p> There has also been work in finding frequent sequences of itemsets over temporal data [6, 22, 29, 20]. 1.1 Contribution The main limitation of almost all proposed algorithms <ref> [2, 5, 21, 23, 3] </ref> is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads. Moreover, these algorithms use complicated hash structures which entails additional overhead in maintaining and searching them, and they typically suffer from poor cache locality [25]. <p> For each class C, we first identify its covering set, given as fy 2 Cjcov (y) 6= ;; and cov (y) 6 cov (z); for any z 2 C; z &lt; yg. For example, consider the class [1], shown in figure 2. cov (2) =<ref> [2] </ref>, since [2] [1]. Similarly, cov (y) =[y], for all y 2 [1]. The covering set of [1] is given by the set f2; 3; 5g. The item 4 is not in the covering set since, cov (4) = f5; 6g is a subset of cov (3) = f4; 5; 6g.
Reference: [3] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I. Verkamo. </author> <title> Fast discovery of association rules. </title> <editor> In U. Fayyad and et al, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: One of the central KDD tasks is the discovery of association rules [1]. The prototypical application is the analysis of supermarket sales or basket data <ref> [2, 5, 3] </ref>. Basket data consists of items bought by a customer along with the transaction date, time, price, etc. The association rule discovery task identifies the group of items most often purchased along with another group of items. <p> Given m items, there are potentially 2 m frequent itemsets, which form a lattice of subsets over I. However, only a small fraction of the whole lattice space is frequent. This paper presents efficient methods to discover these frequent itemsets. The rule discovery step is relatively easy <ref> [3] </ref>. Once the support of frequent itemsets is known, rules of the form X Y ) Y (where Y X), are generated for all frequent itemsets X, provided the rules meet a desired confidence. <p> See [26, 18] for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature <ref> [2, 21, 5, 16, 23, 17, 27, 3, 31] </ref>. Almost all the algorithms use the downward closure property of itemset support to prune the itemset lattice the property that all subsets of a frequent itemset must themselves be frequent. <p> The first algorithm AIS [2] generates candidates on-the-fly. All frequent itemsets from the previous 1 transaction that are contained in the new transaction are extended with other items in that transaction. This results in to many unnecessary candidates. The Apriori algorithm <ref> [21, 5, 3] </ref> which uses a better candidate generation procedure was shown to be superior to earlier approaches [2, 17, 16]. The DHP algorithm [23] collects approximate support of candidates in the previous pass for further pruning, however, this optimization may be detrimental to performance [4]. <p> There has also been work in finding frequent sequences of itemsets over temporal data [6, 22, 29, 20]. 1.1 Contribution The main limitation of almost all proposed algorithms <ref> [2, 5, 21, 23, 3] </ref> is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads. Moreover, these algorithms use complicated hash structures which entails additional overhead in maintaining and searching them, and they typically suffer from poor cache locality [25]. <p> The next step yields the frequent class, [134] = f5; 6g ( producing the maximal itemset 13456). Most current algorithms use this approach. For example, the process of generating C k from L k1 used in Apriori <ref> [3] </ref>, and related algorithms [27, 23], is a pure bottom-up exploration of the lattice space. <p> Another issue is whether some preliminary invariant information can be gleaned during this process. There are two possible layouts of the target dataset for association mining the horizontal and the vertical layout. Horizontal Data Layout This is the format standardly used in the literature (see e.g., <ref> [5, 21, 3] </ref>). Here a dataset consists of a list of transactions. Each transaction has a transaction identifier (TID) followed by a list of items in that transaction. This format imposes some computation overhead during the support counting step. <p> Note also that the database itself requires the same amount of memory in both the horizontal and vertical formats (this is obvious from figure 6). 12 5 Algorithms for Frequent Itemset Discovery We first give a brief overview of the well known previous algorithms: 5.1 Apriori Algorithm Apriori <ref> [5, 21, 3] </ref> uses the downward closure property of itemset support that any subset of a frequent itemset must also be frequent. <p> We used different synthetic databases which mimic the transactions in a retailing environment, and were generated using the procedure described in [5]. These have been used as benchmark databases for many association rules algorithms <ref> [5, 16, 23, 27, 3] </ref>. The different database parameters varied in our experiments are the number of transactions D, average transaction size T , and the average size of a maximal potentially frequent itemset I.
Reference: [4] <author> R. Agrawal and J. Shafer. </author> <title> Parallel mining of association rules. </title> <journal> In IEEE Trans. on Knowledge and Data Engg., </journal> <pages> pages 8(6) 962-969, </pages> <year> 1996. </year>
Reference-contexts: The Apriori algorithm [21, 5, 3] which uses a better candidate generation procedure was shown to be superior to earlier approaches [2, 17, 16]. The DHP algorithm [23] collects approximate support of candidates in the previous pass for further pruning, however, this optimization may be detrimental to performance <ref> [4] </ref>. All these algorithms make multiple passes over the database, once for each iteration k. The Partition algorithm [27] minimizes I/O by scanning the database only twice. It partitions the database into small chunks which can be handled in memory. <p> Approaches using only general-purpose DBMS systems and relational algebra operations have been studied [16, 17], but these don't compare favorably with the specialized approaches. A number of parallel algorithms have also been proposed <ref> [24, 4, 32, 11, 14, 33] </ref>. All the above solutions are applicable to only binary data, i.e., either an item is present in a transaction or it isn't. <p> Any class with only 1 member can be eliminated since no candidates can be generated from it. Thus we can discard the class [D]. This idea of partitioning L k1 into equivalence classes was independently proposed in <ref> [4, 32] </ref>. The equivalence partitioning was used in [32] to parallelize the candidate generation step. It was also used in [4] to partition the candidates into disjoint sets. <p> Thus we can discard the class [D]. This idea of partitioning L k1 into equivalence classes was independently proposed in [4, 32]. The equivalence partitioning was used in [32] to parallelize the candidate generation step. It was also used in <ref> [4] </ref> to partition the candidates into disjoint sets. At any intermediate step of the algorithm when the set of frequent itemsets, L k for k 2, has been determined we can generate the set of potential maximal frequent itemsets from L k .
Reference: [5] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules. </title> <booktitle> In 20th VLDB Conf., </booktitle> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: One of the central KDD tasks is the discovery of association rules [1]. The prototypical application is the analysis of supermarket sales or basket data <ref> [2, 5, 3] </ref>. Basket data consists of items bought by a customer along with the transaction date, time, price, etc. The association rule discovery task identifies the group of items most often purchased along with another group of items. <p> See [26, 18] for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature <ref> [2, 21, 5, 16, 23, 17, 27, 3, 31] </ref>. Almost all the algorithms use the downward closure property of itemset support to prune the itemset lattice the property that all subsets of a frequent itemset must themselves be frequent. <p> The first algorithm AIS [2] generates candidates on-the-fly. All frequent itemsets from the previous 1 transaction that are contained in the new transaction are extended with other items in that transaction. This results in to many unnecessary candidates. The Apriori algorithm <ref> [21, 5, 3] </ref> which uses a better candidate generation procedure was shown to be superior to earlier approaches [2, 17, 16]. The DHP algorithm [23] collects approximate support of candidates in the previous pass for further pruning, however, this optimization may be detrimental to performance [4]. <p> There has also been work in finding frequent sequences of itemsets over temporal data [6, 22, 29, 20]. 1.1 Contribution The main limitation of almost all proposed algorithms <ref> [2, 5, 21, 23, 3] </ref> is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads. Moreover, these algorithms use complicated hash structures which entails additional overhead in maintaining and searching them, and they typically suffer from poor cache locality [25]. <p> Another issue is whether some preliminary invariant information can be gleaned during this process. There are two possible layouts of the target dataset for association mining the horizontal and the vertical layout. Horizontal Data Layout This is the format standardly used in the literature (see e.g., <ref> [5, 21, 3] </ref>). Here a dataset consists of a list of transactions. Each transaction has a transaction identifier (TID) followed by a list of items in that transaction. This format imposes some computation overhead during the support counting step. <p> Note also that the database itself requires the same amount of memory in both the horizontal and vertical formats (this is obvious from figure 6). 12 5 Algorithms for Frequent Itemset Discovery We first give a brief overview of the well known previous algorithms: 5.1 Apriori Algorithm Apriori <ref> [5, 21, 3] </ref> uses the downward closure property of itemset support that any subset of a frequent itemset must also be frequent. <p> We used different synthetic databases which mimic the transactions in a retailing environment, and were generated using the procedure described in <ref> [5] </ref>. These have been used as benchmark databases for many association rules algorithms [5, 16, 23, 27, 3]. The different database parameters varied in our experiments are the number of transactions D, average transaction size T , and the average size of a maximal potentially frequent itemset I. <p> We used different synthetic databases which mimic the transactions in a retailing environment, and were generated using the procedure described in [5]. These have been used as benchmark databases for many association rules algorithms <ref> [5, 16, 23, 27, 3] </ref>. The different database parameters varied in our experiments are the number of transactions D, average transaction size T , and the average size of a maximal potentially frequent itemset I. <p> The number of maximal potentially frequent itemsets was set at L = 2000, and the number of items at N = 1000. We refer the reader to <ref> [5] </ref> for more detail on the database generation.
Reference: [6] <author> R. Agrawal and R. Srikant. </author> <title> Mining sequential patterns. </title> <booktitle> In Intl. Conf. on Data Engg., </booktitle> <year> 1995. </year>
Reference-contexts: Other extensions of association rules include mining over data where the quantity of items is also considered [28], or mining for rules in the presence of a taxonomy on items <ref> [6, 15] </ref>. There has also been work in finding frequent sequences of itemsets over temporal data [6, 22, 29, 20]. 1.1 Contribution The main limitation of almost all proposed algorithms [2, 5, 21, 23, 3] is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads. <p> Other extensions of association rules include mining over data where the quantity of items is also considered [28], or mining for rules in the presence of a taxonomy on items [6, 15]. There has also been work in finding frequent sequences of itemsets over temporal data <ref> [6, 22, 29, 20] </ref>. 1.1 Contribution The main limitation of almost all proposed algorithms [2, 5, 21, 23, 3] is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads.
Reference: [7] <author> C. Berge. </author> <title> Hypergraphs: Combinatorics of Finite Sets. </title> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference-contexts: Each equivalence class 4 is a potential maximal frequent itemset. For example, the class [1], generates the maximal itemset 12345678. 2.2 Maximal Uniform Hypergraph Clique Clustering Let the set of items I denote the vertex set. A hypergraph <ref> [7] </ref> on I is a family H = fE 1 ; E 2 ; :::; E n g of edges or subsets of I, such that E i 6= ;, and [ n i=1 E i = I.
Reference: [8] <author> C. Bron and J. Kerbosch. </author> <title> Finding all cliques of an undirected graph. </title> <journal> In Communications of the ACM, </journal> <volume> 16(9) </volume> <pages> 575-577, </pages> <month> Sept. </month> <year> 1973. </year>
Reference-contexts: Some of the factors affecting the edge density include decreasing support and increasing transaction size. The effect of these parameters is studied in section 6. A number of other clique generating algorithms were brought to our notice after we had chosen the above algorithm. The algorithm proposed in <ref> [8] </ref> was shown to have superior performance than the Bierstone algorithm. Some other newer algorithms for this problem are presented in [10, 30]. <p> This is borne out in the graphs for T20.I2.D100K with decreasing support, and in figure 11 b) as the transaction size increases for a fixed support value. We expect to reduce the overhead of the clique generation by implementing the algorithms proposed in <ref> [8, 10, 30] </ref>, which were shown to be superior to the Bierstone algorithm [19], a modification of which is used in the current implementation. 6.2 Join and Memory Statistics (a) Number of Intersections (b) Memory Usage techniques are able to reduce the joins to different extents, providing the key to their
Reference: [9] <author> S. Brin, R. Motwani, J. Ullman, and S. Tsur. </author> <title> Dynamic itemset counting and implication rules for market basket data. </title> <booktitle> In ACM SIGMOD Conf. Management of Data, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Another way to minimize the I/O overhead is to work with only a small sample of the database. An analysis of the effectiveness of sampling for association mining was presented in [34], and [31] presents an exact algorithm that finds all rules using sampling. The recently proposed DIC algorithm <ref> [9] </ref> dynamically counts candidates of varying length as the database scan progresses, and thus is able to reduce the number of scans. Approaches using only general-purpose DBMS systems and relational algebra operations have been studied [16, 17], but these don't compare favorably with the specialized approaches.
Reference: [10] <author> N. Chiba and T. Nishizeki. </author> <title> Arboricity and subgraph listing algorithms. </title> <journal> In SIAM J. Computing, </journal> <volume> 14(1) </volume> <pages> 210-223, </pages> <month> Feb. </month> <year> 1985. </year>
Reference-contexts: A number of other clique generating algorithms were brought to our notice after we had chosen the above algorithm. The algorithm proposed in [8] was shown to have superior performance than the Bierstone algorithm. Some other newer algorithms for this problem are presented in <ref> [10, 30] </ref>. <p> This is borne out in the graphs for T20.I2.D100K with decreasing support, and in figure 11 b) as the transaction size increases for a fixed support value. We expect to reduce the overhead of the clique generation by implementing the algorithms proposed in <ref> [8, 10, 30] </ref>, which were shown to be superior to the Bierstone algorithm [19], a modification of which is used in the current implementation. 6.2 Join and Memory Statistics (a) Number of Intersections (b) Memory Usage techniques are able to reduce the joins to different extents, providing the key to their
Reference: [11] <author> D. Cheung, V. Ng, A. Fu, and Y. Fu. </author> <title> Efficient mining of association rules in distributed databases. </title> <journal> In IEEE Trans. on Knowledge and Data Engg., </journal> <pages> pages 8(6) 911-922, </pages> <year> 1996. </year>
Reference-contexts: Approaches using only general-purpose DBMS systems and relational algebra operations have been studied [16, 17], but these don't compare favorably with the specialized approaches. A number of parallel algorithms have also been proposed <ref> [24, 4, 32, 11, 14, 33] </ref>. All the above solutions are applicable to only binary data, i.e., either an item is present in a transaction or it isn't.
Reference: [12] <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> The KDD process for extracting useful knowledge from volumes of data. </title> <booktitle> In Communications of the ACM Data Mining and Knowledge Discovery in Databases, </booktitle> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: For example, all pairs of elements of [1] are joined to produce new equivalence classes of frequent 3-itemsets, namely <ref> [12] </ref> = f3; 5g (producing the maximal itemset 1235), [13] = f4; 5; 6g, and [14] = f5; 6g. The next step yields the frequent class, [134] = f5; 6g ( producing the maximal itemset 13456). Most current algorithms use this approach. <p> This is extended to 1356 by joining 156 with 13, and then to 13456, and finally we find that 123456 is infrequent. The only remaining element is 12. We simply join this with each of the other elements producing the frequent itemset class <ref> [12] </ref>, which generates the other maximal itemset 1235. The bottom-up, top-down, and hybrid approaches are contrasted in figure 4, and the pseudo-code for all the schemes is shown in figure 5. 9 Input: F k = fI 1 ::I n g cluster of frequent k-itemsets. <p> S 2 ,do F 3 = fY j :sup minsupj Y j = (I i " X j ); 8X j 2 S 1 g; if F 3 6= ; then Bottom-Up (F 3 ); end; 4 The KDD Process and Database Layout The KDD process consists of various steps <ref> [12] </ref>. The initial step consists of creating the target dataset by focusing on certain attributes or via data samples. The database creation may require removing unnecessary information and supplying missing data, and transformation techniques for data reduction and projection.
Reference: [13] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Co., </publisher> <year> 1979. </year>
Reference-contexts: If the new clique is a subset of any clique already in the maximal list, then it is not inserted. The conditions for the above test are shown in line 8. For general graphs the maximal clique decision problem is NP-Complete <ref> [13] </ref>. However, the equivalence class graph is usually sparse and the maximal cliques can be enumerated efficiently. As the edge density increases the clique based approaches may suffer. Some of the factors affecting the edge density include decreasing support and increasing transaction size. <p> For example, all pairs of elements of [1] are joined to produce new equivalence classes of frequent 3-itemsets, namely [12] = f3; 5g (producing the maximal itemset 1235), <ref> [13] </ref> = f4; 5; 6g, and [14] = f5; 6g. The next step yields the frequent class, [134] = f5; 6g ( producing the maximal itemset 13456). Most current algorithms use this approach.
Reference: [14] <author> E.-H. Han, G. Karypis, and V. Kumar. </author> <title> Scalable parallel data mining for association rules. </title> <booktitle> In ACM SIGMOD Conf. Management of Data, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Approaches using only general-purpose DBMS systems and relational algebra operations have been studied [16, 17], but these don't compare favorably with the specialized approaches. A number of parallel algorithms have also been proposed <ref> [24, 4, 32, 11, 14, 33] </ref>. All the above solutions are applicable to only binary data, i.e., either an item is present in a transaction or it isn't. <p> For example, all pairs of elements of [1] are joined to produce new equivalence classes of frequent 3-itemsets, namely [12] = f3; 5g (producing the maximal itemset 1235), [13] = f4; 5; 6g, and <ref> [14] </ref> = f5; 6g. The next step yields the frequent class, [134] = f5; 6g ( producing the maximal itemset 13456). Most current algorithms use this approach.
Reference: [15] <author> J. Han and Y. Fu. </author> <title> Discovery of multiple-level association rules from large databases. </title> <booktitle> In 21st VLDB Conf., </booktitle> <year> 1995. </year> <month> 21 </month>
Reference-contexts: Other extensions of association rules include mining over data where the quantity of items is also considered [28], or mining for rules in the presence of a taxonomy on items <ref> [6, 15] </ref>. There has also been work in finding frequent sequences of itemsets over temporal data [6, 22, 29, 20]. 1.1 Contribution The main limitation of almost all proposed algorithms [2, 5, 21, 23, 3] is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads.
Reference: [16] <author> M. Holsheimer, M. Kersten, H. Mannila, and H. Toivonen. </author> <title> A perspective on databases and data mining. </title> <booktitle> In 1st Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: See [26, 18] for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature <ref> [2, 21, 5, 16, 23, 17, 27, 3, 31] </ref>. Almost all the algorithms use the downward closure property of itemset support to prune the itemset lattice the property that all subsets of a frequent itemset must themselves be frequent. <p> This results in to many unnecessary candidates. The Apriori algorithm [21, 5, 3] which uses a better candidate generation procedure was shown to be superior to earlier approaches <ref> [2, 17, 16] </ref>. The DHP algorithm [23] collects approximate support of candidates in the previous pass for further pruning, however, this optimization may be detrimental to performance [4]. All these algorithms make multiple passes over the database, once for each iteration k. <p> The recently proposed DIC algorithm [9] dynamically counts candidates of varying length as the database scan progresses, and thus is able to reduce the number of scans. Approaches using only general-purpose DBMS systems and relational algebra operations have been studied <ref> [16, 17] </ref>, but these don't compare favorably with the specialized approaches. A number of parallel algorithms have also been proposed [24, 4, 32, 11, 14, 33]. All the above solutions are applicable to only binary data, i.e., either an item is present in a transaction or it isn't. <p> Furthermore, the horizontal layout seems suitable only for the bottom-up exploration of the frequent border. It appears to be extremely complicated to implement the hybrid approach using the horizontal format. Vertical Data Layout In the vertical (or inverted) layout (also called the decomposed storage structure <ref> [16] </ref>), a dataset consists of a list of items, with each item followed by its tid-list | the list of all the transactions identifiers containing the item. An example of successful use of this layout can be found in [16, 27, 33, 35]. <p> An example of successful use of this layout can be found in <ref> [16, 27, 33, 35] </ref>. <p> We used different synthetic databases which mimic the transactions in a retailing environment, and were generated using the procedure described in [5]. These have been used as benchmark databases for many association rules algorithms <ref> [5, 16, 23, 27, 3] </ref>. The different database parameters varied in our experiments are the number of transactions D, average transaction size T , and the average size of a maximal potentially frequent itemset I.
Reference: [17] <author> M. Houtsma and A. Swami. </author> <title> Set-oriented mining of association rules in relational databases. </title> <booktitle> In 11th Intl. Conf. Data Engineering, </booktitle> <year> 1995. </year>
Reference-contexts: See [26, 18] for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature <ref> [2, 21, 5, 16, 23, 17, 27, 3, 31] </ref>. Almost all the algorithms use the downward closure property of itemset support to prune the itemset lattice the property that all subsets of a frequent itemset must themselves be frequent. <p> This results in to many unnecessary candidates. The Apriori algorithm [21, 5, 3] which uses a better candidate generation procedure was shown to be superior to earlier approaches <ref> [2, 17, 16] </ref>. The DHP algorithm [23] collects approximate support of candidates in the previous pass for further pruning, however, this optimization may be detrimental to performance [4]. All these algorithms make multiple passes over the database, once for each iteration k. <p> The recently proposed DIC algorithm [9] dynamically counts candidates of varying length as the database scan progresses, and thus is able to reduce the number of scans. Approaches using only general-purpose DBMS systems and relational algebra operations have been studied <ref> [16, 17] </ref>, but these don't compare favorably with the specialized approaches. A number of parallel algorithms have also been proposed [24, 4, 32, 11, 14, 33]. All the above solutions are applicable to only binary data, i.e., either an item is present in a transaction or it isn't.
Reference: [18] <author> M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A. I. Verkamo. </author> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> In 3rd Intl. Conf. Information and Knowledge Management, </booktitle> <pages> pages 401-407, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: If the important issue in the first step is that of "quantity" or performance, due to its compute intensive nature, the dominant concern in the second step is that of "quality" of rules that are generated. See <ref> [26, 18] </ref> for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature [2, 21, 5, 16, 23, 17, 27, 3, 31].
Reference: [19] <author> G. D. Mulligan and D. G. Corneil. </author> <title> Corrections to Bierstone's Algorithm for Generating Cliques. </title> <journal> In J. Association of Computing Machinery, </journal> <volume> 19(2) </volume> <pages> 244-247, </pages> <month> Apr. </month> <year> 1972. </year>
Reference-contexts: For example, while equivalence class clustering produced the potential maximal frequent itemset 12345678, the hypergraph clique clustering produces a more refined set f1235; 1258; 1278; 13456; 1568g for equivalence class [1]. Clique Generation The maximal cliques are discovered using an algorithm similar to the Bierstone's algorithm <ref> [19] </ref> for generating cliques. For a class [x], and y 2 [x], y is said to cover the subset of [x], given by cov (y) = [y] " [x]. <p> We expect to reduce the overhead of the clique generation by implementing the algorithms proposed in [8, 10, 30], which were shown to be superior to the Bierstone algorithm <ref> [19] </ref>, a modification of which is used in the current implementation. 6.2 Join and Memory Statistics (a) Number of Intersections (b) Memory Usage techniques are able to reduce the joins to different extents, providing the key to their performance.
Reference: [20] <author> H. Mannila and H. Toivonen. </author> <title> Discovering generalized episodes using minimal oc-curences. </title> <booktitle> In 2nd Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <year> 1996. </year>
Reference-contexts: Other extensions of association rules include mining over data where the quantity of items is also considered [28], or mining for rules in the presence of a taxonomy on items [6, 15]. There has also been work in finding frequent sequences of itemsets over temporal data <ref> [6, 22, 29, 20] </ref>. 1.1 Contribution The main limitation of almost all proposed algorithms [2, 5, 21, 23, 3] is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads.
Reference: [21] <author> H. Mannila, H. Toivonen, and I. Verkamo. </author> <title> Efficient algorithms for discovering association rules. </title> <note> In AAAI Wkshp. Knowledge Discovery in Databases, </note> <month> July </month> <year> 1994. </year>
Reference-contexts: See [26, 18] for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature <ref> [2, 21, 5, 16, 23, 17, 27, 3, 31] </ref>. Almost all the algorithms use the downward closure property of itemset support to prune the itemset lattice the property that all subsets of a frequent itemset must themselves be frequent. <p> The first algorithm AIS [2] generates candidates on-the-fly. All frequent itemsets from the previous 1 transaction that are contained in the new transaction are extended with other items in that transaction. This results in to many unnecessary candidates. The Apriori algorithm <ref> [21, 5, 3] </ref> which uses a better candidate generation procedure was shown to be superior to earlier approaches [2, 17, 16]. The DHP algorithm [23] collects approximate support of candidates in the previous pass for further pruning, however, this optimization may be detrimental to performance [4]. <p> There has also been work in finding frequent sequences of itemsets over temporal data [6, 22, 29, 20]. 1.1 Contribution The main limitation of almost all proposed algorithms <ref> [2, 5, 21, 23, 3] </ref> is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads. Moreover, these algorithms use complicated hash structures which entails additional overhead in maintaining and searching them, and they typically suffer from poor cache locality [25]. <p> Another issue is whether some preliminary invariant information can be gleaned during this process. There are two possible layouts of the target dataset for association mining the horizontal and the vertical layout. Horizontal Data Layout This is the format standardly used in the literature (see e.g., <ref> [5, 21, 3] </ref>). Here a dataset consists of a list of transactions. Each transaction has a transaction identifier (TID) followed by a list of items in that transaction. This format imposes some computation overhead during the support counting step. <p> Note also that the database itself requires the same amount of memory in both the horizontal and vertical formats (this is obvious from figure 6). 12 5 Algorithms for Frequent Itemset Discovery We first give a brief overview of the well known previous algorithms: 5.1 Apriori Algorithm Apriori <ref> [5, 21, 3] </ref> uses the downward closure property of itemset support that any subset of a frequent itemset must also be frequent.
Reference: [22] <author> H. Mannila, H. Toivonen, and I. Verkamo. </author> <title> Discovering frequent episodes in sequences. </title> <booktitle> In 1st Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <year> 1995. </year>
Reference-contexts: Other extensions of association rules include mining over data where the quantity of items is also considered [28], or mining for rules in the presence of a taxonomy on items [6, 15]. There has also been work in finding frequent sequences of itemsets over temporal data <ref> [6, 22, 29, 20] </ref>. 1.1 Contribution The main limitation of almost all proposed algorithms [2, 5, 21, 23, 3] is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads.
Reference: [23] <author> J. S. Park, M. Chen, and P. S. Yu. </author> <title> An effective hash based algorithm for mining association rules. </title> <booktitle> In ACM SIGMOD Intl. Conf. Management of Data, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: See [26, 18] for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature <ref> [2, 21, 5, 16, 23, 17, 27, 3, 31] </ref>. Almost all the algorithms use the downward closure property of itemset support to prune the itemset lattice the property that all subsets of a frequent itemset must themselves be frequent. <p> This results in to many unnecessary candidates. The Apriori algorithm [21, 5, 3] which uses a better candidate generation procedure was shown to be superior to earlier approaches [2, 17, 16]. The DHP algorithm <ref> [23] </ref> collects approximate support of candidates in the previous pass for further pruning, however, this optimization may be detrimental to performance [4]. All these algorithms make multiple passes over the database, once for each iteration k. The Partition algorithm [27] minimizes I/O by scanning the database only twice. <p> There has also been work in finding frequent sequences of itemsets over temporal data [6, 22, 29, 20]. 1.1 Contribution The main limitation of almost all proposed algorithms <ref> [2, 5, 21, 23, 3] </ref> is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads. Moreover, these algorithms use complicated hash structures which entails additional overhead in maintaining and searching them, and they typically suffer from poor cache locality [25]. <p> The next step yields the frequent class, [134] = f5; 6g ( producing the maximal itemset 13456). Most current algorithms use this approach. For example, the process of generating C k from L k1 used in Apriori [3], and related algorithms <ref> [27, 23] </ref>, is a pure bottom-up exploration of the lattice space. Since this is a bottom-up approach all the frequent subsets of the maximal frequent itemsets are generated in intermediate steps of the traversal. 3.2 Top-Down Search The bottom-up approach doesn't make full use of the clustering information. <p> We used different synthetic databases which mimic the transactions in a retailing environment, and were generated using the procedure described in [5]. These have been used as benchmark databases for many association rules algorithms <ref> [5, 16, 23, 27, 3] </ref>. The different database parameters varied in our experiments are the number of transactions D, average transaction size T , and the average size of a maximal potentially frequent itemset I.
Reference: [24] <author> J. S. Park, M. Chen, and P. S. Yu. </author> <title> Efficient parallel data mining for association rules. </title> <booktitle> In ACM Intl. Conf. Information and Knowledge Management, </booktitle> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Approaches using only general-purpose DBMS systems and relational algebra operations have been studied [16, 17], but these don't compare favorably with the specialized approaches. A number of parallel algorithms have also been proposed <ref> [24, 4, 32, 11, 14, 33] </ref>. All the above solutions are applicable to only binary data, i.e., either an item is present in a transaction or it isn't.
Reference: [25] <author> S. Parthasarathy, M. J. Zaki, and W. Li. </author> <title> Application driven memory placement for dynamic data structures. </title> <type> Technical Report URCS TR 653, </type> <institution> University of Rochester, </institution> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: Moreover, these algorithms use complicated hash structures which entails additional overhead in maintaining and searching them, and they typically suffer from poor cache locality <ref> [25] </ref>. The problem with Partition, even though it makes only two scans, is that, as the number of partitions is increased, the number of locally frequent itemsets increases.
Reference: [26] <author> G. Piatetsky-Shapiro. </author> <title> Discovery, presentation and analysis of strong rules. </title> <editor> In G. P.-S. et al, editor, KDD. </editor> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: If the important issue in the first step is that of "quantity" or performance, due to its compute intensive nature, the dominant concern in the second step is that of "quality" of rules that are generated. See <ref> [26, 18] </ref> for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature [2, 21, 5, 16, 23, 17, 27, 3, 31].
Reference: [27] <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In 21st VLDB Conf., </booktitle> <year> 1995. </year>
Reference-contexts: See [26, 18] for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature <ref> [2, 21, 5, 16, 23, 17, 27, 3, 31] </ref>. Almost all the algorithms use the downward closure property of itemset support to prune the itemset lattice the property that all subsets of a frequent itemset must themselves be frequent. <p> The DHP algorithm [23] collects approximate support of candidates in the previous pass for further pruning, however, this optimization may be detrimental to performance [4]. All these algorithms make multiple passes over the database, once for each iteration k. The Partition algorithm <ref> [27] </ref> minimizes I/O by scanning the database only twice. It partitions the database into small chunks which can be handled in memory. <p> The next step yields the frequent class, [134] = f5; 6g ( producing the maximal itemset 13456). Most current algorithms use this approach. For example, the process of generating C k from L k1 used in Apriori [3], and related algorithms <ref> [27, 23] </ref>, is a pure bottom-up exploration of the lattice space. Since this is a bottom-up approach all the frequent subsets of the maximal frequent itemsets are generated in intermediate steps of the traversal. 3.2 Top-Down Search The bottom-up approach doesn't make full use of the clustering information. <p> An example of successful use of this layout can be found in <ref> [16, 27, 33, 35] </ref>. <p> +) C k = Set of New Candidates; for all transactions t 2 D for all k-subsets s of t if (s 2 C k ) s:count + +; L k = fc 2 C k jc:count minimum supportg; Set of all frequent itemsets = S 5.2 Partition Algorithm Partition <ref> [27] </ref> logically divides the horizontal database into a number of non-overlapping partitions. Each partition is read, transformed into vertical format on-the-fly, and all locally frequent itemsets are generated via tid-list intersections. All such itemsets are merged and a second pass is made through all the partitions. <p> We used different synthetic databases which mimic the transactions in a retailing environment, and were generated using the procedure described in [5]. These have been used as benchmark databases for many association rules algorithms <ref> [5, 16, 23, 27, 3] </ref>. The different database parameters varied in our experiments are the number of transactions D, average transaction size T , and the average size of a maximal potentially frequent itemset I. <p> It also saves some computation overhead of hash trees, since it also uses simple intersections to compute frequent itemsets. These results are in agreement with previous experiments comparing these two algorithms <ref> [27] </ref>. One major problem with Partition is that as the number of partitions increases, the number of locally frequent itemsets, which are not globally frequent, increases.
Reference: [28] <author> R. Srikant and R. Agrawal. </author> <title> Mining quantitative association rules in large relational tables. </title> <booktitle> In ACM SIGMOD Conf. Management of Data, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: All the above solutions are applicable to only binary data, i.e., either an item is present in a transaction or it isn't. Other extensions of association rules include mining over data where the quantity of items is also considered <ref> [28] </ref>, or mining for rules in the presence of a taxonomy on items [6, 15].
Reference: [29] <author> R. Srikant and R. Agrawal. </author> <title> Mining sequential patterns: Generalizations and performance improvements. </title> <booktitle> In 5th Intl. Conf. Extending Database Technology, </booktitle> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: Other extensions of association rules include mining over data where the quantity of items is also considered [28], or mining for rules in the presence of a taxonomy on items [6, 15]. There has also been work in finding frequent sequences of itemsets over temporal data <ref> [6, 22, 29, 20] </ref>. 1.1 Contribution The main limitation of almost all proposed algorithms [2, 5, 21, 23, 3] is that they make repeated passes over the disk-resident database partition, incurring high I/O overheads.
Reference: [30] <author> S. Tsukiyama, M. Ide, H. Ariyoshi, and I. Shirakawa. </author> <title> A new algorithm for generating all the maximal independent sets. </title> <journal> In SIAM J. Computing, </journal> <volume> 6(3) </volume> <pages> 505-517, </pages> <month> Sept. </month> <year> 1977. </year>
Reference-contexts: A number of other clique generating algorithms were brought to our notice after we had chosen the above algorithm. The algorithm proposed in [8] was shown to have superior performance than the Bierstone algorithm. Some other newer algorithms for this problem are presented in <ref> [10, 30] </ref>. <p> This is borne out in the graphs for T20.I2.D100K with decreasing support, and in figure 11 b) as the transaction size increases for a fixed support value. We expect to reduce the overhead of the clique generation by implementing the algorithms proposed in <ref> [8, 10, 30] </ref>, which were shown to be superior to the Bierstone algorithm [19], a modification of which is used in the current implementation. 6.2 Join and Memory Statistics (a) Number of Intersections (b) Memory Usage techniques are able to reduce the joins to different extents, providing the key to their
Reference: [31] <author> H. Toivonen. </author> <title> Sampling large databases for association rules. </title> <booktitle> In 22nd VLDB Conf., </booktitle> <year> 1996. </year>
Reference-contexts: See [26, 18] for some approaches to this problem. In this paper we only consider the frequent itemsets discovery step. Related Work Several algorithms for mining associations have been proposed in the literature <ref> [2, 21, 5, 16, 23, 17, 27, 3, 31] </ref>. Almost all the algorithms use the downward closure property of itemset support to prune the itemset lattice the property that all subsets of a frequent itemset must themselves be frequent. <p> Another way to minimize the I/O overhead is to work with only a small sample of the database. An analysis of the effectiveness of sampling for association mining was presented in [34], and <ref> [31] </ref> presents an exact algorithm that finds all rules using sampling. The recently proposed DIC algorithm [9] dynamically counts candidates of varying length as the database scan progresses, and thus is able to reduce the number of scans. <p> The problem with Partition, even though it makes only two scans, is that, as the number of partitions is increased, the number of locally frequent itemsets increases. While this can be reduced by randomizing the partition selection, results from sampling experiments <ref> [34, 31] </ref> indicate that randomized partitions will have a large number of frequent itemsets in common. Partition can thus spend a lot of time in performing redundant computation. Our work contrasts to these approaches in several ways. <p> Sampling experiments <ref> [31, 34] </ref> indicate that this is a feasible approach. Once the superset has been determined we can easily verify the "true" frequent itemsets among them, Our current implementation uses the pre-processing approach due to its simplicity. We plan to implement the sampling approach in a later paper. <p> One major problem with Partition is that as the number of partitions increases, the number of locally frequent itemsets, which are not globally frequent, increases. While this can be reduced by randomizing the partition selection, sampling experiments <ref> [34, 31] </ref> indicate that randomized partitions will have a large number of frequent itemsets in common. Partition can thus spend a lot of time in performing these redundant intersections. ClusterApr scans the database only once, and outperforms Apriori in almost all cases, and generally lies between Apriori and Partition.
Reference: [32] <author> M. J. Zaki, M. Ogihara, S. Parthasarathy, and W. Li. </author> <title> Parallel data mining for associa-tion rules on shared-memory multi-processors. </title> <booktitle> In Supercomputing'96, </booktitle> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: Approaches using only general-purpose DBMS systems and relational algebra operations have been studied [16, 17], but these don't compare favorably with the specialized approaches. A number of parallel algorithms have also been proposed <ref> [24, 4, 32, 11, 14, 33] </ref>. All the above solutions are applicable to only binary data, i.e., either an item is present in a transaction or it isn't. <p> Any class with only 1 member can be eliminated since no candidates can be generated from it. Thus we can discard the class [D]. This idea of partitioning L k1 into equivalence classes was independently proposed in <ref> [4, 32] </ref>. The equivalence partitioning was used in [32] to parallelize the candidate generation step. It was also used in [4] to partition the candidates into disjoint sets. <p> Any class with only 1 member can be eliminated since no candidates can be generated from it. Thus we can discard the class [D]. This idea of partitioning L k1 into equivalence classes was independently proposed in [4, 32]. The equivalence partitioning was used in <ref> [32] </ref> to parallelize the candidate generation step. It was also used in [4] to partition the candidates into disjoint sets.
Reference: [33] <author> M. J. Zaki, S. Parthasarathy, and W. Li. </author> <title> A localized algorithm for parallel association mining. </title> <booktitle> In 9th ACM Symp. Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Approaches using only general-purpose DBMS systems and relational algebra operations have been studied [16, 17], but these don't compare favorably with the specialized approaches. A number of parallel algorithms have also been proposed <ref> [24, 4, 32, 11, 14, 33] </ref>. All the above solutions are applicable to only binary data, i.e., either an item is present in a transaction or it isn't. <p> Partition can thus spend a lot of time in performing redundant computation. Our work contrasts to these approaches in several ways. We present new algorithms for fast discovery of association rules based on our ideas in <ref> [33, 35] </ref>. The proposed algorithms scan the pre-processed database exactly once greatly reducing I/O costs. The new algorithms are characterized in terms of the clustering information used to group related itemsets, and in terms of the lattice traversal schemes used to search for frequent itemsets. <p> An example of successful use of this layout can be found in <ref> [16, 27, 33, 35] </ref>.
Reference: [34] <author> M. J. Zaki, S. Parthasarathy, W. Li, and M. Ogihara. </author> <title> Evaluation of sampling for data mining of association rules. </title> <booktitle> In 7th Intl. Wkshp. Research Issues in Data Engg, </booktitle> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: Another way to minimize the I/O overhead is to work with only a small sample of the database. An analysis of the effectiveness of sampling for association mining was presented in <ref> [34] </ref>, and [31] presents an exact algorithm that finds all rules using sampling. The recently proposed DIC algorithm [9] dynamically counts candidates of varying length as the database scan progresses, and thus is able to reduce the number of scans. <p> The problem with Partition, even though it makes only two scans, is that, as the number of partitions is increased, the number of locally frequent itemsets increases. While this can be reduced by randomizing the partition selection, results from sampling experiments <ref> [34, 31] </ref> indicate that randomized partitions will have a large number of frequent itemsets in common. Partition can thus spend a lot of time in performing redundant computation. Our work contrasts to these approaches in several ways. <p> Sampling experiments <ref> [31, 34] </ref> indicate that this is a feasible approach. Once the superset has been determined we can easily verify the "true" frequent itemsets among them, Our current implementation uses the pre-processing approach due to its simplicity. We plan to implement the sampling approach in a later paper. <p> One major problem with Partition is that as the number of partitions increases, the number of locally frequent itemsets, which are not globally frequent, increases. While this can be reduced by randomizing the partition selection, sampling experiments <ref> [34, 31] </ref> indicate that randomized partitions will have a large number of frequent itemsets in common. Partition can thus spend a lot of time in performing these redundant intersections. ClusterApr scans the database only once, and outperforms Apriori in almost all cases, and generally lies between Apriori and Partition.
Reference: [35] <author> M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. </author> <title> New algorithms for fast discovery of association rules. </title> <booktitle> In 3rd Intl. Conf. on Knowledge Discovery and Data Mining, </booktitle> <month> Aug. </month> <year> 1997. </year>
Reference-contexts: Partition can thus spend a lot of time in performing redundant computation. Our work contrasts to these approaches in several ways. We present new algorithms for fast discovery of association rules based on our ideas in <ref> [33, 35] </ref>. The proposed algorithms scan the pre-processed database exactly once greatly reducing I/O costs. The new algorithms are characterized in terms of the clustering information used to group related itemsets, and in terms of the lattice traversal schemes used to search for frequent itemsets. <p> An example of successful use of this layout can be found in <ref> [16, 27, 33, 35] </ref>.
References-found: 35

