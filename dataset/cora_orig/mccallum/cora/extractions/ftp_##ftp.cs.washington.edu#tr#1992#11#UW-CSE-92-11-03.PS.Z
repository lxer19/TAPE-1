URL: ftp://ftp.cs.washington.edu/tr/1992/11/UW-CSE-92-11-03.PS.Z
Refering-URL: http://www.cs.washington.edu/research/arch/balanced.html
Root-URL: 
Title: Balanced Scheduling:  
Abstract: Instruction scheduling when memory latency is uncertain Daniel R. Kerns Department of Computer Science and Engineering University of Washington Technical Report 92-11-03 November 1992 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: Two architectural innovations make it worthwhile to reconsider how to schedule behind load instructions. The first is processor designs that do not stall on unsatisfied load references (called nonblocking loads) 1 through the use of lockup free caches [19, 20, 16, 13], multiple hardware contexts <ref> [2, 1] </ref> or an instruction lookahead scheme [2]. Nonblocking loads allow a processor to continue executing other instructions while a load is in progress. Although the design requires more complex hardware, more instruction level parallelism can be exploited, and therefore programs execute faster. <p> Club." float func (a, b) float *a, *b; f a [2] = b [3]; ) float a [HUGE], b [HUGE]; float new func ( a, b) float * a, * b; f a [2] = b [3]; load of b [3] must be considered dependent on the store of a <ref> [1] </ref>. Our transformation results in new func. <p> If the function func in Figure 8 were produced by f2c, the fortran standard would assume that array a and array b were disjoint; therefore the load for b [3] could be scheduled before the store of a <ref> [1] </ref>. However, the C semantics for func insert a true data dependence between the store of a [1] and the load of b [3]. This dependence is an artifact of the Fortran-to-C translation and does not exist in the original program. <p> Figure 8 were produced by f2c, the fortran standard would assume that array a and array b were disjoint; therefore the load for b [3] could be scheduled before the store of a <ref> [1] </ref>. However, the C semantics for func insert a true data dependence between the store of a [1] and the load of b [3]. This dependence is an artifact of the Fortran-to-C translation and does not exist in the original program. Our compiler takes advantage of the fortran semantics by performing a parallelism-exposing transformation on the input C programs. <p> This configuration is referred to as L80-N (30,5) and has a mean latency of 7.6. In this case the 30 cycle latency is a reasonable design point, since the cache satisfies most requests. The model is intended to be representative of Alewife-like systems <ref> [1] </ref>, where a commodity processor might be incorporated into a shared memory machine. 5 Experimental Results The first set of results is the percentage improvement in execution time of the balanced scheduler over the traditional scheduler (positive values indicate an improvement due to balanced scheduling).
Reference: [2] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera Computer System. </title> <booktitle> In 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6. </pages> <publisher> The ACM Press/SIGARCH, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Two architectural innovations make it worthwhile to reconsider how to schedule behind load instructions. The first is processor designs that do not stall on unsatisfied load references (called nonblocking loads) 1 through the use of lockup free caches [19, 20, 16, 13], multiple hardware contexts <ref> [2, 1] </ref> or an instruction lookahead scheme [2]. Nonblocking loads allow a processor to continue executing other instructions while a load is in progress. Although the design requires more complex hardware, more instruction level parallelism can be exploited, and therefore programs execute faster. <p> The first is processor designs that do not stall on unsatisfied load references (called nonblocking loads) 1 through the use of lockup free caches [19, 20, 16, 13], multiple hardware contexts [2, 1] or an instruction lookahead scheme <ref> [2] </ref>. Nonblocking loads allow a processor to continue executing other instructions while a load is in progress. Although the design requires more complex hardware, more instruction level parallelism can be exploited, and therefore programs execute faster. The second innovation is machines that have a large variance in memory response time. <p> Table 2: The benchmarks in the "Perfect Club." float func (a, b) float *a, *b; f a <ref> [2] </ref> = b [3]; ) float a [HUGE], b [HUGE]; float new func ( a, b) float * a, * b; f a [2] = b [3]; load of b [3] must be considered dependent on the store of a [1]. Our transformation results in new func. <p> Table 2: The benchmarks in the "Perfect Club." float func (a, b) float *a, *b; f a <ref> [2] </ref> = b [3]; ) float a [HUGE], b [HUGE]; float new func ( a, b) float * a, * b; f a [2] = b [3]; load of b [3] must be considered dependent on the store of a [1]. Our transformation results in new func. <p> If a ninth load instruction is issued, the processor blocks until one of the eight outstanding loads completes. The third processor model (called len-8) restricts the maximum number of cycles a load instruction can take before blocking, as in the Tera Computer <ref> [2] </ref>. 3 In this model, if a load instruction has been outstanding for eight cycles, the processor blocks until the data is returned. The balanced scheduler has not been specifically configured for any of the processor models.
Reference: [3] <author> ANS X3.9-1978. </author> <title> American National Standard Programming language FORTRAN. </title> <institution> American National Standards Institute, </institution> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Table 2: The benchmarks in the "Perfect Club." float func (a, b) float *a, *b; f a [2] = b <ref> [3] </ref>; ) float a [HUGE], b [HUGE]; float new func ( a, b) float * a, * b; f a [2] = b [3]; load of b [3] must be considered dependent on the store of a [1]. Our transformation results in new func. <p> Table 2: The benchmarks in the "Perfect Club." float func (a, b) float *a, *b; f a [2] = b <ref> [3] </ref>; ) float a [HUGE], b [HUGE]; float new func ( a, b) float * a, * b; f a [2] = b [3]; load of b [3] must be considered dependent on the store of a [1]. Our transformation results in new func. <p> Table 2: The benchmarks in the "Perfect Club." float func (a, b) float *a, *b; f a [2] = b <ref> [3] </ref>; ) float a [HUGE], b [HUGE]; float new func ( a, b) float * a, * b; f a [2] = b [3]; load of b [3] must be considered dependent on the store of a [1]. Our transformation results in new func. <p> Instruction scheduling is effected, because load instructions are not free to move above stores. Since this problem severely restricts a scheduler's ability to exploit load level parallelism, we apply a transformation which more correctly models the dependences in the fortran program and increases the available parallelism. The fortran standard <ref> [3] </ref> specifically disallows aliasing among dummy arguments (formal parameters) if there will be any stores to the dummy arguments. If the function func in Figure 8 were produced by f2c, the fortran standard would assume that array a and array b were disjoint; therefore the load for b [3] could be <p> fortran standard <ref> [3] </ref> specifically disallows aliasing among dummy arguments (formal parameters) if there will be any stores to the dummy arguments. If the function func in Figure 8 were produced by f2c, the fortran standard would assume that array a and array b were disjoint; therefore the load for b [3] could be scheduled before the store of a [1]. However, the C semantics for func insert a true data dependence between the store of a [1] and the load of b [3]. This dependence is an artifact of the Fortran-to-C translation and does not exist in the original program. <p> the fortran standard would assume that array a and array b were disjoint; therefore the load for b <ref> [3] </ref> could be scheduled before the store of a [1]. However, the C semantics for func insert a true data dependence between the store of a [1] and the load of b [3]. This dependence is an artifact of the Fortran-to-C translation and does not exist in the original program. Our compiler takes advantage of the fortran semantics by performing a parallelism-exposing transformation on the input C programs.
Reference: [4] <author> M. Berry, D. Chen, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Samah, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum, and J. Martin. </author> <title> The perfect club: Effective performance evaluation of supercomputers. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 3(3), </volume> <month> Fall </month> <year> 1989. </year>
Reference-contexts: The processor models differed in their ability to exploit load level parallelism; each was coupled with three different memory systems, that exhibit dissimilar latency behavior. Both the balanced scheduler and the traditional scheduler were incorporated into the gcc [21] compiler and generated code for the Perfect Club benchmarks <ref> [4] </ref>. Performance improvements for balanced scheduling averaged 3% to 18% over the traditional list scheduler, for different processor and system model combinations. The remainder of this report is organized as follows. Section 2 introduces balanced scheduling, and 2 section 3 describes the algorithm in more detail. <p> Loop unrolling is an optimization that increases instruction level parallelism. Due to a conflict with the way we use profiling information (section 4.3), gcc's unrolling capability is not usable for these experiments. Therefore, unrolling was performed manually. 4.2 Workload The workload consisted of the Perfect Club suite of benchmarks <ref> [4] </ref>. (See Table 4.2) Since these programs are written in fortran, they were converted to C using f2c [7]. The Fortran-to-C converter produces C programs that correctly represent the semantics of the original fortran programs.
Reference: [5] <author> Bradley Efron. </author> <title> The jackknife, the bootstrap, and other resampling plans. </title> <booktitle> SIAM/CBMS-NSF Regional conference series in applied mathematics volume 38, </booktitle> <address> Philadelphia, PA, </address> <year> 1982. </year>
Reference-contexts: The number 30 represents an arbitrary choice which is large enough to avoid statistical noise. Second, we measure the accuracy of our results by generating confidence intervals. Confidence intervals are computed for percentage improvement using a bootstrapping <ref> [5] </ref> procedure. From the 30 sample runtimes, we randomly draw 30 samples, with replacement, in order to generate a second sample mean. This process is repeated until we have 100 sample means for the block.
Reference: [6] <author> John R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <booktitle> ACM doctoral dissertation award; 1985. </booktitle> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In this report we present a code scheduling algorithm, called balanced scheduling, that has been specifically designed to tolerate a wide range of variance in load latency over the entire execution of a program. Balanced scheduling works within the context of a traditional list scheduler <ref> [9, 15, 23, 8, 6] </ref>, but uses a new method for calculating load instruction weights. <p> Section 4 explains our experimental methodology; Section 5 presents the experimental results. Section 6 discusses extensions and other applications of the balanced scheduling algorithm. The conclusion follows in section 7. 2 Balanced Scheduling The traditional approach to instruction scheduling that considers machine resource constraints is list scheduling <ref> [9, 15, 23, 8, 6] </ref>. The primary data structure used by list schedulers is the code DAG, in which nodes represent instructions and edges represent dependences between them. <p> Our algorithm does not take advantage of load instructions whose first use is outside the basic block containing the load instruction. Some work, namely Trace Scheduling <ref> [6] </ref>, directly addresses load instructions whose first use is in future blocks by creating large basic blocks containing instructions from many blocks. Since trace scheduling uses list scheduling at the lowest level, our work is compatible with a trace scheduler.
Reference: [7] <author> S. I. Feldman, David M. Gay, Mark W. Maimone, and N. L. Schryer. </author> <title> A Fortran-to-C converter. </title> <institution> Computer Science Technical Report 149, AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ 07974, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Therefore, unrolling was performed manually. 4.2 Workload The workload consisted of the Perfect Club suite of benchmarks [4]. (See Table 4.2) Since these programs are written in fortran, they were converted to C using f2c <ref> [7] </ref>. The Fortran-to-C converter produces C programs that correctly represent the semantics of the original fortran programs. However, these C programs are conservative translations: after being compiled by a C compiler, they will most likely execute more slowly than if they were compiled by a fortran compiler.
Reference: [8] <author> Phillip B. Gibbons and Steven S. Muchnick. </author> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> Proceedings of the SIGPLAN 1986 Symposium on Compiler Construction, SIGPLAN Notices, </booktitle> <volume> 21(7), </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: In this report we present a code scheduling algorithm, called balanced scheduling, that has been specifically designed to tolerate a wide range of variance in load latency over the entire execution of a program. Balanced scheduling works within the context of a traditional list scheduler <ref> [9, 15, 23, 8, 6] </ref>, but uses a new method for calculating load instruction weights. <p> Section 4 explains our experimental methodology; Section 5 presents the experimental results. Section 6 discusses extensions and other applications of the balanced scheduling algorithm. The conclusion follows in section 7. 2 Balanced Scheduling The traditional approach to instruction scheduling that considers machine resource constraints is list scheduling <ref> [9, 15, 23, 8, 6] </ref>. The primary data structure used by list schedulers is the code DAG, in which nodes represent instructions and edges represent dependences between them. <p> A set of heuristics is then applied to decide which instruction from the ready list should be scheduled next; the heuristics used depend on the particular list scheduler. For example, Gibbons and Muchnick <ref> [8] </ref> first schedule the instruction with the greatest operation latency. If more than one instruction qualifies, their scheduler breaks the tie by choosing the instruction (s) with the greatest number of successors.
Reference: [9] <author> John L. Hennessy and Thomas R. Gross. </author> <title> Code generation and reorganization in the presence of pipeline constraints. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 120-127, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: In this report we present a code scheduling algorithm, called balanced scheduling, that has been specifically designed to tolerate a wide range of variance in load latency over the entire execution of a program. Balanced scheduling works within the context of a traditional list scheduler <ref> [9, 15, 23, 8, 6] </ref>, but uses a new method for calculating load instruction weights. <p> Section 4 explains our experimental methodology; Section 5 presents the experimental results. Section 6 discusses extensions and other applications of the balanced scheduling algorithm. The conclusion follows in section 7. 2 Balanced Scheduling The traditional approach to instruction scheduling that considers machine resource constraints is list scheduling <ref> [9, 15, 23, 8, 6] </ref>. The primary data structure used by list schedulers is the code DAG, in which nodes represent instructions and edges represent dependences between them.
Reference: [10] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The first processor model (called unlimited) can dispatch non-blocking load instructions with no limit on the number of loads outstanding. This model is similar to theoretical dataflow machines <ref> [10] </ref>. It is of interest because it exposes the maximum benefit that processor parallelism can achieve. The second (called max-8) allows a maximum of eight load instructions to be simultaneously executing. If a ninth load instruction is issued, the processor blocks until one of the eight outstanding loads completes.
Reference: [11] <author> Mark Donald Hill. </author> <title> Aspects of Cache Memory and Instruction Buffer Performance. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: The model represents a typical workstation-class risc processor that implements nonblocking load instructions, such as the Motorola 88000 series [16]. It is simulated with cache hit rates of 80% and 95%, modeling first level caches of 4K and 32K bytes, respectively <ref> [11] </ref>. Four configurations are modeled, and are referred to as Lhr (hl,ml), where Lhr stands for lockup-free caches with a hit rate of hr, and hl and ml are hit and miss latencies, respectively. The second model has a memory interconnection network and no cache.
Reference: [12] <author> Gerry Kane. </author> <title> mips RISC Architecture. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: The final heuristic selects the instruction that was generated the earliest. Our list scheduler is a bottom-up scheduler, therefore we generate schedules in reverse order by scheduling from the leaves of the Code DAG toward the roots. The compiler has been configured for the mips risc processor <ref> [12] </ref>. gcc's intermediate language, rtl, is not sufficiently risc-like for an instruction scheduler to get maximum benefit, since some primitive operations in rtl are actually multi-cycle macros. In the context of this work, memory-to-memory copies are the most notable, since it is load instructions that we are concentrating on scheduling.
Reference: [13] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <year> 1981. </year> <month> 23 </month>
Reference-contexts: Two architectural innovations make it worthwhile to reconsider how to schedule behind load instructions. The first is processor designs that do not stall on unsatisfied load references (called nonblocking loads) 1 through the use of lockup free caches <ref> [19, 20, 16, 13] </ref>, multiple hardware contexts [2, 1] or an instruction lookahead scheme [2]. Nonblocking loads allow a processor to continue executing other instructions while a load is in progress. Although the design requires more complex hardware, more instruction level parallelism can be exploited, and therefore programs execute faster.
Reference: [14] <author> Monica Lam. </author> <title> Software pipelining: An effective scheduling technique for vliw machines. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, SIGPLAN Notices, </booktitle> <pages> pages 318-328, </pages> <year> 1988. </year>
Reference-contexts: Some work, namely Trace Scheduling [6], directly addresses load instructions whose first use is in future blocks by creating large basic blocks containing instructions from many blocks. Since trace scheduling uses list scheduling at the lowest level, our work is compatible with a trace scheduler. Another technique, software pipelining <ref> [18, 14] </ref>, actively generates schedules where the first use of the register defined by a load instruction can be along cyclic edges in the flow graph. In particular, it can be in a future iteration of the same block with the load instruction.
Reference: [15] <author> E. Lawler, J. K. Lenstra, C. Martel, B. Simons, and L. Stockmeyer. </author> <title> Pipeline scheduling: A survey. </title> <institution> Research Report RJ-5738, IBM, </institution> <month> July </month> <year> 1987. </year>
Reference-contexts: In this report we present a code scheduling algorithm, called balanced scheduling, that has been specifically designed to tolerate a wide range of variance in load latency over the entire execution of a program. Balanced scheduling works within the context of a traditional list scheduler <ref> [9, 15, 23, 8, 6] </ref>, but uses a new method for calculating load instruction weights. <p> Section 4 explains our experimental methodology; Section 5 presents the experimental results. Section 6 discusses extensions and other applications of the balanced scheduling algorithm. The conclusion follows in section 7. 2 Balanced Scheduling The traditional approach to instruction scheduling that considers machine resource constraints is list scheduling <ref> [9, 15, 23, 8, 6] </ref>. The primary data structure used by list schedulers is the code DAG, in which nodes represent instructions and edges represent dependences between them.
Reference: [16] <author> Motorola. </author> <title> MC88100 RISC Microprocessor User's Manual. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Two architectural innovations make it worthwhile to reconsider how to schedule behind load instructions. The first is processor designs that do not stall on unsatisfied load references (called nonblocking loads) 1 through the use of lockup free caches <ref> [19, 20, 16, 13] </ref>, multiple hardware contexts [2, 1] or an instruction lookahead scheme [2]. Nonblocking loads allow a processor to continue executing other instructions while a load is in progress. Although the design requires more complex hardware, more instruction level parallelism can be exploited, and therefore programs execute faster. <p> The first has a data cache. A load instruction's data is returned after 2 cycles on a cache hit and either 5 or 10 cycles on a cache miss. The model represents a typical workstation-class risc processor that implements nonblocking load instructions, such as the Motorola 88000 series <ref> [16] </ref>. It is simulated with cache hit rates of 80% and 95%, modeling first level caches of 4K and 32K bytes, respectively [11].
Reference: [17] <author> Krishna V. Palem and Barbara B. Simons. </author> <title> Scheduling time-critical instructions on RISC machines. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1990. </year>
Reference-contexts: The final heuristic picks the the instruction with the largest sum of the latencies along the longest path from the instruction node to a leaf node. Other styles of list schedulers include those that combine several levels of heuristics into a single weight and schedule in decreasing weight order <ref> [17, 24] </ref> and update scheduling weights dynamically [23]. Our heuristics are described in detail in Section 4.1. If a processor exposes the variations in actual memory reference latency to the compiler through non-blocking load instructions, instruction scheduling becomes more complicated.
Reference: [18] <author> B. R. Rau and C. D. Glaeser. </author> <title> Some scheduling techniques and an easily schedulable horizontal architecture for high performance scientific computing. </title> <booktitle> In Proc. 14th Annual Symposium on Microprogramming, </booktitle> <pages> pages 183-198. </pages> <publisher> SIGMICRO, IEEE, </publisher> <month> October </month> <year> 1981. </year>
Reference-contexts: Some work, namely Trace Scheduling [6], directly addresses load instructions whose first use is in future blocks by creating large basic blocks containing instructions from many blocks. Since trace scheduling uses list scheduling at the lowest level, our work is compatible with a trace scheduler. Another technique, software pipelining <ref> [18, 14] </ref>, actively generates schedules where the first use of the register defined by a load instruction can be along cyclic edges in the flow graph. In particular, it can be in a future iteration of the same block with the load instruction.
Reference: [19] <author> C. Scheurich and M. Dubois. </author> <title> Lockup-free caches in high-performance multiprocessors. </title> <journal> Journal of Parallel and Distributed Processing, </journal> <volume> 11(1) </volume> <pages> 25-36, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Two architectural innovations make it worthwhile to reconsider how to schedule behind load instructions. The first is processor designs that do not stall on unsatisfied load references (called nonblocking loads) 1 through the use of lockup free caches <ref> [19, 20, 16, 13] </ref>, multiple hardware contexts [2, 1] or an instruction lookahead scheme [2]. Nonblocking loads allow a processor to continue executing other instructions while a load is in progress. Although the design requires more complex hardware, more instruction level parallelism can be exploited, and therefore programs execute faster.
Reference: [20] <author> G. S. Sohi and M. Franklin. </author> <title> High-bandwidth data memory systems for superscalar processor. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 53-62, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Two architectural innovations make it worthwhile to reconsider how to schedule behind load instructions. The first is processor designs that do not stall on unsatisfied load references (called nonblocking loads) 1 through the use of lockup free caches <ref> [19, 20, 16, 13] </ref>, multiple hardware contexts [2, 1] or an instruction lookahead scheme [2]. Nonblocking loads allow a processor to continue executing other instructions while a load is in progress. Although the design requires more complex hardware, more instruction level parallelism can be exploited, and therefore programs execute faster.
Reference: [21] <author> Richard Stallman. </author> <title> The GNU project optimizing C compiler. Free Software Foundation, </title> <publisher> Inc., </publisher> <address> 675 Mass Ave, Cambridge, MA 02139, USA. </address>
Reference-contexts: The processor models differed in their ability to exploit load level parallelism; each was coupled with three different memory systems, that exhibit dissimilar latency behavior. Both the balanced scheduler and the traditional scheduler were incorporated into the gcc <ref> [21] </ref> compiler and generated code for the Perfect Club benchmarks [4]. Performance improvements for balanced scheduling averaged 3% to 18% over the traditional list scheduler, for different processor and system model combinations. The remainder of this report is organized as follows. <p> We used several alternatives for each model, to demonstrate that balanced scheduling works well on architectures that contribute to latency uncertainty in different ways. The processor and system models we used are described in sections 4.4 and 4.5. 4.1 Compiler We modified the gnu gcc version 2.2.2 compiler <ref> [21] </ref> to perform balanced instruction scheduling. The default instruction scheduler within gcc was replaced by a new module that can schedule using either the traditional or balanced approaches. In addition, several changes were made to gcc to increase scheduling effectiveness and improve instruction level parallelism.
Reference: [22] <author> Robert Endre Tarjan. </author> <title> Data Structures and Network Algorithms, </title> <booktitle> volume 44 of Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, Pennsylvania, </address> <year> 1983. </year>
Reference-contexts: Connected component analysis is done for each instruction in the code DAG; therefore, the entire algorithm has a worst case time complexity of O (n 2 ff n). 2 ff is the inverse Ackerman function. As a function of n, it increases very slowly and may be considered constant <ref> [22] </ref>. 7 Contribution by Total Load L1 L2 L3 L4 L5 L6 X1 X2 X3 X4 Weight L1 0 1 1 1 1 1 1 1 1 1 10 L3 1/4 0 0 0 0 0 1/3 1/3 1/3 1/3 2 5/12 L5 1/4 0 0 1/2 0 0 1/3 1/3
Reference: [23] <author> H. S. Warren, Jr. </author> <title> Instruction scheduling for the IBM RISC System/6000 processor. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 34(1), </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: In this report we present a code scheduling algorithm, called balanced scheduling, that has been specifically designed to tolerate a wide range of variance in load latency over the entire execution of a program. Balanced scheduling works within the context of a traditional list scheduler <ref> [9, 15, 23, 8, 6] </ref>, but uses a new method for calculating load instruction weights. <p> Section 4 explains our experimental methodology; Section 5 presents the experimental results. Section 6 discusses extensions and other applications of the balanced scheduling algorithm. The conclusion follows in section 7. 2 Balanced Scheduling The traditional approach to instruction scheduling that considers machine resource constraints is list scheduling <ref> [9, 15, 23, 8, 6] </ref>. The primary data structure used by list schedulers is the code DAG, in which nodes represent instructions and edges represent dependences between them. <p> Other styles of list schedulers include those that combine several levels of heuristics into a single weight and schedule in decreasing weight order [17, 24] and update scheduling weights dynamically <ref> [23] </ref>. Our heuristics are described in detail in Section 4.1. If a processor exposes the variations in actual memory reference latency to the compiler through non-blocking load instructions, instruction scheduling becomes more complicated.
Reference: [24] <author> Michael J. Woodard. </author> <title> Personal communication. Scheduling techniques used in Sun SPARC compilers, </title> <month> September </month> <year> 1992. </year> <month> 24 </month>
Reference-contexts: The final heuristic picks the the instruction with the largest sum of the latencies along the longest path from the instruction node to a leaf node. Other styles of list schedulers include those that combine several levels of heuristics into a single weight and schedule in decreasing weight order <ref> [17, 24] </ref> and update scheduling weights dynamically [23]. Our heuristics are described in detail in Section 4.1. If a processor exposes the variations in actual memory reference latency to the compiler through non-blocking load instructions, instruction scheduling becomes more complicated. <p> In particular, it can be in a future iteration of the same block with the load instruction. In that case, the first use precedes the load instruction in the code DAG. A technique such as Palem and Simons'[17] or Woodard's <ref> [24] </ref> might provide some of the benefits of balancing in the software pipelined environment. Superscalar processors are fully supported by the balanced scheduling algorithm as presented here.
References-found: 24

