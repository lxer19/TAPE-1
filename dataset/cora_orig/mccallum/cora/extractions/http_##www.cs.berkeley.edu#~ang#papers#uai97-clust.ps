URL: http://www.cs.berkeley.edu/~ang/papers/uai97-clust.ps
Refering-URL: http://www.cs.berkeley.edu/~ang/
Root-URL: http://www.cs.berkeley.edu/~ang/
Title: An Information-Theoretic Analysis of Hard and Soft Assignment Methods for Clustering  
Author: Michael Kearns Yishay Mansour Andrew Y. Ng 
Address: Florham Park, New Jersey  Tel Aviv, Israel  Pittsburgh, Pennsylvania  
Affiliation: AT&T Labs Research  Tel Aviv University  Carnegie Mellon University  
Abstract: Assignment methods are at the heart of many algorithms for unsupervised learning and clustering | in particular, the well-known K-means and Expectation-Maximization (EM) algorithms. In this work, we study several different methods of assignment, including the "hard" assignments used by K-means and the "soft" assignments used by EM. While it is known that K-means minimizes the distortion on the data and EM maximizes the likelihood, little is known about the systematic differences of behavior between the two algorithms. Here we shed light on these differences via an information-theoretic analysis. The cornerstone of our results is a simple decomposition of the expected distortion, showing that K-means (and its extension for inferring general parametric densities from unlabeled sample data) must implicitly manage a trade-off between how similar the data assigned to each cluster are, and how the data are balanced among the clusters. How well the data are balanced is measured by the entropy of the partition defined by the hard assignments. In addition to letting us predict and verify systematic differences between K-means and EM on specific examples, the decomposition allows us to give a rather general argument showing that K-means will consistently find densities with less "overlap" than EM. We also study a third natural assignment method that we call posterior assignment, that is close in spirit to the soft assignments of EM, but leads to a surprisingly different algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.M. Cover and J.A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley-Interscience, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Algorithms for density estimation, clustering and unsupervised learning are an important tool in machine learning. Two classical algorithms are the K-means algorithm <ref> [7, 1, 3] </ref> and the Expectation-Maximization (EM) algorithm [2]. These algorithms have been applied in a wide variety of settings, including parameter estimation in hidden Markov models for speech recognition [8], estimation of conditional probability tables in belief networks for probabilistic inference [6], and various clustering problems [3]. <p> For any class P of densities over a space X, weighted K-means over P takes as input a set S of data points and outputs a pair of densities P 0 ; P 1 2 P, as well as a weight ff 0 2 <ref> [0; 1] </ref>. (Again, the generalization to the case of K densities and K weights is straightforward.) The algorithm begins with random choices for the P b 2 P and ff 0 , and then repeatedly executes the following three steps: * (WTA Assignment) Set S 0 to be the set of <p> ; oe 0 ) and P 1 = N ( 1 ; oe 1 ), where 0 ; oe 0 ; 1 ; oe 1 2 &lt; are the parameters to be adjusted by the algorithms. (The weighted versions of both algorithms also output the weight parameter ff 0 2 <ref> [0; 1] </ref>.) In the case of EM, the output is interpreted as representing a mixture distribution, which is evaluated by its KL divergence from the sampling density.
Reference: [2] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the em algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-39, </pages> <year> 1977. </year>
Reference-contexts: 1 Introduction Algorithms for density estimation, clustering and unsupervised learning are an important tool in machine learning. Two classical algorithms are the K-means algorithm [7, 1, 3] and the Expectation-Maximization (EM) algorithm <ref> [2] </ref>. These algorithms have been applied in a wide variety of settings, including parameter estimation in hidden Markov models for speech recognition [8], estimation of conditional probability tables in belief networks for probabilistic inference [6], and various clustering problems [3]. <p> The EM algorithm can be used to find a mixture density model of the form ff 0 P 0 +(1 ff 0 )P 1 . It is known that the mixture model found by EM will be a local minimum of the log-loss <ref> [2] </ref> (which is equivalent to a local maximum of the likelihood), the empirical analogue of the KL divergence. The K-means algorithm is often viewed as a vector quantization algorithm (and is sometimes referred to as the Lloyd-Max algorithm in the vector quantization literature).
Reference: [3] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction Algorithms for density estimation, clustering and unsupervised learning are an important tool in machine learning. Two classical algorithms are the K-means algorithm <ref> [7, 1, 3] </ref> and the Expectation-Maximization (EM) algorithm [2]. These algorithms have been applied in a wide variety of settings, including parameter estimation in hidden Markov models for speech recognition [8], estimation of conditional probability tables in belief networks for probabilistic inference [6], and various clustering problems [3]. <p> These algorithms have been applied in a wide variety of settings, including parameter estimation in hidden Markov models for speech recognition [8], estimation of conditional probability tables in belief networks for probabilistic inference [6], and various clustering problems <ref> [3] </ref>. At a high level, K-means and EM appear rather similar: both perform a two-step iterative optimization, performed repeatedly until convergence. <p> However, relatively little seems to be known about the precise relationship between the two loss functions and their attendant heuristics. The structural similarity of EM and K-means often leads to their being considered closely related or even roughly equivalent. Indeed, Duda and Hart <ref> [3] </ref> go as far as saying that K-means can be viewed as "an approximate way to obtain maximum likelihood estimates for the means", which is the goal of density estimation in general and EM in particular.
Reference: [4] <author> A. Gersho. </author> <title> On the structure of vector quantiz-ers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 28(2) </volume> <pages> 157-166, </pages> <year> 1982. </year>
Reference-contexts: Note that although K-means will not increase the K-means loss at any iteration, this does not mean that each of the terms in Equation (8) will not increase; indeed, we will see examples where this is not the case. It has been often observed in the vector quantization literature <ref> [4] </ref> that at each iteration, the means estimated by K-means must in fact be the true means of the points assigned to them | but this does not imply, for instance, that the terms KL (Q b jjP b ) are nonincreasing (because, for example, Q b can also change with
Reference: [5] <author> J. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: It is interesting to note that clustering algorithms in which data points have explicit repulsive effects on distant centroids have been proposed in the literature on K-means and self-organizing maps <ref> [5] </ref>. From the preceding discussion, it might be natural to expect that, as for K-means, minimizing the posterior loss over a density class P would be more likely to lead to P 0 and P 1 that are "different" from one another than, say, classical density estimation over P.
Reference: [6] <author> S. L. Lauritzen. </author> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 19 </volume> <pages> 191-201, </pages> <year> 1995. </year>
Reference-contexts: Two classical algorithms are the K-means algorithm [7, 1, 3] and the Expectation-Maximization (EM) algorithm [2]. These algorithms have been applied in a wide variety of settings, including parameter estimation in hidden Markov models for speech recognition [8], estimation of conditional probability tables in belief networks for probabilistic inference <ref> [6] </ref>, and various clustering problems [3]. At a high level, K-means and EM appear rather similar: both perform a two-step iterative optimization, performed repeatedly until convergence.
Reference: [7] <author> J. MacQueen. </author> <title> Some methods for classification and analysis of multivariate observations. </title> <booktitle> In Proceedings of the Fifth Berkeley Symposium on Mathematics, Statistics and Probability, </booktitle> <volume> volume 1, </volume> <pages> pages 281-296, </pages> <year> 1967. </year>
Reference-contexts: 1 Introduction Algorithms for density estimation, clustering and unsupervised learning are an important tool in machine learning. Two classical algorithms are the K-means algorithm <ref> [7, 1, 3] </ref> and the Expectation-Maximization (EM) algorithm [2]. These algorithms have been applied in a wide variety of settings, including parameter estimation in hidden Markov models for speech recognition [8], estimation of conditional probability tables in belief networks for probabilistic inference [6], and various clustering problems [3]. <p> The K-means algorithm is often viewed as a vector quantization algorithm (and is sometimes referred to as the Lloyd-Max algorithm in the vector quantization literature). It is known that K-means will find a local minimum of the distortion or quantization error on the data <ref> [7] </ref>, which we will discuss at some length. Thus, for both the fractional and WTA assignment methods, there is a natural and widely used iterative optimization heuristic (EM and K-means, respectively), and it is known what loss function is (locally) minimized by each algorithm (log-loss and distortion, respectively).
Reference: [8] <author> L. Rabiner and B. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: Two classical algorithms are the K-means algorithm [7, 1, 3] and the Expectation-Maximization (EM) algorithm [2]. These algorithms have been applied in a wide variety of settings, including parameter estimation in hidden Markov models for speech recognition <ref> [8] </ref>, estimation of conditional probability tables in belief networks for probabilistic inference [6], and various clustering problems [3]. At a high level, K-means and EM appear rather similar: both perform a two-step iterative optimization, performed repeatedly until convergence.
References-found: 8

