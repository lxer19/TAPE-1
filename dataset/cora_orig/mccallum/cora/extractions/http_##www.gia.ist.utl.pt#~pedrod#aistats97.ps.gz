URL: http://www.gia.ist.utl.pt/~pedrod/aistats97.ps.gz
Refering-URL: http://www.gia.ist.utl.pt/~pedrod/
Root-URL: 
Email: pedrod@ics.uci.edu  
Title: Bayesian Model Averaging in Rule Induction  
Author: Pedro Domingos 
Keyword: Predictive modeling, classification, rule induction, model uncertainty, multiple models, Bayesian methods.  
Note: Partly supported by JNICT/PRAXIS XXI and NATO scholarships. Thanks also to Dennis Kibler.  
Web: http://www.ics.uci.edu/~pedrod  
Address: Irvine, California 92697, U.S.A.  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: Bayesian model averaging (BMA) can be seen as the optimal approach to any induction task. It can reduce error by accounting for model uncertainty in a principled way, and its usefulness in several areas has been empirically verified. However, few attempts to apply it to rule induction have been made. This paper reports a series of experiments designed to test the utility of BMA in this field. BMA is applied to combining multiple rule sets learned from different subsets of the training data, to combining multiple rules covering a test example, to inducing technical rules for foreign exchange trading, and to inducing conjunctive concepts. In the first two cases, BMA is observed to produce lower accuracies than the ad hoc methods it is compared with. In the last two cases, BMA is observed to typically produce the same result as simply using the best (maximum-likelihood) rule, even though averaging is performed over all possible rules in the space, the domains are highly noisy, and the samples are medium-to small-sized. In all cases, this is observed to be due to BMA's consistent tendency to assign highly asymmetric weights to different models, even when their accuracy differs by little, with most models (often all but one) effectively having no influence on the outcome. Thus the effective number of models being averaged is much smaller for BMA than for common ad hoc methods, leading to a smaller reduction in variance. This suggests that the success of the multiple models approach to rule induction is primarily due to this variance reduction, and not to its being a closer approximation to the Bayesian ideal. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Ali and M. Pazzani. </author> <title> Classification using Bayes averaging of multiple, relational rule-based models. </title> <editor> In D. Fisher and H.-J. Lenz, editors, </editor> <title> Learning from Data: </title> <journal> Artificial Intelligence and Statistics V, </journal> <pages> pages 207-217. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1996. </year>
Reference-contexts: Buntine [4] has successfully applied this approach to decision tree induction (see also [16]). Applications of BMA to rule induction have been carried out by Kononenko [10] and by Ali and Pazzani <ref> [1] </ref>. The latter found that it often improved accuracy relative to using the single "best" rule set. <p> This ad hoc approach was compared with BMA on eight of the larger databases in the UCI repository [14] 2 . BMA was applied in the following form, very similar to that of [4] and <ref> [1] </ref>. Let n be the sample size, ~x the training examples, ~c the corresponding class labels, and H the set of models induced (i.e., each element h of H is a rule set).
Reference: [2] <author> J. O. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: Most combination methods in the literature are ad hoc in the sense that they have been empirically observed to work well, but have no firm theoretical foundation. The Bayesian approach <ref> [2] </ref> provides such a foundation, and the hope is that closer adherence to it will produce improved results over more heuristic methods. In essence, Bayesian model averaging (BMA) weights each model by its posterior probability (i.e., its probability given the observed data).
Reference: [3] <author> L. Breiman. </author> <title> Bias, variance and arcing classifiers. </title> <type> Technical Report 460, </type> <institution> Statistics Department, University of California at Berkeley, Berkeley, </institution> <address> CA, </address> <year> 1996. </year> <note> Available electronically as ftp://ftp.stat.berkeley.edu/users/breiman/arcall.ps.Z. </note>
Reference-contexts: An alternative view of multiple models attributes the error reduction they produce to variance reduction <ref> [3, 9] </ref>, and views giving a significant weight to a model as potentially useful, even if that model is not as good as the best.
Reference: [4] <author> W. L. Buntine. </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> School of Computing Science, University of Technology, </institution> <address> Sydney, Australia, </address> <year> 1990. </year>
Reference-contexts: A plausible approximation is then to only attempt to find several of the most probable models, and average over these. Buntine <ref> [4] </ref> has successfully applied this approach to decision tree induction (see also [16]). Applications of BMA to rule induction have been carried out by Kononenko [10] and by Ali and Pazzani [1]. The latter found that it often improved accuracy relative to using the single "best" rule set. <p> This ad hoc approach was compared with BMA on eight of the larger databases in the UCI repository [14] 2 . BMA was applied in the following form, very similar to that of <ref> [4] </ref> and [1]. Let n be the sample size, ~x the training examples, ~c the corresponding class labels, and H the set of models induced (i.e., each element h of H is a rule set). <p> Let r be this rule, n r the total number of examples it wins, and n r;c i the number of examples of class c i that it wins. Then: ^ P r (x i ; c i jh) = n r This is analogous to the treatment in <ref> [4] </ref>, using the partition induced by the rules in the same way [4] uses the partition induced by a decision tree. <p> Then: ^ P r (x i ; c i jh) = n r This is analogous to the treatment in <ref> [4] </ref>, using the partition induced by the rules in the same way [4] uses the partition induced by a decision tree. <p> However, the results described in this paper are still surprising, and require interpretation. If multiple models derive their power from being a closer approximation of the Bayesian ideal, as Buntine <ref> [4] </ref> suggests, then BMA should produce higher accuracy than ad hoc averaging.
Reference: [5] <author> P. Chan, S. Stolfo, and D. Wolpert, </author> <title> editors. Proceedings of the AAAI-96 Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms. </title> <publisher> AAAI Press, </publisher> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: When multiple plausible models exist, ignoring all but one will in general be suboptimal with respect to the goal of minimizing error. Combining multiple models in some way then becomes an attractive alternative, and it has recently received much attention (e.g., <ref> [5] </ref>). Most combination methods in the literature are ad hoc in the sense that they have been empirically observed to work well, but have no firm theoretical foundation.
Reference: [6] <author> P. Clark and R. Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the Sixth European Working Session on Learning, </booktitle> <pages> pages 151-163, </pages> <address> Porto, Portugal, </address> <year> 1991. </year> <title> Springer-Verlag. 8 This might not be true if those other models all voted in the same direction; but in practice this is very unlikely, and they will tend to cancel each other out, further enhancing this effect. </title>
Reference-contexts: Recent versions of the CN2 algorithm <ref> [6] </ref> let each rule vote for each class with the number of examples of that class it covers. C4.5RULES [17] gives precedence to rules of the class for which the fewest false positives are produced.
Reference: [7] <author> P. Domingos. </author> <title> Unifying instance-based and rule-based induction. </title> <journal> Machine Learning, </journal> <volume> 24 </volume> <pages> 141-168, </pages> <year> 1996. </year>
Reference-contexts: The latter found that it often improved accuracy relative to using the single "best" rule set. In this paper we compare BMA with ad hoc methods for combining multiple rule sets. 2 Bayesian Averaging of Rule Sets RISE <ref> [7] </ref> is a rule induction system that assigns each test example to the class of the nearest rule according to a similarity measure, and thus implicitly partitions the instance space into the regions won by each of the rules.
Reference: [8] <author> P. Domingos. </author> <title> Using partitioning to speed up specific-to-general rule induction. </title> <booktitle> In Proceedings of the AAAI-96 Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms, </booktitle> <address> Portland, OR, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This combination was originally performed by letting each rule set vote for the class it predicts, with a weight equal to the training-set accuracy 1 of the rule that won the example in that rule set <ref> [8] </ref>. This ad hoc approach was compared with BMA on eight of the larger databases in the UCI repository [14] 2 . BMA was applied in the following form, very similar to that of [4] and [1].
Reference: [9] <author> J. H. Friedman. </author> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. </title> <type> Technical report, </type> <institution> Department of Statistics and Stanford Linear Accelerator Center, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1996. </year> <note> Available electronically as ftp://playfair.stanford.edu/pub/friedman/kdd.ps.Z. </note>
Reference-contexts: An alternative view of multiple models attributes the error reduction they produce to variance reduction <ref> [3, 9] </ref>, and views giving a significant weight to a model as potentially useful, even if that model is not as good as the best.
Reference: [10] <author> I. Kononenko. </author> <title> Combining decisions of multiple rules. </title> <editor> In B. du Boulay and V. Sgurev, editors, </editor> <booktitle> Artificial Intelligence V: Methodology, Systems, Applications, </booktitle> <pages> pages 87-96. </pages> <address> El-sevier, Amsterdam, </address> <year> 1992. </year>
Reference-contexts: A plausible approximation is then to only attempt to find several of the most probable models, and average over these. Buntine [4] has successfully applied this approach to decision tree induction (see also [16]). Applications of BMA to rule induction have been carried out by Kononenko <ref> [10] </ref> and by Ali and Pazzani [1]. The latter found that it often improved accuracy relative to using the single "best" rule set.
Reference: [11] <author> B. LeBaron. </author> <title> Technical trading rules and regime shifts in foreign exchange. </title> <type> Technical report, </type> <institution> Department of Economics, University of Wisconsin at Madison, Madison, WI, </institution> <year> 1991. </year>
Reference-contexts: An approach that is used by some traders, and that has been validated by large-scale empirical studies <ref> [11] </ref>, involves the use of so-called technical rules of the form "If the s-day moving average of the currency's exchange rate rises above the t-day one, buy; else sell." The choice of s and t, with t &gt; s, can be made empirically.
Reference: [12] <author> D. MacKay. </author> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 415-447, </pages> <year> 1992. </year>
Reference-contexts: prediction as the "best" conjunction 83.9% of the time for a = 7, decreasing to 64:4% for a = 13. 7 6 Discussion It is well known that, in some applications, the model with highest posterior probability dominates all others, making BMA equivalent to approaches that simply pick that model <ref> [12] </ref>. However, the results described in this paper are still surprising, and require interpretation. If multiple models derive their power from being a closer approximation of the Bayesian ideal, as Buntine [4] suggests, then BMA should produce higher accuracy than ad hoc averaging.
Reference: [13] <author> D. Madigan, A. E. Raftery, C. T. Volinsky, and J. A. Hoeting. </author> <title> Bayesian model averaging. </title> <booktitle> In Proceedings of the AAAI-96 Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms, </booktitle> <address> Portland, OR, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: In essence, Bayesian model averaging (BMA) weights each model by its posterior probability (i.e., its probability given the observed data). In areas such as regression and discrete graphical models, it has been verified to produce improvements over using the single most likely model <ref> [13] </ref>). Applications of BMA to machine learning methods have been sparser. One of the main difficulties is that, in most cases, the number of possible models is far too large to be exhaustively considered, and no closed form for the relevant sums (or integrals) is known.
Reference: [14] <author> C. J. Merz, P. M. Murphy, and D. W. Aha. </author> <title> UCI repository of machine learning databases. Machine-readable data repository, </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA, </address> <year> 1996. </year>
Reference-contexts: This ad hoc approach was compared with BMA on eight of the larger databases in the UCI repository <ref> [14] </ref> 2 . BMA was applied in the following form, very similar to that of [4] and [1]. Let n be the sample size, ~x the training examples, ~c the corresponding class labels, and H the set of models induced (i.e., each element h of H is a rule set).
Reference: [15] <author> T. Niblett. </author> <title> Constructing decision trees in noisy domains. </title> <booktitle> In Proceedings of the Second European Working Session on Learning, </booktitle> <pages> pages 67-78, </pages> <address> Bled, Yugoslavia, </address> <year> 1987. </year> <pages> Sigma. </pages>
Reference-contexts: Let n be the sample size, ~x the training examples, ~c the corresponding class labels, and H the set of models induced (i.e., each element h of H is a rule set). Then, by Bayes's Theorem, and assuming the examples are drawn independently: 1 With the Laplace correction <ref> [15] </ref>. 2 Credit, diabetes, annealing, chess, hypothyroid, splice junctions, mushroom and shuttle. P r (hj~x; ~c) = P r (~x; ~c) i=1 where the data prior P r (~x; ~c) is the same for all models, and can be ignored.
Reference: [16] <author> J. J. Oliver and D. J. </author> <title> Hand. On pruning and averaging decision trees. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 430-437, </pages> <address> Tahoe City, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A plausible approximation is then to only attempt to find several of the most probable models, and average over these. Buntine [4] has successfully applied this approach to decision tree induction (see also <ref> [16] </ref>). Applications of BMA to rule induction have been carried out by Kononenko [10] and by Ali and Pazzani [1]. The latter found that it often improved accuracy relative to using the single "best" rule set.
Reference: [17] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Recent versions of the CN2 algorithm [6] let each rule vote for each class with the number of examples of that class it covers. C4.5RULES <ref> [17] </ref> gives precedence to rules of the class for which the fewest false positives are produced. ITRULE [18] assumes all the rule left-hand sides are independent of each other given the class, and thus makes direct use of Bayes' theorem.
Reference: [18] <author> P. Smyth, R. M. Goodman, and C. Higgins. </author> <title> A hybrid rule-based/Bayesian classifier. </title> <booktitle> In Proceedings of the Ninth European Conference on Artificial Intelligence, </booktitle> <pages> pages 610-615, </pages> <address> Stockholm, Sweden, 1990. </address> <publisher> Pitman. </publisher>
Reference-contexts: Recent versions of the CN2 algorithm [6] let each rule vote for each class with the number of examples of that class it covers. C4.5RULES [17] gives precedence to rules of the class for which the fewest false positives are produced. ITRULE <ref> [18] </ref> assumes all the rule left-hand sides are independent of each other given the class, and thus makes direct use of Bayes' theorem. BMA can potentially be applied to this problem, since rules can be regarded as alternative models for the region where they intersect.
Reference: [19] <author> A. S. Weigend, B. A. Huberman, and D. E. Rumelhart. </author> <title> Predicting sunspots and exchange rates with connectionist networks. </title> <editor> In M. Casdagli and S. Eubank, editors, </editor> <booktitle> Nonlinear Modeling and Forecasting, </booktitle> <pages> pages 395-432. </pages> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1992. </year>
Reference-contexts: 2: ^ P r (hj~x; ~c) / n ++ ! n ++ n + n + ! n + n (7) A comparison of this approach with the single most accurate rule was carried out using daily data on five currencies for the years 1973-87, from the Chicago Mercantile Exchange <ref> [19] </ref>. The first ten years were used for training (2341 examples) and the remainder for testing. The fact that the domain is extremely noisy (typical accuracies are only slightly above 50%), and that no rule can claim to be the "right" model, favors the use of BMA.
References-found: 19

