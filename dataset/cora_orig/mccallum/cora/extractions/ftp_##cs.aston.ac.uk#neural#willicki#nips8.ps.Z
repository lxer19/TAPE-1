URL: ftp://cs.aston.ac.uk/neural/willicki/nips8.ps.Z
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: c.k.i.williams@aston.ac.uk  carl@cs.toronto.edu  
Title: In Advances in Neural Information Processing Systems 8  Gaussian Processes for Regression  
Author: eds. D. S. Touretzky, M. C. Mozer, M. E. Hasselmo, Christopher K. I. Williams Carl Edward Rasmussen 
Address: Birmingham B4 7ET, UK  Toronto, ONT, M5S 1A4, Canada  
Affiliation: Neural Computing Research Group Aston University  Department of Computer Science University of Toronto  
Note: MIT Press, 1996.  
Abstract: The Bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over functions. In this paper we investigate the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations. Two methods, using optimization and averaging (via Hybrid Monte Carlo) over hyperparameters have been tested on a number of challenging problems and have produced excellent results. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cressie, N. A. C. </author> <year> (1993). </year> <title> Statistics for Spatial Data. </title> <publisher> Wiley. </publisher>
Reference-contexts: Gaussian processes have also been used in the geostatistics field <ref> (e.g. Cressie, 1993) </ref>, and are known there as "kriging", but this literature has concentrated on the case where the input space is two or three dimensional, rather than considering more general input spaces. <p> The theory described in section 2 deals only with the prediction of a scalar quantity Y , so predictors were constructed for the two outputs separately, although a joint prediction is possible within the Gaussian process framework <ref> (see co-kriging, x3.2.3 in Cressie, 1993) </ref>. Two experiments were conducted, the first using only the two "true" inputs, and the second one using all six inputs. In this section we report results using maximum likelihood training; similar results were obtained with HMC.
Reference: <author> Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. </author> <year> (1987). </year> <title> Hybrid Monte Carlo. </title> <journal> Physics Letters B, </journal> <volume> 195 </volume> <pages> 216-222. </pages>
Reference-contexts: It is not feasible to do this integration analytically, but the Markov chain Monte Carlo method of Hybrid Monte Carlo (HMC) <ref> (Duane et al, 1987) </ref> seems promising for this application. We assign broad Gaussians priors to the hyperparameters, and use Hybrid Monte Carlo to give us samples from the posterior.
Reference: <author> Girosi, F., Jones, M., and Poggio, T. </author> <year> (1995). </year> <title> Regularization Theory and Neural Networks Architectures. </title> <journal> Neural Computation, </journal> <volume> 7(2) </volume> <pages> 219-269. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1992). </year> <title> A Practical Bayesian Framework for Backpropagation Networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 448-472. </pages>
Reference-contexts: For neural networks the prior over functions has a complex form which means that implementations must either make approximations <ref> (e.g. MacKay, 1992) </ref> or use Monte Carlo approaches to evaluating integrals (Neal, 1993). As Neal (1995) has argued, there is no reason to believe that, for real-world problems, neural network models should be limited to nets containing only a "small" number of hidden units. <p> The results rank the methods in the order (lowest error first) a full-blown Bayesian treatment of neural networks using HMC, Gaussian 6 processes, ensembles of neural networks trained using cross validation and weight decay, the Evidence framework for neural networks <ref> (MacKay, 1992) </ref>, and MARS. We are currently working on assessing the statistical significance of this ordering. 5 DISCUSSION We have presented the method of regression with Gaussian processes, and shown that it performs well on a suite of real-world problems.
Reference: <author> MacKay, D. J. C. </author> <year> (1993). </year> <title> Bayesian Methods for Backpropagation Networks. </title> <editor> In van Hemmen, J. L., Domany, E., and Schulten, K., editors, </editor> <title> Models of Neural Networks II. </title> <publisher> Springer. </publisher>
Reference: <author> Neal, R. M. </author> <year> (1993). </year> <title> Bayesian Learning via Stochastic Dynamics. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> Vol. 5, </volume> <pages> pages 475-482. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: For neural networks the prior over functions has a complex form which means that implementations must either make approximations (e.g. MacKay, 1992) or use Monte Carlo approaches to evaluating integrals <ref> (Neal, 1993) </ref>. As Neal (1995) has argued, there is no reason to believe that, for real-world problems, neural network models should be limited to nets containing only a "small" number of hidden units.
Reference: <author> Neal, R. M. </author> <year> (1995). </year> <title> Bayesian Learning for Neural Networks. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference: <author> Poggio, T. and Girosi, F. </author> <year> (1990). </year> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of IEEE, </booktitle> <volume> 78 </volume> <pages> 1481-1497. </pages>
Reference: <author> Rasmussen, C. E. </author> <year> (1996). </year> <title> A Practical Monte Carlo Implementation of Bayesian Learning. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Wahba, G. </author> <year> (1990). </year> <title> Spline Models for Observational Data. </title> <booktitle> Society for Industrial and Applied Mathematics. CBMS-NSF Regional Conference series in applied mathematics. </booktitle> <pages> 7 </pages>
Reference-contexts: The v 0 variable gives the overall scale of the local correlations. This covariance function is valid for all input dimen-sionalities as compared to splines, where the integrated squared mth derivative is only a valid regularizer for 2m &gt; d <ref> (see Wahba, 1990) </ref>. a 0 and a 1 are variables controlling the scale the of bias and linear contributions to the covariance. The last term accounts for the noise on the data; v 1 is the variance of the noise. <p> ARMA models used in time series analysis and spline smoothing <ref> (e.g. Wahba, 1990 and earlier references therein) </ref> correspond to Gaussian process prediction with 1 We call the hyperparameters as they correspond closely to hyperparameters in neural networks; in effect the weights have been integrated out exactly. 3 a particular choice of covariance function 2 .
References-found: 10

