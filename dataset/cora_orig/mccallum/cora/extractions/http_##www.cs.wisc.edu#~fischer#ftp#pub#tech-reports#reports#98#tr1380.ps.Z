URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/reports/98/tr1380.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/reports/98/
Root-URL: http://www.cs.wisc.edu
Title: Threshold Data Structures and Coding Theory  
Author: &lt; Feliz Cumplea~nos a Don Manuel Eric Bach Marcos Kiwi 
Date: July 21, 1998  
Abstract: Data structures for combinatorial objects are traditionally designed to handle objects up to a certain size. We introduce the idea of threshold data structures: representations that allow a richer collection of operations on small objects than large ones. As illustrations of the general concept we discuss threshold data structures for sets and multisets, and show how the former can be applied to cache memory design. Consider threshold representations for subsets of a universe of size n, supporting insertions and deletions at any level, and enumeration of sets whose size does not exceed the threshold t. We derive lower bounds on the space used by any such representation. When t is fixed and n ! 1 (the case of interest in memory design), any such representation must use, asymptotically, at least (t + 1) log 2 n bits of memory. Applying the theory of error-correcting codes, we design a structure, efficiently supporting the required operations, whose space consumption matches the lower bound. Similar results are proved for multisets.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anonymous, </author> <title> Product Specification: AHA 4011 10 MBytes/sec Reed-Solomon Error Correction Device, Advanced Hardware Architectures, </title> <publisher> Inc., </publisher> <address> Pullman, Washington, </address> <note> no date. [Web site: http://www.aha.com.] </note>
Reference-contexts: Indeed, commercial Reed-Solomon decoders, which are programmed for differing code sizes and hence do not have the best possible performance, achieve latencies around 20 sec when n = 255 <ref> [1] </ref>. 4 A Lower Bound on Memory Usage The data structure for subsets of discussed in Sect. 2 requires about (t + 1) log 2 jj bits of memory.
Reference: [2] <author> E. Bach and J. Shallit, </author> <title> Algorithmic Number Theory, vol. 1: Efficient Algorithms, </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Finally, the assumption that d and n are large may be unrealistic for applications. For this reason it is worthwhile to see what can be proved using standard arithmetic algorithms, which use O (m 2 ) bit operations to multiply and divide elements of GF (2 m ) <ref> [2, Chap. 6] </ref>. With this assumptions, insertions and deletions cost O (t (log n) 2 ) bit operations. To implement enumerations we must use the Berlekamp-Massey algorithm, which costs O (m 2 d 2 ) as before, and find the d roots of the degree d polynomial g (X). <p> By the Chinese Remainder Theorem we have R ~ = GF (2 m ) GF (2 m ) : We first find a matrix F for the GF (2)-linear map a 7! a 2 on R. This will use O ((md) 3 ) bit operations <ref> [2, Ex. 7.15] </ref>. If F a = 0, we have a = (a 1 ; : : : ; a d ) with a i either 0 or 1, and if a 6= 0; 1, then we can split g by computing gcd (a; g). <p> We note that S 1 6= 0. Our approach has been to reduce a cubic equation to an easily solved quartic. Alternatively, one can extract a square root and a cube root and then solve an auxiliary quadratic equation [8]. Cube root computation can be reduced to exponentiation; see <ref> [2, Thm. 7.3.2] </ref>. Theorem 5 (Enumeration for d = 3.) Let ff 1 ; ff 2 ; ff 3 be distinct. Then D = S 3 1 + S 3 6= 0. <p> achieve this, we first determine the monic squarefree polynomials g 1 ; : : : ; g r and the positive integers e 1 ; : : : ; e r such that g = g e 1 r . (Cost: O (d 3 (log n) 2 ) bit operations <ref> [2, Theorem 7.5.2] </ref>.) Then, for i 2 f1; : : : ; rg, we find the d i roots of g i as described in Sect. 2. (Cost: O ((d i log n) 3 ) bit operations for i 2 f1; : : : ; rg.) Recalling that P r i=1
Reference: [3] <author> E. R. Berlekamp, </author> <title> Algebraic Coding Theory. </title> <publisher> McGraw-Hill, </publisher> <year> 1968. </year>
Reference-contexts: threshold data structures with additional examples. 4 2 A Threshold Data Structure for Dynamic-sets In this section we describe a threshold data structure relying on algorithms from the theory of error-correcting codes, in particular the algebraic decoding algorithm for binary BCH codes invented by Peterson [17] and refined by Berlekamp <ref> [3] </ref> and Massey [15]. We will now describe a t-threshold data structure for subsets of . We henceforth identify with a subset of GF (2 m ) fl , the nonzero elements of the finite field of order 2 m . <p> Suppose that jSj = d, a value that can be read from the counter. Determining the ff's from the power sums is exactly the problem that is solved when decoding binary BCH codes. Let us briefly outline how this is done. Using the Berlekamp-Massey algorithm <ref> [3, 15] </ref>, we take the power sums S 1 ; S 2 ; : : : ; S 2d1 and produce the coefficients of the polynomial f (X) = ff2S In coding theory, this polynomial is called the error locator. <p> Clearly, deletions have the same cost. Now consider enumerations. Note that d = jSj is available from the counter C. We can obtain the required S 2i and run the Berlekamp-Massey algorithm using O (d 2 m 2 ) bit operations <ref> [3, 15] </ref>. Finally, Shoup [19, p. 14] has proved that a degree d polynomial with all its zeroes in GF (2 m ) can be completely factored using O ((d log n) 2+o (1) ) bit operations. Since m = O (log n), the result follows.
Reference: [4] <author> E. R. Berlekamp, H. Rumsey, and G. Solomon, </author> <title> On the solution of algebraic equations over finite fields, </title> <journal> Inform. Control, </journal> <volume> vol. 10, </volume> <pages> pages 553-564, </pages> <year> 1967. </year>
Reference-contexts: On GF (2 m ), squaring is an invertible GF (2)-linear map, given in the power basis by a matrix we will denote by F . Berlekamp, Rumsey, and Solomon <ref> [4] </ref> observed that square roots in GF (2 m ) can be found by applying F 1 . Their method for solving the quadratic equation y 2 + y = fl ; whose left hand side is a GF (2)-linear function of y, goes as follows. <p> Then 0 @ . . . 1 A = U 1 B fl m1 fl 1 C Taking y 0 = 0; 1 gives two values for y. (The equation has two solutions or none. We will only be concerned with the first case. For a solvability criterion, see <ref> [4, p. 555] </ref>.) Equations of the form z 4 + az 2 + bz = c are equivalent to M z = (F 2 + aF + bI)z = c and solvable by Gaussian elimination [4, p. 562]. <p> We will only be concerned with the first case. For a solvability criterion, see [4, p. 555].) Equations of the form z 4 + az 2 + bz = c are equivalent to M z = (F 2 + aF + bI)z = c and solvable by Gaussian elimination <ref> [4, p. 562] </ref>. If matrices for F 2 and powers of ~ times F and I are precomputed, these can be added together to form M . This scheme uses about 2m 3 bits of memory, but we can reduce this by spending more time. <p> If y is a solution to y 2 + y = S 3 =S 3 then (in some order) ff 1 = y=S 1 ; ff 2 = ff 1 + S 1 : Proof: Follows from the discussion in <ref> [4] </ref>. We note that S 1 6= 0. Our approach has been to reduce a cubic equation to an easily solved quartic. Alternatively, one can extract a square root and a cube root and then solve an auxiliary quadratic equation [8]. <p> The further substitution z = 1=y and some algebra gives the result. The main idea of this method is already in <ref> [4] </ref>, but the simple formulas for 0 i do not seem to be in the literature. It is also possible to solve a quartic equation by extracting two square roots and solving one auxiliary cubic and three auxiliary quartic equations [8].
Reference: [5] <author> R. E. Blahut, </author> <title> Theory and Practice of Error Control Codes, </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: If we are willing to transmit all the power sums, then insertions and deletions can be done using O (t log n) bit operations. Second, the running time for enumerations can be improved if randomized algorithms are permitted. Let us sketch a proof of this. Blahut <ref> [5, p. 340] </ref> shows that the Berlekamp-Massey algorithm can be implemented with O (t (log t) 2 log log t) multiplications in GF (2 m ).
Reference: [6] <author> E. L. Blokh, </author> <title> Method of decoding Bose-Chaudhuri triple error correcting codes, </title> <booktitle> Engineering Cybernetics 3, </booktitle> <year> 1964, </year> <pages> pages 22-32. </pages>
Reference-contexts: If C = 1 + S 5 )=D, the equation y 4 + Cy 2 + Dy = 0 has distinct nonzero solutions y 1 ; y 2 ; y 3 . We may take (in some order) ff i = y i + S 1 . Proof: Blokh <ref> [6, pp. 27] </ref> shows that D 6= 0 and y 3 + Cy + D = 0, from which the result follows. Theorem 6 (Polynomial coefficients for d = 4.) Let ff 1 ; : : : ; ff 4 be distinct.
Reference: [7] <author> L. M. Censier and P. Feautrier, </author> <title> A new solution to coherence problems in multicache systems, </title> <journal> IEEE Trans. Comput. </journal> <volume> C-27, </volume> <year> 1978, </year> <pages> pages 1112-1118. </pages>
Reference-contexts: There is a naive solution to the above stated problem that is correct even when t = n. Associate to the block an n bit vector representing S whose coordinates are indexed by the labels assigned to processors. (In the literature, these bits are called presence flags <ref> [7] </ref>.) Set the ff-th bit of the vector only if ff 2 S. Deletions and insertions can be easily implemented by complementing the ff-th coordinate of the vector. To implement Enumerate ;n simply list all the indices ff for which the ff-th bit of the vector is set.
Reference: [8] <author> C.-L. Chen, </author> <title> Formulas for the solutions of quadratic equations over GF (2 m ), IEEE Trans. </title> <journal> Inform. Theory IT-28, </journal> <year> 1982, </year> <pages> pages 792-794. </pages>
Reference-contexts: We note that S 1 6= 0. Our approach has been to reduce a cubic equation to an easily solved quartic. Alternatively, one can extract a square root and a cube root and then solve an auxiliary quadratic equation <ref> [8] </ref>. Cube root computation can be reduced to exponentiation; see [2, Thm. 7.3.2]. Theorem 5 (Enumeration for d = 3.) Let ff 1 ; ff 2 ; ff 3 be distinct. Then D = S 3 1 + S 3 6= 0. <p> The main idea of this method is already in [4], but the simple formulas for 0 i do not seem to be in the literature. It is also possible to solve a quartic equation by extracting two square roots and solving one auxiliary cubic and three auxiliary quartic equations <ref> [8] </ref>. Assuming that matrices for F 1 , F 2 , and powers of ~ times F and I have been precomputed, Table 1 summarizes the complexity of the above described methods. There are specific methods for larger d, but we limit ourselves to a few remarks.
Reference: [9] <author> D. T. Chi, </author> <title> A new fast Reed-Solomon decoding algorithm without Chien search, </title> <booktitle> in Proc. 1993 IEEE Military Communications Conference, </booktitle> <publisher> IEEE Press, </publisher> <year> 1993, </year> <pages> pages 948-952. </pages>
Reference-contexts: To do this, decoders use the Berlekamp-Massey algorithm (or a variant thereof), followed by a Chien search for the roots of a polynomial. This search method uses special hardware to reduce the time per root to one clock cycle. Chi <ref> [9, pp. 951] </ref> analyzes the performance of several popular Reed-Solomon decoders. He assumes that the decoder has 2t parallel units, each capable of one multiplication or division, plus one addition, in GF (2 m ) per clock cycle.
Reference: [10] <author> R. T. Chien, B. D. Cunningham, and I. B. Oldham, </author> <title> Hybrid methods for finding roots of a polynomial with application to BCH decoding, </title> <journal> IEEE Trans. Inform. Theory IT-15, </journal> <year> 1969, </year> <pages> pages 328-335. </pages>
Reference-contexts: If matrices for F 2 and powers of ~ times F and I are precomputed, these can be added together to form M . This scheme uses about 2m 3 bits of memory, but we can reduce this by spending more time. First, as discussed in <ref> [10, pp. 331] </ref>, if a 6= 0, the substitution z = aw leads to the standard form M 0 w = (F 2 + F + b 0 I)w = c 0 : (If a = 0, the form is the same without the F .) This cuts the memory needed
Reference: [11] <author> R. T. Chien and W. W. Frazer, </author> <title> An application of coding theory to document retrieval, </title> <journal> IEEE Trans. Inform. Theory IT-12, </journal> <year> 1966, </year> <pages> pages 92-96. </pages>
Reference-contexts: The idea of threshold data structures is intriguing, and quite possibly our ideas will find other uses. In some sense, then, our technique is a solution looking for more problems. 1.3 Related Work Chien and Frazer <ref> [11] </ref> applied BCH codes to solve an information retrieval problem. One is given a large set of documents, indexed by the presence of one or more features. Given a set S of features, one wishes to list all the documents whose feature sets T contain S.
Reference: [12] <author> T. Ericson and V. I. Levenshtein, </author> <title> Superimposed codes in the Hamming space, </title> <journal> IEEE Trans. Inform. Theory 40, </journal> <year> 1994, </year> <pages> pages 1882-1893. </pages>
Reference-contexts: When S and T have cardinality t, deciding if S T is reduced to BCH decoding. Such codes were further studied by Ericson and Levenshtein <ref> [12] </ref>, who imposed the additional constraint that they should be resistant to transmission errors, and proved various bounds on their size. The goal of this work was to economically represent the set of active users in a token ring network.
Reference: [13] <author> E. Kaltofen and V. Shoup, </author> <title> Fast polynomial factorization over high algebraic extensions of finite fields, </title> <booktitle> in Proc. 1997 International Symposium on Symbolic and Algebraic Computation, </booktitle> <editor> ed. W. Kuchlin, </editor> <publisher> ACM Press, </publisher> <year> 1997, </year> <pages> pages 184-188. </pages>
Reference-contexts: Let us sketch a proof of this. Blahut [5, p. 340] shows that the Berlekamp-Massey algorithm can be implemented with O (t (log t) 2 log log t) multiplications in GF (2 m ). Kaltofen and Shoup <ref> [13, Theorem 3] </ref> prove that a degree d polynomial f in GF (2 m )[X] can be factored into its irreducible factors by a randomized (Las Vegas) algorithm using O (md (d 1+o (1) + m 0:67+o (1) )) bit operations.
Reference: [14] <author> F. J. MacWilliams and N. J. A. Sloane, </author> <title> The Theory of Error-Correcting Codes, </title> <publisher> North-Holland, </publisher> <year> 1997. </year>
Reference-contexts: Indeed, the most expensive operation supported by our data structure is essentially equivalent to single decoding step in a Bose-Chaudhuri-Hocquenghem (BCH) t error correcting code <ref> [14, Ch. 3 & Ch. 9] </ref>. <p> Indeed, the most expensive operation supported by our data structure is essentially equivalent to single decoding step in a Bose-Chaudhuri-Hocquenghem (BCH) t error correcting code [14, Ch. 3 & Ch. 9]. For such codes, specially designed efficient hardware level decoding procedures exist <ref> [14, Ch. 9,x6] </ref>. (For further properties of these codes we refer the reader to [14] and [21].) In summary, we provide a satisfactory solution to a "threshold" version of the cache coherence problem which is optimal in terms of memory usage. <p> For such codes, specially designed efficient hardware level decoding procedures exist [14, Ch. 9,x6]. (For further properties of these codes we refer the reader to <ref> [14] </ref> and [21].) In summary, we provide a satisfactory solution to a "threshold" version of the cache coherence problem which is optimal in terms of memory usage.
Reference: [15] <author> J. L. Massey, </author> <title> Shift register synthesis and BCH decoding. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-15, </volume> <year> 1969, </year> <pages> pages 122-127. </pages>
Reference-contexts: with additional examples. 4 2 A Threshold Data Structure for Dynamic-sets In this section we describe a threshold data structure relying on algorithms from the theory of error-correcting codes, in particular the algebraic decoding algorithm for binary BCH codes invented by Peterson [17] and refined by Berlekamp [3] and Massey <ref> [15] </ref>. We will now describe a t-threshold data structure for subsets of . We henceforth identify with a subset of GF (2 m ) fl , the nonzero elements of the finite field of order 2 m . <p> Suppose that jSj = d, a value that can be read from the counter. Determining the ff's from the power sums is exactly the problem that is solved when decoding binary BCH codes. Let us briefly outline how this is done. Using the Berlekamp-Massey algorithm <ref> [3, 15] </ref>, we take the power sums S 1 ; S 2 ; : : : ; S 2d1 and produce the coefficients of the polynomial f (X) = ff2S In coding theory, this polynomial is called the error locator. <p> Clearly, deletions have the same cost. Now consider enumerations. Note that d = jSj is available from the counter C. We can obtain the required S 2i and run the Berlekamp-Massey algorithm using O (d 2 m 2 ) bit operations <ref> [3, 15] </ref>. Finally, Shoup [19, p. 14] has proved that a degree d polynomial with all its zeroes in GF (2 m ) can be completely factored using O ((d log n) 2+o (1) ) bit operations. Since m = O (log n), the result follows.
Reference: [16] <author> A. M. Michelson and A. H. Levesque, </author> <title> Error-control Techniques for Digital Communication, </title> <publisher> Wiley, </publisher> <year> 1985. </year>
Reference-contexts: There are specific methods for larger d, but we limit ourselves to a few remarks. In general, the polynomial coefficients i are rational functions of the power sums S i . These functions are given explicitly for d 6 in <ref> [16, pp. 144-147] </ref>. For degree 5 equations, Trager and Winograd [20] have given an elegant method, relying on inversion of an m fi m matrix, for finding their roots. Finally, we consider the hardware performance of our methods.
Reference: [17] <author> W. W. Peterson, </author> <title> Encoding and error-correction procedures for the Bose-Chaudhuri codes, </title> <journal> IRE Trans. Inform. Theory IT-6, </journal> <year> 1960, </year> <pages> pages 459-470. </pages>
Reference-contexts: we illustrate the concept of threshold data structures with additional examples. 4 2 A Threshold Data Structure for Dynamic-sets In this section we describe a threshold data structure relying on algorithms from the theory of error-correcting codes, in particular the algebraic decoding algorithm for binary BCH codes invented by Peterson <ref> [17] </ref> and refined by Berlekamp [3] and Massey [15]. We will now describe a t-threshold data structure for subsets of . We henceforth identify with a subset of GF (2 m ) fl , the nonzero elements of the finite field of order 2 m . <p> S 4 Proof: From Newton's identities we have 0 @ S 2 S 1 1 0 S 6 S 5 S 4 S 3 C 0 @ 2 4 C 0 @ S 3 S 7 C The matrix has determinant D = Q i&lt;j (ff i ff j ) <ref> [17, p. 462] </ref>, which cannot vanish.
Reference: [18] <author> Y. R. Shayan and T. Le-Ngoc, </author> <title> A cellular structure for a versatile Reed-Solomon decoder, </title> <journal> IEEE Trans. Computers 48, </journal> <year> 1997, </year> <pages> pages 80-85. 16 </pages>
Reference-contexts: into account that we can obtain the syndromes with one clock cycle and that we don't need the error values, we find that enumeration can be done within 6t + n + 1 cycles. (The time for full decoding is about double this.) More decoder designs can be found in <ref> [18] </ref> and the references therein. With specialized hardware, it should be possible to perform an enumerate operation in a few microseconds when n = 10 2 10 3 .
Reference: [19] <author> V. Shoup, </author> <title> A fast deterministic algorithm for factoring polynomials over finite fields of small charac-teristic, </title> <booktitle> in Proc. 1991 International Symposium on Symbolic and Algebraic Computation, </booktitle> <editor> ed. S. Watt, </editor> <publisher> ACM Press, </publisher> <year> 1991, </year> <pages> pages 14-21. </pages>
Reference-contexts: Clearly, deletions have the same cost. Now consider enumerations. Note that d = jSj is available from the counter C. We can obtain the required S 2i and run the Berlekamp-Massey algorithm using O (d 2 m 2 ) bit operations [3, 15]. Finally, Shoup <ref> [19, p. 14] </ref> has proved that a degree d polynomial with all its zeroes in GF (2 m ) can be completely factored using O ((d log n) 2+o (1) ) bit operations. Since m = O (log n), the result follows. Apropos of this result, some remarks are appropriate.
Reference: [20] <author> B. Trager and S. Winograd, </author> <note> to appear. </note>
Reference-contexts: In general, the polynomial coefficients i are rational functions of the power sums S i . These functions are given explicitly for d 6 in [16, pp. 144-147]. For degree 5 equations, Trager and Winograd <ref> [20] </ref> have given an elegant method, relying on inversion of an m fi m matrix, for finding their roots. Finally, we consider the hardware performance of our methods.
Reference: [21] <author> J. H. van Lint, </author> <title> Introduction to Coding Theory, 2nd edition, </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: For such codes, specially designed efficient hardware level decoding procedures exist [14, Ch. 9,x6]. (For further properties of these codes we refer the reader to [14] and <ref> [21] </ref>.) In summary, we provide a satisfactory solution to a "threshold" version of the cache coherence problem which is optimal in terms of memory usage.
Reference: [22] <author> W.-D. Weber and A. Gupta, </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proc. 3rd Int. Conf. Arch. Supp. Prog. Langs. Op. Sys. (ASPLOS III), </booktitle> <pages> pages 243-256, </pages> <year> 1989. </year>
Reference-contexts: Thus, one is interested in very fast solutions to the cache coherence problem that have simple and efficient hardware implementations. Studies of sharing patterns indicate that there are essentially two cases: either a few processors share a block, or all of them do. (See, for example, <ref> [22] </ref>.) For this reason, it is worthwhile to handle the first case quickly, if that can be done, before resorting to a broadcast. In addition, since we must ensure that extant copies of every block are identical, we wish to do this with a small amount of memory. <p> Writing to a "new" block, can be handled with Enumerate followed by Insert (ff). Using this trick, t can be effectively increased by 1 at little additional cost. Second, we note that we will only be interested in small values of t. For example, Weber and Gupta <ref> [22] </ref> suggest that in most sharing patterns, the number of readers is less than 4. For such cases, a feasible alternative to the general enumeration procedure is to directly compute the i 's and then find the roots of a small degree polynomial.
Reference: [23] <author> D. Wood, et al., </author> <title> Mechanisms for cooperative shared memory, </title> <journal> CMG Transactions, </journal> <volume> Issue 84, </volume> <pages> pages 51-62, </pages> <year> 1994. </year> <note> [Also in Proc. </note> <editor> 20th Ann. </editor> <booktitle> Intern. Symp. Computer Architecture, </booktitle> <pages> pages 156-168. </pages> <publisher> IEEE Press, </publisher> <year> 1993.] </year> <month> 17 </month>
Reference-contexts: Clearly, the above data structure is very "simple". Moreover, insertions, deletions, and enumerations can be implemented efficiently. But, our problem arises in the design of memory systems. In this context, memory is limited. Empirical studies of directory protocols consider 32-byte blocks <ref> [23, Sect. 3.5] </ref>. On a system with 1024 processors, the naive solution incurs a 400% memory overhead! Hence, it is unsatisfactory. For t = 1, there is an elegant solution.
References-found: 23

