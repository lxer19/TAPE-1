URL: http://l2r.cs.uiuc.edu/~danr/Teaching/CS491-98/Papers/valiant.ps.gz
Refering-URL: http://l2r.cs.uiuc.edu/~danr/Teaching/CS491-98/491-list.html
Root-URL: http://www.cs.uiuc.edu
Title: A Neuroidal Architecture for Cognitive Computation  
Author: Leslie G. Valiant 
Address: Cambridge, MA 02138  
Affiliation: Division of Engineering and Applied Sciences Harvard University  
Abstract: An architecture is described for designing systems that acquire and manipulate large amounts of unsystematized, or so-called com-monsense, knowledge. Its aim is to exploit to the full those aspects of computational learning that are known to offer powerful solutions in the acquisition and maintenance of robust knowledge bases. The architecture makes explicit the requirements on the basic computational tasks that are to be performed and is designed to make these computationally tractable even for very large databases. The main claims are that (i) the basic learning tasks are tractable and (ii) tractable learning offers viable approaches to a range of issues that have been previously identified as problematic for artificial intelligence systems that are entirely programmed. In particular, attribute efficiency holds a central place in the definition of the learning tasks. Among the issues that learning offers to resolve are robustness to inconsistencies, robustness to incomplete information and resolving among alternatives.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J.R. Anderson. </author> <title> The Architecture of Cognition. </title> <publisher> Harvard University Press, </publisher> <year> 1983. </year>
Reference-contexts: In this respect the proposed architecture differs from other general approaches to cognitive architectures that have been described, such as <ref> [1] </ref>, [37], and [38], in which inductive learning plays a much smaller role. We note that our use of PAC semantics suggests a modified Turing test.
Reference: 2. <author> J.R. Anderson. </author> <title> Rules of the Mind. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1993. </year>
Reference: 3. <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: In the case of inductive learning the issue of noise has been studied extensively. On the theoretical side a range of noise models have been considered, ranging from a malicious adversarial model <ref> [3] </ref> to the more benign random classification noise model, where the only noise is in the classification of the examples and this is random [3]. At least for the more benign models there are some powerful general techniques for making learning algorithms cope with noise in some generality [16]. <p> On the theoretical side a range of noise models have been considered, ranging from a malicious adversarial model <ref> [3] </ref> to the more benign random classification noise model, where the only noise is in the classification of the examples and this is random [3]. At least for the more benign models there are some powerful general techniques for making learning algorithms cope with noise in some generality [16].
Reference: 4. <author> A. Blum. </author> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 373-386, </pages> <year> 1992. </year>
Reference: 5. <author> A. Blum, et al. </author> <title> A polynomial time algorithm for learning noisy linear threshold functions. </title> <booktitle> In Proc. 37th IEEE Symp. on Theory of Computing, </booktitle> <pages> pages 330-338, </pages> <year> 1996. </year>
Reference-contexts: At least for the more benign models there are some powerful general techniques for making learning algorithms cope with noise in some generality [16]. For the problem of learning linear separators there exist theoretical results that show that there is no fundamental computational impediment to overcoming random classification noise <ref> [5, 8] </ref>. Currently somewhat complex algorithms are needed to establish this rigorously.
Reference: 6. <author> T. Bylander. </author> <title> Learning linear threshold functions in the presence of classification noise. </title> <booktitle> In Proc. 7th ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 340-347, </pages> <year> 1994. </year>
Reference: 7. <author> N. Cesa-Bianchi, et al. </author> <title> How to use expert advice. </title> <booktitle> In Proc. 25th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 382-39, </pages> <year> 1993. </year>
Reference: 8. <author> E. Cohen. </author> <title> Learning noisy perceptrons by a perceptron in polynomial time. </title> <booktitle> In Proc 38th IEEE Symp. on Foundation of Computer Science, </booktitle> <pages> pages 514-523, </pages> <year> 1997. </year>
Reference-contexts: At least for the more benign models there are some powerful general techniques for making learning algorithms cope with noise in some generality [16]. For the problem of learning linear separators there exist theoretical results that show that there is no fundamental computational impediment to overcoming random classification noise <ref> [5, 8] </ref>. Currently somewhat complex algorithms are needed to establish this rigorously.
Reference: 9. <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Inf. and Computation, </journal> <volume> 82(3) </volume> <pages> 247-266, </pages> <year> 1989. </year>
Reference: 10. <editor> M.L. Ginsberg. </editor> <booktitle> Readings in Nonmonotonic Reasoning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1989. </year>
Reference-contexts: The difficulty is that such systems need to take a generic view of how to treat incomplete information they need a uniform theory, such as circumscription or the closed world assumption that takes positions on how to resolve the unknown <ref> [10, 29, 31] </ref>. PAC circuit semantics offers an advantage here it resolves the unknown separately in each case by using information learned from past experience of cases where similar features to the case in hand were similarly unknown. <p> A motivating observation here is that gates in a circuit take values at all times. Consider the paradigm of nonmonotonic reasoning exemplified by the widely discussed example of the bird called Tweety <ref> [10] </ref> . The system is invited to take positions on whether Tweety flies both before and after it is revealed to it that Tweety is, in fact, a penguin. Suppose that in a brain, for example, there is a two-valued gate intended to recognize penguins.
Reference: 11. <author> A.R. Golding and D. Roth. </author> <title> Applying Winnow to context-sensitive spelling correction. </title> <booktitle> In Proc 13th Int. Conf. on Machine Learning, </booktitle> <pages> pages 182-190, </pages> <address> San Francisco, CA, 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It appears that some mechanism for attribute-efficiency is essential to any large scale learning system. The effectiveness of Winnow itself has been demonstrated in a variety of experiments. A striking example in the cognitive domain is offered in the work of Golding and Roth on spelling correction <ref> [11] </ref>. Even in the presence of tens of thousands of variables, Winnow is able to learn accurately from few examples, sometimes fewer than 100. The question arises whether attribute efficient learning is possible for more expressive knowledge representations. Recently it has been found that this is indeed the case. <p> It would have to acknowledge and compensate for the poverty of what we might be able to program currently as feature detectors. The Golding-Roth paper <ref> [11] </ref> is a good example, however, of how one might start. They take a linguistic corpus and create complex features out of single words, grammatical parts of speech of single words, and boolean conjunctions of these for sets of words that occur in close proximity in the text.
Reference: 12. <author> J.Y. Halpern. </author> <title> An analysis of first-order logics of probability. </title> <journal> In Artificial Intelligence Journal, </journal> <volume> volume 46, </volume> <pages> pages 311-350, </pages> <year> 1990. </year>
Reference: 13. <author> D. Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221, </pages> <year> 1988. </year>
Reference-contexts: The phenomenon of efficient attribute efficient learning in the PAC sense was first pointed out by Haussler <ref> [13] </ref>. A striking and remarkable embodiment of this idea followed in the form of Littlestone's Winnow algorithm [26] for learning linear threshold functions. The algorithm is similar in form to the classical perceptron algorithm except that the updates to the weights are multiplicative rather than additive. <p> An area in which complex representations may need to be learned is that of strategies and plans. Here production systems are widely believed to have useful expressivity [38]. Khardon has shown that a rich class of these can be learned using decision list algorithms <ref> [13] </ref>. The dilemma here is that no attribute-efficient learning algorithm is known for decision lists, unless the degree d is small, as explained in the previous section.
Reference: 14. <author> D. Haussler. </author> <title> Learning conjunctive concepts in structural domains. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 7-40, </pages> <year> 1989. </year>
Reference: 15. <author> M. Kearns and M. Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(4) </volume> <pages> 807-837, </pages> <year> 1993. </year>
Reference: 16. <author> M.J. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proc. 25th ACM Symp. on Theory of Computing, </booktitle> <address> New York, 392-401 1993. </address> <publisher> ACM Press. </publisher>
Reference-contexts: At least for the more benign models there are some powerful general techniques for making learning algorithms cope with noise in some generality <ref> [16] </ref>. For the problem of learning linear separators there exist theoretical results that show that there is no fundamental computational impediment to overcoming random classification noise [5, 8]. Currently somewhat complex algorithms are needed to establish this rigorously.
Reference: 17. <author> M.J. Kearns and R.E. Schapire. </author> <title> Efficient distribution-free probabilistic concepts. </title> <institution> J. Comput. Syst. Sci., 48(3):464, </institution> <year> 1994. </year>
Reference-contexts: Having circuits that have a probabilistic rather than a deterministic interpretation is an extension that may be considered, but this appears to make the learning task computationally less tractable <ref> [17] </ref>. There is little evidence that probabilistic processes are central to human reasoning [49].
Reference: 18. <author> M.J. Kearns and U.V. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference: 19. <author> R. Khardon. </author> <title> Learning to take actions. </title> <booktitle> In Proc. National Conference on Artificial Intelligence, </booktitle> <pages> pages 787-792. </pages> <publisher> AAAI, </publisher> <year> 1996. </year>
Reference-contexts: A third way that projection learning can be used is best viewed as an application outside the architecture that is attribute efficient in a weaker sense. Consider a sequential covering algorithm, as in Rivest [41], for learning decision lists, or Khardon's extension to propositional production rule systems <ref> [19] </ref>. In the simplest case such a covering algorithm works as follows: it looks successively for a literal, say x 3 , that is the most predictive single literal, in some sense, of the function f being learned.
Reference: 20. <author> R. Khardon and D. Roth. </author> <title> Learning to reason. </title> <booktitle> In AAAI94, </booktitle> <pages> pages 682-687. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: 21. <author> R. Khardon and D. Roth. </author> <title> Learning to reason with a restricted view. </title> <booktitle> In Proc. 8th ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 301-310, </pages> <year> 1995. </year>
Reference: 22. <author> J. Kivinen and M.K. Warmuth. </author> <title> The perceptron algorithm vs. Winnow: linear vs. logarithmic mistake bounds when few input variables are relevant. </title> <booktitle> In Proc. 8th ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 289-296, </pages> <year> 1995. </year>
Reference: 23. <author> D. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 1. </volume> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: To do this we shall, for simplicity, impose here the constraint that aggregates of circuit units are acyclic. We can then form a topological sort of their nodes (e.g. Knuth <ref> [23] </ref> ) and for one such topologically sorted order evaluate each node in succession.
Reference: 24. <author> P. Langley, D. Klahr, and R Neches. </author> <title> Production System Models of Learning and Development. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference: 25. <editor> D.B. Lenat, et al. </editor> <title> CYC: Toward programs with common sense. </title> <journal> In CACM, </journal> <volume> volume 33:8, </volume> <pages> pages 30-49, </pages> <year> 1990. </year>
Reference-contexts: Most general approaches to AI attempt to describe a uniform method for building a knowledge base starting from a blank slate. Facts and reasoning about the most universal concepts such as time and space are then formalized in the same framework as is more specialized knowledge (e.g. <ref> [25] </ref>). In the neuroidal framework there is room reserved for treating the universal concepts differently from the others.
Reference: 26. <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: The block diagram may ultimately contain feedback or cycles. Some rules are further specified for how the circuits can change in the process of knowledge acquisition. For the gates, linear threshold units in our case, some update rules are given, such as the perceptron algorithm [42] or Winnow <ref> [26] </ref>, that specify the supervised inductive learning process. In addition some further rules are given to allow the acquisition of programmed knowledge. In particular new output gates may be added and the parameters or weights of that gate assigned appropriate values so that it fires under the intended conditions. <p> The phenomenon of efficient attribute efficient learning in the PAC sense was first pointed out by Haussler [13]. A striking and remarkable embodiment of this idea followed in the form of Littlestone's Winnow algorithm <ref> [26] </ref> for learning linear threshold functions. The algorithm is similar in form to the classical perceptron algorithm except that the updates to the weights are multiplicative rather than additive. <p> gives the algorithm the remarkable property that when learning a monotone k-variable Boolean disjunction over fx 1 ; ; x n g the number of examples needed for convergence, whether in the PAC or mistake-bounded sense, is upper bounded by ck log 2 n, where c is a small constant, <ref> [26, 27] </ref>. Thus the sample complexity is linear in k, the number of relevant variables, and logarithmic in the number of irrelevant ones. <p> Thus the sample complexity is linear in k, the number of relevant variables, and logarithmic in the number of irrelevant ones. Littlestone's Theorem 9 <ref> [26] </ref> adapted to the case when coefficients can be both positive and negative (his Example 6) has the following more general statement; For X f0; 1g n suppose that for the functions g : X ! f0; 1g there exist -1 ; -2 ; ; -n 0 and -1 ; -2
Reference: 27. <author> N. Littlestone. </author> <title> From on-line to batch learning. </title> <booktitle> In Proc. 2nd Workshop on Computational Learing Theory, </booktitle> <pages> pages 269-284, </pages> <year> 1989. </year>
Reference-contexts: gives the algorithm the remarkable property that when learning a monotone k-variable Boolean disjunction over fx 1 ; ; x n g the number of examples needed for convergence, whether in the PAC or mistake-bounded sense, is upper bounded by ck log 2 n, where c is a small constant, <ref> [26, 27] </ref>. Thus the sample complexity is linear in k, the number of relevant variables, and logarithmic in the number of irrelevant ones. <p> Then clearly ffi = 1. Hence 4.2 becomes O (k log n). In all these cases the algorithm can be adapted so that it has similar bounds in the PAC model <ref> [27] </ref>. For linear inequalities of the form 4.1, we see that the particular example given is equivalent to 1 4 (x 7 + (1 x 2 ) + (1 x 5 ) + (1 x 8 )) 1 so that ffi = 1 4 .
Reference: 28. <author> J. McCarthy. </author> <title> Programs with commonsense. </title> <booktitle> In Proc. Teddington Conference on the Mechanization of Thought Processes, </booktitle> <address> London, </address> <year> 1959. </year> <month> HMSO. </month>
Reference: 29. <author> J. McCarthy. </author> <title> Circumscription a form of non-monotonic reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 27-39, </pages> <year> 1980. </year>
Reference-contexts: The difficulty is that such systems need to take a generic view of how to treat incomplete information they need a uniform theory, such as circumscription or the closed world assumption that takes positions on how to resolve the unknown <ref> [10, 29, 31] </ref>. PAC circuit semantics offers an advantage here it resolves the unknown separately in each case by using information learned from past experience of cases where similar features to the case in hand were similarly unknown.
Reference: 30. <author> J. McCarthy and P.J. Hayes. </author> <title> Some philosophical problems from the standpoint of artificial intelligence. </title> <editor> In D. Michie, editor, </editor> <booktitle> Machine Intelligence, volume 4, </booktitle> <address> New York, 1969. </address> <publisher> American Elsevier. </publisher>
Reference-contexts: The semantics we shall describe here, PAC circuit semantics, or PAC semantics for short, is based on the notion of computationally feasible learning of functions that are probably approximately correct [50]. To explain the contrast in viewpoints consider the situation calculus described in McCarthy and Hayes <ref> [30] </ref>. There, a situation is "the complete state of the world", and general facts are relations among situations. Thus P ) Q means that for all situations for which P holds Q holds also.
Reference: 31. <author> D. McDermott and J. Doyle. </author> <title> Nonmonotonic logic I. </title> <journal> Artificial Intelligence, </journal> <volume> 13(1), </volume> <year> 1980. </year>
Reference-contexts: The difficulty is that such systems need to take a generic view of how to treat incomplete information they need a uniform theory, such as circumscription or the closed world assumption that takes positions on how to resolve the unknown <ref> [10, 29, 31] </ref>. PAC circuit semantics offers an advantage here it resolves the unknown separately in each case by using information learned from past experience of cases where similar features to the case in hand were similarly unknown.
Reference: 32. <author> G.A. Miller. </author> <title> The magical number seven, plus or minus two: Some limits on our capacity for processing information. </title> <journal> Psychological Review, </journal> <volume> 63 </volume> <pages> 81-97, </pages> <year> 1956. </year>
Reference: 33. <author> M. Minsky. </author> <title> A framework for representing knowledge. </title> <editor> In P.H. Winston, editor, </editor> <booktitle> The Psychology of Computer Vision, </booktitle> <address> New York, 1975. </address> <publisher> McGraw Hill. </publisher>
Reference-contexts: Thus a rule Q ) R may hold if context P is true, but not necessarily otherwise. Frames in the sense of Minsky <ref> [33] </ref> can be viewed as contexts that have a rich set of associated rules. The PAC semantics of such a rule is that on the subdomain in which P holds, Q ) R is the case, at least with high probability.
Reference: 34. <author> M. Minsky and S. Papert. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1969. </year>
Reference: 35. <author> S. Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> pages 363-391, </pages> <year> 1990. </year>
Reference: 36. <author> S. Muggleton. </author> <title> Inductive logic programming: derivations, successes and shortcomings. </title> <journal> SIGART Bulletin, </journal> <volume> 5(1) </volume> <pages> 5-11, </pages> <year> 1994. </year> <note> Also other articles in same volume. </note>
Reference: 37. <author> A. Newell. </author> <title> Unified Theories of Cognition. </title> <publisher> Harvard University Press, </publisher> <year> 1990. </year>
Reference-contexts: In this respect the proposed architecture differs from other general approaches to cognitive architectures that have been described, such as [1], <ref> [37] </ref>, and [38], in which inductive learning plays a much smaller role. We note that our use of PAC semantics suggests a modified Turing test.
Reference: 38. <author> A. Newell and H.A. Simon. </author> <title> Human Problem Solving. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1972. </year>
Reference-contexts: An area in which complex representations may need to be learned is that of strategies and plans. Here production systems are widely believed to have useful expressivity <ref> [38] </ref>. Khardon has shown that a rich class of these can be learned using decision list algorithms [13]. The dilemma here is that no attribute-efficient learning algorithm is known for decision lists, unless the degree d is small, as explained in the previous section. <p> In this respect the proposed architecture differs from other general approaches to cognitive architectures that have been described, such as [1], [37], and <ref> [38] </ref>, in which inductive learning plays a much smaller role. We note that our use of PAC semantics suggests a modified Turing test.
Reference: 39. <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1988. </year>
Reference: 40. <author> L. Pitt and L.G. Valiant. </author> <title> Computational limits on learning from examples. </title> <journal> J. ACM, </journal> <volume> 35 </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: we note that there are theoretical results that show for certain classes of Boolean functions that extending the representation of the learner to threshold functions makes the original class polynomial time learnable, while restricting the learner to the minimal representation needed for expressing these functions would make the task NP-compete <ref> [40] </ref>. Our richer representation, therefore, has not only the obvious advantage of being able to express more, but has the additional computational benefits of making Boolean domains potentially easier to learn.
Reference: 41. <author> R.L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: In the last case we would have n X z i k where z i is a variable over the reals that is given value 1 if x i = 1, and zero otherwise. A further class that can be so expressed is that of 1-decision lists <ref> [41] </ref>. <p> A third way that projection learning can be used is best viewed as an application outside the architecture that is attribute efficient in a weaker sense. Consider a sequential covering algorithm, as in Rivest <ref> [41] </ref>, for learning decision lists, or Khardon's extension to propositional production rule systems [19].
Reference: 42. <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: The block diagram may ultimately contain feedback or cycles. Some rules are further specified for how the circuits can change in the process of knowledge acquisition. For the gates, linear threshold units in our case, some update rules are given, such as the perceptron algorithm <ref> [42] </ref> or Winnow [26], that specify the supervised inductive learning process. In addition some further rules are given to allow the acquisition of programmed knowledge.
Reference: 43. <author> D. Roth. </author> <title> Learning to reason: the non-monotonic case. </title> <booktitle> Proc. Int. Joint Conf. Art. Intl., </booktitle> <pages> pages 1178-118, </pages> <year> 1995. </year>
Reference-contexts: We regard defaults as rules where certain predicates are assumed to have the undetermined value in the precondition. Their validity arises from the fact that they had proved accurate in the past, or that they could be deduced from those that had. Further examples are given in <ref> [43, 52, 53] </ref>. 4.8 Reasoning Much effort has been put into seeing whether the various formalisms that have been suggested for reasoning in AI, at least those outside the probabilistic realm, can be formulated within predicate calculus.
Reference: 44. <author> D. Roth. </author> <title> A connectionist framework for reasoning: Reasoning with examples. </title> <booktitle> In Proc. National Conference on Artificial Intelligence, </booktitle> <pages> pages 1256-1261. </pages> <publisher> AAAI, </publisher> <year> 1996. </year>
Reference: 45. <author> S. Russell and P. Norvig. </author> <booktitle> Artificial Intelligence. </booktitle> <publisher> Prentice Hall, </publisher> <address> Upper Saddle River, NJ, </address> <year> 1995. </year>
Reference-contexts: Systems so embedded would then have the addition benefits of a powerful learning capability. Let us consider the following restriction of the language of predicate calculus (e.g. <ref> [45] </ref> p. 186). As constants we will choose the base objects A B = fa 1 ; ; a g of the image unit. We allow a set R B of base relations of arities that are arbitrary but upper bounded by a constant ff. We use no function symbols. <p> This dichotomy exists also in other work on learning relations, such as inductive logic programming <ref> [45] </ref>, but needs to be addressed in a different way here because of the circuit orientation. An overriding concern for us throughout is, of course, that the complexity of manipulating relations be controlled (c.f. [46]). Suppose we have a connection. <p> Some of these alternative formalisms, and the necessary transformations are described by Russell and Norvig <ref> [45] </ref>. Instead of reformulating this body of work so as to fit in with our architecture, we will be content here to claim that a substantial part of it can be supported directly without change.
Reference: 46. <author> M. Tambe, A. Newell, </author> <title> and P.S. Rosenbloom. The problem of expensive chunks and its solution by restricting expressiveness. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 299-348, </pages> <year> 1990. </year>
Reference-contexts: This dichotomy exists also in other work on learning relations, such as inductive logic programming [45], but needs to be addressed in a different way here because of the circuit orientation. An overriding concern for us throughout is, of course, that the complexity of manipulating relations be controlled (c.f. <ref> [46] </ref>). Suppose we have a connection. R 1 (x 1 ; ; x n ) ! R 2 (y 1 ; ; y k ) and an associated connection binding that specifies x 1 = y 2 and x 2 = y 3 .
Reference: 47. <author> A.M. </author> <title> Turing. </title> <journal> Computing machinery and intelligence. </journal> <volume> Mind 59, </volume> <pages> pages 433-460, </pages> <year> 1950. </year> <title> (Reprinted in Collected Works of A.M. Turing: </title> <booktitle> Mechanical Intelligence, </booktitle> <address> (D.C. </address> <publisher> Ince, ed.), North-Holland, </publisher> <year> 1992). </year>
Reference-contexts: We note that our use of PAC semantics suggests a modified Turing test. His basic criterion for whether a machine could think was that the performance of the machine should be indistinguishable from that of a human to an interrogator communicating via a teleprinter <ref> [47] </ref>. The significance of this informally stated criterion is that it is a purely behavioral one. What PAC semantics offers is a precise way of formulating such behavioral criteria.
Reference: 48. <author> A.M. </author> <title> Turing. Solvable and unsolvable problems. </title> <journal> Science News, </journal> <volume> 31 </volume> <pages> 7-23, </pages> <year> 1954. </year> <title> (Reprinted in Collected Works of A.M. Turing: Mechanical Intelligence (D.C. Ince, </title> <editor> ed.) </editor> <publisher> North-Holland 1992). </publisher>
Reference: 49. <author> A. Tversky and D. Kahnemann. </author> <title> Causal schemata in judgments under uncertainty. In Progress in Social Psychology, </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1977. </year>
Reference-contexts: Having circuits that have a probabilistic rather than a deterministic interpretation is an extension that may be considered, but this appears to make the learning task computationally less tractable [17]. There is little evidence that probabilistic processes are central to human reasoning <ref> [49] </ref>. While we do not exclude extensions to probabilistic representations, we do not consider them here. 4 Some Algorithmic Mechanisms In this section we shall enumerate a series of algorithmic techniques for manipulating knowledge in a system having the described architecture.
Reference: 50. <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Comm. of ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: It is the need for robustness that forces us to look beyond logic, at notions centered on learning. The semantics we shall describe here, PAC circuit semantics, or PAC semantics for short, is based on the notion of computationally feasible learning of functions that are probably approximately correct <ref> [50] </ref>. To explain the contrast in viewpoints consider the situation calculus described in McCarthy and Hayes [30]. There, a situation is "the complete state of the world", and general facts are relations among situations. Thus P ) Q means that for all situations for which P holds Q holds also. <p> Thus we are learning somewhat economically a subclass of DNF formulae that have m monomials, each with at most k + 1 literals. To learn these via general (k + 1)-DNF learning methods <ref> [50] </ref> would be much more expensive, and intractable if r and n are both large. For example, if we constructed all the (k + 1)-monomials we would have about n k+1 of them, which exceeds n 2 if k &gt; 1.
Reference: 51. <author> L.G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 560-566, </pages> <address> Los Angeles, CA, 1985. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One approach is to generate for any variable set the set of conjunctions of all pairs of them. Another choice is to create conjunctions of just those pairs that occur with high correlation. More generally one can generate some set of polynomialy generable combinations <ref> [51] </ref>. The intention is that large numbers of variables, even if most are irrelevant, will not degrade performance in the presence of attribute efficient learning algorithms. We are suggesting that the way to evaluate this architecture is by experimentation. In that connection we note here one aspect.
Reference: 52. <author> L.G. Valiant. </author> <title> Circuits of the Mind. </title> <publisher> Oxford University Press, </publisher> <year> 1994. </year>
Reference-contexts: We shall describe a candidate for such an architecture. This candidate emerged from a study that attempted to look at the issues of functional adequacy, computational feasibility and biological constraints together <ref> [52] </ref>. We call the architecture neuroidal since it respects the most basic constraints imposed by that model of neural computation. One feature of that study was that it was a "whole systems" study. <p> However, it can contain further information, such as a real number that expresses a confidence level for the validity of the predicate. (c.f. neuroids in <ref> [52] </ref>). Each node can be thought of as representing an existentially quantified relation, e.g. 9x 1 9x 2 9x 3 R (x 1 ; x 2 ; x 3 ) where the existentially quantified variables range over the objects a 1 ; ; a in the scene. <p> Hence once the system has reached stability in learning it will cope with instances of incomplete information exactly as it does with ones with complete information <ref> [52] </ref>. One could say further that in natural learning systems instances of incomplete information are the natural ones, and usually the only ones available. Pursuing this example further, suppose that the system has a rule "bird = 1 and penguin = 0 ) flies = 1". <p> We regard defaults as rules where certain predicates are assumed to have the undetermined value in the precondition. Their validity arises from the fact that they had proved accurate in the past, or that they could be deduced from those that had. Further examples are given in <ref> [43, 52, 53] </ref>. 4.8 Reasoning Much effort has been put into seeing whether the various formalisms that have been suggested for reasoning in AI, at least those outside the probabilistic realm, can be formulated within predicate calculus.
Reference: 53. <author> L.G. Valiant. </author> <title> Rationality. </title> <booktitle> In Proc. 8th Ann. Conference on Computational Learning Theory, </booktitle> <pages> pages 3-14. </pages> <publisher> ACM Press, </publisher> <year> 1995. </year>
Reference-contexts: We regard defaults as rules where certain predicates are assumed to have the undetermined value in the precondition. Their validity arises from the fact that they had proved accurate in the past, or that they could be deduced from those that had. Further examples are given in <ref> [43, 52, 53] </ref>. 4.8 Reasoning Much effort has been put into seeing whether the various formalisms that have been suggested for reasoning in AI, at least those outside the probabilistic realm, can be formulated within predicate calculus.
Reference: 54. <author> L.G. Valiant. </author> <title> A neuroidal architecture for cognitive computation. </title> <type> Technical Report TR-11-96, </type> <institution> Harvard University, </institution> <year> 1996. </year>
Reference: 55. <author> L.G. Valiant. </author> <title> Projection learning. </title> <type> Technical Report TR-19-97, </type> <institution> Harvard University, </institution> <year> 1997. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: The size of the coefficients grows exponentially, however, with n. In the special case that most of the c j have one value, this growth is more limited. In particular, it is shown in <ref> [55] </ref> that if c j has value 1 for d of the m values of j then the decision list can be expressed as a linear inequality with integer coefficients, where the magnitudes of the coefficients, and also their sum, is upper bounded by (2m=d) d . <p> Suppose that for the various choices of 2 J the function learned that approximates f (x) on (x) = 1 is f 0 (x). Then P (x)(x) is taken as the approximation of f that has been learned. The main result in <ref> [55] </ref> states that if the f (x)(x) belong to a class that is learnable attribute efficiently on the restricted domain fx j ((x) = 1g by an algorithm A say, and if a disjunction can be learned attribute efficiently by an algorithm B that shares certain specified properties with Winnow, then
References-found: 55

