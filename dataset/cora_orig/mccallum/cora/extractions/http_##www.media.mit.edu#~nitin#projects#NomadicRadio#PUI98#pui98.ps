URL: http://www.media.mit.edu/~nitin/projects/NomadicRadio/PUI98/pui98.ps
Refering-URL: http://www.media.mit.edu/~nitin/projects/NomadicRadio/
Root-URL: http://www.media.mit.edu
Email: -clarkson, nitin, sandy@media.mit.edu  
Title: Auditory Context Awareness via Wearable Computing  
Author: Brian Clarkson, Nitin Sawhney and Alex Pentland 
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: Perceptual Computing Group and Speech Interface Group MIT Media Laboratory  
Abstract: We describe a system for obtaining environmental context through audio for applications and user interfaces. We are interested in identifying specific auditory events such as speakers, cars, and shutting doors, and auditory scenes such as the office, supermarket, or busy street. Our goal is to construct a system that is real-time and robust to a variety of real-world environments. The current system detects and classifies events and scenes using a HMM framework. We design the system around an adaptive structure that relies on unsupervised training for segmentation of sound scenes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bregman, Albert S. </author> <title> Auditory Scene Analysis: The Perceptual Organization of Sound. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: There are still many problems to overcome. A major one is adapting models trained in one environment (such as a voice at the office) for use in other environments (the same voice in a car). This is a problem of overlapping sounds, which work by Bregman <ref> [1] </ref> and Brown & Cooke [11] should provide insight. However, our preliminary ASA system has allowed us to experiment with the use of auditory context in actual wearable/mobile applications.
Reference: [2] <author> Ellis, Daniel P. </author> <title> Prediction-driven Computational Scene Analysis, </title> <type> Ph.D. </type> <institution> Thesis in Media Arts and Sciences, MIT. </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: Saint-Arnaud [6] presents a framework for sound texture classification that models both short-term and long-term auditory events. We also use this concept to organize sound classes into scenes and their constituent objects. Ellis <ref> [2] </ref> starts with a few descriptive elements (noise clouds, clicks, and wefts) and attempts to describe sound scenes in terms of these. Brown and Cooke [11] construct a symbolic description of the auditory scene by segregating sounds based on common F0 contours, onset times and offset times.
Reference: [3] <author> Feiten, B. and S. Gunzel, </author> <title> Automatic Indexing of a Sound Database using Self-Organizing Neural Nets. </title> <journal> Computer Music Journal, </journal> <volume> 18:3, </volume> <pages> pp. 53-65, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: Foote [12] used MFCCs with decision trees and histograms to separate various audio clips. He claimed these techniques could be used to train classifiers for perceptual qualities (e.g. brightness, harmonicity, etc.). Other researches have used cluster [4] and neural net-based <ref> [3] </ref> approaches for similar level of sound classification. Saint-Arnaud [6] presents a framework for sound texture classification that models both short-term and long-term auditory events. We also use this concept to organize sound classes into scenes and their constituent objects.
Reference: [4] <author> Nicolas Saint-Arnaud, </author> <title> Classification of Sound Textures. M.S. Thesis in Media Arts and Sciences, </title> <publisher> MIT. </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: Foote [12] used MFCCs with decision trees and histograms to separate various audio clips. He claimed these techniques could be used to train classifiers for perceptual qualities (e.g. brightness, harmonicity, etc.). Other researches have used cluster <ref> [4] </ref> and neural net-based [3] approaches for similar level of sound classification. Saint-Arnaud [6] presents a framework for sound texture classification that models both short-term and long-term auditory events. We also use this concept to organize sound classes into scenes and their constituent objects. <p> Many examples of the same type of sound form a distribution of time series which our system models with a HMM. Hidden Markov Models capture the temporal characteristics as well as the spectral content of an event. Systems like Schreirers [5], Saint-Arnaud <ref> [4] </ref>, and Foote [12] ignore this temporal knowledge. Since our ASA system is event-driven, the process of compiling these training examples is made easier. The event detection produces a sequence of events such as in Figure 3.
Reference: [5] <author> Schreirer, Eric and Malcolm Slaney, </author> <title> Construction and Evaluation of a Robust Multi-feature Speech/Music Discriminator, </title> <booktitle> Proc. ICASSP-97, </booktitle> <pages> Apr 21-24, </pages> <address> Munich, Germany, </address> <year> 1997. </year>
Reference-contexts: Related Work Several approaches have been used in the past to distinguish auditory characteristics in speech, music and audio data using multiple features and a variety of classification techniques. Scheirer <ref> [5] </ref> used a multidimensional classification framework on speech/music data (recorded from radio stations) by examining 13 features to measure distinct properties of speech and music signals. They have concluded that not all features are necessary to perform accurate classification. <p> Many examples of the same type of sound form a distribution of time series which our system models with a HMM. Hidden Markov Models capture the temporal characteristics as well as the spectral content of an event. Systems like Schreirers <ref> [5] </ref>, Saint-Arnaud [4], and Foote [12] ignore this temporal knowledge. Since our ASA system is event-driven, the process of compiling these training examples is made easier. The event detection produces a sequence of events such as in Figure 3.
Reference: [6] <author> Wold, E., T. Blum, D. Keislar, and J. Wheaton. </author> <title> Content-based Classification Search and Retrieval of Audio. </title> <journal> IEEE Multimedia Magazine, </journal> <month> Fall </month> <year> 1996. </year>
Reference-contexts: Foote [12] used MFCCs with decision trees and histograms to separate various audio clips. He claimed these techniques could be used to train classifiers for perceptual qualities (e.g. brightness, harmonicity, etc.). Other researches have used cluster [4] and neural net-based [3] approaches for similar level of sound classification. Saint-Arnaud <ref> [6] </ref> presents a framework for sound texture classification that models both short-term and long-term auditory events. We also use this concept to organize sound classes into scenes and their constituent objects.
Reference: [7] <author> Sawhney, Nitin. </author> <title> Situational Awareness from Environmental Sounds. Project Report for Pattie Maes, </title> <publisher> MIT Media Lab, </publisher> <month> June </month> <year> 1997. </year> <note> http://www.media.mit.edu/~nitin/papers/Env_Snds/E nvSnds.html </note>
Reference-contexts: Notice that the gross spectral content is distinctive for each speaker. (Frequency is vertical and time is horizontal.) spectral density (PSD) <ref> [7] </ref>, energy, and RASTA coefficients. RASTA is appropriate for modeling speech events such as speech recognition. The more direct spectral measurements, such as cepstral, LPC, and MFC, give better discrimination of general sounds (such as speaker and environment classification).
Reference: [8] <author> Sawhney, </author> <title> Nitin Contextual Awareness, Messaging and Communication in Nomadic Audio Environments, </title> <type> MS Thesis, </type> <institution> Media Arts and Sciences, MIT, </institution> <month> June </month> <year> 1998. </year> <note> http://www.media.mit.edu/~nitin/msthesis/ </note>
Reference-contexts: The last option is necessary for detecting the case where more than one sound object may be present in the event. Application Nomadic Radio is a wearable computing platform that provides a unified audio interface to a number of remote information services <ref> [8] </ref>. Messages such as email, voice mail, hourly news, and calendar events are automatically downloaded to the device throughout the day, and the user must be notified at an appropriate time.
Reference: [9] <author> Horvitz, Eric and Jed Lengyel. </author> <title> Perception, Attention, and Resources: A Decision-Theoretic Approach to Graphics Rendering. </title> <booktitle> Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI97), </booktitle> <address> Providence, RI, </address> <month> Aug. </month> <pages> 1-3, </pages> <year> 1997, </year> <pages> pp. 238-249. </pages>
Reference-contexts: A key issue is that of handling interruptions to the listener in a manner that reduces disruption, while providing timely notifications for relevant messages. This approach is similar to prior work by <ref> [9] </ref> on using perceptual costs and focus of attention for a probabilistic model of scaleable graphics rendering.
Reference: [10] <author> Pfeiffer, Silvia and Fischer, Stephan and Effelsberg, Wolfgang. </author> <title> Automatic Audio Content Analysis. </title> <institution> University of Mannheim, </institution> <year> 1997. </year>
Reference: [11] <author> Brown, G. J. and Cooke, M. </author> <title> Computational Auditory Scene Analysis: A representational approach. </title> <institution> University of Sheffield, </institution> <year> 1992. </year>
Reference-contexts: We also use this concept to organize sound classes into scenes and their constituent objects. Ellis [2] starts with a few descriptive elements (noise clouds, clicks, and wefts) and attempts to describe sound scenes in terms of these. Brown and Cooke <ref> [11] </ref> construct a symbolic description of the auditory scene by segregating sounds based on common F0 contours, onset times and offset times. Unlike others, these two systems were designed for real-world complex sound scenes. Many of the design decisions for our system have been motivated by their work. <p> A simple solution is to adapt the threshold or equivalently scale the energy. The system keeps a running estimate of the energy statistics and continually normalizes the energy to zero mean and unit variance (similar to Browns onset detector <ref> [11] </ref>). The effect is that after a period of silence the system is hypersensitive and after a period of loud sound the system grows desensitized. Figure 1 shows the effects of adaptation for a simple tone (actual energy is on top and adapted energy is on the bottom). <p> A major one is adapting models trained in one environment (such as a voice at the office) for use in other environments (the same voice in a car). This is a problem of overlapping sounds, which work by Bregman [1] and Brown & Cooke <ref> [11] </ref> should provide insight. However, our preliminary ASA system has allowed us to experiment with the use of auditory context in actual wearable/mobile applications. In the future, a variety of software agents can utilize the environmental context our system provides to aid users in their tasks.
Reference: [12] <author> Foote, Jonathan. </author> <title> A Similarity Measure for Automatic Audio Classification. </title> <institution> Institute of Systems Science, </institution> <year> 1997. </year>
Reference-contexts: They have concluded that not all features are necessary to perform accurate classification. This suggests using a set of features that automatically adapt to the classification task. Foote <ref> [12] </ref> used MFCCs with decision trees and histograms to separate various audio clips. He claimed these techniques could be used to train classifiers for perceptual qualities (e.g. brightness, harmonicity, etc.). Other researches have used cluster [4] and neural net-based [3] approaches for similar level of sound classification. <p> Many examples of the same type of sound form a distribution of time series which our system models with a HMM. Hidden Markov Models capture the temporal characteristics as well as the spectral content of an event. Systems like Schreirers [5], Saint-Arnaud [4], and Foote <ref> [12] </ref> ignore this temporal knowledge. Since our ASA system is event-driven, the process of compiling these training examples is made easier. The event detection produces a sequence of events such as in Figure 3. Only those events that contain the sound object to be recognized need to be labeled.
References-found: 12

