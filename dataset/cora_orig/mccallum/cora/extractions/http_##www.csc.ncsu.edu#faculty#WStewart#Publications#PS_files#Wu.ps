URL: http://www.csc.ncsu.edu/faculty/WStewart/Publications/PS_files/Wu.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/WStewart/Publications/Publications.html
Root-URL: http://www.csc.ncsu.edu
Email: billy@markov.csc.ncsu.edu wu@moon.hood.caltech.edu  
Title: Numerical Experiments with Iteration and Aggregation for Markov Chains  
Author: William J. Stewart Wei Wu 
Keyword: Key words: Large Markov Chain Models; Near-Complete-Decomposability; Itera tion and Aggregation; Numerical Experiments.  
Note: Research supported in part by NSF (DDM-8906248)  
Date: August 16, 1996  
Address: Raleigh, N.C. 27695-8206  
Affiliation: Department of Computer Science North Carolina State University  
Abstract: This paper describes an iterative aggregation/disaggregation method for computing the stationary probability vector of a nearly completely decomposable Markov chain. The emphasis is on the implementation of the algorithm and on the results that are obtained when it is applied to three modelling examples that have been used in the analysis of computer/communication systems. Where applicable, a comparison with standard iterative and direct methods for solving the same problems, is made. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.V. Aho, J.E. Hopcroft and J.D. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass. </address> <year> 1974. </year>
Reference-contexts: The algorithm we used is that of R.E. Tarjan, [16]. It is the depth-first search (DFS) algorithm which searches in the forward (deeper) direction as long as possible. Details of the non-recursive algorithm of DFS are given in <ref> [1] </ref>. Coding details for DFS are given in [7]. The complexity of this algorithm is O (jV j + jEj), where jV j is the number of vertices and jEj is the number of edges in the graph.
Reference: [2] <author> W.E. </author> <title> Arnoldi. The principle of minimized iteration in the solution of the matrix eigenvalue problem. </title> <journal> Quarterly Journal of Applied Mathematics, </journal> <volume> 9 </volume> <pages> 17-29, </pages> <year> 1951. </year>
Reference-contexts: The constraints most commonly used are orthogonality type constraints. A projection method that has been used for Markov chain models is Arnoldi's method. <ref> [2, 10] </ref>.
Reference: [3] <author> P. Boyer, A. Dupuis and A. </author> <title> Khelladi A Simple Model for Repeated Calls due to Time-Outs. </title> <editor> CNET, LAA/SLC/EVP, Route de Tregastel, </editor> <address> 22301 Lannion, France, </address> <year> 1988. </year>
Reference-contexts: 8 7 6 5 4 3 2 1 Number of aggregates 206 2 2 2 2 2 2 2 2 2 55 4.2 Example 2: A Telecommunications Model The model illustrated in figure 2 has been used to determine the effect of impatient telephone customers on a computerized telephone exchange, <ref> [3] </ref>. In this model a request is made by a customer for service. The customer is prepared to wait for a certain period of time to get a reply.
Reference: [4] <author> Cao, W-L, and Stewart, W.J. </author> <title> Iterative aggregation/disaggregation techniques for nearly uncoupled Markov chains. </title> <journal> Journal of the Association for Computing Machinery 32, </journal> <volume> 3 (1985), </volume> <pages> 702-719. </pages>
Reference-contexts: On the other hand, IAD methods have proven to be very beneficial in the NCD case. Additionally, the only proofs of convergence for IAD methods that are currently available, depend on the fact that the transition probability matrix is nearly completely decomposable, <ref> [4] </ref>. For a review of the state of the art of these methods, and a discussions of many of the major issues involved, the interested reader should consult [12]. Within the IAD algorithm structure it is necessary to solve smaller matrix problems. <p> (m) = z (m) L (D U ) 1 (26) Therefore, this aggregation/disaggregation method is equivalent to the iterative for mula (m) = (m1) I (m1) L (D U ) 1 (27) It may be shown that , the exact stationary probability vector, is a fixed point of equation (27), <ref> [4] </ref>. We now turn our attention to some implementation details. The critical points are steps (c) through (e). In step (c), it is more efficient to compute P ij e only once for each block and to store it somewhere for use in all future iterations.
Reference: [5] <author> Courtois, P.J. </author> <title> Decomposability; Queueing and Computer System Applications. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida, </address> <year> 1977. </year>
Reference-contexts: The concept was later extended to Markov chains and the performance analysis of computer systems by Courtois, <ref> [5] </ref>.
Reference: [6] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1986. </year>
Reference-contexts: A rather large amount of memory is needed for the current implementation of our algorithm. Note that we have to partition the matrix into block form. The partitioned blocks were stored in sparse row-wise compact form, <ref> [6] </ref>, using work arrays arr (nz); jarr (nz) and iarr (iardim). The array arr holds the non-zero elements; elements of row i come before those of row i + 1, but the elements within a row are not necessarily in any order.
Reference: [7] <author> J.E. Hopcroft and R.J. Tarjan. </author> <title> Efficient Algorithms for Graph Manipulation," </title> <journal> Communications of the Association for Computing Machinery, </journal> <month> 16:6 (June, </month> <year> 1973), </year> <pages> pp. 372-378. </pages>
Reference-contexts: The algorithm we used is that of R.E. Tarjan, [16]. It is the depth-first search (DFS) algorithm which searches in the forward (deeper) direction as long as possible. Details of the non-recursive algorithm of DFS are given in [1]. Coding details for DFS are given in <ref> [7] </ref>. The complexity of this algorithm is O (jV j + jEj), where jV j is the number of vertices and jEj is the number of edges in the graph.
Reference: [8] <author> Koury, R., McAllister, D.F., and Stewart, W.J. </author> <title> Methods for computing stationary distributions of nearly completely decomposable Markov chains. </title> <journal> SIAM Journal of Algebraic and Discrete Mathematics 5, </journal> <volume> 2 (1984), </volume> <pages> 164-186. </pages>
Reference-contexts: The examples chosen for this study are the same as those presented in [9]. Some conclusions derived from the study are presented in the last section. This study differs from previous studies (such as <ref> [8] </ref>) in a number of important ways. It is not assumed that the matrix has been prearranged into "normal ncd form", so that our starting point is the development of a heuristic to find the ncd components and to order the state space accordingly.
Reference: [9] <author> B. Philippe, Y.Saad and W.J. </author> <title> Stewart Numerical methods in Markov chain modelling, </title> <type> Technical Report TR 89-21, N. </type> <institution> Carolina State Unversity, </institution> <address> Raleigh, N.C. 27695-8206, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: It is not clear that IAD methods out-perform other iterative methods such as preconditioned Generalized Minimal Residual, (GMRES) and Arnoldi or even Successive Over-Relaxation, (SOR) with a good adaptive scheme for approximating an appropriate 2 relaxation parameter, [14], in non-NCD models. In <ref> [9] </ref>, it is shown that when the matrices are not NCD then the quoted iterative methods are extremely effective. On the other hand, IAD methods have proven to be very beneficial in the NCD case. <p> Within the IAD algorithm structure it is necessary to solve smaller matrix problems. A number of numerical solution methods are available for this and in <ref> [9] </ref>, a large variety are analyzed and tested. In section (1), for the sake of completeness, we very briefly review these methods. IAD methods were not considered in [9], (due primarily to space limitations). <p> A number of numerical solution methods are available for this and in <ref> [9] </ref>, a large variety are analyzed and tested. In section (1), for the sake of completeness, we very briefly review these methods. IAD methods were not considered in [9], (due primarily to space limitations). In section (2), we provide some background on IAD methods and present the particular IAD method on which we conducted our numerical experiments. <p> In section (4), we describe the model problems and discuss our computational experience with these problems. The examples chosen for this study are the same as those presented in <ref> [9] </ref>. Some conclusions derived from the study are presented in the last section. This study differs from previous studies (such as [8]) in a number of important ways. <p> A number of numerical methods may be used and in this section we shall briefly review them. These methods are analyzed in detail, and efficient implementation strategies provided, in <ref> [9] </ref>. 1.1 Direct Solution Methods For certain Markov chain problems, the non-zero structure of the transition matrix is such that direct methods are very efficient and highly accurate. This is the case for example, when the non-zero elements lie close to the diagonal. <p> We refer to our implementation of this algorithm as GE (for Gaussian Elimination). For more information on analysis and implementation, the interested reader should refer to <ref> [9] </ref>. 1.2 Single vector iterations The simplest iteration method for computing the stationary probability vector from an irreducible stochastic matrix P is the single vector iteration x (k+1) = x (k) P One problem with this simple scheme is that its rate of convergence can be very slow. <p> These models have been taken from the literature and have been used to aid computer/communication system design. Additionally, they have already been used as a basis of comparison for numerical solution methods other than IAD, <ref> [9] </ref>. <p> Processes which terminate their service at the SM or FD queue return to the CPU queue. Symbolically, completion of a command is represented by a departure of the process from the CPU to the terminals. More details on this model, including the parameters used may be found in <ref> [9] </ref>. The model was solved for 20 users in the system, yielding a stochastic matrix of order 1,771 with 11,011 non-zero elements and also for 50 users, yielding a matrix of order 23,426 with 156,026 non-zero elements. <p> Although the results in <ref> [9] </ref> clearly demonstrate that the SOR method is unsatisfactory for solving the entire system, our results indicate that SOR can be successfully applied to solve the individual blocks. <p> PCGMR/iluth with o = 0:01 even failed to converge. This suggests that a preconditioner of higher quality should be used. The SOR methods performs poorly to solve this system without the A/D steps (See <ref> [9] </ref>). For the larger case, PCGMR and PCARN gave the most satisfactory result while SOR failed completely. [FXPTIT/GE] are not satisfactory compared to [PCGMR/GE] and [PCARN/GE]. 18 4.3 Example 3: A Multi-Class, Finite Buffer, Priority System This model, like model number 2, has also applicability to telecommunications modelling. <p> PCARN with ilu0 performs best. * As expected, iluth with o = 0:001 requires more computational overhead than iluth with o = 0:01. * Likewise, iluk with kmax = 10 requires more computational overhead than iluk with kmax = 5. * Based on the results in <ref> [9] </ref>, the preconditioned power iteration, FXPTIT, with ilu0 failed to solve the original system. <p> But they suffer from higher computational and memory storage requirements. As a compromise, we should generally choose a moderate threshold or k value. However, the results show that care must be taken not to choose a threshold that is too large. Note also that some results in <ref> [9] </ref> show that, for these problems, when an incomplete factorization into ~ L ~ U +E is performed, a smaller norm of E does not always guarantee a better preconditioner! 8.
Reference: [10] <author> Y. Saad. </author> <title> Variations on arnoldi's method for computing eigenelements of large un-symmetric matrices. </title> <journal> Linear Algebra and its Applicatons, </journal> <volume> 34 </volume> <pages> 269-295, </pages> <year> 1980. </year>
Reference-contexts: The constraints most commonly used are orthogonality type constraints. A projection method that has been used for Markov chain models is Arnoldi's method. <ref> [2, 10] </ref>.
Reference: [11] <author> Y. Saad and M.H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving non-symmetric linear systems. </title> <journal> SIAM Journal on Scientific and Statistical Compututing, </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: obtaining a new starting vector for the Krylov subspace is to choose that vector in the subspace that minimizes the residual, i.e. the vector v of the Krylov subspace for which jj (P T I)vjj 2 is closest to zero. this approach is referred to as GMRES (Generalized Minimal RESidual), <ref> [11] </ref>. Finally, it is possible to apply all the preconditioning techniques discussed previously to these algorithms.
Reference: [12] <author> Schweitzer, P.J. </author> <title> A survey of aggregation-disaggregation in large Markov chains. in Numerical Solution of Markov Chains; Editor, </title> <editor> William J. Stewart, </editor> <publisher> Marcel Dekker Publishers, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Consequently an error arises. This error will be small if the assumption is approximately true. In this paper we only consider the application of iterative aggregation/disaggregation methods to NCD examples even though they have been applied to models that are not NCD, <ref> [12] </ref>. It is not clear that IAD methods out-perform other iterative methods such as preconditioned Generalized Minimal Residual, (GMRES) and Arnoldi or even Successive Over-Relaxation, (SOR) with a good adaptive scheme for approximating an appropriate 2 relaxation parameter, [14], in non-NCD models. <p> For a review of the state of the art of these methods, and a discussions of many of the major issues involved, the interested reader should consult <ref> [12] </ref>. Within the IAD algorithm structure it is necessary to solve smaller matrix problems. A number of numerical solution methods are available for this and in [9], a large variety are analyzed and tested. In section (1), for the sake of completeness, we very briefly review these methods.
Reference: [13] <author> Simon, H.A., and Ando, A. </author> <title> Aggregation of variables in dynamic systems. </title> <booktitle> Economet-rica 29 (1961), </booktitle> <pages> 111-138 </pages>
Reference-contexts: Finally, it is possible to apply all the preconditioning techniques discussed previously to these algorithms. In fact, this is the way that these projection techniques are usually used. 6 2 Decompositional Methods 2.1 NCD Markov Chains The pioneering work on NCD systems was performed by Simon and Ando, <ref> [13] </ref>, in investigating the dynamic behaviour of linear systems as they apply to economic models. The concept was later extended to Markov chains and the performance analysis of computer systems by Courtois, [5].
Reference: [14] <author> Seelen, </author> <title> L.P. An algorithm for Ph/Ph/c queues. </title> <journal> European Journal of Operations Research, </journal> <volume> Vol. 23, </volume> <year> (1986), </year> <pages> 118-127. 24 </pages>
Reference-contexts: It is not clear that IAD methods out-perform other iterative methods such as preconditioned Generalized Minimal Residual, (GMRES) and Arnoldi or even Successive Over-Relaxation, (SOR) with a good adaptive scheme for approximating an appropriate 2 relaxation parameter, <ref> [14] </ref>, in non-NCD models. In [9], it is shown that when the matrices are not NCD then the quoted iterative methods are extremely effective. On the other hand, IAD methods have proven to be very beneficial in the NCD case. <p> A value of m = 10 was found to be satisfactory. 22 (b) For SOR, the optimal ! is difficult to find before run time, particularly when each block system may possess its own optimal ! value. The adoptive technique used by Seelen, <ref> [14] </ref>, to approximate this value and found to be effective in certain queueing systems, might be of value in this respect. Over the large number of experiments that we conducted, we found that values in the range 1.3-1.5 generally worked best. 6.
Reference: [15] <author> Stewart, W.J. </author> <title> A comparison of numerical techniques in Markov modelling. </title> <journal> Commu--nications of the Association for Computing machinery, Vol.21, No.2, </journal> <pages> 144-152, </pages> <year> 1978. </year>
Reference-contexts: It is the same model discussed by Stewart, <ref> [15] </ref> and is similar to that of Vantilborgh, [17].
Reference: [16] <author> R.J. </author> <title> Tarjan Depth first search and linear graph algorithms. </title> <journal> SIAM Journal of Computing 1:2,146-160, </journal> <year> 1972 </year>
Reference-contexts: To efficiently solve problems such as this on directed graphs, we need to visit the vertices and edges of the graph in a systematic fashion. The algorithm we used is that of R.E. Tarjan, <ref> [16] </ref>. It is the depth-first search (DFS) algorithm which searches in the forward (deeper) direction as long as possible. Details of the non-recursive algorithm of DFS are given in [1]. Coding details for DFS are given in [7].

References-found: 16

