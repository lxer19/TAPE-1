URL: ftp://ftp.cs.columbia.edu/reports/reports-1992/cucs-026-92.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1992.html
Root-URL: http://www.cs.columbia.edu
Title: Estimating the Largest Eigenvalue by the Power and Lanczos Algorithms with a Random Start  
Author: J. Kuczynski H. Wozniakowski 
Date: July, 1989  
Affiliation: Institute of Computer Science, Polish Academy of Sciences  Department of Computer Science, Columbia University and Institute of Informatics, University of Warsaw  
Pubnum: CUCS-026-92  
Abstract: Our problem is to compute an approximation to the largest eigenvalue of an n fi n large symmetric positive definite matrix with relative error at most ". We consider only algorithms that use Krylov information [b; Ab; : : : ; A k b] consisting of k matrix-vector multiplications for some unit vector b. If the vector b is chosen deterministically then the problem cannot be solved no matter how many matrix-vector multiplications are performed and what algorithm is used. If, however, the vector b is chosen randomly with respect to the uniform distribution over the unit sphere, then the problem can be solved on the average and probabilistically. More precisely, for a randomly chosen vector b we study the power and Lanczos algorithms. For the power algorithm (method) we prove sharp bounds on the average relative error and on the probabilistic relative failure. For the Lanczos algorithm we present only upper bounds. In particular, ln(n)=k characterizes the average relative error of the power algorithm, fl Supported in part by the National Science Foundation under Grant DCR-86-03674.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. E. P. Box and M. E. Muller. </author> <title> A Note on the Generation of Random Normal Deviates. </title> <journal> Ann. Math. Statist. </journal> <volume> 29 (1958), </volume> <pages> 610-611. </pages>
Reference-contexts: That is, (b; i ) 6= 0 for i = 1; 2; : : : ; n. This property is guaranteed if, for example, A is unreduced tridiagonal and b = <ref> [1; 0; : : : ; 0] </ref>. Although it is impossible to guarantee that (b; i ) 6= 0 for all i 2 [1; n], it is intuitively clear that (b; i ) 6= 0 should hold for "almost all" vectors b. <p> This property is guaranteed if, for example, A is unreduced tridiagonal and b = [1; 0; : : : ; 0]. Although it is impossible to guarantee that (b; i ) 6= 0 for all i 2 <ref> [1; n] </ref>, it is intuitively clear that (b; i ) 6= 0 should hold for "almost all" vectors b. This is definitely the case if the vector b is chosen randomly, say, with uniform distribution on the n-dimensional sphere of radius one. <p> 2 (h (b)) d ~ b ! 1=2 2 c n1 0 c n1 B 0 arctan 2 (h (b)) 0 X b 2 2 (k1) + i: x i &gt;fi i x i (1 x i ) 2 A d ~ b A ; for any number fi 2 <ref> [0; 1] </ref>. Here, c n1 = (n1)=2 =(1 + (n 1)=2) is the Lebesgue measure of the (n 1)-dimensional unit ball. 17 Consider the function H (t) = (1 t) 2 t 2 (k1) . <p> Therefore, for any fi 2 <ref> [0; 1] </ref> we have e Lan 2 c n1 inf c n1 4 B 0 i: x i fi i Q 2 (x i ) (1 x i ) 2 d ~ b 1 (1 fi) 2 B 0 (1 jjbjj 2 n1 ) d ~ b : w (fi) = <p> Then w (fi) T 2 p 1 + 1 fi 4 e 4 (k1) 1fi : (19) p 1 fi 2 <ref> [0; 1] </ref>. Then e Lan q Note that for k1 p 0:103 ln (n (k1) 4 ), part (a) of Theorem 3.2 trivially holds since e Lan k 1 : Assume thus that k 1 &gt; p 0:103 ln n (k 1) 4 . <p> Note that H (x) = (1 " x) x 2 (k1) for x 2 <ref> [0; 1] </ref> attains its maximum value at x fl = (1 ") (1 1=(2k 1)) and H (x fl ) = (1 ") 2k1 (1 1=(2k 1)) 2 (k1) =(2k 1). <p> Consider Q (x) = U 2 (k1) ( x=(1 ")) = U 2 (k1) (1= 1 "); x 2 <ref> [0; 1] </ref>: Since U 2 (k1) is even, Q is a polynomial of degree k 1. Clearly, Q (1) = 1, so Q 2 P k (1). <p> Indeed, using the polar coordinates b = (t) = [ 1 (t); : : : ; n (t)] with t = [r; t 1 ; : : : ; t n1 ] 2 <ref> [0; 1] </ref> fi [0; ] n1 and i+1 = r sin t i cos t i+1 cos t n1 ; i = 1; 2; : : : ; n 1; we have jdet ( 0 )j = r n1 j cos t 2 cos 2 t 3 cos n2 t n1
Reference: [2] <author> A. W. Chou. </author> <title> On the Optimality of Krylov Information. </title> <editor> J. </editor> <booktitle> Complexity 3 (1987), </booktitle> <pages> 26-40. </pages>
Reference: [3] <author> J. D. Dixon. </author> <title> Estimating Extremal Eigenvalues and Condition Numbers of Matrices. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 20 (1983), </volume> <pages> 812-814. </pages>
Reference: [4] <author> I. S. Gradshteyn and I. W. Ryzhik. </author> <title> Table of Integrals, Series, and Products. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: For the Lanczos algorithm we are only able to present upper bounds on its average relative error, see Theorem 3.2. We show that independently of the distribution of eigenvalues of the matrix A, the relative error is bounded by 2:575 (ln (n)=(k 1)) 2 for k 2 <ref> [4; n 1] </ref>, and that the relative error is zero if k is no less than the total number of distinct eigenvalues. To check the quality of this upper bound we performed many numerical tests. They are reported in Section 6. <p> Theorem 3.2 Let ~ Lan be the Lanczos algorithm defined by (4). (a) For any symmetric positive definite matrix A, let m denote the number of distinct eigenvalues of A. Then for k m; for k 2 <ref> [4; m 1] </ref>; k 1 2:575 ln n ! 2 (b) For any symmetric positive definite matrix A, let p; p &lt; n; denote the multiplicity of the largest eigenvalue 1 , and let p+1 and n be the second largest and the smallest eigenvalue of A. <p> Take Q (x) = i=2 Then Q 2 P k (1) and the integrand in (16) vanishes for b 1 6= 0. Since b 1 = 0 for a set of measure zero, we have e Lan k = 0, as claimed. Assume now that k 2 <ref> [4; m 1] </ref>.
Reference: [5] <author> W. Kahan and B. N. Parlett. </author> <title> How Far Should We Go with the Lanczos Process? in Sparse Matrix Computations eds. </title> <editor> J. Bunch and D. Rose, </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> (1976), </year> <pages> 131-144. </pages>
Reference: [6] <author> S. Kaniel. </author> <title> Estimates for Some Computational Techniques in Linear Algebra. </title> <journal> Math. Comp. </journal> <volume> 20 (1966), </volume> <pages> 369-378. </pages>
Reference: [7] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Science, </booktitle> <volume> Vol. 2: </volume> <booktitle> Seminumerical Algorithms. 2nd ed. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA., </address> <year> 1981. </year>
Reference: [8] <author> J. Kuczynski. </author> <title> On the Optimal Solution of Large Eigenpair Problems. </title> <editor> J. </editor> <booktitle> Complexity 2 (1986), </booktitle> <pages> 131-162. </pages>
Reference: [9] <author> A. S. Nemirovsky and D. B. Yudin. </author> <title> Problem Complexity and Method Efficiency in Optimization. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1983. </year>
Reference: [10] <author> D. P. O'Leary, G. W. Stewart and J. S. Vandergraft. </author> <title> Estimating the Largest Eigenvalue of a Positive Definite Matrix. </title> <journal> Math. Comput. </journal> <volume> 33 (1979), </volume> <pages> 1289-1292. </pages>
Reference: [11] <author> C. C. Paige. </author> <title> The Computation of Eigenvalues and Eigenvectors of Very Large Sparse Matrices. </title> <type> Ph. D. Thesis, </type> <institution> University of London, </institution> <year> 1971. </year>
Reference: [12] <author> C. C. Paige. </author> <title> Computational Variants of the Lanczos Method for the Eigen-problem. </title> <journal> J. Inst. Math. Appl. </journal> <volume> 10 (1972), </volume> <pages> 373-381. </pages>
Reference: [13] <author> B. N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, N. J., </address> <year> 1980. </year>
Reference: [14] <author> B. N. Parlett. </author> <title> Private communication. </title>
Reference: [15] <author> B. N. Parlett, H. Simon and L. M. Stringer. </author> <title> On Estimating the Largest Eigenvalue with the Lanczos Algorithm. </title> <journal> Math. Comput. </journal> <volume> 38 (1982), </volume> <pages> 153-165. </pages>
Reference: [16] <author> Y. Saad. </author> <title> On the Rates of Convergence of the Lanczos and the Block Lanczos Methods. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 17 (1980), </volume> <pages> 687-706. </pages>
Reference: [17] <author> D. S. Scott. </author> <title> Analysis of the Symmetric Lanczos Process. </title> <type> Ph. D. Thesis, </type> <institution> University of California at Berkeley, Berkeley, CA., </institution> <note> Memorandum NCB/ERLM 78/40, 1978. 38 </note>
Reference: [18] <author> B. T. Smith, J. M. Boyle, B. S. Garbov, Y. Ikebe, V. C. Klema, C. B. Moler. </author> <title> Matrix Eigensystem Routines-EISPACK Guide. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1974. </year>
Reference: [19] <author> J. F. Traub, G. W. Wasilkowski and H. Wozniakowski. </author> <title> Information-Based Complexity. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference: [20] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford Univ. Press, </publisher> <address> London and New York, </address> <year> 1965. </year> <month> 39 </month>
References-found: 20

