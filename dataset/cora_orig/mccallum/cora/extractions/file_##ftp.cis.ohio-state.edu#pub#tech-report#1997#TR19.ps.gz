URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1997/TR19.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Email: E-mail: fdai,pandag@cis.ohio-state.edu  
Phone: Tel: (614)-292-5199, Fax: (614)-292-2911  
Author: Dai and Dhabaleswar K. Panda Dhabaleswar K. Panda 
Keyword: Parallel architecture, distributed shared memory systems, performance modeling, interconnection networks, wormhole routing, topology, directory-based protocols, and cache coherence.  
Address: Columbus, OH 43210-1277  
Affiliation: Dept. of Computer and Information Science The Ohio State University,  
Note: Donglai  Contact Author:  This research is supported in part by NSF Grant MIP-9309627 and NSF Career Award MIP-9502294.  
Abstract: How Can We Design Better Networks for DSM Systems? fl Abstract: Most DSM research in current years have ignored the impact of interconnection network altogether. Similarly, most of the interconnection network research have focused on better network designs by using synthetic (uniform/non-uniform) traffic. Both these trends do not lead to any concrete guidelines about designing better networks for the emerging Distributed Shared Memory (DSM) paradigm. In this paper, we address these issues by taking a three-step approach. First, we propose a comprehensive parameterized model to estimate the performance of an application on a DSM system. This model takes into account all key aspects of a DSM system: application, processor, memory hierarchy, coherence protocol, network, etc. Next, using this model we evaluate the impact of different network design choices (link speed, link width, topology, ratio between router to physical link delay) on the overall performance of DSM applications and establish guidelines for designing better networks for DSM systems. Finally, we use simulations of SPLASH2 benchmark suites to validate our design guidelines. Some of the important design guidelines established in this paper are: 1) better performance is achieved by increasing link speed instead of link width, 2) changing topology of a network under constant bisection bandwidth constraint is not at all beneficial, 3) network contention experienced by short messages is very crucial to the overall performance, etc. These guidelines together with several others lays a good foundation for designing better networks for current and future generation DSM systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Chandra, J. R. Larus, and A. Rogers. </author> <booktitle> Where is time spent in message-passing and shared-memory programs? In The sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 61-73, </pages> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: It can also identify the bottlenecks associated with different components of the system. In the following section, we consider this model to study the impact of network design choices. It can be observed from the the recent literature <ref> [1, 10] </ref> that the current generation DSM systems and application characteristics exhibit the following range of values for the parameters used in our model: T grn from 2 to 500 processor cycles, R hit from 0.90 to 0.99, R miss from 0.01 to 0.10, T hit is from 0 to 2
Reference: [2] <author> T. Chen and J.-L. Baer. </author> <title> A Performance Study of Software and Hardware Data Prefetching Schemes. </title> <booktitle> In Proceedings of the 21st ACM Annual International Symposium on Computer Architecture (ISCA-21), </booktitle> <pages> pages 223-232, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Such a model of the processing node is powerful enough to capture almost all the characteristics introduced by current latency reduction or latency tolerance techniques, such as lockup-free caches [17], hardware and software data prefetching <ref> [2, 14] </ref>, relaxed memory consistency [8], speculative load and execution [16], multithreading [13], etc. Let us consider the behavior of a typical application on a processing node of a DSM system for a short duration, as depicted in Fig. 1 (b). Assume two threads are mapped onto this processor.
Reference: [3] <author> D. Dai and D. K. Panda. </author> <title> Reducing Cache Invalidation Overheads in Wormhole DSMs Using Multidestination Message Passing. </title> <booktitle> In International Conference on Parallel Processing, pages I:138-145, </booktitle> <address> Chicago, IL, </address> <month> Aug </month> <year> 1996. </year>
Reference-contexts: On a memory block access, the first word of the block is returned in 30 processor cycles (150 ns). The successive words in the block follow in a pipelined fashion. The machine is assumed to be using a full-mapped, invalidation-based, three-state directory coherence protocol <ref> [9, 7, 3] </ref>. The synchronization protocol assumed in the system is the QOLB protocol similar to the one used in DASH [8]. The node simulator models the internal structures, and the queuing and contention at the node controller, main memory, and cache.
Reference: [4] <author> D. Dai and D. K. Panda. </author> <title> How Much Does Network Contention Affect Distributed Shared Memory Performance? . Technical Report OSU-CISRC-2/97-TR14, </title> <institution> Dept. of Computer and Information Science, The Ohio State University, </institution> <month> Febuary </month> <year> 1997. </year> <month> 15 </month>
Reference-contexts: Thus, for a DSM system with a given node architecture and cache coherency protocol, the existing network design studies do not provide guidelines on the impact of different network components (link speed, link width, topology, routing adaptivity, router delay, etc.) on the overall performance of the system. In <ref> [4] </ref>, we have reported the performance impact of network contention under various designs of cache and memory module, processor speed, and different network components. But how to design better networks for DSM systems to reduce or avoid the degradation of performance caused by contention remains a challenge.
Reference: [5] <author> W. J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Network. </title> <journal> IEEE Trans--actions on Computers, </journal> <pages> pages 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: For a fair comparison, we consider two different strategies for varying topologies: under constant link width constraint and under constant bisection bandwidth constraint <ref> [5] </ref>. Under the former constraint, link width is kept constant as the dimension of the network changes. Under the second constraint, link width is changed so as to maintain constant bisection bandwidth. Let us consider three different topologies: 2d (8fi8), 4d (4fi4fi4) and 6d (2fi2fi2fi2fi2fi2) hypercube.
Reference: [6] <author> J. Duato, S. Yalamanchili, and L. Ni. </author> <title> Interconnection Networks: An Engineering Approach. </title> <publisher> The IEEE Press, </publisher> <year> 1996. </year>
Reference-contexts: Thus, these studies do not provide any concrete guidelines for designing efficient networks for DSM systems. During the recent years, a lot of studies have also taken place in the interconnection network community to design better networks <ref> [6] </ref>. These studies have considered the impact of topology, routing adaptivity, link width, link speed, router delay, etc. on the overall network performance. However, the design guidelines established in these studies are based on synthetic traffic (uniform/non-uniform/hot-spot), arbitrary/bi-modal message lengths, and message generation intervals following some probability distributions. <p> Various network contention delays are typically caused by limited resources in the network like queue sizes, injection channels, consumption channels, physical links, router/switch contention, etc. Since it is difficult to model network contention analytically <ref> [6] </ref>, we consider no-contention message latencies in this section. In the following section, we derive message latencies with contention from simulation results and compare them with the no-contention latencies. Consider the average message latencies for short and long messages, as indicated in Eqns. 8 and 9, respectively.
Reference: [7] <author> D. Lenoski et al. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: On a memory block access, the first word of the block is returned in 30 processor cycles (150 ns). The successive words in the block follow in a pipelined fashion. The machine is assumed to be using a full-mapped, invalidation-based, three-state directory coherence protocol <ref> [9, 7, 3] </ref>. The synchronization protocol assumed in the system is the QOLB protocol similar to the one used in DASH [8]. The node simulator models the internal structures, and the queuing and contention at the node controller, main memory, and cache.
Reference: [8] <author> D. Lenoski et al. </author> <title> The Stanford DASH Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In a hardware DSM system this latency is affected by various design choices such as node architecture, memory hierarchy, coherence protocol, and interconnection network. Most DSM research in recent years has focussed on the design of node architectures, coherence protocols, memory consistency techniques, etc. <ref> [8, 12] </ref>. Many of these studies have either ignored the interconnection network altogether or considered very idealistic model of the network a model with fixed delay, infinite bandwidth, and no contention. Thus, these studies do not provide any concrete guidelines for designing efficient networks for DSM systems. <p> Such a model of the processing node is powerful enough to capture almost all the characteristics introduced by current latency reduction or latency tolerance techniques, such as lockup-free caches [17], hardware and software data prefetching [2, 14], relaxed memory consistency <ref> [8] </ref>, speculative load and execution [16], multithreading [13], etc. Let us consider the behavior of a typical application on a processing node of a DSM system for a short duration, as depicted in Fig. 1 (b). Assume two threads are mapped onto this processor. <p> The successive words in the block follow in a pipelined fashion. The machine is assumed to be using a full-mapped, invalidation-based, three-state directory coherence protocol [9, 7, 3]. The synchronization protocol assumed in the system is the QOLB protocol similar to the one used in DASH <ref> [8] </ref>. The node simulator models the internal structures, and the queuing and contention at the node controller, main memory, and cache. Table 2 shows the memory hierarchy parameters and node controller occupancy delays used in our simulated architecture.
Reference: [9] <author> J. Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <year> 1994. </year>
Reference-contexts: In this paper we take on such a challenge and approach it in three steps. First, we develop a parameterized performance model for a CC-NUMA architecture using a wormhole [15] k-ary n-cube interconnection network. The node architecture is assumed to be similar to the FLASH system <ref> [9] </ref>. <p> to validate these results as well as study the impact of message contention on these guidelines. 4 Simulation Results In order to validate the analytical model and the guidelines derived in the last section, we simulated a DSM system with an architecture and coherence protocol similar to the FLASH machine <ref> [9] </ref> but with some differences described later. In this section, first, we describe the default architectural parameters used in the simulation. Then briefly, we discuss the applications in our experiments. <p> On a memory block access, the first word of the block is returned in 30 processor cycles (150 ns). The successive words in the block follow in a pipelined fashion. The machine is assumed to be using a full-mapped, invalidation-based, three-state directory coherence protocol <ref> [9, 7, 3] </ref>. The synchronization protocol assumed in the system is the QOLB protocol similar to the one used in DASH [8]. The node simulator models the internal structures, and the queuing and contention at the node controller, main memory, and cache.
Reference: [10] <author> S. C. Woo et al. </author> <title> The SPLASH-2 Programs: Chracterization and Methodological Considerations. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <year> 1995. </year>
Reference-contexts: The simulator models the processor, cache, and memory access references at an instruction level and models the network at a flit transfer level. The evaluations are based on four representative applications, ported from the SPLASH2 suite <ref> [10] </ref>, running on a 64 processor system. Our study establishes several guidelines for designing better networks for DSM systems. <p> It can also identify the bottlenecks associated with different components of the system. In the following section, we consider this model to study the impact of network design choices. It can be observed from the the recent literature <ref> [1, 10] </ref> that the current generation DSM systems and application characteristics exhibit the following range of values for the parameters used in our model: T grn from 2 to 500 processor cycles, R hit from 0.90 to 0.99, R miss from 0.01 to 0.10, T hit is from 0 to 2 <p> Therefore, at most one outstanding request can be issued by a processor. A load/store miss stalls the processor until the first word of data is returned. We used four applications | Barnes, LU, Radix, and Water in our simulation. These are ported from the widely used SPLASH2 suite <ref> [10] </ref>. Table 3 lists problem sizes for the various applications. These applications are well-structured and heavily optimized for DSM machines. For more information about the applications, the reader is requested to refer to [10]. 4.2 Performance Metrics We present all our simulation results in two sets: execution time and network latency. <p> These are ported from the widely used SPLASH2 suite <ref> [10] </ref>. Table 3 lists problem sizes for the various applications. These applications are well-structured and heavily optimized for DSM machines. For more information about the applications, the reader is requested to refer to [10]. 4.2 Performance Metrics We present all our simulation results in two sets: execution time and network latency. This helps us to correlate the behavior of underlying network with the overall DSM system performance and provide important insights into the network design issues.
Reference: [11] <author> J. L. Hennessy and D. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Mor-gan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The cache was assumed to operate in dual-port mode using write-back and write-allocate policies. The instruction latencies, issue rules, and memory interface were modeled based on the DLX design <ref> [11] </ref>. The memory bus is assumed to be 8 bytes wide. On a memory block access, the first word of the block is returned in 30 processor cycles (150 ns). The successive words in the block follow in a pipelined fashion.
Reference: [12] <author> D. A. Koufaty, X. Chen, D. K. Poulsen, and J. Torrellas. </author> <title> Data Forwarding in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <pages> pages 255-264, </pages> <year> 1995. </year>
Reference-contexts: In a hardware DSM system this latency is affected by various design choices such as node architecture, memory hierarchy, coherence protocol, and interconnection network. Most DSM research in recent years has focussed on the design of node architectures, coherence protocols, memory consistency techniques, etc. <ref> [8, 12] </ref>. Many of these studies have either ignored the interconnection network altogether or considered very idealistic model of the network a model with fixed delay, infinite bandwidth, and no contention. Thus, these studies do not provide any concrete guidelines for designing efficient networks for DSM systems.
Reference: [13] <author> J. Laudon, A. Gupta, and J. Horowitz. </author> <title> Interleaving: A multihtreading technique targeting multiprocessors and workstations. </title> <booktitle> In The sixth International Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 308-318, </pages> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Such a model of the processing node is powerful enough to capture almost all the characteristics introduced by current latency reduction or latency tolerance techniques, such as lockup-free caches [17], hardware and software data prefetching [2, 14], relaxed memory consistency [8], speculative load and execution [16], multithreading <ref> [13] </ref>, etc. Let us consider the behavior of a typical application on a processing node of a DSM system for a short duration, as depicted in Fig. 1 (b). Assume two threads are mapped onto this processor.
Reference: [14] <author> T. Mowry and A. Gupta. </author> <title> Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1991. </year>
Reference-contexts: Such a model of the processing node is powerful enough to capture almost all the characteristics introduced by current latency reduction or latency tolerance techniques, such as lockup-free caches [17], hardware and software data prefetching <ref> [2, 14] </ref>, relaxed memory consistency [8], speculative load and execution [16], multithreading [13], etc. Let us consider the behavior of a typical application on a processing node of a DSM system for a short duration, as depicted in Fig. 1 (b). Assume two threads are mapped onto this processor.
Reference: [15] <author> L. Ni and P. K. McKinley. </author> <title> A Survey of Wormhole Routing Techniques in Direct Networks. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 62-76, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: In this paper we take on such a challenge and approach it in three steps. First, we develop a parameterized performance model for a CC-NUMA architecture using a wormhole <ref> [15] </ref> k-ary n-cube interconnection network. The node architecture is assumed to be similar to the FLASH system [9].
Reference: [16] <author> A. Rogers and Kai Li. </author> <title> Software support for speculative loads. </title> <booktitle> In The Fifth International Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 38-50, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Such a model of the processing node is powerful enough to capture almost all the characteristics introduced by current latency reduction or latency tolerance techniques, such as lockup-free caches [17], hardware and software data prefetching [2, 14], relaxed memory consistency [8], speculative load and execution <ref> [16] </ref>, multithreading [13], etc. Let us consider the behavior of a typical application on a processing node of a DSM system for a short duration, as depicted in Fig. 1 (b). Assume two threads are mapped onto this processor.
Reference: [17] <author> G. Sohi and M. Franklin. </author> <title> High-performance data memory systems for superscalar processors. </title> <booktitle> In The Fourth International Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pages 53-62, </pages> <month> April </month> <year> 1991. </year> <month> 16 </month>
Reference-contexts: Let us assume that the processing node in our modeled system supports multithreading and multiple outstanding requests. Such a model of the processing node is powerful enough to capture almost all the characteristics introduced by current latency reduction or latency tolerance techniques, such as lockup-free caches <ref> [17] </ref>, hardware and software data prefetching [2, 14], relaxed memory consistency [8], speculative load and execution [16], multithreading [13], etc. Let us consider the behavior of a typical application on a processing node of a DSM system for a short duration, as depicted in Fig. 1 (b).
References-found: 17

