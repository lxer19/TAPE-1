URL: http://www.is.cs.cmu.edu/papers/multimodal/93.issd.ps.gz
Refering-URL: http://www.cs.cmu.edu:80/afs/cs.cmu.edu/user/tue/WWW/resume.html
Root-URL: 
Email: tue@cs.cmu.edu  ahw@cs.cmu.edu  
Title: MULTIMODAL HUMAN-COMPUTER INTERACTION  
Author: Minh Tue Vo Alex Waibel 
Keyword: Multiple modalities, multimodal interface, speech recognition, lip-reading, eye-tracking, gesture recognition, handwriting recognition  
Address: Pittsburgh, PA 15213-3890, U.S.A.  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Austin and R. Schwartz. </author> <title> A Comparison of Several Approximate Algorithms for Finding N-best Hypothe-ses. </title> <booktitle> In Proc. ICASSP91, </booktitle> <pages> pp. 701-704. </pages>
Reference-contexts: Following acoustic phonetic modeling, an efficient search algorithms must find the most likely word sequence in real time. The search module of the recognizer builds a sorted list of sentence hypotheses using the word-dependent N-best algorithm <ref> [1] </ref>. The resulting N-best list is resorted using tri-grams to further improve results. Resorting improves the word accuracy for the best scoring hypothesis (created using smoothed bigrams) from 91.5% to 98.8% [22] on a conference registration task [18].
Reference: [2] <author> S. Baluja and D. Pomerleau. </author> <title> Non-Intrusive Gaze Track ing Using Artificial Neural Networks. </title> <booktitle> To appear in Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: At Carnegie Mellon we have developed a neural-network-based non-intrusive gaze tracker based on camera input only (Baluja and Pomerleau <ref> [2] </ref>); the user is neither required to wear any special equipment, nor required to keep his head still. Input to the system comes from a camera mounted on top of the computer monitor. An infrared light source creates a specular reection on the eye. <p> The output units are organized with 50 output units for specifying the X coordinate, and 50 units for the Y coordinate. The gaussian output representation used is similar to that used in ALVINN [12]. Training is performed by backpropagation <ref> [2] </ref>. 50 X output Units 50 Y output Units 4 Hidden Units 15x30 Input Retina The current system works at 10 Hz. The best accuracy we have achieved is 1.5 degrees with the freedom of head move-ment up to 30 cm.
Reference: [3] <author> U. Bodenhausen and S. Manke. </author> <title> Connectionist Architec tural Learning for High Performance Character and Speech Recognition. </title> <booktitle> In Proc. </booktitle> <address> ICASSP93. </address>
Reference-contexts: T ime Delay Neural Network Dynamic T ime W arping SS scores ... layer layer layer The MS-TDNN [6] was applied successfully to overcome the problem of recognizing continuous (cursive) handwriting (Bodenhausen and Manke <ref> [3] </ref>). This problem is much more difficult than the single character problem because of the need for automatic segmentation; however, it is possible to resolve the type of conicts presented above using context.
Reference: [4] <author> C. Bregler, H. Hild, S. Manke, and A. Waibel. </author> <title> Improv ing Connected Letter Recognition by Lipreading. </title> <booktitle> In Proc. </booktitle> <address> ICASSP93. </address>
Reference-contexts: In order to exploit lip-reading as a source of information complementary to speech, we developed a lip-reading system based on the MS-TDNN and testing it on a letter spelling task for the German alphabet (Bregler et al. <ref> [4] </ref>). The recognition performance is understandably poor (31% using lip-reading only) because some phonemes cannot be distinguished using pure visual information; however, the thrust of this work is to show how a state-of-the-art speech recognition system can be significantly improved by considering additional visual information in the recognition process. <p> We arranged to record acoustic and visual data in parallel (see face of the speaker are recorded in real-time (30 frames/sec) and saved as 256x256 pixel images with 8-bit grey-level information per pixel. We applied two alternative preprocessing techniques: histogram normalized grey-value coding and 2-dimensional Fourier transformation <ref> [4] </ref>, performed on an area of interest (AOI) centered around the lips. As recognition system we use a modular MS-TDNN (see Figure 6 in section 3.1). The visual information is processed by one of two front-end TDNNs. <p> The visual information is processed by one of two front-end TDNNs. The classification is based on visemes, or the smallest set of visually distinguishable units in speech, which is a subset of phonemes. Simulation results are presented in section 3.1. Related papers are listed in <ref> [4] </ref>. 2.3 Eye-tracking The goal of gaze tracking is to determine where a person is looking from the appearance of his eye. <p> The system was constructed for the connected German letter spelling task which features a small but highly ambiguous vocabulary, with no grammar or other high-level information. The system is described in detail in <ref> [4] </ref>. The system shown in Figure 6 is based on a modular MS-TDNN architecture [8]. The preprocessed acoustic and visual data are fed into two front-end TDNNs [17], respectively. Backpropagation is used to train the networks separately in a bootstrapping phase, to fit phoneme targets. <p> The preprocessed acoustic and visual data are fed into two front-end TDNNs [17], respectively. Backpropagation is used to train the networks separately in a bootstrapping phase, to fit phoneme targets. The last layers (phone-state layers) of the TDNNs are combined using entropy weights <ref> [4] </ref>, and the DTW algorithm [10] is applied to find the optimal path of phone-hypotheses for the word models.
Reference: [5] <author> I. Guyon, P. Albrecht, Y. LeCun, J. Denker, and W. </author> <title> Hub bard. Design of a Neural Network Character Recognizer for a Touch Terminal. </title> <journal> Pattern Recognition, </journal> <year> 1990. </year>
Reference-contexts: This dynamic representation was motivated by its successful use in handwritten character recognition <ref> [5] </ref>. Results of experiments described in that work suggest that the time-sequential signal contains more information relevant to classification than the static image, leading to better performance. In our current implementation, the stream of data from the digitizing tablet is preprocessed [5] by normalizing and resampling the coordinates to eliminate differences <p> motivated by its successful use in handwritten character recognition <ref> [5] </ref>. Results of experiments described in that work suggest that the time-sequential signal contains more information relevant to classification than the static image, leading to better performance. In our current implementation, the stream of data from the digitizing tablet is preprocessed [5] by normalizing and resampling the coordinates to eliminate differences in size and drawing speed, and extracting local geometric information such as the direction of pen movement and the curvature of the trajectory. <p> The TDNN [17] has been applied successfully for on-line single character recognition <ref> [5] </ref>. With single character, no automatic segmentation is necessary; however, some conicts may arise that are unresolvable without context information.
Reference: [6] <author> P. Haffner, M. Franzini, and A. Waibel. </author> <title> Integrating Time Alignment and Neural Networks for High Performance Continuous Speech Recognition. </title> <booktitle> In Proc. </booktitle> <address> ICASSP91. </address>
Reference-contexts: T ime Delay Neural Network Dynamic T ime W arping SS scores ... layer layer layer The MS-TDNN <ref> [6] </ref> was applied successfully to overcome the problem of recognizing continuous (cursive) handwriting (Bodenhausen and Manke [3]). This problem is much more difficult than the single character problem because of the need for automatic segmentation; however, it is possible to resolve the type of conicts presented above using context.
Reference: [7] <author> A. Hauptmann. </author> <title> Speech and Gestures for Graphic Image Manipulation. </title> <booktitle> In Proc. </booktitle> <address> CHI89, </address> <publisher> ACM Press, </publisher> <pages> pp. 241-245. </pages>
Reference: [8] <author> H. Hild and A. Waibel. </author> <title> Connected Letter Recognition with a Multi-State Time Delay Neural Network. </title> <booktitle> Neural Information Processing Systems (NIPS 5). </booktitle>
Reference-contexts: The system was constructed for the connected German letter spelling task which features a small but highly ambiguous vocabulary, with no grammar or other high-level information. The system is described in detail in [4]. The system shown in Figure 6 is based on a modular MS-TDNN architecture <ref> [8] </ref>. The preprocessed acoustic and visual data are fed into two front-end TDNNs [17], respectively. Backpropagation is used to train the networks separately in a bootstrapping phase, to fit phoneme targets.
Reference: [9] <author> X. Huang, F. Alleva, H. Hon, M. Hwang, K. Lee, and R. Rosenfeld. </author> <title> The SPHINX-II Speech Recognition System: An Overview. Computer Speech and Language (in press), </title> <year> 1993. </year>
Reference-contexts: Some of these will be described in the following. 2.1.1 Large Vocabulary Continuous Speech Recognition Amongst the pure HMM systems, the Sphinx system is available as a large vocabulary speaker independent speech recognition server <ref> [9] </ref>. Some experiments aimed at speech interface design are carried out using this server.
Reference: [10] <author> H. Ney. </author> <title> The Use of a One-Stage Dynamic Programming Algorihtm for Connected Word Recognition. </title> <booktitle> In Proc. </booktitle> <address> ICASSP84. </address>
Reference-contexts: The MS-TDNN integrates the recognition and segmentation processes by combining the high accuracy character recognition capabilities of a TDNN with a non-linear time alignment procedure (Dynamic Time Warping) <ref> [10] </ref> for finding an optimal alignment between strokes and characters in handwritten continuous words (see Figure 4). <p> The preprocessed acoustic and visual data are fed into two front-end TDNNs [17], respectively. Backpropagation is used to train the networks separately in a bootstrapping phase, to fit phoneme targets. The last layers (phone-state layers) of the TDNNs are combined using entropy weights [4], and the DTW algorithm <ref> [10] </ref> is applied to find the optimal path of phone-hypotheses for the word models.
Reference: [11] <author> C. Nodine, H. Kundel, L. Toto, and E. Krupinski. </author> <title> Recording and Analyzing Eye-position Data Using a Microcomputer Workstation. Behavior Research Methods, </title> <type> Instruments & Computers 24 (3), </type> <year> 1992, </year> <pages> pp. 475-584 </pages>
Reference-contexts: Two potential uses of a gaze tracker are as an alternative to the mouse as an input modality [20] and as an analysis tool for human-computer interaction studies <ref> [11] </ref>. The direction of eye fixation can also be used to determine the users focus of attention in a multimodal interface; for instance, knowing whether the user is looking at the screen or somewhere else while talking may be important in deciding whether automated speech recognition should be activated.
Reference: [12] <author> D. Pomerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <type> Ph.D. Thesis, </type> <institution> Carnegie Mellon University, CMU-CS-92-115. </institution>
Reference-contexts: The output units are organized with 50 output units for specifying the X coordinate, and 50 units for the Y coordinate. The gaussian output representation used is similar to that used in ALVINN <ref> [12] </ref>. Training is performed by backpropagation [2]. 50 X output Units 50 Y output Units 4 Hidden Units 15x30 Input Retina The current system works at 10 Hz. The best accuracy we have achieved is 1.5 degrees with the freedom of head move-ment up to 30 cm.
Reference: [13] <author> R. Rose and D. Paul. </author> <title> A Hidden Markov Model Based Keyword Recognition Systems. </title> <booktitle> In Proc. </booktitle> <address> ICASSP90. </address>
Reference: [14] <author> O. Schmidbauer and J. Tebelskis. </author> <title> An LVQ-based Refer ence Model for Speaker-Adaptive Speech Recognition. </title> <booktitle> In Proc. ICASSP92, </booktitle> <pages> pp. 441-444. </pages>
Reference: [15] <author> J. Tebelskis and A. Waibel. </author> <title> Performance Through Con sistency: MS-TDNNs for Large Vocabulary Continuous Speech Recognition. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: [16] <author> M.T. Vo and A. Waibel. </author> <title> A Multimodal Human-Com puter Interface: Combination of Speech and Gesture Recognition. </title> <booktitle> In Adjunct Proc. </booktitle> <address> InterCHI93. </address>
Reference-contexts: We developed a multimodal text editor capable of recognizing speech and gesture commands (Vo and Waibel <ref> [16] </ref>). <p> error rate by 40-50%, Acoustic TDNN Visual TDNN Phoneme/Viseme State Layer Hidden Layer Input Layer Combined Layer DTW Layer Letter Hypotheses (26 German Letters) especially in the presence of noise. 3.2 Speech and Gesture Recognition Joint interpretation of multimodal events was successfully demonstrated in our speech and gesture-based text editor <ref> [16] </ref>. Figure 8 shows a block diagram of the multimodal inter-preter module. The TDNN-based gesture recognizer was described in section 2.4. For the speech component we use the word spotter (sec-tion 2.1.2) coupled with a semantic-fragment parser [19].
Reference: [17] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. </author> <title> Phoneme Recognition Using Time-Delay Neural Networks. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <year> 1989. </year>
Reference-contexts: At Carnegie Mellon we have imple mented and tested such a system on two spontaneous continu-ous speech databases (Zeppenfeld et al. [23][24]). The word spotting system architecture is based upon the Time Delay Neural Network (TDNN) <ref> [17] </ref>, and more recently the Multi-State Time Delay Neural Network (MS-TDNN) [6][23]. <p> These features are believed to hold discrimina-tory information that could help in the recognition process. 2.4.2 Gesture Classification Using Neural Networks We use a TDNN <ref> [17] </ref> (see Figure 3) to classify each preprocessed time-sequential signal as a gesture among the predefined set of 8 gestures. Each gesture in the set is represented by an output neuron. The network is trained on a set of manu-ally-classified gestures using a modified backpropagation algorithm [17]. <p> We use a TDNN <ref> [17] </ref> (see Figure 3) to classify each preprocessed time-sequential signal as a gesture among the predefined set of 8 gestures. Each gesture in the set is represented by an output neuron. The network is trained on a set of manu-ally-classified gestures using a modified backpropagation algorithm [17]. The output neuron with the highest activation level determines the recognized gesture. <p> Like in speech recognition the main problem of recognizing continuous words is that character or stroke boundaries are not known (in particular if no pen lifts or white space indicate these boundaries) and an optimal time alignment has to be found. The TDNN <ref> [17] </ref> has been applied successfully for on-line single character recognition [5]. With single character, no automatic segmentation is necessary; however, some conicts may arise that are unresolvable without context information. <p> The system is described in detail in [4]. The system shown in Figure 6 is based on a modular MS-TDNN architecture [8]. The preprocessed acoustic and visual data are fed into two front-end TDNNs <ref> [17] </ref>, respectively. Backpropagation is used to train the networks separately in a bootstrapping phase, to fit phoneme targets. The last layers (phone-state layers) of the TDNNs are combined using entropy weights [4], and the DTW algorithm [10] is applied to find the optimal path of phone-hypotheses for the word models.
Reference: [18] <author> A. Waibel, A. Jain, A. McNair, H. Saito, A. Hauptmann, J. Tebelskis. </author> <title> JANUS: a Speech-to-speech Translation System Using Connectionist and Symbolic Processing Strategies. </title> <booktitle> In Proc. </booktitle> <address> ICASSP91. </address>
Reference-contexts: The resulting N-best list is resorted using tri-grams to further improve results. Resorting improves the word accuracy for the best scoring hypothesis (created using smoothed bigrams) from 91.5% to 98.8% [22] on a conference registration task <ref> [18] </ref>. Speed and memory requirements have been dramatically improved from earlier versions by using information collected in the first-best search for aggressive pruning in the N-best search; by dynamically adapting MULTIMODAL HUMAN-COMPUTER INTERACTION Minh Tue Vo and Alex Waibel School of Computer Science, Carnegie Mellon University Pittsburgh, PA 15213-3890, U.S.A.
Reference: [19] <author> W. Ward. </author> <title> Understanding Spontaneous Speech: the Phoe nix System. </title> <booktitle> In Proc. ICASSP91, </booktitle> <pages> pp. 365-367. </pages>
Reference-contexts: Figure 8 shows a block diagram of the multimodal inter-preter module. The TDNN-based gesture recognizer was described in section 2.4. For the speech component we use the word spotter (sec-tion 2.1.2) coupled with a semantic-fragment parser <ref> [19] </ref>. The word spotter was trained to spot 11 keywords representing editing commands such as move, delete,... and textual units such as character, word,... The effect is to let the user speak naturally without having to worry about grammar and vocabulary, as long as the utterance contains the relevant keywords.
Reference: [20] <author> C. Ware and H. Mikaelian. </author> <title> An Evaluation of an Eye Tracker as a Device for Computer Input. </title> <booktitle> In Human Fac-tors in Computing Systems IV, </booktitle> <year> 1987. </year>
Reference-contexts: Related papers are listed in [4]. 2.3 Eye-tracking The goal of gaze tracking is to determine where a person is looking from the appearance of his eye. Two potential uses of a gaze tracker are as an alternative to the mouse as an input modality <ref> [20] </ref> and as an analysis tool for human-computer interaction studies [11].
Reference: [21] <author> J. Wilpon, L. Miller, and P. Modi. </author> <title> Improvements and Applications for Keyword Recognition Using Hidden Markov Modeling Techniques. </title> <booktitle> In Proc. </booktitle> <address> ICASSP91. </address>
Reference: [22] <editor> M. Woszczyna et al. </editor> <booktitle> Recent Advances in JANUS:A Speech Translation System. In Proc. </booktitle> <address> EURO-SPEECH93. </address>
Reference-contexts: The search module of the recognizer builds a sorted list of sentence hypotheses using the word-dependent N-best algorithm [1]. The resulting N-best list is resorted using tri-grams to further improve results. Resorting improves the word accuracy for the best scoring hypothesis (created using smoothed bigrams) from 91.5% to 98.8% <ref> [22] </ref> on a conference registration task [18].
Reference: [23] <author> T. Zeppenfeld and A. Waibel. </author> <title> A Hybrid Neural Network, Dynamic Programming Word Spotter. </title> <booktitle> In Proc. </booktitle> <address> ICASSP92. </address>
Reference: [24] <author> T. Zeppenfeld, R. Houghton, and A. Waibel. </author> <title> Improving the MS-TSNN for Word Spotting. </title> <booktitle> In Proc. </booktitle> <address> ICASSP93. </address>
Reference-contexts: a system on two spontaneous continu-ous speech databases (Zeppenfeld et al. [23]<ref> [24] </ref>). The word spotting system architecture is based upon the Time Delay Neural Network (TDNN) [17], and more recently the Multi-State Time Delay Neural Network (MS-TDNN) [6][23]. A diagram of the basic network architecture is shown in Figure 1. [24] presents several recent improvements such as training with noise, average spectrum removal, equal occurrence keyword training, word duration modelling, state duration modelling, enforced minimum state durations, training with context frames, and keyword variant modeling. <p> Training and testing of the system was performed on two separate databases, the Roadrally corpus, and the new Switchboard corpus <ref> [24] </ref>. The systems performance is measured by plotting the keyword detection rate for several false alarm rates per keyword per hour (fa/(kw*hr)). By changing the thresholds of the word-output units, the detection rate can be improved at the expense of increasing the number of false alarms. <p> The Figure of Merit (FOM) for the system is the averaged keyword detection rate over the false alarms from 0 to 10 fa/(kw*hr). Our system achieves an FOM = 72.2% for the Roadrally corpus and 50.9% on the much more difficult Switchboard corpus <ref> [24] </ref>. These figures compare favorably to those of other keyword spotting systems in its class evaluated by DARPA. Our word spotting system has proved to be a viable alternative to the much larger full vocabulary speech recognition systems.
References-found: 24

