URL: http://www.cs.umn.edu/Users/dept/users/kumar/cluster-hyper.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Clustering Based On Association Rule Hypergraphs  
Author: Eui-Hong (Sam) Han George Karypis Vipin Kumar Bamshad Mobasher 
Keyword: Data mining, clustering, association rules, hypergraph partitioning.  
Note: This work was supported by NSF grant ASC-9634719, Army Research Office contract DA/DAAH04-95-1-0538, Cray Research Inc. Fellowship, and IBM Partnership Award, the content of which does not necessarily reflect the policy of the government, and no official endorsement should be inferred. Access to computing facilities was provided by AHPCRC,  and NSF grant CDA-9414015. See http://www.cs.umn.edu/han for other related papers.  
Address: 4-192 EECS Bldg., 200 Union St. SE Minneapolis, MN 55455, USA  
Affiliation: Department of Computer Science University of Minnesota  Minnesota Supercomputer Institute, Cray Research Inc.,  
Pubnum: Technical Report  
Email: fhan,karypis,kumar,mobasherg@cs.umn.edu  
Date: 97-019 (June 18, 1997)  
Abstract: A short version of this paper appears in Workshop on Research Issues on Data Mining and Knowledge Discovery 1997 Abstract Traditional clustering algorithms, used in data mining for transactional databases, are mainly concerned with grouping transactions, but they do not generally provide an adequate mechanism for grouping items found within these transactions. Item clustering, on the other hand, can be useful in many data mining applications. We propose a new method for clustering related items in transactional databases that is based on partitioning an association rule hypergraph, where each association rule defines a hyperedge. We also discuss some of the applications of item clustering, such as the discovery of meta-rules among item clusters, and clustering of transactions. We evaluated our scheme experimentally on data from a number of domains, and, wherever applicable, compared it with AutoClass. In our experiment with stock-market data, our clustering scheme is able to successfully group stocks that belong to the same industry group. In the experiment with congressional voting data, this method is quite effective in finding clusters of transactions that correspond to either democrat or republican voting patterns. We found clusters of segments of protein-coding sequences from protein coding database that share the same functionality and thus are very valuable to biologist for determining functionality of new proteins. We also found clusters of related words in documents retrieved from the World Wide Web (a common and important application in information retrieval). These experiments demonstrate that our approach holds promise in a wide range of domains, and is much faster than traditional clustering algorithms such as AutoClass. 
Abstract-found: 1
Intro-found: 1
Reference: [AGM C 90] <author> Stephen Altschul, Warren Gish, Webb Miller, Eugene Myers, and David Lipman. </author> <title> Basic local alignment search tool. </title> <journal> Journal of Molecular Biology, </journal> <volume> 215 </volume> <pages> 403-410, </pages> <year> 1990. </year>
Reference-contexts: To rapidly determine the function of many previously unknown genes, biologists quickly generate short segments of protein-coding sequences (called expressed sequence tags, or ESTs) and match each EST against the sequences of the known proteins, using similarity matching algorithms <ref> [AGM C 90, PL88] </ref>. If the EST clusters are available that are related to each other functionally, then biologists can match the new protein against the EST clusters to find those EST clusters that match the protein most closely.
Reference: [AIS93] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proc. of 1993 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: The second step is to generate association rules from these frequent item-sets. The computation of finding the frequent item-sets is usually much more expensive than finding the rules from these frequent item-sets. A number of algorithms have been developed for discovering frequent item-sets <ref> [AIS93, AS94, HS95] </ref>. Apriori algorithm presented in [AS94] is one of the most efficient algorithms available.
Reference: [AMS C 96] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. </author> <title> Fast discovery of association rules. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In this paper, we propose a new methodology for clustering related items in transactions using association rules and hypergraph partitioning. Association rule discover in data mining has been used to discover relationships in very large data repositories <ref> [AMS C 96, HS95] </ref>. Here we explore the feasibility and advantages of using the discovered association rules to cluster closely related items into groups. <p> Here we explore the feasibility and advantages of using the discovered association rules to cluster closely related items into groups. Our algorithm for item clustering uses as its basis the frequent item sets, derived as part of association rule discovery, that meet a minimum support criterion <ref> [AMS C 96] </ref>. These frequent item sets are then used to group items into hyper-graph edges, and a hypergraph partitioning algorithm [KAKS97] is used to find the item clusters. We also explore the use of discovered item clusters to cluster related transactions containing the data items. <p> modeling and the clustering algorithm. 3 TID Items 1 Bread, Coke, Milk 2 Beer, Bread 3 Beer, Coke, Diaper, Milk 4 Beer, Bread, Diaper, Milk 5 Coke, Diaper, Milk Table 1: Transactions from supermarket. 3.1 Association Rules Association rules capture the relationship of items that are present in a transaction <ref> [AMS C 96] </ref>. Let T be the set of transactions where each transaction is a subset of the item-set I . <p> The problem of finding association rules has been shown to be linearly scalable with respect to the number of transactions <ref> [AMS C 96] </ref>. Very fast and highly efficient algorithms such as Apriori are able to quickly find association rules in very large databases. For example, our experiments with Apriori have shown that the rules in more than 1 million transactions can be found in less than hundreds of seconds. <p> We chose AutoClass for comparison because AutoClass can handle data with the mixture of continuous and discrete attributes, where distance or similarity measures are difficult to define, and is known for producing good quality clusters. In all of our experiments, we used a locally implemented version of Apriori algorithm <ref> [AMS C 96] </ref> to find the association rules and construct the association-rule hypergraph. We used the average of confidences of the association rules as a weight for the corresponding hyperedges. For partitioning the association-rule hy-pergraph, we used HMETIS [KAKS97].
Reference: [AS94] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules. </title> <booktitle> In Proc. of the 20th VLDB Conference, </booktitle> <pages> pages 487-499, </pages> <address> Santiago, Chile, </address> <year> 1994. </year> <month> 19 </month>
Reference-contexts: The second step is to generate association rules from these frequent item-sets. The computation of finding the frequent item-sets is usually much more expensive than finding the rules from these frequent item-sets. A number of algorithms have been developed for discovering frequent item-sets <ref> [AIS93, AS94, HS95] </ref>. Apriori algorithm presented in [AS94] is one of the most efficient algorithms available. <p> The computation of finding the frequent item-sets is usually much more expensive than finding the rules from these frequent item-sets. A number of algorithms have been developed for discovering frequent item-sets [AIS93, AS94, HS95]. Apriori algorithm presented in <ref> [AS94] </ref> is one of the most efficient algorithms available. This algorithm has been experimentally shown to be linearly scalable with respect to the size of the database [AS94], and can also be implemented on parallel computers [HKK97] to use their large memory and processing power effectively. 4 3.2 Hypergraph Modeling A <p> A number of algorithms have been developed for discovering frequent item-sets [AIS93, AS94, HS95]. Apriori algorithm presented in <ref> [AS94] </ref> is one of the most efficient algorithms available. This algorithm has been experimentally shown to be linearly scalable with respect to the size of the database [AS94], and can also be implemented on parallel computers [HKK97] to use their large memory and processing power effectively. 4 3.2 Hypergraph Modeling A hypergraph [Ber76] H D .V ; E / consists of a set of vertices (V ) and a set of hyperedges (E ).
Reference: [Ber76] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> American Elsevier, </publisher> <year> 1976. </year>
Reference-contexts: This algorithm has been experimentally shown to be linearly scalable with respect to the size of the database [AS94], and can also be implemented on parallel computers [HKK97] to use their large memory and processing power effectively. 4 3.2 Hypergraph Modeling A hypergraph <ref> [Ber76] </ref> H D .V ; E / consists of a set of vertices (V ) and a set of hyperedges (E ). A hyper-graph is an extension of a graph in the sense that each hyperedge can connect more than two vertices.
Reference: [CHY96] <author> M.S. Chen, J. Han, and P.S. Yu. </author> <title> Data mining: An overview from database perspective. </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> 8(6) </volume> <pages> 866-883, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Clustering in data mining <ref> [SAD C 93, CHY96, GHK C 96] </ref> is a discovery process that groups a set of data such that the intracluster similarity is maximized and the intercluster similarity is minimized [CHY96]. These discovered clusters are used to explain the characteristics of the data distribution. <p> 1 Introduction Clustering in data mining [SAD C 93, CHY96, GHK C 96] is a discovery process that groups a set of data such that the intracluster similarity is maximized and the intercluster similarity is minimized <ref> [CHY96] </ref>. These discovered clusters are used to explain the characteristics of the data distribution.
Reference: [CS96] <author> P. Cheeseman and J. Stutz. </author> <title> Baysian classification (autoclass): Theory and results. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In transaction databases, attributes generally represent items that could potentially occur in one transaction. If I is the number of different items, then each transaction can be represented by a point in an I - dimensional space. Traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> focus mainly on grouping together such transactions based on some measure of similarity or distance. These techniques, however, do not generally provide an adequate mechanism for grouping related items that appear within transactions. <p> We evaluated our item clustering schemes on five different data sets, namely S&P500 stock data, US congressional voting data, protein coding data, Web document data and US census data. Wherever applicable, we compared our results with the those of AutoClass <ref> [CS96] </ref>. These experiments demonstrate that our approach is applicable in a wide range of domains and provide better clusters and is much faster than AutoClass. <p> Section 3 presents clustering of items and Section 4 discusses applications of the item clusters . Section 5 presents the experimental results. Section 6 contains conclusion and future works. 2 Related Work Clustering of transactions can be done using methods that have been studied in several areas including statistics <ref> [DJ80, Lee81, CS96] </ref>, machine learning [SD90, Fis95], and data mining [NH94, CS96]. Most of the these approaches are based on either probability, distance or similarity measure. <p> Section 5 presents the experimental results. Section 6 contains conclusion and future works. 2 Related Work Clustering of transactions can be done using methods that have been studied in several areas including statistics [DJ80, Lee81, CS96], machine learning [SD90, Fis95], and data mining <ref> [NH94, CS96] </ref>. Most of the these approaches are based on either probability, distance or similarity measure. Distance-based methods such as k-means method [JD88], nearest-neighbor method [LF78] and CLARANS algorithm [NH94] are effective when the distance between two data points can be easily defined. <p> However, these multidimensional 2 scaling methods have high computational complexity of O.n 2 / where n is the number of transactions [JD88]. Probability based methods do not require distance measure and are more applicable in data mining domain where attribute space has mixture of discrete and continuous attributes. AutoClass <ref> [CS96] </ref> is based on the probabilistic mixture modeling [TSM85] and handles the mixture of attributes.
Reference: [DJ80] <author> R. Dubes and A.K. Jain. </author> <title> Clustering methodologies in exploratory data analysis. </title> <editor> In M.C. Yovits, editor, </editor> <booktitle> Advances in Computers. </booktitle> <publisher> Academic Press Inc., </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: In transaction databases, attributes generally represent items that could potentially occur in one transaction. If I is the number of different items, then each transaction can be represented by a point in an I - dimensional space. Traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> focus mainly on grouping together such transactions based on some measure of similarity or distance. These techniques, however, do not generally provide an adequate mechanism for grouping related items that appear within transactions. <p> Most of these clustering algorithms are able to effectively cluster transactions only when the dimensionality of the space (i.e., the number of different items) is relatively small and most of the items are present in each transaction <ref> [DJ80, SD90, NH94] </ref>. However, these schemes fail to produce meaningful clusters, if the number of items is large and/or the fraction of the items present in each transaction is small. This type of data-sets are quite common in many data mining domains (e.g., market basket analysis). <p> Section 3 presents clustering of items and Section 4 discusses applications of the item clusters . Section 5 presents the experimental results. Section 6 contains conclusion and future works. 2 Related Work Clustering of transactions can be done using methods that have been studied in several areas including statistics <ref> [DJ80, Lee81, CS96] </ref>, machine learning [SD90, Fis95], and data mining [NH94, CS96]. Most of the these approaches are based on either probability, distance or similarity measure.
Reference: [EG95] <author> T. Eiter and G. Gottlob. </author> <title> Identifying the minimal transversals of a hypergraph and related problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 24(6) </volume> <pages> 1278-1304, </pages> <year> 1995. </year>
Reference-contexts: The use of hypergraphs in data mining has been studied recently. For example [GKMT97] shows that the problem of finding maximal elements in a lattice of patterns is closely related to the hypergraph transversal problem <ref> [EG95] </ref> and explore the use of known algorithms for finding maximal elements to solve the hyper-graph transversal problem.
Reference: [Fis95] <author> D. Fisher. </author> <title> Optimization and simplification of hierarchical clusterings. </title> <booktitle> In Proc. of the First Int'l Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 118-123, </pages> <address> Montreal, Quebec, </address> <year> 1995. </year>
Reference-contexts: In transaction databases, attributes generally represent items that could potentially occur in one transaction. If I is the number of different items, then each transaction can be represented by a point in an I - dimensional space. Traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> focus mainly on grouping together such transactions based on some measure of similarity or distance. These techniques, however, do not generally provide an adequate mechanism for grouping related items that appear within transactions. <p> Section 5 presents the experimental results. Section 6 contains conclusion and future works. 2 Related Work Clustering of transactions can be done using methods that have been studied in several areas including statistics [DJ80, Lee81, CS96], machine learning <ref> [SD90, Fis95] </ref>, and data mining [NH94, CS96]. Most of the these approaches are based on either probability, distance or similarity measure. Distance-based methods such as k-means method [JD88], nearest-neighbor method [LF78] and CLARANS algorithm [NH94] are effective when the distance between two data points can be easily defined.
Reference: [Fra92] <author> W.B. Frakes. </author> <title> Stemming algorithms. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: These word clusters can then be used to find similar documents from the Web, or potentially serve as a description or label for classifying documents [WP97]. We collected 87 documents from the Network for Excellence in Manufacturing (NEM Online) 1 site. From these documents we used a stemming algorithm <ref> [Fra92] </ref> to find the distinct stems that appear in them. There were a total of 5772 distinct word stems. We then constructed 87 transactions (one for each document) such that a distinct word stem is in the transaction only if it is contained in the document.
Reference: [GHK C 96] <author> M. Ganesh, E.H. Han, V. Kumar, S. Shekhar, and J. Srivastava. </author> <title> Visual data mining: Framework and algorithm development. </title> <type> Technical Report TR-96-021, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1996. </year>
Reference-contexts: 1 Introduction Clustering in data mining <ref> [SAD C 93, CHY96, GHK C 96] </ref> is a discovery process that groups a set of data such that the intracluster similarity is maximized and the intercluster similarity is minimized [CHY96]. These discovered clusters are used to explain the characteristics of the data distribution.
Reference: [GKMT97] <author> D. Gunopulos, R. Khardon, H. Mannila, and H. Toivonen. </author> <title> Data mining, hypergraph transversals, </title> <booktitle> and machine learning. In Proc. of Symposium on Principles of Database Systems, </booktitle> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: In data mining domain, where the n and d is large, AutoClass' run time can be unacceptably high. The use of hypergraphs in data mining has been studied recently. For example <ref> [GKMT97] </ref> shows that the problem of finding maximal elements in a lattice of patterns is closely related to the hypergraph transversal problem [EG95] and explore the use of known algorithms for finding maximal elements to solve the hyper-graph transversal problem.
Reference: [HF95] <author> J. Han and Y. Fu. </author> <title> Discovery of multiple-level association rules from large databases. </title> <booktitle> In Proc. of the 21st VLDB Conference, </booktitle> <address> Zurich, Switzerland, </address> <year> 1995. </year>
Reference-contexts: We discretize the attributes in Table 3 and find association rules from these meta transactions. The results are association rules among different item clusters. Generalized or multi-level association rule discovery algorithms <ref> [SA95, HF95] </ref> require the 8 hierarchy to be provided explicitly from the user. However, in many real life domains, the hierarchy of items may be unavailable or hard to define. Item clusters provide a structure that is discovered from the transactions without any user provided information.
Reference: [HKK97] <author> E.H. Han, G. Karypis, and V. Kumar. </author> <title> Scalable parallel data mining for association rules. </title> <booktitle> In Proc. of 1997 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: Apriori algorithm presented in [AS94] is one of the most efficient algorithms available. This algorithm has been experimentally shown to be linearly scalable with respect to the size of the database [AS94], and can also be implemented on parallel computers <ref> [HKK97] </ref> to use their large memory and processing power effectively. 4 3.2 Hypergraph Modeling A hypergraph [Ber76] H D .V ; E / consists of a set of vertices (V ) and a set of hyperedges (E ).
Reference: [HS95] <author> M. A. W. Houtsma and A. N. Swami. </author> <title> Set-oriented mining for association rules in relational databases. </title> <booktitle> In Proc. of the 11th Int'l Conf. on Data Eng., </booktitle> <pages> pages 25-33, </pages> <address> Taipei, Taiwan, </address> <year> 1995. </year>
Reference-contexts: In this paper, we propose a new methodology for clustering related items in transactions using association rules and hypergraph partitioning. Association rule discover in data mining has been used to discover relationships in very large data repositories <ref> [AMS C 96, HS95] </ref>. Here we explore the feasibility and advantages of using the discovered association rules to cluster closely related items into groups. <p> The second step is to generate association rules from these frequent item-sets. The computation of finding the frequent item-sets is usually much more expensive than finding the rules from these frequent item-sets. A number of algorithms have been developed for discovering frequent item-sets <ref> [AIS93, AS94, HS95] </ref>. Apriori algorithm presented in [AS94] is one of the most efficient algorithms available.
Reference: [JD88] <author> A.K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Most of the these approaches are based on either probability, distance or similarity measure. Distance-based methods such as k-means method <ref> [JD88] </ref>, nearest-neighbor method [LF78] and CLARANS algorithm [NH94] are effective when the distance between two data points can be easily defined. <p> However, these multidimensional 2 scaling methods have high computational complexity of O.n 2 / where n is the number of transactions <ref> [JD88] </ref>. Probability based methods do not require distance measure and are more applicable in data mining domain where attribute space has mixture of discrete and continuous attributes. AutoClass [CS96] is based on the probabilistic mixture modeling [TSM85] and handles the mixture of attributes.
Reference: [KA96] <author> A.J. Knobbe and P.W. Adriaans. </author> <title> Analysing binary associations. </title> <booktitle> In Proc. of the Second Int'l Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 311-314, </pages> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: The clustering or grouping of association rules have been proposed in [TKR C 95], [LSW97] and <ref> [KA96] </ref>. [TKR C 95] proposed clustering of association rules that have the same right-hand side to structure association rules for better understanding. <p> The algorithm does not handle the discrete attributes on the left-hand side of the rules. In both of these works, the focus is on finding clusters of association rules that have the same right hand side rather than on finding item clusters. <ref> [KA96] </ref> also proposed to cluster database attributes based on binary associations. In this approach, the association graph is constructed by taking items in a database as vertex set and considering binary associations among the items as edges in the graph. <p> It is suggested that successive removal of edges from the minimum spanning tree would produce clusters of attributes, but it is noted that a good criterion for continuing the clustering process is hard to define <ref> [KA96] </ref>. 3 Clustering of Items Our algorithm for clustering related items consists of the following two steps.
Reference: [KAKS97] <author> G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. </author> <title> Multilevel hypergraph partitioning: Application in VLSI domain. </title> <booktitle> In Proceedings ACM/IEEE Design Automation Conference, </booktitle> <year> 1997. </year>
Reference-contexts: Our algorithm for item clustering uses as its basis the frequent item sets, derived as part of association rule discovery, that meet a minimum support criterion [AMS C 96]. These frequent item sets are then used to group items into hyper-graph edges, and a hypergraph partitioning algorithm <ref> [KAKS97] </ref> is used to find the item clusters. We also explore the use of discovered item clusters to cluster related transactions containing the data items. <p> Thus computing these association rules is not a bottleneck. Furthermore, many times these rules will be already available as a result of an earlier analysis. Hypergraph partitioning is a well studied problem and highly efficient algorithms such as HMETIS have been developed <ref> [KAKS97] </ref>. In particular, the complexity of HMETIS for a k-way partitioning is O..V C E / log k/ where V is the number of vertices and E is the number of edges. <p> We used the average of confidences of the association rules as a weight for the corresponding hyperedges. For partitioning the association-rule hy-pergraph, we used HMETIS <ref> [KAKS97] </ref>. HMETIS is a multilevel partitioning algorithm that has been shown to quickly produce partitions that minimize the sum of the weight of the hyperedges that straddle partitions (i.e., minimize the hyperedge cut). HMETIS produces k-way partitions, where k (i.e., the number of partitions) is specified by the user.
Reference: [Lee81] <author> R.C.T. Lee. </author> <title> Clustering analysis and its applications. </title> <editor> In J.T. Toum, editor, </editor> <booktitle> Advances in Information Systems Science. </booktitle> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1981. </year> <month> 20 </month>
Reference-contexts: In transaction databases, attributes generally represent items that could potentially occur in one transaction. If I is the number of different items, then each transaction can be represented by a point in an I - dimensional space. Traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> focus mainly on grouping together such transactions based on some measure of similarity or distance. These techniques, however, do not generally provide an adequate mechanism for grouping related items that appear within transactions. <p> Section 3 presents clustering of items and Section 4 discusses applications of the item clusters . Section 5 presents the experimental results. Section 6 contains conclusion and future works. 2 Related Work Clustering of transactions can be done using methods that have been studied in several areas including statistics <ref> [DJ80, Lee81, CS96] </ref>, machine learning [SD90, Fis95], and data mining [NH94, CS96]. Most of the these approaches are based on either probability, distance or similarity measure.
Reference: [LF78] <author> S.Y. Lu and K.S. Fu. </author> <title> A sentence-to-sentence clustering procedure for pattern analysis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8 </volume> <pages> 381-389, </pages> <year> 1978. </year>
Reference-contexts: Most of the these approaches are based on either probability, distance or similarity measure. Distance-based methods such as k-means method [JD88], nearest-neighbor method <ref> [LF78] </ref> and CLARANS algorithm [NH94] are effective when the distance between two data points can be easily defined.
Reference: [LSW97] <author> B. Lent, A. Swami, and J. Widom. </author> <title> Clustering association rules. </title> <booktitle> In Proc. of the 13th Int'l Conf. on Data Eng., </booktitle> <address> Birmingham, U.K., </address> <year> 1997. </year>
Reference-contexts: The clustering or grouping of association rules have been proposed in [TKR C 95], <ref> [LSW97] </ref> and [KA96]. [TKR C 95] proposed clustering of association rules that have the same right-hand side to structure association rules for better understanding. The clustering is accomplished by defining a distance between association rules (the number of rows where the two rules differ). [LSW97] proposed a heuristic approach to clustering <p> been proposed in [TKR C 95], <ref> [LSW97] </ref> and [KA96]. [TKR C 95] proposed clustering of association rules that have the same right-hand side to structure association rules for better understanding. The clustering is accomplished by defining a distance between association rules (the number of rows where the two rules differ). [LSW97] proposed a heuristic approach to clustering two attribute association rules, based on the geometric properties of the two-dimensional grid. Association rules of the form A ^ B ) C, where A and B are continuous attributes and C is categorical, are clustered in this approach.
Reference: [MJHS96] <author> B. Mobasher, N. Jain, E.H. Han, and J. Srivastava. </author> <title> Web mining: Pattern discovery from world wide web transactions. </title> <type> Technical Report TR-96-050, </type> <institution> Department of Computer Science, University of Minnesota, M inneapolis, </institution> <year> 1996. </year>
Reference: [MM96] <author> C.J. Merz and P.M. Murphy. </author> <title> UCI repository of machine learning databases. </title> <note> In http://www.ics.uci.edu/ mlearn/MLRepository.html, </note> <year> 1996. </year>
Reference-contexts: Rules 3 shows that the Gold cluster and the Financial cluster move in opposite directions, which is a widely understood market behavior. 5.2 Congressional Voting Records Database The second data set that we experimented with is the Congressional Voting Records Database provided by <ref> [MM96] </ref>. This data set includes 435 transactions each corresponding to one Congressman's vote on 16 key issues. Thus, for each bill the data set has two different items, one corresponding to YES or NO vote. <p> AutoClass is particularly surprising since the dimensionality of the data set is relatively small (only 87 distinct items) and there are a relatively small number of transactions. 5.5 US Census Database Finally, to test the scalability of our item-clustering algorithm on a large data set we used the Adult Database <ref> [MM96] </ref>, which is a subset of US Census data. This database has 48,842 transactions, 6 continuous and 8 discrete attributes. One transaction corresponds to one person's census data information. The continuous attributes were discretized based on the schemes proposed in [SA96] in the preprocessing step.
Reference: [NH94] <author> R. Ng and J. Han. </author> <title> Efficient and effective clustering method for spatial data mining. </title> <booktitle> In Proc. of the 20th VLDB Conference, </booktitle> <pages> pages 144-155, </pages> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: In transaction databases, attributes generally represent items that could potentially occur in one transaction. If I is the number of different items, then each transaction can be represented by a point in an I - dimensional space. Traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> focus mainly on grouping together such transactions based on some measure of similarity or distance. These techniques, however, do not generally provide an adequate mechanism for grouping related items that appear within transactions. <p> Most of these clustering algorithms are able to effectively cluster transactions only when the dimensionality of the space (i.e., the number of different items) is relatively small and most of the items are present in each transaction <ref> [DJ80, SD90, NH94] </ref>. However, these schemes fail to produce meaningful clusters, if the number of items is large and/or the fraction of the items present in each transaction is small. This type of data-sets are quite common in many data mining domains (e.g., market basket analysis). <p> Section 5 presents the experimental results. Section 6 contains conclusion and future works. 2 Related Work Clustering of transactions can be done using methods that have been studied in several areas including statistics [DJ80, Lee81, CS96], machine learning [SD90, Fis95], and data mining <ref> [NH94, CS96] </ref>. Most of the these approaches are based on either probability, distance or similarity measure. Distance-based methods such as k-means method [JD88], nearest-neighbor method [LF78] and CLARANS algorithm [NH94] are effective when the distance between two data points can be easily defined. <p> Most of the these approaches are based on either probability, distance or similarity measure. Distance-based methods such as k-means method [JD88], nearest-neighbor method [LF78] and CLARANS algorithm <ref> [NH94] </ref> are effective when the distance between two data points can be easily defined. Transaction data, however, often contains a mixture of attributes, and so the distance between data points in the multidimensional space of attributes may be very hard to define, thus making distance-based methods inapplicable.
Reference: [NRS C 95] <author> T. Newman, E.F. Retzel, E. Shoop, E. Chi, and C. Somerville. </author> <title> Arabidopsis thaliana expressed sequence tags: Generation, analysis and dissemination. In Plant Genome III: </title> <booktitle> International Conference on the Status of Plant Genome Research, </booktitle> <address> San Diego, CA, </address> <year> 1995. </year>
Reference-contexts: This led to a hypergraph with 407 vertices and 128,082 hyperedges. We used HMETIS to find 40 partitions, out of which 39 of them satisfied the fitness criteria. These 39 EST clusters were then given to biologist (the authors of <ref> [NRS C 95, SCC C 95] </ref>) to determine whether or not they are related. Their analysis showed that most of the EST clusters found by our algorithm correspond to ESTs that are related.
Reference: [PL88] <author> William R. Pearson and David J. Lipman. </author> <title> Improved tools for biological sequence comparison. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 85 </volume> <pages> 2444-2448, </pages> <year> 1988. </year>
Reference-contexts: To rapidly determine the function of many previously unknown genes, biologists quickly generate short segments of protein-coding sequences (called expressed sequence tags, or ESTs) and match each EST against the sequences of the known proteins, using similarity matching algorithms <ref> [AGM C 90, PL88] </ref>. If the EST clusters are available that are related to each other functionally, then biologists can match the new protein against the EST clusters to find those EST clusters that match the protein most closely.
Reference: [SA95] <author> R. Srikant and R. Agrawal. </author> <title> Mining generalized association rules. </title> <booktitle> In Proc. of the 21st VLDB Conference, </booktitle> <pages> pages 407-419, </pages> <address> Zurich, Switzerland, </address> <year> 1995. </year>
Reference-contexts: We discretize the attributes in Table 3 and find association rules from these meta transactions. The results are association rules among different item clusters. Generalized or multi-level association rule discovery algorithms <ref> [SA95, HF95] </ref> require the 8 hierarchy to be provided explicitly from the user. However, in many real life domains, the hierarchy of items may be unavailable or hard to define. Item clusters provide a structure that is discovered from the transactions without any user provided information.
Reference: [SA96] <author> R. Srikant and R. Agrawal. </author> <title> Mining quantitative association rules in large relational tables. </title> <booktitle> In Proc. of 1996 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Montreal, Quebec, </address> <year> 1996. </year>
Reference-contexts: For discrete attributes with multiple values, each value of the attribute is considered to be a separate item. Continuous attributes can be handled once they have been discretized using techniques proposed in <ref> [SA96] </ref>. 3.4 Computational Complexity There are two distinct phases in our item-clustering algorithm, namely finding the association rules and partitioning the association-rule hypergraph. The problem of finding association rules has been shown to be linearly scalable with respect to the number of transactions [AMS C 96]. <p> This database has 48,842 transactions, 6 continuous and 8 discrete attributes. One transaction corresponds to one person's census data information. The continuous attributes were discretized based on the schemes proposed in <ref> [SA96] </ref> in the preprocessing step. This dis-cretization of the continues attributes, increased the total number of items from 14 to 108. We constructed the association-rule hypergraph by finding all association rules that have a support of at least 0.1%, which lead to a hypergraph with 108 vertices and 541 hyperedges.
Reference: [SAD C 93] <author> M. Stonebraker, R. Agrawal, U. Dayal, E. J. Neuhold, and A. Reuter. </author> <title> DBMS research at a crossroads: The vienna update. </title> <booktitle> In Proc. of the 19th VLDB Conference, </booktitle> <pages> pages 688-692, </pages> <address> Dublin, Ire-land, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Clustering in data mining <ref> [SAD C 93, CHY96, GHK C 96] </ref> is a discovery process that groups a set of data such that the intracluster similarity is maximized and the intercluster similarity is minimized [CHY96]. These discovered clusters are used to explain the characteristics of the data distribution.
Reference: [SCC C 95] <author> E. Shoop, E. Chi, J. Carlis, P. Bieganski, J. Riedl, N. Dalton, T. Newman, and E. Retzel. </author> <title> Implementation and testing of an automated EST processing and analysis system. </title> <editor> In Lawrence Hunter and Bruce Shriver, editors, </editor> <booktitle> Proceedings of the 28th Annual Hawaii International Conference on System Sciences, </booktitle> <volume> volume 5, </volume> <pages> pages 52-61. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: This led to a hypergraph with 407 vertices and 128,082 hyperedges. We used HMETIS to find 40 partitions, out of which 39 of them satisfied the fitness criteria. These 39 EST clusters were then given to biologist (the authors of <ref> [NRS C 95, SCC C 95] </ref>) to determine whether or not they are related. Their analysis showed that most of the EST clusters found by our algorithm correspond to ESTs that are related.
Reference: [SD90] <author> J.W. Shavlik and T.G. Dietterich. </author> <booktitle> Readings in Machine Learning. </booktitle> <address> Morgan-Kaufman, </address> <year> 1990. </year>
Reference-contexts: In transaction databases, attributes generally represent items that could potentially occur in one transaction. If I is the number of different items, then each transaction can be represented by a point in an I - dimensional space. Traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> focus mainly on grouping together such transactions based on some measure of similarity or distance. These techniques, however, do not generally provide an adequate mechanism for grouping related items that appear within transactions. <p> Most of these clustering algorithms are able to effectively cluster transactions only when the dimensionality of the space (i.e., the number of different items) is relatively small and most of the items are present in each transaction <ref> [DJ80, SD90, NH94] </ref>. However, these schemes fail to produce meaningful clusters, if the number of items is large and/or the fraction of the items present in each transaction is small. This type of data-sets are quite common in many data mining domains (e.g., market basket analysis). <p> Section 5 presents the experimental results. Section 6 contains conclusion and future works. 2 Related Work Clustering of transactions can be done using methods that have been studied in several areas including statistics [DJ80, Lee81, CS96], machine learning <ref> [SD90, Fis95] </ref>, and data mining [NH94, CS96]. Most of the these approaches are based on either probability, distance or similarity measure. Distance-based methods such as k-means method [JD88], nearest-neighbor method [LF78] and CLARANS algorithm [NH94] are effective when the distance between two data points can be easily defined.
Reference: [Sho96] <author> Elizabeth Shoop. </author> <title> The Design and Implementation of an Extended Database System to Support Biological Sequence Similarity Analysis. </title> <type> PhD thesis, </type> <institution> University of Minnesota, </institution> <month> September </month> <year> 1996. </year>
Reference: [TKR C 95] <author> H. Toivonen, M. Klemettinen, P. Ronkainen, K. Hatonen, and H. Mannila. </author> <title> Pruning and grouping discovered association rules. </title> <booktitle> In ECML-95 Workshop on Statistics, Machine Learning, and Knowledge Discovery in Databases,, </booktitle> <address> Heraklion, Greece, </address> <year> 1995. </year> <month> 21 </month>
Reference-contexts: The clustering or grouping of association rules have been proposed in <ref> [TKR C 95] </ref>, [LSW97] and [KA96]. [TKR C 95] proposed clustering of association rules that have the same right-hand side to structure association rules for better understanding. <p> The clustering or grouping of association rules have been proposed in <ref> [TKR C 95] </ref>, [LSW97] and [KA96]. [TKR C 95] proposed clustering of association rules that have the same right-hand side to structure association rules for better understanding.
Reference: [TSM85] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Dis--tributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: Probability based methods do not require distance measure and are more applicable in data mining domain where attribute space has mixture of discrete and continuous attributes. AutoClass [CS96] is based on the probabilistic mixture modeling <ref> [TSM85] </ref> and handles the mixture of attributes. However, the underlying expectation-maximization (EM) algorithm [TSM85] has the computational complexity of O.kd 2 n I / where k is the number of clusters, d is the number of attributes, n is the number of transactions and I is the average number of iterations <p> Probability based methods do not require distance measure and are more applicable in data mining domain where attribute space has mixture of discrete and continuous attributes. AutoClass [CS96] is based on the probabilistic mixture modeling <ref> [TSM85] </ref> and handles the mixture of attributes. However, the underlying expectation-maximization (EM) algorithm [TSM85] has the computational complexity of O.kd 2 n I / where k is the number of clusters, d is the number of attributes, n is the number of transactions and I is the average number of iterations of the EM algorithm.
Reference: [WP97] <author> Marilyn Wulfekuhler and Bill Punch. </author> <title> Finding salient features for personal web page categories. </title> <booktitle> In 6th WWW Conference, </booktitle> <address> Santa Clara, CA, </address> <year> 1997. </year> <month> 22 </month>
Reference-contexts: Specifically, we are interested in finding clusters of related words that tend to appear together in a document. These word clusters can then be used to find similar documents from the Web, or potentially serve as a description or label for classifying documents <ref> [WP97] </ref>. We collected 87 documents from the Network for Excellence in Manufacturing (NEM Online) 1 site. From these documents we used a stemming algorithm [Fra92] to find the distinct stems that appear in them. There were a total of 5772 distinct word stems.
References-found: 36

