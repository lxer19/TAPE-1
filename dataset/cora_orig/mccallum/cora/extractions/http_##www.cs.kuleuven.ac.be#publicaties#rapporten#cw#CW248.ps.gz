URL: http://www.cs.kuleuven.ac.be/publicaties/rapporten/cw/CW248.ps.gz
Refering-URL: http://www.cs.kuleuven.ac.be/publicaties/rapporten/CW1997.html
Root-URL: 
Title: Controlling Generalisation and Polyvariance in Partial Deduction of Normal Logic Programs  
Author: Michael Leuschel Bern Martens Danny De Schreye 
Affiliation: Department of Computing Science, K.U.Leuven  
Date: 248, February 1997  
Pubnum: Report CW  
Abstract: In this paper, we further elaborate global control for partial deduction: For which atoms, among possibly infinitely many, should partial deductions be produced, meanwhile guaranteeing correctness as well as termination, and providing ample opportunities for fine-grained polyvariance? Our solution is based on two ingredients. First, we use the well-known concept of a characteristic tree to guide abstraction (or gen-eralisation) and polyvariance, and aim for producing one specialised procedure per characteristic tree generated. Previous work along this line failed to provide abstraction correctly dealing with characteristic trees. We show how this can be rectified in an elegant way. Secondly, we structure combinations of atoms and associated characteristic trees in global trees registering "causal" relationships among such pairs. This will allow us to spot looming non-termination and consequently perform proper generalisation in order to avert the danger, without having to impose a depth bound on characteristic trees. Leaving unspecified the specific local control one may wish to plug in, the resulting global control strategy enables partial deduction that always terminates in an elegant, non ad hoc way, while providing excellent specialisation as well as fine-grained (but reasonable) polyvariance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers | Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: In the proof of this lemma we need to combine characteristic paths. A characteristic path being a sequence, we can simply concatenate two characteristic paths ffi 1 ; ffi 2 . For this we will use the standard notation ffi 1 ffi 2 from formal language theory <ref> [1, 29] </ref>. Lemma 3.23 Let P be a program, (A; t A ) and (A; t fl A ) safe P -characteristic atoms with t A t fl A . <p> We then try to collapse nodes with identical characteristic trees using the well-known algorithm for minimisation of finite state automata <ref> [1, 29] </ref>: we start by putting all characteristic atoms with the same characteristic tree into the same class, and subsequently split these classes if corresponding body atoms fall into different classes. <p> Adapting the algorithm from <ref> [1, 29] </ref> to our needs, we start out by generating three classes (one for each characteristic tree) of states: * C 1 = f (member (a; [a; b; c; djT ]); t); (member (a; L); t ); (member (b; L); t )g, * C 2 = f (member (a; [b; c;
Reference: [2] <author> K. Benkerimi and P. M. Hill. </author> <title> Supporting transformations for the partial evaluation of logic programs. </title> <journal> Journal of Logic and Computation, </journal> <volume> 3(5) </volume> <pages> 469-486, </pages> <month> October </month> <year> 1993. </year> <month> 49 </month>
Reference-contexts: Independence requires that no two atoms in A have a common instance. This ensures that no two specialised predicate definitions match the same (run-time) call. Usually this condition is satisfied by performing a renaming of the atoms in A, see <ref> [20, 2] </ref>. Closedness (as well as the related notion of coveredness in [3]) basically requires that every atom in the body of a resultant is matched by a specialised 2 To avoid the problematic resultant A A. 4 predicate definition. <p> The result then follows in a rather straightforward manner from the Theorems 3.5 and 4.11 in <ref> [2] </ref>. In [2] the filtering has been split into 2 phases: one which does just the renaming to ensure independence (called partial deduction with dynamic renaming; correctness of this phase is proven in Theorem 3.5 of [2]) and one which does the filtering (called post-processing renaming; the correctness of this phase <p> The result then follows in a rather straightforward manner from the Theorems 3.5 and 4.11 in <ref> [2] </ref>. In [2] the filtering has been split into 2 phases: one which does just the renaming to ensure independence (called partial deduction with dynamic renaming; correctness of this phase is proven in Theorem 3.5 of [2]) and one which does the filtering (called post-processing renaming; the correctness of this phase is proven <p> then follows in a rather straightforward manner from the Theorems 3.5 and 4.11 in <ref> [2] </ref>. In [2] the filtering has been split into 2 phases: one which does just the renaming to ensure independence (called partial deduction with dynamic renaming; correctness of this phase is proven in Theorem 3.5 of [2]) and one which does the filtering (called post-processing renaming; the correctness of this phase is proven in Theorem 4.11 of [2]). <p> split into 2 phases: one which does just the renaming to ensure independence (called partial deduction with dynamic renaming; correctness of this phase is proven in Theorem 3.5 of <ref> [2] </ref>) and one which does the filtering (called post-processing renaming; the correctness of this phase is proven in Theorem 4.11 of [2]). <p> that: * P 0 corresponds to partial deduction with dynamic renaming and post-processing renaming for the multiset of atoms A = fA j (A; t ) 2 Ag (indeed the same atom could in principle occur in several characteristic atoms, this is not a problem however as the results in <ref> [2] </ref> carry over to multisets of atoms | alternatively one could add an extra unused argument to P 0 , ff (G) and A and then place different variables in that new position to transform the multiset A into an ordinary set). * P 0 [ f ff (G)g is A-covered <p> Three minor technical issues have to be addressed in order to reuse the theorems from <ref> [2] </ref>: * Theorem 3.5 of [2] requires that no renaming be performed on G, i.e. ff (G) must be equal to G. <p> Three minor technical issues have to be addressed in order to reuse the theorems from <ref> [2] </ref>: * Theorem 3.5 of [2] requires that no renaming be performed on G, i.e. ff (G) must be equal to G. <p> Trivially the query new (X 1 ; : : : ; X k ) and G are equivalent wrt c.a.s. and finite failure (see also Lemma 2.2 in [20]). * Theorem 4.11 of <ref> [2] </ref> requires that G contains no variables or predicates in A. The requirement about the variables is not necessary in our case because we do not base our renaming on the mgu. <p> The requirement about the predicates is another way of ensuring that ff (G) must be equal to G, which can be circumvented in a similar way as for the first point above. * Theorems 3.5 and 4.11 of <ref> [2] </ref> require that the predicates of the renaming do not occur in the original P . Our Definition 3.12 does not require this. This is of no importance as the original program is always "completely thrown away" in our approach. <p> Our Definition 3.12 does not require this. This is of no importance as the original program is always "completely thrown away" in our approach. We can still apply these theorems by using an intermediate renaming 0 which satisfies the requirements of Theorems 3.5 and 4.11 of <ref> [2] </ref> and then applying an additional one step post-processing renaming 00 , with ff = 0 00 , along with an extra application of Theorem 4.11 of [2]. 2 3.3.2 Correctness for Safe Characteristic Atoms To prove the correctness for safe characteristic atoms we need the following adaptation of Lemma 4.12 <p> We can still apply these theorems by using an intermediate renaming 0 which satisfies the requirements of Theorems 3.5 and 4.11 of <ref> [2] </ref> and then applying an additional one step post-processing renaming 00 , with ff = 0 00 , along with an extra application of Theorem 4.11 of [2]. 2 3.3.2 Correctness for Safe Characteristic Atoms To prove the correctness for safe characteristic atoms we need the following adaptation of Lemma 4.12 from [49] (we just formalise the use in [49] of "corresponding SLDNF-derivation" in terms of characteristic paths).
Reference: [3] <author> K. Benkerimi and J. W. Lloyd. </author> <title> A partial evaluation procedure for logic programs. </title> <editor> In S. Debray and M. Hermenegildo, editors, </editor> <booktitle> Proceedings of the North American Conference on Logic Programming, </booktitle> <pages> pages 343-358. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In a nutshell, the local level decides on how SLD (NF)- trees for individual atoms should be built. The (branches of the) resulting trees allow to construct specialised clauses for the given atoms <ref> [49, 3] </ref>. <p> Then the set of resultants resultants (t ) is defined to be fA 1 G 1 ; : : : ; A n G n g. Partial deduction, as defined e.g. in <ref> [49, 3] </ref>, uses the resultants for a given set of atoms A to construct the specialised program and for each atom in A a different spe-cialised predicate definition is generated, replacing the original definition in P . <p> This ensures that no two specialised predicate definitions match the same (run-time) call. Usually this condition is satisfied by performing a renaming of the atoms in A, see [20, 2]. Closedness (as well as the related notion of coveredness in <ref> [3] </ref>) basically requires that every atom in the body of a resultant is matched by a specialised 2 To avoid the problematic resultant A A. 4 predicate definition. This guarantees that A forms a complete description of all possible computations that can occur at run-time of the specialised program.
Reference: [4] <author> R. Bol. </author> <title> Loop checking in partial deduction. </title> <journal> The Journal of Logic Programming, </journal> 16(1&2):25-46, 1993. 
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs <ref> [21, 64, 4, 6, 52, 54] </ref> have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when it has to decide on control issues such as
Reference: [5] <author> M. Bruynooghe. </author> <title> A practical framework for the abstract interpretation of logic programs. </title> <journal> The Journal of Logic Programming, </journal> <volume> 10 </volume> <pages> 91-124, </pages> <year> 1991. </year>
Reference-contexts: Very few abstract interpretations of logic programs use infinite domains of infinite height (some notable exceptions are <ref> [5, 30, 27] </ref>) and to our knowledge all of them have some a priori limitation of the precision, at least in practice.
Reference: [6] <author> M. Bruynooghe, D. De Schreye, and B. Martens. </author> <title> A general criterion for avoiding infinite unfolding during partial deduction. </title> <journal> New Generation Computing, </journal> <volume> 11(1) </volume> <pages> 47-79, </pages> <year> 1992. </year>
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs <ref> [21, 64, 4, 6, 52, 54] </ref> have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when it has to decide on control issues such as <p> known reverse with accumulating parameter where a list type check on the accumulator has been added. (1) rev ([]; Acc; Acc) (2) rev ([H jT ]; Acc; Res) ls (Acc); rev (T; [HjAcc]; Res) (3) ls ([]) As can be noticed in Figure 4, (determinate [21, 18, 41] and well-founded <ref> [6, 53, 52] </ref>, among others) unfolding produces an infinite number of different characteristic atoms, all with a different characteristic tree. <p> Importing and adapting m-trees from [54], we have overcome the need for a depth bound on characteristic trees to guarantee termination of partial deduction. Plugging in a depth bound 48 free local control strategy (see e.g. <ref> [6, 52] </ref>), we thus obtain a fully automatic, concrete partial deduction method that always terminates and produces precise and reasonable polyvariance, without resorting to any ad hoc techniques. To the best of our knowledge, this is the very first such method.
Reference: [7] <author> D. Chan and M. Wallace. </author> <title> A treatment of negation during partial evaluation. </title> <editor> In H. Abramson and M. Rogers, editors, </editor> <booktitle> Meta-Programming in Logic Programming, Proceedings of the Meta88 Workshop, </booktitle> <month> June </month> <year> 1988, </year> <pages> pages 299-318. </pages> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference: [8] <author> C. Consel and O. Danvy. </author> <title> Tutorial notes on partial evaluation. </title> <booktitle> In Proceedings of POPL'93, </booktitle> <address> Charleston, South Carolina, </address> <month> January </month> <year> 1993. </year> <note> ACM Press. </note>
Reference-contexts: 1 Introduction A major concern in the specialisation of functional (see e.g. <ref> [8, 31, 67] </ref>) as well as logic programs (see e.g [49, 35, 19, 57, 10]) has been the issue of control: How can the transformation process be guided in such a way that termination is guaranteed and results are satisfactory? This problem has been tackled from two (until now) largely separate <p> Partial evaluation of functional programs <ref> [8, 31] </ref> has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs [21, 64, 4, 6, 52, 54] have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior
Reference: [9] <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation and application to logic programs. </title> <journal> The Journal of Logic Programming, </journal> <volume> 13(2 </volume> & 3):103-179, 1992. 
Reference-contexts: A similar effect can be obtained, in the context of definite programs, via the trace terms of [23]. Algorithm 4.29 can also be seen as performing an abstract interpretation on an in-finite domain of infinite height (i.e. the ascending chain condition of <ref> [9] </ref> is not satisfied) and without a priori limitation of the precision (i.e., if possible, we do not perform any abstraction at all and obtain simply the concrete results).
Reference: [10] <author> D. De Schreye, M. Leuschel, and B. Martens. </author> <title> Tutorial on program specialisation (abstract). </title> <editor> In J. Lloyd, editor, </editor> <booktitle> Proceedings of ILPS'95, the International Logic Programming Symposium, </booktitle> <address> Portland, USA, December 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: 1 Introduction A major concern in the specialisation of functional (see e.g. [8, 31, 67]) as well as logic programs (see e.g <ref> [49, 35, 19, 57, 10] </ref>) has been the issue of control: How can the transformation process be guided in such a way that termination is guaranteed and results are satisfactory? This problem has been tackled from two (until now) largely separate angles: the so-called off-line versus on-line approaches.
Reference: [11] <author> D. A. de Waal. </author> <title> Analysis and Transformation of Proof Procedures. </title> <type> PhD thesis, </type> <institution> University of Bristol, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: As already mentioned earlier, the partial deduction method of [18] was extended in <ref> [11] </ref> by adorning characteristic trees with a depth-k abstraction of the corresponding 46 Mixtus paddy sp Benchmark RT Size TT RT Size TT RT Size TT advisor 0.31 809 0.85 0.31 809 0.10 0.40 463 0.29 contains.kmp 0.16 533 2.48 0.11 651 0.55 0.75 985 1.13 depth.lam 0.04 1881 4.15 0.02
Reference: [12] <author> D. A. de Waal and J. Gallagher. </author> <title> Specialisation of a unification algorithm. </title> <editor> In T. Clement and K.-K. Lau, editors, </editor> <title> Logic Program Synthesis and Transformation. </title> <booktitle> Proceedings of LOPSTR'91, </booktitle> <pages> pages 205-220, </pages> <address> Manchester, UK, </address> <year> 1991. </year>
Reference: [13] <author> D. A. de Waal and J. Gallagher. </author> <title> The applicability of logic program analysis and transformation to theorem proving. </title> <editor> In A. Bundy, editor, </editor> <booktitle> Automated Deduction| CADE-12, </booktitle> <pages> pages 207-221. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference: [14] <author> S. Debray and N.-W. Lin. </author> <title> Cost analysis of logic programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(5) </volume> <pages> 826-875, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This would probably result in a yet (slightly) more precise abstraction, causing a yet smaller global precision loss. Finally, one might also try to incorporate more detailed efficiency and cost estimations into the global control, e.g. based on <ref> [14, 15] </ref>, in order to analyse the trade-off between improved specialisation and increased polyvariance and code size. 5 Experimental Results and Discussion 5.1 Systems In this section we present an implementation of the ideas of the preceding sections, as well as an extensive set of experiments which highlight the practical benefits
Reference: [15] <author> S. Debray, P. Lopez Garca, M. Hermenegildo, and N.-W. Lin. </author> <title> Estimating the computational cost of logic programs. </title> <editor> In B. Le Charlier, editor, </editor> <booktitle> Proceedings of SAS'94, </booktitle> <volume> LNCS 864, </volume> <pages> pages 255-265, </pages> <address> Namur, Belgium, </address> <month> September </month> <year> 1994. </year> <note> Springer-Verlag. </note>
Reference-contexts: This would probably result in a yet (slightly) more precise abstraction, causing a yet smaller global precision loss. Finally, one might also try to incorporate more detailed efficiency and cost estimations into the global control, e.g. based on <ref> [14, 15] </ref>, in order to analyse the trade-off between improved specialisation and increased polyvariance and code size. 5 Experimental Results and Discussion 5.1 Systems In this section we present an implementation of the ideas of the preceding sections, as well as an extensive set of experiments which highlight the practical benefits
Reference: [16] <author> N. Dershowitz. </author> <title> Termination of rewriting. </title> <journal> Journal of Symbolic Computation, </journal> <volume> 3 </volume> <pages> 69-116, </pages> <year> 1987. </year>
Reference-contexts: We also say that V is a well-quasi order (wqo) on V . An interesting wqo is the homeomorphic embedding relation . It has been adapted from <ref> [16, 17] </ref>, where it is used in the context of term rewriting systems, for use in super-compilation in [65]. Its usefulness as a stop criterion for partial evaluation is also discussed and advocated in [50]. Some complexity results can be found in [66] (also summarised in [50]). <p> To the best of our knowledge, this is the very first such method. Along the way, we have defined generalisation and embedding on characteristic atoms, refining the homeomorphic embedding relation from <ref> [16, 17, 50, 65] </ref> into fl , and showing that the latter is more suitable in a logic programming setting. We have also touched upon a post-processing intended to sift superfluous polyvariance, possibly produced by the main algorithm.
Reference: [17] <author> N. Dershowitz and J.-P. Jouannaud. </author> <title> Rewrite systems. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <volume> Vol. B, </volume> <pages> pages 243-320. </pages> <publisher> Elsevier, MIT Press, </publisher> <year> 1990. </year> <month> 50 </month>
Reference-contexts: We also say that V is a well-quasi order (wqo) on V . An interesting wqo is the homeomorphic embedding relation . It has been adapted from <ref> [16, 17] </ref>, where it is used in the context of term rewriting systems, for use in super-compilation in [65]. Its usefulness as a stop criterion for partial evaluation is also discussed and advocated in [50]. Some complexity results can be found in [66] (also summarised in [50]). <p> Proposition 4.17 The relation is a wqo on the set of expressions over a finite alphabet. Proof (Proofs similar to this one are standard in the literature. We include it for completeness.) We first need the following concept from <ref> [17] </ref>. Let be a relation on a set S of functors (of arity 0). <p> containing the (infinite) set of variables V and the (finite) set of functors and predicates F , as the least relation satisfying: * x y if x 2 V ^ y 2 V This relation is a wqo on S (because F is finite) and hence by Kruskal's theorem (see <ref> [17] </ref>), its embedding extension to terms, emb , which is by definition identical to , is a wqo on the set of expressions. 2 The intuition behind Definition 4.15 is that when some structure re-appears within a larger one, it is homeomorphically embedded by the latter. <p> Let us extend fl to F by (only) adding that ca fl ca. fl is still a wqo on F , and hence, by Kruskal's theorem (see e.g. <ref> [17] </ref>), its embedding extension (see proof of Proposition 4.17) to terms constructed from F is also a wqo. Let us also restrict ourselves to terms ca (A; T ) constructed by using this functor exactly once such that A is an atom and T the representation of some characteristic tree. <p> To the best of our knowledge, this is the very first such method. Along the way, we have defined generalisation and embedding on characteristic atoms, refining the homeomorphic embedding relation from <ref> [16, 17, 50, 65] </ref> into fl , and showing that the latter is more suitable in a logic programming setting. We have also touched upon a post-processing intended to sift superfluous polyvariance, possibly produced by the main algorithm.
Reference: [18] <author> J. Gallagher. </author> <title> A system for specialising logic programs. </title> <type> Technical Report TR-91-32, </type> <institution> University of Bristol, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: So, in this paper, it is to the latter, global, level that we turn our attention. The concept of characteristic trees has been advocated as a suitable and refined basis for the global control of partial deduction by Gallagher and Bruynooghe in <ref> [21, 18] </ref>. The main idea is, instead of using the syntactic structure to decide upon polyvariance, to examine the specialisation behaviour of the atoms to be specialised: only if this behaviour is sufficiently different from one atom to the other should different specialised versions be generated. <p> An abstraction operator is an operator which maps every finite set of atoms to a finite abstraction of it. The following generic scheme, based on a similar one in <ref> [18, 19] </ref>, describes the basic layout of practically all algorithms for controlling partial deduction. <p> So an abstraction operator should focus on the "essential" structure of an SLDNF-tree and for instance disregard the particular substitutions and goals within the tree. The following two definitions, adapted from <ref> [18] </ref>, do just that: they characterise the essential structure of SLDNF-derivations and trees. Definition 2.6 Let G 0 be a goal and let P be a normal program whose clauses are numbered. <p> A 2 without loosing any specialisation. @ @R @ @R ? member (a; [a]) 2 member (a; []) (2)(1) fail member (a; [a; b]) 2 member (a; [b]) member (a; []) (2) fail In summary, characteristic trees seem to be an almost ideal vehicle for a refined control of polyvariance <ref> [21, 18] </ref>, a fact we will try to exploit in the following. 2.4 An Abstraction Operator using Characteristic Trees The following definition captures a first attempt at using characteristic trees for the control of polyvariance. <p> As shown in [41], these losses of precision can also lead to non-termination of the partial deduction process, further illustrating the importance of preserving characteristic trees upon generalisation. Also note that the abstraction operators presented in [21] and <ref> [18] </ref> basically behave like the operator chabs P;U and encounter exactly the same problems (see [41] for more details). <p> We will prove correctness as well as termination of this algorithm, given certain conditions. As in <ref> [18, 19] </ref>, we first define an abstraction operator. One can notice that, by definition, the abstraction operator preserves the characteristic trees. Definition 3.25 Let A be a set of characteristic atoms. <p> Now, the algorithm of the previous section, as well as all earlier approaches based on characteristic trees <ref> [21, 18, 41] </ref>, achieves the mentioned finiteness condition at the cost of imposing an ad hoc (typically very large) depth bound on characteristic trees. However, for a fairly large class of realistic programs (and unfolding rules), the number of different characteristic trees generated, is not naturally 25 bounded. <p> The following is the well known reverse with accumulating parameter where a list type check on the accumulator has been added. (1) rev ([]; Acc; Acc) (2) rev ([H jT ]; Acc; Res) ls (Acc); rev (T; [HjAcc]; Res) (3) ls ([]) As can be noticed in Figure 4, (determinate <ref> [21, 18, 41] </ref> and well-founded [6, 53, 52], among others) unfolding produces an infinite number of different characteristic atoms, all with a different characteristic tree. <p> But one can also simulate a (global tree oriented) version of Algorithm 3.27 using depth bounds to ensure termination. All unfolding rules of ecce were complemented by simple more specific resolution steps in the style of sp <ref> [18] </ref>. Constructive negation (see [7],[26]) has not yet been incorporated, but the selection of ground negative literals is allowed. Post-processing removal of unnecessary polyvariance, using the algorithm outlined in Section 4.6, determinate post-unfolding as well as redundant argument filtering (see [47]) were enabled throughout the experiments discussed below. <p> The settings ecce-d and ecce-x use Algorithm 4.29, with a different unfolding rule, while ecce-x-10 uses a (global tree oriented) version of Algorithm 3.27 with a depth bound of 10 to ensure termination. We also compare with mixtus [64, 63], paddy [59] and sp <ref> [18, 19] </ref>, of which the following versions have been used: mixtus 0.3.3, the version of paddy delivered with eclipse 3.5.1 and a version of sp dating from September 25 th , 1995. <p> The strategy is refined with a so-called "look-ahead" to detect failure at a deeper level (see e.g. <ref> [19, 18] </ref>). This approach is used by ecce-d and sp. 11 5.2 Experiments The benchmark programs are taken from [40], short descriptions can be found in Appendix A. <p> As already mentioned earlier, the partial deduction method of <ref> [18] </ref> was extended in [11] by adorning characteristic trees with a depth-k abstraction of the corresponding 46 Mixtus paddy sp Benchmark RT Size TT RT Size TT RT Size TT advisor 0.31 809 0.85 0.31 809 0.10 0.40 463 0.29 contains.kmp 0.16 533 2.48 0.11 651 0.55 0.75 985 1.13 depth.lam
Reference: [19] <author> J. Gallagher. </author> <title> Tutorial on specialisation of logic programs. </title> <booktitle> In Proceedings of PEPM'93, the ACM Sigplan Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 88-98. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction A major concern in the specialisation of functional (see e.g. [8, 31, 67]) as well as logic programs (see e.g <ref> [49, 35, 19, 57, 10] </ref>) has been the issue of control: How can the transformation process be guided in such a way that termination is guaranteed and results are satisfactory? This problem has been tackled from two (until now) largely separate angles: the so-called off-line versus on-line approaches. <p> In partial deduction of logic programs, one distinguishes two levels of control <ref> [19, 54] </ref>: the local and the global level. In a nutshell, the local level decides on how SLD (NF)- trees for individual atoms should be built. The (branches of the) resulting trees allow to construct specialised clauses for the given atoms [49, 3]. <p> Gallagher in <ref> [19] </ref> writes that providing adequate global control seems much harder than handling the local level. So, in this paper, it is to the latter, global, level that we turn our attention. <p> An abstraction operator is an operator which maps every finite set of atoms to a finite abstraction of it. The following generic scheme, based on a similar one in <ref> [18, 19] </ref>, describes the basic layout of practically all algorithms for controlling partial deduction. <p> We will prove correctness as well as termination of this algorithm, given certain conditions. As in <ref> [18, 19] </ref>, we first define an abstraction operator. One can notice that, by definition, the abstraction operator preserves the characteristic trees. Definition 3.25 Let A be a set of characteristic atoms. <p> As for Algorithm 3.27, we need the notation chatom (A; P; U ) (see Definition 3.26). Also, without loss of generality, we suppose the initial goal to consist of a single atom. As in e.g. <ref> [19, 54] </ref> (but unlike Algorithm 3.27), Algorithm 4.29 does not output a spe-cialised program, but rather a set of (characteristic) atoms from which the actual code can be generated in a straightforward way. Most of the algorithm is self-explanatory, except perhaps the For-loop. <p> The settings ecce-d and ecce-x use Algorithm 4.29, with a different unfolding rule, while ecce-x-10 uses a (global tree oriented) version of Algorithm 3.27 with a depth bound of 10 to ensure termination. We also compare with mixtus [64, 63], paddy [59] and sp <ref> [18, 19] </ref>, of which the following versions have been used: mixtus 0.3.3, the version of paddy delivered with eclipse 3.5.1 and a version of sp dating from September 25 th , 1995. <p> The strategy is refined with a so-called "look-ahead" to detect failure at a deeper level (see e.g. <ref> [19, 18] </ref>). This approach is used by ecce-d and sp. 11 5.2 Experiments The benchmark programs are taken from [40], short descriptions can be found in Appendix A.
Reference: [20] <author> J. Gallagher and M. Bruynooghe. </author> <title> Some low-level transformations for logic programs. </title> <editor> In M. Bruynooghe, editor, </editor> <booktitle> Proceedings of Meta90 Workshop on Meta Programming in Logic, </booktitle> <pages> pages 229-244, </pages> <address> Leuven, Belgium, </address> <year> 1990. </year>
Reference-contexts: Independence requires that no two atoms in A have a common instance. This ensures that no two specialised predicate definitions match the same (run-time) call. Usually this condition is satisfied by performing a renaming of the atoms in A, see <ref> [20, 2] </ref>. Closedness (as well as the related notion of coveredness in [3]) basically requires that every atom in the body of a resultant is matched by a specialised 2 To avoid the problematic resultant A A. 4 predicate definition. <p> Trivially the query new (X 1 ; : : : ; X k ) and G are equivalent wrt c.a.s. and finite failure (see also Lemma 2.2 in <ref> [20] </ref>). * Theorem 4.11 of [2] requires that G contains no variables or predicates in A. The requirement about the variables is not necessary in our case because we do not base our renaming on the mgu. <p> Lemma 2.2 in <ref> [20] </ref>). <p> We now define the function h : Expr !IN by h (t) = s (t) v (t). The well-founded measure function h has the property that h (t) 0 and h (t) &gt; 0 for any non-variable t. The following important lemma is proven for h (:) in <ref> [20] </ref> (see also [54]). Lemma 3.30 If A and B are expressions such that B is strictly more general than A, then h (A) &gt; h (B). 23 It follows that, for every expression A, there are no infinite chains of strictly more general expressions.
Reference: [21] <author> J. Gallagher and M. Bruynooghe. </author> <title> The derivation of an algorithm for program spe-cialisation. </title> <journal> New Generation Computing, </journal> <volume> 9(3 </volume> & 4):305-333, 1991. 
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs <ref> [21, 64, 4, 6, 52, 54] </ref> have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when it has to decide on control issues such as <p> So, in this paper, it is to the latter, global, level that we turn our attention. The concept of characteristic trees has been advocated as a suitable and refined basis for the global control of partial deduction by Gallagher and Bruynooghe in <ref> [21, 18] </ref>. The main idea is, instead of using the syntactic structure to decide upon polyvariance, to examine the specialisation behaviour of the atoms to be specialised: only if this behaviour is sufficiently different from one atom to the other should different specialised versions be generated. <p> A 2 without loosing any specialisation. @ @R @ @R ? member (a; [a]) 2 member (a; []) (2)(1) fail member (a; [a; b]) 2 member (a; [b]) member (a; []) (2) fail In summary, characteristic trees seem to be an almost ideal vehicle for a refined control of polyvariance <ref> [21, 18] </ref>, a fact we will try to exploit in the following. 2.4 An Abstraction Operator using Characteristic Trees The following definition captures a first attempt at using characteristic trees for the control of polyvariance. <p> As shown in [41], these losses of precision can also lead to non-termination of the partial deduction process, further illustrating the importance of preserving characteristic trees upon generalisation. Also note that the abstraction operators presented in <ref> [21] </ref> and [18] basically behave like the operator chabs P;U and encounter exactly the same problems (see [41] for more details). <p> The following well-founded measure function is taken from <ref> [21] </ref> (also in the extended version of [54]): Definition 3.29 Let Expr denote the sets of expressions. <p> Now, the algorithm of the previous section, as well as all earlier approaches based on characteristic trees <ref> [21, 18, 41] </ref>, achieves the mentioned finiteness condition at the cost of imposing an ad hoc (typically very large) depth bound on characteristic trees. However, for a fairly large class of realistic programs (and unfolding rules), the number of different characteristic trees generated, is not naturally 25 bounded. <p> The following is the well known reverse with accumulating parameter where a list type check on the accumulator has been added. (1) rev ([]; Acc; Acc) (2) rev ([H jT ]; Acc; Res) ls (Acc); rev (T; [HjAcc]; Res) (3) ls ([]) As can be noticed in Figure 4, (determinate <ref> [21, 18, 41] </ref> and well-founded [6, 53, 52], among others) unfolding produces an infinite number of different characteristic atoms, all with a different characteristic tree.
Reference: [22] <author> J. Gallagher and D. A. de Waal. </author> <title> Deletion of redundant unary type predicates from logic programs. </title> <editor> In K.-K. Lau and T. Clement, editors, </editor> <title> Logic Program Synthesis and Transformation. </title> <booktitle> Proceedings of LOPSTR'92, </booktitle> <pages> pages 151-167, </pages> <address> Manchester, UK, </address> <year> 1992. </year>
Reference-contexts: For instance, in the standard reverse with accumulating parameter, the accumulator is only copied in the end, but never influences the computation. As illustrated by Example 4.1 above, this state of affairs will often already be changed when one adds type checking in the style of <ref> [22] </ref> to even the simplest logic programs. Among larger and more sophisticated programs, cases like the above become more and more frequent, even in the absence of type checking. For instance, in an explicit unification algorithm, one accumulating parameter is the substitution built so far.
Reference: [23] <author> J. Gallagher and L. Lafave. </author> <title> Regular approximations of computation paths in logic and functional languages. </title> <editor> In O. Danvy, R. Gluck, and P. Thiemann, editors, </editor> <booktitle> Proceedings of the 1996 Dagstuhl Seminar on Partial Evaluation, </booktitle> <volume> LNCS 1110, </volume> <pages> pages 115-136, </pages> <publisher> Schlo Dagstuhl, </publisher> <year> 1996. </year>
Reference-contexts: The results and discussions of this paper remain valid independently of whether this normalisation is applied or not. A similar effect can be obtained, in the context of definite programs, via the trace terms of <ref> [23] </ref>.
Reference: [24] <author> R. Gluck. </author> <title> On the generation of specialisers. </title> <journal> Journal of Functional Programming, </journal> <volume> 4(4) </volume> <pages> 499-514, </pages> <year> 1994. </year>
Reference-contexts: It also renders specialisation more readily amenable to self-application [32, 26] and the realisation of the well-known Futamura projections. Most specialisation research in logic programming has not been concerned with anything beyond the first Futamura (or specialiser <ref> [24] </ref>) projection. Self-application consequently being less of an issue, most specialisers for logic programs focus on (fully automatic) on-line control. 1 It is within this well established on-line control tradition that the present work provides a novel and important contribution.
Reference: [25] <author> R. Gluck, J. Jtrgensen, B. Martens, and M. Strensen. </author> <title> Controlling conjunctive partial deduction of definite logic programs. </title> <editor> In H. Kuchen and S. Swierstra, editors, </editor> <booktitle> Proceedings of the International Symposium on Programming Languages, Implementations, Logics and Programs (PLILP'96), </booktitle> <volume> LNCS 1140, </volume> <pages> pages 152-166, </pages> <address> Aachen, Germany, </address> <month> September </month> <year> 1996. </year> <note> Extended version as Technical Report CW 226, K.U. Leuven. Accessible via http://www.cs.kuleuven.ac.be/~lpai. </note>
Reference-contexts: Recent work brought a closer integration of abstract interpretation and partial deduction [46], as well as an extension of partial deduction <ref> [43, 25] </ref> to incorporate more powerful unfold/fold-like transformations [57], allowing for example to eliminate unnecessary variables from programs [61]. <p> This opens up a whole range of challenging new control issues. It turned out that the global control techniques presented in the current paper can be extended and that they significantly contribute in that context too (see <ref> [43, 25, 34] </ref>). Acknowledgements We would like to thank John Gallagher and Maurice Bruynooghe for interesting remarks and for pointing out several improvements.
Reference: [26] <author> C. A. Gurr. </author> <title> A Self-Applicable Partial Evaluator for the Logic Programming Language Godel. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Bristol, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Separating out this part of control simplifies considerably the core specialisation program, thus often reducing the overall complexity of the transformation process. It also renders specialisation more readily amenable to self-application <ref> [32, 26] </ref> and the realisation of the well-known Futamura projections. Most specialisation research in logic programming has not been concerned with anything beyond the first Futamura (or specialiser [24]) projection.
Reference: [27] <author> N. Heintze. </author> <title> Practical aspects of set based analysis. </title> <booktitle> In Proceedings of the Joint International Conference and Symposium on Logic Programming, </booktitle> <pages> pages 765-779, </pages> <address> Washington D.C., 1992. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Very few abstract interpretations of logic programs use infinite domains of infinite height (some notable exceptions are <ref> [5, 30, 27] </ref>) and to our knowledge all of them have some a priori limitation of the precision, at least in practice.
Reference: [28] <author> P. Hill and J. Gallagher. </author> <title> Meta-programming in logic programming. </title> <type> Technical Report 94.22, </type> <institution> School of Computer Studies, University of Leeds, </institution> <year> 1994. </year> <booktitle> To be published in Handbook of Logic in Artificial Intelligence and Logic Programming, </booktitle> <volume> Vol. 5. </volume> <publisher> Oxford Science Publications, Oxford University Press. </publisher>
Reference-contexts: For instance, in an explicit unification algorithm, one accumulating parameter is the substitution built so far. It heavily influences the computation because new bindings have to be added and checked for compatibility with the current substitution. Another example is the "mixed" meta-interpreter of <ref> [28, 42] </ref> (sometimes called I nstanceDemo; part of it is depicted in Figure 5) for the ground representation in which the goals are "lifted" to the non-ground representation 26 @ ? 2 rev (L; []; R) rev (T; [H]; R) @ ? (1) (2) ls ([H]); rev (T 0 ; [H
Reference: [29] <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: In the proof of this lemma we need to combine characteristic paths. A characteristic path being a sequence, we can simply concatenate two characteristic paths ffi 1 ; ffi 2 . For this we will use the standard notation ffi 1 ffi 2 from formal language theory <ref> [1, 29] </ref>. Lemma 3.23 Let P be a program, (A; t A ) and (A; t fl A ) safe P -characteristic atoms with t A t fl A . <p> We then try to collapse nodes with identical characteristic trees using the well-known algorithm for minimisation of finite state automata <ref> [1, 29] </ref>: we start by putting all characteristic atoms with the same characteristic tree into the same class, and subsequently split these classes if corresponding body atoms fall into different classes. <p> As stated in <ref> [29] </ref>, the complexity of this algorithm is O (kn 2 ) where n is the maximum number of states (in our case the number of characteristic atoms) and k the number of symbols (in our case the maximum number of body atoms). <p> Adapting the algorithm from <ref> [1, 29] </ref> to our needs, we start out by generating three classes (one for each characteristic tree) of states: * C 1 = f (member (a; [a; b; c; djT ]); t); (member (a; L); t ); (member (b; L); t )g, * C 2 = f (member (a; [b; c;
Reference: [30] <author> G. Janssens and M. Bruynooghe. </author> <title> Deriving descriptions of possible values of program variables by means of abstract interpretation. </title> <journal> The Journal of Logic Programming, </journal> <volume> 13(2 </volume> & 3):205-258, 1992. 
Reference-contexts: Very few abstract interpretations of logic programs use infinite domains of infinite height (some notable exceptions are <ref> [5, 30, 27] </ref>) and to our knowledge all of them have some a priori limitation of the precision, at least in practice.
Reference: [31] <author> N. D. Jones, C. K. Gomard, and P. Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction A major concern in the specialisation of functional (see e.g. <ref> [8, 31, 67] </ref>) as well as logic programs (see e.g [49, 35, 19, 57, 10]) has been the issue of control: How can the transformation process be guided in such a way that termination is guaranteed and results are satisfactory? This problem has been tackled from two (until now) largely separate <p> Partial evaluation of functional programs <ref> [8, 31] </ref> has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs [21, 64, 4, 6, 52, 54] have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior
Reference: [32] <author> N. D. Jones, P. Sestoft, and H. Stndergaard. </author> <title> Mix: a self-applicable partial evaluator for experiments in compiler generation. </title> <journal> LISP and Symbolic Computation, </journal> <volume> 2(1) </volume> <pages> 9-50, </pages> <year> 1989. </year>
Reference-contexts: Separating out this part of control simplifies considerably the core specialisation program, thus often reducing the overall complexity of the transformation process. It also renders specialisation more readily amenable to self-application <ref> [32, 26] </ref> and the realisation of the well-known Futamura projections. Most specialisation research in logic programming has not been concerned with anything beyond the first Futamura (or specialiser [24]) projection.
Reference: [33] <author> J. Jtrgensen and M. Leuschel. </author> <title> Efficiently generating efficient generating extensions in Prolog. </title> <editor> In O. Danvy, R. Gluck, and P. Thiemann, editors, </editor> <booktitle> Proceedings of the 1996 Dagstuhl Seminar on Partial Evaluation, </booktitle> <volume> LNCS 1110, </volume> <pages> pages 238-262, </pages> <publisher> Schlo Dagstuhl, </publisher> <year> 1996. </year> <note> Extended version as Technical Report CW 221, K.U. Leuven. Accessible via http://www.cs.kuleuven.ac.be/~lpai. </note>
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs [21, 64, 4, 6, 52, 54] have concentrated on on-line control. (Some exceptions are <ref> [69, 56, 38, 33] </ref>.) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when it has to decide on control issues such as structure propagation, termination and the right amount of polyvariance.
Reference: [34] <author> J. Jtrgensen, M. Leuschel, and B. Martens. </author> <title> Conjunctive partial deduction in practice. </title> <editor> In J. Gallager, editor, </editor> <booktitle> Pre-Proceedings of the International Workshop on Logic Program Synthesis and Transformation (LOPSTR'96), </booktitle> <pages> pages 46-62, </pages> <address> Stockholm, Swe-den, </address> <month> August </month> <year> 1996. </year> <note> Also in the Proceedings of BENELOG'96. Extended version as Technical Report CW 242, K.U. Leuven. </note>
Reference-contexts: This opens up a whole range of challenging new control issues. It turned out that the global control techniques presented in the current paper can be extended and that they significantly contribute in that context too (see <ref> [43, 25, 34] </ref>). Acknowledgements We would like to thank John Gallagher and Maurice Bruynooghe for interesting remarks and for pointing out several improvements.
Reference: [35] <author> J. Komorowski. </author> <title> An introduction to partial deduction. </title> <editor> In A. Pettorossi, editor, </editor> <booktitle> Proceedings Meta'92, Lecture Notes in Computer Science 649, </booktitle> <pages> pages 49-69. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction A major concern in the specialisation of functional (see e.g. [8, 31, 67]) as well as logic programs (see e.g <ref> [49, 35, 19, 57, 10] </ref>) has been the issue of control: How can the transformation process be guided in such a way that termination is guaranteed and results are satisfactory? This problem has been tackled from two (until now) largely separate angles: the so-called off-line versus on-line approaches. <p> The partial input, that a partial evaluator is given, then takes the form of a partially instantiated goal G 0 . One often uses the term partial deduction to refer to partial evaluation of pure logic programs (see <ref> [35] </ref>), a convention we will adhere to in this paper.
Reference: [36] <author> J. Lam and A. Kusalik. </author> <title> A comparative analysis of partial deductors for pure Prolog. </title> <type> Technical report, </type> <institution> Department of Computational Science, University of Saskatchewan, Canada, </institution> <month> May </month> <year> 1990. </year> <month> Revised April </month> <year> 1991. </year>
Reference-contexts: This approach is used by ecce-d and sp. 11 5.2 Experiments The benchmark programs are taken from [40], short descriptions can be found in Appendix A. In addition to the "Lam & Kusalik" benchmarks (originally in <ref> [36] </ref>) they contain a whole set of more involved and practical examples, like e.g. a model-elimination theorem prover and a meta-interpreter for an imperative language. For the experimentation, we tried to be as realistic as possible and measure what a normal user sees.
Reference: [37] <author> J.-L. Lassez, M. Maher, and K. Marriott. </author> <title> Unification revisited. </title> <editor> In J. Minker, editor, </editor> <booktitle> Foundations of Deductive Databases and Logic Programming, </booktitle> <pages> pages 587-625. </pages> <publisher> Morgan-Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: This is generally not such a good idea. Indeed, two atoms can be unfolded and specialised in a very similar way in the context 3 Also known as anti-unification or least general generalisation, see for instance <ref> [37] </ref>. 5 of one program P 1 , while in the context of another program P 2 their specialisation be--haviour can be drastically different.
Reference: [38] <author> M. Leuschel. </author> <title> Partial evaluation of the "real thing". </title> <editor> In L. Fribourg and F. Turini, editors, </editor> <title> Logic Program Synthesis and Transformation | Meta-Programming in Logic. </title> <booktitle> Proceedings of LOPSTR'94 and META'94, Lecture Notes in Computer Science 883, </booktitle> <pages> pages 122-137, </pages> <address> Pisa, Italy, June 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs [21, 64, 4, 6, 52, 54] have concentrated on on-line control. (Some exceptions are <ref> [69, 56, 38, 33] </ref>.) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when it has to decide on control issues such as structure propagation, termination and the right amount of polyvariance. <p> With that, the concretisation definition for characteristic atoms scales up and the technique will ensure correct specialisation. It should also be possible to incorporate the if-then-else into characteristic trees (and then use a specialisation technique similar to <ref> [38] </ref>). Also, the embedding relation of Definition 4.15 (and the relations fl and fl ca based on it) has to be adapted. Indeed, some built-ins (like = ::=2 or is=2) can be used to dynamically construct infinitely many new constants and functors and thus is no longer a wqo.
Reference: [39] <author> M. Leuschel. </author> <title> Ecological partial deduction: Preserving characteristic trees without constraints. </title> <editor> In M. Proietti, editor, </editor> <title> Logic Program Synthesis and Transformation. </title> <booktitle> Proceedings of LOPSTR'95, Lecture Notes in Computer Science 1048, </booktitle> <pages> pages 1-16, </pages> <address> Utrecht, The Netherlands, </address> <month> September </month> <year> 1995. </year> <note> Springer-Verlag. </note>
Reference-contexts: Section 6 subsequently concludes the paper. Finally, we would like to mention that part of the material in this paper was presented in a somewhat preliminary form in two earlier workshop papers: * <ref> [39] </ref> describes how to impose characteristic trees and perform set-based partial de duction with characteristic atoms, * while [45] elaborates the former approach into one using global trees, and thus not requiring any depth bound. 2 Preliminaries and Motivations Throughout this paper, we suppose familiarity with basic notions in logic programming <p> Finally, termination is guaranteed by Proposition 3.32, given that the number of distinct characteristic trees is finite. 2 The method for partial deduction as described in this section, using the framework of Section 3, has been called ecological partial deduction in <ref> [39] </ref> because it guarantees the preservation of characteristic trees. A prototype partial deduction system, using Algorithm 3.27, has been implemented and experiments with it can be found in [39] and [55]. We will however further improve the algorithm in the next section and present extensive benchmarks in Section 5.2. <p> method for partial deduction as described in this section, using the framework of Section 3, has been called ecological partial deduction in <ref> [39] </ref> because it guarantees the preservation of characteristic trees. A prototype partial deduction system, using Algorithm 3.27, has been implemented and experiments with it can be found in [39] and [55]. We will however further improve the algorithm in the next section and present extensive benchmarks in Section 5.2. Let us conclude this section with some comments related to [41], which also solves the problem of preserving characteristic trees upon generalisation.
Reference: [40] <author> M. Leuschel. </author> <title> The ecce partial deduction system and the dppd library of benchmarks. </title> <note> Obtainable via http://www.cs.kuleuven.ac.be/~lpai, 1996. </note>
Reference-contexts: The system which integrates the ideas of this paper, called ecce, is publicly available in <ref> [40] </ref> and is actually an implementation of a generic version of Algorithm 4.29 which allows the advanced user to change and even implement e.g. the unfolding rule as well as the abstraction operator and non-termination detection method. <p> The strategy is refined with a so-called "look-ahead" to detect failure at a deeper level (see e.g. [19, 18]). This approach is used by ecce-d and sp. 11 5.2 Experiments The benchmark programs are taken from <ref> [40] </ref>, short descriptions can be found in Appendix A. In addition to the "Lam & Kusalik" benchmarks (originally in [36]) they contain a whole set of more involved and practical examples, like e.g. a model-elimination theorem prover and a meta-interpreter for an imperative language.
Reference: [41] <author> M. Leuschel and D. De Schreye. </author> <title> An almost perfect abstraction operation for partial deduction using characteristic trees. </title> <type> Technical Report CW 215, </type> <institution> Departement Computerwetenschappen, K.U. Leuven, Belgium, </institution> <month> October </month> <year> 1995. </year> <note> Submitted for Publication. Accessible via http://www.cs.kuleuven.ac.be/~lpai. 52 </note>
Reference-contexts: Fortunately, in the context of finite, non-failing derivations of atomic goals, the information about whether the derivation associated with a characteristic path is incomplete or successful is already implicitly present and no further precision would be gained by adding it. This is proven in <ref> [41] </ref>. Also, once the top-level goal is known, the characteristic path is sufficient to reconstruct all the intermediate goals as well as the final one. Now that we have characterised derivations, we can characterise goals by characteris-ing the derivations of their associated finite SLDNF-trees. <p> Note that a "perfect" program for member (a; [a; b]); member (a; [a]) would just consist of clause (1'). As shown in <ref> [41] </ref>, these losses of precision can also lead to non-termination of the partial deduction process, further illustrating the importance of preserving characteristic trees upon generalisation. Also note that the abstraction operators presented in [21] and [18] basically behave like the operator chabs P;U and encounter exactly the same problems (see [41] <p> <ref> [41] </ref>, these losses of precision can also lead to non-termination of the partial deduction process, further illustrating the importance of preserving characteristic trees upon generalisation. Also note that the abstraction operators presented in [21] and [18] basically behave like the operator chabs P;U and encounter exactly the same problems (see [41] for more details). If however we manage to keep precision, we will obtain an abstraction operator for partial deduction with optimal local precision (in the sense that all the local specialisation achieved by the unfolding rule is preserved by the abstraction) and which guarantees termination. <p> We will however further improve the algorithm in the next section and present extensive benchmarks in Section 5.2. Let us conclude this section with some comments related to <ref> [41] </ref>, which also solves the problem of preserving characteristic trees upon generalisation. In fact, [41] achieves this by incorporating disequality constraints into the partial deduction process. <p> We will however further improve the algorithm in the next section and present extensive benchmarks in Section 5.2. Let us conclude this section with some comments related to <ref> [41] </ref>, which also solves the problem of preserving characteristic trees upon generalisation. In fact, [41] achieves this by incorporating disequality constraints into the partial deduction process. Note that in this section and paper, the characteristic tree t inside a characteristic atom (A; t ) can also be seen as an implicit representation of constraints on A. <p> Note that in this section and paper, the characteristic tree t inside a characteristic atom (A; t ) can also be seen as an implicit representation of constraints on A. However, here these constraints are used only locally and are not propagated towards other characteristic atoms, while in <ref> [41] </ref> the constraints are propagated and thus used globally. Whether this has any significant influence in practice remains to be seen. On the other hand, the method in this section is conceptually simpler and can handle any unfolding rule as well as normal logic programs, while [41] is currently limited to <p> characteristic atoms, while in <ref> [41] </ref> the constraints are propagated and thus used globally. Whether this has any significant influence in practice remains to be seen. On the other hand, the method in this section is conceptually simpler and can handle any unfolding rule as well as normal logic programs, while [41] is currently limited to purely determinate unfoldings (without a lookahead) and definite programs. 4 Removing Depth Bounds by Adding Global Trees Having solved the first problem related to characteristic trees, their preservation upon generalisation, we now turn to the second problem: getting rid of the associated depth bound. 4.1 The <p> Now, the algorithm of the previous section, as well as all earlier approaches based on characteristic trees <ref> [21, 18, 41] </ref>, achieves the mentioned finiteness condition at the cost of imposing an ad hoc (typically very large) depth bound on characteristic trees. However, for a fairly large class of realistic programs (and unfolding rules), the number of different characteristic trees generated, is not naturally 25 bounded. <p> The following is the well known reverse with accumulating parameter where a list type check on the accumulator has been added. (1) rev ([]; Acc; Acc) (2) rev ([H jT ]; Acc; Res) ls (Acc); rev (T; [HjAcc]; Res) (3) ls ([]) As can be noticed in Figure 4, (determinate <ref> [21, 18, 41] </ref> and well-founded [6, 53, 52], among others) unfolding produces an infinite number of different characteristic atoms, all with a different characteristic tree. <p> One could also try to use an altogether more accurate abstraction operator than taking an msg on 41 characteristic atoms. For instance, one can endeavour to extend the constraint based abstraction operator proposed in <ref> [41] </ref> to normal programs and arbitrary unfolding rules. This would probably result in a yet (slightly) more precise abstraction, causing a yet smaller global precision loss.
Reference: [42] <author> M. Leuschel and D. De Schreye. </author> <title> Towards creating specialised integrity checks through partial evaluation of meta-interpreters. </title> <booktitle> In Proceedings of PEPM'95, the ACM Sig-plan Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 253-263, </pages> <address> La Jolla, California, June 1995. </address> <publisher> ACM Press. </publisher>
Reference-contexts: For instance, in an explicit unification algorithm, one accumulating parameter is the substitution built so far. It heavily influences the computation because new bindings have to be added and checked for compatibility with the current substitution. Another example is the "mixed" meta-interpreter of <ref> [28, 42] </ref> (sometimes called I nstanceDemo; part of it is depicted in Figure 5) for the ground representation in which the goals are "lifted" to the non-ground representation 26 @ ? 2 rev (L; []; R) rev (T; [H]; R) @ ? (1) (2) ls ([H]); rev (T 0 ; [H
Reference: [43] <author> M. Leuschel, D. De Schreye, and A. de Waal. </author> <title> A conceptual embedding of folding into partial deduction: Towards a maximal integration. </title> <editor> In M. Maher, editor, </editor> <booktitle> Proceedings of the Joint International Conference and Symposium on Logic Programming JICSLP'96, </booktitle> <pages> pages 319-332, </pages> <address> Bonn, Germany, </address> <month> September </month> <year> 1996. </year> <note> MIT Press. Extended version as Technical Report CW 225, K.U. Leuven. Accessible via http://www.cs.kuleuven.ac.be/~lpai. </note>
Reference-contexts: Recent work brought a closer integration of abstract interpretation and partial deduction [46], as well as an extension of partial deduction <ref> [43, 25] </ref> to incorporate more powerful unfold/fold-like transformations [57], allowing for example to eliminate unnecessary variables from programs [61]. <p> This opens up a whole range of challenging new control issues. It turned out that the global control techniques presented in the current paper can be extended and that they significantly contribute in that context too (see <ref> [43, 25, 34] </ref>). Acknowledgements We would like to thank John Gallagher and Maurice Bruynooghe for interesting remarks and for pointing out several improvements.
Reference: [44] <author> M. Leuschel and B. Martens. </author> <title> Partial deduction of the ground representation and its application to integrity checking. </title> <editor> In J. Lloyd, editor, </editor> <booktitle> Proceedings of ILPS'95, the International Logic Programming Symposium, </booktitle> <pages> pages 495-509, </pages> <address> Portland, USA, </address> <month> December </month> <year> 1995. </year> <note> MIT Press. Extended version as Technical Report CW 210, K.U. Leuven. Accessible via http://www.cs.kuleuven.ac.be/~lpai. </note>
Reference: [45] <author> M. Leuschel and B. Martens. </author> <title> Global control for partial deduction through characteristic atoms and global trees. </title> <editor> In O. Danvy, R. Gluck, and P. Thiemann, editors, </editor> <booktitle> Proceedings of the 1996 Dagstuhl Seminar on Partial Evaluation, </booktitle> <volume> LNCS 1110, </volume> <pages> pages 263-283, </pages> <publisher> Schlo Dagstuhl, </publisher> <year> 1996. </year> <note> Extended version as Technical Report CW 220, K.U. Leuven. Accessible via http://www.cs.kuleuven.ac.be/~lpai. </note>
Reference-contexts: Finally, we would like to mention that part of the material in this paper was presented in a somewhat preliminary form in two earlier workshop papers: * [39] describes how to impose characteristic trees and perform set-based partial de duction with characteristic atoms, * while <ref> [45] </ref> elaborates the former approach into one using global trees, and thus not requiring any depth bound. 2 Preliminaries and Motivations Throughout this paper, we suppose familiarity with basic notions in logic programming [48] and partial deduction [49]. Notational conventions are standard and self-evident. <p> Indeed, although by generalising CA B into M we are actually sure that M will not embed any characteristic atom in H (for a proof of this property see <ref> [45] </ref>) it might embed other characteristic atoms in the tree. Take for example a branch in a global tree which contains the characteristic atoms (p (f (a)); t ) and (p (X ); t ).
Reference: [46] <author> M. Leuschel and D. Schreye. </author> <title> Logic program specialisation: How to be more specific. </title> <editor> In H. Kuchen and S. Swierstra, editors, </editor> <booktitle> Proceedings of the International Symposium on Programming Languages, Implementations, Logics and Programs (PLILP'96), </booktitle> <volume> LNCS 1140, </volume> <pages> pages 137-151, </pages> <address> Aachen, Germany, </address> <month> September </month> <year> 1996. </year> <note> Extended version as Technical Report CW 232, K.U. Leuven. Accessible via http://www.cs.kuleuven.ac.be/~lpai. </note>
Reference-contexts: At the local control level, a number of issues are still open: fully automatic satisfactory unfolding of meta-interpreters and a good treatment of truly non-determinate programs are among the most pressing. Recent work brought a closer integration of abstract interpretation and partial deduction <ref> [46] </ref>, as well as an extension of partial deduction [43, 25] to incorporate more powerful unfold/fold-like transformations [57], allowing for example to eliminate unnecessary variables from programs [61].
Reference: [47] <author> M. Leuschel and M. H. Strensen. </author> <title> Redundant argument filtering of logic programs. </title> <editor> In J. Gallager, editor, </editor> <booktitle> Pre-Proceedings of the International Workshop on Logic Program Synthesis and Transformation (LOPSTR'96), </booktitle> <pages> pages 63-77, </pages> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1996. </year> <note> Extended version as Technical Report CW 243, K.U. Leuven. </note>
Reference-contexts: Also, a goal G is covered by A in P iff every atom A occurring in G is a concretisation of a characteristic atom in A. 5 The FAR filtering algorithm of <ref> [47] </ref> can be used to further improve the specialised program by removing the redundant argument of m 1 . 14 The main correctness result for partial deduction with characteristic atoms is as fol-lows: Theorem 3.16 Let P be a normal program, G a goal, A any finite set of P -characteristic <p> Constructive negation (see [7],[26]) has not yet been incorporated, but the selection of ground negative literals is allowed. Post-processing removal of unnecessary polyvariance, using the algorithm outlined in Section 4.6, determinate post-unfolding as well as redundant argument filtering (see <ref> [47] </ref>) were enabled throughout the experiments discussed below. The ecce system also handles a lot of Prolog built-ins, like for instance =, is, &lt;, =&lt;, &lt;, &gt;=, nonvar, ground, number, atomic, call, n==, n=.
Reference: [48] <author> J. Lloyd. </author> <title> Foundations of Logic Programming. </title> <publisher> Springer Verlag, </publisher> <year> 1987. </year>
Reference-contexts: describes how to impose characteristic trees and perform set-based partial de duction with characteristic atoms, * while [45] elaborates the former approach into one using global trees, and thus not requiring any depth bound. 2 Preliminaries and Motivations Throughout this paper, we suppose familiarity with basic notions in logic programming <ref> [48] </ref> and partial deduction [49]. Notational conventions are standard and self-evident. In particular, in programs, we denote variables through strings starting with (or usually just consisting of ) an upper-case symbol, while the notations of constants, functions and predicates begin with a lower-case character.
Reference: [49] <author> J. W. Lloyd and J. C. Shepherdson. </author> <title> Partial evaluation in logic programming. </title> <journal> The Journal of Logic Programming, </journal> <volume> 11(3& </volume> 4):217-242, 1991. 
Reference-contexts: 1 Introduction A major concern in the specialisation of functional (see e.g. [8, 31, 67]) as well as logic programs (see e.g <ref> [49, 35, 19, 57, 10] </ref>) has been the issue of control: How can the transformation process be guided in such a way that termination is guaranteed and results are satisfactory? This problem has been tackled from two (until now) largely separate angles: the so-called off-line versus on-line approaches. <p> In a nutshell, the local level decides on how SLD (NF)- trees for individual atoms should be built. The (branches of the) resulting trees allow to construct specialised clauses for the given atoms <ref> [49, 3] </ref>. <p> The (branches of the) resulting trees allow to construct specialised clauses for the given atoms [49, 3]. At the global level on the other hand, one typically attends to the overall correctness of the resulting program (satisfying the closedness condition in <ref> [49] </ref>) and strives to achieve the "right" amount of polyvariance, producing sufficiently many (but not too much) specialised versions for each predicate definition in the original program. <p> characteristic trees and perform set-based partial de duction with characteristic atoms, * while [45] elaborates the former approach into one using global trees, and thus not requiring any depth bound. 2 Preliminaries and Motivations Throughout this paper, we suppose familiarity with basic notions in logic programming [48] and partial deduction <ref> [49] </ref>. Notational conventions are standard and self-evident. In particular, in programs, we denote variables through strings starting with (or usually just consisting of ) an upper-case symbol, while the notations of constants, functions and predicates begin with a lower-case character. <p> Then the set of resultants resultants (t ) is defined to be fA 1 G 1 ; : : : ; A n G n g. Partial deduction, as defined e.g. in <ref> [49, 3] </ref>, uses the resultants for a given set of atoms A to construct the specialised program and for each atom in A a different spe-cialised predicate definition is generated, replacing the original definition in P . <p> Partial deduction, as defined e.g. in [49, 3], uses the resultants for a given set of atoms A to construct the specialised program and for each atom in A a different spe-cialised predicate definition is generated, replacing the original definition in P . Under the conditions stated in <ref> [49] </ref>, namely closedness and independence, correctness of the spe-cialised program is guaranteed. Independence requires that no two atoms in A have a common instance. This ensures that no two specialised predicate definitions match the same (run-time) call. <p> This is actually what a lot of practical partial evaluators for functional or logic programming languages do, but is unlike e.g. the definitions in <ref> [49] </ref>. <p> then applying an additional one step post-processing renaming 00 , with ff = 0 00 , along with an extra application of Theorem 4.11 of [2]. 2 3.3.2 Correctness for Safe Characteristic Atoms To prove the correctness for safe characteristic atoms we need the following adaptation of Lemma 4.12 from <ref> [49] </ref> (we just formalise the use in [49] of "corresponding SLDNF-derivation" in terms of characteristic paths). Lemma 3.18 Let R be the resultant of a finite (possibly incomplete) SLDNF-derivation for P [f Ag whose characteristic path is ffi. <p> renaming 00 , with ff = 0 00 , along with an extra application of Theorem 4.11 of [2]. 2 3.3.2 Correctness for Safe Characteristic Atoms To prove the correctness for safe characteristic atoms we need the following adaptation of Lemma 4.12 from <ref> [49] </ref> (we just formalise the use in [49] of "corresponding SLDNF-derivation" in terms of characteristic paths). Lemma 3.18 Let R be the resultant of a finite (possibly incomplete) SLDNF-derivation for P [f Ag whose characteristic path is ffi. <p> If ^t = ; then A 0 fails finitely. Therefore, because a finite SLDNF-derivation from A 0 to G 0 exists, we can deduce by persistence of failure (Lemma 4.10 in <ref> [49] </ref>), that G 0 must also fail finitely. If ^t 6= ; then the largest prefix ffi 0 C of ffi C , such that 9^fl:ffi 0 C ^fl 2 ^t , must exist (the smallest one being hi). <p> Finally, as ffi C is an extension of ffi 0 C (l ffi m), we know that a finite (possibly empty) SLDNF-derivation from G to G 0 exists and therefore, by persistence of failure (Lemma 4.10 in <ref> [49] </ref>), G 0 must also finitely fail. 2 We now present a correctness theorem for safe characteristic atoms.
Reference: [50] <author> R. Marlet. </author> <title> Vers une Formalisation de l' Evaluation Partielle. </title> <type> PhD thesis, </type> <institution> Universite de Nice - Sophia Antipolis, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: An interesting wqo is the homeomorphic embedding relation . It has been adapted from [16, 17], where it is used in the context of term rewriting systems, for use in super-compilation in [65]. Its usefulness as a stop criterion for partial evaluation is also discussed and advocated in <ref> [50] </ref>. Some complexity results can be found in [66] (also summarised in [50]). Recall that expressions are formulated using the alphabet A P which we implicitly assume underlying the programs and queries under consideration. <p> Its usefulness as a stop criterion for partial evaluation is also discussed and advocated in <ref> [50] </ref>. Some complexity results can be found in [66] (also summarised in [50]). Recall that expressions are formulated using the alphabet A P which we implicitly assume underlying the programs and queries under consideration. Remember that it may contain symbols occurring in no program and query but that it contains only finitely many constant, function and predicate symbols. <p> As is argued in <ref> [50] </ref> and [65], this provides a good starting point for detecting growing structures created by possibly non-terminating processes. However, as can be observed in Example 4.16, the homeomorphic embedding relation as defined in Definition 4.15 is rather crude wrt variables. <p> control and what may be termed thus in supercompilation [67, 68, 65]. (A distinction between local and global control is not yet made in supercompilation, but we feel that this situation is likely to change in the near future.) We already pointed out that the inspiration for using derives from <ref> [50] </ref> and [65]. In the latter, a generalisation strategy for positive supercompilation (no negative information propagation while driving) is proposed. It uses to compare nodes in a marked partial process tree (a notion roughly corresponding to marked or global trees in partial deduction). <p> To the best of our knowledge, this is the very first such method. Along the way, we have defined generalisation and embedding on characteristic atoms, refining the homeomorphic embedding relation from <ref> [16, 17, 50, 65] </ref> into fl , and showing that the latter is more suitable in a logic programming setting. We have also touched upon a post-processing intended to sift superfluous polyvariance, possibly produced by the main algorithm.
Reference: [51] <author> B. Martens. </author> <title> On the Semantics of Meta-Programming and the Control of Partial Deduction in Logic Programming. </title> <type> PhD thesis, </type> <institution> Departement Computerwetenschappen, K.U.Leuven, Belgium, </institution> <month> February </month> <year> 1994. </year>
Reference: [52] <author> B. Martens and D. De Schreye. </author> <title> Automatic finite unfolding using well-founded measures. </title> <journal> The Journal of Logic Programming, </journal> <volume> 28(2) </volume> <pages> 89-146, </pages> <month> August </month> <year> 1996. </year> <note> Abridged and 53 revised version of Technical Report CW180, Departement Computerwetenschappen, K.U.Leuven, October 1993, accessible via http://www.cs.kuleuven.ac.be/~lpai. </note>
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs <ref> [21, 64, 4, 6, 52, 54] </ref> have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when it has to decide on control issues such as <p> As common in partial deduction, the notion of SLDNF-trees is extended to also allow incomplete SLDNF-trees which, in addition to success and failure leaves, may also contain leaves where no literal has been selected for a further derivation step. Leaves of the latter kind will be called dangling <ref> [52] </ref>. 3 2.1 Partial Deduction In contrast to (full) evaluation, a partial evaluator is given a program P along with a part of its input, called the static input. <p> known reverse with accumulating parameter where a list type check on the accumulator has been added. (1) rev ([]; Acc; Acc) (2) rev ([H jT ]; Acc; Res) ls (Acc); rev (T; [HjAcc]; Res) (3) ls ([]) As can be noticed in Figure 4, (determinate [21, 18, 41] and well-founded <ref> [6, 53, 52] </ref>, among others) unfolding produces an infinite number of different characteristic atoms, all with a different characteristic tree. <p> It requires however a number of ad hoc settings. (In the future, we plan experiments with unfolding along the lines of <ref> [52] </ref> which is free of such elements.) For instance for ecce-x and ecce-x-10 we used the settings (see [64]) max rec = 2, max depth = 2, maxf inite = 7, maxnondeterm = 10 and only allowed non-determinate unfolding when no user predicates were to the left of the selected literal. <p> Indeed, first, ensuring termina tion through a well-quasi-ordering is structurally much more costly than the alternative of using a well-founded ordering. The latter only requires comparison with a single "an cestor" object and can be enforced without any search through "ancestor lists" (see <ref> [52] </ref>). Testing for well-quasi-ordering, however, unavoidably does entail such searching and re peated comparisons with several ancestors. Moreover, in our particular case, checking fl ca on characteristic atoms is in itself a quite costly operation, adding to the innate complexity of maintaining a well-quasi-ordering. <p> Importing and adapting m-trees from [54], we have overcome the need for a depth bound on characteristic trees to guarantee termination of partial deduction. Plugging in a depth bound 48 free local control strategy (see e.g. <ref> [6, 52] </ref>), we thus obtain a fully automatic, concrete partial deduction method that always terminates and produces precise and reasonable polyvariance, without resorting to any ad hoc techniques. To the best of our knowledge, this is the very first such method.
Reference: [53] <author> B. Martens, D. De Schreye, and T. Horvath. </author> <title> Sound and complete partial deduction with unfolding based on well-founded measures. </title> <booktitle> Theoretical Computer Science, </booktitle> <address> 122(1-2):97-117, </address> <year> 1994. </year>
Reference-contexts: known reverse with accumulating parameter where a list type check on the accumulator has been added. (1) rev ([]; Acc; Acc) (2) rev ([H jT ]; Acc; Res) ls (Acc); rev (T; [HjAcc]; Res) (3) ls ([]) As can be noticed in Figure 4, (determinate [21, 18, 41] and well-founded <ref> [6, 53, 52] </ref>, among others) unfolding produces an infinite number of different characteristic atoms, all with a different characteristic tree.
Reference: [54] <author> B. Martens and J. Gallagher. </author> <title> Ensuring global termination of partial deduction while allowing flexible polyvariance. </title> <editor> In L. Sterling, editor, </editor> <booktitle> Proceedings ICLP'95, </booktitle> <pages> pages 597-613, </pages> <address> Kanagawa, Japan, </address> <month> June </month> <year> 1995. </year> <note> MIT Press. Extended version as Technical Report CSTR-94-16, </note> <institution> University of Bristol. </institution>
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs <ref> [21, 64, 4, 6, 52, 54] </ref> have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when it has to decide on control issues such as <p> In partial deduction of logic programs, one distinguishes two levels of control <ref> [19, 54] </ref>: the local and the global level. In a nutshell, the local level decides on how SLD (NF)- trees for individual atoms should be built. The (branches of the) resulting trees allow to construct specialised clauses for the given atoms [49, 3]. <p> In Section 4 we then solve the second problem, blending (a slightly adapted version of) the general framework in <ref> [54] </ref> with the framework developed in Section 3. We thus obtain an elegant, sophisticated and precise apparatus for on-line global control of partial deduction. Extensive benchmarks, illustrating the practical benefits of this approach, as well as some discussions, can be found in Section 5, while . <p> The following well-founded measure function is taken from [21] (also in the extended version of <ref> [54] </ref>): Definition 3.29 Let Expr denote the sets of expressions. <p> The well-founded measure function h has the property that h (t) 0 and h (t) &gt; 0 for any non-variable t. The following important lemma is proven for h (:) in [20] (see also <ref> [54] </ref>). Lemma 3.30 If A and B are expressions such that B is strictly more general than A, then h (A) &gt; h (B). 23 It follows that, for every expression A, there are no infinite chains of strictly more general expressions. <p> l mng (GrT; N gT; InSub1; OutSub) Example query: make non ground (struct (f; [var (1); var (2); var (1)]); F ) ; c.a.s. fF=struct (f; [Z; V; Z])g 4.2 Partial Deduction using Global Trees A general framework for global control, not relying on any depth bounds, is proposed in <ref> [54] </ref>. Marked trees (m-trees) are introduced to register descendency relationships among atoms at the global level. These trees are subdivided into classes of nodes and associated measure functions map nodes to well-founded sets. <p> Accordingly, the global tree nodes will not be labelled by plain atoms as in <ref> [54] </ref>, but by entire characteristic atoms. The rest of this section, then, contains the formal elaboration of this new approach. <p> (A 1 ; dt 1 e) fl emb ca (A 2 ; dt 2 e) iff (A 1 ; t 1 ) fl and hence A; fl ca is well-quasi-ordered. 2 35 4.4 Global Trees and Characteristic Atoms In this subsection, we adapt and instantiate the m-tree concept presented in <ref> [54] </ref> according to our particular needs in this paper. Definition 4.25 A global tree fl P for a program P is a (finitely branching) tree where nodes can be either marked or unmarked and each node carries a label which is a P - characteristic atom. <p> As for Algorithm 3.27, we need the notation chatom (A; P; U ) (see Definition 3.26). Also, without loss of generality, we suppose the initial goal to consist of a single atom. As in e.g. <ref> [19, 54] </ref> (but unlike Algorithm 3.27), Algorithm 4.29 does not output a spe-cialised program, but rather a set of (characteristic) atoms from which the actual code can be generated in a straightforward way. Most of the algorithm is self-explanatory, except perhaps the For-loop. <p> It is our current understanding that both the addition of something similar to characteristic trees and the use of the refined fl embedding can lead to improvements of the method proposed in [65]. Finally, it is interesting to return to an observation already made in Section 4 of <ref> [54] </ref>: Neighbourhoods of order "n", forming the basis for generalisation in (full) supercompilation [68], are essentially the same as classes of atoms (or goals) with an identical depth n characteristic tree. <p> We have then developed an even more sophisticated online global control technique for partial deduction of normal logic programs. Importing and adapting m-trees from <ref> [54] </ref>, we have overcome the need for a depth bound on characteristic trees to guarantee termination of partial deduction.
Reference: [55] <author> D. Meulemans. Partiele deductie: Een substantiele vergelijkende studie. </author> <type> Master's thesis, </type> <institution> Departement Computerwetenschappen, K.U. Leuven, Belgium, </institution> <year> 1995. </year>
Reference-contexts: A prototype partial deduction system, using Algorithm 3.27, has been implemented and experiments with it can be found in [39] and <ref> [55] </ref>. We will however further improve the algorithm in the next section and present extensive benchmarks in Section 5.2. Let us conclude this section with some comments related to [41], which also solves the problem of preserving characteristic trees upon generalisation.
Reference: [56] <author> T. Mogensen and A. Bondorf. Logimix: </author> <title> A self-applicable partial evaluator for Prolog. </title> <editor> In K.-K. Lau and T. Clement, editors, </editor> <title> Logic Program Synthesis and Transformation. </title> <booktitle> Proceedings of LOPSTR'92, </booktitle> <pages> pages 214-227. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs [21, 64, 4, 6, 52, 54] have concentrated on on-line control. (Some exceptions are <ref> [69, 56, 38, 33] </ref>.) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when it has to decide on control issues such as structure propagation, termination and the right amount of polyvariance.
Reference: [57] <author> A. Pettorossi and M. Proietti. </author> <title> Transformation of logic programs: Foundations and techniques. </title> <journal> The Journal of Logic Programming, </journal> <volume> 19& 20 </volume> <pages> 261-320, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: 1 Introduction A major concern in the specialisation of functional (see e.g. [8, 31, 67]) as well as logic programs (see e.g <ref> [49, 35, 19, 57, 10] </ref>) has been the issue of control: How can the transformation process be guided in such a way that termination is guaranteed and results are satisfactory? This problem has been tackled from two (until now) largely separate angles: the so-called off-line versus on-line approaches. <p> Recent work brought a closer integration of abstract interpretation and partial deduction [46], as well as an extension of partial deduction [43, 25] to incorporate more powerful unfold/fold-like transformations <ref> [57] </ref>, allowing for example to eliminate unnecessary variables from programs [61]. The latter extension boils down to the lifting of entire goals (instead of separate atoms) to the global level, as for instance in supercompilation (where non-atomic goals translate into nested function calls).
Reference: [58] <author> D. Poole and R. Goebel. </author> <title> Gracefully adding negation and disjunction to Prolog. </title> <editor> In E. Shapiro, editor, </editor> <booktitle> Proceedings of the Third International Conference on Logic Programming, </booktitle> <pages> pages 635-641. </pages> <publisher> Springer Verlag, </publisher> <year> 1986. </year>
Reference: [59] <author> S. Prestwich. </author> <title> The PADDY partial deduction system. </title> <type> Technical Report ECRC-92-6, </type> <institution> ECRC, Munich, Germany, </institution> <year> 1992. </year>
Reference-contexts: The settings ecce-d and ecce-x use Algorithm 4.29, with a different unfolding rule, while ecce-x-10 uses a (global tree oriented) version of Algorithm 3.27 with a depth bound of 10 to ensure termination. We also compare with mixtus [64, 63], paddy <ref> [59] </ref> and sp [18, 19], of which the following versions have been used: mixtus 0.3.3, the version of paddy delivered with eclipse 3.5.1 and a version of sp dating from September 25 th , 1995. <p> For mixtus and paddy we used the respective default settings of the systems. Note that the "mixtus-like" unfolding of ecce, mixtus and paddy differ slightly from each other, probably due to some details not outlined or fully explained in <ref> [64, 59] </ref> as well as the fact that the different global control regimes influence the behaviour of the "mixtus-like" local control. * Determinate unfolding: Only (except once) select atoms that match a single clause head.
Reference: [60] <author> S. Prestwich. </author> <title> Online partial deduction of large programs. </title> <booktitle> In Proceedings of PEPM'93, the ACM Sigplan Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 111-118. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Even though ecce is still a prototype, the transformation times are reasonable and usually close to the ones of mixtus. ecce can certainly be speeded up considerably, maybe even by using the ideas in <ref> [60] </ref> which help paddy to be (except for one glitch) the fastest system overall. Note that even the system ecce-x-10 based on a depth bound of 10, outperforms the existing systems for speed of the specialised programs.
Reference: [61] <author> M. Proietti and A. Pettorossi. Unfolding-definition-folding, </author> <title> in this order, for avoiding unnecessary variables in logic programs. </title> <editor> In J. Ma luszynski and M. Wirsing, editors, </editor> <booktitle> Proceedings of the 3rd International Symposium on Programming Language Implementation and Logic Programming, </booktitle> <address> PLILP'91, </address> <publisher> LNCS 528, Springer Verlag, </publisher> <pages> pages 347-358, </pages> <year> 1991. </year>
Reference-contexts: Recent work brought a closer integration of abstract interpretation and partial deduction [46], as well as an extension of partial deduction [43, 25] to incorporate more powerful unfold/fold-like transformations [57], allowing for example to eliminate unnecessary variables from programs <ref> [61] </ref>. The latter extension boils down to the lifting of entire goals (instead of separate atoms) to the global level, as for instance in supercompilation (where non-atomic goals translate into nested function calls). This opens up a whole range of challenging new control issues.
Reference: [62] <author> G. Puebla and M. Hermenegildo. </author> <title> Implementation of multiple specialization in logic programs. </title> <booktitle> In Proceedings of PEPM'95, the ACM Sigplan Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 77-87, </pages> <address> La Jolla, Cal-ifornia, June 1995. </address> <publisher> ACM Press. </publisher>
Reference-contexts: ) #1 (member (b; T ); t ) #1 ? = #2 (member (a; [a; b; c; djT ]); t ) (member (a; [b; c; djT ]); t 0 ) #1 (member (X; T ); t ) #1 A similar use of this minimisation algorithm is made in [70] and <ref> [62] </ref>. The former aims at minimising polyvariance in the context of multiple specialisation by optimising compilers. The latter, in a somewhat different context, studies polyvariant parallelisation and specialisation of logic programs based on abstract interpretation.
Reference: [63] <author> D. Sahlin. </author> <title> An Automatic Partial Evaluator for Full Prolog. </title> <type> PhD thesis, </type> <institution> Swedish Institute of Computer Science, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: To remedy this, the constants and functors are partitioned into the static ones, occurring in the original program and the partial deduction query, and the dynamic ones. (This approach is also used in <ref> [63, 64] </ref>.) The set of dynamic constants and functors is possibly infinite, and we will therefore treat it like the infinite set of variables in Definition 4.15 by adding the following rule to the ecce system: 42 f (s 1 ; : : : ; s m ) g (t 1 <p> The settings ecce-d and ecce-x use Algorithm 4.29, with a different unfolding rule, while ecce-x-10 uses a (global tree oriented) version of Algorithm 3.27 with a depth bound of 10 to ensure termination. We also compare with mixtus <ref> [64, 63] </ref>, paddy [59] and sp [18, 19], of which the following versions have been used: mixtus 0.3.3, the version of paddy delivered with eclipse 3.5.1 and a version of sp dating from September 25 th , 1995. <p> Basically, the above mentioned systems use two different unfolding rules : * "mixtus-like " unfolding: This is the unfolding strategy explained in <ref> [64, 63] </ref> which in general unfolds deeply enough to solve the "fully unfoldable" problems 10 but also has safeguards against excessive unfolding and code explosion.
Reference: [64] <author> D. Sahlin. Mixtus: </author> <title> An automatic partial evaluator for full Prolog. </title> <journal> New Generation Computing, </journal> <volume> 12(1) </volume> <pages> 7-51, </pages> <year> 1993. </year> <month> 54 </month>
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs <ref> [21, 64, 4, 6, 52, 54] </ref> have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when it has to decide on control issues such as <p> To remedy this, the constants and functors are partitioned into the static ones, occurring in the original program and the partial deduction query, and the dynamic ones. (This approach is also used in <ref> [63, 64] </ref>.) The set of dynamic constants and functors is possibly infinite, and we will therefore treat it like the infinite set of variables in Definition 4.15 by adding the following rule to the ecce system: 42 f (s 1 ; : : : ; s m ) g (t 1 <p> The settings ecce-d and ecce-x use Algorithm 4.29, with a different unfolding rule, while ecce-x-10 uses a (global tree oriented) version of Algorithm 3.27 with a depth bound of 10 to ensure termination. We also compare with mixtus <ref> [64, 63] </ref>, paddy [59] and sp [18, 19], of which the following versions have been used: mixtus 0.3.3, the version of paddy delivered with eclipse 3.5.1 and a version of sp dating from September 25 th , 1995. <p> Basically, the above mentioned systems use two different unfolding rules : * "mixtus-like " unfolding: This is the unfolding strategy explained in <ref> [64, 63] </ref> which in general unfolds deeply enough to solve the "fully unfoldable" problems 10 but also has safeguards against excessive unfolding and code explosion. <p> It requires however a number of ad hoc settings. (In the future, we plan experiments with unfolding along the lines of [52] which is free of such elements.) For instance for ecce-x and ecce-x-10 we used the settings (see <ref> [64] </ref>) max rec = 2, max depth = 2, maxf inite = 7, maxnondeterm = 10 and only allowed non-determinate unfolding when no user predicates were to the left of the selected literal. The method ecce-x-10 was also complemented by a level 10 depth bound. <p> For mixtus and paddy we used the respective default settings of the systems. Note that the "mixtus-like" unfolding of ecce, mixtus and paddy differ slightly from each other, probably due to some details not outlined or fully explained in <ref> [64, 59] </ref> as well as the fact that the different global control regimes influence the behaviour of the "mixtus-like" local control. * Determinate unfolding: Only (except once) select atoms that match a single clause head.
Reference: [65] <author> M. Strensen and R. Gluck. </author> <title> An algorithm of generalization in positive supercompila--tion. </title> <editor> In J. Lloyd, editor, </editor> <booktitle> Proceedings of ILPS'95, the International Logic Programming Symposium, </booktitle> <pages> pages 465-479, </pages> <address> Portland, USA, December 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional <ref> [67, 68, 65] </ref> and partial deduction of logic programs [21, 64, 4, 6, 52, 54] have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when <p> We also say that V is a well-quasi order (wqo) on V . An interesting wqo is the homeomorphic embedding relation . It has been adapted from [16, 17], where it is used in the context of term rewriting systems, for use in super-compilation in <ref> [65] </ref>. Its usefulness as a stop criterion for partial evaluation is also discussed and advocated in [50]. Some complexity results can be found in [66] (also summarised in [50]). Recall that expressions are formulated using the alphabet A P which we implicitly assume underlying the programs and queries under consideration. <p> As is argued in [50] and <ref> [65] </ref>, this provides a good starting point for detecting growing structures created by possibly non-terminating processes. However, as can be observed in Example 4.16, the homeomorphic embedding relation as defined in Definition 4.15 is rather crude wrt variables. <p> Some additional discussions, further motivating the use of global trees and characteristic atoms, can be found in Appendix D. We conclude this section with a brief discussion on the relation between our global control and what may be termed thus in supercompilation <ref> [67, 68, 65] </ref>. (A distinction between local and global control is not yet made in supercompilation, but we feel that this situation is likely to change in the near future.) We already pointed out that the inspiration for using derives from [50] and [65]. <p> what may be termed thus in supercompilation [67, 68, 65]. (A distinction between local and global control is not yet made in supercompilation, but we feel that this situation is likely to change in the near future.) We already pointed out that the inspiration for using derives from [50] and <ref> [65] </ref>. In the latter, a generalisation strategy for positive supercompilation (no negative information propagation while driving) is proposed. It uses to compare nodes in a marked partial process tree (a notion roughly corresponding to marked or global trees in partial deduction). <p> These nodes, however, only contain syntactical information corresponding to ordinary atoms (or rather goals, see Section 6). It is our current understanding that both the addition of something similar to characteristic trees and the use of the refined fl embedding can lead to improvements of the method proposed in <ref> [65] </ref>. Finally, it is interesting to return to an observation already made in Section 4 of [54]: Neighbourhoods of order "n", forming the basis for generalisation in (full) supercompilation [68], are essentially the same as classes of atoms (or goals) with an identical depth n characteristic tree. <p> To the best of our knowledge, this is the very first such method. Along the way, we have defined generalisation and embedding on characteristic atoms, refining the homeomorphic embedding relation from <ref> [16, 17, 50, 65] </ref> into fl , and showing that the latter is more suitable in a logic programming setting. We have also touched upon a post-processing intended to sift superfluous polyvariance, possibly produced by the main algorithm.
Reference: [66] <author> J. Stillman. </author> <title> Computational Problems in Equational Theorem Proving. </title> <type> PhD thesis, </type> <institution> State University of New York at Albany, </institution> <year> 1988. </year>
Reference-contexts: It has been adapted from [16, 17], where it is used in the context of term rewriting systems, for use in super-compilation in [65]. Its usefulness as a stop criterion for partial evaluation is also discussed and advocated in [50]. Some complexity results can be found in <ref> [66] </ref> (also summarised in [50]). Recall that expressions are formulated using the alphabet A P which we implicitly assume underlying the programs and queries under consideration. Remember that it may contain symbols occurring in no program and query but that it contains only finitely many constant, function and predicate symbols.
Reference: [67] <author> V. Turchin. </author> <title> The concept of a supercompiler. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(3) </volume> <pages> 292-325, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction A major concern in the specialisation of functional (see e.g. <ref> [8, 31, 67] </ref>) as well as logic programs (see e.g [49, 35, 19, 57, 10]) has been the issue of control: How can the transformation process be guided in such a way that termination is guaranteed and results are satisfactory? This problem has been tackled from two (until now) largely separate <p> Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional <ref> [67, 68, 65] </ref> and partial deduction of logic programs [21, 64, 4, 6, 52, 54] have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when <p> Some additional discussions, further motivating the use of global trees and characteristic atoms, can be found in Appendix D. We conclude this section with a brief discussion on the relation between our global control and what may be termed thus in supercompilation <ref> [67, 68, 65] </ref>. (A distinction between local and global control is not yet made in supercompilation, but we feel that this situation is likely to change in the near future.) We already pointed out that the inspiration for using derives from [50] and [65].
Reference: [68] <author> V. F. Turchin. </author> <title> The algorithm of generalization in the supercompiler. </title> <editor> In D. Bjtrner, A. P. Ershov, and N. D. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 531-549. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional <ref> [67, 68, 65] </ref> and partial deduction of logic programs [21, 64, 4, 6, 52, 54] have concentrated on on-line control. (Some exceptions are [69, 56, 38, 33].) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when <p> Some additional discussions, further motivating the use of global trees and characteristic atoms, can be found in Appendix D. We conclude this section with a brief discussion on the relation between our global control and what may be termed thus in supercompilation <ref> [67, 68, 65] </ref>. (A distinction between local and global control is not yet made in supercompilation, but we feel that this situation is likely to change in the near future.) We already pointed out that the inspiration for using derives from [50] and [65]. <p> Finally, it is interesting to return to an observation already made in Section 4 of [54]: Neighbourhoods of order "n", forming the basis for generalisation in (full) supercompilation <ref> [68] </ref>, are essentially the same as classes of atoms (or goals) with an identical depth n characteristic tree.
Reference: [69] <author> D. Weise, R. Conybeare, E. Ruf, and S. Seligman. </author> <title> Automatic online partial evaluation. </title> <booktitle> In Proceedings of the Conference on Functional Programming Languages and Computer Architectures, </booktitle> <volume> LNCS 523, </volume> <pages> pages 165-191, </pages> <publisher> Harvard University, </publisher> <address> 1991. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Partial evaluation of functional programs [8, 31] has mainly stressed the former, while supercompilation of functional [67, 68, 65] and partial deduction of logic programs [21, 64, 4, 6, 52, 54] have concentrated on on-line control. (Some exceptions are <ref> [69, 56, 38, 33] </ref>.) Basically, within the off-line approach, an analysis phase (by hand and/or automatically), prior to the specialisation proper, provides annotations that guide the specialiser when it has to decide on control issues such as structure propagation, termination and the right amount of polyvariance.
Reference: [70] <author> W. Winsborough. </author> <title> Multiple specialization using minimal-function graph semantics. </title> <journal> The Journal of Logic Programming, </journal> <volume> 13(2 & 3):259-290, 1992. 55 </volume>
Reference-contexts: t 0 ) #1 (member (b; T ); t ) #1 ? = #2 (member (a; [a; b; c; djT ]); t ) (member (a; [b; c; djT ]); t 0 ) #1 (member (X; T ); t ) #1 A similar use of this minimisation algorithm is made in <ref> [70] </ref> and [62]. The former aims at minimising polyvariance in the context of multiple specialisation by optimising compilers. The latter, in a somewhat different context, studies polyvariant parallelisation and specialisation of logic programs based on abstract interpretation.
References-found: 70

