URL: ftp://hpsl.cs.umd.edu/pub/papers/ics95.parallel-dag.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Email: ftchong@ai.mit.edu  shamik@cs.umd.edu  brewer@cs.berkeley.edu  saltz@cs.umd.edu  
Title: Parallelization of Fine-grained Irregular DAGs  
Author: Frederic T. Chong Shamik D. Sharma Eric A. Brewer Joel Saltz 
Address: Berkeley  
Affiliation: Massachusetts Institute of Technology University of Maryland, College Park University of California,  
Abstract: This paper examines parallelization of computations that can be characterized as a fine-grained, irregular, directed acyclic graph of tasks. Such computations typically arise in sparse matrix applications where loops may have loop-carried data dependencies. The fine grain of the computation and the irregular nature of the dependencies make such loops extremely hard to parallelize efficiently. We use runtime preprocessing to parallelize such computations. A preprocessing step analyzes the DAG and determines an optimized mapping of tasks to processors, an appropriate order of executing these tasks on each processor and a set of synchronization constraints. This information is used to distribute data across processors, and to schedule computation, communication and synchronization. We use sparse-matrix triangular solution as our example application and compare a range of runtime preprocessed techniques for its parallelization. Whereas previous studies have achieved maximum speedups of less than 4 on even simple banded matrices, we achieve a speedup of almost 35 on 128 processors of the CM-5 on an irregular matrix with 5300 rows. We find that send and receive overheads dominate the time of our application. Efficient parallelization of such loops requires even more support for low latency communication.
Abstract-found: 1
Intro-found: 1
Reference: [ACD + 91] <author> Anant Agarwal, D. Chaiken, G. D'Souza, K. johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, Beng-Hong Lim, Gino Maa, D. Nussbaum, M. Parkin, and D. Yeung. </author> <title> The mit alewife machine: A large scale distributed memory multiprocessor. </title> <type> Technical Report Technical Memo - 454, </type> <institution> LCS/MIT, </institution> <year> 1991. </year>
Reference-contexts: Our final version, dynamic-fine version uses a cyclic distribution and lazy presence counters for synchronization. 5 Experiments We perform our experiments on two platforms, TMC CM-5 [Thi93] and the MIT Alewife <ref> [ACD + 91] </ref>. A brief overview of these machines is given in Section 5.1. The matrices we use for benchmarking our application were obtained from the Harwell-Boeing set | the characteristics of these matrices are described in Section 5.2.
Reference: [ACM88] <author> Arvind, David E. Culler, and Gino K. Maa. </author> <title> Assessing the benefits of fine-grained parallelism in dataflow programs. </title> <booktitle> In Supercomputing `88. IEEE, </booktitle> <year> 1988. </year>
Reference-contexts: The time spent idling due to load imbalances can be removed by using synchronization mechanisms of finer granularity. We use presence counters <ref> [ACM88] </ref> for this purpose. A presence counter at each node of the DAG is used to keep track of the number of dependencies that have been satisfied.
Reference: [AKK + 93] <author> Anant Agarwal, J. Kubiatowicz, D. Kranz, Beng-Hong Lim, D. Yeung, G. D'Souza, and Mike Parkin. Sparcle: </author> <title> An evolutionary processor design for large-scale multiprocessors. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: For efficient communication and accurate timings, we made extensive use of the Strata communications library [BB94]. Alewife : The MIT Alewife machine is an experimental multiprocessor that provides both distributed cache-coherent shared- memory and user-level message passing with DMA. Each Alewife node consists of a 33-MHz Sparcle processor <ref> [AKK + 93] </ref>, 64KB of direct-mapped cache, a 4 MB portion of shared memory, 2MB of private (unshared) memory, a floating-point coprocessor, and a mesh routing chip from Caltech. The nodes communicate via messages through a direct network with a mesh topology using wormhole routing.
Reference: [BB94] <author> Eric A. Brewer and Robert D. Blumofe. Strata: </author> <title> A high-performance communications library. </title> <note> Technical Report (to appear). </note> <institution> MIT Laboratory for Computer Science, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: The CM-5E, a later version of the CM-5, uses 40Mhz Viking SPARC processors but is otherwise identical to a CM-5. For efficient communication and accurate timings, we made extensive use of the Strata communications library <ref> [BB94] </ref>. Alewife : The MIT Alewife machine is an experimental multiprocessor that provides both distributed cache-coherent shared- memory and user-level message passing with DMA.
Reference: [BK94] <author> Eric A. Brewer and Bradley C. Kuszmaul. </author> <title> How to get good performance from the CM-5 data network. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> To Appear. 15 </note>
Reference-contexts: Using measurements for global permutations from <ref> [BK94] </ref>, we estimate that we can complete the redistribution for bcspwr10 on 8 processors in about 13 ms. dsc-fine, solves bcspwr10 on 8 processors about 2.1 ms faster than dynamic-fine which uses a cyclic distribution.
Reference: [CS] <author> Frederic T. Chong and Robert Schreiber. </author> <title> Parallel sparse triangular solution with partitioned inverses and prescheduled DAGs. Submitted to the 1995 Workshop on Solving Irregular Problems on Distributed Memory Machines, </title> <address> Santa Barbara, California. </address>
Reference-contexts: Our experimental results are based primarily on solving incompletely factored matrices taken from the BCSPWR set of the Harwell-Boeing matrices [DGL92]. By employing the method of partitioned inverses [PA92], our triangular solution implementation can also be successfully used with completely factored matrices as demonstrated in <ref> [CS] </ref>. Speedups are rarely reported for sparse triangular solvers. Heath and Raghavan [HR93] report good results for factorization on the Intel iPSC/860, but their triangular solution exhibits constant or decreasing performance as the number of processors increase.
Reference: [CSBS95] <author> Frederic T. Chong, Shamik D. Sharma, Eric A. Brewer, and Joel Saltz. </author> <title> Multiprocessor runtime support for irregular DAGs. </title> <editor> In Rajiv K. Kalia and Priya Vashishta, editors, </editor> <title> Toward Teraflop Computing and New Grand Challenge Applications. </title> <publisher> Nova Science Publishers, Inc., </publisher> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Level scheduling specifies an ordering of tasks but does not suggest a distribution of nodes across processors. We use a cyclic distribution of nodes with level scheduling | we found a cyclic distribution to be superior to other oblivious distributions in previous studies <ref> [CSBS95] </ref>. level-barrier uses barriers to synchronize between levels.
Reference: [CY93] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Implementing an irregular application on a distributed memory architecture. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 169-178, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Saltz et. al. [SMC91] also studied parallelization of loops with loop-carried dependencies using runtime preprocessing. Their study was in the context of shared-memory architectures. We use their preprocessing technique on message-passing architectures and compare it with other methods. Chakrabarti and Yelick <ref> [CY93] </ref> have studied the Grobner-basis problem, which results in an irregular DAG. They use consumer-driven techniques, using multi-threading to hide latency of fetching data. Since our application is critical-path bound, we use a data-driven approach instead.
Reference: [DGL92] <author> Ian S. Duff, Roger G. Grimes, and John G. Lewis. </author> <title> User's guide for the Harwell-Boeing sparse matrix collection. </title> <type> Technical Report TR/PA/92/86, CERFACS, 42 Ave G. Coriolis, </type> <address> 31057 Toulouse Cedex, France, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The sparsity patterns of sparse matrices vary widely. Our experimental results are based primarily on solving incompletely factored matrices taken from the BCSPWR set of the Harwell-Boeing matrices <ref> [DGL92] </ref>. By employing the method of partitioned inverses [PA92], our triangular solution implementation can also be successfully used with completely factored matrices as demonstrated in [CS]. Speedups are rarely reported for sparse triangular solvers. <p> A single-chip Communications and Memory Management Unit (CMMU) implements the cache coherence protocol by synthesizing messages to other nodes, and also implements the user-level message-passing interface and network queues. 5.2 Benchmarks We chose four power grid matrices from the Harwell-Boeing <ref> [DGL92] </ref> benchmark set for our experiments. These sparse matrices come from the BCSPWR set, and represent parts of the Western US Power Network and the Eastern US Power Network. Our matrices were incompletely factored, with the sparsity pattern being identical to that of the original matrix.
Reference: [ETM93] <institution> Analysis of performance accelerator running ETMSP. </institution> <type> Technical Report TR-102856, </type> <institution> Performance Processors, Inc., </institution> <address> Palo Alto, California 94301, </address> <month> October </month> <year> 1993. </year> <note> Research Project 8010-31. </note>
Reference-contexts: Sometimes there are tens to hundreds of solves performed using a single factorization. For example, the time spent performing solves is equal to or double that of the time spent factoring in the sequential execution of the ETMSP power grid code <ref> [ETM93] </ref>. 4 Runtime Preprocessing Methods Our approach to parallelizing DAG computations is based on prescheduling computations at run-time, a strategy commonly known as runtime preprocessing (also called an inspector-executor strategy) [SMC91]. A preprocessing step (called an inspector) is used to optimize the execution of the actual computation (called the executor).
Reference: [GY92] <author> Apostolos Gerasoulis and Tao Yang. </author> <title> A comparison of clustering heuristics for scheduling directed acyclic graphs on multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 276-291, </pages> <year> 1992. </year>
Reference-contexts: Communication and synchronization requirements can be reduced by a clever mapping of DAG nodes to processors but neither can be eliminated in the usual case. Low-latency communication and efficient synchronization remain critical requirements for parallelization of such computations. Unlike previous studies on parallel execution of coarse-grained task-graphs <ref> [GY92] </ref>, this paper examines extremely fine-grained computations; each DAG node may represent computation equivalent to only a few (1 to 100) floating point operations. Our example application is triangular solution of a sparse incompletely factored matrix. <p> Rubin [Rub92] studied a regular fine-grain DAG computation arising from a conjugate-gradient problem on the Monsoon dataflow machine. Yeung and Agarwala [YA93] studied the same problem on Alewife, using a shared-memory implementation. Yang and Gerasoulis <ref> [GY92] </ref> have examined the parallelization of irregular coarse-grain task-graphs, using compile-time preprocessing. We apply their preprocessing methods to fine-grained task-graphs and compare it against other methods. Saltz et. al. [SMC91] also studied parallelization of loops with loop-carried dependencies using runtime preprocessing. Their study was in the context of shared-memory architectures. <p> Since all dependencies flow from nodes at a lower level to nodes at a higher level, barriers ensure that all dependencies are always satisfied. 4.2 dsc-barrier | Dominant Sequence Clustering Dominant Sequence Clustering (DSC) is a compile-time scheduling and mapping algorithm developed by Yang and Gerasoulis <ref> [GY92] </ref> that runs on sequential platforms. Although we expect our approach to be most useful when preprocessing is carried out in parallel at runtime, we use DSC in this study to provide an upper bound on prescheduled performance.
Reference: [HR93] <author> Michael T. Heath and Padma Raghavan. </author> <title> Distributed solution of sparse linear systems. </title> <type> Technical Report UIUCDCS-R-93-1793, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <address> IL 61801, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Chakrabarti and Yelick [CY93] have studied the Grobner-basis problem, which results in an irregular DAG. They use consumer-driven techniques, using multi-threading to hide latency of fetching data. Since our application is critical-path bound, we use a data-driven approach instead. Heath and Raghavan <ref> [HR93] </ref>, Rothberg [Rot93], Lucas et. al. [LBT87], Venugopal et. al. [VN91] have studied parallel sparse matrix solution. <p> By employing the method of partitioned inverses [PA92], our triangular solution implementation can also be successfully used with completely factored matrices as demonstrated in [CS]. Speedups are rarely reported for sparse triangular solvers. Heath and Raghavan <ref> [HR93] </ref> report good results for factorization on the Intel iPSC/860, but their triangular solution exhibits constant or decreasing performance as the number of processors increase. They obtain speedups of no more than 4 regardless of the number of processors, even for banded matrices.
Reference: [L + 92] <author> Charles E. Leiserson et al. </author> <title> The network architecture of the connection machine CM-5. </title> <booktitle> In Symposium on Parallel Architectures and Algorithms, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: To keep our study architecturally general, we do not use the vector units. The CM-5 has a fat-tree-based network <ref> [L + 92] </ref> which provides fairly uniform communications delays and sustainable bandwidth. The CM-5E, a later version of the CM-5, uses 40Mhz Viking SPARC processors but is otherwise identical to a CM-5. For efficient communication and accurate timings, we made extensive use of the Strata communications library [BB94].
Reference: [LBT87] <author> Robert F. Lucas, Tom Blank, and Jerome J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: Chakrabarti and Yelick [CY93] have studied the Grobner-basis problem, which results in an irregular DAG. They use consumer-driven techniques, using multi-threading to hide latency of fetching data. Since our application is critical-path bound, we use a data-driven approach instead. Heath and Raghavan [HR93], Rothberg [Rot93], Lucas et. al. <ref> [LBT87] </ref>, Venugopal et. al. [VN91] have studied parallel sparse matrix solution. <p> They obtain speedups of no more than 4 regardless of the number of processors, even for banded matrices. Lucas, Blank, and Tiemann <ref> [LBT87] </ref> report similar results. While the time spent in a single triangular solve was much less than the time spent in factorization in these studies, there are many applications where the solve time is important. Sometimes there are tens to hundreds of solves performed using a single factorization.
Reference: [Moy91] <author> Steven A. Moyer. </author> <title> Performance of the iPSC/860 node architecture. </title> <type> Technical Report IPC-TR-91-007, </type> <institution> Institute for Parallel Computation, School of Engineering and Applied Science, University of Virginia, </institution> <address> Charlottesville, VA 22903, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: The runtime cost of the one-time preprocessing step is amortized over all executions of the executor. Depending on the number of times the executor step 0 We estimated speedups assuming a sequential solve speed similar to that of uncached DAXPY on a single iPSC/860 node <ref> [Moy91] </ref>. 3 circles. The computation is broken into five DAG nodes, each representing a row of L, an element of x, and an element of b. Each DAG node computes the element of x associated with it.
Reference: [PA92] <author> Alex Pothen and Fernando Alvarado. </author> <title> A fast rerdering algorithm for parallel sparse triangular solution. </title> <journal> SIAM Journal of Scientific and Statistical Computation, </journal> <volume> 13 </volume> <pages> 645-653, </pages> <year> 1992. </year>
Reference-contexts: The sparsity patterns of sparse matrices vary widely. Our experimental results are based primarily on solving incompletely factored matrices taken from the BCSPWR set of the Harwell-Boeing matrices [DGL92]. By employing the method of partitioned inverses <ref> [PA92] </ref>, our triangular solution implementation can also be successfully used with completely factored matrices as demonstrated in [CS]. Speedups are rarely reported for sparse triangular solvers.
Reference: [Rot93] <author> Edward Rothberg. </author> <title> Performance of panel and block approaches to sparse cholesky factorization on the ipsc/860 and paragon multiprocessors. </title> <type> Technical Report N/A, </type> <institution> Intel Supercomputer Systems Division, Beaverton, </institution> <address> OR, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Chakrabarti and Yelick [CY93] have studied the Grobner-basis problem, which results in an irregular DAG. They use consumer-driven techniques, using multi-threading to hide latency of fetching data. Since our application is critical-path bound, we use a data-driven approach instead. Heath and Raghavan [HR93], Rothberg <ref> [Rot93] </ref>, Lucas et. al. [LBT87], Venugopal et. al. [VN91] have studied parallel sparse matrix solution.
Reference: [Rub92] <author> Norman Rubin. </author> <title> Data flow computing and the conjugate gradient method. </title> <type> Technical Report MCRC-TR-25, </type> <institution> Motorola Cambridge Research Center, </institution> <year> 1992. </year>
Reference-contexts: Section 4 describes the methods we examine for parallelizing such applications. Section 5 compares the performance of these methods and isolates the overheads. We conclude with a brief discussion in Section 6. 2 Related Work There have been very few studies on parallelizing fine-grain irregular DAGs. Rubin <ref> [Rub92] </ref> studied a regular fine-grain DAG computation arising from a conjugate-gradient problem on the Monsoon dataflow machine. Yeung and Agarwala [YA93] studied the same problem on Alewife, using a shared-memory implementation. Yang and Gerasoulis [GY92] have examined the parallelization of irregular coarse-grain task-graphs, using compile-time preprocessing.
Reference: [Sal90] <author> Joel H. Saltz. </author> <title> Aggregation methods for solving sparse triangular systems on multiprocessors. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 11(1) </volume> <pages> 123-144, </pages> <month> Jan </month> <year> 1990. </year>
Reference-contexts: These four methods use different distribution, ordering and synchronization techniques, as summarized in Table 1. 4.1 level-barrier | Level Scheduling The first preprocessing method we consider, called level-barrier, uses Global level scheduling to order the tasks. Global level scheduling <ref> [Sal90] </ref>, partitions the nodes of the DAG into levels L 1 : : : L n , such that the nodes in level L i depend only on nodes in L 1 : : : L i1 . <p> Each processor partially orders its local nodes based on their levels nodes at the same level can be ordered in any fashion. The resulting schedule is called a level schedule. The topological sort needed in the preprocessing step to obtain a global level schedule, can be parallelized <ref> [Sal90] </ref>; currently, however, we perform level scheduling sequentially. Level scheduling specifies an ordering of tasks but does not suggest a distribution of nodes across processors.
Reference: [SMC91] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 603-611, </pages> <year> 1991. </year>
Reference-contexts: Our example application is triangular solution of a sparse incompletely factored matrix. This computation is used in iterative sparse matrix solution techniques and is broadly applicable to a wide range of scientific and commercial applications. Our approach to parallelizing DAG computations is based on runtime preprocessing <ref> [SMC91] </ref>. In this approach, the computation is divided into two stages. First, a preprocessing step analyzes the computation at runtime and generates information which is used to optimize the second stage, which performs the actual computation. <p> Yeung and Agarwala [YA93] studied the same problem on Alewife, using a shared-memory implementation. Yang and Gerasoulis [GY92] have examined the parallelization of irregular coarse-grain task-graphs, using compile-time preprocessing. We apply their preprocessing methods to fine-grained task-graphs and compare it against other methods. Saltz et. al. <ref> [SMC91] </ref> also studied parallelization of loops with loop-carried dependencies using runtime preprocessing. Their study was in the context of shared-memory architectures. We use their preprocessing technique on message-passing architectures and compare it with other methods. Chakrabarti and Yelick [CY93] have studied the Grobner-basis problem, which results in an irregular DAG. <p> equal to or double that of the time spent factoring in the sequential execution of the ETMSP power grid code [ETM93]. 4 Runtime Preprocessing Methods Our approach to parallelizing DAG computations is based on prescheduling computations at run-time, a strategy commonly known as runtime preprocessing (also called an inspector-executor strategy) <ref> [SMC91] </ref>. A preprocessing step (called an inspector) is used to optimize the execution of the actual computation (called the executor). The runtime cost of the one-time preprocessing step is amortized over all executions of the executor.
Reference: [Thi93] <institution> Thinking Machines Corporation, Cambridge, MA. </institution> <type> CM-5 Technical Summary, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: Our final version, dynamic-fine version uses a cyclic distribution and lazy presence counters for synchronization. 5 Experiments We perform our experiments on two platforms, TMC CM-5 <ref> [Thi93] </ref> and the MIT Alewife [ACD + 91]. A brief overview of these machines is given in Section 5.1. The matrices we use for benchmarking our application were obtained from the Harwell-Boeing set | the characteristics of these matrices are described in Section 5.2.
Reference: [VN91] <author> Sesh Venugopal and Vijay K. Naik. </author> <title> Effects of partitioning and scheduling sparse matrix factorization on communication and load balance. </title> <booktitle> In Supercomputing `91, </booktitle> <pages> pages 866-875, </pages> <year> 1991. </year>
Reference-contexts: They use consumer-driven techniques, using multi-threading to hide latency of fetching data. Since our application is critical-path bound, we use a data-driven approach instead. Heath and Raghavan [HR93], Rothberg [Rot93], Lucas et. al. [LBT87], Venugopal et. al. <ref> [VN91] </ref> have studied parallel sparse matrix solution.
Reference: [YA93] <author> Donald Yeung and Anant Agarwal. </author> <title> Experience with fine-grain synchronization in MIMD machines for preconditioned conjugate gradient. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 187-197, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year> <month> 16 </month>
Reference-contexts: We conclude with a brief discussion in Section 6. 2 Related Work There have been very few studies on parallelizing fine-grain irregular DAGs. Rubin [Rub92] studied a regular fine-grain DAG computation arising from a conjugate-gradient problem on the Monsoon dataflow machine. Yeung and Agarwala <ref> [YA93] </ref> studied the same problem on Alewife, using a shared-memory implementation. Yang and Gerasoulis [GY92] have examined the parallelization of irregular coarse-grain task-graphs, using compile-time preprocessing. We apply their preprocessing methods to fine-grained task-graphs and compare it against other methods.
References-found: 23

