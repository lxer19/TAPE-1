URL: http://www.tc.cornell.edu/~liao/nlp.ps.Z
Refering-URL: http://www.tc.cornell.edu/~liao/papers.html
Root-URL: http://www.tc.cornell.edu
Title: A REDUCED HESSIAN METHOD FOR CONSTRAINED OPTIMIZATION  
Author: Ai-Ping Liao 
Keyword: Key words. nonlinear programming, reduced Hessian, global conver gence  
Abstract: Reduced Hessian methods have been shown to be successful for equality constrained problems. However there are few results on reduced Hessian methods for general constrained problems. In this paper we propose a method for general constrained problems, based on Byrd and Schnabel's basis-independent algorithm. It can be regarded as a smooth extension of the standard reduced Hessian method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Avriel. </author> <title> Nonlinear Programming: Analysis and Methods. </title> <publisher> Prentice-Hall, Inc. </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1976. </year>
Reference-contexts: In order to concentrate on the main points we assume that KKT points exist for (P) and (QP). The conditions for the existence of a KKT point can be found, for example, in Avriel <ref> [1] </ref>.
Reference: [2] <author> R. Byrd and R. Schnabel. </author> <title> Continuity of the null space basis and constrained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 35 </volume> <pages> 32-41, </pages> <year> 1986. </year>
Reference-contexts: Although reduced Hessian methods are well developed for equality constrained problems, there is little work on generalizing this method for general constrained problems. We find that the Byrd-Schnabel basis-independent algorithm <ref> [2] </ref> can be generalized for general constrained problems. The central idea of the Byrd-Schnabel algorithm [2] is that before being used for generating the current search direction the approximation to the reduced Hessian is shifted to the current basis. <p> Although reduced Hessian methods are well developed for equality constrained problems, there is little work on generalizing this method for general constrained problems. We find that the Byrd-Schnabel basis-independent algorithm <ref> [2] </ref> can be generalized for general constrained problems. The central idea of the Byrd-Schnabel algorithm [2] is that before being used for generating the current search direction the approximation to the reduced Hessian is shifted to the current basis. In the next section, we will generalize this idea to general nonlinear programming by employing an active set strategy and present our algorithm. <p> It would be preferable for an algorithm to be independent of the choice of the null space basis. By shifting the reduced Hessian approximation from the current active space to a new active space Byrd and Schnabel <ref> [2] </ref> give an update formula which is independent of the choice of the null space basis. Suppose that B k is the current approximation to the reduced Hessian of the Lagrangian (with the current active space).
Reference: [3] <author> R. H. Byrd and J. Nocedal. </author> <title> An analysis of reduced Hessian methods for constrained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 49 </volume> <pages> 285-323, </pages> <year> 1991. </year>
Reference-contexts: This is the motivation for reduced Hessian methods. Reduced Hessian methods (for equality constrained problems) have been proposed by Murray and Wright [15], Gabay [10], Gilbert [11], Coleman and Conn [5], Nocedal and Overton [16], and Byrd and Nocedal <ref> [3] </ref>. Although reduced Hessian methods are well developed for equality constrained problems, there is little work on generalizing this method for general constrained problems. We find that the Byrd-Schnabel basis-independent algorithm [2] can be generalized for general constrained problems. <p> the l 1 function proposed by Han [13]. !(x; r) := f (x) + X r i jc i (x)j i=m 0 +1 4 During each iteration, the "penalty vector" r k := (r 1 k ; : : : ; r m k ) T is updated as follows <ref> [3] </ref>: for each i = 1; 2; : : : ; m, r i ( k1 if r i k j + k j + 2 otherwise, (3) where is some positive constant. As a consequence r k j k j+, i.e. r i k j i for all i. <p> The "Maratos effect," which could destroy the superlinear convergence rate, may occur in the algorithm. Fortunately, there are already some techniques to overcome it-for example, the "watchdog" method [4], or the second order correction technique (see, for example, Byrd and Nocedal <ref> [3] </ref>). 5 Implementation considerations In this section we propose some implementation techniques for the proposed algorithm and present some limited numerical tests. 1. Since our algorithm is independent of the choice of the null basis, any efficient algorithm for obtaining a null basis Z can be used. 2.
Reference: [4] <author> R. M. Chamberlain, C. Lemarechal, H. C. Pedersen, and M. J. D. Powell. </author> <title> The watchdog technique for forcing convergence in algorithms for constrained optimization. </title> <journal> Mathematical Programming Study, </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1982. </year>
Reference-contexts: It is also established that a sequence of "midpoints," in a closely related algorithm, is locally (1-step) Q-superlinearly convergent. The "Maratos effect," which could destroy the superlinear convergence rate, may occur in the algorithm. Fortunately, there are already some techniques to overcome it-for example, the "watchdog" method <ref> [4] </ref>, or the second order correction technique (see, for example, Byrd and Nocedal [3]). 5 Implementation considerations In this section we propose some implementation techniques for the proposed algorithm and present some limited numerical tests. 1.
Reference: [5] <author> T. F. Coleman and A. R. Conn. </author> <title> On the local convergence of a quasi-Newton method for the nonlinear programming problem. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 21 </volume> <pages> 755-769, </pages> <year> 1984. </year>
Reference-contexts: This is the motivation for reduced Hessian methods. Reduced Hessian methods (for equality constrained problems) have been proposed by Murray and Wright [15], Gabay [10], Gilbert [11], Coleman and Conn <ref> [5] </ref>, Nocedal and Overton [16], and Byrd and Nocedal [3]. Although reduced Hessian methods are well developed for equality constrained problems, there is little work on generalizing this method for general constrained problems. We find that the Byrd-Schnabel basis-independent algorithm [2] can be generalized for general constrained problems.
Reference: [6] <author> T. F. Coleman and A. Liao. </author> <title> The local convergence of the Byrd-Schnabel algorithm for constrained optimization. </title> <journal> Appl. Math. Lett., </journal> <volume> 6 </volume> <pages> 37-42, </pages> <year> 1993. </year>
Reference-contexts: Theorem 2.1 If B k is positive definite and y T k s k &gt; 0, fi k &gt; 0, then B k+1 is also positive definite. Proof. The proof is straightforward. See Coleman and Liao <ref> [6] </ref> for details. 2 3 Global behavior We now consider the global convergence of the algorithm in previous section. We note that Powell [19] gives a more general global convergence result by analyzing a very basic algorithm. <p> When all the constraints are equalities the algorithm is the same as the Byrd-Schnabel algorithm, or more precisely, the algorithm of Coleman and Liao <ref> [6] </ref>. Coleman and Liao [6] show that the algorithm converges locally 2-step Q-superlinearly. It is also established that a sequence of "midpoints," in a closely related algorithm, is locally (1-step) Q-superlinearly convergent. The "Maratos effect," which could destroy the superlinear convergence rate, may occur in the algorithm. <p> When all the constraints are equalities the algorithm is the same as the Byrd-Schnabel algorithm, or more precisely, the algorithm of Coleman and Liao <ref> [6] </ref>. Coleman and Liao [6] show that the algorithm converges locally 2-step Q-superlinearly. It is also established that a sequence of "midpoints," in a closely related algorithm, is locally (1-step) Q-superlinearly convergent. The "Maratos effect," which could destroy the superlinear convergence rate, may occur in the algorithm.
Reference: [7] <author> W. C. Davidon. </author> <title> Variable metric methods for minimization. </title> <type> Report ANL-5990, </type> <institution> Argonne National Labs, </institution> <year> 1959. </year>
Reference-contexts: 1 Introduction Quasi-Newton methods have made great progress since Davidon <ref> [7] </ref> proposed the first method in this class in 1959, and now are among the most powerful methods to solve smooth programming problems. These methods possess a superlinear convergent rate, and need only first order derivatives; they perform very well in practice.
Reference: [8] <author> J. E. Dennis, Jr. and R. B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1983. </year>
Reference-contexts: Let k ( B k+1 fi k I)T k + fi k I: Set k to k + 1 and go to step 1. 2 Remarks: 1 See, for example, Dennis and Schnabel <ref> [8] </ref>. 3 1. As in the Byrd-Schnabel algorithm, the above algorithm is independent of the choice of basis of Z k .
Reference: [9] <author> R. Fletcher. </author> <title> Numerical experiments with an exact l 1 penalty function method. </title> <editor> In O. Mangasarian, R. Meyer, and S. Robinson, editors, </editor> <booktitle> Nonlinear Programming 4, </booktitle> <pages> pages 99-129, </pages> <address> New York, 1981. </address> <publisher> Academic Press. </publisher>
Reference-contexts: We further note that this algorithm is very concise and clearly a smooth extension of the Byrd Schnabel algorithm. 2. The linearizations in the (QP) of the original nonlinear constraints may introduce inconsistencies. Some methods have been proposed for overcoming this difficulty, see, for example, Fletcher <ref> [9] </ref> and Powell [18]. Another difficulty is that the (QP) may be rank deficient and the problem may be unbounded, this is caused by improperly predicating the actual active set. Gurwitz and Overton [12] propose some strategies for overcoming this difficulty.
Reference: [10] <author> D. Gabay. </author> <title> Reduced quasi-Newton methods with feasibility improvement for nonlinearly constrained optimization. </title> <journal> Mathematical Programming Study, </journal> <volume> 16 </volume> <pages> 18-44, </pages> <year> 1982. </year>
Reference-contexts: This is the motivation for reduced Hessian methods. Reduced Hessian methods (for equality constrained problems) have been proposed by Murray and Wright [15], Gabay <ref> [10] </ref>, Gilbert [11], Coleman and Conn [5], Nocedal and Overton [16], and Byrd and Nocedal [3]. Although reduced Hessian methods are well developed for equality constrained problems, there is little work on generalizing this method for general constrained problems.
Reference: [11] <author> J. C. Gilbert. </author> <title> Maintaining the positive definiteness of the matrices in reduced Hessian methods for equality constrained optimization. </title> <type> Technical Report, </type> <institution> WP-87-123, IIASA, Laxenburg, Austria, </institution> <year> 1987. </year> <month> 15 </month>
Reference-contexts: This is the motivation for reduced Hessian methods. Reduced Hessian methods (for equality constrained problems) have been proposed by Murray and Wright [15], Gabay [10], Gilbert <ref> [11] </ref>, Coleman and Conn [5], Nocedal and Overton [16], and Byrd and Nocedal [3]. Although reduced Hessian methods are well developed for equality constrained problems, there is little work on generalizing this method for general constrained problems.
Reference: [12] <author> C. B. Gurwitz and M. L. Overton. </author> <title> Sequential quadratic programming meth-ods based on approximating a projected hessian matrix. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 </volume> <pages> 631-653, </pages> <year> 1989. </year>
Reference-contexts: Some methods have been proposed for overcoming this difficulty, see, for example, Fletcher [9] and Powell [18]. Another difficulty is that the (QP) may be rank deficient and the problem may be unbounded, this is caused by improperly predicating the actual active set. Gurwitz and Overton <ref> [12] </ref> propose some strategies for overcoming this difficulty. For simplying the analysis we assume that unboundedness does not occur. To conclude this section we give the following result showing that the update formula in this algorithm preserves positive definiteness.
Reference: [13] <author> S.-P. Han. </author> <title> A globally convergent method for nonlinear programming. </title> <journal> J. Optim. Th. and Appl., </journal> <volume> 22 </volume> <pages> 297-309, </pages> <year> 1977. </year>
Reference-contexts: The conditions for the existence of a KKT point can be found, for example, in Avriel [1]. The merit function we use for the line search is the l 1 function proposed by Han <ref> [13] </ref>. !(x; r) := f (x) + X r i jc i (x)j i=m 0 +1 4 During each iteration, the "penalty vector" r k := (r 1 k ; : : : ; r m k ) T is updated as follows [3]: for each i = 1; 2; : <p> It is straightforward to show, see Han <ref> [13] </ref>, for example, that if x; d satisfy the constraints of (QP) then r!(x; d; r) = rf (x) T d X r i jc i (x)j i=m 0 +1 where i = i (x; d) = 0 if c i (x) 0 Lemma 3.1 r!(x; d; r) is an upper
Reference: [14] <author> W. Hock and K. Schittkowski. </author> <title> Test Examples for Nonlinear Programming Codes. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1981. </year>
Reference-contexts: We tested our algorithm (without the second order correction) on some standard test problems. The comparison of this algorithm with Powell's algorithm VF02AD is shown in Table 1. The test problems are selected from Hock and Schittkowski <ref> [14] </ref>. The results of VF02AD are those reported in [14]. Numbers in parentheses are the numbers of iterations. The runs were performed using MAT-LAB on a Sun 4 (SPARC station). The abbreviations in the table are the same as those in [14], i.e., No: number of the test problem. <p> We tested our algorithm (without the second order correction) on some standard test problems. The comparison of this algorithm with Powell's algorithm VF02AD is shown in Table 1. The test problems are selected from Hock and Schittkowski <ref> [14] </ref>. The results of VF02AD are those reported in [14]. Numbers in parentheses are the numbers of iterations. The runs were performed using MAT-LAB on a Sun 4 (SPARC station). The abbreviations in the table are the same as those in [14], i.e., No: number of the test problem. Code: name of the program. <p> The test problems are selected from Hock and Schittkowski <ref> [14] </ref>. The results of VF02AD are those reported in [14]. Numbers in parentheses are the numbers of iterations. The runs were performed using MAT-LAB on a Sun 4 (SPARC station). The abbreviations in the table are the same as those in [14], i.e., No: number of the test problem. Code: name of the program. NF: number of objective function evaluations. NG: number of constraint functions evaluations. NDF: number of gradient evaluations of the objective function. NDG: number of gradient evaluations of the constraints. FV: objective function value.
Reference: [15] <author> W. Murray and M. H. Wright. </author> <title> Projected Lagragian methods based on the trajectories of penalty and barrier functions. </title> <type> Technical Report, 78-23, </type> <institution> Systems Optimization Laboratory, Stanford University, </institution> <year> 1978. </year>
Reference-contexts: This is the motivation for reduced Hessian methods. Reduced Hessian methods (for equality constrained problems) have been proposed by Murray and Wright <ref> [15] </ref>, Gabay [10], Gilbert [11], Coleman and Conn [5], Nocedal and Overton [16], and Byrd and Nocedal [3]. Although reduced Hessian methods are well developed for equality constrained problems, there is little work on generalizing this method for general constrained problems.
Reference: [16] <author> J. Nocedal and M. Overton. </author> <title> Projected Hessian updating algorithms for non-linearly constrained optimization. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 22 </volume> <pages> 821-850, </pages> <year> 1985. </year>
Reference-contexts: This is the motivation for reduced Hessian methods. Reduced Hessian methods (for equality constrained problems) have been proposed by Murray and Wright [15], Gabay [10], Gilbert [11], Coleman and Conn [5], Nocedal and Overton <ref> [16] </ref>, and Byrd and Nocedal [3]. Although reduced Hessian methods are well developed for equality constrained problems, there is little work on generalizing this method for general constrained problems. We find that the Byrd-Schnabel basis-independent algorithm [2] can be generalized for general constrained problems.
Reference: [17] <author> M. J. D. Powell. </author> <title> The convergence of variable metric methods for nonlinearly constrained optimization calculations. </title> <editor> In O. Mangasarian, R. Meyer, and S. Robinson, editors, </editor> <booktitle> Nonlinear Programming 3, </booktitle> <pages> pages 27-63, </pages> <address> New York and London, 1978. </address> <publisher> Academic Press. </publisher>
Reference-contexts: The success of quasi-Newton methods for unconstrained minimization led to research on applying this kind of method to constrained problems. The basic work was done by Biggs, Han, Powell, et al. in the 1970's. The method proposed by Powell [18] <ref> [17] </ref> works well for general constrained problems. However, none of this work has been proved to have Q-superlinear convergence in the non-convex case.
Reference: [18] <author> M. J. D. Powell. </author> <title> A fast algorithm for nonlinear constrained optimization calculations. </title> <editor> In G. A. Watson, editor, </editor> <booktitle> Numerical Analysis, </booktitle> <pages> pages 144-157, </pages> <address> Berlin, Heidelberg, New York, </address> <year> 1978. </year> <booktitle> Proceedings, Biennial Conference, </booktitle> <address> Dundee 1977, </address> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York. </address>
Reference-contexts: The success of quasi-Newton methods for unconstrained minimization led to research on applying this kind of method to constrained problems. The basic work was done by Biggs, Han, Powell, et al. in the 1970's. The method proposed by Powell <ref> [18] </ref> [17] works well for general constrained problems. However, none of this work has been proved to have Q-superlinear convergence in the non-convex case. <p> We further note that this algorithm is very concise and clearly a smooth extension of the Byrd Schnabel algorithm. 2. The linearizations in the (QP) of the original nonlinear constraints may introduce inconsistencies. Some methods have been proposed for overcoming this difficulty, see, for example, Fletcher [9] and Powell <ref> [18] </ref>. Another difficulty is that the (QP) may be rank deficient and the problem may be unbounded, this is caused by improperly predicating the actual active set. Gurwitz and Overton [12] propose some strategies for overcoming this difficulty. For simplying the analysis we assume that unboundedness does not occur. <p> NG: number of constraint functions evaluations. NDF: number of gradient evaluations of the objective function. NDG: number of gradient evaluations of the constraints. FV: objective function value. VC: sum of constraint violations. Our limited numerical tests show that our algorithm performs similarly to Powell's algorithm <ref> [18] </ref> measured by the total number of iterations, but our algorithm needs more function and gradient evaluations which is mainly due to the extra evaluation of the gradient of the Lagrangian for computing y k of (2). <p> This algorithm possesses a local (2-step) Q-superlinearly convergence rate. Its performance in practice seems similar to Powell's algorithm <ref> [18] </ref> in the number of iterations but it needs more function and gradient evaluations. However, extensive numerical tests are needed for evaluating the overall performance of this algorithm; this will be the subject of future work. Acknowledgment: I would like to thank Professor Michael J.
Reference: [19] <author> M. J. D. Powell. </author> <title> Variable metric methods for condtrained optimization. </title> <editor> In A. Bachem, M. Grotschel, and B. Korte, editors, </editor> <booktitle> Mathematical Programming: The State of the Art, </booktitle> <pages> pages 288-311, </pages> <address> Berlin, 1983. </address> <publisher> Springer-Verlag. </publisher> <pages> 16 </pages>
Reference-contexts: Proof. The proof is straightforward. See Coleman and Liao [6] for details. 2 3 Global behavior We now consider the global convergence of the algorithm in previous section. We note that Powell <ref> [19] </ref> gives a more general global convergence result by analyzing a very basic algorithm. In order to concentrate on the main points we assume that KKT points exist for (P) and (QP). The conditions for the existence of a KKT point can be found, for example, in Avriel [1].
References-found: 19

