URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR384.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Email: gasser@cs.indiana.edu  
Title: Learning Words in Time: Towards a Modular Connectionist Account of the Acquisition of Receptive Morphology  
Author: Michael Gasser 
Date: June 10, 1993  
Address: Bloomington, IN, USA  
Affiliation: Computer Science and Linguistics Departments Indiana University  
Abstract: To have learned the morphology of a natural language is to have the capacity both to recognize and to produce words consisting of novel combinations of familiar morphemes. Most recent work on the acquisition of morphology takes the perspective of production, but it is receptive morphology which comes first in the child. This paper presents a connectionist model of the acquisition of the capacity to recognize morphologically complex words. The model takes sequences of phonetic segments as inputs and maps them onto output units representing the meanings of lexical and grammatical morphemes. It consists of a simple recurrent network with separate hidden-layer modules for the tasks of recognizing the root and the grammatical morphemes of the input word. Experiments with artificial language stimuli demonstrate that the model generalizes to novel words for morphological rules of all but one of the major types found in natural languages and that a version of the network with unassigned hidden-layer modules can learn to assign them to the output recognition tasks in an efficient manner. I also argue that for rules involving reduplication, that is, the copying of portions of a root, the network requires separate recurrent subnetworks for sequences of larger units such as syllables. The network can learn to develop its own syllable representations which not only support the recognition of reduplication but also provide the basis for learning to produce, as well as recognize, morphologically complex words. The model makes many detailed predictions about the learning difficulty of particular morphological rules.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bates, E., Bretherton, I., & Snyder, L. </author> <year> (1988). </year> <title> From First Words to Grammar: Individual Differences and Dissociable Mechanisms. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference: <author> Berko, J. </author> <year> (1958). </year> <title> The child's learning of English morphology. </title> <booktitle> Word, </booktitle> <volume> 14, </volume> <pages> 150-177. </pages>
Reference-contexts: However, these models, with the exception of Gasser & Lee (1991), the predecessor of the model described here, treat word production only. The focus on production apparently results from an interest in modeling results like those of <ref> (Berko, 1958) </ref>, which are production-oriented tests of a knowledge of rules. But children learn to understand words before they can produce them, and, as argued in more detail below, the production capacity must build on the comprehension capacity.
Reference: <author> Chater, N. & Conkey, P. </author> <year> (1992). </year> <title> Finding linguistic structure with recurrent neural networks. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 14, </volume> <pages> 402-407. </pages>
Reference-contexts: Backpropagation would have to proceed back to the beginning of the sequence to achieve a true gradient. In practice, however, the approximation which results from the cutoff seems to permit the effective learning of sequences as well as generalization to related novel sequences <ref> (Chater & Conkey, 1992) </ref>. The recognition network tested in the first set of experiments reported here is a simple recurrent network which takes phonetic segments as inputs and is trained to activate one unit each for the set of morphemes making up the input word.
Reference: <author> Corina, D. P. </author> <year> (1991). </year> <title> Towards an Understanding of the Syllable: Evidence from Linguistic, Psychological, and Connectionist Investigations of Syllable Structure. </title> <type> Ph.D. thesis, </type> <institution> University of California, </institution> <address> San Diego. </address>
Reference: <author> Cottrell, G. W. & Plunkett, K. </author> <year> (1991). </year> <title> Learning the past tense in a recurrent network: acquiring the mapping from meaning to sounds. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 13, </volume> <pages> 328-333. </pages>
Reference: <author> Cutler, A. & Norris, D. </author> <year> (1988). </year> <title> The role of strong syllables in segmentation in lexical access. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 14, </volume> <pages> 113-121. </pages>
Reference: <author> Daugherty, K. & Seidenberg, M. </author> <year> (1992). </year> <title> Rules or connections? the past tense revisited. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 14, </volume> <pages> 259-264. </pages>
Reference: <author> Dell, G. S. </author> <year> (1986). </year> <title> A spreading activation theory of retrieval in sentence production. </title> <journal> Psychological Review, </journal> <volume> 93, </volume> <pages> 283-321. </pages>
Reference: <author> Doutriaux, A. & Zipser, D. </author> <year> (1990). </year> <title> Unsupervised discovery of speech segments using recurrent networks. </title> <editor> In Touretzky, D., Elman, J., Sejnowski, T., & Hinton, G. (Eds.), </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <pages> pp. 303-309. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-211. </pages>
Reference: <author> Elman, J. L. </author> <year> (1991). </year> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> 195-225. </pages>
Reference: <author> Fahlman, S. E. </author> <year> (1989). </year> <title> Faster-learning variations on back-propagation: an empirical study. </title> <editor> In Touretzky, D. S., Hinton, G., & Sejnowski, T. (Eds.), </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pp. 38-51. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference: <author> Gasser, M. & Lee, C.-D. </author> <year> (1990). </year> <title> Networks that learn about phonological feature persistence. </title> <journal> Connection Science, </journal> <volume> 2(4), </volume> <pages> 265-278. </pages> <note> 57 Gasser, </note> <author> M. & Lee, C.-D. </author> <year> (1991). </year> <title> A short term memory architecture for the learning of morpho-phonemic rules. </title> <editor> In Lippmann, R. P., Moody, J. E., & Touretzky, D. S. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pp. 605-611. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Gasser, M. & Smith, L. B. </author> <year> (1993). </year> <title> Learning noun and adjective meanings: a connectionist account. </title> <type> Tech. rep. 382, </type> <institution> Indiana University, Computer Science Department, Bloomington, </institution> <note> IN. </note>
Reference: <author> Gasser, M. </author> <year> (1992). </year> <title> Learning distributed syllable representations. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 14, </volume> <pages> 396-401. </pages>
Reference: <author> Goldsmith, J. </author> <year> (1992). </year> <title> Local modeling in phonology. </title> <editor> In Davis, S. (Ed.), </editor> <booktitle> Connectionism: Theory and Practice, </booktitle> <pages> pp. 229-246. </pages> <publisher> Oxford University Press, </publisher> <address> New York. </address>
Reference: <author> Gupta, P. & MacWhinney, B. </author> <year> (1992). </year> <title> Integrating category acquisition with inflectional marking: a model of the german nominal system. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 14, </volume> <pages> 253-258. </pages>
Reference: <author> Hare, M. & Elman, J. </author> <year> (1992). </year> <title> A connectionist account of English inflectional morphology: evidence from language change. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 14, </volume> <pages> 265-270. </pages>
Reference: <author> Hare, M., Corina, D., & Cottrell, G. W. </author> <year> (1990). </year> <title> A connectionist perspective on prosodic structure. </title> <booktitle> In Proccedings of the Annual Meeting of the Berkeley Linguistic Society, </booktitle> <pages> 15. </pages>
Reference: <author> Hare, M. L. </author> <year> (1990). </year> <title> The role of similarity in Hungarian vowel harmony: a connectionist account. </title> <journal> Connection Science, </journal> <volume> 2. </volume>
Reference: <author> Hertz, J., Krogh, A., & Palmer, R. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: Because an explicit target is available, the learning of word recognition can be seen as an instance of supervised learning <ref> (Hertz, Krogh, & Palmer, 1991) </ref>. <p> This mechanism is a part of the symbolic model of morphological acquisition described in MacWhinney (1978). In this case, however, there is still no target for production. What is available rather is some measure of how correct the output was. This would thus be an instance of reinforcement learning <ref> (Hertz et al., 1991) </ref>, which is considerably less powerful than the supervised learning which characterizes word recognition. This is due to the credit assignment problem: when the output is wrong, there is no direct way of knowing what led to the error.
Reference: <author> Hinton, G. </author> <year> (1990). </year> <title> Mapping part-whole hierarchies into connectionist networks. </title> <journal> Artificial Intelligence, </journal> <volume> 46, </volume> <pages> 47-75. </pages>
Reference-contexts: Modularity of this kind is reasonable again if what the system has to learn about sequences of syllables has little in common with what it has to learn about sequences of phonemes. This type of modularity is to be contrasted with what Hinton calls between-level sharing <ref> (Hinton, 1990) </ref>, which is called for in domains, such as vision, in which the same general knowledge applies to different levels. For language modularity may make sense. This leaves the problem of what sort of system could organize itself in this fashion.
Reference: <author> Hoeffner, J. </author> <year> (1992). </year> <title> Are rules a thing of the past? the acquisition of verbal morphology by an attractor network. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <volume> 14, </volume> <pages> 861-866. </pages>
Reference: <author> Hogg, R. & McCully, C. B. </author> <year> (1987). </year> <title> Metrical Phonology: A Coursebook. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference-contexts: Thus there would be a process of division into units and a portion of the network which treats one of these units at a time as a primitive input. One such level which seems plausible is that of metrical feet <ref> (Hogg & McCully, 1987) </ref>, multisyllabic units defined in terms of the patterns of stress on the syllables. Division into such units might be possible on the basis of stress, much as division into syllables may be based on sonority. Syllables and higher-level units are motivated for reduplication.
Reference: <author> Jacobs, R. A., Jordan, M. I., & Barto, A. G. </author> <year> (1991). </year> <title> Task decomposition through competition in a modular connectionist architecture: the what and where vision tasks. </title> <journal> Cognitive Science, </journal> <volume> 15, </volume> <pages> 219-250. </pages>
Reference-contexts: But is there any way to alleviate any of these causes of poor performance? The last point, that of conflicting demands placed on the network, is a familiar one for backpropagation networks which are required to perform more than one task using a single hidden layer, the result being crosstalk <ref> (Jacobs, Jordan, & Barto, 1991) </ref>. In spatial crosstalk, the network performs both conflicting tasks simultaneously; in temporal crosstalk, the tasks are performed at different times during processing. <p> of the conflict between root and inflection recognition. 6.3 Learning Morphological Rules in a Modular Network 6.3.1 A Modular Architecture for Recognition One obvious answer to the problem of two conflicting tasks in a single network is to assign the tasks to separate networks, or separate modules within one network <ref> (Jacobs et al., 1991) </ref>. A network with separate hidden layers for each task would obviate crosstalk, resulting ultimately in better generalization for both of the tasks.
Reference: <author> Jordan, M. </author> <year> (1986). </year> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 531-546 Hillsdale, New Jersey. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Kempen, G. & Hoenkamp, E. </author> <year> (1987). </year> <title> An incremental procedural grammar for sentence formulation. </title> <journal> Cognitive Science, </journal> <volume> 11, </volume> <pages> 201-258. </pages>
Reference-contexts: And production of a word, or some portion of a word, is often initiated before the word has been completely formulated <ref> (Kempen & Hoenkamp, 1987) </ref>. Thus it is simply not the case that entire words (or other units) are available as static chunks for word recognition or production. 5 Linguistic forms, then, are temporal objects.
Reference: <author> Kim, J. J., Pinker, S., Prince, A., & Prasada, S. </author> <year> (1991). </year> <title> Why no mere mortal has ever flown out to center field. </title> <journal> Cognitive Science, </journal> <volume> 15, </volume> <pages> 173-218. </pages> <address> 58 Lee, C.-D. </address> <year> (1991). </year> <title> Learning to Perceive and Produce Words in Connectionist Networks. </title> <type> Ph.D. thesis, </type> <institution> Indiana University, Bloomington. </institution>
Reference-contexts: While this seems to make networks poorly suited for phenomena such as morphological rules, nearly everyone seems to agree that something like a connectionist pattern associator is required for low-level cognitive phenomena, including for example, the learning of irregular morphology <ref> (Kim, Pinker, Prince, & Prasada, 1991) </ref>. For the sake of parsimony then, if nothing else, it is of interest to know how far connectionist networks can go in modeling the learning or morphological rules. <p> This is deemed necessary to explain the fact that speakers treat polysemous words in a unitary fashion morphologically: the past tense of get is got no matter what it means <ref> (Kim et al., 1991) </ref>. 2.2 Types of Morphological Processes What sorts of possibilities are there for the ways in which roots and inflections combine to form words? The most common type is affixation, by which the inflections (the affixes) are attached to the root (or, more precisely, the stem).
Reference: <author> MacWhinney, B. & Leinbach, J. </author> <year> (1991). </year> <title> Implementations are not conceptualization: revising the verb learning model. </title> <journal> Cognition, </journal> <volume> 40, </volume> <pages> 121-157. </pages>
Reference: <author> MacWhinney, B. </author> <year> (1978). </year> <title> The acquisition of morphophonology. </title> <booktitle> Monographs of the Society for Research in Child Development, </booktitle> <pages> 43(1-2, </pages> <note> Serial No. 174). </note>
Reference: <author> Marcus, G. F., Pinker, S., Ullman, M., Hollander, M., Rosen, T. J., & Xu, F. </author> <year> (1992). </year> <title> Overregulariza-tion in language acquisition. </title> <booktitle> Monographs of the Society for Research in Child Development, </booktitle> <volume> 57(4, Serial No. </volume> <pages> 228). </pages>
Reference-contexts: But for children the immediate task is that of understanding the words they hear and producing 1 There is disagreement, however, on how frequent and long-lasting a phenomenon it is <ref> (Marcus, Pinker, Ullman, Hollander, Rosen, & Xu, 1992) </ref>. 3 words which others will understand. That is, they must learn to map forms onto meanings and meanings onto forms.
Reference: <author> Marcus, G. F., Brinkmann, U., Clahsen, H., Wiese, R., Woest, A., & Pinker, S. </author> <year> (1993). </year> <title> German inflection: the exception that proves the rule. </title> <type> Tech. rep. Occasional Paper #47, </type> <institution> MIT Center for Cognitive Science, </institution> <address> Cambridge, MA. </address>
Reference: <author> Markman, E. M. </author> <year> (1989). </year> <title> Categorization and Naming in Children: Problems of Induction. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: This implements a version of the mutual exclusivity hypothesis 19 <ref> (Markman, 1989) </ref>, which has been applied to the learning of word meanings. Mutual exclusivity fails when a single input can map onto more than one output category, that is, homonymy in our case.
Reference: <author> Marslen-Wilson, W. D. & Tyler, L. K. </author> <year> (1980). </year> <title> The temporal structure of spoken language understanding. </title> <journal> Cognition, </journal> <volume> 8, </volume> <pages> 1-71. </pages>
Reference: <author> Martin, J. </author> <year> (1988). </year> <title> Subtractive morphology as dissociation. </title> <booktitle> In Proceedings of the Seventh West Coast Conference of Formal Linguistics Stanford, </booktitle> <address> CA. </address> <publisher> Stanford Linguistics Association. </publisher>
Reference-contexts: In 3 Some would argue that such cases involve separate prefixation and suffixation processes. The arguments need not concern us here. 6 Koasati, for example, verbs have plural forms which are formed by deleting a portion of the singular forms: lasaplin `lick (singular)', laslin `lick (plural)' <ref> (Martin, 1988) </ref>. A very different sort of process, which I shall refer to as mutation, consists in an alternation in one or more of the root segments.
Reference: <author> Mtenje, A. </author> <year> (1987). </year> <title> Tone shift principles in the CHICHE WA verb: a case for a tone lexicon. </title> <journal> Lingua, </journal> <volume> 72, </volume> <pages> 169-209. </pages>
Reference-contexts: Thus in this languages changes in tone alone can indicate a difference in tense <ref> (Mtenje, 1987) </ref>. One final possibility may be considered either a form of affixation or a form of mutation, but it deserves special mention because of its complexity. It is best known for verbs in the Semitic languages.
Reference: <author> Norris, D. </author> <year> (1990). </year> <title> A dynamic-net model of human speech recognition. </title> <editor> In Altmann, G. T. M. (Ed.), </editor> <booktitle> Cognitive Models of Speech Processing: Psycholinguistic and Computational Perspectives, </booktitle> <pages> pp. 87-104. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Pinker, S. & Prince, A. </author> <year> (1988). </year> <title> On language and connectionism: analysis of a parallel distributed processing model of language acquisition. </title> <journal> Cognition, </journal> <pages> 73-193. </pages>
Reference: <author> Pinker, S. </author> <year> (1984). </year> <title> Language Learnability and Language Development. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Plunkett, K. & Marchman, V. </author> <year> (1991). </year> <title> U-shaped learning and frequency effects in a multi-layered perceptron: implications for child language acquisition. </title> <journal> Cognition, </journal> <volume> 38, </volume> <pages> 1-60. </pages>
Reference: <author> Port, R. </author> <year> (1990). </year> <title> Representation and recognition of temporal patterns. </title> <journal> Connection Science, </journal> <volume> 2, </volume> <pages> 151-176. </pages>
Reference: <author> Pullum, G. </author> <year> (1982). </year> <title> Letter. </title> <journal> Linguistics, </journal> <volume> 20, </volume> <pages> 339-344. </pages>
Reference-contexts: In this section I discuss 4 This use of distributional evidence has been criticized on the grounds that the absence of a feature from the world's languages may be a historical accident rather than a reflection of some deep-seated processing or learning mechanism <ref> (Pullum, 1982) </ref>.
Reference: <author> Regier, T. </author> <year> (1992). </year> <title> The Acquisition of Lexical Semantics for Spatial Terms: A Connectionist Model of Perceptual Categorization. </title> <type> Ph.D. thesis, </type> <institution> University of California, Berkeley. </institution> <note> 59 Rumelhart, </note> <author> D. E. & McClelland, J. L. </author> <year> (1986). </year> <title> On learning the past tense of English verbs. </title> <editor> In McClelland, J. L. & Rumelhart, D. E. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Volume 2, </volume> <pages> pp. 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Selkirk, E. O. </author> <year> (1982). </year> <title> The syllable. </title> <editor> In van der Hulst, H. & Smith, N. (Eds.), </editor> <title> The Structure of Phonological Representations, Part II. </title> <publisher> Foris, Dordrecht. </publisher>
Reference: <author> Servan-Schreiber, D., Cleeremans, A., & McClelland, J. L. </author> <year> (1988). </year> <title> Encoding sequential structures in simple recurrent networks. </title> <type> Tech. rep. </type> <institution> CMU-CS-88-183, Carnegie Mellon University. </institution>
Reference: <author> Spencer, A. </author> <year> (1991). </year> <title> Morphological Theory. </title> <publisher> Basil Blackwell, Oxford. </publisher>
Reference-contexts: For a more comprehensive treatment, see <ref> (Spencer, 1991) </ref>. The approach adopted here will be the traditional linguistic one: I will be looking at the phenomena from the perspective of the language as a system, rather than from the perspective of a human (or machine) user or learner.
Reference: <author> Stevens, A. </author> <year> (1968). </year> <title> Madurese Phonology and Morphology. </title> <publisher> American Oriental Society, </publisher> <address> New Haven, CT. </address>
Reference-contexts: An example from Madurese, a Malayo-Polynesian language spoken in Indonesia: buwaqan `fruit', waq-buwaqan `fruits'. Here the sequence waq in the singular form is copied onto the front of the root to form the plural <ref> (Stevens, 1968) </ref>. The fact that a portion of the root is copied makes reduplication considerably more complex than affixation. The statement of a reduplication rule seems to require a variable of a sort not necessary for an affixation rule.
Reference: <author> Touretzky, D. & Wheeler, D. </author> <year> (1990). </year> <title> A computational basis for phonology. </title> <editor> In Touretzky, D. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2 San Mateo, </booktitle> <address> CA. </address> <publisher> IEEE, Morgan Kaufmann. </publisher>
Reference: <author> Waibel, A. </author> <year> (1989). </year> <title> Modular construction of time-delay neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 39-46. 60 </pages>
Reference-contexts: Feedforward connectionist networks can be outfitted with a short-term memory capacity through the use of time delays on connections from inputs <ref> (e.g., Waibel, 1989) </ref>. For example, if in addition to the ordinary connections from input units to hidden or output units, there are additional sets of input connections with delays of one and two time steps, then the system always has access to a window of width 3.
References-found: 49

