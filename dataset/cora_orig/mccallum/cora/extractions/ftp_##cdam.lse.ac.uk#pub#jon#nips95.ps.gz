URL: ftp://cdam.lse.ac.uk/pub/jon/nips95.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: jon@dcs.rhbnc.ac.uk  
Title: Learning Model Bias minimum number of examples requred to learn a single task, and O(a
Author: Jonathan Baxter 
Note: n where O(a) is a bound on the  
Address: London  
Affiliation: Department of Computer Science Royal Holloway College, University of  
Abstract: In this paper the problem of learning appropriate domain-specific bias is addressed. It is shown that this can be achieved by learning many related tasks from the same domain, and a theorem is given bounding the number tasks that must be learnt. A corollary of the theorem is that if the tasks are known to possess a common internal representation or preprocessing then the number of examples required per task for good generalisation when learning n tasks simultaneously scales like O(a + b tive support for the theoretical results is reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. S. Abu-Mostafa. </author> <title> Learning from Hints in Neural Networks. </title> <journal> Journal of Com-plecity, </journal> <volume> 6 </volume> <pages> 192-198, </pages> <year> 1989. </year>
Reference-contexts: Once suitable biases have been found the actual learning task is relatively trivial. Exisiting methods of bias generally require the input of a human expert in the form of heuristics, hints <ref> [1] </ref>, domain knowledge, etc. Such methods are clearly limited by the accuracy and reliability of the expert's knowledge and also by the extent to which that knowledge can be transferred to the learner. <p> Definition 1 Let H = fHg be a hypothesis space family. Let H = fh 2 H: H 2 Hg. For any h: X ! Y , define a map h: X fi Y ! <ref> [0; 1] </ref> by h (x; y) = (h (x) y) 2 . Note the abuse of notation: h stands for two different functions depending on its argument. <p> Note the abuse of notation: h stands for two different functions depending on its argument. Given a sequence of n functions ~ h = (h 1 ; : : : ; h n ) let ~ h: (X fi Y ) n ! <ref> [0; 1] </ref> be the function (x 1 ; y 1 ; : : :; x n ; y n ) 7! 1 n i=1 h i (x i ; y i ). <p> Let H n be the set of all such functions where the h i are all chosen from H. Let H n = fH n : H 2 Hg. For each H 2 H define H fl : P ! <ref> [0; 1] </ref> by H fl (P ) = inf h2H E (h; P ) and let H fl = fH fl : H 2 H g. 1 This assumes the infimum in (3) is attained. Definition 2 Given a set of functions H from any space Z to [0; 1], and <p> P ! <ref> [0; 1] </ref> by H fl (P ) = inf h2H E (h; P ) and let H fl = fH fl : H 2 H g. 1 This assumes the infimum in (3) is attained. Definition 2 Given a set of functions H from any space Z to [0; 1], and any prob-ability measure on Z, define the pseudo-metric d P on H by d P (h; h 0 ) = Z Denote the smallest "-cover of (H; d P ) by N ("; H; d P ).
Reference: [2] <author> J. Baxter. </author> <title> A Bayesian Model of Bias Learning. </title> <note> Submitted to COLT 1996, </note> <year> 1995. </year>
Reference-contexts: In addition, the number of examples required per task to learn n tasks independently was shown to be upper bounded by O (a + b=n) for appropriate environments. See <ref> [2] </ref> for an analysis of bias learning within an Information theoretic framework which leads to an exact a + b=n-type bound.
Reference: [3] <author> J. Baxter. </author> <title> Learning Internal Representations. </title> <type> PhD thesis, </type> <institution> Department of Mathematics and Statistics, The Flinders University of South Australia, </institution> <year> 1995. </year> <note> Draft copy in Neuroprose Archive under "/pub/neuroprose/Thesis/baxter.thesis.ps.Z". </note>
Reference-contexts: A more general formulation based on statistical decision theory is given in <ref> [3] </ref>. <p> Call G (H; n; "; ffi) the n-task gain of H. Using the fact <ref> [3] </ref> that C ("; H ) C ("; [H n ] ) C ("; H ) ; and the formula for m from theorem 1, we have, 1 G (H; n; "; ffi) n: 2 The bounds in theorem 1 can be improved to O 1 if all H 2 H <p> Let W R be the total number of weights in the representation network and W O be the number of weights in an individual output network. Suppose also that all the nodes in each network are Lipschitz bounded 3 . Then it can be shown <ref> [3] </ref> that ln C ("; [H n ] ) = O W O + W R " and ln C ("; H fl ) = O W R ln 1 . <p> Separate simulations were performed with n ranging from 1 to 21 in steps of four and m ranging from 1 to 171 in steps of 10. Further details of the experimental procedure may be found in <ref> [3] </ref>, chapter 4. Once the network had sucessfully learnt the n training sets its generalization ability was tested on all n functions used to generate the training set. <p> Although there is insufficient space to show the representation error here (see <ref> [3] </ref> for the details), it was found that the representation error monotonically decreased with the number of tasks learnt, verifying the theoretical conclusions. The representation's output for all inputs is shown in figure 3 for sample sizes (n; m) = (1; 131); (5; 31) and (13; 31) . <p> This too was experimentally verified although there is insufficient space to present the results here (see <ref> [3] </ref>). 5 Conclusion I have introduced a formal model of bias learning and shown that (under mild restrictions) a learner can sample sufficiently many times from sufficiently many tasks to learn bias that is appropriate for the entire environment.
Reference: [4] <author> J. Baxter. </author> <title> Learning Internal Representations. </title> <booktitle> In Proceedings of the Eighth International Conference on Computational Learning Theory, </booktitle> <address> Santa Cruz, California, 1995. </address> <publisher> ACM Press. </publisher>
Reference: [5] <author> R. Caruana. </author> <title> Learning Many Related Tasks at the Same Time with Backpropagation. </title> <booktitle> In Advances in Neural Information Processing 5, </booktitle> <year> 1993. </year>
Reference-contexts: Other authors that have empirically investigated the idea of learning multiple related tasks include <ref> [5] </ref> and [8]. 2 Learning Bias For the sake of argument I consider learning problems that amount to minimizing the mean squared error of a function h over some training set D. A more general formulation based on statistical decision theory is given in [3].
Reference: [6] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Comput., </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction It has been argued (see <ref> [6] </ref>) that the main problem in machine learning is the biasing of a learner's hypothesis space sufficiently well to ensure good generalisation from a small number of examples. Once suitable biases have been found the actual learning task is relatively trivial.
Reference: [7] <author> W. S. Lee, P. L. Bartlett, and R. C. Williamson. </author> <title> Sample Complexity of Agnostic Learning with Squared Loss. </title> <note> In preparation, </note> <year> 1995. </year>
Reference-contexts: ] ) C ("; H ) ; and the formula for m from theorem 1, we have, 1 G (H; n; "; ffi) n: 2 The bounds in theorem 1 can be improved to O 1 if all H 2 H are convex and the error is the squared loss <ref> [7] </ref>. Thus, at least in the worst case analysis here, learning n tasks in the same environ-ment can result in anything from no gain at all to an n-fold reduction in the number of examples required per task.
Reference: [8] <author> T. M. Mitchell and S. Thrun. </author> <title> Learning One More Thing. </title> <type> Technical Report CMU-CS-94-184, CMU, </type> <year> 1994. </year>
Reference-contexts: Other authors that have empirically investigated the idea of learning multiple related tasks include [5] and <ref> [8] </ref>. 2 Learning Bias For the sake of argument I consider learning problems that amount to minimizing the mean squared error of a function h over some training set D. A more general formulation based on statistical decision theory is given in [3].
References-found: 8

