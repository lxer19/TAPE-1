URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/rationality-postulates-mcc.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/
Root-URL: http://www.csc.ncsu.edu
Email: msingh@mcc.com  
Phone: (512) 338-3431  
Title: Formalizing Rationality Postulates for Intentions  
Author: Munindar P. Singh 
Note: Copyright c fl1993 Microelectronics and Computer Technology Corporation. All Rights Reserved. Shareholders and Associates of MCC may reproduce and distribute these materials for internal purposes by retaining MCC's copyright notice, proprietary legends, and markings on all complete and partial copies.  
Address: 3500 West Balcones Center Drive Austin, TX 78759, USA  
Affiliation: MCC Nonconfidential Microelectronics and Computer Technology Corporation Information Systems Division  
Abstract: MCC Technical Report Number KBNL-087-93 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Fahiem Bacchus, Josh Tenenberg, and Johannes A. Koomen. </author> <title> A non-reified temporal logic. </title> <booktitle> In First Conference on Knowledge Representation and Reasoning, </booktitle> <pages> pages 2-10, </pages> <year> 1989. </year>
Reference-contexts: Standard CTL* (and, therefore, L) are non-reified|this helps us make more natural claims about the past and future along scenarios. As Bacchus et al. have shown recently, non-reification is not a problem for representing temporal knowledge, so I shall not attempt to modify it <ref> [1] </ref>. L allows us to distinguish between present, past and future naturally, and to keep temporal considerations separate from probabilistic ones|this contrasts with Haddawy's logic in which past and future cannot be distinguished except by naming specific times, and the probability function incorporates an implicit future tense operator [8, x4].
Reference: [2] <author> Michael E. Bratman. </author> <title> Intention, Plans, and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Dudley's first postulate that an intention leads to a belief in its success is well received in much of AI and Philosophy [12, p. 102], [14, p. 43], [3, 7, 19] and <ref> [2, p. 37] </ref> (who allows an "expectation," rather than a full belief). However, it can be safely weakened to allow a less confident agent than Dudley. <p> One idea is to fall back upon the notion of commitment as defined by Harman [9, p. 94] and Bratman <ref> [2, ch. 2] </ref>. An agent has an intention only as long as he is committed to it. As before, let q entail p.
Reference: [3] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> Intention is choice with commitment. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 213-261, </pages> <year> 1990. </year>
Reference-contexts: Recently, there has been some interest in the formalization of the semantics of intentions|i.e., of the conditions under which an agent may or may not be said to have a certain intention <ref> [3, 12, 19, 18] </ref>. However, before any theory of intentions can be applied to actual AI problems, we need to go beyond the semantics of intentions to consider how intelligent agents should reason with them. <p> Dudley's first postulate that an intention leads to a belief in its success is well received in much of AI and Philosophy [12, p. 102], [14, p. 43], <ref> [3, 7, 19] </ref> and [2, p. 37] (who allows an "expectation," rather than a full belief). However, it can be safely weakened to allow a less confident agent than Dudley.
Reference: [4] <author> E. A. Emerson. </author> <title> Temporal and modal logic. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume B. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: As explained in x3, in the model, probability is assigned to scenarios directly. 5 The Formal Language The formal language of this paper, L, is CTL* (a propositional branching time logic <ref> [4] </ref>), augmented with quantification over basic actions; a function, Prob; and predicates, Believes, Intends, Acts-for and Performs (as explained in x4). I also assume that all the arithmetic 6 needed is available. <p> The satisfaction conditions for the temporal operators too are adapted from those in <ref> [4] </ref>. Formally, we have the following definitions: 1. M j= w;t iff hw; ti 2 [[ ]] 2. M j= w;t p ^ q iff M j= w;t p ^ M j= w;t q 4.
Reference: [5] <author> E. Allen Emerson and Joseph Y. Halpern. </author> <title> "Sometimes" and "Not Never" Revisited: On Branching versus Linear Time Temporal Logic. </title> <journal> Journal of the ACM, </journal> <volume> 33(1) </volume> <pages> 151-178, </pages> <year> 1986. </year>
Reference-contexts: I also assume that all the arithmetic 6 needed is available. CTL* is a logic that subsumes all propositional temporal logics <ref> [5] </ref>, and so is ideal for our purposes in representing temporal knowledge|it allows all the qualitative statements we may wish to make. Standard CTL* (and, therefore, L) are non-reified|this helps us make more natural claims about the past and future along scenarios.
Reference: [6] <author> Michael P. Georgeff. </author> <title> Planning. </title> <editor> In J. F. Traub, editor, </editor> <booktitle> Annual Review of Computer Science, </booktitle> <volume> Vol 2. </volume> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Intentions and reasoning about intentions are a crucial part of many important subareas of AI|e.g., planning, plan recognition, situated action, and natural language understanding <ref> [6, 14] </ref>. Recently, there has been some interest in the formalization of the semantics of intentions|i.e., of the conditions under which an agent may or may not be said to have a certain intention [3, 12, 19, 18]. <p> The action an agent chooses to perform depends on the other intentions and beliefs he may have|the details of this are the concern of the planning community <ref> [6] </ref>, and are not focused on here. Acting for an intention is a cognitive concept|it depends on the agent's internal state, rather than on the world. <p> This would hold if, e.g., the agent had a currently adopted plan corresponding to his intention. This postulate is apparent in the suggestion of Georgeff that intentions be treated as "adopted goals" <ref> [6] </ref>, and is sufficiently strong to capture the relationship between intentions and actions, e.g., to make Dudley do something to save Nell's life. 4. Intentions are "adopted goals;" i.e., they must be acted for immediately.
Reference: [7] <author> H. Paul Grice. </author> <title> Intentions and uncertainty. </title> <booktitle> In Proceedings of the British Academy, </booktitle> <pages> pages 263-279, </pages> <year> 1971. </year>
Reference-contexts: Dudley's first postulate that an intention leads to a belief in its success is well received in much of AI and Philosophy [12, p. 102], [14, p. 43], <ref> [3, 7, 19] </ref> and [2, p. 37] (who allows an "expectation," rather than a full belief). However, it can be safely weakened to allow a less confident agent than Dudley.
Reference: [8] <author> Peter Haddawy. </author> <title> Time, chance, and action. </title> <booktitle> In Sixth Conference on Uncertainty in AI, </booktitle> <year> 1990. </year>
Reference-contexts: Since I propose a probabilistic solution to the problems of x2, the model should also include some considerations of probability. Recently, a model for time, chance and action was proposed by Haddawy <ref> [8] </ref>, who adapted it from the models that philosophers use for the semantics of conditionals|time is considered, but the choices made by an agent are not [20]. Thus this model, while interesting, has some shortcomings that obstruct its application to intentions, and indeed to the process of choosing actions. <p> The sets of times in the history of each world are disjoint. A world and time are a "situation." The construction so far has the same effect as Haddawy's (with respect to branching time), but we get his first two constraints for free <ref> [8, x4] </ref>. <p> Haddawy considers objective probability, but as "purely a function of the state of the world," and not dependent on the agent's internal state <ref> [8, x2] </ref>. This is stronger than van Frassen's requirement that only the total history (as against the current state) determines chance [20, p. 336]. But both have the problem that the agent's internal state is ignored altogether. <p> While this is not crucial for the main claims of this paper, taking probability as applying to scenarios rather than propositions (as in <ref> [8, 11] </ref> [20, p. 334]) has some advantages. For one we can speak directly of the choice of actions by an agent as picking out a class of scenarios for actualization|an agent can control the world to some extent by constraining the scenario that would be actualized in this way. <p> L allows us to distinguish between present, past and future naturally, and to keep temporal considerations separate from probabilistic ones|this contrasts with Haddawy's logic in which past and future cannot be distinguished except by naming specific times, and the probability function incorporates an implicit future tense operator <ref> [8, x4] </ref>. By allowing quantification over scenarios, L also permits rationality postulates on intentions to be stated naturally. Now I define the syntax of L. Given a set of atomic formulae, , L may be defined by the following rules. 1.
Reference: [9] <author> Gilbert Harman. </author> <title> Change in View. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: One idea is to fall back upon the notion of commitment as defined by Harman <ref> [9, p. 94] </ref> and Bratman [2, ch. 2]. An agent has an intention only as long as he is committed to it. As before, let q entail p.
Reference: [10] <author> William L. Harper, Robert Stalnaker, and Glenn Pearce, </author> <title> editors. IFS: Conditionals, Belief, Decision, Chance, and Time. </title> <address> D. </address> <publisher> Reidel, </publisher> <address> Dordrecht, Netherlands, </address> <year> 1981. </year>
Reference: [11] <author> David Lewis. </author> <title> A subjectivist's guide to objective chance. </title> <booktitle> In [10], </booktitle> <pages> pages 267-297. </pages> <year> 1981. </year>
Reference-contexts: While this is not crucial for the main claims of this paper, taking probability as applying to scenarios rather than propositions (as in <ref> [8, 11] </ref> [20, p. 334]) has some advantages. For one we can speak directly of the choice of actions by an agent as picking out a class of scenarios for actualization|an agent can control the world to some extent by constraining the scenario that would be actualized in this way. <p> These concepts are treated purely qualitatively; they can straightforwardly be analyzed using subjective probabilities as in <ref> [11, 20] </ref> for a more realistic analysis. Performs of an agent x and action a is true over a subscenario on which x does a. Only objective probability is considered here and is treated as a function from propositions to the unit interval, [0 : : : 1].
Reference: [12] <author> Drew McDermott. </author> <title> A temporal logic for reasoning about processes and plans. </title> <journal> Cognitive Science, </journal> <volume> 6(2) </volume> <pages> 101-155, </pages> <year> 1982. </year>
Reference-contexts: Recently, there has been some interest in the formalization of the semantics of intentions|i.e., of the conditions under which an agent may or may not be said to have a certain intention <ref> [3, 12, 19, 18] </ref>. However, before any theory of intentions can be applied to actual AI problems, we need to go beyond the semantics of intentions to consider how intelligent agents should reason with them. <p> This informal discussion is important since it motivates the technical approach of this paper. 2 2.1 Dudley Dolittle One of the oldest problems in this context is due to McDermott <ref> [12, p. 102] </ref>: A movie hero, Dudley, sees his heroine, Nell, lying tied to a railroad track. He figures "Nell is going to be mashed" and decides to save her. <p> Dudley's first postulate that an intention leads to a belief in its success is well received in much of AI and Philosophy <ref> [12, p. 102] </ref>, [14, p. 43], [3, 7, 19] and [2, p. 37] (who allows an "expectation," rather than a full belief). However, it can be safely weakened to allow a less confident agent than Dudley.
Reference: [13] <author> D. F. Pears. </author> <title> Intention and belief. </title> <editor> In Bruce Vermazen and Merrill Hintikka, editors, </editor> <title> Essays on Davidson: Actions and Events. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> UK, </address> <year> 1985. </year> <month> 14 </month>
Reference-contexts: Then if he really intends to achieve p, he should not intend to achieve it! This paradox is adapted from Pears, and arises with any simple formalization of intentions <ref> [13] </ref>. The problem here is that we want Dudley and Les to act on their intentions, but do not want Ken to act on his, at least not immediately.
Reference: [14] <author> Martha E. Pollack. </author> <title> Inferring Domain Plans in Question Answering. </title> <type> PhD thesis, </type> <institution> Uni--versity of Pennsylvania, </institution> <year> 1986. </year>
Reference-contexts: 1 Introduction Intentions and reasoning about intentions are a crucial part of many important subareas of AI|e.g., planning, plan recognition, situated action, and natural language understanding <ref> [6, 14] </ref>. Recently, there has been some interest in the formalization of the semantics of intentions|i.e., of the conditions under which an agent may or may not be said to have a certain intention [3, 12, 19, 18]. <p> Dudley's first postulate that an intention leads to a belief in its success is well received in much of AI and Philosophy [12, p. 102], <ref> [14, p. 43] </ref>, [3, 7, 19] and [2, p. 37] (who allows an "expectation," rather than a full belief). However, it can be safely weakened to allow a less confident agent than Dudley.
Reference: [15] <author> Munindar P. Singh. </author> <title> Intentions, commitments and rationality. </title> <booktitle> In 13th Annual Conference of the Cognitive Science Society, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: One could, of course, go all the way and bring in the concept of expected utilities. A solution based on expected utilities would work also, and might be preferable in cases where they are needed for other purposes anyway, e.g., in the approach used in <ref> [15] </ref> to formalize commitments, and in [17] to formalize precommit-ments. However, the key intuitions that I wish to exploit for the arguments of this paper are all statable in a simple probabilistic framework. <p> Such a move would be necessary when one tries to give a more concrete account of how limited rational agents adopt and drop their intentions <ref> [15, 17] </ref>. Postulates for planning may also be stated in the same general scheme|not only would such postulates be a step toward relating rationality with planning, they would also provide a sound basis for agents to infer the plans of others.
Reference: [16] <author> Munindar P. Singh. </author> <title> A logic of situated know-how. </title> <booktitle> In National Conference on Artificial Intelligence (AAAI), </booktitle> <month> July </month> <year> 1991. </year>
Reference: [17] <author> Munindar P. Singh. </author> <title> On the commitments and precommitments of limited agents. </title> <booktitle> In IJCAI Workshop on the Theoretical and Practical Design of Rational Agents, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: A solution based on expected utilities would work also, and might be preferable in cases where they are needed for other purposes anyway, e.g., in the approach used in [15] to formalize commitments, and in <ref> [17] </ref> to formalize precommit-ments. However, the key intuitions that I wish to exploit for the arguments of this paper are all statable in a simple probabilistic framework. <p> Such a move would be necessary when one tries to give a more concrete account of how limited rational agents adopt and drop their intentions <ref> [15, 17] </ref>. Postulates for planning may also be stated in the same general scheme|not only would such postulates be a step toward relating rationality with planning, they would also provide a sound basis for agents to infer the plans of others.
Reference: [18] <author> Munindar P. Singh. </author> <title> Multiagent Systems: A Theoretical Framework for Intentions, Know-How, and Communications. </title> <publisher> Springer Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1993, </year> <note> to appear. </note>
Reference-contexts: Recently, there has been some interest in the formalization of the semantics of intentions|i.e., of the conditions under which an agent may or may not be said to have a certain intention <ref> [3, 12, 19, 18] </ref>. However, before any theory of intentions can be applied to actual AI problems, we need to go beyond the semantics of intentions to consider how intelligent agents should reason with them.
Reference: [19] <author> Munindar P. Singh and Nicholas M. Asher. </author> <title> A logic of intentions and beliefs. </title> <journal> Journal of Philosophical Logic, </journal> <note> 1992. In press. </note>
Reference-contexts: Recently, there has been some interest in the formalization of the semantics of intentions|i.e., of the conditions under which an agent may or may not be said to have a certain intention <ref> [3, 12, 19, 18] </ref>. However, before any theory of intentions can be applied to actual AI problems, we need to go beyond the semantics of intentions to consider how intelligent agents should reason with them. <p> Dudley's first postulate that an intention leads to a belief in its success is well received in much of AI and Philosophy [12, p. 102], [14, p. 43], <ref> [3, 7, 19] </ref> and [2, p. 37] (who allows an "expectation," rather than a full belief). However, it can be safely weakened to allow a less confident agent than Dudley.
Reference: [20] <author> Bas C. van Frassen. </author> <title> A temporal framework for conditionals and chance. </title> <booktitle> In [10], </booktitle> <pages> pages 323-340. </pages> <year> 1981. </year>
Reference-contexts: Recently, a model for time, chance and action was proposed by Haddawy [8], who adapted it from the models that philosophers use for the semantics of conditionals|time is considered, but the choices made by an agent are not <ref> [20] </ref>. Thus this model, while interesting, has some shortcomings that obstruct its application to intentions, and indeed to the process of choosing actions. The model I propose here is based on possible worlds. As diagramed in Figure 1, each possible world has a branching history of times. <p> Haddawy considers objective probability, but as "purely a function of the state of the world," and not dependent on the agent's internal state [8, x2]. This is stronger than van Frassen's requirement that only the total history (as against the current state) determines chance <ref> [20, p. 336] </ref>. But both have the problem that the agent's internal state is ignored altogether. <p> While this is not crucial for the main claims of this paper, taking probability as applying to scenarios rather than propositions (as in [8, 11] <ref> [20, p. 334] </ref>) has some advantages. For one we can speak directly of the choice of actions by an agent as picking out a class of scenarios for actualization|an agent can control the world to some extent by constraining the scenario that would be actualized in this way. <p> These concepts are treated purely qualitatively; they can straightforwardly be analyzed using subjective probabilities as in <ref> [11, 20] </ref> for a more realistic analysis. Performs of an agent x and action a is true over a subscenario on which x does a. Only objective probability is considered here and is treated as a function from propositions to the unit interval, [0 : : : 1].
References-found: 20

