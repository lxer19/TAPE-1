URL: http://www.aic.nrl.navy.mil/~gordon/papers/msl96.ps
Refering-URL: http://www.aic.nrl.navy.mil/~gordon/pubs.html
Root-URL: 
Email: gordon@aic.nrl.navy.mil  devika@cs.rice.edu  
Title: A Comparison of Action Selection Learning Methods  
Author: Diana F. Gordon Devika Subramanian 
Address: Code 5510 4555 Overlook Avenue, S.W. Washington, D.C. 20375  Houston, TX 77005  
Affiliation: Naval Research Laboratory,  Department of Computer Science Rice University  
Abstract: Our goal is to develop a hybrid cognitive model of how humans acquire skills on complex cognitive tasks. We are pursuing this goal by designing hybrid computational architectures for the NRL Navigation task, which requires competent senso-rimotor coordination. In this paper, we empirically compare two methods for control knowledge acquisition (reinforcement learning and a novel variant of action models), as well as a hybrid of these methods, with human learning on this task. Our results indicate that the performance of our action models approach more closely approximates the rate of human learning on the task than does reinforcement learning or the hybrid. We also experimentally explore the impact of background knowledge on system performance. By adding knowledge used by the action models system to the benchmark reinforcement learner, we elevate its performance above that of the action models system. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Arbib, M.A. </author> <year> 1972. </year> <title> The Metaphorical Brain. </title> <publisher> NY: Wi-ley and Sons Publishers. </publisher>
Reference-contexts: By far the most widely used machine learning method for tasks like ours is reinforcement learning. Reinforcement learning is mathematically sufficient for learning policies for our task, yet has no explicit world model. More common in the cognitive science literature are action models, e.g., <ref> (Arbib, 1972) </ref>, which require building explicit representations of the dynamics of the world to choose actions.
Reference: <author> Breiman, L., Friedman, J.H. Olshen, R.A.. & Stone, C.J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <address> Bel-mont, CA: </address> <publisher> Wadsworth International Group Publishers. </publisher>
Reference: <author> Chapman, D. & Kaelbling, L. </author> <year> 1991. </year> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 726-731). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Drescher, G.L. </author> <year> 1991. </year> <title> Made-Up Minds. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Gordon, D., Schultz, A., Grefenstette, J., Ballas, J., & Perez, M. </author> <year> 1994. </year> <title> User's Guide to the Navigation and Collision Avoidance Task. </title> <institution> Naval Research Laboratory Technical Report AIC-94-013. </institution>
Reference-contexts: Data from Human Subjects In the experiments with humans, seven subjects were used, and each ran for two or three 45-minute sessions with the simulations. We instrumented 1 the simulation to gather execution traces for subsequent analysis <ref> (Gordon et al., 1994) </ref>.
Reference: <author> Gordon, D. & Subramanian, D. </author> <year> 1993. </year> <title> A Multistrat-egy Learning Scheme for Agent Knowledge Acquisition. </title> <journal> Informatica, </journal> <volume> 17, </volume> <pages> 331-346. </pages>
Reference: <author> Jordan, M.I. & Rumelhart, D.E. </author> <year> 1992. </year> <title> Forward models: supervised learning with a distal teacher. </title> <journal> Cognitive Science, </journal> <volume> 16, </volume> <pages> 307-354. </pages>
Reference: <author> Lin, L. </author> <year> 1992. </year> <title> Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 293-321. </pages>
Reference-contexts: While q-learning with explicit state representations addresses the temporal credit assignment problem, it is standard practice to use input generalization and neural networks to also address the structural credit assignment problem, e.g., <ref> (Lin, 1992) </ref>. <p> The result is improved q-values at the output nodes. We selected q-learning as a benchmark algorithm with which to compare because the literature reports a wide range of successes with this algorithm, including on tasks with aspects similar to the NRL Navigation task, e.g., see <ref> (Lin, 1992) </ref>. Our implementation uses standard q-learning with neural networks. One network corresponds to each action (i.e., there are three turn networks corresponding to turn left, turn right, and go straight; speed is fixed at a level frequently found in the human execution traces, i.e., 20/40).
Reference: <author> Mahadevan, S. </author> <year> 1992. </year> <title> Enhancing transfer in reinforcement learning by building stochastic models of robot actions. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 290-299). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Moore, A. </author> <year> 1992. </year> <title> Fast, robust adaptive control by learning only forward models. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> 4, </volume> <pages> (pp. 571-578). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Arbib (1972) and Drescher (1991) provide examples in the psychological literature, STRIPS (Nilsson, 1980) is a classic example in the AI literature, and Sutton uses them in DYNA (Sutton, 1988). The learning of action models has been studied in the neural networks <ref> (Moore, 1992) </ref>, machine learning (Sutton, 1990; Ma-hadevan, 1992), and cognitive science (Munro, 1987; Jordan & Rumelhart, 1992) communities. Our algorithm uses two functions: M : sensors fi actions ! sensors P : sensors ! &lt; M is an action model, which our method represents as a decision tree.
Reference: <author> Munro, P. </author> <year> 1987. </year> <title> A dual back-propagation scheme for scalar reward learning. </title> <booktitle> In Proceedings of the Ninth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 165-176). </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Nilsson, N. </author> <year> 1980. </year> <booktitle> Principles of Artificial Intelligence. </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga Publishing Company. </publisher>
Reference-contexts: Action models (i.e., forward models) have appeared in multidisciplinary sources in the literature. Arbib (1972) and Drescher (1991) provide examples in the psychological literature, STRIPS <ref> (Nilsson, 1980) </ref> is a classic example in the AI literature, and Sutton uses them in DYNA (Sutton, 1988). The learning of action models has been studied in the neural networks (Moore, 1992), machine learning (Sutton, 1990; Ma-hadevan, 1992), and cognitive science (Munro, 1987; Jordan & Rumelhart, 1992) communities.
Reference: <author> Quinlan, J.R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-107. </pages>
Reference-contexts: Our algorithm uses two functions: M : sensors fi actions ! sensors P : sensors ! &lt; M is an action model, which our method represents as a decision tree. The decision trees are learned using Quinlan's C4.5 system <ref> (Quinlan, 1986) </ref>. 4 P rates the 4 We are not claiming humans use decision trees for action models; however, we use this implementation because it appears to have a computational speed that is needed for desirability of various sensor configurations. P embod-ies background (relevance) knowledge about the task.
Reference: <author> Rissanen, J. </author> <year> 1983. </year> <title> Minimum Description Length Principle. </title> <type> Report RJ 4131 (45769), </type> <institution> IBM Research Laboratory, </institution> <address> San Jose. </address>
Reference: <author> Rumelhart, D.E. & McClelland, J.L. </author> <year> 1986. </year> <title> Parallel Distributed Processing : Explorations in the Mi-crostructure of Cognition. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Skinner, B.F. </author> <year> 1984. </year> <title> Selection by consequences. </title> <journal> The Behavior and Brain Sciences, </journal> <volume> 7, </volume> <pages> 477-510. </pages>
Reference-contexts: More common in the cognitive science literature are action models, e.g., (Arbib, 1972), which require building explicit representations of the dynamics of the world to choose actions. Reinforcement learning Reinforcement learning has been studied extensively in the psychological literature, e.g., <ref> (Skinner, 1984) </ref>, and has recently become very popular in the machine learning literature, e.g., (Sutton, 1988; Lin, 1992; Gordon & Subramanian, 1993).
Reference: <author> Sutton, R. </author> <year> 1988. </year> <title> Learning to Predict by the Meth--ods of Temporal Differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: Action models (i.e., forward models) have appeared in multidisciplinary sources in the literature. Arbib (1972) and Drescher (1991) provide examples in the psychological literature, STRIPS (Nilsson, 1980) is a classic example in the AI literature, and Sutton uses them in DYNA <ref> (Sutton, 1988) </ref>. The learning of action models has been studied in the neural networks (Moore, 1992), machine learning (Sutton, 1990; Ma-hadevan, 1992), and cognitive science (Munro, 1987; Jordan & Rumelhart, 1992) communities.
Reference: <author> Sutton, R. </author> <year> 1990. </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 216-224). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Watkins, C. </author> <year> 1989. </year> <title> Learning from Delayed Rewards. Doctoral dissertation. </title> <address> Cambridge, England: Cam-bridge University. </address>
Reference-contexts: Reinforcement learning provides a method for modeling the acquisition of the policy function: F : sensors ! actions Currently, the most popular type of reinforcement learning is q-learning, developed by Watkins, which is based on ideas from temporal difference learning, as well as conventional dynamic programming <ref> (Watkins, 1989) </ref>. It requires estimating the q-value of a sensor configuration s, i.e., q (s; a) is a prediction of the utility of taking action a in a world state represented by s. The q-values are updated during learning based on minimizing a temporal difference error.
References-found: 19

