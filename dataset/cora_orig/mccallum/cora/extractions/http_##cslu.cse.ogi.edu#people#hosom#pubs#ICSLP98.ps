URL: http://cslu.cse.ogi.edu/people/hosom/pubs/ICSLP98.ps
Refering-URL: http://cslu.cse.ogi.edu/people/hosom/
Root-URL: http://www.cse.ogi.edu
Email: email: -hosom,cole-@cse.ogi.edu  email: cosi@csrf.pd.cnr.it  
Title: EVALUATION AND INTEGRATION OF NEURAL-NETWORK TRAINING TECHNIQUES FOR CONTINUOUS DIGIT RECOGNITION  for Spoken Language Understanding (CSLU)  
Author: John-Paul Hosom*, Ronald A. Cole*, and Piero Cosi** C. N. R. Via G. Anghinoni, 
Note: Institute of Phonetics  
Web: www: http://www.cse.ogi.edu/CSLU  www: http://www.csrf.pd.cnr.it  
Address: P.O. Box 91000, Portland Oregon 97291-1000 USA  10 35121 Padova ITALY  
Affiliation: Center  Oregon Graduate Institute of Science and Technology (OGI)  
Abstract: This paper describes a set of experiments on neural-network training and search techniques that, when combined, have resulted in a 54% reduction in error on the continuous digits recognition task. The best system had word-level accuracy of 97.52% on a test set of the OGI 30K Numbers corpus, which contains naturally-produced continuous digit strings recorded over telephone channels. Experiments investigated effects of the feature set, the amount of data used for training, the type of context-dependent categories to be recognized, the values for duration limits, and the type of grammar. The experiments indicate that the grammar and duration limits had a greater effect on recognition accuracy than the output categories, cepstral features, or a 50% increase in the amount of training data. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Cosi, P., Hosom, J.P., Shalkwyk, J., Sutton, S., and Cole, R.A., </author> <title> Connected Digit Recognition Experiments with the OGI Toolkits Neural Network and HMM Based Recognizers, </title> <address> IVTTA-ETWR-98, Turin, </address> <month> Sep. </month> <year> 1998, </year> <note> accepted for publication. </note>
Reference-contexts: 1. INTRODUCTION The recognizers in the CSLU Toolkit use a hybrid HMM/ANN framework <ref> [1] </ref>. In these systems, frame-based recognition is done with context-dependent sub-phonetic states, where the state probability estimation is computed using a neural network. We have developed a set of procedures within the Toolkit for training special-purpose recognizers for tasks such as continuous digit recognition. <p> This method is simple enough that a bright highschool student can complete the tutorial in a few days. On the continuous digits task, the training procedure yields recognition results that compare favorably to standard HMM systems <ref> [1] </ref>. This paper shows how competitive performance was achieved by optimizing several of the parameters used in training and incorporating new training techniques. 2. CORPUS The OGI 30K Numbers corpus [2] was used for training, development, and testing.
Reference: 2. <author> Cole, R.A., Fanty, M., Noel, M., and Lander, T., </author> <title> Telephone Speech Corpus Development at CSLU, </title> <address> ICSLP-94, Yokohama, </address> <month> September </month> <year> 1994, </year> <pages> pp. 1815-1818. </pages>
Reference-contexts: On the continuous digits task, the training procedure yields recognition results that compare favorably to standard HMM systems [1]. This paper shows how competitive performance was achieved by optimizing several of the parameters used in training and incorporating new training techniques. 2. CORPUS The OGI 30K Numbers corpus <ref> [2] </ref> was used for training, development, and testing. The data in this corpus were collected from thousands of people within the United States who recited their telephone number, street address, zip code, or other numeric information over the telephone in a natural speaking style.
Reference: 3. <author> Wei, W. and Van Vuuren, S., </author> <title> Improved Neural Network Training of Inter-Word Context Units for Connected Digit Recognition, </title> <journal> ICASSP-98, </journal> <volume> vol. </volume> <pages> 1, </pages> <address> Seattle, </address> <month> May </month> <year> 1998, </year> <pages> pp. 497-500. </pages>
Reference-contexts: As many as 2000 samples per category are collected for training. Neural-network training is done with standard backpropagation on a fully-connected feed-forward network. The training is adjusted to use the negative penalty modification proposed by Wei and van Vuuren <ref> [3] </ref>.
Reference: 4. <author> Boite, J.M., Bourlard, H., Dhoore, B., and Haesen, </author> <note> M., </note>
Reference-contexts: The garbage word is defined as a word with a single context-independent category; the value of this category is not an output of the neural network, but is computed as the N th highest output from the neural network at each frame <ref> [4] </ref>. In this study, N was set to 5. Training is done for 30 iterations, and the best network iteration is determined by word-level evaluation of each iteration on the development set data.
References-found: 4

