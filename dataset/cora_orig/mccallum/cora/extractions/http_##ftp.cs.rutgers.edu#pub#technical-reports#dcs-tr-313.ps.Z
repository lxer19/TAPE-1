URL: http://ftp.cs.rutgers.edu/pub/technical-reports/dcs-tr-313.ps.Z
Refering-URL: http://www.cs.rutgers.edu/~hristesc/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Author: Gabriela Hristescu 
Note: 1 Presented as essay for MS degree in Computer Science at Rutgers University. This work was mainly done as part of the Fall 93 CS605 course at Rutgers University. It was also extended during Spring 94 COS451 project work at Princeton University.  
Date: October 1994  
Affiliation: Rutgers University Department of Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Selim G. Akl and Kelly A. Lyons, </author> <title> Parallel Computational Geometry, </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: The Delaunay triangulation guarantees that the minimum angle of its triangles is maximum over all triangulations, which is a very useful property. In what follows we will focus only on methods that produce a triangulation rather than a "good" one. Merks's Algorithm <ref> [1] </ref>: In the first step, the convex hull of a set of n points S is computed in O (logn) time using O (n) processors using an algorithm from [17]. <p> Merging the triangulated subsequences in O (logn) time with O (n) processors is accomplished using a data structure similar to a segment tree. The entire algorithm takes O (logn) time with O (n) processors on a CREW PRAM. Wang and Tsin's Algorithm <ref> [1] </ref>: In this case, the set of n points is partitioned into n 1=2 subsets of size n 1=2 each. The problem is solved recursively on each subset and during the algorithm, the convex hull of each of the subsets is created.
Reference: [2] <author> F.P. Preparata and M. I. Shamos, </author> <title> Computational Geometry: An Introduction, </title> <publisher> Springer Verlag 1985. </publisher>
Reference-contexts: Many algorithms for computing the Convex Hull of n points in the plane have been designed, to mention some of them <ref> [2] </ref>: Jarvis's march running in O (n fl h) where h is number of hull edges, thus having a worst case complexity of O (n 2 ); the Incremental algorithm based on a presorting of the points by x-coordinate followed by a processing scan, thus running in O (Sort (n))+O (n)
Reference: [3] <author> C. A. Wang and Y. H. Tsin, </author> <title> An O(log n) time parallel algorithm for triangulating a set of points in the plane, </title> <journal> Information Processing Letters, </journal> <volume> Vol 25, </volume> <year> 1987, </year> <pages> 55-60. </pages>
Reference-contexts: It is based on reducing the problem of triangulating a set of points in the convex hull of S to triangulating points inside triangles. An algorithm by Wang and Tsin <ref> [3] </ref> achieves the same running time, using also a multiway divide and conquer, but with a different decomposition of the problem. They partition the set of points into subsets, triangulating them within the convex hulls and funnel polygons which are 3 formed. <p> In what follows, we present a step by step description of the parallel triangulation scheme, based on the approach outlined by Wang and Tsin <ref> [3] </ref>.
Reference: [4] <author> E. Merks, </author> <title> An optimal parallel algorithm for triangulating a set of points in the plane, </title> <journal> International Journal of Parallel Programming, </journal> <volume> Vol 15, No. 5, </volume> <year> 1986, </year> <pages> 399-411. </pages>
Reference-contexts: ElGindy [21] has formulated an algorithm, which can triangulate a planar set of points in O (log 2 n) time using O (n=logn) processors, thereby achieving linear speedup. Merks <ref> [4] </ref> proposed an algorithm which triangulates a set of n points S in O (logn) time using O (n) processors and O (n) space.
Reference: [5] <author> P.D. MacKenzie and Q.F. Stout, </author> <title> Asymptotically efficient hypercube algorithms for computational geometry, </title> <booktitle> Proceedings of the Third Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, Maryland, </address> <month> October </month> <year> 1990, </year> <pages> 8-11. </pages>
Reference-contexts: This algorithm was adapted to run on an O (n)-processor hypercube by MacKenzie and Stout <ref> [5] </ref> by dividing the set of points in n 1=4 subsets of size n 3=4 each. At each stage of the recursion 8 O (Sort (n; n)) time is used, where Sort (n,n) is the time needed to sort n numbers on an O (n)- processor hypercube.
Reference: [6] <author> F. Dehne, A. Fabri and A. Rau-Chaplin, </author> <title> Scalable Parallel Geometric Algorithms for Coarse Grained Multicomputers, </title> <booktitle> Proceedings of the ACM Symposium on Computational Geometry, </booktitle> <year> 1993. </year>
Reference-contexts: Therefore, there is a tremendous need to develop algorithms for those theoretical models of parallel computation, which can be easily mapped on the existing large scale parallel architectures. Some progress in this direction has been done by Dehne, Fabri and Rau-Chaplin <ref> [6] </ref> who have studied scalable parallel computational geometry algorithms for the Coarse Grained Multicom-puter model (CGM). <p> We also analyze a distributed all-tangent computation which we implemented in O (plog (n=p)). For the funnel polygon triangulation we proposed both a suboptimal straightforward solution and an optimal one based on global sorting suggested in the model proposed by Dehne, Fabri and Rau-Chaplin <ref> [6] </ref>. In Section 4 we make further considerations related to the performance analysis of the implementation we made. Data sets for both expected average and worst case are provided and timed. <p> Thus, this step takes time: T= C (n/p). T CH = 2*C (n/p)+Sort (n/p,1). 3.3 All-Tangents Parallel Algorithm The All-Tangents parallel algorithm was implemented by parallelizing the sequential algorithm of Mehlhorn [9] using the scheme suggested by Dehne et al. <ref> [6] </ref>.
Reference: [7] <author> A. </author> <title> Rau-Chaplin On Parallel Data Structures and Parallel Geometric Applications for Mul-ticomputers, </title> <type> Ph.D. Thesis, </type> <institution> Carleton Univ., </institution> <address> Ottawa, Canada, </address> <month> November </month> <year> 1992. </year>
Reference: [8] <author> G.E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton S. J. Smith and M. Zhanga, </author> <title> A Comparison of Sorting Algorithms for the Connection Machine CM-2. </title>
Reference-contexts: This sample is sorted and the p-1 splitters are selected by including those keys in the sample that have ranks s, 2*s, 3*s... (p-1)*s. Some sample sort algorithms reported in literature <ref> [8] </ref> choose an oversampling ratio of s=1, which results in a relatively large deviation of the bucket sizes. If s&gt;1 is chosen as suggested [8], it is guaranteed with high probability that no bucket contains many more keys than the average. <p> Some sample sort algorithms reported in literature <ref> [8] </ref> choose an oversampling ratio of s=1, which results in a relatively large deviation of the bucket sizes. If s&gt;1 is chosen as suggested [8], it is guaranteed with high probability that no bucket contains many more keys than the average. The time for the third phase of the algorithms depends highly on the maximum numbers (L) of keys in a single bucket. <p> The expected value of the bucket expansion depends on the oversampling ratio s and on the total number of processors p. It is extremely unlikely that the bucket expansion will be significantly worse than expected. In <ref> [8] </ref> it is showed that the probability that the bucket expansion is greater than some factor ff &gt; 1 is P r [L &gt; ff (n=p)] n fl e 1 (11=ff) 2 ffs=2 . <p> The running time of the sample sort depends linearly on both the oversampling ratio and the bucket expansion. In practice, oversampling ratios of s=32 or s=64 yield bucket expansion of less than 2 <ref> [8] </ref>. 2.3 Computing All-Tangents The input of this phase consists of a collection of convex polygons, CH i , each of them residing at a different processor, where CH i is to the left of CH j , if i&lt;j. <p> Phase 1: Selecting the splitters Step 1.1 Each processor selects a set of s keys ( s being the oversampling ratio) from among those stored locally in its memory. This can be done randomly, but in our implementation one every (n/p)/s key is selected. Blelloch et al. <ref> [8] </ref> propose as typical values for s 32 or 64 for data sizes of order 10 6 , but it must be tunned depending on the number of keys per processor (n/p). We evaluated the bucket expansion as a function of oversampling ratio for different numbers of keys.
Reference: [9] <author> K. Mehlhorn, </author> <title> Data structures and algorithms 3: multidimensional searching and computational geometry, </title> <note> in EATCS Monographs in Theoretical Computer Science, </note> <editor> W. Brauer, G. Rozenberg and A. Salomaa (Editors), </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: The main idea is to initiate a distributed computation for each tangent between any pair of processors for both upper and lower tangents. The core algorithm is an adaptation of the solution described by Mehlhorn <ref> [9] </ref>. 13 A and B. To identify the upper tangent rq, the points r 2 Upper (A) and q 2 Upper (B) must be computed (r has to be on the upper chain of A, between the leftmost and the rightmost points of A). <p> The oriented line r i q j can either touch, enter or leave A and B. According to its relative position, we can distinguish 9 cases. All-Tangents Algorithm <ref> [9] </ref>: Case 1: r i q j touches in r i and q j . The line is tangent to A and B and the algorithm completes with h=i and k=j. Case 2: r i q j touches in r i and enters in q j . <p> Thus, this step takes time: T= C (n/p). T CH = 2*C (n/p)+Sort (n/p,1). 3.3 All-Tangents Parallel Algorithm The All-Tangents parallel algorithm was implemented by parallelizing the sequential algorithm of Mehlhorn <ref> [9] </ref> using the scheme suggested by Dehne et al. [6].
Reference: [10] <author> R. L. Graham, </author> <title> An efficient algorithm for determining the convex hull of a finite planar set, </title> <journal> Information Processing Letters, </journal> <volume> Vol 1,1972, </volume> <pages> 132-133. </pages>
Reference-contexts: Thus: T Sort =Sort (p*s,p)+(p-1)*R (p-1)+(n/p)*C (log p)+((n/p)/k)*R (k)+Sort ((n/p)fi (s; n),1) 3.2 Algorithms for Convex Hull and the Triangulation of Local Points At each processor, the computation of the Convex Hull of its n/p local points is performed sequentially using Graham's Scan Algorithm <ref> [10] </ref>: Step 1. The rightmost lowest point P 0 is found in C (n/p) time and considered as origin. Step 2. It then sorts the remaining n/p-1 points angularly about P 0 breaking ties in favor of the one closest to this origin.
Reference: [11] <author> R. Miller and Q. F. Stout, </author> <title> Efficient parallel convex hull algorithms, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-37, No. 12, </volume> <month> December </month> <year> 1988, </year> <pages> 1605-1618. </pages>
Reference: [12] <author> R. Miller and Q. F. Stout, </author> <title> Mesh computer algorithms for computational geometry, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol C-38, No. 3, </volume> <month> March </month> <year> 1989, </year> <pages> 321-340. </pages>
Reference-contexts: Processors are interconnected through a high speed network supporting message-based communication (Intel Paragon, Intel iPSC/860, CM-5). Although any real machine can be used to simulate the PRAM model, it is nevertheless true that algorithms designed for network-based models will better match such architectures. Miller and Stout <ref> [12] </ref> have proposed parallel algorithms of cost O (n 1=2 ) for some formal geometric problems, running on a mesh computer of size n. <p> However, for parallel geometric algorithms to be relevant in practice, such algorithms must be scalable, that is, they must be applicable and efficient for a wide range of ratios n/p. Miller and Stout <ref> [12] </ref> tried to extend the applicability of their results using a mapping method to simulate a mesh of size C*n on a mesh of size n and argue that these will not affect the asymptotic running time of the algorithms.
Reference: [13] <author> J. O'Rourke, </author> <title> Computational Geometry in C, </title> <publisher> Cambridge University Press, draft, </publisher> <year> 1993. </year> <month> 39 </month>
Reference: [14] <author> A. Aggarwal, B. Chazelle, L. J. Guibas, C. O'Dunlaing and C. K. Yap, </author> <title> Parallel computa-tional geometry, </title> <journal> Algorithmica, </journal> <volume> Vol 3, </volume> <year> 1988, </year> <pages> 293-327. </pages>
Reference: [15] <author> R. Cypher and C. G. Plaxton, </author> <title> Deterministic sorting in nearly logarithmic time on the hypercube and related computers, </title> <booktitle> Proceedings of the Twenty-Second Annual ACM Symposium on Theory of Computation, </booktitle> <address> Baltimore, </address> <month> May </month> <year> 1990, </year> <pages> 193-203. </pages>
Reference: [16] <author> X. Guan and M. A. Langston, </author> <title> Time-Space optimal parallel merging and sorting, </title> <booktitle> International Conference on Parallel Processing, </booktitle> <year> 1989. </year>
Reference: [17] <author> M. J. Atallah and M. T. Goodrich, </author> <title> Efficient parallel solutions to some geometric problems, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol 3, </volume> <year> 1986, </year> <pages> 492-507. </pages>
Reference-contexts: In what follows we will focus only on methods that produce a triangulation rather than a "good" one. Merks's Algorithm [1]: In the first step, the convex hull of a set of n points S is computed in O (logn) time using O (n) processors using an algorithm from <ref> [17] </ref>. After the rightmost lowest point O is found and the rest of the points p i are sorted by angle i that p i makes with O in the positive x-direction. This sorted sequence is split by lines through the extreme points of S and O.
Reference: [18] <author> R. Cole and M. T. Goodrich, </author> <title> Optimal Parallel Algorithms for Point-Set and Polygon Problems, </title> <journal> Algorithmica, Springer-Verlag, </journal> <volume> Vol. 7, </volume> <year> 1992, </year> <pages> 3-23. </pages>
Reference: [19] <author> R. Cypher and J. L. S. Sanz, </author> <title> Optimal Sorting on reduced architectures. </title>
Reference: [20] <author> M. Goodrich and M. Atallah, </author> <title> Efficient Plane Sweep in Parallel, </title> <booktitle> Proc. of the Second Symp. on Computational Geometry, </booktitle> <address> York Heights,(June 1985). </address>
Reference-contexts: Goodrich and Atallah <ref> [20] </ref> have devised a parallel analog of the plane sweep that can triangulate a planar set of points in O (lognloglogn) time using O (n) processors and O (n) space.
Reference: [21] <author> H. ElGindy, </author> <title> An Optimal Parallel Algorithm for Triangulating Simplicial Points Sets in Space, </title> <type> Internal Report, </type> <institution> University of Pennsylvania (April 1986). </institution>
Reference-contexts: Goodrich and Atallah [20] have devised a parallel analog of the plane sweep that can triangulate a planar set of points in O (lognloglogn) time using O (n) processors and O (n) space. ElGindy <ref> [21] </ref> has formulated an algorithm, which can triangulate a planar set of points in O (log 2 n) time using O (n=logn) processors, thereby achieving linear speedup.
Reference: [22] <author> A. Yao, </author> <title> A lower bound to finding convex hulls, </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 28 (1981), </volume> <pages> 780-787. 40 </pages>
Reference-contexts: These tasks are performed by parallel algorithms, involving intense inter-processor communication. 2.1 Computing Convex Hull and Inside Triangulation Since the problem of sorting is linear-time reducible to the Convex Hull problem, finding the ordered convex hull of n points in the plane requires (nlogn) time <ref> [22] </ref>.
References-found: 22

