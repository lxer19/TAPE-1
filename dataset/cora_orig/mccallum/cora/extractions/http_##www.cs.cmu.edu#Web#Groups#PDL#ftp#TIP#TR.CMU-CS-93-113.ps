URL: http://www.cs.cmu.edu/Web/Groups/PDL/ftp/TIP/TR.CMU-CS-93-113.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/project/multiC-sys-sw/WWW/publications.html
Root-URL: 
Title: Prefetching  
Author: R. Hugo Patterson*, Garth A. Gibson, M. Satyanarayanan 
Affiliation: *Department of Electrical and Computer Engineering, Carnegie Mellon University  
Note: CMU-CS-93-113 Appeared in ACM Operating Systems Review, V 27(2), April, 1993, pp.  
Date: February 1993  21-34.  
Abstract: School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3890 A Status Report on Research in Transparent Informed Abstract This paper focuses on extending the power of caching and prefetching to reduce file read latencies by exploiting application level hints about future I/O accesses. We argue that systems that disclose high-level knowledge can transfer optimization information across module boundaries in a manner consistent with sound software engineering principles. Such Transparent Informed Prefetching (TIP) systems provide a technique for converting the high throughput of new technologies such as disk arrays and log-structured file systems into low latency for applications. Our preliminary experiments show that even without a high-throughput I/O subsystem TIP yields reduced execution time of up to 30% for applications obtaining data from a remote file server and up to 13% for applications obtaining data from a single local disk. These experiments indicate that greater performance benefits will be available when TIP is integrated with low level resource management policies and highly parallel I/O subsystems such as disk arrays. This material is based (in part) upon work supported by the National Science Foundation under grant number ECD-8907068, the Avionics Laboratory, Wright Research and Development Center, Aeronautical Systems Division (AFSC), U.S. Air Force, Wright-Patterson AFB, Ohio 45433-6543 under Contract F33615-90-C-1465, ARPA Order No. 7597, an IBM Corporation Research Initiation Grant, and a Digital Equipment Corporation External Research Project Grant. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the funding agencies. The government has certain rights in this material. 
Abstract-found: 1
Intro-found: 1
Reference: [Amdahl67] <author> Amdahl, G.M., </author> <title> Validity of the single processor approach to achieving large scale computing capabilities, </title> <booktitle> Proc. AFIPS 1967 Spring Joint Computer Conference, V 30, </booktitle> <address> Atlan-tic City, New Jersey, </address> <month> April </month> <year> 1967, </year> <pages> pp. 483-485. </pages>
Reference-contexts: Portable computers, which frequently spin down their disks and are only weakly connected to networks, suffer additional latencies [Kistler92]. Applications that depend on the massive storage of optical disk jukeboxes or robotic tape libraries must wait while the desired media is fetched and mounted. Amdahls Law <ref> [Amdahl67] </ref> tells us that unless I/O subsystem performance keeps pace with improvements in the rest of the system, I/O will increasingly constrain the performance of the system as a whole. 2.2 Range of Solutions On the bright side, a number of innovations have relieved certain aspects of the I/O bottleneck.
Reference: [Baker91] <author> Baker, M.G., Hartman, J.H., Kupfer, M.D., Shirriff, K.W., and Ousterhout, J.K., </author> <title> Measurements of a Distributed File System, </title> <booktitle> Proc. of the 13th Symp. on Operating System Principles, </booktitle> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991, </year> <pages> pp. 198-212. </pages>
Reference-contexts: For caches to compensate for the growing gap between processor and I/O performance, their miss ratios will have to drop proportionately. Is such improvement likely? Table 2 compares the performance predicted for a variety of operating system file cache sizes in 1985 [Ousterhout85] with that observed in 1991 <ref> [Baker91] </ref> by a group at Berkeley. The first observation, based on the 1985 data, is that increasing the size of an already large cache does not reduce the miss ratio much. <p> Organizing and fetching data in large file blocks [McKusick84] effectively prefetches unrequested Cache Size Miss Ratio 1985 BSD Study 390KB 1MB 2MB 4MB 8MB 16MB 1991 Study 7MB (avg) Table 2. Comparison of caching performance in 1985 and 1991. The numbers in this table are drawn from [Ousterhout85] and <ref> [Baker91] </ref>. The 1985 tracing study of the UNIX 4.2 BSD file system predicted cache performance for a range of cache sizes assuming a 30 second ush back policy for writes. The 1991 study measured cache performance on a number of workstations running Sprite.
Reference: [Cate92] <author> Cate, V., </author> <title> Alex --- A Global Filesystem, </title> <booktitle> Proceedings of the Usenix File Systems Workshop, </booktitle> <address> Ann Arbor, MI, </address> <month> May </month> <year> 1992, </year> <pages> pp. 1-11. </pages>
Reference-contexts: The speed of computations using only primary memory is increasing five to ten times faster than the speed with which blocks on magnetic disk can be accessed [Patterson88, Ousterhout89]. In addition, 2 some new technologies further increase the gap between processor and I/O performance. Distributed and wide area <ref> [Cate92] </ref> file systems slow I/O with substantial network transmission and server delays. Portable computers, which frequently spin down their disks and are only weakly connected to networks, suffer additional latencies [Kistler92].
Reference: [Chou85] <author> Chou, H. T., DeWitt, D. J., </author> <title> An Evaluation of Buffer Management Strategies for Relational Database Systems, </title> <booktitle> Proc. of the 11th Int. Conf. on Very Large Data Bases, </booktitle> <address> Stock-holm, </address> <year> 1985, </year> <pages> pp. 127-141. </pages>
Reference-contexts: Make could give hints about the whole process to a TIP system. Hints for more complex, non-sequential access patterns are also possible. An example from the supercomputer domain is stride access to large matrices [Miller91]. Data base applications often can predict their accesses to satisfy a query <ref> [Chou85, Stonebraker81, Selinger79] </ref>. Interactive applications, too, could provide hints. For example, when a bank customer inserts their card in an automatic teller machine, a hint about the customers identity could be given, and the customers account records retrieved while the customer enters their Personal Identification Number. <p> Finally, TIP uses its knowledge of future I/O requests to improve cache management and reduce cache miss ratios. It is often possible to outperform an LRU page replacement algorithm, even without prefetching <ref> [Chou85, Korner90] </ref>. Unneeded blocks can be released early and needed blocks can be held longer. <p> Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms <ref> [Sacco82, Chou85, Cornell89, Ng91] </ref> and for I/O optimizations [Selinger79]. We hope to extend these techniques to prefetching. Also, our work emphasizes a more solid partitioning of function between application and operating system.
Reference: [Cornell89] <author> Cornell, D. W., Yu, P. S., </author> <title> Integration of Buffer Management and Query Optimization in Relational Database Environment, </title> <booktitle> Proc. of the 15th Int. Conf. on Very Large Data Bases, </booktitle> <address> Amsterdam, </address> <month> Aug. </month> <year> 1989, </year> <pages> pp. 247-255. </pages>
Reference-contexts: Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms <ref> [Sacco82, Chou85, Cornell89, Ng91] </ref> and for I/O optimizations [Selinger79]. We hope to extend these techniques to prefetching. Also, our work emphasizes a more solid partitioning of function between application and operating system.
Reference: [Feiertag71] <author> Feiertag, R. J., Organisk, E. I., </author> <title> The Multics Input/Output System, </title> <booktitle> Proc. of the 3rd Symp. on Operating System Principles, </booktitle> <year> 1971, </year> <pages> pp 35-41. </pages>
Reference-contexts: This result is strong evidence that we cannot rely on increased cache sizes to give us arbitrarily low cache miss ratios. 4 data in the latter portions of the block. More explicitly, the file system can readahead sequential blocks of a file <ref> [Smith85, Feiertag71] </ref>. Recently, other researchers have explored prefetching for more complex access patterns by inferring future requests from a users I/O request stream [Kotz91, Tait91, Palmer91, Korner90]. For well behaved applications, these techniques can effectively prefetch data and reduce cache misses.
Reference: [Gibson91] <author> Gibson, G. A., </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <type> Ph.D. dissertation, </type> <institution> University of California, Berkeley, </institution> <type> technical report UCB/CSD 91/613, </type> <month> April </month> <year> 1991. </year> <note> Available in MIT Press 1991 ACM distinguished dissertation series, </note> <year> 1992. </year>
Reference-contexts: Table 1 summarizes the most commonly used techniques to address the throughput and latency components of both read and write I/O performance. Redundant Arrays of Inexpensive Disks (RAID) take advantage of steadily decreasing disk diameter and cost per byte to address I/O throughput <ref> [Gibson91] </ref>. They provide parallel transfer to speed servicing of large requests. They can also service many small requests simultaneously to provide high service rates for workloads consisting of many concurrent, randomly distributed, small accesses.
Reference: [Grimshaw91] <author> Grimshaw, A.S., Loyot Jr., </author> <title> E.C., ELFS: Object-Oriented Extensible File Systems, </title> <note> 13 Computer Science Report No. </note> <institution> TR-91-14, University of Virginia, </institution> <month> July 8, </month> <year> 1991. </year>
Reference-contexts: But, we carry this approach further and ask for hints from applications themselves to bridge the gap between applications and the operating system. Recently, researchers have proposed an object-oriented file system layered on top of the Unix file system called ELFS <ref> [Grimshaw91] </ref>. ELFS has knowledge of file structure and high-level file operations that allow it to help prefetch and caching operations. However, ELFS emphasizes user control over file activity.
Reference: [Kim86] <author> Kim, M.Y., </author> <title> Synchronized Disk Interleaving, </title> <journal> IEEE Trans. on Computers, V. </journal> <volume> C-35 (11), </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Today, file read latency is the most significant bottleneck for high performance input and output. Other aspects of I/O performance benefit from recent advances in disk bandwidth and throughput resulting from disk arrays <ref> [Kim86, Salem86, Livny87, Patterson88, Reddy89] </ref>, and in write performance derived from buffered write-behind and the Log-structured File System [Rosenblum91]. The development of distributed file systems operating over networks with diverse bandwidth [Spector89, Satya85, Nelson88] only exacerbates the problem.
Reference: [Kistler92] <author> Kistler, J.J., Satyanarayanan, M., </author> <title> Disconnected Operation in the Coda File System, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> V10 (1), </volume> <month> February </month> <year> 1992, </year> <pages> pp. 3-25. </pages>
Reference-contexts: In addition, 2 some new technologies further increase the gap between processor and I/O performance. Distributed and wide area [Cate92] file systems slow I/O with substantial network transmission and server delays. Portable computers, which frequently spin down their disks and are only weakly connected to networks, suffer additional latencies <ref> [Kistler92] </ref>. Applications that depend on the massive storage of optical disk jukeboxes or robotic tape libraries must wait while the desired media is fetched and mounted.
Reference: [Korner90] <author> Korner, K., </author> <title> Intelligent Caching for Remote File Service, </title> <booktitle> Proc. of the Tenth Int. Conf. on Distributed Computing Systems, </booktitle> <year> 1990, </year> <month> pp.220-226. </month>
Reference-contexts: More explicitly, the file system can readahead sequential blocks of a file [Smith85, Feiertag71]. Recently, other researchers have explored prefetching for more complex access patterns by inferring future requests from a users I/O request stream <ref> [Kotz91, Tait91, Palmer91, Korner90] </ref>. For well behaved applications, these techniques can effectively prefetch data and reduce cache misses. However, for the many applications whose access patterns appear random or that touch data only once, such techniques are ineffective. <p> Finally, TIP uses its knowledge of future I/O requests to improve cache management and reduce cache miss ratios. It is often possible to outperform an LRU page replacement algorithm, even without prefetching <ref> [Chou85, Korner90] </ref>. Unneeded blocks can be released early and needed blocks can be held longer. <p> We hope to extend these techniques to prefetching. Also, our work emphasizes a more solid partitioning of function between application and operating system. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests <ref> [Kotz91, Tait91, Palmer91, Korner90] </ref>. Our view of the problem is perhaps most similar to Korners who recognized the value of high-level hints as a means of bridging levels of abstraction from files to disk blocks. Her characterizations of access patterns, like ours, are at a high level of abstraction.
Reference: [Kotz91] <author> Kotz, D., Ellis, C.S., </author> <title> Practical Prefetching Techniques for Parallel File Systems, </title> <booktitle> Proc. First International Conf. on Parallel and Distributed Information Systems, </booktitle> <address> Miami Beach, Florida, </address> <month> Dec. </month> <pages> 4-6, </pages> <year> 1991, </year> <pages> pp. 182-189. </pages>
Reference-contexts: More explicitly, the file system can readahead sequential blocks of a file [Smith85, Feiertag71]. Recently, other researchers have explored prefetching for more complex access patterns by inferring future requests from a users I/O request stream <ref> [Kotz91, Tait91, Palmer91, Korner90] </ref>. For well behaved applications, these techniques can effectively prefetch data and reduce cache misses. However, for the many applications whose access patterns appear random or that touch data only once, such techniques are ineffective. <p> We hope to extend these techniques to prefetching. Also, our work emphasizes a more solid partitioning of function between application and operating system. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests <ref> [Kotz91, Tait91, Palmer91, Korner90] </ref>. Our view of the problem is perhaps most similar to Korners who recognized the value of high-level hints as a means of bridging levels of abstraction from files to disk blocks. Her characterizations of access patterns, like ours, are at a high level of abstraction.
Reference: [Livny87] <author> Livny, M., Khoshafian, S., Boral, H., </author> <booktitle> Multidisk Management Algorithms, Proc. of the 1987 ACM Conf. on Measurement and Modeling of Computer Systems (SIGMETRICS), </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Today, file read latency is the most significant bottleneck for high performance input and output. Other aspects of I/O performance benefit from recent advances in disk bandwidth and throughput resulting from disk arrays <ref> [Kim86, Salem86, Livny87, Patterson88, Reddy89] </ref>, and in write performance derived from buffered write-behind and the Log-structured File System [Rosenblum91]. The development of distributed file systems operating over networks with diverse bandwidth [Spector89, Satya85, Nelson88] only exacerbates the problem.
Reference: [McKusick84] <author> McKusick, M. K., Joy, W. J., Lefer, S. J., Fabry, R. S., </author> <title> A Fast File System for UNIX, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> V 2 (3), </volume> <month> August </month> <year> 1984, </year> <pages> pp. 181-197. </pages>
Reference-contexts: There is no provision for balancing competing demands for buffers and I/O bandwidth from concurrently running applications. More commonly, file systems provide prefetching based on the assumption of sequential file access. Organizing and fetching data in large file blocks <ref> [McKusick84] </ref> effectively prefetches unrequested Cache Size Miss Ratio 1985 BSD Study 390KB 1MB 2MB 4MB 8MB 16MB 1991 Study 7MB (avg) Table 2. Comparison of caching performance in 1985 and 1991. The numbers in this table are drawn from [Ousterhout85] and [Baker91].
Reference: [Miller91] <author> Miller, E., </author> <title> Input/Output Behavior of Supercomputing Applications, </title> <note> University of Cal-ifornia Technical Report UCB/CSD 91/616, </note> <month> January </month> <year> 1991, </year> <type> Masters Thesis. </type>
Reference-contexts: Make could give hints about the whole process to a TIP system. Hints for more complex, non-sequential access patterns are also possible. An example from the supercomputer domain is stride access to large matrices <ref> [Miller91] </ref>. Data base applications often can predict their accesses to satisfy a query [Chou85, Stonebraker81, Selinger79]. Interactive applications, too, could provide hints.
Reference: [Mummert92] <author> Mummert, L., Satyanarayanan, M., </author> <title> Efficient and Portable File Reference Tracing in a Distributed Workstation Environment, </title> <institution> Carnegie Mellon University, </institution> <note> manuscript in preparation. </note>
Reference-contexts: The command and the prefetch process then run concurrently. slightly so that the first thing it does is fork off a prefetching process. The parent process then simply continues on, executing the standard make code. The files to prefetch were determined in advance using a file system tracing facility <ref> [Mummert92] </ref> and hard-wired into the prefetcher. Thus, the prefetching was based 8 on accurate knowledge of the accesses to be performed. The prefetch mechanism differs in the local and remote cases. In the local case, the prefetch process sequentially reads files in 64K chunks using standard system calls. <p> With such straightforward programming styles, we expect that precise hints can be extracted automatically. In some cases, even programmers may not fully understand the file accesses their programs make. An access pattern profiler could be built on top of an efficient file system tracing facility <ref> [Mummert92] </ref>. The profiler could help programmers improve the quality of their hints, or it could be used directly to gen 11 erate hints for future runs.
Reference: [Nelson88] <author> Nelson, M. N., Welch, B. W., Ousterhout, J. K., </author> <title> Caching in the Sprite Network File System, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> V 6 (1), </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: Other aspects of I/O performance benefit from recent advances in disk bandwidth and throughput resulting from disk arrays [Kim86, Salem86, Livny87, Patterson88, Reddy89], and in write performance derived from buffered write-behind and the Log-structured File System [Rosenblum91]. The development of distributed file systems operating over networks with diverse bandwidth <ref> [Spector89, Satya85, Nelson88] </ref> only exacerbates the problem. In this paper, we argue that prefetching based on application level information is a feasible and effective strategy for reducing file access read latency in both local and network file systems.
Reference: [Ng91] <author> Ng, R., Faloutsos, C., Sellis, T., </author> <title> Flexible Buffer Allocation Based on Marginal Gains, </title> <booktitle> Proc. of the 1991 ACM Conf. on Management of Data (SIGMOD), </booktitle> <pages> pp. 387-396. </pages>
Reference-contexts: Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms <ref> [Sacco82, Chou85, Cornell89, Ng91] </ref> and for I/O optimizations [Selinger79]. We hope to extend these techniques to prefetching. Also, our work emphasizes a more solid partitioning of function between application and operating system.
Reference: [Ousterhout85] <author> Ousterhout, J.K., Da Costa, H., Harrison, D., Kunze, J.A., Kupfer, M., and Thompson, J.G., </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System, </title> <booktitle> Proc. of the 10th Symp. on Operating System Principles, </booktitle> <address> Orcas Island, WA, </address> <month> December </month> <year> 1985, </year> <pages> pp. 15-24. </pages>
Reference-contexts: For caches to compensate for the growing gap between processor and I/O performance, their miss ratios will have to drop proportionately. Is such improvement likely? Table 2 compares the performance predicted for a variety of operating system file cache sizes in 1985 <ref> [Ousterhout85] </ref> with that observed in 1991 [Baker91] by a group at Berkeley. The first observation, based on the 1985 data, is that increasing the size of an already large cache does not reduce the miss ratio much. <p> Organizing and fetching data in large file blocks [McKusick84] effectively prefetches unrequested Cache Size Miss Ratio 1985 BSD Study 390KB 1MB 2MB 4MB 8MB 16MB 1991 Study 7MB (avg) Table 2. Comparison of caching performance in 1985 and 1991. The numbers in this table are drawn from <ref> [Ousterhout85] </ref> and [Baker91]. The 1985 tracing study of the UNIX 4.2 BSD file system predicted cache performance for a range of cache sizes assuming a 30 second ush back policy for writes. The 1991 study measured cache performance on a number of workstations running Sprite.
Reference: [Ousterhout89] <author> Ousterhout, J., Douglis, F., </author> <title> Beating the I/O Bottleneck: A Case for Log-Structured File Systems, </title> <journal> ACM Operating Systems Review, </journal> <volume> V 23 (1), </volume> <month> January </month> <year> 1989, </year> <pages> pp. 11-28. </pages>
Reference-contexts: The speed of computations using only primary memory is increasing five to ten times faster than the speed with which blocks on magnetic disk can be accessed <ref> [Patterson88, Ousterhout89] </ref>. In addition, 2 some new technologies further increase the gap between processor and I/O performance. Distributed and wide area [Cate92] file systems slow I/O with substantial network transmission and server delays.
Reference: [Palmer91] <author> Palmer, </author> <title> M.L., Zdonik, S.B., FIDO: A Cache that Learns to Fetch, </title> <institution> Brown University Technical Report CS-90-15, </institution> <year> 1991. </year>
Reference-contexts: More explicitly, the file system can readahead sequential blocks of a file [Smith85, Feiertag71]. Recently, other researchers have explored prefetching for more complex access patterns by inferring future requests from a users I/O request stream <ref> [Kotz91, Tait91, Palmer91, Korner90] </ref>. For well behaved applications, these techniques can effectively prefetch data and reduce cache misses. However, for the many applications whose access patterns appear random or that touch data only once, such techniques are ineffective. <p> We hope to extend these techniques to prefetching. Also, our work emphasizes a more solid partitioning of function between application and operating system. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests <ref> [Kotz91, Tait91, Palmer91, Korner90] </ref>. Our view of the problem is perhaps most similar to Korners who recognized the value of high-level hints as a means of bridging levels of abstraction from files to disk blocks. Her characterizations of access patterns, like ours, are at a high level of abstraction.
Reference: [Patterson88] <author> Patterson, D., Gibson, G., Katz, R., </author> <title> A, A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proc. of the 1988 ACM Conf. on Management of Data (SIGMOD), </booktitle> <address> Chi-cago, IL, </address> <month> June </month> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: 1 Introduction Today, file read latency is the most significant bottleneck for high performance input and output. Other aspects of I/O performance benefit from recent advances in disk bandwidth and throughput resulting from disk arrays <ref> [Kim86, Salem86, Livny87, Patterson88, Reddy89] </ref>, and in write performance derived from buffered write-behind and the Log-structured File System [Rosenblum91]. The development of distributed file systems operating over networks with diverse bandwidth [Spector89, Satya85, Nelson88] only exacerbates the problem. <p> The speed of computations using only primary memory is increasing five to ten times faster than the speed with which blocks on magnetic disk can be accessed <ref> [Patterson88, Ousterhout89] </ref>. In addition, 2 some new technologies further increase the gap between processor and I/O performance. Distributed and wide area [Cate92] file systems slow I/O with substantial network transmission and server delays.
Reference: [Reddy89] <author> Reddy, A.L.N., Banerjee, P., </author> <title> Evaluation of Multiple-Disk I/O Systems, </title> <journal> IEEE Tans. on Computers, </journal> <month> December </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Today, file read latency is the most significant bottleneck for high performance input and output. Other aspects of I/O performance benefit from recent advances in disk bandwidth and throughput resulting from disk arrays <ref> [Kim86, Salem86, Livny87, Patterson88, Reddy89] </ref>, and in write performance derived from buffered write-behind and the Log-structured File System [Rosenblum91]. The development of distributed file systems operating over networks with diverse bandwidth [Spector89, Satya85, Nelson88] only exacerbates the problem.
Reference: [Rosenblum91] <author> Rosenblum, M., Ousterhout, J.K., </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> Operating Systems Review (Proceedings of the 13th SOSP), </booktitle> <volume> Volume 25 (5), </volume> <month> October </month> <year> 1991, </year> <pages> pp 1-15. </pages>
Reference-contexts: Other aspects of I/O performance benefit from recent advances in disk bandwidth and throughput resulting from disk arrays [Kim86, Salem86, Livny87, Patterson88, Reddy89], and in write performance derived from buffered write-behind and the Log-structured File System <ref> [Rosenblum91] </ref>. The development of distributed file systems operating over networks with diverse bandwidth [Spector89, Satya85, Nelson88] only exacerbates the problem. <p> They can also service many small requests simultaneously to provide high service rates for workloads consisting of many concurrent, randomly distributed, small accesses. RAIDs do not perform quite as well for small writes as they do for reads, but the Log-Structured File System (LFS) <ref> [Rosenblum91] </ref> can help by combining small writes into large ones that RAIDs handle effectively. Thus enhanced, RAIDs increase I/O subsystem throughput dramatically for both large accesses and concurrent small accesses. And, they do so in a manner that can scale with increasing processor performance.
Reference: [Sacco82] <author> Sacco, G.M., Schkolnick, M., </author> <title> A Mechanism for Managing the Buffer Pool in a Relational Database System Using the Hot Set Model, </title> <booktitle> Proc. of the Eighth Int. Conf. on Very Large Data Bases, </booktitle> <month> September, </month> <year> 1982, </year> <pages> pp. 257-262. </pages>
Reference-contexts: Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms <ref> [Sacco82, Chou85, Cornell89, Ng91] </ref> and for I/O optimizations [Selinger79]. We hope to extend these techniques to prefetching. Also, our work emphasizes a more solid partitioning of function between application and operating system.
Reference: [Salem86] <author> Salem, K. Garcia-Molina, H., </author> <title> Disk Striping, </title> <booktitle> Proc. of the 2nd IEEE Int. Conf. on Data Engineering, </booktitle> <year> 1986. </year> <month> 14 </month>
Reference-contexts: 1 Introduction Today, file read latency is the most significant bottleneck for high performance input and output. Other aspects of I/O performance benefit from recent advances in disk bandwidth and throughput resulting from disk arrays <ref> [Kim86, Salem86, Livny87, Patterson88, Reddy89] </ref>, and in write performance derived from buffered write-behind and the Log-structured File System [Rosenblum91]. The development of distributed file systems operating over networks with diverse bandwidth [Spector89, Satya85, Nelson88] only exacerbates the problem.
Reference: [Satya85] <author> Satyanarayanan, M., Howard, J. Nichols, D., Sidebotham, R., Spector, A., West, M., </author> <title> The ITC Distributed File System: </title> <booktitle> Principles and Design, Proc. of the Tenth Symp. on Operating Systems Principles, ACM, </booktitle> <month> December </month> <year> 1985, </year> <pages> pp. 35-50. </pages>
Reference-contexts: Other aspects of I/O performance benefit from recent advances in disk bandwidth and throughput resulting from disk arrays [Kim86, Salem86, Livny87, Patterson88, Reddy89], and in write performance derived from buffered write-behind and the Log-structured File System [Rosenblum91]. The development of distributed file systems operating over networks with diverse bandwidth <ref> [Spector89, Satya85, Nelson88] </ref> only exacerbates the problem. In this paper, we argue that prefetching based on application level information is a feasible and effective strategy for reducing file access read latency in both local and network file systems. <p> The remote tests were run on two Decstation 5000/200 also running Mach, one of them the client, and the other the server for the Coda File System. Coda is a descendent of the Andrew File System <ref> [Satya85] </ref>, but with enhancements for availability and mobility. In this system, whole files are transferred to requesting client machines which then cache them on local disks. Reads and writes are applied through the clients buffer cache to its local disks copy.
Reference: [Satya90] <author> Satyanarayanan, M., Kistler, J. J., Kumar, P., Okasaki, M. E., Siegel, E. H., Steere, D. C., Coda: </author> <title> A Highly Available File System for a Distributed Workstation Environment, </title> <journal> IEEE Trans. on Computers, </journal> <volume> V C-39 (4), </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: These experiments show a 13% reduction in the execution time of the make of an X windows application with ideal hints accessing a single disk, and a 20% reduction in the same programs execution time when accessing data remotely in the Coda distributed file system <ref> [Satya90] </ref>. A second example, the grep text search for a simple pattern in 58 files stored remotely in Coda, achieves a 30% reduction in its execution time when the shell issues the command arguments as a hint in parallel with initiating the search. <p> Our experiments evaluated TIP for accesses both to a local disk and over a network to the Coda distributed file system <ref> [Satya90] </ref>. In both cases, there was only a single request server, either a local disk or a remote file server. This fact limited the potential benefits we could expect to achieve because there was little or no opportunity for concurrency in the I/O subsystem.
Reference: [Selinger79] <author> Selinger, P.G., Astrahan, </author> <title> M.M., Chamberlin, D.D., Lorie, R.A., Price, T.G., Access Path Selection in a Relational Database Management System, </title> <booktitle> Proc. of the 1979 ACM Conf. on Management of Data (SIGMOD), </booktitle> <address> Boston, MA, </address> <month> May, </month> <year> 1979, </year> <pages> pp. 23-34. </pages>
Reference-contexts: Make could give hints about the whole process to a TIP system. Hints for more complex, non-sequential access patterns are also possible. An example from the supercomputer domain is stride access to large matrices [Miller91]. Data base applications often can predict their accesses to satisfy a query <ref> [Chou85, Stonebraker81, Selinger79] </ref>. Interactive applications, too, could provide hints. For example, when a bank customer inserts their card in an automatic teller machine, a hint about the customers identity could be given, and the customers account records retrieved while the customer enters their Personal Identification Number. <p> Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms [Sacco82, Chou85, Cornell89, Ng91] and for I/O optimizations <ref> [Selinger79] </ref>. We hope to extend these techniques to prefetching. Also, our work emphasizes a more solid partitioning of function between application and operating system. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests [Kotz91, Tait91, Palmer91, Korner90].
Reference: [Seltzer90] <author> Seltzer, M. I., Chen, P. M., Ousterhout, J. K., </author> <title> Disk Scheduling Revisited, </title> <booktitle> Proc. of the Winter 1990 USENIX Technical Conf., </booktitle> <address> Washington DC, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: This creates new opportunities for storage subsystem optimizations. Deep queues of prefetch requests will not encumber demand requests with queuing delays if demand requests have higher priority. For disks, deeper queues allow better arm and rotation scheduling <ref> [Seltzer90] </ref>. For network I/O, multiple prefetch requests can be batched together, reducing network and protocol processing overhead. In both cases, deeper queues increase throughput that TIP uses to reduce read latency and application execution time.
Reference: [Smith85] <author> Smith, A.J., </author> <title> Disk Cache--Miss Ratio Analysis and Design Considerations, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> V 3 (3), </volume> <month> August </month> <year> 1985, </year> <pages> pp. 161-203. </pages>
Reference-contexts: This result is strong evidence that we cannot rely on increased cache sizes to give us arbitrarily low cache miss ratios. 4 data in the latter portions of the block. More explicitly, the file system can readahead sequential blocks of a file <ref> [Smith85, Feiertag71] </ref>. Recently, other researchers have explored prefetching for more complex access patterns by inferring future requests from a users I/O request stream [Kotz91, Tait91, Palmer91, Korner90]. For well behaved applications, these techniques can effectively prefetch data and reduce cache misses.
Reference: [Spector89] <author> Spector, A.Z., Kazar, </author> <title> M.L., Wide Area File Service and The AFS Experimental System, </title> <journal> Unix Review, </journal> <volume> V 7 (3), </volume> <month> March, </month> <year> 1989. </year>
Reference-contexts: Other aspects of I/O performance benefit from recent advances in disk bandwidth and throughput resulting from disk arrays [Kim86, Salem86, Livny87, Patterson88, Reddy89], and in write performance derived from buffered write-behind and the Log-structured File System [Rosenblum91]. The development of distributed file systems operating over networks with diverse bandwidth <ref> [Spector89, Satya85, Nelson88] </ref> only exacerbates the problem. In this paper, we argue that prefetching based on application level information is a feasible and effective strategy for reducing file access read latency in both local and network file systems.
Reference: [Stonebraker81] <author> Stonebraker, Michael, </author> <title> Operating System Support for Database Management, </title> <journal> Communications of the ACM, </journal> <volume> V 24 (7), </volume> <month> July </month> <year> 1981, </year> <pages> pp. 412-418. </pages>
Reference-contexts: Make could give hints about the whole process to a TIP system. Hints for more complex, non-sequential access patterns are also possible. An example from the supercomputer domain is stride access to large matrices [Miller91]. Data base applications often can predict their accesses to satisfy a query <ref> [Chou85, Stonebraker81, Selinger79] </ref>. Interactive applications, too, could provide hints. For example, when a bank customer inserts their card in an automatic teller machine, a hint about the customers identity could be given, and the customers account records retrieved while the customer enters their Personal Identification Number. <p> Hints are now widely enough understood that they appear in various existing implementations. For example, Sun Microsystems operating system provides two advise system calls that instruct the virtual memory systems policy decisions [SunOS-vadvise]. Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge <ref> [Stonebraker81] </ref>. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms [Sacco82, Chou85, Cornell89, Ng91] and for I/O optimizations [Selinger79]. We hope to extend these techniques to prefetching. Also, our work emphasizes a more solid partitioning of function between application and operating system.
Reference: [Tait91] <author> Tait, C.D., Duchamp, D., </author> <title> Detection and Exploitation of File Working Sets, </title> <booktitle> Proc. of the 11th Int. Conf. on Distributed Computing Systems, </booktitle> <address> Arlington, TX, </address> <month> May, </month> <year> 1991, </year> <pages> pp. 2-9. </pages>
Reference-contexts: More explicitly, the file system can readahead sequential blocks of a file [Smith85, Feiertag71]. Recently, other researchers have explored prefetching for more complex access patterns by inferring future requests from a users I/O request stream <ref> [Kotz91, Tait91, Palmer91, Korner90] </ref>. For well behaved applications, these techniques can effectively prefetch data and reduce cache misses. However, for the many applications whose access patterns appear random or that touch data only once, such techniques are ineffective. <p> We hope to extend these techniques to prefetching. Also, our work emphasizes a more solid partitioning of function between application and operating system. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests <ref> [Kotz91, Tait91, Palmer91, Korner90] </ref>. Our view of the problem is perhaps most similar to Korners who recognized the value of high-level hints as a means of bridging levels of abstraction from files to disk blocks. Her characterizations of access patterns, like ours, are at a high level of abstraction.
Reference: [Trivedi79] <author> Trivedi, </author> <title> K.S., An Analysis of Prepaging, </title> <journal> Computing, </journal> <volume> V 22 (3), </volume> <year> 1979, </year> <pages> pp. 191-210. </pages>
Reference-contexts: This is an example of how hints can be used to pass information through multiple layers of software. 6 Related Work The idea of giving hints is not new. For example, Trivedi suggested using programmer or compiler generated hints for prepaging <ref> [Trivedi79] </ref>. Hints are now widely enough understood that they appear in various existing implementations. For example, Sun Microsystems operating system provides two advise system calls that instruct the virtual memory systems policy decisions [SunOS-vadvise].
References-found: 35

