URL: http://robotics.stanford.edu/users/sahami/papers-dir/spam.ps
Refering-URL: http://robotics.stanford.edu/users/sahami/papers.html
Root-URL: 
Email: horvitzg@microsoft.com sahami@cs.stanford.edu  
Title: A Bayesian Approach to Filtering Junk E-Mail  
Author: Mehran Sahami Susan Dumais David Heckerman Eric 
Address: Gates Building 1A  Redmond, WA 98052-6399 Stanford, CA 94305-9010 fsdumais, heckerma,  
Affiliation: Horvitz  Computer Science Department Microsoft Research Stanford University  
Abstract: In addressing the growing problem of junk E-mail on the Internet, we examine methods for the automated construction of filters to eliminate such unwanted messages from a user's mail stream. By casting this problem in a decision theoretic framework, we are able to make use of probabilistic learning methods in conjunction with a notion of differential misclassification cost to produce filters which are especially appropriate for the nuances of this task. While this may appear, at first, to be a straight-forward text classification problem, we show that by considering domain-specific features of this problem in addition to the raw text of E-mail messages, we can produce much more accurate filters. Finally, we show the efficacy of such filters in a real world usage scenario, arguing that this technology is mature enough for deployment. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cohen, W. W. </author> <year> 1996. </year> <title> Learning rules that classify email. </title> <booktitle> In Proceedings of the 1996 AAAI Spring Symposium on Machine Learning in Information Access. </booktitle>
Reference-contexts: This, in turn, can lead to the construction of much more accurate junk filters for each user. Along these lines, methods have recently been suggested for automatically learning rules to classify Email <ref> (Cohen 1996) </ref>. While such approaches have shown some success for general classification tasks based on the text of messages, they have not been employed specifically with the task of filtering junk mail in mind.
Reference: <author> Cooper, G. F., and Herskovits, E. </author> <year> 1992. </year> <title> A bayesian method for the induction of probabilistic networks from data. </title> <booktitle> Machine Learning 9 </booktitle> <pages> 309-347. </pages>
Reference-contexts: Formally, this yields P (X = x j C = c k ) = i More recently, there has been a great deal of work on learning much more expressive Bayesian networks from data <ref> (Cooper & Herskovits 1992) </ref> (Heckerman, Geiger, & Chickering 1995) as well as methods for learning networks specifically for classification tasks (Friedman, Geiger, & Goldszmidt 1997) (Sahami 1996).
Reference: <author> Cover, T. M., and Thomas, J. A. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <publisher> Wiley. </publisher>
Reference-contexts: Next, we compute the mutual information M I (X i ; C) between each feature X i and the class C <ref> (Cover & Thomas 1991) </ref>, given by M I (X i ; C) = X i =x i ;C=c P (X i ; C) : We select the 500 features for which this value is greatest as the feature set from which to build a classifier.
Reference: <author> Friedman, N.; Geiger, D.; and Goldszmidt, M. </author> <year> 1997. </year> <title> Bayesian network classifiers. </title> <booktitle> Machine Learning 29 </booktitle> <pages> 131-163. </pages>
Reference-contexts: (X = x j C = c k ) = i More recently, there has been a great deal of work on learning much more expressive Bayesian networks from data (Cooper & Herskovits 1992) (Heckerman, Geiger, & Chickering 1995) as well as methods for learning networks specifically for classification tasks <ref> (Friedman, Geiger, & Goldszmidt 1997) </ref> (Sahami 1996). These later approaches allow for a limited form of dependence between feature variables, so as to relax the restrictive assumptions of the Naive Bayesian classifier. Figure 1 contrasts the structure of the Naive Bayesian classifier with that of the more expressive classifiers.
Reference: <author> Good, I. J. </author> <year> 1965. </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods. </title> <publisher> M.I.T. Press. </publisher>
Reference-contexts: The oldest and most restrictive form of such assumptions is embodied in the Naive Bayesian classifier <ref> (Good 1965) </ref> which assumes that each feature X i is conditionally independent of every other feature, given the class variable C.
Reference: <author> Heckerman, D.; Geiger, D.; and Chickering, D. </author> <year> 1995. </year> <title> Learning bayesian networks: The combination of knowledge and statistical data. </title> <booktitle> Machine Learning 20 </booktitle> <pages> 197-243. </pages>
Reference-contexts: Formally, this yields P (X = x j C = c k ) = i More recently, there has been a great deal of work on learning much more expressive Bayesian networks from data (Cooper & Herskovits 1992) <ref> (Heckerman, Geiger, & Chickering 1995) </ref> as well as methods for learning networks specifically for classification tasks (Friedman, Geiger, & Goldszmidt 1997) (Sahami 1996). These later approaches allow for a limited form of dependence between feature variables, so as to relax the restrictive assumptions of the Naive Bayesian classifier.
Reference: <author> Joachims, T. </author> <year> 1997. </year> <title> Text categorization with support vector machines: Learning with many relevant features. </title> <type> Technical Report LS8-Report, </type> <institution> Universitaet Dortmund. </institution>
Reference-contexts: Our experiments also show the need for methods aimed at controlling the variance in parameter estimates for text categorization problems. This result is further corroborated by more extensive experiments showing the efficacy of Support Vector Machines (SVMs) in text domains <ref> (Joachims 1997) </ref>. SVMs are known to provide explicit controls on parameter variance during learning (Vapnik 1995) and hence they seem particularly well suited for text categorization. Thus, we believe that using SVMs in a decision theo retic framework that incorporates asymmetric misclas--sification costs is a fruitful venue for further research.
Reference: <author> Koller, D., and Sahami, M. </author> <year> 1996. </year> <title> Toward optimal feature selection. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> 284-292. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Note that the initial feature set that we select from can include both word-based as well as hand-crafted phrasal and other domain-specific features. Previous work in feature selection <ref> (Koller & Sahami 1996) </ref> (Yang & Pedersen 1997) has indicated that such information theoretic approaches are quite effective for text classification problems.
Reference: <author> Koller, D., and Sahami, M. </author> <year> 1997. </year> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <pages> 170-178. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There has recently been a good deal of work in automatically generating probabilistic text classification models such as the Naive Bayesian classifier (Lewis & Ringuette 1994) (Mitchell 1997) (McCallum et al. 1998) as well as more expressive Bayesian classifiers <ref> (Koller & Sahami 1997) </ref>. Continuing in this vein, we seek to employ such Bayesian classification techniques to the problem of junk E-mail filtering. <p> In this way we hope to obtain better classification probability estimates and thus make more accurate costs sensitive classifications. Finally, we are also interested in extending this work to automatically classify messages into a user's hierarchical mail folder structure using the Pachinko Machine classifier <ref> (Koller & Sahami 1997) </ref>. In this way we hope to provide not just a junk mail filter, but an entire message organization system to users.
Reference: <author> Lewis, D. D., and Ringuette, M. </author> <year> 1994. </year> <title> Comparison of two learning algorithms for text categorization. </title> <booktitle> In Proceedings of SDAIR, </booktitle> <pages> 81-93. </pages>
Reference-contexts: Given these, it becomes possible to classify junk E-mail within a Decision Theoretic framework. There has recently been a good deal of work in automatically generating probabilistic text classification models such as the Naive Bayesian classifier <ref> (Lewis & Ringuette 1994) </ref> (Mitchell 1997) (McCallum et al. 1998) as well as more expressive Bayesian classifiers (Koller & Sahami 1997). Continuing in this vein, we seek to employ such Bayesian classification techniques to the problem of junk E-mail filtering.
Reference: <author> McCallum, A.; Rosenfeld, R.; Mitchell, T.; and Ng, A. </author> <year> 1998. </year> <title> Improving text classification by shrinkage in a hierarchy of classes. </title> <booktitle> In Machine Learning: Proceedings of the Fifteenth International Conference. </booktitle>
Reference-contexts: Given these, it becomes possible to classify junk E-mail within a Decision Theoretic framework. There has recently been a good deal of work in automatically generating probabilistic text classification models such as the Naive Bayesian classifier (Lewis & Ringuette 1994) (Mitchell 1997) <ref> (McCallum et al. 1998) </ref> as well as more expressive Bayesian classifiers (Koller & Sahami 1997). Continuing in this vein, we seek to employ such Bayesian classification techniques to the problem of junk E-mail filtering.
Reference: <author> Mitchell, T. M. </author> <year> 1997. </year> <title> Machine Learning. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Given these, it becomes possible to classify junk E-mail within a Decision Theoretic framework. There has recently been a good deal of work in automatically generating probabilistic text classification models such as the Naive Bayesian classifier (Lewis & Ringuette 1994) <ref> (Mitchell 1997) </ref> (McCallum et al. 1998) as well as more expressive Bayesian classifiers (Koller & Sahami 1997). Continuing in this vein, we seek to employ such Bayesian classification techniques to the problem of junk E-mail filtering.
Reference: <author> Pagallo, G., and Haussler, D. </author> <year> 1990. </year> <title> Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 71-99. </pages>
Reference-contexts: Such behavior has been seen in other contexts, such as decision tree induction, and is known as the data fragmentation problem <ref> (Pagallo & Haussler 1990) </ref>. Real Usage Scenario The two test E-mail collections described so far were obtained by classifying existing E-mail folders. The users from which these collections were gathered had already viewed and deleted many legitimate messages by the time the data was sampled.
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: Probabilistic Classification In order to build probabilistic classifiers to detect junk E-mail, we employ the formalism of Bayesian networks. A Bayesian network is a directed, acyclic graph that compactly represents a probability distribution <ref> (Pearl 1988) </ref>. In such a graph, each random variable X i is denoted by a node. A directed edge between two nodes indicates a probabilistic influence (dependency) from the variable denoted by the parent node to that of the child.
Reference: <author> Sahami, M. </author> <year> 1996. </year> <title> Learning limited dependence bayesian classifiers. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 335-338. </pages>
Reference-contexts: = c k ) = i More recently, there has been a great deal of work on learning much more expressive Bayesian networks from data (Cooper & Herskovits 1992) (Heckerman, Geiger, & Chickering 1995) as well as methods for learning networks specifically for classification tasks (Friedman, Geiger, & Goldszmidt 1997) <ref> (Sahami 1996) </ref>. These later approaches allow for a limited form of dependence between feature variables, so as to relax the restrictive assumptions of the Naive Bayesian classifier. Figure 1 contrasts the structure of the Naive Bayesian classifier with that of the more expressive classifiers. <p> Note that the initial feature set that we select from can include both word-based as well as hand-crafted phrasal and other domain-specific features. Previous work in feature selection <ref> (Koller & Sahami 1996) </ref> (Yang & Pedersen 1997) has indicated that such information theoretic approaches are quite effective for text classification problems.
Reference: <author> Salton, G., and McGill, M. J. </author> <year> 1983. </year> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill Book Company. </publisher>
Reference-contexts: In the context of text classification, specifically junk E-mail filtering, it becomes necessary to represent mail messages as feature vectors so as to make such Bayesian classification methods directly applicable. To this end, we use the Vector Space model <ref> (Salton & McGill 1983) </ref> in which we define each dimension of this space as corresponding to a given word in the entire corpus of messages seen. Each individual message can then be represented as a binary vector denoting which words are present and absent in the message.
Reference: <author> Spertus, E. </author> <year> 1997. </year> <title> Smokey: Automatic recognition of hostile messages. </title> <booktitle> In Proceedings of Innovative Applications of Artificial Intelligence (IAAI), </booktitle> <pages> 1058-1065. </pages>
Reference-contexts: As a result, such systems have not focused on the specific features which distinguish junk from legitimate E-mail. The more domain specific work along these lines has focused on detecting "flame" (e.g., hostile) messages <ref> (Spertus 1997) </ref>. This research has looked specifically at particular features that are indicative of "flames", which in general are quite different than those used for junk mail filtering.
Reference: <author> Vapnik, V. </author> <year> 1995. </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: This result is further corroborated by more extensive experiments showing the efficacy of Support Vector Machines (SVMs) in text domains (Joachims 1997). SVMs are known to provide explicit controls on parameter variance during learning <ref> (Vapnik 1995) </ref> and hence they seem particularly well suited for text categorization. Thus, we believe that using SVMs in a decision theo retic framework that incorporates asymmetric misclas--sification costs is a fruitful venue for further research.
Reference: <author> Yang, Y., and Pedersen, J. </author> <year> 1997. </year> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <pages> 412-420. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Note that the initial feature set that we select from can include both word-based as well as hand-crafted phrasal and other domain-specific features. Previous work in feature selection (Koller & Sahami 1996) <ref> (Yang & Pedersen 1997) </ref> has indicated that such information theoretic approaches are quite effective for text classification problems. Using Domain-Specific Features In our first set of experiments, we seek to determine the efficacy of using features that are hand-crafted specifically for the problem of junk E-mail detection.
Reference: <author> Zipf, G. K. </author> <year> 1949. </year> <title> Human Behavior and the Principle of Least Effort. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: First, such dimensionality reduction helps provide an explicit control on the model variance resulting from estimating many parameters. Moreover, feature selection also helps to attenuate the degree to which the independence assumption is violated by the Naive Bayesian classifier. We first employ a Zipf's Law-based analysis <ref> (Zipf 1949) </ref> of the corpus of E-mail messages to eliminate words that appear fewer than three times as having little resolving power between messages.
References-found: 20

