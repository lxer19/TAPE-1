URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/opitz.mlc94.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/opitz.mlc94.ps.abstract.html
Root-URL: 
Email: fopitz, shavlikg@cs.wisc.edu  
Title: Using Genetic Search to Refine Knowledge-Based Neural Networks  
Author: W. Cohen H. Hirsh, David W. Opitz and Jude W. Shavlik 
Affiliation: 1210  Computer Sciences Department University of Wisconsin Madison  
Date: 1994  
Address: San Francisco, CA,  W. Dayton St.  Madison, WI 53706  
Note: Appears in Machine Learning: Proceedings of the Eleventh International Conference,  eds, Morgan Kaufmann,  
Abstract: An ideal inductive-learning algorithm should exploit all available resources, such as computing power and domain-specific knowledge, to improve its ability to generalize. Connectionist theory-refinement systems have proven to be effective at utilizing domain-specific knowledge; however, most are unable to exploit available computing power. This weakness occurs because they lack the ability to refine the topology of the networks they produce, thereby limiting generalization, especially when given impoverished domain theories. We present the Regent algorithm, which uses genetic algorithms to broaden the type of networks seen during its search. It does this by using (a) the domain theory to help create an initial population and (b) crossover and mutation operators specifically designed for knowledge-based networks. Experiments on three real-world domains indicate that our new algorithm is able to significantly increase generalization compared to a standard connectionist theory-refinement system, as well as our previous algorithm for growing knowledge-based networks.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D. </author> <year> (1987). </year> <title> A Connectionist Machine for Genetic Hillclimbing. </title> <publisher> Kluwer, Norwell, </publisher> <address> MA. </address>
Reference-contexts: Once we are able to consider tens of thousands of networks, we plan to investigate incorporating diversity-promoting techniques. Regent can be considered a Lamarckian 5 , genetic-hillclimbing algorithm <ref> (Ackley, 1987) </ref>, since it performs local optimizations on individuals, then passes the successful optimizations on to offspring.
Reference: <author> Ackley, D. & Littman, M. </author> <year> (1994). </year> <title> A case for lamarck-ian evolution. </title> <editor> In Langton, C., editor, </editor> <booktitle> Artificial Life III, </booktitle> <pages> (pp. 3-10), </pages> <address> Redwood City, CA. </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Baffes, P. & Mooney, R. </author> <year> (1993). </year> <title> Symbolic revision of theories with M-of-N rules. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 1135-1140), </pages> <address> Chambery, France. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dean, T. & Boddy, M. </author> <year> (1988). </year> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 49-54), </pages> <address> St. Paul, MN. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus, given the rapid growth in computing power, we believe it is important to develop techniques that tradeoff the expense of large numbers of computing cycles for gains in predictive accuracy. Analogous to anytime planning techniques <ref> (Dean & Boddy, 1988) </ref>, we believe machine learning researchers should create anytime learning algorithms. 2 Such learning algorithms should produce a good concept quickly, then continue to search concept space, reporting the new "best" concept whenever one is found.
Reference: <author> Deb, K. & Goldberg, D. </author> <year> (1989). </year> <title> An investigation of niche and species formation in genetic function optimization. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> (pp. 42-50), </pages> <address> Ar-lington, VA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We plan to estimate the classification similarity between two networks with the examples in the validation set; hence, networks that classify the validation set in a similar fashion will have a low chance of survival. Another diversity-promoting alternative we plan to investigate is to create subpopulations <ref> (Deb & Goldberg, 1989) </ref>, and then combine a network from each subpopulation. Since we are searching through many candidate networks, it is important to be able to recognize the networks that are likely to generalize the best.
Reference: <author> Dodd, N. </author> <year> (1990). </year> <title> Optimization of network structure using genetic techniques. </title> <booktitle> In Proceedings of the IEEE International Joint Conference on Neural Networks (volume III), </booktitle> <pages> (pp. 965-970), </pages> <address> Paris. </address>
Reference: <author> Fahlman, S. & Lebiere, C. </author> <year> (1989). </year> <title> The cascade-correlation learning architecture. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA. </address>
Reference: <author> Farmer, J. D. & Belin, A. </author> <year> (1992). </year> <title> Artificial life: The coming evolution. </title> <editor> In Langton, C., Taylor, C., Farmer, J. D., & Rasmussen, S., editors, </editor> <booktitle> Artificial Life II, </booktitle> <pages> (pp. 815-840), </pages> <address> Redwood City, CA. </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Fletcher, J. & Obradovic, Z. </author> <year> (1993). </year> <title> Combining prior symbolic knowledge and constructive neural network learning. </title> <journal> Connection Science, </journal> <volume> 5 </volume> <pages> 365-375. </pages>
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209. </pages>
Reference: <author> Fu, L. </author> <year> (1989). </year> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 325-340. </pages>
Reference: <author> Ginsberg, A. </author> <year> (1990). </year> <title> Theory reduction, theory revision, </title> <booktitle> and retranslation. In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 777-782), </pages> <address> Boston, MA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: Regent adds newly trained networks to the population only if their validation-set correctness is better than or equal to an existing member of the population. When Regent replaces a member, it chooses the member having the lowest correctness (ties are broken by choosing the oldest member). Other techniques <ref> (Goldberg, 1989) </ref>, such as replacing the member nearest the new candidate network, can promote diverse populations; however, we do not want to promote diversity at the expense of decreased generalization. Once we are able to consider tens of thousands of networks, we plan to investigate incorporating diversity-promoting techniques. <p> When combining multiple networks, Hansen and Sala-mon (1990) showed that an increase in generalization is likely if the individual networks tend to be independent in their errors. To help promote this independence, we plan to investigate incorporating diversity-promoting techniques <ref> (Goldberg, 1989) </ref>, thus minimizing the similarity of the networks in our final population. Instead of replacing the network with the lowest validation-set error, we will replace the network that has both a poor validation-set performance and classifies examples similarly to other networks in the population. <p> We plan to estimate the classification similarity between two networks with the examples in the validation set; hence, networks that classify the validation set in a similar fashion will have a low chance of survival. Another diversity-promoting alternative we plan to investigate is to create subpopulations <ref> (Deb & Goldberg, 1989) </ref>, and then combine a network from each subpopulation. Since we are searching through many candidate networks, it is important to be able to recognize the networks that are likely to generalize the best.
Reference: <author> Hansen, L. & Salamon, P. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 993-1001. </pages>
Reference: <author> Harp, S., Samad, T., & Guha, A. </author> <year> (1989). </year> <title> Designing application-specific neural networks using the genetic algorithm. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2), </booktitle> <pages> (pp. 447-454), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holland, J. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference: <author> Kitano, H. </author> <year> (1990a). </year> <title> Designing neural networks using genetic algorithms with graph generation system. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 461-476. </pages>
Reference: <author> Kitano, H. </author> <year> (1990b). </year> <title> Empirical studies on the speed of convergence of neural network training using genetic algorithms. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 789-795), </pages> <address> Boston, MA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Koza, J. </author> <year> (1992). </year> <title> Genetic Programming. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Koza, J. & Rice, J. </author> <year> (1991). </year> <title> Genetic generation of both the weights and architectures for a neural network. </title> <booktitle> In International Joint Conference on Neural Networks (volume 2), </booktitle> <pages> (pp. 397-404), </pages> <address> Seattle, WA. </address>
Reference: <author> Lacher, R., Hruska, S., & Kuncicky, D. </author> <year> (1992). </year> <title> Back-propagation learning in expert networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(1) </volume> <pages> 62-72. </pages>
Reference: <author> Litzkow, M., Livny, M., & Mutka, M. </author> <year> (1988). </year> <title> Condor | a hunter of idle workstations. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computing Systems. </booktitle> <publisher> Computer Society Press. </publisher>
Reference-contexts: As it searches, Regent keeps the network that has the lowest validation-set error as the best concept seen so far, breaking ties by choosing the smaller network in an application of Occam's Razor. A parallel version trains many candidate networks at the same time using the Condor system <ref> (Litzkow et al., 1988) </ref>, which runs jobs on idle workstations. A diverse initial population will broaden the types of networks Regent considers during its search; however, we still need to utilize the domain theory when generating this population. Regent does this by randomly perturbing the Kbann network at various nodes.
Reference: <author> MacKay, D. J. </author> <year> (1992). </year> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference-contexts: Future work, then, is to investigate selection methods that do not use a validation set, which would also allow us to use all the training instances to train the networks. Such techniques include minimum description length methods (Rissanen, 1983), Generalized Prediction Error (Moody, 1991), and Bayesian methods <ref> (MacKay, 1992) </ref>. Since the correct theory may be quite different from the initial domain theory, we plan to evaluate including, in the initial population of networks, a variety of networks not obtained directly from the domain theory.
Reference: <author> Mezard, M. & Nadal, J.-P. </author> <year> (1989). </year> <title> Learning in feed-forward layered networks: The tiling algorithm. </title> <journal> Journal of Physics A, </journal> <volume> 22 </volume> <pages> 2191-2204. </pages>
Reference: <author> Miller, G., Todd, P., & Hegde, S. </author> <year> (1989). </year> <title> Designing neural networks using genetic algorithms. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> (pp. 379-384), </pages> <address> Arlington, VA. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: Of these methods, many directly encode each connection in the network (Miller et al., 1989; Oliker et al., 1992; Schiffmann et al., 1992). These methods are relatively straightforward to implement, and are good at fine tuning small networks <ref> (Miller et al., 1989) </ref>; however, they do not scale well since they require very large matrices to represent large networks (Yao, 1993).
Reference: <author> Montana, D. & Davis, L. </author> <year> (1989). </year> <title> Training feedfor-ward networks using genetic algorithms. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 762-767), </pages> <address> Detroit, MI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moody, J. </author> <year> (1991). </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In Moody, J., Han-son, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4), </booktitle> <pages> (pp. 847-854), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Future work, then, is to investigate selection methods that do not use a validation set, which would also allow us to use all the training instances to train the networks. Such techniques include minimum description length methods (Rissanen, 1983), Generalized Prediction Error <ref> (Moody, 1991) </ref>, and Bayesian methods (MacKay, 1992). Since the correct theory may be quite different from the initial domain theory, we plan to evaluate including, in the initial population of networks, a variety of networks not obtained directly from the domain theory.
Reference: <author> Oliker, S., Furst, M., & Maimon, O. </author> <year> (1992). </year> <title> A distributed genetic algorithm for neural network design and training. </title> <journal> Complex Systems, </journal> <volume> 6 </volume> <pages> 459-477. </pages>
Reference: <author> Opitz, D. & Shavlik, J. </author> <year> (1993). </year> <title> Heuristically expanding knowledge-based neural networks. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 1360-1365), </pages> <address> Chambery, France. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: TopGen <ref> (Opitz & Shavlik, 1993) </ref> is an improvement over these systems; it heuristically searches through the space of possible network topologies by adding hidden nodes to the neural representation of the domain theory. TopGen showed statistically-significant improvements over Kbann in several real-world domains (Opitz & Shavlik, 1993); however, in this paper we <p> TopGen <ref> (Opitz & Shavlik, 1993) </ref> is an improvement over these systems; it heuristically searches through the space of possible network topologies by adding hidden nodes to the neural representation of the domain theory. TopGen showed statistically-significant improvements over Kbann in several real-world domains (Opitz & Shavlik, 1993); however, in this paper we empirically show that Top-Gen suffers because it only considers simple expansions of the Kbann network. To address this limitation, we broaden the type of topologies that TopGen considers by using genetic algorithms (GAs). <p> TopGen showed statistically-significant improvements over Kbann in several real-world domains, and comparative experiments with a simple approach to adding nodes verified that new nodes must be added in an intelligent manner <ref> (Opitz & Shavlik, 1993) </ref>. Despite this success, TopGen suffers in that it only considers larger networks that contain the original Kbann network as subgraphs. <p> Also, this approach adds nodes until training set error is sufficiently small, thus producing only one possible network. TopGen compared favorably to a similar technique that also added nodes off to the side of Kbann <ref> (Opitz & Shavlik, 1993) </ref>. Other related work includes applications of GAs to neural networks. GAs have been applied in two different ways: (1) to optimize the connection weights in a fixed topology and (2) to optimize the topology of the network. <p> Kbann (Towell & Shavlik, in press) has been shown to be effective at translating a domain theory into a neural network; however, Kbann suffers in that it does not alter its topology. TopGen <ref> (Opitz & Shavlik, 1993) </ref> improved the Kbann algorithm by using available computer power to search for effective places to add nodes to the Kbann network; however, we show empirically that TopGen suffers from restricting its search to expansions of the Kbann network, and is unable to improve its performance after searching
Reference: <author> Ourston, D. & Mooney, R. </author> <year> (1994). </year> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66(2) </volume> <pages> 273-309. </pages>
Reference: <author> Pazzani, M. & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94. </pages>
Reference: <author> Rissanen, J. </author> <year> (1983). </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431. </pages>
Reference-contexts: Future work, then, is to investigate selection methods that do not use a validation set, which would also allow us to use all the training instances to train the networks. Such techniques include minimum description length methods <ref> (Rissanen, 1983) </ref>, Generalized Prediction Error (Moody, 1991), and Bayesian methods (MacKay, 1992). Since the correct theory may be quite different from the initial domain theory, we plan to evaluate including, in the initial population of networks, a variety of networks not obtained directly from the domain theory.
Reference: <author> Schiffmann, W., Joost, M., & Werner, R. </author> <year> (1992). </year> <title> Synthesis and performance analysis of multilayer neural network architectures. </title> <type> Technical Report 16, </type> <institution> University of Koblenz, Institute for Physics. </institution>
Reference: <author> Towell, G. </author> <year> (1991). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI. </institution>
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1993). </year> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101. </pages>
Reference: <author> Towell, G. & Shavlik, J. </author> <title> (in press). </title> <booktitle> Knowledge-based artificial neural networks. Artificial Intelligence. </booktitle>
Reference: <author> Tresp, V., Hollatz, J., & Ahmad, S. </author> <year> (1992). </year> <title> Network structuring and training using rule-based knowledge. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 5), </booktitle> <pages> (pp. 871-878), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Whitley, D. & Hanson, T. </author> <year> (1989). </year> <title> Optimizing neural networks using faster, more accurate genetic search. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> (pp. 391-396), </pages> <address> Arling-ton, VA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Yao, X. </author> <year> (1993). </year> <title> Evolutionary artificial neural networks. </title> <journal> International Journal of Neural Systems, </journal> <volume> 4(3) </volume> <pages> 203-221. </pages>
Reference-contexts: Techniques that use only GAs to optimize weights (Whitley & Hanson, 1989; Mon-tana & Davis, 1989) have compared competitively with gradient-based training algorithms; however, one problem with GAs is their inefficiency in fine-tuned local search, thus the scalability of these methods are in question <ref> (Yao, 1993) </ref>. Kitano (1990b) presents a method that combines GAs with backpropagation. He does this by using the GA to determine the starting weights for a network, which is then refined by backpropagation. <p> These methods are relatively straightforward to implement, and are good at fine tuning small networks (Miller et al., 1989); however, they do not scale well since they require very large matrices to represent large networks <ref> (Yao, 1993) </ref>. Other techniques (Harp et al., 1989; 6 The relationship between connectionist theory-refinement systems and purely symbolic ones has been extensively covered (Towell, 1991; Baffes & Mooney, 1993); thus we do not discuss it here. Kitano, 1990a; Dodd, 1990) only encode the most im-portant features of the network. <p> Kitano, 1990a; Dodd, 1990) only encode the most im-portant features of the network. These indirect encoding schemes can evolve different sets of parameters along with the network's topology and have been shown to have good scalability <ref> (Yao, 1993) </ref>. Regent differs from both the direct and indirect methods in that it does not explicitly encode its networks.
References-found: 39

