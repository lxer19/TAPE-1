URL: ftp://ftp.cs.columbia.edu/reports/reports-1989/cucs-451-89.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1989.html
Root-URL: http://www.cs.columbia.edu
Email: pkc@cs.columbia.edu  
Title: INDUCTIVE LEARNING WITH BCT  
Author: Philip K. Chan 
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Date: August, 1989  
Pubnum: CUCS-451-89  
Abstract: BCT (Binary Classification Tree) is a system that learns from examples and represents learned concepts as a binary polythetic decision tree. Polythetic trees differ from monothetic decision trees in that a logical combination of multiple (versus a single) attribute values may label each tree arc. Statistical evaluations are used to recursively partition the concept space in two and expand the tree. As with standard decision trees, leaves denote classifications. Classes are predicted for unseen instances by traversing appropriate branches in the tree to the leaves. Empirical results demonstrated that BCT is generally more accurate or comparable to two earlier systems. The workshop version of this paper is in Proceedings of the Sixth International Workshop on Machine Learning (pp. 104-108), June 30 - July 2, 1989, Ithaca, NY: Morgan Kaufmann. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Chan, P. K. </author> <year> (1988). </year> <title> A critical review of CN2: A polythetic classifier system. </title> <type> Technical report CS-88-09, </type> <institution> Department of Computer Science, Vanderbilt University, Nashville, TN. </institution>
Reference-contexts: For comparative purposes, reimplementations of CN2 <ref> (reimplemented by Chan, 1988) </ref> and ID3 (courtesy of Doug Fisher) were also tested on the same data sets. For each domain, we divided the data into two disjoint subsets, training and testing. <p> Thus, selecting either attribute will not result in a sizable information gain (Quinlan, 1986) in ID3. On the other hand, the polythetic evaluation approach allows BCT and CN2 to discover the correlations among attributes. Unfortunately, CN2 has a less-than-optimal heuristic <ref> (Chan, 1988) </ref>, which prevents CN2 from fully exploring its advantage in polythetic evaluation. Moreover, BCT did not excel ID3 or CN2 in the natural domains. The comparable performance of BCT was due to limited search of the space of complexes, which might result in skipping informative complexes.
Reference: <author> Chan, P. K. </author> <year> (1989). </year> <title> Inductive learning with BCT. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 104-108). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283. </pages>
Reference-contexts: As in AQ (Michalski, 1973) BCT extensively searches for concept descriptions that are consistent with the training examples. Like ID3 (Quinlan, 1986) the system partitions the concept space recursively to identify the most effective class descriptions. Similar to CN2 <ref> (Clark & Niblett, 1989) </ref> it uses a polythetic approach (based on multiple attribute values instead of single ones) in evaluating the quality of concepts. Moreover, noise-free examples can rarely be obtained and are not assumed during the inductive process.
Reference: <author> Kullback, S. </author> <year> (1959). </year> <title> Information theory and statistics. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: It measures how effective a complex is in partitioning the concept space in two. When a complex is evaluated, two frequency distributions of classes in the training examples are calculatedone satisfies the complex (t-dist) and the other does not (f-dist). Kullback's estimated discrimination information statistic, 2 I, <ref> (Kullback, 1959) </ref> is then adapted to measure how dependent t-dist and f-dist are on the binary values of the complex (in other words, how different the two distributions are). The more dependent the distributions are, the more effective the complex is.
Reference: <author> Michalski, R. S. </author> <year> (1973). </year> <title> Discovering classification rules using variable-valued logic system VL1. </title> <booktitle> Advanced Papers of the Third IJCAI (pp. </booktitle> <pages> 162-172). </pages> <address> Stanford, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: INTRODUCTION BCT (Binary Classification Tree) is a system that learns from examples. Given a set of preclassified instances, it searches for logical descriptions that accurately represent these classes. BCT is an attempt to unify valuable features exhibited by similar systems. As in AQ <ref> (Michalski, 1973) </ref> BCT extensively searches for concept descriptions that are consistent with the training examples. Like ID3 (Quinlan, 1986) the system partitions the concept space recursively to identify the most effective class descriptions.
Reference: <author> Mingers, J. </author> <year> (1989). </year> <title> An empirical comparison of selection measures for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 319-342. </pages>
Reference: <author> Mitchell, T. M. </author> <year> (1982). </year> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18, </volume> <pages> 203-226. </pages>
Reference: <author> Pagallo, G. & Haussler, D. </author> <year> (1988). </year> <title> Feature discovery in empirical learning. </title> <type> Technical report UCSC-CRL-88-08, </type> <institution> Department of Information and Computer Science, University of California, </institution> <address> Santa Cruz, CA. </address>
Reference-contexts: ID3 trees, however, cannot simulate BCT trees. BCT, as well as ID3, uses a divide-and-conquer approach, where the concept space is recursively decomposed into two smaller subspaces in a top-down fashion. In contrast, CN2 follows a separate-and-conquer approach <ref> (Pagallo & Haussler, 1988) </ref>, where subspaces of the concept space are individually singled out in a sequential fashion. In CN2's approach, the choice of the next best partitioning complex is independent of the possibilities for partitioning the rest of the subspace.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Given a set of preclassified instances, it searches for logical descriptions that accurately represent these classes. BCT is an attempt to unify valuable features exhibited by similar systems. As in AQ (Michalski, 1973) BCT extensively searches for concept descriptions that are consistent with the training examples. Like ID3 <ref> (Quinlan, 1986) </ref> the system partitions the concept space recursively to identify the most effective class descriptions. Similar to CN2 (Clark & Niblett, 1989) it uses a polythetic approach (based on multiple attribute values instead of single ones) in evaluating the quality of concepts. <p> Since classifications depend on the combination of the two attributes, values of either one of them cannot partition the data adequately. Thus, selecting either attribute will not result in a sizable information gain <ref> (Quinlan, 1986) </ref> in ID3. On the other hand, the polythetic evaluation approach allows BCT and CN2 to discover the correlations among attributes. Unfortunately, CN2 has a less-than-optimal heuristic (Chan, 1988), which prevents CN2 from fully exploring its advantage in polythetic evaluation.
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Generating production rules from decision trees. </title> <booktitle> Proceedings of the Tenth IJCAI (pp. </booktitle> <pages> 304-307). </pages> <address> Milan, Italy: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Currently, the author is investigating a variant of the beam search and other statistical/information techniques or combinations of them. Lastly, future explorations might involve strategically converting a BCT tree to a production system and eliminating insignificant nodes <ref> (Quinlan, 1987) </ref>. Acknowledgments I would like to thank Andrea Danyluk, Tom Ellman, Doug Fisher, and Sal Stolfo for their invaluable comments on an earlier draft of this paper.
Reference: <author> Rivest, R. L. </author> <year> (1987). </year> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 229-246. </pages>
Reference-contexts: The comparable performance of BCT was due to limited search of the space of complexes, which might result in skipping informative complexes. Another reason might be the inherent monothetic characteristics of those data. As in ID3, BCT represents knowledge in a decision tree, which contrasts to decision list <ref> (Rivest, 1987) </ref> representation used in CN2. However, BCT's decision tree logically subsumes the tree representation in ID3 and is equivalent to the list representation in CN2.
References-found: 11

