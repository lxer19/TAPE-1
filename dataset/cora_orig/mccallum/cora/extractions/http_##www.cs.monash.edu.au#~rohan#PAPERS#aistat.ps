URL: http://www.cs.monash.edu.au/~rohan/PAPERS/aistat.ps
Refering-URL: http://www.cs.monash.edu.au/~rohan/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: (rohan@cs.monash.edu.au)  (jono@cs.monash.edu.au)  
Title: Finding Overlapping Distributions with MML  
Author: ROHAN A. BAXTER JONATHAN J. OLIVER 
Address: Clayton, Victoria, 3168, AUSTRALIA  
Affiliation: Computer Science Department Monash University  
Abstract: This paper considers an aspect of mixture modelling. Significantly overlapping distributions require more data for their parameters to be accurately estimated than well separated distributions. For example, two Gaussian distributions are considered to significantly overlap when their means are within three standard deviations of each other. If insufficient data is available, only a single component distribution will be estimated, although the data originates from two component distributions. We consider how much data is required to distinguish two component distributions from one distribution in mixture modelling using the minimum message length (MML) criterion. First, we perform experiments which show the MML criterion performs well relative to other Bayesian criteria. Second, we make two improvements to the existing MML estimates, that improve its performance with overlapping distributions. 
Abstract-found: 1
Intro-found: 1
Reference: [Bax95] <author> R.A. Baxter. </author> <title> Finding overlapping distributions with MML. </title> <type> Technical Report 244, </type> <institution> Dept. of Computer Science, Monash University, Clayton 3168, Australia, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: We may avoid this assumption by using the observed Fisher Information as an estimate of the expected Fisher Information. The derivation of the terms needed are in <ref> [Bax95] </ref>. MML mixture modelling programs, such as Snob, will evaluate thousands of candidate models as part of the search for the best one. <p> This makes it desirable to find a computationally cheaper approximation. We call this approximation 1 , the Efficient Fisher Information. This approximation is derived in <ref> [Bax95] </ref>. We repeated the experiments of the last section. In Figure (1), we graph the log of determinant of the expected, observed and efficient Fisher information matrix, when the separation is 10, for differing n. We see that the three Fisher Informations roughly parallel each other.
Reference: [BC91] <author> A.R. </author> <title> Barron and T.M. Cover. Minimum complexity density estimation. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> 37 </volume> <pages> 1034-1054, </pages> <year> 1991. </year>
Reference-contexts: We can demonstrate that there is no bias by noting that MML will choose the more complex model given enough data. The question is then: does MML choose the more complex model soon enough? How efficient is it? This has been answered theoretically by Barron and Cover <ref> [BC91] </ref>. The results here provide an empirical efficiency curve for the simple mixture models considered. In section (3), we experimentally find just how many data points are required for MML to find the correct number of components with high probability for different separation of components.
Reference: [BO94] <author> R.A. Baxter and J.J. Oliver. </author> <title> MDL and MML: similarities and differences. </title> <type> TR 207, </type> <institution> Dept. of Computer Science, Monash University, </institution> <address> Clay-ton, Victoria 3168, Australia, </address> <year> 1994. </year> <note> Available on the WWW from http://www.cs.monash.edu.au/ ~ jono. </note>
Reference-contexts: They are not identical, because the negative logarithm of a Bayes factor is not a message length in the MML context (although it is a code length as used in Rissanen's Stochastic Complexity <ref> [BO94, Ris89] </ref>.) If no model has a significantly shorter message length, (and there is plenty of data), then this may indicate model misspecification. If there is little data and flat priors, then there may be not enough data to distinguish between models.
Reference: [BOH96] <author> R.A. Baxter, J.J. Oliver, and D. </author> <title> Hand. Fitting finite gaussian mixture models using minimum message length estimation. </title> <journal> The IMS Bulletin, </journal> <volume> 25(4), </volume> <month> Jul/Aug </month> <year> 1996. </year>
Reference-contexts: k X log p j (Fisher terms) log f (~x) + 2 (likelihood terms) 2.1 Why use MML? A referee has asked us to establish: why using MML is a good idea for model selection in mixture modelling? We intended to let the existing MML literature speak for this (see <ref> [OBW96, BOH96] </ref>, but we give a brief synopsis here. Our use of MML is based on theoretical and experimental properties. First, MML's theoretical properties include that is non-asymptotic, consistent, invariant under data and parameter transformations [WF87].
Reference: [CH96] <author> D. Chickering and D. Heckerman. </author> <title> Efficient Approximation for the Marginal Likelihood of Incomplete Data given a Bayesian Network. </title> <booktitle> In UAI'96, </booktitle> <pages> pages 158-168. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: The network structure for this model contains the single root node, C, and leaf nodes x i each having only C as a parent. The number of states of C determines the number of classes <ref> [CH96] </ref>. We consider the univariate case and concentrate on models where the component distributions are Gaussian, i.e., f j (x) ~ N ( j ; 2 j ). We estimate the parameters, k, j , j and p j using Wallace's Minimum Message Length (MML) method. <p> First, we treat the expected sufficient statistics for the incomplete data as if they were the actual sufficient statistics for the unavailable complete data <ref> [CH96] </ref>. Second, we only use the diagonal entries of the expected Fisher Information matrix. <p> These approximations, include the Bayesian Information Criterion (BIC) [Sch78], which is equivalent to Rissanen's Minimum Description Length (MDL) measure [Ris78]. They also include approximations used by Draper [Dra93], and Cheeseman and Stutz [CS95]. A recent review is found in <ref> [CH96] </ref>. Second, since MML is not reliant on the difficult-to-compute marginal likelihood of the data, it has a computational advantage in experiments and applications.
Reference: [CS88] <author> J.H. Conway and N.J.A. Sloane. </author> <title> Sphere Packings, Lattices and Groups. </title> <address> Springer-Verlag,New York, </address> <year> 1988. </year>
Reference-contexts: We used values for n p from Table 2.3 of Conway and Sloane <ref> [CS88] </ref>. constants, n p . The prior distribution is described in [OBW96].
Reference: [CS95] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (AUTOCLASS): Theory and results. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P Smyth, and R. Uthu-rusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> The AAAI Press, </publisher> <address> Menlo Park, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction An important part of mixture modelling is determining the number of components which best describes some data. MML has been successfully used in mixture mod-elling for unsupervised classification [WB68, WD94, OBW96]. Two prominent mixture model programs include Autoclass <ref> [CSK + 88, CS95] </ref> and Snob [WB68, Wal90, WD94]. For a comparison of Snob and Autoclass see [UN96, Upa95]. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the classes overlap and data items can belong probabilistically to more than one class. <p> This is not the case for the family of approximations based on the Laplace approximation. These approximations, include the Bayesian Information Criterion (BIC) [Sch78], which is equivalent to Rissanen's Minimum Description Length (MDL) measure [Ris78]. They also include approximations used by Draper [Dra93], and Cheeseman and Stutz <ref> [CS95] </ref>. A recent review is found in [CH96]. Second, since MML is not reliant on the difficult-to-compute marginal likelihood of the data, it has a computational advantage in experiments and applications.
Reference: [CSK + 88] <author> P. Cheeseman, M. Self, J. Kelly, W. Taylor, D. Freeman, and J. Stutz. </author> <title> Bayesian classification. </title> <booktitle> In Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 607-611, </pages> <address> Saint Paul, Minnesota, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction An important part of mixture modelling is determining the number of components which best describes some data. MML has been successfully used in mixture mod-elling for unsupervised classification [WB68, WD94, OBW96]. Two prominent mixture model programs include Autoclass <ref> [CSK + 88, CS95] </ref> and Snob [WB68, Wal90, WD94]. For a comparison of Snob and Autoclass see [UN96, Upa95]. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the classes overlap and data items can belong probabilistically to more than one class.
Reference: [Dra93] <author> D. Draper. </author> <title> Assessment and propagation of model uncertainty. </title> <type> Technical Report 124, </type> <institution> Dept. of Statistics, University of California, </institution> <address> Los Angeles, </address> <year> 1993. </year>
Reference-contexts: This is not the case for the family of approximations based on the Laplace approximation. These approximations, include the Bayesian Information Criterion (BIC) [Sch78], which is equivalent to Rissanen's Minimum Description Length (MDL) measure [Ris78]. They also include approximations used by Draper <ref> [Dra93] </ref>, and Cheeseman and Stutz [CS95]. A recent review is found in [CH96]. Second, since MML is not reliant on the difficult-to-compute marginal likelihood of the data, it has a computational advantage in experiments and applications.
Reference: [MR93] <author> K.L. Mengersen and C.P. Robert. </author> <title> Testing for mixtures via entropy distance and gibbs sampling. </title> <type> Technical Report 9240, </type> <institution> Document de travail, Crest, Insee. Paris, </institution> <year> 1993. </year>
Reference-contexts: MML's behaviour on small data sets, compared to a host of other methods, was experimentally examined in [OBW96]. 2.1.1 MML's performanced compared to MCMC Methods Bayesian MCMC methods for identifying the number of components are still under development [Rob96, RG96]. Mengersen and Robert <ref> [MR93] </ref> test for the presence of a mixture, while Richardson and Green [RG96] consider varying numbers of components.
Reference: [MW92] <author> J.S. Marron and M.P. Wand. </author> <title> Exact Mean Integrated Squared Error. </title> <journal> Annals of Statistics, </journal> <volume> 20 </volume> <pages> 712-736, </pages> <year> 1992. </year>
Reference-contexts: of two normal distributions studied by Richardson and Green: model #6: 0:5N (1; ( 2 3 ) 2 ) + 0:5N (1; ( 2 model #7: 0:5N (1:5; ( 1 2 ) 2 ) + 0:5N (1:5; ( 1 These are models #6 and #7 used by Marron and Wand <ref> [MW92] </ref>, which represent bimodal distributions, moderately- and well-separated respectively. We generated n = 50 and n = 250 data points and report results from 50 replications in table (1). We cannot make any hard comparisons, because the priors and search (for estimates) used by the two methods differ.
Reference: [OBW96] <author> J.J. Oliver, R.A. Baxter, and C.S. Wallace. </author> <title> Unsupervised Learning using MML. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference (ICML 96), </booktitle> <pages> pages 364-372. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction An important part of mixture modelling is determining the number of components which best describes some data. MML has been successfully used in mixture mod-elling for unsupervised classification <ref> [WB68, WD94, OBW96] </ref>. Two prominent mixture model programs include Autoclass [CSK + 88, CS95] and Snob [WB68, Wal90, WD94]. For a comparison of Snob and Autoclass see [UN96, Upa95]. <p> In this paper, we quantify this difficulty in terms of the number of data items needed for the MML criterion to `discover' two overlapping classes [WC92]. We focus primarily on the MML criterion's performance, although alternative criteria have previously been investigated and compared <ref> [OBW96] </ref>. Among those criteria considered, we found that MML performed very well. A new batch of Bayesian results have appeared recently based on MCMC sampling methods. It will be interesting to see whether these methods offer any advantages. <p> In section (3), we experimentally find just how many data points are required for MML to find the correct number of components with high probability for different separation of components. The MML estimates use approximations requiring the assumption of well-separated classes <ref> [OBW96] </ref>. We consider two alternative approximations which reduce the reliance on this assumption in section (4). The results here will assist mixture modellers. They will help mixture mod-eller practitioners decide a priori whether they have enough data to successfully find two classes for a particular separation. <p> We used values for n p from Table 2.3 of Conway and Sloane [CS88]. constants, n p . The prior distribution is described in <ref> [OBW96] </ref>. <p> Second, we only use the diagonal entries of the expected Fisher Information matrix. Having instantiated the terms of Equation (1), the expression we wish to minimise is then <ref> [OBW96] </ref>: M essLen (~x; ) k log 1 pop n p log n p log k! (prior terms) + j=1 p 2 + 2 1 k X log p j (Fisher terms) log f (~x) + 2 (likelihood terms) 2.1 Why use MML? A referee has asked us to establish: why <p> k X log p j (Fisher terms) log f (~x) + 2 (likelihood terms) 2.1 Why use MML? A referee has asked us to establish: why using MML is a good idea for model selection in mixture modelling? We intended to let the existing MML literature speak for this (see <ref> [OBW96, BOH96] </ref>, but we give a brief synopsis here. Our use of MML is based on theoretical and experimental properties. First, MML's theoretical properties include that is non-asymptotic, consistent, invariant under data and parameter transformations [WF87]. <p> A recent review is found in [CH96]. Second, since MML is not reliant on the difficult-to-compute marginal likelihood of the data, it has a computational advantage in experiments and applications. MML's behaviour on small data sets, compared to a host of other methods, was experimentally examined in <ref> [OBW96] </ref>. 2.1.1 MML's performanced compared to MCMC Methods Bayesian MCMC methods for identifying the number of components are still under development [Rob96, RG96]. Mengersen and Robert [MR93] test for the presence of a mixture, while Richardson and Green [RG96] consider varying numbers of components.
Reference: [Raf94] <author> A.E. Raftery. </author> <title> Bayesian model selection in sociology. </title> <type> Working Paper 12, </type> <institution> Dept. of Statistics, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: In practice, these are not the case and we consider model differences of more than 8 nits to indicate higher probability with confidence. The interpretation of message length differences is similar, but not identical, to the interpretation of Bayes factors <ref> [Raf94] </ref>.
Reference: [RG96] <author> S. Richardson and P.J. Green. </author> <title> On Bayesian analysis of mixtures with an unknown number of components. </title> <institution> Mathematics Research Report S-96-01, University of Bristol, </institution> <year> 1996. </year>
Reference-contexts: MML's behaviour on small data sets, compared to a host of other methods, was experimentally examined in [OBW96]. 2.1.1 MML's performanced compared to MCMC Methods Bayesian MCMC methods for identifying the number of components are still under development <ref> [Rob96, RG96] </ref>. Mengersen and Robert [MR93] test for the presence of a mixture, while Richardson and Green [RG96] consider varying numbers of components. <p> Mengersen and Robert [MR93] test for the presence of a mixture, while Richardson and Green <ref> [RG96] </ref> consider varying numbers of components.
Reference: [Ris78] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: This is not the case for the family of approximations based on the Laplace approximation. These approximations, include the Bayesian Information Criterion (BIC) [Sch78], which is equivalent to Rissanen's Minimum Description Length (MDL) measure <ref> [Ris78] </ref>. They also include approximations used by Draper [Dra93], and Cheeseman and Stutz [CS95]. A recent review is found in [CH96]. Second, since MML is not reliant on the difficult-to-compute marginal likelihood of the data, it has a computational advantage in experiments and applications.
Reference: [Ris89] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> N.J., </address> <year> 1989. </year>
Reference-contexts: They are not identical, because the negative logarithm of a Bayes factor is not a message length in the MML context (although it is a code length as used in Rissanen's Stochastic Complexity <ref> [BO94, Ris89] </ref>.) If no model has a significantly shorter message length, (and there is plenty of data), then this may indicate model misspecification. If there is little data and flat priors, then there may be not enough data to distinguish between models.
Reference: [Rob96] <author> C. Robert. </author> <title> Mixtures of distributions:inference and estimation, chapter 24. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1996. </year>
Reference-contexts: MML's behaviour on small data sets, compared to a host of other methods, was experimentally examined in [OBW96]. 2.1.1 MML's performanced compared to MCMC Methods Bayesian MCMC methods for identifying the number of components are still under development <ref> [Rob96, RG96] </ref>. Mengersen and Robert [MR93] test for the presence of a mixture, while Richardson and Green [RG96] consider varying numbers of components.
Reference: [RW95] <author> K. Roeder and L. Wasserman. </author> <title> Practical Bayesian Density Estimation Using Mixtures of Normals. </title> <type> Technical Report 633, </type> <institution> Dept. of Statistics, Carnegie-Mellon University, </institution> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Such a small study is not very discriminatory. We note that Bayes Factors, based on Laplace's Approximations, used by Richardson and Green, also gave similar results. Roeder and Wasserman <ref> [RW95] </ref> also achieve similar results using BIC=MDL. Mengersen and Robert use an interesting novel approach based on the Kullback-Leibler distance measure. Their method shares MML's advantage in being non-asymptotic.
Reference: [Sch78] <author> G. Schwarz. </author> <title> Estimating dimension of a model. </title> <journal> Ann. Stat., </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: The MML estimate used here requires some regularity conditions for both the likelihood and prior, but it does not require asymptotic properties. This is not the case for the family of approximations based on the Laplace approximation. These approximations, include the Bayesian Information Criterion (BIC) <ref> [Sch78] </ref>, which is equivalent to Rissanen's Minimum Description Length (MDL) measure [Ris78]. They also include approximations used by Draper [Dra93], and Cheeseman and Stutz [CS95]. A recent review is found in [CH96].
Reference: [UN96] <author> M.A. </author> <title> Upal and E.M. Neufeld. Comparison of Unsupervised Classifiers. In D.L. Dowe, K.B. Korb, </title> <editor> and J.J. Oliver, editors, </editor> <booktitle> Proceedings of the ISIS Information, Statistics and Induction in Science, </booktitle> <address> Melbourne, Australia, </address> <pages> pages 342-353, </pages> <address> Singapore, </address> <month> 20-23 August </month> <year> 1996. </year> <title> World Scientific. </title>
Reference-contexts: MML has been successfully used in mixture mod-elling for unsupervised classification [WB68, WD94, OBW96]. Two prominent mixture model programs include Autoclass [CSK + 88, CS95] and Snob [WB68, Wal90, WD94]. For a comparison of Snob and Autoclass see <ref> [UN96, Upa95] </ref>. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the classes overlap and data items can belong probabilistically to more than one class. However this advantage is tempered by the additional difficulty in parameter estimation where the classes overlap.
Reference: [Upa95] <author> M. A. Upal. </author> <title> Montel carlo comparison of non-hierarchical unsupervised classifiers, </title> <year> 1995. </year>
Reference-contexts: MML has been successfully used in mixture mod-elling for unsupervised classification [WB68, WD94, OBW96]. Two prominent mixture model programs include Autoclass [CSK + 88, CS95] and Snob [WB68, Wal90, WD94]. For a comparison of Snob and Autoclass see <ref> [UN96, Upa95] </ref>. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the classes overlap and data items can belong probabilistically to more than one class. However this advantage is tempered by the additional difficulty in parameter estimation where the classes overlap.
Reference: [Wal90] <author> C.S. Wallace. </author> <title> Classification by minimum-message-length inference. </title> <editor> In S.G. Akl et al., editors, </editor> <booktitle> Advances in Computing and Information- ICCI 1990, </booktitle> <pages> pages 72-81, </pages> <address> Niagara Falls, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction An important part of mixture modelling is determining the number of components which best describes some data. MML has been successfully used in mixture mod-elling for unsupervised classification [WB68, WD94, OBW96]. Two prominent mixture model programs include Autoclass [CSK + 88, CS95] and Snob <ref> [WB68, Wal90, WD94] </ref>. For a comparison of Snob and Autoclass see [UN96, Upa95]. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the classes overlap and data items can belong probabilistically to more than one class.
Reference: [WB68] <author> C.S. Wallace and D.M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11(2) </volume> <pages> 195-209, </pages> <year> 1968. </year>
Reference-contexts: 1 Introduction An important part of mixture modelling is determining the number of components which best describes some data. MML has been successfully used in mixture mod-elling for unsupervised classification <ref> [WB68, WD94, OBW96] </ref>. Two prominent mixture model programs include Autoclass [CSK + 88, CS95] and Snob [WB68, Wal90, WD94]. For a comparison of Snob and Autoclass see [UN96, Upa95]. <p> 1 Introduction An important part of mixture modelling is determining the number of components which best describes some data. MML has been successfully used in mixture mod-elling for unsupervised classification [WB68, WD94, OBW96]. Two prominent mixture model programs include Autoclass [CSK + 88, CS95] and Snob <ref> [WB68, Wal90, WD94] </ref>. For a comparison of Snob and Autoclass see [UN96, Upa95]. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the classes overlap and data items can belong probabilistically to more than one class.
Reference: [WC92] <author> M.P. Windham and A. Cutler. </author> <title> Information ratios for validating mixture models. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 87(420) </volume> <pages> 1188-1192, </pages> <year> 1992. </year>
Reference-contexts: However this advantage is tempered by the additional difficulty in parameter estimation where the classes overlap. In this paper, we quantify this difficulty in terms of the number of data items needed for the MML criterion to `discover' two overlapping classes <ref> [WC92] </ref>. We focus primarily on the MML criterion's performance, although alternative criteria have previously been investigated and compared [OBW96]. Among those criteria considered, we found that MML performed very well. A new batch of Bayesian results have appeared recently based on MCMC sampling methods.
Reference: [WD94] <author> C.S. Wallace and D.L. Dowe. </author> <title> Intrinsic classification by MML the Snob program. </title> <booktitle> In Proceedings of the 7th Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 37-44, </pages> <address> Singapore, 1994. </address> <publisher> World Scientific. </publisher>
Reference-contexts: 1 Introduction An important part of mixture modelling is determining the number of components which best describes some data. MML has been successfully used in mixture mod-elling for unsupervised classification <ref> [WB68, WD94, OBW96] </ref>. Two prominent mixture model programs include Autoclass [CSK + 88, CS95] and Snob [WB68, Wal90, WD94]. For a comparison of Snob and Autoclass see [UN96, Upa95]. <p> 1 Introduction An important part of mixture modelling is determining the number of components which best describes some data. MML has been successfully used in mixture mod-elling for unsupervised classification [WB68, WD94, OBW96]. Two prominent mixture model programs include Autoclass [CSK + 88, CS95] and Snob <ref> [WB68, Wal90, WD94] </ref>. For a comparison of Snob and Autoclass see [UN96, Upa95]. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the classes overlap and data items can belong probabilistically to more than one class.
Reference: [WF87] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> J. R. Statist. Soc B, </journal> <volume> 49(3) </volume> <pages> 240-265, </pages> <year> 1987. </year>
Reference-contexts: We consider the univariate case and concentrate on models where the component distributions are Gaussian, i.e., f j (x) ~ N ( j ; 2 j ). We estimate the parameters, k, j , j and p j using Wallace's Minimum Message Length (MML) method. Wallace and Freeman <ref> [WF87] </ref> give us a formula which estimates the message length M essLen (~x; ) log h () + 1 log det (F ()) n p + 2 where h () is a prior distribution over parameter values, det (F ()) is the determinant of the expected Fisher Information matrix. f (~x) <p> Our use of MML is based on theoretical and experimental properties. First, MML's theoretical properties include that is non-asymptotic, consistent, invariant under data and parameter transformations <ref> [WF87] </ref>. The MML estimate used here requires some regularity conditions for both the likelihood and prior, but it does not require asymptotic properties. This is not the case for the family of approximations based on the Laplace approximation.
References-found: 26

