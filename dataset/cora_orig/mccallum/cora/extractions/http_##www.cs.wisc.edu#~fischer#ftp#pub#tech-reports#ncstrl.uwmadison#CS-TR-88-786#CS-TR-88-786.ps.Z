URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-88-786/CS-TR-88-786.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-88-786/
Root-URL: http://www.cs.wisc.edu
Email: jdchoi@ibm.com  bart@cs.wisc.edu  netzer@cs.wisc.edu  
Title: Techniques for Debugging Parallel Programs with Flowback Analysis  
Author: Jong-Deok Choi Barton P. Miller Robert H. B. Netzer 
Keyword: Sequent Symmetry shared-memory multiprocessor. Index Items debugging, parallel program, flowback analysis, incremental tracing, semantic analysis, program dependence graph.  
Note: PPD is currently being implemented for the C programming language on a  Research supported in part by National Science Foundation grants CCR-8703373 and CCR-8815928, Office of Naval Research Contract N00014-89-J-1222, and a Digital Equipment Corporation External Research Grant. TR 786 To appear in ACM Trans. on Programming Languages and Systems  
Address: P.O. Box 704 Yorktown Heights, NY 10598  1210 W. Dayton Street Madison, Wisconsin 53706  
Affiliation: IBM T.J. Watson Research Center  Computer Sciences Department University of Wisconsin-Madison  
Abstract: Flowback analysis is a powerful technique for debugging programs. It allows the programmer to examine dynamic dependences in a program's execution history without having to re-execute the program. The goal is to present to the programmer a graphical view of the dynamic program dependences. We are building a system, called PPD, that performs flowback analysis while keeping the execution time overhead low. We also extend the semantics of flowback analysis to parallel programs. This paper describes details of the graphs and algorithms needed to implement efficient flowback analysis for parallel programs. Execution time overhead is kept low by recording only a small amount of trace during a program's execution. We use semantic analysis and a technique called incremental tracing to keep the time and space overhead low. As part of the semantic analysis, PPD uses a static program dependence graph structure that reduces the amount of work done at compile time and takes advantage of the dynamic information produced during execution time. Parallel programs have been accommodated in two ways. First, the flowback dependences can span process boundaries; i.e., the most recent modification to a variable might be traced to a different process than the one that contains the current reference. The static and dynamic program dependence graphs of the individual processes are tied together with synchronization and data dependence information to form complete graphs that represent the entire program. Second, our algorithms will detect potential data race conditions in the access to shared variables. The programmer can be directed to the cause of the race condition. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Guide to Parallel Programming on Sequent Computer Systems, Sequent Computer Systems, Inc. </institution> <year> (1985). </year>
Reference-contexts: It also tells whether a given global variable of a parallel program is a shared variable. (Sequent C has two additional key words to support parallel programming <ref> [1] </ref>: shared and private.) The variables in the IUSE and IMOD sets of the summarizing block are the variables that will be written to the log (described in Section 5) at execution time.
Reference: [2] <author> A. Aho, R. Sethi, and J. Ullman, </author> <booktitle> Compilers: Principles, Techniques, and Tools, </booktitle> <address> Addison-Wesly, Reading, Mass. </address> <year> (1986). </year>
Reference-contexts: For programs without labels that are potential targets of branching statements such as goto, the branch dependence graph is identical to the TR 786 / To appear in ACM Trans. on Programming Languages and Systems 11 abstract syntax tree <ref> [2] </ref> of the program, with the basic blocks being the leaf nodes of the tree. Thus, the branch dependence graphs can be built at compile-time without control-flow analysis. <p> The IUSE set is the set of variables that might be referenced before they are defined by a statement in this block; it is the set of upwards-exposed used variables <ref> [2] </ref> of this block. The IMOD set is the set of variables whose values might be defined by statements in this block. <p> Because a definition of an array element is a preserving definition <ref> [2] </ref>, which fails to prevent any uses reached by the definition from being upwards-exposed, a use of an array element always creates an entry in the IUSE set of the control block.
Reference: [3] <author> F. Allen, M. Burke, R. Cytron, J. Ferrante, W. Hsieh, and V. Sarkar, </author> <title> ``A Framework For Determining Useful Parallelism,'' </title> <booktitle> Proc. of the ACM 1988 Intl. Conf. on Supercomputing, </booktitle> <pages> pp. </pages> <month> 207-215 (July </month> <year> 1988). </year>
Reference-contexts: Since the dynamic trace information effectively unrolls all loops, computing data dependence direction vectors [39], which are approximate compile-time characterizations of dependences, is unnecessary to show execution-time dependences. Although computing data dependence direction vectors is essential for automatic loop parallelization <ref> [3] </ref>, it is unnecessary because we can reconstruct this information at execution time. Moreover, because we require the actual paths of control flow taken at run-time (obtained from the dynamic trace), we need not approximate such information at compile-time. We therefore do not construct a precise static control flow graph.
Reference: [4] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante, </author> <title> ``An Overview of the PTRAN Analysis System for Multiprocessing,'' </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pp. </pages> <month> 617-640 </month> <year> (1988). </year>
Reference-contexts: This paper describes the mechanisms used by PPD to efficiently implement flowback analysis for parallel programs. These mechanisms include program dependence graphs and semantic analysis techniques such as interprocedural analysis <ref> [4, 13] </ref> and data-flow analysis [23]. The goal of PPD is to aid debugging by displaying dynamic program dependences. These dependences should guide the programmer from manifestations of erroneous program behavior (the failure) to the corresponding erroneous program state (the error) to the cause of the problem (the bug). <p> Our approach is certainly more pessimistic than approaches that use the MOD set of a procedure, computed interprocedurally, as the basis of determining TR 786 / To appear in ACM Trans. on Programming Languages and Systems 19 what might be modified by a subroutine call <ref> [4] </ref>. However, our approach is simple to implement and not overly pessimistic in that we do not assume all the global variables but only those that are used or defined in a control block might be modified during the execution of a subroutine called in the control block.
Reference: [5] <author> R. Allen and K. Kennedy, </author> <title> ``Automatic Translation of FORTRAN Programs to Vector Form,'' </title> <journal> ACM Trans. on Prog. Lang. and Systems 90(4) pp. </journal> <month> 491-542 (October </month> <year> 1987). </year>
Reference-contexts: Each node of the data dependence graph is labeled with the statement number and either an identifier or an expression. The data dependence graph has three edge types: data dependence, flow, and linking edges. The data dependence edge represents a true dependence <ref> [5, 26] </ref>. (A statement S 2 has a true dependence upon another statement S 1 , if S 2 uses output of S 1 .) A flow edge from n i to n j is defined when the event hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh : singular node g2a s16: g2 = g2 * a; :
Reference: [6] <author> T. R. Allen and D. A. Padua, </author> <title> ``Debugging Fortran on a Shared Memory Machine,'' </title> <booktitle> Proc. of the 1987 Intl. Conf. on Parallel Processing, </booktitle> <pages> pp. </pages> <month> 721-727 (August </month> <year> 1987). </year>
Reference-contexts: For this purpose we maintain bit-vectors (representing basic blocks) during execution, and set a bit every time execution enters a basic block <ref> [6] </ref>. The size of these bit-vectors is computed at compile-time (by inspecting the simplified static graph, described in the next subsection). From the run-time trace of these bit-vectors, the sets of scalar shared variables that were read and written can be determined. <p> However, this approach always detects data races when they exist and only reports a data race when at least one occurs [33, 34]. TR 786 / To appear in ACM Trans. on Programming Languages and Systems 39 This data race detection scheme is similar to other methods <ref> [6, 14, 19, 33, 34] </ref>, with the exception of pairing the P and V operations. 6.1.4. Data Dependences for Parallel Programs When the user requests to see a dependence for a read of a shared variable, ``SV'', we must locate the most recent modification to that variable.
Reference: [7] <author> V. Balasundaram and K. Kennedy, </author> <title> ``A Technique for Summarizing Data Access and Its Use in Parallelism Enhancing Transformations,'' </title> <booktitle> Proc. of the ACM SIGPLAN '89 Conf. on Prog. Lang. Design and Implementation, </booktitle> <pages> pp. </pages> <address> 41-53 Portland, OR, </address> <month> (June </month> <year> 1989). </year>
Reference-contexts: Since only the access type (either read or write) must be recorded, and not the value, optimizations can be performed to reduce the associated execution-time overhead. For example, instead of writing a trace record for each access, regions of the array that were accessed can sometimes be summarized <ref> [7] </ref> by a single record, resulting in a trace whose length is proportional to a small fraction of the number of array elements accessed. The execution can be analyzed for the presence of data races in one of two ways. <p> One possible way to reduce this overhead is to generate a smaller log entry containing only the particular row (or other part) of the matrix that is actually accessed by employing techniques for succinctly summarizing data accesses in arrays <ref> [7] </ref>. Array logging can also cause some interesting performance anomalies. Notice that test program SH_PATH_2 shows a slight improvement in CPU time (the sum of user and system time) with the code generated by the PPD compiler. <p> However, larger increases in the execution time come from test programs that access only part of arrays in loops. One possible way to reduce this overhead is to employ techniques for succinctly summarizing data accesses in arrays <ref> [7] </ref>. With a more sophisticated dependence analysis for such complex objects, we expect a reduction in the execution-time overhead. Execution-time trace sizes are generally small (less than 1 Mbyte in all cases).
Reference: [8] <author> R. M. Balzer, </author> <title> ``EXDAMS EXtendable Debugging and Monitoring System,'' </title> <booktitle> Proc. of AFIPS Spring Joint Computer Conf. </booktitle> <pages> 34 pp. </pages> <month> 567-580 </month> <year> (1969). </year>
Reference-contexts: While most programmers have experience debugging sequential programs and have developed satisfactory debugging strategies, debugging parallel programs has proven more difficult. The Parallel Program Debugger (PPD) [31] is a debugging system for parallel programs running on shared-memory multiprocessors (hereafter, called ``multiprocessors''). PPD efficiently implements a technique called flowback analysis <ref> [8] </ref>, which provides information on the data and control flow between events in a program's execution. PPD provides this information while keeping both the execution-time and debug-time overhead low.
Reference: [9] <author> J.P. Banning, </author> <title> ``An efficient way to find the side effects of procedure calls and the aliases of variables,'' </title> <booktitle> Proc. of the 1979 ACM Symp. on Principles of Prog. Lang., </booktitle> <pages> pp. </pages> <address> 29-41 San Antonio, TX, </address> <month> (January </month> <year> 1979). </year>
Reference-contexts: We now use terminology from Banning <ref> [9] </ref>. TR 786 / To appear in ACM Trans. on Programming Languages and Systems 13 3.2. Data Dependence Graph Each control block (except for summarizing and dummy blocks) has a data dependence graph that shows only the dependences between statements belonging to that block. <p> Our methods can be extended to handle the special case of aliases resulting from reference parameters in languages like Pascal or FORTRAN. Our approach is to identify, at compile time, potential aliases resulting from reference parameters <ref> [9, 10] </ref>. In the static data dependence graphs, we link together (with linking edges) all nodes representing writes to variables that are potential aliases. In the prelog for a subroutine containing reference parameters that are potential aliases, the address of each such reference parameter is recorded.
Reference: [10] <author> J.M. Barth, </author> <title> ``A practical interprocedural data flow analysis algorithm,'' </title> <journal> Comm. of the ACM 21(9) pp. </journal> <month> 724-736 (September </month> <year> 1978). </year>
Reference-contexts: Our methods can be extended to handle the special case of aliases resulting from reference parameters in languages like Pascal or FORTRAN. Our approach is to identify, at compile time, potential aliases resulting from reference parameters <ref> [9, 10] </ref>. In the static data dependence graphs, we link together (with linking edges) all nodes representing writes to variables that are potential aliases. In the prelog for a subroutine containing reference parameters that are potential aliases, the address of each such reference parameter is recorded.
Reference: [11] <author> J. D. Choi, </author> <title> ``Parallel Program Debugging with Flowback Analysis,'' </title> <type> Ph.D. </type> <institution> Thesis (Also Computer Sciences Dept. </institution> <type> Tech. Rep. #871), </type> <institution> Univ. of Wisconsin-Madison, </institution> <month> (August </month> <year> 1989). </year>
Reference-contexts: Figure 3.5 shows the pre-graph and the post-graph for control block H in Figure 3.1. We outline how to build the pre-graph and the post-graph in this section. Detailed algorithms for building these graphs appear in <ref> [11] </ref>. <p> This process may need to be repeated if the e-block did not actually modify the variable or if the last modification of the variable in the e-block occurred before a nested e-block that also potentially modifies the variable <ref> [11] </ref>. When we construct more than one e-block out of a subroutine because of debugging time efficiency considerations, we sometimes need to locate an e-block that might write a local variable. <p> The synchronization edge between n 1,1 and n 2,1 can be viewed as a generalized flow edge that spans the two processes. We now describe how to construct synchronization edges for programs that use semaphores. Other synchronization primitives (such as messages, rendezvous, etc.) can also be handled <ref> [11] </ref>. In general, we construct a synchronization edge between two nodes if we can identify the temporal ordering between them. <p> There are several approaches to ordering events in a parallel program execution <ref> [11, 15, 17, 18, 28, 33] </ref>. Although the ordering between two events can be determined by searching for a path in the graph, a more efficient representation of the happened-before relation can be constructed that allows the order between any two events to be determined in constant time. <p> Such a representation is con TR 786 / To appear in ACM Trans. on Programming Languages and Systems 37 structed by scanning the graph and computing, for each node, vectors that show the earliest (or latest) nodes in all processes that happened before (or after) that node <ref> [11] </ref>. 6.1.3. Data Races Once the events in the execution of a parallel program have been ordered, flowback analysis can be performed. Dependences that span process boundaries can be successfully located when the execution is data-race free.
Reference: [12] <author> J. D. Choi and B. P. Miller, </author> <title> ``Code Generation and Separate Compilation in a Parallel Program Debugger,'' </title> <booktitle> in Research Monographs on Parallel and Distributed Computing, </booktitle> <editor> ed. D. Padua, </editor> <publisher> MIT Press and Pitman Publishing (1990). </publisher>
Reference-contexts: They also allow for easy identification of which e-blocks might use or modify a given variable. Section 5 discusses how these data structures work together with the log and incremental tracing. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh In previous papers <ref> [12, 31] </ref>, we used different terminology for these sets as follows: IMOD was previously referred to as DEFINED, IUSE as USED, MOD as GDEFINED, and USE as GUSED. We now use terminology from Banning [9]. TR 786 / To appear in ACM Trans. on Programming Languages and Systems 13 3.2.
Reference: [13] <author> K. Cooper, K. Kennedy, and L. Torczon, </author> <title> ``The Impact of Interprocess Analysis and Optimization in the R n programming Environment,'' </title> <journal> ACM Trans. on Prog. Lang. and Systems 8(4) pp. </journal> <month> 491-523 (October </month> <year> 1986). </year>
Reference-contexts: This paper describes the mechanisms used by PPD to efficiently implement flowback analysis for parallel programs. These mechanisms include program dependence graphs and semantic analysis techniques such as interprocedural analysis <ref> [4, 13] </ref> and data-flow analysis [23]. The goal of PPD is to aid debugging by displaying dynamic program dependences. These dependences should guide the programmer from manifestations of erroneous program behavior (the failure) to the corresponding erroneous program state (the error) to the cause of the problem (the bug).
Reference: [14] <author> A. Dinning and E. Schonberg, </author> <title> ``An Empirical Comparison of Monitoring Algorithms for Access Anomaly Detection,'' Procs. </title> <booktitle> of ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> (March </month> <year> 1990). </year>
Reference-contexts: However, this approach always detects data races when they exist and only reports a data race when at least one occurs [33, 34]. TR 786 / To appear in ACM Trans. on Programming Languages and Systems 39 This data race detection scheme is similar to other methods <ref> [6, 14, 19, 33, 34] </ref>, with the exception of pairing the P and V operations. 6.1.4. Data Dependences for Parallel Programs When the user requests to see a dependence for a read of a shared variable, ``SV'', we must locate the most recent modification to that variable.
Reference: [15] <author> P. A. Emrath, S. Ghosh, and D. A. Padua, </author> <title> ``Event Synchronization Analysis for Debugging Parallel Programs,'' </title> <booktitle> Supercomputing '89, </booktitle> <pages> pp. </pages> <address> 580-588 Reno, NV, </address> <month> (November </month> <year> 1989). </year>
Reference-contexts: There are several approaches to ordering events in a parallel program execution <ref> [11, 15, 17, 18, 28, 33] </ref>. Although the ordering between two events can be determined by searching for a path in the graph, a more efficient representation of the happened-before relation can be constructed that allows the order between any two events to be determined in constant time.
Reference: [16] <author> J. Ferrante, K. Ottenstein, and J. Warren, </author> <title> ``The Program Dependence Graph and Its Use in Optimization,'' </title> <journal> ACM Trans. on Prog. Lang. and Systems 9(3) pp. </journal> <month> 319-349 (July </month> <year> 1987). </year>
Reference-contexts: STATIC PROGRAM DEPENDENCE GRAPH The static program dependence graph (static graph) shows the potential dependences between program components, such as data dependences [26] and branch dependences (similar to control dependences <ref> [16] </ref> ). The static graph is also the basic building block of the dynamic program dependence graph (dynamic graph). The static graph is a variation of the program dependence graph introduced by Kuck [25]. <p> STATIC PROGRAM CONTROLLER PPD PACKAGE EMULATION LOG INPUT USER OUTPUT : control flow : information flow : system provided : compiler generated : generated during debugging phase SOURCE FILES PROGRAM DATABASE : generated during previous phases TRACES DEPENDENCE GRAPH hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh purpose of optimizing, vectorizing, and parallelizing transformations of the program <ref> [16, 25-27, 36] </ref>. The main concern in this class is to decide whether there exist any potential dependences between two sets of statements. Second, the program dependence graph is used to extract slices from a program. <p> A slice of a program with respect to variable v and program point p is the set of all the statements that might affect the value of v at p [38]. Such slices can be used for integrating program variants [21] and for program debugging <ref> [16, 35, 37, 38] </ref>. One common attribute of the two classes of applications is that they do not use the dynamic information obtained during program execution. However, in PPD, we augment the static graph with the dynamic information obtained during execution and debugging in building the dynamic graph. <p> Branch Dependence Graph The outer layer of the static graph is the branch dependence graph. This (static) branch dependence graph, which is always a tree, is developed from syntactic program analysis (i.e., at parse-time). In Section 4.3, we compare this graph with the control dependence graph <ref> [16] </ref>. The static branch dependence graph consists of nodes called control blocks and branch dependence edges between these nodes. Figure 3.1 shows an example branch dependence graph. <p> The branch dependence is concerned about the actual program control flow in an execution instance of a program, while control dependence in Program Dependence Graphs (PDG) <ref> [16] </ref> is concerned about the potential program control flow in a program. Details on branch dependences and their relationship to control dependences are presented in Section 4.3. A synchronization edge shows the initiation and termination of synchronization events between processes, such as semaphore operations or sending and receiving messages. <p> We first give an intuitive description of how dynamic branch dependence edges are constructed given static branch dependence graphs and trace information. We then provide formal description of the mechanism. Finally, we compare branch dependences to control dependences used in the PDG by Ferrante, et al <ref> [16] </ref>. There are two cases when we add branch dependence edges, reflecting the two ways program control can flow from one basic block (source block) to another basic block (target block). First, the source block can contain a (conditional or unconditional) branch statement that transfers control to the target. <p> no conditional statement), then no incoming edge is constructed for B. ` Recall that for programs without either explicit or implicit goto statements (such as a break-less case statement falling through to the following case), our notion of branch dependence is identical to control dependence used by Ferrante, et al <ref> [16] </ref>. However, for programs containing goto statements, branch dependences are different than control dependences. A major objective of flowback analysis is to show the flow of a particular execution instance and not to speculate on possible control flows in the execution. <p> One natural candidate for constructing an e-block is the subroutine, since the entry and the exit points are well defined. (Actually, an e-block could be any node of the control dependence graph in PDG <ref> [16] </ref>, since the entry and exit points of each node in PDG are well defined.) The size of e-blocks is crucial to the performance of the system during the execution and debugging phases.
Reference: [17] <author> C. J. Fidge, </author> <title> ``Partial Orders for Parallel Debugging,'' </title> <booktitle> Proc. of the SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 183-194 Madison, WI, </address> <month> (May </month> <year> 1988). </year> <note> Also appears in TR 786 / To appear in ACM Trans. on Programming Languages and Systems 50 SIGPLAN Notices 24(1) (January 1989). </note>
Reference-contexts: There are several approaches to ordering events in a parallel program execution <ref> [11, 15, 17, 18, 28, 33] </ref>. Although the ordering between two events can be determined by searching for a path in the graph, a more efficient representation of the happened-before relation can be constructed that allows the order between any two events to be determined in constant time.
Reference: [18] <author> D. P. Helmbold, C. E. McDowell, and J.-Z. Wang, </author> <title> ``Analyzing Traces with Anonymous Synchronization,'' </title> <booktitle> Proc. of the 1990 Intl. Conf. on Parallel Processing, </booktitle> <pages> pp. </pages> <address> II-70-II-77 St. Charles, IL, </address> <month> (August </month> <year> 1990). </year>
Reference-contexts: There are several approaches to ordering events in a parallel program execution <ref> [11, 15, 17, 18, 28, 33] </ref>. Although the ordering between two events can be determined by searching for a path in the graph, a more efficient representation of the happened-before relation can be constructed that allows the order between any two events to be determined in constant time.
Reference: [19] <author> R. Hood, K. Kennedy, and J. Mellor-Crummey, </author> <title> ``Parallel Program Debugging with On-the-fly Anomaly Detection,'' </title> <booktitle> Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> (November </month> <year> 1990). </year>
Reference-contexts: However, this approach always detects data races when they exist and only reports a data race when at least one occurs [33, 34]. TR 786 / To appear in ACM Trans. on Programming Languages and Systems 39 This data race detection scheme is similar to other methods <ref> [6, 14, 19, 33, 34] </ref>, with the exception of pairing the P and V operations. 6.1.4. Data Dependences for Parallel Programs When the user requests to see a dependence for a read of a shared variable, ``SV'', we must locate the most recent modification to that variable.
Reference: [20] <author> E. Horowitz and S. Sahni, </author> <title> Fundamentals of Data Structures, </title> <publisher> Computer Science Press (1983). </publisher>
Reference-contexts: The subroutine does not contain a loop or accesses to a static variable, making that subroutine a target of log optimization (see Section 5.3). SH_PATH_1 computes the shortest paths from a city to 99 other cities using an algorithm described by Horowitz and Sahni <ref> [20] </ref>. SH_PATH_2 is the same as SH_PATH_1 except that it computes the shortest paths from all of the 100 cities to all the other cities. CLASS is a program that emulates course registration for students, such as registering for courses, and dropping from courses.
Reference: [21] <author> S. Horwitz, J. Prins, and T. Reps, </author> <title> ``Integrating Non-interfering Versions of Programs,'' </title> <journal> ACM Trans. on Prog. Lang. and Systems 11(3) pp. </journal> <month> 345-387 (July </month> <year> 1989). </year>
Reference-contexts: A slice of a program with respect to variable v and program point p is the set of all the statements that might affect the value of v at p [38]. Such slices can be used for integrating program variants <ref> [21] </ref> and for program debugging [16, 35, 37, 38]. One common attribute of the two classes of applications is that they do not use the dynamic information obtained during program execution.
Reference: [22] <author> S. Horwitz, P. Pfeiffer, and T. Reps, </author> <title> ``Dependence Analysis for Pointer Variables,'' </title> <booktitle> Proc. of the ACM SIGPLAN 1989 Conf. on Prog. Lang. Design and Implementation, </booktitle> <year> (1989). </year>
Reference-contexts: However, we are investigating ways to reduce the potentially large amount of execution-time traces due to pointers and dynamic objects by using a method similar to <ref> [22, 30] </ref>. Our methods can be extended to handle the special case of aliases resulting from reference parameters in languages like Pascal or FORTRAN. Our approach is to identify, at compile time, potential aliases resulting from reference parameters [9, 10].
Reference: [23] <author> K. Kennedy, </author> <title> ``A Survey of Data-flow Analysis Techniques,'' Program Flow Analysis: Theory and Applications, </title> <editor> S. S. Muchnick and N. D. Jones, </editor> <booktitle> Eds., </booktitle> <pages> pp. 5-54 Prentice-Hall, </pages> <address> Englewood Cliffs, N.J., </address> <year> (1981 </year> ). 
Reference-contexts: This paper describes the mechanisms used by PPD to efficiently implement flowback analysis for parallel programs. These mechanisms include program dependence graphs and semantic analysis techniques such as interprocedural analysis [4, 13] and data-flow analysis <ref> [23] </ref>. The goal of PPD is to aid debugging by displaying dynamic program dependences. These dependences should guide the programmer from manifestations of erroneous program behavior (the failure) to the corresponding erroneous program state (the error) to the cause of the problem (the bug).
Reference: [24] <author> B. Kernighan and D. Ritchie, </author> <title> The C Programming Language, </title> <publisher> Prentice Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey (1978). </address>
Reference-contexts: Our current algorithms assume that the underlying machine architecture has a sequentially consistent memory system [29] (as is the case on the Sequent Symmetry). The techniques in this paper are described in terms of the C programming language <ref> [24] </ref>, but they should generalize to other imperative languages. We address a large part of the C language, including primitives for synchronization. We discuss a simple approach to pointer variables but this is a topic that needs further investigation. This paper is organized as follows.
Reference: [25] <author> D. J. Kuck, Y. Muraoka, and S. C. Chen, </author> <title> ``On the Number of Operations Simultaneously Executable in FORTRAN-like Programs and Their Speed-up,'' </title> <journal> IEEE Trans. on Computers, </journal> <pages> pp. </pages> <month> 1293-1310 (December </month> <year> 1972). </year>
Reference-contexts: The static graph is also the basic building block of the dynamic program dependence graph (dynamic graph). The static graph is a variation of the program dependence graph introduced by Kuck <ref> [25] </ref>. Since then, there have been numerous variations that can be categorized into two classes according to their applications.
Reference: [26] <author> D. J. Kuck, </author> <title> The Structure of Computers and Computations, </title> <publisher> John Wiley and Sons, </publisher> <address> New York (1978). </address>
Reference-contexts: STATIC PROGRAM DEPENDENCE GRAPH The static program dependence graph (static graph) shows the potential dependences between program components, such as data dependences <ref> [26] </ref> and branch dependences (similar to control dependences [16] ). The static graph is also the basic building block of the dynamic program dependence graph (dynamic graph). The static graph is a variation of the program dependence graph introduced by Kuck [25]. <p> Each node of the data dependence graph is labeled with the statement number and either an identifier or an expression. The data dependence graph has three edge types: data dependence, flow, and linking edges. The data dependence edge represents a true dependence <ref> [5, 26] </ref>. (A statement S 2 has a true dependence upon another statement S 1 , if S 2 uses output of S 1 .) A flow edge from n i to n j is defined when the event hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh : singular node g2a s16: g2 = g2 * a; :
Reference: [27] <author> D. J. Kuck, R. H. Kuhn, B. Leasure, D. A. Padua, and M. Wolfe, </author> <title> ``Dependence Graphs and Compiler Optimizations,'' </title> <booktitle> Proc. of the 1981 ACM Symp. on Principles of Prog. Lang., </booktitle> <pages> pp. </pages> <address> 207-218 Wil-liamsburg, Va., </address> <month> (January 26-28 </month> <year> 1981). </year>
Reference: [28] <author> L. Lamport, </author> <title> ``Time, Clocks, and the Ordering of Events in a Distributed System,'' </title> <journal> Comm. of the ACM 21(7) pp. </journal> <month> 558-565 (July </month> <year> 1978). </year>
Reference-contexts: We partially order the nodes and edges of the parallel dynamic graph by defining the happened-before relation <ref> [28] </ref>, fi , as follows: 1) For any two nodes n 1 and n 2 of the parallel dynamic graph, n 1 fi n 2 is true if n 2 is reachable from n 1 by following any sequence of internal and synchronization edges. 2) For two edges e 1 and <p> There are several approaches to ordering events in a parallel program execution <ref> [11, 15, 17, 18, 28, 33] </ref>. Although the ordering between two events can be determined by searching for a path in the graph, a more efficient representation of the happened-before relation can be constructed that allows the order between any two events to be determined in constant time.
Reference: [29] <author> L. Lamport, </author> <title> ``How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs,'' </title> <journal> IEEE Trans. on Computers C-28(9) pp. </journal> <month> 690-691 (September </month> <year> 1979). </year>
Reference-contexts: While we are not addressing automatic parallelism, many of our techniques might be extended to such systems. Our current algorithms assume that the underlying machine architecture has a sequentially consistent memory system <ref> [29] </ref> (as is the case on the Sequent Symmetry). The techniques in this paper are described in terms of the C programming language [24], but they should generalize to other imperative languages. We address a large part of the C language, including primitives for synchronization.
Reference: [30] <author> J. R. Larus and P. N. Hilfinger, </author> <title> ``Detecting Conflicts Between Structure Accesses,'' </title> <booktitle> Proc. of the ACM SIGPLAN 1988 Conf. on Prog. Lang. Design and Implementation, </booktitle> <pages> pp. </pages> <address> 21-34 Atlanta, Georgia, </address> <month> (June </month> <year> 1988). </year>
Reference-contexts: However, we are investigating ways to reduce the potentially large amount of execution-time traces due to pointers and dynamic objects by using a method similar to <ref> [22, 30] </ref>. Our methods can be extended to handle the special case of aliases resulting from reference parameters in languages like Pascal or FORTRAN. Our approach is to identify, at compile time, potential aliases resulting from reference parameters [9, 10].
Reference: [31] <author> B. P. Miller and J. D. Choi, </author> <title> ``A Mechanism for Efficient Debugging of Parallel Programs,'' </title> <booktitle> Proc. of the ACM SIGPLAN 1988 Conf. on Prog. Lang. Design and Implementation, </booktitle> <pages> pp. </pages> <address> 135-144 Atlanta, Georgia, </address> <month> (June </month> <year> 1988). </year>
Reference-contexts: While most programmers have experience debugging sequential programs and have developed satisfactory debugging strategies, debugging parallel programs has proven more difficult. The Parallel Program Debugger (PPD) <ref> [31] </ref> is a debugging system for parallel programs running on shared-memory multiprocessors (hereafter, called ``multiprocessors''). PPD efficiently implements a technique called flowback analysis [8], which provides information on the data and control flow between events in a program's execution. <p> However they do not contain variables that cannot be accessed outside the corresponding e-block, except for upwards-exposed static variables. For example, those four sets for a subroutine do not contain variables local to the subroutine, although static variables are treated the same way as global variables. The program database <ref> [31] </ref> contains the scope information of each variable, telling whether a given variable is a global variable, a variable local to a subroutine, a static variable (in C), or a formal parameter of a subroutine. <p> They also allow for easy identification of which e-blocks might use or modify a given variable. Section 5 discusses how these data structures work together with the log and incremental tracing. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh In previous papers <ref> [12, 31] </ref>, we used different terminology for these sets as follows: IMOD was previously referred to as DEFINED, IUSE as USED, MOD as GDEFINED, and USE as GUSED. We now use terminology from Banning [9]. TR 786 / To appear in ACM Trans. on Programming Languages and Systems 13 3.2. <p> Details on the fine trace generation and debugging time activities are given in <ref> [31] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh log interval: I 4 I 2 I 1 t 5 t 4 t 3 t 2 t 1 t 0 time: hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh TR 786 / To appear in ACM Trans. on Programming Languages and Systems 30 5.2. <p> To facilitate identifying such log intervals, we obtain the IMOD set of each e-block at compile time and keep it as part of the program database <ref> [31] </ref>. We also keep in the program database, for each variable that might be accessed by more than one e-block, the list of e-blocks that contain the variable in their IMOD sets. We call the list the e-block table.
Reference: [32] <author> B. P. Miller, </author> <title> ``The Frequency of Dynamic Pointer References in ``C'' Programs,'' </title> <journal> SIGPLAN Notices 23(6) pp. </journal> <month> 152-156 (June </month> <year> 1988). </year>
Reference-contexts: This approach will be viable if the dynamic frequency of pointer references is low. For example, tracing a pointer access requires approximately 20 assembly language instructions, and if one out of every ten instructions is a pointer reference <ref> [32] </ref>, the tracing will slow execution by a factor of three. However, we are investigating ways to reduce the potentially large amount of execution-time traces due to pointers and dynamic objects by using a method similar to [22, 30].
Reference: [33] <author> R. H. B. Netzer and B. P. Miller, </author> <title> ``Detecting Data Races in Parallel Program Executions,'' in Languages and Compilers for Parallel Computing, </title> <editor> ed. D. Gelernter, T. Gross, A. Nicolau, and D. Padua, </editor> <title> MIT Press (1991). </title> <booktitle> Also appears in Proc. of the 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> (August </month> <year> 1990). </year>
Reference-contexts: There are several approaches to ordering events in a parallel program execution <ref> [11, 15, 17, 18, 28, 33] </ref>. Although the ordering between two events can be determined by searching for a path in the graph, a more efficient representation of the happened-before relation can be constructed that allows the order between any two events to be determined in constant time. <p> However, this approach always detects data races when they exist and only reports a data race when at least one occurs <ref> [33, 34] </ref>. TR 786 / To appear in ACM Trans. on Programming Languages and Systems 39 This data race detection scheme is similar to other methods [6, 14, 19, 33, 34], with the exception of pairing the P and V operations. 6.1.4. <p> However, this approach always detects data races when they exist and only reports a data race when at least one occurs [33, 34]. TR 786 / To appear in ACM Trans. on Programming Languages and Systems 39 This data race detection scheme is similar to other methods <ref> [6, 14, 19, 33, 34] </ref>, with the exception of pairing the P and V operations. 6.1.4. Data Dependences for Parallel Programs When the user requests to see a dependence for a read of a shared variable, ``SV'', we must locate the most recent modification to that variable.
Reference: [34] <author> R. H. B. Netzer and B. P. Miller, </author> <title> ``Improving the Accuracy of Data Race Detection,'' </title> <booktitle> Proc. of ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> (April </month> <year> 1991). </year> <note> TR 786 / To appear in ACM Trans. on Programming Languages and Systems 51 </note>
Reference-contexts: However, this approach always detects data races when they exist and only reports a data race when at least one occurs <ref> [33, 34] </ref>. TR 786 / To appear in ACM Trans. on Programming Languages and Systems 39 This data race detection scheme is similar to other methods [6, 14, 19, 33, 34], with the exception of pairing the P and V operations. 6.1.4. <p> However, this approach always detects data races when they exist and only reports a data race when at least one occurs [33, 34]. TR 786 / To appear in ACM Trans. on Programming Languages and Systems 39 This data race detection scheme is similar to other methods <ref> [6, 14, 19, 33, 34] </ref>, with the exception of pairing the P and V operations. 6.1.4. Data Dependences for Parallel Programs When the user requests to see a dependence for a read of a shared variable, ``SV'', we must locate the most recent modification to that variable.
Reference: [35] <author> K. J. Ottenstein and L. M. Ottenstein, </author> <title> ``The Program Dependence Graph In A Software Development Environment,'' </title> <journal> SIGPLAN Notices 19(5) pp. </journal> <month> 177-184 (May </month> <year> 1984). </year>
Reference-contexts: A slice of a program with respect to variable v and program point p is the set of all the statements that might affect the value of v at p [38]. Such slices can be used for integrating program variants [21] and for program debugging <ref> [16, 35, 37, 38] </ref>. One common attribute of the two classes of applications is that they do not use the dynamic information obtained during program execution. However, in PPD, we augment the static graph with the dynamic information obtained during execution and debugging in building the dynamic graph. <p> This select node has an incoming data dependence edge from the index node and an incoming linking edge from node ``s6: A'', the most recent modification of array ``A'' in the control block. The above mechanisms are similar to the ideas used for array related dependences in <ref> [35] </ref>. The actual data dependences for each array read are determined during debugging and are reflected in the dynamic graph. Once the fine traces for the e-block containing an array read are generated, the index values of all array accesses in that e-block will be known.
Reference: [36] <author> R. Towle, </author> <title> ``Control and Data Dependence for Program Transformations,'' </title> <type> Ph.D. </type> <institution> Thesis (Also Dept. of Computer Science Tech. </institution> <type> Report 76-788), </type> <institution> University of Illinois, Urbana-Champaign, </institution> <month> (March </month> <year> 1976). </year>
Reference-contexts: STATIC PROGRAM CONTROLLER PPD PACKAGE EMULATION LOG INPUT USER OUTPUT : control flow : information flow : system provided : compiler generated : generated during debugging phase SOURCE FILES PROGRAM DATABASE : generated during previous phases TRACES DEPENDENCE GRAPH hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh purpose of optimizing, vectorizing, and parallelizing transformations of the program <ref> [16, 25-27, 36] </ref>. The main concern in this class is to decide whether there exist any potential dependences between two sets of statements. Second, the program dependence graph is used to extract slices from a program.
Reference: [37] <author> M. Weiser, </author> <title> ``Programmers Use Slices When Debugging,'' </title> <journal> Comm. of the ACM 25(7)(July 1982). </journal>
Reference-contexts: A slice of a program with respect to variable v and program point p is the set of all the statements that might affect the value of v at p [38]. Such slices can be used for integrating program variants [21] and for program debugging <ref> [16, 35, 37, 38] </ref>. One common attribute of the two classes of applications is that they do not use the dynamic information obtained during program execution. However, in PPD, we augment the static graph with the dynamic information obtained during execution and debugging in building the dynamic graph.
Reference: [38] <author> M. Weiser, </author> <title> ``Program Slicing,'' </title> <journal> IEEE Trans. on Software Engineering SE-10(4) pp. </journal> <month> 352-357 (July </month> <year> 1984). </year>
Reference-contexts: Second, the program dependence graph is used to extract slices from a program. A slice of a program with respect to variable v and program point p is the set of all the statements that might affect the value of v at p <ref> [38] </ref>. Such slices can be used for integrating program variants [21] and for program debugging [16, 35, 37, 38]. One common attribute of the two classes of applications is that they do not use the dynamic information obtained during program execution. <p> A slice of a program with respect to variable v and program point p is the set of all the statements that might affect the value of v at p [38]. Such slices can be used for integrating program variants [21] and for program debugging <ref> [16, 35, 37, 38] </ref>. One common attribute of the two classes of applications is that they do not use the dynamic information obtained during program execution. However, in PPD, we augment the static graph with the dynamic information obtained during execution and debugging in building the dynamic graph.

References-found: 38

