URL: http://polaris.cs.uiuc.edu/reports/1404.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: ON THE IMPLEMENTATION AND EFFECTIVENESS OF AUTOSCHEDULING FOR SHARED-MEMORY MULTIPROCESSORS  
Author: BY JOS E EDUARDO MOREIRA 
Degree: THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Electrical Engineering in the Graduate College of the  
Date: 1987  1988  1990  
Address: S~ao Paulo,  S~ao Paulo,  S~ao Paulo,  1995 Urbana, Illinois  
Affiliation: Bach., Physics, Universidade de  Engr., Electrical Engineering, Universidade de  Mestr., Electrical Engineering, Universidade de  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. A. Patterson and J. L. Hennessy, </author> <title> Computer Architecture: A Quantitative Approach. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1990. </year>
Reference-contexts: INTRODUCTION 1.1 Problem Specification The ultimate goal of parallel processing has been the ever-increasing performance by interconnecting many conventional processors into powerful computers. Today, there are strong indications that the best method to achieve this "El Dorado" <ref> [1] </ref> is building systems based on state of the art commercial microprocessors. The reasons for this include: * Large investments: Commercial microprocessors represent a very large market, with some products selling on the order of millions of units a year (Pentium, PowerPC). <p> its activation frame *) for (all T i j T i is a subtask of T ) do PERC (T i ) if (size (A i ) is known) then if (T i is a loop and P is known) then A A [ A i [0] [ A i <ref> [1] </ref> [ A i [P 1] else A A [ A i end if end PERC instances of T i for each instance of T . <p> The size of the vector is equal to the level of the activation frame plus 1. For a display vector D [n + 1], element D [0] points to the current activation frame, D <ref> [1] </ref> points to the immediate parent, D [2] to the grandparent and so on, through D [n] which points to the root of the cactus stack (the activation frame with global variables). <p> The autoscheduling code for a DAG is composed of an entry block, a body, and an exit block. The entry block performs the following functions: 1. It allocates the activation frame for the DAG. 129 typedef struct af_global - void* DisplayVector <ref> [1] </ref>; Array&lt;Real&gt; A; Array&lt;Real&gt; B; Array&lt;Real&gt; C; - af_global; typedef struct af_main - void* DisplayVector [2]; Array&lt;Real&gt; X; Array&lt;Real&gt; Y; - af_main; typedef struct af_swap - void* DisplayVector [2]; Parameter&lt;Integer&gt; N; Array&lt;Real&gt; X; Array&lt;Real&gt; Y; Integer I; Array&lt;Real&gt; T; Array&lt;Real&gt; W; - af_swap; 130 typedef struct af_doall - void* DisplayVector [3]; <p> also has a pointer to the immediate 139 (* Code for SUBROUTINE S *) (* Interface *) void S (P x ; : : : ; P z ) f af local = new activation frame; (* let V be the display vector of the activation frame *) af local-&gt;V <ref> [1] </ref> = af global; af local-&gt;V [0] = af local; (* initialize parameters *) af local-&gt;x.Initialize (P x ); ... af local-&gt;z.Initialize (P z ); initialize local arrays; (* execute body of SUBROUTINE *) subroutine task S (af local); delete af local; g (* Body *) void subroutine task S (activation <p> af local) f statement list g 140 (* Code for type FUNCTION F *) (* Interface *) type F (P x ; : : : ; P z ) f af local = new activation frame; (* let V be the display vector of the activation frame *) af local-&gt;V <ref> [1] </ref> = af global; af local-&gt;V [0] = af local; (* initialize parameters *) af local-&gt;x.Initialize (P x ); ... af local-&gt;z.Initialize (P z ); initialize local arrays; (* execute body of FUNCTION *) type Temp = function task F (af local); delete af local; return Temp; g (* Body *) <p> B.3 Instruction Set Architecture The instruction set architecture of our RISC processor was derived from that of the DLX processor, a "generic" RISC processor <ref> [1] </ref>. As we progress with our experiments of autoscheduled code, we can try different variations of the architecture, keeping those changes that improve the performance at a compatible cost, and dropping those that do not. Our instruction set is listed in Tables B.1, B.2, and B.3. <p> We initially set the network latency to zero, in the instruction level simulator, run this simulation and obtain the address trace. We then run the address trace through the network simulation and obtain the following results: Latency [ 0] = 0 Latency <ref> [ 1] </ref> = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ <p> The average latency is 8.7 cycles. Therefore we repeat our experiment, this time setting the network latency to nine cycles, in the instruction level simulator. We obtain the following latency results: Latency [ 0] = 0 Latency <ref> [ 1] </ref> = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ <p> We repeated the experiment, with the network latency set to seven cycles, and obtained the following results: 281 Latency [ 0] = 0 Latency <ref> [ 1] </ref> = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [
Reference: [2] <author> J. J. Dongarra, </author> <title> "Performance of various computers using standard linear equations software," </title> <type> Tech. Rep. </type> <institution> CS-89-85, Computer Science Department, University of Tennessee, </institution> <year> 1994. </year>
Reference-contexts: supercomputers (64 bits), clock frequencies of the same order as supercomputers (300 MHz for the newest Alpha, as opposed to 240 MHz for the Cray C90), and competitive overall absolute performance (387 Mflops for a one-processor Cray C90 and 140 Mflops for the top of the line IBM POWER2 processors <ref> [2] </ref>). 1 * Software availability: Many large scale software initiatives, such as OSF, FSF, ACE, and PowerOpen, actively develop high quality software for systems based on commercial microprocessors. Most large computer manufacturers are now offering multiprocessor systems based on commercial microprocessors. <p> The size of the vector is equal to the level of the activation frame plus 1. For a display vector D [n + 1], element D [0] points to the current activation frame, D [1] points to the immediate parent, D <ref> [2] </ref> to the grandparent and so on, through D [n] which points to the root of the cactus stack (the activation frame with global variables). Besides the display vector, the struct also contains the declaration of the variables local to the corresponding activation frame. <p> The entry block performs the following functions: 1. It allocates the activation frame for the DAG. 129 typedef struct af_global - void* DisplayVector [1]; Array&lt;Real&gt; A; Array&lt;Real&gt; B; Array&lt;Real&gt; C; - af_global; typedef struct af_main - void* DisplayVector <ref> [2] </ref>; Array&lt;Real&gt; X; Array&lt;Real&gt; Y; - af_main; typedef struct af_swap - void* DisplayVector [2]; Parameter&lt;Integer&gt; N; Array&lt;Real&gt; X; Array&lt;Real&gt; Y; Integer I; Array&lt;Real&gt; T; Array&lt;Real&gt; W; - af_swap; 130 typedef struct af_doall - void* DisplayVector [3]; Integer I; // iteration index Integer J; Integer K; Array&lt;Real&gt; Z; - af_doall; typedef struct <p> It allocates the activation frame for the DAG. 129 typedef struct af_global - void* DisplayVector [1]; Array&lt;Real&gt; A; Array&lt;Real&gt; B; Array&lt;Real&gt; C; - af_global; typedef struct af_main - void* DisplayVector <ref> [2] </ref>; Array&lt;Real&gt; X; Array&lt;Real&gt; Y; - af_main; typedef struct af_swap - void* DisplayVector [2]; Parameter&lt;Integer&gt; N; Array&lt;Real&gt; X; Array&lt;Real&gt; Y; Integer I; Array&lt;Real&gt; T; Array&lt;Real&gt; W; - af_swap; 130 typedef struct af_doall - void* DisplayVector [3]; Integer I; // iteration index Integer J; Integer K; Array&lt;Real&gt; Z; - af_doall; typedef struct af_dag - void* DisplayVector [4]; BitVector Branches; BitVector Processed; lock ProcesLock; int Mode; <p> We initially set the network latency to zero, in the instruction level simulator, run this simulation and obtain the address trace. We then run the address trace through the network simulation and obtain the following results: Latency [ 0] = 0 Latency [ 1] = 0 Latency <ref> [ 2] </ref> = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ <p> The average latency is 8.7 cycles. Therefore we repeat our experiment, this time setting the network latency to nine cycles, in the instruction level simulator. We obtain the following latency results: Latency [ 0] = 0 Latency [ 1] = 0 Latency <ref> [ 2] </ref> = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ <p> We repeated the experiment, with the network latency set to seven cycles, and obtained the following results: 281 Latency [ 0] = 0 Latency [ 1] = 0 Latency <ref> [ 2] </ref> = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [
Reference: [3] <author> J. Hoeflinger, </author> <title> "Cedar Fortran programmer's manual," </title> <type> Tech. Rep. 1157, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: This results in low overhead and also provides the ability to adapt to varying run-time conditions. The compiler accepts as input serial or parallel programs coded in Cedar Fortran (a parallel superset of Fortran 77) <ref> [3] </ref>. The initial passes read the source program and build a set of data structures representing it. Several program transformations can then be applied to extract and exhibit more parallelism. The hierarchical task graph (HTG) is used as our intermediate representation. <p> In the case of distributed memory machines, consideration is given to the problems of data partitioning and distribution, and work allocation in order to maximize accesses to local memory. The compiler support for loop parallelism and data distribution is based on features of Cedar Fortran <ref> [3] </ref>, Fortran D [4], Cray MPP Fortran [5], and HPF [6]. In both the centralized and distributed memory cases, an autoscheduling program can execute on a physical partition of a multiprocessor. This partition is not necessarily fixed, but may vary throughout the execution of the program. <p> We also mention other parallel programming languages of interest. 3.1.1 Cedar Fortran The autoscheduling compiler compiles Cedar Fortran <ref> [3] </ref> source code. HTGIL, the intermediate language used in the compilation process, was influenced by Cedar Fortran. Cedar Fortran is a parallel programming language especially designed for the Cedar Multiprocessor. It allows parallelism in the form of concurrent loops and vector operations to be expressed directly in the language. <p> [1]; Array&lt;Real&gt; A; Array&lt;Real&gt; B; Array&lt;Real&gt; C; - af_global; typedef struct af_main - void* DisplayVector [2]; Array&lt;Real&gt; X; Array&lt;Real&gt; Y; - af_main; typedef struct af_swap - void* DisplayVector [2]; Parameter&lt;Integer&gt; N; Array&lt;Real&gt; X; Array&lt;Real&gt; Y; Integer I; Array&lt;Real&gt; T; Array&lt;Real&gt; W; - af_swap; 130 typedef struct af_doall - void* DisplayVector <ref> [3] </ref>; Integer I; // iteration index Integer J; Integer K; Array&lt;Real&gt; Z; - af_doall; typedef struct af_dag - void* DisplayVector [4]; BitVector Branches; BitVector Processed; lock ProcesLock; int Mode; Array&lt;Integer&gt; T; - af_dag; typedef struct af_task1 - void* DisplayVector [5]; Real X; - af_task1; typedef struct af_task2 - void* DisplayVector [5]; <p> We then run the address trace through the network simulation and obtain the following results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency <ref> [ 3] </ref> = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ <p> The average latency is 8.7 cycles. Therefore we repeat our experiment, this time setting the network latency to nine cycles, in the instruction level simulator. We obtain the following latency results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency <ref> [ 3] </ref> = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ <p> We repeated the experiment, with the network latency set to seven cycles, and obtained the following results: 281 Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency <ref> [ 3] </ref> = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [
Reference: [4] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koebel, U. Kremer, C.-W. Tseng, and M.-Y. Wu, </author> <title> "Fortran D language specification," </title> <type> Tech. Rep. </type> <institution> COMP TR90-141, Department of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: In the case of distributed memory machines, consideration is given to the problems of data partitioning and distribution, and work allocation in order to maximize accesses to local memory. The compiler support for loop parallelism and data distribution is based on features of Cedar Fortran [3], Fortran D <ref> [4] </ref>, Cray MPP Fortran [5], and HPF [6]. In both the centralized and distributed memory cases, an autoscheduling program can execute on a physical partition of a multiprocessor. This partition is not necessarily fixed, but may vary throughout the execution of the program. <p> They can be used to save temporary values that each processor computed. CDOALL and CDOACROSS loops do not allow ENDLOOP statements. 3.1.2 Fortran D Fortran D <ref> [4, 21, 22, 23, 24] </ref> is a version of Fortran enhanced with data decomposition specifications. Fortran D is targeted at data-parallel numerical applications, and is primarily concerned with the exploitation of fine-grain parallelism at the array operation level. <p> Processors first execute the iterations and tasks assigned to the corresponding virtual processors, but the shared address space of autoscheduling ultimately allows any task to be executed in any processor. The following approach for data distribution in autoscheduling borrows features from Fortran D <ref> [4] </ref>, HPF [6], and Cray MPP Fortran [5]. The user first defines a decomposition, an abstract problem domain that corresponds to a grid where the data are placed and operations are performed. <p> - af_main; typedef struct af_swap - void* DisplayVector [2]; Parameter&lt;Integer&gt; N; Array&lt;Real&gt; X; Array&lt;Real&gt; Y; Integer I; Array&lt;Real&gt; T; Array&lt;Real&gt; W; - af_swap; 130 typedef struct af_doall - void* DisplayVector [3]; Integer I; // iteration index Integer J; Integer K; Array&lt;Real&gt; Z; - af_doall; typedef struct af_dag - void* DisplayVector <ref> [4] </ref>; BitVector Branches; BitVector Processed; lock ProcesLock; int Mode; Array&lt;Integer&gt; T; - af_dag; typedef struct af_task1 - void* DisplayVector [5]; Real X; - af_task1; typedef struct af_task2 - void* DisplayVector [5]; Real Y; - af_task2; 131 2. <p> We then run the address trace through the network simulation and obtain the following results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency <ref> [ 4] </ref> = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ <p> Therefore we repeat our experiment, this time setting the network latency to nine cycles, in the instruction level simulator. We obtain the following latency results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency <ref> [ 4] </ref> = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ <p> We repeated the experiment, with the network latency set to seven cycles, and obtained the following results: 281 Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency <ref> [ 4] </ref> = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [
Reference: [5] <author> D. M. Pase, T. MacDonald, and A. Meltzer, </author> <title> "MPP Fortran programming model," </title> <type> tech. rep., </type> <institution> Cray Research, Inc., </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: The compiler support for loop parallelism and data distribution is based on features of Cedar Fortran [3], Fortran D [4], Cray MPP Fortran <ref> [5] </ref>, and HPF [6]. In both the centralized and distributed memory cases, an autoscheduling program can execute on a physical partition of a multiprocessor. This partition is not necessarily fixed, but may vary throughout the execution of the program. <p> Variables K and X are declared local to the J loop. INDEPENDENT, NEW (J) DO I = 1, N INDEPENDENT, NEW (X,K) 26 DO J = 1, N DO K = 1, N END DO END DO 3.1.4 Cray MPP Fortran Cray MPP Fortran <ref> [5] </ref> is a parallel programming language for the Cray T3D. It supports different programming styles at the program: message passing, shared data, data parallel, and work sharing. <p> The following approach for data distribution in autoscheduling borrows features from Fortran D [4], HPF [6], and Cray MPP Fortran <ref> [5] </ref>. The user first defines a decomposition, an abstract problem domain that corresponds to a grid where the data are placed and operations are performed. Points in the grid correspond to points in the iteration space of the problem. 55 The user also defines a virtual processor array. <p> Array&lt;Real&gt; W; - af_swap; 130 typedef struct af_doall - void* DisplayVector [3]; Integer I; // iteration index Integer J; Integer K; Array&lt;Real&gt; Z; - af_doall; typedef struct af_dag - void* DisplayVector [4]; BitVector Branches; BitVector Processed; lock ProcesLock; int Mode; Array&lt;Integer&gt; T; - af_dag; typedef struct af_task1 - void* DisplayVector <ref> [5] </ref>; Real X; - af_task1; typedef struct af_task2 - void* DisplayVector [5]; Real Y; - af_task2; 131 2. It links the local activation frame with all the ancestor frames in the cactus stack by building the display vector. 3. It initializes the bit vectors Branches and Satisfied. 4. <p> [3]; Integer I; // iteration index Integer J; Integer K; Array&lt;Real&gt; Z; - af_doall; typedef struct af_dag - void* DisplayVector [4]; BitVector Branches; BitVector Processed; lock ProcesLock; int Mode; Array&lt;Integer&gt; T; - af_dag; typedef struct af_task1 - void* DisplayVector <ref> [5] </ref>; Real X; - af_task1; typedef struct af_task2 - void* DisplayVector [5]; Real Y; - af_task2; 131 2. It links the local activation frame with all the ancestor frames in the cactus stack by building the display vector. 3. It initializes the bit vectors Branches and Satisfied. 4. It initializes the scheduling lock SatisLock. 5. <p> We then run the address trace through the network simulation and obtain the following results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency <ref> [ 5] </ref> = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ <p> We obtain the following latency results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency <ref> [ 5] </ref> = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ <p> We repeated the experiment, with the network latency set to seven cycles, and obtained the following results: 281 Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency <ref> [ 5] </ref> = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [
Reference: [6] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification, </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: The compiler support for loop parallelism and data distribution is based on features of Cedar Fortran [3], Fortran D [4], Cray MPP Fortran [5], and HPF <ref> [6] </ref>. In both the centralized and distributed memory cases, an autoscheduling program can execute on a physical partition of a multiprocessor. This partition is not necessarily fixed, but may vary throughout the execution of the program. <p> ENDFOR 21 states that iteration i is to be executed by the processor to which A (i) is assigned. Therefore processor p 1 executes iterations f1; 2; 3; 4g. The ON clause is a feature borrowed from the language Kali [25]. 3.1.3 HPF The High Performance Fortran (HPF) <ref> [6, 26, 27] </ref> language was designed as a set of extensions and modifications to Fortran 90 to support data parallel programming. The ability to achieve top performance on MIMD and SIMD computers with nonuniform memory access was one of the main goals of the project. <p> The mechanism includes the following steps: * Data are partitioned at compile time across sets of virtual processors defined by the user (or by a smart compiler), in a manner similar to HPF <ref> [6] </ref>. * Iterations of parallel loops and tasks can be mapped to virtual processors. <p> Processors first execute the iterations and tasks assigned to the corresponding virtual processors, but the shared address space of autoscheduling ultimately allows any task to be executed in any processor. The following approach for data distribution in autoscheduling borrows features from Fortran D [4], HPF <ref> [6] </ref>, and Cray MPP Fortran [5]. The user first defines a decomposition, an abstract problem domain that corresponds to a grid where the data are placed and operations are performed. <p> We then run the address trace through the network simulation and obtain the following results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency <ref> [ 6] </ref> = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ <p> We obtain the following latency results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency <ref> [ 6] </ref> = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ <p> We repeated the experiment, with the network latency set to seven cycles, and obtained the following results: 281 Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency <ref> [ 6] </ref> = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [
Reference: [7] <author> M. J. Flynn, </author> <title> "Very high-speed computing systems," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 12, </volume> <pages> pp. 1901-1909, </pages> <month> December </month> <year> 1966. </year>
Reference-contexts: Appendix C describes the interconnection network simulator that complements the instruction-level simulator. 7 CHAPTER 2 SHARED-MEMORY ARCHITECTURES The target architecture for the implementation of autoscheduling addressed in this thesis is the shared-memory multiprocessor model. A shared-memory multiprocessor is an MIMD (Multiple instruction, multiple data streams <ref> [7] </ref>) computer, composed of multiple control flow (von Neumann) processors, each executing its own stream of instructions and capable of accessing a shared-address space. A survey of parallel computer architectures is presented in [8]. <p> then run the address trace through the network simulation and obtain the following results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency <ref> [ 7] </ref> = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency [ <p> We obtain the following latency results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency <ref> [ 7] </ref> = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ <p> with the network latency set to seven cycles, and obtained the following results: 281 Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency <ref> [ 7] </ref> = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [
Reference: [8] <author> R. Duncan, </author> <title> "A survey of parallel computer architectures," </title> <journal> IEEE Computer, </journal> <volume> vol. 23, </volume> <pages> pp. 5-16, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: A shared-memory multiprocessor is an MIMD (Multiple instruction, multiple data streams [7]) computer, composed of multiple control flow (von Neumann) processors, each executing its own stream of instructions and capable of accessing a shared-address space. A survey of parallel computer architectures is presented in <ref> [8] </ref>. In contrast to a shared-memory multiprocessor, a private-memory multiprocessor is an architecture in which each processor has its own separate address space. Processors in this architecture communicate by passing messages explicitly. For this reason, private-memory multiprocessors are commonly known as message-passing architectures. <p> through the network simulation and obtain the following results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency <ref> [ 8] </ref> = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency [ 17] = 86 Latency [ <p> We obtain the following latency results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency <ref> [ 8] </ref> = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ <p> to seven cycles, and obtained the following results: 281 Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency <ref> [ 8] </ref> = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [
Reference: [9] <author> G. Bell, </author> <title> "Ultracomputers: A teraflop before its time," </title> <journal> Communications of the ACM, </journal> <volume> vol. 35, </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: Processors in this architecture communicate by passing messages explicitly. For this reason, private-memory multiprocessors are commonly known as message-passing architectures. Some authors also reserve the name multiprocessor for shared-address space MIMD computers, and use the term multi-computer for private-memory computers <ref> [9] </ref>. It is worth noting that shared memory and message passing are architectural features because, following the definition by Lorin [10], they define a machine's programming model and rules for program correctness. <p> obtain the following results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency <ref> [ 9] </ref> = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency [ 17] = 86 Latency [ 18] = 46 280 Latency <p> the following latency results: Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency <ref> [ 9] </ref> = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ <p> the following results: 281 Latency [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency <ref> [ 9] </ref> = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [
Reference: [10] <author> H. </author> <title> Lorin, </title> <booktitle> Computer Architecture and Organization. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: Some authors also reserve the name multiprocessor for shared-address space MIMD computers, and use the term multi-computer for private-memory computers [9]. It is worth noting that shared memory and message passing are architectural features because, following the definition by Lorin <ref> [10] </ref>, they define a machine's programming model and rules for program correctness. Shared-memory and message-passing can also be organizational features, that is, descriptions of the internal structure and behavior of the computer. It is possible to implement either architecture on top of each organization. <p> [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency <ref> [ 10] </ref> = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency [ 17] = 86 Latency [ 18] = 46 280 Latency [ 19] = 3673 --------------------- <p> [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency <ref> [ 10] </ref> = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total <p> [ 0] = 0 Latency [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency <ref> [ 10] </ref> = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total
Reference: [11] <institution> Cray Research, Inc, Cray C90 Systems, </institution> <year> 1995. </year> <note> Available from http://www.cray.com. </note>
Reference-contexts: UMA machines are also referred to as centralized shared-memory machines. Note that the memory modules can be physically distributed in multiple boards, and may even be in the same boards as processors. Examples of UMA machines are the Cray C90 <ref> [11] </ref> and the Alliant FX/2800 [12]. The presence of caches associated with processors, a common approach in multiprocessors, disturbs the UMA behavior of a dance-hall organization. <p> [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency <ref> [ 11] </ref> = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency [ 17] = 86 Latency [ 18] = 46 280 Latency [ 19] = 3673 --------------------- Total = 362996 Average = <p> [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency <ref> [ 11] </ref> = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380992 Average = 7.134187 <p> [ 1] = 0 Latency [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency <ref> [ 11] </ref> = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380019 Average = 7.170676
Reference: [12] <institution> Alliant Computer Systems Corporation, Littleton, MA, FX/2800 System Description, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: UMA machines are also referred to as centralized shared-memory machines. Note that the memory modules can be physically distributed in multiple boards, and may even be in the same boards as processors. Examples of UMA machines are the Cray C90 [11] and the Alliant FX/2800 <ref> [12] </ref>. The presence of caches associated with processors, a common approach in multiprocessors, disturbs the UMA behavior of a dance-hall organization. <p> [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency <ref> [ 12] </ref> = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency [ 17] = 86 Latency [ 18] = 46 280 Latency [ 19] = 3673 --------------------- Total = 362996 Average = 8.718440 We note that the <p> [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency <ref> [ 12] </ref> = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380992 Average = 7.134187 Since we increased the network <p> [ 2] = 0 Latency [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency <ref> [ 12] </ref> = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380019 Average = 7.170676 We have achieved a fixed
Reference: [13] <institution> Kendall Square Research Corporation, Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: Some machines have a cache-only memory architecture (COMA) <ref> [13] </ref>, in which all the data resides in caches, as shown in Figure 2.3. These organizations are also pseudo-UMA in that the time-variant section of the address space in the local cache can be accessed much faster than the section mapped to remote caches. <p> The Sun SparcCenter 2000 [14], the SGI Challenge [15], and the HP-9000 model T500 [16] are examples of pseudo-UMA machines because the local cache in each processor causes nonuniformity of access. The Kendall Square KSR1 <ref> [13] </ref> is a COMA machine. A typical NUMA organization is shown in Figure 2.4. The memory modules are associated with processors, which can access their local modules much faster than the remote modules. <p> [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency <ref> [ 13] </ref> = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency [ 17] = 86 Latency [ 18] = 46 280 Latency [ 19] = 3673 --------------------- Total = 362996 Average = 8.718440 We note that the minimum latency observed is seven <p> [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency <ref> [ 13] </ref> = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380992 Average = 7.134187 Since we increased the network latency, in the instruction level <p> [ 3] = 0 Latency [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency <ref> [ 13] </ref> = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380019 Average = 7.170676 We have achieved a fixed point for the latency, and
Reference: [14] <institution> Sun Microsystems, Inc., Mountain View, CA, </institution> <note> SPARCcenter 2000 Product Overview. Available from http://www.sun.com. </note>
Reference-contexts: These organizations are also pseudo-UMA in that the time-variant section of the address space in the local cache can be accessed much faster than the section mapped to remote caches. The Sun SparcCenter 2000 <ref> [14] </ref>, the SGI Challenge [15], and the HP-9000 model T500 [16] are examples of pseudo-UMA machines because the local cache in each processor causes nonuniformity of access. The Kendall Square KSR1 [13] is a COMA machine. A typical NUMA organization is shown in Figure 2.4. <p> [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency <ref> [ 14] </ref> = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency [ 17] = 86 Latency [ 18] = 46 280 Latency [ 19] = 3673 --------------------- Total = 362996 Average = 8.718440 We note that the minimum latency observed is seven cycles, as expected (the (8; <p> [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency <ref> [ 14] </ref> = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380992 Average = 7.134187 Since we increased the network latency, in the instruction level simulator, the rate at which <p> [ 4] = 0 Latency [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency <ref> [ 14] </ref> = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380019 Average = 7.170676 We have achieved a fixed point for the latency, and we conclude that the average
Reference: [15] <author> M. Galles and E. Williams, </author> <title> "Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor," </title> <type> tech. rep., </type> <institution> Silicon Graphics Computer Systems, </institution> <year> 1994. </year> <note> Available from http://www.sgi.com. </note>
Reference-contexts: These organizations are also pseudo-UMA in that the time-variant section of the address space in the local cache can be accessed much faster than the section mapped to remote caches. The Sun SparcCenter 2000 [14], the SGI Challenge <ref> [15] </ref>, and the HP-9000 model T500 [16] are examples of pseudo-UMA machines because the local cache in each processor causes nonuniformity of access. The Kendall Square KSR1 [13] is a COMA machine. A typical NUMA organization is shown in Figure 2.4. <p> This justifies the approach in this thesis: considering two specialized machine models. The SGI Challenge multiprocessor <ref> [15] </ref> was used as a target machine for the implementation of autoscheduling in this thesis. The Challenge can be configured with up to 36 MIPS R4400 processors. Each processor runs at 150 MHz and has a peak performance of 75 Mflops. <p> [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency <ref> [ 15] </ref> = 906 Latency [ 16] = 396 Latency [ 17] = 86 Latency [ 18] = 46 280 Latency [ 19] = 3673 --------------------- Total = 362996 Average = 8.718440 We note that the minimum latency observed is seven cycles, as expected (the (8; 2) network has three stages <p> [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency <ref> [ 15] </ref> = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380992 Average = 7.134187 Since we increased the network latency, in the instruction level simulator, the rate at which processors present requests to the <p> [ 5] = 0 Latency [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency <ref> [ 15] </ref> = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380019 Average = 7.170676 We have achieved a fixed point for the latency, and we conclude that the average latency of the network, during
Reference: [16] <author> T. B. Alexander, K. G. Robertson, D. T. Lindsay, D. L. Rogers, J. R. Obermeyer, J. R. Keller, K. Y. Oka, and M. M. Jones, </author> <title> II, "Corporate business servers: An alterntive to mainframes for business computing," </title> <journal> Hewlett-Packard Journal, </journal> <pages> pp. 8-30, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: These organizations are also pseudo-UMA in that the time-variant section of the address space in the local cache can be accessed much faster than the section mapped to remote caches. The Sun SparcCenter 2000 [14], the SGI Challenge [15], and the HP-9000 model T500 <ref> [16] </ref> are examples of pseudo-UMA machines because the local cache in each processor causes nonuniformity of access. The Kendall Square KSR1 [13] is a COMA machine. A typical NUMA organization is shown in Figure 2.4. <p> [ 6] = 0 Latency [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency <ref> [ 16] </ref> = 396 Latency [ 17] = 86 Latency [ 18] = 46 280 Latency [ 19] = 3673 --------------------- Total = 362996 Average = 8.718440 We note that the minimum latency observed is seven cycles, as expected (the (8; 2) network has three stages of switch, therefore the minimum <p> [ 6] = 0 Latency [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency <ref> [ 16] </ref> = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380992 Average = 7.134187 Since we increased the network latency, in the instruction level simulator, the rate at which processors present requests to the network decreased (our processor does <p> [ 6] = 0 Latency [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency <ref> [ 16] </ref> = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380019 Average = 7.170676 We have achieved a fixed point for the latency, and we conclude that the average latency of the network, during the execution of the complex
Reference: [17] <author> Cray Research, Inc., </author> <title> Cray T3D System Architecture Overview Manual, </title> <note> 1993. Available from http://www.cray.com. </note>
Reference-contexts: They are commonly referred to as distributed shared-memory machines. As in the case of UMA machines, processors can have private caches, adding another level to the hierarchy of memory accesses. 9 The Cray T3D <ref> [17] </ref> is an example of a NUMA machine, while the Convex Exemplar [18] is a cluster NUMA machine. A survey of distributed shared memory systems can be found in [19]. Different approaches are used to enhance performance by exploiting nonuniformity of memory accesses. <p> [ 7] = 100215 Latency [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency <ref> [ 17] </ref> = 86 Latency [ 18] = 46 280 Latency [ 19] = 3673 --------------------- Total = 362996 Average = 8.718440 We note that the minimum latency observed is seven cycles, as expected (the (8; 2) network has three stages of switch, therefore the minimum latency is three cycles in <p> [ 7] = 333496 Latency [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency <ref> [ 17] </ref> = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380992 Average = 7.134187 Since we increased the network latency, in the instruction level simulator, the rate at which processors present requests to the network decreased (our processor does not yet allow multiple pending <p> [ 7] = 318550 Latency [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency <ref> [ 17] </ref> = 0 Latency [ 18] = 0 Latency [ 19] = 0 --------------------- Total = 380019 Average = 7.170676 We have achieved a fixed point for the latency, and we conclude that the average latency of the network, during the execution of the complex matrix multiply program, is seven
Reference: [18] <institution> Convex Computer Corporation, Richardson, TX, Exemplar SPP1000, </institution> <year> 1994. </year> <title> In Japanese. </title>
Reference-contexts: They are commonly referred to as distributed shared-memory machines. As in the case of UMA machines, processors can have private caches, adding another level to the hierarchy of memory accesses. 9 The Cray T3D [17] is an example of a NUMA machine, while the Convex Exemplar <ref> [18] </ref> is a cluster NUMA machine. A survey of distributed shared memory systems can be found in [19]. Different approaches are used to enhance performance by exploiting nonuniformity of memory accesses. The importance of the approach depends on the source of the uniformity. <p> [ 8] = 103453 Latency [ 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency [ 17] = 86 Latency <ref> [ 18] </ref> = 46 280 Latency [ 19] = 3673 --------------------- Total = 362996 Average = 8.718440 We note that the minimum latency observed is seven cycles, as expected (the (8; 2) network has three stages of switch, therefore the minimum latency is three cycles in the forward network, one cycle <p> [ 8] = 43974 Latency [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency <ref> [ 18] </ref> = 0 Latency [ 19] = 0 --------------------- Total = 380992 Average = 7.134187 Since we increased the network latency, in the instruction level simulator, the rate at which processors present requests to the network decreased (our processor does not yet allow multiple pending memory requests), and the network <p> [ 8] = 58298 Latency [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency <ref> [ 18] </ref> = 0 Latency [ 19] = 0 --------------------- Total = 380019 Average = 7.170676 We have achieved a fixed point for the latency, and we conclude that the average latency of the network, during the execution of the complex matrix multiply program, is seven cycles. 282
Reference: [19] <author> B. Nitzberg and V. Lo, </author> <title> "Distributed shared memory: A survey of issues and algorithms," </title> <journal> IEEE Computer, </journal> <volume> vol. 24, </volume> <month> August </month> <year> 1991. </year>
Reference-contexts: A survey of distributed shared memory systems can be found in <ref> [19] </ref>. Different approaches are used to enhance performance by exploiting nonuniformity of memory accesses. The importance of the approach depends on the source of the uniformity. When caches cause nonuniformity, the data access pattern can have a significant impact on performance. <p> 9] = 70088 Latency [ 10] = 41828 Latency [ 11] = 23233 Latency [ 12] = 11318 Latency [ 13] = 5380 Latency [ 14] = 2374 Latency [ 15] = 906 Latency [ 16] = 396 Latency [ 17] = 86 Latency [ 18] = 46 280 Latency <ref> [ 19] </ref> = 3673 --------------------- Total = 362996 Average = 8.718440 We note that the minimum latency observed is seven cycles, as expected (the (8; 2) network has three stages of switch, therefore the minimum latency is three cycles in the forward network, one cycle in memory, and three cycles in <p> [ 9] = 3428 Latency [ 10] = 88 Latency [ 11] = 3 Latency [ 12] = 1 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency <ref> [ 19] </ref> = 0 --------------------- Total = 380992 Average = 7.134187 Since we increased the network latency, in the instruction level simulator, the rate at which processors present requests to the network decreased (our processor does not yet allow multiple pending memory requests), and the network latency consequently decreased (because there <p> [ 9] = 2971 Latency [ 10] = 187 Latency [ 11] = 9 Latency [ 12] = 2 Latency [ 13] = 1 Latency [ 14] = 1 Latency [ 15] = 0 Latency [ 16] = 0 Latency [ 17] = 0 Latency [ 18] = 0 Latency <ref> [ 19] </ref> = 0 --------------------- Total = 380019 Average = 7.170676 We have achieved a fixed point for the latency, and we conclude that the average latency of the network, during the execution of the complex matrix multiply program, is seven cycles. 282
Reference: [20] <author> M. Papamarcos and J. Patel, </author> <title> "A low overhead coherent solution for multiprocessors with private cache memories," </title> <booktitle> in Proceedings of 11th International Symposium on Computer Architecture, </booktitle> <address> (NY), </address> <pages> pp. 348-354, </pages> <publisher> IEEE, </publisher> <year> 1984. </year>
Reference-contexts: The cache line sizes are 16 B for the level-1 data cache and 128 B for the level-2 cache. All caches are direct mapped. The caches are kept coherent by hardware through a write-invalidate protocol similar to the Illinois Protocol <ref> [20] </ref>. The processors are connected to the main memory through a single system bus, which also carries the cache coherence traffic. The bus operates at 47.6 MHz, and with a 256-bit data width, it can sustain transfer rates of 1.2 GB/s.
Reference: [21] <author> A. Choudry, S. Hiranandani, G. Fox, K. Kennedy, S. Ranka, C. Koelbel, and C.-W. Tseng, </author> <title> "Compiling Fortran 77D and 90D for MIMD distributed-memory machines," </title> <booktitle> in Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pp. 4-11, </pages> <year> 1992. </year>
Reference-contexts: They can be used to save temporary values that each processor computed. CDOALL and CDOACROSS loops do not allow ENDLOOP statements. 3.1.2 Fortran D Fortran D <ref> [4, 21, 22, 23, 24] </ref> is a version of Fortran enhanced with data decomposition specifications. Fortran D is targeted at data-parallel numerical applications, and is primarily concerned with the exploitation of fine-grain parallelism at the array operation level. <p> It supports both loop and functional parallelisms, in the form of PARALLEL LOOP and PARALLEL CASE constructs, respectively. Techniques for the compilation of Fortran dialects with data distribution features (Fortran D, HPF) are discussed in <ref> [21, 23, 74, 75] </ref>. Compile-based, automatic (as opposed to user-specified) data distribution is discussed in [76, 77, 78, 79, 80]. 38 Decomposition and scheduling algorithms that attempt to co-locate process and data in shared-memory multiprocessors, in order to reduce the costs of remote memory accesses, are discussed in [81].
Reference: [22] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C.-W. Tseng, </author> <title> "Interprocedural compilation of Fortran D for MIMD distributed memory machines," </title> <booktitle> in Proceedings of Supercomputing'92, </booktitle> <pages> pp. 522-534, </pages> <year> 1992. </year>
Reference-contexts: They can be used to save temporary values that each processor computed. CDOALL and CDOACROSS loops do not allow ENDLOOP statements. 3.1.2 Fortran D Fortran D <ref> [4, 21, 22, 23, 24] </ref> is a version of Fortran enhanced with data decomposition specifications. Fortran D is targeted at data-parallel numerical applications, and is primarily concerned with the exploitation of fine-grain parallelism at the array operation level.
Reference: [23] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng, </author> <title> "Compiler support for machine-independent parallel programming in Fortran D," </title> <type> Tech. Rep. </type> <institution> COMP TR91-149, Department of Computer Science, Rice University, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: They can be used to save temporary values that each processor computed. CDOALL and CDOACROSS loops do not allow ENDLOOP statements. 3.1.2 Fortran D Fortran D <ref> [4, 21, 22, 23, 24] </ref> is a version of Fortran enhanced with data decomposition specifications. Fortran D is targeted at data-parallel numerical applications, and is primarily concerned with the exploitation of fine-grain parallelism at the array operation level. <p> It supports both loop and functional parallelisms, in the form of PARALLEL LOOP and PARALLEL CASE constructs, respectively. Techniques for the compilation of Fortran dialects with data distribution features (Fortran D, HPF) are discussed in <ref> [21, 23, 74, 75] </ref>. Compile-based, automatic (as opposed to user-specified) data distribution is discussed in [76, 77, 78, 79, 80]. 38 Decomposition and scheduling algorithms that attempt to co-locate process and data in shared-memory multiprocessors, in order to reduce the costs of remote memory accesses, are discussed in [81].
Reference: [24] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng, </author> <title> "Compiling Fortran D for MIMD distributed-memory machines," </title> <journal> Communications of the ACM, </journal> <volume> vol. 35, </volume> <pages> pp. 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: They can be used to save temporary values that each processor computed. CDOALL and CDOACROSS loops do not allow ENDLOOP statements. 3.1.2 Fortran D Fortran D <ref> [4, 21, 22, 23, 24] </ref> is a version of Fortran enhanced with data decomposition specifications. Fortran D is targeted at data-parallel numerical applications, and is primarily concerned with the exploitation of fine-grain parallelism at the array operation level. <p> Example (1): A particular subset of space descriptors, rectangular sections, can be easily described by regular section descriptors (RSDs) <ref> [24] </ref>, which are sequences of triples (l i : u i : s i ), where l i , u i and s i indicate the lower bound, upper bound, and step, respectively, along the i-th dimension of the RSD. <p> The translation scheme for DOALLs is illustrated in Figure 6.33. The for-nest in the C++ code computes the iteration space. One obvious target for optimization is to compute the iteration space at compile time using, for instance, regular section descriptors (RSDs) <ref> [24] </ref>. 6.7 Run-Time Library The run-time library is a library of functions that provides basic services to the autoschedul-ing program. There are three main types of functions in the library: 1.
Reference: [25] <author> C. Koelbel, P. Mehrotra, and J. V. Rosendale, </author> <title> "Supporting shared data structures on distributed memory machines," </title> <booktitle> in Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> (Seattle, WA), </address> <month> March </month> <year> 1990. </year>
Reference-contexts: ENDFOR 21 states that iteration i is to be executed by the processor to which A (i) is assigned. Therefore processor p 1 executes iterations f1; 2; 3; 4g. The ON clause is a feature borrowed from the language Kali <ref> [25] </ref>. 3.1.3 HPF The High Performance Fortran (HPF) [6, 26, 27] language was designed as a set of extensions and modifications to Fortran 90 to support data parallel programming.
Reference: [26] <author> D. B. Loveman, </author> <title> "High Performance Fortran," </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> vol. 1, </volume> <pages> pp. 24-42, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: ENDFOR 21 states that iteration i is to be executed by the processor to which A (i) is assigned. Therefore processor p 1 executes iterations f1; 2; 3; 4g. The ON clause is a feature borrowed from the language Kali [25]. 3.1.3 HPF The High Performance Fortran (HPF) <ref> [6, 26, 27] </ref> language was designed as a set of extensions and modifications to Fortran 90 to support data parallel programming. The ability to achieve top performance on MIMD and SIMD computers with nonuniform memory access was one of the main goals of the project.
Reference: [27] <author> B. J. N. Wylie, M. G. Norman, and L. J. Clarke, </author> <title> "High Performance Fortran: A perspective," </title> <type> Tech. Rep. </type> <institution> EPCC-TN92-05.04, University of Edinburgh, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: ENDFOR 21 states that iteration i is to be executed by the processor to which A (i) is assigned. Therefore processor p 1 executes iterations f1; 2; 3; 4g. The ON clause is a feature borrowed from the language Kali [25]. 3.1.3 HPF The High Performance Fortran (HPF) <ref> [6, 26, 27] </ref> language was designed as a set of extensions and modifications to Fortran 90 to support data parallel programming. The ability to achieve top performance on MIMD and SIMD computers with nonuniform memory access was one of the main goals of the project.
Reference: [28] <author> S. Benkner, B. M. Chapman, and H. Zima, </author> <title> "Vienna Fortran 90," </title> <booktitle> in Proceedings of the Scalable High Performance Computing Conference SHPCC-92, </booktitle> <pages> pp. 51-59, </pages> <year> 1992. </year> <month> 284 </month>
Reference-contexts: The ability to achieve top performance on MIMD and SIMD computers with nonuniform memory access was one of the main goals of the project. The design of HPF was influenced by Fortran D and Vienna Fortran <ref> [28, 29] </ref>. Just as Fortran D approaches the problem of data partitioning and distribution in two stages, HPF uses three. First, arrays are aligned to each other. Second, arrays are distributed across a user-defined rectilinear arrangement of abstract processors. <p> DO J = 1, N DO K = 1, N END DO END DO CDIR$ DOSHARED (I 1; I 2; : : : ; I n) ON array reference DO I 1 = L 1; U 1; S 1 ... statements END DO END DO 3.1.5 Other languages Vienna Fortran <ref> [28, 30, 31] </ref> is another dialect of Fortran with support for data distribution. Fortran M [32] is an extension to Fortran 77 that supports loop and functional parallelisms in message-passing programs. Extensions to Fortran 90 to support task (functional) parallelism are discussed in [33].
Reference: [29] <author> H. P. Zima and B. M. Chapman, </author> <title> "Compiling for distributed-memory systems," </title> <journal> Proceed--ings of the IEEE, </journal> <volume> vol. 81, </volume> <pages> pp. 264-287, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The ability to achieve top performance on MIMD and SIMD computers with nonuniform memory access was one of the main goals of the project. The design of HPF was influenced by Fortran D and Vienna Fortran <ref> [28, 29] </ref>. Just as Fortran D approaches the problem of data partitioning and distribution in two stages, HPF uses three. First, arrays are aligned to each other. Second, arrays are distributed across a user-defined rectilinear arrangement of abstract processors.
Reference: [30] <author> B. Chapman, P. Mehrotra, and H. Zima, </author> <title> "User defined mappings in Vienna Fortran," </title> <booktitle> in Proceedings of the Workshop on Languages, Compilers and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <pages> pp. 72-75, </pages> <year> 1993. </year>
Reference-contexts: DO J = 1, N DO K = 1, N END DO END DO CDIR$ DOSHARED (I 1; I 2; : : : ; I n) ON array reference DO I 1 = L 1; U 1; S 1 ... statements END DO END DO 3.1.5 Other languages Vienna Fortran <ref> [28, 30, 31] </ref> is another dialect of Fortran with support for data distribution. Fortran M [32] is an extension to Fortran 77 that supports loop and functional parallelisms in message-passing programs. Extensions to Fortran 90 to support task (functional) parallelism are discussed in [33].
Reference: [31] <author> B. Chapman, H. Zima, and P. Mehrotra, </author> <title> "Handling distributed data in Vienna Fortran procedures," </title> <booktitle> in Proceedings of the Fifth International Workshop on Languages and Compilers for Parallel Computing (U. </booktitle> <editor> Banerjee, D. Gelernter, A. Nicolau, and D. Padua, eds.), </editor> <volume> vol. </volume> <booktitle> 757 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 248-263, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: DO J = 1, N DO K = 1, N END DO END DO CDIR$ DOSHARED (I 1; I 2; : : : ; I n) ON array reference DO I 1 = L 1; U 1; S 1 ... statements END DO END DO 3.1.5 Other languages Vienna Fortran <ref> [28, 30, 31] </ref> is another dialect of Fortran with support for data distribution. Fortran M [32] is an extension to Fortran 77 that supports loop and functional parallelisms in message-passing programs. Extensions to Fortran 90 to support task (functional) parallelism are discussed in [33].
Reference: [32] <author> I. T. Foster and K. M. Chandy, </author> <title> "Fortran M: A language for modular parallel programming," </title> <type> tech. rep., </type> <institution> Argonne National Laboratory, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Fortran M <ref> [32] </ref> is an extension to Fortran 77 that supports loop and functional parallelisms in message-passing programs. Extensions to Fortran 90 to support task (functional) parallelism are discussed in [33]. Languages with data distribution features are not limited to dialects of Fortran.
Reference: [33] <author> B. Chapman, P. Mehrotra, J. V. Rosendale, and H. Zima, </author> <title> "A software architecture for multidisciplinary applications: Integrating task and data parallelism," </title> <type> Tech. Rep. 94-18, </type> <institution> Institute for Computer Applications in Science and Engineering, NASA Langley Research Center, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Fortran M [32] is an extension to Fortran 77 that supports loop and functional parallelisms in message-passing programs. Extensions to Fortran 90 to support task (functional) parallelism are discussed in <ref> [33] </ref>. Languages with data distribution features are not limited to dialects of Fortran. Distributed pC++ [34] adds parallel and data distribution constructs to C++. Other parallel versions of C++ include Composition C++ [35], C** [36], COOL [37], and CHARM++ [38].
Reference: [34] <author> F. Bodin, P. Beckmann, D. Gannon, S. Narayana, and S. X. Yang, </author> <title> "Distributed pC++: Basic ideas for an object parallel language," </title> <type> tech. rep., </type> <institution> Indiana University, </institution> <year> 1994. </year>
Reference-contexts: Fortran M [32] is an extension to Fortran 77 that supports loop and functional parallelisms in message-passing programs. Extensions to Fortran 90 to support task (functional) parallelism are discussed in [33]. Languages with data distribution features are not limited to dialects of Fortran. Distributed pC++ <ref> [34] </ref> adds parallel and data distribution constructs to C++. Other parallel versions of C++ include Composition C++ [35], C** [36], COOL [37], and CHARM++ [38]. The pC (parallel C) programming language [39] is a parallel extension to ANSI C.
Reference: [35] <author> K. M. Chandy and C. Kesselman, </author> <title> "Compositional C++: Compositional parallel programming," </title> <booktitle> in Proceedings of the Fifth International Workshop on Languages and Compilers for Parallel Computing (U. </booktitle> <editor> Banerjee, D. Gelernter, A. Nicolau, and D. Padua, eds.), </editor> <volume> vol. </volume> <booktitle> 757 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 1-29, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: Extensions to Fortran 90 to support task (functional) parallelism are discussed in [33]. Languages with data distribution features are not limited to dialects of Fortran. Distributed pC++ [34] adds parallel and data distribution constructs to C++. Other parallel versions of C++ include Composition C++ <ref> [35] </ref>, C** [36], COOL [37], and CHARM++ [38]. The pC (parallel C) programming language [39] is a parallel extension to ANSI C. Concurrent Aggregates (CA) [40, 41] is a parallel language based on LISP.
Reference: [36] <author> J. Larus, </author> <title> "C**: A large-grain, object oriented, data-parallel programming language," </title> <booktitle> in Proceedings of the Fifth International Workshop on Languages and Compilers for Parallel Computing (U. </booktitle> <editor> Banerjee, D. Gelernter, A. Nicolau, and D. Padua, eds.), </editor> <volume> vol. </volume> <booktitle> 757 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 326-341, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: Extensions to Fortran 90 to support task (functional) parallelism are discussed in [33]. Languages with data distribution features are not limited to dialects of Fortran. Distributed pC++ [34] adds parallel and data distribution constructs to C++. Other parallel versions of C++ include Composition C++ [35], C** <ref> [36] </ref>, COOL [37], and CHARM++ [38]. The pC (parallel C) programming language [39] is a parallel extension to ANSI C. Concurrent Aggregates (CA) [40, 41] is a parallel language based on LISP.
Reference: [37] <author> R. Chandra, A. Gupta, and J. L. Hennessy, </author> <title> "COOL: A language for parallel programming," </title> <type> Tech. Rep. </type> <institution> CSL-TR-89-396, Computer Systems Laboratory, Stanford University, </institution> <year> 1989. </year>
Reference-contexts: Extensions to Fortran 90 to support task (functional) parallelism are discussed in [33]. Languages with data distribution features are not limited to dialects of Fortran. Distributed pC++ [34] adds parallel and data distribution constructs to C++. Other parallel versions of C++ include Composition C++ [35], C** [36], COOL <ref> [37] </ref>, and CHARM++ [38]. The pC (parallel C) programming language [39] is a parallel extension to ANSI C. Concurrent Aggregates (CA) [40, 41] is a parallel language based on LISP.
Reference: [38] <author> L. V. Kale and S. Krishnan, "CHARM++: </author> <title> A portable concurrent object oriented system based on C++," </title> <booktitle> in Proceedings of OOPSLA'93, </booktitle> <year> 1993. </year>
Reference-contexts: Languages with data distribution features are not limited to dialects of Fortran. Distributed pC++ [34] adds parallel and data distribution constructs to C++. Other parallel versions of C++ include Composition C++ [35], C** [36], COOL [37], and CHARM++ <ref> [38] </ref>. The pC (parallel C) programming language [39] is a parallel extension to ANSI C. Concurrent Aggregates (CA) [40, 41] is a parallel language based on LISP.
Reference: [39] <author> R. Canetti, L. P. Fertig, S. A. Kravitz, D. Malki, R. Y. Pinter, S. Porat, and A. Teperman, </author> <title> "The parallel C (pC) programming language," </title> <journal> IBM J. Research and Development, </journal> <volume> vol. 35, </volume> <month> September/November </month> <year> 1991. </year>
Reference-contexts: Languages with data distribution features are not limited to dialects of Fortran. Distributed pC++ [34] adds parallel and data distribution constructs to C++. Other parallel versions of C++ include Composition C++ [35], C** [36], COOL [37], and CHARM++ [38]. The pC (parallel C) programming language <ref> [39] </ref> is a parallel extension to ANSI C. Concurrent Aggregates (CA) [40, 41] is a parallel language based on LISP.
Reference: [40] <author> A. A. Chien and W. J. Dally, </author> <title> "Concurrent aggregates (CA)," </title> <booktitle> in Proceedings of the Second Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 187-196, </pages> <publisher> ACM, </publisher> <month> March </month> <year> 1990. </year> <month> 285 </month>
Reference-contexts: Distributed pC++ [34] adds parallel and data distribution constructs to C++. Other parallel versions of C++ include Composition C++ [35], C** [36], COOL [37], and CHARM++ [38]. The pC (parallel C) programming language [39] is a parallel extension to ANSI C. Concurrent Aggregates (CA) <ref> [40, 41] </ref> is a parallel language based on LISP. A survey of object-oriented parallel languages is presented in [42]. 3.2 Macro Data Flow Systems The theory of partioning and scheduling parallel programs for execution with a macro data flow model is described in [43].
Reference: [41] <author> A. A. Chien, V. Karamcheti, J. Plevyak, and X. Zhang, </author> <title> "Concurrent aggregates (CA) language report," </title> <type> tech. rep., </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Distributed pC++ [34] adds parallel and data distribution constructs to C++. Other parallel versions of C++ include Composition C++ [35], C** [36], COOL [37], and CHARM++ [38]. The pC (parallel C) programming language [39] is a parallel extension to ANSI C. Concurrent Aggregates (CA) <ref> [40, 41] </ref> is a parallel language based on LISP. A survey of object-oriented parallel languages is presented in [42]. 3.2 Macro Data Flow Systems The theory of partioning and scheduling parallel programs for execution with a macro data flow model is described in [43].
Reference: [42] <author> B. B. Wyatt, K. Kavi, and S. Hufnagel, </author> <title> "Parallelism in object-oriented languages: A survey," </title> <journal> IEEE Software, </journal> <volume> vol. 9, </volume> <pages> pp. 56-66, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The pC (parallel C) programming language [39] is a parallel extension to ANSI C. Concurrent Aggregates (CA) [40, 41] is a parallel language based on LISP. A survey of object-oriented parallel languages is presented in <ref> [42] </ref>. 3.2 Macro Data Flow Systems The theory of partioning and scheduling parallel programs for execution with a macro data flow model is described in [43]. Theoretical issues on the clustering of task graphs, which assigns tasks to processors, are addressed in [44].
Reference: [43] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <booktitle> Research Monographs in Parallel and Distributed Computing, </booktitle> <address> Cambridge, MA: </address> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Concurrent Aggregates (CA) [40, 41] is a parallel language based on LISP. A survey of object-oriented parallel languages is presented in [42]. 3.2 Macro Data Flow Systems The theory of partioning and scheduling parallel programs for execution with a macro data flow model is described in <ref> [43] </ref>. Theoretical issues on the clustering of task graphs, which assigns tasks to processors, are addressed in [44]. Techniques for processor allocation in macro data flow graphs are also discussed in [45, 46, 47].
Reference: [44] <author> A. Gerasoulis, S. Venugopal, and T. Yang, </author> <title> "Clustering task graphs for message passing architectures," </title> <booktitle> in Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pp. 447-456, </pages> <month> June 11-15, </month> <year> 1990. </year>
Reference-contexts: Theoretical issues on the clustering of task graphs, which assigns tasks to processors, are addressed in <ref> [44] </ref>. Techniques for processor allocation in macro data flow graphs are also discussed in [45, 46, 47]. Due to limited space, we discuss in detail two implementations of macro data flow systems most directly related to autoscheduling.
Reference: [45] <author> G. N. S. Prasanna and A. Agarwal, </author> <title> "Compile-time techniques for processor allocation in macro dataflow graphs for multiprocessors," </title> <booktitle> in Proceedings of the 1992 International Conference on Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 279-283, </pages> <month> August 17-21, </month> <year> 1992. </year>
Reference-contexts: Theoretical issues on the clustering of task graphs, which assigns tasks to processors, are addressed in [44]. Techniques for processor allocation in macro data flow graphs are also discussed in <ref> [45, 46, 47] </ref>. Due to limited space, we discuss in detail two implementations of macro data flow systems most directly related to autoscheduling.
Reference: [46] <author> T. Yang and A. Gerasoulis, </author> <title> "A fast static scheduling algorithm for DAGs on an unbounded number of processors," </title> <booktitle> in Proceedings of Supercomputing'91, </booktitle> <pages> pp. 633-642, </pages> <month> November 18-22, </month> <year> 1991. </year>
Reference-contexts: Theoretical issues on the clustering of task graphs, which assigns tasks to processors, are addressed in [44]. Techniques for processor allocation in macro data flow graphs are also discussed in <ref> [45, 46, 47] </ref>. Due to limited space, we discuss in detail two implementations of macro data flow systems most directly related to autoscheduling.
Reference: [47] <author> T. Yang and A. Gerasoulis, </author> <title> "PYRROS: Static scheduling and code generation for message passing multiprocessors," </title> <booktitle> in Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pp. 428-437, </pages> <month> July 19-23, </month> <year> 1992. </year>
Reference-contexts: Theoretical issues on the clustering of task graphs, which assigns tasks to processors, are addressed in [44]. Techniques for processor allocation in macro data flow graphs are also discussed in <ref> [45, 46, 47] </ref>. Due to limited space, we discuss in detail two implementations of macro data flow systems most directly related to autoscheduling.
Reference: [48] <author> S. Ramaswamy, S. Sapatnekar, and P. Banerjee, </author> <title> "A framework for exploiting data and functional parallelism on distributed memory multicomputers," </title> <type> Tech. Rep. </type> <institution> CRHC-94-10, Center for Reliable and High Performance Computing, University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: Due to limited space, we discuss in detail two implementations of macro data flow systems most directly related to autoscheduling. Then, we briefly discuss other approaches to macro data flow. 30 3.2.1 Paradigm The Paradigm compiler project <ref> [48, 49, 50] </ref> is intended to be a parallelizing compiler for Fortran 77 and HPF programs. When fully implemented, it will automatically annotate Fortran 77 programs with HPF data distribution directives, partition computations and generate communications for HPF programs, and exploit functional and data (loop) parallelism in HPF programs.
Reference: [49] <author> S. Ramaswamy, S. Sapatnekar, and P. Banerjee, </author> <title> "A convex programming approach for exploiting data and functional parallelism on distributed memory multicomputers," </title> <booktitle> in Proceedings of the 1994 Internationcal Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> II:116-125, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Due to limited space, we discuss in detail two implementations of macro data flow systems most directly related to autoscheduling. Then, we briefly discuss other approaches to macro data flow. 30 3.2.1 Paradigm The Paradigm compiler project <ref> [48, 49, 50] </ref> is intended to be a parallelizing compiler for Fortran 77 and HPF programs. When fully implemented, it will automatically annotate Fortran 77 programs with HPF data distribution directives, partition computations and generate communications for HPF programs, and exploit functional and data (loop) parallelism in HPF programs. <p> Inside each node, data parallelism is used across the processors. The scheduling phase decides on a particular scheme of execution for the nodes, using for each one the allocated number of processors. The following example from <ref> [49] </ref> illustrates the approach. Consider the MDG of Figure 3.9. Each of the nodes, N 1 ; N 2 , and N 3 , can be executed in a loop parallel form, with a characteristic efficiency curve (the efficiency curve is a plot of efficiency fi number of processors).
Reference: [50] <author> S. Ramaswamy and P. Banerjee, </author> <title> "Processor allocation and scheduling of macro dataflow graphs on distributed memory multicomputers by the PARADIGM compiler," </title> <booktitle> in Proceedings of the 1993 Internationcal Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> II:134-138, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Due to limited space, we discuss in detail two implementations of macro data flow systems most directly related to autoscheduling. Then, we briefly discuss other approaches to macro data flow. 30 3.2.1 Paradigm The Paradigm compiler project <ref> [48, 49, 50] </ref> is intended to be a parallelizing compiler for Fortran 77 and HPF programs. When fully implemented, it will automatically annotate Fortran 77 programs with HPF data distribution directives, partition computations and generate communications for HPF programs, and exploit functional and data (loop) parallelism in HPF programs.
Reference: [51] <author> H. Kasahara, H. Honda, A. Mogi, A. Ogura, K. Fujiwara, and S. Narita, </author> <title> "A multi-grain parallelizing compilation scheme for OSCAR (optimally scheduled advanced multiprocessor," in Languages and Compilers for Parallel Computing (U. </title> <editor> Banerjee, D. Gelernter, A. Nicolau, and D. Padua, eds.), </editor> <volume> vol. </volume> <booktitle> 589 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 283-297, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: The dynamic scheduling and 31 32 multiple levels of parallelism of autoscheduling allow it to better adapt to conditions that cannot be predicted at compile time. 3.2.2 OSCAR A compiler based multilevel parallelism exploitation scheme was implemented in OSCAR <ref> [51, 52] </ref> (Optimally Scheduled Advanced Multiprocessor), which is a custom-built, shared-memory multiprocessor with up to 16 processor elements (PEs). The processors can be dynamically arranged in up to three clusters.
Reference: [52] <author> H. Kasahara, H. Honda, and S. Narita, </author> <title> "Parallel processing of near fine grain tasks using static scheduling on OSCAR," </title> <booktitle> in Proceedings of Supercomputing'90, </booktitle> <address> New York, New York, </address> <pages> pp. 856-864, </pages> <month> November </month> <year> 1990. </year> <month> 286 </month>
Reference-contexts: The dynamic scheduling and 31 32 multiple levels of parallelism of autoscheduling allow it to better adapt to conditions that cannot be predicted at compile time. 3.2.2 OSCAR A compiler based multilevel parallelism exploitation scheme was implemented in OSCAR <ref> [51, 52] </ref> (Optimally Scheduled Advanced Multiprocessor), which is a custom-built, shared-memory multiprocessor with up to 16 processor elements (PEs). The processors can be dynamically arranged in up to three clusters.
Reference: [53] <author> V. G. Grafe and J. E. Hoch, </author> <title> "The Epsilon-2 hybrid dataflow architecture," </title> <booktitle> in Proceedings of the 35 th IEEE Computer Society Conference. </booktitle> <address> Feb 26-Mar 2, 1990. San Francisco, CA, </address> <pages> pp. 88-93, </pages> <year> 1990. </year>
Reference-contexts: OSCAR also utilizes static scheduling for parallel loops, while parallel loops are scheduled dynamically in autoscheduling. 3.2.3 Other macro data flow systems It is well-recognized, by many authors <ref> [53, 54, 55, 56, 57] </ref>, that the pure data flow and pure control flow (or von Neumann) models of computations are actually the extremes of a whole spectrum of possible models. <p> A strongly connected block is a macro node, that is executed as if it were a single node, and can be implemented by a thread of instructions. The model used by the Epsilon-2 <ref> [53] </ref>, from Sandia National Laboratories, is very similar to the one used by EM-4.
Reference: [54] <author> R. A. </author> <title> Iannucci, "Toward a dataflow/von Neumann hybrid architecture," </title> <booktitle> in Proceedings of the 15th Annual International Symposium on Computer Architecture. </booktitle> <address> Honolulu, HI, </address> <pages> pp. 131-140, </pages> <month> May 30 - June 2, </month> <year> 1988. </year>
Reference-contexts: OSCAR also utilizes static scheduling for parallel loops, while parallel loops are scheduled dynamically in autoscheduling. 3.2.3 Other macro data flow systems It is well-recognized, by many authors <ref> [53, 54, 55, 56, 57] </ref>, that the pure data flow and pure control flow (or von Neumann) models of computations are actually the extremes of a whole spectrum of possible models. <p> Thread creation and synchronization are entirely controlled by the running program, but the architecture is, as in the previous cases, aimed at exploiting fine-grain parallelism, and does not have the granularity control provided by autoscheduling. Iannucci, in <ref> [54] </ref>, identifies the inability to tolerate latencies in memory and communication systems as a major fault in conventional multiprocessor systems, and the poor support for inexpensive synchronization as its main cause.
Reference: [55] <author> G. M. Papadopoulos and K. R. Traub, </author> <title> "Multithreading: A revisionist view of dataflow architectures," </title> <booktitle> in Proceedings of the 18 th International Symposium on Computer Architecture. </booktitle> <month> May 27-30, </month> <year> 1991, </year> <institution> Toronto, </institution> <address> Canada, </address> <pages> pp. 342-351, </pages> <year> 1991. </year>
Reference-contexts: OSCAR also utilizes static scheduling for parallel loops, while parallel loops are scheduled dynamically in autoscheduling. 3.2.3 Other macro data flow systems It is well-recognized, by many authors <ref> [53, 54, 55, 56, 57] </ref>, that the pure data flow and pure control flow (or von Neumann) models of computations are actually the extremes of a whole spectrum of possible models. <p> The ETS architecture directly executes data flow graphs, but the matching operations are performed by the instructions themselves, without the need for expensive matching hardware. Sequential threads of instructions are also supported, so the architecture can be classified as a hybrid. In fact, in <ref> [55] </ref>, Papadopoulos and Traub offer an alternate view of the the ETS architecture, in which multiple, interacting, sequential threads are executed, with extremely efficient forking and joining operations.
Reference: [56] <author> K. Hiraki, </author> <title> "Hybrid dataflow architecture." Slides from a presentation. </title>
Reference-contexts: OSCAR also utilizes static scheduling for parallel loops, while parallel loops are scheduled dynamically in autoscheduling. 3.2.3 Other macro data flow systems It is well-recognized, by many authors <ref> [53, 54, 55, 56, 57] </ref>, that the pure data flow and pure control flow (or von Neumann) models of computations are actually the extremes of a whole spectrum of possible models. <p> There is substantial research being performed by the above authors and others in defining hybrid models that combine the best of both worlds. Some of the new models are more data flow oriented (data flow - von Neumann hybrid, following the classification by Hiraki <ref> [56] </ref>), while others are more control flow oriented (von Neumann data flow hybrid, following the same classification). They are all oriented to overcome the deficiencies of either one of the "pure" models.
Reference: [57] <author> C. Polychronopoulos, </author> <title> "Auto-scheduling: Control flow and data flow come together," </title> <type> Tech. Rep. 1058, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: OSCAR also utilizes static scheduling for parallel loops, while parallel loops are scheduled dynamically in autoscheduling. 3.2.3 Other macro data flow systems It is well-recognized, by many authors <ref> [53, 54, 55, 56, 57] </ref>, that the pure data flow and pure control flow (or von Neumann) models of computations are actually the extremes of a whole spectrum of possible models. <p> The control and data dependence information between tasks, along with the capability for representing parallel loops, allow the dynamic exploitation of parallelism. The formal definition of the HTG and construction algorithms can be found in <ref> [57, 89, 90] </ref>. In this section we present the HTG as an abstract program model. The following definitions are based on the work of [89, 90, 91]. <p> This prototype autoscheduling compiler was the result of theoretical and implementation foundations described in <ref> [57, 89, 91] </ref>. We have been able to generate correct parallel code from a hierarchical task graph intermediate representation of a program. We have also been successful in exploiting the loop and functional parallelisms present in numerical codes.
Reference: [58] <author> G. M. Papadopoulos and D. E. Culler, "Monsoon: </author> <title> an explicit token-store architecture," </title> <booktitle> in Proceedings of the 17 th International Symposium on Computer Architecture. </booktitle> <address> May 28-31, 1990, Seattle, Washington, </address> <pages> pp. 82-91, </pages> <year> 1990. </year>
Reference-contexts: To overcome the difficulties in implementing a pure data flow machine, Papadopoulos and Culler developed a very simple model of dynamic data flow execution called the explicit token store architecture, or ETS, currently used in the Monsoon machine <ref> [58] </ref>. The ETS architecture directly executes data flow graphs, but the matching operations are performed by the instructions themselves, without the need for expensive matching hardware. Sequential threads of instructions are also supported, so the architecture can be classified as a hybrid.
Reference: [59] <author> K. Hiraki, T. Shimada, and K. Nishida, </author> <title> "A hardware design of the SIGMA-1, a data flow computer for scientific computations," </title> <booktitle> in Proceedings of the 1984 International Conference on Parallel Processing, </booktitle> <pages> pp. 524-531, </pages> <year> 1984. </year>
Reference-contexts: This approach reduces the number of synchronization operations as compared to the pure data flow model (once every several instructions instead of once every instruction), resulting in better overall performance. 35 Researchers at Electrotechnical Laboratory, who produced what probably is the only practical data flow computer available (the SIGMA-1 <ref> [59, 60, 61] </ref>), have now developed a new generation of (hybrid) data flow, the EM-4 [62]. The processing element of the machine, called EMC-R, is essentially a RISC processor augmented with hardware for data flow operation (a fetch and matching unit).
Reference: [60] <author> T. Shimada, K. Hiraki, K. Nishida, and S. Sekiguchi, </author> <title> "Evaluation of a prototype data flow processor of the SIGMA-1 for scientific computations," </title> <booktitle> in Proceedings of the 13 th International Symposium on Computer Architecture., </booktitle> <pages> pp. 226-234, </pages> <year> 1986. </year>
Reference-contexts: This approach reduces the number of synchronization operations as compared to the pure data flow model (once every several instructions instead of once every instruction), resulting in better overall performance. 35 Researchers at Electrotechnical Laboratory, who produced what probably is the only practical data flow computer available (the SIGMA-1 <ref> [59, 60, 61] </ref>), have now developed a new generation of (hybrid) data flow, the EM-4 [62]. The processing element of the machine, called EMC-R, is essentially a RISC processor augmented with hardware for data flow operation (a fetch and matching unit).
Reference: [61] <author> K. Hiraki, S. Sekiguchi, and T. Shimada, </author> <title> "Efficient vector processing on a dataflow supercomputer SIGMA-1," </title> <booktitle> in Proceedings of Supercomputing'88, </booktitle> <address> Orlando, FL, Nov 14-18, </address> <year> 1988, </year> <pages> pp. 374-381, </pages> <year> 1988. </year>
Reference-contexts: This approach reduces the number of synchronization operations as compared to the pure data flow model (once every several instructions instead of once every instruction), resulting in better overall performance. 35 Researchers at Electrotechnical Laboratory, who produced what probably is the only practical data flow computer available (the SIGMA-1 <ref> [59, 60, 61] </ref>), have now developed a new generation of (hybrid) data flow, the EM-4 [62]. The processing element of the machine, called EMC-R, is essentially a RISC processor augmented with hardware for data flow operation (a fetch and matching unit).
Reference: [62] <author> S. Sakai, Y. Yamaguchi, K. Hiraki, Y. Kodama, and T. Yuba, </author> <title> "An architecture of a dataflow single chip processor," </title> <booktitle> in Proceedings of the 16 th International Symposium on Computer Architecture, Jerusalen, Israel, </booktitle> <pages> pp. 46-53, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: flow model (once every several instructions instead of once every instruction), resulting in better overall performance. 35 Researchers at Electrotechnical Laboratory, who produced what probably is the only practical data flow computer available (the SIGMA-1 [59, 60, 61]), have now developed a new generation of (hybrid) data flow, the EM-4 <ref> [62] </ref>. The processing element of the machine, called EMC-R, is essentially a RISC processor augmented with hardware for data flow operation (a fetch and matching unit).
Reference: [63] <author> D. E. Culler, A. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek, </author> <title> "Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine," </title> <booktitle> in Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV). </booktitle> <address> April 8-11, 1991. Santa Clara, CA, </address> <pages> pp. 164-175, </pages> <year> 1991. </year>
Reference-contexts: Also related to autoscheduling are the works done by Culler et al. <ref> [63] </ref> at Berkeley and Spertus and Dally [64] at MIT. Both works can be essentially classified as software implementation of data flow models of computation. Spertus and Dally have developed the J-Machine, which offers a special structure, called cfuture, for very fast synchronization.
Reference: [64] <author> E. Spertus and W. J. Dally, </author> <title> "Experiences implementing dataflow on a general-purpose parallel computer," </title> <booktitle> in Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> II-231-235, </address> <month> August 12-16, </month> <year> 1991. </year>
Reference-contexts: Also related to autoscheduling are the works done by Culler et al. [63] at Berkeley and Spertus and Dally <ref> [64] </ref> at MIT. Both works can be essentially classified as software implementation of data flow models of computation. Spertus and Dally have developed the J-Machine, which offers a special structure, called cfuture, for very fast synchronization.
Reference: [65] <author> K. E. Schauser, </author> <title> Compiling Dataflow into Threads. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <month> July </month> <year> 1991. </year> <month> 287 </month>
Reference-contexts: Culler et al. have defined a threaded abstract machine (TAM) with a multilevel scheduling hierarchy and a special instruction set, which has been successfully translated to commercially available instruction sets (MIPS R3000). An implementation of data flow using TAM is described in <ref> [65] </ref>. Both of these projects, however, are more interested in exploiting fine-grain parallelism (typical of data flow) and do not have the generality of autoscheduling.
Reference: [66] <author> R. S. Nikhil and Arvind, </author> <title> "Can dataflow subsume von Neumann computing?," </title> <booktitle> in Proceed--ings of the 16 th International Symposium on Computer Architecture, Jerusalen, Israel, </booktitle> <pages> pp. 46-53, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: An implementation of data flow using TAM is described in [65]. Both of these projects, however, are more interested in exploiting fine-grain parallelism (typical of data flow) and do not have the generality of autoscheduling. Nikhil and Arvind, from MIT, propose in <ref> [66] </ref> an architecture called P-RISC, which, starting from a core RISC architecture, extended it by adding multithreading and three instructions that allow very fast thread creation and synchronization, and procedure start. This allows the exploitation of very fine grain parallelism, with data flow capability.
Reference: [67] <author> C. Polychronopoulos, </author> <title> "Multiprocessing versus multiprogramming," </title> <booktitle> Proceedings of 1989 Int'l. Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <booktitle> vol. II, </booktitle> <pages> pp. 3-230, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: SQs are defined at compile time, while the tasks in autoscheduling are created during run time to better suit the dynamic conditions of execution. 3.3 Other Related Work A first attempt to address the issues of supporting parallel processing and multiprogramming, particularly at the operating system level, is discussed in <ref> [67] </ref>. Further discussion about scheduling issues in multiprogrammed multiprocessors can be found in [68, 69]. Alternatives to the HTG for the representation of parallelism in programs include the Program Dependence Graph (PDG), described in [70], and the Parallel Program Graph (PPG), described in [71, 72].
Reference: [68] <author> A. Gupta, A. Tucker, and L. Stevens, </author> <title> "Making effective use of shared-memory multiprocessors: The process control approach," </title> <type> Tech. Rep. </type> <institution> CSL-TR-91-475A, Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: Further discussion about scheduling issues in multiprogrammed multiprocessors can be found in <ref> [68, 69] </ref>. Alternatives to the HTG for the representation of parallelism in programs include the Program Dependence Graph (PDG), described in [70], and the Parallel Program Graph (PPG), described in [71, 72]. The PDG was implemented in the PTRAN compiler [73]. <p> An example of such systems is the Alliant FX/80 [95]. A hybrid of the two approaches can alleviate some of these problems, but not as effectively as a scheme that dynamically partitions the processors among the executing processes. An example is process control <ref> [68] </ref>. Such a scheme avoids unnecessary context switching, thus improving locality, and ideally keeps each process running on a partition compatible with the parallelism of the program and the total load of the system. Autoscheduling uses the partition allocated to a process effectively and efficiently. <p> However, the exact task size is determined at run time and depends on program properties as well as run-time conditions. The latter depend largely on workload characteristics. Recent processor scheduling systems, such as Process Control <ref> [68] </ref>, provide the capability of space sharing the processors in a multiprocessor to improve utilization and locality of reference. In this case, the number of processors allocated to a particular job depends on other jobs running at the same time.
Reference: [69] <author> S. T. Leutenegger and M. K. Vernon, </author> <title> "Multiprogrammed multiprocessor scheduling issues." </title>
Reference-contexts: Further discussion about scheduling issues in multiprogrammed multiprocessors can be found in <ref> [68, 69] </ref>. Alternatives to the HTG for the representation of parallelism in programs include the Program Dependence Graph (PDG), described in [70], and the Parallel Program Graph (PPG), described in [71, 72]. The PDG was implemented in the PTRAN compiler [73].
Reference: [70] <author> V. Sarkar, </author> <title> "Automatic partitioning of a program dependence graph into parallel tasks," </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 35, </volume> <pages> pp. 779-804, </pages> <month> September/October </month> <year> 1991. </year>
Reference-contexts: Further discussion about scheduling issues in multiprogrammed multiprocessors can be found in [68, 69]. Alternatives to the HTG for the representation of parallelism in programs include the Program Dependence Graph (PDG), described in <ref> [70] </ref>, and the Parallel Program Graph (PPG), described in [71, 72]. The PDG was implemented in the PTRAN compiler [73]. PTRAN addresses several issues on the generation of parallel executable code, including a mechanism for satisfying control and data dependences.
Reference: [71] <author> V. Sarkar, </author> <title> "A concurrent execution semantics for parallel program graphs and program dependence graphs," </title> <booktitle> in Languages and Compilers for Parallel Computing, 5th International Workshop (U. </booktitle> <editor> Banerjee, D. Gelernter, A. Nicolau, and D. Padua, eds.), </editor> <volume> vol. </volume> <booktitle> 757 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 16-30, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: Further discussion about scheduling issues in multiprogrammed multiprocessors can be found in [68, 69]. Alternatives to the HTG for the representation of parallelism in programs include the Program Dependence Graph (PDG), described in [70], and the Parallel Program Graph (PPG), described in <ref> [71, 72] </ref>. The PDG was implemented in the PTRAN compiler [73]. PTRAN addresses several issues on the generation of parallel executable code, including a mechanism for satisfying control and data dependences. It supports both loop and functional parallelisms, in the form of PARALLEL LOOP and PARALLEL CASE constructs, respectively.
Reference: [72] <author> V. Sarkar and B. Simons, </author> <title> "Parallel program graphs and their classification," </title> <booktitle> in Languages and Compilers for Parallel Computing, 6th International Workshop (U. </booktitle> <editor> Banerjee, D. Gel-ernter, A. Nicolau, and D. Padua, eds.), </editor> <volume> vol. </volume> <booktitle> 768 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 633-655, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Further discussion about scheduling issues in multiprogrammed multiprocessors can be found in [68, 69]. Alternatives to the HTG for the representation of parallelism in programs include the Program Dependence Graph (PDG), described in [70], and the Parallel Program Graph (PPG), described in <ref> [71, 72] </ref>. The PDG was implemented in the PTRAN compiler [73]. PTRAN addresses several issues on the generation of parallel executable code, including a mechanism for satisfying control and data dependences. It supports both loop and functional parallelisms, in the form of PARALLEL LOOP and PARALLEL CASE constructs, respectively.
Reference: [73] <author> V. Sarkar, </author> <title> "PTRAN the IBM parallel translation system," </title> <type> Tech. Rep. 70566, </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1990. </year>
Reference-contexts: Alternatives to the HTG for the representation of parallelism in programs include the Program Dependence Graph (PDG), described in [70], and the Parallel Program Graph (PPG), described in [71, 72]. The PDG was implemented in the PTRAN compiler <ref> [73] </ref>. PTRAN addresses several issues on the generation of parallel executable code, including a mechanism for satisfying control and data dependences. It supports both loop and functional parallelisms, in the form of PARALLEL LOOP and PARALLEL CASE constructs, respectively.
Reference: [74] <author> C. Angourt, F. Coelho, F. Irigoin, and R. Keryell, </author> <title> "A linear algebra framework for static HPF code distribution," </title> <type> tech. rep., </type> <institution> Centre de Recherche en Informatique, Ecole Nationale Superieure des Mines de Paris, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: It supports both loop and functional parallelisms, in the form of PARALLEL LOOP and PARALLEL CASE constructs, respectively. Techniques for the compilation of Fortran dialects with data distribution features (Fortran D, HPF) are discussed in <ref> [21, 23, 74, 75] </ref>. Compile-based, automatic (as opposed to user-specified) data distribution is discussed in [76, 77, 78, 79, 80]. 38 Decomposition and scheduling algorithms that attempt to co-locate process and data in shared-memory multiprocessors, in order to reduce the costs of remote memory accesses, are discussed in [81].
Reference: [75] <author> Z. Bozkus, A. Choudhardy, G. Fox, T. Haupt, and S. Ranka, </author> <title> "A compilation approach for Fortran 90D/HPF compilers," </title> <booktitle> in Proceedings of the Sixth International Workshop on Languages and Compilers for Parallel Computing (U. </booktitle> <editor> Banerjee, D. Gelernter, A. Nicolau, and D. Padua, eds.), </editor> <volume> vol. </volume> <booktitle> 768 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 200-215, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: It supports both loop and functional parallelisms, in the form of PARALLEL LOOP and PARALLEL CASE constructs, respectively. Techniques for the compilation of Fortran dialects with data distribution features (Fortran D, HPF) are discussed in <ref> [21, 23, 74, 75] </ref>. Compile-based, automatic (as opposed to user-specified) data distribution is discussed in [76, 77, 78, 79, 80]. 38 Decomposition and scheduling algorithms that attempt to co-locate process and data in shared-memory multiprocessors, in order to reduce the costs of remote memory accesses, are discussed in [81].
Reference: [76] <author> M. Gupta and P. Banerjee, </author> <title> "Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Techniques for the compilation of Fortran dialects with data distribution features (Fortran D, HPF) are discussed in [21, 23, 74, 75]. Compile-based, automatic (as opposed to user-specified) data distribution is discussed in <ref> [76, 77, 78, 79, 80] </ref>. 38 Decomposition and scheduling algorithms that attempt to co-locate process and data in shared-memory multiprocessors, in order to reduce the costs of remote memory accesses, are discussed in [81].
Reference: [77] <author> S. K. Gupta, S. D. Kaushik, C.-H. Huang, J. R. Johnson, R. W. Johnson, and P. Sadayap-pan, </author> <title> "Towards automatic derivation of data distributions from tensor products formulas." </title> <institution> Transparencies from a presentation at the University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: Techniques for the compilation of Fortran dialects with data distribution features (Fortran D, HPF) are discussed in [21, 23, 74, 75]. Compile-based, automatic (as opposed to user-specified) data distribution is discussed in <ref> [76, 77, 78, 79, 80] </ref>. 38 Decomposition and scheduling algorithms that attempt to co-locate process and data in shared-memory multiprocessors, in order to reduce the costs of remote memory accesses, are discussed in [81].
Reference: [78] <author> J. Ramanujam and P. Sadayappan, </author> <title> "Compile-time techniques for data distribution in distributed memory machines," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Techniques for the compilation of Fortran dialects with data distribution features (Fortran D, HPF) are discussed in [21, 23, 74, 75]. Compile-based, automatic (as opposed to user-specified) data distribution is discussed in <ref> [76, 77, 78, 79, 80] </ref>. 38 Decomposition and scheduling algorithms that attempt to co-locate process and data in shared-memory multiprocessors, in order to reduce the costs of remote memory accesses, are discussed in [81].
Reference: [79] <author> A. Rogers and K. Pingali, </author> <title> "Compiling for distributed memory architectures," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 281-298, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Techniques for the compilation of Fortran dialects with data distribution features (Fortran D, HPF) are discussed in [21, 23, 74, 75]. Compile-based, automatic (as opposed to user-specified) data distribution is discussed in <ref> [76, 77, 78, 79, 80] </ref>. 38 Decomposition and scheduling algorithms that attempt to co-locate process and data in shared-memory multiprocessors, in order to reduce the costs of remote memory accesses, are discussed in [81].
Reference: [80] <author> B. M. Chapman, T. Fahringer, H. P. Zima, and P. Mehrotra, </author> <title> "Automatic support for data distribution on distributed memory multiprocessor systems," </title> <booktitle> in Proceedings of the Sixth International Workshop on Languages and Compilers for Parallel Computing (U. </booktitle> <editor> Baner-jee, D. Gelernter, A. Nicolau, and D. Padua, eds.), </editor> <volume> vol. </volume> <booktitle> 768 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 184-199, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Techniques for the compilation of Fortran dialects with data distribution features (Fortran D, HPF) are discussed in [21, 23, 74, 75]. Compile-based, automatic (as opposed to user-specified) data distribution is discussed in <ref> [76, 77, 78, 79, 80] </ref>. 38 Decomposition and scheduling algorithms that attempt to co-locate process and data in shared-memory multiprocessors, in order to reduce the costs of remote memory accesses, are discussed in [81].
Reference: [81] <author> E. Markatos, </author> <title> Scheduling for Locality in Shared-Memory Multiprocessors. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <year> 1993. </year>
Reference-contexts: Compile-based, automatic (as opposed to user-specified) data distribution is discussed in [76, 77, 78, 79, 80]. 38 Decomposition and scheduling algorithms that attempt to co-locate process and data in shared-memory multiprocessors, in order to reduce the costs of remote memory accesses, are discussed in <ref> [81] </ref>. The compilation of Fortran codes for a data flow machine that supports only data dependences is discussed [82]. The alternative formulation proposed in this thesis for execution tags would be well-suited for implementation on a machine with only one type of dependence supported. <p> Therefore, it is important to distribute data and computations in such a manner that the computations performed by a processor involve mostly local memory accesses <ref> [81] </ref>. From a processor's perspective, the higher the locality of access the greater the performance. However, enforcing locality can degrade load balance, since the predefined distribution of work leaves less room for dynamic adjustments of load.
Reference: [82] <author> T. Yasue, H. Yamana, and Y. Muraoka, </author> <title> "A Fortran compiling method for dataflow machines and its prototype compiler for the parallel processing system -Harray-," </title> <booktitle> in Proceedings of the Fifth International Workshop on Languages and Compilers for Parallel Computing (U. </booktitle> <editor> Banerjee, D. Gelernter, A. Nicolau, and D. Padua, eds.), </editor> <volume> vol. </volume> <booktitle> 757 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 482-496, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: The compilation of Fortran codes for a data flow machine that supports only data dependences is discussed <ref> [82] </ref>. The alternative formulation proposed in this thesis for execution tags would be well-suited for implementation on a machine with only one type of dependence supported. Some issues on the parallelization of recursive code (some of our benchmarks are recursive) have been addressed in [83, 84].
Reference: [83] <author> L. J. Hendren and A. Nicolau, </author> <title> "Parallelizing programs with recursive data structures," </title> <type> Tech. Rep. 89-33, </type> <institution> Department of Information and Computer Science, University of Cali-fornia at Irvine, </institution> <year> 1989. </year>
Reference-contexts: The alternative formulation proposed in this thesis for execution tags would be well-suited for implementation on a machine with only one type of dependence supported. Some issues on the parallelization of recursive code (some of our benchmarks are recursive) have been addressed in <ref> [83, 84] </ref>. Additional information on the simultaneous exploitation of task (functional) and data (loop) parallelism on message-passing systems can be found in [85, 86, 87]. A performance evaluation of various alternatives for the implementation of thread management on shared-memory multiprocessors is presented in [88].
Reference: [84] <author> S. Gupta, C.-H. Huang, P. Sadayappan, and R. Johnson, </author> <title> "On the synthesis of parallel programs from tensor product formulas for block recursive algorithms," </title> <booktitle> in Proceedings of the Fifth International Workshop on Languages and Compilers for Parallel Computing (U. </booktitle> <editor> Banerjee, D. Gelernter, A. Nicolau, and D. Padua, eds.), </editor> <volume> vol. </volume> <booktitle> 757 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 264-280, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: The alternative formulation proposed in this thesis for execution tags would be well-suited for implementation on a machine with only one type of dependence supported. Some issues on the parallelization of recursive code (some of our benchmarks are recursive) have been addressed in <ref> [83, 84] </ref>. Additional information on the simultaneous exploitation of task (functional) and data (loop) parallelism on message-passing systems can be found in [85, 86, 87]. A performance evaluation of various alternatives for the implementation of thread management on shared-memory multiprocessors is presented in [88].
Reference: [85] <author> J. Subhlok, D. R. O'Hallaron, T. Gross, P. A. Dinda, and J. Webb, </author> <title> "Communication and memory requirements as the basis for mapping task and data parallel programs," </title> <type> Tech. Rep. </type> <institution> CMU-CS-94-106 R, School of Computer Science, Carnegie Mellon University, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Some issues on the parallelization of recursive code (some of our benchmarks are recursive) have been addressed in [83, 84]. Additional information on the simultaneous exploitation of task (functional) and data (loop) parallelism on message-passing systems can be found in <ref> [85, 86, 87] </ref>. A performance evaluation of various alternatives for the implementation of thread management on shared-memory multiprocessors is presented in [88]. The study covers implementations issues such as queue organization and lock management. The performance analysis includes both analytical models and simulation.
Reference: [86] <author> J. Subhlock, J. M. Stichnoth, D. R. O'Hallaron, and T. Gross, </author> <title> "Exploiting task and data parallelism on a multicomputer," </title> <booktitle> in Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 13-22, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Some issues on the parallelization of recursive code (some of our benchmarks are recursive) have been addressed in [83, 84]. Additional information on the simultaneous exploitation of task (functional) and data (loop) parallelism on message-passing systems can be found in <ref> [85, 86, 87] </ref>. A performance evaluation of various alternatives for the implementation of thread management on shared-memory multiprocessors is presented in [88]. The study covers implementations issues such as queue organization and lock management. The performance analysis includes both analytical models and simulation.
Reference: [87] <author> J. Subhlock, </author> <title> "Automatic mapping of task and data parallel programs for efficient execution on multicomputers," </title> <type> Tech. Rep. </type> <institution> CMU-CS-93-212, School of Computer Science, Carnegie Mellon University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Some issues on the parallelization of recursive code (some of our benchmarks are recursive) have been addressed in [83, 84]. Additional information on the simultaneous exploitation of task (functional) and data (loop) parallelism on message-passing systems can be found in <ref> [85, 86, 87] </ref>. A performance evaluation of various alternatives for the implementation of thread management on shared-memory multiprocessors is presented in [88]. The study covers implementations issues such as queue organization and lock management. The performance analysis includes both analytical models and simulation.
Reference: [88] <author> T. E. Anderson, E. D. Lazowska, and H. M. Levy, </author> <title> "The performance implications of thread management alternatives for shared-memory multiprocessors," </title> <journal> IEEETC, </journal> <volume> vol. 38, </volume> <pages> pp. 1631-1644, </pages> <month> December </month> <year> 1989. </year> <month> 289 </month>
Reference-contexts: Additional information on the simultaneous exploitation of task (functional) and data (loop) parallelism on message-passing systems can be found in [85, 86, 87]. A performance evaluation of various alternatives for the implementation of thread management on shared-memory multiprocessors is presented in <ref> [88] </ref>. The study covers implementations issues such as queue organization and lock management. The performance analysis includes both analytical models and simulation. <p> For an MPP (usually NUMA), a single task queue causes unacceptable contention, especially when task sizes are small, the number of processors is large, or both <ref> [88] </ref>. The single queue also makes it difficult to select specific processors for the execution of each task, which is desirable when the code exhibits a high degree of data locality. For these reasons, we have implemented a distributed queue to be used with NUMA MPPs.
Reference: [89] <author> M. Girkar, </author> <title> Functional Parallelism: Theoretical Foundations and Implementation. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: The control and data dependence information between tasks, along with the capability for representing parallel loops, allow the dynamic exploitation of parallelism. The formal definition of the HTG and construction algorithms can be found in <ref> [57, 89, 90] </ref>. In this section we present the HTG as an abstract program model. The following definitions are based on the work of [89, 90, 91]. <p> The formal definition of the HTG and construction algorithms can be found in [57, 89, 90]. In this section we present the HTG as an abstract program model. The following definitions are based on the work of <ref> [89, 90, 91] </ref>. The hierarchical task graph is a directed acyclic graph (DAG) H = (X; A) where X is a set of nodes and A is a set of arcs. Let X = fx 1 ; x 2 ; : : : ; x n g. <p> When node x i executes, at most, one of its branches is taken and the corresponding variable fi l is set to TRUE. We first discuss the derivation of execution tags as presented in <ref> [89, 90] </ref> and then we will present the alternative formulation derived for this thesis. <p> Bypassing a node can, in turn, produce more branches that are not taken, resulting in more bypassed nodes. The computation of BranNeg (b l ) can be performed from the HTG using the algorithm of Figure 4.1. The original algorithm from <ref> [89] </ref> computes BranNeg from the control flow graph. <p> In [91], the general case of ATGs with control dependences is considered. It has been shown in <ref> [89] </ref> that removing redundant data dependences in an ATG containing control dependences is an NP-complete problem. Both a heuristic and an exact algorithm are presented in [91]. The efficacy and performance of both algorithms are compared. <p> This prototype autoscheduling compiler was the result of theoretical and implementation foundations described in <ref> [57, 89, 91] </ref>. We have been able to generate correct parallel code from a hierarchical task graph intermediate representation of a program. We have also been successful in exploiting the loop and functional parallelisms present in numerical codes.
Reference: [90] <author> M. Girkar and C. Polychronopoulos, </author> <title> "Automatic detection and generation of unstructured parallelism in ordinary programs," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, </volume> <month> April </month> <year> 1992. </year>
Reference-contexts: The control and data dependence information between tasks, along with the capability for representing parallel loops, allow the dynamic exploitation of parallelism. The formal definition of the HTG and construction algorithms can be found in <ref> [57, 89, 90] </ref>. In this section we present the HTG as an abstract program model. The following definitions are based on the work of [89, 90, 91]. <p> The formal definition of the HTG and construction algorithms can be found in [57, 89, 90]. In this section we present the HTG as an abstract program model. The following definitions are based on the work of <ref> [89, 90, 91] </ref>. The hierarchical task graph is a directed acyclic graph (DAG) H = (X; A) where X is a set of nodes and A is a set of arcs. Let X = fx 1 ; x 2 ; : : : ; x n g. <p> When node x i executes, at most, one of its branches is taken and the corresponding variable fi l is set to TRUE. We first discuss the derivation of execution tags as presented in <ref> [89, 90] </ref> and then we will present the alternative formulation derived for this thesis.
Reference: [91] <author> Carl J. Beckmann, </author> <title> Hardware and Software for Functional and Fine Grain Parallelism. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: The formal definition of the HTG and construction algorithms can be found in [57, 89, 90]. In this section we present the HTG as an abstract program model. The following definitions are based on the work of <ref> [89, 90, 91] </ref>. The hierarchical task graph is a directed acyclic graph (DAG) H = (X; A) where X is a set of nodes and A is a set of arcs. Let X = fx 1 ; x 2 ; : : : ; x n g. <p> For the same reason, the new value of PROCESSED has to be stored in a temporary. This has been proven to guarantee that each task will be enqueued only once <ref> [91] </ref>. The entry block of each task must evaluate its firing tag to determine if the body of the task is to be executed. One obvious simplification is that tasks without control dependences do not have to perform this test. <p> The exit block is always executed when the task graph is in parallel mode. This is necessary to set the appropriate bit in PROCESSED and test the successors for execution. The bit-vector implementation for execution tags was introduced by Beckmann <ref> [91] </ref> for data and control conditions. We have adapted it to our alternative formulation based on enabling 71 and firing conditions. In [91], multiple bits of the SATISFIED bit vector (which corresponds to our PROCESSED bit vector) may be set in a single operation. <p> This is necessary to set the appropriate bit in PROCESSED and test the successors for execution. The bit-vector implementation for execution tags was introduced by Beckmann <ref> [91] </ref> for data and control conditions. We have adapted it to our alternative formulation based on enabling 71 and firing conditions. In [91], multiple bits of the SATISFIED bit vector (which corresponds to our PROCESSED bit vector) may be set in a single operation. Another simplification of our formulation is that each task has a single exit block, whereas in [91] a task has as many exit blocks as control-flow branches originating in <p> In <ref> [91] </ref>, multiple bits of the SATISFIED bit vector (which corresponds to our PROCESSED bit vector) may be set in a single operation. Another simplification of our formulation is that each task has a single exit block, whereas in [91] a task has as many exit blocks as control-flow branches originating in it. 5.2 The Task Queue The task identifiers of enabled tasks are placed in the task queue so that processors can fetch the tasks for execution. <p> This threshold is a function of overhead. The details of selecting an appropriate minimum size are beyond the scope of this thesis. Static minimum granularity control can be implemented through task merging, a process described in <ref> [91] </ref>. Task merging combines tasks of the HTG until no task smaller than the minimum grain size remains. This reduces the number of tasks in a given ATG, with a secondary objective of minimizing any adverse impact on the available parallelism of the transformed ATG. <p> The task merging algorithm works by finding the candidate task pair with the minimum penalty of combining and then lumping these two tasks together into a new node. The algorithm iterates until the minimum granularity objective is met. The criteria for choosing the candidate pairs are discussed in <ref> [91] </ref> (at least one of the candidates must be of a size smaller than the minimum threshold). We discuss this algorithm further in Section 6.6.4. Another aspect of static granularity control is to help prevent overly conservative dynamic granularity control decisions. <p> If this same approach were used in parallel execution, it would require that each processor follow a strict order in the allocation and deallocation of frames, thus restricting the freedom in task scheduling <ref> [91] </ref>. The cactus stack organization used in autoscheduling does not associate activation frames to processors, thus allowing activation frames to be allocated and deallocated by different processors. This cactus stack is implemented by allocating and deallocating activation frames from a generic heap. <p> The latter scheme is efficient in performing allocation and deallocation, but the depth of exploited parallelism is limited by the total number of stacks available. Techniques for implementing the cactus stack with one stack per processor, and the associated restrictions 86 in task scheduling, are discussed in <ref> [91, 99] </ref>. Only the generic heap scheme was implemented in autoscheduling. An efficient technique for memory allocation used in the implementation of APL, another system with high requirements for dynamic memory allocation, is described in [100]. <p> The htgilgen pass of Parafrase-2 traverses the internal HTG and outputs an equivalent representation in HTGIL. The code generation parses programs written in HTGIL, builds new internal data structures, and outputs autoscheduling code. HTGIL describes the structure of the HTG using constructs similar to those introduced by Beckmann in <ref> [91] </ref>. The htgilgen pass is derived from the hcodegen pass, also developed by Beckmann. The actual computations performed by the tasks are described by constructs similar to those available in Cedar Fortran. In this section, we present an informal description of HTGIL. <p> An element B k;j = 1 marks that branch k originates on node j (node j is the source for branch k). Submatrix 0 bfib is the zero matrix since there is no dependence relation between any two branches. This representation was first introduced in <ref> [91] </ref>. <p> Percolation of activation frames was discussed in detail in Section 5.4. Its goal is to reduce the overhead in allocating and deallocating activation frames dynamically. Elimination of redundant data dependences and task merging were addressed in detail in <ref> [91] </ref>. They aim at reducing the complexity of exit blocks and overall task scheduling operations. A summary of the discussion in [91] is provided here. 119 6.6.4.1 Elimination of redundant dependences A data dependence in an ATG is said to be redundant if removing the dependence arc does not affect the <p> Its goal is to reduce the overhead in allocating and deallocating activation frames dynamically. Elimination of redundant data dependences and task merging were addressed in detail in <ref> [91] </ref>. They aim at reducing the complexity of exit blocks and overall task scheduling operations. A summary of the discussion in [91] is provided here. 119 6.6.4.1 Elimination of redundant dependences A data dependence in an ATG is said to be redundant if removing the dependence arc does not affect the partial order of tasks required for correct execution. In [91], the general case of ATGs with control dependences is considered. <p> A summary of the discussion in <ref> [91] </ref> is provided here. 119 6.6.4.1 Elimination of redundant dependences A data dependence in an ATG is said to be redundant if removing the dependence arc does not affect the partial order of tasks required for correct execution. In [91], the general case of ATGs with control dependences is considered. It has been shown in [89] that removing redundant data dependences in an ATG containing control dependences is an NP-complete problem. Both a heuristic and an exact algorithm are presented in [91]. <p> In <ref> [91] </ref>, the general case of ATGs with control dependences is considered. It has been shown in [89] that removing redundant data dependences in an ATG containing control dependences is an NP-complete problem. Both a heuristic and an exact algorithm are presented in [91]. The efficacy and performance of both algorithms are compared. The implementation considered in this thesis treats control and data dependences uniformly, as discussed in Section 4.1. In this case, an ATG contains simply dependence arcs that impose a partial order of execution. <p> z) f z = new node z = x + y foreach (t 2 Pred (x)) f G = G + t ! z g foreach (t 2 Succ (y)) f G = G + z ! t g G = G y 122 more general algorithm is described in <ref> [91] </ref>. This algorithm iterates until the desired objective is met. Each iteration selects a list of candidate pairs and then combines the pair with the minimum penalty. The penalty of combining a pair of tasks is the increase in the critical path through the ATG incurred by combining the tasks. <p> The penalty of combining a pair of tasks is the increase in the critical path through the ATG incurred by combining the tasks. The computation of this penalty is explained in detail in <ref> [91] </ref>. The algorithm is shown in Figure 6.19. <p> It is discussed in <ref> [91] </ref> how to compute these values. It suffices to say that they require from the compiler the ability to estimate execution times of tasks. While this may be a simple matter for sequence of statements, it becomes more complicated as tasks include subroutine calls and loops. <p> While this may be a simple matter for sequence of statements, it becomes more complicated as tasks include subroutine calls and loops. Parafrase-2's extensive symbolic analysis capability [109] can be used to derive precise timing information for tasks, but the results from <ref> [91] </ref> show that even simple estimates, such as assuming a large constant time for tasks with subroutine calls or loops, are effective. <p> min (x i ); t min (x j )) min (t max (x i ) t exec (x j ); t max (x j ) t exec (x i )) Redundant dependence removal must be applied both before and after task merging because task merging can create new redundant arcs <ref> [91] </ref>. Activation frame percolation could also be applied as the first optimization. None of these optimizations have been included in the compiler yet. 6.6.5 Code generation The code generator works by traversing the AST top-down, starting at the root. A code generating member function is applied to each node. <p> The pseudo-code and task graphs for this benchmarks are shown in Figure 7.19. The parameter n is set to (32; 64; 128; 256). The maximum achievable speedup from the parallel execution of a task graph can be computed, as discussed in <ref> [91] </ref>, as the ratio of total work in the graph by work in the critical path of the graph. <p> This prototype autoscheduling compiler was the result of theoretical and implementation foundations described in <ref> [57, 89, 91] </ref>. We have been able to generate correct parallel code from a hierarchical task graph intermediate representation of a program. We have also been successful in exploiting the loop and functional parallelisms present in numerical codes. <p> The intermediate program representation on which autoscheduling is based is the hierarchical task graph (HTG), as described in Chapter 4. We devised an intermediate language, HTGIL, to represent the HTG. The structure of this language is based on previous work in our group <ref> [91] </ref> and parallel versions of Fortran: Cedar Fortran, Fortran D, HPF, and Cray MPP Fortran. These languages are described in Chapter 3. An important contribution of this thesis is the development of techniques for the support of data and load distribution in autoscheduling. <p> On the architectural side of our research, we want to experiment with specialized architectures for autoscheduling. We have the tools, in the form of an instruction level simulator, to examine the impact of specific instructions for autoscheduling (as described in <ref> [91] </ref>) in the performance of autoscheduling code. We should also examine architectural enhancements that do not require processor redesign. Finally, we want to investigate more functional parallel applications. We already know that recursive algorithms are a good source of functional parallelism.
Reference: [92] <author> C. J. Beckmann and C. D. Polychronopoulos, </author> <title> "Microarchitecture support for dynamic scheduling of acyclic task graphs," </title> <type> Tech. Rep. 1207, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: This alternative formulation leads to some simplifications in implementing autoscheduling, as discussed in Chapters 5 and 6. It also simplifies the theoretical model by eliminating the need for the BranNeg and Neg sets. Hardware-assisted implementations of autoscheduling, such as described in <ref> [92] </ref>, can benefit from these simplifications. The disadvantage is the penalty of processing every node in the task graph, even if only to test the firing condition. 4.2 The Autoscheduling Model of Computation Autoscheduling uses a macro dataflow execution mode to execute the tasks of an HTG in parallel.
Reference: [93] <author> Per Stenstrom, </author> <title> "VLSI support for cactus stack oriented memory organization," </title> <booktitle> in Proceedings of the 21 st Annual Hawaii International Conference on System Sciences, </booktitle> <volume> vol I, </volume> <pages> pp. 211-220, </pages> <year> 1988. </year>
Reference-contexts: A (time-variant) set of n (t) physical processors assigned to that address space. 3. The autoscheduling code with the tasks of the HTG stored in the code area. 4. A task ready queue allocated in the same address space. 5. A cactus-stack <ref> [93] </ref> allocated from the heap. Using conventional terminology, an executing program is a process. The set of processors n (t) assigned to a process at time t (n (t) : Z + ! [0::P ]) is called a partition and is controlled by the operating system.
Reference: [94] <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: The activation frames of a parallel program cannot be implemented with the simple stack structure normally used in sequential programs <ref> [94] </ref>. A dynamic tree of activation frames, organized as a cactus-stack, is used to support several instances of subroutines and loop iterations active at the same time. For example, consider the code in Figure 4.2 (a). <p> Arrays that belong to a COMMON block must have compile-time constant sizes. Figure 6.22 illustrates the translation of the COMMON blocks of Figure 6.21. One struct is defined for each activation frame declared in the program. Each struct has the declaration for a display vector <ref> [94] </ref>. A display vector is a vector of pointers to activation frames, used to point to all the ancestors of the activation frame in the cactus stack. The size of the vector is equal to the level of the activation frame plus 1.
Reference: [95] <institution> Alliant Computer Systems Corporation, </institution> <address> Acton, MA, </address> <note> CONCENTRIX System Reference Manual, </note> <month> January </month> <year> 1986. </year>
Reference-contexts: An example of such systems is the Alliant FX/80 <ref> [95] </ref>. A hybrid of the two approaches can alleviate some of these problems, but not as effectively as a scheme that dynamically partitions the processors among the executing processes. An example is process control [68].
Reference: [96] <author> S. P. Dandamudi and P. S. P. Cheng, </author> <title> "A hierarchical task queue organization for shared-memory multiprocessor systems," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 6, </volume> <pages> pp. 1-16, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: As we move from a totally centralized (p = 0) to a totally distributed (p = 1) organization, the sensitivity of the efficiency parameter to N and k diminishes. Hierarchical queue organizations serve as an alternative to fully centralized or fully distributed organizations, as discussed in <ref> [96] </ref>. 5.3 Granularity Control Task granularity is one of the fundamental optimization problems in parallel processing. The granularity, or grain size, of a task is informally used to indicate the size of the task, usually measured in number of instructions.
Reference: [97] <author> W. Blume and R. Eigenmann, </author> <title> "Performance analysis of parallelizing compilers on the Perfect Benchmarks programs," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: Dynamic allocation of activation frames is necessary to support reentrant code, recursion, and, most importantly in our case, concurrency. Variable privatization has been demonstrated to be an important feature for parallelization of programs <ref> [97] </ref>. To implement variable privatization with static allocation, it is necessary to expand the loop local variables by adding one dimension to their structure and distributing them across the processors along that dimension. The major disadvantage of doing this is that the maximum number of processors must be known.
Reference: [98] <author> J.-H. Chow and L. Harrison, "Switch-stacks: </author> <title> A scheme for microtasking nested parallel loops," </title> <booktitle> in Proceedings of Supercomputing'90, </booktitle> <pages> pp. 190-199, </pages> <month> November 12-16, </month> <year> 1990. </year>
Reference-contexts: This implementation requires efficient heap management algorithms for maximum effectiveness. Detailed analysis of such algorithms, however, is beyond the scope of this thesis. Another alternative to processor-independent activation frame allocation and deallocation is the switch-stack scheme <ref> [98] </ref>. The latter scheme is efficient in performing allocation and deallocation, but the depth of exploited parallelism is limited by the total number of stacks available. Techniques for implementing the cactus stack with one stack per processor, and the associated restrictions 86 in task scheduling, are discussed in [91, 99].
Reference: [99] <author> S. F. Hummel and E. Schonberg, </author> <title> "Low-overhead scheduling of nested parallelism," </title> <type> Tech. Rep. RC 16424, </type> <institution> IBM Research Division, </institution> <year> 1990. </year>
Reference-contexts: The latter scheme is efficient in performing allocation and deallocation, but the depth of exploited parallelism is limited by the total number of stacks available. Techniques for implementing the cactus stack with one stack per processor, and the associated restrictions 86 in task scheduling, are discussed in <ref> [91, 99] </ref>. Only the generic heap scheme was implemented in autoscheduling. An efficient technique for memory allocation used in the implementation of APL, another system with high requirements for dynamic memory allocation, is described in [100].
Reference: [100] <author> R. Trimble, </author> <title> "Storage management in IBM APL systems," </title> <journal> IBM Systems Journal, </journal> <volume> vol. 30, no. 4, </volume> <pages> pp. 456-468, </pages> <year> 1991. </year>
Reference-contexts: Only the generic heap scheme was implemented in autoscheduling. An efficient technique for memory allocation used in the implementation of APL, another system with high requirements for dynamic memory allocation, is described in <ref> [100] </ref>. The scheme is based on a set of n lists, numbered 0; 1; : : :; n 1, of free cells. Let the minimum size of a cell be K = 2 k bytes. List i is composed of blocks of size 2 k+i . <p> List i is composed of blocks of size 2 k+i . Larger blocks can be divided to create smaller blocks and smaller blocks can be combined to create larger blocks. This scheme for dynamic memory management is commonly known as the buddy scheme. Enhancements described in <ref> [100] </ref> allow it to deliver the necessary performance for APL. Multiple sets of lists, one for each processor, can be used in shared-memory multiprocessors to reduce contention. A discussion of memory allocation costs, comparing the performance of different systems, can be found in [101].
Reference: [101] <author> D. Detlefs, A. Dosser, and B. Zorn, </author> <title> "Memory allocation costs in large C and C++ programs," </title> <type> Tech. Rep. CU-CS-665-93, </type> <institution> Department of Computer Science, University of Col-orado at Boulder, </institution> <year> 1993. </year>
Reference-contexts: Enhancements described in [100] allow it to deliver the necessary performance for APL. Multiple sets of lists, one for each processor, can be used in shared-memory multiprocessors to reduce contention. A discussion of memory allocation costs, comparing the performance of different systems, can be found in <ref> [101] </ref>. The management of activation frames is always a source of overhead. It is large or small depending on how fast activation frames can be allocated and deallocated and how many of those operations are necessary.
Reference: [102] <author> C. D. Polychronopoulos and D. J. Kuck, </author> <title> "Guided self-scheduling: A practical scheduling scheme for parallel supercomputers," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 1425-1439, </pages> <month> December </month> <year> 1987. </year> <month> 290 </month>
Reference-contexts: The chunk-size is determined by the scheduling algorithm being used. Currently, two scheduling algorithms are supported: * Self-scheduling (SS): in this case, the chunk-size is always 1. * Guided self-scheduling (GSS): in this case, the chunk-size C is computed by the expression <ref> [102] </ref> ~ P where R is the number of remaining iterations in the loop (those not yet scheduled) and P is the number of processors in the partition executing the loop.
Reference: [103] <author> D. E. Knuth, </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol. 1, </volume> <booktitle> Fundamental Algorithms. </booktitle> <address> Read--ing, Mass.: </address> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Lists of homogeneous nodes (nodes with the same type of syntactic construct) are implemented as left-recursive subtrees. An example of the production rule and resulting subtree for a statement list is shown in Figure 6.11. Traversing the subtree in preorder <ref> [103] </ref> results in processing the component nodes (statements) in the lexical order of the program. All transformations applied to the HTG are reflected in the AST. Code is also generated from the AST. In addition to the AST, several linked lists of particular types of nodes are built during parsing.
Reference: [104] <author> K. A. Faigin, J. P. Hoeflinger, D. A. Padua, P. M. Petersen, and S. A. Weatherford, </author> <title> "The Polaris internal representation," </title> <journal> International Journal of Parallel Programming, </journal> <volume> vol. 22, no. 5, </volume> <pages> pp. 553-286, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: As an example, consider the class defined for a statement list. The syntax rule is shown in Figure 6.11, and the class declaration in Figure 6.14. Figure 6.15 shows examples of the use of the StmtList class for parsing and code generation. Polaris <ref> [104, 105] </ref> is an example of a compiler that uses object-oriented technology to a much larger extent. With this organization, it is relatively straightforward to apply a function to all nodes of a certain type or set of types.
Reference: [105] <author> B. Blume, R. Eigenmann, K. Faigin, J. Grout, J. Hoeflinger, D. Padua, P. Petersen, B. Pottenger, L. Rauchwerger, P. Tu, and S. Weatherford, </author> <title> "Polaris: The next generation in parallelizing compilers," </title> <booktitle> in Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <pages> pp. </pages> <address> 10.1 - 10.18, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: As an example, consider the class defined for a statement list. The syntax rule is shown in Figure 6.11, and the class declaration in Figure 6.14. Figure 6.15 shows examples of the use of the StmtList class for parsing and code generation. Polaris <ref> [104, 105] </ref> is an example of a compiler that uses object-oriented technology to a much larger extent. With this organization, it is relatively straightforward to apply a function to all nodes of a certain type or set of types.
Reference: [106] <author> S. I. Feldman, D. M. Gay, M. W. Maimone, and N. L. Schryer, </author> <title> "A Fortran-to-C converter," </title> <type> Tech. Rep. </type> <institution> Computing Sciences 149, AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ, </address> <year> 1993. </year>
Reference-contexts: There are five main passes: 1. Parsing. 2. Build Symbol Table. 3. Task Graph Processing. 116 4. HTG Optimizations. 5. Code Generation. Each of these passes is explained in more detail below. Some of the operations performed by the autoscheduling compiler were based on f2c <ref> [106] </ref>. 6.6.1 Parsing The parser is generated automatically from the language grammar with bison and flex. As it parses the program, it builds the AST and the auxiliary lists discussed in Section 6.3.
Reference: [107] <author> J. A. Bondy and U. S. R. Murty, </author> <title> Graph Theory with Applications. </title> <address> New York, NY: </address> <publisher> North-Holland, </publisher> <year> 1976. </year>
Reference: [108] <author> A. V. Aho, M. R. Garey, and J. D. Ullman, </author> <title> "The transitive reduction of a directed graph," </title> <journal> SIAM Journal of Computing, </journal> <volume> vol. 1, </volume> <pages> pp. 131-137, </pages> <month> June </month> <year> 1972. </year>
Reference-contexts: The algorithm of Figure 6.17 has complexity O (mn 4 ) where m and n are the number of arcs and nodes in graph G, respectively. More efficient algorithms for computing the transitive reduction in O (n 2:8 ) time exist <ref> [108] </ref>. 120 procedure Reduction (G) f (* G is a directed graph, G = (X; A) *) H = G* foreach (x ! y) 2 A) f G = G (x ! y) G = G + (x ! y) g The direct benefit of eliminating redundant dependences is to reduce
Reference: [109] <author> M. Haghighat, </author> <title> Symbolic Analysis for Parallelizing Compilers. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: It suffices to say that they require from the compiler the ability to estimate execution times of tasks. While this may be a simple matter for sequence of statements, it becomes more complicated as tasks include subroutine calls and loops. Parafrase-2's extensive symbolic analysis capability <ref> [109] </ref> can be used to derive precise timing information for tasks, but the results from [91] show that even simple estimates, such as assuming a large constant time for tasks with subroutine calls or loops, are effective. <p> Block matrix add. 2. Block matrix multiply. 3. Block triangular matrix add. 4. Block triangular matrix multiply. 5. Life controlled block multiply. It could be argued that for most of these simple benchmarks a smart compiler could perform static load balancing, such as described in <ref> [109] </ref>. However, there are real scientific applications in which the optimal partitioning of the grid for load balance is input-set dependent [122]. In this case, the ideal partitioning cannot be computed at compile time. It is for this class of applications that dynamic scheduling is most helpful.
Reference: [110] <institution> Silicon Graphics, Inc, Mountain View, CA, </institution> <note> Fortran 77 Programmer's Guide. </note>
Reference-contexts: We ran each benchmark three times and took the smallest value. All the benchmarks discussed in Section 7.4 were compiled with the autoscheduling compiler and run on the Challenge. Power Fortran <ref> [110] </ref> versions of the benchmarks that did not require functional parallelism were also tested. This provides a reference for the performance of our compiler. Although the C++ code generated by the autoscheduling compiler is machine independent, some of the run-time library is machine dependent.
Reference: [111] <institution> Silicon Graphics, Inc, Mountain View, CA, </institution> <note> IRIS Power C User's Guide. </note>
Reference-contexts: The lock management routines defined in ulocks.h were used. These routines are listed in Table 7.1. The routine that starts the parallel threads in the SGI is shown in Figure 7.1. This routine starts n threads in parallel, one in each physical processor. We use parallel Power C <ref> [111] </ref> loops to start the threads. As each thread starts, it sets a private flag. The WaitForEverybody routine tests those flags and only returns when all n flags are set. In this way all tasks start execution together.
Reference: [112] <author> S. R. Goldschmidt, </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The more lightweight the thread library, the better the performance of the simulation. 152 153 void SelectProcessor () f new = processor with earliest time; if (CurrentProcessor == new) f return; g else f old = CurrentProcessor; unblock (new); block (old); g TangoLite <ref> [112] </ref> and EPGsim [113]. An illustration of the correspondence of what happens in real time and simulated time is shown in Figure 7.4. A synchronization event is a point in the program where the actions of one processor may interfere with another.
Reference: [113] <author> D. K. Poulsen, </author> <title> Memory Latency Reduction via Data Prefetching and Data Forwarding in Shared Memory Multiprocessors. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year> <type> CSRD Report 1377. </type>
Reference-contexts: The more lightweight the thread library, the better the performance of the simulation. 152 153 void SelectProcessor () f new = processor with earliest time; if (CurrentProcessor == new) f return; g else f old = CurrentProcessor; unblock (new); block (old); g TangoLite [112] and EPGsim <ref> [113] </ref>. An illustration of the correspondence of what happens in real time and simulated time is shown in Figure 7.4. A synchronization event is a point in the program where the actions of one processor may interfere with another. The precise list of synchronization events is discussed below.
Reference: [114] <author> M. Berry, D. Chen, P. Koss, D. Kuck, L. Pointer, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin, </author> <title> "The Perfect Club benchmarks: Effective performance evalution of supercomputers," </title> <journal> International Journal of Supercomputer Applications, Fall 1989, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: The pseudo-code for this benchmark is shown in Figure 7.17. The matrices are of size n fi n and the parameter n was set to (32; 64; 128; 256). 7.4.2.2 TRFD TRFD is one of the Perfect Codes <ref> [114] </ref>. It is an example of a scientific application with very little directly exploitable functional parallelism. For the measurements in this thesis we do not attempt to extract or exploit functional parallelism in TRFD, and we rather concentrate on the exploitation of the plentiful loop parallelism in this code.
Reference: [115] <author> J. B. Andrews, </author> <title> "Parallelization of TRFD," </title> <type> tech. rep., </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: For the measurements in this thesis we do not attempt to extract or exploit functional parallelism in TRFD, and we rather concentrate on the exploitation of the plentiful loop parallelism in this code. We utilize a parallel version of TRFD as described in <ref> [115] </ref>. Most of the work in TRFD is performed by subroutine OLDA, which is called for different values of a parameter n = (10; 15; 20; 25; 30; 35; 40). <p> From the results in memory system results in only a small improvement in performance, because of the high cache locality of matrix multiply. linear, with a small advantage for Power Fortran over autoscheduling. The linear speedup observed is due in part to the intensive hand optimizations described in <ref> [115] </ref>. We should notice, however, that autoscheduling was effective in exploiting the parallelism in this loop-parallel-only benchmark. Again we observe essentially no benefit from a perfect memory system. a benchmark with both loop and functional parallelisms. The Power Fortran version exploits only loop parallelism.
Reference: [116] <author> G. H. Golub and C. F. V. Loan, </author> <title> Matrix Computations. </title> <address> Baltimore, MD: </address> <publisher> The Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: Z = B fi C task 4 () T = A fi D task 5 () R = X Y task 6 () Q = Z + T enddag end 172 7.4.2.4 Strassen's Matrix Multiply (SMM) This benchmark is an implementation of Strassen's recursive matrix multiply algorithm as described in <ref> [116] </ref>.
Reference: [117] <author> D. E. Knuth, </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol. </volume> <month> 3, </month> <title> Sorting and Searching. </title> <address> Reading, Mass.: </address> <publisher> Addison-Wesley, </publisher> <year> 1973. </year> <month> 291 </month>
Reference-contexts: It is scalable with the problem size. 174 Matrix size (n) Speedup 32 1.00 128 37.57 512 312.55 7.4.2.5 Quicksort (QUICK) Quicksort <ref> [117, 118] </ref> is a recursive "divide and conquer" algorithm that sorts a vector of elements (integers). It does so through the following steps: 1. It chooses one of the elements of the array as the pivot. In our implementation, the element chosen is the middle element in the vector. 2.
Reference: [118] <author> N. Wirth, </author> <title> Algorithms + Data Structures = Programs. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice--Hall, </publisher> <year> 1976. </year>
Reference-contexts: It is scalable with the problem size. 174 Matrix size (n) Speedup 32 1.00 128 37.57 512 312.55 7.4.2.5 Quicksort (QUICK) Quicksort <ref> [117, 118] </ref> is a recursive "divide and conquer" algorithm that sorts a vector of elements (integers). It does so through the following steps: 1. It chooses one of the elements of the array as the pivot. In our implementation, the element chosen is the middle element in the vector. 2.
Reference: [119] <author> S. L. Lyons, T. J. Hanratty, and J. B. MacLaughlin, </author> <title> "Large-scale computer simulation of fully developed channel flow with heat transfer," </title> <journal> International Journal of Numerical Methods for Fluids, </journal> <volume> vol. 13, </volume> <pages> pp. 999-1028, </pages> <year> 1991. </year>
Reference-contexts: 2 ) = n + C Q ( n (7:8) The maximum speedup for a vector of n elements is S (n) = C Q (n) we observe scalable functional parallelism. 7.4.2.6 Computational fluid dynamics (CFD) This benchmark is the kernel of a Fourier-Chebyshev spectral Computational Fluid Dynamics (CFD) code <ref> [119] </ref>. The task graph is shown in Figure 7.24. It basically consists of four stages that operate on matrices of size n fi n.
Reference: [120] <author> L. Verlet, </author> <title> "Computer experiments on classical fluids. thermodynamical properties of lennard-jones molecules," </title> <journal> Phys. Rev., </journal> <volume> vol. 159, </volume> <pages> pp. 98-103, </pages> <year> 1968. </year>
Reference-contexts: The functional parallelism is independent of the size of the problem and depends only on the structure of the code. 7.4.2.7 Molecular dynamics (MDJ) This benchmark is extracted from an industrial molecular dynamics code from Asahi Chemical <ref> [120, 121] </ref>. The benchmark is a functional parallel section of the most heavily used subroutine in the code. This section consists of a loop that iterates over all the cells in the grid (total of 1000 cells). Each cell can be processed independently; therefore, the loop can be parallelized.
Reference: [121] <author> B. Quentrec and C. Brot, </author> <title> "New method for searching for neighbors in molecular dynamics computations," </title> <journal> J. Comp. Phys., </journal> <volume> vol. 13, </volume> <pages> pp. 430-432, </pages> <year> 1975. </year>
Reference-contexts: The functional parallelism is independent of the size of the problem and depends only on the structure of the code. 7.4.2.7 Molecular dynamics (MDJ) This benchmark is extracted from an industrial molecular dynamics code from Asahi Chemical <ref> [120, 121] </ref>. The benchmark is a functional parallel section of the most heavily used subroutine in the code. This section consists of a loop that iterates over all the cells in the grid (total of 1000 cells). Each cell can be processed independently; therefore, the loop can be parallelized.
Reference: [122] <author> L. DeRose, K. Gallivan, and E. Gallopoulos, </author> <title> "Land avoidance and load balancing in ocean simulation," </title> <type> Tech. Rep. 1296, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Life controlled block multiply. It could be argued that for most of these simple benchmarks a smart compiler could perform static load balancing, such as described in [109]. However, there are real scientific applications in which the optimal partitioning of the grid for load balance is input-set dependent <ref> [122] </ref>. In this case, the ideal partitioning cannot be computed at compile time. It is for this class of applications that dynamic scheduling is most helpful. Each of the five benchmarks operates on three nfin matrices: A; B, and C.
Reference: [123] <author> M. Lam, </author> <title> "Software pipelining: An effective scheduling technique for VLIW machines," </title> <booktitle> in Proceedings of the SIGPLAN'88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: We would like to investigate how to use the HTG data structure to perform aggressive code optimization, such as global register allocation, superscalar instruction scheduling, and software pipelining <ref> [123] </ref>. We believe the HTG is a self-sufficient data structure that supersedes the more traditional flow and dependence graphs, but we still have to assess the efficiency of optimized machine code generated exclusively from the HTG.
Reference: [124] <author> J. E. Moreira, "LIDEX: </author> <title> A system for description, simulation and analysis of computer architecture and organization," M.S. </title> <type> thesis, </type> <institution> Department of Electrical Engineering, University of S~ao Paulo, </institution> <month> May </month> <year> 1990, </year> <note> in Portuguese. </note>
Reference-contexts: Problem-specific algorithms can also present task graphs with large amounts of functional parallelism. One example is digital system simulation. Instead of having a generic simulator that operates on a data structure describing the system, a specific simulator for that system can be generated <ref> [124] </ref>. The structure of this simulator will be isomorphic to the structure of the digital system and independent subsystems will generate independent tasks in the code. The parallelism of the independent tasks can then be exploited with autoscheduling.

References-found: 124

