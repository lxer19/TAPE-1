URL: ftp://theory.lcs.mit.edu/pub/cilk/ipps96.ps.Z
Refering-URL: http://theory.lcs.mit.edu/~cilk/abstracts/ipps96.html
Root-URL: 
Title: Dag-Consistent Distributed Shared Memory  
Author: Robert D. Blumofe Matteo Frigo Christopher F. Joerg Charles E. Leiserson Keith H. Randall 
Address: 545 Technology Square Cambridge, MA 02139  
Affiliation: MIT Laboratory for Computer Science  
Abstract: We introduce dag consistency, a relaxed consistency model for distributed shared memory which is suitable for multi-threaded programming. We have implemented dag consistency in software for the Cilk multithreaded runtime system running on a Connection Machine CM5. Our implementation includes a dag-consistent distributed cactus stack for storage allocation. We provide empirical evidence of the flexibility and efficiency of dag consistency for applications that include blocked matrix multiplication, Strassen's matrix multiplication algorithm, and a Barnes-Hut code. Although Cilk schedules the executions of these programs dynamically, their performances are competitive with statically scheduled implementations in the literature. We also prove that the number F P of page faults incurred by a user program running on P processors can be related to the number F 1 of page faults running serially by the formula F P F 1 + 2Cs, where C is the cache size and s is the number of thread migrations executed by Cilk's scheduler. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Joshua E. Barnes. </author> <title> A hierarchical O(N log N ) N -body code. Available on the Internet from ftp://hubble. </title> <publisher> ifa.hawaii.edu/pub/barnes/treecode/. </publisher>
Reference-contexts: We have implemented irregular applications that employ 2MB. The lower curve is for the matrixmul code in Figure 3 and the upper two curves are for an optimized version that uses no temporary storage. Cilk's dag-consistent shared memory, including a port of a Barnes-Hut N -body simulation <ref> [1] </ref> and an implementation of Strassen's algorithm [33] for matrix multiplication. These irregular applications provide a good test of Cilk's ability to schedule computations dynamically.
Reference: [2] <author> Monica Beltrametti, Kenneth Bobey, and John R. Zorbas. </author> <title> The control mechanism for the Myrias parallel computer system. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 21-30, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The notion that independent tasks may have incoherent views of each others' memory is not new to Cilk. The BLAZE [26] language incorporated a memory semantics similar to that of dag consistency into a PASCAL-like language. The Myrias <ref> [2] </ref> computer was designed to support a relaxed memory semantics similar to dag consistency, with many of the mechanisms implemented in hardware. Loosely-Coherent Memory [23] allows for a range of consistency protocols and uses compiler support to direct their use.
Reference: [3] <author> Robert D. Blumofe. </author> <title> Executing Multithreaded Programs Ef ficiently. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: We anticipate that it should be possible to memory-map files and use our existing dag-consistency mechanisms to provide a parallel, asynchronous, I/O capability for Cilk. We are also currently working on porting dag-consistent shared memory to our Cilk-NOW <ref> [3] </ref> adaptively parallel, fault-tolerant, network-of-workstations system. We are us ing operating system hooks to make the use of shared memory be transparent to the user.
Reference: [4] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kusz maul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 207-216, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: In this paper, we introduce dag consistency, a relaxed consistency model based on user-level threads which we have implemented for Cilk <ref> [4] </ref>, a C-based multithreaded language and runtime system. Dag consistency is defined on the dag of threads that make up a parallel computation. <p> The sync statement in line 21 causes the procedure to suspend until all the procedures it spawned have finished. 1 Shown is Cilk-3 code, which provides explicit linguistic support for shared-memory operations and call/return semantics for coordinating threads. The original Cilk-1 system <ref> [4] </ref> used explicit continuation passing to coordinate threads. <p> Like any Cilk multithreaded computation <ref> [4] </ref>, the parallel instruction stream of matrixmul can be viewed as a spawn tree of procedures broken into a directed acyclic graph, or dag, of threads. The spawn tree is exactly analogous to a traditional call tree. <p> Consequently, the processor can run with the speed of a serial algorithm with no overheads. Moreover, in Cilk, communication to enforce dependencies does not occur often <ref> [4] </ref>. It is worth mentioning that BACKER actually supports stronger semantics than Definition 1 requires. In fact, Definition 1 allows certain semantic anomalies, but BACKER handles these situations in the intuitively correct way. <p> systems, it might be reasonable to place the backing store on disk a la traditional virtual memory. 6 An analysis of page faults In this section, we examine the number F P of page faults that a Cilk computation incurs when run on P processors using Cilk's randomized work-stealing scheduler <ref> [4] </ref> and the implementation of the BACKER coherence algorithm described in Section 4.
Reference: [5] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Schedul ing multithreaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 356-368, </pages> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The Cilk runtime system automatically schedules the execution of the computation on the processors of a parallel computer using the provably efficient technique of work stealing <ref> [5] </ref>, in which idle processors steal spawned procedures from victim processors chosen at random. When a procedure is stolen, we refer to it and all of its descendant procedures in the spawn tree as a subcomputation.
Reference: [6] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Im plementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [25] and others <ref> [6, 11, 20] </ref>, Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [7, 21, 29] require some degree of hardware support [31] to manage shared memory efficiently.
Reference: [7] <author> David Chaiken and Anant Agarwal. </author> <title> Software-extended co herent shared memory: Performance and cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [25] and others [6, 11, 20], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines <ref> [7, 21, 29] </ref> require some degree of hardware support [31] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions [8, 18, 30, 34] require either an object-oriented programming model or explicit user management of objects.
Reference: [8] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> 9 In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <address> Litchfield Park, Ari-zona, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: In contrast, systems that use cache lines [7, 21, 29] require some degree of hardware support [31] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions <ref> [8, 18, 30, 34] </ref> require either an object-oriented programming model or explicit user management of objects. The idea of dag-consistent shared memory can be extended to the domain of file I/O to allow multiple threads to read and write the same file in parallel.
Reference: [9] <author> Daved E. Culler, Andrea Dusseau, Seth Copen Goldst ein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The dag-consistent shared memory performs at 5 megaflops per processor as long as the work per processor is sufficiently large. This performance compares fairly well with other matrix multiplication codes on the CM5 (that do not use the CM5's vector units). For example, an implementation coded in Split-C <ref> [9] </ref> attains just over 6 megaflops per processor on 64 processors using a static data layout, a static thread schedule, and an optimized assembly-language inner loop. In contrast, Cilk's dag-consistent shared memory is mapped across the processors dynamically, and the Cilk threads performing the computation are scheduled dynamically at runtime.
Reference: [10] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Charles Leiserson is currently Shaw Visiting Professor at the National University of Singapore. Keith Randall was supported in part by a Department of Defense NDSEG Fellowship. relaxed models of shared-memory consistency have been developed <ref> [10, 12, 13] </ref> that compromise on semantics for a faster implementation. By and large, all of these consistency models have had one thing in common: they are processor centric in the sense that they define consistency in terms of actions by physical processors.
Reference: [11] <author> Vincent W. Freeh, David K. Lowenthal, and Gregory R. </author> <title> An drews. Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 201-213, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [25] and others <ref> [6, 11, 20] </ref>, Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [7, 21, 29] require some degree of hardware support [31] to manage shared memory efficiently.
Reference: [12] <author> Guang R. Gao and Vivek Sarkar. </author> <title> Location consistency: Stepping beyond the barriers of memory coherence and seri-alizability. </title> <type> Technical Report 78, </type> <institution> McGill University, School of Computer Science, Advanced Compilers, Architectures, and Parallel Systems (ACAPS) Laboratory, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Charles Leiserson is currently Shaw Visiting Professor at the National University of Singapore. Keith Randall was supported in part by a Department of Defense NDSEG Fellowship. relaxed models of shared-memory consistency have been developed <ref> [10, 12, 13] </ref> that compromise on semantics for a faster implementation. By and large, all of these consistency models have had one thing in common: they are processor centric in the sense that they define consistency in terms of actions by physical processors. <p> Intuitively, a read can see a write in the dag-consistency model only if there is some serial execution order consistent with the dag in which the read sees the write. Unlike sequential consistency, but similar to certain processor-centric models <ref> [12, 14] </ref>, dag consistency allows different reads to return values that are based on different serial orders, but the values returned must respect the dependencies in the dag. The current Cilk mechanisms to support dag-consistent distributed shared memory on the Connection Machine CM5 are implemented in software.
Reference: [13] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phil lip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Charles Leiserson is currently Shaw Visiting Professor at the National University of Singapore. Keith Randall was supported in part by a Department of Defense NDSEG Fellowship. relaxed models of shared-memory consistency have been developed <ref> [10, 12, 13] </ref> that compromise on semantics for a faster implementation. By and large, all of these consistency models have had one thing in common: they are processor centric in the sense that they define consistency in terms of actions by physical processors.
Reference: [14] <author> James R. Goodman. </author> <title> Cache consistency and sequential con sistency. </title> <type> Technical Report 61, </type> <institution> IEEE Scalable Coherent Interface (SCI) Working Group, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Intuitively, a read can see a write in the dag-consistency model only if there is some serial execution order consistent with the dag in which the read sees the write. Unlike sequential consistency, but similar to certain processor-centric models <ref> [12, 14] </ref>, dag consistency allows different reads to return values that are based on different serial orders, but the values returned must respect the dependencies in the dag. The current Cilk mechanisms to support dag-consistent distributed shared memory on the Connection Machine CM5 are implemented in software.
Reference: [15] <author> E. A. Hauck and B. A. Dent. </author> <title> Burroughs' B6500/B7500 stack mechanism. </title> <booktitle> Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 245-251, </pages> <year> 1968. </year>
Reference-contexts: Since Cilk procedures operate in a parallel tree-like fashion, however, we needed some kind of parallel stack. We settled on implementing a cactus-stack <ref> [15, 28, 32] </ref> allocator. From the point of view of a single Cilk procedure, a cactus stack behaves much like an ordinary stack. The procedure can allocate and free memory by incrementing and decre-menting a stack pointer.
Reference: [16] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Archi tecture: a Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: If the runtime system had to operate on every word independently, however, the system would be terribly inefficient. Since extra fetches and reconciles do not adversely affect the BACKER coherence algorithm, we implemented the familiar strategy of grouping objects into pages <ref> [16, Section 8.2] </ref>, each of which is fetched or reconciled as a unit. Assuming that spatial locality exists when objects are accessed, grouping objects helps amortize the runtime system overhead.
Reference: [17] <author> Christopher F. Joerg. </author> <title> The Cilk System for Parallel Multi threaded Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: The original Cilk-1 system [4] used explicit continuation passing to coordinate threads. For a history of the evolution of Cilk, see <ref> [17] </ref>. 2 1 cilk void matrixmul (long nb, shared block *A, shared block *B, shared block *R) 2 -3 if (nb == 1) 4 multiply_block (A, B, R); 5 else -6 shared block *C,*D,*E,*F,*G,*H,*I,*J; 7 shared block *CG,*CH,*EG,*EH, *DI,*DJ,*FI,*FJ; 8 shared page_aligned block tmp [nb*nb]; /* get pointers to input submatrices <p> We assume without loss of generality that each thread performs at most one read or write. We define dag consistency as follows. (See also <ref> [17] </ref>.) Definition 1 The shared memory M of a multithreaded computation G = (V; E) is dag-consistent if the following two conditions hold: 1. <p> The key reason BACKER works is that it is always safe, at any point during the execution, for a processor p to reconcile an object or to flush a clean object. Suppose we arbitrarily insert a reconcile of an object into the computation 2 See <ref> [17] </ref> for details of a lazier coherence algorithm than BACKER based on climbing the spawn tree. 4 performed by p.
Reference: [18] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: These irregular applications provide a good test of Cilk's ability to schedule computations dynamically. We achieve a speedup of 9 on an 8192-particle N -body simulation using 32 processors, which is competitive with other software implementations of distributed shared memory <ref> [18] </ref>. Strassen's algorithm runs as fast as regular matrix multiplication for 2048 fi 2048 matrices, and we coded it in Cilk in a few hours. The remainder of this paper is organized as follows. <p> In contrast, systems that use cache lines [7, 21, 29] require some degree of hardware support [31] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions <ref> [8, 18, 30, 34] </ref> require either an object-oriented programming model or explicit user management of objects. The idea of dag-consistent shared memory can be extended to the domain of file I/O to allow multiple threads to read and write the same file in parallel.
Reference: [19] <author> Edward G. Coffman Jr. and Peter J. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1973. </year>
Reference-contexts: Indeed, at the point when one execution has just taken its Cth page fault, each cache contains exactly the last C distinct pages referenced <ref> [19] </ref>. We can now count the number of page faults during the execution E. The fault behavior of E is the same as the fault behavior of E 0 except for the subcomputation T and the sub-computation, call it U , from which it stole.
Reference: [20] <author> Pete Keleher, Alan L. Cox, Sandhya Dwarkadas, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In USENIX Winter 1994 Conference Proceedings, </booktitle> <pages> pages 115-132, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: We also describe the Cilk language extensions for supporting shared-memory objects and the diff mechanism <ref> [20] </ref> for managing dirty bits. Finally, we discuss minor anomalies in atomicity that can occur when the size of the concrete objects supported by the shared-memory system is different from the abstract objects that the programmer manipulates. The Cilk system on the CM5 supports concrete shared-memory objects of 32-bit words. <p> The CM5 provides no direct hardware support to maintain dirty bits explicitly at the granularity of words. Rather than using dirty bits explicitly, Cilk uses a diff mechanism as is used in the Treadmarks system <ref> [20] </ref>. The diff mechanism computes the dirty bit for an object by comparing that object's value with its value in a copy made at fetch time. Our implementation makes this copy only for pages loaded in read/write 5 mode, thereby avoiding the overhead of copying for read-only pages. <p> Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [25] and others <ref> [6, 11, 20] </ref>, Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [7, 21, 29] require some degree of hardware support [31] to manage shared memory efficiently.
Reference: [21] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Hein lein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford Flash multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [25] and others [6, 11, 20], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines <ref> [7, 21, 29] </ref> require some degree of hardware support [31] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions [8, 18, 30, 34] require either an object-oriented programming model or explicit user management of objects.
Reference: [22] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: 1 Introduction Architects of shared memory for parallel computers have attempted to support Lamport's model of sequential consistency <ref> [22] </ref>: The result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.
Reference: [23] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> LCM: Memory system support for parallel language implementation. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 208-218, </pages> <address> San Jose, Califor-nia, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The BLAZE [26] language incorporated a memory semantics similar to that of dag consistency into a PASCAL-like language. The Myrias [2] computer was designed to support a relaxed memory semantics similar to dag consistency, with many of the mechanisms implemented in hardware. Loosely-Coherent Memory <ref> [23] </ref> allows for a range of consistency protocols and uses compiler support to direct their use. Compared with these systems, Cilk provides a mul-tithreaded programming model based on directed acyclic graphs, which leads to a more flexible linguistic expression of operations on shared memory.
Reference: [24] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Doug las, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: We are currently attempting to characterize the semantics of BACKER fully. 4 Implementation This section describes our implementation of dag-consistent shared memory for the Cilk runtime system running on the Connection Machine Model CM5 parallel supercomputer <ref> [24] </ref>. We also describe the Cilk language extensions for supporting shared-memory objects and the diff mechanism [20] for managing dirty bits.
Reference: [25] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Compared with these systems, Cilk provides a mul-tithreaded programming model based on directed acyclic graphs, which leads to a more flexible linguistic expression of operations on shared memory. Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy <ref> [25] </ref> and others [6, 11, 20], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [7, 21, 29] require some degree of hardware support [31] to manage shared memory efficiently.
Reference: [26] <author> Piyush Mehrotra and Jon Van Rosendale. </author> <title> The BLAZE lan guage: A parallel language for scientific programming. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 339-361, </pages> <year> 1987. </year>
Reference-contexts: To conclude, we briefly outline work in this area and offer some ideas for the future. The notion that independent tasks may have incoherent views of each others' memory is not new to Cilk. The BLAZE <ref> [26] </ref> language incorporated a memory semantics similar to that of dag consistency into a PASCAL-like language. The Myrias [2] computer was designed to support a relaxed memory semantics similar to dag consistency, with many of the mechanisms implemented in hardware.
Reference: [27] <author> Robert C. Miller. </author> <title> A type-checking preprocessor for Cilk 2, a multithreaded C language. </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachu-setts Institute of Technology, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Specifically, in our CM5 implementation, shared memory is kept separate from other user memory, and special operations are required to operate on it. Most painfully, testing for page faults occurs explicitly in software, rather than implicitly in hardware. Our cilk2c type-checking preprocessor <ref> [27] </ref> alleviates some of the discomfort, but a transparent solution that uses hardware support for paging would be preferable. A minor advantage to the software approach we use, however, is that we can support full 64-bit addressing of shared memory on the 32-bit SPARC processors of the CM5 system.
Reference: [28] <author> Joel Moses. </author> <title> The function of FUNCTION in LISP or why the FUNARG problem should be called the envronment problem. </title> <type> Technical Report memo AI-199, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> June </month> <year> 1970. </year>
Reference-contexts: Since Cilk procedures operate in a parallel tree-like fashion, however, we needed some kind of parallel stack. We settled on implementing a cactus-stack <ref> [15, 28, 32] </ref> allocator. From the point of view of a single Cilk procedure, a cactus stack behaves much like an ordinary stack. The procedure can allocate and free memory by incrementing and decre-menting a stack pointer. <p> Procedures can reference common data through the shared portion of their stack address space. Cactus stacks have many of the same limitations as ordinary procedure stacks <ref> [28] </ref>. For instance, a child thread cannot return to its parent a pointer to an object that it has allocated. Similarly, sibling procedures cannot share storage that they create on the stack.
Reference: [29] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [25] and others [6, 11, 20], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines <ref> [7, 21, 29] </ref> require some degree of hardware support [31] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions [8, 18, 30, 34] require either an object-oriented programming model or explicit user management of objects.
Reference: [30] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The design and evalu ation of a shared object system for distributed memory machines. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <address> Mon-terey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In contrast, systems that use cache lines [7, 21, 29] require some degree of hardware support [31] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions <ref> [8, 18, 30, 34] </ref> require either an object-oriented programming model or explicit user management of objects. The idea of dag-consistent shared memory can be extended to the domain of file I/O to allow multiple threads to read and write the same file in parallel.
Reference: [31] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Stev en K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain access control for distributed shared memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <address> San Jose, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Like Ivy [25] and others [6, 11, 20], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [7, 21, 29] require some degree of hardware support <ref> [31] </ref> to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions [8, 18, 30, 34] require either an object-oriented programming model or explicit user management of objects.
Reference: [32] <author> Per Stenstrom. </author> <title> VLSI support for a cactus stack oriented memory organization. </title> <booktitle> Proceedings of the Twenty-First Annual Hawaii International Conference on System Sciences, </booktitle> <volume> volume 1, </volume> <pages> pages 211-220, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Since Cilk procedures operate in a parallel tree-like fashion, however, we needed some kind of parallel stack. We settled on implementing a cactus-stack <ref> [15, 28, 32] </ref> allocator. From the point of view of a single Cilk procedure, a cactus stack behaves much like an ordinary stack. The procedure can allocate and free memory by incrementing and decre-menting a stack pointer.
Reference: [33] <author> Volker Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Nu merische Mathematik, </journal> <volume> 14(3) </volume> <pages> 354-356, </pages> <year> 1969. </year>
Reference-contexts: The lower curve is for the matrixmul code in Figure 3 and the upper two curves are for an optimized version that uses no temporary storage. Cilk's dag-consistent shared memory, including a port of a Barnes-Hut N -body simulation [1] and an implementation of Strassen's algorithm <ref> [33] </ref> for matrix multiplication. These irregular applications provide a good test of Cilk's ability to schedule computations dynamically. We achieve a speedup of 9 on an 8192-particle N -body simulation using 32 processors, which is competitive with other software implementations of distributed shared memory [18].
Reference: [34] <author> Andrew S. Tanenbaum, Henri E. Bal, and M. Frans Kaash oek. </author> <title> Programming a distributed system using shared objects. </title> <booktitle> In Proceedingsof the Second International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 5-12, </pages> <address> Spokane, Washington, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: In contrast, systems that use cache lines [7, 21, 29] require some degree of hardware support [31] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions <ref> [8, 18, 30, 34] </ref> require either an object-oriented programming model or explicit user management of objects. The idea of dag-consistent shared memory can be extended to the domain of file I/O to allow multiple threads to read and write the same file in parallel.
Reference: [35] <author> Matthew J. Zekauskas, Wayne A. Sawdon, and Brian N. </author> <title> Ber shad. Software write detection for a distributed shared memory. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 87-100, </pages> <address> Mon-terey, California, </address> <month> November </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: Our implementation makes this copy only for pages loaded in read/write 5 mode, thereby avoiding the overhead of copying for read-only pages. The diff mechanism imposes extra overhead on each reconcile, but it imposes no extra overhead on each access <ref> [35] </ref>. Dag consistency can suffer from atomicity anomalies when abstract objects that the programmer is reading and writing are larger than the concrete objects supported by the shared-memory system. For example, suppose the programmer is treating two 4-byte concrete objects as one 8-byte abstract object.
References-found: 35

