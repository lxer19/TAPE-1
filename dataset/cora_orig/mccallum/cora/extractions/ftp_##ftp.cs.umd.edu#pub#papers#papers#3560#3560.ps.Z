URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3560/3560.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: ADAPTIVE USE OF ITERATIVE METHODS IN INTERIOR POINT METHODS FOR LINEAR PROGRAMMING  
Author: WEICHUNG WANG AND DIANNE P. O'LEARY 
Date: November 21, 1995  
Abstract: In this work we devise efficient algorithms for finding the search directions for interior point methods applied to linear programming problems. There are two innovations. The first is the use of updating of preconditioners computed for previous barrier parameters. The second is an adaptive automated procedure for determining whether to use a direct or iterative solver, whether to reinitialize or update the preconditioner, and how many updates to apply. These decisions are based on predictions of the cost of using the different solvers to determine the next search direction, given costs in determining earlier directions. These ideas are tested by applying a modified version of the OB1-R code of Lustig, Marsten, and Shanno to a variety of problems from the NETLIB and other collections. If a direct method is appropriate for the problem, then our procedure chooses it, but when an iterative procedure is helpful, substantial gains in efficiency can be obtained. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard Bartels and Linda Kaufman. </author> <title> Cholesky factor updating techniques for rank 2 matrix modifications. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 10(4) </volume> <pages> 557-592, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: This update may be computed as in <ref> [1] </ref> and [7]. If ff is big enough to include most of the large magnitude terms in the summation, then we have factored a matrix that differs from A ^ D 2 A T by a matrix of rank n ff.
Reference: [2] <author> W. Carolan, J. Hill, J. Kennington, S. Niemi, and S. Wichmann. </author> <title> An empirical evaluation of the KORBX algorithms for military airlift applications. </title> <journal> Operations Research, </journal> <volume> 38(2) </volume> <pages> 240-248, </pages> <year> 1990. </year>
Reference-contexts: 1948.82 1584.77 364.05 stocfor3 87 87 .70e-09 .70e-09 142.22 157.28 -15.06 truss 30 30 .80e-09 .80e-09 19.55 22.22 -2.67 wood1p 18 18 .35e-08 .27e-08 12.95 13.77 -0.82 woodw 37 37 .27e-08 .27e-08 25.30 27.65 -2.35 Table 2 Computational results for the larger test problems from NETLIB. 14 Niemi, and Wichmann <ref> [2] </ref>. We report results on problems with between 25; 000 and 370; 000 nonzero elements in the coefficient matrix A. These results suggest that the adaptive algorithm would work well on the bigger problems that we omitted from testing.
Reference: [3] <author> Tamra J. Carpenter and David F. Shanno. </author> <title> An interior point method for quadratic programs based on conjugate projected gradients. </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 </volume> <pages> 5-28, </pages> <year> 1993. </year>
Reference-contexts: At each interior point iteration, an incomplete Cholesky factor was computed and used as the preconditioner. Carpenter and Shanno used a diagonal preconditioner for a conjugate gradient solver for the normal equations in an interior point method for quadratic and linear programs <ref> [3] </ref>. They also considered recomputing the preconditioner every other iteration. Portugal, Resende, Veiga, and Judice introduced a truncated primal-infeasible dual-feasible interior point method, focusing on network flow problems [29]. The preconditioned conjugate gradient algorithm was used to solve the normal equations. <p> Such ill-conditioning is inevitable in the end stages of the interior point method. An alternative to computing the Cholesky factorization on every interior point iteration is to use the preconditioner computed for one fixed value of the barrier parameter for several values of <ref> [3] </ref> [19]. This reduces the computational work in forming the factorization. Preconditioner 2 : QR decomposition.
Reference: [4] <author> P. Chin and A. Vannelli. </author> <title> Computational methods for an LP model of the placement problem. </title> <type> Technical Report UWE&CE-94-02, </type> <institution> Department of Electrical and Computer Engineering, University of Waterloo, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Chin and Vannelli [5] solved a reduced KKT system using the preconditioned conjugate gradient algorithm and Bi-CGSTAB with incomplete factorization. In a different paper <ref> [4] </ref> they used an incomplete factorization as a preconditioner for the normal equations (3). Freund and Jarre [10] employed a symmetric variant of the quasi-minimal residual (QMR) method to solve the KKT systems.
Reference: [5] <author> P. Chin and A. Vannelli. </author> <title> Iterative methods for the augmented equations in large-scale linear programming. </title> <type> Technical Report UWE&CE-94-01, </type> <institution> Department of Electrical and Computer Engineering, University of Waterloo, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: They applied CGLS [28] to determine the search direction. Nash and Sofer investigated the choice of a preconditioner in the positive definite system Z T GZ where Z is rectangular and G is general symmetrici [27]. Chin and Vannelli <ref> [5] </ref> solved a reduced KKT system using the preconditioned conjugate gradient algorithm and Bi-CGSTAB with incomplete factorization. In a different paper [4] they used an incomplete factorization as a preconditioner for the normal equations (3). <p> Preconditioner 4 : Incomplete factorization. The preconditioner can be calculated by using incomplete Cholesky factorization [8], [16], an approximation to the exact Cholesky factorization determined by neglecting small elements in the tri angular matrix [30] or by discarding elements that do not fit a preassigned sparsity pattern <ref> [5] </ref>. An incomplete QR factorization could also be determined. Preconditioner 5 : Updated Cholesky factorization. Rather than discarding one of these preconditioners or keeping it fixed when changes, we can try to update it by a small-rank change, since the normal equations matrix is a continuous function of .
Reference: [6] <author> In Chan Choi, Clyde L. Monma, and David F. Shanno. </author> <title> Further development of a primal-dual interior point method. </title> <journal> ORSA Journal on Computing, </journal> <volume> 2(4) </volume> <pages> 304-311, </pages> <year> 1990. </year>
Reference-contexts: If not, iterative refinement using the factored matrix LP L T is employed repeatedly until the one-norm of the difference is sufficiently small. To deal with the dense columns in A, the LMS algorithm adopts the method suggested by Choi, Monma, and Shanno <ref> [6] </ref>. There are three main disadvantages to direct methods such as that in OB1-R. First, the methods will fail if the matrix K = AD 2 A T is very ill-conditioned.
Reference: [7] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: This update may be computed as in [1] and <ref> [7] </ref>. If ff is big enough to include most of the large magnitude terms in the summation, then we have factored a matrix that differs from A ^ D 2 A T by a matrix of rank n ff.
Reference: [8] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1986. </year>
Reference-contexts: This discussion sets the goals to be accomplished in designing an efficient algorithm. We will assume that the columns of A have been permuted using standard techniques in order to improve sparsity in the Cholesky factor of AD 2 A T (e.g., <ref> [8] </ref>, [22]). 2.1. Direct solvers: Cholesky factorization. Most existing linear programming interior point methods solve the normal equations by direct methods. The careful implementation OB1-R of Lustig, Marsten, and Shanno (LMS) [23] is representative of these methods, and the iterative methods will be compared with this implementation. <p> A standard theorem for the preconditioned conjugate gradient method guarantees termination (in exact arithmetic) in at most (k + 1) steps [16, Chap. 10]. Preconditioner 4 : Incomplete factorization. The preconditioner can be calculated by using incomplete Cholesky factorization <ref> [8] </ref>, [16], an approximation to the exact Cholesky factorization determined by neglecting small elements in the tri angular matrix [30] or by discarding elements that do not fit a preassigned sparsity pattern [5]. An incomplete QR factorization could also be determined. Preconditioner 5 : Updated Cholesky factorization.
Reference: [9] <author> A. V. Fiacco and G. P. McCormick. </author> <title> Nonlinear Programming : Sequential Unconstrained Minimization Techniques. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1968. </year> <booktitle> Reprint : Volume 4 of SIAM Classics in Applied Mathematics, </booktitle> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA 19104-2688, USA, </address> <year> 1990. </year>
Reference-contexts: Thus, the sparsity structure of the problem remains the same, in contrast to the linear systems arising in the simplex algorithm which differ by exchanges of columns of A. The roots of interior point algorithms date back to the algorithms of Fiacco and McCormick <ref> [9] </ref>, but ever since interior point algorithms first gained prominence in 1984 [18], researchers have given attention to speeding up the iteration time through efficient solution of the linear system.
Reference: [10] <author> Roland W. Freund and Florian Jarre. </author> <title> A QMR-based interior-point algorithm for solving linear programs. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories and Institut fur Angewandte Mathematik und Statistik, </institution> <year> 1995. </year>
Reference-contexts: Chin and Vannelli [5] solved a reduced KKT system using the preconditioned conjugate gradient algorithm and Bi-CGSTAB with incomplete factorization. In a different paper [4] they used an incomplete factorization as a preconditioner for the normal equations (3). Freund and Jarre <ref> [10] </ref> employed a symmetric variant of the quasi-minimal residual (QMR) method to solve the KKT systems. They suggested using indefinite SSOR preconditioners to accelerate the convergence. 2 The use of iterative methods has so far produced limited success.
Reference: [11] <author> D. M. Gay. </author> <title> Electronic mail distribution of linear programming test problems. </title> <journal> Mathematical Programming Soc. </journal> <note> COAL Newsletter, </note> <year> 1985. </year>
Reference-contexts: On those, the predicted number of iterations is too low. We now report computational results on various types of linear programming problems. 4.1. The NETLIB problems. We first consider the NETLIB collection of test problems <ref> [11] </ref>. For small problems, forming and factoring the matrices is rather inexpensive, so the adaptive algorithm chooses the direct method and its performance is similar to OB1-R. <p> Table 1 summarizes the problem characteristics. The numbers of rows, columns, and nonzeros indicated in the table refer to the output from the OB1-R preprocessor HPREP and may be different from the data in <ref> [11] </ref>. The tabulated number of nonzero elements of AA T and L count only the lower sub-diagonal part of AA T and L.
Reference: [12] <author> Philip E. Gill and Walter Murray. </author> <title> Newton-type methods for unconstrained and linearly constrained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 7 </volume> <pages> 311-350, </pages> <year> 1974. </year>
Reference-contexts: The elements in matrix D vary significantly and make the matrix K = AD 2 A T very ill-conditioned. The Cholesky factorization of K may not generate a good preconditioner, even if stable methods such as <ref> [12] </ref> are used. For all of these reasons, a direct method is used to determine the final search directions. We also switch to a direct method when OB1-R computes a Cholesky factorization with a zero on the diagonal.
Reference: [13] <author> Philip E. Gill, Walter Murray, Michael A. Saunders, J. A. Tomlin, and Margaret H. Wright. </author> <title> On projected Newton barrier methods for linear programming and an equivalence to Kar-markar's projective method. </title> <journal> Mathematical Programming, </journal> <volume> 36 </volume> <pages> 183-209, </pages> <year> 1986. </year>
Reference-contexts: Mehrotra and Wang [26] used an incomplete Cholesky factor of AD 2 A T as a preconditioner for conjugate gradients in a dual interior point method for network flow problems. Gill, Murray, Saunders, Tomlin, and Wright established the equivalence between Karmarkar's projected method and their projected Newton barrier method <ref> [13] </ref>. They used LSQR [28], preconditioned by an approximation to AD 2 A T , to find the search directions. Gold-farb and Mehrotra developed a relaxed version of Karmarkar's method that allows inexact projection [15]. They applied CGLS [28] to determine the search direction.
Reference: [14] <author> Philip E. Gill, Walter Murray, and Margaret H. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: For all of these reasons, a direct method is used to determine the final search directions. We also switch to a direct method when OB1-R computes a Cholesky factorization with a zero on the diagonal. This contingency could be avoided by using a modified Cholesky factor; see, for example, <ref> [14, Chap. 4] </ref>. 3.2. The adaptive conjugate gradient solver. We now focus on the details of the implementation of the preconditioned conjugate gradient solver in Algorithm 3.1.
Reference: [15] <author> D. Goldfarb and S. Mehrotra. </author> <title> A relaxed version of Karmarkar's method. </title> <journal> Mathematical Programming, </journal> <volume> 40(3) </volume> <pages> 289-315, </pages> <year> 1988. </year>
Reference-contexts: They used LSQR [28], preconditioned by an approximation to AD 2 A T , to find the search directions. Gold-farb and Mehrotra developed a relaxed version of Karmarkar's method that allows inexact projection <ref> [15] </ref>. They applied CGLS [28] to determine the search direction. Nash and Sofer investigated the choice of a preconditioner in the positive definite system Z T GZ where Z is rectangular and G is general symmetrici [27].
Reference: [16] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: The work during each iteration involves one product of K with a vector, one solution of a linear system involving the preconditioner, and several vector operations. More details about the method can be found in <ref> [16] </ref>. The conjugate gradient method preconditioned by the Cholesky factorization of K has somewhat better stability than the direct solver since the method does not require that the spectral radius of the iteration matrix be less than one. Thus, convergence can be achieved even if the factorization is quite inaccurate. <p> We consider some strategies for choosing the preconditioners in the next subsection. 2.3. The preconditioners. Convergence of the conjugate gradient iteration will be rapid if the preconditioned matrix has either a small condition number or great clustering of eigenvalues <ref> [16, Chap. 10] </ref>. We discuss five strategies for preconditioning. The first two are based on complete factorizations of the matrix K. The others factor a sparse portion of K or update a previous preconditioner. <p> The system involving the matrix A is then solved by using the partial factor and the Sherman-Morrison-Woodbury formula <ref> [16, Chap. 2] </ref>. Similarly, we let D 2 S denote the diagonal matrix containing only the elements corresponding to A S . <p> A standard theorem for the preconditioned conjugate gradient method guarantees termination (in exact arithmetic) in at most (k + 1) steps <ref> [16, Chap. 10] </ref>. Preconditioner 4 : Incomplete factorization. <p> A standard theorem for the preconditioned conjugate gradient method guarantees termination (in exact arithmetic) in at most (k + 1) steps [16, Chap. 10]. Preconditioner 4 : Incomplete factorization. The preconditioner can be calculated by using incomplete Cholesky factorization [8], <ref> [16] </ref>, an approximation to the exact Cholesky factorization determined by neglecting small elements in the tri angular matrix [30] or by discarding elements that do not fit a preassigned sparsity pattern [5]. An incomplete QR factorization could also be determined. Preconditioner 5 : Updated Cholesky factorization.
Reference: [17] <author> Clovis C. Gonzaga. </author> <title> Path-following methods for linear programming. </title> <journal> SIAM Review, </journal> <volume> 34(2) </volume> <pages> 167-224, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Newton's method is applied to solve the first order optimality conditions corresponding to each of the logarithmic barrier subproblems. The bulk of the work in such algorithms is the determination of a search direction for each iteration. Gonzaga <ref> [17] </ref> and Wright [32] surveyed interior point methods, and many computational issues are addressed by Lustig, Marsten, and Shanno [24]. Therefore, in this section we focus only on the linear systems arising in interior point methods.
Reference: [18] <author> N. K. Karmarkar. </author> <title> A new polynomial-time algorithm for linear programming. </title> <journal> Combinatorica, </journal> <volume> 4 </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: The roots of interior point algorithms date back to the algorithms of Fiacco and McCormick [9], but ever since interior point algorithms first gained prominence in 1984 <ref> [18] </ref>, researchers have given attention to speeding up the iteration time through efficient solution of the linear system. Direct methods that rely on sparse matrix factorizations have been the most popular approaches (e.g., [23], [31]), although iterative methods for solving linear systems have also received a fair amount of attention.
Reference: [19] <author> N. K. Karmarkar and K. G. Ramakrishnan. </author> <title> Computational results of an interior point algorithm for large scale linear programming. </title> <journal> Mathematical Programming, </journal> <volume> 52 </volume> <pages> 555-586, </pages> <year> 1991. </year>
Reference-contexts: Direct methods that rely on sparse matrix factorizations have been the most popular approaches (e.g., [23], [31]), although iterative methods for solving linear systems have also received a fair amount of attention. Karmarkar and Ramakrishnan reported computational results of Karmarkar's dual projection algorithm using a preconditioned conjugate gradient solver <ref> [19] </ref>. An incomplete Cholesky factorization of the matrix AD 2 A T was computed for one interior point iteration and then used as a preconditioner over several subsequent iterations. In their experiments, Cholesky factorization was performed on average every 2 to 3 iterations. <p> Such ill-conditioning is inevitable in the end stages of the interior point method. An alternative to computing the Cholesky factorization on every interior point iteration is to use the preconditioner computed for one fixed value of the barrier parameter for several values of [3] <ref> [19] </ref>. This reduces the computational work in forming the factorization. Preconditioner 2 : QR decomposition.
Reference: [20] <author> J. L. Kennington and R. V. Helgason. </author> <title> Algorithms for network programming. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1980. </year>
Reference-contexts: Network problems. Minimum cost flow network problems may be solved using linear programming algorithms (although it is generally more efficient to use a network algorithm like <ref> [20] </ref>). We test our algorithm on this class of problems because the matrix AA T and its resulting Cholesky factor tend to be much more dense than the original coefficient matrix A, even if there is no dense column in A.
Reference: [21] <author> D. Klingman, A. Napier, and J. Stutz. NETGEN: </author> <title> A program for generating large scale capaci-tated assignment, transportation, and minimum cost flow network problems. </title> <journal> Management Science, </journal> <volume> 20(5) </volume> <pages> 814-821, </pages> <month> January </month> <year> 1974. </year>
Reference-contexts: Forming and factoring the matrix AD 2 A T is thus quite expensive. We generated minimum cost flow network problems using NETGEN, developed by Klingman, Napier, and Stutz <ref> [21] </ref>. Table 5 gives the parameters we used, except for the number of nodes and arcs. Table 6 summarizes the characteristics of the resulting linear programming problems.
Reference: [22] <author> J. Liu. </author> <title> Modification of the minimum-degree algorithm by multiple elimination. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 11 </volume> <pages> 141-153, </pages> <year> 1985. </year>
Reference-contexts: This discussion sets the goals to be accomplished in designing an efficient algorithm. We will assume that the columns of A have been permuted using standard techniques in order to improve sparsity in the Cholesky factor of AD 2 A T (e.g., [8], <ref> [22] </ref>). 2.1. Direct solvers: Cholesky factorization. Most existing linear programming interior point methods solve the normal equations by direct methods. The careful implementation OB1-R of Lustig, Marsten, and Shanno (LMS) [23] is representative of these methods, and the iterative methods will be compared with this implementation.
Reference: [23] <author> Irvin J. Lustig, Roy E. Marsten, and David F. Shanno. </author> <title> Computational experience with a primal-dual interior point method for linear programming. Linear Algebra and Its Application, </title> <booktitle> 152 </booktitle> <pages> 191-222, </pages> <year> 1991. </year>
Reference-contexts: Direct methods that rely on sparse matrix factorizations have been the most popular approaches (e.g., <ref> [23] </ref>, [31]), although iterative methods for solving linear systems have also received a fair amount of attention. Karmarkar and Ramakrishnan reported computational results of Karmarkar's dual projection algorithm using a preconditioned conjugate gradient solver [19]. <p> Section 3 focuses on our algorithm for the adaptive choice of direct vs. iterative methods and the adaptive choice of a precon-ditioner. Numerical results obtained from a modified version of the OB1-R code of Lustig, Marsten, and Shanno <ref> [23] </ref> are presented in x 4. Final comments are made in x 5. 2. The linear system solvers. Either direct or iterative methods may be used for determining the search directions, the most expensive part of an interior point algorithm. <p> Direct solvers: Cholesky factorization. Most existing linear programming interior point methods solve the normal equations by direct methods. The careful implementation OB1-R of Lustig, Marsten, and Shanno (LMS) <ref> [23] </ref> is representative of these methods, and the iterative methods will be compared with this implementation. <p> Algorithm 3.2 gives a more detailed description of the iterative solver and its preconditioner. 3.1. The interior point algorithm with adaptive solver. Our interior point algorithm, Algorithm 3.1, chooses the initial variables, the step sizes, the barrier parameter, and convergence criteria following standard strategies <ref> [23] </ref>. The linear equation solver, however, has been modified to improve efficiency. In the first iteration of the algorithm, the normal equations (3) are solved directly by factoring K = AD 2 A T = LP L T . Starting from the second iteration, the algorithm uses preconditioned conjugate gradients.
Reference: [24] <author> Irvin J. Lustig, Roy E. Marsten, and David F. Shanno. </author> <title> Interior point methods for linear programming: Computational state of the art. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(1) </volume> <pages> 1-14, </pages> <month> Winter </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: The bulk of the work in such algorithms is the determination of a search direction for each iteration. Gonzaga [17] and Wright [32] surveyed interior point methods, and many computational issues are addressed by Lustig, Marsten, and Shanno <ref> [24] </ref>. Therefore, in this section we focus only on the linear systems arising in interior point methods. For definiteness, we consider the primal-dual formulation of interior point methods, but the linear algebra of primal formulations and dual formulations is similar.
Reference: [25] <author> Sanjay Mehrotra. </author> <title> Implementation of affine scaling methods: Approximate solutions of systems of linear equations using preconditioned conjugate gradient methods. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4(2) </volume> <pages> 103-118, </pages> <year> 1992. </year>
Reference-contexts: In their experiments, Cholesky factorization was performed on average every 2 to 3 iterations. Mehrotra used preconditioned conjugate gradients to solve the normal equations in a dual affine scaling interior point algorithm <ref> [25] </ref>. He addressed issues such as the stopping criterion and the stability of the implementation. At each interior point iteration, an incomplete Cholesky factor was computed and used as the preconditioner.
Reference: [26] <author> Sanjay Mehrotra and Jen-Shan Wang. </author> <title> Conjugate gradient based implementation of interior point methods for network flow problems. </title> <type> Technical Report 95-70.1, </type> <institution> Department of Industrial Engineering and Management Sciences, Northwestern University, </institution> <address> Evanston, IL 60208-3119, U.S.A., </address> <month> October </month> <year> 1995. </year>
Reference-contexts: The preconditioned conjugate gradient algorithm was used to solve the normal equations. They initially used the diagonal of the matrix AD 2 A T as a preconditioner and replaced it by spanning tree preconditioners in later iterations. Mehrotra and Wang <ref> [26] </ref> used an incomplete Cholesky factor of AD 2 A T as a preconditioner for conjugate gradients in a dual interior point method for network flow problems. Gill, Murray, Saunders, Tomlin, and Wright established the equivalence between Karmarkar's projected method and their projected Newton barrier method [13]. <p> We choose the parameter " pcg adaptively: " pcg = 10 8 ; if relgap &gt; 10 2 ; 10 8 fi (relgap) 1 2 ; otherwise, where relgap is the relative duality gap for the previous value of . This is similar to the stopping criterion in <ref> [26] </ref>. If the preconditioned conjugate gradient iteration number exceeds the maximum number of iterations allowed, then the current preconditioner is abandoned and a new preconditioner is determined by Cholesky factorization. If this happens twice, the iterative method is not suitable and we switch to a direct method.
Reference: [27] <author> Stephen G. Nash and Ariela Sofer. </author> <title> Preconditioning of reduced matrices. </title> <type> Technical Report Report 93-01, </type> <institution> Department of Operations Research and Engineering, George Mason University, Fairfax, </institution> <address> VA 22030, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: They applied CGLS [28] to determine the search direction. Nash and Sofer investigated the choice of a preconditioner in the positive definite system Z T GZ where Z is rectangular and G is general symmetrici <ref> [27] </ref>. Chin and Vannelli [5] solved a reduced KKT system using the preconditioned conjugate gradient algorithm and Bi-CGSTAB with incomplete factorization. In a different paper [4] they used an incomplete factorization as a preconditioner for the normal equations (3).
Reference: [28] <author> C. C. Paige and M. A. Saunders. </author> <title> LSQR : An algorithm for sparse linear equations and sparse least squares. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 8 </volume> <pages> 43-71, </pages> <year> 1982. </year>
Reference-contexts: Gill, Murray, Saunders, Tomlin, and Wright established the equivalence between Karmarkar's projected method and their projected Newton barrier method [13]. They used LSQR <ref> [28] </ref>, preconditioned by an approximation to AD 2 A T , to find the search directions. Gold-farb and Mehrotra developed a relaxed version of Karmarkar's method that allows inexact projection [15]. They applied CGLS [28] to determine the search direction. <p> They used LSQR <ref> [28] </ref>, preconditioned by an approximation to AD 2 A T , to find the search directions. Gold-farb and Mehrotra developed a relaxed version of Karmarkar's method that allows inexact projection [15]. They applied CGLS [28] to determine the search direction. Nash and Sofer investigated the choice of a preconditioner in the positive definite system Z T GZ where Z is rectangular and G is general symmetrici [27].
Reference: [29] <author> L. F. Portugal, M. G. C. Resende, G. Veiga, and J. J. Judice. </author> <title> A truncated primal-infeasible dual-feasible network interior point method. </title> <month> November </month> <year> 1994. </year>
Reference-contexts: They also considered recomputing the preconditioner every other iteration. Portugal, Resende, Veiga, and Judice introduced a truncated primal-infeasible dual-feasible interior point method, focusing on network flow problems <ref> [29] </ref>. The preconditioned conjugate gradient algorithm was used to solve the normal equations. They initially used the diagonal of the matrix AD 2 A T as a preconditioner and replaced it by spanning tree preconditioners in later iterations.
Reference: [30] <author> Youcef Saad. </author> <title> SPARSKIT : A Basic Tool Kit for Sparse Matrix Computations, </title> <note> 1994. Version 2 is located in an anonymous ftp area in ftp.cs.umn.edu within directory /dept/sparse. </note>
Reference-contexts: Preconditioner 4 : Incomplete factorization. The preconditioner can be calculated by using incomplete Cholesky factorization [8], [16], an approximation to the exact Cholesky factorization determined by neglecting small elements in the tri angular matrix <ref> [30] </ref> or by discarding elements that do not fit a preassigned sparsity pattern [5]. An incomplete QR factorization could also be determined. Preconditioner 5 : Updated Cholesky factorization.
Reference: [31] <author> Robert J. Vanderbei. </author> <title> LOQO : An interior point code for quadratic programming. </title> <institution> Program in Statistics and Operations Research, Princeton University. rvdb@princeton.edu, </institution> <year> 1995. </year>
Reference-contexts: Direct methods that rely on sparse matrix factorizations have been the most popular approaches (e.g., [23], <ref> [31] </ref>), although iterative methods for solving linear systems have also received a fair amount of attention. Karmarkar and Ramakrishnan reported computational results of Karmarkar's dual projection algorithm using a preconditioned conjugate gradient solver [19]. <p> The last column is the difference between the OB1-R and the adaptive times. A positive difference means the adaptive algorithm is faster. Table 2 shows that both algorithms attain a small relative duality gap except on the problem greenbea, which is well-known to be difficult for interior point methods <ref> [31] </ref>. The algorithms take the same number of values and achieve similar duality gaps except on the problem d6cube. On this problem the adaptive algorithm takes one additional iteration, achieves a duality gap 3 orders of magnitude smaller, and is faster.
Reference: [32] <author> M. H. Wright. </author> <title> Interior methods for constrained optimization. </title> <editor> In A. Iserles, editor, </editor> <booktitle> Acta Numerica 1992, </booktitle> <pages> pages 341-407. </pages> <publisher> Cambridge University Press, </publisher> <address> New York, USA, </address> <year> 1992. </year> <month> 19 </month>
Reference-contexts: Newton's method is applied to solve the first order optimality conditions corresponding to each of the logarithmic barrier subproblems. The bulk of the work in such algorithms is the determination of a search direction for each iteration. Gonzaga [17] and Wright <ref> [32] </ref> surveyed interior point methods, and many computational issues are addressed by Lustig, Marsten, and Shanno [24]. Therefore, in this section we focus only on the linear systems arising in interior point methods.
References-found: 32

