URL: ftp://ftp.research.microsoft.com/pub/debull/dec97-a4final.ps
Refering-URL: http://www.research.microsoft.com/research/db/debull/issues-list.htm
Root-URL: http://www.research.microsoft.com
Author: .Joseph M. Hellerstein 
Note: Bulletin of the Technical Committee on Data Engineering December 1997 Vol. 20 No. 4 IEEE Computer Society Letters Letter from the Editor-in-Chief. .David Lomet 1 Letter from the Special Issue Editor.  Announcements and Notices Very Large Databases (VLDB 98) Conference 46 Knowledge Discovery and Data Mining (KDD-98) Conference 47 1998 International Conference on Data Engineering back cover  
Abstract: Special Issue on Data Reduction Techniques The New Jersey Data Reduction Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Daniel Barbara, William DuMouchel, Christos Faloutsos, Peter J. Haas, Joseph M. Hellerstein, Yannis Ioannidis, H.V. Ja gadish, Theodore Johnson, Raymond Ng, Viswanath Poosala, Kenneth A. Ross, Kenneth C. Sevcik 3 
Abstract-found: 1
Intro-found: 1
Reference: [ACD+88] <author> M. J. Anderson, R. L. Cole, W. S. Davidson, W. D. Lee, P. B. Passe, G. R. Ricard and L. W. </author> <title> 39 Youngren, Index Key Range Estimator. </title> <editor> U. S. Patent 4,774,657, </editor> <publisher> IBM Corp., </publisher> <address> Armonk, NY, </address> <month> Sep. </month> <year> 1988. </year> <month> Filed June 6, </month> <year> 1986. </year>
Reference-contexts: However, it is also possible to make direct use of the nodes of index trees to obtain and exploit aggregate or summary information about a data set; this has been observed by numerous researchers and implemented in some database products <ref> [ACD+88, Ant93b, Aok97] </ref>. The information stored in an index node can be used for such purposes as providing approximate answers to queries or making choices in query optimization. As an example, consider looking only at the upper levels of an existing hierarchical index tree.
Reference: [Agr90] <author> A. </author> <title> Agresti. Categorical Data Analysis. </title> <publisher> Wiley-Interscience 1990. </publisher>
Reference-contexts: The phrase "multiplicative model" is more common in the computer science literature. Such models have been discussed and used since the 1940s or earlier, but especially since the 1970s, when computer algorithms to fit them became widely available. Many text-book treatments of log-linear modeling are available, for example <ref> [Agr90] </ref> and [BFH75]. Sample references from the Computer Science literature are [KK69], [Pea88], and [Mal91]. Log-linear models use only categorical variables continuous variables must be discretized first, and even then the modeling will not make use of the ordinal nature of the categories. <p> values, log-linear models might be useful for data reduction, depending on the complexity of the attributes for labeling endpoints. 5.2 Ordered Data There is no problem with using ordered categories with log-linear models, and some extensions of these models have been proposed to explicitly use the ordinal information, as in <ref> [Agr90] </ref> (Chapter 8). 17 5.3 Unordered Data This type of data is the primary application for log-linear models. 5.4 Sparse Data The log-linear methodology does not require dense data. However, as mentioned above, a very high-dimensional sample will usually have many cells with zero count in its multiway frequency table.
Reference: [Ant92] <author> G. Antoshenkov. </author> <title> Random sampling from pseudo-ranked B + trees. </title> <booktitle> In Proc. 19th Intl. Conf. Very Large Data Bases, </booktitle> <pages> pages 375-382. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Techniques for obtaining simple random samples (SRS's) from databases are developed in <ref> [Ant92, OR86, OR89, OR93, ORX90] </ref>. 32 * Approximate answers to aggregation queries The answer to an aggregation query consists of a small set of summary statistics, such as COUNT, AVERAGE, or MAXIMUM, that is computed from a specified set of records. <p> For example, if records are stored in a B + -tree or a hash file, then SRS's can be obtained using the techniques described in <ref> [Ant92, OR89] </ref> or [ORX90], respectively. In many file systems, pages of records are stored in contiguous blocks called extents, and a main memory data structure called an extent map provides access to the extents and the pages within the extents. <p> For example, data values having a linear ordering can be stored in a B + tree or a ranked B + tree, so that SRS's can be obtained using the methods in <ref> [Ant92, OR89] </ref>. 9.2.2 Sparse Data Depending on how the data is stored, sparseness of data may or may not have a detrimental effect on sample-based estimates.
Reference: [Ant93a] <author> G. Antoshenkov. </author> <title> Dynamic query optimization in Rdb/VMS. </title> <booktitle> In Proc. Eleventh Intl. Conf. Data Engrg., </booktitle> <pages> pages 538-547. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90]. Several authors have outlined complete sampling-based approaches to query optimization <ref> [Ant93a, SBM93, Wil91] </ref>. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm. Typically, records are assigned to processors based on the attribute values of the records.
Reference: [Ant93b] <author> Gennady Antoshenkov. </author> <title> Query Processing in DEC Rdb: Major Issues and Future Challenges. </title> <journal> IEEE Data Engineering Bulletin 16(4) </journal> <pages> 42-45, </pages> <year> 1993. </year>
Reference-contexts: However, it is also possible to make direct use of the nodes of index trees to obtain and exploit aggregate or summary information about a data set; this has been observed by numerous researchers and implemented in some database products <ref> [ACD+88, Ant93b, Aok97] </ref>. The information stored in an index node can be used for such purposes as providing approximate answers to queries or making choices in query optimization. As an example, consider looking only at the upper levels of an existing hierarchical index tree. <p> The overhead of handling insertion and deletion can be ameliorated by using pseudo-ranking <ref> [Ant93b] </ref>, which allows counters to diverge a certain amount from the accurate values. * While counters are the most natural "decoration" that one can add to index entries, there is no reason one can not store additional statistics in the entries, though additional statistics can consume space and further reduce the
Reference: [Aok97] <author> P. M. Aoki. </author> <title> Generalizing "Search" in Generalized Search Trees. </title> <booktitle> Proc. 14th Int'l Conf. on Data Engineering, </booktitle> <address> Orlando, FL, </address> <month> Feb. </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: However, it is also possible to make direct use of the nodes of index trees to obtain and exploit aggregate or summary information about a data set; this has been observed by numerous researchers and implemented in some database products <ref> [ACD+88, Ant93b, Aok97] </ref>. The information stored in an index node can be used for such purposes as providing approximate answers to queries or making choices in query optimization. As an example, consider looking only at the upper levels of an existing hierarchical index tree. <p> Generalized Search Trees [HNP95] support arbitrary keys of this nature, and Aoki's extensions to them <ref> [Aok97] </ref> extend the idea of psuedo-ranking to support extensible "divergence control" for arbitrary statistics. 8.5 Indexes and Histograms It is asserted above that indexes can be viewed as hierarchical histograms, but not all flavors of histograms can be conveniently supported in index trees.
Reference: [Ben75] <author> J. L. Bentley. </author> <title> Multidimensional binary search trees used for associative searching. </title> <journal> Comm ACM, </journal> <volume> 18(9) </volume> <pages> 509-517, </pages> <month> September </month> <year> 1975. </year>
Reference: [Ber92] <author> Michael W. Berry. </author> <title> Large-scale sparse singular value computations. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 6(1) </volume> <pages> 13-49, </pages> <month> Spring </month> <year> 1992. </year>
Reference-contexts: For example, in the Latent Semantic Indexing method (LSI), SVD is used on very sparse document-term matrices. [FD92]. Fast sparse-matrix SVD algorithms have been recently developed <ref> [Ber92] </ref>. 2.3.3 Skewed Data SVD can handle skewed data.
Reference: [BFH75] <author> Y. Bishop, S. Fienberg, and P. Holland. </author> <title> Discrete Multivariate Analysis: Theory and Practice. </title> <publisher> MIT Press, </publisher> <year> 1975. </year>
Reference-contexts: Such models have been discussed and used since the 1940s or earlier, but especially since the 1970s, when computer algorithms to fit them became widely available. Many text-book treatments of log-linear modeling are available, for example [Agr90] and <ref> [BFH75] </ref>. Sample references from the Computer Science literature are [KK69], [Pea88], and [Mal91]. Log-linear models use only categorical variables continuous variables must be discretized first, and even then the modeling will not make use of the ordinal nature of the categories.
Reference: [Bir63] <author> M. </author> <title> Birch. Maximum Likelihood in Three-way Contingency Tables. </title> <journal> J. Roy. Statist. Soc., </journal> <volume> B25:220-233, </volume> <year> 1963. </year>
Reference-contexts: Using log-linear models involves two steps: choosing a general form (how many factors to use and what sets of attributes are associated with each factor) and then estimating the numerical values of the array elements for each factor (parameter estimation). An important result due to <ref> [Bir63] </ref> is that, given the results of step one, the parameter estimation problem only requires as input the marginal proportions corresponding to the combinations of attributes making up the factor arrays.
Reference: [BKK96] <author> S. Berthold, D. Keim, and H. P. Kriegel. X-Tree: </author> <title> An Indexing Structure for High Dimensional Data. </title> <booktitle> Proc. 22nd VLDB, </booktitle> <pages> pages 10-21, </pages> <month> August </month> <year> 1996. </year> <institution> Mumbai, India. </institution>
Reference-contexts: the tree in locally dense regions of the space, or by placing split boundaries in the dense areas to retain some balance in the populations associated with various tree nodes. 8.7.4 High-Dimensional Data Some index trees have been developed explicitly to address high dimensional problems (e.g., TV-trees [LJF94] and X-trees <ref> [BKK96] </ref>). The efficacy of these structures remains in doubt, especially in light of recent results on the hardness of indexing high-dimensional space [HKP97].
Reference: [BKSS90] <author> N. Beckmann, H. Kriegel, R. Schneider, and B. Seeger. </author> <title> The R*-tree: An Efficient and Robust Access Method For Points and Rectangles. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 322-331, </pages> <address> Atlantic City, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The most common example is the R-tree [Gut84] and its variants: the R*-tree <ref> [BKSS90] </ref> and R+-tree [SRF87]. Other multidimensional search trees include quad-trees [FB74], k-D-B-trees [Rob81], hB-trees [LS90], and TV-trees [LJF94]. Multidimensional data can also be transformed into unidimensional data using a space-filling curve [Jag90]; after transformation, a 27 B+-tree can be used to index the resulting unidimensional data.
Reference: [BM72] <author> Rudolf Bayer and Edward M. McCreight. </author> <title> Organization and Maintenance of Large Ordered Indices. </title> <journal> Acta Informatica, </journal> <pages> pages 173-189, </pages> <month> January </month> <year> 1972. </year>
Reference-contexts: Typically they are used to speed up selection queries on one-dimensional data sets ordered on a single key attribute. B+-trees are the most common and significant form of index tree for disk-resident one-dimensional data <ref> [BM72, Com79] </ref>. For data sets of higher dimension (i.e., those organized and accessed on the basis of values of two or more attributes in combination), a variety of other types of disk-based index trees have been developed and used.
Reference: [BS97] <author> D. Barbara and M. Sullivan. Quasi-Cubes: </author> <title> A space-efficient way to support approximate multidimensional databases. </title> <type> Technical Report, </type> <institution> Department of Information and Software Systems Engineering, George Mason University, </institution> <year> 1997. </year>
Reference-contexts: A first cut of the answer consists of the estimated values for all the points requested. That answer can be polished by retrieving the real values of the outliers progressively replacing the estimated values for those data points. A technique similar to this has been successfully used in <ref> [BS97] </ref>. * Incremental maintenance: As new data gets incorporated in the dataset, the relevant model (s) need to be updated to reflect the effect of this data.
Reference: [Cat92] <author> J. Catlett. Peepholing: </author> <title> Choosing attributes efficiently for megainduction. </title> <booktitle> In Proc. Ninth Intl. Work. Machine Learning, </booktitle> <pages> pages 49-54. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Several authors have suggested that certain data mining algorithms can yield satisfactory approximate results when applied to a random sample of the data <ref> [Cat92, JL96, KM94] </ref>. There are many different types of samples.
Reference: [Coc77] <author> W. G. Cochran. </author> <title> Sampling Techniques. </title> <publisher> Wiley, </publisher> <address> New York, third edition, </address> <year> 1977. </year>
Reference-contexts: of lower, more manageable dimensionality. 31 9 Sampling The notion that a large set of data can be represented by a small random sample of the data elements goes back to the end of the nineteenth century and has led to the development of a large body of survey-sampling techniques <ref> [Coc77, SSW92, Sud76] </ref>. Over the past fifteen years, there has been increasing interest in the application of sampling ideas to database management systems (DBMS's). <p> In a "shared-nothing" parallel DBMS, for example, a stratum might correspond to the records stored at a specified processing node; see [SN92] for a discussion of why, in parallel DBMS's, stratified sampling usually is preferable to simple random sampling. Other types of samples abound <ref> [Coc77, DC72, SSW92, Sud76] </ref>; we focus on simple, cluster, and stratified samples since these are the most common types of reduced data sets found in DBMS's. Sampling is well-suited to the progressive refinement of a reduced data set: to "refine" the data set further, simply take more samples.
Reference: [Com79] <author> D. Comer. </author> <title> The Ubiquitous B-Tree. </title> <journal> Computing Surveys, </journal> <volume> 11(2) </volume> <pages> 121-137, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: Typically they are used to speed up selection queries on one-dimensional data sets ordered on a single key attribute. B+-trees are the most common and significant form of index tree for disk-resident one-dimensional data <ref> [BM72, Com79] </ref>. For data sets of higher dimension (i.e., those organized and accessed on the basis of values of two or more attributes in combination), a variety of other types of disk-based index trees have been developed and used.
Reference: [CR94] <author> C.M. Chen and N. Roussopoulos. </author> <title> Adaptive Selectivity Estimation Using Query Feedback. </title> <booktitle> In Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Minneapolis, Min-nesota, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The updating of the model can be achieved by using techniques similar to those described in <ref> [CR94] </ref> to update polynomial models for selectivity estimation. The techniques use a method called recursive least-square-error [You84] to avoid a lot of expensive recomputation. 5 Log-Linear Models Log-linear modeling is a methodology for approximating discrete multidimensional probability distributions.
Reference: [Cra46] <author> H. Cramer. </author> <title> Mathematical Methods of Statistics. </title> <publisher> Princeton University Press, </publisher> <year> 1946. </year>
Reference-contexts: The basic methodology outlined above has been extended in several different directions. * For SRS's, central limit theorems (and hence sample-size formulas and confidence intervals) have been established for large classes of summary statistics other than population averages, for example, population moments <ref> [Cra46, Chapter 28] </ref>, maximum likelihood estimators [Cra46, Chapter 33], and U-statistics [Hoe48]. Moreover, the "delta method" can be used to derive new central limit theorems from old. <p> The basic methodology outlined above has been extended in several different directions. * For SRS's, central limit theorems (and hence sample-size formulas and confidence intervals) have been established for large classes of summary statistics other than population averages, for example, population moments [Cra46, Chapter 28], maximum likelihood estimators <ref> [Cra46, Chapter 33] </ref>, and U-statistics [Hoe48]. Moreover, the "delta method" can be used to derive new central limit theorems from old.
Reference: [Cra94] <author> Richard E. Crandall. </author> <title> Projects in Scientific Computation. </title> <publisher> Springer-Verlag New York, Inc., </publisher> <year> 1994. </year>
Reference-contexts: Following the literature, the appropriate value for the constant C is 1= p 2, because it makes the transformation matrix orthonormal (eg., see Eq. 8). An orthonormal matrix is a matrix which has columns that are unit vectors and that are mutually orthogonal. Adapting the notation (eg., from <ref> [Cra94] </ref> [VM]), the Haar transform is defined as follows: d l;i = 1= 2 (s l1;2i s l1;2i+1 ) l = 0; : : : ; L; i = 0; : : : ; n=2 l+1 1 (5) with s l;i = 1= 2 (s l1;2i + s l1;2i+1 ) l
Reference: [Dau92] <author> Ingrid Daubechies. </author> <title> Ten Lectures on Wavelets. </title> <publisher> Capital City Press, </publisher> <address> Montpelier, Vermont, </address> <year> 1992. </year> <institution> Society for Industrial and Applied Mathematics (SIAM), </institution> <address> Philadelphia, PA. </address>
Reference-contexts: There are numerous wavelet transforms [PTVF96], some popular ones being the so-called Daubechies-4 and Daubechies-6 transforms <ref> [Dau92] </ref>. 3.1.1 Discussion The computational complexity of the above transforms is O (n), as can be verified from Eq. 5-7. In addition to their computational speed, there is a fascinating relationship between wavelets, multireso-lution methods (like quadtrees or the pyramid structures in machine vision), and fractals. <p> Naturally occurring scenes tend to excite only few of the neurons, implying that a wavelet transform will achieve excellent compression for such images. Similarly, the human ear seems to use a wavelet transform to analyze a sound, at least in the very first stage <ref> [Dau92, p. 6] </ref> [WS93]. In conclusion, the Discrete Wavelet Transform (DWT) achieves even better energy concentration than the DFT and Discrete Cosine (DCT) transforms, for natural signals [PTVF96, p. 604].
Reference: [DC72] <author> T. R. Dell and J. L. </author> <title> Clutter. Ranked set sampling theory with order statistics background. </title> <journal> Biometrics, </journal> <volume> 28 </volume> <pages> 545-555, </pages> <year> 1972. </year>
Reference-contexts: In a "shared-nothing" parallel DBMS, for example, a stratum might correspond to the records stored at a specified processing node; see [SN92] for a discussion of why, in parallel DBMS's, stratified sampling usually is preferable to simple random sampling. Other types of samples abound <ref> [Coc77, DC72, SSW92, Sud76] </ref>; we focus on simple, cluster, and stratified samples since these are the most common types of reduced data sets found in DBMS's. Sampling is well-suited to the progressive refinement of a reduced data set: to "refine" the data set further, simply take more samples. <p> rank a small set of records in approximate order of increasing value of f without actually evaluating f itself, then ranked set sampling techniques can be used to estimate averages, quantiles, and other summary statistics using many fewer function evaluations than are required by simple random sampling; see, for example, <ref> [DC72, SS88] </ref> and references therein.
Reference: [DH73] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year> <month> 40 </month>
Reference-contexts: SVD is a popular and powerful operation, and it has been used in numerous applications, such as statistical analysis (as the driving engine behind the Principal Component Analysis [Jol86]), 6 text retrieval under the name of Latent Semantic Indexing [Dum94], pattern recognition and dimen-sionality reduction as the Karhunen-Loeve (KL) transform <ref> [DH73] </ref>, and face recognition [TP91]. SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. See [Str80] or [PTVF96] for more details. The latter citation also gives `C' code.
Reference: [DNSS92] <author> D. DeWitt, J. F. Naughton, D. A. Schneider, and S. Seshadri. </author> <title> Practical skew handling algorithms for parallel joins. </title> <booktitle> In Proc. 19th Intl. Conf. Very Large Data Bases, </booktitle> <pages> pages 27-40. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: The goal is to determine a rule that assigns approximately the same number of records to each processor. Sampling can be used to estimate the distribution of attribute values and hence obtain good assignment rules. The parallel join-algorithms in <ref> [DNSS92] </ref> and the algorithms for efficient loading of parallel grid files in [LRS93], for example, use sampling in this manner. * Support for auditing Various types of auditing require retrieval of a random sample of the records in a database or, in relational DBMS's, a random sample from the tuples in <p> This data structure can be exploited to efficiently obtain a SRSWR of pages by repeatedly generating a random number between 1 and the number of pages and then using the extent map to retrieve the corresponding page <ref> [DNSS92] </ref>. If a SRS of records (rather than pages) is required, then extent-map sampling can be combined with an acceptance/rejection (A/R) technique as described, for example, in [Olk93].
Reference: [DS40] <author> W. Deming and F. Stephan. </author> <title> On a least squares adjustment of a sampled frequency table when the expected marginal totals are known. </title> <journal> Annals Math. Stat., </journal> <volume> 11 </volume> <pages> 427-444, </pages> <year> 1940] </year>
Reference-contexts: In general, however, an iterative method will be required to obtain the maximum likelihood (maximum entropy) estimates for scaling factors to be applied to the marginals. The most common such method is iterative proportional scaling, generally attributed to <ref> [DS40] </ref>, which is guaranteed to converge to a unique solution whenever the marginal arrays have all positive elements and are consistent with each other.
Reference: [Dum94] <author> Susan T. Dumais. </author> <title> Latent semantic indexing (lsi) and trec-2. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Second Text Retrieval Conference (TREC-2), </booktitle> <pages> pages 105-115, </pages> <address> Gaithersburg, MD, </address> <month> March </month> <year> 1994. </year> <note> NIST. Special publication 500-215. </note>
Reference-contexts: SVD is a popular and powerful operation, and it has been used in numerous applications, such as statistical analysis (as the driving engine behind the Principal Component Analysis [Jol86]), 6 text retrieval under the name of Latent Semantic Indexing <ref> [Dum94] </ref>, pattern recognition and dimen-sionality reduction as the Karhunen-Loeve (KL) transform [DH73], and face recognition [TP91]. SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. See [Str80] or [PTVF96] for more details. The latter citation also gives `C' code.
Reference: [EKX95] <author> M. Ester, H.P. Kriegel and X. Xu. </author> <title> (1995) Knowledge Discovery in Large Spatial Databases: Focusing Techniques for Efficient Class Identification, </title> <booktitle> Proc. Fourth International Symposium on Large Spatial Databases. </booktitle>
Reference-contexts: It is highly tunable, depending on how much CPU time the user can afford. Focusing techniques based on spatial access methods (e.g., R* trees, Voronoi diagrams) are later developed to reduce the I/O cost required by CLARANS <ref> [EKX95] </ref>. By employing a balanced tree structure called CF tree, BIRCH makes explicit and takes full advantage of the amount of available buffering space [ZRL96]. A single scan of the dataset gives a basic clustering, and additional scans can be used to improve the quality further.
Reference: [EKXS96] <author> M. Ester, H.P. Kriegel, J. Sander and X. Xu. </author> <title> (1996) A Density-based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, </title> <booktitle> Proc. Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pp. 226-231. </pages>
Reference-contexts: Relying on the parameters of the size of the neighborhood and the minimum number of data points in the neighborhood, DBSCAN connects regions of sufficiently high densities into clusters <ref> [EKXS96] </ref>. As such, it does a better job finding elongated clusters than most of the algorithms mentioned above. It uses an R* tree to achieve good performance. Finally, STING is 4 There are a few methods that tolerate a limited degree of overlap between clusters. <p> Thus, clustering algorithms should scale linearly with the number of dimensions. However, in practice, the situation is not as rosy, particularly for those algorithms that rely on various kinds of indexing to facilitate processing. For instance, for algorithms relying on trees (e.g., BIRCH [ZRL96] and DBSCAN <ref> [EKXS96] </ref>) the O (logn) factor degrades to O (n) as the dimensionality increases.
Reference: [EN82] <author> J. Ernvall and O. Nevalainen. </author> <title> An algorithm for unbiased random sampling. </title> <journal> Comput. J., </journal> <volume> 25 </volume> <pages> 45-47, </pages> <year> 1982. </year>
Reference-contexts: Of course, for purposes of data reduction one usually is interested in small samples, and the hashing method given in <ref> [EN82] </ref> can be used to update a small to medium sized SRSWOR efficiently. The ease of producing and maintaining a random sample depends on the available sampling frame, that is, the available mechanism for randomly accessing elements of the record set.
Reference: [Fal96] <author> Christos Faloutsos. </author> <title> Searching Multimedia Databases by Content. </title> <publisher> Kluwer Academic Inc., </publisher> <year> 1996. </year> <note> ISBN 0-7923-9777-0. </note>
Reference-contexts: Observation 2.4: The i-th row vector of U fi fl gives the coordinates of the i-th data vector ("cus tomer"), when it is projected in the new space dictated by SVD. For more details and additional properties of the SVD, see [KJF97] or <ref> [Fal96] </ref>. 2.2 Distance-Only Data SVD can be applied to any attribute-types, including un-ordered ones, like `car-type' or `customer name', as we saw earlier. It will naturally group together similar `customer-names' into customer groups with similar behavior. 9 2.3 Multi-Dimensional Data As described, SVD is tailored to 2-d matrices.
Reference: [FD92] <author> Peter W. Foltz and Susan T. Dumais. </author> <title> Personalized information delivery: an analysis of information filtering methods. </title> <journal> Comm. of ACM (CACM), </journal> <volume> 35(12) </volume> <pages> 51-60, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: For example, in the Latent Semantic Indexing method (LSI), SVD is used on very sparse document-term matrices. <ref> [FD92] </ref>. Fast sparse-matrix SVD algorithms have been recently developed [Ber92]. 2.3.3 Skewed Data SVD can handle skewed data.
Reference: [Fie93] <author> D.J. </author> <title> Field. Scale-invariance and self-similar `wavelet' transforms: an analysis fo natural scenes and mammalian visual systems. </title> <editor> In M. Farge, J.C.R. Hunt, and J.C. Vassilicos, editors, </editor> <booktitle> Wavelets, Fractals, and Fourier Transforms, </booktitle> <pages> pages 151-193. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1993. </year>
Reference-contexts: The reason is that wavelets, like quadtrees, will need only a few non-zero coefficients for regions of the image (or the time sequence) that are smooth (i.e., homogeneous), while they will spend more effort on the `high activity' areas. It is believed <ref> [Fie93] </ref> that the mammalian retina consists of neurons which are tuned 11 each to a different wavelet. Naturally occurring scenes tend to excite only few of the neurons, implying that a wavelet transform will achieve excellent compression for such images.
Reference: [FB74] <author> R. A. Finkel and J. L. Bentley. Quad-Trees: </author> <title> A Data Structure For Retrieval On Composite Keys. </title> <journal> ACTA Informatica, </journal> <volume> 4(1) </volume> <pages> 1-9, </pages> <year> 1974. </year>
Reference-contexts: The most common example is the R-tree [Gut84] and its variants: the R*-tree [BKSS90] and R+-tree [SRF87]. Other multidimensional search trees include quad-trees <ref> [FB74] </ref>, k-D-B-trees [Rob81], hB-trees [LS90], and TV-trees [LJF94]. Multidimensional data can also be transformed into unidimensional data using a space-filling curve [Jag90]; after transformation, a 27 B+-tree can be used to index the resulting unidimensional data.
Reference: [Fis87] <author> D. Fisher. </author> <title> (1987) Acquisition via Incremental Conceptual Clustering, </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 2. </pages>
Reference-contexts: Thus, with only one scan of the dataset, clustering can be achieved by using the stored information but without recourse to the individual objects. 7.1.3 Machine Learning Methods There are a few clustering methods developed in the machine learning community. They are mostly probability-based approaches <ref> [Fis87] </ref>. And typically, they make the assumption that the probability distributions on different attributes are independent of each other. In practice, this is often too strong an assumption, because correlation may exist between attributes.
Reference: [FL95] <author> C. Faloutsos and K. Lin. </author> <title> FastMap: a Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets. </title> <booktitle> InProc. 1995 ACM SIGMOD Intl. Conf. Management of Data, </booktitle> <pages> pages 163-174. </pages>
Reference-contexts: It uses multiresolution analysis, and it models well the early signal processing operations of the human eye and human ear. 3.2 Distance-Only Data In this case, DWT can only be applied after the data have been mapped to an k-dimensional space, with, e.g., Multidimensional scaling, or FastMap <ref> [FL95] </ref>. 3.3 Multi-Dimensional Data As mentioned, the DWT can be applied to an k-dimensional hyper-cube.
Reference: [GG97] <author> V. Gaede and O. Gunther. </author> <title> Multidimensional Access Methods. </title> <journal> ACM Computing Surveys, </journal> <note> 1997. To appear. </note>
Reference-contexts: Multidimensional data can also be transformed into unidimensional data using a space-filling curve [Jag90]; after transformation, a 27 B+-tree can be used to index the resulting unidimensional data. A survey of multidimensional indexes is given by Gaede and Gunther <ref> [GG97] </ref>. 8.2 A Generalized Picture of Index Trees The canonical rough picture of a database index tree appears in Figure 5. It is typically a balanced tree, with high fanout. The internal nodes are used as a directory.
Reference: [GGMS96] <author> S. Ganguly, P. B. Gibbons, Y. Matias, and A. Silberschatz. </author> <title> Bifocal sampling for skew-resistant join size estimation. </title> <booktitle> In Proc. 1996 ACM SIGMOD Intl. Conf. Management of Data, </booktitle> <pages> pages 271-281. </pages> <publisher> ACM Press, </publisher> <year> 1996. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, <ref> [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90] </ref>. Several authors have outlined complete sampling-based approaches to query optimization [Ant93a, SBM93, Wil91]. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm.
Reference: [Gly82] <author> P. W. Glynn. </author> <title> Asymptotic theory for nonparametric confidence intervals. </title> <type> Technical Report 63, </type> <institution> Department of Operations Research, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1982. </year>
Reference-contexts: Finally, for data sets with moderate skew, "corrected" confidence intervals with improved coverage properties can in principle be computed using an extension (to the setting of discrete data values) of the "second-order pivotal transformations" discussed in <ref> [Gly82] </ref>. 9.2.4 High-Dimensional Data One potential difficulty caused by high-dimensional data is the large amount of space required to store a sample of a given size. If storage is limited, then the size of the sample may be too small to provide sufficiently accurate estimates.
Reference: [GM96] <author> Phillip Gibbons and Yossi Matias. </author> <title> Space efficient maintenance of top sellers list in large databases. </title> <type> Unpublished manuscript, </type> <institution> Bell Labs, </institution> <year> 1996. </year>
Reference-contexts: values). * Progressive resolution refinement: End-biased histograms directly provide the finest partitioning of data (into individual values) and hence can not provide further resolution. * Incremental maintenance: Gibbons and Matias have designed efficient techniques with theoretical bounds on accuracy to incrementally maintain the highest frequency values in a database relation <ref> [GM96] </ref>. 7 Clustering Techniques In the past 30 years, cluster analysis has been widely studied in statistics. The objective is to identify clusters embedded in the data.
Reference: [GMP97] <author> Phillip B. Gibbons, Yossi Matias, and Viswanath Poosala. </author> <title> Fast incremental maintenance of approximate histograms. </title> <booktitle> Proc. of the 23rd Int. Conf. on Very Large Databases, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: Multi-dimensional histograms have also been studied in detail [MD88, PI97]. By building histograms on multiple attributes together, these techniques are able to capture dependencies between those attributes. Incremental maintenance techniques for histograms and samples have also been investigated <ref> [GMP97] </ref>, as has the use of histograms in parallel-join load balancing [PI96]. Finally, there are several sources where one may find extensive discussions of histogram-based estimation techniques [Koo80, IP95b, MCS88, Poo97]. <p> This approach leads to inaccurate estimates from outdated histograms and can be quite expensive when used on a database with very large number of relations. Recent work has shown that some classes of histograms can be maintained efficiently and accurately using incremental techniques <ref> [GMP97] </ref>. These techniques make use of a (possibly disk-resident) backing sample, which is also incrementally maintained as a uniformly random representative of the underlying data. <p> V2 and Oracle 7 SQL Server to estimate a variety of catalog statistics from samples of the base relations, and there is ongoing research into sampling-based methods for estimating such key catalog statistics as quantiles and "column cardinality" (the number of distinct values of an attribute in a relation); see <ref> [GMP97, HNSS95, PIHS96] </ref>. Sampling is much less expensive than exact computation of catalog statistics from entire relations; such cost reduction is important since catalog statistics must be recomputed periodically as the database changes over time. <p> many sampling algorithms, the cost of obtaining a sample is proportional to the size of the sample, and not the size of the database; this is in contrast to other data reduction techniques that require at least one complete pass through the data. (Sometimes, as in the case of histograms <ref> [GMP97, PIHS96] </ref>, sampling can be combined with another data reduction technique, yielding an approximate reduction of the data that is relatively cheap to obtain.) It is also relatively inexpensive to update a sample as the underlying data changes; see [GMP97, OR92] for some updating methods. <p> pass through the data. (Sometimes, as in the case of histograms [GMP97, PIHS96], sampling can be combined with another data reduction technique, yielding an approximate reduction of the data that is relatively cheap to obtain.) It is also relatively inexpensive to update a sample as the underlying data changes; see <ref> [GMP97, OR92] </ref> for some updating methods. The adequacy of sampling as a data-reduction technique depends crucially on how the sample is to be used. We focus on perhaps the most common use of a sample: estimation of the answer to an aggregation query.
Reference: [Gut84] <author> A. Guttman. R-Trees: </author> <title> A Dynamic Index Structure For Spatial Searching. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 47-57, </pages> <address> Boston, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: For data sets of higher dimension (i.e., those organized and accessed on the basis of values of two or more attributes in combination), a variety of other types of disk-based index trees have been developed and used. The most common example is the R-tree <ref> [Gut84] </ref> and its variants: the R*-tree [BKSS90] and R+-tree [SRF87]. Other multidimensional search trees include quad-trees [FB74], k-D-B-trees [Rob81], hB-trees [LS90], and TV-trees [LJF94].
Reference: [Haa96] <author> P. J. Haas. </author> <title> Hoeffding inequalities for join-selectivity estimation and online aggregation. </title> <institution> IBM Research Report RJ 10040, IBM Almaden Research Center, </institution> <address> San Jose, CA, </address> <year> 1996. </year> <month> 41 </month>
Reference-contexts: Sampling provides a means of obtaining quick, approximate answers to a variety of aggregation queries. Sampling techniques for aggregation queries in object-relational DBMS's have been studied in [HOD91, HOT88, HOT89, ODT+91]. These techniques also have been studied in the context of online-aggregation systems <ref> [Haa96, Haa97, HHW97] </ref>. In such a system, the user can observe the progress of an aggregation query and control execution on the fly; the records observed so far are viewed as a random sample of the set of all records in the database. <p> This difficulty has been at least partially overcome: formulas for estimators, large sample confidence intervals, and conservative confidence intervals corresponding to a variety of complex aggregation queries can be found in <ref> [Haa96, Haa97] </ref>. These formulas explicitly take into account the statistical dependence between the tuples in the sample version of the output relation.
Reference: [Haa97] <author> P. J. Haas. </author> <title> Large-sample and deterministic confidence intervals for online aggregation. </title> <booktitle> In Proc. Ninth Intl. Conf. Scientific and Statist. Database Management, </booktitle> <pages> pages 51-63. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1997. </year>
Reference-contexts: Sampling provides a means of obtaining quick, approximate answers to a variety of aggregation queries. Sampling techniques for aggregation queries in object-relational DBMS's have been studied in [HOD91, HOT88, HOT89, ODT+91]. These techniques also have been studied in the context of online-aggregation systems <ref> [Haa96, Haa97, HHW97] </ref>. In such a system, the user can observe the progress of an aggregation query and control execution on the fly; the records observed so far are viewed as a random sample of the set of all records in the database. <p> This difficulty has been at least partially overcome: formulas for estimators, large sample confidence intervals, and conservative confidence intervals corresponding to a variety of complex aggregation queries can be found in <ref> [Haa96, Haa97] </ref>. These formulas explicitly take into account the statistical dependence between the tuples in the sample version of the output relation.
Reference: [HHW97] <author> J. M. Hellerstein, P. J. Haas, and H. J. Wang. </author> <title> Online aggregation. </title> <booktitle> In Proc. 1997 ACM SIGMOD Intl. Conf. Managment of Data. </booktitle> <publisher> ACM Press, </publisher> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: Sampling provides a means of obtaining quick, approximate answers to a variety of aggregation queries. Sampling techniques for aggregation queries in object-relational DBMS's have been studied in [HOD91, HOT88, HOT89, ODT+91]. These techniques also have been studied in the context of online-aggregation systems <ref> [Haa96, Haa97, HHW97] </ref>. In such a system, the user can observe the progress of an aggregation query and control execution on the fly; the records observed so far are viewed as a random sample of the set of all records in the database.
Reference: [HKP97] <author> Joseph M. Hellerstein, Elias Koutsoupias, and Christos H. Papadimitriou. </author> <title> On the Analysis of Indexing Schemes. </title> <booktitle> In Proc. 16th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <pages> pages 249-256, </pages> <address> Tucson, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: The efficacy of these structures remains in doubt, especially in light of recent results on the hardness of indexing high-dimensional space <ref> [HKP97] </ref>.
Reference: [HNP95] <author> J. M. Hellerstein, J. F. Naughton, and A. Pfeffer. </author> <title> Generalized Search Trees for Database Systems (Extended Abstract). </title> <booktitle> In Proc. 21st International Conference on Very Large Data Bases, </booktitle> <address> Zurich, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: Most space-parititoning trees are not balanced, which renders them less useful for disk-based storage; typically they are mapped to another representation when saved to disk. Though there are many variations of index trees for both one-dimensional and multi-dimensional data, they all have shared properties. The Generalized Search Tree (GiST) <ref> [HNP95] </ref> is a template index tree that provides a common basis for describing and easily implementing a variety of index trees. In our discussion here, we focus on properties that tend to be shared by many if not all index tree variants. <p> Generalized Search Trees <ref> [HNP95] </ref> support arbitrary keys of this nature, and Aoki's extensions to them [Aok97] extend the idea of psuedo-ranking to support extensible "divergence control" for arbitrary statistics. 8.5 Indexes and Histograms It is asserted above that indexes can be viewed as hierarchical histograms, but not all flavors of histograms can be conveniently
Reference: [HNSS95] <author> P. J. Haas, J. F. Naughton, S. Seshadri, and L. </author> <title> Stokes. Sampling-based estimation of the number of distinct values of an attribute. </title> <booktitle> In Proc. 21st Intl. Conf. Very Large Data Bases, </booktitle> <pages> pages 311-322. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: V2 and Oracle 7 SQL Server to estimate a variety of catalog statistics from samples of the base relations, and there is ongoing research into sampling-based methods for estimating such key catalog statistics as quantiles and "column cardinality" (the number of distinct values of an attribute in a relation); see <ref> [GMP97, HNSS95, PIHS96] </ref>. Sampling is much less expensive than exact computation of catalog statistics from entire relations; such cost reduction is important since catalog statistics must be recomputed periodically as the database changes over time. <p> On the other hand, it can be extremely hard to estimate statistics such as the number of distinct values of a specified attribute when the data is skewed in frequency. Some distinct-value estimation procedures 37 that can deal with moderate skew are discussed in <ref> [HNSS95, HS96] </ref>; a drawback of these procedures is that the sample size required for a specified degree of accuracy depends on the size of the data set. Even sums or averages can be hard to estimate when the data is skewed in value.
Reference: [HNSS96] <author> P. J. Haas, J. F. Naughton, S. Seshadri, and A. N. Swami. </author> <title> Selectivity and cost estimation for joins based on random sampling. </title> <journal> J. Comput. System Sci., </journal> <volume> 52 </volume> <pages> 550-569, </pages> <year> 1996. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, <ref> [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90] </ref>. Several authors have outlined complete sampling-based approaches to query optimization [Ant93a, SBM93, Wil91]. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm.
Reference: [HOD91] <author> W. Hou, G. Ozsoyoglu, and E. Dogdu. </author> <title> Error-constrained COUNT query evaluation in relational databases. </title> <booktitle> In Proc. 1991 ACM SIGMOD Intl. Conf. Managment of Data, </booktitle> <pages> pages 278-287. </pages> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, <ref> [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90] </ref>. Several authors have outlined complete sampling-based approaches to query optimization [Ant93a, SBM93, Wil91]. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm. <p> Sampling provides a means of obtaining quick, approximate answers to a variety of aggregation queries. Sampling techniques for aggregation queries in object-relational DBMS's have been studied in <ref> [HOD91, HOT88, HOT89, ODT+91] </ref>. These techniques also have been studied in the context of online-aggregation systems [Haa96, Haa97, HHW97]. <p> In practice, either two-phase or sequential procedures can be used to estimate 2 (f ) and control the sample size automatically; see, for example, <ref> [HS92, HOD91] </ref>. Similarly, a priori bounds on the function f often are available in practice, so that 28 can be used to determine the required sample size. The above calculations also can be turned around to yield estimates of the precision of b n for a specified sample size n.
Reference: [Hoe48] <author> W. Hoeffding. </author> <title> A class of statistics with asymptotically normal distribution. </title> <journal> Ann. Math. Statist., </journal> <volume> 19 </volume> <pages> 293-325, </pages> <year> 1948. </year>
Reference-contexts: outlined above has been extended in several different directions. * For SRS's, central limit theorems (and hence sample-size formulas and confidence intervals) have been established for large classes of summary statistics other than population averages, for example, population moments [Cra46, Chapter 28], maximum likelihood estimators [Cra46, Chapter 33], and U-statistics <ref> [Hoe48] </ref>. Moreover, the "delta method" can be used to derive new central limit theorems from old. <p> Then can be estimated by b n = n ! 1 n X n X d (X i ; X j ); where d is the distance function. The estimator b n is a U-statistic <ref> [Hoe48] </ref>, and therefore is unbiased and consistent for . (An estimator b n is consistent for a parameter if b n converges to as n 36 increases.) Moreover, there is a well-developed methodology for obtaining confidence intervals for b n .
Reference: [Hoe63] <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> J. Amer. Statist. Assoc., </journal> <volume> 58 </volume> <pages> 13-30, </pages> <year> 1963. </year>
Reference-contexts: This approximate result is valid when * is small. Conservative sample-size formulas can be derived from inequalities developed by Hoeffding <ref> [Hoe63] </ref>. For example, suppose that l f (r i ) u 34 for 1 i N and set p = 2 1 p for 0 &lt; p &lt; 1.
Reference: [HOT88] <author> W. Hou, G. Ozsoyoglu, and B. Taneja. </author> <title> Statistical estimators for relational algebra expressions. </title> <booktitle> In Proc. Seventh ACM SIGACT-SIGMOD-SIGART Symp. Principles of Database Sys., </booktitle> <pages> pages 276-287. </pages> <publisher> ACM Press, </publisher> <year> 1988. </year>
Reference-contexts: Sampling provides a means of obtaining quick, approximate answers to a variety of aggregation queries. Sampling techniques for aggregation queries in object-relational DBMS's have been studied in <ref> [HOD91, HOT88, HOT89, ODT+91] </ref>. These techniques also have been studied in the context of online-aggregation systems [Haa96, Haa97, HHW97].
Reference: [HOT89] <author> W. Hou, G. Ozsoyoglu, and B. Taneja. </author> <title> Processing aggregate relational queries with hard time constraints. </title> <booktitle> In Proc. 1989 ACM SIGMOD Intl. Conf. Managment of Data, </booktitle> <pages> pages 68-77. </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: Sampling provides a means of obtaining quick, approximate answers to a variety of aggregation queries. Sampling techniques for aggregation queries in object-relational DBMS's have been studied in <ref> [HOD91, HOT88, HOT89, ODT+91] </ref>. These techniques also have been studied in the context of online-aggregation systems [Haa96, Haa97, HHW97].
Reference: [HS92] <author> P. J. Haas and A. N. Swami. </author> <title> Sequential sampling procedures for query size estimation. </title> <booktitle> In Proc. 1992 ACM SIGMOD Intl. Conf. Managment of Data, </booktitle> <pages> pages 1-11. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, <ref> [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90] </ref>. Several authors have outlined complete sampling-based approaches to query optimization [Ant93a, SBM93, Wil91]. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm. <p> In practice, either two-phase or sequential procedures can be used to estimate 2 (f ) and control the sample size automatically; see, for example, <ref> [HS92, HOD91] </ref>. Similarly, a priori bounds on the function f often are available in practice, so that 28 can be used to determine the required sample size. The above calculations also can be turned around to yield estimates of the precision of b n for a specified sample size n.
Reference: [HS95] <author> P. J. Haas and A. N. Swami. </author> <title> Sampling-based selectivity estimation using augmented frequent value statistics. </title> <booktitle> In Proc. Eleventh Intl. Conf. Data Engrg., </booktitle> <pages> pages 522-531. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, <ref> [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90] </ref>. Several authors have outlined complete sampling-based approaches to query optimization [Ant93a, SBM93, Wil91]. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm. <p> For example, if there is a combined index on the attributes of interest, then additional samples from the qualifying subset can efficiently be obtained by sampling from the index. As another example, Haas and Swami <ref> [HS95] </ref> describe a method for estimating the selectivity of a join in which the sample is augmented with frequency counts for certain join-attribute values that are frequent in some relations and infrequent in other relations. <p> Alternatively, the sample can be supplemented with a small set of records having highly nonstandard function values. These values can be combined with the sample-based estimate in a manner similar to that in <ref> [HS95] </ref>.
Reference: [HS96] <author> P. J. Haas and L. </author> <title> Stokes. Estimating the number of classes in a finite population. </title> <institution> IBM Research Report RJ 10025, IBM Almaden Research Center, </institution> <address> San Jose, CA, </address> <year> 1996. </year>
Reference-contexts: On the other hand, it can be extremely hard to estimate statistics such as the number of distinct values of a specified attribute when the data is skewed in frequency. Some distinct-value estimation procedures 37 that can deal with moderate skew are discussed in <ref> [HNSS95, HS96] </ref>; a drawback of these procedures is that the sample size required for a specified degree of accuracy depends on the size of the data set. Even sums or averages can be hard to estimate when the data is skewed in value.
Reference: [IC93] <author> Yannis Ioannidis and Stavros Christodoulakis. </author> <title> Optimal histograms for limiting worst-case error propagation in the size of join results. </title> <journal> ACM TODS, </journal> <year> 1993. </year>
Reference-contexts: Efficient sampling-based techniques exist for computing all classes of histograms and are given in [PIHS96]. Most of the work on histograms is in the context of evaluating their accuracy in estimating the result sizes of queries containing selections [Koo80, PSC84] and joins <ref> [IC93, Ioa93, IP95a] </ref>. Multi-dimensional histograms have also been studied in detail [MD88, PI97]. By building histograms on multiple attributes together, these techniques are able to capture dependencies between those attributes. <p> For example, equi-depth histograms [Koo80, MD88, PSC84] work well for range queries only when the data distribution has low skew, while V-Optimal (F,F) histograms <ref> [IC93, Ioa93, IP95a] </ref> have only been proven optimal for equality joins and selections when a list of all the attribute values in each bucket is maintained. Earlier work has shown that the most accurate and practical histograms belong to the V-Optimal (V,A) and MaxDiff (V,A) classes [Poo97].
Reference: [Inf97] <institution> Informix Corporation. Technical Brief: Informix Metacube Explorer, </institution> <year> 1997. </year> <note> http://www.informix.com/informix/products/techbrfs/metacube. </note>
Reference-contexts: Online application processing (OLAP) systems compute many aggregate statistics of interest, and several OLAP products now support sampling-based estimation; see, for example, <ref> [Inf97] </ref>. * Data mining Data mining algorithms typically are applied to extremely large data sets. Several authors have suggested that certain data mining algorithms can yield satisfactory approximate results when applied to a random sample of the data [Cat92, JL96, KM94]. There are many different types of samples.
Reference: [Ioa93] <author> Yannis Ioannidis. </author> <title> Universality of serial histograms. </title> <booktitle> Proc. of the 19th Int. Conf. on Very Large Databases, </booktitle> <pages> pages 256-267, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Efficient sampling-based techniques exist for computing all classes of histograms and are given in [PIHS96]. Most of the work on histograms is in the context of evaluating their accuracy in estimating the result sizes of queries containing selections [Koo80, PSC84] and joins <ref> [IC93, Ioa93, IP95a] </ref>. Multi-dimensional histograms have also been studied in detail [MD88, PI97]. By building histograms on multiple attributes together, these techniques are able to capture dependencies between those attributes. <p> For example, equi-depth histograms [Koo80, MD88, PSC84] work well for range queries only when the data distribution has low skew, while V-Optimal (F,F) histograms <ref> [IC93, Ioa93, IP95a] </ref> have only been proven optimal for equality joins and selections when a list of all the attribute values in each bucket is maintained. Earlier work has shown that the most accurate and practical histograms belong to the V-Optimal (V,A) and MaxDiff (V,A) classes [Poo97]. <p> Some of the research on histogram-based join result size estimation has shown the benefits of storing values with extreme frequencies. The class of end-biased histograms contains a few high-frequency values and a few low-frequency values in singleton buckets and the rest in a single large bucket <ref> [Ioa93] </ref>. 3 . These histograms are less expensive to construct than the general class of histograms, occupy less space, and often offer equally high accuracies for join queries. A few commercial systems also employ singleton buckets for selectivity estimation purposes.
Reference: [IP95a] <author> Yannis Ioannidis and Viswanath Poosala. </author> <title> Balancing histogram optimality and practicality for query result size estimation. </title> <booktitle> Proc. of ACM SIGMOD Conf, </booktitle> <pages> pages 233-244, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Efficient sampling-based techniques exist for computing all classes of histograms and are given in [PIHS96]. Most of the work on histograms is in the context of evaluating their accuracy in estimating the result sizes of queries containing selections [Koo80, PSC84] and joins <ref> [IC93, Ioa93, IP95a] </ref>. Multi-dimensional histograms have also been studied in detail [MD88, PI97]. By building histograms on multiple attributes together, these techniques are able to capture dependencies between those attributes. <p> For example, equi-depth histograms [Koo80, MD88, PSC84] work well for range queries only when the data distribution has low skew, while V-Optimal (F,F) histograms <ref> [IC93, Ioa93, IP95a] </ref> have only been proven optimal for equality joins and selections when a list of all the attribute values in each bucket is maintained. Earlier work has shown that the most accurate and practical histograms belong to the V-Optimal (V,A) and MaxDiff (V,A) classes [Poo97]. <p> It has been shown that end-biased histograms are quite accurate in estimating join results sizes, particularly when the data is skewed <ref> [IP95a] </ref>. On the other hand, since they do not approximate the entire data distribution, they can not be used effectively for estimating the result sizes of selection predicates.
Reference: [IP95b] <author> Yannis Ioannidis and Viswanath Poosala. </author> <title> Histogram-based solutions to diverse database estimation problems. </title> <journal> IEEE Data Engineering Bulletin, </journal> <volume> 18(3) </volume> <pages> 10-18, </pages> <month> December </month> <year> 1995. </year> <month> 42 </month>
Reference-contexts: Incremental maintenance techniques for histograms and samples have also been investigated [GMP97], as has the use of histograms in parallel-join load balancing [PI96]. Finally, there are several sources where one may find extensive discussions of histogram-based estimation techniques <ref> [Koo80, IP95b, MCS88, Poo97] </ref>. In the following sections, the effectiveness of histograms in approximating different kinds of data is studied. 6.2 Distance-Only Data The current histogram techniques cannot approximate such data, because they rely on information about the placement of data in a multi-dimensional space.
Reference: [Jag90] <author> H. V. Jagadish. </author> <title> Linear Clustering of Objects With Multiple Attributes. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 332-342, </pages> <address> Atlantic City, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The most common example is the R-tree [Gut84] and its variants: the R*-tree [BKSS90] and R+-tree [SRF87]. Other multidimensional search trees include quad-trees [FB74], k-D-B-trees [Rob81], hB-trees [LS90], and TV-trees [LJF94]. Multidimensional data can also be transformed into unidimensional data using a space-filling curve <ref> [Jag90] </ref>; after transformation, a 27 B+-tree can be used to index the resulting unidimensional data. A survey of multidimensional indexes is given by Gaede and Gunther [GG97]. 8.2 A Generalized Picture of Index Trees The canonical rough picture of a database index tree appears in Figure 5.
Reference: [JL96] <author> G. H. John and P. Langley. </author> <title> Static versus dynamic sampling for data mining. </title> <booktitle> In Proc. Second Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 367-370. </pages> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: Several authors have suggested that certain data mining algorithms can yield satisfactory approximate results when applied to a random sample of the data <ref> [Cat92, JL96, KM94] </ref>. There are many different types of samples.
Reference: [Jol86] <author> I.T. Jolliffe. </author> <title> Principal Component Analysis. </title> <publisher> Springer Verlag, </publisher> <year> 1986. </year>
Reference-contexts: SVD is a popular and powerful operation, and it has been used in numerous applications, such as statistical analysis (as the driving engine behind the Principal Component Analysis <ref> [Jol86] </ref>), 6 text retrieval under the name of Latent Semantic Indexing [Dum94], pattern recognition and dimen-sionality reduction as the Karhunen-Loeve (KL) transform [DH73], and face recognition [TP91]. SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. <p> Also, recall that the rank of a matrix is the highest number of linearly independent rows (or columns). Eq. 2 equivalently states that a matrix X can be brought in the following form, the so-called spectral decomposition <ref> [Jol86, p. 11] </ref>: X = 1 u 1 fi v t 2 + : : : + r u r fi v t where u i , and v i are column vectors of the U and V matrices respectively, and i the diagonal elements of the matrix fl.
Reference: [KD80] <author> P. M. Kroonenberg and J. De Leeuw. </author> <title> Principal Component Analysis of Three-Mode Data By Means of Alternating Least Squares Algorithms. </title> <journal> Psychometrika, </journal> <volume> 45 </volume> <pages> 69-97, </pages> <year> 1980. </year>
Reference-contexts: Higher dimensionalities can be handled by reducing the problem to 2 dimensions. For example, for the DataCube (`product', `customer', `date')(`dollars-spent') we could create two attributes, such as `product' and (`customer' fi `date'). Direct extension to 3-dimensional SVD has been studied, under the name of 3-mode PCA <ref> [KD80] </ref>. 2.3.1 Ordered and Unordered Attributes SVD can handle them all, as mentioned under the 'Distance-Only' subsection above. 2.3.2 Sparse Data SVD can handle sparse data. For example, in the Latent Semantic Indexing method (LSI), SVD is used on very sparse document-term matrices. [FD92].
Reference: [KJF97] <author> F. Korn, H.V. Jagadish and C. Faloutsos. </author> <title> Efficiently Supporting ad Hoc Queries in Large Datasets of Time Sequences. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 289-300, </pages> <address> Tucson, </address> <year> 1997. </year>
Reference-contexts: Observation 2.4: The i-th row vector of U fi fl gives the coordinates of the i-th data vector ("cus tomer"), when it is projected in the new space dictated by SVD. For more details and additional properties of the SVD, see <ref> [KJF97] </ref> or [Fal96]. 2.2 Distance-Only Data SVD can be applied to any attribute-types, including un-ordered ones, like `car-type' or `customer name', as we saw earlier. It will naturally group together similar `customer-names' into customer groups with similar behavior. 9 2.3 Multi-Dimensional Data As described, SVD is tailored to 2-d matrices.
Reference: [KM94] <author> J. Kivinen and H. Mannila. </author> <title> The power of sampling in knowledge discovery. </title> <booktitle> In Proc. Thirteenth ACM SIGACT-SIGMOD-SIGART Symp. Principles of Database Sys., </booktitle> <pages> pages 77-85. </pages> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: Several authors have suggested that certain data mining algorithms can yield satisfactory approximate results when applied to a random sample of the data <ref> [Cat92, JL96, KM94] </ref>. There are many different types of samples.
Reference: [Knu73] <author> D. E. Knuth. </author> <title> Sorting and Searching, </title> <booktitle> volume 3 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1973. </year>
Reference-contexts: Trees with such counts are said to be ranked <ref> [Knu73] </ref>. Ranked trees with non-overlapping keys truly are hierarchical histograms, and allow for arbitrary refinement of buckets by traversing pointers. The advantages of ranking do not come without cost.
Reference: [Koo80] <author> R. P. Kooi. </author> <title> The optimization of queries in relational databases. </title> <type> PhD thesis, </type> <institution> Case Western Reserver University, </institution> <month> Sept </month> <year> 1980. </year>
Reference-contexts: Efficient sampling-based techniques exist for computing all classes of histograms and are given in [PIHS96]. Most of the work on histograms is in the context of evaluating their accuracy in estimating the result sizes of queries containing selections <ref> [Koo80, PSC84] </ref> and joins [IC93, Ioa93, IP95a]. Multi-dimensional histograms have also been studied in detail [MD88, PI97]. By building histograms on multiple attributes together, these techniques are able to capture dependencies between those attributes. <p> Incremental maintenance techniques for histograms and samples have also been investigated [GMP97], as has the use of histograms in parallel-join load balancing [PI96]. Finally, there are several sources where one may find extensive discussions of histogram-based estimation techniques <ref> [Koo80, IP95b, MCS88, Poo97] </ref>. In the following sections, the effectiveness of histograms in approximating different kinds of data is studied. 6.2 Distance-Only Data The current histogram techniques cannot approximate such data, because they rely on information about the placement of data in a multi-dimensional space. <p> The same applies to approximating multi-dimensional data within a single attribute as well (e.g., polygons). 6.4 Aspects of histogram usage * Accuracy: Although histograms are used in many systems, many of the histograms proposed in earlier works are not always effective or practical. For example, equi-depth histograms <ref> [Koo80, MD88, PSC84] </ref> work well for range queries only when the data distribution has low skew, while V-Optimal (F,F) histograms [IC93, Ioa93, IP95a] have only been proven optimal for equality joins and selections when a list of all the attribute values in each bucket is maintained.
Reference: [KK69] <author> H. Ku and S. Kullback. </author> <title> Approximating discrete probability distributions. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-15:444-447, </volume> <year> 1969. </year>
Reference-contexts: Such models have been discussed and used since the 1940s or earlier, but especially since the 1970s, when computer algorithms to fit them became widely available. Many text-book treatments of log-linear modeling are available, for example [Agr90] and [BFH75]. Sample references from the Computer Science literature are <ref> [KK69] </ref>, [Pea88], and [Mal91]. Log-linear models use only categorical variables continuous variables must be discretized first, and even then the modeling will not make use of the ordinal nature of the categories.
Reference: [KR90] <author> L. Kaufman and P.J. Rousseeuw. </author> <title> (1990) Finding Groups in Data: an Introduction to Cluster Analysis, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: The fundamental question here is: how many natural clusters there are in the given dataset. The answer to this question are typically highly subjective and remains an open issue in cluster analysis <ref> [KR90] </ref>. Existing clustering algorithms deal with this issue in one of two ways. 24 7.1 Overview of Existing Algorithms 7.1.1 Statistical Methods The first way is to avoid answering the question entirely by giving a complete clustering of the dataset. <p> In each step, it picks a cluster to split into two. This process continues until it produces n clusters. While hierarchical methods have been successfully applied to many biological applications (e.g., for producing taxonomies of animals and plants, and classification of diseases <ref> [KR90] </ref>), they are well known to suffer from the weakness that they can never undo what was done previously. Once an agglomerative method merges two objects, these objects are always in one cluster. And once a divisive method separates two objects, these objects are never re-grouped into the same cluster. <p> Furthermore, because the data may be mainly disk-resident, there is also the emphasis of minimizing I/O cost. Based on randomized search, CLARANS can be viewed as an extension to the k-medoids algorithm <ref> [NH94, KR90] </ref>. It is highly tunable, depending on how much CPU time the user can afford. Focusing techniques based on spatial access methods (e.g., R* trees, Voronoi diagrams) are later developed to reduce the I/O cost required by CLARANS [EKX95]. <p> As such, it does a better job finding elongated clusters than most of the algorithms mentioned above. It uses an R* tree to achieve good performance. Finally, STING is 4 There are a few methods that tolerate a limited degree of overlap between clusters. See <ref> [KR90] </ref> for more details. 25 a hierarchical cell structure that stores statistical information (e.g., density) about the objects in the cells [WYM97].
Reference: [Kuk93] <author> A. Y. C. Kuk. </author> <title> A kernel method for estimating finite population distribution functions using auxilliary information. </title> <journal> Biometrika, </journal> <volume> 80 </volume> <pages> 385-392, </pages> <year> 1993. </year>
Reference-contexts: Regression techniques [SSW92, Part II] can provide an effective means of combining information in the sample with other available information; see also <ref> [RKM90, Kuk93] </ref>. 9.2.3 Skewed Data Data that is skewed in frequency but not in value does not cause problems when a sample is used to estimate summary statistics that are sums, averages, or smooth functions of sums and averages.
Reference: [LJF94] <author> King-Ip Lin, H. V. Jagadish, and Christos Faloutsos. </author> <title> The TV-tree: An Index Structure for High-Dimensional Data. </title> <journal> VLDB Journal 3(4) </journal> <pages> 517-542, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: The most common example is the R-tree [Gut84] and its variants: the R*-tree [BKSS90] and R+-tree [SRF87]. Other multidimensional search trees include quad-trees [FB74], k-D-B-trees [Rob81], hB-trees [LS90], and TV-trees <ref> [LJF94] </ref>. Multidimensional data can also be transformed into unidimensional data using a space-filling curve [Jag90]; after transformation, a 27 B+-tree can be used to index the resulting unidimensional data. <p> deeper development of the tree in locally dense regions of the space, or by placing split boundaries in the dense areas to retain some balance in the populations associated with various tree nodes. 8.7.4 High-Dimensional Data Some index trees have been developed explicitly to address high dimensional problems (e.g., TV-trees <ref> [LJF94] </ref> and X-trees [BKK96]). The efficacy of these structures remains in doubt, especially in light of recent results on the hardness of indexing high-dimensional space [HKP97].
Reference: [LNS90] <author> R. J. Lipton, J. F. Naughton, and D. A. Schneider. </author> <title> Practical selectivity estimation through adaptive sampling. </title> <booktitle> In Proc. 1990 ACM SIGMOD Intl. Conf. Managment of Data, </booktitle> <pages> pages 1-11. </pages> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, <ref> [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90] </ref>. Several authors have outlined complete sampling-based approaches to query optimization [Ant93a, SBM93, Wil91]. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm.
Reference: [LNSS93] <author> R. J. Lipton, J. F. Naughton, D. A. Schneider, and S. Seshadri. </author> <title> Efficient sampling strategies for relational database operations. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 116 </volume> <pages> 195-226, </pages> <year> 1993. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, <ref> [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90] </ref>. Several authors have outlined complete sampling-based approaches to query optimization [Ant93a, SBM93, Wil91]. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm.
Reference: [LRS93] <author> J. Li, D. Rotem, and J. Srivastava. </author> <title> Algorithms for loading parallel grid files. </title> <booktitle> In Proc. 1993 ACM SIGMOD Intl. Conf. Managment of Data, </booktitle> <pages> pages 347-356. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Sampling can be used to estimate the distribution of attribute values and hence obtain good assignment rules. The parallel join-algorithms in [DNSS92] and the algorithms for efficient loading of parallel grid files in <ref> [LRS93] </ref>, for example, use sampling in this manner. * Support for auditing Various types of auditing require retrieval of a random sample of the records in a database or, in relational DBMS's, a random sample from the tuples in the output relation of a query.
Reference: [LS90] <author> D. B. Lomet and B. Salzberg. </author> <title> The hB-Tree: A Multiattribute Indexing Method. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 15(4) </volume> <pages> 625-58, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The most common example is the R-tree [Gut84] and its variants: the R*-tree [BKSS90] and R+-tree [SRF87]. Other multidimensional search trees include quad-trees [FB74], k-D-B-trees [Rob81], hB-trees <ref> [LS90] </ref>, and TV-trees [LJF94]. Multidimensional data can also be transformed into unidimensional data using a space-filling curve [Jag90]; after transformation, a 27 B+-tree can be used to index the resulting unidimensional data.
Reference: [LSS97] <author> M. Luo, S. L. Stokes, and T. W. Sager. </author> <title> Estimation of the CDF of a finite population using a calibration sample. Environ. </title> <journal> Ecol. Statist., </journal> <note> 1997. To appear. </note>
Reference-contexts: In a similar vein, Luo et al. <ref> [LSS97] </ref> provide estimation methods that require accurate evaluations of f on a small subset of the sample and cheap, inaccurate measures of f on the remainder of the sample. 10 Conclusions Database technology, as a field, may have matured in contexts such as banking and payroll, where providing complete accuracy and
Reference: [Mal89] <author> F. Malvestuto. </author> <title> Computing the maximum-entropy extension of discrete probability distributions. </title> <journal> Comput. Statist. Data Anal., </journal> <volume> 8 </volume> <pages> 299-311, </pages> <year> 1989. </year>
Reference-contexts: In addition, the computed approximation will fit the input marginal distributions exactly. Another application of the methodology occurs when only certain marginal tables are available, and it is required to extend the probability distribution to the complete array, as in <ref> [Mal89] </ref>. The estimation of the parameter arrays can sometimes, for certain assumptions of factor combinations called decomposable models or graphical models, be quite simple, involving just simple arithmetic products and ratios of the given marginal probabilities.
Reference: [Mal91] <author> F. Malvestuto. </author> <title> Approximating Discrete Probability Distributions with Decomposable Models. </title> <journal> Trans. Systems, Man, Cybernetics, </journal> <volume> 21(5) </volume> <pages> 1287-1294, </pages> <year> 1991. </year>
Reference-contexts: Many text-book treatments of log-linear modeling are available, for example [Agr90] and [BFH75]. Sample references from the Computer Science literature are [KK69], [Pea88], and <ref> [Mal91] </ref>. Log-linear models use only categorical variables continuous variables must be discretized first, and even then the modeling will not make use of the ordinal nature of the categories. <p> Even if the data base represents millions of entities, the vast majority of the cells will have zero count. In such situations, there is obviously a great advantage to choosing a decomposable model. Among others <ref> [Mal91] </ref> discusses how to search the set of decomposable models for a good fitting model. As in all such model choice problems, one must consider the usual tradeoff between parsimony and variance reduction on the one hand, and adequacy of representation on the other.
Reference: [MCS88] <author> M. V. Mannino, P. Chu, and T. Sager. </author> <title> Statistical profile estimation in database systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 20(3) </volume> <pages> 192-221, </pages> <month> Sept </month> <year> 1988. </year>
Reference-contexts: Incremental maintenance techniques for histograms and samples have also been investigated [GMP97], as has the use of histograms in parallel-join load balancing [PI96]. Finally, there are several sources where one may find extensive discussions of histogram-based estimation techniques <ref> [Koo80, IP95b, MCS88, Poo97] </ref>. In the following sections, the effectiveness of histograms in approximating different kinds of data is studied. 6.2 Distance-Only Data The current histogram techniques cannot approximate such data, because they rely on information about the placement of data in a multi-dimensional space.
Reference: [MD88] <author> M. Muralikrishna and David J Dewitt. </author> <title> Equi-depth histograms for estimating selectivity factors for multi-dimensional queries. </title> <booktitle> Proc. of ACM SIGMOD Conf, </booktitle> <pages> pages 28-36, </pages> <year> 1988. </year> <month> 43 </month>
Reference-contexts: Most of the work on histograms is in the context of evaluating their accuracy in estimating the result sizes of queries containing selections [Koo80, PSC84] and joins [IC93, Ioa93, IP95a]. Multi-dimensional histograms have also been studied in detail <ref> [MD88, PI97] </ref>. By building histograms on multiple attributes together, these techniques are able to capture dependencies between those attributes. Incremental maintenance techniques for histograms and samples have also been investigated [GMP97], as has the use of histograms in parallel-join load balancing [PI96]. <p> The same applies to approximating multi-dimensional data within a single attribute as well (e.g., polygons). 6.4 Aspects of histogram usage * Accuracy: Although histograms are used in many systems, many of the histograms proposed in earlier works are not always effective or practical. For example, equi-depth histograms <ref> [Koo80, MD88, PSC84] </ref> work well for range queries only when the data distribution has low skew, while V-Optimal (F,F) histograms [IC93, Ioa93, IP95a] have only been proven optimal for equality joins and selections when a list of all the attribute values in each bucket is maintained.
Reference: [NH94] <author> R. Ng and J. Han. </author> <title> (1994) Efficient and Effective Clustering Method for Spatial Data Mining, </title> <booktitle> Proc. 1994 VLDB, </booktitle> <pages> pp. 144-155. </pages>
Reference-contexts: Furthermore, because the data may be mainly disk-resident, there is also the emphasis of minimizing I/O cost. Based on randomized search, CLARANS can be viewed as an extension to the k-medoids algorithm <ref> [NH94, KR90] </ref>. It is highly tunable, depending on how much CPU time the user can afford. Focusing techniques based on spatial access methods (e.g., R* trees, Voronoi diagrams) are later developed to reduce the I/O cost required by CLARANS [EKX95].
Reference: [NS90] <author> J. F. Naughton and S. Seshadri. </author> <title> On estimating the size of projections. </title> <booktitle> In Proc. Third Intl. Conf. Database Theory, </booktitle> <pages> pages 499-513. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, <ref> [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90] </ref>. Several authors have outlined complete sampling-based approaches to query optimization [Ant93a, SBM93, Wil91]. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm.
Reference: [ODT+91] <author> G. Ozsoyoglu, K. Du, A. Tjahjana, W. Hou, and D. Y. Rowland. </author> <title> On estimating COUNT, SUM, and AVERAGE relational algebra queries. </title> <editor> In D. Dimitris Karagiannis, editor, </editor> <booktitle> Database and Expert Systems Applications, Proceedings of the International Conference in Berlin, Germany, 1991 (DEXA 91), </booktitle> <pages> pages 406-412. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Sampling provides a means of obtaining quick, approximate answers to a variety of aggregation queries. Sampling techniques for aggregation queries in object-relational DBMS's have been studied in <ref> [HOD91, HOT88, HOT89, ODT+91] </ref>. These techniques also have been studied in the context of online-aggregation systems [Haa96, Haa97, HHW97].
Reference: [Olk93] <author> F. Olken. </author> <title> Random Sampling from Databases. </title> <type> Ph.D. Dissertation, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <year> 1993. </year> <note> Available as Tech. Report LBL-32883, </note> <institution> Lawrence Berkeley Laboratories, Berkeley, </institution> <address> CA. </address>
Reference-contexts: Examples of auditing applications include financial records auditing, fissile materials auditing, statistical quality control, and epidemiological studies. Other applications, such as market research and secure statistical DBMS's also require retrieval of random record sets; see <ref> [Olk93, Section 1.6] </ref> for further examples and references. <p> Examples of auditing applications include financial records auditing, fissile materials auditing, statistical quality control, and epidemiological studies. Other applications, such as market research and secure statistical DBMS's also require retrieval of random record sets; see [Olk93, Section 1.6] for further examples and references. Olken <ref> [Olk93, Section 1.5.2] </ref> has made the case that the most efficient approach to obtaining a random sample of records is to incorporate sampling into the DBMS, thereby avoiding both unnecessary record fetches and the overhead of passing data across the application/DBMS interface. <p> If a SRS of records (rather than pages) is required, then extent-map sampling can be combined with an acceptance/rejection (A/R) technique as described, for example, in <ref> [Olk93] </ref>. The idea behind A/R sampling is to accept a sampled page with a probability equal to the number of records on the page divided by the maximum number of records on a page; otherwise the page is rejected. <p> One method is to materialize a SRS of the tuples in the output relation (using A/R techniques as described above and in <ref> [Olk93] </ref>) and then compute the estimate of the summary statistic.
Reference: [OR86] <author> F. Olken and D. Rotem. </author> <title> Simple random sampling from relational databases. </title> <booktitle> In Proc. 12th Intl. Conf. Very Large Data Bases, </booktitle> <pages> pages 160-169, </pages> <year> 1986. </year>
Reference-contexts: Techniques for obtaining simple random samples (SRS's) from databases are developed in <ref> [Ant92, OR86, OR89, OR93, ORX90] </ref>. 32 * Approximate answers to aggregation queries The answer to an aggregation query consists of a small set of summary statistics, such as COUNT, AVERAGE, or MAXIMUM, that is computed from a specified set of records.
Reference: [OR89] <author> F. Olken and D. Rotem. </author> <title> Random sampling from B + trees. </title> <booktitle> In Proc. 15th Intl. Conf. Very Large Data Bases, </booktitle> <pages> pages 269-277, </pages> <year> 1989. </year>
Reference-contexts: Techniques for obtaining simple random samples (SRS's) from databases are developed in <ref> [Ant92, OR86, OR89, OR93, ORX90] </ref>. 32 * Approximate answers to aggregation queries The answer to an aggregation query consists of a small set of summary statistics, such as COUNT, AVERAGE, or MAXIMUM, that is computed from a specified set of records. <p> For example, if records are stored in a B + -tree or a hash file, then SRS's can be obtained using the techniques described in <ref> [Ant92, OR89] </ref> or [ORX90], respectively. In many file systems, pages of records are stored in contiguous blocks called extents, and a main memory data structure called an extent map provides access to the extents and the pages within the extents. <p> For example, data values having a linear ordering can be stored in a B + tree or a ranked B + tree, so that SRS's can be obtained using the methods in <ref> [Ant92, OR89] </ref>. 9.2.2 Sparse Data Depending on how the data is stored, sparseness of data may or may not have a detrimental effect on sample-based estimates.
Reference: [OR92] <author> F. Olken and D. Rotem. </author> <title> Maintenance of materialized views of sampling queries. </title> <booktitle> In Proc. Eighth Intl. Conf. Data Engrg., </booktitle> <pages> pages 632-641. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: pass through the data. (Sometimes, as in the case of histograms [GMP97, PIHS96], sampling can be combined with another data reduction technique, yielding an approximate reduction of the data that is relatively cheap to obtain.) It is also relatively inexpensive to update a sample as the underlying data changes; see <ref> [GMP97, OR92] </ref> for some updating methods. The adequacy of sampling as a data-reduction technique depends crucially on how the sample is to be used. We focus on perhaps the most common use of a sample: estimation of the answer to an aggregation query.
Reference: [OR93] <author> F. Olken and D. Rotem. </author> <title> Sampling from spatial datatbases. </title> <booktitle> In Proc. Ninth Intl. Conf. Data Engrg., </booktitle> <pages> pages 199-208. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Techniques for obtaining simple random samples (SRS's) from databases are developed in <ref> [Ant92, OR86, OR89, OR93, ORX90] </ref>. 32 * Approximate answers to aggregation queries The answer to an aggregation query consists of a small set of summary statistics, such as COUNT, AVERAGE, or MAXIMUM, that is computed from a specified set of records.
Reference: [ORX90] <author> F. Olken, D. Rotem, and P. Xu. </author> <title> Random sampling from hash files. </title> <booktitle> In Proc. 1990 ACM SIGMOD Intl. Conf. Managment of Data, </booktitle> <pages> pages 375-386. </pages> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: Techniques for obtaining simple random samples (SRS's) from databases are developed in <ref> [Ant92, OR86, OR89, OR93, ORX90] </ref>. 32 * Approximate answers to aggregation queries The answer to an aggregation query consists of a small set of summary statistics, such as COUNT, AVERAGE, or MAXIMUM, that is computed from a specified set of records. <p> For example, if records are stored in a B + -tree or a hash file, then SRS's can be obtained using the techniques described in [Ant92, OR89] or <ref> [ORX90] </ref>, respectively. In many file systems, pages of records are stored in contiguous blocks called extents, and a main memory data structure called an extent map provides access to the extents and the pages within the extents.
Reference: [Pea88] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kauffman, </publisher> <address> Palo Alto, </address> <year> 1988. </year>
Reference-contexts: Such models have been discussed and used since the 1940s or earlier, but especially since the 1970s, when computer algorithms to fit them became widely available. Many text-book treatments of log-linear modeling are available, for example [Agr90] and [BFH75]. Sample references from the Computer Science literature are [KK69], <ref> [Pea88] </ref>, and [Mal91]. Log-linear models use only categorical variables continuous variables must be discretized first, and even then the modeling will not make use of the ordinal nature of the categories.
Reference: [PI96] <author> Viswanath Poosala and Yannis Ioannidis. </author> <title> Estimation of query-result distribution and its application in parallel-join load balancing. </title> <booktitle> Proc. of the 22nd Int. Conf. on Very Large Databases, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: They are used mainly for selectivity estimation purposes within a query optimizer. They have also been used in query execution (e.g., for parallel-join load balancing <ref> [PI96] </ref>) and there is work in progress on using them for approximate query answering. 6.1 Definitions In what follows, histograms are defined in the context of a single attribute. The extensions to multiple attributes can be found elsewhere [PI97]. <p> Multi-dimensional histograms have also been studied in detail [MD88, PI97]. By building histograms on multiple attributes together, these techniques are able to capture dependencies between those attributes. Incremental maintenance techniques for histograms and samples have also been investigated [GMP97], as has the use of histograms in parallel-join load balancing <ref> [PI96] </ref>. Finally, there are several sources where one may find extensive discussions of histogram-based estimation techniques [Koo80, IP95b, MCS88, Poo97].
Reference: [PI97] <author> Viswanath Poosala and Yannis Ioannidis. </author> <title> Selectivity estimation without the attribute value independence assumption. </title> <booktitle> Proc. of the 23rd Int. Conf. on Very Large Databases, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: The extensions to multiple attributes can be found elsewhere <ref> [PI97] </ref>. The domain D of an attribute X in relation R is the set of all possible values of X and the (finite) value set V ( D) is the set of values of X that are actually present in R. <p> Most of the work on histograms is in the context of evaluating their accuracy in estimating the result sizes of queries containing selections [Koo80, PSC84] and joins [IC93, Ioa93, IP95a]. Multi-dimensional histograms have also been studied in detail <ref> [MD88, PI97] </ref>. By building histograms on multiple attributes together, these techniques are able to capture dependencies between those attributes. Incremental maintenance techniques for histograms and samples have also been investigated [GMP97], as has the use of histograms in parallel-join load balancing [PI96].
Reference: [PIHS96] <author> V. Poosala, Y. E. Ioannidis, P. J. Haas, and E. J. Shekita. </author> <title> Improved histograms for selectivity estimation of range predicates. </title> <booktitle> In Proc. 1996 ACM SIGMOD Intl. Conf. Managment of Data, </booktitle> <pages> pages 294-305. </pages> <publisher> ACM Press, </publisher> <year> 1996. </year>
Reference-contexts: Theis difference lies in the exact choice of bucket boundaries chosen. In an equi-width histogram, the widths of all buckets' ranges are the same; in an equi-depth (or equi-height) histogram, the total number of tuples having the attribute values associated with each bucket is the same. In <ref> [PIHS96] </ref>, a set of key properties that characterize histograms have been identified, forming the basis for a taxonomy of histograms. <p> We refer to a histogram with c, u, and s as the partition constraint, source parameter, and sort parameter as the c (s; u) histogram. Figure 4 provides an overview of the new combinations that were introduced in <ref> [PIHS96] </ref> together with the traditional combinations. Efficient sampling-based techniques exist for computing all classes of histograms and are given in [PIHS96]. <p> Figure 4 provides an overview of the new combinations that were introduced in <ref> [PIHS96] </ref> together with the traditional combinations. Efficient sampling-based techniques exist for computing all classes of histograms and are given in [PIHS96]. Most of the work on histograms is in the context of evaluating their accuracy in estimating the result sizes of queries containing selections [Koo80, PSC84] and joins [IC93, Ioa93, IP95a]. Multi-dimensional histograms have also been studied in detail [MD88, PI97]. <p> into buckets (e.g., bucket per each week), in which case the values inside a bucket (often) need not be stored (e.g., days of the week) and the above problem disappears. 6.3.1 Sparse Data Histograms have been shown in earlier work to be highly effective in approximating sparse and dense data <ref> [PIHS96] </ref>. <p> When enhanced with a usual histogram on the remaining data, the combined set of statistics has been shown to be highly accurate. These combined statistics are in fact also used in DB2 and are known as Compressed histograms <ref> [PIHS96] </ref>. In the following sections, the effectiveness of using singleton buckets is discussed. <p> V2 and Oracle 7 SQL Server to estimate a variety of catalog statistics from samples of the base relations, and there is ongoing research into sampling-based methods for estimating such key catalog statistics as quantiles and "column cardinality" (the number of distinct values of an attribute in a relation); see <ref> [GMP97, HNSS95, PIHS96] </ref>. Sampling is much less expensive than exact computation of catalog statistics from entire relations; such cost reduction is important since catalog statistics must be recomputed periodically as the database changes over time. <p> many sampling algorithms, the cost of obtaining a sample is proportional to the size of the sample, and not the size of the database; this is in contrast to other data reduction techniques that require at least one complete pass through the data. (Sometimes, as in the case of histograms <ref> [GMP97, PIHS96] </ref>, sampling can be combined with another data reduction technique, yielding an approximate reduction of the data that is relatively cheap to obtain.) It is also relatively inexpensive to update a sample as the underlying data changes; see [GMP97, OR92] for some updating methods.
Reference: [Poo97] <author> Viswanath Poosala. </author> <title> Histogram-based estimation techniques in databases. </title> <type> PhD thesis, </type> <institution> Univ. of Wisconsin-Madison, </institution> <year> 1997. </year>
Reference-contexts: Incremental maintenance techniques for histograms and samples have also been investigated [GMP97], as has the use of histograms in parallel-join load balancing [PI96]. Finally, there are several sources where one may find extensive discussions of histogram-based estimation techniques <ref> [Koo80, IP95b, MCS88, Poo97] </ref>. In the following sections, the effectiveness of histograms in approximating different kinds of data is studied. 6.2 Distance-Only Data The current histogram techniques cannot approximate such data, because they rely on information about the placement of data in a multi-dimensional space. <p> Earlier work has shown that the most accurate and practical histograms belong to the V-Optimal (V,A) and MaxDiff (V,A) classes <ref> [Poo97] </ref>. Briefly, these histograms group contiguous ranges of values into buckets and avoid grouping attribute values with highly different areas. These histograms have been shown to be highly accurate for both join and selection queries [Poo97]. * Progressive resolution refinement: A histogram on flat (non-hierarchical) data can not be used to <p> the most accurate and practical histograms belong to the V-Optimal (V,A) and MaxDiff (V,A) classes <ref> [Poo97] </ref>. Briefly, these histograms group contiguous ranges of values into buckets and avoid grouping attribute values with highly different areas. These histograms have been shown to be highly accurate for both join and selection queries [Poo97]. * Progressive resolution refinement: A histogram on flat (non-hierarchical) data can not be used to provide different levels of resolution.
Reference: [PSC84] <author> Gregory Piatetsky-Shapiro and Charles Connell. </author> <title> Accurate estimation of the number of tuples satisfying a condition. </title> <booktitle> Proc. of ACM SIGMOD Conf, </booktitle> <pages> pages 256-276, </pages> <year> 1984. </year>
Reference-contexts: Efficient sampling-based techniques exist for computing all classes of histograms and are given in [PIHS96]. Most of the work on histograms is in the context of evaluating their accuracy in estimating the result sizes of queries containing selections <ref> [Koo80, PSC84] </ref> and joins [IC93, Ioa93, IP95a]. Multi-dimensional histograms have also been studied in detail [MD88, PI97]. By building histograms on multiple attributes together, these techniques are able to capture dependencies between those attributes. <p> The same applies to approximating multi-dimensional data within a single attribute as well (e.g., polygons). 6.4 Aspects of histogram usage * Accuracy: Although histograms are used in many systems, many of the histograms proposed in earlier works are not always effective or practical. For example, equi-depth histograms <ref> [Koo80, MD88, PSC84] </ref> work well for range queries only when the data distribution has low skew, while V-Optimal (F,F) histograms [IC93, Ioa93, IP95a] have only been proven optimal for equality joins and selections when a list of all the attribute values in each bucket is maintained.
Reference: [PTVF96] <author> W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. </author> <title> Numerical Recipes in C, </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. See [Str80] or <ref> [PTVF96] </ref> for more details. The latter citation also gives `C' code. Example 1: Table 1 provides an example of the kind of matrix that is typical in warehousing applications, where rows are customers, columns are days, and the values are the dollar amounts spent on phone calls each day. <p> Proof: See <ref> [PTVF96, p. 59] </ref>. 2 Recall that a matrix U is called column-orthonormal if its columns u i are mutually orthogonal unit vectors. Equivalently: U t fi U = I, where I is the identity matrix. <p> We focus first on 1-dimensional signals; the DWT can be applied to signals of any dimensionality, by applying it first on the first dimension, then the second, etc. <ref> [PTVF96] </ref>. Contrary to the DFT, there are more than one Wavelet transforms. The simplest to describe and code is the Haar transform. <p> There are numerous wavelet transforms <ref> [PTVF96] </ref>, some popular ones being the so-called Daubechies-4 and Daubechies-6 transforms [Dau92]. 3.1.1 Discussion The computational complexity of the above transforms is O (n), as can be verified from Eq. 5-7. <p> Similarly, the human ear seems to use a wavelet transform to analyze a sound, at least in the very first stage [Dau92, p. 6] [WS93]. In conclusion, the Discrete Wavelet Transform (DWT) achieves even better energy concentration than the DFT and Discrete Cosine (DCT) transforms, for natural signals <ref> [PTVF96, p. 604] </ref>. <p> In fact, it has been very successful in image compression <ref> [PTVF96] </ref>, where a grayscale image is treated as a 2-d matrix. 3.3.1 Ordered and Unordered Attributes DWT will give good results for ordered attributes, when successive values tend to be correlated (which is typically the case in real datasets). <p> The general procedure to do least square fitting for Multiple Regression can be found in <ref> [PTVF96] </ref>. It is also possible to use nonlinear functions to perform data regression. Equation 13 shows an example of a nonlinear regression between variables Y and X.
Reference: [RKM90] <author> J. N. K. Rao, J. G. Kovar, and H. J. Mantel. </author> <title> On estimating distribution functions and quantiles from survey data using auxilliary information. </title> <journal> Biometrika, </journal> <volume> 77 </volume> <pages> 365-375, </pages> <year> 1990. </year>
Reference-contexts: Regression techniques [SSW92, Part II] can provide an effective means of combining information in the sample with other available information; see also <ref> [RKM90, Kuk93] </ref>. 9.2.3 Skewed Data Data that is skewed in frequency but not in value does not cause problems when a sample is used to estimate summary statistics that are sums, averages, or smooth functions of sums and averages.
Reference: [Rob81] <author> J.T. Robinson. </author> <title> The K-D-B-Tree: A Search Structure for Large Multidimensional Dynamic Indexes. </title> <booktitle> Proceedings ACM SIGMOD, </booktitle> <pages> pages 10-18, </pages> <year> 1981. </year>
Reference-contexts: The most common example is the R-tree [Gut84] and its variants: the R*-tree [BKSS90] and R+-tree [SRF87]. Other multidimensional search trees include quad-trees [FB74], k-D-B-trees <ref> [Rob81] </ref>, hB-trees [LS90], and TV-trees [LJF94]. Multidimensional data can also be transformed into unidimensional data using a space-filling curve [Jag90]; after transformation, a 27 B+-tree can be used to index the resulting unidimensional data.
Reference: [SBM93] <author> K. D. Seppi, J. W. Barnes, and C. N. Morris. </author> <title> A Bayesian approach to database query optimization. </title> <journal> ORSA J. Comput., </journal> <volume> 5 </volume> <pages> 410-419, </pages> <year> 1993. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90]. Several authors have outlined complete sampling-based approaches to query optimization <ref> [Ant93a, SBM93, Wil91] </ref>. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm. Typically, records are assigned to processors based on the attribute values of the records.
Reference: [Sch81] <author> M. Scholl. </author> <title> New File Organizations Based on Dynamic Hashing. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(1) </volume> <pages> 194-211, </pages> <month> March </month> <year> 1981. </year> <month> 44 </month>
Reference: [SN92] <author> S. Seshadri and J. F. Naughton. </author> <title> Sampling issues in parallel database systems. </title> <booktitle> In Advances in Database Technology- EDBT '92, 3rd Intl. Conf. Extending Database Technology, Lecture Notes in Computer Science, </booktitle> <pages> pages 328-343. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: The selected records then form a stratified sample. In a "shared-nothing" parallel DBMS, for example, a stratum might correspond to the records stored at a specified processing node; see <ref> [SN92] </ref> for a discussion of why, in parallel DBMS's, stratified sampling usually is preferable to simple random sampling. Other types of samples abound [Coc77, DC72, SSW92, Sud76]; we focus on simple, cluster, and stratified samples since these are the most common types of reduced data sets found in DBMS's.
Reference: [SRF87] <author> T. Sellis, N. Roussopoulos, and C. Faloutsos. </author> <title> The R+-Tree: A Dynamic Index For Multi-Dimensional Objects. </title> <booktitle> In Proc. 13th International Conference on Very Large Data Bases, </booktitle> <pages> pages 507-518, </pages> <address> Brighton, </address> <month> September </month> <year> 1987. </year>
Reference-contexts: The most common example is the R-tree [Gut84] and its variants: the R*-tree [BKSS90] and R+-tree <ref> [SRF87] </ref>. Other multidimensional search trees include quad-trees [FB74], k-D-B-trees [Rob81], hB-trees [LS90], and TV-trees [LJF94]. Multidimensional data can also be transformed into unidimensional data using a space-filling curve [Jag90]; after transformation, a 27 B+-tree can be used to index the resulting unidimensional data.
Reference: [SS88] <author> S. L. Stokes and T. W. Sager. </author> <title> Characterization of a ranked-set sample with application to estimating distribution functions. </title> <journal> J. Amer. Statist. Assoc., </journal> <volume> 83 </volume> <pages> 374-381, </pages> <year> 1988. </year>
Reference-contexts: rank a small set of records in approximate order of increasing value of f without actually evaluating f itself, then ranked set sampling techniques can be used to estimate averages, quantiles, and other summary statistics using many fewer function evaluations than are required by simple random sampling; see, for example, <ref> [DC72, SS88] </ref> and references therein.
Reference: [SSW92] <author> C.-E. Sarndal, B. Swensson, and J. Wretman. </author> <title> Model Assisted Survey Sampling. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: of lower, more manageable dimensionality. 31 9 Sampling The notion that a large set of data can be represented by a small random sample of the data elements goes back to the end of the nineteenth century and has led to the development of a large body of survey-sampling techniques <ref> [Coc77, SSW92, Sud76] </ref>. Over the past fifteen years, there has been increasing interest in the application of sampling ideas to database management systems (DBMS's). <p> In a "shared-nothing" parallel DBMS, for example, a stratum might correspond to the records stored at a specified processing node; see [SN92] for a discussion of why, in parallel DBMS's, stratified sampling usually is preferable to simple random sampling. Other types of samples abound <ref> [Coc77, DC72, SSW92, Sud76] </ref>; we focus on simple, cluster, and stratified samples since these are the most common types of reduced data sets found in DBMS's. Sampling is well-suited to the progressive refinement of a reduced data set: to "refine" the data set further, simply take more samples. <p> Suppose that the inclusion probability i for record r i is known a priori for each i. Then it is not hard to show that the estimator b n (f ) = r i 2S i is unbiased for (f ), provided that each i is positive. See <ref> [SSW92] </ref> for a comprehensive discussion of such "Horvitz-Thompson" estimators and their associated confidence intervals. * Estimation methods also are available when the summary statistic of interest is computed from the tuples in the output relation formed by executing a relational query over a set of base relations. <p> As another example, Haas and Swami [HS95] describe a method for estimating the selectivity of a join in which the sample is augmented with frequency counts for certain join-attribute values that are frequent in some relations and infrequent in other relations. Regression techniques <ref> [SSW92, Part II] </ref> can provide an effective means of combining information in the sample with other available information; see also [RKM90, Kuk93]. 9.2.3 Skewed Data Data that is skewed in frequency but not in value does not cause problems when a sample is used to estimate summary statistics that are sums,
Reference: [Str80] <author> Gilbert Strang. </author> <title> Linear Algebra and its Applications. </title> <publisher> Academic Press, </publisher> <year> 1980. </year> <note> 2nd edition. </note>
Reference-contexts: SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. See <ref> [Str80] </ref> or [PTVF96] for more details. The latter citation also gives `C' code.
Reference: [Sud76] <author> S. Sudman. </author> <title> Applied Sampling. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: of lower, more manageable dimensionality. 31 9 Sampling The notion that a large set of data can be represented by a small random sample of the data elements goes back to the end of the nineteenth century and has led to the development of a large body of survey-sampling techniques <ref> [Coc77, SSW92, Sud76] </ref>. Over the past fifteen years, there has been increasing interest in the application of sampling ideas to database management systems (DBMS's). <p> In a "shared-nothing" parallel DBMS, for example, a stratum might correspond to the records stored at a specified processing node; see [SN92] for a discussion of why, in parallel DBMS's, stratified sampling usually is preferable to simple random sampling. Other types of samples abound <ref> [Coc77, DC72, SSW92, Sud76] </ref>; we focus on simple, cluster, and stratified samples since these are the most common types of reduced data sets found in DBMS's. Sampling is well-suited to the progressive refinement of a reduced data set: to "refine" the data set further, simply take more samples.
Reference: [TP91] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: and powerful operation, and it has been used in numerous applications, such as statistical analysis (as the driving engine behind the Principal Component Analysis [Jol86]), 6 text retrieval under the name of Latent Semantic Indexing [Dum94], pattern recognition and dimen-sionality reduction as the Karhunen-Loeve (KL) transform [DH73], and face recognition <ref> [TP91] </ref>. SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. See [Str80] or [PTVF96] for more details. The latter citation also gives `C' code.
Reference: [VM] <author> Brani Vidakovic and Peter Mueller. </author> <title> Wavelets for Kids. </title> <institution> Duke University, Durham, NC. ftp://ftp.isds.duke.edu/pub/Users/brani/papers/. </institution>
Reference-contexts: Following the literature, the appropriate value for the constant C is 1= p 2, because it makes the transformation matrix orthonormal (eg., see Eq. 8). An orthonormal matrix is a matrix which has columns that are unit vectors and that are mutually orthogonal. Adapting the notation (eg., from [Cra94] <ref> [VM] </ref>), the Haar transform is defined as follows: d l;i = 1= 2 (s l1;2i s l1;2i+1 ) l = 0; : : : ; L; i = 0; : : : ; n=2 l+1 1 (5) with s l;i = 1= 2 (s l1;2i + s l1;2i+1 ) l =
Reference: [Wil91] <author> D. E. Willard. </author> <title> Optimal sample cost residues for differential database batch query problems. </title> <journal> J. ACM, </journal> <volume> 38 </volume> <pages> 104-119, </pages> <year> 1991. </year>
Reference-contexts: In an effort to avoid these problems, a number of researchers have considered approaches in which selectivities and costs are estimated directly from a sample; see, for example, [GGMS96, HNSS96, HS92, HS95, HOD91, LNS90, LNSS93, NS90]. Several authors have outlined complete sampling-based approaches to query optimization <ref> [Ant93a, SBM93, Wil91] </ref>. * Parallel processing of queries Balancing the workload between processors is a critical objective of any parallel query-processing algorithm. Typically, records are assigned to processors based on the attribute values of the records.
Reference: [WS93] <author> Kuansan Wang and Shihab Shamma. </author> <title> Spectral shape analysis in the central auditory system. </title> <address> NNSP, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Naturally occurring scenes tend to excite only few of the neurons, implying that a wavelet transform will achieve excellent compression for such images. Similarly, the human ear seems to use a wavelet transform to analyze a sound, at least in the very first stage [Dau92, p. 6] <ref> [WS93] </ref>. In conclusion, the Discrete Wavelet Transform (DWT) achieves even better energy concentration than the DFT and Discrete Cosine (DCT) transforms, for natural signals [PTVF96, p. 604].
Reference: [WW85] <author> R.J. Wonnacott and T.H. Wonnacott. </author> <title> Introductory Statistics. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: However, most of the competitors will run into similar problems, too (and, probably, sooner than DWT). 4 Regression Regression is a popular technique that attempts to model data as a function of the values of a multidimensional vector. The simplest form of regression is that of Linear Regression <ref> [WW85] </ref>, in which a variable Y is modeled as a linear function of another variable X, using Equation 9. 12 Y = ff + fiX (9) The parameters ff and fi specify the line and are to be estimated by using the data at hand. <p> One way of getting better accuracy progressively is by reducing the influence that outliers have in the model by giving them less weight in the least square regression. This method is known as biweight regression or robust regression; an example of this is the use of weighted least squares <ref> [WW85] </ref>. The first thing to do is determine whether a data value is an outlier.
Reference: [WYM97] <author> Wei Wang, Jiong Yang, and R. Muntz. STING: </author> <title> A Statistical Information Grid Approach to Spatial Data Mining . Proc. </title> <booktitle> 23rd VLDB, </booktitle> <pages> pages 186-195, </pages> <month> August </month> <year> 1997. </year> <title> Athens, </title> <address> Greece. </address>
Reference-contexts: It uses an R* tree to achieve good performance. Finally, STING is 4 There are a few methods that tolerate a limited degree of overlap between clusters. See [KR90] for more details. 25 a hierarchical cell structure that stores statistical information (e.g., density) about the objects in the cells <ref> [WYM97] </ref>. Thus, with only one scan of the dataset, clustering can be achieved by using the stored information but without recourse to the individual objects. 7.1.3 Machine Learning Methods There are a few clustering methods developed in the machine learning community. They are mostly probability-based approaches [Fis87]. <p> For instance, for algorithms relying on trees (e.g., BIRCH [ZRL96] and DBSCAN [EKXS96]) the O (logn) factor degrades to O (n) as the dimensionality increases. Similarly, for algorithms using a grid structure (e.g., STING <ref> [WYM97] </ref>), processing is exponential with respect to the number of dimensions. 8 Index Trees 8.1 Descriptions and References Index trees of various types are widely used to organize and access large data sets. <p> In a manner similar to that used for B+-trees, occupancy information (either exact or approximate) can be associated with each pointer to a hyperrectangular area in the multi-dimensional index tree structure, see e.g., <ref> [WYM97] </ref>. 2. Unordered (Flat/Hierarchical) The Generalized Search Tree was designed expressly to handle "flat" or "encapsulated" data. In GiSTs, user-defined operations may be implemented to choose an insertion location for new data, to partition data when nodes fill, and to label pointers with keys.
Reference: [You84] <author> P. Young. </author> <title> Recursive estimation and time-series analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: The updating of the model can be achieved by using techniques similar to those described in [CR94] to update polynomial models for selectivity estimation. The techniques use a method called recursive least-square-error <ref> [You84] </ref> to avoid a lot of expensive recomputation. 5 Log-Linear Models Log-linear modeling is a methodology for approximating discrete multidimensional probability distributions. The multi-way table of joint probabilities is approximated by a product of lower-order tables.
Reference: [ZRL96] <author> T. Zhang, R. Ramakrishnan and M. Livny. </author> <title> (1996) BIRCH: an Efficient Data Clustering Method for Very Large Databases, </title> <booktitle> Proc. 1996 SIGMOD, </booktitle> <pages> pp. 103-114. 45 46 47 48 </pages>
Reference-contexts: Focusing techniques based on spatial access methods (e.g., R* trees, Voronoi diagrams) are later developed to reduce the I/O cost required by CLARANS [EKX95]. By employing a balanced tree structure called CF tree, BIRCH makes explicit and takes full advantage of the amount of available buffering space <ref> [ZRL96] </ref>. A single scan of the dataset gives a basic clustering, and additional scans can be used to improve the quality further. <p> Thus, clustering algorithms should scale linearly with the number of dimensions. However, in practice, the situation is not as rosy, particularly for those algorithms that rely on various kinds of indexing to facilitate processing. For instance, for algorithms relying on trees (e.g., BIRCH <ref> [ZRL96] </ref> and DBSCAN [EKXS96]) the O (logn) factor degrades to O (n) as the dimensionality increases.
References-found: 116

