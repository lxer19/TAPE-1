URL: http://www.math.tau.ac.il/~megiddo/psfiles/peter1.ps.gz
Refering-URL: http://www.math.tau.ac.il/~megiddo/pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Using Fast Matrix Multiplication to Find Basic Solutions  
Author: Peter A. Beling and Nimrod Megiddo 
Keyword: Key Words: Computational complexity, linear programming, basic solutions, interior-point methods, fast matrix multiplication.  
Date: Februaray 1993  
Abstract: We consider the problem of finding a basic solution to a system of linear constraints (in standard form) given a non-basic solution to the system. We show that the known arithmetic complexity bounds for this problem admit considerable improvement. Our technique, which is similar in spirit to that used by Vaidya to find the best complexity bounds for linear programming, is based on reducing much of the computation involved to matrix multiplication. Consequently, our complexity bounds in their most general form are a function of the complexity of matrix multiplication. Using the best known algorithm for matrix multiplication, we achieve a running time of O(m 1:62 n) arithmetic operations for an m fi n problem in standard form. Previously, the best bound was O(m 2 n) arithmetic operations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: Denote by V (k) the number of arithmetic operations required to invert a k fi k matrix. Fact 3.3. T (k) = (k 2 ). Fact 3.4. T (k) = (V (k)), i.e., T (k) = O (V (k)) and V (k) = O (T (k)). See, e.g., <ref> [ 1 ] </ref> for proof of Fact 3.3 and [ 6 ] for proof of Fact 3.4.
Reference: [2] <author> D. Coppersmith and S. Winograd, </author> <title> "Matrix multiplication via arithmetic progressions," </title> <note> Journal of Symbolic Computation 9 (1990) 251-280. </note>
Reference-contexts: But since ffi is necessarily an upper bound on *, this last expression is O (m 3ffi 2ffi n), as claimed. Since it is known that two k fi k matrices can be multiplied in O (k 2:38 ) arithmetic operations <ref> [ 2 ] </ref> , we have the following as a trivial corollary: Corollary 3.8. The m fi n basis crashing problem can be solved in O (m 1:62 n) arithmetic operations.
Reference: [3] <author> G. Golub and C. Van Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: Since A ^ B 6 is a rank ` perturbation of A B , the well-known Sherman-Morrison-Woodbury formula (see, e.g., <ref> [ 3 ] </ref> ), gives a closed-form expression for A 1 ^ B in terms of A 1 B . In particular, A 1 B A 1 B U ) 1 V T A 1 where I is the ` fi ` identity matrix.
Reference: [4] <author> W. Marlow, </author> <title> Mathematics for Operations Research, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Moreover, the initialization step of finding a basic sequence B for A and computing A 1 B can be done in O (m 2 n) time by (say) using Gauss-Jordan pivots to reduce the left-hand side of the matrix [A I ] (cf. <ref> [ 4 ] </ref> ). Hence, we have the following bound on the complexity of the general problem: Proposition 2.2. The m fi n basis crashing problem can be solved in O (m 2 n) arithmetic operations. 3. <p> Form the set ^ C = C [ N and the matrix A N = ([A I]) 1 B A N . Step 2. Apply the Gauss-Jordan pivoting procedure (see, e.g., <ref> [ 4 ] </ref> ) to reduce the left-hand side of the matrix [ A N I], updating the basic sequence appropriately as columns of the identity matrix are found, and selecting each pivot element so that 7 an artificial index leaves the basic sequence whenever possible.
Reference: [5] <author> N. Megiddo, </author> <title> "On finding primal- and dual-optimal bases," </title> <note> ORSA Journal of Computing 3 (1991) 63-65. </note>
Reference: [6] <author> V. Pan, </author> <title> How to multiply natrices faster?, </title> <booktitle> Lecture Notes in Computer Science Vol. </booktitle> <volume> 179, </volume> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: Fact 3.3. T (k) = (k 2 ). Fact 3.4. T (k) = (V (k)), i.e., T (k) = O (V (k)) and V (k) = O (T (k)). See, e.g., [ 1 ] for proof of Fact 3.3 and <ref> [ 6 ] </ref> for proof of Fact 3.4. Proof of Proposition 3.2: The dominant effort in step 1 of the algorithm is the multiplication of the m fi m matrix A 1 B by the m fi r matrix A N .
Reference: [7] <author> P. Vaidya, </author> <title> "Speeding-up linear programming using fast matrix multiplication," </title> <booktitle> in: Proceedings of the 30th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <address> Los Angeles, </address> <year> 1989, </year> <pages> pp. 332-337. </pages>
Reference-contexts: In this note we show that this bound admits considerable improvement. Our technique, which is similar in spirit to that used by Vaidya <ref> [ 7 ] </ref> to improve the complexity bounds for linear programming, is based on reducing much of the computation in basis crashing to matrix multiplication. Consequently, our complexity bounds in their most general form are a function of the complexity of matrix multiplication.
Reference: [8] <author> S. Winograd, </author> <title> Arithmetic Complexity of Computations, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1980. </year>
Reference-contexts: For simplicity, we call this problem basis crashing. We are interested in the arithmetic complexity of basis crashing, i.e., the number of elementary arithmetic operations needed to solve the problem as a function of its dimension. (See, e.g., <ref> [ 8 ] </ref> for detailed material on arithmetic complexity in general.) Previously, the best arithmetic complexity bound known for the m fi n basis crashing problem was O (m 2 n) arithmetic operations. In this note we show that this bound admits considerable improvement.
Reference: [9] <author> S. Winograd, </author> <title> "Algebraic complexity theory," </title> <booktitle> in: Handbook of Theoretical Computer Science, Volume A, </booktitle> <editor> J. van Leeuwen, ed., </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990, </year> <pages> pp. 633-672. </pages>
References-found: 9

