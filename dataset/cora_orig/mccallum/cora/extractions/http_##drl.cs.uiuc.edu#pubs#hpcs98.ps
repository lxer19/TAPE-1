URL: http://drl.cs.uiuc.edu/pubs/hpcs98.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/hpcs98.html
Root-URL: http://www.cs.uiuc.edu
Title: 1 SPEEDING UP AUTOMATIC PARALLEL I/O PERFORMANCE OPTIMIZATION IN PANDA  
Author: Y. Chen and M. Winslett 
Note: 1.1 INTRODUCTION  
Abstract: The large number of system components and their complex interactions in a parallel I/O system, together with dynamically changing I/O patterns in scientific applications, impose a great challenge in selecting optimal I/O plans for an anticipated I/O workload in a target execution environment. Previous research has shown that a model-based approach that uses a performance model of the parallel I/O system to predict the performance of the parallel I/O system for a given I/O plan, coupled with an effective search algorithm, i.e., simulated annealing, can identify a high quality I/O plan automatically. However, to be truly successful, such automatic strategies must not only be capable of selecting optimal I/O plans for a parallel I/O system, but also be able to select them quickly. In this paper, we study the cost of optimization when using the model-based approach. We identify the major performance factors that affect the optimization time, and present techniques used to speed up the process. Our performance results obtained from an IBM SP show that with these techniques, our prototype optimization engine can select optimal I/O plans for our tested I/O systems on parallel machines are significantly more complex than those on traditional sequential machines, where only a small number of disks are attached to a host processor. The large number and variety of system components, including I/O processors, communication networks, and disks, calls for cases in less than five minutes.
Abstract-found: 1
Intro-found: 1
Reference: <author> ASA (1993). </author> <title> Adaptive Simulated Annealing (ASA). </title> <publisher> ftp.alumni.caltech.edu: /pub/ingber/ASA-shar. </publisher>
Reference-contexts: We used most of the default ASA parameter settings with a few exceptions. The modified parameter settings are listed in Table 1.2 and other parameter settings are listed in <ref> (ASA, 1993) </ref>. We also compared ASA with GA, an optimization algorithm described in (Chen et al., 1997b).
Reference: <author> Box, M., Davies, D., and Swann, W. </author> <year> (1969). </year> <title> Non-linear optimization techniques. In ICI Monograph No. 5, </title> <publisher> Edinburgh. Oliver & Boyd. </publisher>
Reference: <author> Chen, Y. </author> <year> (1998). </year> <title> Automatic parallel I/O performance optimization in Panda. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois. </institution> <note> SPEEDING UP AUTOMATIC PARALLEL I/O PERFORMANCE OPTIMIZATION 15 Chen, </note> <author> Y., Winslett, M., Cho, Y., and Kuo, S. </author> <year> (1997a). </year> <title> Automatic parallel I/O performance optimization in Panda. </title> <note> Submitted for publication. </note>
Reference: <author> Chen, Y., and Winslett, M. </author> <year> (1997b). </year> <title> Automatic parallel I/O performance optimization using Genetic Algorithms. </title> <note> Submitted for publication. </note>
Reference-contexts: In Panda, we have experimented with two search algorithms, i.e., adaptive simulated annealing (Ingber, 1996) and a genetic algorithm (Goldberg, 1989). (Chen et al., 1997a) and <ref> (Chen et al., 1997b) </ref> discuss these two algorithms with Panda in detail respectively. The performance results gathered from an IBM SP indicate that adaptive simulated annealing in general performs better than the genetic algorithm. <p> We used most of the default ASA parameter settings with a few exceptions. The modified parameter settings are listed in Table 1.2 and other parameter settings are listed in (ASA, 1993). We also compared ASA with GA, an optimization algorithm described in <ref> (Chen et al., 1997b) </ref>. With this set of ASA parameter settings, both the exhaustive search and ASA methods selected the same optimal Panda parameter settings, but GA stopped at suboptimal solution points that are about 5% worse than the optimals. method.
Reference: <author> Chen, Y., Winslett, M., Kuo, S., Cho, Y., Subramaniam, M., and Seamons, K. </author> <year> (1996). </year> <title> Performance modeling for the Panda array I/O library. </title> <booktitle> In Proceedings of Supercomputing '96. </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: to reducing the size of solution space, and an adaptive approach, incremental tuning based on performance goals, to the efficient use of SA in Panda. 1.4.1 A classification-based approach to reducing the model evaluation cost We briefly show how the Panda performance model is derived; details can be found in <ref> (Chen et al., 1996) </ref>.
Reference: <author> Davies, O. </author> <year> (1978). </year> <title> The design and analysis of industrial experiments. Longman Group Limited, </title> <booktitle> 2nd edition. </booktitle>
Reference-contexts: Hence, we do not tune this annealing scale parameter. This tuning strategy can be improved in the future by using more general experiment design methodologies, such as the fraction of factorial experiments <ref> (Davies, 1978) </ref>. The tuning process is repeated until either the goal is reached or a pre-defined maximum number of iterations is reached.
Reference: <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic algorithms in search, optimization and machine learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: In Panda, we have experimented with two search algorithms, i.e., adaptive simulated annealing (Ingber, 1996) and a genetic algorithm <ref> (Goldberg, 1989) </ref>. (Chen et al., 1997a) and (Chen et al., 1997b) discuss these two algorithms with Panda in detail respectively. The performance results gathered from an IBM SP indicate that adaptive simulated annealing in general performs better than the genetic algorithm.
Reference: <author> Graefe, G. </author> <year> (1993). </year> <title> Query evaluation techniques for large databases. </title> <journal> Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170. </pages>
Reference-contexts: However, no details on the cost of these automatic methods are available. Despite the lack of automatic optimization work in the parallel I/O world, automatic performance optimization is not uncommon in many other research areas, such as database and operating system research. <ref> (Graefe, 1993) </ref> provides a fairly complete discussion of many query processing and optimization techniques used for database systems and the design of query optimizers in database systems. (Swami and Gupta, 1988) compared several optimization algorithms used to optimize large join queries, including an iterative improvement method, 14 simulated annealing, a perturbation
Reference: <author> Hooke, R. and Jeeves, T. </author> <year> (1961). </year> <title> "Direct Search" solution of numerical and statistical problems. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 8 </volume> <pages> 212-229. </pages>
Reference-contexts: Devising specialized heuristic optimization algorithms for a large complex system with nonlinearly interdependent parameters can be also extremely difficult. Numerical algorithms such as Newton's method, or direct search methods <ref> (Hooke and Jeeves, 1961) </ref> are inappropriate since the cost model may not be continuous or differentiable. We believe that randomized search algorithms, e.g., simulated annealing (SA) (Kirkpatrick et al., 1983) and genetic algorithms (GAs) (Gold-berg, 1989), are good candidates to solve this problem.
Reference: <author> Ingber, L. </author> <year> (1991). </year> <title> Statistical mechanics of neocortical interactions: A scaling paradigm applied to electroencephalography. </title> <journal> Physics Review, </journal> <volume> A 44(6) </volume> <pages> 4017-4060. </pages>
Reference-contexts: ASA, however, takes into such considerations and generates more efficient annealing schedules for a given problem. ASA has been successfully used in many different disciplines, such as combat analysis <ref> (Ingber et al., 1991) </ref> and neuroscience (Ingber, 1991). 1.7 CONCLUSIONS In this paper, we identified the major factors that affect the cost optimization using the model-based approach. <p> ASA, however, takes into such considerations and generates more efficient annealing schedules for a given problem. ASA has been successfully used in many different disciplines, such as combat analysis (Ingber et al., 1991) and neuroscience <ref> (Ingber, 1991) </ref>. 1.7 CONCLUSIONS In this paper, we identified the major factors that affect the cost optimization using the model-based approach. We discussed how the cost of individual model evaluations, the size of the solution space, and the search algorithms have profound implications for the cost of optimization.
Reference: <author> Ingber, L. </author> <year> (1996). </year> <title> Adaptive simulated annealing (ASA): Lessons learned. </title> <journal> Control and Cybernetics, </journal> <volume> 25 </volume> <pages> 33-54. </pages>
Reference-contexts: In Panda, we have experimented with two search algorithms, i.e., adaptive simulated annealing <ref> (Ingber, 1996) </ref> and a genetic algorithm (Goldberg, 1989). (Chen et al., 1997a) and (Chen et al., 1997b) discuss these two algorithms with Panda in detail respectively. The performance results gathered from an IBM SP indicate that adaptive simulated annealing in general performs better than the genetic algorithm. <p> It can be extremely difficult to find a generally efficient optimal annealing schedule for many problems. As a step towards providing easy-to-use SPEEDING UP AUTOMATIC PARALLEL I/O PERFORMANCE OPTIMIZATION 7 and adapt SA facilities to users to solve their optimization problems, Lester Ing-ber <ref> (Ingber, 1996) </ref> developed the Adaptive Simulated Annealing package, which attempts to provide an adaptive environment in which users can easily tune the adaptive simulated annealing algorithm (ASA), a special simulate annealing algorithm that takes into consideration the finite ranges and sensitivities of different parameters and uses importance-sampling techniques to generate efficient
Reference: <author> Ingber, L., Fujio, H., and Wehner, M. </author> <year> (1991). </year> <title> Mathematical comparison of combat computer models to exercise data. </title> <journal> Mathl. Comput. Modelling, </journal> <volume> 15(1) </volume> <pages> 65-90. </pages>
Reference-contexts: ASA, however, takes into such considerations and generates more efficient annealing schedules for a given problem. ASA has been successfully used in many different disciplines, such as combat analysis <ref> (Ingber et al., 1991) </ref> and neuroscience (Ingber, 1991). 1.7 CONCLUSIONS In this paper, we identified the major factors that affect the cost optimization using the model-based approach. <p> ASA, however, takes into such considerations and generates more efficient annealing schedules for a given problem. ASA has been successfully used in many different disciplines, such as combat analysis (Ingber et al., 1991) and neuroscience <ref> (Ingber, 1991) </ref>. 1.7 CONCLUSIONS In this paper, we identified the major factors that affect the cost optimization using the model-based approach. We discussed how the cost of individual model evaluations, the size of the solution space, and the search algorithms have profound implications for the cost of optimization.
Reference: <author> Ioannidis, Y. and Wong, E. </author> <year> (1987). </year> <title> Query optimization by simulated annealing. </title> <booktitle> In Proceedings of the 1987 ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 9-22. </pages>
Reference-contexts: Their performance results showed that the simple iterative improvement is superior to all other methods when the amount of time allowed to perform optimization is small. However, as the optimization time increases, the simulated annealing becomes the winner. <ref> (Ioannidis and Wong, 1987) </ref> also showed how to use simulated annealing to optimize recursive queries. Their performance results suggest that with carefully selected simulated annealing parameter settings, the simulated annealing can identify optimal solutions in a relatively short time.
Reference: <author> Kirkpatrick, S., Jr., C. G., and Vecchi, M. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220(4598) </volume> <pages> 671-680. </pages>
Reference-contexts: Numerical algorithms such as Newton's method, or direct search methods (Hooke and Jeeves, 1961) are inappropriate since the cost model may not be continuous or differentiable. We believe that randomized search algorithms, e.g., simulated annealing (SA) <ref> (Kirkpatrick et al., 1983) </ref> and genetic algorithms (GAs) (Gold-berg, 1989), are good candidates to solve this problem. In this paper, we show how simulated annealing (SA) can be used to identify high quality parameter settings efficiently. <p> Their performance results suggest that with carefully selected simulated annealing parameter settings, the simulated annealing can identify optimal solutions in a relatively short time. Simulated annealing has been widely used in many different disciplines. <ref> (Kirkpatrick et al., 1983) </ref> presented examples of using SA to find optimal wiring for computer chips. The early simulated annealing methods typically use Boltzmann annealing (BA) (Szu and Hartley, 1987) that samples infinite parameter solution space and ignores the sensitivities of different parameters, and hence BA can be inefficient.
Reference: <author> Madhyasta, T., Elford, C., and Reed, D. </author> <year> (1996). </year> <title> Optimizing input/output using adaptive file system policies. </title> <booktitle> In Proceedings of the Fifth NASA Goddard Conference on Mass Storage Systems, </booktitle> <pages> pages II:493-514. </pages>
Reference-contexts: A recent effort has focused on automatically selecting efficient file system caching and prefetching policies in PPFS using two different I/O access pattern classification approaches. In <ref> (Madhyasta et al., 1996) </ref> and (Madhyasta and Reed, 1996), a trained neural network is used to recognize the application I/O access patterns based on a pre-defined classification of patterns. A Hidden SPEEDING UP AUTOMATIC PARALLEL I/O PERFORMANCE OPTIMIZATION 13 sequence with different numbers of compute and I/O nodes. <p> A recent effort has focused on automatically selecting efficient file system caching and prefetching policies in PPFS using two different I/O access pattern classification approaches. In (Madhyasta et al., 1996) and <ref> (Madhyasta and Reed, 1996) </ref>, a trained neural network is used to recognize the application I/O access patterns based on a pre-defined classification of patterns. A Hidden SPEEDING UP AUTOMATIC PARALLEL I/O PERFORMANCE OPTIMIZATION 13 sequence with different numbers of compute and I/O nodes.
Reference: <author> Madhyasta, T. and Reed, D. </author> <year> (1996). </year> <title> Intelligent, adaptive file system policy selection. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 172-179. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: A recent effort has focused on automatically selecting efficient file system caching and prefetching policies in PPFS using two different I/O access pattern classification approaches. In <ref> (Madhyasta et al., 1996) </ref> and (Madhyasta and Reed, 1996), a trained neural network is used to recognize the application I/O access patterns based on a pre-defined classification of patterns. A Hidden SPEEDING UP AUTOMATIC PARALLEL I/O PERFORMANCE OPTIMIZATION 13 sequence with different numbers of compute and I/O nodes. <p> A recent effort has focused on automatically selecting efficient file system caching and prefetching policies in PPFS using two different I/O access pattern classification approaches. In (Madhyasta et al., 1996) and <ref> (Madhyasta and Reed, 1996) </ref>, a trained neural network is used to recognize the application I/O access patterns based on a pre-defined classification of patterns. A Hidden SPEEDING UP AUTOMATIC PARALLEL I/O PERFORMANCE OPTIMIZATION 13 sequence with different numbers of compute and I/O nodes.
Reference: <author> Poole, J. </author> <year> (1994). </year> <title> Preliminary survey of I/O intensive applications. </title> <type> Technical Report CCSF-38, </type> <institution> Scalable I/O Initiative, Caltech Concurrent Supercomputing Facilities, Caltech. </institution>
Reference-contexts: We discuss related work in section 1.6, and draw conclusions and outline the future work in section 1.7. 1.2 PANDA BASICS Panda is designed for large-scale scientific applications performing collective array I/O operations on distributed-memory multiprocessors, a commonly observed I/O pattern in scientific applications <ref> (Poole, 1994) </ref>. In this paper, we base our discussion on Panda 2.1 with each node either dedicated to application computation or I/O. The dedicated I/O nodes are called "Panda servers" and the compute nodes are called "Panda clients".
Reference: <author> Swami, A. and Gupta, A. </author> <year> (1988). </year> <title> Optimization of large join queries. </title> <booktitle> In Proceedings of the 1988 ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 8-17, </pages> <address> Chicago, IL. </address>
Reference-contexts: in the parallel I/O world, automatic performance optimization is not uncommon in many other research areas, such as database and operating system research. (Graefe, 1993) provides a fairly complete discussion of many query processing and optimization techniques used for database systems and the design of query optimizers in database systems. <ref> (Swami and Gupta, 1988) </ref> compared several optimization algorithms used to optimize large join queries, including an iterative improvement method, 14 simulated annealing, a perturbation walk, and a quasi-random sampling.
Reference: <author> Szu, H. and Hartley, R. </author> <year> (1987). </year> <title> Fast simulated annealing. </title> <journal> Physics Review, </journal> <note> A 122(3-4):157-162. </note>
Reference-contexts: Simulated annealing has been widely used in many different disciplines. (Kirkpatrick et al., 1983) presented examples of using SA to find optimal wiring for computer chips. The early simulated annealing methods typically use Boltzmann annealing (BA) <ref> (Szu and Hartley, 1987) </ref> that samples infinite parameter solution space and ignores the sensitivities of different parameters, and hence BA can be inefficient. ASA, however, takes into such considerations and generates more efficient annealing schedules for a given problem.
References-found: 19

