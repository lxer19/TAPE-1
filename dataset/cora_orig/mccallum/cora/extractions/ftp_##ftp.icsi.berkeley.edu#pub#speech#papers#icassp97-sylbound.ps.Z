URL: ftp://ftp.icsi.berkeley.edu/pub/speech/papers/icassp97-sylbound.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/real/papers.html
Root-URL: http://www.icsi.berkeley.edu
Email: fsulin, shire, steveng, morgang@icsi.berkeley.edu  
Title: INTEGRATING SYLLABLE BOUNDARY INFORMATION INTO SPEECH RECOGNITION  
Author: Su-Lin Wu, Michael L. Shire, Steven Greenberg, Nelson Morgan 
Address: 1947 Center Street, Suite 600, Berkeley, CA 94704-1198, USA  Berkeley, Berkeley, CA 94720, USA  
Affiliation: International Computer Science Institute,  University of California at  
Note: Presented at ICASSP-97, Munich, vol. 2 pp. 987-990.  
Abstract: In this paper we examine the proposition that knowledge of the timing of syllabic onsets may be useful in improving the performance of speech recognition systems. A method of estimating the location of syllable onsets derived from the analysis of energy trajectories in critical band channels has been developed, and a syllable-based decoder has been designed and implemented that incorporates this onset information into the speech recognition process. For a small, continuous speech recognition task the addition of artificial syllabic onset information (derived from advance knowledge of the word transcriptions) lowers the word error rate by 38%. Incorporating acoustically-derived syllabic onset information reduces the word error rate by 10% on the same task. The latter experiment has highlighted representational issues on coordinating acoustic and lexical syllabifications, a topic we are beginning to explore. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Antonio Bonafonte, Rafael Estany, and Eugenio Vives. </author> <title> Study of subword units for spanish speech recognition. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 3, </volume> <pages> pages 1607-1610, </pages> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year> <pages> ESCA. </pages>
Reference-contexts: 1. INTRODUCTION Automatic speech recognition (ASR) systems typically rely upon phoneme- or sub-phoneme-based Hidden Markov models (HMMs) that are concatenated into word and sentence elements. Although syllable-based recognition has been successfully used in several languages (including Span-ish <ref> [1] </ref> and Chinese [2]), the syllable has been not been fully exploited for the automatic recognition of English.
Reference: [2] <author> Sung-Chien Lin, Lee-Feng Chien, Keh-Jiann Chen, and Lin-Shan Lee. </author> <title> A syllable-based very-large-vocabulary voice retrieval system for Chinese databases with textual attributes. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 1, </volume> <pages> pages 203-206, </pages> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year> <pages> ESCA. </pages>
Reference-contexts: 1. INTRODUCTION Automatic speech recognition (ASR) systems typically rely upon phoneme- or sub-phoneme-based Hidden Markov models (HMMs) that are concatenated into word and sentence elements. Although syllable-based recognition has been successfully used in several languages (including Span-ish [1] and Chinese <ref> [2] </ref>), the syllable has been not been fully exploited for the automatic recognition of English.
Reference: [3] <author> Dominic W. Massaro. </author> <title> Perceptual units in speech recognition. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 102(2) </volume> <pages> 199-208, </pages> <year> 1974. </year>
Reference-contexts: In this paper we investigate the possibility that syllabic onsets can be derived from the acoustic speech signal, and that this onset information can be incorporated into the decoding process in a manner sufficient to improve recognition performance. Evidence from both psychoacoustic and psycholinguisti-cal research <ref> [3, 4, 5] </ref>, as well as a model by one of the authors [6], suggests that the syllable is a basic perceptual unit for speech processing in humans.
Reference: [4] <author> Douglas O'Shaughnessy. </author> <title> Speech Communication, </title> <booktitle> chapter 5, </booktitle> <pages> pages 164-203. </pages> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: In this paper we investigate the possibility that syllabic onsets can be derived from the acoustic speech signal, and that this onset information can be incorporated into the decoding process in a manner sufficient to improve recognition performance. Evidence from both psychoacoustic and psycholinguisti-cal research <ref> [3, 4, 5] </ref>, as well as a model by one of the authors [6], suggests that the syllable is a basic perceptual unit for speech processing in humans.
Reference: [5] <author> Juan Segui, Emmanuel Dupoux, and Jacques Mehler. </author> <title> The role of the syllable in speech segmentation, phoneme identification and lexical access. </title> <editor> In Gerry Altmann, editor, </editor> <booktitle> Cognitive Models of Speech Processing, chapter 12, </booktitle> <pages> pages 263-280. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In this paper we investigate the possibility that syllabic onsets can be derived from the acoustic speech signal, and that this onset information can be incorporated into the decoding process in a manner sufficient to improve recognition performance. Evidence from both psychoacoustic and psycholinguisti-cal research <ref> [3, 4, 5] </ref>, as well as a model by one of the authors [6], suggests that the syllable is a basic perceptual unit for speech processing in humans.
Reference: [6] <author> Steven Greenberg. </author> <title> Understanding speech understanding: Towards a unified theory of speech perception. </title> <booktitle> In Proceedings of the ESCA Workshop (ETRW) on The Auditory Basis of Speech Perception, </booktitle> <pages> pages 1-8, </pages> <address> Keele, United Kingdom, </address> <month> July </month> <year> 1996. </year> <pages> ESCA. </pages>
Reference-contexts: Evidence from both psychoacoustic and psycholinguisti-cal research [3, 4, 5], as well as a model by one of the authors <ref> [6] </ref>, suggests that the syllable is a basic perceptual unit for speech processing in humans. The syllable was proposed as a basic unit of automatic (computer) speech recognition as early as 1975 [7, 8], and this idea has been periodically reexamined (e.g. in [9, 10, 11, 12, 13]).
Reference: [7] <author> Osamu Fujimura. </author> <title> Syllable as a unit of speech recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-23(1):82-87, </volume> <month> February </month> <year> 1975. </year>
Reference-contexts: The syllable was proposed as a basic unit of automatic (computer) speech recognition as early as 1975 <ref> [7, 8] </ref>, and this idea has been periodically reexamined (e.g. in [9, 10, 11, 12, 13]). The syllabic level confers several potential benefits; for one, syllabic boundaries are more precisely defined than phonetic segment boundaries in both the speech waveform and in spectrographic displays.
Reference: [8] <author> Paul Mermelstein. </author> <title> Automatic segmentation of speech into syllabic units. </title> <journal> J. Acoust. Soc. Am, </journal> <volume> 58(4) </volume> <pages> 880-883, </pages> <month> October </month> <year> 1975. </year>
Reference-contexts: The syllable was proposed as a basic unit of automatic (computer) speech recognition as early as 1975 <ref> [7, 8] </ref>, and this idea has been periodically reexamined (e.g. in [9, 10, 11, 12, 13]). The syllabic level confers several potential benefits; for one, syllabic boundaries are more precisely defined than phonetic segment boundaries in both the speech waveform and in spectrographic displays. <p> These structural regularities can, in principle, be exploited to reliably estimate syllabic boundaries. Previous research on detecting syllable boundaries and using this information to improve recognition accuracy is reported for English <ref> [8, 9, 10] </ref> and for German [12, 13]. In this communication we describe a perceptually-oriented method for the automatic delineation of syllabic onsets. Artificial neural networks (NNs) are used to classify both phonetic segments and potential syllabic onsets.
Reference: [9] <author> M.J. Hunt, M. Lennig, and P. Mermelstein. </author> <title> Experiments in syllable-based recognition of continuous speech. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 3, </volume> <pages> pages 880-883, </pages> <address> Denver, Colorado, </address> <month> April </month> <year> 1980. </year> <note> IEEE. </note>
Reference-contexts: The syllable was proposed as a basic unit of automatic (computer) speech recognition as early as 1975 [7, 8], and this idea has been periodically reexamined (e.g. in <ref> [9, 10, 11, 12, 13] </ref>). The syllabic level confers several potential benefits; for one, syllabic boundaries are more precisely defined than phonetic segment boundaries in both the speech waveform and in spectrographic displays. <p> These structural regularities can, in principle, be exploited to reliably estimate syllabic boundaries. Previous research on detecting syllable boundaries and using this information to improve recognition accuracy is reported for English <ref> [8, 9, 10] </ref> and for German [12, 13]. In this communication we describe a perceptually-oriented method for the automatic delineation of syllabic onsets. Artificial neural networks (NNs) are used to classify both phonetic segments and potential syllabic onsets.
Reference: [10] <author> P.D. Green, N. R. Kew, and D. A. Miller. </author> <title> Speech representations in the sylk recognition project. </title> <editor> In M. P. Cooke, S. W. Beet, and M. D. Crawford, editors, </editor> <booktitle> Visual Representation of Speech Signals, chapter 26, </booktitle> <pages> pages 265-272. </pages> <publisher> John Wiley, </publisher> <year> 1993. </year>
Reference-contexts: The syllable was proposed as a basic unit of automatic (computer) speech recognition as early as 1975 [7, 8], and this idea has been periodically reexamined (e.g. in <ref> [9, 10, 11, 12, 13] </ref>). The syllabic level confers several potential benefits; for one, syllabic boundaries are more precisely defined than phonetic segment boundaries in both the speech waveform and in spectrographic displays. <p> These structural regularities can, in principle, be exploited to reliably estimate syllabic boundaries. Previous research on detecting syllable boundaries and using this information to improve recognition accuracy is reported for English <ref> [8, 9, 10] </ref> and for German [12, 13]. In this communication we describe a perceptually-oriented method for the automatic delineation of syllabic onsets. Artificial neural networks (NNs) are used to classify both phonetic segments and potential syllabic onsets.
Reference: [11] <author> Kenneth W. Church. </author> <title> Phonological parsing and lexical retrieval. </title> <editor> In Uli H. Frauenfelder and Lorraine Komisar-jevsky Tyler, editors, </editor> <title> Spoken Word Recognition, </title> <journal> Cognition Special Issues, </journal> <volume> chapter 3, </volume> <pages> pages 53-69. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: The syllable was proposed as a basic unit of automatic (computer) speech recognition as early as 1975 [7, 8], and this idea has been periodically reexamined (e.g. in <ref> [9, 10, 11, 12, 13] </ref>). The syllabic level confers several potential benefits; for one, syllabic boundaries are more precisely defined than phonetic segment boundaries in both the speech waveform and in spectrographic displays.
Reference: [12] <author> W. Reichl and G. Ruske. </author> <title> Syllable segmentation of continuous speech with artificial neural networks. </title> <booktitle> In Eu-rospeech, </booktitle> <pages> pages 1771-1774, </pages> <address> Berlin, Germany, </address> <month> Septem-ber </month> <year> 1993. </year>
Reference-contexts: The syllable was proposed as a basic unit of automatic (computer) speech recognition as early as 1975 [7, 8], and this idea has been periodically reexamined (e.g. in <ref> [9, 10, 11, 12, 13] </ref>). The syllabic level confers several potential benefits; for one, syllabic boundaries are more precisely defined than phonetic segment boundaries in both the speech waveform and in spectrographic displays. <p> These structural regularities can, in principle, be exploited to reliably estimate syllabic boundaries. Previous research on detecting syllable boundaries and using this information to improve recognition accuracy is reported for English [8, 9, 10] and for German <ref> [12, 13] </ref>. In this communication we describe a perceptually-oriented method for the automatic delineation of syllabic onsets. Artificial neural networks (NNs) are used to classify both phonetic segments and potential syllabic onsets. In a departure from previous research, we focus on continuous, naturally-spoken English. 2.
Reference: [13] <author> Katrin Kirchhoff. </author> <title> Syllable-level desynchronisation of phonetic features for speech recognition. </title> <booktitle> In ICSLP, </booktitle> <volume> volume 4, </volume> <pages> pages 2274-2276, </pages> <address> Philadephia, Pennsylva-nia, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: The syllable was proposed as a basic unit of automatic (computer) speech recognition as early as 1975 [7, 8], and this idea has been periodically reexamined (e.g. in <ref> [9, 10, 11, 12, 13] </ref>). The syllabic level confers several potential benefits; for one, syllabic boundaries are more precisely defined than phonetic segment boundaries in both the speech waveform and in spectrographic displays. <p> These structural regularities can, in principle, be exploited to reliably estimate syllabic boundaries. Previous research on detecting syllable boundaries and using this information to improve recognition accuracy is reported for English [8, 9, 10] and for German <ref> [12, 13] </ref>. In this communication we describe a perceptually-oriented method for the automatic delineation of syllabic onsets. Artificial neural networks (NNs) are used to classify both phonetic segments and potential syllabic onsets. In a departure from previous research, we focus on continuous, naturally-spoken English. 2.
Reference: [14] <author> Norman R. French, Charles W. Carter, Jr., and Walter Koenig, Jr. </author> <title> The words and sounds of telephone conversations. </title> <journal> The Bell System Technical Journal, </journal> <volume> IX:290-325, </volume> <month> April </month> <year> 1930. </year>
Reference-contexts: Detailed statistical analyses of sponta onset features. neous informal discourse indicate that the syllabic structure of conversational English is not as complicated as has been generally supposed. For example, data gathered from telephone conversations in <ref> [14] </ref> and the Switchboard corpus [15, 16] indicate that over 80% of the word tokens in these corpora are monosyllabic, and more than 85% of the syllables are of the canonical consonant-vowel (CV), vowel-consonant (VC), V, or CVC varieties.
Reference: [15] <author> John J. Godfrey, Edward C. Holliman, and Jane Mc-Daniel. </author> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 517-520, </pages> <address> San Francisco, California, </address> <month> March </month> <year> 1992. </year> <note> IEEE. </note>
Reference-contexts: Detailed statistical analyses of sponta onset features. neous informal discourse indicate that the syllabic structure of conversational English is not as complicated as has been generally supposed. For example, data gathered from telephone conversations in [14] and the Switchboard corpus <ref> [15, 16] </ref> indicate that over 80% of the word tokens in these corpora are monosyllabic, and more than 85% of the syllables are of the canonical consonant-vowel (CV), vowel-consonant (VC), V, or CVC varieties. These structural regularities can, in principle, be exploited to reliably estimate syllabic boundaries.
Reference: [16] <author> Steven Greenberg, Joy Hollenback, and Dan Ellis. </author> <title> The Switchboard transcription project. </title> <type> Technical report, </type> <institution> International Computer Science Institute, </institution> <year> 1997. </year>
Reference-contexts: Detailed statistical analyses of sponta onset features. neous informal discourse indicate that the syllabic structure of conversational English is not as complicated as has been generally supposed. For example, data gathered from telephone conversations in [14] and the Switchboard corpus <ref> [15, 16] </ref> indicate that over 80% of the word tokens in these corpora are monosyllabic, and more than 85% of the syllables are of the canonical consonant-vowel (CV), vowel-consonant (VC), V, or CVC varieties. These structural regularities can, in principle, be exploited to reliably estimate syllabic boundaries.
Reference: [17] <author> Donald D. Greenwood. </author> <title> Critical bandwidth and the frequency coordinates of the basilar membrane. </title> <journal> JASA, </journal> <volume> 33 </volume> <pages> 1344-1356, </pages> <year> 1961. </year>
Reference-contexts: Large values in this representation correspond to positive utterance `seven seven oh four five'. The vertical lines denote syllable onsets as derived from hand-transcribed phone labels. going energy regions where hypothesized syllable onset characteristics occur. The channel outputs are subsequently averaged over a region spanning nine critical bands <ref> [17] </ref>, the result of which is illustrated in Figure 2. Features derived from this procedure are updated every 10 ms.
Reference: [18] <author> Hynek Hermansky and Nelson Morgan. </author> <title> RASTA processing of speech. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2(4) </volume> <pages> 578-589, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The channel outputs are subsequently averaged over a region spanning nine critical bands [17], the result of which is illustrated in Figure 2. Features derived from this procedure are updated every 10 ms. The resulting vectors are concatenated with log-RASTA <ref> [18] </ref> features computed over a 25-ms frame every 10 ms, and this combination is used as the input to a neural network classifier for estimating the location of syllabic onsets.
Reference: [19] <author> Martin Oerder and Hermann Ney. </author> <title> Word graphs: An efficient interface between continuous-speech recognition and language understanding. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 2, </volume> <pages> pages 119-122, </pages> <address> Minneapolis, Minnesota, </address> <month> April </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: The decoder processes phonetic probabilities from a neural network using a conventional Viterbi algorithm using a bigram syllable grammar and creates a syllable graph (a derivative of the word graph as defined in <ref> [19] </ref>). The syllable graph serves as input to the program's stack decoder [20, 21], along with a bigram word grammar, to determine the most likely sequence of words.
Reference: [20] <author> Frederick Jelinek. </author> <title> Fast sequential decoding algorithm using a stack. </title> <journal> IBM J. Res. Develop., </journal> <volume> 13 </volume> <pages> 675-685, </pages> <month> November </month> <year> 1969. </year>
Reference-contexts: The decoder processes phonetic probabilities from a neural network using a conventional Viterbi algorithm using a bigram syllable grammar and creates a syllable graph (a derivative of the word graph as defined in [19]). The syllable graph serves as input to the program's stack decoder <ref> [20, 21] </ref>, along with a bigram word grammar, to determine the most likely sequence of words.
Reference: [21] <author> Steve Renals and Mike Hochberg. </author> <title> Efficient evaluation of the LVCSR search space using the noway decoder. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 149-152, </pages> <address> Atlanta, Georgia, </address> <month> May </month> <year> 1996. </year> <note> IEEE. </note>
Reference-contexts: The decoder processes phonetic probabilities from a neural network using a conventional Viterbi algorithm using a bigram syllable grammar and creates a syllable graph (a derivative of the word graph as defined in [19]). The syllable graph serves as input to the program's stack decoder <ref> [20, 21] </ref>, along with a bigram word grammar, to determine the most likely sequence of words.
Reference: [22] <author> Frank K. Soong and Eng-Fong Huang. </author> <title> A tree-trellis based fast search for finding the N best sentence hypotheses in continuous speech recognition. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 705-708, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year> <note> IEEE. </note>
Reference-contexts: The syllable graph serves as input to the program's stack decoder [20, 21], along with a bigram word grammar, to determine the most likely sequence of words. This procedure is a variation on the multiple-pass decoding method (related to the approach used in <ref> [22] </ref> and [23]) and enables the use of a complex language model at a higher stage of linguistic representation. The additional complexity of the decoder design permits the explicit representation of the relationship of phones to syllables and syllables to words.
Reference: [23] <author> P. Kenny, R. Hollan, V. Gupta, M Lennig, P Mermel-stein, and D. O'Shaughnessy. </author> <title> A fl -admissible heuristics for rapid lexical access. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 689-692, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year> <note> IEEE. </note>
Reference-contexts: The syllable graph serves as input to the program's stack decoder [20, 21], along with a bigram word grammar, to determine the most likely sequence of words. This procedure is a variation on the multiple-pass decoding method (related to the approach used in [22] and <ref> [23] </ref>) and enables the use of a complex language model at a higher stage of linguistic representation. The additional complexity of the decoder design permits the explicit representation of the relationship of phones to syllables and syllables to words.
Reference: [24] <institution> Center for Spoken Language Understanding, Department of Computer Science and Engineering, Oregon Graduate Institute. Numbers corpus, release 1.0, </institution> <year> 1995. </year>
Reference-contexts: Syllabic onset information is introduced as an additional probability input into the decoder at the level of the syllable graph. 4. RECOGNITION EXPERIMENTS Recognition experiments were performed on a subset of the OGI Numbers corpus <ref> [24] </ref>. This corpus contains continuous, naturally spoken utterances of many different speakers saying numbers from a vocabulary of thirty words. A sample utterance from the database is "eighteen thirty one." The example in Figure 2 is also derived from the Numbers corpus.
Reference: [25] <author> Godfrey Dewey. </author> <title> Relative Frequency of English Speech Sounds, </title> <booktitle> volume 4 of Harvard Studies in Education. </booktitle> <publisher> Harvard University Press, </publisher> <address> Cambridge, </address> <year> 1923. </year>
Reference: [26] <author> Zhihong Hu, Johan Schalkwyk, Etienne Barnard, and Ronald Cole. </author> <title> Speech recognition using syllable-like units. </title> <booktitle> In ICSLP, </booktitle> <volume> volume 2, </volume> <pages> pages 1117-1120, </pages> <address> Philade-phia, Pennsylvania, </address> <month> October </month> <year> 1996. </year>
References-found: 26

