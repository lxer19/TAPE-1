URL: http://www.cs.monash.edu.au/~jono/Open_Uni/TR4-94.ps
Refering-URL: http://www.cs.monash.edu.au/~rohan/
Root-URL: 
Email: (D.J.Hand@open.ac.uk)  
Author: Jonathan Oliver and David Hand . JONATHAN J. OLIVER DAVID HAND 
Address: Walton Hall, Milton Keynes, MK7 6AA, UK  
Affiliation: Department of Statistics Open University  
Note: July 6th 1994 Amended November 14th 1994 and December 5th 1996 (C) Copyright  Contents  
Abstract: Tech Report 4-94 Department of Statistics, Open University, Walton Hall, MK7 6AA, UK Tech Report 205 Department of Computer Science, Monash University, Clayton, Vic. 3168, Australia Abstract: This paper examines the minimum encoding approaches to inference, Minimum Message Length (MML) and Minimum Description Length (MDL). This paper was written with the objective of providing an introduction to this area for statisticians. We describe coding techniques for data, and examine how these techniques can be applied to perform inference and model selection. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Allison, C.S. Wallace, and C.N. Yee. </author> <title> Finite-state models in the alignment of macromolecules. </title> <journal> Journal of Molecular Evolution, </journal> <volume> 35 </volume> <pages> 77-89, </pages> <year> 1992. </year>
Reference-contexts: In this section, we consider codes for non-regular structures such as classification trees 11 [5] and finite state automata [8, 9]. MML has been applied to a range of other non-regular problems including Clustering [23, 4], DNA string alignment <ref> [1] </ref> and Decision Graphs [12]. 4.1 Classification Trees 4.1.1 Description of Classification Trees A classification tree is a non-parametric model which is used to perform classification. A tree partitions an object space into regions, and associates a probability for each class with each region.
Reference: [2] <author> R.A. Baxter and D.L. Dowe. </author> <title> Model selection in linear regression using the MML criterion. </title> <editor> In J.A. Storer and M. Cohn, editors, </editor> <booktitle> Proc. 4'th IEEE Data Compression Conference, </booktitle> <pages> page 498, </pages> <address> Snowbird, Utah, March 1994. </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference-contexts: This is entirely sensible when dealing with models of similar complexity, but this approach breaks down when comparing models of distinct complexity classes. For example, by taking enough terms in a polynomial function, one can exactly fit any arbitrary distribution of a finite number of points <ref> [2] </ref>.
Reference: [3] <author> R.A. Baxter and J.J. Oliver. </author> <title> MDL and MML: Similarities and differences. </title> <type> Technical report TR 207, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <year> 1994. </year> <note> Available on the WWW from http://www.cs.monash.edu.au/ ~ jono. </note>
Reference-contexts: We concentrate on the Wallace approach [22, 23, 24, 25] | for details of the Rissanen approach see [18, 19, 20]. The second report (Oliver and Baxter [13]) describes similarities and differences between MML inference and Bayesian inference. The third report (Baxter and Oliver <ref> [3] </ref>) describes similarities and differences between Wallace's MML approach and Rissanen's MDL approach. We assume we have some measurements from the real world, and a set of models, M = fm 1 ; m 2 ; : : : ; g, with which we attempt to explain those measurements. <p> There is debate over the role of the prior distribution implied by a code (discussed in Section 5.3 and Baxter and Oliver <ref> [3] </ref>). 5.1 Non-Redundant Codes A code dictionary for theories is non-redundant if there are not two (or more) codewords for the same theory. <p> Instead the majority of the MDL work uses the concept of a universal prior distribution over the set of positive integers to represent complete prior ignorance [18]. Similarities and differences between MML and MDL are discussed in Baxter and Oliver <ref> [3] </ref>. 5.4 Relationship with Bayesianism MML can be interpreted as a form of Bayesianism.
Reference: [4] <author> D.M. Boulton and C.S. Wallace. </author> <title> A program for numerical classification. </title> <journal> Computer Journal, </journal> <volume> 13 </volume> <pages> 63-69, </pages> <year> 1970. </year>
Reference-contexts: In this section, we consider codes for non-regular structures such as classification trees 11 [5] and finite state automata [8, 9]. MML has been applied to a range of other non-regular problems including Clustering <ref> [23, 4] </ref>, DNA string alignment [1] and Decision Graphs [12]. 4.1 Classification Trees 4.1.1 Description of Classification Trees A classification tree is a non-parametric model which is used to perform classification. A tree partitions an object space into regions, and associates a probability for each class with each region.
Reference: [5] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: They offer methods to avoid including a preamble describing the accuracy of the AOPVs. 4 Codes for Non-Regular Structures The basic principles of MML can be applied to a wide range of model structures. In this section, we consider codes for non-regular structures such as classification trees 11 <ref> [5] </ref> and finite state automata [8, 9].
Reference: [6] <author> G.J. Chaitin. </author> <title> Information-theoretic computational complexity. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 20 </volume> <pages> 10-15, </pages> <year> 1974. </year>
Reference-contexts: Given a particular encoding scheme, we define the code length of the data as the length of the encoded binary string. The complexity of a string, S, is the length of the shortest string that can be constructed to describe S. Chaitin <ref> [6] </ref> used Godel's Incompleteness Theorem to demonstrate that it is in fact impossible to determine the complexity of an arbitrary string. MML takes a pragmatic approach to this problem | approximating the complexity of a string, S, as the length of the shortest string we can find that defines S.
Reference: [7] <author> T.M. </author> <title> Cover and A.T. Joy. Elements of Information Theory. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: OLIVER and HAND, TR 4-94 Page 20 5.2 The Prior Distribution Implied by a Code A non-redundant prefix code for theories implies a probability distribution over those theories. Such codes satisfy the Kraft inequality <ref> [7] </ref>: X m i 2 M 2 message length (m i ) 1 (17) If the code is also efficient then X m i 2 M 2 message length (m i ) = 1 (18) Therefore, we may construct a normalised probability distribution over theories by assigning: P rob (m i
Reference: [8] <author> M.P. Georgeff and C.S. Wallace. </author> <title> A general criterion for inductive inference. </title> <editor> In T. O'Shea, editor, </editor> <booktitle> Advances in artificial intelligence : proceedings of the Sixth European Conference on Artificial Intelligence, </booktitle> <pages> pages 473-482, </pages> <address> Amsterdam, 1984. </address> <publisher> North Holland. </publisher>
Reference-contexts: In this section, we consider codes for non-regular structures such as classification trees 11 [5] and finite state automata <ref> [8, 9] </ref>. MML has been applied to a range of other non-regular problems including Clustering [23, 4], DNA string alignment [1] and Decision Graphs [12]. 4.1 Classification Trees 4.1.1 Description of Classification Trees A classification tree is a non-parametric model which is used to perform classification. <p> Each time a symbol is parsed, the automaton changes state. The new state is found by following the arc labelled by the current input symbol. When the automata parses the delimiter "/", it returns to the initial state. Georgeff and Wallace <ref> [8, 9] </ref> presented a method for inferring probabilistic finite state automata (PFSA) from a sequence of symbols.
Reference: [9] <author> M.P. Georgeff and C.S. Wallace. </author> <title> A general selection criterion for inductive inference. </title> <type> Technical report TR 44, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: In this section, we consider codes for non-regular structures such as classification trees 11 [5] and finite state automata <ref> [8, 9] </ref>. MML has been applied to a range of other non-regular problems including Clustering [23, 4], DNA string alignment [1] and Decision Graphs [12]. 4.1 Classification Trees 4.1.1 Description of Classification Trees A classification tree is a non-parametric model which is used to perform classification. <p> Each time a symbol is parsed, the automaton changes state. The new state is found by following the arc labelled by the current input symbol. When the automata parses the delimiter "/", it returns to the initial state. Georgeff and Wallace <ref> [8, 9] </ref> presented a method for inferring probabilistic finite state automata (PFSA) from a sequence of symbols. <p> Hence, we can use `0' to specify Super-rt-sue and `1' to specify Mx-missile. 14 To specify Mx-missile. 15 We note that Georgeff and Wallace <ref> [9] </ref> left off the final delimeter in string S. OLIVER and HAND, TR 4-94 Page 18 where N is the number of states in the PFSA, Delim is the number of delimiters in S, and S t is the t th symbol in the string.
Reference: [10] <author> D.J. Hand, F. Daly, A.D. Lunn, K.J. McConway, and E. Ostrowski. </author> <title> Data set 231: Husbands and wives. </title> <booktitle> In Small Data Sets, </booktitle> <pages> pages 179-181. </pages> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1994. </year>
Reference-contexts: Specifying to which cell a parameter value belongs requires log 2 ceiling ( ba AOP V ) bits. 3.2 Example of Encoding a set of Heights 181 184 166 178 162 174 171 174 172 180 Consider the problem of encoding the set of heights in Figure 8 <ref> [10] </ref> using a two part message. To illustrate, we encode these heights using a code dictionary constructed from a normal density function, such as the density function for heights discussed in Section 2.7.
Reference: [11] <author> D.A. Lelewer and D.S. Hirschberg. </author> <title> Data compression. </title> <journal> ACM Computing Surveys, </journal> <volume> 19:261, </volume> <year> 1987. </year>
Reference-contexts: The codes constructed in this section are equivalent to optimal Huffman codes <ref> [11] </ref>.
Reference: [12] <author> J.J. Oliver. </author> <title> Decision graphs an extension of decision trees. </title> <booktitle> In Proceedings of the Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 343-350, </pages> <year> 1993. </year> <note> Extended version available as TR 173, </note> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Australia. </address> <note> Available on the WWW from http://www.cs.monash.edu.au/ ~ jono. </note>
Reference-contexts: In this section, we consider codes for non-regular structures such as classification trees 11 [5] and finite state automata [8, 9]. MML has been applied to a range of other non-regular problems including Clustering [23, 4], DNA string alignment [1] and Decision Graphs <ref> [12] </ref>. 4.1 Classification Trees 4.1.1 Description of Classification Trees A classification tree is a non-parametric model which is used to perform classification. A tree partitions an object space into regions, and associates a probability for each class with each region.
Reference: [13] <author> J.J. Oliver and R.A. Baxter. </author> <title> MML and Bayesianism: Similarities and differences. </title> <type> Technical report TR 206, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <year> 1994. </year> <note> Available on the WWW from http://www.cs.monash.edu.au/ ~ jono. </note>
Reference-contexts: This report was written with the objective of providing an introduction to minimum encoding inference for statisticians. We concentrate on the Wallace approach [22, 23, 24, 25] | for details of the Rissanen approach see [18, 19, 20]. The second report (Oliver and Baxter <ref> [13] </ref>) describes similarities and differences between MML inference and Bayesian inference. The third report (Baxter and Oliver [3]) describes similarities and differences between Wallace's MML approach and Rissanen's MDL approach. <p> In a Bayesian framework, we do not construct the posterior probability of a model, but construct a posterior density over models. The MML approach effectively partitions the posterior density to estimate the posterior probability of models. These issues are discussed at length in Oliver and Baxter <ref> [13] </ref>. OLIVER and HAND, TR 4-94 Page 21 6 Criteria Used for Model Selection Many criteria have been suggested for selecting a model given some data.
Reference: [14] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: to the proportion of training objects from each class which would be assigned to that leaf; for example, if the leaf has more Democrats than Republicans, then this suggests that objects falling in this leaf are more likely to be Democrats. 11 Termed Decision Trees in the Machine Learning literature <ref> [14, 15] </ref>. OLIVER and HAND, TR 4-94 Page 17 4.1.2 Codes for Classification Trees We assume the following coding conventions.
Reference: [15] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: to the proportion of training objects from each class which would be assigned to that leaf; for example, if the leaf has more Democrats than Republicans, then this suggests that objects falling in this leaf are more likely to be Democrats. 11 Termed Decision Trees in the Machine Learning literature <ref> [14, 15] </ref>. OLIVER and HAND, TR 4-94 Page 17 4.1.2 Codes for Classification Trees We assume the following coding conventions.
Reference: [16] <author> J.R. Quinlan and R.L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: Similarly, if a single theory can be used to describe a set of measurements in two (or more) distinct ways, then the coding scheme for the data is redundant. Quinlan and Rivest <ref> [16] </ref> suggested a redundant code for describing N data items which were split into 2 classes. Their message took the form: * A specification of the most common class (1 bit). * A specification how many items Z were in the most common class.
Reference: [17] <author> Robert F. Rice. </author> <title> Some practical noiseless coding techniques, Part II, Module PSI14,K+. </title> <type> JPL Publication 91-3, </type> <institution> Jet Propulsion Laboratories, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: This particular code is an example of a Rice code <ref> [17] </ref>. OLIVER and HAND, TR 4-94 Page 4 2.2 Coding A Single Integer from a Uniform Distribution We can encode an integer value x that takes a value in the range [0 :: (2 N 1)] in a codeword of N bits, by using a binary code.
Reference: [18] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11 </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: This report was written with the objective of providing an introduction to minimum encoding inference for statisticians. We concentrate on the Wallace approach [22, 23, 24, 25] | for details of the Rissanen approach see <ref> [18, 19, 20] </ref>. The second report (Oliver and Baxter [13]) describes similarities and differences between MML inference and Bayesian inference. The third report (Baxter and Oliver [3]) describes similarities and differences between Wallace's MML approach and Rissanen's MDL approach. <p> Instead the majority of the MDL work uses the concept of a universal prior distribution over the set of positive integers to represent complete prior ignorance <ref> [18] </ref>. Similarities and differences between MML and MDL are discussed in Baxter and Oliver [3]. 5.4 Relationship with Bayesianism MML can be interpreted as a form of Bayesianism.
Reference: [19] <author> J. Rissanen. </author> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 49 </volume> <pages> 223-239, </pages> <year> 1987. </year>
Reference-contexts: This report was written with the objective of providing an introduction to minimum encoding inference for statisticians. We concentrate on the Wallace approach [22, 23, 24, 25] | for details of the Rissanen approach see <ref> [18, 19, 20] </ref>. The second report (Oliver and Baxter [13]) describes similarities and differences between MML inference and Bayesian inference. The third report (Baxter and Oliver [3]) describes similarities and differences between Wallace's MML approach and Rissanen's MDL approach.
Reference: [20] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year> <title> OLIVER and HAND, </title> <type> TR 4-94 Page 23 </type>
Reference-contexts: This report was written with the objective of providing an introduction to minimum encoding inference for statisticians. We concentrate on the Wallace approach [22, 23, 24, 25] | for details of the Rissanen approach see <ref> [18, 19, 20] </ref>. The second report (Oliver and Baxter [13]) describes similarities and differences between MML inference and Bayesian inference. The third report (Baxter and Oliver [3]) describes similarities and differences between Wallace's MML approach and Rissanen's MDL approach. <p> Wallace and Freeman [24, page 241] take a Bayesian interpretation of Minimum Encoding Inference and conclude: "there can be no substitute for careful specification of whatever prior knowledge is available" Rissanen <ref> [20] </ref> acknowledges the relation between coding and Bayesian priors. However, he rejects the notion that a probability distribution can "capture prior knowledge in an adequate manner".
Reference: [21] <author> J. Schlimmer and R. Granger. </author> <title> Incremental learning from noisy data. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 317-354, </pages> <year> 1986. </year>
Reference-contexts: A tree partitions an object space into regions, and associates a probability for each class with each region. A new object is classified by examining the probabilities in the region where it falls. A tree from the Voting domain (described in <ref> [21] </ref>) is illustrated in Figure 10. In this domain, each object represents a member of the US congress, the class represents whether they are from the Republican or Democratic party, and the attributes take their values from the member's voting pattern on a number of issues.
Reference: [22] <author> C.S. Wallace. </author> <title> Classification by minimum-message-length inference. </title> <editor> In G. Goos and J. Hartmanis, editors, </editor> <booktitle> Advances in Computing and Information - ICCI '90, </booktitle> <pages> pages 72-81. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: This technical report is the first in a series of three technical reports describing aspects of the minimum encoding approach to inference. This report was written with the objective of providing an introduction to minimum encoding inference for statisticians. We concentrate on the Wallace approach <ref> [22, 23, 24, 25] </ref> | for details of the Rissanen approach see [18, 19, 20]. The second report (Oliver and Baxter [13]) describes similarities and differences between MML inference and Bayesian inference.
Reference: [23] <author> C.S. Wallace and D.M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: This technical report is the first in a series of three technical reports describing aspects of the minimum encoding approach to inference. This report was written with the objective of providing an introduction to minimum encoding inference for statisticians. We concentrate on the Wallace approach <ref> [22, 23, 24, 25] </ref> | for details of the Rissanen approach see [18, 19, 20]. The second report (Oliver and Baxter [13]) describes similarities and differences between MML inference and Bayesian inference. <p> AOP V gives us: @E (M essLen) @AOP V 1 + 12 2 (10) Setting @E (MessLen) @AOP V to 0 and solving for AOP V gives us the optimal value: AOP V = 12 (11) Using a similar approach, Wallace and Boulton <ref> [23] </ref> established that the optimal value for (and hence the MML estimate for ) is the unbiased estimate of the standard deviation: = s (12) and that the optimal value of AOP V is 10 : AOP V = s 6 (13) The minimum expected message length is then approximately: E <p> Furthermore, the message length in this region varied from 97.86 bits (when = m; = s) to 103.61 bits (when = 176:09; = 3:99). 10 In the derivation of this result, Equation (27) <ref> [23, Page 188] </ref> should read c 2 12w 2 = 1 OLIVER and HAND, TR 4-94 Page 16 3.3.4 Sending AOPVs The two part messages used in this section cannot be interpreted unless the receiver knows in advance the AOPVs used by the sender. <p> In this section, we consider codes for non-regular structures such as classification trees 11 [5] and finite state automata [8, 9]. MML has been applied to a range of other non-regular problems including Clustering <ref> [23, 4] </ref>, DNA string alignment [1] and Decision Graphs [12]. 4.1 Classification Trees 4.1.1 Description of Classification Trees A classification tree is a non-parametric model which is used to perform classification. A tree partitions an object space into regions, and associates a probability for each class with each region.
Reference: [24] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 49 </volume> <pages> 240-252, </pages> <year> 1987. </year>
Reference-contexts: This technical report is the first in a series of three technical reports describing aspects of the minimum encoding approach to inference. This report was written with the objective of providing an introduction to minimum encoding inference for statisticians. We concentrate on the Wallace approach <ref> [22, 23, 24, 25] </ref> | for details of the Rissanen approach see [18, 19, 20]. The second report (Oliver and Baxter [13]) describes similarities and differences between MML inference and Bayesian inference. <p> Using the principle of indifference, we assume that a parameter value is uniformly distributed locally around the sample parameter value. In particular, we assume that a parameter value is uniformly distributed in a region which is of size AOPV of that parameter <ref> [24, page 244] </ref>. For example, we assume that is uniformly distributed in the region [m 2 AOP V ] and hence we view it as being equally likely that we code as 178, as it is that we code as 174. <p> Hence, we must consider three part codes, the first part describes the AOPVs, in addition to the model part, and the data part. This would appear to lead to an infinite regress, since now we need a coding scheme to describe AOPVs. Wallace and Freeman <ref> [24, page 245] </ref> argue that in many cases a three part code is not necessary. They offer methods to avoid including a preamble describing the accuracy of the AOPVs. 4 Codes for Non-Regular Structures The basic principles of MML can be applied to a wide range of model structures. <p> Wallace and Freeman <ref> [24, page 241] </ref> take a Bayesian interpretation of Minimum Encoding Inference and conclude: "there can be no substitute for careful specification of whatever prior knowledge is available" Rissanen [20] acknowledges the relation between coding and Bayesian priors. <p> We then examined some non-regular problems (classification trees and probabilistic finite state automata), and presented coding scheme for these problems. Wallace and Freeman <ref> [24, page 252] </ref> conclude that: "It is in completely non-regular problems that the minimum message length approach will produce most strikingly different and, we believe, practically useful results". 8 Acknowledgments This work was carried out with the support of the Defence Research Agency, Malvern.
Reference: [25] <author> C.S. Wallace and J.D. Patrick. </author> <title> Coding decision trees. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 7-22, </pages> <year> 1993. </year> <title> OLIVER and HAND, </title> <type> TR 4-94 Page 24 </type>
Reference-contexts: This technical report is the first in a series of three technical reports describing aspects of the minimum encoding approach to inference. This report was written with the objective of providing an introduction to minimum encoding inference for statisticians. We concentrate on the Wallace approach <ref> [22, 23, 24, 25] </ref> | for details of the Rissanen approach see [18, 19, 20]. The second report (Oliver and Baxter [13]) describes similarities and differences between MML inference and Bayesian inference. <p> The encoding is most efficient when P L is the proportion of leaves in the tree, and P S is the proportion of splits in the tree <ref> [25] </ref>. Using this encoding method results in a message of length 17:16 bits. 4.2 Probabilistic Finite State Automata 4.2.1 Description of Probabilistic Finite State Automata A Probabilistic Finite State Automata (PFSA) is a state machine which can be used to explain how a string was generated.
References-found: 25

