URL: http://www.cs.berkeley.edu/~dfb/papers/cachepad.ps
Refering-URL: http://www.cs.berkeley.edu/~dfb/publist.html
Root-URL: 
Title: A Compiler Framework for Restructuring Data Declarations to Enhance Cache and TLB Effectiveness  
Author: David F. Bacon Jyh-Herng Chow Dz-ching R. Ju Kalyan Muthukumar Vivek Sarkar 
Address: 555 Bailey Avenue, San Jose, CA 95141  
Affiliation: Application Development Technology Institute IBM Software Solutions Division  
Abstract: It has been observed that memory access performance can be improved by restructuring data declarations, using simple transformations such as array dimension padding and inter-array padding (array alignment) to reduce the number of misses in the cache and TLB (translation lookaside buffer). These transformations can be applied to both static and dynamic array variables. In this paper, we provide a padding algorithm for selecting appropriate padding amounts, which takes into account various cache and TLB effects collectively within a single framework. In addition to reducing the number of misses, we identify the importance of reducing the impact of cache miss jamming by spreading cache misses more uniformly across loop iterations. We translate undesirable cache and TLB behaviors into a set of constraints on padding amounts and propose a heuristic algorithm of polynomial time complexity to find the padding amounts to satisfy these constraints. The goal of the padding algorithm is to select padding amounts so that there are no set conflicts and no offset conflicts in the cache and TLB, for a given loop. In practice, this algorithm can efficiently find small padding amounts to satisfy these constraints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> PF77 Shared Memory Parallelizer and Pre Compiler for the IBM POWER/4 System User's Guide (Preliminary Draft), </institution> <year> 1993. </year> <note> Applied Parallel Research. </note>
Reference-contexts: These techniques are not new. For example, dimension padding has been proposed in [2] to reduce cache set conflicts for a single array reference in a loop nest, and alignment of arrays to cache line boundaries has been suggested to reduce false sharing <ref> [10, 1] </ref>. While some of the issues discussed in this paper have been addressed in isolation by past work, individual consideration of different cache and translation lookaside buffer (TLB) related performance problems can lead to conflicting solutions.
Reference: [2] <author> David Bailey. </author> <title> Unfavorable Strides in Cache Memory Systems. </title> <type> Technical Report RNR-92-015, </type> <institution> NASA Ames Research Center, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: The two primary techniques for data declaration restructuring are array dimension padding (changing the array size) and inter-array padding (repositioning the array starting ad dress). These techniques are not new. For example, dimension padding has been proposed in <ref> [2] </ref> to reduce cache set conflicts for a single array reference in a loop nest, and alignment of arrays to cache line boundaries has been suggested to reduce false sharing [10, 1]. <p> Subsection 5.1 discusses extensions to Algorithm 1 for taking stride efficiency into account. 5.1 Stride Efficiency The issue of stride efficiency has been addressed in past work, e.g. <ref> [2] </ref>. We have extended Algorithm 1 to take stride efficiency into account. <p> Definition 9 (Cache Efficiency Factor of Ref j (X i )) An efficiency factor is defined as Y =(SA), where Y is the number of cache lines that still remain in the cache after SA iterations. We use the function proposed in <ref> [2] </ref> to approximate the cache efficiency factor. Suppose s = Stride (Ref j (X i )), and s=(SW ) is very close to a simple fraction a=b, b S so that D = jbs aSW j is small. <p> An extreme case of a stride with a low efficiency is a large power of two, for which all the accessed cache lines fall into the same cache set. However, non-power-of-two strides can also have poor efficiency [3]. See <ref> [2] </ref> for details. A low efficiency factor for Ref j (X i ) indicates that the cache lines containing the accessed elements of array X i fall into a small number of cache sets.
Reference: [3] <author> Jeanne Ferrante, Vivek Sarkar, and Wendy Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness. </title> <booktitle> Lecture Notes in Computer Science, (589), 1991. Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, California, USA, </address> <month> August </month> <year> 1991. </year> <title> Edited by U. </title> <editor> Banerjee, D. Gelernter, A. Nicolau, D. </editor> <address> Padua. </address>
Reference-contexts: It is also known that cache misses can be reduced, under the constraints of limited cache size and associativity, by improving program locality through program restructuring, in particular loop nest restructuring, since loops often contain most program computation (as in <ref> [12, 5, 3] </ref>). For example, loop blocking (tiling) has been used so that cached data can be reused before they are flushed out by other competing accesses for the same cache set (cache interference in [7]). <p> An extreme case of a stride with a low efficiency is a large power of two, for which all the accessed cache lines fall into the same cache set. However, non-power-of-two strides can also have poor efficiency <ref> [3] </ref>. See [2] for details. A low efficiency factor for Ref j (X i ) indicates that the cache lines containing the accessed elements of array X i fall into a small number of cache sets.
Reference: [4] <author> Elana D. Granston. </author> <title> Toward a Compile Time Methodology for Reducing False Sharing and Communication Traffic in Shared Virtual Memory Systems. </title> <booktitle> In Proc. of Sixth Workshop on Language and Compilers for Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: For example, loop blocking (tiling) has been used so that cached data can be reused before they are flushed out by other competing accesses for the same cache set (cache interference in [7]). Loop blocking has also been used to eliminate false sharing in shared memory multiprocessors <ref> [4] </ref>. These loop restructuring techniques assume a fixed, known data layout for the program and try to restructure the access pattern in a loop nest to match the given data layout. This paper investigates another dimension of improving cache effectiveness by restructuring data declarations, and thus changing the data layout.
Reference: [5] <author> Manish Gupta and David A. Padua. </author> <title> Effects of Program Parallelization and Stripmining Transformation on Cache Performance in a Multiprocessor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages I.301-I.304, </pages> <year> 1991. </year>
Reference-contexts: It is also known that cache misses can be reduced, under the constraints of limited cache size and associativity, by improving program locality through program restructuring, in particular loop nest restructuring, since loops often contain most program computation (as in <ref> [12, 5, 3] </ref>). For example, loop blocking (tiling) has been used so that cached data can be reused before they are flushed out by other competing accesses for the same cache set (cache interference in [7]).
Reference: [6] <author> High Performance Fortran Forum. </author> <title> High Per formance Fortran. Language Specification Version 0.4, </title> <month> December </month> <year> 1992. </year>
Reference-contexts: Otherwise, the framework may decide to seek a solution that satisfies the most important constraints. We observe that the legality conditions for performing these storage transformations (no sequence association or storage association) are similar to the legality conditions for redistributing arrays in the High Performance Fortran language <ref> [6] </ref>. Based on the design that debugging information is generated after data declaration restructuring, these storage transformations have no adverse effect on program debugging. Based on this framework, we present algorithms that solve several problems that were found to have significant impact on cache and TLB effectiveness.
Reference: [7] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In ACM International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <year> 1991. </year>
Reference-contexts: For example, loop blocking (tiling) has been used so that cached data can be reused before they are flushed out by other competing accesses for the same cache set (cache interference in <ref> [7] </ref>). Loop blocking has also been used to eliminate false sharing in shared memory multiprocessors [4]. These loop restructuring techniques assume a fixed, known data layout for the program and try to restructure the access pattern in a loop nest to match the given data layout. <p> By loop restructuring, cache and TLB performance can be optimized for each individual loop nest, but this approach is confined to the existing data layout. There have been studies that combine these two approaches. Data copying is a technique that has been suggested in <ref> [7, 9] </ref>. Instead of using a fixed array layout in every loop nest, the compiler can allocate specialized temporary arrays for each individual loop nest that will exhibit better cache behavior than the original arrays.
Reference: [8] <author> D. H. Lawrie and C. R. Vora. </author> <title> The prime mem ory system for array access. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-31(5):435-442, </volume> <month> October </month> <year> 1982. </year>
Reference-contexts: Therefore, by adjusting the cache offsets, cache misses can be distributed. We note that the cache offset conflict problem is very similar to the memory bank conflict problem on vector machines. However, our data declaration restructuring framework is different from the frameworks, such as a prime memory system <ref> [8] </ref>, which have been proposed to tackle the memory bank conflict problem. Cache efficiency factor: Cache efficiency factor is a measure of cache utilization by examining the spread of referenced cache lines in the entire cache for a single syntactic array reference in a loop nest.
Reference: [9] <author> Olivier Temam, Elana Granston, and William Jalby. </author> <title> To Copy or Not to Copy: A Compile-Time Technique for Assessing When Data Copying Should be Used to Eliminate Cache Conflicts. </title> <booktitle> In Proc. Supercomputing '93, </booktitle> <pages> pages 410-419, </pages> <year> 1993. </year>
Reference-contexts: By loop restructuring, cache and TLB performance can be optimized for each individual loop nest, but this approach is confined to the existing data layout. There have been studies that combine these two approaches. Data copying is a technique that has been suggested in <ref> [7, 9] </ref>. Instead of using a fixed array layout in every loop nest, the compiler can allocate specialized temporary arrays for each individual loop nest that will exhibit better cache behavior than the original arrays.
Reference: [10] <author> Josep Torrellas, Monica S. Lam, and John L. Hennessy. </author> <title> Shared Data Placement Optimizations to Reduce Multiprocessor Cache Miss Rates. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages II.266-II.270, </pages> <year> 1990. </year>
Reference-contexts: These techniques are not new. For example, dimension padding has been proposed in [2] to reduce cache set conflicts for a single array reference in a loop nest, and alignment of arrays to cache line boundaries has been suggested to reduce false sharing <ref> [10, 1] </ref>. While some of the issues discussed in this paper have been addressed in isolation by past work, individual consideration of different cache and translation lookaside buffer (TLB) related performance problems can lead to conflicting solutions.
Reference: [11] <author> Steven W. White. POWER2: </author> <title> Architecture and Performance. </title> <booktitle> In Proceedings of COMP-CON '94, </booktitle> <pages> pages 384-388, </pages> <year> 1994. </year>
Reference-contexts: Jamming occurs on sophisticated architectures that allow processing to continue during a cache miss, because only a small number of outstanding misses are allowed before further instruction dispatch must be stalled. For example, on the IBM Power2 <ref> [11] </ref> all memory references are issued by the two fixed-point units, each of which can have a single outstanding load without causing an interlock. The latency for a load with a cache miss is 8 cycles. If another miss occurs before the first miss is completed, the CPU will stall. <p> However, our solution for eliminating cache set conflicts is applicable to physically-addressed caches, if the operating system favors the mapping of consecutive virtual memory pages to consecutive physical memory pages. We use the IBM RISC/6000 model 590 system (built on the Power2 processor architecture <ref> [11] </ref>) as the target architecture in all our examples because the Power2/590 is a state-of-the-art superscalar processor with a high cache bandwidth. However, our results are applicable to other modern cache-based machine architectures as well. <p> Example 1 Consider the Power2 architecture, which contains a 256KB data cache that is 4-way set-associative with 256 sets and 256 bytes per cache line <ref> [11] </ref>. Consider the following loop nest where the six memory locations accessed in the first iteration are mapped to different cache lines of the same cache set. Assume all variables of data type REAL*4 unless stated otherwise.
Reference: [12] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In ACM Sigplan Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <year> 1991. </year>
Reference-contexts: It is also known that cache misses can be reduced, under the constraints of limited cache size and associativity, by improving program locality through program restructuring, in particular loop nest restructuring, since loops often contain most program computation (as in <ref> [12, 5, 3] </ref>). For example, loop blocking (tiling) has been used so that cached data can be reused before they are flushed out by other competing accesses for the same cache set (cache interference in [7]).
References-found: 12

