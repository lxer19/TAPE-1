URL: file://ftp.cs.utexas.edu/pub/mooney/papers/wolfie-proposal-95.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/cthomp/pubs.html
Root-URL: 
Email: cthomp@cs.utexas.edu  
Title: Corpus-Based Lexical Acquisition For Semantic Parsing  
Author: Cynthia A. Thompson Supervising Professor: Dr. Raymond J. Mooney 
Date: February 7, 1996  
Address: 2.124 Taylor Hall Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Abstract: Building accurate and efficient natural language processing (NLP) systems is an important and difficult problem. There has been increasing interest in automating this process. The lexicon, or the mapping from words to meanings, is one component that is typically difficult to update and that changes from one domain to the next. Therefore, automating the acquisition of the lexicon is an important task in automating the acquisition of NLP systems. This proposal describes a system, Wolfie (WOrd Learning From Interpreted Examples), that learns a lexicon from input consisting of sentences paired with representations of their meanings. Preliminary experimental results show that this system can learn correct and useful mappings. The correctness is evaluated by comparing a known lexicon to one learned from the training input. The usefulness is evaluated by examining the effect of using the lexicon learned by Wolfie to assist a parser acquisition system, where previously this lexicon had to be hand-built. Future work in the form of extensions to the algorithm, further evaluation, and possible applications is discussed. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J. F. </author> <year> (1995). </year> <title> Natural Language Understanding. </title> <address> Benjamin/Cummings, Menlo Park, CA. </address>
Reference-contexts: Verbs and nouns should be organized into different types of hierarchy: verbs in a "manner-of" hierarchy, and nouns in a hyponymy (subordination) hierarchy, as in WordNet. Other types of structured lexicons are possible, such as organizing verbs by their subcategorization frames <ref> (Allen, 1995) </ref>, as in Brent (1991). For example, where currently only [ingest] is learned as the meaning of ate, the system could also learn that it takes an animate agent and optional patient. This information would also further help Chill with its learning process.
Reference: <author> Anderson, J. R. </author> <year> (1977). </year> <title> Induction of augmented transition networks. </title> <journal> Cognitive Science, </journal> <volume> 1, </volume> <pages> 125-157. </pages>
Reference: <author> Beckwith, R., Fellbaum, C., Gross, D., & Miller, G. </author> <year> (1991). </year> <title> Wordnet: A lexical database organized on psycholinguistic principles. </title> <editor> In Zernik, U. (Ed.), </editor> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, </title> <journal> pp. </journal> <pages> 211-232. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Other approaches to the lexical acquisition problem depend on knowledge of syntax to assist in lexical learning (Berwick & Pilato, 1987). Most systems (Brent, 1991) have not demonstrated the ability to tie in to the rest of a language learning system. Though there are existing computational lexicons (e.g., WordNet, <ref> (Beckwith, Fellbaum, Gross, & Miller, 1991) </ref>) and on-line dictionaries (e.g., Longman Dictionary of Contemporary English, (Proctor, 1978)), automated lexical acquisition is important for several reasons. First, language is constantly changing: new words are created and additional senses are added to existing words. <p> The current research was an attempt to see what could be done without any background knowledge. However, the system should be able to gain even more power by exploiting such resources. For example, the WordNet database <ref> (Beckwith et al., 1991) </ref> could be used to help break ties between multiple (word; meaning) pairs.
Reference: <author> Berwick, B. </author> <year> (1985). </year> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Berwick, R. </author> <year> (1983). </year> <title> Learning word meanings from examples. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 459-461. </pages>
Reference: <author> Berwick, R. C., & Pilato, S. </author> <year> (1987). </year> <title> Learning syntax by automata induction. </title> <journal> Machine Learning, </journal> <volume> 2 (1), </volume> <pages> 9-38. </pages>
Reference-contexts: Many current systems (Merialdo, 1994; Charniak, Hendrickson, Jacobson, & Perkowitz, 1993) learn to tag the syntactic categories of words with no claim to learning a deeper meaning. Other approaches to the lexical acquisition problem depend on knowledge of syntax to assist in lexical learning <ref> (Berwick & Pilato, 1987) </ref>. Most systems (Brent, 1991) have not demonstrated the ability to tie in to the rest of a language learning system.
Reference: <author> Brent, M. </author> <year> (1990). </year> <title> Semantic classification of verbs from their syntactic contexts: Automated lexicography with implications for child language acquisition. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. 428-437. </pages>
Reference: <author> Brent, M. </author> <year> (1991). </year> <title> Automatic acquisition of subcategorization frames from untagged text. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 209-214. </pages> <note> 34 Brill, </note> <author> E. </author> <year> (1993). </year> <title> Automatic grammar induction and parsing free text: A transformation--based approach. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 259-265 Columbus, Ohio. </address>
Reference-contexts: Other approaches to the lexical acquisition problem depend on knowledge of syntax to assist in lexical learning (Berwick & Pilato, 1987). Most systems <ref> (Brent, 1991) </ref> have not demonstrated the ability to tie in to the rest of a language learning system.
Reference: <author> Brunk, C., & Pazzani, M. </author> <year> (1995). </year> <title> A lexically based semantic bias for theory revision. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 81-89 San Francisco, CA. </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> A case-based apprach to knowledge acquisition for domain-specific sentence analysis. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. 798-803. </pages>
Reference: <author> Cartwright, T., & Brent, M. </author> <year> (1994). </year> <title> Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition. </title> <booktitle> In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society Hillsdale, </booktitle> <address> NJ. </address>
Reference: <author> Charniak, E. </author> <year> (1993). </year> <title> Statistical Language Learning. </title> <publisher> MIT Press. </publisher>
Reference: <author> Charniak, E., Hendrickson, C., Jacobson, N., & Perkowitz, M. </author> <year> (1993). </year> <title> Equations for part-of-speech tagging. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 784-789 Washington, D.C. </address>
Reference: <author> Church, </author> & <title> Hanks (1990). Word association norms, mutual information and lexicography. </title> <journal> Computational Linguistics, </journal> <volume> 16 (1), </volume> <pages> 22-29. </pages> <address> de Marcken, C. </address> <year> (1995). </year> <title> Acquiring a lexicon from unsegmented speech. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 311-313 Cambridge, MA. </address>
Reference: <author> Dyer, M. </author> <year> (1991). </year> <title> Lexical acquisition through symbol recirculation in distributed connectionist networks. </title> <editor> In Zernik, U. (Ed.), </editor> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, </title> <journal> pp. </journal> <pages> 309-337. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Feldman, J., Lakoff, G., Stolke, A., & Weber, S. </author> <year> (1990). </year> <title> Miniature language acquisition: A touchstone for cognitive science. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 686-693 Cambridge, MA. </address>
Reference-contexts: Some work attempting to ground representations more firmly in the world has been attempted (Siskind, 1994; Feldman, Lakoff, Stolke, & Weber, 1990). In the Miniature Language Acquisition task <ref> (Feldman et al., 1990) </ref>, the work done to date splits up the task into several subparts. In one part (Regier, 1991), the network learns to associate scenes (represented by line drawings) with a spatial term.
Reference: <author> Fillmore, C. J. </author> <year> (1968). </year> <title> The case for case. In Bach, </title> <editor> E., & Harms, R. T. (Eds.), </editor> <booktitle> Universals in Linguistic Theory. </booktitle> <publisher> Holt, Reinhart and Winston, </publisher> <address> New York. </address>
Reference-contexts: The parsing formalism used is a shift-reduce parser. For example, Chill can learn to parse sentences into case-role representations when given a sample of sentence/case-role pairings <ref> (Fillmore, 1968) </ref> and background knowledge providing an overly general parsing template. As a second example, Chill can learn to parse sentences into a database query language when given a sample of sentence/query pairs and a lexicon of the acceptable commands of the query language.
Reference: <author> Fisher, C. </author> <year> (1994). </year> <title> When it is better to receive than to give: Syntactic and conceptual constraints on vocabulary growth. </title> <editor> In Gleitman, L., & Landau, B. (Eds.), </editor> <booktitle> The Acquisition of the Lexicon, </booktitle> <pages> pp. 333-375. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Children do not likely memorize all inputs and then pick some cut off point at which to generalize over them. More likely, salient features are remembered and recalled the next time a similar input is present. Also, lexical learning most likely does not take place in isolation <ref> (Fisher, 1994) </ref>. At the same time, concepts and syntax are being acquired. Using feedback between the different modules, the learning task may become more constrained and thus simplified.
Reference: <author> Fukumoto, F., & Tsujii, J. </author> <year> (1995). </year> <title> Representation and acquisition of verbal polysemy. </title> <booktitle> In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, </booktitle> <pages> pp. </pages> <address> 39-44 Stanford, CA. </address>
Reference: <author> Gazdar, G., & Mellish, C. </author> <year> (1989). </year> <title> Natural Language Processing in Prolog. </title> <publisher> Adison-Wesley Publishing Company, </publisher> <address> New York. </address> <note> 35 Granger, </note> <author> R. </author> <year> (1977). </year> <title> FOUL-UP: a program that figures out meanings of words from context. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial intelligence, </booktitle> <pages> pp. 172-178. </pages>
Reference: <author> Grimshaw, J. </author> <year> (1981). </year> <title> Form, function, and the language acquisition device. </title> <editor> In Baker, C., & McCarthy, J. (Eds.), </editor> <booktitle> The logical problem of language acquisition, </booktitle> <pages> pp. 165-182. </pages> <publisher> M.I.T. Press, </publisher> <address> Cambridge, MA and London, England. </address>
Reference: <author> Haruno, M. </author> <year> (1995). </year> <title> A case frame learning method for japanese polysemous verbs. </title> <booktitle> In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, </booktitle> <pages> pp. </pages> <address> 45-50 Stanford, CA. </address>
Reference: <author> Hastings, P., & Lytinen, S. </author> <year> (1994). </year> <title> The ups and downs of lexical acquisition. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 754-759. </pages>
Reference: <author> Hoffmann, C., & O'Donnell, M. </author> <year> (1982). </year> <title> Pattern matching in trees. </title> <journal> Journal of the ACM, </journal> <volume> 29 (1), </volume> <pages> 68-95. </pages>
Reference: <author> Jackendoff, R. </author> <year> (1990). </year> <title> Semantic Structures. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Kay, M., & Roescheisen, M. </author> <year> (1993). </year> <title> Text-translation alignment. </title> <journal> Computational Linguistics, </journal> <volume> 19 (1), </volume> <pages> 121-142. </pages>
Reference: <author> Kazman, R. </author> <year> (1994). </year> <title> Simulating the child's acquisition of the lexicon and syntax-experiences with Babel. </title> <journal> Machine Learning, </journal> <volume> 16, </volume> <pages> 87-120. </pages>
Reference: <author> Landau, B. </author> <year> (1994). </year> <title> Where's what and what's where: the language of objects in space. </title> <editor> In Gleitman, L., & Landau, B. (Eds.), </editor> <booktitle> The Acquisition of the Lexicon, </booktitle> <pages> pp. 259-296. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Second is the cognitive motivation. When learning language, children have access to spoken sentences together with some sensory input, part of which typically corresponds to the meaning of those sentences. It has been hypothesized <ref> (Landau, 1994) </ref> that children associate utterances with the co-occurring extra-linguistic context in order to narrow down the number of possible meanings for the words in the utterance. The second thing available to the learner is a procedure, C, for breaking down and building up sentence meanings.
Reference: <author> Langley, P. </author> <year> (1994). </year> <title> Simplicity and representation change in grammar induction. </title> <type> unpublished manuscript. </type>
Reference: <author> Lebowitz, M. </author> <year> (1987). </year> <title> Experiments with incremental concept formation: </title> <journal> UNIMEM. Machine Learning, </journal> <volume> 2 (2), </volume> <pages> 103-138. </pages>
Reference-contexts: A tree-matching algorithm could handle this, however, by turning each labeled arc into a labeled node and putting new unlabeled arcs between it and the previously attached nodes. Other systems which deal with finding generalizations of structured objects include Oc-cam (Pazzani, 1985), UNIMEM <ref> (Lebowitz, 1987) </ref>, and Labyrinth (Thompson & Langley, 1991). 5 Algorithm Description and Example Several approaches could be taken to solve the lexical acquisition problem defined in Section 3.
Reference: <author> Leech, G. </author> <year> (1974). </year> <title> Semantics. </title> <publisher> Penguin Books Inc. </publisher>
Reference: <author> Magerman, D. M. </author> <year> (1994). </year> <title> Natrual Lagnuage Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. thesis, </type> <institution> Stanford University. </institution>
Reference: <author> Markman, E. </author> <year> (1994). </year> <title> Constraints on word meaning in early language acquisition. </title> <editor> In Gleit-man, L., & Landau, B. (Eds.), </editor> <booktitle> The Acquisition of the Lexicon, </booktitle> <pages> pp. 199-207. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Our method eliminates this problem by allowing multiple meanings for one word. To aid a learning system using input closer to what a child experiences, certain assumptions common in the literature could be used to restrict the search. These include <ref> (Markman, 1994) </ref> * the whole-object assumption: terms refer to objects as a whole rather than their parts or other properties, * the taxonomic assumption: words are likely to be extendible to objects or entities of like kind, and * the mutual exclusivity assumption: two labels should not be used for the
Reference: <author> McClelland, J. L., & Kawamoto, A. H. </author> <year> (1986). </year> <title> Mechanisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Merialdo, B. </author> <year> (1994). </year> <title> Tagging English text with a probabilistic model. </title> <journal> Computational Linguistics, </journal> <volume> 20 (2), </volume> <pages> 155-172. </pages>
Reference: <author> Miikkulainen, R. </author> <year> (1993). </year> <title> Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Miller, G., Beckwith, R., Fellbaum, C., Gross, D., & Miller, K. </author> <year> (1993). </year> <title> Introduction to WordNet: An on-line lexical database. </title> <note> Available by ftp to clarity.princeton.edu. </note>
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 281-297. </pages> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: TLGGs between pairs of sentence representations are used to help narrow down the number of hypothesized meanings for a word. This effectively builds generalizations of sentence representations, in a manner similar to Golem <ref> (Muggleton & Feng, 1992) </ref>. Each member in the initial set of hypothesized meanings for a word must appear in at least two representations of sentences in which the word appears This means the search is significantly narrower than that of the histogram method.
Reference: <author> Pazzani, M. J. </author> <year> (1985). </year> <title> Explanation and generalization based memory. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 323-328 Irvine, CA. </address>
Reference-contexts: A tree-matching algorithm could handle this, however, by turning each labeled arc into a labeled node and putting new unlabeled arcs between it and the previously attached nodes. Other systems which deal with finding generalizations of structured objects include Oc-cam <ref> (Pazzani, 1985) </ref>, UNIMEM (Lebowitz, 1987), and Labyrinth (Thompson & Langley, 1991). 5 Algorithm Description and Example Several approaches could be taken to solve the lexical acquisition problem defined in Section 3.
Reference: <author> Pinker, S. </author> <year> (1993). </year> <title> Resolving a learnability paradox in the acquisition of the verb lexicon. </title> <editor> In Rice, M., & Schiefelbusch, R. (Eds.), </editor> <booktitle> The Teachability of Language, </booktitle> <pages> pp. 13-61. </pages> <editor> Paul H. </editor> <publisher> Brookes Publishing Co., Inc. </publisher>
Reference-contexts: However, some discussion of current theories for human language learning is relevant. Pinker <ref> (Pinker, 1993) </ref> 32 proposes the thematic core theory for acquisition of verb meanings. Here, there are linkages between semantic and syntactic structures.
Reference: <author> Plotkin, G. D. </author> <year> (1970). </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., & Michie, D. (Eds.), </editor> <booktitle> Machine Intelligence (Vol. </booktitle> <volume> 5). </volume> <publisher> Elsevier North-Holland, </publisher> <address> New York. </address>
Reference: <author> Proctor, P. (Ed.). </author> <year> (1978). </year> <title> Longman Dictionary of Contemporary English. </title> <publisher> Longman Group, Harlow, Essex, </publisher> <address> UK. </address>
Reference-contexts: Most systems (Brent, 1991) have not demonstrated the ability to tie in to the rest of a language learning system. Though there are existing computational lexicons (e.g., WordNet, (Beckwith, Fellbaum, Gross, & Miller, 1991)) and on-line dictionaries (e.g., Longman Dictionary of Contemporary English, <ref> (Proctor, 1978) </ref>), automated lexical acquisition is important for several reasons. First, language is constantly changing: new words are created and additional senses are added to existing words. Second, existing lexicons are often customized to one domain, and new domains require slightly or even radically different meanings for many words.
Reference: <author> Regier, T. </author> <year> (1991). </year> <title> Learning spatial concepts using a partially-structured connectionist architecture. </title> <type> Tech. rep. </type> <institution> TR-91-050, Berkeley. </institution>
Reference-contexts: Some work attempting to ground representations more firmly in the world has been attempted (Siskind, 1994; Feldman, Lakoff, Stolke, & Weber, 1990). In the Miniature Language Acquisition task (Feldman et al., 1990), the work done to date splits up the task into several subparts. In one part <ref> (Regier, 1991) </ref>, the network learns to associate scenes (represented by line drawings) with a spatial term.
Reference: <author> Riloff, E. </author> <year> (1993). </year> <title> Automatically constructing a dictionary for information extraction tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. 811-816. </pages>
Reference: <author> Salveter, S. </author> <year> (1979). </year> <title> Inferring conceptual graphs. </title> <journal> Cognitive Science, </journal> <volume> 3 (2), </volume> <pages> 151-166. </pages>
Reference: <author> Salveter, S. </author> <year> (1982). </year> <title> Inferring building blocks for knowledge representation. </title> <editor> In Lehnert, W., & Ringle, M. (Eds.), </editor> <booktitle> Strategies for Natural Language Processing, </booktitle> <pages> pp. 327-344. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Schank, R. C. </author> <year> (1975). </year> <title> Conceptual Information Processing. </title> <publisher> North-Holland, Oxford. </publisher>
Reference-contexts: Wolfie (WOrd Learning From Interpreted Examples) learns this mapping from training examples consisting of sentences paired with their semantic representations. The semantic representation currently used is a tree-based representation, derived from the representational theory of Conceptual Dependency (CD) <ref> (Schank, 1975) </ref>. The output of the system can be used to assist a larger language acquisition system; in particular, it is currently used as part of the input to Chill (Zelle & Mooney, 1993), a parser acquisition system.
Reference: <author> Selfridge, M. </author> <year> (1986). </year> <title> A computer model of child language learning. </title> <journal> Artificial Intelligence, </journal> <volume> 29 (2). </volume>
Reference: <author> Siskind, J. M. </author> <year> (1992). </year> <title> Naive Physics, Event Perception, Lexical Semantics and Language Acquisition. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference-contexts: In a first-order logical representation, something similar to Siskind's fracturing technique could be used <ref> (Siskind, 1992) </ref>. For the current representation, trees are broken up into all connected subgraphs.
Reference: <author> Siskind, J. M. </author> <year> (1994). </year> <title> Lexical acquisition in the presence of noise and homonymy. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 760-766. </pages>
Reference-contexts: In the current formalism, the lexical entry for each word is contained in the representation for the sentence. This means that these entries are somehow implicitly known in order to build the training sentences. This is an assumption commonly made in the literature <ref> (Siskind, 1994) </ref> and is a reasonable assumption from which to begin. Note, however, that it is initially unknown which pieces of the sentence representation are due to which words in the sentence. This is what the algorithm discovers. There is, perhaps, a more satisfying way to address the above issue.
Reference: <author> Stolke, A. </author> <year> (1990). </year> <title> Learning feature-based semantics with simple recurrent networks. </title> <type> Tech. rep. TR-90-015, </type> <institution> International Computer Science Institute, Berkely, </institution> <address> CA. </address>
Reference-contexts: In the Miniature Language Acquisition task (Feldman et al., 1990), the work done to date splits up the task into several subparts. In one part (Regier, 1991), the network learns to associate scenes (represented by line drawings) with a spatial term. In another part <ref> (Stolke, 1990) </ref>, the network learns to extract semantic representations that could conceivably be output from a vision system when given input in the form of a sequence of words. 9 Related Work There are two predominant views of the lexical acquisition task to be found in the related literature.
Reference: <author> Suppes, P., Liang, L., & Bottner, M. </author> <year> (1991). </year> <title> Complexity issues in robotic machine learning of natural language. In Lam, </title> <editor> L., & Naroditsky, V. (Eds.), </editor> <booktitle> Modeling Complex Phenomena, Proceedings of the 3rd Woodward Conference, </booktitle> <pages> pp. 102-127. </pages> <publisher> Springer-Verlag. </publisher>
Reference: <author> Thompson, K., & Langley, P. </author> <year> (1991). </year> <title> Concept formation in structured domains. </title> <editor> In Fisher, D., Pazzani, M., & Langley, P. (Eds.), </editor> <title> Concept Formation: </title> <journal> Knowledge and Experience in Unsupervised Learning, </journal> <pages> pp. 127-161. </pages> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: A tree-matching algorithm could handle this, however, by turning each labeled arc into a labeled node and putting new unlabeled arcs between it and the previously attached nodes. Other systems which deal with finding generalizations of structured objects include Oc-cam (Pazzani, 1985), UNIMEM (Lebowitz, 1987), and Labyrinth <ref> (Thompson & Langley, 1991) </ref>. 5 Algorithm Description and Example Several approaches could be taken to solve the lexical acquisition problem defined in Section 3.
Reference: <author> Tishby, N., & Gorin, A. </author> <year> (1994). </year> <title> Algebraic learning of statistical associations for language acquisition. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8, </volume> <pages> 51-78. </pages>
Reference: <author> Velardi, P. </author> <year> (1991). </year> <title> Acquiring a semantic lexicon for natural language processing. </title> <editor> In Zernik, U. (Ed.), </editor> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, </title> <journal> pp. </journal> <pages> 341-367. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Wolff, J. G. </author> <year> (1987). </year> <title> Cognitive development as optimisation. </title> <editor> In Bolc, L. (Ed.), </editor> <booktitle> Computational Models of Learning. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Zelle, J. M. </author> <year> (1995). </year> <title> Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. </title> <type> Ph.D. thesis, </type> <institution> University of Texas, Austin, TX. </institution>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1993). </year> <title> Learning semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 817-822 Washington, D.C. </address>
Reference-contexts: The semantic representation currently used is a tree-based representation, derived from the representational theory of Conceptual Dependency (CD) (Schank, 1975). The output of the system can be used to assist a larger language acquisition system; in particular, it is currently used as part of the input to Chill <ref> (Zelle & Mooney, 1993) </ref>, a parser acquisition system. Currently, Chill requires a word/meaning lexicon as background knowledge in order to learn to parse into deep semantic representations. By using Wolfie, one of the inputs to Chill is automatically provided, thus easing the task of parser acquisition.
Reference: <author> Zernick, U. </author> <year> (1991). </year> <title> Train1 vs. train2: Tagging word senses in corpus. </title> <editor> In Zernik, U. (Ed.), </editor> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, </title> <journal> pp. </journal> <pages> 91-112. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address> <month> 38 </month>
References-found: 59

