URL: http://www.clark.net/pub/kainen/not-cont.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00449.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Approximation by neural networks is not continuous  
Author: Paul C. Kainen Vera K-urkova Andrew Vogt 
Keyword: nonlinear approximation, one-hidden-layer neural networks, rates of approximation, continuous selection, metric projection, proximinal set, Chebyshev set, n-width, geometry of Banach spaces.  
Address: Czech Republic  
Affiliation: Georgetown University  Institute of Computer Science Academy of Sciences of the  Georgetown University  
Abstract: It is shown that in a Banach space X satisfying mild conditions, for an infinite, independent subset G, there is no continuous best approximation map from X to the n-span, span n G. The hypotheses are satisfied when X is an L p -space, 1 &lt; p &lt; 1, and G is the set of functions computed by the hidden units of a typical neural network (e.g., Gaussian, Heaviside or hyperbolic tangent). If G is finite and span n G is not a subspace of X, it is also shown that there is no continuous map from X to span n G within any positive constant of a best approximation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Braess. </author> <title> Nonlinear Approximation Theory. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1980. </year>
Reference-contexts: The dimension of a linear space is the cardinality of a basis. For x; y in a linear space X we denote by [x; y] the closed (line) segment connecting x and y, i.e. [x; y] = fx + (y x); 2 <ref> [0; 1] </ref>g. Similarly, (x; y] = [x; y] fxg. Recall that a subset Y X is convex if [x; y] Y whenever x; y 2 X. A subset Y is called positively homogeneous if for all a &gt; 0, aY := fay : y 2 Y g = Y . <p> For example, in the uniform norm best approximation 8 by rational functions of a given degree fails to be continuous (Braess <ref> [1] </ref>), and we have shown that continuity also fails in the case of neural networks. Questions concerning existence of best approximation, with or without constraints on the parameters, can be investigated by the methods used here.
Reference: [2] <author> L. N. H. Bunt. Bijdrage tot de theorie der konvekse puntverzamelingen. </author> <type> Thesis Univ. </type> <institution> Groningen, </institution> <address> Amsterdam, </address> <year> 1934. </year>
Reference-contexts: Closed convex sets in reflexive, strictly convex Banach spaces are Chebyshev (see, e.g., Singer [19, pp 111,364]). Bunt <ref> [2] </ref> showed that the converse (Chebyshev implies convex) holds in finite-dimensional Hilbert spaces (it is an open question whether this is true in the infinite-dimensional Hilbert case).
Reference: [3] <author> C. K. Chui, X. Li, H. N. Mhaskar. </author> <title> Neural networks for localized approximation. </title> <booktitle> Mathematics of Computation 63, </booktitle> <pages> 607-623, </pages> <year> 1994. </year>
Reference-contexts: For the Heaviside activation function # : R ! R with #(t) = 0 for t &lt; 0 and #(t) = 1 for t 0, Chui, Li and Mhaskar <ref> [3] </ref> proved linear independence of the set of functions on the d-cube computable by Heaviside perceptrons that contain for each pair of characteristic functions of complementary half-spaces only one representative.
Reference: [4] <author> R. DeVore, R. Howard, C. Micchelli. </author> <title> Optimal nonlinear approximation. </title> <booktitle> Manuscripta Mathematica 63, </booktitle> <pages> 469-478, </pages> <year> 1989. </year>
Reference-contexts: Continuity of an approximation operator is a great advantage in the classical (linear) theory since it allows one to estimate worst-case error using methods of algebraic topology (see e.g. [12], [16], [18]). Extending these algebraic-topological proof techniques to nonlinear approximation, DeVore, Howard and Micchelli <ref> [4] </ref> utilized a concept of continuous nonlinear width. They measured the worst-case error in approximation of elements in a compact subset using a parametrized set of functions as the approximants when the parameters are selected continuously. <p> a continuous function satisfying (iii) follows immediately from the non-existence of a continuous function satisfying (ii) since achieving the multiplicative approximation to within (1+), where = "=kxZk, implies achieving the additive approximation within ". 2 In contrast to Theorem 3.6, when Z is a finite-dimensional subspace, DeVore, Howard and Micchelli <ref> [4, Theorem 2.1] </ref> showed that for any Banach space X and any " &gt; 0, there exists a continuous function : X ! Z satisfying (ii). 4 Application to neural networks One-hidden-layer neural networks with a single linear output unit compute func tions of the form n X w i g
Reference: [5] <author> A. Friedman. </author> <title> Foundations of Modern Analysis. </title> <publisher> Dover, </publisher> <address> New York, </address> <year> 1982. </year>
Reference: [6] <author> L. Gurvits, P. Koiran. </author> <title> Approximation and learning of convex superposi-tions. </title> <journal> J. of Computer and System Sciences 55(1), </journal> <pages> 161-170, </pages> <year> 1997. </year>
Reference-contexts: Questions concerning existence of best approximation, with or without constraints on the parameters, can be investigated by the methods used here. Relevant results include those of Gurvits and Koiran <ref> [6] </ref> and K-urkova [13] on compactness and closure of certain sets of Heaviside functions. Suitability of an approximation scheme can be measured by the worst-case approximation error.
Reference: [7] <author> R. Hecht-Nielsen. </author> <title> On the algebraic structure of of feedforward network weight spaces. </title> <booktitle> In Advanced Neural Computers (pp.129- 135). </booktitle> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: The question of linear independence of parametrized sets of functions representing neural networks was first considered by Hecht-Nielsen <ref> [7] </ref> who investigated multiple global minima in parameter spaces. He conjectured that for a perceptron network with hyperbolic tangent as its activation, the neural network's resulting (input/output) function would determine network parametrization uniquely, up to sign flips and permutations of hidden units. This was proved by Sussmann [20].
Reference: [8] <author> R.Huotari, W. Li. </author> <title> Continuities of metric projection and geometric con-sequences. </title> <editor> J. </editor> <booktitle> of Approximation Theory 90, </booktitle> <pages> 319-339, </pages> <year> 1997. </year>
Reference-contexts: Then [az 1 ; ax] and [az 2 ; ax] are in N , ax is a splitting point, and the argument in Theorem 3.2 can be applied. 2 The problem of convexity of Chebyshev sets in a general Banach space has been studied extensively (see e.g. [10], <ref> [8] </ref>). Closed convex sets in reflexive, strictly convex Banach spaces are Chebyshev (see, e.g., Singer [19, pp 111,364]). Bunt [2] showed that the converse (Chebyshev implies convex) holds in finite-dimensional Hilbert spaces (it is an open question whether this is true in the infinite-dimensional Hilbert case).
Reference: [9] <author> P. C. Kainen, V. K-urkova, V. Kreinovich, O. Sirisengtaksin. </author> <title> Uniqueness of network parametrization and faster learning. Neural, Parallel and Scientific Computations 2, </title> <address> 459- 466, </address> <year> 1994. </year>
Reference-contexts: K-urkova and Neruda [15] proved the linear independence of Gaussian radial-basis-functions ffl (bkxvk); v 2 R d ; b &gt; 0g, where fl (t) = exp (t 2 ). Characterization of linearly independent families for different types of activation functions was given by Kainen, K-urkova, Kreinovich and Sirisengtaksin <ref> [9] </ref>, while K-urkova and Kainen [14] developed some general theory for corresponding functional equations. In digital implementations of neural networks, one always has a finite set G; hence, span n G is a finite union of finite-dimensional subspaces.
Reference: [10] <author> V. Klee. </author> <title> Convexity of Chebyshev sets. </title> <journal> Math. </journal> <volume> Annalen 142, </volume> <pages> 202-304, </pages> <year> 1961. </year>
Reference-contexts: Then [az 1 ; ax] and [az 2 ; ax] are in N , ax is a splitting point, and the argument in Theorem 3.2 can be applied. 2 The problem of convexity of Chebyshev sets in a general Banach space has been studied extensively (see e.g. <ref> [10] </ref>, [8]). Closed convex sets in reflexive, strictly convex Banach spaces are Chebyshev (see, e.g., Singer [19, pp 111,364]). Bunt [2] showed that the converse (Chebyshev implies convex) holds in finite-dimensional Hilbert spaces (it is an open question whether this is true in the infinite-dimensional Hilbert case).
Reference: [11] <author> A. N. </author> <title> Kolmogorov. Uber die beste Annaherung von Funktionen einer gegenen Funktionenklasse. </title> <journal> Annals of Math. </journal> <volume> 37, </volume> <pages> 107-111, </pages> <year> 1936. </year>
Reference-contexts: When K, the set of functions to be approximated, and Y , the approximating set, are both subsets of a normed linear space (X; k:k) recall that the deviation of K from Y is defined as d (K; Y ) = sup x2K kx Y k. Kolmogorov <ref> [11] </ref> defined the (linear) n-width of a (usually compact) subset K of X to be the least possible deviation from any n-dimensional subspace of X, i.e., inffd (K; Y ) : Y is an n-dimensional subspace of Xg.
Reference: [12] <author> M. G. Krein, M. A. Krasnoselski, D. P. Milman. </author> <title> On deficiency numbers of linear operators in Banach spaces and on some geometric problems. </title> <journal> Sbornik Trudov Inst. Mat. Akad. Nauk SSSR 11, </journal> <pages> 97-112, </pages> <year> 1948. </year>
Reference-contexts: Continuity of an approximation operator is a great advantage in the classical (linear) theory since it allows one to estimate worst-case error using methods of algebraic topology (see e.g. <ref> [12] </ref>, [16], [18]). Extending these algebraic-topological proof techniques to nonlinear approximation, DeVore, Howard and Micchelli [4] utilized a concept of continuous nonlinear width.
Reference: [13] <author> V. K-urkova. </author> <title> Approximation of functions by perceptron networks with bounded number of hidden units. </title> <booktitle> Neural Networks 8, </booktitle> <pages> 745-750, </pages> <year> 1995. </year>
Reference-contexts: Questions concerning existence of best approximation, with or without constraints on the parameters, can be investigated by the methods used here. Relevant results include those of Gurvits and Koiran [6] and K-urkova <ref> [13] </ref> on compactness and closure of certain sets of Heaviside functions. Suitability of an approximation scheme can be measured by the worst-case approximation error.
Reference: [14] <author> V. K-urkova, P. C. Kainen. </author> <title> Singularities of finite scaling functions. </title> <journal> Applied Math. Letters 9, </journal> <pages> 33-37, </pages> <year> 1996. </year>
Reference-contexts: Characterization of linearly independent families for different types of activation functions was given by Kainen, K-urkova, Kreinovich and Sirisengtaksin [9], while K-urkova and Kainen <ref> [14] </ref> developed some general theory for corresponding functional equations. In digital implementations of neural networks, one always has a finite set G; hence, span n G is a finite union of finite-dimensional subspaces. Such a subset is itself finite-dimensional, and enjoys the nice topological property of bounded compactness.
Reference: [15] <author> V. K-urkova, R. Neruda. </author> <title> Uniqueness of functional representations by Gaussian basis function networks. </title> <booktitle> Proceedings of ICANN'94 (pp.471-474). </booktitle> <publisher> Springer, </publisher> <address> London, </address> <year> 1994. </year>
Reference-contexts: K-urkova and Neruda <ref> [15] </ref> proved the linear independence of Gaussian radial-basis-functions ffl (bkxvk); v 2 R d ; b &gt; 0g, where fl (t) = exp (t 2 ).
Reference: [16] <author> G. G. Lorentz. </author> <title> Approximation of Functions. </title> <editor> Halt, Reinhart, and Win-ston, </editor> <address> New York, </address> <year> 1966. </year>
Reference-contexts: Continuity of an approximation operator is a great advantage in the classical (linear) theory since it allows one to estimate worst-case error using methods of algebraic topology (see e.g. [12], <ref> [16] </ref>, [18]). Extending these algebraic-topological proof techniques to nonlinear approximation, DeVore, Howard and Micchelli [4] utilized a concept of continuous nonlinear width. They measured the worst-case error in approximation of elements in a compact subset using a parametrized set of functions as the approximants when the parameters are selected continuously. <p> Strict convexity is equivalent to the following condition: whenever for three points x; y; z 2 X the triangle inequality becomes equality, i.e. kx zk = kx yk + ky zk, then y 2 [x; z] (see e.g. <ref> [16] </ref>). Uniform convexity implies strict convexity and L p -spaces are uniformly convex for p 2 (1; 1). A Banach space X is a complete normed linear space.
Reference: [17] <author> H. N. Mhaskar. </author> <title> Versatile Gaussian networks. </title> <booktitle> In Proceedings of IEEE Workshop on Nonlinear Image and Signal Processing. </booktitle> <pages> (pp. 70-73). </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference-contexts: In recent years, various authors derived upper bounds on rates of approximation by neural networks. Some of these upper bounds were achieved using continuous approximation operators on sets of functions defined by a smoothness condition (see e.g. <ref> [17] </ref>). Continuity of an approximation operator is a great advantage in the classical (linear) theory since it allows one to estimate worst-case error using methods of algebraic topology (see e.g. [12], [16], [18]).
Reference: [18] <author> A. Pinkus. </author> <title> n-Width in Approximation Theory. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1985. </year>
Reference-contexts: Continuity of an approximation operator is a great advantage in the classical (linear) theory since it allows one to estimate worst-case error using methods of algebraic topology (see e.g. [12], [16], <ref> [18] </ref>). Extending these algebraic-topological proof techniques to nonlinear approximation, DeVore, Howard and Micchelli [4] utilized a concept of continuous nonlinear width. They measured the worst-case error in approximation of elements in a compact subset using a parametrized set of functions as the approximants when the parameters are selected continuously.
Reference: [19] <author> I. Singer. </author> <title> Best Approximation in Normed Linear Spaces by Elements of Linear Subspaces. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1970. </year>
Reference-contexts: Let X be a normed linear space and let Y be a non-empty subset. For x 2 X, let kx Y k = inf y2Y kx yk. The functional e Y (x) = kx Y k is uniformly continuous; see e.g. <ref> [19, p.391] </ref>. Let P (Y ) denote the set of all subsets of Y . <p> Closed convex sets in reflexive, strictly convex Banach spaces are Chebyshev (see, e.g., Singer <ref> [19, pp 111,364] </ref>). Bunt [2] showed that the converse (Chebyshev implies convex) holds in finite-dimensional Hilbert spaces (it is an open question whether this is true in the infinite-dimensional Hilbert case).
Reference: [20] <author> H. J. Sussmann. </author> <title> Uniqueness of the weights for minimal feedforward nets with a given intput-output map. </title> <booktitle> Neural Networks 5, </booktitle> <pages> 589-594, </pages> <year> 1992. </year>
Reference-contexts: He conjectured that for a perceptron network with hyperbolic tangent as its activation, the neural network's resulting (input/output) function would determine network parametrization uniquely, up to sign flips and permutations of hidden units. This was proved by Sussmann <ref> [20] </ref>. Thus, with t denoting hyperbolic tangent, a parametrized family F t = ft (v x + b); (v; b) 2 Ag, where A R d fi R contains for each pair of sign-flipped hidden unit parameters (v; b) and (v; b) only one element, is linearly independent.
Reference: [21] <author> L. P. </author> <title> Vlasov. On Chebyshev subsets of a Banach space. </title> <journal> Doklady Akad. Nauk SSSR 141, </journal> <pages> 19-20, </pages> <year> 1961. </year>
Reference: [22] <author> L. P. </author> <title> Vlasov. Almost convex and Chebyshev sets. </title> <journal> Math. Notes Acad. Sci. </journal> <volume> USSR 8, </volume> <pages> 776-779, </pages> <year> 1970. </year> <month> 10 </month>
Reference-contexts: Closed convex sets in reflexive, strictly convex Banach spaces are Chebyshev (see, e.g., Singer [19, pp 111,364]). Bunt [2] showed that the converse (Chebyshev implies convex) holds in finite-dimensional Hilbert spaces (it is an open question whether this is true in the infinite-dimensional Hilbert case). Vlasov <ref> [22] </ref> gave a sufficient condition on the norm and the subset projection: In a Banach space with strictly convex dual, every Chebyshev subset with continuous metric projection is convex. Using Vlasov's result and some elementary arguments, we obtain the following results.
References-found: 22

