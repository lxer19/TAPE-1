URL: ftp://ftp.cse.ucsc.edu/pub/ml/OHWCpaper.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00397.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: WORST CASE PREDICTION OVER SEQUENCES UNDER LOG LOSS  
Author: MANFRED OPPER AND DAVID HAUSSLER 
Abstract: We consider the game of sequentially assigning probabilities to future data based on past observations under logarithmic loss. We are not making probabilistic assumptions about the generation of the data, but consider a situation where a player tries to minimize his loss relative to the loss of the (with hindsight) best distribution from a target class for the worst sequence of data. We give bounds on the minimax regret in terms of the metric entropies of the target class with respect to suitable distances between distributions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Kolmogorov, A. N. and V. M. Tihomirov, </author> <title> *-Entropy and *-Capacity of Sets in Functional Spaces, </title> <journal> Amer. Math. Soc. Translations (Ser. </journal> <volume> 2), 17, </volume> <month> 277-364 </month> <year> (1961). </year>
Reference-contexts: Upper Bound on the Minimax Regret. For the following, the definition of metric entropy, also called Kolmogorov *-entropy, is needed <ref> [1] </ref>. Definition 1. Let D be a metric and (S; D) be a complete separable metric space. A partition of S is a collection f i g of Borel subsets of S that are pairwise disjoint and whose union is S. <p> By the packing number M (*; S; D) we denote the cardinality of the largest finite *-separated subset WORST CASE PREDICTION OVER SEQUENCES UNDER LOG LOSS 5 of S, or 1 if arbitrarily large such sets exist. The following lemma is easily verified <ref> [1] </ref>. Lemma 1.
Reference: [2] <author> V.G. Vovk, </author> <title> Aggregating strategies, </title> <booktitle> Proceedings of the 1990 conference on computational learning theory, </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> 371-381 </month> <year> (1990). </year>
Reference-contexts: Recently, new approaches to the prediction on sequences of data that avoid the assumption of randomness have found a great deal of interest in computational learning theory (see e.g. <ref> [2, 3, 4] </ref>) and information theory [5, 6, 7, 14]. For a collection of recent work, see the webpage http:// www-stat.wharton.upenn.edu/Seq96/ of a workshop on prediction over sequences, held at UC Santa Cruz in 1996.
Reference: [3] <author> N. Cesa-Bianchi, Y. Freund, D.H. Helmbold, D. Haussler, R.E. Schapire, and M.K. Warmuth, </author> <title> How to use expert advice, </title> <booktitle> in 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> 382-392, </pages> <address> San Diego, CA (1993). </address>
Reference-contexts: Recently, new approaches to the prediction on sequences of data that avoid the assumption of randomness have found a great deal of interest in computational learning theory (see e.g. <ref> [2, 3, 4] </ref>) and information theory [5, 6, 7, 14]. For a collection of recent work, see the webpage http:// www-stat.wharton.upenn.edu/Seq96/ of a workshop on prediction over sequences, held at UC Santa Cruz in 1996.
Reference: [4] <author> Y. Freund, </author> <title> Predicting a binary sequence as well as the optimal biased coin, </title> <booktitle> Proceedings of the ninth annual conference on computational learning theory, </booktitle> <publisher> ACM Press, </publisher> <year> (1996). </year>
Reference-contexts: Recently, new approaches to the prediction on sequences of data that avoid the assumption of randomness have found a great deal of interest in computational learning theory (see e.g. <ref> [2, 3, 4] </ref>) and information theory [5, 6, 7, 14]. For a collection of recent work, see the webpage http:// www-stat.wharton.upenn.edu/Seq96/ of a workshop on prediction over sequences, held at UC Santa Cruz in 1996. <p> R (q; y n ) = inf sup sup 2fi p (y n ) Bounds and asymptotic expressions for this minimax regret have been obtained for finite dimensional parametric families of distributions, such as probability mass functions over a finite alphabet or distributions which are smooth functions of the parameters <ref> [4, 5, 6, 7] </ref>. 1 Here and in what follows, -(y n ) and p (y n ) are used to denote the n-fold products of the distributions and p respectively, evaluated at the point y n . 4 MANFRED OPPER AND DAVID HAUSSLER In this paper, we give a general <p> A similar result was obtained for parametric families in <ref> [4, 6, 7, 9] </ref>. Whether this will be true in significantly more general settings is a problem for further research. 7. Appendix. Proof of lemma (3): Write T n IET n as a sum of Martingale-differences d j , i.e.
Reference: [5] <author> J. Shtarkov, </author> <title> Coding of discrete sources with unknown statistics, </title> <booktitle> In: Topics in Information Theory, </booktitle> <pages> 559-574, </pages> <editor> I. Csiszar and P. Elias, editors, </editor> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1975. </year>
Reference-contexts: Recently, new approaches to the prediction on sequences of data that avoid the assumption of randomness have found a great deal of interest in computational learning theory (see e.g. [2, 3, 4]) and information theory <ref> [5, 6, 7, 14] </ref>. For a collection of recent work, see the webpage http:// www-stat.wharton.upenn.edu/Seq96/ of a workshop on prediction over sequences, held at UC Santa Cruz in 1996. <p> R (q; y n ) = inf sup sup 2fi p (y n ) Bounds and asymptotic expressions for this minimax regret have been obtained for finite dimensional parametric families of distributions, such as probability mass functions over a finite alphabet or distributions which are smooth functions of the parameters <ref> [4, 5, 6, 7] </ref>. 1 Here and in what follows, -(y n ) and p (y n ) are used to denote the n-fold products of the distributions and p respectively, evaluated at the point y n . 4 MANFRED OPPER AND DAVID HAUSSLER In this paper, we give a general <p> The bounds can be applied to nonparametric families of distributions, where, to our knowledge, minimax results for arbitrary sequences have not been obtained. Our calculation is based on the explicit solution to the minimax problem (1), which was given by Shtarkov <ref> [5] </ref>. He found that the distribution ^q n (y n ) = R minimizes the worst case regret sup y n R (q; y n ).
Reference: [6] <author> A.R. Barron and Q. Xie, </author> <title> Asymptotic minimax loss for data compression, gambling, and prediction, </title> <booktitle> Proceedings of the ninth annual conference on computational learning theory, </booktitle> <publisher> ACM Press, </publisher> <year> (1996). </year>
Reference-contexts: Recently, new approaches to the prediction on sequences of data that avoid the assumption of randomness have found a great deal of interest in computational learning theory (see e.g. [2, 3, 4]) and information theory <ref> [5, 6, 7, 14] </ref>. For a collection of recent work, see the webpage http:// www-stat.wharton.upenn.edu/Seq96/ of a workshop on prediction over sequences, held at UC Santa Cruz in 1996. <p> R (q; y n ) = inf sup sup 2fi p (y n ) Bounds and asymptotic expressions for this minimax regret have been obtained for finite dimensional parametric families of distributions, such as probability mass functions over a finite alphabet or distributions which are smooth functions of the parameters <ref> [4, 5, 6, 7] </ref>. 1 Here and in what follows, -(y n ) and p (y n ) are used to denote the n-fold products of the distributions and p respectively, evaluated at the point y n . 4 MANFRED OPPER AND DAVID HAUSSLER In this paper, we give a general <p> A similar result was obtained for parametric families in <ref> [4, 6, 7, 9] </ref>. Whether this will be true in significantly more general settings is a problem for further research. 7. Appendix. Proof of lemma (3): Write T n IET n as a sum of Martingale-differences d j , i.e.
Reference: [7] <author> J. Rissanen, </author> <title> Fisher Information and Stochastic Complexity, </title> <journal> IEEE Trans. on Inf. Theory 42, </journal> <pages> 40-47, </pages> <year> (1996). </year>
Reference-contexts: Recently, new approaches to the prediction on sequences of data that avoid the assumption of randomness have found a great deal of interest in computational learning theory (see e.g. [2, 3, 4]) and information theory <ref> [5, 6, 7, 14] </ref>. For a collection of recent work, see the webpage http:// www-stat.wharton.upenn.edu/Seq96/ of a workshop on prediction over sequences, held at UC Santa Cruz in 1996. <p> R (q; y n ) = inf sup sup 2fi p (y n ) Bounds and asymptotic expressions for this minimax regret have been obtained for finite dimensional parametric families of distributions, such as probability mass functions over a finite alphabet or distributions which are smooth functions of the parameters <ref> [4, 5, 6, 7] </ref>. 1 Here and in what follows, -(y n ) and p (y n ) are used to denote the n-fold products of the distributions and p respectively, evaluated at the point y n . 4 MANFRED OPPER AND DAVID HAUSSLER In this paper, we give a general <p> A similar result was obtained for parametric families in <ref> [4, 6, 7, 9] </ref>. Whether this will be true in significantly more general settings is a problem for further research. 7. Appendix. Proof of lemma (3): Write T n IET n as a sum of Martingale-differences d j , i.e.
Reference: [8] <author> T. Cover and Joy A. Thomas, </author> <title> Elements of Information Theory, Wiley Series in Telecommunications, </title> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Logarithmic loss has an important meaning in data compression, where any assignment of probabilities to data values can be considered as an assignment of possible codelengths to the data using a uniquely decodable code <ref> [8] </ref>. The total log loss is (ignoring the problems of truncating continuous data values and the rounding of integers) proportional to the length of the compressed sequence of data. <p> For an interpretation of logarithmic loss in terms of the wealth achieved in gambling, where probabilities stand for the relative amount of money bet on future data values, see <ref> [8, 9] </ref> At the end of the game, the learner has suffered a total loss L (q; y n ) = t=1 All n predictions q (y t jy t1 ), t = 1; : : : ; n can be composed into a single joint distribution q (y n )
Reference: [9] <author> T.M Cover and E. Ordentlich, </author> <title> Universal portfolios with side information, </title> <journal> IEEE Transactions on Information Theory 42(2), </journal> <pages> 348-363, </pages> <year> (1996). </year>
Reference-contexts: For an interpretation of logarithmic loss in terms of the wealth achieved in gambling, where probabilities stand for the relative amount of money bet on future data values, see <ref> [8, 9] </ref> At the end of the game, the learner has suffered a total loss L (q; y n ) = t=1 All n predictions q (y t jy t1 ), t = 1; : : : ; n can be composed into a single joint distribution q (y n ) <p> A similar result was obtained for parametric families in <ref> [4, 6, 7, 9] </ref>. Whether this will be true in significantly more general settings is a problem for further research. 7. Appendix. Proof of lemma (3): Write T n IET n as a sum of Martingale-differences d j , i.e.
Reference: [10] <author> Aad W. van der Vaart and Jon A. Wellner, </author> <title> Weak Convergence and Empirical Processes, </title> <booktitle> Springer Series in Statistics, </booktitle> <year> 1996. </year>
Reference-contexts: We begin with some elementary steps that cast the problem into a form where the tools of empirical process theory <ref> [11, 10] </ref> can be applied. <p> of zero mean random variables fZ : 2 fig is called a sub-Gaussian process with respect to the seminorm D on fi, if for any ; 0 2 fi, 2 t 2 =D 2 (; 0 ) : The following lemma easily follows from Corollary 2.2.8 on page 101 of <ref> [10] </ref> Lemma 4. Let fZ : 2 fig be a sub-Gaussian process under the norm D with finite packing numbers M (*; fi; D) for all * &gt; 0.
Reference: [11] <author> M. Ledoux and M. Talagrand, </author> <title> Probability in Banach Spaces: Isoperimetry and Processes, </title> <publisher> Springer Verlag, </publisher> <address> Berlin (1991). </address>
Reference-contexts: We begin with some elementary steps that cast the problem into a form where the tools of empirical process theory <ref> [11, 10] </ref> can be applied. <p> Then IEe T n exp [ 2 The lemma is proved in the appendix using Lemma 6.16 of <ref> [11] </ref>. To apply this lemma, let fi = fi k and X (y) = log p k (y) p (y) ; WORST CASE PREDICTION OVER SEQUENCES UNDER LOG LOSS 7 so that T n = S n . <p> The proof is based on the following inequality jd j j sup jX (y j )j + IE sup jX (y)j (9) which is due to V. Yurinskii and is proved in Lemma 6.16 on page 163 of <ref> [11] </ref>. For completeness, we give a sketch of the proof here.
Reference: [12] <author> D. Haussler and M. Opper, </author> <title> Mutual Information, Metric Entropy, and Risk in Estimation of Probability Distributions, </title> <journal> Annals of Statistics 25 (6), </journal> <volume> (Decem-ber, </volume> <year> 1997). </year>
Reference-contexts: This means that the learner, trying to be prepared for the worst distribution of sequences, should minimize the risk 1 sup Z See <ref> [12] </ref> and papers cited there for a discussion this average loss framework and the results that can be obtained there. We will now go beyond the average loss framework and analyze a strategy which aims at performing well on individual sequences. <p> not smaller than the minimax risk in the framework where the data are generated at random from a distribution in fi, that is, from equation (1) R n inf sup Z A general lower bound on the latter quantity for product distributions p (y n ) was recently obtained in <ref> [12] </ref>. From Lemma 7, part 1 of [12] and Equation (6) above, we get Lemma 5. Assume (fi; D H ) is totally bounded. <p> the framework where the data are generated at random from a distribution in fi, that is, from equation (1) R n inf sup Z A general lower bound on the latter quantity for product distributions p (y n ) was recently obtained in <ref> [12] </ref>. From Lemma 7, part 1 of [12] and Equation (6) above, we get Lemma 5. Assume (fi; D H ) is totally bounded. <p> As can be shown <ref> [12] </ref> for this example, the lower bound (5) yields the same exponent for increase of R n with n as the upper bound.
Reference: [13] <author> G.F. Clements, </author> <title> Entropy of several sets of real valued functions, </title> <journal> Pacific J. Math. </journal> <volume> 13, </volume> <month> 1085 </month> <year> (1963). </year>
Reference-contexts: If we further assume that all densities are uniformly bounded away from zero, we can use a result of <ref> [13] </ref> to show that the metric entropy behaves like K (*; fi; D 1 ) = const 1 1 for * ! 0, which yields R n const n 1 WORST CASE PREDICTION OVER SEQUENCES UNDER LOG LOSS 9 for large n and r + fl &gt; 1 2 .
Reference: [14] <author> M.J. Weinberger, N. Merhav and M. Feder, </author> <title> Optimal Sequential Probability Assignment for Individual Sequences, </title> <journal> IEEE Trans. on Inf. Theory 40, </journal> <month> 384-396 </month> <year> (1994). </year>
Reference-contexts: Recently, new approaches to the prediction on sequences of data that avoid the assumption of randomness have found a great deal of interest in computational learning theory (see e.g. [2, 3, 4]) and information theory <ref> [5, 6, 7, 14] </ref>. For a collection of recent work, see the webpage http:// www-stat.wharton.upenn.edu/Seq96/ of a workshop on prediction over sequences, held at UC Santa Cruz in 1996.
Reference: [15] <author> W. Hoeffding, </author> <title> Probability Inequalities for Sums of Bounded Random Variables, </title> <journal> American Statistical Association Journal 58, </journal> <pages> 13-30, </pages> <year> (1963). </year>
Reference-contexts: Then U (y n ) = Z (y n ) Z 0 (y n ). As in Equation 3, it is clear that jU i j 2D 1 (; 0 ). Thus U is a sum of n bounded i.i.d. random variables. Hence, we may apply Hoeffding's inequality <ref> [15] </ref> to obtain Pr (jU j &gt; t) 2 exp t 2 =(2nD 2 fl Since U = Z Z 0 , this shows that Z is sub-Gaussian with respect to D = nD 1 .
References-found: 15

