URL: http://www-ai.ijs.si/AramKaralic/bibliography/1991f.ps
Refering-URL: http://www-ai.ijs.si/AramKaralic/bibliography/1991f.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: aram.karalic@ijs.si and bojan.cestnik@ijs.si  
Title: The Bayesian Approach to Tree-Structured Regression  
Author: Aram Karalic and Bojan Cestnik 
Keyword: regression trees, m-estimate, probability distribution, continuous class.  
Address: Jamova 39, 61111 Ljubljana Yugoslavia  
Affiliation: Jozef Stefan Institute  
Abstract: In the context of inductive learning, the Bayesian approach turned out to be very successful in estimating probabilities of events when there are only a few learning examples. The m-probability estimate was developed to handle such situations. In this paper we present the m-distribution estimate, an extension to the m-probability estimate which, besides the estimation of probabilities, covers also the estimation of probability distributions. We focus on its application in the construction of regression trees. The theoretical results were incorporated into a system for automatic induction of regression trees. The results of applying the upgraded system to several domains are presented and compared to previous results. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A., & Stone, C.J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth Int. Group, </publisher> <address> Belmont, California. </address>
Reference-contexts: The influence of the prior distribution is governed by the choice of the parameter m. When building regression trees we are not interested in the exact shape of a class distribution. In fact, only the mean value and the standard deviation are required <ref> (Breiman et al. 1984) </ref>. Due to the probabilistic treatment of unknown values, we must also assign a weight to every example. <p> The same domain is used in <ref> (Breiman et al., 1984) </ref>. In each experiment, the learning set contained 210 examples and the testing set 90 examples. GRIND | The Steel Grinding Domain: Vibration signals generated by the grinding wheel and the workpiece were detected by an accelerometer sensor and processed by a spectral analyser. <p> The tree which performed best on testing example set was then selected. Each experiment was repeated 10 times, with different learning and testing example sets. Classification errors of the selected trees were compared with the errors obtained by using cost-complexity pruning, described in <ref> (Breiman et al., 1984) </ref>. Results, which are the averages of 10 experiments, are presented in Fig. 2.
Reference: <author> Cestnik, B., Kononenko, I., & Bratko, I., </author> <year> (1987), </year> <title> "ASSISTANT 86: A Knowledge-Elicitation Tool for Sophisticated Users", in Progress in Machine Learning, </title> <editor> ed. by I.Bratko and N.Lavrac, </editor> <publisher> Sigma Press, Wilmslow. </publisher>
Reference: <author> Cestnik, B. </author> <year> (1990), </year> <title> "Estimating probabilities: A Crucial Task in Machine Learning", </title> <booktitle> Proceedings of ECAI-90, </booktitle> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Estimation of probabilities is considered to be a very important task in machine learning (Ces-tnik, 1990). One way to cope with this problem is the Bayesian approach. The use of this approach in estimating probabilities in machine learning in general was elaborated in <ref> (Cestnik, 1990) </ref>, where a new method for estimating probabilities, the m-estimate, was introduced. A particular application of the approach in classification tree pruning was presented in (Cestnik & Bratko, 1991). <p> In this section a particular way of modifying a class distribution given the evidence is described. The dependency is derived by analogy with the interpretation of discrete m-probability estimate. The probability estimate p in the discrete case is, according to <ref> (Cestnik, 1990) </ref>, defined by: p = N + m m p a where p a denotes the prior probability estimate, N is size of the current example set, and p c the relative frequency of the event in the current example set. <p> With m = 2 we would, by (y) = N + m m (y) obtain the estimated probability distribution , depicted in Fig. 1c. In practice, it is difficult to obtain prior probability distribution estimates. The problem is solved as in <ref> (Cestnik, 1990) </ref> by taking the probability distribution of the class from the whole training set as a prior distribution. During the tree building process, each distribution in the internal nodes of the tree is estimated by combining the prior distribution with the distribution obtained from examples in the node.
Reference: <author> Cestnik, B., & Bratko, I. </author> <year> (1991), </year> <title> "On Estimating Probabilities in Tree Pruning", </title> <booktitle> Proceedings of EWSL-91, Porto, </booktitle> <address> Portugal, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: The use of this approach in estimating probabilities in machine learning in general was elaborated in (Cestnik, 1990), where a new method for estimating probabilities, the m-estimate, was introduced. A particular application of the approach in classification tree pruning was presented in <ref> (Cestnik & Bratko, 1991) </ref>. In this paper we propose an extension of the m-probability estimate to cover, in addition to the estimation of the probability of a single class (i.e. discrete probability distribution), also the estimation of a probability distribution of a continuous class (continuous probability distribution).
Reference: <author> Filipic, B., Junkar, M., Bratko, I., & Karalic, A. </author> <year> (1991), </year> <title> "An Application of Machine Learning to a Metal-Working Process", </title> <booktitle> Proceedings of ITI-91, </booktitle> <address> Cavtat, Yugoslavia, </address> <month> 10-14 June </month> <year> 1991. </year>
Reference: <author> Good, I.J. </author> <year> (1965), </year> <title> The Estimation of Probabilities, </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: First, one has to assume some initial (prior) value of the parameter. Then, some experiments are performed. According to the results of the experiments, the initial value is modified to obtain the final (posterior) value of the parameter <ref> (Good, 1965) </ref>. Let us illustrate this by a simple example. Suppose we want to estimate the height of a person named Jane and our prior knowledge consists of the facts "Jane is female" and " Jane is 23 years old".
Reference: <author> Junkar, M., Filipic, B., & Bratko, I. </author> <year> (1991), </year> <title> "Identifying the grinding process by means of inductive machine learning", </title> <booktitle> Preprints of the first CIRP Workshop of the Intelligent Manufacturing Systems, </booktitle> <address> Budapest, Hungary, </address> <pages> pp. 195-203. </pages>
Reference: <author> Kononenko, I., </author> <year> (1985), </year> <title> The Development of the Inductive Learning System Assistant, M.Sc. </title> <type> thesis, </type> <institution> Edvard Kardelj University, Ljubljana (in Slovene). </institution>
Reference: <author> Quinlan, J.R. </author> <year> (1986), </year> <title> "Induction of Decision Trees", </title> <journal> Machine Learning, </journal> <volume> Vol. 1, </volume> <pages> pp. 81-106. </pages>
Reference-contexts: The construction algorithm for regression trees is analogous to the one used by TDIDT algorithms such as ID3 <ref> (Quinlan, 1986) </ref> or ASSISTANT (Kononenko, 1985; Cestnik et al. 1987). Since the class is continuous, an estimate of standard deviation of the class values is used as the measure of impurity of a node.
References-found: 9

