URL: ftp://dec002.cmpe.boun.edu.tr/people/ethem/papers/stipr.ps.Z
Refering-URL: http://www.cmpe.boun.edu.tr/~ethem/
Root-URL: 
Email: falpaydin,kaynakcg@boun.edu.tr  
Title: Cascading Classifiers  
Author: Ethem Alpaydn, Cenk Kaynak 
Address: Istanbul TR-80815 Turkey  
Affiliation: Department of Computer Engineering Bogazi~ci University  
Abstract: We propose a multistage recognition method built as a cascade of a multi-layer perceptron (MLP) and a k-nearest neighbor (k-NN) classifier. MLP, being a distributed method, generalizes to learn a "rule" and the k-NN, being a local method, learns the localized "exceptions" rejected by the "rule." Because the rule-learner handles a large percentage of the examples using a simple and general rule, only a small subset of the training set is stored as exceptions during training. Similarly during testing, most patterns are handled by the MLP and few are handled by k-NN thus causing only a small increase in memory and computation. A multistage method like cascading is a better approach than multiexpert methods like voting and stacking where all learners are used for all cases; the extra computation and memory for the second learner is unnecessary if we are sufficiently certain that the first one's response is correct. We discuss how such a system can be trained using cross validation. This method is tested on the real-world application of handwritten digit recognition.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Drucker, H., Schapire, R., Simard, P. </author> <title> (1993) "Improving Performance in Neural Networks Using a Boosting Algorithm," </title> <booktitle> in Advances in Neural Information Processing Systems 5, </booktitle> <editor> S. J. Hanson, J. Cowan, L. Giles (Eds.), </editor> <address> 42-49, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Cascading is most similar to boosting <ref> [1] </ref> where the second learns the exceptions to the first and the third learning the cases on which the first two do not agree.
Reference: [2] <author> Garris, M. D., Blue, J. L., Candela, G. T., Dimmick, D. L., Geist, J., Grother, P. J., Janet, S. A., Wilson, C. L. </author> <title> NIST Form-Based Handprint Recognition System, </title> <type> NISTIR 5469, </type> <year> 1994. </year>
Reference-contexts: with one standard error bars (MLP and k-NN are also given for comparison) (c) % k-NN called during test and (d) % of distance computations made (d=a*c). 3 Empirical Comparison The database we use to test performance contains handwritten digits created using the set of programs made available by NIST <ref> [2] </ref>. The 32 by 32 normalized bitmaps are low-pass filtered and undersampled to get 8 by 8 matrices where each element is an integer in the range 0 to 16. 44 people filled in forms which are randomly divided into two clusters of 30 and 14 forms.
Reference: [3] <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., Hinton, G. E. </author> <title> (1991) "Adaptive Mixtures of Local Experts," </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 79-87. </pages>
Reference-contexts: In that case, the level 1 learner learns to correct the output of the level 0 learner. This is like what the exception-learner in cascading is doing. Cascading can also be viewed as a mixture of experts architecture <ref> [3] </ref>.
Reference: [4] <author> Pudil, P., Novovicova, J., Blaha, S., Kittler, J. </author> <title> (1992) "Multistage Pattern Recognition with Reject Option," </title> <booktitle> 11th IAPR International Conference on Pattern Recognition B, II, </booktitle> <pages> 92-95. </pages>
Reference-contexts: The idea in cascaded classifiers is to have two learners, one to learn the rule and the other to learn the exceptions. This is a multistage pattern recognition approach <ref> [4] </ref> where inputs rejected by the first stage are handled by a second stage using costlier features or decision making mechanism that is too expensive to use for all inputs. A rule by definition is valid everywhere, for all inputs. <p> This takes into account the knowledge of the rule-learner and is especially useful if the patterns stored are noisy. In multistage classification methods <ref> [4] </ref>, classifiers using simpler to extract features are used to recognize well-formed cases before those that use features that are more complex and costly to extract are used to recognize patterns of poorer quality. In our approach, it is not the features that get more complex but the classifier. <p> Otherwise we reject. of cascading is equivalent to 1 defining the threshold of decision. We aim finding a bound on the complexity of the second classifier that guarantees decreasing average risk. We follow work done by Pudil et al. <ref> [4] </ref> here. We are interested in using a second classifier to classify those rejected by the first. This second classifier may use costlier features or a more expensive classification scheme and thus is to be used as rarely as possible.
Reference: [5] <author> Wolpert, D. H. </author> <title> (1992) "Stacked Generalization," </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 241-259. </pages>
Reference-contexts: In certain respects, the cascading method can be likened to other methods in the neural network literature for combining multiple learners: If we add the outputs of the rule- and exception-learners, when is one, we get simple voting [6]. Stacking <ref> [5] </ref> has a variant where there is only one level 0 learner. In that case, the level 1 learner learns to correct the output of the level 0 learner. This is like what the exception-learner in cascading is doing.
Reference: [6] <author> Xu, L., Krzy_zak, A., Suen, C. Y. </author> <title> (1992) "Methods of Combining Multiple Classifiers and Their Applications to Handwriting Recognition," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 22, </volume> <pages> 418-435. </pages>
Reference-contexts: In certain respects, the cascading method can be likened to other methods in the neural network literature for combining multiple learners: If we add the outputs of the rule- and exception-learners, when is one, we get simple voting <ref> [6] </ref>. Stacking [5] has a variant where there is only one level 0 learner. In that case, the level 1 learner learns to correct the output of the level 0 learner. This is like what the exception-learner in cascading is doing.
References-found: 6

