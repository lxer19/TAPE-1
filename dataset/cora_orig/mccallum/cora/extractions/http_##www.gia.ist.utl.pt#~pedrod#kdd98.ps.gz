URL: http://www.gia.ist.utl.pt/~pedrod/kdd98.ps.gz
Refering-URL: http://www.gia.ist.utl.pt/~pedrod/
Root-URL: 
Email: pedrod@gia.ist.utl.pt  
Title: Occam's Two Razors: The Sharp and the Blunt razor: Given two models with the same
Author: Pedro Domingos 
Keyword: Occam's Two Razors  
Note: First  should be pre ferred because simplicity is desirable in itself.  
Address: Lisbon 1096, Portugal  
Affiliation: Artificial Intelligence Group Instituto Superior Tecnico  
Abstract: Occam's razor has been the subject of much controversy. This paper argues that this is partly because it has been interpreted in two quite different ways, the first of which (simplicity is a goal in itself) is essentially correct, while the second (simplicity leads to greater accuracy) is not. The paper reviews the large variety of theoretical arguments and empirical evidence for and against the "second razor," and concludes that the balance is strongly against it. In particular, it builds on the case of (Schaffer, 1993) and (Webb, 1996) by considering additional theoretical arguments and recent empirical evidence that the second razor fails in most domains. A version of the first razor more appropriate to KDD is proposed, and we argue that continuing to apply the second razor risks causing significant opportunities to be missed. 1 William of Occam's famous razor states that "Nun-quam ponenda est pluralitas sin necesitate," which, approximately translated, means "Entities should not be multiplied beyond necessity" (Tornay 1938). It was born in the late Middle Ages as a criticism of scholastic philosophy, whose theories grew ever more elaborate without any corresponding improvement in predictive power. In the intervening centuries it has come to be seen as one of the fundamental tenets of modern science, and today it is often invoked by learning theorists and KDD practitioners as a justification for preferring simpler models over more complex ones. However, formulating Occam's razor in KDD terms it trickier than might appear at first. Leaving aside for the moment the question of how to measure simplicity, let generalization error of a model be its error rate on unseen examples, and training-set error be its error on the examples it was learned from. Then the formulation that is perhaps closest to Occam's original intent is: 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. S. </author> <year> 1989. </year> <title> Learning from hints in neural networks. </title> <journal> Journal of Complexity 6 </journal> <pages> 192-198. </pages>
Reference: <author> Akaike, H. </author> <year> 1978. </year> <title> A Bayesian analysis of the minimum AIC procedure. </title> <journal> Annals of the Institute of Statistical Mathematics 30A:9-14. </journal>
Reference-contexts: Criteria of this type include AIC <ref> (Akaike 1978) </ref>, BIC (Schwarz 1978), and many others. Similar criteria with an information-theoretic interpretation, like MML (Wallace & Boulton 1968) and MDL (Rissanen 1978) are discussed below. Consider BIC, the first criterion to be explicitly proposed as an approximation to Bayesian model averaging.
Reference: <author> Bernardo, J. M., and Smith, A. F. M. </author> <year> 1994. </year> <title> Bayesian Theory. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference: <author> Bishop, C. M. </author> <year> 1995. </year> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford, UK: Oxford University Press. </publisher>
Reference-contexts: Incorporating such constraints can simultaneously improve accuracy (by reducing the search needed to find an accurate model) and comprehensibility (by making the results of induction consistent with previous knowledge). Weak constraints are often sufficient ((Abu-Mostafa 1989; Donoho & Rendell 1996; Pazzani, Mani, & Shankle 1997); see also <ref> (Bishop 1995) </ref>, Section 8.7). If we accept the fact that the most accurate models will not always be simple or easily understandable, we should allow an explicit trade-off between the two.
Reference: <author> Blumer, A.; Ehrenfeucht, A.; Haussler, D.; and War-muth, M. K. </author> <year> 1987. </year> <title> Occam's razor. </title> <journal> Information Processing Letters 24 </journal> <pages> 377-380. </pages>
Reference-contexts: By this result, a decision tree with one million nodes extracted from a set of ten such trees is preferable to one with ten nodes extracted from a set of a million, given the same training-set error. Put another way, the results in <ref> (Blumer et al. 1987) </ref> only say that if we select a sufficiently small set of models prior to looking at the data, and by good fortune one of those models closely agrees with the data, we can be confident that it will also do well on future data.
Reference: <author> Cestnik, B., and Bratko, I. </author> <year> 1988. </year> <title> Learning redundant rules in noisy domains. </title> <booktitle> In Proceedings of the Eighth European Conference on Artificial Intelligence, </booktitle> <pages> 348-356. </pages> <address> Munich, Germany: </address> <publisher> Pitman. </publisher>
Reference: <author> Cheeseman, P. </author> <year> 1990. </year> <title> On finding the most probable model. </title> <editor> In Shrager, J., and Langley, P., eds., </editor> <title> Computational Models of Scientific Discovery and Theory Formation. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 73-95. </pages>
Reference-contexts: This has led some researchers to believe that a trade-off between error and complexity is "a direct consequence of Bayes' theorem, requiring no additional assumptions" <ref> (Cheeseman 1990) </ref>. However, this belief is founded on a confusion between assigning the shortest codes to the most probable hypotheses and a priori considering that the syntactically simplest models in the representation being used (e.g., the decision trees with fewest nodes) are the most probable ones.
Reference: <author> Chickering, D. M., and Heckerman, D. </author> <year> 1997. </year> <title> Efficent approximations for the marginal likelihood of Bayesian networks with hidden variables. </title> <booktitle> Machine Learning 29 </booktitle> <pages> 181-212. </pages>
Reference: <author> Clark, P., and Matwin, S. </author> <year> 1993. </year> <title> Using qualitative models to guide inductive learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 49-56. </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Clearwater, S., and Provost, F. </author> <year> 1990. </year> <title> RL4: A tool for knowledge-based induction. </title> <booktitle> In Proceedings of the Second IEEE International Conference on Tools for Artificial Intelligence, </booktitle> <pages> 24-30. </pages> <address> San Jose, CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Cohen, P. R., and Jensen, D. </author> <year> 1997. </year> <title> Overfitting explained. </title> <booktitle> In Preliminary Papers of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> 115-122. </pages> <address> Fort Lauderdale, FL: </address> <booktitle> Society for Artificial Intelligence and Statistics. </booktitle>
Reference: <author> Cover, T. M., and Thomas, J. A. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: However, there is no guarantee that it will select the most accurate model. Rissanen simply proposes it as a fundamental principle. The closely-related minimum message length (MML) principle (Wallace & Boulton 1968) is derived by taking the logarithm of Bayes' theorem and noting that, according to information theory <ref> (Cover & Thomas 1991) </ref>, logarithms of probabilities can be seen as (minus) the lengths of the most efficient codes for the corresponding events. This has led some researchers to believe that a trade-off between error and complexity is "a direct consequence of Bayes' theorem, requiring no additional assumptions" (Cheeseman 1990). <p> The second razor clearly favors the flat earth theory, being a linear model, while the spherical one is quadratic and no better at explaining everyday observations in the Middle Ages. Another favorite example is relativity vs. Newton's laws. The following passage is from <ref> (Cover & Thomas 1991) </ref>: In the end, we choose the simplest explanation that is consistent with the observed data.
Reference: <author> Craven, M. W. </author> <year> 1996. </year> <title> Extracting Comprehensible Models from Trained Neural Networks. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Sciences, University of Wis-consin - Madison, Madison, WI. </institution>
Reference: <author> Datta, P., and Kibler, D. </author> <year> 1995. </year> <title> Learning prototypical concept descriptions. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 158-166. </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Domingos, P., and Pazzani, M. </author> <year> 1997. </year> <title> On the optimality of the simple Bayesian classifier under zero-one loss. </title> <booktitle> Machine Learning 29 </booktitle> <pages> 103-130. </pages>
Reference: <author> Domingos, P. </author> <year> 1996. </year> <title> Unifying instance-based and rule-based induction. </title> <booktitle> Machine Learning 24 </booktitle> <pages> 141-168. </pages>
Reference: <author> Domingos, P. </author> <year> 1997a. </year> <title> Knowledge acquisition from examples via multiple models. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> 98-106. </pages> <address> Nashville, TN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Domingos, P. </author> <year> 1997b. </year> <title> Why does bagging work? A Bayesian account and its implications. </title> <booktitle> In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 155-158. </pages> <address> Newport Beach, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Domingos, P. </author> <year> 1998. </year> <title> A process-oriented heuristic for model selection. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Machine Learning. </booktitle> <address> Madison, WI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Donoho, S., and Rendell, L. </author> <year> 1996. </year> <title> Constructive induction using fragmentary knowledge. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> 113-121. </pages> <address> Bari, Italy: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Elomaa, T. </author> <year> 1994. </year> <title> In defense of C4.5: Notes on learning one-level decision trees. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 62-69. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A similar measure for decision trees would always achieve the Bayes rate (lowest error possible). At most, these experiments suggest that the advantage of going to more complex models is small; they do not imply that simpler models are better <ref> (Elomaa 1994) </ref>. However, as we shall see below, more recent results call even this conclusion into question.
Reference: <author> Fisher, D. H., and Schlimmer, J. C. </author> <year> 1988. </year> <title> Concept simplification and prediction accuracy. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> 22-28. </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Friedman, J. H. </author> <year> 1996. </year> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. </title> <type> Technical report, </type> <institution> Department of Statistics and Stanford Linear Accelerator Center, Stanford University, Stanford, </institution> <address> CA. ftp://- playfair.stanford.edu/pub/friedman/kdd.ps.Z. </address>
Reference: <author> Gams, M. </author> <year> 1989. </year> <title> New measurements highlight the importance of redundant knowledge. </title> <booktitle> In Proceedings of the Fourth European Working Session on Learning, </booktitle> <pages> 71-79. </pages> <address> Montpellier, France: </address> <publisher> Pitman. </publisher>
Reference: <author> Haussler, D. </author> <year> 1988. </year> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <booktitle> Artificial Intelligence 36 </booktitle> <pages> 177-221. </pages>
Reference-contexts: BIC penalizes the model structure's dimension because higher-order spaces effectively contain many more models than lower-order ones, and thus contain many more low-likelihood models along with the "best" one (s). (In precise terms, higher-order model structures have a higher VC dimension <ref> (Haussler 1988) </ref>; or, considering finite-precision numbers, they literally contain more models.) For example, the model space defined by ax 2 + bx + c contains many more models than the one defined by ax + b.
Reference: <author> Holte, R. C. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-91. </pages>
Reference: <author> Jensen, D., and Schmill, M. </author> <year> 1997. </year> <title> Adjusting for multiple comparisons in decision tree pruning. </title> <booktitle> In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 195-198. </pages> <address> Newport Beach, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Langley, P. </author> <year> 1996. </year> <title> Induction of condensed determinations. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 327-330. </pages> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Thus the first razor is justified. However, simplicity and comprehensibility are not always equivalent. For example, a decision table <ref> (Langley 1996) </ref> may be larger than a similarly accurate decision tree, but more easily understood because all lines in the table use the same attributes. Induced models are also more comprehensible if they are consistent with previous knowledge, even if this makes them more complex (Pazzani, Mani, & Shankle 1997).
Reference: <author> Lawrence, S.; Giles, C. L.; and Tsoi, A. C. </author> <year> 1997. </year> <title> Lessons in neural network training: Overfitting may be harder than expected. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> 540-545. </pages> <address> Providence, RI: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Lee, Y.; Buchanan, B. G.; and Aronis, J. M. </author> <year> 1998. </year> <title> Knowledge-based learning in exploratory sci ence: Learning rules to predict rodent carcinogenicity. </title> <booktitle> Machine Learning 30 </booktitle> <pages> 217-240. </pages>
Reference: <author> MacKay, D. </author> <year> 1992. </year> <title> Bayesian interpolation. </title> <booktitle> Neural Computation 4 </booktitle> <pages> 415-447. </pages>
Reference: <author> Mingers, J. </author> <year> 1989. </year> <title> An empirical comparison of pruning methods for decision tree induction. </title> <booktitle> Machine Learning 4 </booktitle> <pages> 227-243. </pages>
Reference-contexts: Pruning A simple empirical argument for the second razor might be stated as "Pruning works." Indeed, pruning often leads to models that are both simpler and more accurate than the corresponding unpruned ones <ref> (Mingers 1989) </ref>. However, it can also lead to lower accuracy (Schaffer 1993). It is easy to think of simple problems where pruning can only hurt accuracy (e.g., applying a decision tree algorithm like C4.5 to learning a noise-free, diagonal frontier).
Reference: <author> Mitchell, T. M. </author> <year> 1980. </year> <title> The need for biases in learning generalizations. </title> <type> Technical report, </type> <institution> Rutgers University, Computer Science Department, </institution> <address> New Brunswick, NJ. </address>
Reference: <author> Murphy, P., and Pazzani, M. </author> <year> 1994. </year> <title> Exploring the decision forest: An empirical investigation of Occam's razor in decision tree induction. </title> <journal> Journal of Artificial Intelligence Research 1 </journal> <pages> 257-275. </pages>
Reference: <author> Murthy, S., and Salzberg, S. </author> <year> 1995. </year> <title> Lookahead and pathology in decision tree induction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1025-1031. </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ourston, D., and Mooney, R. J. </author> <year> 1994. </year> <title> Theory refinement combining analytical and empirical methods. </title> <booktitle> Artificial Intelligence 66 </booktitle> <pages> 273-309. </pages>
Reference-contexts: Again, these results do not imply a preference for simpler models, but for restricting search. Suitably constrained, decision-tree or rule induction algorithms can be as stable as simpler ones, and more accurate. Theory revision systems (e.g., <ref> (Ourston & Mooney 1994) </ref>) are an example of this: they can produce accurate theories that are quite complex with comparatively little search, by making incremental changes to an initial theory that is already complex. Physics, Etc.
Reference: <author> Pazzani, M.; Mani, S.; and Shankle, W. R. </author> <year> 1997. </year> <title> Beyond concise and colorful: Learning intelligible rules. </title> <booktitle> In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 235-238. </pages> <address> Newport Beach, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Induced models are also more comprehensible if they are consistent with previous knowledge, even if this makes them more complex <ref> (Pazzani, Mani, & Shankle 1997) </ref>. A better form of the first razor would perhaps state that given two models with the same generalization error, the more comprehensible one should be preferred. What exactly makes a model comprehensible is largely domain-dependent, but also a matter for cognitive research.
Reference: <author> Pearl, J. </author> <year> 1978. </year> <title> On the connection between the complexity and credibility of inferred models. </title> <journal> International Journal of General Systems 4 </journal> <pages> 255-264. </pages>
Reference: <author> Piatetsky-Shapiro, G. </author> <year> 1996. </year> <title> Editorial comments. </title> <publisher> KDD Nuggets 96:28. </publisher>
Reference: <author> Quinlan, J. R., and Cameron-Jones, R. M. </author> <year> 1995. </year> <title> Oversearching and layered search in empirical learning. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1019-1024. </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R., and Rivest, R. L. </author> <year> 1989. </year> <title> Inferring decision trees using the minimum description length principle. </title> <booktitle> Information and Computation 80 </booktitle> <pages> 227-248. </pages>
Reference-contexts: The Information-Theoretic Argument The minimum description length (MDL) principle (Ris-sanen 1978) is perhaps the form in which the second razor is most often applied (e.g., <ref> (Quinlan & Rivest 1989) </ref>). According to this principle, the "best" model is the one which minimizes the total number of bits needed to encode the model and the data.
Reference: <author> Quinlan, J. R. </author> <year> 1996. </year> <title> Bagging, boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 725-730. </pages> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Finally, the success of multiple-model approaches in almost all commonly-used datasets (e.g., <ref> (Quinlan 1996) </ref>) shows that large error reductions can systematically result from sharply increased complexity.
Reference: <author> Rao, J. S., and Potts, W. J. E. </author> <year> 1997. </year> <title> Visualizing bagged decision trees. </title> <booktitle> In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining. </booktitle> <address> Newport Beach, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Rao, R. B.; Gordon, D.; and Spears, W. </author> <year> 1995. </year> <title> For every action, is there really an equal and opposite reaction? Analysis of the conservation law for generalization performance. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 471-479. </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although these results negate the second razor in a mathematical sense, they still leave open the possibility that it will apply in most (or all) real-world domains <ref> (Rao, Gor-don, & Spears 1995) </ref>. This is a matter for empirical study, which the next two sections address.
Reference: <author> Rissanen, J. </author> <year> 1978. </year> <title> Modeling by shortest data description. </title> <type> Automatica 14 </type> <pages> 465-471. </pages>
Reference-contexts: Criteria of this type include AIC (Akaike 1978), BIC (Schwarz 1978), and many others. Similar criteria with an information-theoretic interpretation, like MML (Wallace & Boulton 1968) and MDL <ref> (Rissanen 1978) </ref> are discussed below. Consider BIC, the first criterion to be explicitly proposed as an approximation to Bayesian model averaging.
Reference: <author> Schaffer, C. </author> <year> 1993. </year> <title> Overfitting avoidance as bias. </title> <booktitle> Machine Learning 10 </booktitle> <pages> 153-178. </pages>
Reference-contexts: Pruning A simple empirical argument for the second razor might be stated as "Pruning works." Indeed, pruning often leads to models that are both simpler and more accurate than the corresponding unpruned ones (Mingers 1989). However, it can also lead to lower accuracy <ref> (Schaffer 1993) </ref>. It is easy to think of simple problems where pruning can only hurt accuracy (e.g., applying a decision tree algorithm like C4.5 to learning a noise-free, diagonal frontier).
Reference: <author> Schaffer, C. </author> <year> 1994. </year> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 259-265. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schuurmans, D.; Ungar, L. H.; and Foster, D. P. </author> <year> 1997. </year> <title> Characterizing the generalization performance of model selection strategies. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> 340-348. </pages> <address> Nashville, TN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schuurmans, D. </author> <year> 1997. </year> <title> A new metric-based approach to model selection. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> 552-558. </pages> <address> Providence, RI: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Schwarz, G. </author> <year> 1978. </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics 6 </journal> <pages> 461-464. </pages>
Reference-contexts: Criteria of this type include AIC (Akaike 1978), BIC <ref> (Schwarz 1978) </ref>, and many others. Similar criteria with an information-theoretic interpretation, like MML (Wallace & Boulton 1968) and MDL (Rissanen 1978) are discussed below. Consider BIC, the first criterion to be explicitly proposed as an approximation to Bayesian model averaging.
Reference: <author> Tornay, S. C. </author> <year> 1938. </year> <title> Ockham: Studies and Selections. </title> <address> La Salle, IL: </address> <publisher> Open Court. </publisher>
Reference: <author> Vapnik, V. N. </author> <year> 1995. </year> <title> The Nature of Statistical Learning Theory. </title> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The model structure class = sign (sin ax), with a single parameter, has an infinite VC dimension, because it can discriminate an arbitrarily large, arbitrarily labeled set of points on the x axis <ref> (Vapnik 1995, p. 78) </ref>. Overfitting Is Due to Multiple Testing According to conventional wisdom, overfitting is caused by overly complex models, and Occam's razor combats it by introducing a preference for simpler ones.
Reference: <author> Wallace, C. S., and Boulton, D. M. </author> <year> 1968. </year> <title> An information measure for classification. </title> <journal> Computer Journal 11 </journal> <pages> 185-194. </pages>
Reference-contexts: Criteria of this type include AIC (Akaike 1978), BIC (Schwarz 1978), and many others. Similar criteria with an information-theoretic interpretation, like MML <ref> (Wallace & Boulton 1968) </ref> and MDL (Rissanen 1978) are discussed below. Consider BIC, the first criterion to be explicitly proposed as an approximation to Bayesian model averaging. <p> However, there is no guarantee that it will select the most accurate model. Rissanen simply proposes it as a fundamental principle. The closely-related minimum message length (MML) principle <ref> (Wallace & Boulton 1968) </ref> is derived by taking the logarithm of Bayes' theorem and noting that, according to information theory (Cover & Thomas 1991), logarithms of probabilities can be seen as (minus) the lengths of the most efficient codes for the corresponding events.
Reference: <author> Webb, G. I. </author> <year> 1996. </year> <title> Further experimental evidence against the utility of Occam's razor. </title> <journal> Journal of Artificial Intelligence Research 4 </journal> <pages> 397-417. </pages>
Reference: <author> Webb, G. I. </author> <year> 1997. </year> <title> Decision tree grafting. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 846-851. </pages> <address> Nagoya, Japan: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Wolpert, D. </author> <year> 1996. </year> <title> The lack of a priori distinctions between learning algorithms. </title> <booktitle> Neural Computation 8 </booktitle> <pages> 1341-1390. </pages>
References-found: 56

