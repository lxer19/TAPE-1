URL: ftp://ftp.cse.unsw.edu.au/pub/users/andrewt/publications/1997/3.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/publications/1997/SCSE_publications.html
Root-URL: 
Email: Email: fbjtong,jas,anneg@cse.unsw.edu.au  
Phone: Telephone: +61 2 385 3980 Fax: +61 2 385 1813  
Title: Query Size Estimation using Machine Learning  
Author: Banchong Harangsri John Shepherd Anne Ngu 
Keyword: Query Size Estimation, Query Opti-misation, Machine Learning  
Address: Sydney 2052, AUSTRALIA.  
Affiliation: School of Computer Science and Engineering, The University of New South Wales,  
Abstract: In a previous paper [6], we introduced the notion of using machine learning techniques to solve the problem of query size estimation in database query optimisation. In this paper, we build on this work by describing a new generic algorithm to correct the training set of queries for our machine learning method in response to updates. The training set correction algorithm is not only useful in the context of our machine learning approach, but is also useful for improving existing query size estimation methods whose performance deteriorates in the presence of high update loads. A by-product of our correction algorithm is that training sets can be fixed-size, allowing the error-level to be set in advance. Experimental results show that our machine learning technique performs well (and better than alternative methods) after the correction algorithm is applied. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen and C. J. Stone. </author> <title> Classification and Regression Tress. </title> <publisher> Chapman & Hall, Inc., </publisher> <year> 1984. </year>
Reference-contexts: Our overall approach is to derive size estimation functions using machine learning techniques. We proceed as follows: 1. use feedback from a training set of queries to construct a model tree (or regression tree) <ref> [1] </ref>; the model tree contains a collection of spe cialised, accurate cost estimation functions 2. to estimate the result size of a given query (q u ): determine the three queries fq 1 ; q 2 ; q 3 g in the training set that are most similar to q u <p> The machine learning mechanism that we use is a combination of model-based learning and instance-based learning originally proposed by Quinlan [15]. It involves construction of a model tree, a kind of regression tree originally proposed by Breiman et. al. <ref> [1] </ref>. Our implementation of the method has been tailored for use in this application, the two major differences being that we have omitted the pruning and smoothing procedures from Quinlan's original implementation.
Reference: [2] <author> C. M. Chen and N. Roussopoulos. </author> <title> Adaptive Selectivity Estimation using Query Feedback. </title> <booktitle> In Proceedings of 1994 ACM-SIGMOD International Conference on Management of Data, </booktitle> <year> 1994. </year>
Reference-contexts: Previous work can be classified into four categories <ref> [17, 2] </ref>: non-parametric, parametric, sampling and curve-fitting. We briefly describe each of them here; more details can be found in the references given above. Non-parametric methods are table- or histogram-based [14, 13]. <p> The accuracy of these methods is not adversely affected by database updates, as the samples are determined dynamically and always reflect the current data distribution. Curve-fitting methods <ref> [17, 2] </ref> are based on polynomial regression to find the set of coefficients that minimises some least-squared error criterion. The actual criterion differs substantially from method to method. The curve-fitting method proposed by [17], scans entire relations and uses regression to determine the distributions of attribute values in each relation. <p> That is, as long as the distributions of attribute values remain fixed, the method performs satisfactorily. However, if the distributions change, then the accuracy of the size estimates may deteriorate significantly. To repair the situation, re-scanning of relations is required. The curve-fitting method proposed by <ref> [2] </ref>, called adaptive selectivity estimation (or ASE), also attempts to determine data distributions using regression. However, instead of scanning relations and computing distributions directly, it uses feedback from query execution. <p> We believe that our approach is more effective, and more widely applicable than the approach used by ASE. Our method has these advantages over ASE: generic update algorithm: The algorithm we describe in section 4, can also be used (with minor modifications) with the size estimation methods proposed in <ref> [17, 2] </ref>. In other words, we can adapt size-estimation methods proposed for retrieval-only or retrieval-intensive environments for use with dynamic (high-update) databases. <p> In other words, we can adapt size-estimation methods proposed for retrieval-only or retrieval-intensive environments for use with dynamic (high-update) databases. Details of how to achieve this may be found in [7]. fixed-size list: The scheme in <ref> [2] </ref> maintains a list of recent query feedback, but also needs to maintain outdated query feedback (i.e. feedback that was obtained prior to updates). <p> = x 121 endif22 endif23 else24 if relational operator op rel is "=" then25 if update type (r) is "insert" then26 x = x + 127 else28 x = x 129 endif30 endif31 endif32 endif33 endfor34 endfor35 ated four relations which use the same data distri butions as specified in <ref> [2] </ref>. Note that despite the use of the same data distributions, (1) the queries used in this paper and [2] are different and (2) the underlying attribute values generated by our random generator and ASE's generator are not the same. <p> "insert" then26 x = x + 127 else28 x = x 129 endif30 endif31 endif32 endif33 endfor34 endfor35 ated four relations which use the same data distri butions as specified in <ref> [2] </ref>. Note that despite the use of the same data distributions, (1) the queries used in this paper and [2] are different and (2) the underlying attribute values generated by our random generator and ASE's generator are not the same. The data distributions are specified for a sin gle attribute of each relation. Tables 3 (a) and 3 (b) from [2] describe the parameters of distributions. <p> distributions, (1) the queries used in this paper and <ref> [2] </ref> are different and (2) the underlying attribute values generated by our random generator and ASE's generator are not the same. The data distributions are specified for a sin gle attribute of each relation. Tables 3 (a) and 3 (b) from [2] describe the parameters of distributions. Note that, due to space limitations, we present results only for the normal and chi-squared distri butions in this paper. <p> The other two measures, relative and absolute errors, were also used in <ref> [2] </ref> to measure the performance of ASE. <p> The aim of this experiment was to demonstrate that the combination of M5 and our generic update algorithm still performs very well even in the presence of very high update loads. We follow the update workload specification as used in <ref> [2] </ref>. The basic idea is to alter the distribution of values of the attribute of interest. <p> relativ e error p er query (queries) (e) Avg Rel. err, (10) 1 3 5 ase m5 absolute error p er query (queries) (f) Avg Abs. err, (10) LOAD4: (11,2250,U (150; 550), [-150,550], 0.8), (17,2250,N (150; 80), [100,300], 0.7), (29,2250,F (10; 6), [75,325], 0.6) The first three were used in <ref> [2] </ref> and each uses the same data distributions (i.e., uniform distributions for all or normal distributions for all), whereas the last was designed to see how M5 performs in the presence of update workload over a range of different data distributions. <p> Four experiments with the workloads specified above were conducted using the relation with the normal distribution (see Table 3 (b)). We used a training set containing 400 queries. Another 40 queries in the test set (the same queries as used in <ref> [2] </ref>) were used to measure the performance of M5. The scatter plots of the actual against estimated values of query result size are displayed in Figure 5 3 . <p> This algorithm can also be used to maintain data structures in other query size estimation methods: the distinct-value-frequency list (x i ; f (x i )) in [17] and the query feedback list (l i ; h i ; s i ) in <ref> [2] </ref>. This update algorithm improves the performance of all of these methods in the presence of updates. The following work remains to be done to improve our method: * We need to extend the current machine learning technique to deal with more complex selection criteria and with joins.
Reference: [3] <author> S. Christodoulakis. </author> <title> Estimating Block Transfers and Join Sizes. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference, </booktitle> <pages> pages 40-54, </pages> <year> 1983. </year>
Reference-contexts: Histogram-based methods give more accurate results when the number of partitions is higher. Because histograms need to be stored, the quest for high accuracy can lead to considerable storage costs. These methods can deal with data changing over time, but at the cost of periodically rescanning relations. Parametric methods <ref> [16, 3, 4] </ref> rely on assumptions about the underlying data distributions of attributes. These methods have low storage costs (simply the parameters of the distributions) and can produce estimates very quickly.
Reference: [4] <author> S. Christodoulakis. </author> <title> Estimating Record Selectivities. </title> <journal> Information System, </journal> <volume> Volume 8, Number 2, </volume> <pages> pages 105-115, </pages> <year> 1983. </year>
Reference-contexts: Histogram-based methods give more accurate results when the number of partitions is higher. Because histograms need to be stored, the quest for high accuracy can lead to considerable storage costs. These methods can deal with data changing over time, but at the cost of periodically rescanning relations. Parametric methods <ref> [16, 3, 4] </ref> rely on assumptions about the underlying data distributions of attributes. These methods have low storage costs (simply the parameters of the distributions) and can produce estimates very quickly.
Reference: [5] <author> P. Haas and A. Swami. </author> <title> Sequential Sampling Procedures for Query Size Estimation. </title> <booktitle> In ACM SIGMOD Conference on the Management of Data, </booktitle> <pages> pages 341-350, </pages> <year> 1992. </year>
Reference-contexts: These methods also have problems if the data distribution changes over time as a result of database updates. Sampling methods <ref> [11, 5] </ref> execute the queries to be optimised on small subsets (samples) of the real database, and use the results of these trials to determine cost estimates. These methods can give very accurate estimates for complex query plans, since they are effectively "pre-executing" plans to determine the costs.
Reference: [6] <author> B. Harangsri, J. Shepherd and A. Ngu. </author> <title> Query Size Estimation using Machine Learning. </title> <booktitle> In Proceedings of the 1996 International Conference on Artificial Intelligence (TAAI-96) (joint with 1996 International Computer Symposium (ICS'96)), </booktitle> <institution> National Sun Yat-Sen University, Kaohsiung, Taiwan, R.O.C., </institution> <month> Decem-ber </month> <year> 1996. </year>
Reference-contexts: join, but we assume here that there is some way to extract only the simple operations from these queries. 3 Size Estimation with Retrieval Queries In this section, we summarise our method of using machine learning to solve the query size estimation problem; this method is described in detail in <ref> [6] </ref>. The machine learning mechanism that we use is a combination of model-based learning and instance-based learning originally proposed by Quinlan [15]. It involves construction of a model tree, a kind of regression tree originally proposed by Breiman et. al. [1]. <p> To estimate the size of an unseen query q u , we proceed as follows: (1) find the three queries fq 1 ; q 2 ; q 3 g in the training set Q that are "most similar" to q u according to a similarity measure simval q i (see <ref> [6] </ref>) (2) use the model tree to compute an estimate of the size of the result of q u (i.e. compute M (q u )) (3) produce the final size estimate ^ S q u by combining M (q u ) with the estimated sizes for each of q 1 , <p> i (M (q i ) M (q u )) ; i = 1; 2; 3 3 X ~ S q i fl w q i ; w q i = simval q i P 3 i=1 simval q i : The detailed rationale for this approach can be found in <ref> [6] </ref>. With respect to performance, there are two aspects to be considered in this scheme: the cost of building the model tree, and the cost of using the model tree to compute query size estimates. Both of these costs are dependent on the size of the training set.
Reference: [7] <author> B. Harangsri, J. Shepherd and A. Ngu. </author> <title> Query Size Estimation using Machine Learning. </title> <type> Technical report, </type> <institution> The University of New South Wales, School of Computer Science and Engineering, </institution> <address> Sydney 2052, AUSTRALIA, </address> <year> 1996. </year>
Reference-contexts: Experimental work in <ref> [7] </ref> has shown that, of the methods mentioned above, ASE and our machine learning produce the most accurate estimates, and so we consider only these two methods in the experiments in this paper. <p> In other words, we can adapt size-estimation methods proposed for retrieval-only or retrieval-intensive environments for use with dynamic (high-update) databases. Details of how to achieve this may be found in <ref> [7] </ref>. fixed-size list: The scheme in [2] maintains a list of recent query feedback, but also needs to maintain outdated query feedback (i.e. feedback that was obtained prior to updates). <p> Note also that we have ex perimented with a range of other relations and data distributions and found that the results presented here hold generally. Full details of our experiments may be found in <ref> [7] </ref>. We measure the performance of query size es timation using three separate measures of the dis crepancy between estimated and observed values: residual, relative error and absolute error. <p> The following work remains to be done to improve our method: * We need to extend the current machine learning technique to deal with more complex selection criteria and with joins. Several possible approaches to this are presented in <ref> [7] </ref>. * We also need to demonstrate experimentally that an extension of our method produces accurate query size estimates for these more complex kinds of queries. <p> 10000 actual estimated x "conv-norm-m5.scat" (b) M5, N (200; 150) 5000 15000 0 5000 10000 15000 20000 actual estimated x "conv-chi-ase.scat" (c) ASE, (10) 5000 15000 0 5000 10000 15000 20000 actual estimated x "conv-chi-m5.scat" (d) M5, (10) with more complex selection criteria suggest that out method still performs effectively <ref> [7] </ref>.
Reference: [8] <author> W. Hou, G. Ozsoyoglu and B. K. Taneja. </author> <title> Statistical Estimators for Relational Algebra Expressions. </title> <booktitle> In Proceedings of the ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, </booktitle> <pages> pages 276-287, </pages> <year> 1988. </year>
Reference: [9] <author> Y. E. Ioannidis and S. Christodoulakis. </author> <title> On the Propagation of Errors in the Size of Join Results. In Proceedings of the ACM-SIGMOD 0 4000 8000 12000 0 2000 4000 6000 8000 10000 12000 14000 actual estimated x "updw1-m5.scat" (a) Workload 1 0 4000 8000 12000 0 2000 4000 6000 8000 10000 12000 14000 actual estimated x "updw2-m5.scat" (b) Workload 2 0 4000 8000 12000 actual estimated x "updw3-m5.scat" (c) Workload 3 0 4000 8000 12000 actual estimated x "updw4-m5.scat" (d) Workload 4 Intl. </title> <booktitle> Conf. on Management of Data, </booktitle> <pages> pages 268-277, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction A query optimiser for a database system aims to determine the most efficient execution plan for each query submitted to the system. Choosing an efficient plan relies on cost estimates derived from statistics maintained by the database system. Analysis in <ref> [9] </ref> showed that even very small inaccuracies in the cost estimates can cause the optimiser to choose a very poor (suboptimal) execution plan. Very accurate estimates for the cost of database operations are thus critical to the effective operation of query optimisers.
Reference: [10] <author> N. Kamel and R. King. </author> <title> A Method of Data Distribution Based on Texture Analysis. </title> <booktitle> In Proceedings of the ACM SIGMOD Intl. Conf. on Management of Data, </booktitle> <pages> pages 319-325, </pages> <year> 1985. </year>
Reference: [11] <author> R. J. Lipton, J. F. Naughton and D. A. Schneider. </author> <title> Practical Selectivity Estimation through Adaptive Sampling. </title> <booktitle> In Proceedings of ACM SIGMOD, </booktitle> <pages> pages 1-12, </pages> <year> 1990. </year>
Reference-contexts: These methods also have problems if the data distribution changes over time as a result of database updates. Sampling methods <ref> [11, 5] </ref> execute the queries to be optimised on small subsets (samples) of the real database, and use the results of these trials to determine cost estimates. These methods can give very accurate estimates for complex query plans, since they are effectively "pre-executing" plans to determine the costs.
Reference: [12] <author> M. Mannino, P. Chu and T. Sager. </author> <title> Statistical Profile Estimation in Database Systems. </title> <journal> ACM Computing Surveys, </journal> <volume> Volume 20, Number 3, </volume> <pages> pages 191-221, </pages> <month> september </month> <year> 1988. </year>
Reference: [13] <author> M. Muralikrishma and D. DeWitt. </author> <title> Equi-depth Histograms for Estimating Selectivity Factors for Multi-Dimensional Queries. </title> <booktitle> In Proceedings of the ACM SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 28-36, </pages> <year> 1988. </year>
Reference-contexts: Previous work can be classified into four categories [17, 2]: non-parametric, parametric, sampling and curve-fitting. We briefly describe each of them here; more details can be found in the references given above. Non-parametric methods are table- or histogram-based <ref> [14, 13] </ref>. A histogram may be built for an attribute in a relation by partitioning the attribute domain into intervals and counting the number of tuples in each interval. These methods require scanning an entire relation to build up attribute histograms.
Reference: [14] <author> G. Piatetsky-Shapiro and C. Connell. </author> <title> Accurate Estimation of the Number of Tuples Satisfying a Condition. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference, </booktitle> <pages> pages 256-276, </pages> <address> 1984. Boston, Mass, June, </address> <publisher> ACM, </publisher> <address> New York. </address>
Reference-contexts: Previous work can be classified into four categories [17, 2]: non-parametric, parametric, sampling and curve-fitting. We briefly describe each of them here; more details can be found in the references given above. Non-parametric methods are table- or histogram-based <ref> [14, 13] </ref>. A histogram may be built for an attribute in a relation by partitioning the attribute domain into intervals and counting the number of tuples in each interval. These methods require scanning an entire relation to build up attribute histograms.
Reference: [15] <author> J. R. Quinlan. </author> <title> Combining Instance-Based and Model-Based Learning. </title> <booktitle> In Proceedings of Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The machine learning mechanism that we use is a combination of model-based learning and instance-based learning originally proposed by Quinlan <ref> [15] </ref>. It involves construction of a model tree, a kind of regression tree originally proposed by Breiman et. al. [1]. <p> The machine learning technique described in Section 3 was implemented in a learning machine called M5 developed by Quinlan <ref> [15] </ref>. Throughout this section, we also refer to our overall query size estimation method by the name M5, even though the M5 machine learning is just one component. Our experiments investigated two aspects of query size estimation.
Reference: [16] <editor> P.G. Selinger, M.M. Astrahan, D.D. Chamber-lin, R.A. Lorie and T.G. Price. </editor> <title> Access Path Selection in a Relational Database Management System. </title> <booktitle> In ACM SIGMOD, </booktitle> <pages> pages 23-34, </pages> <address> 1979. Boston, MA, </address> <month> June </month> <year> 1979. </year>
Reference-contexts: Histogram-based methods give more accurate results when the number of partitions is higher. Because histograms need to be stored, the quest for high accuracy can lead to considerable storage costs. These methods can deal with data changing over time, but at the cost of periodically rescanning relations. Parametric methods <ref> [16, 3, 4] </ref> rely on assumptions about the underlying data distributions of attributes. These methods have low storage costs (simply the parameters of the distributions) and can produce estimates very quickly.
Reference: [17] <author> W. Sun, Y. Ling, N. Rishe and Y. Deng. </author> <title> An Instant and Accurate Size Estimation Method for Joins and Selection in a Retrieval-Intensive Environment. </title> <booktitle> In Proceedings of ACM SIG-MOD, </booktitle> <pages> pages 79-88, </pages> <year> 1993. </year>
Reference-contexts: Previous work can be classified into four categories <ref> [17, 2] </ref>: non-parametric, parametric, sampling and curve-fitting. We briefly describe each of them here; more details can be found in the references given above. Non-parametric methods are table- or histogram-based [14, 13]. <p> The accuracy of these methods is not adversely affected by database updates, as the samples are determined dynamically and always reflect the current data distribution. Curve-fitting methods <ref> [17, 2] </ref> are based on polynomial regression to find the set of coefficients that minimises some least-squared error criterion. The actual criterion differs substantially from method to method. The curve-fitting method proposed by [17], scans entire relations and uses regression to determine the distributions of attribute values in each relation. <p> Curve-fitting methods [17, 2] are based on polynomial regression to find the set of coefficients that minimises some least-squared error criterion. The actual criterion differs substantially from method to method. The curve-fitting method proposed by <ref> [17] </ref>, scans entire relations and uses regression to determine the distributions of attribute values in each relation. This approach is effective only for low-update database systems. That is, as long as the distributions of attribute values remain fixed, the method performs satisfactorily. <p> We believe that our approach is more effective, and more widely applicable than the approach used by ASE. Our method has these advantages over ASE: generic update algorithm: The algorithm we describe in section 4, can also be used (with minor modifications) with the size estimation methods proposed in <ref> [17, 2] </ref>. In other words, we can adapt size-estimation methods proposed for retrieval-only or retrieval-intensive environments for use with dynamic (high-update) databases. <p> This algorithm can also be used to maintain data structures in other query size estimation methods: the distinct-value-frequency list (x i ; f (x i )) in <ref> [17] </ref> and the query feedback list (l i ; h i ; s i ) in [2]. This update algorithm improves the performance of all of these methods in the presence of updates.
References-found: 17

