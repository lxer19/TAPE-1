URL: http://www.csc.calpoly.edu/~tpederse/aaai99.ps.gz
Refering-URL: http://www.csc.calpoly.edu/~tpederse/pubs.html
Root-URL: http://www.csc.calpoly.edu
Email: tpederse@csc.calpoly.edu  
Title: Search Techniques for Learning Probabilistic Models of Word Sense Disambiguation  
Author: Ted Pedersen 
Address: San Luis Obispo, CA 93407  
Affiliation: Palo, Alto, CA  Department of Computer Science California Polytechnic State University  
Date: March 22-24, 1999,  
Note: Appears in the Working Notes of the AAAI Spring Symposium on Search Techniques for Problem Solving Under Uncertainty and Incomplete Information,  
Abstract: The development of automatic natural language understanding systems remains an elusive goal. Given the highly ambiguous nature of the syntax and semantics of natural language, it is not possible to develop rule-based approaches to understanding even very limited domains of text. The difficulty in specifying a complete set of rules and their exceptions has led to the rise of probabilistic approaches where models of natural language are learned from large corpora of text. However, this has proven a challenge since natural language data is both sparse and skewed and the space of possible models is huge. In this paper we discuss several search techniques used in learning the structure of probabilistic models of word sense disambiguation. We present an experimental comparison of backward and forward sequential searches as well as a model averaging approach to the problem of resolving the meaning of ambiguous words in text. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> 1974. </year> <title> A new look at the statistical model identification. </title> <journal> IEEE Transactions on Automatic Control AC-19(6):716-723. </journal>
Reference-contexts: Evaluation Criteria The degradation and improvement in fit of candidate models relative to the current model is assessed by an evaluation criterion. We employ Akaike's Information Criteria (AIC) <ref> (Akaike 1974) </ref> and the Bayesian Information Criteria (BIC) (Schwarz 1978) as evaluation criteria.
Reference: <author> Bruce, R., and Wiebe, J. </author> <year> 1994. </year> <title> Word-sense disambiguation using decomposable models. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 139-146. </pages>
Reference-contexts: This model then serves as a classifier that determines the most probable sense of an ambiguous word, given the context in which it occurs. In this paper context is represented by a set of features developed in <ref> (Bruce & Wiebe 1994) </ref>. There is one morphological feature describing the ambigu-ous word, four part-of-speech features describing the surrounding words, and three co-occurrence features indicating if certain key words occur in the sentence with the ambiguous word. <p> This is the selected model and is the ultimate result of the sequential model selection process. Search Strategy We employ both backward sequential search (Wermuth 1976) and forward sequential search (Dempster 1972) as search strategies. Backward sequential search for probabilistic models of word sense disambiguation was introduced in <ref> (Bruce & Wiebe 1994) </ref> while forward sequential search was introduced in (Pedersen, Bruce, & Wiebe 1997). <p> Thus a Naive Mix formulated with backward search can potentially contain many irrelevant dependencies while a forward search only includes the most important dependencies. Experimental Results The sense-tagged text used in these experiments was created by <ref> (Bruce & Wiebe 1994) </ref> and is fully described in (Bruce, Wiebe, & Pedersen 1996).
Reference: <author> Bruce, R.; Wiebe, J.; and Pedersen, T. </author> <year> 1996. </year> <title> The measure of a model. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> 101-112. </pages>
Reference-contexts: Thus a Naive Mix formulated with backward search can potentially contain many irrelevant dependencies while a forward search only includes the most important dependencies. Experimental Results The sense-tagged text used in these experiments was created by (Bruce & Wiebe 1994) and is fully described in <ref> (Bruce, Wiebe, & Pedersen 1996) </ref>. It consists of every sentence from the ACL/DCI Wall Street Journal corpus that contains any of the nouns interest, bill, concern, and drug, any of the verbs close, help, agree, and include, or any of the adjectives chief, public, last, and common. <p> As a result backward search with BIC is more aggressive in removing dependencies than is backward search with AIC; likewise forward search with BIC is less likely to add dependencies than is forward search with AIC. The feature set employed is shown in <ref> (Bruce, Wiebe, & Pedersen 1996) </ref> to be very indicative of word senses; thus the tendency of BIC to eliminate or not include features in the model works to its disadvantage. However, BIC may be the most appropriate evaluation criterion when dealing with data that includes irrelevant features.
Reference: <author> Darroch, J.; Lauritzen, S.; and Speed, T. </author> <year> 1980. </year> <title> Markov fields and log-linear interaction models for contingency tables. </title> <journal> The Annals of Statistics 8(3) </journal> <pages> 522-539. </pages>
Reference-contexts: The three words represented by these features are highly indicative of particular senses, as determined by a statistical test of independence. Decomposable Models We restrict our attention to decomposable log-linear models <ref> (Darroch, Lauritzen, & Speed 1980) </ref>, a subset of the class of graphical models (Whittaker 1990). In any graphical model, feature variables are either dependent or conditionally independent of one another.
Reference: <author> Dempster, A. </author> <year> 1972. </year> <title> Covariance selection. </title> <booktitle> Biomet-ricka 28 </booktitle> <pages> 157-175. </pages>
Reference-contexts: This is the selected model and is the ultimate result of the sequential model selection process. Search Strategy We employ both backward sequential search (Wermuth 1976) and forward sequential search <ref> (Dempster 1972) </ref> as search strategies. Backward sequential search for probabilistic models of word sense disambiguation was introduced in (Bruce & Wiebe 1994) while forward sequential search was introduced in (Pedersen, Bruce, & Wiebe 1997).
Reference: <author> Duda, R., and Hart, P. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: We show the disambiguation accuracy of models selected using both forward and backward searches as well as a Naive Mix formulated from a forward sequential search using AIC. In addition, we report the accuracy of two no-search techniques, the majority classify and the Naive Bayesian classifier <ref> (Duda & Hart 1973) </ref>. The majority classifier assumes that the parametric form is the model of independence and classifies every usage of an ambiguous word with its most frequent sense from the training data.
Reference: <author> Pedersen, T., and Bruce, R. </author> <year> 1997. </year> <title> A new supervised learning algorithm for word sense disambiguation. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> 604-609. </pages>
Reference-contexts: We present a number of different approaches to locating such models. Sequential model selection finds a single parametric form that is judged to achieve the best balance between model complexity and fit for a given corpus of text. We extend this methodology with the Naive Mix <ref> (Pedersen & Bruce 1997) </ref>, an averaged probabilistic model based on the sequence of parametric forms generated during a sequential model search. This paper includes an experimental comparison of these approaches and discusses possible further extensions to these methodologies. <p> Search Strategy We employ both backward sequential search (Wermuth 1976) and forward sequential search (Dempster 1972) as search strategies. Backward sequential search for probabilistic models of word sense disambiguation was introduced in (Bruce & Wiebe 1994) while forward sequential search was introduced in <ref> (Pedersen, Bruce, & Wiebe 1997) </ref>. Forward searches evaluate models of increasing complexity based on how much candidate models improve upon the fit of the current model, while backward searches evaluate candidate models based on how much they degrade the fit of the current model.
Reference: <author> Pedersen, T.; Bruce, R.; and Wiebe, J. </author> <year> 1997. </year> <title> Sequential model selection for word sense disambiguation. </title> <booktitle> In Proceedings of the Fifth Conference on Applied Natural Language Processing, </booktitle> <pages> 388-395. </pages>
Reference-contexts: We present a number of different approaches to locating such models. Sequential model selection finds a single parametric form that is judged to achieve the best balance between model complexity and fit for a given corpus of text. We extend this methodology with the Naive Mix <ref> (Pedersen & Bruce 1997) </ref>, an averaged probabilistic model based on the sequence of parametric forms generated during a sequential model search. This paper includes an experimental comparison of these approaches and discusses possible further extensions to these methodologies. <p> Search Strategy We employ both backward sequential search (Wermuth 1976) and forward sequential search (Dempster 1972) as search strategies. Backward sequential search for probabilistic models of word sense disambiguation was introduced in (Bruce & Wiebe 1994) while forward sequential search was introduced in <ref> (Pedersen, Bruce, & Wiebe 1997) </ref>. Forward searches evaluate models of increasing complexity based on how much candidate models improve upon the fit of the current model, while backward searches evaluate candidate models based on how much they degrade the fit of the current model.
Reference: <author> Pedersen, T. </author> <year> 1998. </year> <title> Learning Probabilistic Models of Word Sense Disambiguation. </title> <type> Ph.D. Dissertation, </type> <institution> Southern Methodist University. </institution>
Reference-contexts: The models that are ultimately selected presumably differ somewhat and could be averaged together in a randomized variant of the Naive Mix. Acknowledgments Portions of this paper appear in a different form in the author's Ph.D. thesis <ref> (Pedersen 1998) </ref>. That work was directed by Dr. Rebecca Bruce and her assistance and guidance is deeply appreciated.
Reference: <author> Schwarz, G. </author> <year> 1978. </year> <title> Estimating the dimension of a model. </title> <journal> The Annals of Statistics 6(2) </journal> <pages> 461-464. </pages>
Reference-contexts: Evaluation Criteria The degradation and improvement in fit of candidate models relative to the current model is assessed by an evaluation criterion. We employ Akaike's Information Criteria (AIC) (Akaike 1974) and the Bayesian Information Criteria (BIC) <ref> (Schwarz 1978) </ref> as evaluation criteria.
Reference: <author> Wermuth, N. </author> <year> 1976. </year> <title> Model search among multiplicative models. </title> <type> Biometrics 32 </type> <pages> 253-263. </pages>
Reference-contexts: This is the selected model and is the ultimate result of the sequential model selection process. Search Strategy We employ both backward sequential search <ref> (Wermuth 1976) </ref> and forward sequential search (Dempster 1972) as search strategies. Backward sequential search for probabilistic models of word sense disambiguation was introduced in (Bruce & Wiebe 1994) while forward sequential search was introduced in (Pedersen, Bruce, & Wiebe 1997).
Reference: <author> Whittaker, J. </author> <year> 1990. </year> <title> Graphical Models in Applied Mul-tivariate Statistics. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: The three words represented by these features are highly indicative of particular senses, as determined by a statistical test of independence. Decomposable Models We restrict our attention to decomposable log-linear models (Darroch, Lauritzen, & Speed 1980), a subset of the class of graphical models <ref> (Whittaker 1990) </ref>. In any graphical model, feature variables are either dependent or conditionally independent of one another.
References-found: 12

