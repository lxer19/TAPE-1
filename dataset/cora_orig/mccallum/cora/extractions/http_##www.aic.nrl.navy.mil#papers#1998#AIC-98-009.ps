URL: http://www.aic.nrl.navy.mil/papers/1998/AIC-98-009.ps
Refering-URL: http://www.aic.nrl.navy.mil/~aha/pub-details.html
Root-URL: 
Email: faha,breslowg@aic.nrl.navy.mil  
Title: Comparing Simplification Procedures for Decision Trees on an Economics Classification Task  
Author: David W. Aha Leonard A. Breslow 
Keyword: Case-based reasoning, indexing, decision trees simplifying decision trees, classification  
Note: NRL Technical Report NRL/FR/551098-9881, crosslisted at NCARAI as AIC-98-009  
Web: http://www.aic.nrl.navy.mil/~ faha,breslowg  
Address: Code 5510 Washington, DC 20375  
Affiliation: Navy Center for Applied Research in Artificial Intelligence Naval Research Laboratory,  
Abstract: Several commercial case-based reasoning (CBR) shells now use decision trees to index cases, including REMIND (Cognitive Systems Inc.), KATE (AcknoSoft), THE EASY REASONER (The Haley Enterprise), and KNOWLEDGE BUILDER (ServiceSoft). These trees serve to expedite case retrieval and to generate comprehensible explanations of case retrieval behavior. Unfortunately, induced trees are often large and complex, reducing their explanatory power. To combat this problem, some commercial systems contain an option for simplifying decision trees. However, while many methods for simplifying decision trees exist, they have not been systematically compared and most have not been applied to case retrieval. This report builds on our previous survey and initial empirical comparison of tree simplification procedures. In this report, we compare them on a specific, challenging task that is the focus of an existing CBR effort. We examine which tree simplification procedures are useful for this task and suggest which ones should be included in a commercial CBR tool. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aamodt, A. and E. </author> <title> Plaza (1994), Case-Based Reasoning: Foundational Issues, Methodological Variations, and System Approaches, AI Communications 7, 3959. Comparing Simplification Procedures for Decision Trees 15 Aha, D.W. and R.L. Bankert (1996), A Comparative Evaluation of Sequential Feature Selection Algorithms, </title> <booktitle> in Artificial Intelligence and Statistics V, </booktitle> <publisher> (Springer-Verlag, </publisher> <address> New York), </address> <pages> pp. </pages> <year> 199206. </year>
Reference-contexts: CBR is a popular artificial intelligence problem solving methodology that implements a form of computational analogy, where cases are commonly thought of as hproblem,solutioni pairs. The case-based reasoning problem-solving cycle can be defined by the following four steps <ref> (Aamodt and Plaza, 1994) </ref>: * Retrieve: Given a new problem, retrieve a set of cases from a given case library whose problems are similar to the new problem. * Reuse: Attempt to apply the solutions of one or more of these stored cases to the new problem. * Revise: Based on
Reference: <author> Auer, P., R.C. Holte, and W. </author> <title> Maass (1995), Theory and Applications of Agnostic PAC-Learning with Small Decision Trees, </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> Tahoe City, CA, </address> <pages> pp. </pages> <publisher> 2129 (Morgan Kaufmann, </publisher> <address> San Francisco, CA). </address>
Reference-contexts: This can occur when specific combinations of indices (i.e., along paths of the complete tree) have high quality, but they individually do not exceed the predetermined threshold. Nonetheless, we selected a pre-pruner named T2 <ref> (Auer et al., 1995) </ref> for our experiments because (1) it performed well in comparison studies and (2) it is an extreme foil for our baseline (i.e., C4.5 R7 without post-pruning) in that it limits trees to a depth of 2. Second, post-pruners simplify trees in a post-processing stage.
Reference: <author> Breslow, L. and D.W. </author> <title> Aha (1997a), Simplifying Decision Trees: A Survey, </title> <journal> Knowledge Engineering Review 12, </journal> <volume> 140. </volume>
Reference-contexts: Not surprisingly, the trees they generate are large and difficult to comprehend, which is unacceptable to many users. We surveyed the literature for algorithms that simplify decision trees and created a categorization framework for them <ref> (Breslow and Aha, 1997a) </ref>. We also obtained and installed copies of at least one procedure for each of our framework's subcategories, allowing us to compare them empirically on benchmark classification tasks (Breslow and Aha, 1997b).
Reference: <author> Breslow, L. and D.W. </author> <title> Aha (1997b), Comparing Tree-Simplification Procedures, </title> <booktitle> Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, </address> <pages> pp. </pages> <note> 6774 (Unpublished). </note>
Reference-contexts: We surveyed the literature for algorithms that simplify decision trees and created a categorization framework for them (Breslow and Aha, 1997a). We also obtained and installed copies of at least one procedure for each of our framework's subcategories, allowing us to compare them empirically on benchmark classification tasks <ref> (Breslow and Aha, 1997b) </ref>. Our objective was to improve our understanding of the conditions for which each approach is preferred, where the dependent variables were the complexity of the indexing structure (usually a tree) and its classification accuracy. <p> The improved accuracy resulting from multivariate tests in C4.5 comes at a price of increased tree complexity. While our previous empirical study <ref> (Breslow and Aha, 1997b) </ref> focussed on a moderate number of rather simple data sets, this report is complementary because it focuses instead on a more complex data set.
Reference: <author> Brodley, C.E. and P.E. </author> <title> Utgoff (1995), Multivariate Decision Trees, </title> <booktitle> Machine Learning 19, </booktitle> <pages> 4577. </pages>
Reference-contexts: We refer to all other multivariate TDIDT algorithms as data-driven because they rely solely on the data for guidance. In this subcategory, we selected two promising algorithms. The first is LMDT <ref> (Brodley and Utgoff, 1995) </ref>, which targets numeric features by training perceptrons at each node. The second, XOFN (Zheng, 1995), targets symbolic features. Its tests evaluate conjunctions of an arbitrary set of feature-value pairs.
Reference: <author> Cherkauer, K.J. and J.W. </author> <title> Shavlik (1996), Growing Simpler Decision Trees to Facilitate Knowledge Discovery, </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Portland, OR, </address> <pages> pp. </pages> <publisher> 315318 (AAAI Press, </publisher> <address> San Mateo, CA). </address>
Reference-contexts: This has been shown to dramatically reduce tree size. We selected one example from each subcategory. SET-GEN <ref> (Cherkauer and Shavlik, 1996) </ref> performs feature selection, using a genetic algorithm to search the space of feature subsets. In contrast, Robust C4.5 (RC4.5) (John, 1995) implements an iterative case selection procedure.
Reference: <author> Daelemans, W., A. van den Bosch, and T. </author> <month> Weijters </month> <year> (1997), </year> <title> IGTree: Using Trees for Compression and Classification in Lazy Learning Algorithms, </title> <journal> Artificial Intelligence Review 11, </journal> <volume> 407423. </volume>
Reference-contexts: We are not aware of any commercial CBR tool that uses an explicit rule set to index cases. This seems a promising approach, although C4.5 RULES is known to be prohibitively slow when the induced decision trees are huge <ref> (e.g., Daelemans et al., 1997) </ref>. Transforming trees to graphs for challenging data sets also warrants investigation. In contrast, algorithms whose search costs are closely tied with the dimensionality of the prediction task are not practical for high-dimensional tasks, such as the economics data set.
Reference: <author> Esposito, F., D. Malerba, and G. </author> <title> Semeraro (1993), Decision Tree Pruning as a Search in the State Space, </title> <booktitle> Proceedings of the Sixth European Conference on Machine Learning, </booktitle> <address> Vienna, Austria, </address> <pages> pp. </pages> <publisher> 165184 (Springer-Verlag, </publisher> <address> Heidelberg, Germany). </address>
Reference: <author> Fayyad, </author> <title> U.M. and K.B. Irani (1992), The Attribute Selection Problem in Decision Tree Generation, </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <pages> pp. </pages> <publisher> 104110 (AAAI Press, </publisher> <address> San Mateo, CA). </address>
Reference-contexts: T2 had the lowest average accuracy (89.5%). It was fooled into selecting symbolic features that have a large number of values per feature. This is a well-known problem with naive index selection algorithms <ref> (Fayyad and Irani, 1992) </ref>, and it has been addressed in several algorithms (e.g., Quinlan, 1993).
Reference: <author> Gaines, B.R. </author> <year> (1995), </year> <title> Structured and Unstructured Induction with EDAGs, </title> <booktitle> Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Montreal, Canada, </address> <publisher> (AAAI Press, </publisher> <address> San Mateo, CA). </address>
Reference: <author> Gaines, B.R. </author> <year> (1996), </year> <title> Transforming Rules and Trees into Comprehensible Knowledge Structures, Advances in Knowledge Discovery and Data Mining (MIT Press, </title> <address> Cambridge, MA). </address>
Reference-contexts: We did not expect to have trees of this size in our investigation of the economics data set and thus included C4.5 RULES in our experiments. Some researchers have also developed promising algorithms for inducing decision graphs. For our experiments, we selected INDUCT <ref> (Gaines, 1996) </ref>, which induces exception directed acyclic graphs (EDAGs). EDAGs have several interesting properties. In particular, they relax two key constraints on tree induction.
Reference: <author> John, G. </author> <year> (1995), </year> <title> Robust Decision Trees: Removing Outliers in Databases, </title> <booktitle> Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Montreal, Canada, </address> <pages> pp. </pages> <publisher> 174 179 (AAAI Press, </publisher> <address> San Mateo, CA). </address>
Reference-contexts: This has been shown to dramatically reduce tree size. We selected one example from each subcategory. SET-GEN (Cherkauer and Shavlik, 1996) performs feature selection, using a genetic algorithm to search the space of feature subsets. In contrast, Robust C4.5 (RC4.5) <ref> (John, 1995) </ref> implements an iterative case selection procedure. That is, it induces a tree, post-prunes it, discards cases that are misclassified by it, and iterates unless the new tree correctly classifies all of the remaining cases.
Reference: <author> Messmer, B.T. and H. </author> <title> Bunke (1995), Subgraph Isomorphism in Polynomial Time, </title> <type> Technical Report IAM 95-003, </type> <institution> University of Bern, Institute of Computer Science and Applied Mathematics, Bern, Switzerland. </institution>
Reference: <author> Mingers, J. </author> <year> (1989), </year> <title> An Empirical Comparison of Pruning Methods for Decision Tree Induction, </title> <booktitle> Machine Learning 4, </booktitle> <pages> 227243. </pages>
Reference: <author> Pagallo, G. and D. </author> <title> Haussler (1990), Boolean Feature Discovery in Empirical Learning, </title> <booktitle> Machine Learning 5, </booktitle> <pages> 71100. </pages>
Reference-contexts: We call these algorithms hypothesis-driven because the tree represents a hypothesis of the true concept description. One such algorithm is FRINGE <ref> (Pagallo and Haussler, 1990) </ref>, variants of which have been shown to solve the subtree replication problem for several benchmark tasks. We obtained a recent version of FRINGE but did not use it because it is limited to binary attributes and binary classification tasks.
Reference: <author> Perez, E. and L.A. </author> <title> Rendell (1995), Using Multidimensional Projection to Find Relations, </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> Tahoe City, CA, </address> <pages> pp. </pages> <publisher> 447455 (Morgan Kaufmann, </publisher> <address> San Francisco, CA). </address>
Reference-contexts: Alternatives include rule sets and graphs. C4.5 RULES (Quinlan, 1993) is a promising simplification method that transforms trees to rules. It tends to greatly reduce the complexity of the concept description and often increases test accuracy <ref> (e.g., Perez and Rendell, 1995) </ref>. However, Daelemans et al. (1997) reported that C4.5 RULES is prohibitively slow when transforming large (i.e., &gt; 30,000 nodes) decision trees induced by C4.5.
Reference: <author> Quinlan, J.R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning (Morgan Kaufmann, </title> <address> San Mateo, CA). </address> <note> 16 Aha and Breslow Quinlan, </note> <author> J.R. </author> <year> (1996), </year> <title> Improved Use of Continuous Attributes in C4.5, </title> <journal> Journal of Artificial Intelligence Research 4, </journal> <volume> 7790. </volume>
Reference-contexts: Tree expansion is terminated whenever the stopping criterion is satisfied. The post-processing routine inputs the tree and attempts to improve it along some measures (e.g., reduce its size). We use C4.5 Release 7 (C4.5 R7) <ref> (Quinlan, 1993) </ref>, without post-pruning, as the baseline procedure in our experiments. It is a frequently used TDIDT algorithm whose index tests consist of testing only a single feature value. 2 It uses an information theoretic (i.e., gain ratio) test to evaluate the quality of splits. <p> Both algorithms performed well compared with C4.5 R7, but have not been compared with one another. 4.5 Alternative Data Structures Algorithms in the final category transform a decision tree to an alternative data structure. Alternatives include rule sets and graphs. C4.5 RULES <ref> (Quinlan, 1993) </ref> is a promising simplification method that transforms trees to rules. It tends to greatly reduce the complexity of the concept description and often increases test accuracy (e.g., Perez and Rendell, 1995). <p> T2 had the lowest average accuracy (89.5%). It was fooled into selecting symbolic features that have a large number of values per feature. This is a well-known problem with naive index selection algorithms (Fayyad and Irani, 1992), and it has been addressed in several algorithms <ref> (e.g., Quinlan, 1993) </ref>. <p> Furthermore, the current implementations for SET-GEN, XOFN, and LMDT are prohibitively slow for studies involving (additional) tuning of their parameter settings, at least for this economics data set. Thus, these were also not included here. 5.2.1 Accuracy and Size The remaining algorithms are all variants of C4.5 <ref> (Quinlan, 1993) </ref>. Three of them, namely C4.5 R7, C4.5 R8, and ROBUST C4.5, had highly similar behavior.
Reference: <author> Ragavan, H. and L. </author> <title> Rendell (1993), Lookahead Feature Construction for Learning Hard Concepts, </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, MA, </address> <pages> pp. </pages> <publisher> 252259 (Morgan Kaufmann, </publisher> <address> San Francisco, CA). </address>
Reference-contexts: However, our task has only one continuous feature (out of 194 features), so we cannot expect it to greatly outperform C4.5 R7 here and only investigate whether it substantially reduces speed on this economics prediction task. For the final subcategory, we selected LFC (Lookahead Feature Construction) <ref> (Ragavan et al., 1993) </ref>. Lookahead procedures evaluate the quality of a given split using information on the quality of subsequent splits. This can be expensive, so LFC constrains lookahead, using a branch-and-bound search. LFC caches the results of lookahead by generating multivariate features, as described in Section 4.2.
Reference: <author> Utgoff, P.E. </author> <year> (1996), </year> <title> Decision Tree Induction Based on Efficient Tree Restructuring, </title> <type> Technical Report 95-18, </type> <institution> University of Massachusetts, Department of Computer Science, </institution> <address> Amherst, MA. </address>
Reference: <author> Utgoff, P.E. and J.A. </author> <month> Clouse </month> <year> (1996), </year> <title> A Kolmogorov-Smirnoff Metric for Decision Tree Induction, </title> <type> Technical Report 96-3, </type> <institution> University of Massachusetts, Department of Computer Science, </institution> <address> Amherst, MA. </address>
Reference: <author> Zheng, Z. </author> <year> (1995), </year> <title> Constructing Nominal X-of-N Attributes, </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada, </address> <pages> pp. </pages> <publisher> 10641070 (Morgan Kaufmann, </publisher> <address> San Francisco, CA). </address>
Reference-contexts: We refer to all other multivariate TDIDT algorithms as data-driven because they rely solely on the data for guidance. In this subcategory, we selected two promising algorithms. The first is LMDT (Brodley and Utgoff, 1995), which targets numeric features by training perceptrons at each node. The second, XOFN <ref> (Zheng, 1995) </ref>, targets symbolic features. Its tests evaluate conjunctions of an arbitrary set of feature-value pairs.
References-found: 21

