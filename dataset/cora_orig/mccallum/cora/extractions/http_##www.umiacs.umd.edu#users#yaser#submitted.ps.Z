URL: http://www.umiacs.umd.edu/users/yaser/submitted.ps.Z
Refering-URL: http://www.umiacs.umd.edu/users/yaser/publications.html
Root-URL: 
Title: Computing 3-D Head Orientation from a Monocular Image Sequence  
Author: Thanarat Horprasert, Yaser Yacoob and Larry S. Davis 
Address: College Park, MD 20742  
Affiliation: Computer Vision Laboratory University of Maryland  
Abstract: An approach for estimating 3D head orientation in a monocular image sequence is proposed. The approach employs recently developed image-based parameterized tracking for face and face features to locate the area in which a sub-pixel parameterized shape estimation of the eye's boundary is performed. This involves tracking of five points (four at the eye corners and the fifth is the tip of the nose). We describe an approach that relies on the coarse structure of the face to compute orientation relative to the camera plane. Our approach employs projective invariance of the cross-ratios of the eye corners and anthropometric statistics to estimate the head yaw, roll and pitch. Analytical and experimental results are reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. J. Black and Y. Yacoob, </author> <title> "Tracking and Recognizing Facial Expressions in Image Sequences, using Local Parameterized Models of Image Motion", </title> <address> ICCV, </address> <year> 1995, </year> <pages> 374-381. </pages>
Reference-contexts: Their method also depends on the distance between the eyes and mouth which often changes during facial expressions. In this paper, a new approach for head orientation estimation is proposed. The approach employs recently developed image-based parameterized tracking <ref> [1] </ref> for face and face features to locate the area in which a sub-pixel parameterized shape estimation of the eye boundaries would be performed. This results in tracking of five points, four at the eye corners and the fifth at the tip of the nose. <p> This approach consists of the following stages: 1. Region tracking of the face and the face features based on parameterized motion models (see <ref> [1] </ref>). 2. Sub-pixel estimation of eye-corners and nose-tip. 3. Computing 3D orientation from these five points. of which stages (1) and (3) have been designed and implemented.
Reference: [2] <author> R. Chellappa, C.L. Wilson and S.A. Sirohey. </author> <title> Human and machine recognition of faces: A survey. </title> <journal> Proc. of IEEE, </journal> <volume> Vol 83, </volume> <year> 1995, </year> <month> pp.705-740. </month>
Reference-contexts: Instead, we obtain it by first categorizing the observed face with respect to the variables of gender, race and age (see <ref> [2] </ref>) and then use tabulated anthropometric data to estimate the mean and expected error (used to estimate accuracy of pitch recovery see Section 3) of p 0 . Let N denote the average length of the nasal bridge and E denote the average length of the eye fissure (Biocular width).
Reference: [3] <author> L.G. Farkas, </author> <title> "Anthropometry of the Head and Face" 2nd edition, </title> <publisher> Raven Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction We present an algorithm for estimating the orientation of a human face from a single monocular image. The algorithm takes advantage of the geometric symmetries of typical faces to compute the yaw and roll components of orientation, and anthropometric modeling <ref> [3, 6] </ref> to estimate the pitch component. Estimating head orientation is central in vision-based animation, gaze estimation and as a component of inferring the intentions of agents from their actions. We seek an approach that requires no prior knowledge of the exact face structure of the individual being observed. <p> Our model for head estimation assumes that the four eye-corners are co-linear in 3D; this assumption can be relaxed to account for a slight horizontal tilt in the eyes of Asian subjects (see <ref> [3] </ref> for statistics on the deviation of the eye corners from co-linearity). Let upper case letters denote coordinates and distances in 3D and lower case letters denote their respective 2D projections.
Reference: [4] <author> A. Gee and R.Cipolla, </author> <title> "Estimating Gaze from a Single View of a Face," </title> <booktitle> ICPR'94, </booktitle> <pages> 758-760, </pages> <year> 1994. </year>
Reference-contexts: For estimating head The support of the Defense Advanced Research Projects Agency (DARPA Order No. C635) under Contract N00014-95-1-0521 is gratefully acknowledged. orientation, we assume that both eyes and the nose are visible, thus avoiding near-profile poses. Several approaches have recently been proposed for estimating head orientation <ref> [5, 4] </ref>. In [5] the orientation is modeled as a linear combination of disparities between facial regions and several face models. Gee and Cipolla [4] estimate head orientation based on knowledge of the individual face geometry and assuming a weak perspective imaging model. <p> Several approaches have recently been proposed for estimating head orientation [5, 4]. In [5] the orientation is modeled as a linear combination of disparities between facial regions and several face models. Gee and Cipolla <ref> [4] </ref> estimate head orientation based on knowledge of the individual face geometry and assuming a weak perspective imaging model. Their method also depends on the distance between the eyes and mouth which often changes during facial expressions. In this paper, a new approach for head orientation estimation is proposed.
Reference: [5] <author> A. Tsukamoto, C. Lee and S. Tsuji, </author> <title> "Detection and Pose Estimation of Human Face with Synthesized Image Models," </title> <booktitle> ICPR'94, </booktitle> <pages> 754-757, </pages> <year> 1994. </year>
Reference-contexts: For estimating head The support of the Defense Advanced Research Projects Agency (DARPA Order No. C635) under Contract N00014-95-1-0521 is gratefully acknowledged. orientation, we assume that both eyes and the nose are visible, thus avoiding near-profile poses. Several approaches have recently been proposed for estimating head orientation <ref> [5, 4] </ref>. In [5] the orientation is modeled as a linear combination of disparities between facial regions and several face models. Gee and Cipolla [4] estimate head orientation based on knowledge of the individual face geometry and assuming a weak perspective imaging model. <p> C635) under Contract N00014-95-1-0521 is gratefully acknowledged. orientation, we assume that both eyes and the nose are visible, thus avoiding near-profile poses. Several approaches have recently been proposed for estimating head orientation [5, 4]. In <ref> [5] </ref> the orientation is modeled as a linear combination of disparities between facial regions and several face models. Gee and Cipolla [4] estimate head orientation based on knowledge of the individual face geometry and assuming a weak perspective imaging model.
Reference: [6] <author> J. Young, </author> <title> Head and Face Anthropometry of Adult U.S. Citizens, </title> <type> Government Report DOT/FAA/AM-93/10, </type> <month> July </month> <year> 1993. </year> <title> different face structures. (solid line) is the plot of ff computed by employing statistic anthropometry, while (dash line) is the plot of ff computed by utilizing the real measurements of model's face structure. </title>
Reference-contexts: 1 Introduction We present an algorithm for estimating the orientation of a human face from a single monocular image. The algorithm takes advantage of the geometric symmetries of typical faces to compute the yaw and roll components of orientation, and anthropometric modeling <ref> [3, 6] </ref> to estimate the pitch component. Estimating head orientation is central in vision-based animation, gaze estimation and as a component of inferring the intentions of agents from their actions. We seek an approach that requires no prior knowledge of the exact face structure of the individual being observed.
References-found: 6

