URL: ftp://ftp.cs.wisc.edu/wwt/isca95_dsi.ps
Refering-URL: http://www.cs.wisc.edu/~david/david.html
Root-URL: 
Email: -alvy,david-@cs.wisc.edu  
Title: Dynamic Self-Invalidation: Reducing Coherence Overhead in Shared-Memory Multiprocessors  
Author: Alvin R. Lebeck and David A. Wood 
Address: Madison, Wisconsin 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: This work is supported in part by NSF PYI Award CCR-9157366, NSF Grants CDA-9024618 and MIP-9225097, donations from Thinking Machines Corp., Digital Equipment Corp., Xerox Corp., and by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant F33615-94-1-1525 and ARPA order no. B550 The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Wright Laboratory Avionics Directorate or the U.S. Government. Abstract This paper introduces dynamic self-invalidation (DSI), a new technique for reducing cache coherence overhead in shared-memory multiprocessors. DSI eliminates invalidation messages by having a processor automatically invalidate its local copy of a cache block before a conicting access by another processor. Eliminating invalidation overhead is particularly important under sequential consistency, where the latency of invalidating outstanding copies can increase a programs critical path. DSI is applicable to software, hardware, and hybrid coherence schemes. In this paper we evaluate DSI in the context of hardware directory-based write-invalidate coherence protocols. Our results show that DSI reduces execution time of a sequentially consistent full-map coherence protocol by as much as 41%. This is comparable to an implementation of weak consistency that uses a coalescing write-buffer to allow up to 16 outstanding requests for exclusive blocks. When used in conjunction with weak consistency, DSI can exploit tear-off blockswhich eliminate both invalidation and acknowledgment messages for a total reduction in messages of up to 26%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Implementing Sequential Consistency In Cache-Based Systems. </title> <booktitle> In ICPP90, </booktitle> <pages> pages I47I50, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The acknowledgments can be sent directly to the requesting processor [29], or collected by the directory which forwards a single acknowledgment. The processor stalls at synchronization operations, depending on the specific consistency model, until all preceding writes are acknowledged. Adve and Hill proposed a similar scheme for sequential consistency <ref> [1] </ref>; however, they do not provide any quantitative results. As discussed in Section 3.3, self-invalidation can eliminate acknowledgment messages when combined with weak consistency. 3 Dynamic Self-Invalidation In this section we present a general framework for performing dynamic self-invalidation (DSI).
Reference: [2] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering - A New Definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 214, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Weak consistency models [2,17,19] also reduce the impact of coherence overhead. A system that provides weak consistency appears sequentially consistent provided that the program satisfies a particular synchronization model <ref> [2] </ref>. Weak consistency models allow the use of memory access buffering techniquese.g., write buffers. They also allow the directory to respond with the data in parallel with the invalidation of outstanding copies, and the processor can proceed as soon as it receives the data.
Reference: [3] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280 289, </pages> <year> 1988. </year>
Reference-contexts: In this paper we evaluate DSI in the context of a full-map directory-based hardware cache coherence protocol <ref> [3] </ref>. We assume a typical write-invalidate protocol with three states (see outstanding readable and writable copy (Exclusive). A processor must obtain an exclusive copy of a block before modifying it; the directory enforces this by sending explicit invalidation messages to eliminate any outstanding copies. <p> This section presents two techniques for the directory controller to identify which blocks should be self-invalidated: additional states and version numbers. Both implementations are extensions to a standard three state full-map directory-based write-invalidate protocol, such as Dir n NB <ref> [3] </ref>. Both implementations use the sharing history to speculate about the future: blocks that have recently had con-icting accessesand hence would have needed invalidationsare candidates for self-invalidation. Thus, shared-readable blocks are marked for self-invalidation if they have been modified since the last reference by the processor. <p> The tear-off blocks can be self-invalidated in a single cycle using a simple ash clear circuit; the exclusive blocks must be sequentially self-invalidated using one of the techniques described above. 5 Performance Evaluation In this section we evaluate the effectiveness of DSI by comparing it to a full-map protocol <ref> [3] </ref>. Section 5.2 evaluates the detection and self-invalidation mechanisms under sequential consistency.
Reference: [4] <author> Thomas E. Anderson. </author> <title> The Performance Implications of Spin-Waiting Alternatives for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II170II174, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: A further caveat is that using tear-off blocks with sequential consistency does not guarantee forward progress. If a processor obtains a tear-off block containing a spin lock <ref> [4] </ref>, it may never experience a subsequent cache miss. The spin lock will never be invalidated and the processor will not proceed. To overcome this, the tear-off block could be self-invalidated periodically, e.g., at context switches. Tear-off blocks are potentially much more significant under weaker consistency models.
Reference: [5] <author> Thomas E. Anderson, David E. Culler, David A. Patterson, </author> <title> and the NOW team. A Case for NOW (Networks of Workstations). </title> <journal> IEEE Micro. </journal> <note> To appear. </note>
Reference-contexts: Our results indicate that DSI will have the greatest impact in systems with large, multi-megabyte caches, e.g., a portion of main memory [21,33], since data is seldom replaced, or with relatively slow networks, such as networks of workstations <ref> [5] </ref>. This paper is organized as follows. Section 2 reviews invalidation-based coherence protocols and discusses related work. Section 3 presents dynamic self-invalidation and discusses the design space.
Reference: [6] <author> Brian Case. </author> <title> SPARC V9 Adds Wealth of New Features. </title> <type> Microprocessor Report, 7(9), </type> <month> February </month> <year> 1993. </year>
Reference-contexts: Migratory data optimizations [12,38] speculate about future write requests by the same processor when responding to a read request. Self-invalidation is complementary to these optimizations and could be combined with them. For example, the SPARC V9 prefetch-read-once instruction <ref> [6] </ref> indicates that a block should be prefetched, but then self-invalidated after the first reference. Weak consistency models [2,17,19] also reduce the impact of coherence overhead. A system that provides weak consistency appears sequentially consistent provided that the program satisfies a particular synchronization model [2].
Reference: [7] <author> L. M. Censier and P. Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):11121118, </volume> <month> December </month> <year> 1978. </year>
Reference: [8] <author> David Chaiken, John Kubiatowics, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224 234, </pages> <month> April </month> <year> 1991. </year>
Reference: [9] <author> Hoichi Cheong and Alexander V. Veidenbaum. </author> <title> Compiler-Directed Cache Management in Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6):3948, </volume> <month> June </month> <year> 1990. </year>
Reference: [10] <author> Trishul M. Chilimbi and James R. Larus. Cachier: </author> <title> A Tool for Automatically Inserting CICO Annotations. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II8998, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Other self-invalidation techniques combine memory system directives with a conventional directory-based write-invalidate protocol. In CICO, the programmer [22,40] or a profile-based tool <ref> [10] </ref> annotates the program with check_in directives to inform the memory system when it should invalidate cache blocks. In contrast to compiler-directed coherence, check_in directives are only performance hints to the memory system; the directory hardware is still responsible for correctness.
Reference: [11] <author> Lynn Choi and Pen-Chung Yew. </author> <title> A Compiler-Directed Cache Coherence Scheme with Improved Intertask Locality. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <pages> pages 773782, </pages> <month> Nov </month> <year> 1994. </year>
Reference: [12] <author> Alan L. Cox and Robert J. Fowler. </author> <title> Adaptive Cache Coherency for Detecting Migratory Shared Data. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 98108, </pages> <month> May </month> <year> 1993. </year>
Reference: [13] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 262273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Application Programs This table describes the benchmarks used in this paper. Sparse is locally-written [40], EM3D is from the Berkeley Split-C group <ref> [13] </ref>, Barnes and Ocean are from the Stanford SPLASH suite [36], and Tomcatv is a locally written, parallel version of the SPEC benchmark. writes are acknowledged. The processor also stalls on read misses until the block is obtained.
Reference: [14] <author> Ron Cytron, Steve Karlovsky, and Kevin P. McAuliffe. </author> <title> Automatic Management of Programmable Caches. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages 229238, </pages> <month> Aug </month> <year> 1988. </year>
Reference: [15] <author> Fredrik Dahlgren, Michel Dubois, and Per Stenstrom. </author> <title> Combined Performance Gains of Simple Cache Protocol Extensions. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 187 197, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Similarly, a cache controller can identify blocks for self-invalidation by maintaining information for recently invalidated blocks <ref> [15] </ref> (e.g., the number of times a block is invalidated). When servicing a cache miss, this history information is used by the controller to decide if it should self-invalidate this block at a later time. 3.2 Performing Self-Invalidation Software, hardware, or a combination can be used to perform self-invalidation.
Reference: [16] <author> Ervan Darnell and Ken Kennedy. </author> <title> Cache Coherence Using Local Knowledge. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 720729, </pages> <month> Nov </month> <year> 1993. </year>
Reference: [17] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory Access Buffering in Multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434442, </pages> <month> June </month> <year> 1986. </year>
Reference: [18] <author> Vincent W. Freeh, David K. Lowenthal, and Gregory R. Andrews. </author> <title> Distributed Filaments: Efficient Fine-Grain Parallelism on a Cluster of Workstations. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 201213, </pages> <month> November </month> <year> 1994. </year>
Reference: [19] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Philip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1526, </pages> <month> June </month> <year> 1990. </year>
Reference: [20] <author> Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254263, </pages> <month> May </month> <year> 1991. </year>
Reference: [21] <author> E. Hagersten, A. Landin, and S. </author> <title> Haridi. </title> <booktitle> DDMA Cache-Only Memory Architecture. IEEE Computer, </booktitle> <pages> pages 4454, </pages> <month> September </month> <year> 1992. </year>
Reference: [22] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4):300318, </volume> <month> November </month> <year> 1993. </year> <note> Earlier version appeared in ASPLOS V, </note> <month> Oct. </month> <year> 1992. </year>
Reference: [23] <author> Norman P. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In addition, if we can identify synchronization operations, such as test&set or swap, then we can also ush the FIFO at those points. Implementing the FIFO requires the addition of a small memory to store the identity of the blocks to self-invalidate. This buffersimilar to a victim cache <ref> [23] </ref> or the HP PA7200 assist cache [25]is unlikely to exceed 64 entries. Nonetheless, this is an attractive approach since it does not rely on any information from the processor. If the cache controller can identify synchronization operations, then there are other schemes for performing self-invalidation.
Reference: [24] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwanenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operations Systems. </title> <type> Technical Report COMP TR93-214, </type> <institution> Department of Computer Science, Rice University, </institution> <month> November </month> <year> 1993. </year>
Reference: [25] <author> Gordon Kurpanek, Ken Chan, Jason Zheng, Eric Delano, and William Bryg. PA7200: </author> <title> A PA-RISC Processor with Integrated High Performance MP Bus Interface. </title> <booktitle> In Comp-con, </booktitle> <pages> pages 375382, </pages> <year> 1994. </year>
Reference: [26] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: DSI with tear-off blocks eliminates both invalidation and acknowledgment messages. Tear-off blocks potentially reduce both the total message traffic and the directory controller occupancy. The latter may have a significant effect on systems that cannot process local memory accesses in parallel with protocol events (e.g., FLASH <ref> [26] </ref>). The results in Table 3 show that DSI reduces the total number of messages by up to 17% for a 256K-byte cache and 26% for a 2M-byte cache.
Reference: [27] <author> Leslie Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: A processor must obtain an exclusive copy of a block before modifying it; the directory enforces this by sending explicit invalidation messages to eliminate any outstanding copies. The overhead of these invalidation messages is particularly significant under sequential consistency <ref> [27] </ref>, the programming model most programmers implicitly assume. A multiprocessor is sequentially consistent if the execution corresponds to some interleaving of the processes on a uniprocessor.
Reference: [28] <author> A. R. Lebeck. </author> <title> Tools and Techniques for Memory System Design and Analysis. </title> <type> PhD thesis, </type> <institution> University of Wisconsin at Madison, </institution> <note> expected August 1995. </note> <institution> Computer Sciences Department. </institution>
Reference-contexts: However, the overall latency will be proportional to the number of cache frames, even though many blocks may not be self-invalidated. We can reduce this latency using a circuit that sequences through only the blocks that must be self-invalidated. One implementation uses a modified ash clear circuit <ref> [28] </ref> to determine the next cache set that contains a block to self-invalidate, and requires an encoder to recreate the cache index.
Reference: [29] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):6379, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Section 3 presents dynamic self-invalidation and discusses the design space. Section 4 describes our implementations of dynamic self-invalidation protocols, Section 5 evaluates their performance, and Section 6 concludes our paper. 2 Background and Related Work DSI techniques are applicable to hardware <ref> [29] </ref>, software [35], and hybrid systems [8,26,33]. In this paper we evaluate DSI in the context of a full-map directory-based hardware cache coherence protocol [3]. We assume a typical write-invalidate protocol with three states (see outstanding readable and writable copy (Exclusive). <p> Compiler-directed coherence [9,14,16,30] eliminates the directory, placing the entire burden of maintaining cache coherence on the compiler. 1. It is possible to have P2 respond immediately with the data, and P3 send an acknowledgment directly to P1 <ref> [29] </ref>. However, P1 stalls until the acknowledgment is received. <p> They also allow the directory to respond with the data in parallel with the invalidation of outstanding copies, and the processor can proceed as soon as it receives the data. The acknowledgments can be sent directly to the requesting processor <ref> [29] </ref>, or collected by the directory which forwards a single acknowledgment. The processor stalls at synchronization operations, depending on the specific consistency model, until all preceding writes are acknowledged. Adve and Hill proposed a similar scheme for sequential consistency [1]; however, they do not provide any quantitative results.
Reference: [30] <author> Sang Lyul Min and Jean-Loup Baer. </author> <title> Design and Analysis of a Scalable Cache Coherence Scheme Based on Clocks and Timestamps. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(1):2544, </volume> <month> January </month> <year> 1992. </year>
Reference: [31] <author> Todd Mowry and Anoop Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2):87106, </volume> <month> June </month> <year> 1991. </year>
Reference: [32] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4860, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In Section 5.3, we evaluate the benefit of adding dynamic self-invalidation to a weak con sistency implementation that allows up to 16 outstanding requests for exclusive blocks. 5.1 Methodology We use a modified version of the Wisconsin Wind Tunnel <ref> [32] </ref> to simulate 32-processor systems with 256K-byte and 2M-byte 4-way set-associative caches with 32-byte blocks. Cache misses occupy the cache controller for 3 cycles and the directory controller for 10 cycles, plus message injection time.
Reference: [33] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Typhoon and Tempest: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325336, </pages> <month> April </month> <year> 1994. </year>
Reference: [34] <author> Christoph Ernst Scheurich. </author> <title> Access Ordering and Coherence in Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Southern California, </institution> <month> May </month> <year> 1989. </year> <note> Also available as technical report No. CENG 89-19. </note>
Reference-contexts: For these blockscalled tear-off blocksthe directory does not track the outstanding copy. Scheurich observed that the invalidation of a cache block could be delayed until the subsequent cache miss and still maintain sequential consistency <ref> [34] </ref>. The intuition behind this observation is that a processor can continue to access data until it sees new data generated by another processor. To maintain sequential consistency the cache controller must invalidate tear-off blocks at subsequent cache misses. Therefore, a cache may contain at most one tear-off block.
Reference: [35] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steve K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <note> Submitted for publication, </note> <month> March </month> <year> 1994. </year>
Reference-contexts: Section 3 presents dynamic self-invalidation and discusses the design space. Section 4 describes our implementations of dynamic self-invalidation protocols, Section 5 evaluates their performance, and Section 6 concludes our paper. 2 Background and Related Work DSI techniques are applicable to hardware [29], software <ref> [35] </ref>, and hybrid systems [8,26,33]. In this paper we evaluate DSI in the context of a full-map directory-based hardware cache coherence protocol [3]. We assume a typical write-invalidate protocol with three states (see outstanding readable and writable copy (Exclusive).
Reference: [36] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. Computer Architecture News, </title> <address> 20(1):544, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Application Programs This table describes the benchmarks used in this paper. Sparse is locally-written [40], EM3D is from the Berkeley Split-C group [13], Barnes and Ocean are from the Stanford SPLASH suite <ref> [36] </ref>, and Tomcatv is a locally written, parallel version of the SPEC benchmark. writes are acknowledged. The processor also stalls on read misses until the block is obtained. We present results from five benchmarks in our evaluation of DSI, see Table 1.
Reference: [37] <author> B. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. </title> <booktitle> In Proceedings of the Int. </booktitle> <publisher> Soc. for Opt. Engr, </publisher> <pages> pages 241248, </pages> <year> 1982. </year>
Reference: [38] <author> Per Stenstrom, Mats Brorsson, and Lars Sandberg. </author> <title> Adaptive Cache Coherence Protocol Optimized for Migratory Sharing. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 109118, </pages> <month> May </month> <year> 1993. </year>
Reference: [39] <author> C. K. Tang. </author> <title> Cache System Design in the Tightly Coupled Multiprocessor System. </title> <booktitle> In Proc. AFIPS, </booktitle> <pages> pages 749753, </pages> <year> 1976. </year>
Reference: [40] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156168, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Application Programs This table describes the benchmarks used in this paper. Sparse is locally-written <ref> [40] </ref>, EM3D is from the Berkeley Split-C group [13], Barnes and Ocean are from the Stanford SPLASH suite [36], and Tomcatv is a locally written, parallel version of the SPEC benchmark. writes are acknowledged. The processor also stalls on read misses until the block is obtained.
References-found: 40

