URL: ftp://cse.ogi.edu/pub/neural/papers/levinLeenMoody93.PCP.ps.Z
Refering-URL: http://www.cse.ogi.edu/~tleen/
Root-URL: http://www.cse.ogi.edu
Title: Fast Pruning Using Principal Components  
Author: J. Cowan, G. Tesauro, and J. Asriel U. Levin, Todd K. Leen and John E. Moody 
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute  
Date: 1994  
Address: San Mateo, CA,  P.O. Box 91000, Portland, OR 97291-1000  
Note: Appears in Advances in Neural Information Processing 6,  Alspector, eds., Morgan Kaufmann,  
Abstract: We present a new algorithm for eliminating excess parameters and improving network generalization after supervised training. The method, "Principal Components Pruning (PCP)", is based on principal component analysis of the node activations of successive layers of the network. It is simple, cheap to implement, and effective. It requires no network retraining, and does not involve calculating the full Hessian of the cost function. Only the weight and the node activity correlation matrices for each layer of nodes are required. We demonstrate the efficacy of the method on a regression problem using polynomial basis functions, and on an economic time series prediction problem using a two-layer, feedforward network.
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> (1970). </year> <title> Statistical predictor identification. </title> <institution> Ann. Inst. Stat. Math., 22:203. </institution>
Reference-contexts: The quantity ~w T i ~w i i measures the effect of the i th eigen-coordinate on the output error; it serves as our saliency measure for the weight ~w i . Relying on Akaike's Final Prediction error (FPE) <ref> (Akaike, 1970) </ref>, the average test set error for the original model is given by J [W ] = N pm where pm is the number of parameters in the model.
Reference: <author> Hassibi, B., Stork, D., and Wolff, G. </author> <year> (1992). </year> <title> Optimal brain surgeon and general network pruning. </title> <type> Technical Report 9235, </type> <institution> RICOH California Research Center, </institution> <address> Menlo Park, CA. </address>
Reference-contexts: The diagonal assumption is inaccurate and can lead to the removal of the wrong weights. The method also requires retraining the pruned network, which is computationally expensive. Optimal Brain Surgeon (OBS) <ref> (Hassibi et al., 1992) </ref> removes the "diagonal" assumption but is impractical for large nets. Early stopping monitors the error on a validation set and halts learning when this error starts to increase.
Reference: <author> Jolliffe, I. T. </author> <year> (1986). </year> <title> Principal Component Analysis. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We propose a new method for eliminating excess parameters and improving network generalization. The method, "Principal Components Pruning (PCP)", is based on principal component analysis (PCA) and is simple, cheap and effective. 2 Background and Motivation PCA <ref> (Jolliffe, 1986) </ref> is a basic tool to reduce dimension by eliminating redundant variables. In this procedure one transforms variables to a basis in which the covari-ance is diagonal and then projects out the low variance directions.
Reference: <author> Le Cun, Y., Denker, J., and Solla, S. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605, </pages> <address> Denver 1989. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: This is costly and does not take into account correlations between the neuron activities. Eliminating small weights does not properly account for a weight's effect on the output error. Optimal Brain Damage (OBD) <ref> (Le Cun et al., 1990) </ref> removes those weights that least affect the training error based on a diagonal approximation of the Hessian. The diagonal assumption is inaccurate and can lead to the removal of the wrong weights. The method also requires retraining the pruned network, which is computationally expensive. <p> In this procedure one transforms variables to a basis in which the covari-ance is diagonal and then projects out the low variance directions. While application of PCA to remove input variables is useful in some cases <ref> (Leen et al., 1990) </ref>, there is no guarantee that low variance variables have little effect on error. We propose a saliency measure, based on PCA, that identifies those variables that have the least effect on error.
Reference: <author> Leen, T. K., Rudnick, M., and Hammerstrom, D. </author> <year> (1990). </year> <title> Hebbian feature discovery improves classifier efficiency. </title> <booktitle> In Proceedings of the IEEE/INNS International Joint Conference on Neural Networks, pages I-51 to I-56. </booktitle>
Reference-contexts: In this procedure one transforms variables to a basis in which the covari-ance is diagonal and then projects out the low variance directions. While application of PCA to remove input variables is useful in some cases <ref> (Leen et al., 1990) </ref>, there is no guarantee that low variance variables have little effect on error. We propose a saliency measure, based on PCA, that identifies those variables that have the least effect on error.
Reference: <author> Moody, J. </author> <year> (1992). </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In Moody, J., Hanson, S., and Lippman, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 847-854. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A complete analysis of the effects on generalization performance of removing eigen-nodes in a nonlinear network is beyond the scope of this short paper. However, it can be shown that removing eigen-nodes with low saliency reduces the effective number of parameters <ref> (Moody, 1992) </ref> and should usually improve generalization. Also, as will be discussed in the next section, our PCP algorithm is related to the OBD and OBS pruning methods. <p> those weights for which the contribution is small can be deleted. 4 Relation to Hessian-Based Methods The effect of our PCP method is to reduce the rank of each layer of weights in a network by the removal of the least salient eigen-nodes, which reduces the effective number of parameters <ref> (Moody, 1992) </ref>. This is in contrast to the OBD and OBS methods which reduce the rank by eliminating actual weights. PCP differs further from OBD and OBS in that it does not require that the network be trained to a local minimum of the error. <p> We are currently exploring nonlinear extensions of our linearized approach. These involve computing a block-diagonal Hessian in which the block corresponding to each unit differs from the correlation matrix for that layer by a nonlinear factor.The analysis makes use of GPE <ref> (Moody, 1992) </ref> rather than FPE. Acknowledgements One of us (TKL) thanks Andreas Weigend for stimulating discussions that provided some of the motivation for this work. AUL and JEM gratefully acknowledge the support of the Advanced Research Projects Agency and the Office of Naval Research under grant ONR N00014-92-J-4062.
Reference: <author> Moody, J., Levin, A., and Rehfuss, S. </author> <year> (1993). </year> <title> Predicting the u.s. index of industrial production. </title> <booktitle> Neural Network World, </booktitle> <volume> 3 </volume> <pages> 791-794. </pages> <booktitle> in Proceedings of Parallel Applications in Statistics and Economics '93. </booktitle>
Reference-contexts: It is likely to be useful when there are correlations of signal activities. The method is substantially cheaper to implement than OBS and is likely to yield better network performance than OBD. 8 7 Preliminary results on this problem have been described briefly in <ref> (Moody et al., 1993) </ref>, and a detailed account of this work will be presented elsewhere. 8 See section 4 for a discussion of the block-diagonal Hessian interpretation of our method.
Reference: <author> Mozer, M. and Smolensky, P. </author> <year> (1989). </year> <title> Skeletonization: A technique for trimming the fat from a network via relevance assesment. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 1, </volume> <pages> pages 107-115. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A promising approach is to start with a large, fully-connected network and through pruning or regularization, increase model bias in order to reduce model variance and improve generalization. Review of existing algorithms In recent years, several methods have been proposed. Skeletonization <ref> (Mozer and Smolensky, 1989) </ref> removes the neurons that have the least effect on the output error. This is costly and does not take into account correlations between the neuron activities. Eliminating small weights does not properly account for a weight's effect on the output error.
Reference: <author> Weigend, A. S. and Rumelhart, D. E. </author> <year> (1991). </year> <title> Generalization through minimal networks with application to forecasting. </title> <editor> In Keramidas, E. M., editor, </editor> <booktitle> INTERFACE'91 23rd Symposium on the Interface: Computing Science and Statistics, </booktitle> <pages> pages 362-370. </pages> <editor> 9 (Weigend and Rumelhart, </editor> <title> 1991) called the rank of the covariance matrix of the node activities the "effective dimension of hidden units" and discussed it in the context of early stopping. </title>
References-found: 9

