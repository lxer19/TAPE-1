URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/chrislee/www/iros98.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/chrislee/www/publications.html
Root-URL: 
Title: Reduced-dimension Representations of Human Performance Data for Human-to-Robot Skill Transfer  
Author: Christopher Lee Yangsheng Xu yz 
Address: Pittsburgh, PA, USA  Hong Kong, Hong Kong  
Affiliation: Robotics Institute, Carnegie Mellon University,  Department of Mechanical and Automation Engineering The Chinese University of  
Abstract: Despite the large amount of research currently directed toward programming robots by demonstration, a significant problem with this method of human-to-robot skill transfer has not yet been addressed: developing representations of human performances which isolate the intrinsic dimensions of the performances (and thus the skills which guide them) within high-dimensional, raw human performance data. In this paper we propose the use of three methods for representing high-dimensional human performance data within lower-dimensional spaces: principal-component analysis (PCA), nonlinear principal component analysis (NLPCA), and sequential nonlinear principal component analysis (SNLPCA). We compare the appropriateness of these methods for modeling a simple human grasping operation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. T. Jolliffe, </author> <title> Principal component analysis. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: This data set was then analyzed using the PCA, NLPCA, and SNLPCA techniques presented in the following sections. 3 PCA Principal component analysis (PCA) or the Karhunen-Loeve transform <ref> [1] </ref> is a well-understood and commonly used method which, when given a set of multidimensional vectors, finds a linear mapping between these vectors and each lower-dimensional space such that when the vectors are mapped to a lower-dimensional space and then mapped back to the original space, the sum-of-squared errors of the
Reference: [2] <author> M. A. Kramer, </author> <title> Nonlinear principal component analysis using au-toassociative neural networks, </title> <journal> AIChe Jounal, </journal> <volume> vol. 37, </volume> <pages> pp. 23343, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Several methods proposed for performing NLPCA include the use of autoasso-ciative neural networks, as described by Kramer <ref> [2] </ref>; principal curves analysis, as described by Hastie and Stuet-zle [3], and Dong and McAvoy [4]; adaptive principal surfaces, as described by LeBlanc and Tibshirani [5]; and optimizing neural network inputs, as presented by Tan and Mavrovouniotis [6]. <p> The selection criteria for the value of M was not cross-validation error (although it is used for early-stopping of the weight-optimization algorithm), but rather the information theoretic criterion described in <ref> [2] </ref>: AIC = ln (e) + 2N w =N d ; (10) where N w is the number of weights in the network, N d is the number of training vectors times the dimension of the training vectors, and e = E=(2N ) is the average sum of squares error. <p> This was possible because the grasping motion is a smooth directed path in configuration space, and because the forms of the mapping functions (7) and (8) are well suited to learning such smooth mappings. 5 SNLPCA Kramer's sequential NLPCA algorithm (SNLPCA) <ref> [2] </ref>, is a modification to the NLPCA method which produces a nonlinear factorization, and where the training process prioritizes each resulting feature as to its relative power in explaining the variations of the training set.
Reference: [3] <author> T. Hastie and W. Steutzle, </author> <title> Principal curves, </title> <journal> Journal of the American Statistical Association, </journal> <volume> vol. 84, </volume> <pages> pp. 50216, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Several methods proposed for performing NLPCA include the use of autoasso-ciative neural networks, as described by Kramer [2]; principal curves analysis, as described by Hastie and Stuet-zle <ref> [3] </ref>, and Dong and McAvoy [4]; adaptive principal surfaces, as described by LeBlanc and Tibshirani [5]; and optimizing neural network inputs, as presented by Tan and Mavrovouniotis [6]. <p> However, Malthouse indicates that Kramer's NLPCA method also tends to result in suboptimal projections, and indicates that methods based on the principal curves methods <ref> [3, 4, 5] </ref> tend to result in better parameterizations. We plan in our future work to compare these methods to the ones presented in this paper in terms of their efficiency, accuracy, and feasibility for modeling human performance data.
Reference: [4] <author> D. Dong and T. McAvoy, </author> <title> Nonlinear principal component analysis-based on principal curves and neural networks, </title> <journal> Computers & Chemical Engineering, </journal> <volume> vol. 20, </volume> <pages> pp. 6578, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Several methods proposed for performing NLPCA include the use of autoasso-ciative neural networks, as described by Kramer [2]; principal curves analysis, as described by Hastie and Stuet-zle [3], and Dong and McAvoy <ref> [4] </ref>; adaptive principal surfaces, as described by LeBlanc and Tibshirani [5]; and optimizing neural network inputs, as presented by Tan and Mavrovouniotis [6]. In this section we will focus on Kramer's method for NLPCA, and in Section 5 we look at a modification of that method called SNLPCA. <p> However, Malthouse indicates that Kramer's NLPCA method also tends to result in suboptimal projections, and indicates that methods based on the principal curves methods <ref> [3, 4, 5] </ref> tend to result in better parameterizations. We plan in our future work to compare these methods to the ones presented in this paper in terms of their efficiency, accuracy, and feasibility for modeling human performance data.
Reference: [5] <author> M. LeBlanc and R. Tibshirani, </author> <title> Adaptive principal surfaces, </title> <journal> Journal of the American Statistical Society, </journal> <volume> vol. 89, </volume> <pages> pp. 5364, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Several methods proposed for performing NLPCA include the use of autoasso-ciative neural networks, as described by Kramer [2]; principal curves analysis, as described by Hastie and Stuet-zle [3], and Dong and McAvoy [4]; adaptive principal surfaces, as described by LeBlanc and Tibshirani <ref> [5] </ref>; and optimizing neural network inputs, as presented by Tan and Mavrovouniotis [6]. In this section we will focus on Kramer's method for NLPCA, and in Section 5 we look at a modification of that method called SNLPCA. <p> However, Malthouse indicates that Kramer's NLPCA method also tends to result in suboptimal projections, and indicates that methods based on the principal curves methods <ref> [3, 4, 5] </ref> tend to result in better parameterizations. We plan in our future work to compare these methods to the ones presented in this paper in terms of their efficiency, accuracy, and feasibility for modeling human performance data.
Reference: [6] <author> S. Tan and M. L. Mavrovouniotis, </author> <title> Reducing data dimensionality through optimizing neural network inputs, </title> <journal> AIChe Jounal, </journal> <volume> vol. 41, </volume> <pages> pp. 14711480, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: performing NLPCA include the use of autoasso-ciative neural networks, as described by Kramer [2]; principal curves analysis, as described by Hastie and Stuet-zle [3], and Dong and McAvoy [4]; adaptive principal surfaces, as described by LeBlanc and Tibshirani [5]; and optimizing neural network inputs, as presented by Tan and Mavrovouniotis <ref> [6] </ref>. In this section we will focus on Kramer's method for NLPCA, and in Section 5 we look at a modification of that method called SNLPCA. Kramer's method for NLPCA involves training a neural network with three hidden layers, such as the one shown in Figure 4.
Reference: [7] <author> G. Cybenko, </author> <title> Approximation by superpositions of a sigmoidal function, </title> <journal> Math. Contr. Signals Syst., </journal> <volume> vol. 2, </volume> <pages> pp. 303314, </pages> <year> 1989. </year>
Reference-contexts: Given enough mapping units, these functional forms may approximate any bounded, continuous multidimensional nonlinear function v = f (u) with arbitrary precision <ref> [7] </ref>.
Reference: [8] <author> R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu, </author> <title> A limited memory algorithm for bound constrained optimization, </title> <journal> SIAM Journal of Scientific Computing, </journal> <volume> vol. 16, no. 5, </volume> <pages> pp. 11901208, </pages> <year> 1995. </year>
Reference-contexts: However, if an explicitly prioritized factorization is desired, Kramer's sequential NLPCA algorithm (SNLPCA), discussed in Section 5, may be used. We used Kramer's NLPCA method to analyze the data set from Section 2. The neural networks were trained using the L-BFGS-B implementation of Byrd et. al <ref> [8] </ref>. We used a network architecture with linear units for the bottleneck and output layers, and without direct interconnections between the input and bottleneck layers, nor between the bottleneck and output layers.
Reference: [9] <author> E. C. Malthouse, </author> <title> Limitations of nonlinear PCA as performed with generic neural neworks, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 9, </volume> <pages> pp. 16573, </pages> <month> January </month> <year> 1998. </year>
Reference-contexts: For faithful reconstruction of a given hand configuration from a reduced-dimensional representation, PCA analysis is the simplest and most effective method. Higher-dimensional NLPCA and SNLPCA models might potentially be more appropriate for more complex skills, particularly those involving more sensory feedback. Malthouse <ref> [9] </ref> discusses Kramer's NLPCA method and indicates that it has several important limitations, including an inability to model curves and surfaces that intersect themselves, and an inability to parameterize curves with parameterizations involving discontinuous jumps.
References-found: 9

