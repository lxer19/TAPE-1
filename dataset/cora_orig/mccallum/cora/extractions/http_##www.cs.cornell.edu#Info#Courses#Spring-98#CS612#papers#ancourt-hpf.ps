URL: http://www.cs.cornell.edu/Info/Courses/Spring-98/CS612/papers/ancourt-hpf.ps
Refering-URL: http://www.cs.cornell.edu/Info/Courses/Spring-98/CS612/
Root-URL: 
Title: A Linear Algebra Framework for Static HPF Code Distribution  
Author: Corinne Ancourt Fabien Coelho Fran~cois Irigoin Ronan Keryell 
Date: December 31, 1993  
Address: 35, rue Saint Honore, 77305 Fontainebleau cedex, France  
Affiliation: Centre de Recherche en Informatique, Ecole Nationale Superieure des Mines de Paris  
Abstract: High Performance Fortran (hpf) was developed to support data parallel programming for simd and mimd machines with distributed memory. The programmer is provided a familiar uniform logical address space and specifies the data distribution by directives. The compiler then exploits these directives to allocate arrays in the local memories, to assign computations to elementary processors and to migrate data between processors when required. We show here that linear algebra is a powerful framework to encode hpf directives and to synthesize distributed code with space-efficient array allocation, tight loop bounds and vectorized communications for INDEPENDENT loops. The generated code includes traditional optimizations such as guard elimination, message vectorization, and aggregation, overlap analysis,... The systematic use of an affine framework makes it possible to prove the compilation scheme correct. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Many partial optimization techniques are integrated in our direct synthesis approach: message vectorization, and aggregation [19], overlap analysis [15]. A new storage management scheme is also proposed. Moreover other optimizations techniques may be applied to the generated code such as vec-torization [38], loop invariant code motion <ref> [1] </ref> and software pipelining [14, 37].
Reference: [2] <author> Corinne Ancourt. </author> <title> Generation automatique de codes de transfert pour multiprocesseurs a me-moires locales. </title> <type> PhD thesis, </type> <institution> Universite Paris VI, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: If regions are not precise enough because convex hulls are used to summarize multiple references, it is possible to use additional parameters to exactly express a set of references <ref> [2] </ref>. <p> c) 2 compute (p)) X 0 (S X 0 (`; c)) = f (Y 0 (S Y 0 (`; c)), be parametrically enumerated in basis (p; u), where u is a basis for Y 0 , the local part of Y. send and receive are polyhedral sets and algorithms in <ref> [2] </ref> can be used. If the last component of u is allocated contiguously, vector messages can be generated. 3.5 Output SPMD code The generic output spmd code, parametric on the local processor p, is shown in Figure 6. <p> Experiments are needed to find out the best approach. Note that an extra-loop is generated. Y diagonal can be enumerated with only two loops and three are generated. This is due to the use of an imprecise projection algorithm but does not endanger correctness <ref> [2] </ref>. Further work is needed in this area. 5.5 Integer divide One implementation of the integer divide is finally shown. The divider is assumed strictly positive, as is the case in all call sites. It necessary because Fortran remainder is not positive for negative numbers.
Reference: [3] <author> Corinne Ancourt and Fran~cois Irigoin. </author> <title> Scanning Polyhedra with DO Loops. </title> <booktitle> In Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Algorithms presented in <ref> [3] </ref> can be used to gener ate the loop nest enumerating the local iterations. 8 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages When S is of rank jaj, optimal code is generated because no projections are required.
Reference: [4] <author> Corinne Ancourt and Fran~cois Irigoin. </author> <title> Automatic Code Distribution. </title> <booktitle> In Third Workshop on Compilers for Parallel Computers, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: When Y (or X, or Z,...) is referenced many times in the input loop or in the input program, these references must be clustered according to their connex-ity in the dependence graph <ref> [4] </ref>. Input dependencies are taken into account as well as usual ones (flow-, anti- and output-dependencies).
Reference: [5] <author> Fran~coise Andr e, Olivier Ch eron, and Jean-Louis Pazat. </author> <title> Compiling Sequential Programs for Distributed Memory Parallel Computers with Pandore II. </title> <type> Technical report, </type> <institution> IRISA, </institution> <month> May </month> <year> 1992. </year>
Reference: [6] <author> Fran~coise Andr e, Jean-Louis Pazat, and Henry Thomas. </author> <title> Pandore: A System to Manage Data Distribution. </title> <note> Publication Interne 519, IRISA, </note> <month> February </month> <year> 1990. </year>
Reference: [7] <author> Thomas Brandes. </author> <title> Efficient Data Parallel Programming without Explicit Message Passing for Distributed Memory Multiprocessors. </title> <type> Internal Report AHR-92 4, </type> <institution> High Performance Computing Center, German National Research Institute for Computer Science, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: The definition of the new language, hpf, was frozen in May 1993 [13] and the first compilers are already available <ref> [7, 8, 11, 36, 24] </ref>. This quick delivery was made possible by defining a subset of the language which only allows static distribution of data and prohibits dynamic redistribution.
Reference: [8] <author> Thomas Brandes. </author> <title> Adaptor: A Compilation System for Data Parallel Fortran Programs. </title> <type> Technical report, </type> <institution> High Performance Computing Center, German National Research Institute for Computer Science, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: The definition of the new language, hpf, was frozen in May 1993 [13] and the first compilers are already available <ref> [7, 8, 11, 36, 24] </ref>. This quick delivery was made possible by defining a subset of the language which only allows static distribution of data and prohibits dynamic redistribution.
Reference: [9] <author> David Callahan and Ken Kennedy. </author> <title> Compiling Programs for Distributed-Memory Multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <year> 1988. </year>
Reference-contexts: work Techniques to generate distributed code from sequential or parallel code using a uniform memory space have been extensively studied since 1988 [9, 28, 40, 31, 15, 22, 16, 6, 25, 2, 21, 27, 32, 23, 5, 19, The most obvious, most general and safest technique is called run-time resolution <ref> [9, 28, 31] </ref>. Each instruction is guarded by a condition that is true for processors that must execute it. <p> This rewriting scheme is easy to implement [11] but very inefficient at run-time because guards, tests, sends and receives are pure overhead. Moreover every processor has to execute the whole control flow of the program, and even for parallel loop, communications may sequentialize the program at run-time <ref> [9] </ref>. Many optimization techniques have been introduced to handle specific cases.
Reference: [10] <author> Siddhartha Chatterjee, John R. Gilbert, Fred J. E. Long, Robert Schreiber, and Shang-Hua Teng. </author> <title> Generating Local Addresses and Communication Sets for Data-Parallel Programs. </title> <booktitle> In Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1993. </year>
Reference-contexts: 66 67 68 69 70 71 72 73 74 75 76 77 78 79 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 0 2 4 6 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 <ref> [10] </ref>). The diagonal matrix P describes the processor geometry and each element of its diagonal is the number of processors in the corresponding dimension. It is a normalized version of the homogeneous matrix e P which declares the processors geometry and offset as specified by the user. <p> c c c c c c c c c c c c c c c c c c c c c c s s s s s s s 0 2 4 6 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 (from <ref> [10] </ref>). of the non-inverted matrix. The non-strict upper bound is 1. For instance the example given in [10] (Figure 4) is already normalized and can be translated in the following set of affine constraints D = (43); T = (128); A = (3); t 0 = (0); R = (1); C <p> c c c c c s s s s s s s 0 2 4 6 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 (from <ref> [10] </ref>). of the non-inverted matrix. The non-strict upper bound is 1. For instance the example given in [10] (Figure 4) is already normalized and can be translated in the following set of affine constraints D = (43); T = (128); A = (3); t 0 = (0); R = (1); C = (4); P = (4); L = (3); b = (42); S = (3). <p> This is the case when array sections are used as in <ref> [10, 18, 34] </ref>. <p> The local array elements are packed onto local memories, dimension by dimension. The geometric intuition of the packing scheme for one dimension, still in the example in <ref> [10] </ref> displayed on Figure 4, is shown in Figure 8 for the first processor of the dimension. The "ffi" and "*" are just used to support the geometrical intuition of the change of frame. <p> Constraints may also be simplified, for instance if the concerned elements just match a cycle. Moreover, it is possible to generate the loop nest directly on u 0 , when u is not used in the loop body. For the main example in <ref> [10] </ref>, such transformations produce the code shown in Figure 11. In the general resolution (Section 4.1) the cycle variables c were put after the local offsets `. The induced inner loop nest is then on c. <p> T (0,0) !HPF$ processors P (0:3,0:3) !HPF$ distribute & !HPF$ T (cyclic (4),cyclic (4)) onto P !HPF$ independent (I,J) do I = 1, 14 X (3*I,3*J) = Y (3*I,3*J) + enddo enddo !HPF$ independent (I) do I = 0, 14 enddo This is an extension of an example given in <ref> [10] </ref> showing that allocation of hpf arrays may be nontrivial. The reference to X in the first loop requires an allocation of X 0 and the computation of new loop bounds. <p> Four recent publications tackle any alignment and distribution but restrict references to array sections. Each dimension is independent of the others as was assumed in Section 4.3. Paalvast et al. are dealing with distribution without template. The subscript function can be an affine alignment function [29]. Chatterjee et al. <ref> [10] </ref> developed a finite state machine approach to enumerate local elements. No memory space is wasted and local array elements are ordered by Fortran lexico-graphic order exactly like user array elements. <p> They use array sections but compute some of the coefficients at run-time. Gupta et al. solve the block distribution case and use processor virtualization to handle cyclic distributions. Arrays are densely allocated as in <ref> [10] </ref> and the initial order is preserved but no formulae are given.
Reference: [11] <author> Fabien Coelho. </author> <title> Etude de la Compilation du High Performance Fortran. </title> <type> Master's thesis, </type> <institution> Universite Paris VI, </institution> <month> September </month> <year> 1993. </year> <institution> Rapport de DEA Systemes Informatiques, stage effectue au CRI, ENSMP. Rapport EMP-CRI E/178. </institution>
Reference-contexts: The definition of the new language, hpf, was frozen in May 1993 [13] and the first compilers are already available <ref> [7, 8, 11, 36, 24] </ref>. This quick delivery was made possible by defining a subset of the language which only allows static distribution of data and prohibits dynamic redistribution. <p> Each memory address is checked before it is referenced to decide whether the address is local and the reference is executed, whether it is remote, and a receive is executed, or whether it is remotely accessed and a send is executed. This rewriting scheme is easy to implement <ref> [11] </ref> but very inefficient at run-time because guards, tests, sends and receives are pure overhead. Moreover every processor has to execute the whole control flow of the program, and even for parallel loop, communications may sequentialize the program at run-time [9]. <p> Gupta et al. solve the block distribution case and use processor virtualization to handle cyclic distributions. Arrays are densely allocated as in [10] and the initial order is preserved but no formulae are given. Stichnoth uses the dual method for array allocation as in <ref> [11] </ref>, that is blocks are first compressed, and the cycle number is used as a second argument. 7 Conclusion The generation of efficient SPMD code from an HPF program is not a simple task and, up to now, many attempts have provided partial solutions and many techniques. <p> Future work includes an implementation in our 16 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages hpf compiler <ref> [11] </ref>, extensions to optimize sequential loops, including i/o loops, to overlap communication and computation, and to handle indirections. Acknowledgments We are thankful to Beatrice Apvrille for her many questions and to Pierre Jouvelot for his careful reading and the many improvements he suggested.
Reference: [12] <author> Paul Feautrier. </author> <title> Parametric Integer Programming. </title> <journal> RAIRO Recherche Operationnelle, </journal> <volume> 22 </volume> <pages> 243-268, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: A combined cost function might even be better by taking advantage of the Manhattan distance to minimize the number of hops and of the lexicographical minimum to insure uniqueness. These problems can be cast as linear parametric problems and solved <ref> [12] </ref>. When no replication occurs, elementary data communications implied by send Y and receive Y can 2 This could be avoided by exchanging data first with processors p 0 such that p 0 &lt; p and then with processors such that p 0 &gt; p, using the lexicographic order.
Reference: [13] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0 CRPC-TR 92225, Center for Research on Parallel Computation, </note> <institution> Rice University, Houston, USA, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The definition of the new language, hpf, was frozen in May 1993 <ref> [13] </ref> and the first compilers are already available [7, 8, 11, 36, 24]. This quick delivery was made possible by defining a subset of the language which only allows static distribution of data and prohibits dynamic redistribution. <p> the local processor spaces can also be used to translate sequential loops with a run-time resolution mecha nism or with some optimizations. 1 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages The reader is assumed knowledgeable in hpf directives <ref> [13] </ref> and optimization techniques for hpf [15, 36]. The paper is organized as follows. Section 2 shows how hpf directives can be expressed as affine constraints and normalized to simplify the compilation process and its description. <p> Alignments are specified dimension-wise with integer affine expressions as template subscript expressions. Each array index can be used at most once in a template subscript expression in any given alignment <ref> [13] </ref>, and each subscript expression cannot contain more than one index. <p> The extents (n in BLOCK (n) or CYCLIC (n)) are stored in a diagonal matrix, C. Such a distribution is not linear according to its definition <ref> [13, page 27] </ref> but may be written as a linear relation between the processor coordinate p, the template coordinate t and two additional variables, ` and c: Vector ` represents the offset within one block in one processor and vector c represents the number of wrap-arounds that must be performed to
Reference: [14] <author> Franco Gasperoni and Uwe Scheiegelshohn. </author> <title> Scheduling Loop on Parallel Processors: A Simple Algorithm with Close to Optimum Performance. </title> <note> Lecture Note, INRIA, </note> <year> 1992. </year>
Reference-contexts: A new storage management scheme is also proposed. Moreover other optimizations techniques may be applied to the generated code such as vec-torization [38], loop invariant code motion [1] and software pipelining <ref> [14, 37] </ref>. Future work includes an implementation in our 16 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages hpf compiler [11], extensions to optimize sequential loops, including i/o loops, to overlap communication and computation, and to handle indirections.
Reference: [15] <author> Hans Michael Gerndt. </author> <title> Automatic Paralleliza-tion for Distributed-Memory Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> University of Vienna, </institution> <year> 1989. </year>
Reference-contexts: Manufacturers and research laboratories, lead by Digital and Rice University, decided in 1991 to shift part of the burden onto compilers by providing the programmer a uniform address space to allocate objects and a (mainly) implicit way to express par fl fancourt,coelho,irigoin,keryellg@cri.ensmp.fr. allelism. Numerous research projects <ref> [15, 19, 36] </ref> and a few commercial products had shown that this goal could be achieved and the High Performance Fortran Forum was set up to select the most useful functionalities and to standardize the syntax. <p> These three steps, local memory allocation, local iteration enumeration and data communication, are put together as a general compilation scheme for parallel loops, known as INDEPENDENT in hpf, with affine bounds and subscript expressions. This compilation scheme directly generates optimized code which includes techniques such as guard elimination <ref> [15] </ref>, message vectorization and aggregation [19, 36], and overlap analysis [15]. There are no restrictions neither on the kind of distribution, block or cyclic, nor on the rank of array references. The memory allocation part is independent of parallel loops and can always be used. <p> This compilation scheme directly generates optimized code which includes techniques such as guard elimination <ref> [15] </ref>, message vectorization and aggregation [19, 36], and overlap analysis [15]. There are no restrictions neither on the kind of distribution, block or cyclic, nor on the rank of array references. The memory allocation part is independent of parallel loops and can always be used. <p> be used to translate sequential loops with a run-time resolution mecha nism or with some optimizations. 1 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages The reader is assumed knowledgeable in hpf directives [13] and optimization techniques for hpf <ref> [15, 36] </ref>. The paper is organized as follows. Section 2 shows how hpf directives can be expressed as affine constraints and normalized to simplify the compilation process and its description. <p> This optimization is known as overlap analysis <ref> [15] </ref>. Once remote values are copied into the overlapping Y 0 , all elements of view Y (p) can be accessed uniformly in Y 0 with no overhead. <p> Thus the local cache and/or prefetch mechanisms, if any, are efficiently used. The packing scheme is also compatible with overlap analysis techniques <ref> [15] </ref>. Local array declarations are extended to provide space for border elements that are owned by neighbor processors, and to simplify accesses to non-local elements. The overlap is induced by relaxing constraints on `, which is transformed through the scheme as relaxed constraints on u 0 2 . <p> Moreover every processor has to execute the whole control flow of the program, and even for parallel loop, communications may sequentialize the program at run-time [9]. Many optimization techniques have been introduced to handle specific cases. Gerndt introduced overlap analysis in <ref> [15] </ref> for block distributions. 15 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages When local array parts are allocated with the necessary overlap and when parallel loops are translated, the instruction guards can very often be moved in the loop <p> Such a scheme could reuse HPF distribution to map HPF processors on physical processors. Many partial optimization techniques are integrated in our direct synthesis approach: message vectorization, and aggregation [19], overlap analysis <ref> [15] </ref>. A new storage management scheme is also proposed. Moreover other optimizations techniques may be applied to the generated code such as vec-torization [38], loop invariant code motion [1] and software pipelining [14, 37].
Reference: [16] <author> Hans Michael Gerndt and Hans Peter Zima. </author> <title> Optimizing Communication in Superb. </title> <booktitle> In CONPAR90, </booktitle> <pages> pages 300-311, </pages> <year> 1990. </year>
Reference: [17] <author> Philippe Granger. </author> <title> Analyses Semantiques de Congruence. </title> <type> PhD thesis, </type> <institution> Ecole Polytechnique, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Linearity of access to temporary elements is preserved. The correctness of the scheme is shown by proving that a unique location y 0 is associated to each y. Further insight on this problem, the minimal covering of a set by interval congruences, can be found in Granger <ref> [17] </ref> and Masdupuy [26]. 12 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages ? H ? ? ?? . . . . . . . . . . . . . . . . . ? ? a X a
Reference: [18] <author> S. K. S. Gupta, S. D. Kaushik, S. Mufti, S. Sharma, C.-H. Huang, and P. Sadayap-pan. </author> <title> On compiling Array Expressions for Efficient Execution on Distributed-Memory Machines. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages II-301-II-305, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: This is the case when array sections are used as in <ref> [10, 18, 34] </ref>. <p> Note that the code generated in Figure 7 may be used to compute the fsm. In fact the lower iteration of the innermost loop is computed by the algorithm that constructs the fsm. Two papers by Stichnoth [34] on one hand and Gupta et al. <ref> [18] </ref> on the other present two similar methods to solve the same problem. They use array sections but compute some of the coefficients at run-time. Gupta et al. solve the block distribution case and use processor virtualization to handle cyclic distributions.
Reference: [19] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Manufacturers and research laboratories, lead by Digital and Rice University, decided in 1991 to shift part of the burden onto compilers by providing the programmer a uniform address space to allocate objects and a (mainly) implicit way to express par fl fancourt,coelho,irigoin,keryellg@cri.ensmp.fr. allelism. Numerous research projects <ref> [15, 19, 36] </ref> and a few commercial products had shown that this goal could be achieved and the High Performance Fortran Forum was set up to select the most useful functionalities and to standardize the syntax. <p> This compilation scheme directly generates optimized code which includes techniques such as guard elimination [15], message vectorization and aggregation <ref> [19, 36] </ref>, and overlap analysis [15]. There are no restrictions neither on the kind of distribution, block or cyclic, nor on the rank of array references. The memory allocation part is independent of parallel loops and can always be used. <p> Our scheme can also be extended to cope with processor virtualization if the virtualization scheme is expressed with affine constraints. Such a scheme could reuse HPF distribution to map HPF processors on physical processors. Many partial optimization techniques are integrated in our direct synthesis approach: message vectorization, and aggregation <ref> [19] </ref>, overlap analysis [15]. A new storage management scheme is also proposed. Moreover other optimizations techniques may be applied to the generated code such as vec-torization [38], loop invariant code motion [1] and software pipelining [14, 37].
Reference: [20] <author> Fran~cois Irigoin. </author> <title> Interprocedural Analyses for Programming Environment. </title> <editor> In Jack J. Don-gara and Bernard Tourancheau, editors, </editor> <booktitle> Environments and Tools for Parallel Scientific Computing, </booktitle> <pages> pages 333-350, </pages> <address> Saint-Hilaire-du-Touvet, </address> <month> September </month> <year> 1992. </year> <note> North-Holland, Ams-terdam, NSF-CNRS. 17 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages </note>
Reference-contexts: If Y is a distributed array, its local elements must be taken into account as a special reference and be accessed with (p; c) instead of i. The definition of view is thus altered to take into account regions <ref> [35, 20] </ref> of arrays. The equations due to subscript expression S Y are replaced by a parametric polyhedral subset of Y which can be automatically computed.
Reference: [21] <author> Charles Koelbel and Piyush Mehrotra. </author> <title> Compiling Global Name-Space Parallel Loops for Distributed Execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference: [22] <author> Charles Koelbel, Piyush Mehrotra, and John Van Rosendale. </author> <title> Supporting Shared Data Structures on Distributed Memory Architectures. </title> <type> Technical Report ASD 915, </type> <institution> Purdue University, </institution> <month> January </month> <year> 1990. </year>
Reference: [23] <author> Oded Lempel, Shlomit S. Pinter, and Eli Turiel. </author> <title> Parallelizing a C Dialect for Distributed Memory MIMD Machines. </title> <booktitle> In Language and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference: [24] <author> John Levesque. </author> <title> FORGE 90 and High Performance Fortran. Applied Parallel Research, </title> <publisher> Inc., </publisher> <year> 1992. </year>
Reference-contexts: The definition of the new language, hpf, was frozen in May 1993 [13] and the first compilers are already available <ref> [7, 8, 11, 36, 24] </ref>. This quick delivery was made possible by defining a subset of the language which only allows static distribution of data and prohibits dynamic redistribution.
Reference: [25] <author> J. Li and Marina Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference: [26] <author> Fran~cois Masdupuy. </author> <title> Array Indices Relational Semantic Analysis using Rational Cosets and Trapezoids. </title> <type> PhD thesis, </type> <institution> Ecole des mines de Paris, </institution> <month> December </month> <year> 1993. </year> <note> Technical Report EMP-CRI A/247. </note>
Reference-contexts: The correctness of the scheme is shown by proving that a unique location y 0 is associated to each y. Further insight on this problem, the minimal covering of a set by interval congruences, can be found in Granger [17] and Masdupuy <ref> [26] </ref>. 12 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages ? H ? ? ?? . . . . . . . . . . . . . . . . . ? ? a X a Y X x
Reference: [27] <author> John Merlin. </author> <title> Techniques for the Automatic Parallelisation of `Distributed Fortran 90'. </title> <type> SNARC 92 02, </type> <institution> University of Southampton, </institution> <year> 1992. </year>
Reference: [28] <author> Ravi Mirchandaney, Joel S. Saltz, Roger M. Smith, David M. Nicol, and Kay Crowley. </author> <title> Principles of Runtime Support for Parallel Processors. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: work Techniques to generate distributed code from sequential or parallel code using a uniform memory space have been extensively studied since 1988 [9, 28, 40, 31, 15, 22, 16, 6, 25, 2, 21, 27, 32, 23, 5, 19, The most obvious, most general and safest technique is called run-time resolution <ref> [9, 28, 31] </ref>. Each instruction is guarded by a condition that is true for processors that must execute it.
Reference: [29] <author> Edwin M. Paalvast, Henk J. Sips, and A.J. van Gemund. </author> <title> Automatic Parallel Program Generation and Optimization from Data Decompositions. </title> <booktitle> In 1991 International Conference on Parallel Processing | Volume II : Software, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Four recent publications tackle any alignment and distribution but restrict references to array sections. Each dimension is independent of the others as was assumed in Section 4.3. Paalvast et al. are dealing with distribution without template. The subscript function can be an affine alignment function <ref> [29] </ref>. Chatterjee et al. [10] developed a finite state machine approach to enumerate local elements. No memory space is wasted and local array elements are ordered by Fortran lexico-graphic order exactly like user array elements.
Reference: [30] <author> William Pugh. </author> <title> A Pratical Algorithm for Exact Array Dependence Analysis. </title> <journal> CACM, </journal> <volume> 35(8) </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Otherwise, the quality of the control overhead depends on the accuracy of integer projections <ref> [30] </ref> but the correctness does not.
Reference: [31] <author> Anne Rogers and Keshav Pingali. </author> <title> Process Decomposition Through Locality of Reference. </title> <booktitle> In ACM SIGPLAN International Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: work Techniques to generate distributed code from sequential or parallel code using a uniform memory space have been extensively studied since 1988 [9, 28, 40, 31, 15, 22, 16, 6, 25, 2, 21, 27, 32, 23, 5, 19, The most obvious, most general and safest technique is called run-time resolution <ref> [9, 28, 31] </ref>. Each instruction is guarded by a condition that is true for processors that must execute it.
Reference: [32] <author> Anne Rogers and Keshav Pingali. </author> <title> Compiling for distributed memory architectures, </title> <month> June </month> <year> 1992. </year> <title> communication personnelle, sans doute un rapport technique. </title>
Reference: [33] <author> Alexander Schrijver. </author> <title> Theory of linear and integer programming. </title> <publisher> Wiley, </publisher> <address> New-York, </address> <year> 1986. </year>
Reference-contexts: This allows each processor to scan its part of F with minimal control overhead. Parametric solution of HPF equations Set F is implicitly defined but a parametric definition is needed to enumerate its elements. An Her-mite form <ref> [33] </ref> of integer matrix F is used to find the parameters. This form associates to F three matrices H, P and Q, such that H = P F Q. P is a permutation, H a lower triangular integer matrix and Q a unimodular change of basis.
Reference: [34] <author> James M. Stichnoth. </author> <title> Efficient Compilation of Array Statements for Private Memory Multicomputers. </title> <type> CMU-CS-93 109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: This is the case when array sections are used as in <ref> [10, 18, 34] </ref>. <p> Note that the code generated in Figure 7 may be used to compute the fsm. In fact the lower iteration of the innermost loop is computed by the algorithm that constructs the fsm. Two papers by Stichnoth <ref> [34] </ref> on one hand and Gupta et al. [18] on the other present two similar methods to solve the same problem. They use array sections but compute some of the coefficients at run-time. Gupta et al. solve the block distribution case and use processor virtualization to handle cyclic distributions.
Reference: [35] <author> Remi Triolet, Paul Feautrier, and Fran~cois Irigoin. </author> <title> Direct Parallelization of Call Statements. </title> <booktitle> In Proceedings of the ACM Symposium on Compiler Construction, </booktitle> <year> 1986. </year>
Reference-contexts: If Y is a distributed array, its local elements must be taken into account as a special reference and be accessed with (p; c) instead of i. The definition of view is thus altered to take into account regions <ref> [35, 20] </ref> of arrays. The equations due to subscript expression S Y are replaced by a parametric polyhedral subset of Y which can be automatically computed.
Reference: [36] <author> Chau-Wen Tseng. </author> <title> An Optimising Fortran D Compiler for MIMD Distributed Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, Texas, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Manufacturers and research laboratories, lead by Digital and Rice University, decided in 1991 to shift part of the burden onto compilers by providing the programmer a uniform address space to allocate objects and a (mainly) implicit way to express par fl fancourt,coelho,irigoin,keryellg@cri.ensmp.fr. allelism. Numerous research projects <ref> [15, 19, 36] </ref> and a few commercial products had shown that this goal could be achieved and the High Performance Fortran Forum was set up to select the most useful functionalities and to standardize the syntax. <p> The definition of the new language, hpf, was frozen in May 1993 [13] and the first compilers are already available <ref> [7, 8, 11, 36, 24] </ref>. This quick delivery was made possible by defining a subset of the language which only allows static distribution of data and prohibits dynamic redistribution. <p> This compilation scheme directly generates optimized code which includes techniques such as guard elimination [15], message vectorization and aggregation <ref> [19, 36] </ref>, and overlap analysis [15]. There are no restrictions neither on the kind of distribution, block or cyclic, nor on the rank of array references. The memory allocation part is independent of parallel loops and can always be used. <p> be used to translate sequential loops with a run-time resolution mecha nism or with some optimizations. 1 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages The reader is assumed knowledgeable in hpf directives [13] and optimization techniques for hpf <ref> [15, 36] </ref>. The paper is organized as follows. Section 2 shows how hpf directives can be expressed as affine constraints and normalized to simplify the compilation process and its description. <p> The communication loops can be rearranged to generate vector messages. Tseng <ref> [36] </ref> presents lots of additional techniques (message aggregation and coalescing, message and vector message pipelining, computation replication, collective communication: : :). He assumes affine loop bounds and array subscripts to perform most optimizations. He only handles block and cyclic (1) distributions and the alignment coefficient must be 1.
Reference: [37] <author> Jian Wang and Christine Eisenbeis. </author> <title> Decomposed Software Pipelining: A New Approach to Exploit Instruction Level Parallelism for Loop Programs. In Working Conference on Architectures and Compilation Techniques for Fine and Medium Grain Parallelism, </title> <month> January </month> <year> 1993. </year>
Reference-contexts: A new storage management scheme is also proposed. Moreover other optimizations techniques may be applied to the generated code such as vec-torization [38], loop invariant code motion [1] and software pipelining <ref> [14, 37] </ref>. Future work includes an implementation in our 16 Submitted to the Special Issue on Compiling and Run-Time Issue for Distributed Address Space Machines Journal of Programming Languages hpf compiler [11], extensions to optimize sequential loops, including i/o loops, to overlap communication and computation, and to handle indirections.
Reference: [38] <author> Hans Zima and Barbara Chapman. </author> <title> Supercom-pilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: basis (I; J 0 ) become: e L = B 1 0 3 0 5 0 1 A ; e Li 0 4 15 3 ; a X = e S X i 7 0 10 ; a Y = e S Y i This transformation corresponds to loop normalization <ref> [38] </ref>. 2.4 Normalization To clarify the exposition of our compilation scheme, hpf declarations and loop nests as on Figure 2 are normalized to put the loop nests in the form shown on Figure 3. <p> Many partial optimization techniques are integrated in our direct synthesis approach: message vectorization, and aggregation [19], overlap analysis [15]. A new storage management scheme is also proposed. Moreover other optimizations techniques may be applied to the generated code such as vec-torization <ref> [38] </ref>, loop invariant code motion [1] and software pipelining [14, 37].
Reference: [39] <author> Hans Zima and Barbara Mary Chapman. </author> <title> Compiling for Distributed-Memory Systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <month> February </month> <year> 1993. </year>
Reference: [40] <author> Hans Peter Zima, H. J. Bast, and Hans Michael Gerndt. </author> <title> SUPERB : A Tool for Semi-Automatic MIMD/SIMD Parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 18 </month>
References-found: 40

