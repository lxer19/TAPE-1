URL: http://www.cs.toronto.edu/~frey/pp-allerton.ps.Z
Refering-URL: http://www.cs.toronto.edu/~frey/index.html
Root-URL: 
Email: brendan@comm.utoronto.ca and frank@comm.utoronto.ca  
Title: Probability Propagation and Iterative Decoding  
Author: Brendan J. Frey and Frank R. Kschischang 
Date: 1996.  
Note: In Proceedings of the 34 th Allerton Conference, Champaign-Urbana, Illinois,  
Address: Toronto  
Affiliation: Department of Electrical and Computer Engineering University of  
Abstract: In this paper, we present a unified graphical model framework for describing codes and deriving iterative decoding algorithms. We illustrate how the following systems can be described using graphical models: turbo-codes, serially-concatenated convolutional codes, frame-oriented turbo-codes, low-density parity-check codes, product codes, and convolutional codes on channels with memory. Recently proposed iterative decoding algorithms (e.g., turbo-decoding) can be viewed as a simple message passing procedure on the graphical model. This framework provides the means to derive new iterative decoders for new codes quite easily.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year> <note> Revised second printing. </note>
Reference-contexts: We observe that the basic computational paradigm underlying these recently developed techniques appears to be "probability propagation in graphical models," a technique developed in the past decade in the expert systems literature, most notably by Pearl and his co-workers (see e.g., <ref> [1] </ref>). Interestingly, Gallager's algorithm [2] for decoding low-density parity-check codes is an example of an iterative decoding algorithm that also is essentially an instance of "probability propagation" published over 30 years ago, but mostly forgotten in the interim. <p> We generalize Pearl's distributed soldier counting example <ref> [1, p. 155] </ref> in the following way. Suppose that HQ wishes to obtain a head count for the entire network. <p> Fig. 3b with an additional check vertex connected to v 1 , v 5 , v 6 and v 7 will also map to the MRF in Fig. 3a. 3.3 Bayesian Networks We now introduce Bayesian networks that, unlike Markov random fields and Tanner graphs, have directed acyclic graphical representations <ref> [1] </ref>. <p> can compute P (rjO) using the following formula: P (rjO) / 8j i X h P (rjfq i g 8i ) Y P (q k jO q k Ar ) ; (5) where the symbol " r /" indicates equality up to a constant that is independent of r. (See <ref> [1] </ref> for derivations.) In order to get the actual value of P (rjO), the quantity computed from the right hand side of (5) must be scaled so that P r P (rjO) = 1.
Reference: [2] <author> R. G. Gallager, </author> <title> Low-Density Parity-Check Codes. </title> <address> Cambridge, MA: </address> <publisher> M.I.T. Press, </publisher> <year> 1963. </year>
Reference-contexts: We observe that the basic computational paradigm underlying these recently developed techniques appears to be "probability propagation in graphical models," a technique developed in the past decade in the expert systems literature, most notably by Pearl and his co-workers (see e.g., [1]). Interestingly, Gallager's algorithm <ref> [2] </ref> for decoding low-density parity-check codes is an example of an iterative decoding algorithm that also is essentially an instance of "probability propagation" published over 30 years ago, but mostly forgotten in the interim. Recently, Wiberg, Loeliger and Kotter [3] have refocussed attention on graphical models for codes. <p> Examples of compound codes include turbo-codes [5], serially concatenated convolutional codes [6], frame-oriented turbo-codes [7], low-density parity-check codes <ref> [2] </ref>, and iteratively decoded product codes [8, 9]. 2 A Simple "Spy Network" We now give a simple example of a distributed algorithm that computes a result via local message passing. This example, though simplistic, serves to illustrate many of the important properties of probability propagation algorithms. <p> A frame-oriented turbo-code [7] is shown in Fig. 6c. This code was designed to eliminate the need for trellis termination when using short block lengths. Fig. 6d shows a low-density parity check code <ref> [2] </ref>. This code consists of parity check restrictions (upper vertices) on subsets of the codeword symbols (middle vertices). MacKay and Neal [4] were the first to describe Gallager's codes using Bayesian networks. Fig. 6e shows a systematic product code.
Reference: [3] <author> N. Wiberg, H.-A. Loeliger, and R. Kotter, </author> <title> "Codes and iterative decoding on general graphs," </title> <journal> European Trans. on Telecommun., </journal> <volume> vol. 6, </volume> <pages> pp. 513-525, </pages> <address> Sep./Oct. </address> <year> 1995. </year>
Reference-contexts: Interestingly, Gallager's algorithm [2] for decoding low-density parity-check codes is an example of an iterative decoding algorithm that also is essentially an instance of "probability propagation" published over 30 years ago, but mostly forgotten in the interim. Recently, Wiberg, Loeliger and Kotter <ref> [3] </ref> have refocussed attention on graphical models for codes. <p> Our treatment follows the slightly different presentation of Wiberg, et. al. <ref> [3] </ref>. A Tanner graph is a bipartite graph representation for a check structure, similar to the one described above. In such a graph, there are two types of vertices corresponding to the variables and the "checks", respectively, with no edges connecting vertices of the same type.
Reference: [4] <author> D. J. C. MacKay and R. M. Neal, </author> <title> "Good codes based on very sparse matrices." </title> <note> Submitted to IEEE Transactions on Information Theory, </note> <year> 1996. </year>
Reference-contexts: They show that such graphical models provide a natural setting in which to describe and study iterative decoding techniques, much as the code trellis is an appropriate model in which to describe and study "conventional" maximum-likelihood soft-decision decoding using the Viterbi algorithm. MacKay and Neal in <ref> [4] </ref> have also observed that iterative decoding is also essentially an instance of probability or "belief" propagation in a graphical model. <p> This code was designed to eliminate the need for trellis termination when using short block lengths. Fig. 6d shows a low-density parity check code [2]. This code consists of parity check restrictions (upper vertices) on subsets of the codeword symbols (middle vertices). MacKay and Neal <ref> [4] </ref> were the first to describe Gallager's codes using Bayesian networks. Fig. 6e shows a systematic product code. The distribution for the parity symbol in each row (column) is conditioned on the information symbols in its row (column), and is nonzero only for even parity row-wise (column-wise) configurations.
Reference: [5] <author> C. Berrou, A. Glavieux, and P. Thitimajshima, </author> <title> "Near Shannon limit error-correcting coding and decoding: Turbo codes," </title> <booktitle> in Proc. IEEE Int. Conf. Commun. </booktitle> <address> (ICC), (Geneva, Switzerland), </address> <pages> pp. 1064-1070, </pages> <year> 1993. </year>
Reference-contexts: Examples of compound codes include turbo-codes <ref> [5] </ref>, serially concatenated convolutional codes [6], frame-oriented turbo-codes [7], low-density parity-check codes [2], and iteratively decoded product codes [8, 9]. 2 A Simple "Spy Network" We now give a simple example of a distributed algorithm that computes a result via local message passing. <p> In fact, it is straightforward to prove that turbo-decoding is probability propagation for this compound network. For the sake of simplicity, we will avoid the mathematical details. The turbo-decoding algorithm uses the forward-backward algorithm (or an approximation to it) to process each constituent trellis. The algorithm uses "extrinsic information" <ref> [5, 9] </ref> produced by the previous step, when processing the next trellis. This is the information that is passed from one trellis to the other through the information symbols. <p> For "compound codes," however, the Bayesian networks can be broken into tractable subnetworks, each describing a "constituent code" and in which probability propagation can be applied. Iterating over these constituent decoders can result in excellent decoding performance, as demonstrated by Berrou, et al. <ref> [5] </ref>. We have shown that many recently proposed iterative decoders can be described as message passing on a code graph.
Reference: [6] <author> S. Benedetto and G. Montorsi, </author> <title> "Iterative decoding of serially concatenated convolutional codes," </title> <journal> Electronics Letters, </journal> <volume> vol. 32, </volume> <pages> pp. 1186-1188, </pages> <year> 1996. </year>
Reference-contexts: Examples of compound codes include turbo-codes [5], serially concatenated convolutional codes <ref> [6] </ref>, frame-oriented turbo-codes [7], low-density parity-check codes [2], and iteratively decoded product codes [8, 9]. 2 A Simple "Spy Network" We now give a simple example of a distributed algorithm that computes a result via local message passing. <p> This compound code consists of two chain-type networks that are connected using a different ordering of the information symbol vertices. Whereas the upper chain directly uses the information sequence, the lower chain uses a permuted version of the information sequence. A serially concatenated convolutional compound code <ref> [6] </ref> is shown in Fig. 6b. This system is essentially the same as Forney's concatenated codes [13], except that a convolutional code is used for both the inner and outer codes, instead of the more popular Reed-Solomon outer code. A frame-oriented turbo-code [7] is shown in Fig. 6c.
Reference: [7] <author> C. Berrou and M. </author> <title> Jezequel, </title> <journal> "Frame-oriented convolutional turbo-codes," Electronics Letters, </journal> <volume> vol. 32, </volume> <pages> pp. 1362-1364, </pages> <year> 1996. </year>
Reference-contexts: Examples of compound codes include turbo-codes [5], serially concatenated convolutional codes [6], frame-oriented turbo-codes <ref> [7] </ref>, low-density parity-check codes [2], and iteratively decoded product codes [8, 9]. 2 A Simple "Spy Network" We now give a simple example of a distributed algorithm that computes a result via local message passing. <p> A serially concatenated convolutional compound code [6] is shown in Fig. 6b. This system is essentially the same as Forney's concatenated codes [13], except that a convolutional code is used for both the inner and outer codes, instead of the more popular Reed-Solomon outer code. A frame-oriented turbo-code <ref> [7] </ref> is shown in Fig. 6c. This code was designed to eliminate the need for trellis termination when using short block lengths. Fig. 6d shows a low-density parity check code [2]. This code consists of parity check restrictions (upper vertices) on subsets of the codeword symbols (middle vertices).
Reference: [8] <author> J. Lodge, R. Young, P. Hoeher, and J. Hagenauer, </author> <title> "Separable MAP `filters' for the decoding of product and concatenated codes," </title> <booktitle> in Proceedings of IEEE International Conference on Communications, </booktitle> <pages> pp. 1740-1745, </pages> <year> 1993. </year>
Reference-contexts: Examples of compound codes include turbo-codes [5], serially concatenated convolutional codes [6], frame-oriented turbo-codes [7], low-density parity-check codes [2], and iteratively decoded product codes <ref> [8, 9] </ref>. 2 A Simple "Spy Network" We now give a simple example of a distributed algorithm that computes a result via local message passing. This example, though simplistic, serves to illustrate many of the important properties of probability propagation algorithms.
Reference: [9] <author> J. Hagenauer, E. Offer, and L. Papke, </author> <title> "Iterative decoding of binary block and convolutional codes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 429-445, </pages> <year> 1996. </year>
Reference-contexts: Examples of compound codes include turbo-codes [5], serially concatenated convolutional codes [6], frame-oriented turbo-codes [7], low-density parity-check codes [2], and iteratively decoded product codes <ref> [8, 9] </ref>. 2 A Simple "Spy Network" We now give a simple example of a distributed algorithm that computes a result via local message passing. This example, though simplistic, serves to illustrate many of the important properties of probability propagation algorithms. <p> In fact, it is straightforward to prove that turbo-decoding is probability propagation for this compound network. For the sake of simplicity, we will avoid the mathematical details. The turbo-decoding algorithm uses the forward-backward algorithm (or an approximation to it) to process each constituent trellis. The algorithm uses "extrinsic information" <ref> [5, 9] </ref> produced by the previous step, when processing the next trellis. This is the information that is passed from one trellis to the other through the information symbols.
Reference: [10] <author> R. Kindermann and J. L. Snell, </author> <title> Markov Random Fields and their Applications. </title> <address> Providence, Rhode Island: </address> <publisher> American Mathematical Society, </publisher> <year> 1980. </year>
Reference-contexts: These graphical models will be useful for describing the structure of codes, and are the key to "probability propagation" and iterative decoding. 3.1 Markov Random Fields Markov random fields (MRFs) are graphical models based on undirected graphs (see textbook reference <ref> [10] </ref>).
Reference: [11] <author> R. M. Tanner, </author> <title> "A recursive approach to low complexity codes," </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> vol. IT-27, </volume> <pages> pp. 533-547, </pages> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: The MRF places equal probability on all codewords that satisfy even parity in cliques q 1 , q 2 and q 3 . 3.2 Tanner Graphs Tanner graphs were introduced in <ref> [11] </ref> for the construction of good long error-correcting codes in terms of shorter codes. Our treatment follows the slightly different presentation of Wiberg, et. al. [3]. A Tanner graph is a bipartite graph representation for a check structure, similar to the one described above.
Reference: [12] <author> L. E. Baum and T. Petrie, </author> <title> "Statistical inference for probabilistic functions of finite state markov chains," </title> <journal> Annals of Mathematical Statistics, </journal> <volume> vol. 37, </volume> <pages> pp. 1559-1563, </pages> <year> 1966. </year>
Reference-contexts: By grouping information and state variables together, we eliminate all undirected cycles, at the expense of increasing the complexity of the state variables. The forward-backward algorithm (a.k.a. the "BCJR algorithm") for computing P (u k jy) for such a Markov-type model was first developed by Baum and Petrie <ref> [12] </ref>. It turns out that this algorithm is identical to the algorithm that results when probability propagation is applied to the corresponding Bayesian network.
Reference: [13] <author> G. D. Forney, Jr., </author> <title> Concatenated Codes. </title> <address> Cambridge MA.: </address> <publisher> MIT Press, </publisher> <year> 1966. </year>
Reference-contexts: Whereas the upper chain directly uses the information sequence, the lower chain uses a permuted version of the information sequence. A serially concatenated convolutional compound code [6] is shown in Fig. 6b. This system is essentially the same as Forney's concatenated codes <ref> [13] </ref>, except that a convolutional code is used for both the inner and outer codes, instead of the more popular Reed-Solomon outer code. A frame-oriented turbo-code [7] is shown in Fig. 6c. This code was designed to eliminate the need for trellis termination when using short block lengths.
References-found: 13

