URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/phrensy/pub/papers/BerthomeFMPP94.ps
Refering-URL: http://www.cs.cmu.edu/~bmm/papers.html
Root-URL: 
Title: Sorting-Based Selection Algorithms for Hypercubic Networks  
Author: P. Berthome A. Ferreira B. M. Maggs S. Perennes ; C. G. Plaxton 
Date: August 1997  
Abstract: This paper presents several deterministic algorithms for selecting the kth largest record from a set of n records on any n-node hypercubic network. All of the algorithms are based on the selection algorithm of Cole and Yap, as well as on various sorting algorithms for hypercubic networks. Our fastest algorithm runs in O(lg n lg fl n) time, very nearly matching the trivial (lg n) lower bound. Previously, the best upper bound known for selection was O(lg n lg lg n). A key subroutine in our O(lg n lg fl n) time selection algorithm is a sparse version of the Sharesort algorithm that sorts n records using p processors, p n, in O(lg n(lg lg p lg lg p n ) 2 ) time.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ajtai, J. Komlos, W. L. Steiger, and E. Szemeredi. </author> <title> Deterministic selection in O(log log n) parallel time. </title> <booktitle> In Proceedings of the 18th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 188-195, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Cole and Yap [5] then described an O ((lg lg n) 2 ) selection algorithm for this model. The running time was later improved to O (lg lg n) by Ajtai, Komlos, Steiger, and Szemeredi <ref> [1] </ref>. The comparisons performed by the latter algorithm are specified by an expander graph, however, making it unlikely that this algorithm can be efficiently implemented on a hypercubic network. 3 A different set of upper and lower bounds hold in the PRAM models.
Reference: [2] <author> P. Beame and J. H-astad. </author> <title> Optimal bounds for decision problems on the CRCW PRAM. </title> <booktitle> In Proceedings of the 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 83-93, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: The comparisons performed by the latter algorithm are specified by an expander graph, however, making it unlikely that this algorithm can be efficiently implemented on a hypercubic network. 3 A different set of upper and lower bounds hold in the PRAM models. Beame and H-astad <ref> [2] </ref> proved an (lg n= lg lg n) lower bound on the time for selection in the CRCW comparison PRAM using a polynomial number of processors. Vishkin [11] discovered an O (lg n lg lg n) time PRAM algorithm that uses O (n= lg n lg lg n) processors.
Reference: [3] <author> M. Blum, R. W. Floyd, V. R. Pratt, R. L. Rivest, and R. E. Tarjan. </author> <title> Time bounds for selection. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 7 </volume> <pages> 448-461, </pages> <year> 1973. </year>
Reference-contexts: Plaxton also showed that any deterministic algorithm for solving the selection problem on a p-processor hypercubic network requires ((n=p) lg lg p + lg p) time in the worst case [9]. Since the selection problem can be solved in linear time sequentially <ref> [3] </ref>, the lower bound implies that it is not possible to design a deterministic hypercubic selection algorithm with linear speedup. For n = p the lower bound is (lg n), which is the diameter of the network.
Reference: [4] <author> R. Cole. </author> <title> An optimally efficient parallel selection algorithm. </title> <journal> Information Processing Letters, </journal> <volume> 26 </volume> <pages> 295-299, </pages> <year> 1988. </year>
Reference-contexts: The algorithm is work-efficient (i.e., exhibits optimal speedup) because the processor time product is equal to the time, O (n), of the fastest sequential algorithm for this problem. Cole <ref> [4] </ref> later found an O (lg n lg fl n) time work-efficient PRAM algorithm. 1.5 Outline A "basic" selection algorithm that runs in O (lg n lg lg n) time is presented in Section 2. Several faster selection algorithms are presented in the remainder of the paper.
Reference: [5] <author> R. Cole and C. K. Yap. </author> <title> A parallel median algorithm. </title> <journal> Information Processing Letters, </journal> <volume> 20 </volume> <pages> 137-139, </pages> <year> 1985. </year>
Reference-contexts: The fastest runs in O (lg n lg fl n) time on an n-node network. The fastest previously known algorithm ran in O (lg n lg lg n) time [9]. The algorithms use a technique called succesive sampling, which was previously used by Cole and Yap <ref> [5] </ref> to solve the selection problem in an idealized model of computation called the parallel comparision model. <p> Our algorithm uses a technique called successive sampling. This technique is also used in the algorithm of Cole and Yap <ref> [5] </ref>. Given a set S of n records and an integer k, 0 k &lt; n, a successive sampling algorithm computes lower and upper approximations as follows. <p> The lower bound implies a lower bound on the time to select the kth smallest record as well. Valiant also showed how to find the largest record in O (lg lg n) time. Cole and Yap <ref> [5] </ref> then described an O ((lg lg n) 2 ) selection algorithm for this model. The running time was later improved to O (lg lg n) by Ajtai, Komlos, Steiger, and Szemeredi [1]. <p> Finally, in Section 7 we show how to avoid the non-uniformity introduced in Section 6. 2 An O (lg n lg lg n) selection algorithm In this section, we develop an efficient subroutine for selection refinement based on the parallel comparison model algorithm of Cole and Yap <ref> [5] </ref>. There are two major differences.
Reference: [6] <author> R. E. Cypher and C. G. Plaxton. </author> <title> Deterministic sorting in nearly logarithmic time on the hypercube and related computers. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 47 </volume> <pages> 501-548, </pages> <year> 1993. </year>
Reference-contexts: The algorithms also use as subroutines sorting algorithms for hypercubic networks due to Nassimi and Sahni [8] and Cypher and Plaxton <ref> [6] </ref>. 1 Laboratoire de l'Informatique du Parallelisme, CNRS, Ecole Normale Superieure de Lyon, 46, Allee d'Italie, 69364 Lyon Cedex 07, France. <p> On the other hand, the performance of selection refinement algorithms depends heavily on the cost of sorting "small" sets of records (i.e., sorting n records using p n processors). For the hypercube, the fastest n-record n-processor sorting algorithm known is the Sharesort algorithm of Cypher and Plaxton <ref> [6] </ref>, which runs in O (lg n (lg lg n) 2 ) time. (A non-uniform version of the Sharesort algorithm runs in O (lg n lg lg n) time [6].) In addition to Sharesort, we will make use of Nassimi and Sahni's sparse enumeration sort [8], which sort n records on <p> For the hypercube, the fastest n-record n-processor sorting algorithm known is the Sharesort algorithm of Cypher and Plaxton <ref> [6] </ref>, which runs in O (lg n (lg lg n) 2 ) time. (A non-uniform version of the Sharesort algorithm runs in O (lg n lg lg n) time [6].) In addition to Sharesort, we will make use of Nassimi and Sahni's sparse enumeration sort [8], which sort n records on p processors, p n, in O ((lg n lg p)=(lg p lg n)) time. (Note that sparse enumeration sort runs in optimal O (lg n) time if p n <p> An O (lg n lg fl n) algorithm is presented in Section 6. This time bound is obtained at the expense of using a nonuniform variant of the Sharesort algorithm <ref> [6] </ref> that requires a certain amount of preprocessing. <p> to Select 1 (S; p; k) with n = jSj = p runs in O (lg n lg (3) n) time. 5 An O (lg n lg n) algorithm We can improve the time bound achieved in Section 4 by making use of the Sharesort algorithm of Cypher and Plaxton <ref> [6] </ref>. Several variants of that algorithm exist; in particular, detailed descriptions of two versions of Sharesort may be found in [6]. Both of these variants are designed to sort n records on an n-processor hypercubic network. <p> time. 5 An O (lg n lg n) algorithm We can improve the time bound achieved in Section 4 by making use of the Sharesort algorithm of Cypher and Plaxton <ref> [6] </ref>. Several variants of that algorithm exist; in particular, detailed descriptions of two versions of Sharesort may be found in [6]. Both of these variants are designed to sort n records on an n-processor hypercubic network. The first algorithm runs in O (lg n (lg lg n) 3 ) time and the second algorithm, which is somewhat more complicated, runs in O (lg n (lg lg n) 2 ) time. <p> There exists a non-uniform deterministic algorithm for sorting these records in time O (lg n (lg lg p lg lg p 13 Proof: As indicated in Section 5, there are a number of variants of the Sharesort algorithm of Cypher and Plaxton <ref> [6] </ref>. These algorithms differ solely in the way that the so-called shared key sorting subroutine is implemented. <p> Perhaps the simplest variant of Sharesort runs in O (lg n lg lg n) time and relies upon an optimal logarithmic time shared key sorting subroutine. This particular result is mentioned in the original Sharesort paper <ref> [6] </ref> and more fully described by Leighton [7, Section 3.5.3]. Although it is the fastest of the Sharesort variants, this sorting algorithm suffers from the disadvantage that it is non-uniform. <p> Let M (x) denote the task of merging x sorted lists of length x 4 . One possible recurrence for performing the merge is (minor technical details related to integrality constraints are dealt with in <ref> [6] </ref> and will not be addressed here) M (n 1=5 ) M (n 4=45 ) + M (n 1=9 ) + O (lg n) + SKS (n); (3) where SKS (n) denotes the time required to solve the shared key sorting problem. (To justify the preceding recurrence, apply Equation (1) of <p> and will not be addressed here) M (n 1=5 ) M (n 4=45 ) + M (n 1=9 ) + O (lg n) + SKS (n); (3) where SKS (n) denotes the time required to solve the shared key sorting problem. (To justify the preceding recurrence, apply Equation (1) of <ref> [6] </ref> with a = 1 5 lg n, b = 4a, a 0 = 4 b 0 = 4a 0 , a 00 = 1 9 lg n, b 00 = 4a 00 , and observe that the additive O (a lg a) term corresponds to the sum of O (a) <p> The fastest known uniform version of the shared key sorting subroutine takes O (lg n lg lg n) time, which leads to an O (lg n (lg lg n) 2 ) running time for the corresponding uniform variant of Sharesort <ref> [6] </ref>. In the following, we show how to adapt this uniform version of Sharesort to obtain a uniform version of a "sparse" Sharesort, that is, an algorithm for sorting n records on p n processors. <p> By a similar analysis as that provided in Section 6, the theorem follows if we can prove that SSKS (n; p) = O (lg n (lg lg p lg lg p n )): (5) We now describe how to modify the SharedKeySort algorithm of <ref> [6] </ref> to obtain a "SparseSharedKeySort" routine satisfying Equation (5). Because the complete description of the SharedKeySort algorithm is quite lengthy, we will content ourselves with a description of the differences between SparseSharedKeySort and SharedKeySort. Fortunately these differences are minor. <p> Fortunately these differences are minor. We begin by observing that SharedKeySort consists of a call to subroutine PlanRoute followed by a call to subroutine DoRoute <ref> [6, Section 7.1] </ref>. For the case n = p considered in [6], each of these two subroutines runs in O (lg n lg lg n) time. <p> Fortunately these differences are minor. We begin by observing that SharedKeySort consists of a call to subroutine PlanRoute followed by a call to subroutine DoRoute [6, Section 7.1]. For the case n = p considered in <ref> [6] </ref>, each of these two subroutines runs in O (lg n lg lg n) time. <p> Let us consider subroutine DoRoute first, since it is simpler to deal with than PlanRoute. Looking at the three-parameter recurrence for the running time of DoRoute appearing in <ref> [6, Section 7.3] </ref>, we observe that: (i) the third parameter does not affect the running time and hence can be ignored, and (ii) we can assume without loss of generality that the first two parameters are equal since they are equal in every recursive call. <p> DoRoute is upper-bounded by a recurrence of the form T (n) 2T (O ( n lg n)) + O (lg n); (6) where the parameter n above corresponds to 2 a+b (which can be assumed to be equal to 2 2a , by observation (ii) above) in the recurrence of <ref> [6, Section 7.3] </ref>. <p> Looking at the three-parameter recurrence for the running time of PlanRoute appearing in <ref> [6, Section 7.2] </ref>, we observe that: (i) the third parameter does not affect the running time and hence can be ignored, and (ii) we can assume without loss of generality that the first two parameters are equal since they are equal in every recursive call. <p> by a recurrence of the form T (n) T (O ( n lg n)) + O (lg n lg lg n); (7) where the parameter n above corresponds to 2 a+b (which can be assumed to be equal to 2 2a , by observation (ii) above) in the recurrence of <ref> [6, Section 7.2] </ref>. This recurrence solves to give T (n) = O (lg n lg lg n). <p> In order to see that this lg lg n factor can in fact be replaced by lg lg p lg lg p n , we need to understand how the lg lg n factor arises. A cursory examination of the PlanRoute algorithm <ref> [6, Section 7.2] </ref> reveals that the lg lg n factor corresponds to the number of "classes" into which the set of n input records is partitioned. The partitioning into classes is performed through lg lg n calls to the subroutine Balance.
Reference: [7] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, and Hypercubes. </title> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: All of the algorithms described in this paper use the edges of the hypercube in a very restricted way. At each time step, only the edges associated with a single dimension are used, and consecutive dimensions are used on consecutive steps. Such algorithms are called normal <ref> [7, Section 3.1.4] </ref>. The bounded-degree variants of the hypercube, including the butterfly, cube-connected cycles, and shu*e-exchange network, can all simulate any normal hypercube algorithm with constant slowdown [7, Sections 3.2.3 and 3.3.3]. <p> Such algorithms are called normal [7, Section 3.1.4]. The bounded-degree variants of the hypercube, including the butterfly, cube-connected cycles, and shu*e-exchange network, can all simulate any normal hypercube algorithm with constant slowdown <ref> [7, Sections 3.2.3 and 3.3.3] </ref>. For simplicity, we will describe all of the algorithms in terms of the hypercube. 1.2 Selection refinement Like most selection algorithms, the algorithms in this paper use a technique called selection refinement. <p> Perhaps the simplest variant of Sharesort runs in O (lg n lg lg n) time and relies upon an optimal logarithmic time shared key sorting subroutine. This particular result is mentioned in the original Sharesort paper [6] and more fully described by Leighton <ref> [7, Section 3.5.3] </ref>. Although it is the fastest of the Sharesort variants, this sorting algorithm suffers from the disadvantage that it is non-uniform.
Reference: [8] <author> D. Nassimi and S. Sahni. </author> <title> Parallel permutation and sorting algorithms and a new generalized connection network. </title> <journal> JACM, </journal> <volume> 29 </volume> <pages> 642-667, </pages> <year> 1982. </year> <month> 18 </month>
Reference-contexts: The algorithms use a technique called succesive sampling, which was previously used by Cole and Yap [5] to solve the selection problem in an idealized model of computation called the parallel comparision model. The algorithms also use as subroutines sorting algorithms for hypercubic networks due to Nassimi and Sahni <ref> [8] </ref> and Cypher and Plaxton [6]. 1 Laboratoire de l'Informatique du Parallelisme, CNRS, Ecole Normale Superieure de Lyon, 46, Allee d'Italie, 69364 Lyon Cedex 07, France. <p> algorithm of Cypher and Plaxton [6], which runs in O (lg n (lg lg n) 2 ) time. (A non-uniform version of the Sharesort algorithm runs in O (lg n lg lg n) time [6].) In addition to Sharesort, we will make use of Nassimi and Sahni's sparse enumeration sort <ref> [8] </ref>, which sort n records on p processors, p n, in O ((lg n lg p)=(lg p lg n)) time. (Note that sparse enumeration sort runs in optimal O (lg n) time if p n 1+" for some positive constant ".) The fastest previously known algorithm for solving the selection problem <p> There are two major differences. First, we use Nassimi and Sahni's sparse enumeration sort <ref> [8] </ref> instead of a constant time sort (as is possible in the parallel comparison model), and second we obtain a total running time that is proportional to the running time of the largest call to sparse enumeration sort, whereas in the Cole-Yap algorithm, the running time is proportional to the number
Reference: [9] <author> C. G. Plaxton. </author> <title> Efficient Computation on Sparse Interconnection Networks. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: The fastest runs in O (lg n lg fl n) time on an n-node network. The fastest previously known algorithm ran in O (lg n lg lg n) time <ref> [9] </ref>. The algorithms use a technique called succesive sampling, which was previously used by Cole and Yap [5] to solve the selection problem in an idealized model of computation called the parallel comparision model. <p> enumeration sort runs in optimal O (lg n) time if p n 1+" for some positive constant ".) The fastest previously known algorithm for solving the selection problem on a hypercubic network is due to Plaxton and runs in O (lg n lg lg n) time on an n-node network <ref> [9] </ref>. Of course, the selection problem can also be solved in O (lg n (lg lg n) 2 ) time using Sharesort. <p> Plaxton also showed that any deterministic algorithm for solving the selection problem on a p-processor hypercubic network requires ((n=p) lg lg p + lg p) time in the worst case <ref> [9] </ref>. Since the selection problem can be solved in linear time sequentially [3], the lower bound implies that it is not possible to design a deterministic hypercubic selection algorithm with linear speedup. For n = p the lower bound is (lg n), which is the diameter of the network. <p> Theorem 2 Any call to BasicSelect (S; p; k) with n = jSj = p runs in O (lg n lg lg n) time. Procedure BasicSelect is essentially equivalent to a selection algorithm described by Plax-ton in <ref> [9] </ref>.
Reference: [10] <author> L. G. Valiant. </author> <title> Parallelism in comparison problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 4 </volume> <pages> 348-355, </pages> <year> 1975. </year>
Reference-contexts: Since the selection problem can be solved in linear time sequentially [3], the lower bound implies that it is not possible to design a deterministic hypercubic selection algorithm with linear speedup. For n = p the lower bound is (lg n), which is the diameter of the network. In <ref> [10] </ref>, Valiant proved an (lg lg n) lower bound on the time to find the largest record in a set of n records using n processors in an idealized model called the parallel comparison model.
Reference: [11] <author> U. Vishkin. </author> <title> An optimal parallel algorithm for selection. </title> <booktitle> In Parallel and Distributed Computing, Volume 4 of Advances in Computing Research, </booktitle> <pages> pages 79-86. </pages> <publisher> JAI Press, </publisher> <address> Greenwich, CT, </address> <year> 1987. </year> <month> 19 </month>
Reference-contexts: Beame and H-astad [2] proved an (lg n= lg lg n) lower bound on the time for selection in the CRCW comparison PRAM using a polynomial number of processors. Vishkin <ref> [11] </ref> discovered an O (lg n lg lg n) time PRAM algorithm that uses O (n= lg n lg lg n) processors. The algorithm is work-efficient (i.e., exhibits optimal speedup) because the processor time product is equal to the time, O (n), of the fastest sequential algorithm for this problem.
References-found: 11

