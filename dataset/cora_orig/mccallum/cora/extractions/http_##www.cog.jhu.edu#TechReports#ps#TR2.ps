URL: http://www.cog.jhu.edu/TechReports/ps/TR2.ps
Refering-URL: http://www.cog.jhu.edu/TechReports/index.html
Root-URL: 
Title: Learnability in Optimality Theory (short version)  
Phone: Fax: (410) 516-8020  
Author: Bruce Tesar Paul Smolensky 
Date: October 1996  
Address: Baltimore, MD 21218-2685  
Note: F (410) 516-5250  
Abstract: Technical Report JHU ! CogSci ! 96 ! 2 http://www.cogsci.jhu.edu/TechReports/ TR-Request@cogsci.jhu.edu 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackema, Peter, and Ad Neeleman. </author> <title> In press. Optimal questions. In Proceedings of the Workshop on Optimality in SyntaxIs the best good enough? Cambridge, </title> <address> Mass: </address> <publisher> MIT Press and MIT Working Papers in Linguistics. </publisher>
Reference: <author> Angluin, Dana. </author> <year> 1978. </year> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control 45:117135. </journal>
Reference-contexts: The idea is that structural constraints will only be demoted below the faithfulness constraints in response to the appearance of marked forms in observed overt structures. This proposal is similar in spirit to the Subset Principle <ref> (Angluin 1978, Berwick 1986, Pinker 1986, Wexler and Manzini 1987) </ref>. Because .CV. syllables are unmarked, i.e., they violate no structural constraints, all languages include them in their syllable structure inventory; other, marked, syllable structures may or may not appear in the inventory.
Reference: <author> Baum, L. E., and T. Petrie. </author> <year> 1966. </year> <title> Statistical inference for probabilistic functions of finite state Markov chains. </title> <journal> Annals of Mathematical Statistics 37, </journal> <volume> 15591563. </volume>
Reference: <author> Bahl, L. R., F. Jelinek and R. L. </author> <title> Mercer 1983, A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-5, </journal> <volume> 179190. </volume>
Reference: <author> Brown, P. F., J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. </author> <title> L. </title>
Reference: <author> Mercer, and P. S. Roossin. </author> <year> 1990. </year> <title> A statistical approach to machine translation. </title> <note> Computational Linguistics 16. </note>
Reference: <author> Berwick, Robert. </author> <year> 1986. </year> <title> The acquisition of syntactic knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Billings, Loren, and Catherine Rudin. </author> <year> 1994. </year> <title> Optimality and superiority: A new approach to overt multiple wh ordering. </title> <booktitle> In Proceedings of the Third Symposium on Formal Approaches to Slavic Linguistics. </booktitle>
Reference: <author> Clark, Eve. </author> <year> 1987. </year> <title> The principle of contrast: A constraint on language acquisition. </title> <editor> In Brian MacWhinney, ed., </editor> <booktitle> Mechanisms of language acquisition,. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: This is a consequence of a principle implicit in EDCD. This principle states that the learner should assume that the observed description is optimal for the corresponding input, and that it is the only optimal description. This principle resembles other proposed learning principles, such as Clarks Principle of Contrast <ref> (E. Clark 1987) </ref> and Wexlers Uniqueness Principle (Wexler 1981). EDCD makes vigorous use of this learning principle. In fact, it is possible for the algorithm to run endlessly when presented data from a non-totally-ranked stratified hierarchy.
Reference: <author> Clark, Robin. </author> <year> 1990. </year> <title> Papers on learnability and natural selection. </title> <journal> Technical reports in formal and computational linguistics, </journal> <volume> No. </volume> <pages> 1, </pages> <institution> Universit de Genve. </institution>
Reference-contexts: It is significant that a trigger provides information about the value of a single parameter, rather than relationships between the values of several parameters. This property 9 is further reinforced by a proposed constraint on learning, the Single Value Constraint <ref> (R. Clark 1990, Gibson and Wexler 1994) </ref>: successive hypotheses considered by a learner may differ by the value of at most one parameter.
Reference: <author> Clements, G. N and S.J. Keyser. </author> <title> CV phonology. </title> <address> Cambridge, MA: MIT Press. </address> <note> 58Tesar & Smolensky Learnability in Optimality Theory Dempster, </note> <author> A. P., N. M. Laird, and D. B. Rubin.1977. </author> <title> Maximum likelihood estimation from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B, 39:138. </journal>
Reference: <author> Demuth, Katherine. </author> <year> 1995. </year> <title> Markedness and the development of prosodic structure. </title> <editor> In Jill Beckman, ed., </editor> <volume> NELS 25, </volume> <pages> 1325. </pages> <address> Amherst, MA: GLSA, </address> <institution> University of Massachusetts. </institution>
Reference-contexts: But if no positive evidence is provided for admitting marked syllables into the inventory, the initial, smallest, inventory will remain. One notable advantage of the latter proposal is that it accords well with recent work in child phonological aquisition <ref> (Demuth 1995, Gnanadesikan 1995, Levelt 1995) </ref>. This work has argued that a range of empirical generalizations concerning phonological acquisition can be modelled by constraint reranking.
Reference: <author> Dresher, B. Elan, and Jonathan Kaye. </author> <year> 1990. </year> <title> A computational learning model for metrical phonology. </title> <journal> Cognition 34, </journal> <pages> 137-195. </pages>
Reference: <author> Gibson, Edward, and Ken Wexler. </author> <year> 1994. </year> <title> Triggers. Linguistic Inquiry, </title> <type> 25(4). </type> <institution> Gnanadesikan, Amalia. </institution> <year> 1995. </year> <title> Markedness and faithfulness constraints in child phonology. </title> <address> Ms., </address> <month> October </month> <year> 1995. </year>
Reference: <author> Grimshaw, Jane. </author> <year> 1993. </year> <title> Minimal projection, heads, and inversion. </title> <institution> Ms. Rutgers University, </institution> <address> New Brunswick, NJ. </address>
Reference-contexts: For interesting aspects of syntax, this is pretty much all that need be said. In OT analyses of grammatical voice systems (Legendre, Raymond and Smolensky 1993), inversion <ref> (Grimshaw 1993, to appear) </ref>, wh-questions (Billings and Rudin 1994; Legendre et al. 1995, Ackema and Neeleman, in press; Legendre, Smolensky and Wilson, in press), and null subjects (Grimshaw and Samek-Lodovici 1995, Samek-Lodovici 1995, Grimshaw and Samek-Lodovici 1996), the set of underlying forms is universal, and all cross-linguistic variation arises from the
Reference: <author> Grimshaw, Jane. </author> <title> To appear. Projection, heads, and optimality. Linguistic Inquiry. </title>
Reference: <author> Grimshaw, Jane, and Vieri Samek-Lodovici. </author> <year> 1995. </year> <title> Optimal subjects. In Papers in Optimality Theory, </title> <editor> eds. J. Beckman, L. W. Dickey, and S. </editor> <address> Urbanczyk. </address> <institution> University of Massachusetts occasional papers 18. Amherst, MA: GLSA, University of Massachusetts. </institution>
Reference-contexts: In OT analyses of grammatical voice systems (Legendre, Raymond and Smolensky 1993), inversion (Grimshaw 1993, to appear), wh-questions (Billings and Rudin 1994; Legendre et al. 1995, Ackema and Neeleman, in press; Legendre, Smolensky and Wilson, in press), and null subjects <ref> (Grimshaw and Samek-Lodovici 1995, Samek-Lodovici 1995, Grimshaw and Samek-Lodovici 1996) </ref>, the set of underlying forms is universal, and all cross-linguistic variation arises from the grammar: the constraint ranking is all that need be learned.
Reference: <author> Grimshaw, Jane, and Vieri Samek-Lodovici. </author> <title> In press. Optimal subjects and subject universals. In Proceedings of the Workshop on Optimality in SyntaxIs the best good enough? Cambridge, </title> <address> Mass: </address> <publisher> MIT Press and MIT Working Papers in Linguistics. </publisher>
Reference: <author> Hamburger, Henry, and Ken Wexler. </author> <year> 1973. </year> <title> Identifiability of a class of transformational grammars. </title> <editor> In K. J. J. Hintikka, J. M. E. Moravcsik, and P. Suppes, eds., </editor> <title> Approaches to natural language. Dordrecht: Reidel. 59Tesar & Smolensky Learnability in Optimality Theory Haussler, </title> <editor> David. </editor> <year> 1996. </year> <title> Probably Approximately Correct learning and decision-theoretic generalizations. </title> <editor> In Paul Smolensky, Michael C. Mozer, and David E. Rumelhart, eds., </editor> <booktitle> Mathematical perspectives on neural networks. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Publishers. </publisher>
Reference-contexts: The inputs in these syntactic analyses are all some kind of predicate/argument structure, the kind of semantic structure that has often been taken as available to the syntactic learner independently of the overt data <ref> (e.g., Hamburger and Wexler 1973) </ref>. In phonology, however, there is nearly always an additional layer to the question of the underlying forms.
Reference: <author> Hinton, Geoffrey. </author> <year> 1989. </year> <title> Connectionist learning procedures. </title> <journal> Artificial Intelligence 40:185234. </journal>
Reference-contexts: This feature of the language learning problem is challenging, but not at all special to language, as it turns out. Even in such mundane contexts as a computer learning to recognize handwritten digits, this same problem arises. This problem has been extensively studied in the learning theory literature <ref> (often under the name unsupervised learning, e.g., Hinton 1989) </ref>.
Reference: <author> Jakobson, Roman. </author> <year> 1962. </year> <title> Selected writings 1: Phonological studies. The Hague: </title> <publisher> Mouton. </publisher>
Reference-contexts: Universal Grammar may impose restrictions on the possible rankings of Con. Analysis of all rankings of the CVT constraints reveals a typology of basic CV syllable structures that explains Jakobsons typological generalizations <ref> (Jakobson 1962, Clements and Keyser 1983) </ref>: see P&S:6. In this typology, licit syllables may have required or optional onsets, and, independently, forbidden or optional codas. One further principle of OT will figure in our analysis of learnability, richness of the 10Tesar & Smolensky Learnability in Optimality Theory base.
Reference: <author> Kearns, Michael, and Umesh Vazirani. </author> <year> 1994. </year> <title> An introduction to computational learning theory. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Legendre, Graldine, William Raymond, and Paul Smolensky. </author> <year> 1993. </year> <title> Analytic typology of case marking and grammatical voice. </title> <booktitle> Proceedings of the Berkeley Linguistics Society, </booktitle> <pages> 19. </pages>
Reference-contexts: For interesting aspects of syntax, this is pretty much all that need be said. In OT analyses of grammatical voice systems <ref> (Legendre, Raymond and Smolensky 1993) </ref>, inversion (Grimshaw 1993, to appear), wh-questions (Billings and Rudin 1994; Legendre et al. 1995, Ackema and Neeleman, in press; Legendre, Smolensky and Wilson, in press), and null subjects (Grimshaw and Samek-Lodovici 1995, Samek-Lodovici 1995, Grimshaw and Samek-Lodovici 1996), the set of underlying forms is universal, and
Reference: <author> Legendre,Graldine, Paul Smolensky, and Colin Wilson. </author> <title> In press. When is less more? Faithfulness and minimal links in wh-chains. In Proceedings of the Workshop on Optimality in SyntaxIs the best good enough? Cambridge, </title> <address> Mass: </address> <publisher> MIT Press and MIT Working Papers in Linguistics. </publisher>
Reference: <author> Legendre, Graldine, Colin Wilson, Paul Smolensky, Kristin Homer, and William Raymond. </author> <year> 1995. </year> <note> Optimality and wh-extraction. In Papers in Optimality Theory, eds. J. </note>
Reference: <author> Beckman, L. W. Dickey, and S. </author> <month> Urbanczyk. </month> <institution> University of Massachusetts Occasional Papers 18. Amherst, MA: GLSA, University of Massachusetts. Levelt, </institution> <address> Clara. </address> <year> 1995. </year> <title> Unfaithful kids: Place of Articulation patterns in early child language. </title> <institution> Paper presented at the Department of Cognitive Science, Johns Hopkins University, Baltimore, Md., </institution> <month> September, </month> <year> 1995. </year> <title> 60Tesar & Smolensky Learnability in Optimality Theory McCarthy, </title> <editor> John, and Alan Prince. </editor> <year> 1995. </year> <note> Faithfulness and reduplicative identity. In Papers in Optimality Theory, </note> <editor> eds. J. Beckman, L. W. Dickey, and S. </editor> <address> Urbanczyk. </address> <institution> University of Massachusetts Occasional Papers 18. Amherst, MA: GLSA, University of Massachusetts. </institution>
Reference: <author> Ndas, Arthur, and Robert. L. Mercer. </author> <year> 1996. </year> <title> Hidden Markov Models and some connections with artificial neural nets. </title> <editor> In Paul Smolensky, Michael C. Mozer, </editor> <publisher> and David E. </publisher>
Reference: <editor> Rumelhart, eds., </editor> <booktitle> Mathematical perspectives on neural networks. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Publishers. </publisher>
Reference: <author> Niyogi, Partha, and Robert C. Berwick. </author> <year> 1993. </year> <title> Formalizing triggers: A learning model for finite spaces. </title> <journal> A.I. </journal> <volume> Memo No. 1449. </volume> <booktitle> Artificial Intelligence Laboratory, </booktitle> <publisher> MIT. </publisher>
Reference: <author> Pinker, Steven. </author> <year> 1986. </year> <title> Productivity and conservatism in language acquisition. In Language learning and concept acquisition, </title> <editor> ed. W. Demopoulos and A. Marras, </editor> <publisher> Ablex, </publisher> <address> Norwood, NJ. </address>
Reference: <author> Pitt, Leonard, and Leslie Valiant 1988. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the ACM 35, </journal> <pages> 965-984. </pages>
Reference: <author> Prince, Alan. </author> <year> 1993. </year> <title> Internet communication, </title> <month> September 26. </month>
Reference: <author> Prince, Alan and Paul Smolensky. </author> <year> 1991. </year> <title> Notes on connectionism and Harmony Theory in linguistics. </title> <type> Technical Report CU-CS-533-91. </type> <institution> Department of Computer Science, University of Colorado, Boulder. </institution>
Reference: <author> Prince, Alan, and Paul Smolensky. </author> <year> 1993. </year> <title> Optimality Theory: Constraint interaction in generative grammar. </title> <institution> Ms. Rutgers University, New Brunswick, NJ, and University of Colorado, Boulder. </institution> <note> To appear as Linguistic Inquiry Monograph, </note> <institution> MIT Press, </institution> <address> Cambridge, MA. </address> <note> 61Tesar & Smolensky Learnability in Optimality Theory Pulleyblank, </note> <author> Douglas, and William J. Turkel. </author> <title> In press. The logical problem of language acquisition in Optimality Theory. In Proceedings of the Workshop on Optimality in SyntaxIs the best good enough? Cambridge, </title> <address> Mass: </address> <publisher> MIT Press and MIT Working Papers in Linguistics. </publisher>
Reference: <author> Pulleyblank, Douglas, and William J. Turkel. </author> <year> 1995. </year> <title> Traps in constraint ranking space. </title> <note> Paper presented at Maryland Mayfest 95: Formal Approaches to Learnability. </note>
Reference: <author> Safir, Ken. </author> <year> 1987. </year> <title> Comments on Wexler and Manzini. </title> <editor> In Thomas Roeper and Edwin Williams, eds., </editor> <title> Parameter setting, 7789. </title> <publisher> Dordrecht: Reidel. </publisher> <address> Samek-Lodovici, Vieri. </address> <year> 1995. </year> <title> Topic and focus effects on clause structure: An Optimality Theoretic analysis. </title> <type> Doctoral Dissertation, </type> <institution> Department of Linguistics, Rutgers University. </institution>
Reference-contexts: Unfortunately, this results in a conflict between the goals of learnability, which favor independent parameters with restricted effects, and the goals of linguistic theory, which favor parameters with wideranging effects and greater explanatory power <ref> (see Safir 1987 for a discussion of this conflict) </ref>. Optimality Theory may provide the opportunity for this conflict to be avoided. In Optimality Theory, interaction between constraints is not only possible but explanatorily crucial.
Reference: <author> Smolensky, Paul. </author> <year> 1996a. </year> <title> Statistical perspectives on neural networks. </title> <editor> In Paul Smolensky, Michael C. Mozer, and David E. Rumelhart, eds., </editor> <booktitle> Mathematical perspectives on neural networks. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Publishers. </publisher>
Reference: <author> Smolensky, Paul. </author> <year> 1996b. </year> <title> On the comprehension/production dilemma in child language. Linguistic Inquiry 27. </title>
Reference: <author> Smolensky, Paul. </author> <year> 1996c. </year> <title> The initial state and richness of the base in Optimality Theory. </title> <type> Technical Report, </type> <institution> Department of Cognitive Science, Johns Hopkins University. </institution>
Reference: <author> Tesar, Bruce. </author> <year> 1994. </year> <title> Parsing in Optimality Theory: A dynamic programming approach. </title> <type> Technical Report CU-CS-714-94, </type> <month> April </month> <year> 1994. </year> <institution> Department of Computer Science, University of Colorado, Boulder. </institution>
Reference-contexts: The process of computing optimal structural descriptions for underlying forms (production-directed parsing) has already been addressed elsewhere. Algorithms which are provably correct for significant classes of OT grammars have been developed, based upon dynamic programming <ref> (Tesar 1994, 1995ab, in press) </ref>. For positive initial results in applying similar techniques to robust interpretive parsing, see Tesar, in preparation a.
Reference: <author> Tesar, Bruce. </author> <year> 1995a. </year> <title> Computing optimal forms in Optimality Theory: Basic syllabification. </title> <type> Technical Report CU-CS-763-95, </type> <month> February </month> <year> 1995. </year> <institution> Department of Computer Science, University of Colorado, Boulder. </institution> <note> 62Tesar & Smolensky Learnability in Optimality Theory Tesar, </note> <author> Bruce. </author> <year> 1995b. </year> <title> Computational Optimality Theory. </title> <type> Doctoral Dissertation, </type> <institution> Department of Computer Science, University of Colorado at Boulder. </institution>
Reference: <author> Tesar, Bruce. </author> <title> In press. Error-driven learning in Optimality Theory via the efficient computation of optimal forms. In Proceedings of the Workshop on Optimality in SyntaxIs the best good enough? Cambridge, </title> <address> Mass: </address> <publisher> MIT Press and MIT Working Papers in Linguistics. </publisher>
Reference: <author> Tesar, Bruce. </author> <title> In preparation a. Robust interpretive parsing with alignment constraints. </title>
Reference: <editor> Tesar, Bruce. In preparation b. </editor> <title> An iterative strategy for language learning. </title>
Reference: <author> Tesar, Bruce, and Paul Smolensky. </author> <year> 1993. </year> <title> The learnability of Optimality Theory: An algorithm and some basic complexity results. </title> <type> Technical Report CU-CS-678-93, </type> <month> Oct. </month> <year> 1993. </year> <institution> Department of Computer Science, University of Colorado at Boulder. </institution>
Reference: <author> Tesar, Bruce, and Paul Smolensky. </author> <year> 1995. </year> <title> The learnability of Optimality Theory. </title> <booktitle> Proceedings of the Thirteenth West Coast Conference on Formal Linguistics, </booktitle> <pages> 122137. </pages>
Reference: <author> Wexler, Kenneth. </author> <year> 1981. </year> <title> Some issues in the theory of learnability. </title> <note> In C. Baker and J. </note>
Reference-contexts: This principle states that the learner should assume that the observed description is optimal for the corresponding input, and that it is the only optimal description. This principle resembles other proposed learning principles, such as Clarks Principle of Contrast (E. Clark 1987) and Wexlers Uniqueness Principle <ref> (Wexler 1981) </ref>. EDCD makes vigorous use of this learning principle. In fact, it is possible for the algorithm to run endlessly when presented data from a non-totally-ranked stratified hierarchy.
Reference: <editor> McCarthy, eds., </editor> <title> The logical problem of language acquisition, </title> <type> 3052. </type> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Wexler, Kenneth, and Peter Culicover. </author> <year> 1980. </year> <title> Formal principles of language acquisition. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: In fact, CD need be executed only when there is a mismatch between the correct parse and the optimal parse assigned by the current ranking. This is an error-driven learning algorithm <ref> (Wexler and Culicover 1980) </ref>. Each observed parse is compared with a computed parse of 27Tesar & Smolensky Learnability in Optimality Theory the input. If the two parses match, no error occurs, and so no learning takes place.
Reference: <author> Wexler, Kenneth, and M. Rita Manzini. </author> <year> 1987. </year> <title> Parameters and learnability in binding theory. </title>
Reference-contexts: In fact, this property of independence has been proposed as a principle for grammars <ref> (Wexler and Manzini 1987) </ref>. Unfortunately, this results in a conflict between the goals of learnability, which favor independent parameters with restricted effects, and the goals of linguistic theory, which favor parameters with wideranging effects and greater explanatory power (see Safir 1987 for a discussion of this conflict).
Reference: <editor> In Thomas Roeper and Edwin Williams, eds., </editor> <title> Parameter setting. </title> <publisher> Dordrecht: Reidel. </publisher> <address> Revised Oct. 20, </address> <year> 1996 </year>
References-found: 51

