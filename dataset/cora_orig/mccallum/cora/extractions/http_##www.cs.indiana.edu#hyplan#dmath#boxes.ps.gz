URL: http://www.cs.indiana.edu/hyplan/dmath/boxes.ps.gz
Refering-URL: http://www.cs.indiana.edu/hyplan/dmath.html
Root-URL: http://www.cs.indiana.edu
Email: bshouty@cpsc.ucalgary.ca  goldbepw@sun.aston.ac.uk  sg@cs.wustl.edu  dmath@cs.indiana.edu  
Title: Exact Learning of Discretized Geometric Concepts  
Author: Nader H. Bshouty Paul W. Goldberg Sally A. Goldman H. David Mathias 
Date: December 13, 1996  
Address: Calgary, Alberta, Canada T2N 1N4  Birmingham B4 7ET, UK  St. Louis, MO 63130  Bloomington, IN 47405  
Affiliation: Department of Computer Science The University of Calgary  Dept. of Comp. Sci. and Applied Math. Aston University  Dept. of Computer Science Washington University  Computer Science Dept. Indiana University  
Abstract: We first present an algorithm that uses membership and equivalence queries to exactly identify a discretized geometric concept defined by the union of m axis-parallel boxes in d-dimensional discretized Euclidean space where each coordinate can have n discrete values. This algorithm receives at most md counterexamples and uses time and membership queries polynomial in m and log n for any constant d. Furthermore, all equivalence queries can be formulated as the union of O(md log m) axis-parallel boxes. Next, we show how to extend our algorithm to efficiently learn, from only equivalence queries, any discretized geometric concept generated from any number of halfspaces with any number of known (to the learner) slopes in a constant dimensional space. In particular, our algorithm exactly learns (from equivalence queries only) unions of discretized axis-parallel boxes in constant dimensional space in polynomial time. Furthermore, this equivalence query only algorithm can be modified to handle a polynomial number of lies in the counterexamples provided by the environment. Finally, we introduce a new complexity measure that better captures the complexity of the union of m boxes than simply the number of boxes and the dimension. Our new measure, oe, is the number of segments in the target, where a segment is a maximum portion of one of the sides of the target that lies entirely inside or entirely outside each of the other halfspaces defining the target. We present a modification of our first algorithm that uses time and queries polynomial in oe and log n. In fact, the time and queries (both membership and equivalence) used by this single algorithm are polynomial for either m or d constant. fl Portions of this paper appear in preliminary form in [18] and [8]. y This research was supported in part by the NSERC of Canada. z This research was performed while visiting Washington University with support from NSF NYI Grant CCR-9357707, and while at Sandia National Labs with support from the U.S. Department of Energy under contract DE-AC04-76AL85000. x Supported in part by NSF Grant CCR-9110108 and an NSF NYI Grant CCR-9357707 with matching funds provided by Xerox PARC and WUTA. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research [6, 14, 16, 27, 29, 30, 31, 33]. We study the problem of learning geometric concepts under the model of learning with queries <ref> [1] </ref> in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. To apply such a learning model to a geometric domain, it is necessary to look at a discretized (or digitalized) version of the domain. <p> This paper subsumes the results presented by Goldberg et al. [21] and includes several results given by Bshouty et al. [11]. 2 Learning Model The learning model we use in this paper is that of learning with queries developed by An-gluin <ref> [1] </ref>. When applied to our class of discretized geometric concepts, the learner's goal is to learn exactly how an unknown target concept, g, drawn from the concept class G 2 N d n , classifies as positive or negative all instances from the instance space N d n . <p> The learner is permitted time polynomial in 1=*, 1=ffi, the size of an example, and the size of the target concept to formulate a hypothesis. The relationship between the PAC model and the query model is well understood. Angluin <ref> [1] </ref> showed that any class that is learnable using only equivalence queries is also PAC learnable. The relationship is unchanged by the addition of membership queries to each model. Blum [8] showed that PAC learnability does not imply query learnability.
Reference: [2] <author> Dana Angluin and Martin Krikis. </author> <title> Learning with malicious membership queries and exceptions. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 57-66, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: In the query learning model, Angluin and Krikis <ref> [2] </ref> examine the case in which membership queries can be answered incorrectly under adversarial control. There has also been some work on learning discretized geometric concepts defined by non axis-parallel hyperplanes. Maass and Turan [32] study the problem of learning a single discretized halfspace using only equivalence queries.
Reference: [3] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: Observe that the class considered by Frazier et al. is a generalization of the class of DNF formulas in which all variables only appear negated. While there has been some work addressing the general issue of mislabeled training examples in the PAC model <ref> [3, 26, 36, 25] </ref>, there has been little research on learning geometric concepts with noise. Auer [6] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [4] <author> Esther Arkin, Patrice Belleville, Joseph Mitchell, David Mount, Kathleen Romanik, Steven Salzberg, and Diana Souvaine. </author> <title> Testing simple polygons. </title> <booktitle> In Proceedings of the 5th Canadian Conference on Computational Geometry, </booktitle> <pages> pages 387-392, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Thus such work can be thought of as learning a convex polygon from only membership queries along with a single positive example. However, when working with non-convex objects, the probe used in such work is more powerful than a membership query. Geometric testing (for example see <ref> [34, 4] </ref>) is a subarea of geometric probing involved with 8 solving verification problems.
Reference: [5] <author> Esther Arkin, Hank Meijer, Joseph Mitchell, David Rappaport and Steven Skiena. </author> <booktitle> Decision Trees for Geometric Models In Proceedings of the 9th Annual Symposium on Computational Geometry, </booktitle> <pages> pages 369-378, </pages> <month> May </month> <year> 1993. </year>
Reference: [6] <author> Peter Auer. </author> <title> On-line learning of rectangles in noisy environments. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 253-261, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [6, 14, 16, 27, 29, 30, 31, 33] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> They showed that if the learner is restricted to make only equivalence queries in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification [27, 30]. Auer <ref> [6] </ref> improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. <p> While there has been some work addressing the general issue of mislabeled training examples in the PAC model [3, 26, 36, 25], there has been little research on learning geometric concepts with noise. Auer <ref> [6] </ref> investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [7] <author> P. Belleville and T.C. Shermer. </author> <title> Probing polygons mimimally is hard. Computational Geometry: </title> <journal> Theory and Applications, </journal> <volume> Vol. 2, </volume> <pages> pages 255-265, </pages> <year> 1993. </year>
Reference: [8] <author> Avrim Blum. </author> <title> Separating distribution-free and mistake-bounded learning models over the Boolean domain. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 211-218, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The relationship between the PAC model and the query model is well understood. Angluin [1] showed that any class that is learnable using only equivalence queries is also PAC learnable. The relationship is unchanged by the addition of membership queries to each model. Blum <ref> [8] </ref> showed that PAC learnability does not imply query learnability. <p> One must select hypotheses for the equivalence queries so that sufficient progress is made with each counterexample. This requirement of selecting a "smart" hypothesis makes the problem of obtaining an efficient algorithm to learn exactly the class S n significantly harder than obtaining the corresponding PAC result. Also Blum <ref> [8] </ref> has shown that if one-way functions exist then there exist functions that are PAC-learnable but not exactly learnable.
Reference: [9] <author> Avrim Blum and Steven Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-389, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: it would be interesting to see if S n can be efficiently learned in time polynomial in m and log n for d = O (log m) or in time polynomial in d and log n for m = O (log d) (i.e. a generalization of the Blum and Rudich <ref> [9] </ref> result that O (log n)-term DNF formulas are exactly learnable). Of course, since S n generalizes the class of DNF formulas, it seems very unlikely that one could develop an algorithm for the unrestricted case of m box d n that is polynomial in m, log n, and d.
Reference: [10] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Closely related to the problem of learning the union of discretized boxes, is the problem of learning the union of non-discretized boxes in the PAC model [37]. Blumer et al. <ref> [10] </ref> present an algorithm to PAC-learn an m-fold union of boxes in E d by drawing a sufficiently large sample of size m 0 = poly i * ; lg 1 j , and then performing a greedy covering over the at most i 2d boxes defined by the sample. <p> More generally, the results from Blumer et al. <ref> [10] </ref> show that under the PAC model any concise hypothesis that is consistent with the data is satisfactory. In other words, the PAC model provides no suitable basis for distinction among different consistent hypotheses.
Reference: [11] <author> Nader Bshouty, </author> <year> 1994. </year> <type> Personal communication. </type>
Reference-contexts: Finally, in Section 10 we conclude with some open problems. This paper subsumes the results presented by Goldberg et al. [21] and includes several results given by Bshouty et al. <ref> [11] </ref>. 2 Learning Model The learning model we use in this paper is that of learning with queries developed by An-gluin [1]. <p> This algorithm appears in <ref> [11] </ref> along with the equivalence-query algorithms presented here in Sections 6 and 7. While the Chen and Homer result is very similar to our result of Section 6, they use a very different technique to obtain the result.
Reference: [12] <author> Nader H. Bshouty, Sally A. Goldman, Thomas R. Hancock, and Sleiman Matar. </author> <title> Asking questions to minimize errors. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 41-50, </pages> <month> July </month> <year> 1993. </year> <month> 32 </month>
Reference-contexts: We present this algorithm, in part, because it introduces the approach used to obtain our other results and also because it uses very few equivalence queries, which is of interest if one's goal is to minimize the number of prediction errors made by the learner <ref> [12] </ref>. Next, in Section 6 we describe a modification of this algorithm that efficiently learns the union of boxes in constant dimensional space with only equivalence queries. <p> serves two purposes: (1) the other algorithms presented build upon this basic algorithm and thus for ease of exposition we present it here, and (2) it uses very few equivalence queries, which is of interest if one's goal is to minimize the number of prediction errors made by the learner <ref> [12] </ref>.
Reference: [13] <author> W. J. Bultman and W. Maass. </author> <title> Fast identification of geometric objects with membership queries. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 337-353, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There has also been work on learning non axis-parallel discretized rectangles with only equivalence queries. Maass and Turan [31] show an (n) information theoretic lower bound on the number of equivalence queries when the hypotheses must be drawn from the concept class. Contrasting this lower bound, Bultman and Maass <ref> [13] </ref> give an efficient algorithm that uses membership and equivalence queries to learn this class using O (log n) equivalence queries. Their algorithm returns a hypothesis consisting of a description of the vertices and edges of the polygon.
Reference: [14] <author> Zhixiang Chen. </author> <title> Learning unions of two rectangles in the plane with equivalence queries. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 243-252. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [6, 14, 16, 27, 29, 30, 31, 33] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> More recently, Chen <ref> [14] </ref> gave an algorithm that used equivalence queries to learn general unions of two boxes in the (discretized) plane. The algorithm uses O (log 2 n) equivalence queries, and involves a detailed case analysis of the shapes formed by the two rectangles.
Reference: [15] <author> Zhixiang Chen and Steven Homer. </author> <title> The bounded injury priority method and the learn-ability of unions of rectangles. </title> <type> Unpublished manuscript, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: The hypothesis class of their algorithm is the union of 8m 2 2 rectangles. In work independent of ours, Chen and Homer <ref> [15] </ref> have improved upon their earlier result by giving an algorithm that learns any concept from S n using O (m 2 (d+1) d 2 log 2d+1 n) equivalence queries by efficiently applying the bounded injury method from recursive function theory.
Reference: [16] <author> Zhixiang Chen and Wolfgang Maass. </author> <title> On-line learning of rectangles. </title> <journal> Machine Learning, </journal> <volume> 17 </volume> <pages> 23-50, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [6, 14, 16, 27, 29, 30, 31, 33] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> An algorithm making O (2 d log n) equivalence queries was given by Maass and Turan [29]. The best result known for learning the class box d n was provided by Chen and Maass <ref> [16] </ref>. They gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [17] <author> V. Chvatal. </author> <title> A greedy heuristic for the set covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3) </volume> <pages> 233-235, </pages> <year> 1979. </year>
Reference-contexts: Thus our goal is to find the union of as few boxes as possible that "cover" all of the positive regions. We now describe how to formulate this problem as a set covering problem for which we can then use the standard greedy set covering heuristic <ref> [17] </ref> to perform the conversion. The set X of objects to cover will simply contain all positive regions in h. Thus jXj (4m + 1) d . Then the set F of subsets of X will be made as follows.
Reference: [18] <author> Joseph C. Culberson and Robert A. Reckhow. </author> <title> Covering polygons is hard. </title> <booktitle> In 29th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 601-611. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1988. </year>
Reference-contexts: Note that it is NP-hard to find a minimum covering of a concept from S n by individual boxes <ref> [18] </ref>. Recall that a consistent hypothesis h essentially encodes the set of positive regions. Thus our goal is to find the union of as few boxes as possible that "cover" all of the positive regions.
Reference: [19] <author> Herbert Edelsbrunner. </author> <title> Algorithms in Combinatorial Geometry. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: We use the following lemma to bound the maximum number of regions that will be contained in G H . Lemma 2 Any t, d-dimensional hyperplanes in a (d + 1)-dimensional space divide the space into at most t d+1 + 1 regions. This result is well-known. See Edelsbrunner <ref> [19] </ref> for a proof. We are now ready to analyze our algorithm Learn-General-Slopes. Theorem 3 Let S be a set of slopes.
Reference: [20] <author> Mike Frazier, Sally Goldman, Nina Mishra, and Leonard Pitt. </author> <title> Learning from a consistently ignorant teacher. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Also Blum [8] has shown that if one-way functions exist then there exist functions that are PAC-learnable but not exactly learnable. Finally, under a variation of the PAC model in which membership queries can be made, Frazier et al. <ref> [20] </ref> have given an algorithm to PAC-learn the m-fold union of boxes in E d for which each box is entirely contained within the positive quadrant and contains the origin. 7 Furthermore, their algorithm learns this subclass of general unions of boxes in time poly-nomial in both m and d.
Reference: [21] <author> Paul W. Goldberg, Sally A. Goldman, and H. David Mathias. </author> <title> Learning unions of boxes with membership and equivalence queries. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: In Section 9 we present our new complexity measure and describe a modification of our first algorithm that runs in polynomial time with respect to this complexity measure. Finally, in Section 10 we conclude with some open problems. This paper subsumes the results presented by Goldberg et al. <ref> [21] </ref> and includes several results given by Bshouty et al. [11]. 2 Learning Model The learning model we use in this paper is that of learning with queries developed by An-gluin [1].
Reference: [22] <author> Yan-Bin Jia and Michael Erdmann. </author> <booktitle> The Complexity of Sensing by Point Sampling In Proceedings of the First Workshop of the Algorithmic Foundations of Robotics, </booktitle> <year> 1994. </year>
Reference: [23] <author> Yan-Bin Jia and Michael Erdmann. </author> <title> Geometric Sensing of Known Planar Shapes. </title> <journal> International Journal of Robotics Research, </journal> <note> to appear, </note> <year> 1996. </year>
Reference: [24] <author> Steven Homer and Zhixiang Chen. </author> <title> Fast learning unions of rectangles with queries. </title> <type> Unpublished manuscript, </type> <month> July </month> <year> 1993. </year>
Reference-contexts: More recently, Chen [14] gave an algorithm that used equivalence queries to learn general unions of two boxes in the (discretized) plane. The algorithm uses O (log 2 n) equivalence queries, and involves a detailed case analysis of the shapes formed by the two rectangles. Chen and Homer <ref> [24] </ref> presented an algorithm to learn the union of m rectangles in the plane using O (m 3 log n) queries (both membership and equivalence) and O (m 5 log n) time. The hypothesis class of their algorithm is the union of 8m 2 2 rectangles.
Reference: [25] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <month> May </month> <year> 1988. </year> <month> 33 </month>
Reference-contexts: Observe that the class considered by Frazier et al. is a generalization of the class of DNF formulas in which all variables only appear negated. While there has been some work addressing the general issue of mislabeled training examples in the PAC model <ref> [3, 26, 36, 25] </ref>, there has been little research on learning geometric concepts with noise. Auer [6] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [26] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: Observe that the class considered by Frazier et al. is a generalization of the class of DNF formulas in which all variables only appear negated. While there has been some work addressing the general issue of mislabeled training examples in the PAC model <ref> [3, 26, 36, 25] </ref>, there has been little research on learning geometric concepts with noise. Auer [6] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [27] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [6, 14, 16, 27, 29, 30, 31, 33] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> One of the geometric concepts that they studied was the class box d n . They showed that if the learner is restricted to make only equivalence queries in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification <ref> [27, 30] </ref>. Auer [6] improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries.
Reference: [28] <author> Philip M. Long and Manfred K. Warmuth. </author> <title> Composite geometric concepts and polynomial predictability. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 273-287. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Thus for d constant this algorithm runs in polynomial time. Long and Warmuth <ref> [28] </ref> present an algorithm to PAC-learn this same class by again drawing a sufficiently large sample and constructing a hypothesis that consists of at most m (2d) m boxes consistent with the sample.
Reference: [29] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexamples. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 262-267, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [6, 14, 16, 27, 29, 30, 31, 33] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. An algorithm making O (2 d log n) equivalence queries was given by Maass and Turan <ref> [29] </ref>. The best result known for learning the class box d n was provided by Chen and Maass [16]. They gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [30] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [6, 14, 16, 27, 29, 30, 31, 33] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> studied here has also been considered in the PAC model, as summarized in the next section. 3 By size we mean the number of bits to encode the example. 5 3 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [30, 31] </ref>. One of the geometric concepts that they studied was the class box d n . <p> One of the geometric concepts that they studied was the class box d n . They showed that if the learner is restricted to make only equivalence queries in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification <ref> [27, 30] </ref>. Auer [6] improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. <p> So for m constant this yields an efficient PAC algorithm. We note that either of these PAC algorithms can be applied to the class S n giving efficient PAC algorithms for this class for either d constant or m constant. As discussed by Maass and Turan <ref> [30] </ref>, the task of a concept learning algorithm is to provide a "smart" hypothesis based on the data available. In other words, the hypothesis must be carefully chosen so that as much information as possible is obtained from each counterexample.
Reference: [31] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Algorithms and lower bounds for on-line learning of geometrical concepts. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 251-269, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [6, 14, 16, 27, 29, 30, 31, 33] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> studied here has also been considered in the PAC model, as summarized in the next section. 3 By size we mean the number of bits to encode the example. 5 3 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [30, 31] </ref>. One of the geometric concepts that they studied was the class box d n . <p> There has also been work on learning non axis-parallel discretized rectangles with only equivalence queries. Maass and Turan <ref> [31] </ref> show an (n) information theoretic lower bound on the number of equivalence queries when the hypotheses must be drawn from the concept class.
Reference: [32] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> How fast can a threshold gate learn? In G. </title> <editor> Drastal, S.J. Hanson, and R. Rivest, editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems: Constraints and Prospects, </booktitle> <pages> pages 318-414. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: In the query learning model, Angluin and Krikis [2] examine the case in which membership queries can be answered incorrectly under adversarial control. There has also been some work on learning discretized geometric concepts defined by non axis-parallel hyperplanes. Maass and Turan <ref> [32] </ref> study the problem of learning a single discretized halfspace using only equivalence queries. They give an efficient algorithm using O (d 2 (log d+log n)) queries and give an information theoretic lower bound of d on the number of queries when all hypotheses are discretized halfspaces.
Reference: [33] <author> Wolfgang Maass and Manfred Warmuth. </author> <title> Efficient learning with virtual threshold gates. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning, </booktitle> <pages> pages 378-386. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [6, 14, 16, 27, 29, 30, 31, 33] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> While the Chen and Homer result is very similar to our result of Section 6, they use a very different technique to obtain the result. Also, our algorithm only uses O ((8d 2 m log n) d ) equivalence queries. Finally, in other independent work, Maass and Warmuth <ref> [33] </ref> have developed, as part of a more general result, an algorithm to learn any concept from S n using O (md log n) equivalence queries and O i j computation time.
Reference: [34] <author> Kathleen Romanik and Carl Smith. </author> <title> Testing geometric objects. </title> <type> Technical Report UMIACS-TR-90-69, </type> <institution> University of Maryland College Park, Department of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: Thus such work can be thought of as learning a convex polygon from only membership queries along with a single positive example. However, when working with non-convex objects, the probe used in such work is more powerful than a membership query. Geometric testing (for example see <ref> [34, 4] </ref>) is a subarea of geometric probing involved with 8 solving verification problems.
Reference: [35] <author> S. Skiena. </author> <title> Problems in geometric probing. </title> <journal> Algorithmica, </journal> <volume> 4 </volume> <pages> 599-605, </pages> <year> 1989. </year>
Reference-contexts: Their algorithm returns a hypothesis consisting of a description of the vertices and edges of the polygon. Computational geometry researchers have looked at the slightly related problem of geometric probing (for example see <ref> [35] </ref>). Geometric probing studies how to identify, verify or determine some property of an unknown geometrical object using a measuring device known as a probe.
Reference: [36] <author> Robert H. Sloan. </author> <title> Four types of noise in data for PAC learning. </title> <journal> Information Processing Letters, </journal> <volume> 54 </volume> <pages> 157-162, </pages> <year> 1992. </year>
Reference-contexts: Observe that the class considered by Frazier et al. is a generalization of the class of DNF formulas in which all variables only appear negated. While there has been some work addressing the general issue of mislabeled training examples in the PAC model <ref> [3, 26, 36, 25] </ref>, there has been little research on learning geometric concepts with noise. Auer [6] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [37] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year> <month> 34 </month>
Reference-contexts: We seek a more efficient approach than testing each counterexample we receive using a membership query, especially in view of the fact that we are interested in algorithms that may only use equivalence queries. Another important learning model is the PAC model introduced by Valiant <ref> [37] </ref>. In this model the learner is presented with labeled examples chosen at random according to an unknown, arbitrary distribution D over the instance space. <p> Closely related to the problem of learning the union of discretized boxes, is the problem of learning the union of non-discretized boxes in the PAC model <ref> [37] </ref>.
References-found: 37

