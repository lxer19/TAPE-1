URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-253.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: popat@media.mit.edu, picard@media.mit.edu  
Title: PROBABILITY MODEL APPLIED TO IMAGE RESTORATION AND COMPRESSION  
Author: Kris Popat and Rosalind W. Picard 
Address: Cambridge, MA 02139  
Affiliation: Massachusetts Institute of Technology,  
Note: CLUSTER-BASED  
Pubnum: Media Laboratory, E15-391  
Abstract: M.I.T. Media Laboratory Perceptual Computing Group, Technical Report #253 Also to appear in Proc. ICASSP, Adelaide, Australia, April 1994 ABSTRACT The performance of a statistical signal processing system is determined in large part by the accuracy of the probabilistic model it employs. Accurate modeling often requires working in several dimensions, but doing so can introduce dimensionality-related difficulties. A recently introduced model circumvents some of these difficulties while maintaining accuracy sufficient to account for much of the high-order, nonlinear statistical interdependence of samples. Properties of this model are reviewed, and its power demonstrated by application to image restoration and compression. Also described is a vector quantization (VQ) scheme which employs the model in entropy coding a Z N - lattice. The scheme has the advantage over standard VQ of bounding maximum instantaneous errors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard A. Tapia and James R. Thompson. </author> <title> Non-parametric Probability Density Estimation. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1978. </year>
Reference-contexts: The model should describe the joint behavior of pixels that are statistically related. Unfortunately, as the number of pixels (dimension) increases, two fundamental problems arise: the available training data becomes relatively sparse, and the effective alphabet becomes so large as to be unmanageable. Kernel estimation <ref> [1] </ref> alleviates the sparse data problem by making a smoothness assumption about the underlying probability law. Traditional kernel estimates contain the training data, which can make the estimates unwieldy. Also, with traditional kernel estimates, the problem of the large alphabet remains.
Reference: [2] <author> Kris Popat and R.W. </author> <title> Picard. A novel cluster-based probability model for texture synthesis, classification, and compression. </title> <booktitle> In Proc. SPIE Visual Communications '93, </booktitle> <address> Cambridge, Mass., </address> <year> 1993. </year>
Reference-contexts: A modeling technique for random vectors which alleviates these problems has recently been proposed, and its ap This work was supported in part by HP Labs and NEC Corp. plication to textured data considered <ref> [2] </ref>. The model combines kernel estimation with clustering, yielding a semipara-metric probability mass function (PMF) estimate which summarizes | rather than contains | the training data. Because the model is cluster based, it is inferable from a limited set of training data, despite the high dimensionality. <p> The resulting rates were between approximately 4.5 and 5.5 bits/pixel. This performance range is similar to that reported recently for other lossless compression approaches [9]. We believe that we can improve upon these results. For example, a hierarchical approach (similar to the one we used in texture synthesis <ref> [2] </ref>) is likely to result in better compression in the larger homogeneous regions of an image. A popular method of lossless grayscale image compression is to apply the Langdon-Rissanen binary scheme (or a variant) to bit-planes of the image, rather than to the original image [9].
Reference: [3] <author> M.J.D. Powell. </author> <title> Radial basis functions for multivariate interpolation: A review. </title> <editor> In J.C. Mason and M.G. Cox (eds.), editors, </editor> <title> Algorithms for Approximation. </title> <publisher> Claren-don Press, Oxford, </publisher> <address> U.K., </address> <year> 1987. </year>
Reference-contexts: In the restoration phase, the final component of the vector is unknown; its value is estimated by maximizing its PMF conditioned on the degraded pixels. and where C n is a normalizing constant chosen such that P M Regarding quality of the estimate, work on radial basis functions <ref> [3] </ref> suggests that q (x) can well approximate any p (x) that satisfies certain smoothness constraints.
Reference: [4] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Questions remain about convergence of q (x) to p (x) as the number of clusters increases; also, more research is required to determine precise approximation properties of the model with respect to criteria such as information divergence <ref> [4] </ref>. 3. APPLICATION TO MAXIMUM-LIKELIHOOD IMAGE RESTORATION Suppose that an image has been degraded and we wish to restore it. It is assumed that a large set of (original, degraded) image pairs is available for the purpose of training.
Reference: [5] <author> Y. Linde, A. Buzo, and R. M. Gray. </author> <title> An algorithm for vector quantizer design. </title> <journal> IEEE Trans. Comm., </journal> <volume> COM-28:84-95, </volume> <month> Jan. </month> <year> 1980. </year>
Reference-contexts: First, vectors are formed for each original pixel in the training set, as shown in Figure 1. Next, the necessary quantities in (1) are estimated by cluster analysis, as described in the previous section. For the examples in this paper, the clustering was carried out using the LBG algorithm <ref> [5] </ref>. For each pixel location in a given degraded image, let X 1 ; : : : ; X N1 denote the observed values of the neighborhood pixels, and let x N be the original (unknown) pixel value.
Reference: [6] <author> Allen Gersho. </author> <title> Optimal nonlinear interpolative vector quantization. </title> <journal> IEEE Trans. Comm., </journal> <volume> 38(9) </volume> <pages> 1285-1287, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: This method of restoration is similar in spirit to an interpolative vector quantization technique proposed by Ger-sho <ref> [6] </ref>. Both have the potential to "learn" nonlinear statistical relationships from training data and to use those relationships to fill in missing values. However, the techniques differ in one important respect.
Reference: [7] <author> Glen Langdon and Jorma Rissanen. </author> <title> Compression of black-white images with arithmetic coding. </title> <journal> IEEE Trans. Comm., </journal> <volume> COM-29:858-867, </volume> <month> June </month> <year> 1981. </year>
Reference-contexts: As a consequence, restored values are not limited to only those appearing explicitly in a codebook. The technique is more than a lookup table; it uses the available information to synthesize the missing value. 4. APPLICATION TO LOSSLESS COMPRESSION OF GRAYSCALE IMAGES Langdon and Rissanen <ref> [7] </ref> have described an efficient reversible compression scheme for binary images. In their system, each pixel is arithmetically encoded using a PMF that is conditioned on a nearby set of previously encoded pixels, i.e., on a neighborhood of pixels that precede it in 2 grayscale images. compression.
Reference: [8] <author> Kris Popat. </author> <title> Scalar quantization with arithmetic coding. </title> <type> Master's thesis, </type> <institution> Dept. of Elec. Eng. and Comp. Science, M.I.T., </institution> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: have the same conditioning information. (At the top and left boundaries, unavailable conditioning pixels are arbitrarily set to 128; the resulting local inefficiency has little effect on the overall bit rate.) The compression system was applied to several 8-bit monochrome images of natural scenes, using a 16-bit K-ary arithmetic coder <ref> [8] </ref> and the 10-pixel conditioning neighborhood shown in Figure 4. In each case, the cluster-based model was trained on a set of 20 images that did not include the test image. The resulting rates were between approximately 4.5 and 5.5 bits/pixel.
Reference: [9] <author> Majid Rabbani and Paul W. Jones. </author> <title> Digital image compression techniques. </title> <publisher> SPIE Optical Engineering Press, Bellingham, </publisher> <address> Washington, </address> <year> 1991. </year>
Reference-contexts: In each case, the cluster-based model was trained on a set of 20 images that did not include the test image. The resulting rates were between approximately 4.5 and 5.5 bits/pixel. This performance range is similar to that reported recently for other lossless compression approaches <ref> [9] </ref>. We believe that we can improve upon these results. For example, a hierarchical approach (similar to the one we used in texture synthesis [2]) is likely to result in better compression in the larger homogeneous regions of an image. <p> A popular method of lossless grayscale image compression is to apply the Langdon-Rissanen binary scheme (or a variant) to bit-planes of the image, rather than to the original image <ref> [9] </ref>. Although a K-ary source can always be reversibly decomposed into dlog 2 Ke binary sources, there is an important difference between direct and bit-plane encoding. The difference is in ease of modeling.
Reference: [10] <author> J.H. Conway and N.J.A. Sloane. </author> <title> Sphere packings, lattices, and groups. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: The usual approach is to choose an appropriate lattice, then scale and truncate it in such a way that rate and mean-square-error (MSE) performance is comparable to that of full-search VQ <ref> [10, 11, 12, 3 13] </ref>. Lattice VQ also has the potential of limiting maximum errors, provided that an untruncated lattice is used [14]. The idea is illustrated in Figure 5. <p> While it is true that a Z N lattice is suboptimal in terms of space-filling, the performance penalty for using it instead of a more efficient lattice is known to be quite small. This has been established theoretically in the case of asymptotically high rate <ref> [10, 15] </ref>, and experimentally in the low- and medium-rate regions for some sources [14]. In the example illustrated in Figure 5, the rate-MSE performance was examined and found to be comparable to that of standard VQ.
Reference: [11] <author> J.H. Conway and N.J.A. Sloane. </author> <title> Fast quantizing and decoding algorithms for lattice quantizers and codes. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-28:227-232, </volume> <month> March </month> <year> 1982. </year>
Reference-contexts: The usual approach is to choose an appropriate lattice, then scale and truncate it in such a way that rate and mean-square-error (MSE) performance is comparable to that of full-search VQ <ref> [10, 11, 12, 3 13] </ref>. Lattice VQ also has the potential of limiting maximum errors, provided that an untruncated lattice is used [14]. The idea is illustrated in Figure 5.
Reference: [12] <author> D.G. Jeong and J.D. Gibson. </author> <title> Lattice vector quantization for image coding. </title> <booktitle> In ICASSP-89: 1989 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Glasgow, UK, </address> <month> May </month> <year> 1989. </year> <note> IEEE. </note>
Reference-contexts: The usual approach is to choose an appropriate lattice, then scale and truncate it in such a way that rate and mean-square-error (MSE) performance is comparable to that of full-search VQ <ref> [10, 11, 12, 3 13] </ref>. Lattice VQ also has the potential of limiting maximum errors, provided that an untruncated lattice is used [14]. The idea is illustrated in Figure 5.
Reference: [13] <author> M. Antonini, M. Barlaud, and P. Mathieu. </author> <title> Image coding using lattice vector quantization of wavelet coefficients. </title> <booktitle> In ICASSP-91: 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Toronto, Canada, </address> <month> April </month> <year> 1991. </year> <note> IEEE. </note>
Reference: [14] <author> Takanori Senoo and Bernd Girod. </author> <title> Vector quantization for entropy coding of image subbands. </title> <journal> IEEE Trans. Image Proc., </journal> <volume> 1 </volume> <pages> 526-533, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Lattice VQ also has the potential of limiting maximum errors, provided that an untruncated lattice is used <ref> [14] </ref>. The idea is illustrated in Figure 5. For untruncated-lattice VQ to perform well for nonuniformly distributed input (the case of interest in image coding), it is essential that the lattice points be efficiently entropy coded. <p> For untruncated-lattice VQ to perform well for nonuniformly distributed input (the case of interest in image coding), it is essential that the lattice points be efficiently entropy coded. The difficulty lies in the astronomically large alphabet of lattice points that must be handled. Senoo and Girod <ref> [14] </ref> attack the problem by efficiently entropy encoding only those points that occur frequently in the training data, using a simple fixed-length code for the remaining points. <p> This has been established theoretically in the case of asymptotically high rate [10, 15], and experimentally in the low- and medium-rate regions for some sources <ref> [14] </ref>. In the example illustrated in Figure 5, the rate-MSE performance was examined and found to be comparable to that of standard VQ.
Reference: [15] <author> Tom D. Lookabaugh and Robert M. Gray. </author> <title> High-resolution quantization theory and the vector quan-tizer advantage. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-35:1020-1033, </volume> <year> 1989. </year>
Reference-contexts: While it is true that a Z N lattice is suboptimal in terms of space-filling, the performance penalty for using it instead of a more efficient lattice is known to be quite small. This has been established theoretically in the case of asymptotically high rate <ref> [10, 15] </ref>, and experimentally in the low- and medium-rate regions for some sources [14]. In the example illustrated in Figure 5, the rate-MSE performance was examined and found to be comparable to that of standard VQ.
Reference: [16] <author> Alex Pentland and Bradley Horowitz. </author> <title> A practical approach to fractal-based image compression. </title> <booktitle> In Proc. IEEE Data Comp. Conf., </booktitle> <address> Utah, </address> <year> 1991. </year> <month> 4 </month>
Reference-contexts: For example, in any image subband coding system, nonlinear statistical interdependence exists both within and across subbands, despite critical sampling. This interdependence might be handled by the proposed technique. Pentland and Horowitz <ref> [16] </ref> describe a hierarchical technique in which statistical interdependence is exploited by conditioning the entropy coding of a VQ output on the corresponding VQ output at a coarser resolution level.
References-found: 16

