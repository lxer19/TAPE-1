URL: file://ftp.cis.ohio-state.edu/pub/communication/papers/icpp96-clustered-collective.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~panda/cluster_pub.html
Root-URL: 
Email: Email: fbasak,pandag@cis.ohio-state.edu  
Title: Designing Processor-cluster Based Systems: Interplay Between Cluster Organizations and Broadcasting Algorithms  
Author: Debashis Basak and Dhabaleswar K. Panda 
Address: Columbus, OH 43210-1277  
Affiliation: Department of Computer and Information Science The Ohio State University,  
Date: Aug. 1996  
Note: To be presented at the International Conference on Parallel Processing,  
Abstract: Past research on designing processor-cluster based parallel systems has focused mainly on studying the packaging technologies affecting the inter-cluster network. To make such a design approach more attractive, there is a strong need to understand the details about the topology inside the cluster, its memory organization, and the impact of this organization on system performance. In this paper we analyze the communication costs for accessing inter-cluster and intra-cluster memories under different cluster organizations. The merits of these organizations are evaluated based on the performance of a commonly used U mesh broadcast algorithm. Our results indicate that tightly coupled cluster organizations with shared access to memory offer faster intra-cluster communication. This leads to such organizations to outperform loosely coupled cluster organizations. We also demonstrate that such faster intra-cluster access in clustered systems can be exploited to design better collective communication algorithms. We propose a new broadcasting algorithm on clustered meshes - clus mesh which outperforms existing u mesh on clustered systems by up to 20%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Basak and D. K. Panda. </author> <title> Designing Clustered Multiprocessor Systems under Packaging and Technological Advancements. </title> <address> OSU-CISRC-11/95-TR51, </address> <year> 1995. </year> <note> To appear in IEEE TPDS. </note>
Reference-contexts: 1 Introduction With advancements in VLSI and packaging technologies it has become cost-effective to integrate multiple processing elements into a multi-chip or board module <ref> [1] </ref>. This is leading to the development of parallel systems using such processor-clusters as building blocks instead of single processors, thus allowing a modular and hierarchical approach to building large systems. Prominent examples of processor-cluster based systems are the Stanford DASH, Intel Paragon, and Cray-T3D. <p> Prominent examples of processor-cluster based systems are the Stanford DASH, Intel Paragon, and Cray-T3D. Typically, the interconnections connecting the processor-clusters of these systems are scalable meshes, tori, or multistage networks. In recent years research related to designing processor-cluster based systems has emphasized mostly on packaging and cost-efficiency aspects <ref> [1, 3] </ref>. These results are geared towards deriving optimal inter-cluster topologies under packaging and pinout constraints. While deriving such optimal topologies researchers have primarily emphasized on the load on network bisection and evaluated different topologies with respect to average message latency and throughput.
Reference: [2] <author> D. Basak and D. K. Panda. </author> <title> Designing Processor-cluster Based Systems: Interplay Between Cluster Organizations and Collective Communication Algorithms. </title> <address> OSU-CISRC-1/96-TR05, </address> <year> 1996. </year>
Reference-contexts: This allows a system with a given total number of processors to be interconnected by using fewer network interfaces and network routers leading to a more cost-effective design [3]. The interface offers a number of channels to and from the network router. These channels (referred to as injection/consumption channels <ref> [2] </ref>) are used to inject/consume messages to/from other clusters. We use i to denote the number of injection or consumption channels per cluster. Currently, typical value for i is 1 to 2 [2]. 3 Processor-cluster organizations and com munication costs In this section we present four different cluster organizations as shown <p> These channels (referred to as injection/consumption channels <ref> [2] </ref>) are used to inject/consume messages to/from other clusters. We use i to denote the number of injection or consumption channels per cluster. Currently, typical value for i is 1 to 2 [2]. 3 Processor-cluster organizations and com munication costs In this section we present four different cluster organizations as shown in Fig. 2, two loosely coupled: star and direct-network and two tightly coupled: bus and x-bar. The inter-cluster network connecting clusters is kept the same when considering different cluster organizations. <p> Explanation of these parameters and discussion on the different clustered organizations are detailed in <ref> [2] </ref>. The shmem get transfer across clusters requires inter-cluster message communication. For popular wormhole routing a cost penalty of (t s + mt p ) cycles can be derived for this operation independent of the cluster organization. <p> Detailed derivations and explanations for these expressions are presented in <ref> [2] </ref>. <p> For a given system with N C clusters, this phase requires (dlog 2 N C e) steps of inter-cluster message communication. Assuming the broadcast data consists of m words, the time for the inter-cluster phase can be derived as (dlog 2 N C e)(t s + mt p ) <ref> [2] </ref>. The time for the intra-cluster phase is a function of the cluster organization. For the four different cluster organizations discussed earlier, Table 2 summarizes the time required for this phase. <p> For the four different cluster organizations discussed earlier, Table 2 summarizes the time required for this phase. The basic approach for broadcasting inside a cluster is to employ, whenever possible, a U mesh-like divide and conquer strategy. Detailed derivations of the expressions in Table 2 are presented in <ref> [2] </ref>. Table 2: Time required for intra-cluster phase of the U mesh under different cluster organizations (i c and b c are assumed). <p> For broadcasting in larger systems, including 2D mesh interconnected systems, Clus mesh has been shown to similarly outperform U mesh <ref> [2] </ref>. 5.2 Comparison of Clus mesh and U mesh per formance U mesh on a system having N C clusters each of size c. <p> In each plot as the cluster size is increased the number of clusters in the system falls. This can lead to a fall <ref> [2] </ref> in the relative gains of Clus mesh over U mesh as depicted in the plots.
Reference: [3] <author> D. Basak, D. K. Panda and M. Banikazemi. </author> <title> Benefits of Processor Clustering in Designing Large Parallel Systems: When and How? In Proc. </title> <booktitle> of the IPPS, </booktitle> <pages> pp. 286-290, </pages> <year> 1996. </year>
Reference-contexts: Prominent examples of processor-cluster based systems are the Stanford DASH, Intel Paragon, and Cray-T3D. Typically, the interconnections connecting the processor-clusters of these systems are scalable meshes, tori, or multistage networks. In recent years research related to designing processor-cluster based systems has emphasized mostly on packaging and cost-efficiency aspects <ref> [1, 3] </ref>. These results are geared towards deriving optimal inter-cluster topologies under packaging and pinout constraints. While deriving such optimal topologies researchers have primarily emphasized on the load on network bisection and evaluated different topologies with respect to average message latency and throughput. <p> Current processor-clustered organizations connect each cluster to the inter-cluster network via a single interface, referred to as the network interface. This allows a system with a given total number of processors to be interconnected by using fewer network interfaces and network routers leading to a more cost-effective design <ref> [3] </ref>. The interface offers a number of channels to and from the network router. These channels (referred to as injection/consumption channels [2]) are used to inject/consume messages to/from other clusters. We use i to denote the number of injection or consumption channels per cluster.
Reference: [4] <author> Cray Reasearch Inc. </author> <title> Cray T3D System Architecture Overview, </title> <year> 1993. </year>
Reference-contexts: The inter-cluster network connecting clusters is kept the same when considering different cluster organizations. Let us analyze the communication delay to transfer m words to a processor's own memory from the memory of another processor. This basic operation is similar to the shmem get primitive <ref> [4] </ref> supported on the Cray T3D system.
Reference: [5] <author> P. K. McKinley et al. </author> <title> Unicast-based Multicast Communication in Wormhole-routed Networks. </title> <journal> IEEE TPDS, </journal> <volume> 5(12) </volume> <pages> 1252-1265, </pages> <month> Dec </month> <year> 1994. </year>
Reference-contexts: Similar analysis can be easily extended to other collective communication operations such as multi--cast, gather, reduction, and barrier synchronization. The U mesh algorithm has been proposed in <ref> [5] </ref> as an optimal algorithm to achieve multicasts and broadcasts in non-clustered mesh interconnected systems. To perform a multicast/broadcast to (N 1) destinations the algorithm orders the N 1 destinations and source into a dimension-ordered chain and achieves the operation in dlog 2 N e contention-free steps. <p> This demonstrates that tightly-coupled cluster con figurations are indeed necessary to offer faster intra-cluster access and to fully exploit communication locality. 5 Broadcasting on clustered meshes Tightly coupled cluster organizations lead to faster intra-cluster communication. However, existing algorithms like U Mesh <ref> [5] </ref> designed to be optimal under the assumption of flat communication cost may not remain optimal in clustered systems with differential intra- and inter-cluster costs and available multiple injection/consumption channels per cluster.
Reference: [6] <author> B. A. Nayfeh et al. </author> <title> The impact of shared-cache clustering in small-scale shared-memory multiprocessors. </title> <booktitle> In Proc. of the HPCA-2, </booktitle> <pages> pp. 74-84, </pages> <year> 1996. </year>
Reference-contexts: The intra-cluster organization heavily depends on the programming model intended for a system. In order to support shared memory programming it has been shown to be beneficial to allow shared memory between processors inside a cluster <ref> [6] </ref>. However, it is not clear whether distributed memory systems need to provide shared-memory access between sibling processors (processors within a cluster)? From a naive point of view such sharing does not seem to be beneficial due to the message passing programming model.
References-found: 6

