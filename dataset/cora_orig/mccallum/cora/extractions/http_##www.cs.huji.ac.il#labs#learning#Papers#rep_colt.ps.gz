URL: http://www.cs.huji.ac.il/labs/learning/Papers/rep_colt.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: http://www.cs.huji.ac.il
Email: e-mail: ftishby,lidrortg@cs.huji.ac.il  
Title: Approximate Faithful Embedding in Learning (Extended abstract)  
Author: Naftali Tishby and Lidror Troyansky 
Date: January 17, 1997  
Address: Jerusalem, Israel  
Affiliation: Institute of Computer Science and Center for Neural Computation The Hebrew University,  
Abstract: In this paper we consider the problem of embedding the input and hypotheses of boolean function classes in other classes, such that the natural metric structure of the two spaces is approximately preserved. We first prove some general properties of such embedding and then suggest and discuss possible approximate embedding in the class of "half-spaces" (single layer perceptrons) with dimension polynomial in the VC dimension of the original problem. Our main result is that such an approximate embedding by half-spaces is possible for a class of problems, which we call "informative". These are homogeneous learnable concept classes for which the dual problem (learning the input from labels of random hypotheses) has a similar VC-dimension, and the variance of the generalization error is bounded from zero. This is a distribution dependent property which we need to hold for most distributions on instances and hypotheses. We argue that many important learning classes are "informative" for typical distributions, e.g. geometric concepts, neural networks, decision trees, etc.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alon, N., P. Frankl, and V. Rodl, </author> <title> "Geometrical Realization of Set Systems and Probabilistic Communication Complexity", </title> <booktitle> STOC-85,pp. </booktitle> <pages> 277-280, </pages> <year> 1985. </year>
Reference-contexts: This completes the proof of the main theorem. 2 5 Discussion Our results raises several interesting questions. First, it seems from the paper of Alon et. al <ref> [1] </ref> and from the work Ben-David and Litman [2] that there is a lower bound on the dimension of exact Euclidean embedding which is of the order of the size of the space itself and not its VC dimension.
Reference: [2] <author> Ben-David, S. and A. Litman, </author> <title> "Combinatorial Variability of Vapnik Chervonenkis Classes", </title> <type> Preprint. </type>
Reference-contexts: This completes the proof of the main theorem. 2 5 Discussion Our results raises several interesting questions. First, it seems from the paper of Alon et. al [1] and from the work Ben-David and Litman <ref> [2] </ref> that there is a lower bound on the dimension of exact Euclidean embedding which is of the order of the size of the space itself and not its VC dimension. <p> Acknowledgments We are very grateful to Shai Ben-David and Nati Linial for important discussions and observations. We thank Shai Ben-David for sharing with us his preliminary independent results <ref> [2] </ref>.
Reference: [3] <author> Floyed, S. and M. Warmuth, </author> <title> "Sample compression, learnability, and the Vapnik-Chervonenkis dimension", </title> <booktitle> Machine Learning,21:1-36, </booktitle> <year> 1995 </year>
Reference-contexts: The issue of representation of concept classes by embedding in other classes was raised already by Littlestone and Warmuth [6] and later on by Floyd and Warmuth <ref> [3] </ref> and by Pitt and Warmuth [8] who raised the issues of exact embedding and of sample compression by embedding of learning classes. The issue of a good representation of the inputs is also known to be of crucial importance for learning. <p> It is well known that: Theorem 2 (Vapnik) A sufficient condition for [PAC] learnability of (H; X ) is that dim V C H is finite. Our embedding results rely on the symmetry between X and H and on properties of the dual learning problem <ref> [3, 7] </ref>. Definition 4 (The Dual Learning problem) The problem of learning an input x 2 X from labels of a random sample of hypotheses h (m) in H, distributed according to P m is called the dual problem to the original hypothesis learning problem.
Reference: [4] <author> Haussler, D., </author> <title> "Decision-theoretic generalization of the PAC model for neural net and other learning applications." </title> <journal> Information and Computation, </journal> <volume> 95(2): </volume> <pages> 129-161, </pages> <year> 1991. </year>
Reference: [5] <author> Haussler D., M. Kearns, H. S. Seung, and N. Tishby, </author> <title> "Rigorous Learning Curve Bounds from Statistical Mechanics" Proceedings of COLT-94, </title> <publisher> ACM, </publisher> <address> NY, pp.76-87, </address> <year> 1994. </year>
Reference-contexts: In this case our embedding is meaningless. 5.4 Relation to "learning curves" An interesting question, which motivated this study, is how are the learning curves <ref> [5] </ref> effected by faithful embedding. We keep the answer to this question to another work [12] where we also present many experimental (numerical) applications of our method.
Reference: [6] <author> Littlestone, N., and M. Warmuth, </author> <title> "Relating Data Compression and Learnability", </title> <type> Unpublished manuscript, </type> <year> 1996. </year>
Reference-contexts: The issue of representation of concept classes by embedding in other classes was raised already by Littlestone and Warmuth <ref> [6] </ref> and later on by Floyd and Warmuth [3] and by Pitt and Warmuth [8] who raised the issues of exact embedding and of sample compression by embedding of learning classes. The issue of a good representation of the inputs is also known to be of crucial importance for learning.
Reference: [7] <author> Pach, J. and P. Agarwal, </author> <title> Combinatorial Geometry, </title> <editor> J. </editor> <publisher> Wiley, </publisher> <address> New York, </address> <year> (1995). </year>
Reference-contexts: It is well known that: Theorem 2 (Vapnik) A sufficient condition for [PAC] learnability of (H; X ) is that dim V C H is finite. Our embedding results rely on the symmetry between X and H and on properties of the dual learning problem <ref> [3, 7] </ref>. Definition 4 (The Dual Learning problem) The problem of learning an input x 2 X from labels of a random sample of hypotheses h (m) in H, distributed according to P m is called the dual problem to the original hypothesis learning problem. <p> To avoid confusion we call the hypothesis learning the direct problem. It is easy to verify that: Theorem 3 (Dual VC dimension <ref> [7] </ref>) The direct and the dual problems are [PAC] learnable together.
Reference: [8] <author> Pitt, L. and M. Warmuth, </author> <title> "Prediction Preserving reducibility", </title> <journal> Journal of Computer and System Sciences,41:430-467, </journal> <year> 1990. </year>
Reference-contexts: The issue of representation of concept classes by embedding in other classes was raised already by Littlestone and Warmuth [6] and later on by Floyd and Warmuth [3] and by Pitt and Warmuth <ref> [8] </ref> who raised the issues of exact embedding and of sample compression by embedding of learning classes. The issue of a good representation of the inputs is also known to be of crucial importance for learning.
Reference: [9] <author> Pitt, L. and L. Valiant, </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the ACM, </journal> <volume> 35 </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction One of the most fundamental problems in learning is to define and find an adequate representation of the input the hypothesis spaces. Within the context of PAC learning, the importance of hypothesis representation was first explored by Pitt and Valiant <ref> [9] </ref> who showed that k-term DNF is not efficiently PAC learnable using a hypothesis class of k-term DNF, but is efficiently PAC learnable using k-CNF.
Reference: [10] <author> Rosenblatt, F., </author> <title> Principles of Neurodynamics, </title> <publisher> Spartan, </publisher> <address> New-York, </address> <year> (1962). </year>
Reference-contexts: Close inputs therefore tend to have the same label by most 1 hypotheses, and the decision surfaces become, in many cases, simpler. In this work we study some sufficient conditions under which the decision surfaces can be approximated by hyperplanes. Ever since the seminal work of Rosenblatt <ref> [10] </ref> on the perceptron, his idea of random embedding every pattern recognition problem in high dimension Euclidean space such that it becomes linearly separable, remained intriguing and open.
Reference: [11] <author> Schapire, R.E., and Y. Freund, </author> <title> "A decision theoretic generalization of on-line learning and its application to boosting". </title> <booktitle> In EuroColt '95,pp. </booktitle> <pages> 23-27, </pages> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference: [12] <author> Tishby, N., and L. Troyansky, </author> <note> in preparation. </note>
Reference-contexts: In this case our embedding is meaningless. 5.4 Relation to "learning curves" An interesting question, which motivated this study, is how are the learning curves [5] effected by faithful embedding. We keep the answer to this question to another work <ref> [12] </ref> where we also present many experimental (numerical) applications of our method. It is important to mention here, however, that both the learning curve and the approximate embedability by half-spaces are determined by the distribution of the generalization errors in the class.
Reference: [13] <author> Vpanik, V., </author> <title> The Nature of Statistical Learning Theory, </title> <publisher> Springer-Verlag, </publisher> <address> New-York, </address> <year> (1995). </year> <month> 10 </month>
Reference-contexts: While our work share some common feature with other methods, most noticeable are the method of "boosting"[11] and Vapnik's "Support Vector" machines <ref> [13] </ref>, the method we describe allow an efficient embedding which does not depend on the target concept.
References-found: 13

