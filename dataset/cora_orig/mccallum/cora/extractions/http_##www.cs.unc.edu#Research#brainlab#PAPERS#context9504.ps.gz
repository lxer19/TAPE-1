URL: http://www.cs.unc.edu/Research/brainlab/PAPERS/context9504.ps.gz
Refering-URL: http://www.cs.unc.edu/Research/brainlab/
Root-URL: http://www.cs.unc.edu
Email: E-mail marshall@cs.unc.edu.  
Title: bdNeural Networks 8:335-362, April 1995.ce Adaptive Perceptual Pattern Recognition by Self-Organizing Neural Networks: Context, Uncertainty,
Author: JONATHAN A. MARSHALL 
Keyword: Running title: Context, Uncertainty, Multiplicity, and Scale Keywords|Masking fields, Anti-Hebbian learning, Distributed coding, Adaptive constraint satisfaction, Decorrelators, Excitatory+inhibitory (EXIN) learning, Transparency, Segmentation.  
Note: Requests for reprints should be sent to the author  Telephone 919-962-1887.  
Address: Chapel Hill  3175, Sitterson Hall,  Chapel Hill, NC 27599-3175, U.S.A.  
Affiliation: Department of Computer Science University of North Carolina at  at Department of Computer Science, CB  University of North Carolina,  
Abstract: A new context-sensitive neural network, called an "EXIN" (excitatory+inhibitory) network, is described. EXIN networks self-organize in complex perceptual environments, in the presence of multiple superimposed patterns, multiple scales, and uncertainty. The networks use a new inhibitory learning rule, in addition to an excitatory learning rule, to allow superposition of multiple simultaneous neural activations (multiple winners), under strictly regulated circumstances, instead of forcing winner-take-all pattern classifications. The multiple activations represent uncertainty or multiplicity in perception and pattern recognition. Perceptual scission (breaking of linkages) between independent category groupings thus arises and allows effective global context-sensitive segmentation constraint satisfaction, and exclusive credit attribution. A Weber Law neuron-growth rule lets the network learn and classify input patterns despite variations in their spatial scale. Applications of the new techniques include segmentation of superimposed auditory or biosonar signals, segmentation of visual regions, and representation of visual transparency. Acknowledgements: Supported in part by a UNC-CH Junior Faculty Development Award, an ORAU Junior Faculty Enhancement Award from Oak Ridge Associated Universities, the Office of Naval Research (Cognitive and Neural Sciences, N00014-93-1-0208), the National Eye Institute (EY09669), the University of Minnesota Center for Research in Learning, Perception, and Cognition, the National Institute of Child Health and Human Development (HD-07151), and the Minnesota Supercomputer Institute (Visiting Research Scholar award and Supercomputer Resource Grant). The author thanks Albert Nigrin, Christina Burbeck, John Hummel, Stephen Aylward, Robert Hubbard, William Gnadt, Michael Cohen, Sharon Chen, Vinay Gupta, George Kalarickal, and Charles Schmitt for their helpful comments on the paper, and R. Eric Fredericksen for help with the software. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Adelson, E.H. & Movshon, J.A. </author> <year> (1982). </year> <title> Phenomenal coherence of moving visual patterns. </title> <journal> Nature, </journal> <volume> 300, </volume> <pages> 523-525. </pages>
Reference: <author> Amari, S. </author> <year> (1977). </year> <title> Neural theory of association and concept formation. </title> <journal> Biological Cybernetics, </journal> <volume> 26, </volume> <pages> 175-185. </pages>
Reference: <author> Amari, S. & Takeuchi, A. </author> <year> (1978). </year> <title> Mathematical theory on formation of category detecting nerve cells. </title> <journal> Biological Cybernetics, </journal> <volume> 29, </volume> <pages> 127-136. </pages>
Reference: <author> Anderson, J.A., Silverstein, J.W., Ritz, S.A., & Jones, R.S. </author> <year> (1977). </year> <title> Distinctive features, categorical perception, and probability learning: Some applications of a neural model. </title> <journal> Psychological Review, </journal> <volume> 84 (5), </volume> <pages> 413-451. </pages>
Reference: <author> Barlow, H.B. </author> <year> (1980). </year> <title> The absolute efficiency of perceptual decisions. </title> <journal> Philosophical Transactions of the Royal Society of London, Ser. B, </journal> <volume> 290, </volume> <pages> 71-82. </pages>
Reference: <author> Barto, A.G., Sutton, R.S., & Anderson, C.W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 834-846. </pages>
Reference-contexts: Discussion 8.1. Basis for Inhibitory Selectivity: Temporal Overlap and Common Input The inhibitory selectivity provided by EXIN networks implements adaptively a fundamental and general principle of perception: the principle of exclusive allocation (Bregman, 1990), or credit assignment <ref> (Barto, Sutton, & Anderson, 1983) </ref>. That is, the "credit" for a given input feature is "assigned" exclusively to a single representation. The learned inhibitory constraints tend to prevent more than one neuron strongly excited by the feature from becoming active simultaneously.
Reference: <author> Becker, S. & Hinton, G.E. </author> <year> (1992). </year> <title> A self-organizing neural network that discovers surfaces in random-dot stereograms. </title> <journal> Nature, </journal> <volume> 355, </volume> <pages> 161-163. </pages>
Reference: <author> Bienenstock, E.L., Cooper, L.N., & Munro, P.W. </author> <year> (1982). </year> <title> Theory for the development of neuron selectivity: Orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal of Neuroscience, </journal> <volume> 2, 1, </volume> <pages> 32-48. </pages>
Reference: <author> Blakemore, C., Carpenter, R.H.S., & Georgeson, M.A. </author> <year> (1970). </year> <title> Lateral inhibition between orientation detectors in the human visual system. </title> <journal> Nature, </journal> <volume> 228, </volume> <pages> 37-39. </pages>
Reference: <author> Bregman, A.S. </author> <year> (1990). </year> <title> Auditory scene analysis: The perceptual organization of sound. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: This global context-sensitive constraint satisfaction property is extremely important in perceptual processing because it lets globally optimal representations overcome locally optimal ones. EXIN networks implement a form of exclusive allocation <ref> (Bregman, 1990) </ref>, whereby "a sensory element should not be used in more than one description at a time" (Bregman, 1990, p. 12). A sensory element can potentially participate in many different pattern groupings. <p> This global context-sensitive constraint satisfaction property is extremely important in perceptual processing because it lets globally optimal representations overcome locally optimal ones. EXIN networks implement a form of exclusive allocation (Bregman, 1990), whereby "a sensory element should not be used in more than one description at a time" <ref> (Bregman, 1990, p. 12) </ref>. A sensory element can potentially participate in many different pattern groupings. For example, Layer 1 neuron b projects excitatory connections to several different Layer 2 neurons - ab, abc, and bc. <p> Discussion 8.1. Basis for Inhibitory Selectivity: Temporal Overlap and Common Input The inhibitory selectivity provided by EXIN networks implements adaptively a fundamental and general principle of perception: the principle of exclusive allocation <ref> (Bregman, 1990) </ref>, or credit assignment (Barto, Sutton, & Anderson, 1983). That is, the "credit" for a given input feature is "assigned" exclusively to a single representation. The learned inhibitory constraints tend to prevent more than one neuron strongly excited by the feature from becoming active simultaneously. <p> Benefits of Exclusive Allocation Exclusive allocation is a desirable property because it prevents any given piece of data from counting as evidence for multiple patterns simultaneously <ref> (Bregman, 1990) </ref>. There are many examples from vision and other perceptual modalities where a given datum should be allowed to count as evidence for only one pattern at a time. The credit for a given datum should be assigned to a single pattern.
Reference: <author> Cannon, M.W. & Fullencamp, S.C. </author> <year> (1990). </year> <title> Inhibitory interactions in suprathreshold vision. </title> <journal> Investigative Ophthalmology and Visual Science, </journal> <volume> 31, 4, </volume> <pages> 323. </pages>
Reference: <author> Carlson, A. </author> <year> (1990). </year> <title> Anti-Hebbian learning in a non-linear neural network. </title> <journal> Biological Cybernetics, </journal> <volume> 64, </volume> <pages> 171-176. </pages>
Reference: <author> Carpenter, G.A. & Grossberg, S. </author> <year> (1987a). </year> <title> A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 37, </volume> <pages> 54-115. </pages>
Reference: <author> Carpenter, G.A. & Grossberg, S. </author> <year> (1987b). </year> <title> ART 2: Stable self-organization of pattern recognition codes for analog input patterns. </title> <journal> Applied Optics, </journal> <volume> 26, </volume> <pages> 4919-4930. </pages>
Reference: <author> Carpenter, G.A. & Grossberg, S. </author> <year> (1990). </year> <title> ART 3: Hierarchical search using chemical transmitters in self-organizing pattern recognition architectures. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 129-152. </pages>
Reference: <author> Carpenter, G.A. & Grossberg, S. </author> <year> (1992). </year> <title> Fuzzy ARTMAP: Supervised learning, recognition, and prediction by a self-organizing neural network. </title> <journal> IEEE Communications Magazine, </journal> <volume> 30, </volume> <pages> 38-49. </pages>
Reference: <author> Carpenter, G.A., Grossberg, S., & Rosen, D.B. </author> <year> (1991a). </year> <title> ART2-A: An adaptive resonance algorithm for rapid category learning and recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 493-504. </pages>
Reference: <author> Carpenter, G.A., Grossberg, S., & Rosen, D.B. </author> <year> (1991b). </year> <title> Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system. </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 759-771. </pages>
Reference: <author> Cherry, E.C. </author> <year> (1953). </year> <title> Some experiments on the recognition of speech with one and with two ears. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 25, </volume> <pages> 975-979. </pages>
Reference-contexts: The segmentation or parsing capabilities of EXIN networks may be useful in modeling audition and speech perception as well. For example, at a cocktail party <ref> (Cherry, 1953) </ref>, human auditory systems extract and maintain simultaneous, relatively independent representations of multiple sound groupings (e.g., multiple speakers, music, clattering dinnerware), even though they are all superimposed on a single shared input channel (air vibrations).
Reference: <author> Cohen, M.A. & Grossberg, S. </author> <year> (1983). </year> <title> Absolute stability of global pattern formation and parallel memory storage by competitive neural networks. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-13, </volume> <pages> 815-826. </pages>
Reference: <author> Cohen, M.A. & Grossberg, S. </author> <year> (1986). </year> <title> Neural dynamics of speech and language coding: Developmental programs, perceptual grouping, and competition for short term memory. </title> <journal> Human Neurobiology, </journal> <volume> 5, </volume> <pages> 1-22. </pages> <note> 39 Cohen, </note> <author> M.A. & Grossberg, S. </author> <year> (1987). </year> <title> Masking fields: A massively parallel neural architecture for learning, recognizing, and predicting multiple groupings of patterned data. </title> <journal> Applied Optics, </journal> <volume> 26, </volume> <pages> 1866-1891. </pages>
Reference-contexts: If the normalization parameter is (unrealistically) viewed as a neuron's physical size, then each neuron can also be imagined as having a finite dendritic surface area available for implantation of synaptic receptor sites <ref> (Cohen & Grossberg, 1986) </ref>. If a neuron's receptor area is already fully allocated, then the neuron must physically grow to increase that area when additional input connections are required. Then a given input signal would produce a smaller effect. <p> However, as written in their original papers <ref> (Cohen & Grossberg, 1986, 1987) </ref>, the complete computation of the overlap function is physically unrealizable unless activity correlation factors are also used. Suppose their network's Layer 1 neurons were spatially ordered: (a; b; c; d).
Reference: <author> Coolen, A.C.C. & Kuijk, F.W. </author> <year> (1989). </year> <title> A learning mechanism for invariant pattern recognition in neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 495-506. </pages>
Reference: <author> DeSieno, D. </author> <year> (1988). </year> <title> Adding a conscience to competitive learning. </title> <booktitle> IEEE international conference on neural networks, I., </booktitle> <pages> 117-124. </pages>
Reference: <author> Duda, R.O. & Hart, P.E. </author> <year> (1972). </year> <title> Use of the Hough Transform to detect lines and curves in pictures. </title> <journal> Communications of the ACM, </journal> <volume> 15, </volume> <pages> 11-15. </pages>
Reference: <author> Easton, P. & Gordon, P.E. </author> <year> (1984). </year> <title> Stabilization of Hebbian neural nets by inhibitory learning. </title> <journal> Biological Cybernetics, </journal> <volume> 51, </volume> <pages> 1-9. </pages>
Reference: <author> Field, D.J. </author> <year> (1987). </year> <title> Relations between the statistics of natural images and the response properties of cortical cells. </title> <journal> Journal of the Optical Society of America A, </journal> <volume> 4, 12, </volume> <pages> 2379-2394. </pages>
Reference: <author> Foldiak, P. </author> <year> (1989). </year> <title> Adaptive network for optimal linear feature extraction. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, </address> <month> June </month> <year> 1989, </year> <pages> I., 401-405. </pages>
Reference: <author> Foldiak, P. </author> <year> (1990). </year> <title> Forming sparse representations by local anti-Hebbian learning. </title> <journal> Biological Cybernetics, </journal> <volume> 64, 2, </volume> <pages> 165-170. </pages>
Reference-contexts: Thus, the network exploits a temporary instability in the balance of inhibition to equalize the response frequency of all the Layer 2 neurons. This equalization effect is similar to that achieved by "conscience" rules (DeSieno, 1988; Van den Bout & Miller, 1989) and by adaptive threshold rules <ref> (Foldiak, 1990, 1992) </ref>. Nigrin (1990abc, 1992, 1993) has proposed a network, called SONNET, that also uses a form of adaptive "size" normalization, combined with excitatory and inhibitory learning. Both EXIN networks and SONNET networks self-organize to have similar connectivity patterns and neuron sizes.
Reference: <author> Foldiak, P. </author> <year> (1991). </year> <title> Learning invariance from transformation sequences. </title> <journal> Neural Computation, </journal> <volume> 3, 2, </volume> <pages> 194-200. </pages>
Reference: <author> Foldiak, P. </author> <year> (1992). </year> <title> Models of sensory coding. </title> <type> Technical Report CUED/F-INFENG/TR 91, </type> <institution> Department of Engineering, University of Cambridge. </institution>
Reference: <author> Fukushima, K. </author> <year> (1986). </year> <title> A neural network model for selective attention in visual pattern recognition. </title> <journal> Biological Cybernetics, </journal> <volume> 55, </volume> <pages> 5-15. </pages>
Reference-contexts: which use WTA dynamics (Becker & Hinton, 1992; Hinton & Becker, 1990; Marr & Poggio, 1976), either are limited to representing a single object or surface at a single depth value for any given retinotopic location or are forced to alternate via an attentional mechanism their representations of multiple objects <ref> (Fukushima, 1986) </ref>. The segmentation or parsing capabilities of EXIN networks may be useful in modeling audition and speech perception as well.
Reference: <author> Grossberg, S. </author> <year> (1972). </year> <title> Neural expectation: Cerebellar and retinal analogs of cells fired by learnable or unlearned pattern classes. </title> <journal> Kybernetik, </journal> <volume> 10, </volume> <pages> 49-57. </pages>
Reference-contexts: The activity level x i of each Layer 2 neuron changes according to a shunting equation <ref> (Grossberg, 1972, 1982b) </ref>: dt where A, B, and C are constants, and where E i and I i represent the neuron's total excitatory and inhibitory input signals, respectively. Because equation (8) is a shunting equation, neuron activation levels are forced to remain within a bounded range.
Reference: <author> Grossberg, S. </author> <year> (1976a). </year> <title> On the development of feature detectors in the visual cortex with applications to learning and reaction-diffusion systems. </title> <journal> Biological Cybernetics, </journal> <volume> 21, </volume> <pages> 145-159. </pages>
Reference: <author> Grossberg, S. </author> <year> (1976b). </year> <title> Adaptive pattern classification and universal recoding: I. Parallel development and coding of neural feature detectors. </title> <journal> Biological Cybernetics, </journal> <volume> 23, </volume> <pages> 121-134. </pages>
Reference: <author> Grossberg, S. </author> <year> (1976c). </year> <title> Adaptive pattern classification and universal recoding: II. Feedback, expectation, olfaction, and illusions. </title> <journal> Biological Cybernetics, </journal> <volume> 23, </volume> <pages> 187-202. </pages>
Reference: <author> Grossberg, S. </author> <year> (1978). </year> <title> A theory of human memory: Self-organization and performance of sensory-motor codes, maps, and plans. </title> <booktitle> In Progress in Theoretical Biology, 5. </booktitle> <address> San Diego: </address> <publisher> Academic Press, </publisher> <pages> 233-374. </pages>
Reference: <author> Grossberg, S. </author> <year> (1980). </year> <title> How does a brain build a cognitive code? Psychological Review, </title> <booktitle> 87, </booktitle> <pages> 1-51. </pages>
Reference: <author> Grossberg, S. </author> <year> (1982a). </year> <title> Processing of expected and unexpected events during conditioning and attention: A psychophysiological theory. </title> <journal> Psychological Review, </journal> <volume> 89, </volume> <pages> 529-572. </pages>
Reference: <author> Grossberg, S. </author> <year> (1982b). </year> <title> Studies of mind and brain. </title> <address> Boston: </address> <publisher> Reidel Press. </publisher>
Reference-contexts: The excitatory learning rule can be expressed mathematically as a differential equation <ref> (Grossberg, 1982b) </ref>. Let z + ji represent the weight of the excitatory connection from neuron j to neuron i. <p> Then neuron j is more likely to become activated than i, because j can suppress i's activity. However, when j does become activated, the rule causes z ji to weaken, heading toward restored symmetry. The inhibitory learning rule of equation (7) is an outstar <ref> (Grossberg, 1982b) </ref> rule, unlike the excitatory learning rule of equation (6), which is an instar rule.
Reference: <author> Grossberg, S. </author> <year> (1986). </year> <title> The adaptive self-organization of serial order in behavior: Speech, language, and motor control. In E.C. </title> <editor> Schwab & H.C. Nusbaum (Eds.), </editor> <booktitle> Pattern recognition by humans and machines, </booktitle> <volume> vol. 1: </volume> <booktitle> Speech recognition. </booktitle> <address> San Diego: </address> <publisher> Academic Press. </publisher>
Reference-contexts: If the normalization parameter is (unrealistically) viewed as a neuron's physical size, then each neuron can also be imagined as having a finite dendritic surface area available for implantation of synaptic receptor sites <ref> (Cohen & Grossberg, 1986) </ref>. If a neuron's receptor area is already fully allocated, then the neuron must physically grow to increase that area when additional input connections are required. Then a given input signal would produce a smaller effect. <p> However, as written in their original papers <ref> (Cohen & Grossberg, 1986, 1987) </ref>, the complete computation of the overlap function is physically unrealizable unless activity correlation factors are also used. Suppose their network's Layer 1 neurons were spatially ordered: (a; b; c; d).
Reference: <author> Grossberg, S. & Marshall, J.A. </author> <year> (1989). </year> <title> Stereo boundary fusion by cortical complex cells: A system of maps, filters, and feedback networks for multiplexing distributed data. Neural Networks, </title> <type> 2, 29-51 Hebb, </type> <institution> D.O. </institution> <year> (1949). </year> <title> The organization of behavior. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Hendry, S.H.C., Fuchs, J., deBlas, A.J., & Jones, E.G. </author> <year> (1990). </year> <title> Distribution and plasticity of immunocytochemically localized GABA A receptors in adult monkey visual cortex. </title> <journal> Journal of Neuroscience, </journal> <volume> 10, 7, </volume> <pages> 2438-2450. </pages>
Reference: <author> Hildreth, E.C. </author> <year> (1983). </year> <title> Computing the velocity field along contours. </title> <booktitle> Proceedings of the ACM SIGGRAPH/SIGART Interdisciplinary Workshop on Motion, Toronto: </booktitle> <pages> 26-32. </pages> <note> 40 Hinton, </note> <author> G.E. & Becker, S. </author> <year> (1990). </year> <title> An unsupervised learning procedure that discovers surfaces in random-dot stereograms. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Washington DC, </address> <month> January </month> <year> 1990, </year> <pages> I., 218-222. </pages>
Reference: <author> Hopfield, J.J. </author> <year> (1982). </year> <title> Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 79, </volume> <pages> 2554-2558. </pages>
Reference: <author> Hough, P.V.C. </author> <year> (1962). </year> <title> Method and means for recognizing complex patterns. </title> <type> U.S. Patent 3,069,654, U.S. Patent Office. </type>
Reference: <institution> Hubel, D.H. & Wiesel, T.N. </institution> <year> (1962). </year> <title> Receptive fields, binocular interactions, and functional architecture in cat's visual cortex. </title> <journal> Journal of Physiology, </journal> <volume> 160, </volume> <pages> 106-154. </pages>
Reference-contexts: pattern-motion (Movshon, Adelson, Gizzi, & Newsome, 1985) in the aperture problem (Adelson & Movshon, 1982; Hildreth, 1983; Marr, 1982; Marr & Ullman, 1981), the development of end-stopping and length sensitivity (Hubel & Wiesel, 1977; Kato, Bishop, & Orban, 1978; Orban, Kato, & Bishop, 1979), and the development of orientation sensitivity <ref> (Hubel & Wiesel, 1962) </ref> in neurons capable of representing edge intersections (Walters, 1987).
Reference: <institution> Hubel, D.H. & Wiesel, T.N. </institution> <year> (1977). </year> <title> Functional architecture of macaque monkey visual cortex. </title> <journal> Proceedings of the Royal Society of London, Ser. B, </journal> <volume> 198, </volume> <pages> 1-59. </pages>
Reference: <author> Kato, H., Bishop, P.O., & Orban, G.A. </author> <year> (1978). </year> <title> Hypercomplex and simple/complex cell classifications in cat striate cortex. </title> <journal> Journal of Neurophysiology, </journal> <volume> 41, </volume> <pages> 1071-1095. </pages>
Reference: <author> Kersten, D. </author> <year> (1987). </year> <title> Predictability and redundancy of natural images. </title> <journal> Journal of the Optical Society of America A, </journal> <volume> 4, 12, </volume> <pages> 2395-2400. </pages>
Reference: <author> Kohonen, T. </author> <year> (1982). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43, </volume> <pages> 59-69. </pages>
Reference: <author> Kohonen, T. </author> <year> (1984). </year> <title> Self-organization and associative memory. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Lehky, S.R. & Sejnowski, T.J. </author> <year> (1990). </year> <title> Neural model of stereoacuity and depth interpolation based on a distributed representation of stereo disparity. </title> <journal> Journal of Neuroscience, </journal> <volume> 10, 7, </volume> <pages> 2281-2299. </pages>
Reference: <author> Linsker, R. </author> <year> (1986a). </year> <title> From basic network principles to neural architecture: Emergence of spatial-opponent cells. </title> <booktitle> Proceedings of the National Academy of Sciences of the U.S.A., </booktitle> <volume> 83, </volume> <pages> 7508-7512. </pages>
Reference: <author> Linsker, R. </author> <year> (1986b). </year> <title> From basic network principles to neural architecture: Emergence of orientation-selective cells. </title> <booktitle> Proceedings of the National Academy of Sciences of the U.S.A., </booktitle> <volume> 83, </volume> <pages> 8390-8394. </pages>
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> Computer, </journal> <volume> 21, </volume> <pages> 105-117. </pages>
Reference-contexts: The infrequently-activated neurons can then become active more often. The network thus tends to keep all neurons active with roughly the same frequency. This property tends to maximize the information content of each neuron's activation <ref> (Linsker, 1988) </ref>. Although the inhibitory learning rule automatically maintains a rough symmetry of reciprocal inhibitory weights, it also allows reciprocal inhibitory weights to differ temporarily until roughly equal frequency of activation is achieved. 3. Wholes vs.
Reference: <author> Marr, D. </author> <year> (1982). </year> <title> Vision: A computational investigation into the human representation and processing of visual information. </title> <address> San Francisco: </address> <publisher> W.H. Freeman and Company. </publisher>
Reference: <author> Marr, D. & Poggio, T. </author> <year> (1976). </year> <title> Cooperative computation of stereo disparity. </title> <journal> Science, </journal> <volume> 194, </volume> <pages> 283-287. </pages>
Reference: <author> Marr, D. & Ullman, S. </author> <year> (1981). </year> <title> Directional selectivity and its use in early visual processing. </title> <journal> Proceedings of the Royal Society of London, Ser. B, </journal> <volume> 211, </volume> <pages> 151-180. </pages>
Reference: <author> Marshall, J.A. </author> <year> (1989a). </year> <title> Neural networks for computational vision: Motion segmentation and stereo fusion. </title> <type> Ph.D. Dissertation, </type> <institution> Boston University Computer Science Department. Ann Arbor, Michigan: University Microfilms Inc. </institution>
Reference-contexts: Such a network could therefore be considered inefficient . 8 2.5. Inhibitory Learning How can a neural network avoid getting stuck in such inefficient structural states? A parsimonious method <ref> (Marshall, 1989ab, 1990acdef, 1991, 1992ab) </ref> achieves the desired result, using only strictly local self-organization processes. <p> One inhibitory learning rule specifies that whenever a neuron is active, its output inhibitory connections to other active neurons become gradually stronger (i.e., more inhibitory), while its output inhibitory connections to inactive neurons become gradually weaker. The inhibitory rule can also be expressed mathematically as a differential equation <ref> (Marshall, 1989ab, 1990acdf, 1991, 1992ab) </ref>. Let z ji represent the weight of the inhibitory connection from neuron j to neuron i.
Reference: <author> Marshall, J.A. </author> <year> (1989b). </year> <title> Self-organizing neural network architectures for computing visual depth from motion parallax. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Washington DC, </address> <month> June </month> <year> 1989, </year> <booktitle> II., </booktitle> <pages> 227-234. </pages>
Reference: <author> Marshall, J.A. </author> <year> (1990a). </year> <title> Self-organizing neural networks for perception of visual motion. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 45-74. </pages>
Reference: <author> Marshall, J.A. </author> <year> (1990b). </year> <title> Development of length-selectivity in hypercomplex-type cells. </title> <journal> Investigative Ophthalmology and Visual Science, </journal> <volume> 31, 4, </volume> <pages> 397. </pages>
Reference: <author> Marshall, J.A. </author> <year> (1990c). </year> <title> A self-organizing scale-sensitive neural network. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> San Diego, </address> <month> June </month> <year> 1990, </year> <booktitle> III., </booktitle> <pages> 649-654. </pages>
Reference-contexts: and abc (H), while the other Layer 2 neuron responds to neither pattern. (I) Inhibitory input weights to unused neuron gradually weaken, until (J) unused neuron begins to respond. (K) Formerly unused neuron now becomes selective for a pattern, via excitatory learning rule. (L) Inhibitory weights move toward restored symmetry. <ref> (Reprinted with permission from Marshall, 1990c.) </ref> For the network to choose the largest complete matching representation for an input pattern, the input pattern sensitivities of all the network's Layer 2 neurons must be properly balanced. 5 Incorrectly balanced connection weights could cause the network to choose a representation at too large
Reference: <author> Marshall, J.A. </author> <year> (1990d). </year> <title> Representation of uncertainty in self-organizing neural networks. </title> <booktitle> Proceedings of the International Neural Network Conference, </booktitle> <address> Paris, France, </address> <month> July </month> <year> 1990, </year> <pages> 809-812. </pages>
Reference-contexts: activation. (H) But if inhibition weight is then reduced (thinner arrows), multiple Layer 2 neurons could simultaneously respond to the ambiguous input pattern, possibly at a lower activation level (partially filled circles). (I) Then the new disambiguating information could cause the correct Layer 2 neuron to win, suppressing incorrect classifications. <ref> (Reprinted with permission from Marshall, 1990d.) </ref> slight differences in neuron input levels result in great differences in neuron activations (Figure 6C). Over a sufficient number of exposures to input patterns, each neuron tends to acquire a different sensitivity (Grossberg, 1976ab; Kohonen, 1984) (Figure 6D).
Reference: <author> Marshall, J.A. </author> <year> (1990e). </year> <title> Self-organizing neural network for computing stereo disparity and transparency. </title> <booktitle> Optical Society of America Annual Meeting Technical Digest, </booktitle> <address> Boston, </address> <month> November </month> <year> 1990, </year> <month> 268. </month>
Reference-contexts: On the other hand, an EXIN network maintains lateral feedback inhibition between neurons that code conflicting matches for any single feature and drops inhibition between neurons that 26 code matches between different features <ref> (Marshall, 1990e, 1992b) </ref>. Its exclusive allocation property thereby implements the stereo uniqueness constraint, forcing the EXIN network to assign at most a single match to each feature. In visual edge detection, the same principle applies. <p> Preliminary results have also been successful in using the EXIN learning rules to model the development of neural representation of binocular disparity surfaces <ref> (Marshall, 36 1990e, 1992b) </ref>, including stereo transparency segmentation (Prazdny, 1985; Weinshall, 1989; Lehky & Sejnowski, 1990); in this case, the selectively reduced inhibition lets the network simultaneously represent multiple transparently overlaid surfaces in depth within the same 2-D image region.

Reference: <author> Marshall, J.A. </author> <year> (1992b). </year> <title> Unsupervised learning of contextual constraints in neural networks for simultaneous visual processing of multiple objects. </title> <editor> In S.-S. Chen (Ed.), </editor> <booktitle> Neural and stochastic methods in image and signal processing, Proceedings of the SPIE 1766, </booktitle> <address> San Diego, </address> <pages> pp. 84-93. </pages>
Reference: <author> Martin, K.E. & Marshall, J.A. </author> <year> (1993). </year> <title> Unsmearing visual motion: Development of long-range horizontal intrinsic connections. </title> <editor> In S.J. Hanson, J.D. Cowan, & C.L. Giles (Eds.), </editor> <booktitle> Advances in neural information processing systems, 5, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> pp. 417-424. </pages>
Reference: <author> Movshon, J.A., Adelson, </author> <type> E.H., </type> <institution> Gizzi, M.S., & Newsome, W.T. </institution> <year> (1985). </year> <title> The analysis of moving visual patterns. </title> <editor> In C. Chagas, R. Gattass, & C. Gross (Eds.), </editor> <title> Pattern recognition mechanisms. Vatican City: </title> <journal> Pontifical Academy of Sciences, </journal> <pages> 117-151. </pages>
Reference-contexts: EXIN Network Models in Perception In vision, EXIN networks perform a context-sensitive segmentation, independently representing multiple objects or features at once. Marshall (1989ab, 1990abef, 1991, 1992b) has used EXIN networks to model several aspects of visual perception, including the development of disambiguation mechanisms for pattern-motion <ref> (Movshon, Adelson, Gizzi, & Newsome, 1985) </ref> in the aperture problem (Adelson & Movshon, 1982; Hildreth, 1983; Marr, 1982; Marr & Ullman, 1981), the development of end-stopping and length sensitivity (Hubel & Wiesel, 1977; Kato, Bishop, & Orban, 1978; Orban, Kato, & Bishop, 1979), and the development of orientation sensitivity (Hubel &
Reference: <author> Nelson, J.I. </author> <year> (1985). </year> <title> The cellular basis of perception. </title> <editor> In D. Rose & V.G. Dobson (Eds.), </editor> <title> Models of the visual cortex. </title> <address> Chichester: </address> <publisher> Wiley, </publisher> <pages> 108-122. </pages>
Reference: <author> Nelson, </author> <title> S.B. Temporal interactions in the cat visual system. I. Orientation-selective suppression in the visual cortex. </title> <journal> Journal of Neuroscience, </journal> <volume> 11, 2, </volume> <pages> 344-356. </pages>
Reference: <author> Nigrin, A. </author> <year> (1990a). </year> <title> The real-time classification of temporal sequences with an adaptive resonance circuit. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, </address> <month> January </month> <year> 1990, </year> <editor> I., </editor> <volume> 525|528. </volume>
Reference-contexts: By definition, WTA neural networks are capable of representing each input pattern only as a single, lumped item. Suppose that a simple WTA neural network has learned to recognize three patterns: ab, abc, and cd <ref> (Nigrin, 1990a) </ref> (Figure 5B). The Layer 1 neurons a; b; c; d might represent the speech sounds ^o; l; t~er; n in utterances like "all," "alter," and "turn" (Nigrin, 1993).
Reference: <author> Nigrin, A. </author> <year> (1990b). </year> <title> SONNET: A self-organizing neural network that classifies multiple patterns simultaneously. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> San Diego, </address> <month> June </month> <year> 1990, </year> <booktitle> II., </booktitle> <pages> 313-318. </pages>
Reference: <author> Nigrin, A. </author> <year> (1990c). </year> <title> The stable learning of temporal patterns with an adaptive resonance circuit. </title> <type> Ph.D. Dissertation, </type> <institution> Duke University Computer Science Department. </institution>
Reference: <author> Nigrin, A. </author> <year> (1992). </year> <title> A new architecture for achieving translational invariant recognition of objects. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Baltimore, </address> <month> June </month> <year> 1992, </year> <booktitle> III., </booktitle> <pages> 683-688. </pages>
Reference: <author> Nigrin, A. </author> <year> (1993). </year> <title> Neural networks for pattern recognition. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Suppose that a simple WTA neural network has learned to recognize three patterns: ab, abc, and cd (Nigrin, 1990a) (Figure 5B). The Layer 1 neurons a; b; c; d might represent the speech sounds ^o; l; t~er; n in utterances like "all," "alter," and "turn" <ref> (Nigrin, 1993) </ref>. When one of these input patterns (say abc) is presented to Layer 1 of this neural network (by activating neurons a, b, and c), the corresponding Layer 2 neuron (labeled abc) becomes active, thereby indicating that the pattern is recognized. <p> The behavior of the inhibitory learning rule permits EXIN networks to choose appropriate intermediate levels of inhibition and thereby represent uncertainty via coactivation of multiple neurons. Second, processing of inhibition could be specified in a manner so that inhibition causes WTA behavior only when the neuron is fully activated <ref> (Nigrin, personal communication, 1993) </ref>. One way to implement this specification would be to rewrite the inhibitory summing equation (9) with a faster-than-linear half-rectification like max (0; x j ) 2 , instead of the linear half-rectification max (0; x j ).
Reference: <author> Orban, G.A., Kato, H., & Bishop, P.O. </author> <year> (1979). </year> <title> End-zone region in receptive fields of hypercomplex and other striate neurons in the cat. </title> <journal> Journal of Neurophysiology, </journal> <volume> 42, 3, </volume> <pages> 818-832. </pages>
Reference: <author> Prazdny, K. </author> <year> (1985). </year> <title> Detection of binocular disparities. </title> <journal> Biological Cybernetics, </journal> <volume> 52, </volume> <pages> 93-99. </pages>
Reference: <author> Price, D.J. & Zumbroich, T.J. </author> <year> (1989). </year> <title> Postnatal development of corticocortical efferents from area 17 in the cat's visual cortex. </title> <journal> Journal of Neuroscience, </journal> <volume> 9, 2, </volume> <pages> 600-613. </pages>
Reference: <author> Reeke, G.N., Finkel, L.H., & Edelman, G.M. </author> <year> (1990). </year> <title> Selective recognition automata. </title> <editor> In S.F. Zornetzer, J.L. Davis, & C. Lau (Eds.), </editor> <title> An introduction to neural and electronic networks, </title> <address> New York: </address> <publisher> Academic Press, </publisher> <pages> 203-226. </pages>
Reference-contexts: Unlike supervised learning techniques, such as backpropagation (Rumelhart, Hinton, & Williams, 1986; Werbos, 1974), unsupervised methods have the advantage of not requiring that an external "teacher" be available to program the system; in the lower levels of animal perceptual systems no external teacher is available <ref> (Reeke, Finkel, & Edelman, 1990) </ref>. The ability of unsupervised SONNs to adapt or learn illustrates how certain perceptual response properties might be acquired in animal brains, without requiring detailed genetic specification of the full cortical wiring plan (Marshall, 1990a; Price & Zumbroich, 1989).
Reference: <author> Rubner, J. & Schulten, K. </author> <year> (1990). </year> <title> Development of feature detectors by self-organization. </title> <journal> Biological Cybernetics, </journal> <volume> 62, </volume> <pages> 193-199. </pages>
Reference: <author> Rumelhart, D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning representations by backpropagating errors. </title> <journal> Nature, </journal> <volume> 323, </volume> <pages> 533-536. </pages>
Reference: <author> Sattath, S. & Tversky, A. </author> <year> (1987). </year> <title> On the relation between common and distinctive feature models. </title> <journal> Psychological Review, </journal> <volume> 94, 1, </volume> <pages> 16-22. </pages>
Reference-contexts: The neuron labeled cd becomes wired to respond optimally to pattern d. As a consequence, the linear decorrelator network does not activate the closest match to some unfamiliar patterns, such as pattern c (Figure 5I). On the other hand, EXIN networks become wired to represent the common features <ref> (Sattath & Tversky, 1987) </ref> among the input patterns, as illustrated in Figure 5D,G,J. Besides the linear decorrelator, Foldiak (1990, 1992) has more recently described a nonlinear decorrelator that incorporates an additive adaptive threshold.
Reference: <author> Sereno, M.E. </author> <year> (1986). </year> <title> Neural network model for the measurement of visual motion. </title> <journal> Journal of the Optical Society of America A, </journal> <volume> 3, 13, </volume> <month> P72. </month>
Reference: <author> Sereno, M.E. </author> <year> (1987). </year> <title> Implementing stages of motion analysis in neural networks. </title> <booktitle> Program of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum 42 Associates, </publisher> <pages> 405-416. </pages>
Reference: <author> Sereno, M.E., Kersten, D.J., & Anderson, J.A. </author> <year> (1988). </year> <title> A neural network model of an aspect of motion perception. Science at the John von Neumann National Supercomputer Center: </title> <booktitle> Annual Report FY 1988, </booktitle> <pages> 173-178. </pages>
Reference: <author> Sereno, M.I. </author> <year> (1989). </year> <title> Learning the solution to the aperture problem for pattern motion with a Hebb rule. </title> <editor> In D. Touretzky (Ed.), </editor> <booktitle> Advances in neural information processing systems, 1, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 468-476. </pages>
Reference: <author> Sereno, M.I. & Sereno, M.E. </author> <year> (1990). </year> <title> Learning to see rotation and dilation with a Hebb rule. </title> <editor> In R.P. Lippmann, J.E. Moody, & D. Touretzky (Eds.), </editor> <booktitle> Advances in neural information processing systems, 3, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 320-326. </pages>
Reference: <author> Singer, W. </author> <year> (1983). </year> <title> Neuronal activity as a shaping factor in the self-organization of neuron assemblies. </title> <editor> In E. Basar, H. Flohr, H. Haken, & A. J. Mandell (Eds.), </editor> <booktitle> Synergetics of the brain. </booktitle> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Singer, W. </author> <year> (1985). </year> <title> Activity-dependent self-organization of the mammalian visual cortex. </title> <editor> In D. Rose & V.G. Dobson (Eds.), </editor> <title> Models of the visual cortex. </title> <address> New York: </address> <publisher> Wiley, </publisher> <pages> 123-136. </pages>
Reference: <author> Soodak, R.E. </author> <year> (1991). </year> <title> Reverse-Hebb plasticity leads to optimization and association in a simulated visual cortex. </title> <journal> Visual Neuroscience, </journal> <volume> 6, </volume> <pages> 507-518. </pages>
Reference: <author> Sun, G.Z., Chen, H.H., & Lee, Y.C. </author> <year> (1987). </year> <title> Learning stereopsis with neural networks. </title> <type> Preprint. </type>
Reference: <author> Szeliski, R.S. </author> <year> (1988). </year> <title> Bayesian modeling of uncertainty in low-level vision. </title> <type> Ph.D. Dissertation, Technical Report CMU-CS-88-169, </type> <institution> Carnegie Mellon University Computer Science Department. </institution>
Reference: <author> Van den Bout, D. & Miller, </author> <title> T.K. </title> <booktitle> (1989). TInMANN: The integer Markovian artificial neural network. Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Washington DC, </address> <month> June </month> <year> 1989, </year> <booktitle> II., </booktitle> <pages> 205-211. </pages>
Reference: <author> Voigt, H.F. & Young, E.D. </author> <year> (1990). </year> <title> Cross-correlation analysis of inhibitory interactions in dorsal cochlear nucleus. </title> <journal> Journal of Neurophysiology, </journal> <volume> 64, 5, </volume> <pages> 1590-1610. </pages> <editor> von der Malsburg, C. </editor> <year> (1973). </year> <title> Self-organization of orientation sensitive cells in the striate cortex. </title> <journal> Kybernetik, </journal> <volume> 14, </volume> <pages> 85-100. </pages>
Reference-contexts: Nelson, 1985; S.B. Nelson, 1991), and auditory pitch detection, where inhibition declines with preferred frequency dissimilarity <ref> (Voigt & Young, 1990) </ref>. 8.9. EXIN Network Models in Perception In vision, EXIN networks perform a context-sensitive segmentation, independently representing multiple objects or features at once.
Reference: <author> Walters, D. </author> <year> (1987). </year> <title> Rho-space: A neural network for the detection and representation of oriented edges. </title> <booktitle> Proceedings of the First International Conference on Neural Networks, </booktitle> <address> San Diego, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: (Adelson & Movshon, 1982; Hildreth, 1983; Marr, 1982; Marr & Ullman, 1981), the development of end-stopping and length sensitivity (Hubel & Wiesel, 1977; Kato, Bishop, & Orban, 1978; Orban, Kato, & Bishop, 1979), and the development of orientation sensitivity (Hubel & Wiesel, 1962) in neurons capable of representing edge intersections <ref> (Walters, 1987) </ref>.
Reference: <author> Watson, A.B. </author> <year> (1987). </year> <title> Efficiency of a model human image code. </title> <journal> Journal of the Optical Society of America A, </journal> <volume> 4, 12, </volume> <pages> 2401-2417. </pages>
Reference: <author> Weinshall, D. </author> <year> (1989). </year> <title> Perception of multiple transparent planes in stereo vision. </title> <journal> Nature, </journal> <volume> 341, </volume> <pages> 737-739. </pages>
Reference: <author> Werbos, P. </author> <year> (1974). </year> <title> Beyond regression: New tools for prediction and analysis in the behavioral sciences. </title> <type> Ph.D. Dissertation, </type> <institution> Harvard University. </institution>
Reference: <author> Whitsel, B.L., Favorov, O.V., Kelly, D.G., & Tommerdahl, M. </author> <year> (1990). </year> <title> Mechanisms of dynamic peri- and intra-columnar interactions in somatosensory cortex: Stimulus-specific contrast enhancement by NMDA receptor activation. </title> <editor> In O. Franzen & J. Westman (Eds.), </editor> <booktitle> Information processing in the somatosensory system. </booktitle> <address> London: </address> <publisher> Macmillan Press. </publisher>
Reference: <author> Wilson, H.R. </author> <year> (1988). </year> <title> Development of spatiotemporal mechanisms in infant vision. </title> <journal> Vision Research, </journal> <volume> 28, 5, </volume> <pages> 611-628. </pages>
References-found: 100

