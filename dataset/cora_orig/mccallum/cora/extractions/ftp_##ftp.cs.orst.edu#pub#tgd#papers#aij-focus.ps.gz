URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/aij-focus.ps.gz
Refering-URL: http://ai.iit.nrc.ca/bibliographies/feature-selection.html
Root-URL: 
Email: E-mail: facp07b@saupm00.bitnet  E-mail: tgd@cs.orst.edu  
Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features  
Author: Hussein Almuallim Thomas G. Dietterich 
Keyword: Abbreviated title: Learning with many irrelevant features.  
Note: Correspondence to: Hussein Almuallim,  
Address: Dhahran 31261, Saudi Arabia  Corvallis, OR 97331-3202, U.S.A.  Dhahran 31261, Saudi Arabia.  
Affiliation: Department of Information and Computer Science King Fahd University of Petroleum and Minerals  Department of Computer Science Oregon State University  Department of Information and Computer Science, King Fahd University of Petroleum and Minerals,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> H. Almuallim, </author> <title> Exploiting Symmetry Properties in the Evaluation of Inductive Learning Algorithms: An Empirical Domain-Independent Comparative Study, </title> <type> Technical Report, </type> <institution> 91-30-09, Dept. of Computer Science, Oregon State University, Corvallis, Oregon (1991). </institution>
Reference: [2] <author> H. Almuallim and T. G. Dietterich, </author> <title> Learning With Many Irrelevant Features, </title> <booktitle> in: Proceedings AAAI-91, </booktitle> <address> Anaheim, California (1991) 547-552. </address>
Reference: [3] <author> H. Almuallim, </author> <title> Concept Coverage and Its Application to Two Learning Tasks, </title> <type> Ph.D. Thesis, </type> <institution> Oregon State University, Corvallis, Oregon (1992). </institution>
Reference: [4] <author> H. Almuallim and T. G. Dietterich, </author> <title> Efficient Algorithms for Identifying Relevant Features, </title> <booktitle> in: Proceedings of the Ninth Canadian Conference on Artificial Intelligence, </booktitle> <address> Van-couver, British Columbia (1992) 38-45. </address>
Reference: [5] <author> W. L. Buntine, </author> <title> Myths and legends in learning classification rules, </title> <booktitle> in: Proceedings AAAI-90, </booktitle> <address> Boston, Massachusetts (1990) 736-742. </address>
Reference-contexts: Often, it is difficult even to state the bias in any simple way. Consequently, it is difficult to tell in advance whether the bias is appropriate for a new learning problem. Recently, a few authors <ref> [5, 24] </ref> have advocated a different procedure: (i) adopt a bias over some space of hypotheses (or, equivalently, select a prior probability distribution over the space), (ii) select a scheme for representing hypotheses in the space, and (iii) design an algorithm that implements this bias, at least approximately.
Reference: [6] <author> A. Blumer, A. Ehrenfeucht, D. Haussler and M. Warmuth, </author> <title> Learnability and the Vapnik-Chervonenkis Dimension, </title> <journal> J. ACM, </journal> <month> 36(4) </month> <year> (1989) </year> <month> 929-965. </month>
Reference-contexts: In other words, a consistent learning algorithm is an algorithm whose hypothesis never disagrees with the training sample. In this work, we adopt the notion of Probably Approximately Correct (PAC) learning as defined by Blumer et al. <ref> [6] </ref>. Let L be a learning algorithm and let * and ffi be such that 0 &lt; *; ffi &lt; 1. <p> We now show that this bound is tight by exhibiting an identical lower bound using the methods developed by Blumer et al. <ref> [6] </ref> exploiting the Vapnik-Chervonenkis dimension (VC-dimension). The VC-dimension of a class of concepts C is defined to be the largest integer d such that there exists a set of d instances that can be labeled by the concepts in C in all the 2 d possible ways. <p> Blumer et al. show that the number of examples needed for learning any class of concepts strongly depends on the VC-dimension of the class <ref> [6] </ref>. Specifically, Ehrenfeucht et al. [9] prove the following: Theorem 2 Let C be a class of concepts and 0 &lt; *; ffi &lt; 1. <p> As our measure of performance, we employ the sample complexity|the minimum number of training examples needed to ensure that every concept in the class can be learned in the PAC sense <ref> [6] </ref>. We estimate the sample complexity with respect to fixed learning parameters p; n; *; and ffi and with training samples drawn according to the uniform distribution. In the second experiment, we are interested in the average-case performance of the algorithms.
Reference: [7] <author> A. Blumer, A. Ehrenfeucht, D. Haussler and M. Warmuth, </author> <title> Occam's Razor, </title> <journal> Information Processing Letters, </journal> <month> 24 </month> <year> (1987) </year> <month> 377-380. </month>
Reference-contexts: T c is the right-most column of the truth table of a Boolean function f that represents c defined only on those features in fx 1 ; x 2 ; ; x n g, whose corresponding bits in R c are set to 1. Following Blumer et al. <ref> [7] </ref>, we will let the complexity s (c) for concept c be the number of bits needed to encode c using the above bit-vector representation. <p> To derive an upper bound on the sample complexity of learning algorithms that implement the MIN-FEATURES bias, we make use of the following well-known lemma given by Blumer et al. <ref> [7] </ref>. Lemma 1 Let C be a set of concepts and let L be a consistent learning algorithm which always chooses its hypothesis from C. <p> We conclude with the following open problems: * Is the class C n;s polynomially learnable in the sense of <ref> [7] </ref>? Namely, does there exist a learning algorithm L such that, under any probability distribution, L PAC-learns with respect to parameters * and ffi an arbitrary concept in C n;s using a sample of size polynomial in n; s; 1 * and 1 ffi , and such that L runs in
Reference: [8] <author> V. Chvatal, </author> <title> A Greedy Heuristic For the Set Covering Problem, </title> <journal> Math. Oper. Res., </journal> <volume> 4(3) (1979) 233-235. Vol. 4, No. 3, </volume> <year> 1979. </year>
Reference-contexts: To implement this algorithm, one needs to avoid explicit partitioning of the training sample into 2 jQj groups. We explain how this can be done in the appendix. 6.2 The Simple-Greedy (SG) Algorithm The Simple-Greedy algorithm is based on the well-known greedy heuristic for the minimum set cover problem <ref> [8] </ref>. Starting with the set of all conflicts, the algorithm chooses each time the feature that covers the largest number of conflicts that are not yet covered. The conflicts that are covered by this feature are then removed from the set of conflicts.
Reference: [9] <author> A. Ehrenfeucht, D. Haussler, M. Kearns and L. G. Valiant, </author> <title> A General Lower Bound on the Number of Examples Needed for Learning, </title> <booktitle> in: Proceedings of the First Workshop on Computational Learning Theory (Morgan Kaufmann, </booktitle> <address> San Mateo, CA, </address> <year> 1988) </year> <month> 139-154. </month>
Reference-contexts: Blumer et al. show that the number of examples needed for learning any class of concepts strongly depends on the VC-dimension of the class [6]. Specifically, Ehrenfeucht et al. <ref> [9] </ref> prove the following: Theorem 2 Let C be a class of concepts and 0 &lt; *; ffi &lt; 1.
Reference: [10] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness (Freeman, </title> <address> San Francisco, CA, </address> <year> 1979). </year>
Reference-contexts: 1 ; x 3 ; x 4 g is sufficient to form a consistent hypothesis (e.g., x 1 x 3 _ (x 3 x 4 )), and that all subsets of features of cardinality less than 3 are insufficient. 2 The minimum set cover problem is known to be NP-hard <ref> [10] </ref>. At first glance, this may appear to mean that it is not possible to implement the MIN-FEATURES bias in polynomial time (unless P=NP). However, one should be careful before drawing such a conclusion.
Reference: [11] <author> M. Ichino, and J. Sklansky, </author> <title> Optimum Feature Selection by Zero-One Integer Programming, </title> <journal> IEEE Trans. Sys. Man & Cyb., </journal> <month> SMC-14(5) </month> <year> (1984) </year> <month> 737-746. </month>
Reference-contexts: For example, [14] shows methods for selecting a small subset of features that optimizes the expected error of the nearest neighbor classifier. Similar work has addressed feature selection for the Box classifier <ref> [11] </ref>, the linear classifier [12] and the Bayes classifier [22].
Reference: [12] <author> M. Ichino, and J. Sklansky, </author> <title> Feature Selection for Linear Classifiers, </title> <booktitle> in: The Seventh International Conference on Pattern Recognition, </booktitle> <year> (1984) </year> <month> 124-127. 21 </month>
Reference-contexts: For example, [14] shows methods for selecting a small subset of features that optimizes the expected error of the nearest neighbor classifier. Similar work has addressed feature selection for the Box classifier [11], the linear classifier <ref> [12] </ref> and the Bayes classifier [22]. Other work (aimed at removing feature redundancy when features are highly correlated) is based on performing 4 a principal components analysis to find a reduced set of new uncorrelated features defined by combining the original features using the eigenvectors [18, 19].
Reference: [13] <author> M. Kearns, and M. Li, </author> <title> Learning in the Presence of Malicious Errors, </title> <booktitle> in: Proceedings of the 20th Annual ACM Symposium on the Theory of Computation, </booktitle> <address> Chicago, Illinois (1988) 267-279. </address>
Reference-contexts: In practice, however, training examples are often subject to various kinds of noise affecting the values of the features and/or the classification of the training examples <ref> [13, 15] </ref>. A direct way to deal with noise is to modify the given algorithms by relaxing the requirement of covering all the conflicts generated from the training data.
Reference: [14] <author> J. Kittler, </author> <title> Computational Problems of Feature Selection Pertaining to Large Data Sets, </title> <editor> in: E. S. Gelsma and L. N. Kanal, eds., </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <publisher> (North-Holland, </publisher> <address> Amsterdam, </address> <year> 1980) </year> <month> 405-414. </month>
Reference-contexts: For example, <ref> [14] </ref> shows methods for selecting a small subset of features that optimizes the expected error of the nearest neighbor classifier. Similar work has addressed feature selection for the Box classifier [11], the linear classifier [12] and the Bayes classifier [22].
Reference: [15] <author> P. Laird, </author> <title> Learning from Good and Bad Data (Klawer Academic, </title> <address> Boston, Massachusetts, </address> <year> 1988). </year>
Reference-contexts: In practice, however, training examples are often subject to various kinds of noise affecting the values of the features and/or the classification of the training examples <ref> [13, 15] </ref>. A direct way to deal with noise is to modify the given algorithms by relaxing the requirement of covering all the conflicts generated from the training data.
Reference: [16] <author> N. Littlestone, </author> <title> Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm, </title> <booktitle> Machine Learning, </booktitle> <month> 2 </month> <year> (1988) </year> <month> 285-318. </month>
Reference-contexts: This applies, for example, to the task of learning diagnosis rules for several different diseases from the medical records of a large number of patients. These records usually contain more information than is actually required for describing each disease. Another example (given in <ref> [16] </ref>) involves pattern recognition tasks in which feature detectors automatically extract a large number of features for the learner's consideration, not knowing which might prove useful.
Reference: [17] <author> T. M. Mitchell, </author> <title> Generalization as Search. </title> <booktitle> Artif. Intell., </booktitle> <month> 18 </month> <year> (1982) </year> <month> 203-226. </month>
Reference-contexts: Given a training sample S of some unknown target concept c, let V be the set of all possible hypotheses consistent with S. (V is sometimes called the version space; <ref> [17] </ref>) Let H be the subset of V whose elements have the fewest relevant features. The MIN-FEATURES bias chooses its guess, h, from H arbitrarily. Note that the MIN-FEATURES bias is "incomplete" in the sense that it does not necessarily lead to a unique hypothesis.
Reference: [18] <author> S. D. Morgera, </author> <title> Computational Complexity and VLSI Implementation of an Optimal Feature Selection Strategy, </title> <editor> in: E. S. Gelsma and L. N. Kanal, eds., </editor> <title> Pattern Recognition in Practice II (Elsevier Science Publishers B.V., </title> <publisher> North-Holland, </publisher> <year> 1986) </year> <month> 389-400. </month>
Reference-contexts: Other work (aimed at removing feature redundancy when features are highly correlated) is based on performing 4 a principal components analysis to find a reduced set of new uncorrelated features defined by combining the original features using the eigenvectors <ref> [18, 19] </ref>.
Reference: [19] <author> A. N. Mucciardi and E. E. Gose, </author> <title> A Comparison of Seven Techniques for Choosing Subsets of Pattern Recognition Properties, </title> <journal> IEEE Trans. Computers, </journal> <month> C-20(9) </month> <year> (1971) </year> <month> 1023-1031. </month>
Reference-contexts: Other work (aimed at removing feature redundancy when features are highly correlated) is based on performing 4 a principal components analysis to find a reduced set of new uncorrelated features defined by combining the original features using the eigenvectors <ref> [18, 19] </ref>.
Reference: [20] <author> P. M. Narendra and K. Fukunaga, </author> <title> A Branch and Bound Algorithm for Feature Subset Selection, </title> <journal> IEEE Trans. Computers, </journal> <month> C-26(9) </month> <year> (1977) </year> <month> 917-922. </month>
Reference: [21] <author> G. Pagallo and D. Haussler, </author> <title> Boolean Feature Discovery in Empirical Learning, </title> <booktitle> Machine Learning, </booktitle> <month> 5(1) </month> <year> (1990) </year> <month> 71-100. </month>
Reference-contexts: For example, ID3 [23] has a bias in favor of small decision trees, and small trees would seem to test only a subset of the input features. In the experimental part of this work, we compare ID3 and FRINGE <ref> [21] </ref> to our algorithms. The experiments demonstrate that ID3 and FRINGE do not provide good approximations to the MIN-FEATURES bias|these algorithms often produce hypotheses as output that are much more complex (in terms of the number of input features used) than the hypotheses found by the algorithms we propose.
Reference: [22] <author> C. E. Queiros and E. S. Gelsma, </author> <title> On Feature Selection, </title> <booktitle> in: Proceedings of the Seventh International Conference on Pattern Recognition, </booktitle> <year> (1984) </year> <month> 128-130. </month>
Reference-contexts: For example, [14] shows methods for selecting a small subset of features that optimizes the expected error of the nearest neighbor classifier. Similar work has addressed feature selection for the Box classifier [11], the linear classifier [12] and the Bayes classifier <ref> [22] </ref>. Other work (aimed at removing feature redundancy when features are highly correlated) is based on performing 4 a principal components analysis to find a reduced set of new uncorrelated features defined by combining the original features using the eigenvectors [18, 19].
Reference: [23] <author> J. R. Quinlan, </author> <title> Induction of Decision Trees, </title> <booktitle> Machine Learning, </booktitle> <month> 1(1) </month> <year> (1986) </year> <month> 81-106. </month>
Reference-contexts: This bias can be implemented in two steps: First, we identify the smallest subset of features that is sufficient to construct a hypothesis consistent with the given training examples. Then, we apply some learning procedure (e.g., ID3 <ref> [23] </ref>) that focuses on just those chosen features. For identification of the smallest sufficient subset of features we describe two algorithms. The first algorithm, FOCUS-1, is a straightforward algorithm that runs in time quasi-polynomial in the appropriate learning parameters. <p> Particularly, the Weighted-Greedy algorithm is shown to provide an excellent approximation of the FOCUS algorithms but with substantially lower computational costs. At first glance, it may appear that there are already many algorithms that approximate the MIN-FEATURES bias. For example, ID3 <ref> [23] </ref> has a bias in favor of small decision trees, and small trees would seem to test only a subset of the input features. In the experimental part of this work, we compare ID3 and FRINGE [21] to our algorithms. <p> In general, Q is a sufficient set if and only if no such a pair appears in the training sample. Given a sufficient subset of features, it is easy to construct a consistent hypothesis. For example, the algorithm ID3 <ref> [23] </ref> can be applied to the training sample but restricted to consider only the features in the given subset.
Reference: [24] <author> D. Wolpert, </author> <title> A Mathematical Theory of Generalization: Parts I and II, </title> <journal> Complex Systems, </journal> <month> 4(2) </month> <year> (1990) </year> <month> 151-249. </month>
Reference-contexts: Often, it is difficult even to state the bias in any simple way. Consequently, it is difficult to tell in advance whether the bias is appropriate for a new learning problem. Recently, a few authors <ref> [5, 24] </ref> have advocated a different procedure: (i) adopt a bias over some space of hypotheses (or, equivalently, select a prior probability distribution over the space), (ii) select a scheme for representing hypotheses in the space, and (iii) design an algorithm that implements this bias, at least approximately.
References-found: 24

