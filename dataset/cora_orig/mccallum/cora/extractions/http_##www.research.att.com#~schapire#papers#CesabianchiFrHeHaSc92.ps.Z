URL: http://www.research.att.com/~schapire/papers/CesabianchiFrHeHaSc92.ps.Z
Refering-URL: http://www.research.att.com/~schapire/publist.html
Root-URL: 
Title: How to Use Expert Advice  
Author: Nicolo Cesa-Bianchi Yoav Freund David Haussler David P. Helmbold Robert E. Schapire Manfred K. Warmuth k 
Note: Journal of the Association for Computing Machinery, 44(3):427-485, 1997.  
Abstract: We analyze algorithms that predict a binary value by combining the predictions of several prediction strategies, called experts. Our analysis is for worst-case situations, i.e., we make no assumptions about the way the sequence of bits to be predicted is generated. We measure the performance of the algorithm by the difference between the expected number of mistakes it makes on the bit sequence and the expected number of mistakes made by the best expert on this sequence, where the expectation is taken with respect to the randomization in the predictions. We show that the minimum achievable difference is on the order of the square root of the number of mistakes of the best expert, and we give efficient algorithms that achieve this. Our upper and lower bounds have matching leading constants in most cases. We then show how this leads to certain kinds of pattern recognition/learning algorithms with performance bounds that improve on the best results currently known in this context. We also compare our analysis to the case in which log loss is used instead of the expected number of mistakes.
Abstract-found: 1
Intro-found: 1
Reference: [BEHW89] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: Although the cardinality of H may be infinite, the number of possible binary labelings of the sequence that agree with some function in H is always finite, and in fact, is polynomial in ` if the VC dimension of H is finite (see <ref> [BEHW89] </ref> or [Vap82] for a definition of the VC dimension and its relation to this kind of learning problem).
Reference: [BM93] <author> L. Birge and P. Massart. </author> <title> Rates of convergence for minimum contrast estimators. Probability Theory and Related Fields, </title> <booktitle> 97 </booktitle> <pages> 113-150, </pages> <year> 1993. </year>
Reference-contexts: This kind of relative performance measure is called regret in statistics. General solutions to this regret formulation of the pattern recognition problem have been developed by Vapnik [Vap82], Birge and Massart <ref> [BM93] </ref>, and others. <p> Outside of the pattern recognition literature, this type of problem might be called by many names, such as L 1 regression with a regret formulation of the loss function (in typical statistics literature, see e.g. <ref> [BM93] </ref>), or, as mentioned in the introduction, the agnostic version of PAC learning [KSS94]. The terminology we use here is that from the PAC learning literature. <p> That is, the learning algorithm attempts to minimize 10 the regret E s~D ` (er D (A (s))) er D (H): (15) Bounds on this regret for certain types of learning algorithms can be obtained from the work of Vapnik [Vap82] and Birge and Massart <ref> [BM93] </ref>. The basic idea of their learning algorithms is to predict according to the single hypothesis that suffers the minimal loss over the sample of instances presented to the learner. Vapnik calls this empirical risk minimization. <p> Results for this setting are given in the next section, as is the proof of Theorem 24. The bounds given in Theorem 24 are better than those obtained for this kind of pattern recognition problem by the only other methods that we are aware of <ref> [Vap92, Tal94, BM93] </ref>. Bounds given by Vapnik ([Vap92], Equation (11)) imply a bound in the same form as the second bound in Theorem 24, but with an additional factor of 2 in the leading term. However, Vapnik's bounds hold in more general cases than the one we consider here. <p> It is not clear that this unspecified constant can be made small enough to get practical bounds for small sample size `. Bounds obtained by Birge and Massart also contain constants that are difficult to bound <ref> [BM93] </ref>. Thus our approach to the pattern recognition problem through worst case analysis of the sequence prediction problem appears to be a fruitful one. 5.4 The hold-one-out model of prediction and proof of Theorem 24 In this subsection we discuss a slightly different prediction problem.
Reference: [CBFH + 93] <author> Nicolo Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haussler, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the 43 Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <year> 1993. </year>
Reference-contexts: j + j (1 ff)F i;t (1 ff)y t j) = E y min (ffjjE i yjj 1 + (1 ff)jjF i yjj 1 ) E y (ff min jjE i yjj 1 + (1 ff) min jjF i yjj 1 ) 5 In an earlier version of this paper <ref> [CBFH + 93] </ref>, we incorrectly claimed that the same analysis also applied to all simulatable experts, i.e., experts whose predictions can be calculated as a function only of the preceding outcomes. 11 where the second equality follows from a case analysis of y t = 0 and y t = 1,
Reference: [CBFHW96] <author> N. Cesa-Bianchi, Y. Freund, </author> <title> D.P. Helmbold, and M.K. Warmuth. On-line prediction and conversion strategies. </title> <booktitle> Machine Learning, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Curiously enough, the denominator of ln 2 1+fi is obtained by the Weighted Majority algorithm of Littlestone and Warmuth [LW94] which assumes that the outcomes are binary and predicts binary as well (See <ref> [CBFHW96] </ref> for a detailed treatment of the case when the outcomes are binary). 4.4 Performance for bounded L E So far we have ignored the issue of how fi is chosen. <p> When L E (y) is replaced by K, the 20 upper bound from Theorem 10 can be written L (fi) = 2 ln 2 : It has been shown by Vovk and others <ref> [Vov90, CBFHW96] </ref> that L fl = inf fL (fi) : 0 fi &lt; 1g is the unique value of L satisfying L = 2 2L ; where H (p) is the binary entropy, p log 2 (p) (1 p) log 2 (1 p). <p> This minimum is achieved when fi = K 2L fl K . However, it is difficult to explicitly solve for L fl and the corresponding fi. A recent paper by Cesa-Bianchi et.al. <ref> [CBFHW96] </ref> shows how binary search can be used to choose a value for fi that yields the bound dL fl e.
Reference: [Chu94] <author> Thomas H. Chung. </author> <title> Approximate methods for sequential decision making using expert advice. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 183-189, </pages> <year> 1994. </year>
Reference: [Cov65] <author> Thomas M. </author> <title> Cover. Behaviour of sequential predictors of binary sequences. </title> <booktitle> In Transactions of the Fourth Prague Conference on Information Theory, Statistical Decision Functions, Random Processes, </booktitle> <pages> pages 263-272. </pages> <publisher> Publishing House of the Czechoslovak Academy of Sciences, </publisher> <year> 1965. </year>
Reference-contexts: In this case Theorem 5 implies that the loss of the optimal algorithm MS is worse than the loss of the best expert by the following amount : ` 2 ` i=0 ` ! minfi; ` ig ~ s 2 This result was previously proved by Cover <ref> [Cov65] </ref>; we obtain it as a special case. Strategy MS makes each prediction in terms of the expected loss of the best expert on the remaining trials (where the expectation is taken over the uniformly random choice of outcomes for these trials).
Reference: [CS77] <author> T. M. Cover and A. Shanhar. </author> <title> Compound Bayes predictors for sequences with apparent Markov structure. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-7(6):421-424, </volume> <month> June </month> <year> 1977. </year>
Reference: [Daw84] <author> A. P. Dawid. </author> <title> Statistical theory: The prequential approach. </title> <journal> Journal of the Royal Statistical Society, Series A, </journal> <pages> pages 278-292, </pages> <year> 1984. </year>
Reference-contexts: We then give applications of these results to the theory of pattern recognition [Vap82] and PAC learning [Val84]. We take the extreme position, as advocated by Dawid and Vovk in the theory of prequential probability <ref> [Daw84, Dawar, Daw91, Vov93] </ref>, Rissanen in his theory of stochastic complexity [Ris78, RL81, Ris86, Yam95] and Cover, Lempel and Ziv, Feder and others in the theory of universal prediction and data compression of individual sequences [FMG92, MF93, Cov65, CS77, Han57, fl Universita di Milano (Italy), cesabian@dsi.unimi.it.
Reference: [Daw91] <author> A. Dawid. </author> <title> Prequential analysis, stochastic complexity and Bayesian inference. </title> <booktitle> In Bayesian Statistics 4, </booktitle> <pages> pages 109-125. </pages> <publisher> Oxford University Press, </publisher> <year> 1991. </year>
Reference-contexts: We then give applications of these results to the theory of pattern recognition [Vap82] and PAC learning [Val84]. We take the extreme position, as advocated by Dawid and Vovk in the theory of prequential probability <ref> [Daw84, Dawar, Daw91, Vov93] </ref>, Rissanen in his theory of stochastic complexity [Ris78, RL81, Ris86, Yam95] and Cover, Lempel and Ziv, Feder and others in the theory of universal prediction and data compression of individual sequences [FMG92, MF93, Cov65, CS77, Han57, fl Universita di Milano (Italy), cesabian@dsi.unimi.it.
Reference: [Dawar] <author> A.P. Dawid. </author> <title> Prequential data analysis. Current Issues in Statistical Inference, </title> <note> to appear. </note>
Reference-contexts: We then give applications of these results to the theory of pattern recognition [Vap82] and PAC learning [Val84]. We take the extreme position, as advocated by Dawid and Vovk in the theory of prequential probability <ref> [Daw84, Dawar, Daw91, Vov93] </ref>, Rissanen in his theory of stochastic complexity [Ris78, RL81, Ris86, Yam95] and Cover, Lempel and Ziv, Feder and others in the theory of universal prediction and data compression of individual sequences [FMG92, MF93, Cov65, CS77, Han57, fl Universita di Milano (Italy), cesabian@dsi.unimi.it.
Reference: [DMW88] <author> Alfredo DeSantis, George Markowski, and Mark N. Wegman. </author> <title> Learning probabilistic prediction functions. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 312-328. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: is well known that for the log loss, for any set E of N experts there is a prediction strategy A such that for any sequence y, L A (y) L E (y) log N; where L E (y) is the total log loss of the best expert for y <ref> [Ris86, DMW88, Vov92, HB92, Yam95, KW94] </ref>. 12 The strategy is just the Bayes algorithm with uniform prior on the distributions represented by the experts. A min/max optimal prediction algorithm is known for the case where the experts are simulatable and `, the number of iterations, is known in advance.
Reference: [FFK + 91] <author> A. Fiat, D. Foster, H. Karloff, Y. Rabani, Y. Ravid, and S. Vishwanathan. </author> <title> Competitive algorithms for layered graph traversal. </title> <booktitle> In 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 288-297, </pages> <year> 1991. </year>
Reference-contexts: An alternate logarithmic loss function, often considered in the literature, is discussed briefly in Section 8. 2 This approach is also related to that taken in recent work on the competitive ratio of on-line algorithms, and in particular to work on combining on-line algorithms to obtain the best competitive ratio <ref> [FKL + 91, FFK + 91, FRR94] </ref>, except that we look at the difference in performance rather than the ratio. 2 actual weather turns out to be.
Reference: [FKL + 91] <author> A. Fiat, R. Karp, M. Luby, L. McGeoch, D. Sleator, and N. Young. </author> <title> Competitive paging algorithms. </title> <journal> Journal of Algorithms, </journal> <volume> 12 </volume> <pages> 685-699, </pages> <year> 1991. </year>
Reference-contexts: An alternate logarithmic loss function, often considered in the literature, is discussed briefly in Section 8. 2 This approach is also related to that taken in recent work on the competitive ratio of on-line algorithms, and in particular to work on combining on-line algorithms to obtain the best competitive ratio <ref> [FKL + 91, FFK + 91, FRR94] </ref>, except that we look at the difference in performance rather than the ratio. 2 actual weather turns out to be.
Reference: [FMG92] <author> M. Feder, N. Merhav, and M. Gutman. </author> <title> Universal prediction of individual sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 1258-1270, </pages> <year> 1992. </year>
Reference-contexts: Actually P is a family of algorithms that is related to the algorithm studied by Vovk [Vov90] and the Bayesian, Gibbs and "weighted majority" methods studied by a number of authors [LW94, LLW95, HKS94, STS90, SST92, HB92, HW95], as well as the method developed by Feder, Merhav and Gutman <ref> [FMG92] </ref>. <p> Previous work has shown how to construct an algorithm A such that the ratio L A (y)=L E (y) approaches 1 in the limit <ref> [Vov90, LW94, FMG92] </ref>. In fact, Vovk [Vov90] described an algorithm with the same bound as the one we give in Theorem 10 for the algorithm P. This theorem leaves a parameter to be tuned. Vovk gives an implicit form of the optimum choice of the parameter. <p> However, as is clear from Figure 3, this function lies outside the allowable range for F fi (r), and this is no accident. The Bayes method does not perform well in the worst case for this prediction problem, as was shown in <ref> [HW95, FMG92] </ref>. Hence we must deviate from the Bayes method at this step. This leads to the requirements we have specified for the prediction function F fi (r). <p> One function that satisfies the requirements for F fi is the piecewise linear function 6 F fi (r) = &gt; &lt; 0 if r 1 1 4c if 1 2 + c 2 + c 6 A similar piecewise linear function was suggested by Feder, Merhav and Gutman <ref> [FMG92] </ref>, in a related context. 18 prediction function F fi for fi = 0 (Inequality (6)).
Reference: [FRR94] <author> A. Fiat, Y. Rabani, and Y. Ravid. </author> <title> Competitive k-server algorithms. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48(3) </volume> <pages> 410-428, </pages> <year> 1994. </year>
Reference-contexts: An alternate logarithmic loss function, often considered in the literature, is discussed briefly in Section 8. 2 This approach is also related to that taken in recent work on the competitive ratio of on-line algorithms, and in particular to work on combining on-line algorithms to obtain the best competitive ratio <ref> [FKL + 91, FFK + 91, FRR94] </ref>, except that we look at the difference in performance rather than the ratio. 2 actual weather turns out to be.
Reference: [Gal87] <author> Janos Galambos. </author> <title> The Asymptotic Theory of Extreme Oreder Statistics. </title> <editor> R. E. Kreiger, </editor> <booktitle> second edition, </booktitle> <year> 1987. </year>
Reference: [Han57] <author> James Hannan. </author> <title> Approximation to Bayes risk in repeated play. </title> <booktitle> In Contributions to the theory of games, </booktitle> <volume> volume 3, </volume> <pages> pages 97-139. </pages> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference: [Hay94] <author> S. Haykin. </author> <title> Neural Networks: a comprehensive foundation. </title> <publisher> Macmillan, </publisher> <year> 1994. </year> <month> 44 </month>
Reference-contexts: In Section 4 we present a family of prediction algorithms for the general prediction game. The basic algorithm, which we call P has a real-valued parameter, fi, which controls its behavior. This parameter plays a similar role to the "learning rate" parameter used in gradient based learning algorithms <ref> [Hay94] </ref>. Different choices of fi guarantee different performance bounds for the algorithm. The optimal choice of fi is of critical importance and occupies much of the discussion in Sections 4.4-4.6 and also later in Section 5.4.
Reference: [HB92] <author> David Haussler and Andrew Barron. </author> <title> How well do Bayes methods work for on-line prediction of f+1; 1g values? In Proceedings of the Third NEC Symposium on Computation and Cognition. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: Actually P is a family of algorithms that is related to the algorithm studied by Vovk [Vov90] and the Bayesian, Gibbs and "weighted majority" methods studied by a number of authors <ref> [LW94, LLW95, HKS94, STS90, SST92, HB92, HW95] </ref>, as well as the method developed by Feder, Merhav and Gutman [FMG92]. <p> is well known that for the log loss, for any set E of N experts there is a prediction strategy A such that for any sequence y, L A (y) L E (y) log N; where L E (y) is the total log loss of the best expert for y <ref> [Ris86, DMW88, Vov92, HB92, Yam95, KW94] </ref>. 12 The strategy is just the Bayes algorithm with uniform prior on the distributions represented by the experts. A min/max optimal prediction algorithm is known for the case where the experts are simulatable and `, the number of iterations, is known in advance.
Reference: [HKLW91] <author> David Haussler, Michael Kearns, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 95 </volume> <pages> 129-161, </pages> <year> 1991. </year>
Reference-contexts: Our current methods do not provide these, but standard "confidence boosting" methods can be applied on top of them to achieve good tail bounds <ref> [HKLW91, Lit89] </ref>. More direct methods are given by Littlestone and Warmuth [LW94]. 29 er D , the error with respect to the underlying distribution D. This is often referred to as the problem of over-fitting.
Reference: [HKS94] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 84-114, </pages> <year> 1994. </year>
Reference-contexts: Actually P is a family of algorithms that is related to the algorithm studied by Vovk [Vov90] and the Bayesian, Gibbs and "weighted majority" methods studied by a number of authors <ref> [LW94, LLW95, HKS94, STS90, SST92, HB92, HW95] </ref>, as well as the method developed by Feder, Merhav and Gutman [FMG92].
Reference: [HKW95] <author> David Haussler, Jyrki Kivinen, and Manfred K. Warmuth. </author> <title> Tight worst-case loss bounds for predicting with expert advice. </title> <booktitle> In Computational Learning Theory: Second European Conference, </booktitle> <volume> EuroCOLT '95, </volume> <pages> pages 69-83. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: The results presented in this paper contribute to an ongoing program in information theory and 3 The algorithm has recently been extended to the case when the outcomes are in the interval [0; 1] with the performance bounds as in the binary case <ref> [HKW95] </ref>. 3 statistics to minimize the number of assumptions placed on the actual mechanism generating the observations through the development of robust procedures and strengthened worst-case analysis. <p> We bound the additional loss of the algorithm over the loss of the best expert. Apart from the game-theoretic analysis, our main upper bound is obtained essentially by tuning an algorithm that was first introduced by Vovk (Theorem 15). Other loss functions for the expert framework are considered in <ref> [Vov90, HKW95] </ref>. The paper leaves many open problems. Our lower bounds only address the case when a bound on the length of the sequence of examples is known.
Reference: [HLW94] <author> David Haussler, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Predicting f0; 1g-functions on randomly drawn points. </title> <journal> Information and Computation, </journal> <volume> 115(2) </volume> <pages> 248-292, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction A central problem in statistics and machine learning is the problem of predicting future events based on past observations. In computer science literature in particular, special attention has been given to the case in which the events are simple binary outcomes (e.g. <ref> [HLW94] </ref>). For example, in predicting today's weather, we may choose to consider only the possible outcomes 0 and 1, where 1 indicates that it rains today, and 0 indicates that it does not.
Reference: [HW95] <author> D. Helmbold and M. K. Warmuth. </author> <title> On weak learning. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50(3) </volume> <pages> 551-573, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Actually P is a family of algorithms that is related to the algorithm studied by Vovk [Vov90] and the Bayesian, Gibbs and "weighted majority" methods studied by a number of authors <ref> [LW94, LLW95, HKS94, STS90, SST92, HB92, HW95] </ref>, as well as the method developed by Feder, Merhav and Gutman [FMG92]. <p> However, as is clear from Figure 3, this function lies outside the allowable range for F fi (r), and this is no accident. The Bayes method does not perform well in the worst case for this prediction problem, as was shown in <ref> [HW95, FMG92] </ref>. Hence we must deviate from the Bayes method at this step. This leads to the requirements we have specified for the prediction function F fi (r). <p> In that case ln (1r) is the information gain when the outcome is zero and ln (r) is the information gain when the outcome is one. Furthermore, the prediction function (11) is the normalized information gain when the outcome is zero. See <ref> [HW95] </ref> for a more detailed discussion. As the noise increases, fi ! 1 and all four curves converge to the identity function. <p> For the noise-free case (fi = 0), their algorithm becomes the Gibbs algorithm (see discussion in <ref> [HW95] </ref>). The bound of Theorem 10 (with denominator 2 ln 2 1+fi ) was recently also obtained by Kivinen and Warmuth [KW94] for the case when the outcomes are in [0,1]. <p> A simple argument, which will be given in Section 5.2, bounds the expected error of this learning algorithm. Similar methods were previously used by Helmbold and Warmuth <ref> [HW95] </ref>. Before using algorithm P as the sequence prediction algorithm, we need to choose the parameter fi. We analyze two methods for tuning fi in this context. The first method is to tune fi according to the length of the sample, using the results of Section 4.5.
Reference: [KS94] <author> Michael J. Kearns and Robert E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48(3) </volume> <pages> 464-497, </pages> <year> 1994. </year>
Reference-contexts: Given this input, a hold-one-out prediction algorithm produces a prediction ^y t 2 [0; 1]. The total hold-one-out 11 Thus a static expert is simply a regression function (or "p-concept" <ref> [KS94] </ref>) from the instance space X into [0; 1], the value of which represents a conditional probability of the label 1 given the input instance x t . 33 loss of the prediction algorithm A on outcome sequence y is defined in analogy with the on-line prediction loss as HL A
Reference: [KSS94] <author> Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <journal> Machine Learning, </journal> <volume> 17 </volume> <pages> 115-141, </pages> <year> 1994. </year>
Reference-contexts: This problem can also be described as a special variant of the probably approximately correct (PAC) learning model [Val84] in which nothing is assumed about the "target concept" that generates the examples other than independence between examples (sometimes referred to as agnostic learning <ref> [KSS94] </ref>), and in which the learning algorithm is not required to return a hypothesis in any specific form. Using the prediction strategy P, we develop an algorithm that solves this pattern recognition problem and derive distribution-independent bounds for the performance of this algorithm. <p> The goal of the learning algorithm is to produce a hypothesis whose error (i.e., probability of mistake) is not much worse than the error of the best function in some known class H of functions called the comparison or touchstone class <ref> [KSS94] </ref>. Outside of the pattern recognition literature, this type of problem might be called by many names, such as L 1 regression with a regret formulation of the loss function (in typical statistics literature, see e.g. [BM93]), or, as mentioned in the introduction, the agnostic version of PAC learning [KSS94]. <p> class <ref> [KSS94] </ref>. Outside of the pattern recognition literature, this type of problem might be called by many names, such as L 1 regression with a regret formulation of the loss function (in typical statistics literature, see e.g. [BM93]), or, as mentioned in the introduction, the agnostic version of PAC learning [KSS94]. The terminology we use here is that from the PAC learning literature.
Reference: [KW94] <author> Jyrki Kivinen and Manfred K. Warmuth. </author> <title> Using experts for predicting continuous outcomes. </title> <booktitle> In Computational Learning Theory: </booktitle> <volume> EuroCOLT '93, </volume> <pages> pages 109-120. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: For the noise-free case (fi = 0), their algorithm becomes the Gibbs algorithm (see discussion in [HW95]). The bound of Theorem 10 (with denominator 2 ln 2 1+fi ) was recently also obtained by Kivinen and Warmuth <ref> [KW94] </ref> for the case when the outcomes are in [0,1]. <p> is well known that for the log loss, for any set E of N experts there is a prediction strategy A such that for any sequence y, L A (y) L E (y) log N; where L E (y) is the total log loss of the best expert for y <ref> [Ris86, DMW88, Vov92, HB92, Yam95, KW94] </ref>. 12 The strategy is just the Bayes algorithm with uniform prior on the distributions represented by the experts. A min/max optimal prediction algorithm is known for the case where the experts are simulatable and `, the number of iterations, is known in advance. <p> Is the hold-one-out model necessary to prove the bounds given for the PAC model? Can the same bounds be obtained by simpler algorithms? The upper bound for the main algorithm P of this paper (Theorem 10) has recently been generalized by Warmuth and Kivinen <ref> [KW94] </ref> to the case when the outcomes lie in the interval [0; 1] instead being restricted to be binary as done in this paper.
Reference: [Lit89] <author> N. Littlestone. </author> <title> From on-line to batch learning. </title> <booktitle> In Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 269-284. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Our current methods do not provide these, but standard "confidence boosting" methods can be applied on top of them to achieve good tail bounds <ref> [HKLW91, Lit89] </ref>. More direct methods are given by Littlestone and Warmuth [LW94]. 29 er D , the error with respect to the underlying distribution D. This is often referred to as the problem of over-fitting.
Reference: [LLW95] <author> Nicholas Littlestone, Philip M. Long, and Manfred K. Warmuth. </author> <title> On-line learning of linear functions. </title> <journal> Computational Complexity, </journal> <volume> 5(1) </volume> <pages> 1-23, </pages> <year> 1995. </year>
Reference-contexts: Actually P is a family of algorithms that is related to the algorithm studied by Vovk [Vov90] and the Bayesian, Gibbs and "weighted majority" methods studied by a number of authors <ref> [LW94, LLW95, HKS94, STS90, SST92, HB92, HW95] </ref>, as well as the method developed by Feder, Merhav and Gutman [FMG92].
Reference: [LW94] <author> N. Littlestone and M. K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: Actually P is a family of algorithms that is related to the algorithm studied by Vovk [Vov90] and the Bayesian, Gibbs and "weighted majority" methods studied by a number of authors <ref> [LW94, LLW95, HKS94, STS90, SST92, HB92, HW95] </ref>, as well as the method developed by Feder, Merhav and Gutman [FMG92]. <p> Previous work has shown how to construct an algorithm A such that the ratio L A (y)=L E (y) approaches 1 in the limit <ref> [Vov90, LW94, FMG92] </ref>. In fact, Vovk [Vov90] described an algorithm with the same bound as the one we give in Theorem 10 for the algorithm P. This theorem leaves a parameter to be tuned. Vovk gives an implicit form of the optimum choice of the parameter. <p> The continuous curve corresponds to the bound achieved when fi is chosen as in Theorem 15, and the dotted curve corresponds to the upper bound given in the theorem. Also, Littlestone and Warmuth <ref> [LW94] </ref> prove a bound for their algorithm W M C which has the same form as the bound of Theorem 10, except the denominator 2 ln 2 1+fi is replaced by the smaller function 1 fi. <p> The bound of Theorem 10 (with denominator 2 ln 2 1+fi ) was recently also obtained by Kivinen and Warmuth [KW94] for the case when the outcomes are in [0,1]. Curiously enough, the denominator of ln 2 1+fi is obtained by the Weighted Majority algorithm of Littlestone and Warmuth <ref> [LW94] </ref> which assumes that the outcomes are binary and predicts binary as well (See [CBFHW96] for a detailed treatment of the case when the outcomes are binary). 4.4 Performance for bounded L E So far we have ignored the issue of how fi is chosen. <p> Our current methods do not provide these, but standard "confidence boosting" methods can be applied on top of them to achieve good tail bounds [HKLW91, Lit89]. More direct methods are given by Littlestone and Warmuth <ref> [LW94] </ref>. 29 er D , the error with respect to the underlying distribution D. This is often referred to as the problem of over-fitting. <p> Weaker bounds that are not in the above form have been given by Littlestone and Warmuth <ref> [LW94] </ref>. Our new bounds proven for the PAC model (Section 5) are better that previous bounds but the algorithms are very complicated.
Reference: [MF93] <author> N. Merhav and M. Feder. </author> <title> Universal schemes for sequential decision from individual data sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(4) </volume> <pages> 1280-1292, </pages> <year> 1993. </year>
Reference: [Ris78] <author> Jorma Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: We then give applications of these results to the theory of pattern recognition [Vap82] and PAC learning [Val84]. We take the extreme position, as advocated by Dawid and Vovk in the theory of prequential probability [Daw84, Dawar, Daw91, Vov93], Rissanen in his theory of stochastic complexity <ref> [Ris78, RL81, Ris86, Yam95] </ref> and Cover, Lempel and Ziv, Feder and others in the theory of universal prediction and data compression of individual sequences [FMG92, MF93, Cov65, CS77, Han57, fl Universita di Milano (Italy), cesabian@dsi.unimi.it.
Reference: [Ris86] <author> Jorma Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14(3) </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: We then give applications of these results to the theory of pattern recognition [Vap82] and PAC learning [Val84]. We take the extreme position, as advocated by Dawid and Vovk in the theory of prequential probability [Daw84, Dawar, Daw91, Vov93], Rissanen in his theory of stochastic complexity <ref> [Ris78, RL81, Ris86, Yam95] </ref> and Cover, Lempel and Ziv, Feder and others in the theory of universal prediction and data compression of individual sequences [FMG92, MF93, Cov65, CS77, Han57, fl Universita di Milano (Italy), cesabian@dsi.unimi.it. <p> is well known that for the log loss, for any set E of N experts there is a prediction strategy A such that for any sequence y, L A (y) L E (y) log N; where L E (y) is the total log loss of the best expert for y <ref> [Ris86, DMW88, Vov92, HB92, Yam95, KW94] </ref>. 12 The strategy is just the Bayes algorithm with uniform prior on the distributions represented by the experts. A min/max optimal prediction algorithm is known for the case where the experts are simulatable and `, the number of iterations, is known in advance.
Reference: [RL81] <author> Jorma Rissanen and Glen G. Langdon, Jr. </author> <title> Universal modeling and coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-27(1):12-23, </volume> <month> January </month> <year> 1981. </year>
Reference-contexts: We then give applications of these results to the theory of pattern recognition [Vap82] and PAC learning [Val84]. We take the extreme position, as advocated by Dawid and Vovk in the theory of prequential probability [Daw84, Dawar, Daw91, Vov93], Rissanen in his theory of stochastic complexity <ref> [Ris78, RL81, Ris86, Yam95] </ref> and Cover, Lempel and Ziv, Feder and others in the theory of universal prediction and data compression of individual sequences [FMG92, MF93, Cov65, CS77, Han57, fl Universita di Milano (Italy), cesabian@dsi.unimi.it. <p> The study of sequential coding, on the other hand, is interested in the log loss function y ln p (1 y) ln (1 p). This loss function is closely related to the minimal average coding length that can be achieved by using the given predictions (see Rissanen and Langdon <ref> [RL81] </ref>). 41 2. The predictions made by the "experts" as defined here are not restricted; they can depend on any information that is available to the experts.
Reference: [Sht75] <author> Yu. M. Shtarkov. </author> <title> Coding of descrete sources with unknown statistics. </title> <editor> In I. Csiszar and P. Elias, editors, </editor> <booktitle> Topics in Information Theory, </booktitle> <pages> pages 559-574. </pages> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1975. </year>
Reference-contexts: simplicity the bounds stated the Theorem 24 are actually weaker than what we prove in Equation (30). 6 Worst-case Loss Bounds for the Log Loss It is interesting to relate the min/max analysis, given in Section 3, to results on the problem of optimal universal sequential coding studied by Shtarkov <ref> [Sht75, Sht87] </ref>. The problem of sequential coding is similar to the problem studied in this paper, with two major differences: 1. The loss function that is studied in this paper is jp yj.
Reference: [Sht87] <author> Yu. M. Shtarkov. </author> <title> Universal sequential coding of single messages. </title> <journal> Problems of Information Transmission, </journal> <volume> 23 </volume> <pages> 175-186, </pages> <month> July-September </month> <year> 1987. </year>
Reference-contexts: simplicity the bounds stated the Theorem 24 are actually weaker than what we prove in Equation (30). 6 Worst-case Loss Bounds for the Log Loss It is interesting to relate the min/max analysis, given in Section 3, to results on the problem of optimal universal sequential coding studied by Shtarkov <ref> [Sht75, Sht87] </ref>. The problem of sequential coding is similar to the problem studied in this paper, with two major differences: 1. The loss function that is studied in this paper is jp yj. <p> A min/max optimal prediction algorithm is known for the case where the experts are simulatable and `, the number of iterations, is known in advance. This result is given by Shtarkov <ref> [Sht87] </ref> (Theorem 1). For completeness, we restate the theorem and its proof here using our terminology. Theorem 32 (Shtarkov) For each y 2 f0; 1g ` and each expert E i 2 E, let P i (y) denote the probability of y under expert E i .
Reference: [SST92] <author> H. S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Physical Review A, </journal> <volume> 45(8) </volume> <pages> 6056-6091, </pages> <year> 1992. </year>
Reference-contexts: Actually P is a family of algorithms that is related to the algorithm studied by Vovk [Vov90] and the Bayesian, Gibbs and "weighted majority" methods studied by a number of authors <ref> [LW94, LLW95, HKS94, STS90, SST92, HB92, HW95] </ref>, as well as the method developed by Feder, Merhav and Gutman [FMG92].
Reference: [Sto77] <author> C. J. Stone. </author> <title> Cross-validation: a review. </title> <journal> Math. Operationforsch. Statist. Ser. Statist., </journal> <volume> 9 </volume> <pages> 127-139, </pages> <year> 1977. </year>
Reference-contexts: The motivation for the name "hold-one-out" loss comes from the similarity to the cross-validation procedure of the same name used in statistics <ref> [Sto77] </ref>. The following example illustrates the use of the total hold-one-out loss. Consider a classroom setting in which an instructor is trying to teach students to perform a classification task of some type, say to distinguish earthquakes from underground nuclear explosions, based on seismographic data.
Reference: [STS90] <author> H. Sompolinsky, N. Tishby, and H.S. Seung. </author> <title> Learning from examples in large neural networks. </title> <journal> Physical Review Letters, </journal> <volume> 65 </volume> <pages> 1683-1686, </pages> <year> 1990. </year>
Reference-contexts: Actually P is a family of algorithms that is related to the algorithm studied by Vovk [Vov90] and the Bayesian, Gibbs and "weighted majority" methods studied by a number of authors <ref> [LW94, LLW95, HKS94, STS90, SST92, HB92, HW95] </ref>, as well as the method developed by Feder, Merhav and Gutman [FMG92].
Reference: [Tal94] <author> M. Talagrand. </author> <title> Sharper bounds for Gaussian and empirical processes. </title> <journal> Annals of Probability, </journal> <volume> 22(1) </volume> <pages> 28-76, </pages> <year> 1994. </year>
Reference-contexts: Using the prediction strategy P, we develop an algorithm that solves this pattern recognition problem and derive distribution-independent bounds for the performance of this algorithm. These bounds improve by constant factors some of the (more general) bounds obtained by Vapnik [Vap82] and Talagrand <ref> [Tal94] </ref> on the performance of an empirical loss minimization algorithm. <p> Results for this setting are given in the next section, as is the proof of Theorem 24. The bounds given in Theorem 24 are better than those obtained for this kind of pattern recognition problem by the only other methods that we are aware of <ref> [Vap92, Tal94, BM93] </ref>. Bounds given by Vapnik ([Vap92], Equation (11)) imply a bound in the same form as the second bound in Theorem 24, but with an additional factor of 2 in the leading term. However, Vapnik's bounds hold in more general cases than the one we consider here. <p> Bounds given by Vapnik ([Vap92], Equation (11)) imply a bound in the same form as the second bound in Theorem 24, but with an additional factor of 2 in the leading term. However, Vapnik's bounds hold in more general cases than the one we consider here. Talagrand <ref> [Tal94] </ref> gives similar general bounds without the factor of 2, but with an unspecified constant in the lower order term. It is not clear that this unspecified constant can be made small enough to get practical bounds for small sample size `.
Reference: [Val84] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-42, </pages> <year> 1984. </year>
Reference-contexts: We then give applications of these results to the theory of pattern recognition [Vap82] and PAC learning <ref> [Val84] </ref>. <p> General solutions to this regret formulation of the pattern recognition problem have been developed by Vapnik [Vap82], Birge and Massart [BM93], and others. This problem can also be described as a special variant of the probably approximately correct (PAC) learning model <ref> [Val84] </ref> in which nothing is assumed about the "target concept" that generates the examples other than independence between examples (sometimes referred to as agnostic learning [KSS94]), and in which the learning algorithm is not required to return a hypothesis in any specific form.
Reference: [Vap82] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: We then give applications of these results to the theory of pattern recognition <ref> [Vap82] </ref> and PAC learning [Val84]. <p> Performance is measured relative to the best binary-valued function in a given class of functions, called the comparison class. This kind of relative performance measure is called regret in statistics. General solutions to this regret formulation of the pattern recognition problem have been developed by Vapnik <ref> [Vap82] </ref>, Birge and Massart [BM93], and others. <p> Using the prediction strategy P, we develop an algorithm that solves this pattern recognition problem and derive distribution-independent bounds for the performance of this algorithm. These bounds improve by constant factors some of the (more general) bounds obtained by Vapnik <ref> [Vap82] </ref> and Talagrand [Tal94] on the performance of an empirical loss minimization algorithm. <p> That is, the learning algorithm attempts to minimize 10 the regret E s~D ` (er D (A (s))) er D (H): (15) Bounds on this regret for certain types of learning algorithms can be obtained from the work of Vapnik <ref> [Vap82] </ref> and Birge and Massart [BM93]. The basic idea of their learning algorithms is to predict according to the single hypothesis that suffers the minimal loss over the sample of instances presented to the learner. Vapnik calls this empirical risk minimization. <p> Although the cardinality of H may be infinite, the number of possible binary labelings of the sequence that agree with some function in H is always finite, and in fact, is polynomial in ` if the VC dimension of H is finite (see [BEHW89] or <ref> [Vap82] </ref> for a definition of the VC dimension and its relation to this kind of learning problem).
Reference: [Vap92] <author> V. Vapnik. </author> <title> Principles of risk minimization for learning theory. </title> <editor> In John E. Moody, Steve J. Hanson, and Richard P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Results for this setting are given in the next section, as is the proof of Theorem 24. The bounds given in Theorem 24 are better than those obtained for this kind of pattern recognition problem by the only other methods that we are aware of <ref> [Vap92, Tal94, BM93] </ref>. Bounds given by Vapnik ([Vap92], Equation (11)) imply a bound in the same form as the second bound in Theorem 24, but with an additional factor of 2 in the leading term. However, Vapnik's bounds hold in more general cases than the one we consider here.
Reference: [Vov90] <author> Volodimir G. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: However, we define an algorithm, called P for "Predict", that is simple and efficient, and performs essentially as well as the min/max strategy. Actually P is a family of algorithms that is related to the algorithm studied by Vovk <ref> [Vov90] </ref> and the Bayesian, Gibbs and "weighted majority" methods studied by a number of authors [LW94, LLW95, HKS94, STS90, SST92, HB92, HW95], as well as the method developed by Feder, Merhav and Gutman [FMG92]. <p> Previous work has shown how to construct an algorithm A such that the ratio L A (y)=L E (y) approaches 1 in the limit <ref> [Vov90, LW94, FMG92] </ref>. In fact, Vovk [Vov90] described an algorithm with the same bound as the one we give in Theorem 10 for the algorithm P. This theorem leaves a parameter to be tuned. Vovk gives an implicit form of the optimum choice of the parameter. <p> Previous work has shown how to construct an algorithm A such that the ratio L A (y)=L E (y) approaches 1 in the limit [Vov90, LW94, FMG92]. In fact, Vovk <ref> [Vov90] </ref> described an algorithm with the same bound as the one we give in Theorem 10 for the algorithm P. This theorem leaves a parameter to be tuned. Vovk gives an implicit form of the optimum choice of the parameter. <p> The proof is then concluded by choosing x 0 = 0, x 1 = 1, and ff = 1 a. 4.2 The performance of algorithm P (fi) Algorithm P's performance is summarized by the following theorem, which generalizes a similar result of Vovk <ref> [Vov90] </ref>. Theorem 10 For any 0 fi &lt; 1, for any set E of N experts, and for any binary sequence y of length `, the loss of P (fi) satisfies L P (fi) (y) 2 ln 2 : The proof of the theorem is based on the following lemma. <p> q , we have that N X w i;`+1 w j;`+1 = w j;1 t=1 t=1 The theorem now follows from Lemma 11. 4.3 Discussion of the algorithm Although our algorithm allows any update function U fi (q) between the exponential fi q (used by Vovk in his related work <ref> [Vov90] </ref>) and the linear function 1 (1 fi)q that upper bounds it, it turns out that the linear update has a nice Bayesian interpretation, and thus in some sense may be preferable. <p> for F fi , a piecewise linear function (lin) given in (10), and the function that has been suggested by Vovk's work (vovk) given in (11). where c = (1 + fi) ln ( 2 2 (1 fi) Another possible choice for F fi is suggested by Vovk's work 7 <ref> [Vov90] </ref> F fi (r) = ln (1 r + rfi) + ln ((1 r)fi + r) F fi given in Inequality (6). Recall that fi = 0 corresponds to the case when there is no noise. <p> See [HW95] for a more detailed discussion. As the noise increases, fi ! 1 and all four curves converge to the identity function. Finally, we note that the parameterized bound given in Theorem 10 on the performance of algorithm P was first proved by Vovk <ref> [Vov90] </ref> for his version of F fi and the exponential update. 7 Vovk's algorithm generates its prediction according to the prediction function ^y t = P N ln i=1 w i;t fi ~ i;t + ln i=1 w i;t fi 1~ i;t where the weights are normalized so that they sum <p> When L E (y) is replaced by K, the 20 upper bound from Theorem 10 can be written L (fi) = 2 ln 2 : It has been shown by Vovk and others <ref> [Vov90, CBFHW96] </ref> that L fl = inf fL (fi) : 0 fi &lt; 1g is the unique value of L satisfying L = 2 2L ; where H (p) is the binary entropy, p log 2 (p) (1 p) log 2 (1 p). <p> We bound the additional loss of the algorithm over the loss of the best expert. Apart from the game-theoretic analysis, our main upper bound is obtained essentially by tuning an algorithm that was first introduced by Vovk (Theorem 15). Other loss functions for the expert framework are considered in <ref> [Vov90, HKW95] </ref>. The paper leaves many open problems. Our lower bounds only address the case when a bound on the length of the sequence of examples is known.
Reference: [Vov92] <author> V. G. Vovk. </author> <title> Universal forcasting algorithms. </title> <journal> Information and Computation, </journal> <volume> 96(2) </volume> <pages> 245-277, </pages> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: is well known that for the log loss, for any set E of N experts there is a prediction strategy A such that for any sequence y, L A (y) L E (y) log N; where L E (y) is the total log loss of the best expert for y <ref> [Ris86, DMW88, Vov92, HB92, Yam95, KW94] </ref>. 12 The strategy is just the Bayes algorithm with uniform prior on the distributions represented by the experts. A min/max optimal prediction algorithm is known for the case where the experts are simulatable and `, the number of iterations, is known in advance.
Reference: [Vov93] <author> V. G. Vovk. </author> <title> A logic of probability, with application to the foundations of statistics. </title> <journal> Journal of the Royal Statistical Society Series B-Methodological, </journal> <volume> 55(2) </volume> <pages> 317-351, </pages> <year> 1993. </year>
Reference-contexts: We then give applications of these results to the theory of pattern recognition [Vap82] and PAC learning [Val84]. We take the extreme position, as advocated by Dawid and Vovk in the theory of prequential probability <ref> [Daw84, Dawar, Daw91, Vov93] </ref>, Rissanen in his theory of stochastic complexity [Ris78, RL81, Ris86, Yam95] and Cover, Lempel and Ziv, Feder and others in the theory of universal prediction and data compression of individual sequences [FMG92, MF93, Cov65, CS77, Han57, fl Universita di Milano (Italy), cesabian@dsi.unimi.it.
Reference: [Yam95] <author> Kenji Yamanishi. </author> <title> A loss bound model for on-line stochastic prediction algorithms. </title> <journal> Information and Computation, </journal> <volume> 119(1) </volume> <pages> 39-54, </pages> <year> 1995. </year>
Reference-contexts: We then give applications of these results to the theory of pattern recognition [Vap82] and PAC learning [Val84]. We take the extreme position, as advocated by Dawid and Vovk in the theory of prequential probability [Daw84, Dawar, Daw91, Vov93], Rissanen in his theory of stochastic complexity <ref> [Ris78, RL81, Ris86, Yam95] </ref> and Cover, Lempel and Ziv, Feder and others in the theory of universal prediction and data compression of individual sequences [FMG92, MF93, Cov65, CS77, Han57, fl Universita di Milano (Italy), cesabian@dsi.unimi.it. <p> is well known that for the log loss, for any set E of N experts there is a prediction strategy A such that for any sequence y, L A (y) L E (y) log N; where L E (y) is the total log loss of the best expert for y <ref> [Ris86, DMW88, Vov92, HB92, Yam95, KW94] </ref>. 12 The strategy is just the Bayes algorithm with uniform prior on the distributions represented by the experts. A min/max optimal prediction algorithm is known for the case where the experts are simulatable and `, the number of iterations, is known in advance.
References-found: 47

