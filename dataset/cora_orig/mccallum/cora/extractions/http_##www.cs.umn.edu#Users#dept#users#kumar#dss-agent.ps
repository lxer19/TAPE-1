URL: http://www.cs.umn.edu/Users/dept/users/kumar/dss-agent.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Partitioning-Based Clustering for Web Document Categorization  
Author: Daniel Boley, Maria Gini, Robert Gross, Eui-Hong (Sam) Han, Kyle Hastings, George Karypis, Vipin Kumar, Bamshad Mobasher, and Jerome Moore 
Keyword: clustering, categorization, World Wide Web documents, graph partitioning, association rules, principal component analysis.  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science and Engineering University of Minnesota,  
Abstract: Clustering techniques have been used by many intelligent software agents in order to retrieve, filter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to define a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classification. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can effectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques, which are based on generalizations of graph partitioning, do not require pre-specified ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such as hierarchical agglomeration clustering, and Bayesian classification methods, such as AutoClass. fl This work was supported in part by Army Research Office contract DA/DAAG55-98-1-0441, by Army High Performance Computing Research Center cooperative agreement number DAAH04-95-2-0003/contract number DAAH04-95-C-0008, the content of which does not necessarily reflect the position or the policy of the government, and no official endorsement should be inferred. Additional support was provided by the IBM Partnership Award, and by the IBM SUR equipment grant. Access to computing facilities was provided by Minnesota Supercomputer Institute. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mark Ackerman and et al. </author> <title> Learning probabilistic user profiles. </title> <journal> AI Magazine, </journal> <volume> 18(2) </volume> <pages> 47-56, </pages> <year> 1997. </year>
Reference-contexts: Individual documents are characterized by profile vectors consisting of pairs of lexically affine words, with document similarity a function of how many indices they share. This method may not scale well to large document searches. The Syskill & Webert system <ref> [1] </ref> represents an HTML page with a Boolean feature vector, and then uses naive Bayesian classification to find web pages that are similar, but for only a given single user profile.
Reference: [2] <author> A. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. </author> <title> Fast discovery of association rules. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Figure 1 depicts a portion of a typical supermarket transaction database. The association rule discovery methods <ref> [2] </ref> first find groups of items occurring frequently together in many transactions. Such groups of items are referred to as frequent item sets. In the ARHP method, we use the discovered frequent item sets to form a hypergraph, where vertices are items and each hyperedge represents a frequent item set.
Reference: [3] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. </author> <title> Fast discovery of association rules. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In this case, hyperedges represent the frequent item sets found by the association rule discovery algorithm. Association rules capture the relationships among items that are present in a transaction <ref> [3] </ref>. Let T be the set of transactions where each transaction is a subset of the item-set I, and C be a subset of I.
Reference: [4] <author> Marko Balabanovic, Yoav Shoham, and Yeogirl Yun. </author> <title> An adaptive agent for automated Web browsing. Journal of Visual Communication and Image Representation, </title> <type> 6(4), </type> <year> 1995. </year> <note> http://www-diglib.stanford.edu/cgi-bin/WP/get/SIDL-WP-1995-0023. 15 </note>
Reference-contexts: This method may not scale well to large document searches. The Syskill & Webert system [1] represents an HTML page with a Boolean feature vector, and then uses naive Bayesian classification to find web pages that are similar, but for only a given single user profile. Balabanovic <ref> [4] </ref> presents a system that uses a single well-defined profile to find similar web documents for a 13 user. Candidate web pages are located using best-first search, comparing their word vectors against a user profile vector, and returning the highest -scoring pages.
Reference: [5] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> American Elsevier, </publisher> <year> 1976. </year>
Reference-contexts: These frequent item sets are mapped into hyperedges in a hypergraph. A typical document-feature dataset, represented as a transactional database, is depicted in Figure 2. A hypergraph <ref> [5] </ref> H = (V; E) consists of a set of vertices (V ) and a set of hyperedges (E). A hypergraph is an extension of a graph in the sense that each hyperedge can connect more than two vertices.
Reference: [6] <author> M. W. Berry, S. T. Dumais, and Gavin W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: This has been discussed in more detail in [7]. This method differs from that of Latent Semantic Indexing (LSI) <ref> [6] </ref> in many ways. First of all, LSI was originally formulated for a different purpose, namely as a method to reduce the dimensionality of the search space for the purpose of handling queries: retrieving some documents given a set of search terms. <p> The choice of a small k can lose important features of the data. On the other hand, the choice of a large k can capture most of the important features, but the dimensionality might be too large for the traditional clustering algorithms to work effectively. Latent Semantic Indexing (LSI) <ref> [6] </ref> is a dimensionality reduction technique extensively used in information retrieval domain and is similar in nature to PCA. Instead of finding the singular value decomposition of the covariance matrix, it finds the singular value decomposition of the original n fi m data. <p> In the context of query systems, LSI has been singularly successful in reducing the noise in the data, leading to much higher precision in results from user queries <ref> [6] </ref>. They may also be considered as possible preprocessing modules in the context of unsupervised clustering, and some preliminary experiments in this direction have been carried out using LSI followed by K-means and PDDP, yielding respective entropies of .834 and .859.
Reference: [7] <author> D. L. Boley. </author> <title> Principal Direction Divisive Partitioning. </title> <type> Technical Report TR-97-056, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: By taking advantage of the high degree of sparsity in the term frequency matrix, the Lanczos-based solver is very efficient, with cost proportional to the number of nonzeroes in the term frequency matrix. This has been discussed in more detail in <ref> [7] </ref>. This method differs from that of Latent Semantic Indexing (LSI) [6] in many ways.
Reference: [8] <author> D. L. Boley. </author> <title> Hierarchical taxonomies using divisive partitioning. </title> <type> Technical Report TR-98-012, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1998. </year>
Reference-contexts: In spite of the increased costs, the TFIDF scaling did not lead to any noticeable improvement in the PDDP results in our experiments <ref> [8] </ref>. 6 2.3 Hierarchical Agglomeration Clustering A classical algorithm we have implemented is a bottom up hierarchical agglomeration clustering (HAC) method based on the use of a distance function [13]. We start with trivial clusters, each containing one document. <p> We have tried the PDDP algorithm on a larger dataset (2340 documents) and our experiments show that the algorithm scales up linearly with the number of non-zero entries in the term frequency matrix <ref> [8] </ref>. As each document uses only a small fraction of the entire dictionary of words, the term frequency matrix is very sparse. In this larger dataset only 0.68% of the entries were nonzero.
Reference: [9] <author> Andrei Z. Broder, Steven C. Glassman, and Mark S. Manasse. </author> <title> Syntactic clustering of the Web. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <month> April </month> <year> 1997. </year> <note> http://proceedings.www6conf.org/HyperNews/get/PAPER205.html. </note>
Reference-contexts: results of AutoClass. cluster contains documents that were not clustered by ARHP, which yielded 17 clusters. 12 the documents are clustered. 4 Related Work A number of Web agents use various information retrieval techniques [15] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [10, 9, 12] </ref>. For example, HyPursuit [30] uses semantic information embedded in link structures as well as document content to classify and group documents by the terms they contain and their hyperlink structures. The system requires that information be maintained in the routers. <p> Pattern recognition methods and word clustering using the Hartigan's K-means partitional clustering algorithm are used in [31] to discover salient HTML document features (words) that can be used in finding similar HTML documents on the Web. The clustering algorithm does not scale well to large numbers of documents. Broder <ref> [9] </ref> calculates a sketch for every document on the web and then clusters together similar documents whose sketches exceed a threshold of resemblance. Given a document's URL, similar documents can be easily identified, but an index for the whole WWW needs to be maintained.
Reference: [10] <author> C. Chang and C. Hsu. </author> <title> Customizable multi-engine search tool with clustering. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: results of AutoClass. cluster contains documents that were not clustered by ARHP, which yielded 17 clusters. 12 the documents are clustered. 4 Related Work A number of Web agents use various information retrieval techniques [15] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [10, 9, 12] </ref>. For example, HyPursuit [30] uses semantic information embedded in link structures as well as document content to classify and group documents by the terms they contain and their hyperlink structures. The system requires that information be maintained in the routers.
Reference: [11] <author> P. Cheeseman and J. Stutz. </author> <title> Baysian classification (AutoClass): Theory and results. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Distance-based schemes generally require the calculation of the mean of document clusters, which are often chosen initially at random. In a high dimensional space, the cluster means of randomly chosen clusters will do a poor job at separating documents. Similarly, probabilistic methods such as Bayesian classification used in AutoClass <ref> [11, 29] </ref> do not perform well when the size of the feature space is much larger than the size of the sample set or may depend on the independence of the underlying features. Web documents suffer from both high dimensionality and high correlation among the feature values. <p> The cluster means were scaled by the corresponding cluster sizes to discourage large clusters. 2.4 AutoClass The other algorithm we use is AutoClass. AutoClass <ref> [11] </ref> is based on the probabilistic mixture modeling [29]. Given a set of data X, AutoClass finds maximum parameter values ^ ~ V for a specific probability distribution functions T of the clusters. <p> One of the weaknesses of AutoClass is that the underlying probability model assumes independence of attributes. In many domains, this assumption is too restrictive. Another problem with the basic model is that it does not provide a satisfactory distribution function for ordered discrete attributes <ref> [11] </ref>.
Reference: [12] <author> Liren Chen and Katya Sycara. </author> <title> Webmate : A personal agent for browsing and searching. </title> <booktitle> In Proc. of 2nd International Conference on Autonomous Agents, </booktitle> <year> 1998. </year>
Reference-contexts: results of AutoClass. cluster contains documents that were not clustered by ARHP, which yielded 17 clusters. 12 the documents are clustered. 4 Related Work A number of Web agents use various information retrieval techniques [15] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [10, 9, 12] </ref>. For example, HyPursuit [30] uses semantic information embedded in link structures as well as document content to classify and group documents by the terms they contain and their hyperlink structures. The system requires that information be maintained in the routers.
Reference: [13] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and scene analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: Web documents suffer from both high dimensionality and high correlation among the feature values. We have found that hierarchical agglomeration clustering (HAC) <ref> [13] </ref> is computationally very expensive, and AutoClass has performed poorly on our examples. <p> At each stage in 5 the process, the method (a) selects an unsplit cluster to split, and (b) splits that cluster into two subclusters. For part (a) we use a scatter value, measuring the average distance from the documents in a cluster to the mean <ref> [13] </ref>, though we could also use just the cluster size if it were desired to keep the resulting clusters all approximately the same size. For part (b) we construct a linear discriminant function based on the principal direction (the direction of maximal variance). <p> The leaf nodes then constitute a partitioning of the entire document set. The definition of the hyperplane is based on principal component analysis, similar to the Hotelling or Karhunen-Loeve Transformation <ref> [13] </ref>. We compute the principal direction as the leading eigenvector of the sample covariance matrix. This is the most expensive part, for which we use a fast Lanczos-based singular value solver [16]. <p> of the increased costs, the TFIDF scaling did not lead to any noticeable improvement in the PDDP results in our experiments [8]. 6 2.3 Hierarchical Agglomeration Clustering A classical algorithm we have implemented is a bottom up hierarchical agglomeration clustering (HAC) method based on the use of a distance function <ref> [13] </ref>. We start with trivial clusters, each containing one document. We cycle through a loop in which the two "closest clusters" are merged into one cluster. Each loop cycle reduces the number of clusters by 1, and this is repeated until the desired number of clusters is reached.
Reference: [14] <author> W. B. Frakes. </author> <title> Stemming algorithms. </title> <editor> In W. B. Frakes and R. Baeza-Yates, editors, </editor> <booktitle> Information Retrieval Data Structures and Algorithms, </booktitle> <pages> pages 131-160. </pages> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: Table 1: Setup of experiments. The word lists from all documents were filtered with a stop-list and "stemmed" using Porter's suffix-stripping algorithm [26] as implemented by <ref> [14] </ref>. We derived ten experiments, clustered the documents using 8 the four algorithms described earlier, and analyzed the results. Our objective is to reduce the dimensionality of the clustering problem while retaining the important features of the documents.
Reference: [15] <author> W. B. Frakes and R. Baeza-Yates. </author> <title> Information Retrieval Data Structures and Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: Finally in Section 3 we compare our work with other similar systems and present ideas for future research. 2 Clustering Methods Most of the existing methods for document clustering are based on either probabilistic methods, or distance and similarity measures (see <ref> [15] </ref>). Distance-based methods such as k-means analysis, hierarchical clustering [20] and nearest-neighbor clustering [23] use a selected set of words appearing in different documents as features. Each document is represented by a feature vector, and can be viewed as a point in a multi-dimensional space. <p> without TFIDF scaling, Figure 7 11 shows the results of ARHP, and Figure 8 shows the results of AutoClass. cluster contains documents that were not clustered by ARHP, which yielded 17 clusters. 12 the documents are clustered. 4 Related Work A number of Web agents use various information retrieval techniques <ref> [15] </ref> and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents [10, 9, 12]. For example, HyPursuit [30] uses semantic information embedded in link structures as well as document content to classify and group documents by the terms they contain and their hyperlink structures.
Reference: [16] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins Univ. Press, 3rd edition, </publisher> <year> 1996. </year>
Reference-contexts: The definition of the hyperplane is based on principal component analysis, similar to the Hotelling or Karhunen-Loeve Transformation [13]. We compute the principal direction as the leading eigenvector of the sample covariance matrix. This is the most expensive part, for which we use a fast Lanczos-based singular value solver <ref> [16] </ref>. By taking advantage of the high degree of sparsity in the term frequency matrix, the Lanczos-based solver is very efficient, with cost proportional to the number of nonzeroes in the term frequency matrix. This has been discussed in more detail in [7]. <p> A naive dense matrix solver takes O (n 3 ) operations to compute it and hence is prohibitively expensive. A method which takes advantage of sparsity could be used to speed this up substantially. An example is the Lanczos method <ref> [16] </ref> which has been used with great success in the PDDP algorithm. However, it is considerably more difficult to compute the leading k singular values and vectors in LSI than just the one leading singular vector as in PDDP.
Reference: [17] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hypergraphs. </title> <booktitle> In Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <pages> pages 9-13, </pages> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: Our proposed clustering algorithms, described below, are designed to efficiently handle very high dimensional spaces and large data sets, as shown in the experimental results we describe later. 2.1 Association Rule Hypergraph Partitioning (ARHP) The Association Rule Hypergraph Partitioning (ARHP) <ref> [17, 18] </ref> is a clustering method based on the association rule discovery technique used in data mining. This technique is often used to discover affinities among items in a transactional database (for example, to find sales relationships among items sold in supermarket customer transactions.
Reference: [18] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Hypergraph based clustering in high-dimensional data sets: A summary of results. </title> <journal> Bulletin of the Technical Committee on Data Engineering, </journal> <volume> 21(1), </volume> <year> 1998. </year>
Reference-contexts: Our proposed clustering algorithms, described below, are designed to efficiently handle very high dimensional spaces and large data sets, as shown in the experimental results we describe later. 2.1 Association Rule Hypergraph Partitioning (ARHP) The Association Rule Hypergraph Partitioning (ARHP) <ref> [17, 18] </ref> is a clustering method based on the association rule discovery technique used in data mining. This technique is often used to discover affinities among items in a transactional database (for example, to find sales relationships among items sold in supermarket customer transactions.
Reference: [19] <author> J. E. Jackson. </author> <title> A User's Guide To Principal Components. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year> <month> 16 </month>
Reference-contexts: A TFIDF scheme is used to calculate the word weights, normalized for document length. The system needs to keep a large dictionary and is limited to one user. A well-known and widely used technique for dimensionality reduction is Principal Component Analysis (PCA) <ref> [19] </ref>. Consider a data set with n data items and m variables. PCA computes a covariance matrix of size m fi m, and then calculate the k leading eigenvectors of this covariance matrix. These k leading eigenvectors of this matrix are principal features of the data. <p> PCA provides several guidelines on how to determine the right number of dimension k for given data based on the proportion of variance explained or the characteristic roots of the covariance matrix. However, as noted in <ref> [19] </ref>, different methods provide widely different guidelines for k on the same data, and thus it can be difficult to find the right number of dimension. The choice of a small k can lose important features of the data.
Reference: [20] <author> A.K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering <ref> [20] </ref> and nearest-neighbor clustering [23] use a selected set of words appearing in different documents as features. Each document is represented by a feature vector, and can be viewed as a point in a multi-dimensional space. <p> These k leading eigenvectors of this matrix are principal features of the data. The original data is mapped along these new principal directions. This projected data has lower dimensions and can now be clustered using traditional clustering algorithms such as K-means <ref> [20] </ref>, Hierarchical clustering [20], or AutoClass. PCA provides several guidelines on how to determine the right number of dimension k for given data based on the proportion of variance explained or the characteristic roots of the covariance matrix. <p> These k leading eigenvectors of this matrix are principal features of the data. The original data is mapped along these new principal directions. This projected data has lower dimensions and can now be clustered using traditional clustering algorithms such as K-means <ref> [20] </ref>, Hierarchical clustering [20], or AutoClass. PCA provides several guidelines on how to determine the right number of dimension k for given data based on the proportion of variance explained or the characteristic roots of the covariance matrix.
Reference: [21] <author> G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. </author> <title> Multilevel hypergraph partitioning: Application in VLSI domain. </title> <booktitle> In Proceedings ACM/IEEE Design Automation Conference, </booktitle> <year> 1997. </year>
Reference-contexts: Such groups of items are referred to as frequent item sets. In the ARHP method, we use the discovered frequent item sets to form a hypergraph, where vertices are items and each hyperedge represents a frequent item set. Then a hypergraph partitioning algorithm <ref> [21] </ref> is used to find the item clusters. The similarity among items is captured implicitly by the frequent item sets. <p> Note that by minimizing the hyperedge-cut we essentially minimize the relations that are violated by splitting the items into two groups. Now each of these two parts can be further bisected recursively, until each partition is highly connected. For this task we use HMETIS <ref> [21] </ref>, a multi-level hypergraph partitioning algorithm which can partition very large hypergraphs (of size &gt; 100K nodes) in minutes on personal computers. Once, the overall hypergraph has been partitioned into k parts, we eliminate bad clusters using the following cluster fitness criterion.
Reference: [22] <author> T. Kohonen. </author> <title> Self-Organization and Associated Memory. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: This substantially increases the processing cost for the subsequent clustering method as well as potentially occupying as much space as the original data, depending on the choice of k. 14 The Kohonen Self-Organizing Feature Map <ref> [22] </ref> is a neural network based scheme that projects high dimensional input data into a feature map of a smaller dimension such that the proximity relationships among input data are preserved.
Reference: [23] <author> S.Y. Lu and K.S. Fu. </author> <title> A sentence-to-sentence clustering procedure for pattern analysis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8 </volume> <pages> 381-389, </pages> <year> 1978. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering [20] and nearest-neighbor clustering <ref> [23] </ref> use a selected set of words appearing in different documents as features. Each document is represented by a feature vector, and can be viewed as a point in a multi-dimensional space. There are a number of problems with clustering in a multi-dimensional space using traditional distance-or probability-based methods.
Reference: [24] <author> Yoelle S. Maarek and Israel Z. Ben Shaul. </author> <title> Automatically organizing bookmarks per contents. </title> <booktitle> In Proc. of 5th International World Wide Web Conference, </booktitle> <month> May </month> <year> 1996. </year> <note> http://www5conf.inria.fr/fich html/papers/P37/Overview.html. </note>
Reference-contexts: For example, HyPursuit [30] uses semantic information embedded in link structures as well as document content to classify and group documents by the terms they contain and their hyperlink structures. The system requires that information be maintained in the routers. BO (Bookmark Organizer) <ref> [24] </ref> combines hierarchical agglomerative clustering techniques and user interaction to organize collection of Web documents listed in a personal bookmark file. <p> Broder [9] calculates a sketch for every document on the web and then clusters together similar documents whose sketches exceed a threshold of resemblance. Given a document's URL, similar documents can be easily identified, but an index for the whole WWW needs to be maintained. Maarek <ref> [24] </ref> uses the Hierarchical Agglomerative Clustering method to form clusters of the documents listed in a personal bookmark file. Individual documents are characterized by profile vectors consisting of pairs of lexically affine words, with document similarity a function of how many indices they share.
Reference: [25] <author> J. Moore, E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Web page categorization and feature selection using association rule and principal component clustering. </title> <booktitle> In 7th Workshop on Information Technologies and Systems, </booktitle> <month> Dec </month> <year> 1997. </year>
Reference-contexts: The labeling facilitates an entropy calculation and subsequent references to any page were directed to the archive. This ensures a stable data sample since some pages are fairly dynamic in content. Results we obtained in similar experiments with a smaller set of documents have been previously reported in <ref> [25] </ref>. Those documents were obtained in part from the Network for Excellence in Manufacturing website, on line at http://web.miep.org:80/miep/index.html and were used originally for the experiments described in [31]. <p> In fact, for most algorithms, the experiment J 5 produced results that were better than those obtained by using the full set. It should be noted that the conclusions drawn in the above discussion have been confirmed by an earlier experiment using a totally independent set of documents <ref> [25] </ref>. For any particular experiment, we can better judge the quality of the clustering by looking at the distribution of class labels among clusters.
Reference: [26] <author> M. F. Porter. </author> <title> An algorithm for suffix stripping. </title> <booktitle> Program, </booktitle> <volume> 14(3) </volume> <pages> 130-137, </pages> <year> 1980. </year>
Reference-contexts: Table 1: Setup of experiments. The word lists from all documents were filtered with a stop-list and "stemmed" using Porter's suffix-stripping algorithm <ref> [26] </ref> as implemented by [14]. We derived ten experiments, clustered the documents using 8 the four algorithms described earlier, and analyzed the results. Our objective is to reduce the dimensionality of the clustering problem while retaining the important features of the documents.
Reference: [27] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: We decided to use entropy <ref> [27] </ref> as a measure of goodness of the clusters. When a cluster contains documents from one class only, the entropy value is 0.0 for the cluster and when a cluster contains documents from many different classes, then entropy of the cluster is higher.
Reference: [28] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: First, it is not trivial to define a distance measure in this space. Feature vectors must be scaled to avoid skewing the result by different document lengths or possibly by how common 2 a word is across many documents. Techniques such as TFIDF <ref> [28] </ref> have been proposed precisely to deal with some of these problems, but we have found out in our experiments that using TFIDF scaling does not always help. Second, the number of different words in the documents can be very large. <p> In most of our experiments, we have used the norm scaling, in which each document is represented by a feature vector of word counts, scaled to unit length in the usual Euclidean norm. This leaves the sparsity pattern untouched. An alternate scaling is the TFIDF scaling <ref> [28] </ref>, but this scaling fills in all zero entries with nonzeroes, drastically increasing the cost of the overall algorithm by as much as a factor of 20.
Reference: [29] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: Distance-based schemes generally require the calculation of the mean of document clusters, which are often chosen initially at random. In a high dimensional space, the cluster means of randomly chosen clusters will do a poor job at separating documents. Similarly, probabilistic methods such as Bayesian classification used in AutoClass <ref> [11, 29] </ref> do not perform well when the size of the feature space is much larger than the size of the sample set or may depend on the independence of the underlying features. Web documents suffer from both high dimensionality and high correlation among the feature values. <p> The cluster means were scaled by the corresponding cluster sizes to discourage large clusters. 2.4 AutoClass The other algorithm we use is AutoClass. AutoClass [11] is based on the probabilistic mixture modeling <ref> [29] </ref>. Given a set of data X, AutoClass finds maximum parameter values ^ ~ V for a specific probability distribution functions T of the clusters.
Reference: [30] <author> Ron Weiss, Bienvenido Velez, Mark A. Sheldon, Chanathip Nemprempre, Peter Szilagyi, An-drzej Duda, and David K. Gifford. Hypursuit: </author> <title> A hierarchical network search engine that exploits content-link hypertext clustering. </title> <booktitle> In Seventh ACM Conference on Hypertext, </booktitle> <month> March </month> <year> 1996. </year> <note> http://paris.lcs.mit.edu/rweiss/. </note>
Reference-contexts: For example, HyPursuit <ref> [30] </ref> uses semantic information embedded in link structures as well as document content to classify and group documents by the terms they contain and their hyperlink structures. The system requires that information be maintained in the routers.
Reference: [31] <author> Marilyn R. Wulfekuhler and William F. Punch. </author> <title> Finding salient features for personal Web page categories. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <month> April </month> <year> 1997. </year> <note> http://proceedings.www6conf.org/HyperNews/get/PAPER118.html. 17 </note>
Reference-contexts: Results we obtained in similar experiments with a smaller set of documents have been previously reported in [25]. Those documents were obtained in part from the Network for Excellence in Manufacturing website, on line at http://web.miep.org:80/miep/index.html and were used originally for the experiments described in <ref> [31] </ref>. The experiments we describe in this paper grew out of our initial set of experiments, and were used to validate on a larger dataset the results we obtained with our original experiments. <p> BO (Bookmark Organizer) [24] combines hierarchical agglomerative clustering techniques and user interaction to organize collection of Web documents listed in a personal bookmark file. Pattern recognition methods and word clustering using the Hartigan's K-means partitional clustering algorithm are used in <ref> [31] </ref> to discover salient HTML document features (words) that can be used in finding similar HTML documents on the Web. The clustering algorithm does not scale well to large numbers of documents.
References-found: 31

