URL: http://www.cs.wisc.edu/~jussi/sigmetrics95.ps.gz
Refering-URL: http://www.cs.wisc.edu/~jussi/jussi.html
Root-URL: 
Email: fjussi,mirong@cs.wisc.edu  
Title: Disk-Tape Joins: Synchronizing Disk and Tape Access  
Author: Jussi Myllymaki Miron Livny 
Keyword: tertiary storage, join methods, concurrent I/O  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: Today large amounts of data are stored on tertiary storage media such as magnetic tapes and optical disks. DBMSs typically operate only on magnetic disks since they know how to maneuver disks and how to optimize accesses on them. Tertiary devices present a problem for DBMSs since these devices have dismountable media and have very different operational characteristics compared to magnetic disks. For instance, most tape drives offer very high capacity at low cost but are accessed sequentially, involve lengthy latencies, and deliver lower bandwidth. Typically, the scope of a DBMS's query optimizer does not include tertiary devices, and the DBMS might not even know how to control and operate upon tertiary-resident data. In a three-level hierarchy of storage devices (main memory, disk, tape), the typical solution is to elevate tape-resident data to disk devices, thus bringing such data into the DBMS' control, and then to perform the required operations on disk. This requires additional space on disk and may not give the lowest response time possible. With this challenge in mind, we studied the trade-offs between memory and disk requirements and the execution time of a join with the help of two well-known join methods. The conventional, disk-based Nested Block Join and Hybrid Hash Join were modified to operate directly on tapes. An experimental implementation of the modified algorithms gave us more insight into how the algorithms perform in practice. Our performance analysis shows that a DBMS desiring to operate on tertiary storage will benefit from special algorithms that operate directly on tape-resident data and take into account and exploit the mismatch in disk and tape characteristics. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Blasgen and K. Eswaran. </author> <title> Storage and access in relational data bases. </title> <journal> IBM Syst. J., </journal> <volume> 16(4) </volume> <pages> 363-377, </pages> <year> 1977. </year>
Reference-contexts: Joining two relations is one of the most common operations in a relational DBMS and one of the most costly if done naively. Since the seminal paper on computing joins of relations by Blasgen and Eswaran <ref> [1] </ref>, the database research community has shown great interest in optimizing joins. An important subclass of joins are ad hoc joins which do not rely on the existence of pre-computed access structures such as indices. <p> Most studies on disk-based joins employ a system model comprising main memory and disks where disks are represented by a transfer-only cost model <ref> [1, 2, 4, 8, 12] </ref>. In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded. <p> Our analysis does not include the I/O cost and memory requirement of the final output stream from the join. Output costs are typically not considered in join method analyses since they are the same for all methods <ref> [1, 4, 5, 7, 12] </ref>. Before the join operation begins, the tape which holds S has already been inserted and loaded into the tape drive. Since S fits on a single tape, no media changes are required.
Reference: [2] <author> K. Bratbergsengen. </author> <title> Hashing methods and relational algebra operations. </title> <booktitle> In Proc. Conf. Very Large Databases, </booktitle> <pages> pages 323-333, </pages> <address> Singapore, </address> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: Most studies on disk-based joins employ a system model comprising main memory and disks where disks are represented by a transfer-only cost model <ref> [1, 2, 4, 8, 12] </ref>. In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded.
Reference: [3] <author> B. W. Culp, D. R. Domel, W. T. Gregory, J. J. Kato, G. C. Melton, K. A. Proehl, D. W. Ruska, V. K. Rus-son, and P. </author> <title> Way. Streaming tape drive control electronics. </title> <journal> Hewlett-Packard J., </journal> <volume> 39(3) </volume> <pages> 43-54, </pages> <month> June </month> <year> 1988. </year>
Reference: [4] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation techniques for main memory database systems. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 1-8, </pages> <address> Boston, MA, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: space and processing time? In this paper we examine these questions with the help of 1 In a Hash Join, disk space is required for the original copy of relation S plus a hashed version. 1 two well-known ad hoc join methods: Nested Block Join [8] and Hybrid Hash Join <ref> [4, 12] </ref>. Two join algorithms for tape-resident data that are based on these methods are presented and used to profile the interplay between the amount of disk and memory space allocated to a join operator and how long it takes to process the join. <p> Most studies on disk-based joins employ a system model comprising main memory and disks where disks are represented by a transfer-only cost model <ref> [1, 2, 4, 8, 12] </ref>. In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded. <p> The smaller relation, R, is on disk. We consider only the case where R does not fit in main memory. If it does fit, the simplest approach is to read and hash R into memory, and then scan S. We refer to this case as Simple Hash Join <ref> [4] </ref> from tape. In the disk-tape version of Nested Block Join, the tape-resident relation (which is also the larger one) is selected as the outer relation. In the conventional, disk-based Nested Block Join the outer relation is always the smaller one. <p> M R blocks of main memory are assigned for R tuples, while M S blocks are assigned to store tuples from S. The space overhead incurred when building a hash table for a set of data is denoted by factor F <ref> [4] </ref>. A summary of the notation appears in Table 2. 5.1 Nested Block Join The conventional Nested Block Join (NB) [8] can be used for joining tape-resident S with disk-resident R as follows. <p> from disk into memory compute S i 1 R j j j + 1 change direction of R scan wait until S i+1 copied to I 1k i i + 1 end 5.3 Hybrid Hash Join Hybrid Hash Join (HH) on tapes operates exactly as the classic Hybrid Hash Join <ref> [4, 12] </ref> except that Phase I where both relations are hash partitioned on disk is modified to read relation S from tape. The number of hash partitions stored on disk is B = max (0; jRjF M M1 ). <p> Our analysis does not include the I/O cost and memory requirement of the final output stream from the join. Output costs are typically not considered in join method analyses since they are the same for all methods <ref> [1, 4, 5, 7, 12] </ref>. Before the join operation begins, the tape which holds S has already been inserted and loaded into the tape drive. Since S fits on a single tape, no media changes are required.
Reference: [5] <author> L. M. Haas, M. J. Carey, and M. Livny. </author> <title> SEEKing the truth about ad hoc join costs. </title> <type> Technical Report 1148, </type> <institution> Univ. of Wisconsin at Madison, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In other studies [7] the cost metric has been the number of multi-page I/O requests, regardless of request size. A detailed cost model which combines both cost met-rics has been considered in <ref> [5] </ref>. A tape join study done as part of the Sequioa 2000 project [13] assumes a configuration of two tapes and one tape drive, and hence focuses on media switch delays [11]. The study employs a transfer-only cost model for tape drives, counting the number of chunks, or extents, transferred. <p> Memory size, relative to the size of relations, is typically a key factor in selecting a join method. For disk-based equi-joins, Hybrid Hash Join is commonly viewed as the method of choice when only a small amount of main memory is available, compared to the smaller relation. In <ref> [5] </ref> it is suggested that when a moderate fraction of the smaller relation can fit in memory, Nested Block Join with optimal buffer allocation provides better performance. <p> We assume that all disk accesses are multi-page I/O requests. The cost of a disk access is therefore derived by counting the number of blocks transferred. The cost of seek and latency delays is ignored. As shown in <ref> [5] </ref>, disk seeks and latency play a relatively minor role compared to transfer cost when disk requests are at least moderately large. In our model, the size of all disk requests is assumed to be at least 30 blocks, making seek and latency costs negligible 3 . <p> Our analysis does not include the I/O cost and memory requirement of the final output stream from the join. Output costs are typically not considered in join method analyses since they are the same for all methods <ref> [1, 4, 5, 7, 12] </ref>. Before the join operation begins, the tape which holds S has already been inserted and loaded into the tape drive. Since S fits on a single tape, no media changes are required. <p> It is very similar to the equation derived for the conventional, disk-based Nested Block Join in <ref> [5] </ref> although the scenarios are different. 6 For example, if jRj = 10 M and M R = 0:1 M, relation rocking would save M R jRj = 1% in disk I/O. one disk device, but in practice one would use two disks because the buffer I/O can be effectively parallelized.
Reference: [6] <author> L. M. Haas, M. J. Carey, and M. Livny. </author> <title> Tapes hold data too: Challenges of tuples on tertiary store. </title> <booktitle> In Proc. 11 ACM SIGMOD, </booktitle> <pages> pages 413-417, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: A typical query optimizer of a database management system (DBMS) knows how to deal with only two types of storage devices|primary (i.e. main memory) and secondary (i.e. magnetic disks). Tertiary-resident data is commonly perceived as archived data <ref> [6] </ref> which means that the DBMS is not capable of maneuvering tertiary devices directly. Using services of the operating system, the DBMS first moves all tertiary-resident data to disk, and then optimizes and processes the query. <p> An experimental implementation is described in Section 9. Section 10 describes future work and Section 11 concludes this paper. 2 Related Work Prompted by a SIGMOD Database Challenges paper which highlighted the importance of DBMS access to and control of tertiary storage <ref> [6] </ref>, we set out to study the factors determining the performance of tertiary join operations.
Reference: [7] <author> R. B. Hagmann. </author> <title> An observation on database buffering performance metrics. </title> <booktitle> In Proc. Conf. Very Large Databases, </booktitle> <pages> pages 289-293, </pages> <address> Kyoto, Japan, </address> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded. In other studies <ref> [7] </ref> the cost metric has been the number of multi-page I/O requests, regardless of request size. A detailed cost model which combines both cost met-rics has been considered in [5]. <p> Our analysis does not include the I/O cost and memory requirement of the final output stream from the join. Output costs are typically not considered in join method analyses since they are the same for all methods <ref> [1, 4, 5, 7, 12] </ref>. Before the join operation begins, the tape which holds S has already been inserted and loaded into the tape drive. Since S fits on a single tape, no media changes are required.
Reference: [8] <author> W. Kim. </author> <title> A new way to compute the product and join of relation. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 179-187, </pages> <address> Santa Monica, CA, </address> <month> May </month> <year> 1980. </year>
Reference-contexts: affect the interplay between disk space and processing time? In this paper we examine these questions with the help of 1 In a Hash Join, disk space is required for the original copy of relation S plus a hashed version. 1 two well-known ad hoc join methods: Nested Block Join <ref> [8] </ref> and Hybrid Hash Join [4, 12]. Two join algorithms for tape-resident data that are based on these methods are presented and used to profile the interplay between the amount of disk and memory space allocated to a join operator and how long it takes to process the join. <p> Most studies on disk-based joins employ a system model comprising main memory and disks where disks are represented by a transfer-only cost model <ref> [1, 2, 4, 8, 12] </ref>. In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded. <p> The space overhead incurred when building a hash table for a set of data is denoted by factor F [4]. A summary of the notation appears in Table 2. 5.1 Nested Block Join The conventional Nested Block Join (NB) <ref> [8] </ref> can be used for joining tape-resident S with disk-resident R as follows. <p> Each cycle has an associated time constraint: Reading R from disk should be faster than reading S i from tape. If the constraint holds, the tape drive is streaming and the join is bound by the speed of the tape drive. NBT uses relation rocking <ref> [8, 9] </ref> to save on disk transfers.
Reference: [9] <author> D. Knuth. </author> <title> The Art of Computer Programming, Vol. III: Sorting and Searching. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> Redwood City, CA, </address> <year> 1973. </year>
Reference-contexts: Each cycle has an associated time constraint: Reading R from disk should be faster than reading S i from tape. If the constraint holds, the tape drive is streaming and the join is bound by the speed of the tape drive. NBT uses relation rocking <ref> [8, 9] </ref> to save on disk transfers.
Reference: [10] <author> J. Myllymaki and M. Livny. </author> <title> Disk-tape joins: Synchronizing disk and tape access. </title> <type> Technical Report 1270, </type> <institution> Univ. of Wisconsin at Madison, </institution> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: In other words, as far as the application is concerned, the drive is always streaming. For simplicity, we assume that the tape drive has enough buffer memory to hide these delays. In <ref> [10] </ref>, we present an analysis where smaller buffers are considered and the response time of a tape drive is therefore affected by the stop/start delays. Tape media switch delays which occur in tape robot systems are not included in this cost model.
Reference: [11] <author> S. Sarawagi and M. Stonebraker. </author> <title> Single query optimization for tertiary memory. </title> <type> Technical Report 45, </type> <institution> Univ. of California at Berkeley, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: A detailed cost model which combines both cost met-rics has been considered in [5]. A tape join study done as part of the Sequioa 2000 project [13] assumes a configuration of two tapes and one tape drive, and hence focuses on media switch delays <ref> [11] </ref>. The study employs a transfer-only cost model for tape drives, counting the number of chunks, or extents, transferred. CPU and disk I/O costs are ignored in the cost model. Memory size, relative to the size of relations, is typically a key factor in selecting a join method.
Reference: [12] <author> L. Shapiro. </author> <title> Join processing in database systems with large main memories. </title> <journal> ACM Trans. Database Syst., </journal> <volume> 11(3) </volume> <pages> 239-264, </pages> <month> Sept. </month> <year> 1986. </year>
Reference-contexts: space and processing time? In this paper we examine these questions with the help of 1 In a Hash Join, disk space is required for the original copy of relation S plus a hashed version. 1 two well-known ad hoc join methods: Nested Block Join [8] and Hybrid Hash Join <ref> [4, 12] </ref>. Two join algorithms for tape-resident data that are based on these methods are presented and used to profile the interplay between the amount of disk and memory space allocated to a join operator and how long it takes to process the join. <p> Most studies on disk-based joins employ a system model comprising main memory and disks where disks are represented by a transfer-only cost model <ref> [1, 2, 4, 8, 12] </ref>. In such a cost model the number of pages transferred is the cost metric while the latency penalty of small requests (seek and platter rotation delay) is disregarded. <p> from disk into memory compute S i 1 R j j j + 1 change direction of R scan wait until S i+1 copied to I 1k i i + 1 end 5.3 Hybrid Hash Join Hybrid Hash Join (HH) on tapes operates exactly as the classic Hybrid Hash Join <ref> [4, 12] </ref> except that Phase I where both relations are hash partitioned on disk is modified to read relation S from tape. The number of hash partitions stored on disk is B = max (0; jRjF M M1 ). <p> Our analysis does not include the I/O cost and memory requirement of the final output stream from the join. Output costs are typically not considered in join method analyses since they are the same for all methods <ref> [1, 4, 5, 7, 12] </ref>. Before the join operation begins, the tape which holds S has already been inserted and loaded into the tape drive. Since S fits on a single tape, no media changes are required. <p> Taking into account M S = 0:9M and the fudge factor F , the disk space requirement is M 1:8=F . 6.5 Hybrid Hash Join In <ref> [12] </ref>, it is shown that HH requires at least p jRjF blocks of memory if recursive partitioning is to be avoided.
Reference: [13] <author> M. Stonebraker. </author> <title> An overview of the Sequioa 2000 project. </title> <type> Technical Report 5, </type> <institution> Univ. of California at Berkeley, </institution> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: In other studies [7] the cost metric has been the number of multi-page I/O requests, regardless of request size. A detailed cost model which combines both cost met-rics has been considered in [5]. A tape join study done as part of the Sequioa 2000 project <ref> [13] </ref> assumes a configuration of two tapes and one tape drive, and hence focuses on media switch delays [11]. The study employs a transfer-only cost model for tape drives, counting the number of chunks, or extents, transferred. CPU and disk I/O costs are ignored in the cost model.
Reference: [14] <author> R. Thomas. </author> <title> Cache memory splits computer and tape operations. </title> <booktitle> Computer Design, </booktitle> <volume> 24(13) </volume> <pages> 89-93, </pages> <month> Oct. </month> <year> 1985. </year>
References-found: 14

