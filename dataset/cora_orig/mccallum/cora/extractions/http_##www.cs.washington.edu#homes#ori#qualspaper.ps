URL: http://www.cs.washington.edu/homes/ori/qualspaper.ps
Refering-URL: http://www.cs.washington.edu/homes/ori/
Root-URL: 
Title: The Performance of the Pettis and Hansen Code Reordering Algorithm on CISC and RISC Architectures  
Author: Ori Gershony 
Date: April 30, 1997  
Address: Box 352350  Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering,  University of Washington  
Abstract: The impact of instruction cache misses and branch mispredictions on performance is particularly important for processors which issue multiple instructions per cycle. Code reordering is one technique that can improve the effectiveness of instruction caches and branch prediction. This paper explores the behavior of the Pettis and Hansen code reordering algorithm on CISC and RISC architectures. We show that this algorithm substantially reduces the instruction cache miss rate, the branch target buffer miss rate, and the number of branch mispredicts, and that the decrease in the instruction cache miss rate is greatest for CISC architectures.
Abstract-found: 1
Intro-found: 1
Reference: [Bekerman & Mendelson 95] <author> Bekerman, M. and Mendelson, A. </author> <title> A Performance Analysis of Pentium Processor Systems. </title> <booktitle> In IEEE Micro, </booktitle> <pages> pages 72-83, </pages> <month> October </month> <year> 1995. </year>
Reference: [Bhandarkar & Ding 97] <author> Bhandarkar, D. and Ding, J. </author> <title> Performance Characterization of the Pentium Pro Processor. </title> <booktitle> In IEEE Micro, </booktitle> <pages> pages 288-297, </pages> <year> 1997. </year>
Reference-contexts: The overall performance improvement is equal to the number of cycles lost divided by the CPI. On the Pentium Pro, the CPI for gcc is around 1.4 <ref> [Bhandarkar & Ding 97] </ref>.
Reference: [Bhandarkar 97] <author> Bhandarkar, D. </author> <title> RISC versus CISC: A Tale of Two Chips. </title> <journal> In Computer Architecture News, </journal> <volume> 25, </volume> <pages> pages 1-12, </pages> <month> March 1 </month> <year> 1997. </year>
Reference-contexts: Note that these penalties are not necessarily the same as the ones in the 21164, or the Pentium Pro. In particular, the penalty for a mispredicted branch on the Pentium Pro is 10-15 cycles-quite a bit more than the 5 cycle penalty on the Alpha <ref> [Bhandarkar 97] </ref>, but the branch mispredict rate on the Pentium Pro is also lower because it uses the two-level adaptive scheme of Yeh and Pett [Yeh & Patt 91] which we didn't simulate in this study.
Reference: [Calder & Grunwald 94] <author> Calder, B. and Grunwald, D. </author> <title> Reducing Branch Costs via Branch Alignment. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 242-251, </pages> <month> October 4-7, </month> <year> 1994. </year>
Reference: [Digital Equipment Corporation 94] <author> Digital Equipment Corporation. </author> <title> ATOM user manual, </title> <year> 1994. </year>
Reference-contexts: One way is to reorder the instructions as a post-optimization phase of the compiler. The main disadvantage of this method is that it requires the source code for the compiler, and the work will be specific to a compiler. Alternatively, a binary instrumentation tool such as ATOM <ref> [Digital Equipment Corporation 94] </ref> or Etch [University of Washington 96] can be used to modify an existing binary. A problem with this method is that very few architectures have tools that can reorder the instructions of a binary. <p> A problem with this method is that very few architectures have tools that can reorder the instructions of a binary. Also, optimizing compilers often generate better binaries than those produced by instrumentation tools. We implemented the Pettis and Hansen code reodering algorithm using binary instrumentation tools. We used ATOM <ref> [Digital Equipment Corporation 94] </ref> on an Alpha 21164 and Etch [University of Washington 96] on an Intel Pentium Pro. Although ATOM and Etch have different programming interfaces, their capabilities are quite similar, and we were able to share most of the code between them.
Reference: [Hwu & Chang 89] <author> Hwu, W. and Chang, P. </author> <title> Achieving High Instruction Cache Performance with an Optimizing Compiler. </title> <booktitle> In Proceedings of the ACM, </booktitle> <pages> pages 242-251, </pages> <year> 1989 </year>
Reference-contexts: Pettis and Hansen [Pettis & Hansen 90] proposed the algorithm that we implemented for this work. Their algorithm yields an impressive reduction in the instruction cache miss rate and does not use inlining which can increase the size of the binary. Hwu and Chang <ref> [Hwu & Chang 89] </ref> proposed a similar algorithm, which also includes an inline expansion phase, and which performs layout of traces (basic blocks which tend to execute in sequence) instead of basic blocks.
Reference: [Lee 94] <author> Lee, D. </author> <title> Instruction Cache Effects of Different Code Reordering Algorithms. </title> <type> Quals Report, </type> <institution> University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Third, the instruction cache miss rate is decreased because of the reduction in the number of conflict misses and the increase in the utilization of cache lines <ref> [Lee 94] </ref>. Finally, code positioning can reduce the size of the working set because of fluff removal (movement of rarely used error handling code to the end of the binary). <p> His algorithm is sensitive to the size of the instruction cache, and does not give a comparable increase in the branch fall-through rate, but can yield the lowest instruction cache miss rate. More recently, Lee <ref> [Lee 94] </ref> compared the performance of several code reordering algorithms. His study was restricted to a single architecture (DEC Alpha), and he focused on the improvement in the instruction cache miss rate.
Reference: [Lee et al. 95] <author> Lee, D. and Baer, J. and Calder, B. and Grunwald, D. </author> <title> Instruction Cache Fetch Policies for Speculative Execution. </title> <booktitle> In Proceedings of the ACM, </booktitle> <pages> pages 357-367, </pages> <year> 1995. </year>
Reference: [McFarling 89] <author> McFarling, S. </author> <title> Program Optimization For Instruction Caches. </title> <booktitle> In Proc. 3rd International Conference On Architectural Support For Programming Languages And Operating Systems, </booktitle> <pages> pages 183-191, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Hwu and Chang [Hwu & Chang 89] proposed a similar algorithm, which also includes an inline expansion phase, and which performs layout of traces (basic blocks which tend to execute in sequence) instead of basic blocks. McFarling <ref> [McFarling 89] </ref> takes a slightly different approach: instead of trying to position related basic blocks close to each other, he tries to position them in non-conflicting cache locations.
Reference: [Perl & Sites 96] <author> Perl, S. and Sites, R. </author> <title> Studies of Windows NT Performance using Dynamic Execution Traces. </title> <booktitle> In Proc. 2nd Symposium On Operating Systems Design and Implementation, </booktitle> <pages> pages 169-183, </pages> <year> 1996 </year>
Reference: [Pettis & Hansen 90] <author> Pettis, K. and Hansen, R. </author> <title> Profile Guided Code Positioning. </title> <booktitle> In Proc. Conference On Programming Language Design And Implementation, </booktitle> <pages> pages 16-26, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In particular, exposing aspects of the memory hierarchy and of the branch control mechanism allows code positioning algorithms to improve performance by reordering the basic blocks of a binary. Code positioning algorithms typically use profiling information to decide the order of basic blocks in the final binary <ref> [Pettis & Hansen 90] </ref>. For example, suppose that profiling shows that a branch B is taken 90% of the time. <p> Speculative execution which is the norm in high performance processors imposes very high penalties upon branch mispredictions, which could be reduced with a lower BTB miss rate. In this paper, we evaluate the performance improvement of the Pettis and Hansen code reordering algorithm <ref> [Pettis & Hansen 90] </ref> on both CISC (Intel IA32) and RISC (Dec Alpha) architectures. We show that the algorithm substantially improves the fall-through rate of control transfer instructions, the instruction 1 cache miss rate, and branch prediction. <p> We also show that the improvement in the instruction cache miss rate is higher on the IA32 architecture, and propose several explanations. 1.1 Related Work Several papers published over the past few years have proposed various code reordering algorithms. Pettis and Hansen <ref> [Pettis & Hansen 90] </ref> proposed the algorithm that we implemented for this work. Their algorithm yields an impressive reduction in the instruction cache miss rate and does not use inlining which can increase the size of the binary. <p> This paper extends his work by considering multiple architectures and multiple performance metrics. The rest of this paper In Section 2 we explain the Pettis and Hansen algorithm <ref> [Pettis & Hansen 90] </ref>. We describe our systems, benchmarks and experiments in Section 3, and our results in Section 4. <p> We conclude with suggestions for future work in Section 5 and a summary and a conclusion in Section 6. 2 The Pettis and Hansen Code Reordering Algorithm The Pettis and Hansen code positioning algorithm <ref> [Pettis & Hansen 90] </ref> consists of two main parts: procedure positioning and basic block positioning. Basic block positioning is done separately for each procedure and so code from different procedures is not mixed. We describe this local approach in Section 2.1.
Reference: [University of Washington 96] <institution> University of Washington. Instrumentation and Optimization of Win32 Programs Using Etch, </institution> <year> 1996. </year>
Reference-contexts: The main disadvantage of this method is that it requires the source code for the compiler, and the work will be specific to a compiler. Alternatively, a binary instrumentation tool such as ATOM [Digital Equipment Corporation 94] or Etch <ref> [University of Washington 96] </ref> can be used to modify an existing binary. A problem with this method is that very few architectures have tools that can reorder the instructions of a binary. Also, optimizing compilers often generate better binaries than those produced by instrumentation tools. <p> Also, optimizing compilers often generate better binaries than those produced by instrumentation tools. We implemented the Pettis and Hansen code reodering algorithm using binary instrumentation tools. We used ATOM [Digital Equipment Corporation 94] on an Alpha 21164 and Etch <ref> [University of Washington 96] </ref> on an Intel Pentium Pro. Although ATOM and Etch have different programming interfaces, their capabilities are quite similar, and we were able to share most of the code between them. The measurement process consists of three distinct phases.
Reference: [Yeh & Patt 91] <author> Yeh, T. and Patt, Y. </author> <title> Two-Level Adaptive Branch Prediction. </title> <booktitle> In Proceedings of the 24th Annual ACM/IEEE International Symposium and Workshop on Microarchitecture, </booktitle> <pages> pages 51-61, </pages> <month> November </month> <year> 1991. </year> <month> 14 </month>
Reference-contexts: particular, the penalty for a mispredicted branch on the Pentium Pro is 10-15 cycles-quite a bit more than the 5 cycle penalty on the Alpha [Bhandarkar 97], but the branch mispredict rate on the Pentium Pro is also lower because it uses the two-level adaptive scheme of Yeh and Pett <ref> [Yeh & Patt 91] </ref> which we didn't simulate in this study. We used the same schemes and penalties on both architectures so that we could have a meaningful comparison.
References-found: 13

