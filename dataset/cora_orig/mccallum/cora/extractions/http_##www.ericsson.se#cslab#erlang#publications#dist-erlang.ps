URL: http://www.ericsson.se/cslab/erlang/publications/dist-erlang.ps
Refering-URL: http://www.csd.uu.se/~bjornfot/r1.html
Root-URL: 
Title: Distributed programming in Erlang  
Author: Claes Wikstrom 
Keyword: distributed programming, concurrency, symbolic programming, programming languages, fault tolerance, real-time systems  
Address: Box 1505 S 125 25 Alvsjo Sweden  
Affiliation: Computer Science Laboratory Ellemtel Telecommunications Systems Laboratory  
Abstract: The construction of computer systems consisting of more than one computer is becoming more common. The complexity of such systems is higher than single computer systems. This paper addresses the question of how to simplify the construction of large concurrent distributed systems. To do this we have augmented the functional concurrent programming language Erlang with constructs for distributed programming. Distributed programs written in Erlang typically combine techniques for symbolic functional programming with techniques for distributed programming. In contrast to traditional imperative languages Erlang does not need interface description languages to specify the format of interprocessor messages in a heterogeneous network. This considerably simplifies distributed programming. Distributed Erlang is currently being employed in a number of large software projects within the Ericsson group. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Armstrong, J. L., Williams, M. C. and Virding, S. R., </author> <title> Concurrent Programming in Erlang, </title> <publisher> Prentice Hall (1993) </publisher>
Reference-contexts: Erlang is typeless in the same sense as traditional logic languages, uses pattern matching for variable binding and function selection, has explicit mechanisms to create concurrent processes and advanced facilities for error detection and recovery. Non distributed Erlang is described in <ref> [1] </ref>. An efficient implementation of non distributed Erlang is described in [8]. The construction of large distributed fault-tolerant concurrent real-time system is an extremely complex task.
Reference: [2] <author> Bal, H. </author> <title> Programming distributed systems Prentice Hall 1990. </title>
Reference-contexts: There are no shared resources and each node is completely autonomous. By now there are a number of languages which support distributed programming. In distributed imperative languages such as Orca <ref> [2] </ref> and Concurrent C [7] all inter process communication interfaces have to be specified at compile time. We will show the advantage of not having to specify interfaces at compile time.
Reference: [3] <author> Birell, A.D., Nelsson, B. J. </author> <title> Implementing remote procedure calls ACM Trans. </title> <journal> Comp. Syst. </journal> <volume> Vol. 2 No. </volume> <month> 1 </month> <year> 1984. </year>
Reference-contexts: This is basically done with a small set of new BIFs. A number of typical models for distributed computing have been implemented with the aid of the new BIFs and they can all be found in Erlang standard libraries. This includes for example: Remote Procedure Call (RPC) <ref> [3] </ref>, different broadcast algorithms, a global name server, asynchronous RPC, ISIS like named process groups with mul-ticasting [4], multi node dictionaries, a load distribution package, a parallel evaluator, shared data structures etc. In this section we will give (shortened) implementations of some of these.
Reference: [4] <author> Birman, K., Cooper, R., Marzullo, K., Makpangou, M., Kane, K., Schmuck, F., Wood, M. </author> <note> The ISIS system manual 2.1 Cornell University 1990 </note>
Reference-contexts: This includes for example: Remote Procedure Call (RPC) [3], different broadcast algorithms, a global name server, asynchronous RPC, ISIS like named process groups with mul-ticasting <ref> [4] </ref>, multi node dictionaries, a load distribution package, a parallel evaluator, shared data structures etc. In this section we will give (shortened) implementations of some of these. The new BIFs are as follows: * spawn (Node, Mod, Fun, Arglist).
Reference: [5] <author> Diaz, M., Rubio, B., Troya, J.M. </author> <title> Implementation issues of a distributed real-time logic language, </title> <booktitle> International Conference of Logic Programming, Workshop on 'Integration of Deductive Paradigms', </booktitle> <year> 1994. </year>
Reference-contexts: It is considerably easier to implement distributed applications in Erlang than in these languages due to the high level of abstraction in the Erlang code. Extensions to concurrent logic languages that support the programming of loosely coupled systems can be found in [6] and <ref> [5] </ref>. These system have many similarities with distributed Erlang. From a distributed programming perspective, the main difference is that the unit of concurrency in Concurrent Logic Languages is the goal and the clause. <p> In a loosely coupled system it is not appropriate to have the goals of a clause evaluated in parallel on different nodes due to the overhead that is associated with the network. To circumvent this, the language DRL <ref> [5] </ref> defines an additional unit of concur-rency called the grain, where a grain is a set of predicates. In Erlang the unit of concurrency is the same as the unit of distribution, namely a process.
Reference: [6] <author> Foster, I. </author> <title> Parallel implementation of PARLOG, </title> <booktitle> Proc. 1988 Int. Conf. Parallel Processing (Vol. II), </booktitle> <pages> pp 9-16, </pages> <address> St. Charles, IL. </address>
Reference-contexts: It is considerably easier to implement distributed applications in Erlang than in these languages due to the high level of abstraction in the Erlang code. Extensions to concurrent logic languages that support the programming of loosely coupled systems can be found in <ref> [6] </ref> and [5]. These system have many similarities with distributed Erlang. From a distributed programming perspective, the main difference is that the unit of concurrency in Concurrent Logic Languages is the goal and the clause.
Reference: [7] <author> Gehani, N., Roome, W. </author> <title> Concurrent C, </title> <publisher> The Programming Language Prentice Hall 1989. </publisher>
Reference-contexts: There are no shared resources and each node is completely autonomous. By now there are a number of languages which support distributed programming. In distributed imperative languages such as Orca [2] and Concurrent C <ref> [7] </ref> all inter process communication interfaces have to be specified at compile time. We will show the advantage of not having to specify interfaces at compile time.
Reference: [8] <author> Hausman, B. </author> <title> Turbo Erlang: Approaching the Speed of C Implementations of Logic programming Systems, </title> <editor> ed. Tick. E, </editor> <publisher> Kluwer 1994. </publisher>
Reference-contexts: Non distributed Erlang is described in [1]. An efficient implementation of non distributed Erlang is described in <ref> [8] </ref>. The construction of large distributed fault-tolerant concurrent real-time system is an extremely complex task. The programs that are used to control a telephony network typically consist of several millions lines of source fl email: klacke@erix.ericsson.se y A company jointly owned by Ericsson and Telia AB code. <p> One such abstract machine is described in <ref> [8] </ref>. In this section we describe the implementation of the distribution aspects of the language. In UNIX an Erlang node executes as one UNIX process internally containing many Erlang processes.
Reference: [9] <author> Kunz, T. </author> <title> The influence of different workload descriptions on a heuristic load balancing scheme. </title> <journal> IEEE Trans. Software Eng., </journal> <volume> Vol. 17, No. 7, </volume> <month> July </month> <year> 1991 </year> <month> pp. </month> <pages> 725-730 </pages>
Reference-contexts: This is known as static load distribution. If we want to implement dynamic load distribution, we can use the BIF statistics (run_queue) which returns the number of processes that are scheduled to run. This has been showed in <ref> [9] </ref> to be a most effective way of predicting the future load of a node. 3.4 A pseudo server Finally we will give the implementation for something we call a pseudo server.
Reference: [10] <author> Meer, J. </author> <title> A Prototype demonstrating User Mobility and Flexible Service Profiles. </title> <journal> Ericsson Review. </journal> <volume> no 1, </volume> <year> 1994. </year>
Reference-contexts: Distributed Erlang is not an experimental system. It is currently being used in a number of product projects within Ericsson. One interesting prototype project that has been reported is the implementation of an Intelligent Networks Development System <ref> [10] </ref>. Distributed 2 PASCO'94: First International Symposium on Parallel Symbolic Computation Erlang is implemented as a very loosely coupled system of computers communicating with TCP/IP connections. There are no shared resources and each node is completely autonomous. By now there are a number of languages which support distributed programming.
Reference: [11] <author> Ladin, R., Liskov, B. </author> <title> Garbage Collection of a distributed heap The 12th International Conference on Distributed System, </title> <address> Yokohama Japan, </address> <year> 1992. </year>
Reference-contexts: Each node is autonomous, and since there are no logical variables and network messages are always copied, there are never any references between nodes. This means that we can do local garbage collection. Some algorithms for distributed garbage collection have been proposed, for example <ref> [11] </ref>. We feel however that none of the proposed algorithms are appropriate for the type of fault tolerant, soft real-time systems that Erlang is intended for.
Reference: [12] <author> Liskov, B. </author> <title> Linguistic Support for Efficient Asynchronous Calls in Distributed Systems Proceedings of the SIGPLAN 88. </title>
Reference-contexts: This means that the caller is suspended until the computation terminates. The main disadvantage of RPCs is that we have to do an idle wait for the answer, although we may have something better to do than to wait. The use of promises has been suggested in <ref> [12] </ref>. Promises are an asynchronous variant of remote procedure calls and a promise is a place holder for a future return value from an RPC. This overcomes PASCO'94: First International Symposium on Parallel Symbolic Computation 5 the disadvantages that stem from the synchronous nature of the RPC.
Reference: [13] <editor> SunOs 4.0 reference manual Vol. </editor> <address> 10 1987 Sun Mi-crosystems, </address> <publisher> Inc. </publisher>
Reference-contexts: If we examine the RPC facility from Section 3. it is worth noting that we did not have to specify the interface to the RPC server. In traditional RPC systems, there is usually an interface description language involved. An example of such a language is Sun Microsystems RPC language <ref> [13] </ref>. <p> There is a tradeoff between reliability and speed. Since message passing has to be completely reliable TCP is used as transport medium as opposed to UDP. We have also measured the speed of the RPC application in the paper and compared it to the RPC facility in SunOs 4.1 <ref> [13] </ref>. We have measured the time to do a single RPC. The function we call is a function which simply returns it's argument where the argument is an integer or a structure/tuple containing one integer and three short strings.
References-found: 13

