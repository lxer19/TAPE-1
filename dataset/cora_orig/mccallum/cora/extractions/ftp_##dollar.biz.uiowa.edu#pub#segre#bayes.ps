URL: ftp://dollar.biz.uiowa.edu/pub/segre/bayes.ps
Refering-URL: http://www.biz.uiowa.edu/mansci/faculty/asegre.html
Root-URL: 
Email: wlam@se.cuhk.edu.hk  alberto-segre@uiowa.edu  
Title: A Parallel Learning Algorithm for Bayesian Inference Networks  
Author: Alberto Maria Segre 
Keyword: Machine Learning, Bayesian Networks, Minimum Description Length Principle, Distributed Systems  
Note: Support for this research was provided by the Office of Naval Research through grant N0014-94-1-1178, and by the Advanced Research Project Agency through Rome Laboratory Contract Number F30602-93-C-0018 via Odyssey Research Associates, Incorporated.  
Address: Shatin Hong Kong  Iowa City, Iowa 52242 U.S.A.  
Affiliation: Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong  Department of Management Sciences The University of Iowa  
Abstract: We present a new parallel algorithm for learning Bayesian inference networks from data. Our learning algorithm exploits both properties of the MDL-based score metric, and a distributed, asynchronous, adaptive search technique called nagging. Nagging is intrinsically fault tolerant, has dynamic load balancing features, and scales well. We demonstrate the viability, effectiveness, and scalability of our approach empirically with several experiments using on the order of 20 machines. More specifically, we show that our distributed algorithm can provide optimal solutions for larger problems as well as good solutions for Bayesian networks of up to 150 variables. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Abramson. ARCO1: </author> <title> An application of belief networks to the oil market. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 1-8, </pages> <year> 1991. </year>
Reference-contexts: already make use of this powerful and flexible knowledge representation scheme: * Image Understanding: A system developed at Naval Research Laboratory employs Bayesian networks to perform ship classification from raw sensor images [18]. * Forecasting: The ARCO1 system is able to both reason about and forecast the crude oil market <ref> [1] </ref>. * Information Retrieval: A Bayesian network is used to retrieve documents relevant to a particular information need from a huge collection of information stored in electronic media [9]. * Intelligent Decision Making: The Vista system, developed at NASA Mission Control Center, interprets live telemetry data and assesses the operation of
Reference: [2] <author> I. A. Beinlich, H. J. Suermondt, R. M. Chavez, and G. F. Cooper. </author> <title> The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks. </title> <booktitle> In Proceedings of the 2nd European Conference on Artificial Intelligence in Medicine, </booktitle> <pages> pages 247-256, </pages> <year> 1989. </year>
Reference-contexts: Both real-world and randomly-generated target network structures were used to generate experimental data sets. The first data set is based on the ALARM Bayesian inference network used to model real-world anesthesia problems in an operating room environment <ref> [2] </ref>. This network consists of 37 variables and 46 arcs. The 10,000 case data set associated with this network is commonly used as a benchmark; the input variable ordering used in our experiments is identical to that used previously by other researchers [8, 28].
Reference: [3] <author> R. Bouckaert. </author> <title> Properties of Bayesian belief network learning algorithms. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 102-109, </pages> <year> 1994. </year>
Reference-contexts: A fundamental property of data description length is that the more accurate the network structure, 2 Other researchers have proposed alternative metrics for network description length that differ from the one we have just described <ref> [27, 3] </ref>. We note that the parallelization scheme we propose in subsequent sections of this paper is also applicable to systems based on many of these alternative metrics. 7 the smaller its data description length. Thus, by minimizing this length function, we favor accurate networks.
Reference: [4] <author> W. Buntine. </author> <title> Theory refinement on Baysian networks. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 52-60, </pages> <year> 1991. </year>
Reference-contexts: Clearly, any mechanism that can help automate this task would be beneficial. One technique to cope with this problem is to learn the network model from data pertinent to the domain <ref> [12, 8, 4, 19, 24, 28] </ref>. Unfortunately, since this problem is believed to be NP-complete [7], learning larger models requires exponentially increasing computational resources.
Reference: [5] <author> L. Burnell and E. Horvitz. </author> <title> Structure and chance: Melding logic and probability for software debugging. </title> <journal> Communications of the ACM, </journal> <volume> 38(3) </volume> <pages> 31-41, </pages> <year> 1995. </year>
Reference-contexts: Medical Diagnosis: The PATHFINDER system [10] performs diagnosis of lymph node pathology for over 60 diseases: it has been recently transferred to a commercial system called INTELLIPATH, which is in used by several hundred medical and clinical sites. 1 * Other Applications: Other application areas include, for example, software maintenance <ref> [5] </ref>, natural language understanding [6], troubleshooting [11]. Despite these successful system deployments, systems designers who intend to use Bayesian networks like designers of knowledge-based systems in general encounter the knowledge engineering bottleneck; that is, constructing a network manually is both time consuming and prone to error.
Reference: [6] <author> E. Charniak and R. Goldman. </author> <title> A probabilistic model of plan recognition. </title> <booktitle> In Proceedings of the AAAI National Conference, </booktitle> <pages> pages 160-165, </pages> <year> 1991. </year>
Reference-contexts: system [10] performs diagnosis of lymph node pathology for over 60 diseases: it has been recently transferred to a commercial system called INTELLIPATH, which is in used by several hundred medical and clinical sites. 1 * Other Applications: Other application areas include, for example, software maintenance [5], natural language understanding <ref> [6] </ref>, troubleshooting [11]. Despite these successful system deployments, systems designers who intend to use Bayesian networks like designers of knowledge-based systems in general encounter the knowledge engineering bottleneck; that is, constructing a network manually is both time consuming and prone to error.
Reference: [7] <author> G. F. Cooper. </author> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 393-405, </pages> <year> 1990. </year>
Reference-contexts: Clearly, any mechanism that can help automate this task would be beneficial. One technique to cope with this problem is to learn the network model from data pertinent to the domain [12, 8, 4, 19, 24, 28]. Unfortunately, since this problem is believed to be NP-complete <ref> [7] </ref>, learning larger models requires exponentially increasing computational resources. In this paper, we present a new distributed solution to the Bayesian network learning problem that exploits idle or under-utilized workstations in order to extend the size of the largest problem which can be solved in reasonable time. <p> A fundamental property of network description length is that the higher the topological complexity of the network, the greater its network description length. Since it is widely recognized that conducting inference on highly-connected networks is likely to be intractable <ref> [7] </ref>, they are not very useful in practice. Thus, by minimizing this length function, we favor more useful networks, that is, those having simpler topology. 2 Apart from the simplicity issue, we also need to consider the accuracy issue, or how well a network structure represents the data.
Reference: [8] <author> G. F. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: Clearly, any mechanism that can help automate this task would be beneficial. One technique to cope with this problem is to learn the network model from data pertinent to the domain <ref> [12, 8, 4, 19, 24, 28] </ref>. Unfortunately, since this problem is believed to be NP-complete [7], learning larger models requires exponentially increasing computational resources. <p> using standard statistical methods once the structure is determined, in this paper we focus on the problem of constructing a network structure based on the input data and ordering constraints. 3 A Simple Serial Solution One family of techniques for learning Bayesian network structures is based on a scoring approach <ref> [8, 12, 16, 27] </ref>. <p> Thus scoring systems generally resort to greedy or heuristic search methods that find reasonable - but suboptimal solutions quickly. As for the score metrics, examples such as the BD and K2 metrics in <ref> [12, 8] </ref> are the relative posterior probability of a network structure, while the score metrics used in [14, 27] are cost functions representing the description (message) length of a network structure. <p> This network consists of 37 variables and 46 arcs. The 10,000 case data set associated with this network is commonly used as a benchmark; the input variable ordering used in our experiments is identical to that used previously by other researchers <ref> [8, 28] </ref>. The other data sets used were based on randomly-generated Bayesian network structures with 30, 70, 100, and 150 boolean variables and having vertices of average degree between 2 and 4. <p> For instance, the quality of the solution found is better than the solution obtained by K2, a well-known algorithm for learning Bayesian networks <ref> [8] </ref> (for the same 500 cases, K2 obtains a solution with 7 missing and 2 extra arcs).
Reference: [9] <author> R. Fung and B. Del Favero. </author> <title> Applying Bayesian networks to information retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 38(3) </volume> <pages> 42-48, </pages> <year> 1995. </year>
Reference-contexts: from raw sensor images [18]. * Forecasting: The ARCO1 system is able to both reason about and forecast the crude oil market [1]. * Information Retrieval: A Bayesian network is used to retrieve documents relevant to a particular information need from a huge collection of information stored in electronic media <ref> [9] </ref>. * Intelligent Decision Making: The Vista system, developed at NASA Mission Control Center, interprets live telemetry data and assesses the operation of the space shuttle's propulsion systems [13]. * Process monitoring: General Electric's GEMS expert system monitors power generation equipment performance [17]. * Medical Diagnosis: The PATHFINDER system [10] performs
Reference: [10] <author> D. Heckerman, J. Breese, and B. Nathwani. </author> <title> Toward normative expert systems i: The PATHFINDER project. </title> <booktitle> Methods of Information in Medicine, </booktitle> <volume> 31 </volume> <pages> 90-105, </pages> <year> 1992. </year> <month> 31 </month>
Reference-contexts: media [9]. * Intelligent Decision Making: The Vista system, developed at NASA Mission Control Center, interprets live telemetry data and assesses the operation of the space shuttle's propulsion systems [13]. * Process monitoring: General Electric's GEMS expert system monitors power generation equipment performance [17]. * Medical Diagnosis: The PATHFINDER system <ref> [10] </ref> performs diagnosis of lymph node pathology for over 60 diseases: it has been recently transferred to a commercial system called INTELLIPATH, which is in used by several hundred medical and clinical sites. 1 * Other Applications: Other application areas include, for example, software maintenance [5], natural language understanding [6], troubleshooting
Reference: [11] <author> D. Heckerman, J. Breese, and K. Rommelse. </author> <title> Decision-theoretic troubleshooting. </title> <journal> Com--munications of the ACM, </journal> <volume> 38(3) </volume> <pages> 49-56, </pages> <year> 1995. </year>
Reference-contexts: performs diagnosis of lymph node pathology for over 60 diseases: it has been recently transferred to a commercial system called INTELLIPATH, which is in used by several hundred medical and clinical sites. 1 * Other Applications: Other application areas include, for example, software maintenance [5], natural language understanding [6], troubleshooting <ref> [11] </ref>. Despite these successful system deployments, systems designers who intend to use Bayesian networks like designers of knowledge-based systems in general encounter the knowledge engineering bottleneck; that is, constructing a network manually is both time consuming and prone to error.
Reference: [12] <author> D. Heckerman, D. Geiger, and D. M. Chickering. </author> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: Clearly, any mechanism that can help automate this task would be beneficial. One technique to cope with this problem is to learn the network model from data pertinent to the domain <ref> [12, 8, 4, 19, 24, 28] </ref>. Unfortunately, since this problem is believed to be NP-complete [7], learning larger models requires exponentially increasing computational resources. <p> using standard statistical methods once the structure is determined, in this paper we focus on the problem of constructing a network structure based on the input data and ordering constraints. 3 A Simple Serial Solution One family of techniques for learning Bayesian network structures is based on a scoring approach <ref> [8, 12, 16, 27] </ref>. <p> Thus scoring systems generally resort to greedy or heuristic search methods that find reasonable - but suboptimal solutions quickly. As for the score metrics, examples such as the BD and K2 metrics in <ref> [12, 8] </ref> are the relative posterior probability of a network structure, while the score metrics used in [14, 27] are cost functions representing the description (message) length of a network structure. <p> The structural difference metric, commonly used in Bayesian network learning problems <ref> [12] </ref>, attempts to quantify the solution's structural differences from the original target network.
Reference: [13] <author> E. Horvitz and M. Barry. </author> <title> Display of information for time-critical decision making. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 296-305, </pages> <year> 1995. </year>
Reference-contexts: network is used to retrieve documents relevant to a particular information need from a huge collection of information stored in electronic media [9]. * Intelligent Decision Making: The Vista system, developed at NASA Mission Control Center, interprets live telemetry data and assesses the operation of the space shuttle's propulsion systems <ref> [13] </ref>. * Process monitoring: General Electric's GEMS expert system monitors power generation equipment performance [17]. * Medical Diagnosis: The PATHFINDER system [10] performs diagnosis of lymph node pathology for over 60 diseases: it has been recently transferred to a commercial system called INTELLIPATH, which is in used by several hundred medical
Reference: [14] <author> W. Lam. </author> <title> Bayesian network refinement via machine learning approach. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <note> To appear. </note>
Reference-contexts: As for the score metrics, examples such as the BD and K2 metrics in [12, 8] are the relative posterior probability of a network structure, while the score metrics used in <ref> [14, 27] </ref> are cost functions representing the description (message) length of a network structure. Alternatively, other approaches for this learning problem are characterized by employing conditional independence relations exemplified by the SGS and PC algorithms [23, 22] as well as the techniques developed by Pearl et. al. [29, 19].
Reference: [15] <author> W. Lam and F. Bacchus. </author> <title> Using causal information and local measure to learn Bayesian networks. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 243-250, </pages> <year> 1993. </year>
Reference-contexts: More precisely, we use a score metric for network structures that is a function representing the total description length, L total (B), of a candidate network structure B. In related work, we have designed a scheme for efficiently computing this description length <ref> [15] </ref> of a candidate network structure B by decomposing it by individual variable X i . <p> Interested readers should refer to <ref> [15] </ref> for derivations and more detailed presentation. 6 components: L total (X i ; X i ) = L network (X i ; X i ) + L data (X i ; X i ) where L network and L data are called network description length and data description length, respectively.
Reference: [16] <author> W. Lam and F. Bacchus. </author> <title> Learning Bayesian belief networks an approach based on the MDL principle. </title> <journal> Computational Intelligence, </journal> <volume> 10(3) </volume> <pages> 269-293, </pages> <year> 1994. </year>
Reference-contexts: using standard statistical methods once the structure is determined, in this paper we focus on the problem of constructing a network structure based on the input data and ordering constraints. 3 A Simple Serial Solution One family of techniques for learning Bayesian network structures is based on a scoring approach <ref> [8, 12, 16, 27] </ref>. <p> In this section, we briefly describe a simple, score-based, solution to the Bayesian network structure learning problem that is based on the result of our previous work <ref> [16] </ref>. 3.1 The Minimum Description Length Metric Our approach makes use of the Minimum Description Length (MDL) principle to balancing between simplicity and accuracy.
Reference: [17] <author> M. Morjaia, J. Rink, W. Smith, G. Klempner, C. Burns, and J. Stein. </author> <title> Commercialization of EPRI's generator expert monitoring system (gems). In Expert System Application for the Electric Power Industry, </title> <address> Phoenix, </address> <year> 1993. </year> <note> EPRI. Also: GE techreport GER-3790. </note>
Reference-contexts: huge collection of information stored in electronic media [9]. * Intelligent Decision Making: The Vista system, developed at NASA Mission Control Center, interprets live telemetry data and assesses the operation of the space shuttle's propulsion systems [13]. * Process monitoring: General Electric's GEMS expert system monitors power generation equipment performance <ref> [17] </ref>. * Medical Diagnosis: The PATHFINDER system [10] performs diagnosis of lymph node pathology for over 60 diseases: it has been recently transferred to a commercial system called INTELLIPATH, which is in used by several hundred medical and clinical sites. 1 * Other Applications: Other application areas include, for example, software
Reference: [18] <author> S. A. Musman, L. W. Chang, and L. B. Booker. </author> <title> Application of a real-time control strategy for Bayesian belief networks to ship classification problem solving. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(3) </volume> <pages> 513-526, </pages> <year> 1993. </year>
Reference-contexts: To wit, there are a number of practical applications which already make use of this powerful and flexible knowledge representation scheme: * Image Understanding: A system developed at Naval Research Laboratory employs Bayesian networks to perform ship classification from raw sensor images <ref> [18] </ref>. * Forecasting: The ARCO1 system is able to both reason about and forecast the crude oil market [1]. * Information Retrieval: A Bayesian network is used to retrieve documents relevant to a particular information need from a huge collection of information stored in electronic media [9]. * Intelligent Decision Making:
Reference: [19] <author> J. Pearl and T. S. Verma. </author> <title> A theory of inferred causation. </title> <booktitle> In Proceedings of the 2nd International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 441-452, </pages> <year> 1991. </year>
Reference-contexts: Clearly, any mechanism that can help automate this task would be beneficial. One technique to cope with this problem is to learn the network model from data pertinent to the domain <ref> [12, 8, 4, 19, 24, 28] </ref>. Unfortunately, since this problem is believed to be NP-complete [7], learning larger models requires exponentially increasing computational resources. <p> Alternatively, other approaches for this learning problem are characterized by employing conditional independence relations exemplified by the SGS and PC algorithms [23, 22] as well as the techniques developed by Pearl et. al. <ref> [29, 19] </ref>.
Reference: [20] <author> R. W. Robinson. </author> <title> Counting unlabeled acyclic digraphs. </title> <booktitle> In Proceedings of the 5th Australian Conference on Combinatorial Mathematics, </booktitle> <pages> pages 28-43, </pages> <year> 1976. </year>
Reference-contexts: This approach is characterized by devising a score metric for a candidate network structure, and searching the space of network structures for the best-scoring structure. 5 Since there are exponential number of candidate network structures for a given number of vari-ables <ref> [20] </ref>, finding an optimal solution is infeasible even for problems of moderate size. Thus scoring systems generally resort to greedy or heuristic search methods that find reasonable - but suboptimal solutions quickly.
Reference: [21] <editor> A.M. Segre and D.B. Sturgill. </editor> <title> Using hundreds of workstations to solve first-order logic problems. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <pages> pages 187-192, </pages> <year> 1994. </year>
Reference-contexts: adding a mechanism by which the information is broadcast to all processors would entail additional overhead and increased communication costs, which may result in lower overall performance. 4.2 A Second Parallel Solution Based on our previous work on nagging, a parallel asynchronous search pruning technique for first-order logic theorem proving <ref> [25, 21] </ref>, we now propose a distributed strategy that addresses the shortcomings of the simple partitioning scheme just described.
Reference: [22] <author> P. Spirtes and C. Glymour. </author> <title> An algorithm for fast recovery of sparse causal graphs. </title> <journal> Social Science Computer Review, </journal> <volume> 9(1) </volume> <pages> 62-71, </pages> <year> 1991. </year>
Reference-contexts: Alternatively, other approaches for this learning problem are characterized by employing conditional independence relations exemplified by the SGS and PC algorithms <ref> [23, 22] </ref> as well as the techniques developed by Pearl et. al. [29, 19].
Reference: [23] <author> P. Spirtes, C. Glymour, and R. Scheines. </author> <title> Causality from probability. </title> <booktitle> In Evolving Knowledge in Natural Science and Artificial Intelligence, </booktitle> <pages> pages 181-199, </pages> <year> 1990. </year>
Reference-contexts: Alternatively, other approaches for this learning problem are characterized by employing conditional independence relations exemplified by the SGS and PC algorithms <ref> [23, 22] </ref> as well as the techniques developed by Pearl et. al. [29, 19].
Reference: [24] <author> P. Spirtes and C. Meek. </author> <title> Learning Bayesian networks with discrete variables from data. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 294-299, </pages> <year> 1995. </year>
Reference-contexts: Clearly, any mechanism that can help automate this task would be beneficial. One technique to cope with this problem is to learn the network model from data pertinent to the domain <ref> [12, 8, 4, 19, 24, 28] </ref>. Unfortunately, since this problem is believed to be NP-complete [7], learning larger models requires exponentially increasing computational resources.
Reference: [25] <editor> D.B. Sturgill and A.M. Segre. </editor> <title> A novel asynchronous parallelization scheme for first-order logic. </title> <booktitle> In Proceedings of the Twelfth Conference on Automated Deduction, Nancy, France, Springer-Verlag Lecture Notes in Computer Science v814, </booktitle> <pages> pages 484-498, </pages> <year> 1994. </year>
Reference-contexts: adding a mechanism by which the information is broadcast to all processors would entail additional overhead and increased communication costs, which may result in lower overall performance. 4.2 A Second Parallel Solution Based on our previous work on nagging, a parallel asynchronous search pruning technique for first-order logic theorem proving <ref> [25, 21] </ref>, we now propose a distributed strategy that addresses the shortcomings of the simple partitioning scheme just described.
Reference: [26] <editor> D.B. Sturgill and A.M. Segre. Nagging: </editor> <title> A distributed adversarial search-pruning technique applied to first-order logic. </title> <journal> Journal of Automated Reasoning, </journal> <note> To appear. </note>
Reference-contexts: Thus a nagger helps the master's search along by determining whether a portion of the master's search space is infeasible. Readers interested in a more complete treatment of nagging and its formal properties are referred to <ref> [26] </ref>. Nagging has a number of desirable properties. For example, it is intrinsically fault tolerant, since losing a nagger due to communication or hardware problems will not compromise the master's solution. In addition, unlike most partitioning approaches, nagging is characterized by infrequent and brief communication between processors. <p> The cost/benefit tradeoff of using such a transformation function naturally depends on the distribution of expected search times for the transformed problems. Other problem transformation functions are also feasible. A discussion of how more sophisticated transformation functions can dramatically improve overall performance can be found in <ref> [26] </ref>. 4.2.2 Recursive Nagging A second feature becomes obvious once we note that each nagger executes precisely the same search algorithm as the master, albeit perhaps with some transformation applied to the space searched. If enough processors are available, a nagger can also be nagged recursively by additional processors.
Reference: [27] <author> J. Suzuki. </author> <title> A construction of Bayesian networks from databases based on an mdl principle. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 266-273, </pages> <year> 1993. </year>
Reference-contexts: using standard statistical methods once the structure is determined, in this paper we focus on the problem of constructing a network structure based on the input data and ordering constraints. 3 A Simple Serial Solution One family of techniques for learning Bayesian network structures is based on a scoring approach <ref> [8, 12, 16, 27] </ref>. <p> As for the score metrics, examples such as the BD and K2 metrics in [12, 8] are the relative posterior probability of a network structure, while the score metrics used in <ref> [14, 27] </ref> are cost functions representing the description (message) length of a network structure. Alternatively, other approaches for this learning problem are characterized by employing conditional independence relations exemplified by the SGS and PC algorithms [23, 22] as well as the techniques developed by Pearl et. al. [29, 19]. <p> A fundamental property of data description length is that the more accurate the network structure, 2 Other researchers have proposed alternative metrics for network description length that differ from the one we have just described <ref> [27, 3] </ref>. We note that the parallelization scheme we propose in subsequent sections of this paper is also applicable to systems based on many of these alternative metrics. 7 the smaller its data description length. Thus, by minimizing this length function, we favor accurate networks.
Reference: [28] <author> J. Suzuki. </author> <title> Learning bayesian belief networks based on the minimum description length principle: An efficient algorithm using the B & B technique. </title> <booktitle> In Proceedings of the Thirteenth International Confernece on Machine Learning, </booktitle> <pages> pages 462-470, </pages> <year> 1996. </year> <month> 33 </month>
Reference-contexts: Clearly, any mechanism that can help automate this task would be beneficial. One technique to cope with this problem is to learn the network model from data pertinent to the domain <ref> [12, 8, 4, 19, 24, 28] </ref>. Unfortunately, since this problem is believed to be NP-complete [7], learning larger models requires exponentially increasing computational resources. <p> On the other hand, if the amount of increase in L network is greater than the amount of decrease in L data , then L total will increase and the new parent set is less desirable. Suzuki <ref> [28] </ref> derived a property that characterizes this tradeoff: Property 3a [Suzuki] If L network (X i ; 1 ) L network (X i ; 1 n fX k g) L data (X i ; 1 n fX k g), then L total (X i ; 2 ) &gt; L total (X <p> This network consists of 37 variables and 46 arcs. The 10,000 case data set associated with this network is commonly used as a benchmark; the input variable ordering used in our experiments is identical to that used previously by other researchers <ref> [8, 28] </ref>. The other data sets used were based on randomly-generated Bayesian network structures with 30, 70, 100, and 150 boolean variables and having vertices of average degree between 2 and 4. <p> A previous system that provides a solution similar in quality to our solutions for the 500-case and 1000-case data sets is described in <ref> [28] </ref>. 5 Note that the notion of an optimal solution is only defined with respect to a given data set. In fact, the original network may well not be the optimal network.
Reference: [29] <author> T. Verma and J. Pearl. </author> <title> Equivalence and synthesis of causal models. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 220-227, </pages> <year> 1990. </year> <month> 34 </month>
Reference-contexts: Alternatively, other approaches for this learning problem are characterized by employing conditional independence relations exemplified by the SGS and PC algorithms [23, 22] as well as the techniques developed by Pearl et. al. <ref> [29, 19] </ref>.
References-found: 29

