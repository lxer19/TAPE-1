URL: http://fmg-www.cs.ucla.edu/classes/239_2.fall98/papers/logfs.ps
Refering-URL: http://fmg-www.cs.ucla.edu/classes/239_2.fall98/weekly.html
Root-URL: http://www.cs.ucla.edu
Title: An Implementation of a Log- Structured File System for UNIX  
Author: Margo Seltzer Marshall Kirk McKusick Carl Staelin 
Affiliation: Harvard University Keith Bostic University of California, Berkeley  University of California, Berkeley  Hewlett-Packard Laboratories  
Abstract: Research results [ROSE91] demonstrate that a log-structured file system (LFS) offers the potential for dramatically improved write performance, faster recovery time, and faster file creation and deletion than traditional UNIX file systems. This paper presents a redesign and implementation of the Sprite [ROSE91] log-structured file system that is more robust and integrated into the vnode interface [KLEI86]. Measurements show its performance to be superior to the 4BSD Fast File System (FFS) in a variety of benchmarks and not significantly less than FFS in any test. Unfortunately, an enhanced version of FFS (with read and write clustering) [MCVO91] provides comparable and sometimes superior performance to our LFS. However, LFS can be extended to provide additional functionality such as embedded transactions and versioning, not easily implemented in traditional file systems. 
Abstract-found: 1
Intro-found: 1
Reference: [BAKE91] <author> Baker, M., Hartman., J., Kupfer., M., Shirriff, L., Ousterhout, J., </author> <title> ``Measurements of a Distributed File System,'' </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <address> Monterey, CA, </address> <month> October </month> <year> 1991, </year> <pages> 198-212. </pages> <note> Published as Operating Systems Review 25, </note> <month> 5 (October </month> <year> 1991). </year>
Reference-contexts: BSD-LFS maintains a count of the number of disk blocks that do not contain useful data. It is decremented whenever a new block is created in the cache. Since many files die in the cache <ref> [BAKE91] </ref>, this number is incremented whenever blocks are deleted, even if they were never written to disk. The second form of accounting keeps track of how much space is currently available for writing.
Reference: [BAKE92] <author> Baker, M., Asami, S., Deprit, E., Ousterhout, S., Seltzer, M., </author> <title> ``Non-Volatile Memory for Fast, Reliable File Systems,'' </title> <booktitle> to appear in Proceedings of the Fifth Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Figure 2 shows the physical layout of the LFS. While FFS flushes individual blocks and files on demand, the LFS must gather data into segments. Usually, there will not be enough dirty blocks to fill a complete segment <ref> [BAKE92] </ref>, in which case LFS writes partial segments. A physical segment contains one or more partial segments. For the remainder of this paper, segment will be used to refer to the physical partitioning of the disk, and partial segment will be used to refer to a unit of writing. <p> If no such partial segment is ever found, then all the segments from the initial directory operation on are discarded. Since partial segments are small <ref> [BAKE92] </ref> this should rarely, if ever, happen. Synchronization To maintain the delicate balance between buffer management, free space accounting and the cleaner, synchronization between the components of the system must be carefully managed. Figure 8 shows each of the synchronization relationships.
Reference: [CAR92] <author> Carson, S., Setia, S., </author> <title> ``Optimal Write Batch Size in Log-structured File Systems'', </title> <booktitle> Proceedings of 1992 Usenix Workshop on File Systems, </booktitle> <address> Ann Arbor, MI, </address> <month> May 21-22 </month> <year> 1992, </year> <pages> 79-91. </pages>
Reference-contexts: For disk controllers that do not coalesce contiguous reads, we use 64K staging buffers (briefly allocated from the regular kernel memory pool) to do transfers. The size of the staging buffer was set to the minimum of the maximum transfer sizes for currently supported disks. However, simulation results in <ref> [CAR92] </ref> show that for current disks, the write size minimizing the read response time is typically about two tracks; two tracks is close to 64 kilobytes for the disks on our systems.
Reference: [KAZA90] <author> Kazar, M., Leverett, B., Anderson, O., Vasilis, A., Bottos, B., Chutani, S., Everhart, C., Mason, A., Tu, S., Zayas, E., </author> <title> ``DECorum File System Architectural Overview,'' </title> <booktitle> Proceedings of the 1990 Summer Usenix Anaheim, </booktitle> <address> CA, </address> <month> June </month> <year> 1990, </year> <pages> 151-164. </pages>
Reference-contexts: The synchronous I/O for file creation and deletion provides file system disk data structure recoverability after failures. However, there exist alternative solutions such as NVRAM hardware [MORA90] and logging software <ref> [KAZA90] </ref>. In a UNIX environment, where the vast majority of files are small [OUST85][BAKE91], the seek times between I/O requests for different files can dominate. No solutions to this problem currently exist in the context of FFS.
Reference: [HAER83] <author> Haerder, T. Reuter, A. </author> <title> ``Principles of Transaction-Oriented Database Recovery'', </title> <journal> Computing Surveys, </journal> <volume> 15(4); 1983, </volume> <pages> 237-318. </pages>
Reference-contexts: In contrast to FFS, LFS writes only to the end of the log and is able to locate potential inconsistencies and recover to a consistent physical state quickly. This part of recovery in LFS is more similar to standard database recovery <ref> [HAER83] </ref> than to fsck. It consists of two parts: initializing all the file system structures from the most recent checkpoint and then ``rolling forward'' to incorporate any modifications that occurred subsequently.
Reference: [KLEI86] <author> S. R. Kleiman, "Vnodes: </author> <title> An Architecture for Multiple File System Types in Sun UNIX," </title> <booktitle> Usenix Conference Proceedings, </booktitle> <month> June </month> <year> 1986, </year> <pages> 238-247. </pages>
Reference-contexts: The FFS and LFS implementations have since been merged to share common code. In BSD and similar systems (i.e., SunOS, OSF/1), a file system is defined by two sets of interface functions, vfs operations and vnode operations <ref> [KLEI86] </ref>. Vfs operations affect entire file systems (e.g., mount, unmount, etc.) while vnode operations affect files (open, close, read, write, etc.). Vnode Operations Read the block at the given offset, from a file.
Reference: [KOHL93] <author> Kohl, J., Staelin, C., Stonebraker, M., ``Highlight: </author> <title> Using a Log-structured File System for Tertiary Storage Management,'' </title> <booktitle> Proceedings 1993 Winter Usenix, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: More sophisticated versioning should be only marginally more complicated. Also, the sequential nature of BSD-LFS write patterns makes it nearly ideal for tertiary storage devices <ref> [KOHL93] </ref>. LFS may be extended to include multiple devices in a single file system. If one or more of these devices is a robotic storage device, such as a tape stacker, then the file system may have tremendous storage capacity.
Reference: [MCKU84] <author> Marshall Kirk McKusick, William Joy, Sam Leffler, and R. S. Fabry, </author> <title> ``A Fast File System for UNIX'', </title> <booktitle> ACM Transactions on 1993 Winter USENIX - January 25-29, </booktitle> <address> 1993 - San Diego, </address> <institution> CA 17 Computer Systems, </institution> <month> 2(3), August </month> <year> 1984, </year> <pages> 181-197. </pages>
Reference-contexts: In these file systems, the disk became fragmented over time so that new files tended to be allocated randomly across the disk, requiring a disk seek per file system read or write even when the file was being read sequentially. The Fast File System (FFS) <ref> [MCKU84] </ref> dramatically increased file system performance. It increased the block size, improving bandwidth. It reduced the number and length of seeks by placing related information close together on the disk. For example, blocks within files were allocated on the same or a nearby cylinder. <p> For the tests presented here, the disk was running at 85% utilization, and the cleaner was continually running. Note that FFS and EFS allocate up to 90% of the disk capacity without exhibiting any performance degradation <ref> [MCKU84] </ref>. This result shows that log-structured file systems are much more sensitive to the disk utilization.
Reference: [MCKU90] <author> Marshall Kirk McKusick, Michael J. Karels, Keith Bostic, </author> <title> ``A Pageable Memory Based Filesystem,'' </title> <booktitle> Proceedings of the 1990 Summer Usenix Technical Conference, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 137-144. </pages>
Reference-contexts: The com-mon code is used not only by the FFS and BSD-LFS, but by the memory file system <ref> [MCKU90] </ref> as well. The FFS and BSD-LFS implementations remain responsible for disk allocation, layout, and actual I/O. In moving code from the FFS implementation into the generic UFS area, it was necessary to add seven new vnode and vfs operations.
Reference: [MORA90] <author> Moran, J., Sandberg, R., Coleman, D., Kepecs, J., Lyon, B., </author> <title> Breaking Through the NFS Performance Barrier,'' </title> <booktitle> Proceedings of the 1990 Spring European Unix Users Group, </booktitle> <address> Munich, Germany, 199-206, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The factors limiting FFS performance are synchronous file creation and deletion and seek times between I/O requests for different files. The synchronous I/O for file creation and deletion provides file system disk data structure recoverability after failures. However, there exist alternative solutions such as NVRAM hardware <ref> [MORA90] </ref> and logging software [KAZA90]. In a UNIX environment, where the vast majority of files are small [OUST85][BAKE91], the seek times between I/O requests for different files can dominate. No solutions to this problem currently exist in the context of FFS.
Reference: [MCVO91] <author> McVoy, L., Kleiman, S., </author> <title> ``Extent-like Performance from a Unix File System'', </title> <booktitle> Proceedings Winter Usenix 1991, </booktitle> <address> Dallas, TX, </address> <month> January </month> <year> 1991, </year> <pages> 33-44. </pages>
Reference-contexts: While design decisions took into account the expected performance impact, at this point there is little empirical evidence to support those decisions. The file systems against which LFS is compared are the regular fast file system (FFS), and an enhanced version of FFS similar to that described in <ref> [MCVO91] </ref>, referred to as EFS for the rest of this paper. EFS provides extent-based file system behavior without changing the underlying structures of FFS, by allocating blocks sequentially on disk and clustering multiple block requests.
Reference: [OUST85] <author> Ousterhout, J., Costa, H., Harrison, D., Kunze, J., Kupfer, M., Thompson, J., </author> <title> ``A Trace-Driven Analysis of the UNIX 4.2BSD File System,'' </title> <booktitle> Proceedings of the Tenth Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1985, </year> <pages> 15-24. </pages> <note> Published as Operating Systems Review 19, </note> <month> 5 (December </month> <year> 1985). </year>
Reference: [OUST88] <author> Ousterhout, J., Douglis, F., </author> <title> ``Beating the I/O Bottleneck: A Case for Log Structured File Systems'', </title> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <address> UCB/CSD 88/467, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: In a UNIX environment, where the vast majority of files are small [OUST85][BAKE91], the seek times between I/O requests for different files can dominate. No solutions to this problem currently exist in the context of FFS. The log-structured file system, as proposed in <ref> [OUST88] </ref>, attempts to address both of these problems. The fundamental idea of LFS is to improve file system performance by storing all file system data in a single, continuous log. Such a file system is optimized for writing, because no seek is required between writes. <p> The write-optimization of LFS has the potential for dramatically improving system throughput, as large main-memory file caches effectively cache reads, but do little to improve write performance <ref> [OUST88] </ref>. The goal of the Sprite log-structured file system (Sprite-LFS) [ROSE91] was to design and implement an LFS that would provide acceptable read performance as well as improved write performance.
Reference: [OUST90] <author> Ousterhout, J. </author> <title> ``Why Aren't Operating Systems Getting Faster As Fast as Hardware?'' Proceedings of the 1990 Summer Usenix, </title> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 247-256. </pages>
Reference-contexts: For EFS and FFS, it becomes more difficult for them to allocate blocks optimally when they run on fairly full file systems, so degradation is expected for those systems as well. Software Development Workload The next tests evaluate BSD-LFS in a typical software development environment. The Andrew benchmark <ref> [OUST90] </ref> is often used for this type of measurement. The Andrew benchmark was created by M. Satyanarayanan of the Information Technology Center at Carnegie-Mellon University. It contains five phases. 1. Create a directory hierarchy. 2. Make several copies of the data. 3. Recursively examine the status of every file. 4.
Reference: [ROSE90] <author> Rosenblum, M., Ousterhout, J. K., </author> <title> ``The LFS Storage Manager'', </title> <booktitle> Proceedings of the 1990 Summer Usenix, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 315-324. </pages>
Reference-contexts: The updated meta-data blocks are gathered with the data blocks, and all are written to a segment. As a result, the inodes are no longer in fixed locations, so, LFS requires an additional data structure, called the inode map <ref> [ROSE90] </ref>, that maps inode numbers to disk addresses. ... SUPERBLOCKS SEGMENT 2 ... SEGMENT I INODE DISK ADDRESS 1 INODE DISK ADDRESS N NEXT SEGMENT POINTER DATA CHECKSUM SUMMARY CHECKSUM SEGMENT 1 ... SEGMENT N SEGMENT SUMMARY ... <p> In addition, the kernel maintains a segment usage table that shows the number of ``live'' bytes and the last modified time of each segment. The cleaner uses this table to determine which segments to clean <ref> [ROSE90] </ref>. Figure 2 shows the physical layout of the LFS. While FFS flushes individual blocks and files on demand, the LFS must gather data into segments. Usually, there will not be enough dirty blocks to fill a complete segment [BAKE92], in which case LFS writes partial segments.
Reference: [ROSE91] <author> Rosenblum, M., Ousterhout, J. K., </author> <title> ``The Design and Implementation of a Log-Structured File System'', </title> <booktitle> Proceedings of the Symposium on Operating System Principles, </booktitle> <address> Monterey, CA, </address> <month> October </month> <year> 1991, </year> <pages> 1-15. </pages> <note> Published as Operating Systems Review 25, 5 (October 1991). Also available as Transactions on Computer Systems 10, </note> <month> 1 (February </month> <year> 1992), </year> <pages> 26-52. </pages>
Reference-contexts: The write-optimization of LFS has the potential for dramatically improving system throughput, as large main-memory file caches effectively cache reads, but do little to improve write performance [OUST88]. The goal of the Sprite log-structured file system (Sprite-LFS) <ref> [ROSE91] </ref> was to design and implement an LFS that would provide acceptable read performance as well as improved write performance. <p> To ensure that the cleaner can always run, normal writing is suspended when the number of clean segments drops to two. The cleaning simulation results in <ref> [ROSE91] </ref> show that selection of segments to clean is an important design parameter in minimizing cleaning overhead, and that the cost-benefit policy defined there does extremely well for the simulated work-loads. Briefly, each segment is assigned a cleaning cost and benefit.
Reference: [ROSE92] <author> Rosenblum, M., </author> <title> ``The Design and Implementation of a Log-structured File System'', </title> <type> PhD Thesis, </type> <institution> University of California, Berkeley, </institution> <month> June </month> <year> 1992. </year> <note> Also available as Technical Report UCB/CSD 92/696. </note>
Reference-contexts: In this section we describe the key structural elements of an LFS, contrasting the data structures and recovery to FFS. The complete design and implementation of Sprite-LFS can be found in <ref> [ROSE92] </ref>. Table 1 compares key differences between FFS and LFS. The reasons for these differences will be described in detail in the following sections. <p> Since the files in this benchmark are created and deleted in large groups, most of the blocks read by the cleaner are discarded and most of the reads accomplished nothing. These single-user results are significantly different from those of Sprite-LFS discussed in <ref> [ROSE92] </ref>. There are several reasons for this. As mentioned earlier, a goal of this test is to stress the file systems, so both the cache and the file system are small. The small cache (1.6 megabytes) ensures that both read and write performance to the disk can be measured.
Reference: [SELT90] <author> Seltzer, M., Chen, P., Ousterhout, J., </author> <title> ``Disk Scheduling Revisited,'' </title> <booktitle> Proceedings of the 1990 Winter Usenix, </booktitle> <address> Washington, D.C., </address> <month> January </month> <year> 1990, </year> <pages> 313-324. </pages>
Reference-contexts: Transaction run lengths of greater than 1000 were measured, but there was no noticeable change in performance after the first 1000 transactions. When the cleaner is not running, BSD-LFS behaves as predicted in simulation <ref> [SELT90] </ref>. It shows approximately 20% improvement over the extent-based system. However, the impact of the cleaner is far worse than was expected. With one megabyte segments and the small random I/Os done by TPCB, most segments have only a few dead blocks available for reclamation.
Reference: [SELT91] <author> Seltzer, M., Stonebraker, M., </author> <title> ``Read Optimized File Systems: A Performance Evaluation,'' </title> <booktitle> Proceedings 7th Annual International Conference on Data Engineering, </booktitle> <address> Kobe, Japan, </address> <month> April </month> <year> 1991, </year> <pages> 602-611. </pages>
Reference-contexts: Since managing fragments complicates the file system, we decided to allocate progressively larger blocks instead of using a block/fragment combination. This improvement has not yet been implemented but is similar to the multiblock policy simulated in <ref> [SELT91] </ref>. The Buffer Cache Prior to the integration of BSD-LFS into 4BSD, the buffer cache had been considered file system independent code. However, the buffer cache contains assumptions about how and when blocks are written to disk. <p> Performance Measurements This chapter compares the performance of the redesigned log-structured file system to more traditional, read-optimized file systems on a variety of benchmarks based on real workloads. Read-optimized policies that favor large blocks or contiguous layout perform similarly <ref> [SELT91] </ref>, so we analyze FFS and a variant of FFS that does extent-like allocation. The new log-structured file system was written in November of 1991 and was left largely untouched until late spring 1992, and is therefore a completely untuned implementation.
Reference: [SELT93] <author> Seltzer, M., </author> <title> ``Transaction Support in a Log-Structured File System,'' </title> <booktitle> To appear in the Proceedings of the 1993 International Conference on Data Engineering, </booktitle> <month> April </month> <year> 1993, </year> <note> Vienna. </note>
Reference-contexts: There are three problems with this, in addition to the memory issues discussed in Section 3.1. First, there is no reason to believe that a single cleaning algorithm will work well on all workloads. In fact, measurements in <ref> [SELT93] </ref> show that coalescing randomly updated files would improve sequential read performance dramatically. Second, placing the cleaner in kernel-space makes it difficult to experiment with alternate cleaning policies. <p> Second, since data is never overwritten, before-images of updated pages exist in the file system until reclaimed by the cleaner. An implementation that exploits these two characteristics is described and analyzed in <ref> [SELT93] </ref> on Sprite-LFS, and we plan on doing a prototype implementation of transactions in BSD-LFS. The ``no-overwrite'' characteristic of BSD-LFS makes it ideal for supporting unrm which would undo a file deletion.
Reference: [THOM78] <author> Thompson, K., </author> <title> ``Unix Implementation'', </title> <journal> Bell Systems Technical Journal, </journal> <volume> 57(6), part 2, </volume> <month> July-August </month> <year> 1978, </year> <pages> 1931-1946. </pages>
Reference-contexts: 1. Introduction Early UNIX file systems used a small, fixed block size and made no attempt to optimize block placement <ref> [THOM78] </ref>. They assigned disk addresses to new blocks as they were created (preal-location) and wrote modified blocks back to their original disk addresses (overwrite).

References-found: 21

