URL: http://www.cs.princeton.edu/~liv/papers/tpds.ps
Refering-URL: http://www.cs.princeton.edu/~liv/papers/papers.html
Root-URL: http://www.cs.princeton.edu
Email: fyzhou,liv,jps,lig@cs.princeton.edu  ftoonen,schoinas,markhill,davidg@cs.wisc.edu  
Title: Relaxed Consistency and Coherence Granularity in DSM Systems: A Performance Evaluation  
Author: Yuanyuan Zhou, Liviu Iftode, Jaswinder Pal Singh and Kai Li Brian R. Toonen Ioannis Schoinas, Mark D. Hill, and David A. Wood 
Address: Princeton, NJ 08544  Madison, WI 53705  
Affiliation: Computer Science Department Princeton University  Computer Sciences Department University of Wisconsin, Madison  
Abstract: fl An earlier version of this paper with three protocols appears in the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, June 1997(PPoPP'97). In this paper, we have added the results for the delayed consistency protocol. y Brian R. Toonen is currently at Argonne National Laborator, Argonne, IL 60439, email: toonen@mcs.anl.gov 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. </author> <title> Adaptive Software Cache Management for Distributed Shared Memory Architectures. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: performs 30% to eight times better than the SW-LRC protocol, showing that multiple writer protocols are indeed valuable for irregular applications under SVM. 5.2 Detailed Analysis To understand the reasons for the performance differences, it is useful to classify the applications according to their data access patterns and synchronization behavior <ref> [31, 1, 12] </ref>. In this section, we 10 will first describe application classifications according to the number of writers per coherence unit, spatial data access granularity, and temporal synchronization granularity.
Reference: [2] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway Distributed Shared Memory System. In Proceedings of the IEEE COMPCON '93 Conference, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: Relaxed consistency models introduce additional programmer restrictions in exchange for (hopefully) better performance. Examples of relaxed consistency models include release consistency [10], entry consistency <ref> [2] </ref>, scope consistency [13]. Delayed consistency (DC) [8] is an implementation of release consistency which can be easily implemented in either hardware or software [4, 9]. Lazy release consistency (LRC) [16], is a software implementation of release consistency which delays the coherence action until the acquire 2 time. <p> Examples of relaxed consistency models and systems include release consistency (RC) [10] and its SVM implementation [5], delayed consistency [8] and its software implementations [4, 9], multiple-writer lazy release consistency (LRC) [16] and its implementation [15], entry consistency and prototype <ref> [2] </ref>, automatic update release consistency [11], scope consistency [13], home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency [17]. All prototypes based on relaxed consistency models use virtual memory page sizes as their coherence units.
Reference: [3] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The cache-coherent 50 MHz MBus connects the processors and memory. I/O devices reside on the 25 MHz SBus, which connects to the MBus via a bridge. All nodes run Solaris 2.4. Each node contains a Myrinet network interface <ref> [3] </ref>, which consists of a 7-MIPS custom processor (LANai) and 128KB of memory. The LANai performs limited protocol processing and schedules DMA transfers between the network and LANai memory or LANai memory and SPARC memory. The 16 nodes used in this paper are connected with three Myrinet 8-port crossbar switches.
Reference: [4] <author> L. Borrmann and M. Herdieckerhoff. </author> <title> A Coherency Model for Virtual Shared Memory. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Relaxed consistency models introduce additional programmer restrictions in exchange for (hopefully) better performance. Examples of relaxed consistency models include release consistency [10], entry consistency [2], scope consistency [13]. Delayed consistency (DC) [8] is an implementation of release consistency which can be easily implemented in either hardware or software <ref> [4, 9] </ref>. Lazy release consistency (LRC) [16], is a software implementation of release consistency which delays the coherence action until the acquire 2 time. Most software shared systems today use LRC-based protocols [15] [11] [32] [17]. These relaxed consistency models employ sophisticated protocols to reduce false sharing and fragmentation. <p> The difference between these two protocols is that the standard RC protocol performs invalidations as soon as they are received, while the DC protocol performs them at the next acquire. Our DC implementation is similar to other software DC implementations <ref> [4, 9] </ref>. Our DC implementation handles read misses in the same way as the SC protocol. At a write miss, the DC protocol allows the writer to continue as soon as it receives the ownership for the corresponding block. <p> Examples of relaxed consistency models and systems include release consistency (RC) [10] and its SVM implementation [5], delayed consistency [8] and its software implementations <ref> [4, 9] </ref>, multiple-writer lazy release consistency (LRC) [16] and its implementation [15], entry consistency and prototype [2], automatic update release consistency [11], scope consistency [13], home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency [17].
Reference: [5] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Since then, two main approaches have been taken to deal with the false-sharing and fragmentation problem in SVM systems: relaxing consistency models and providing fine-grained access control. Examples of relaxed consistency models and systems include release consistency (RC) [10] and its SVM implementation <ref> [5] </ref>, delayed consistency [8] and its software implementations [4, 9], multiple-writer lazy release consistency (LRC) [16] and its implementation [15], entry consistency and prototype [2], automatic update release consistency [11], scope consistency [13], home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency [17].
Reference: [6] <author> David Culler, Lok Tin Liu, Richard Martin, and Chad Yoshikawa. </author> <title> LogP Performance Assessment of Fast Network Interfaces. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 35-43, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: With Myrinet hardware, the host (SPARC) processor and Myrinet LANai processor cooperate to send and receive data. Our communication library is based on the LANai Control Program (LCP) used in Berkeley's LAM library <ref> [6] </ref>. The host processor uses loads and stores to move small messages and headers for large messages to and from LANai memory. The LANai processor uses DMA to directly move larger messages through intermediate kernel/user buffers.
Reference: [7] <author> David Culler, Lok Tin Liu, Richard Martin, and Chad Yoshikawa. </author> <title> LogP Performance Assessment of Fast Network Interfaces. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 35-43, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: However, polling introduces needlessly overhead when no message is present. A microbenchmark shows 4-, 64-, 256-, 1K- and 4K-byte messages see round-trip times of 40, 61, 100, 256 and 876 usecs. Large messages achieve bandwidths of about 17 MB/sec, with is close to the values obtained by others <ref> [7, 23] </ref>. 4 Applications To evaluate the performance of the four protocols with different coherence granularities, we used 8 benchmarks from SPLASH-2, including LU decomposition, Ocean, FFT, Water-Nsquared, Volrend, Water-Spatial, Raytrace, and Barnes. We have two versions for Ocean, two versions for Volrend and three versions for Barnes.
Reference: [8] <author> M. Dubois, J.C. Wang, L.A. Barroso, K. Lee, and Y-S Chen. </author> <title> Delayed Consistency and Its Effects on the Miss Rate of Parallel Programs. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 197-206, </pages> <year> 1991. </year>
Reference-contexts: Relaxed consistency models introduce additional programmer restrictions in exchange for (hopefully) better performance. Examples of relaxed consistency models include release consistency [10], entry consistency [2], scope consistency [13]. Delayed consistency (DC) <ref> [8] </ref> is an implementation of release consistency which can be easily implemented in either hardware or software [4, 9]. Lazy release consistency (LRC) [16], is a software implementation of release consistency which delays the coherence action until the acquire 2 time. <p> We studied the combinations of four consistency protocols: sequential consistency (SC) [19], single writer delayed consistency (DC) <ref> [8] </ref>, single-writer lazy release consistency (SW-LRC) [17] and home-based lazy release consistency (HLRC) [32] with four sizes of coherence granularity. We also studied two mechanisms (polling and interrupts) to handle message arrivals for each case. <p> When an invalidation arrives at a node, 4 the message is processed immediately (modulo the polling/interrupt issue); read-only copies are invalidated and read-write copies are written-back to the home node and invalidated. 2.2 DC Protocol The delayed consistency (DC) protocol was initially designed for hardware cache-coherent shared memory multiprocessors <ref> [8] </ref>. A DC protocol is slightly more relaxed than the standard release consistent (RC) protocol [10]. Like the standard RC protocol, the DC protocol delays out-going invalidations until release time. <p> The total number of misses under SC decreases significantly, down to 4%-70% of the polling case. In essence, the interrupt method approximates Dubois, et al.'s DC implementations, which are specifically targeted at reducing the impact of false sharing <ref> [8] </ref>. <p> Since then, two main approaches have been taken to deal with the false-sharing and fragmentation problem in SVM systems: relaxing consistency models and providing fine-grained access control. Examples of relaxed consistency models and systems include release consistency (RC) [10] and its SVM implementation [5], delayed consistency <ref> [8] </ref> and its software implementations [4, 9], multiple-writer lazy release consistency (LRC) [16] and its implementation [15], entry consistency and prototype [2], automatic update release consistency [11], scope consistency [13], home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency [17].
Reference: [9] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy. SoftFLASH: </author> <title> Analyzing the Performance of Clustered Distributed Virtual Shared Memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Relaxed consistency models introduce additional programmer restrictions in exchange for (hopefully) better performance. Examples of relaxed consistency models include release consistency [10], entry consistency [2], scope consistency [13]. Delayed consistency (DC) [8] is an implementation of release consistency which can be easily implemented in either hardware or software <ref> [4, 9] </ref>. Lazy release consistency (LRC) [16], is a software implementation of release consistency which delays the coherence action until the acquire 2 time. Most software shared systems today use LRC-based protocols [15] [11] [32] [17]. These relaxed consistency models employ sophisticated protocols to reduce false sharing and fragmentation. <p> The difference between these two protocols is that the standard RC protocol performs invalidations as soon as they are received, while the DC protocol performs them at the next acquire. Our DC implementation is similar to other software DC implementations <ref> [4, 9] </ref>. Our DC implementation handles read misses in the same way as the SC protocol. At a write miss, the DC protocol allows the writer to continue as soon as it receives the ownership for the corresponding block. <p> Examples of relaxed consistency models and systems include release consistency (RC) [10] and its SVM implementation [5], delayed consistency [8] and its software implementations <ref> [4, 9] </ref>, multiple-writer lazy release consistency (LRC) [16] and its implementation [15], entry consistency and prototype [2], automatic update release consistency [11], scope consistency [13], home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency [17].
Reference: [10] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: During the past few years, two main approaches have been taken to address this problem: relaxing consistency models and providing access control at a fine granularity. Relaxed consistency models introduce additional programmer restrictions in exchange for (hopefully) better performance. Examples of relaxed consistency models include release consistency <ref> [10] </ref>, entry consistency [2], scope consistency [13]. Delayed consistency (DC) [8] is an implementation of release consistency which can be easily implemented in either hardware or software [4, 9]. <p> A DC protocol is slightly more relaxed than the standard release consistent (RC) protocol <ref> [10] </ref>. Like the standard RC protocol, the DC protocol delays out-going invalidations until release time. The difference between these two protocols is that the standard RC protocol performs invalidations as soon as they are received, while the DC protocol performs them at the next acquire. <p> Since then, two main approaches have been taken to deal with the false-sharing and fragmentation problem in SVM systems: relaxing consistency models and providing fine-grained access control. Examples of relaxed consistency models and systems include release consistency (RC) <ref> [10] </ref> and its SVM implementation [5], delayed consistency [8] and its software implementations [4, 9], multiple-writer lazy release consistency (LRC) [16] and its implementation [15], entry consistency and prototype [2], automatic update release consistency [11], scope consistency [13], home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency
Reference: [11] <author> L. Iftode, C. Dubnicki, E. W. Felten, and Kai Li. </author> <title> Improving Release-Consistent Shared Virtual Memory using Automatic Update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year> <month> 27 </month>
Reference-contexts: Lazy release consistency (LRC) [16], is a software implementation of release consistency which delays the coherence action until the acquire 2 time. Most software shared systems today use LRC-based protocols [15] <ref> [11] </ref> [32] [17]. These relaxed consistency models employ sophisticated protocols to reduce false sharing and fragmentation. An alternative approach is to preserve the simplicity of sequential consistency, but find some approach to reduce the coherence granularity. <p> Examples of relaxed consistency models and systems include release consistency (RC) [10] and its SVM implementation [5], delayed consistency [8] and its software implementations [4, 9], multiple-writer lazy release consistency (LRC) [16] and its implementation [15], entry consistency and prototype [2], automatic update release consistency <ref> [11] </ref>, scope consistency [13], home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency [17]. All prototypes based on relaxed consistency models use virtual memory page sizes as their coherence units.
Reference: [12] <author> L. Iftode, J. P. Singh, and Kai Li. </author> <title> Understanding Application Performance on Shared Virtual Memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: performs 30% to eight times better than the SW-LRC protocol, showing that multiple writer protocols are indeed valuable for irregular applications under SVM. 5.2 Detailed Analysis To understand the reasons for the performance differences, it is useful to classify the applications according to their data access patterns and synchronization behavior <ref> [31, 1, 12] </ref>. In this section, we 10 will first describe application classifications according to the number of writers per coherence unit, spatial data access granularity, and temporal synchronization granularity. <p> Write-write false sharing occurs only for multiple writer applications. * Coarse-grain vs. Fine-grain data access Data access granularity affects how the communication to computation ratio changes with the coherence granularity <ref> [12] </ref>. Applications with coarse-grain access tend to access a whole contiguous page at a time. Fine-grain applications are likely to scatter reads and writes across multiple pages. Fine-grain reads can introduce fragmentation with coarse coherence granularity and false sharing. * Coarse-grain vs. <p> A contiguous allocation of partitions using four-dimensional arrays eliminates write-write false sharing by ensuring a single writer for each partition. In FFT, the write access granularity is coarse while the read access granularity is fine-grained for this problem size <ref> [12] </ref> (a processor reads subrows of size 192 bytes from other processors). All the protocols performs poorly.
Reference: [13] <author> L. Iftode, J.P. Singh, and K. Li. </author> <title> Scope Consistency: a Bridge Between Release Consistency and Entry Consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: Relaxed consistency models introduce additional programmer restrictions in exchange for (hopefully) better performance. Examples of relaxed consistency models include release consistency [10], entry consistency [2], scope consistency <ref> [13] </ref>. Delayed consistency (DC) [8] is an implementation of release consistency which can be easily implemented in either hardware or software [4, 9]. Lazy release consistency (LRC) [16], is a software implementation of release consistency which delays the coherence action until the acquire 2 time. <p> Examples of relaxed consistency models and systems include release consistency (RC) [10] and its SVM implementation [5], delayed consistency [8] and its software implementations [4, 9], multiple-writer lazy release consistency (LRC) [16] and its implementation [15], entry consistency and prototype [2], automatic update release consistency [11], scope consistency <ref> [13] </ref>, home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency [17]. All prototypes based on relaxed consistency models use virtual memory page sizes as their coherence units. Another approach is to preserve the sequential consistency model and to find ways to reduce the coherence granularity.
Reference: [14] <author> Dongming Jiang, Hongzhang Shan, and Jaswinder Pal Singh. </author> <title> Application Restructuring and Performance Portability on Shared Virtual Memory and Hardware-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the 6th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Their differences are in data structures, task partitioning, and algorithms for certain phases. Understanding the impact of these differences can give us some insights into the critical performance factors in making applications perform better on such systems <ref> [14] </ref> As mentioned above, all protocols perform poorly for Ocean-Original because its fine grain access pattern causes large (88-99%) fragmentation. Ocean-Rowwise employs a rowwise partitioning strategy instead of the strategy of partitioning into square subblocks used in Ocean-Original.
Reference: [15] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Lazy release consistency (LRC) [16], is a software implementation of release consistency which delays the coherence action until the acquire 2 time. Most software shared systems today use LRC-based protocols <ref> [15] </ref> [11] [32] [17]. These relaxed consistency models employ sophisticated protocols to reduce false sharing and fragmentation. An alternative approach is to preserve the simplicity of sequential consistency, but find some approach to reduce the coherence granularity. <p> Changes are detected 5 by comparing the current (dirty) copy with the clean copy (twin) and recorded in a structure called a diff. The traditional LRC implementation uses a distributed diff scheme where diffs are merged on demand in a distributed fashion <ref> [15] </ref>. To bring a copy up-to-date, diffs must be fetched from the recent writers and then applied in the proper causal order determined using vector timestamps [16]. The HLRC multiple-writer scheme differs from LRC by having the diffs sent and applied eagerly at a designated home of the block. <p> Examples of relaxed consistency models and systems include release consistency (RC) [10] and its SVM implementation [5], delayed consistency [8] and its software implementations [4, 9], multiple-writer lazy release consistency (LRC) [16] and its implementation <ref> [15] </ref>, entry consistency and prototype [2], automatic update release consistency [11], scope consistency [13], home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency [17]. All prototypes based on relaxed consistency models use virtual memory page sizes as their coherence units.
Reference: [16] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Examples of relaxed consistency models include release consistency [10], entry consistency [2], scope consistency [13]. Delayed consistency (DC) [8] is an implementation of release consistency which can be easily implemented in either hardware or software [4, 9]. Lazy release consistency (LRC) <ref> [16] </ref>, is a software implementation of release consistency which delays the coherence action until the acquire 2 time. Most software shared systems today use LRC-based protocols [15] [11] [32] [17]. These relaxed consistency models employ sophisticated protocols to reduce false sharing and fragmentation. <p> This protocol is less relaxed than a multiple-writer lazy release consistency protocol, but more relaxed than the DC protocol because SW-LRC further delays invalidations from the next acquire to the latest possible acquire. Our SW-LRC protocol uses the same timestamp-based coherence control as proposed for Lazy Release Consistency <ref> [16] </ref> but it allows only a single writable copy to co-exist with multiple read-only copies. In this protocol, a write fault causes ownership to migrate but, unlike a sequential consistency protocol, read-only copies are not invalidated. <p> The traditional LRC implementation uses a distributed diff scheme where diffs are merged on demand in a distributed fashion [15]. To bring a copy up-to-date, diffs must be fetched from the recent writers and then applied in the proper causal order determined using vector timestamps <ref> [16] </ref>. The HLRC multiple-writer scheme differs from LRC by having the diffs sent and applied eagerly at a designated home of the block. For every shared block, a single node (usually the first writer) is designated as the block's home. <p> Examples of relaxed consistency models and systems include release consistency (RC) [10] and its SVM implementation [5], delayed consistency [8] and its software implementations [4, 9], multiple-writer lazy release consistency (LRC) <ref> [16] </ref> and its implementation [15], entry consistency and prototype [2], automatic update release consistency [11], scope consistency [13], home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency [17]. All prototypes based on relaxed consistency models use virtual memory page sizes as their coherence units.
Reference: [17] <author> P.J. Keleher. </author> <title> The Relative Importance of Concurrent Writers and Weak Consistency Models. </title> <booktitle> In Proceedings of the IEEE COMPCON '96 Conference, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Lazy release consistency (LRC) [16], is a software implementation of release consistency which delays the coherence action until the acquire 2 time. Most software shared systems today use LRC-based protocols [15] [11] [32] <ref> [17] </ref>. These relaxed consistency models employ sophisticated protocols to reduce false sharing and fragmentation. An alternative approach is to preserve the simplicity of sequential consistency, but find some approach to reduce the coherence granularity. <p> We studied the combinations of four consistency protocols: sequential consistency (SC) [19], single writer delayed consistency (DC) [8], single-writer lazy release consistency (SW-LRC) <ref> [17] </ref> and home-based lazy release consistency (HLRC) [32] with four sizes of coherence granularity. We also studied two mechanisms (polling and interrupts) to handle message arrivals for each case. Our experiments used eight real benchmarks developed for hardware shared memory systems and their variations (12 applications total). <p> invalidation, it delays the invalidation for the corresponding block until the next acquire. 2.3 SW-LRC Protocol The single-writer lazy release consistency (SW-LRC) protocol also allows a single-writer to co-exist with multiple readers, but it further delays the propagations of updates to shared memory to the executions of causally-related acquire operations <ref> [17] </ref>. This protocol is less relaxed than a multiple-writer lazy release consistency protocol, but more relaxed than the DC protocol because SW-LRC further delays invalidations from the next acquire to the latest possible acquire. <p> and its SVM implementation [5], delayed consistency [8] and its software implementations [4, 9], multiple-writer lazy release consistency (LRC) [16] and its implementation [15], entry consistency and prototype [2], automatic update release consistency [11], scope consistency [13], home-based lazy release consistency and its implementations [32], and single-writer lazy release consistency <ref> [17] </ref>. All prototypes based on relaxed consistency models use virtual memory page sizes as their coherence units. Another approach is to preserve the sequential consistency model and to find ways to reduce the coherence granularity. <p> Examples of providing fine-grained access control include taking advantage architectural features such as the ECC bits to trap access faults [29], using software instrumentation for shared reads and writes [29, 28], and building special access control hardware for commodity workstations [25]. Keleher <ref> [17] </ref> compared sequential consistency with single writer and multiple-writer LRC protocols at page granularity and concludes that on average the multiple-writer version is 9% better than the single-writer one and 34% better than the sequential consistency one. His study used 8 applications, 3 of which are irregular.
Reference: [18] <author> Leonidas Kontothanassis, Galen Hunt, Robert Stets, Nikolaos Hardavellas, Michal Cierniak, Srini-vasan Parthasarathy, Wagner Meira, Sandhya Dwarkadas, and Michael Scott. </author> <title> VM-Based Shared Memory on Low-Latency, Remote-Memory-Access Networks. </title> <booktitle> Proc., 24th Annual Int'l. Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: His study used 8 applications, 3 of which are irregular. Our study compares four protocols with different granularities for 12 applications including 7 irregular applications. The Cashmere system <ref> [18] </ref> implemented a multiple writer delayed consistency protocol. Our DC implementation is single writer.
Reference: [19] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocessor Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: The original shared virtual memory (SVM) proposal and prototype [22] uses the traditional virtual memory access protection mechanism to detect access misses and implements a sequential consistency model <ref> [19] </ref>. The main advantage of the approach is that it implements shared memory entirely in software on a network of commodity workstations [21] to run applications developed for hardware shared-memory multiprocessors. A disadvantage is that it restricts the coherence granularity to be a virtual memory page size. <p> We studied the combinations of four consistency protocols: sequential consistency (SC) <ref> [19] </ref>, single writer delayed consistency (DC) [8], single-writer lazy release consistency (SW-LRC) [17] and home-based lazy release consistency (HLRC) [32] with four sizes of coherence granularity. We also studied two mechanisms (polling and interrupts) to handle message arrivals for each case. <p> The original shared virtual memory (SVM) proposal and prototype [22] used the traditional virtual memory access protection mechanism to detect access misses and implemented the sequential consistency model <ref> [19] </ref> on a network of workstations. The coherence unit of the prototype was a 1,024-byte virtual memory page. Since then, two main approaches have been taken to deal with the false-sharing and fragmentation problem in SVM systems: relaxing consistency models and providing fine-grained access control.
Reference: [20] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: This model is generally considered the simplest for programmers, because, informally, a read always returns the result of the most recent write. Our sequential consistency implementation is based on the Stache protocol [26] and is similar to many directory-based hardware implementations <ref> [20] </ref>. On a miss, a request message is sent to the designated home node. If invalidations are required, the home node collects the acknowledgments before forwarding the data to the requesting node.
Reference: [21] <author> K. Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, volume II Software, </booktitle> <pages> pages 94-101, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The main advantage of the approach is that it implements shared memory entirely in software on a network of commodity workstations <ref> [21] </ref> to run applications developed for hardware shared-memory multiprocessors. A disadvantage is that it restricts the coherence granularity to be a virtual memory page size. For systems with large page sizes, false sharing and fragmentation will occur in applications with multiple writer, fine-grained access patterns.
Reference: [22] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 229-239, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: This paper evaluates the performance tradeoffs of the combinations of four consistency protocols with four sizes of coherence granularity for software shared memory implementations on a real hardware platform. The original shared virtual memory (SVM) proposal and prototype <ref> [22] </ref> uses the traditional virtual memory access protection mechanism to detect access misses and implements a sequential consistency model [19]. The main advantage of the approach is that it implements shared memory entirely in software on a network of commodity workstations [21] to run applications developed for hardware shared-memory multiprocessors. <p> The most related work to this paper includes research on relaxing consistency models and providing fine-grained coherence granularity for software coherent shared memory. The original shared virtual memory (SVM) proposal and prototype <ref> [22] </ref> used the traditional virtual memory access protection mechanism to detect access misses and implemented the sequential consistency model [19] on a network of workstations. The coherence unit of the prototype was a 1,024-byte virtual memory page.
Reference: [23] <author> Scott Pakin, Mario Laura, and Andrew Chien. </author> <title> High Performance Messaging on Workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: However, polling introduces needlessly overhead when no message is present. A microbenchmark shows 4-, 64-, 256-, 1K- and 4K-byte messages see round-trip times of 40, 61, 100, 256 and 876 usecs. Large messages achieve bandwidths of about 17 MB/sec, with is close to the values obtained by others <ref> [7, 23] </ref>. 4 Applications To evaluate the performance of the four protocols with different coherence granularities, we used 8 benchmarks from SPLASH-2, including LU decomposition, Ocean, FFT, Water-Nsquared, Volrend, Water-Spatial, Raytrace, and Barnes. We have two versions for Ocean, two versions for Volrend and three versions for Barnes.
Reference: [24] <author> Robert W. Pfile. </author> <title> Typhoon-Zero Implementation: The Vortex Module. </title> <type> Technical report, </type> <institution> Wisconsin University, CS department, </institution> <year> 1995. </year>
Reference-contexts: Two ports of each switch are used to connect to other switches. Each node also contains a Typhoon-0 card that logically performs fine-grain access control checks on all loads and stores by physically snooping memory bus transactions and exploiting inclusion <ref> [24] </ref>. When the Typhoon-0 hardware detects an access control violation, it generates an exception to the shared-memory run-time system via a special fast-exception method supported by the device driver (approximately 5 s). All protocol processing occurs on the faulting processor.
Reference: [25] <author> S. K. Reinhard, R. W. Pfile, and D. A. Wood. </author> <title> Decoupled Hardware Support for Distributed Shared Memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Examples of providing fine-grained access control include taking advantage of architectural features such as the ECC bits to trap access faults [29], using software instrumentation for shared reads and writes [29, 28], and building special access control hardware for commodity workstations <ref> [25] </ref>. The finer the granularity, the less false sharing and fragmentation occur, and hence the less need to use relaxed models. A disadvantage of fine-grain coherence is that the smaller granularity may result in excessive misses and poor remote bandwidth. <p> Examples of providing fine-grained access control include taking advantage architectural features such as the ECC bits to trap access faults [29], using software instrumentation for shared reads and writes [29, 28], and building special access control hardware for commodity workstations <ref> [25] </ref>. Keleher [17] compared sequential consistency with single writer and multiple-writer LRC protocols at page granularity and concludes that on average the multiple-writer version is 9% better than the single-writer one and 34% better than the sequential consistency one. His study used 8 applications, 3 of which are irregular.
Reference: [26] <author> S.K. Reinhardt, J.R. Larus, and D.A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year> <month> 28 </month>
Reference-contexts: This model is generally considered the simplest for programmers, because, informally, a read always returns the result of the most recent write. Our sequential consistency implementation is based on the Stache protocol <ref> [26] </ref> and is similar to many directory-based hardware implementations [20]. On a miss, a request message is sent to the designated home node. If invalidations are required, the home node collects the acknowledgments before forwarding the data to the requesting node.
Reference: [27] <author> ROSS Technology, Inc. </author> <title> SPARC RISC User's Guide: </title> <address> hyperSPARC Edition, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Our implementation extends earlier work [32] by supporting various coherence granularities. 3 Testbed This section describes the platform used for these experiments. The testbed, called the Wisonsin COW (cluster of workstations), consists of 16 dual-processor Sun SPARCStation 20s. Each contains two 66 MHz Ross HyperSPARC processors <ref> [27] </ref>; however, our study only uses one processor on each node. Each processor has a 256 KB L2 cache and each node contains 64 MB of main memory. The cache-coherent 50 MHz MBus connects the processors and memory.
Reference: [28] <author> D.J. Scales, K. Gharachorloo, and C.A. Thekkath. </author> <title> Shasta: A Low Overhead, SOftware-Only Approach for Supporting Fine-Grain Shared Memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Examples of providing fine-grained access control include taking advantage of architectural features such as the ECC bits to trap access faults [29], using software instrumentation for shared reads and writes <ref> [29, 28] </ref>, and building special access control hardware for commodity workstations [25]. The finer the granularity, the less false sharing and fragmentation occur, and hence the less need to use relaxed models. <p> Another approach is to preserve the sequential consistency model and to find ways to reduce the coherence granularity. Examples of providing fine-grained access control include taking advantage architectural features such as the ECC bits to trap access faults [29], using software instrumentation for shared reads and writes <ref> [29, 28] </ref>, and building special access control hardware for commodity workstations [25]. Keleher [17] compared sequential consistency with single writer and multiple-writer LRC protocols at page granularity and concludes that on average the multiple-writer version is 9% better than the single-writer one and 34% better than the sequential consistency one. <p> Finally, this study has not examined all-software systems, since access-control was performed in hardware on the platform we used. It would be interesting to also investigate all-software systems that provide fine-grained access control through software instrumentation of loads and stores <ref> [29, 28] </ref> or page-grained access control entirely through the virtual memory mechanism.
Reference: [29] <author> I. Schoinas, B. Falsafi, A.R. Lebeck, S.K. Reinhardt, J.R. Larus, and D.A. Wood. </author> <title> Fine-grain Access for Distributed Shared Memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: An alternative approach is to preserve the simplicity of sequential consistency, but find some approach to reduce the coherence granularity. Examples of providing fine-grained access control include taking advantage of architectural features such as the ECC bits to trap access faults <ref> [29] </ref>, using software instrumentation for shared reads and writes [29, 28], and building special access control hardware for commodity workstations [25]. The finer the granularity, the less false sharing and fragmentation occur, and hence the less need to use relaxed models. <p> Examples of providing fine-grained access control include taking advantage of architectural features such as the ECC bits to trap access faults [29], using software instrumentation for shared reads and writes <ref> [29, 28] </ref>, and building special access control hardware for commodity workstations [25]. The finer the granularity, the less false sharing and fragmentation occur, and hence the less need to use relaxed models. <p> Another approach is to preserve the sequential consistency model and to find ways to reduce the coherence granularity. Examples of providing fine-grained access control include taking advantage architectural features such as the ECC bits to trap access faults <ref> [29] </ref>, using software instrumentation for shared reads and writes [29, 28], and building special access control hardware for commodity workstations [25]. <p> Another approach is to preserve the sequential consistency model and to find ways to reduce the coherence granularity. Examples of providing fine-grained access control include taking advantage architectural features such as the ECC bits to trap access faults [29], using software instrumentation for shared reads and writes <ref> [29, 28] </ref>, and building special access control hardware for commodity workstations [25]. Keleher [17] compared sequential consistency with single writer and multiple-writer LRC protocols at page granularity and concludes that on average the multiple-writer version is 9% better than the single-writer one and 34% better than the sequential consistency one. <p> Finally, this study has not examined all-software systems, since access-control was performed in hardware on the platform we used. It would be interesting to also investigate all-software systems that provide fine-grained access control through software instrumentation of loads and stores <ref> [29, 28] </ref> or page-grained access control entirely through the virtual memory mechanism.
Reference: [30] <author> Ioannis Schoinas, Babak Falsafi, Mark D. Hill, James R. Larus, Christopher E. Lucas, Shubhendu S. Mukherjee, Steven K. Reinhardt, Eric Schnarr, and David A. Wood. </author> <title> Implementing Fine-Grain Distributed Shared Memory On Commodity SMP Workstations. </title> <type> Technical Report 1307, </type> <month> March </month> <year> 1996. </year>
Reference-contexts: It requires adding 7 instructions at each back edge in an application's control flow graph to check a control register for message arrival. Because the T0 device supports cachable control registers, the common case (that no message has arrived) incurs an overhead of only 6 or 7 cycles <ref> [30] </ref>. When a message does arrive, the round trip time for the mechanism is 1.5 microseconds, which includes the cost of clearing the T0 register with an uncached store.
Reference: [31] <author> W. Weber and A. Gupta. </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> In The Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: performs 30% to eight times better than the SW-LRC protocol, showing that multiple writer protocols are indeed valuable for irregular applications under SVM. 5.2 Detailed Analysis To understand the reasons for the performance differences, it is useful to classify the applications according to their data access patterns and synchronization behavior <ref> [31, 1, 12] </ref>. In this section, we 10 will first describe application classifications according to the number of writers per coherence unit, spatial data access granularity, and temporal synchronization granularity.
Reference: [32] <author> Y. Zhou, L. Iftode, and K. Li. </author> <title> Performance Evaluation of Two Home-Based Lazy Release Consistency Protocols for Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> October </month> <year> 1996. </year> <month> 29 </month>
Reference-contexts: Lazy release consistency (LRC) [16], is a software implementation of release consistency which delays the coherence action until the acquire 2 time. Most software shared systems today use LRC-based protocols [15] [11] <ref> [32] </ref> [17]. These relaxed consistency models employ sophisticated protocols to reduce false sharing and fragmentation. An alternative approach is to preserve the simplicity of sequential consistency, but find some approach to reduce the coherence granularity. <p> We studied the combinations of four consistency protocols: sequential consistency (SC) [19], single writer delayed consistency (DC) [8], single-writer lazy release consistency (SW-LRC) [17] and home-based lazy release consistency (HLRC) <ref> [32] </ref> with four sizes of coherence granularity. We also studied two mechanisms (polling and interrupts) to handle message arrivals for each case. Our experiments used eight real benchmarks developed for hardware shared memory systems and their variations (12 applications total). <p> By including the version numbers in the write-notices and storing them away, the SW-LRC protocol can avoid unnecessary invalidations. 2.4 HLRC Protocol The Home-based Lazy Release Consistency (HLRC) protocol implements the well-known lazy release consistency model but has several performance and implementation advantages <ref> [32] </ref>. Both HLRC and traditional LRC protocols use the same multiple-writer solution based on using "twin"copies to generate "diffs" but with different update schemes. Each writer is allowed to write into its copy once a clean version of the block (twin) has been created. <p> The home's copy of the block is eagerly updated before the next release point. Non-home copies are invalidated on acquire according to the traditional LRC algorithm. A whole block is fetched from the home to bring a non-home copy up-to-date at fault time. Our implementation extends earlier work <ref> [32] </ref> by supporting various coherence granularities. 3 Testbed This section describes the platform used for these experiments. The testbed, called the Wisonsin COW (cluster of workstations), consists of 16 dual-processor Sun SPARCStation 20s. <p> systems include release consistency (RC) [10] and its SVM implementation [5], delayed consistency [8] and its software implementations [4, 9], multiple-writer lazy release consistency (LRC) [16] and its implementation [15], entry consistency and prototype [2], automatic update release consistency [11], scope consistency [13], home-based lazy release consistency and its implementations <ref> [32] </ref>, and single-writer lazy release consistency [17]. All prototypes based on relaxed consistency models use virtual memory page sizes as their coherence units. Another approach is to preserve the sequential consistency model and to find ways to reduce the coherence granularity.
References-found: 32

