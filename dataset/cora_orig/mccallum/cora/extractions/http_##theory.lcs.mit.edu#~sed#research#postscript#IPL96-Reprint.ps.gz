URL: http://theory.lcs.mit.edu/~sed/research/postscript/IPL96-Reprint.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~sed/research/IPL96-abs.html
Root-URL: 
Title: On the Sample Complexity of Noise-Tolerant Learning  
Author: Javed A. Aslam Scott E. Decatur 
Keyword: Machine Learning, Computational Learning Theory, Computational Complexity, Fault Toler ance, Theory of Computation  
Address: Hanover, NH 03755  Cambridge, MA 02139  
Affiliation: Department of Computer Science Dartmouth College  Laboratory for Computer Science Massachusetts Institute of Technology  
Note: To appear in Inofmration Processing Letters, 1996.  
Abstract: In this paper, we further characterize the complexity of noise-tolerant learning in the PAC model. Specifically, we show a general lower bound of log(1=ffi) on the number of examples required for PAC learning in the presence of classification noise. Combined with a result of Simon, we effectively show that the sample complexity of PAC learning in the presence of classification noise is VC(F) "(12) 2 : Furthermore, we demonstrate the optimality of the general lower bound by providing a noise-tolerant learning algorithm for the class of symmetric Boolean functions which uses a sample size within a constant factor of this bound. Finally, we note that our general lower bound compares favorably with various general upper bounds for PAC learning in the presence of classification noise. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: In fact, most of the standard PAC learning algorithms would fail if even a small number of the labelled examples given to the learning algorithm were "noisy." A widely studied model of noise for both theoretical and experimental research is the classification noise model introduced by Angluin and Laird <ref> [1] </ref>. In this model, each example received by the learner is mislabelled randomly and independently with some fixed probability &lt; 1=2. It is not surprising that algorithms for learning in the presence of classification noise use more examples than their corresponding noise-free algorithms. <p> We make use of the following bounds on the tail of the binomial distribution [2, 5, 7]: Lemma 1 For p 2 <ref> [0; 1] </ref> and positive integer m, let LE (p; m; r) denote the probability of at most an r fraction of successes in m independent trials of a Bernoulli random variable with probability of success p. <p> Let GE (p; m; r) denote the probability of at least an r fraction of successes. Then for ff 2 <ref> [0; 1] </ref>, LE (p; m; (1 ff)p) e ff 2 mp=2 LE (p; m; (p ff)) e 2ff 2 m Lemma 2 P P 1 (s) &gt; 1=4. Proof: In order to lower bound P S 2 P 1 (s), we first lower bound P S 1 P 1 (s).
Reference: [2] <author> Dana Angluin and Leslie G. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and match-ings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18(2) </volume> <pages> 155-193, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: We make use of the following bounds on the tail of the binomial distribution <ref> [2, 5, 7] </ref>: Lemma 1 For p 2 [0; 1] and positive integer m, let LE (p; m; r) denote the probability of at most an r fraction of successes in m independent trials of a Bernoulli random variable with probability of success p.
Reference: [3] <author> Javed Aslam and Scott Decatur. </author> <title> General bounds on statistical query learning and PAC learning with noise via hypothesis boosting. </title> <booktitle> In Proceedings of the 34 th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 282-291, </pages> <month> November </month> <year> 1993. </year> <note> To appear in Information and Computation. </note>
Reference-contexts: The results of Talagrand [10] imply that for classes of finite VC-dimension, a sample of size O VC (F) 1 "(12)ffi is sufficient for classification noise learning. This result also relies on the ability to minimize disagreements. Finally, Aslam and Decatur <ref> [3] </ref> have shown that a sample of size ~ O poly (n)+log (1=ffi) " 2 (12) 2 is sufficient for polynomial time classification noise learning of any class known to be learnable in the statistical query model. 2 2 Definitions In this section we give formal definitions of the learning models
Reference: [4] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-865, </pages> <year> 1989. </year>
Reference-contexts: The noise-free bound of Ehrenfeucht et al. [6] and Blumer et al. <ref> [4] </ref> states that VC (F) log (1=ffi) examples are required for learning. In this paper, we show a general lower bound of log (1=ffi) on the number of examples required for PAC learning in the presence of classification noise. <p> Furthermore, the result holds for all algorithms independent of the computational resources used. Finally, note that Theorem 3 is a generalized analog of the general lower bound of Blumer et al. <ref> [4] </ref> and Ehrenfeucht et al. [6] for noise-free learning. 4 Optimality of the General Lower Bound In this section, we show that the general lower bound of Theorem 3 is asymptotically optimal in a very strong sense.
Reference: [5] <author> Herman Chernoff. </author> <title> A measure of the asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Ann. Math. Statist., </journal> <volume> 23 </volume> <pages> 493-507, </pages> <year> 1952. </year>
Reference-contexts: We make use of the following bounds on the tail of the binomial distribution <ref> [2, 5, 7] </ref>: Lemma 1 For p 2 [0; 1] and positive integer m, let LE (p; m; r) denote the probability of at most an r fraction of successes in m independent trials of a Bernoulli random variable with probability of success p.
Reference: [6] <author> Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The noise-free bound of Ehrenfeucht et al. <ref> [6] </ref> and Blumer et al. [4] states that VC (F) log (1=ffi) examples are required for learning. In this paper, we show a general lower bound of log (1=ffi) on the number of examples required for PAC learning in the presence of classification noise. <p> Furthermore, the result holds for all algorithms independent of the computational resources used. Finally, note that Theorem 3 is a generalized analog of the general lower bound of Blumer et al. [4] and Ehrenfeucht et al. <ref> [6] </ref> for noise-free learning. 4 Optimality of the General Lower Bound In this section, we show that the general lower bound of Theorem 3 is asymptotically optimal in a very strong sense.
Reference: [7] <author> Wassily Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> J. Amer. Statist. Assoc., </journal> <volume> 58 </volume> <pages> 13-30, </pages> <year> 1963. </year>
Reference-contexts: We make use of the following bounds on the tail of the binomial distribution <ref> [2, 5, 7] </ref>: Lemma 1 For p 2 [0; 1] and positive integer m, let LE (p; m; r) denote the probability of at most an r fraction of successes in m independent trials of a Bernoulli random variable with probability of success p.
Reference: [8] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: Previous attempts at lower bounds on the sample complexity of classification noise learning yielded suboptimal results and in some cases relied on placing restrictions on the learning algorithm. Laird <ref> [8] </ref> showed that a specific learning algorithm, one which simply chooses the function in the target class F with the fewest disagreements on the sample of data, requires log (jFj=ffi) examples. Note that this result is only applicable to finite target classes. <p> Laird <ref> [8] </ref> has shown that for finite classes, a sample of size O log (jFj=ffi) is sufficient for classification noise learning. This result is not computationally efficient in general, since it relies on the ability to minimize disagreements with respect to a sample. <p> First consider the upper bound for learning finite classes in the presence of classification noise due to Laird <ref> [8] </ref>. Laird has shown that a sample of size O log (jFj=ffi) is sufficient for clas sification noise learning. <p> Theorem 4 The class S of symmetric functions is learnable by symmetric functions in polynomial time by an algorithm which uses a sample of size O VC (S) log (1=ffi) Proof: We make use of a result of Laird <ref> [8] </ref> on learning a class F in the presence of classification noise which states that if an algorithm uses a sample of size 8 ln (jFj=ffi) 3"(12) 2 and outputs the function in 8 the class F which has the fewest disagreements with the sample, then this hypothesis is "-good with
Reference: [9] <author> Hans Ulrich Simon. </author> <title> General bounds on the number of examples needed for learning probabilistic concepts. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 402-411. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Laird [8] showed that a specific learning algorithm, one which simply chooses the function in the target class F with the fewest disagreements on the sample of data, requires log (jFj=ffi) examples. Note that this result is only applicable to finite target classes. Simon <ref> [9] </ref> showed that any algorithm for learning in the presence of classification noise requires VC (F) examples where VC (F ) is the Vapnik-Chervonenkis dimension of F . 1 One could also consider a general lower bound on the sample complexity of noise-free learning to be a general lower bound on <p> with probability at most ffi when the target is f 0 , then the algorithm must fail with probability more than ffi when the target is f 1 . 2 Note that we have not attempted to optimize the constants in this lower bound. 3.1 The Combined Lower Bound Simon <ref> [9] </ref> proves the following lower bound: Theorem 2 PAC learning a function class F in the presence of classification noise requires a sample of size VC (F) 7 Combining Theorems 1 and 2 we obtain the following lower bound on the number of examples required for PAC learning in the presence
Reference: [10] <author> M. Talagrand. </author> <title> Sharper bounds for empirical processes. </title> <note> To appear in Annals of Probability and Its Applications., </note> <year> 1991. </year>
Reference-contexts: Laird [8] has shown that for finite classes, a sample of size O log (jFj=ffi) is sufficient for classification noise learning. This result is not computationally efficient in general, since it relies on the ability to minimize disagreements with respect to a sample. The results of Talagrand <ref> [10] </ref> imply that for classes of finite VC-dimension, a sample of size O VC (F) 1 "(12)ffi is sufficient for classification noise learning. This result also relies on the ability to minimize disagreements.
Reference: [11] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: 1 Introduction In this paper, we derive bounds on the complexity of learning in the presence of noise. We consider the "Probably Approximately Correct" (PAC) model of learning introduced by Valiant <ref> [11] </ref>. In this setting, a learner is given the task of determining a close approximation of an unknown f0; 1g-valued target function f . The learner is given F , a class of functions to which f belongs, and accuracy and confidence parameters " and ffi.
References-found: 11

