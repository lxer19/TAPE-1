URL: ftp://ftp.cs.washington.edu/tr/1998/10/UW-CSE-98-10-05.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-name.html
Root-URL: http://www.cs.washington.edu
Title: Detour: a Case for Informed Internet Routing and Transport  
Author: Stefan Savage Tom Anderson Amit Aggarwal David Becker Neal Cardwell Andy Collins Eric Hoffman John Snell Amin Vahdat Geoff Voelker John Zahorjan 
Address: Seattle  
Affiliation: Department of Computer Science and Engineering University of Washington,  
Pubnum: Technical Report UW-CSE-98-10-05  
Abstract: Despite its obvious success, robustness, and scalability, the Internet suffers from a number of end-to-end performance and availability problems. In this paper, we attempt to quantify the Internet's inefficiencies and then we argue that Internet behavior can be improved by spreading intelligent routers at key access and interchange points to actively manage traffic. Our Detour prototype aims to demonstrate practical benefits to end users, without penalizing non-Detour users, by aggregating traffic information across connections and using more efficient routes to improve Internet performance. 
Abstract-found: 1
Intro-found: 1
Reference: [Anderson et al. 95] <author> Anderson, T., Culler, D., Patterson, D., and Team, T. N. </author> <title> A Case for Now (Networks of Workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1):5464, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: Unfortunately, despite the overriding focus of the Internet design on robustness, for all practical purposes the Internet is the largest performance and availability bottleneck today for end-to-end applications. While it is possible to build highly available end servers using networks of workstations <ref> [Anderson et al. 95] </ref> and RAIDs [Patterson et al. 98], as anyone who has used the Web knows, the path to a server can be very slow and often completely unavailable. The result is lost productivity while users wait for Web documents to be transmitted over the network.
Reference: [Balakrishnan et al. 98] <author> Balakrishnan, H., Padmanabhan, V., Seshan, S., Stemm, M., and Katz, R. </author> <title> TCP Behavior of a Busy Internet Server: Analysis and Improvements. </title> <booktitle> In Proceedings of the IEEE Infocom Conference, </booktitle> <address> San Francisco, CA, </address> <month> March </month> <year> 1998. </year>
Reference-contexts: If the network is congested, these short bursty connections will increase the probability that packets are dropped. Internet transport protocols were designed assuming that packet loss rates were under 1% [Clark 88]. Current packet loss rates have been measured as averaging 5-6% <ref> [Paxson 97, Balakrishnan et al. 98] </ref>. Assumptions about Internet routing have changed as well. The Internet was originally designed to provide universal reachability between networks; all network links were available to carry traffic for any host.
Reference: [Breslau 95] <author> Breslau, L. M. </author> <title> Adaptive Sourcde Routing of Real-Time Traffic in Integrated Services Networks. </title> <type> PhD dissertation, </type> <institution> University of Southern California, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: The 8 early ARPANET used measurement-based adaptive routing, but it was abandoned because of instability fluctuations in load would cause routes to change which in turn would cause the load to fluctuate. However, recent work by <ref> [Breslau 95] </ref> and others demonstrates that a well designed routing system can be both adaptive and stable. Another opportunity we plan to explore is dynamic multi-path routing. Routers in the Internet generally send all packets to a particular destination along the same path.
Reference: [Clark 88] <author> Clark, D. </author> <title> The Design Philosophy of the DARPA Internet Protocols. </title> <booktitle> In Proceedings of ACM SIGCOMM '88, </booktitle> <pages> pages 106114, </pages> <address> Palo Alto, CA, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: As a system, the Internet's growth has been matched only by the major infrastructure projects of the early 1900's: the electric power grid, the automobile, and the telephone network. The Internet's scalability has been achieved only by the single-minded focus of its designers on robustness and adaptability <ref> [Clark 88] </ref>. Over the past three decades, the Internet has added support for automatic name translation, hierarchical routing, congestion avoidance, dynamic address assignment, multicast, mobility, and most recently, attempts at real-time support. The future challenges faced by the Internet will require continued evolution. <p> If the network is congested, these short bursty connections will increase the probability that packets are dropped. Internet transport protocols were designed assuming that packet loss rates were under 1% <ref> [Clark 88] </ref>. Current packet loss rates have been measured as averaging 5-6% [Paxson 97, Balakrishnan et al. 98]. Assumptions about Internet routing have changed as well. The Internet was originally designed to provide universal reachability between networks; all network links were available to carry traffic for any host.
Reference: [Jacobson 88] <author> Jacobson, V. </author> <title> Congestion Avoidance and Control. </title> <booktitle> In Proceedings of ACM SIGCOMM '88, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: It is a reliable, connection oriented protocol and uses a sliding window mechanism for explicit flow control. TCP has a number of mechanisms for learning about and adapting to network resource limitations, first introduced in <ref> [Jacobson 88] </ref> and detailed in [Stevens 94]. They have proved to be immensely successful at preventing 4 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 Fraction of paths measured D p r o a b Default route Alternate route two hosts.
Reference: [Mathis et al. 97] <author> Mathis, M., Semke, J., Mahdavi, J., and Ott, T. </author> <title> The Macroscopic Behavior of the TCP Congestion Avoidance Algorithm). </title> <journal> ACM Computer Communications Review, </journal> <volume> 27(3), </volume> <month> July </month> <year> 1997. </year>
Reference-contexts: For sufficiently long network flows and no timeouts, a simple model for the average bandwidth delivered by TCP in the presence of loss is: BW &lt; ( RT T 1 (p) where p is the probability that a packet is dropped <ref> [Mathis et al. 97] </ref>. The rough intuition behind this model is that 1 p (p) corresponds to the average window size in packets when using the additive increase/multiplicative decrease algorithm. With larger drop rates, fewer packets can be sent before the window is decreased.
Reference: [Nielsen et al. 97] <author> Nielsen, H., Gettys, J., Baird-Smith, A., Prud'hommeaux, E., Lie, H., and Lilley, C. </author> <title> Network Performance Effects of HTTP/1.1, CSS1, </title> <booktitle> and PNG. In Proceedings of ACM SIGCOMM '97, </booktitle> <month> September </month> <year> 1997. </year>
Reference-contexts: There are a number of protocol choices and implementation artifacts that make startup behavior particularly poor and penalize short flows disproportionately. While new protocols, such as HTTP/1.1 <ref> [Nielsen et al. 97] </ref>, promise to have some impact on increasing the average flow size, we expect short flows to be an important part of the traffic mix for some time. The first and most obvious problem is connection setup.
Reference: [Padhye et al. 98] <author> Padhye, J., Firoiu, V., Towsley, D., and Kurose, J. </author> <title> Modeling TCP Throughput: A Simple Model and its Empirical Validation. </title> <booktitle> In Proceedings of ACM SIGCOMM '98, </booktitle> <address> Vancouver, BC, </address> <month> September </month> <year> 1998. </year>
Reference-contexts: Sometimes fast retransmit is not effective and the sender must wait for a timeout, further reducing the achievable bandwidth. Incorporating these cases (see <ref> [Padhye et al. 98] </ref>), brings the average bandwidth for our transfer down to 228Kbps. 3.3 Short Flows Most network flows are short and consequently the situation is usually even worse.
Reference: [Patterson et al. 98] <author> Patterson, D., Gibson, G., and Katz, R. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> In Proceedings of the International Conference on the Management of Data, </booktitle> <pages> pages 109116, </pages> <month> June </month> <year> 1998. </year>
Reference-contexts: Unfortunately, despite the overriding focus of the Internet design on robustness, for all practical purposes the Internet is the largest performance and availability bottleneck today for end-to-end applications. While it is possible to build highly available end servers using networks of workstations [Anderson et al. 95] and RAIDs <ref> [Patterson et al. 98] </ref>, as anyone who has used the Web knows, the path to a server can be very slow and often completely unavailable. The result is lost productivity while users wait for Web documents to be transmitted over the network.
Reference: [Paxson & Floyd 97] <author> Paxson, V. and Floyd, S. </author> <title> Why We Don't Know How To Simulate The Internet. </title> <booktitle> In Proceedings of the 1997 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: The result is lost productivity while users wait for Web documents to be transmitted over the network. The scale, heterogeneity, and dynamic nature of the Internet make it difficult to difficult to determine the exact causes of Internet performance problems <ref> [Paxson & Floyd 97] </ref>. However, it is clear that a number of assumptions have changed since the Internet protocols were designed in the early 80's.
Reference: [Paxson 97] <author> Paxson, V. </author> <title> End-to-End Internet Packet Dynamics. </title> <booktitle> In Proceedings of ACM SIGCOMM '97, </booktitle> <month> September </month> <year> 1997. </year>
Reference-contexts: If the network is congested, these short bursty connections will increase the probability that packets are dropped. Internet transport protocols were designed assuming that packet loss rates were under 1% [Clark 88]. Current packet loss rates have been measured as averaging 5-6% <ref> [Paxson 97, Balakrishnan et al. 98] </ref>. Assumptions about Internet routing have changed as well. The Internet was originally designed to provide universal reachability between networks; all network links were available to carry traffic for any host.
Reference: [Stevens 94] <editor> Stevens, W. R. TCP/IP Illustrated, </editor> <volume> Volume 1. </volume> <publisher> Addison-Wesley, </publisher> <year> 1994. </year> <month> 10 </month>
Reference-contexts: It is a reliable, connection oriented protocol and uses a sliding window mechanism for explicit flow control. TCP has a number of mechanisms for learning about and adapting to network resource limitations, first introduced in [Jacobson 88] and detailed in <ref> [Stevens 94] </ref>. They have proved to be immensely successful at preventing 4 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 Fraction of paths measured D p r o a b Default route Alternate route two hosts.
References-found: 12

