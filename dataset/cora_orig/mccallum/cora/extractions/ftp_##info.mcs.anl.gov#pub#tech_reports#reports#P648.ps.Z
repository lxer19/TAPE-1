URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P648.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Title: TRUNCATED QR ALGORITHMS AND THE SOLUTION OF LARGE-SCALE EIGENVALUE PROBLEMS  
Author: R. B. LEHOUCQ 
Keyword: Key words. QR algorithm, simultaneous iteration, Arnoldi reduction, restarting, eigenvalues.  
Note: AMS subject classifications. 65F15, 65G05  
Abstract: The QR algorithm has emerged as the general-purpose method of choice for computing the Schur decomposition of a matrix. For most large eigenvalue problems, however, the QR algorithm cannot be used because of the explicit storage of the matrix and because often only the action of the matrix upon a vector (or group of vectors) is available. Typically, only a small number of eigenvalues and the associated invariant subspace are required. This article considers a truncated QR algorithm. We show that a truncated QR algorithm is a generalization of Sorensen's implicitly restarted Arnoldi method to block Arnoldi reductions. Moreover, implicitly restarting an Arnoldi reduction is simultaneous iteration with an implicit projection step to accelerate convergence to the invariant subspace of interest. This is a generalization of the Rayleigh-Ritz procedure on a block Krylov subspace for a non Hermitian matrix. The moral of our story is that the large scale eigenvalue problem is intimately involved with the dense one. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. D. Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <note> LAPACK Users' Guide, SIAM, Philadelphia, second ed., </note> <year> 1995. </year>
Reference-contexts: The QR algorithm, developed independently by both Francis [11, 12] and Kublanovskaya [20], instead uses a sequence of unitary similarity transformations. The algorithm iteratively computes an approximation to a Schur decomposition of the matrix. The QR algorithm is implemented in the EISPACK [42] and LAPACK <ref> [1] </ref> software packages. Unfortunately, for large-scale eigenvalue problems, the QR algorithm is not a practical method. An eigenvalue problem is considered large if it cannot be solved with the standard QR algorithm (as implemented in EISPACK and LAPACK).
Reference: [2] <author> W. E. </author> <title> Arnoldi, The principle of minimized iterations in the solution of the matrix eigenvalue problem, </title> <journal> Quart. J. Applied Mathematics, </journal> <volume> 9 (1951), </volume> <pages> pp. 17-29. </pages>
Reference-contexts: Partial Reduction to Band Hessenberg Form. The initial step of the practical QR algorithm reduces A to an upper Hessenberg matrix via a sequence of elementary unitary matrices. Unfortunately, these elementary matrices require accessing the entire matrix, possibly destroying any sparsity or structure the matrix possess. The Arnoldi reduction <ref> [2] </ref>, on the other hand, requires only the application of A with a vector. Moreover, it allows us to sequentially reduce A to upper Hessenberg form, producing the leading portion of the final upper Hessenberg matrix at every step. In fact, this was the motivation in Arnoldi's study.
Reference: [3] <author> J. Baglama, D. Calvetti, and L. Reichel, </author> <title> Iterative methods for the computation of a few eigenvalues of a large symmetric matrix, </title> <journal> BIT, </journal> <volume> 36 (1996), </volume> <pages> pp. 400-440. </pages>
Reference-contexts: Equations (8.7)-(8.8) explain how to apply the mth shift. Thus, even if k eigenvalues are of interest, the value r may be varied in the algorithm of Figure 9.2 from k for an adaptive strategy. This is a variation on a strategy proposed by Baglama, Calvetti, and Reichel <ref> [3] </ref> for symmetric A: They employ Leja shifts and demonstrate how this strategy can outperform an exact shift strategy for small m: They have also extended their results to a block formulation [4]. The question of a near-optimal shift strategy is still the work of current research.
Reference: [4] <author> J. Baglama, D. Calvetti, L. Reichel, and A. Ruttan, </author> <title> Computation of a few close eigen-values of a large matrix with application to liquid crystal modeling, </title> <type> technical report, </type> <institution> Department of Mathematics and Computer science, Kent State University, </institution> <year> 1997. </year>
Reference-contexts: This is a variation on a strategy proposed by Baglama, Calvetti, and Reichel [3] for symmetric A: They employ Leja shifts and demonstrate how this strategy can outperform an exact shift strategy for small m: They have also extended their results to a block formulation <ref> [4] </ref>. The question of a near-optimal shift strategy is still the work of current research. However, it is clear from the results in [52] that an approximation p () to the minimization problem (9.2) is required.
Reference: [5] <author> Z. Bai and J. W. Demmel, </author> <title> On a block implementation of Hessenberg multishift QR iteration, </title> <journal> International Journal of High Speed Computation, </journal> <volume> 1 (1989), </volume> <pages> pp. 97-112. </pages>
Reference-contexts: This allows a complex 8 R. B. LEHOUCQ conjugate pair of shifts to be applied by using the degree 2 polynomial (o 1 )( o 1 ): The LAPACK subroutines SHEQR also allow more than two shifts to be applied. This is based on the multi-shift QR algorithm <ref> [5] </ref> by Bai and Demmel for upper (b = 1) Hessenberg matrices. 7. Computing Eigenvalues. Since our interest is in partial Schur decompositions, we use employ the following convergence criterion.
Reference: [6] <author> Z. Bai and G. W. Stewart, </author> <title> SRRIT | A FORTRAN subroutine to calculate the dominant invariant subspace of a nonsymmetric matrix, </title> <type> Technical Report 2908, </type> <institution> Department of Com 18 R. B. LEHOUCQ puter Science, University of Maryland, </institution> <year> 1992. </year> <note> Submitted to ACM Transactions on Mathematical Software. </note>
Reference-contexts: They show how a QR algorithm performed on H m is equivalent to a truncated QR algorithm on A: 8.1. Subspace Iteration. A classical method of solution for the large-scale eigenvalue problem is subspace (or simultaneous) iteration <ref> [6, 10, 32, 33, 37, 45, 48] </ref>. Subspace iteration was originally introduced by Bauer [7], who called the method Treppeniteration (staircase iteration).
Reference: [7] <author> F. L. Bauer, </author> <title> Das Verfahren der Treppeniteration und verwandte Verfahren zur Losung alge-braischer Eigenwertproblem, </title> <journal> Z. Angew. Mat. Phys., </journal> <volume> 8 (1957), </volume> <pages> pp. 214-235. </pages>
Reference-contexts: Subspace Iteration. A classical method of solution for the large-scale eigenvalue problem is subspace (or simultaneous) iteration [6, 10, 32, 33, 37, 45, 48]. Subspace iteration was originally introduced by Bauer <ref> [7] </ref>, who called the method Treppeniteration (staircase iteration). It is a straightforward method for computing the eigenvalues of largest modulus of a matrix and is a generalization of the power method in that a matrix representation of a subspace of size larger than one is em ployed.
Reference: [8] <author> F. Chatelin, </author> <title> Eigenvalues of Matrices, </title> <publisher> Wiley, </publisher> <year> 1993. </year>
Reference-contexts: Chatelin <ref> [8, pp. 253-257] </ref> and Saad [38, pp. 156-159] provide a discussion that builds upon the work of Stewart [45, 48]. This technique is also referred to as orthonormal iteration 12 R. B. LEHOUCQ with projection. It is a generalization of a Rayleigh-Ritz procedure to a non-Hermitian matrix. <p> fl ` and ffi ` in determining the quality of the R (V ` ) as an eigenspace of A: Since V H ` Z ` = (I+P H P) 1=2 , Stewart [46] shows that the singular values of P are the tangents of the canonical, or principal, angles <ref> [8, 13, 46] </ref> between the two spaces spanned by the columns of V ` and Z ` , respectively. Golub and Wilkinson [16] also examine the many practical difficulties involved when computing invariant subspaces. They conclude that working with a basis of Schur vectors is a better-behaved numerical process.
Reference: [9] <author> J. Cullum and W. E. Donath, </author> <title> A block Lanczos algorithm for computing the q algebraically largest eigenvalues and a corresponding eigenspace for large, sparse symmetric matrices, </title> <booktitle> in Proceedings of the 1974 IEEE Conference on Decision and Control, </booktitle> <address> New York, </address> <year> 1974, </year> <pages> pp. 505-509. </pages>
Reference-contexts: This defines the iteration and is deemed successful if improved estimates to the eigenvalues of A appear in the subsequent reductions. A restarted Arnoldi iteration was introduced by Saad [35] to overcome these difficulties, based on similar ideas developed for the Lanczos process by Paige [28], Cullum and Donath <ref> [9] </ref>, and Golub and Underwood [15]. Karush [19] proposes what appears to be the first example of a restarted iteration. Sadkane considers a restarted block Arnoldi method using Chebyshev polynomials [39]. Scott has produced a block Arnoldi code [40].
Reference: [10] <author> I. S. Duff and J. A. Scott, </author> <title> Computing selected eigenvalues of large sparse unsymmetric matrices using subspace iteration, </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 19 (1993), </volume> <pages> pp. 137-159. </pages>
Reference-contexts: They show how a QR algorithm performed on H m is equivalent to a truncated QR algorithm on A: 8.1. Subspace Iteration. A classical method of solution for the large-scale eigenvalue problem is subspace (or simultaneous) iteration <ref> [6, 10, 32, 33, 37, 45, 48] </ref>. Subspace iteration was originally introduced by Bauer [7], who called the method Treppeniteration (staircase iteration).
Reference: [11] <author> J. G. F. Francis, </author> <title> The QR transformation|part 1, </title> <journal> The Computer Journal, </journal> <volume> 4 (1961), </volume> <pages> pp. 265-271. </pages> <note> [12] , The QR transformation|part 2, The Computer Journal, </note> <month> 4 </month> <year> (1962), </year> <pages> pp. 332-345. </pages>
Reference-contexts: 1. Introduction. The QR algorithm is a general-purpose method for computing all the eigenvalues of a matrix. The LR-iteration of Rutishauser [31], which preceded its discovery, is based on a triangular sequence of similarity transformation. The QR algorithm, developed independently by both Francis <ref> [11, 12] </ref> and Kublanovskaya [20], instead uses a sequence of unitary similarity transformations. The algorithm iteratively computes an approximation to a Schur decomposition of the matrix. The QR algorithm is implemented in the EISPACK [42] and LAPACK [1] software packages.
Reference: [13] <author> G. H. Golub and C. F. V. Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <note> second ed., </note> <year> 1989. </year> <title> [14] , Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, third ed., </address> <year> 1996. </year>
Reference-contexts: fl ` and ffi ` in determining the quality of the R (V ` ) as an eigenspace of A: Since V H ` Z ` = (I+P H P) 1=2 , Stewart [46] shows that the singular values of P are the tangents of the canonical, or principal, angles <ref> [8, 13, 46] </ref> between the two spaces spanned by the columns of V ` and Z ` , respectively. Golub and Wilkinson [16] also examine the many practical difficulties involved when computing invariant subspaces. They conclude that working with a basis of Schur vectors is a better-behaved numerical process.
Reference: [15] <author> G. H. Golub and R. Underwood, </author> <title> The block Lanczos method for computing eigenvalues, in Mathematical Software III, </title> <editor> J. R. Rice, ed., </editor> <address> New York, 1977, </address> <publisher> Academic Press, </publisher> <pages> pp. 361-377. </pages>
Reference-contexts: A restarted Arnoldi iteration was introduced by Saad [35] to overcome these difficulties, based on similar ideas developed for the Lanczos process by Paige [28], Cullum and Donath [9], and Golub and Underwood <ref> [15] </ref>. Karush [19] proposes what appears to be the first example of a restarted iteration. Sadkane considers a restarted block Arnoldi method using Chebyshev polynomials [39]. Scott has produced a block Arnoldi code [40].
Reference: [16] <author> G. H. Golub and J. H. Wilkinson, </author> <title> Ill-conditioned eigensystems and the computation of the Jordan canonical form, </title> <journal> SIAM Review, </journal> <volume> 18 (1976), </volume> <pages> pp. 578-619. </pages>
Reference-contexts: Golub and Wilkinson <ref> [16] </ref> also examine the many practical difficulties involved when computing invariant subspaces. They conclude that working with a basis of Schur vectors is a better-behaved numerical process. Within the context of subspace iteration, Stewart [48] also arrives at the same conclusion.
Reference: [17] <author> R. G. Grimes, J. G. Lewis, and H. D. Simon, </author> <title> A shifted block Lanczos algorithm for solving sparse symmetric generalized eigenproblems, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 15 (1994), </volume> <pages> pp. 228-272. </pages>
Reference-contexts: In many instances, the cost of computing a few matrix vector products is commensurate with that of one matrix vector product. There is also the issue of reliably computing clustered and/or multiple eigenvalues. See <ref> [17] </ref> for references and information on a block Lanczos reduction. Let b &gt; 0, an integer, be the block size.
Reference: [18] <author> Z. Jia, </author> <title> The convergence of generalized Lanczos methods for large unsymmetric eigenproblems, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <month> 15 </month> <year> (1994). </year>
Reference-contexts: He extended his result to a b = 1 Arnoldi reduction in [36] with the assumption that A is diagonalizable. Jia <ref> [18] </ref> removed this restriction. One of Jia's main conclusions is that although a Arnoldi reduction may produce an approximation to an eigenvalue of A, the associated eigenvector may not be well approximated by the reduction.
Reference: [19] <author> W. Karush, </author> <title> An iterative method for finding characteristics vectors of a symmetric matrix, </title> <journal> Pacific J. Mathematics, </journal> <volume> 1 (1951), </volume> <pages> pp. 233-248. </pages>
Reference-contexts: A restarted Arnoldi iteration was introduced by Saad [35] to overcome these difficulties, based on similar ideas developed for the Lanczos process by Paige [28], Cullum and Donath [9], and Golub and Underwood [15]. Karush <ref> [19] </ref> proposes what appears to be the first example of a restarted iteration. Sadkane considers a restarted block Arnoldi method using Chebyshev polynomials [39]. Scott has produced a block Arnoldi code [40]. We call all these related schemes explicitly restarted Arnoldi methods because they are not truncated QR algorithms.
Reference: [20] <author> V. N. Kublanovskaya, </author> <title> On some algorithms for the solution of the complete eigenvalue problem, </title> <institution> USSR Comput. Math. Phys., </institution> <month> 3 </month> <year> (1961), </year> <pages> pp. 637-657. </pages>
Reference-contexts: 1. Introduction. The QR algorithm is a general-purpose method for computing all the eigenvalues of a matrix. The LR-iteration of Rutishauser [31], which preceded its discovery, is based on a triangular sequence of similarity transformation. The QR algorithm, developed independently by both Francis [11, 12] and Kublanovskaya <ref> [20] </ref>, instead uses a sequence of unitary similarity transformations. The algorithm iteratively computes an approximation to a Schur decomposition of the matrix. The QR algorithm is implemented in the EISPACK [42] and LAPACK [1] software packages. Unfortunately, for large-scale eigenvalue problems, the QR algorithm is not a practical method.
Reference: [21] <author> C. </author> <title> Lanczos, An iteration method for the solution of the eigenvalue problem of linear differential and integral operators, </title> <institution> J. Research of the National Bureau of Standards, </institution> <month> 45 </month> <year> (1950), </year> <pages> pp. 255-282. </pages> <note> Research Paper 2133. </note>
Reference-contexts: Moreover, it allows us to sequentially reduce A to upper Hessenberg form, producing the leading portion of the final upper Hessenberg matrix at every step. In fact, this was the motivation in Arnoldi's study. When the matrix A is Hermitian, the Lanczos reduction <ref> [21] </ref> is recovered. Since our concern is in the solution of eigenvalue problems in which A is not only large but expensive to apply, block Arnoldi reductions [39, 40] are considered.
Reference: [22] <author> R. B. Lehoucq and K. J. Maschhoff, </author> <title> A practical implementation of an implicitly re-started block arnoldi method, </title> <type> Preprint MCS-P649-0297, </type> <institution> Argonne National Laboratory, Argonne, Ill, </institution> <year> 1997. </year> <note> In preparation. </note>
Reference-contexts: The recent paper [24] instead explains how to deflate the Ritz pair in an implicit fashion, thus avoiding the need to build a new reduction. Although this implicit deflation technique assumed an unblocked Arnoldi reduction, the scheme is easily extended to a block reduction. The report <ref> [22] </ref> considers a practical implementation of a truncated QR algorithm in detail.
Reference: [23] <author> R. B. Lehoucq and J. A. Scott, </author> <title> An evaluation of software for computing eigenvalues of sparse nonsymmetric matrices, </title> <type> Preprint MCS-P547-1195, </type> <institution> Argonne National Laboratory, Argonne, Ill, </institution> <year> 1996. </year>
Reference-contexts: We remark that both restarting techniques differ only in the polynomial filter applied. For a detailed computational study comparing software based on these two different restarting mechanisms, we refer the reader to <ref> [23] </ref>. For an unblocked Arnoldi reduction, Morgan [27] shows that an implicit restarting mechanism is a better be haved numerical process than an explicit one. 10. Convergence of a Truncated QR Algorithm. Orthonormal iteration with a projection step converges at a linear rate.
Reference: [24] <author> R. B. Lehoucq and D. C. Sorensen, </author> <title> Deflation techniques for an implicitly restarted Arnoldi iteration, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 17 (1996), </volume> <pages> pp. 789-821. </pages>
Reference-contexts: This is an outline of the procedure discussed in [38, pp. 179-182]. Scott [40] employs this deflation scheme within a block Arnoldi reduction. However, these explicit deflation schemes require that the Arnoldi reduction be restarted from scratch in order to deflate the Ritz pair. The recent paper <ref> [24] </ref> instead explains how to deflate the Ritz pair in an implicit fashion, thus avoiding the need to build a new reduction. Although this implicit deflation technique assumed an unblocked Arnoldi reduction, the scheme is easily extended to a block reduction.
Reference: [25] <author> R. B. Lehoucq, D. C. Sorensen, P. Vu, and C. Yang, ARPACK: </author> <title> An implementation of an implicitly re-started Arnoldi method for computing some of the eigenvalues and eigenvec-tors of a large sparse matrix, </title> <note> 1996. See the URL http://www.caam.rice.edu/ARPACK/. </note>
Reference-contexts: A relatively recent variant was developed by Sorensen [43] as a more efficient and numerically stable way to implement restarting. This technique, the implicitly restarted Arnoldi method, is implemented in the ARPACK <ref> [25] </ref> software package. The paper [43] only considered the block size b = 1 and stated that the method is equivalent to a truncated QR algorithm. The results in the preceding section showed a direct connection.
Reference: [26] <author> T. A. Manteuffel, </author> <title> Adaptive procedure for estimating parameters for the nonsymmetric Tchebychev iteration, </title> <journal> Numerische Mathematik, </journal> <volume> 31 (1978), </volume> <pages> pp. 183-208. </pages>
Reference-contexts: The acceleration techniques and hybrid methods presented by Saad in Chapter 7 of [38] attempt to improve explicit restarting by approximately solving the min-max problem of equation (9.2). Motivated by Manteuffel's scheme <ref> [26] </ref>, Saad first proposed the use of Chebyshev polynomials in [37]. A Chebyshev polynomial (A) on an ellipse containing the unwanted Ritz values is applied to the restart vector in an attempt to accelerate convergence of the original era iteration.
Reference: [27] <author> R. B. Morgan, </author> <title> On restarting the Arnoldi method for large nonsymmetric eigenvalue problems, </title> <journal> Mathematics of Computation, </journal> <volume> 65 (1996), </volume> <pages> pp. 1213-1230. </pages>
Reference-contexts: We remark that both restarting techniques differ only in the polynomial filter applied. For a detailed computational study comparing software based on these two different restarting mechanisms, we refer the reader to [23]. For an unblocked Arnoldi reduction, Morgan <ref> [27] </ref> shows that an implicit restarting mechanism is a better be haved numerical process than an explicit one. 10. Convergence of a Truncated QR Algorithm. Orthonormal iteration with a projection step converges at a linear rate.
Reference: [28] <author> C. C. Paige, </author> <title> The computation of eigenvalues and eigenvectors of very large sparse matrices, </title> <type> PhD thesis, </type> <institution> University of London, </institution> <address> London, England, </address> <year> 1971. </year>
Reference-contexts: This defines the iteration and is deemed successful if improved estimates to the eigenvalues of A appear in the subsequent reductions. A restarted Arnoldi iteration was introduced by Saad [35] to overcome these difficulties, based on similar ideas developed for the Lanczos process by Paige <ref> [28] </ref>, Cullum and Donath [9], and Golub and Underwood [15]. Karush [19] proposes what appears to be the first example of a restarted iteration. Sadkane considers a restarted block Arnoldi method using Chebyshev polynomials [39]. Scott has produced a block Arnoldi code [40].
Reference: [29] <author> B. N. Parlett, </author> <title> The Symmetric Eigenvalue Problem, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1980. </year>
Reference-contexts: A further complication is that a representation for the associated invariant subspace is often required. This article consider a truncated QR algorithm. We show that a truncated QR algorithm is equivalent to simultaneous iterations. This relationship allows us to exploit the well-known connection <ref> [29, 47, 50] </ref> between simultaneous iteration and the QR algorithm. In [48], Stewart presented a generalization of the Rayleigh-Ritz method to non-Hermitian matrices. This involved performing an explicit projection step on the matrix with orthonormal columns representing the subspace. <p> The QR Algorithm. We quickly examine the QR algorithm and some of its fundamental properties. A wealth of excellent material exists on the QR algorithm. Thorough introductions are given by Golub and Van Loan [14], Parlett <ref> [29] </ref>, Stew-art [47], Watkins [50, 51] and of course Wilkinson [53]. Figure 4.1 lists the explicitly shifted QR iteration. The following properties are consequences of the iteration. They are easily established using mathematical induction; see, for example, [47, pp. 351-354]. Assume the notation of the algorithm listed in Figure 4.
Reference: [30] <author> A. Ruhe, </author> <title> Rational Krylov sequence methods for eigenvalue computations, Linear Algebra and Its Applications, </title> <booktitle> 58 (1984), </booktitle> <pages> pp. 391-405. </pages>
Reference-contexts: The residual matrix F m is a matrix polynomial function of the initial block. Ruhe <ref> [30] </ref> showed that the least-squares solution of (7.4) gives the coefficients associated with the monic polynomial of degree m that minimizes k ^ m (A)u 1 k over all monic polynomials ^ m of degree m: Saad [36] uses projection arguments to solve the minimization problem.
Reference: [31] <author> H. Rutihauser, </author> <title> Solution of eigenvalue problems with the LR-transformation, </title> <institution> National Bureau of Standards Applied Mathematics Service, </institution> <month> 49 </month> <year> (1958), </year> <pages> pp. </pages> <month> 47-81. </month> <title> [32] , Computational aspects of F.L. Bauer's simultaneous iteration method for symmetric matrices, </title> <journal> Numerische Mathematik, </journal> <volume> 13 (1969), </volume> <pages> pp. </pages> <month> 4-13. </month> <title> [33] , Simultaneous iteration method for symmetric matrices, </title> <journal> Numerische Mathematik, </journal> <volume> 16 (1970), </volume> <pages> pp. 205-223. </pages>
Reference-contexts: 1. Introduction. The QR algorithm is a general-purpose method for computing all the eigenvalues of a matrix. The LR-iteration of Rutishauser <ref> [31] </ref>, which preceded its discovery, is based on a triangular sequence of similarity transformation. The QR algorithm, developed independently by both Francis [11, 12] and Kublanovskaya [20], instead uses a sequence of unitary similarity transformations. The algorithm iteratively computes an approximation to a Schur decomposition of the matrix.

Reference: [39] <author> M. Sadkane, </author> <title> A block Arnoldi-Chebyshev method for computing the leading eigenpairs of large sparse unsymmetric matrices, </title> <journal> Numerische Mathematik, </journal> <volume> 64 (1993), </volume> <pages> pp. 181-193. </pages>
Reference-contexts: In fact, this was the motivation in Arnoldi's study. When the matrix A is Hermitian, the Lanczos reduction [21] is recovered. Since our concern is in the solution of eigenvalue problems in which A is not only large but expensive to apply, block Arnoldi reductions <ref> [39, 40] </ref> are considered. In many instances, the cost of computing a few matrix vector products is commensurate with that of one matrix vector product. There is also the issue of reliably computing clustered and/or multiple eigenvalues. See [17] for references and information on a block Lanczos reduction. <p> Karush [19] proposes what appears to be the first example of a restarted iteration. Sadkane considers a restarted block Arnoldi method using Chebyshev polynomials <ref> [39] </ref>. Scott has produced a block Arnoldi code [40]. We call all these related schemes explicitly restarted Arnoldi methods because they are not truncated QR algorithms. They do not use the implicit (or explicit) QR algorithm on H m as a mechanism to restart a reduction.
Reference: [40] <author> J. A. Scott, </author> <title> An Arnoldi code for computing selected eigenvalues of sparse real unsymmetric matrices, </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 21 (1995), </volume> <pages> pp. 432-475. </pages>
Reference-contexts: In fact, this was the motivation in Arnoldi's study. When the matrix A is Hermitian, the Lanczos reduction [21] is recovered. Since our concern is in the solution of eigenvalue problems in which A is not only large but expensive to apply, block Arnoldi reductions <ref> [39, 40] </ref> are considered. In many instances, the cost of computing a few matrix vector products is commensurate with that of one matrix vector product. There is also the issue of reliably computing clustered and/or multiple eigenvalues. See [17] for references and information on a block Lanczos reduction. <p> Karush [19] proposes what appears to be the first example of a restarted iteration. Sadkane considers a restarted block Arnoldi method using Chebyshev polynomials [39]. Scott has produced a block Arnoldi code <ref> [40] </ref>. We call all these related schemes explicitly restarted Arnoldi methods because they are not truncated QR algorithms. They do not use the implicit (or explicit) QR algorithm on H m as a mechanism to restart a reduction. <p> This is equivalent to working with the deflated Krylov subspace K m ((I zz H )A; U 1 ): As Ritz pairs of this deflated matrix are computed, a partial Schur decomposition is incrementally computed. This is an outline of the procedure discussed in [38, pp. 179-182]. Scott <ref> [40] </ref> employs this deflation scheme within a block Arnoldi reduction. However, these explicit deflation schemes require that the Arnoldi reduction be restarted from scratch in order to deflate the Ritz pair.
Reference: [41] <author> V. Simoncini, </author> <title> Ritz and pseudo-ritz values using matrix polynomials, Linear Algebra and Its Applications, </title> <month> 241-243 </month> <year> (1996), </year> <pages> pp. 787-802. </pages>
Reference-contexts: For a block Arnoldi reduction, the situation is more complicated and there does not appear to be an equivalent minimization property. See <ref> [41] </ref> for a characterization of a block Arnoldi process in terms of matrix polynomials. 8. A Truncated QR Algorithm. The following elementary but technical result is needed for the connection with simultaneous iteration we desire. Lemma 8.1.
Reference: [42] <author> B. T. Smith, J. M. Boyle, J. J. D. B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler, </author> <title> EISPACK Guide, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <note> second ed., </note> <year> 1976. </year> <booktitle> Volume 6 of Lecture Notes in Computer Science. </booktitle>
Reference-contexts: The QR algorithm, developed independently by both Francis [11, 12] and Kublanovskaya [20], instead uses a sequence of unitary similarity transformations. The algorithm iteratively computes an approximation to a Schur decomposition of the matrix. The QR algorithm is implemented in the EISPACK <ref> [42] </ref> and LAPACK [1] software packages. Unfortunately, for large-scale eigenvalue problems, the QR algorithm is not a practical method. An eigenvalue problem is considered large if it cannot be solved with the standard QR algorithm (as implemented in EISPACK and LAPACK).
Reference: [43] <author> D. C. Sorensen, </author> <title> Implicit application of polynomial filters in a k-step Arnoldi method, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 13 (1992), </volume> <pages> pp. 357-385. </pages>
Reference-contexts: This involved performing an explicit projection step on the matrix with orthonormal columns representing the subspace. We show that a block truncated QR algorithm performs simultaneous iteration with an implicit projection step. A block truncated QR algorithm is an extension of Sorensen's implicitly restarted Arnoldi method <ref> [43] </ref> to block Arnoldi reductions. The moral of our fl This work was supported in part by ARPA (U.S. Army ORA4466.01), by the U.S. <p> This strategy not only reduces the number of applications with A but is also more stable than traditional methods of restarting Arnoldi reductions. The next section reviews traditional restarting mechanisms and points to the recent work of Sorensen <ref> [43] </ref> as the impetus behind a truncated QR algorithm. 9. Restarting Arnoldi Reductions. During each step of computing a block Arnoldi reduction, a partial orthogonal reduction of A into a banded upper Hes-senberg matrix is produced. <p> Instead, the matrix A is explicitly applied to some linear combination of the columns of V m : Saad's original scheme used a linear combination of the wanted Ritz vectors. A relatively recent variant was developed by Sorensen <ref> [43] </ref> as a more efficient and numerically stable way to implement restarting. This technique, the implicitly restarted Arnoldi method, is implemented in the ARPACK [25] software package. The paper [43] only considered the block size b = 1 and stated that the method is equivalent to a truncated QR algorithm. <p> A relatively recent variant was developed by Sorensen <ref> [43] </ref> as a more efficient and numerically stable way to implement restarting. This technique, the implicitly restarted Arnoldi method, is implemented in the ARPACK [25] software package. The paper [43] only considered the block size b = 1 and stated that the method is equivalent to a truncated QR algorithm. The results in the preceding section showed a direct connection. The remainder of this section reviews restarting schemes and then end with an example. 9.1. Explicit Polynomial Acceleration. <p> If p &gt; 1, a restart from scratch is not needed|a length r Arnoldi reduction remains. In his paper that introduced implicit restarting, Sorensen <ref> [43] </ref> suggested using the unwanted m k eigenvalues of H m as shifts in line 2. By Lemma 6.2, H + k contains the k eigenvalues of interest.
Reference: [44] <author> A. Stathopoulos, Y. Saad, and K. Wu, </author> <title> Dynamic thick restarting of the Davidson, and the implicitly restarted Arnoldi methods, </title> <type> Tecnical Report 96/123, </type> <institution> University of Minnesota Supercomputing Institute, </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: However, it is clear from the results in [52] that an approximation p () to the minimization problem (9.2) is required. This should help guide the selection of m and r relative to k: The recent report <ref> [44] </ref> discusses an adaptive strategy for symmetric eigenvalue problems. 11.2. Deflation. Saad [38, pp. 234-235] explains how a deflation scheme leads to a far more reliable and efficient algorithm. When a Ritz pair (z; ) has a small residual, it is locked into the leading portion of an Arnoldi reduction.
Reference: [45] <author> G. W. Stewart, </author> <title> Accelerating the orthogonal iteration for the eigenvectors of a Hermitian matrix, </title> <journal> Numerische Mathematik, </journal> <volume> 13 (1969), </volume> <pages> pp. </pages> <month> 362-376. </month> <title> [46] , Error and perturbation bounds for subspaces associated with certain eigenvalue problems, </title> <journal> SIAM Review, </journal> <volume> 15 (1973), </volume> <pages> pp. </pages> <month> 727-764. </month> <title> [47] , Introduction to Matrix Computations, </title> <publisher> Academic Press, </publisher> <address> San Diego, California, </address> <year> 1973. </year> <title> [48] , Simultaneous iteration for computing invariant subspaces of non-Hermitian matrices, </title> <journal> Numerische Mathematik, </journal> <volume> 25 (1976), </volume> <pages> pp. 123-136. </pages>
Reference-contexts: By applying a polynomial p () that emphasizes the desired (or damps the unwanted) eigenvalues, the column space of V m is ordered into V m Q r : This is a generalization of the acceleration technique discussed by Stewart <ref> [45, 48] </ref> for simultaneous iteration. This will be discussed further in x 8.1. We may now exploit the connection between simultaneous iteration and a QR iteration. <p> They show how a QR algorithm performed on H m is equivalent to a truncated QR algorithm on A: 8.1. Subspace Iteration. A classical method of solution for the large-scale eigenvalue problem is subspace (or simultaneous) iteration <ref> [6, 10, 32, 33, 37, 45, 48] </ref>. Subspace iteration was originally introduced by Bauer [7], who called the method Treppeniteration (staircase iteration). <p> Post-multiplying (8.9) with W gives AU 1 W = U 1 W (W H G 1;1 W) + F 1 W (8.10) as an accelerated length-one block Arnoldi reduction. Stewart <ref> [45, 48] </ref> shows how this leads to improved convergence of orthonormal iteration to the dominant invariant subspace. <p> Chatelin [8, pp. 253-257] and Saad [38, pp. 156-159] provide a discussion that builds upon the work of Stewart <ref> [45, 48] </ref>. This technique is also referred to as orthonormal iteration 12 R. B. LEHOUCQ with projection. It is a generalization of a Rayleigh-Ritz procedure to a non-Hermitian matrix. Since it explicitly computes W; it is an explicit projection step. However, there is another way to affect the projection step.
Reference: [49] <author> J. M. Varah, </author> <title> On the separation of two matrices, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 (1979), </volume> <pages> pp. 216-222. </pages>
Reference-contexts: The conclusion now follows directly from Theorem 4.1 of Stewart [46]. The size of fl ` measures the amount of coupling between the R (V ` ) and R (W): The reciprocal of ffi ` measures the sensitivity of the R (Z ` ) as an invariant subspace. Varah <ref> [49] </ref> shows that if the matrices involved are highly nonnormal, the smallest difference between the spectrums of H ` and C ` may be an overestimate of the true separation.
Reference: [50] <author> D. S. Watkins, </author> <title> Understanding the QR algorithm, </title> <journal> SIAM Review, </journal> <volume> 24 (1982), </volume> <pages> pp. </pages> <month> 427-439. </month> <title> [51] , Fundamentals of Matrix Computations, </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: A further complication is that a representation for the associated invariant subspace is often required. This article consider a truncated QR algorithm. We show that a truncated QR algorithm is equivalent to simultaneous iterations. This relationship allows us to exploit the well-known connection <ref> [29, 47, 50] </ref> between simultaneous iteration and the QR algorithm. In [48], Stewart presented a generalization of the Rayleigh-Ritz method to non-Hermitian matrices. This involved performing an explicit projection step on the matrix with orthonormal columns representing the subspace. <p> The QR Algorithm. We quickly examine the QR algorithm and some of its fundamental properties. A wealth of excellent material exists on the QR algorithm. Thorough introductions are given by Golub and Van Loan [14], Parlett [29], Stew-art [47], Watkins <ref> [50, 51] </ref> and of course Wilkinson [53]. Figure 4.1 lists the explicitly shifted QR iteration. The following properties are consequences of the iteration. They are easily established using mathematical induction; see, for example, [47, pp. 351-354]. Assume the notation of the algorithm listed in Figure 4. Theorem 4.1.
Reference: [52] <author> D. S. Watkins and L. Elsner, </author> <title> Convergence of algorithms of decomposition type for the eigenvalue problem, Linear Algebra and Its Applications, </title> <booktitle> 143 (1991), </booktitle> <pages> pp. 19-47. </pages>
Reference-contexts: See [14, p. 333] for details. A comprehensive geometric convergence theory for the shifted QR iteration is presented by Watkins and Elsner <ref> [52] </ref> within the more general framework of generic GR algorithms. A GR algorithm is an iterative procedure in which the QR factorization is replaced with any other decomposition of the form GR = H o I, where R is upper triangular and G is a nonsingular matrix. 4.1. <p> Unlike orthonormal iteration without the projection step, the jth column (1 j b) of V 1 W 1 converges to the j Schur vector (ordered in decreasing order of magnitude) at a rate of j b+1 = j j: See <ref> [52] </ref> for details on more general shifting strategies. For the collection of subspaces underlying a block Arnoldi reduction, the situation is considerably more complicated. <p> Suppose we complete this block Arnoldi reduction to the full one: A V ` W = V ` W G `+1;` E T Watkins and Elsner <ref> [52] </ref> discuss the rate of convergence of kG `+1;` k as a function of the shifts given mild conditions on the initial block U 1 within the context of a QR algorithm. <p> The question of a near-optimal shift strategy is still the work of current research. However, it is clear from the results in <ref> [52] </ref> that an approximation p () to the minimization problem (9.2) is required. This should help guide the selection of m and r relative to k: The recent report [44] discusses an adaptive strategy for symmetric eigenvalue problems. 11.2. Deflation.
Reference: [53] <author> J. H. Wilkinson, </author> <title> The Algebraic Eigenvalue Problem, </title> <publisher> Clarendon Press, Oxford, </publisher> <address> U.K., </address> <year> 1965. </year>
Reference-contexts: The QR Algorithm. We quickly examine the QR algorithm and some of its fundamental properties. A wealth of excellent material exists on the QR algorithm. Thorough introductions are given by Golub and Van Loan [14], Parlett [29], Stew-art [47], Watkins [50, 51] and of course Wilkinson <ref> [53] </ref>. Figure 4.1 lists the explicitly shifted QR iteration. The following properties are consequences of the iteration. They are easily established using mathematical induction; see, for example, [47, pp. 351-354]. Assume the notation of the algorithm listed in Figure 4. Theorem 4.1.
References-found: 40

