URL: http://www.cs.uni-bonn.de/III/lehre/seminare/Handlungsplanung/WS96/nebel1.ps.gz
Refering-URL: http://www.cs.uni-bonn.de/III/lehre/seminare/Handlungsplanung/WS96/
Root-URL: http://cs.uni-bonn.de
Email: e-mail: nebel@informatik.uni-ulm.de  
Title: Representation.  
Author: Bernhard Nebel and 
Date: November 1994  
Address: Stuhlsatzenhausweg 3, D-66123 Saarbrucken, Germany  D-89069 Ulm, Germany  
Affiliation: DFKI,  Fakultat fur Informatik, Universitat Ulm,  
Note: Artificial Intelligence: A Computational Perspective This is a draft version of a paper to appear in G. Brewka, ed., Essentials in Knowledge  This work was partially supported by the European Commission as part of DRUMS-II, the ESPRIT Basic Research Project P6156.  
Abstract: Although the computational perspective on cognitive tasks has always played a major role in Artificial Intelligence, the interest in the precise determination of the computational costs that are required for solving typical AI problems has grown only recently. In this paper, we will describe what insights a computational complexity analysis can provide and what methods are available to deal with the complexity problem. 
Abstract-found: 1
Intro-found: 1
Reference: [ Alchourron et al., 1985 ] <author> Carlos E. Alchourron, Peter Gardenfors, and David Makinson. </author> <title> On the logic of theory change: Partial meet contraction and revision functions. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 50(2) </volume> <pages> 510-530, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: One such problem is the problem of belief revision. This is the problem of changing a belief state, knowledge base, or database in order to accommodate new information that is possibly inconsistent with the belief state. This problem has been studied in philosophical logic <ref> [ Alchourron et al., 1985 ] </ref> , as well as in AI [ Ginsberg, 1986 ] and the field of logical databases [ Fagin et al., 1983 ] . <p> There are many different ways of revising a belief state rationally according to the rationality postulates for revision that have been developed by Alchourron, Makinson, and Gardenfors <ref> [ Alchourron et al., 1985 ] </ref> . In Computer Science, the preferred way seems to be to use a syntax-based revision scheme [ Nebel, 1991a ] that can be sketched as follows.
Reference: [ Allen and Koomen, 1983 ] <author> James F. Allen and Johannes A. Koomen. </author> <title> Planning using a temporal world model. </title> <booktitle> In Proceedings of the 8th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 741-747, </pages> <address> Karlsruhe, Ger-many, </address> <month> August </month> <year> 1983. </year>
Reference-contexts: &lt; Y + , X + &gt; Y , X + = Y + Table 1: The set B of the thirteen basic relations. plaining the performance of the system : : : " Consequently, for natural language understanding [ Allen, 1984; Song and Cohen, 1988 ] , general planning <ref> [ Allen, 1991; Allen and Koomen, 1983 ] </ref> , and presentation planning in a multimedia context [ Feiner et al., 1993 ] , the representation of qualitative temporal relations and reasoning about them is essential.
Reference: [ Allen, 1983 ] <author> James F. Allen. </author> <title> Maintaining knowledge about temporal intervals. </title> <journal> Communications of the ACM, </journal> <volume> 26(11) </volume> <pages> 832-843, </pages> <month> November </month> <year> 1983. </year>
Reference: [ Allen, 1984 ] <author> James F. Allen. </author> <title> Towards a general theory of action and time. </title> <journal> Artificial Intelligence, </journal> <volume> 23(2) </volume> <pages> 123-154, </pages> <year> 1984. </year>
Reference-contexts: Y + X equals Y j X = Y , X &lt; Y + , X + &gt; Y , X + = Y + Table 1: The set B of the thirteen basic relations. plaining the performance of the system : : : " Consequently, for natural language understanding <ref> [ Allen, 1984; Song and Cohen, 1988 ] </ref> , general planning [ Allen, 1991; Allen and Koomen, 1983 ] , and presentation planning in a multimedia context [ Feiner et al., 1993 ] , the representation of qualitative temporal relations and reasoning about them is essential.
Reference: [ Allen, 1991 ] <author> James F. Allen. </author> <title> Temporal reasoning and planning. </title> <editor> In James F. Allen, Henry A. Kautz, Richard N. Pelavin, and Josh D. Tenenberg, editors, </editor> <booktitle> Reasoning about Plans, chapter 1, </booktitle> <pages> pages 1-67. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: &lt; Y + , X + &gt; Y , X + = Y + Table 1: The set B of the thirteen basic relations. plaining the performance of the system : : : " Consequently, for natural language understanding [ Allen, 1984; Song and Cohen, 1988 ] , general planning <ref> [ Allen, 1991; Allen and Koomen, 1983 ] </ref> , and presentation planning in a multimedia context [ Feiner et al., 1993 ] , the representation of qualitative temporal relations and reasoning about them is essential.
Reference: [ Baader, 1990 ] <author> Franz Baader. </author> <title> Terminological cycles in KL-ONE-based knowledge representation languages. </title> <booktitle> In Proceedings of the 8th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 621-626, </pages> <address> Boston, MA, August 1990. </address> <publisher> MIT Press. </publisher> <pages> 29 </pages>
Reference-contexts: Furthermore, this is not an artifact of the algorithmic technique, but the problem of subsumption checking relative to a set of equations seems to be inherently difficult. While subsumption checking in 11 See <ref> [ Baader, 1990; Nebel, 1991b ] </ref> for an analysis of cyclic concept definitions. 24 FL is polynomial, as noted in Section 3.2, it becomes NP-hard if subsump--tion checking has to be done relative to a set of equations [ Nebel, 1990b ] .
Reference: [ Backstrom and Nebel, 1993 ] <author> Christer Backstrom and Bernhard Nebel. </author> <title> Complexity results for SAS + planning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1430-1435, </pages> <address> Chambery, France, </address> <month> August </month> <year> 1993. </year> <note> A long version of this paper appeared as Research Report LiTH-IDA-R-93-34, </note> <institution> Computer and Information Science, Linkoping University, Sweden. </institution>
Reference: [ Balcazar et al., 1988 ] <author> Jose Luis Balcazar, Josep Diaz, and Joaquim Ga-barro. </author> <title> Structural Complexity I. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1988. </year>
Reference: [ Balcazar et al., 1990 ] <author> Jose Luis Balcazar, Josep Diaz, and Joaquim Ga-barro. </author> <title> Structural Complexity II. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1990. </year>
Reference: [ Barton, 1986 ] <author> G. E. Barton. </author> <title> Computational complexity in two-level morphology. </title> <booktitle> In Proceedings of the 14th Annual Meeting of the ACL, </booktitle> <pages> pages 53-59, </pages> <address> New York, NY, </address> <year> 1986. </year>
Reference-contexts: In this case, it may turn out that (almost) all instances occurring in practice have this parameter bounded by a constant. As an example, let us consider the two-level morphology that has been used for word-form recognition in natural language processing [ Koskenniemi, 1983 ] . Barton <ref> [ Barton, 1986 ] </ref> showed that this approach is NP-complete in the number of morphological rules, which means that the processing time can grow exponentially with the number of rules.
Reference: [ Ben-Eliyahu and Dechter, 1991 ] <author> Rachel Ben-Eliyahu and Rina Dechter. </author> <title> Default logic, propositional logic and constraints. </title> <booktitle> In Proceedings of the 9th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 379-385, </pages> <address> Anaheim, CA, July 1991. </address> <publisher> MIT Press. </publisher>
Reference: [ Bergamaschi and Nebel, 1994 ] <author> Sonia Bergamaschi and Bernhard Nebel. </author> <title> Automatic building and validation of complex object database schemata supporting multiple inheritance. </title> <journal> Applied Intelligence, </journal> <volume> 4(2) </volume> <pages> 185-204, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The reason is that definitions occurring in practice are somehow well-structured. They always have only limited depth which leads to a reasonable runtime behavior [ Nebel, 1990b ] ; and this does not only hold for knowledge representation systems based on description logic but also for object-oriented database systems <ref> [ Bergamaschi and Nebel, 1994 ] </ref> . In both cases described above, we have the situation that a problem that has been classified as difficult is nevertheless efficiently solvable in practice.
Reference: [ Brachman and Levesque, 1984 ] <author> Ronald J. Brachman and Hector J. Leves-que. </author> <title> The tractability of subsumption in frame-based description languages. </title> <booktitle> In Proceedings of the 4th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 34-37, </pages> <address> Austin, TX, </address> <year> 1984. </year>
Reference-contexts: For instance, one of the first papers on the complexity of knowledge representation and reasoning <ref> [ Brachman and Levesque, 1984 ] </ref> contains a concise and elegant specification of the semantics for so-called description logics (also called terminological logics or concept languages) [ Brachman and Schmolze, 1985; Nebel and Smolka, 1991; Schmolze and Woods, 1992 ] .
Reference: [ Brachman and Schmolze, 1985 ] <author> Ronald J. Brachman and James G. Schmolze. </author> <title> An overview of the KL-ONE knowledge representation system. </title> <journal> Cognitive Science, </journal> <volume> 9(2) </volume> <pages> 171-216, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: One example for such a complete classification is the paper by Donini et al. [ 1991a ] , in which the complexity of the basic reasoning problem for different description logics is analyzed. Description logics, which have their roots in the knowledge representation formalism kl-one <ref> [ Brachman and Schmolze, 1985 ] </ref> , have been developed to support the representation of the conceptual and terminological part of Artificial Intelligence applications.
Reference: [ Bylander, 1991 ] <author> Tom Bylander. </author> <title> Tractability and Artificial Intelligence. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 3 </volume> <pages> 171-178, </pages> <year> 1991. </year>
Reference-contexts: In fact, coping with NP-hard problems can be viewed as one of the central themes of AI. Usually, in AI the complexity problem is tackled using a practical, experimental approach <ref> [ Bylander, 1991 ] </ref> and theoretical considerations come second. Analytical, complexity theoretic analyses can then "only" explain why a certain approach is successful or why another one is likely to fail.
Reference: [ Bylander, 1994 ] <author> Tom Bylander. </author> <title> The computational complexity of proposi-tional STRIPS planning. </title> <journal> Artificial Intelligence, </journal> <volume> 69(1-2):165-204, </volume> <year> 1994. </year>
Reference-contexts: For instance, as mentioned above, plan generation in a propositional strips framework appears to be a problem for which we have to make quite severe restrictions in order to achieve polyno-miality. The deeper reason for this seems to be that the general problem is PSPACE-complete <ref> [ Bylander, 1994 ] </ref> . Hence, the theoretical difference in complexity also seems to have practical consequences. There exist also problems that appear slightly harder than NP-complete 20 problems but easier than the PSPACE-complete problems. One such problem is the problem of belief revision.
Reference: [ Cadoli and Schaerf, 1991 ] <author> Marco Cadoli and Marco Schaerf. </author> <title> Approximate entailment. </title> <editor> In E. Ardizzone, S. Gaglio, and F. Sorbello, editors, </editor> <booktitle> Trends in Artificial Intelligence: Proc. of the 2nd Congress of the Italian Association for Artificial Intelligence, AI*IA, </booktitle> <pages> pages 68-77. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1991. </year>
Reference: [ Cadoli and Schaerf, 1992 ] <author> Marco Cadoli and Marco Schaerf. </author> <title> Approximation in concept description languages. </title> <editor> In B. Nebel, W. Swartout, and C. Rich, editors, </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the 3rd International Conference, </booktitle> <pages> pages 342-353, </pages> <address> Cambridge, MA, October 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Cadoli and Schaerf, 1993 ] <author> Marco Cadoli and Marco Schaerf. </author> <title> A survey of complexity results for non-monotonic logics, </title> <year> 1993. </year>
Reference: [ Chapman, 1987 ] <author> David Chapman. </author> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelligence, </journal> <volume> 32(3) </volume> <pages> 333-377, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Deciding the validity for partial-order plans consisting of unconditional actions can be shown to a be polynomial time problem [ Nebel and Backstrom, 1994 ] using the technique developed by Chapman <ref> [ Chapman, 1987 ] </ref> . Deciding the corresponding temporal projection problems is NP-hard, however. For this reason, Dean and Boddy's claim that temporal projection is a substantial subproblem of planning (or of plan validation) seems to be at least arguable.
Reference: [ Cheeseman et al., 1991 ] <author> Peter Cheeseman, Bob Kanefsky, and William M. Taylor. </author> <title> Where the really hard problems are. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 331-337, </pages> <address> Sydney, Australia, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: While the restrictions on the above identified parameters lead provably to polynomiality, it can also be the case that a problem parameter predicts the difficulty of a problem with high probability. Empirical investigations of the graph coloring problem <ref> [ Cheeseman et al., 1991 ] </ref> and of the satisfiability problem for constant clause length [ Mitchell et al., 1992 ] seem to suggest that certain problem parameters determine easy and hard problem ranges.
Reference: [ Dean and Boddy, 1987 ] <author> Thomas L. Dean and Mark Boddy. </author> <title> Incremental causal reasoning. </title> <booktitle> In Proceedings of the 6th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 196-201, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1987. </year>
Reference: [ Dean and Boddy, 1988 ] <author> Thomas L. Dean and Mark Boddy. </author> <title> Reasoning about partially ordered events. </title> <journal> Artificial Intelligence, </journal> <volume> 36(3) </volume> <pages> 375-400, </pages> <month> Oc-tober </month> <year> 1988. </year>
Reference: [ Donini et al., 1991a ] <author> Francesco M. Donini, Maurizio Lenzerini, Daniele Nardi, and Werner Nutt. </author> <title> Tractable concept languages. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 458-465, </pages> <address> Sydney, Australia, </address> <month> August </month> <year> 1991. </year> <note> Morgan Kaufmann. 31 </note>
Reference: [ Donini et al., 1991b ] <author> Francesco M. Donini, Maurizio Lenzerini, Daniele Nardi, and Werner Nutt. </author> <title> The complexity of concept languages. </title> <editor> In J. A. Allen, R. Fikes, and E. Sandewall, editors, </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the 2nd International Conference, </booktitle> <pages> pages 151-162, </pages> <address> Cambridge, MA, April 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Donini et al., 1992 ] <author> Francesco M. Donini, Maurizio Lenzerini, Daniele Nardi, Bernhard Hollunder, Werner Nutt, and Alberto Marchetti Spaca-mella. </author> <title> The complexity of existential quantification in concept languages. </title> <journal> Artificial Intelligence, </journal> <volume> 53(2-3):309-327, </volume> <year> 1992. </year>
Reference: [ Doyle and Patil, 1991 ] <author> Jon Doyle and Ramesh S. Patil. </author> <title> Two theses of knowledge representation: Language restrictions, taxonomic classification, and the utility of representation services. </title> <journal> Artificial Intelligence, </journal> <volume> 48(3) </volume> <pages> 261-298, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Based on their observation that a particular description-forming object can make subsumption NP-hard, they propose to restrict the expressiveness of representation languages as one way to achieve guaranteed efficiency. This proposal, which does not seem to be unreasonable from a technical point of view, led to severe criticism, however <ref> [ Doyle and Patil, 1991 ] </ref> . The main point of this criticisms was that restricting the representation language leads in most cases to the fact that the representation language becomes useless. While this is true in the general case, one should consider two points.
Reference: [ Eiter and Gottlob, 1992 ] <author> Thomas Eiter and Georg Gottlob. </author> <title> On the complexity of propositional knowledge base revision, updates, </title> <journal> and counterfac-tuals. Artificial Intelligence, </journal> <volume> 57 </volume> <pages> 227-270, </pages> <year> 1992. </year>
Reference-contexts: Secondly, there is the problem of quantifying over the potentially exponentially many maximal consistent subsets of the belief state. This implies that in order to achieve polynomi-ality, both sources of complexity have to be eliminated. It is not enough to restrict the base logic to, say, propositional Horn logic <ref> [ Eiter and Gottlob, 1992 ] </ref> , but one also has to restrict the number of maximal, consistent subsets [ Nebel, 1994 ] . Interestingly, a number of related reasoning problems can be classified as belonging to the same complexity class.
Reference: [ Eiter and Gottlob, 1993a ] <author> Thomas Eiter and Georg Gottlob. </author> <title> The complexity of logic-based abduction. </title> <editor> In P. Enjalbert, A. Finkel, and K. W. Wagner, editors, </editor> <booktitle> Proceedings Tenth Symposium on Theoretical Aspects of Computing STACS-93, </booktitle> <pages> pages 70-79, </pages> <address> Wurzburg, Germany, </address> <month> February </month> <year> 1993. </year> <note> Springer-Verlag. Extended version to appear in JACM. </note>
Reference-contexts: For instance the cautious reasoning problems in nonmonotonic logics, such as default logic, autoepistemic logic, and circumscription are coNP NP -complete [ Gottlob, 1992; Stillman, 1992; Eiter and Gottlob, 1993b ] , as well as a number of abduction problems <ref> [ Eiter and Gottlob, 1993a ] </ref> . 10 This implies that all these problems are equivalent on an abstract algorithmic level and that all these problems can be more or less directly translated into each other (e.g. [ Gottlob, 1994 ] ).
Reference: [ Eiter and Gottlob, 1993b ] <author> Thomas Eiter and Georg Gottlob. </author> <title> Propositional circumscription and extended closed world reasoning are p 2 -complete. </title> <journal> Theoretical Computer Science, </journal> <volume> 114(2) </volume> <pages> 231-245, </pages> <year> 1993. </year> <note> Addendum 118:315. </note>
Reference-contexts: Interestingly, a number of related reasoning problems can be classified as belonging to the same complexity class. For instance the cautious reasoning problems in nonmonotonic logics, such as default logic, autoepistemic logic, and circumscription are coNP NP -complete <ref> [ Gottlob, 1992; Stillman, 1992; Eiter and Gottlob, 1993b ] </ref> , as well as a number of abduction problems [ Eiter and Gottlob, 1993a ] . 10 This implies that all these problems are equivalent on an abstract algorithmic level and that all these problems can be more or less directly
Reference: [ Enzinger et al., 1994 ] <author> Andrea Enzinger, Frank Puppe, and Gerhard Strube. </author> <title> Problemlosen ohne Suchen? KI, </title> 1/94:73-81, 1994. 
Reference: [ Fagin et al., 1983 ] <author> Ronald Fagin, Jeffrey D. Ullman, and Moshe Y. Vardi. </author> <title> On the semantics of updates in databases. </title> <booktitle> In 2nd ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, </booktitle> <pages> pages 352-365, </pages> <address> Atlanta, Ga., </address> <year> 1983. </year>
Reference-contexts: This problem has been studied in philosophical logic [ Alchourron et al., 1985 ] , as well as in AI [ Ginsberg, 1986 ] and the field of logical databases <ref> [ Fagin et al., 1983 ] </ref> . There are many different ways of revising a belief state rationally according to the rationality postulates for revision that have been developed by Alchourron, Makinson, and Gardenfors [ Alchourron et al., 1985 ] .
Reference: [ Fahlman, 1979 ] <author> Scott E. Fahlman. </author> <title> A System for Representing and Using Real-World Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1979. </year> <month> 32 </month>
Reference-contexts: Inheritance networks are composed out of nodes denoting individuals, concepts, or properties, and positive and negative links between nodes standing for "X is typically Y ," symbolically X ! Y , and "X is typically not a Y ," symbolically X 6! Y <ref> [ Fahlman, 1979; Touretzky, 1986 ] </ref> . A typical example of an inheritance network is given in From the network in Figure 1 we may conclude, for instance, that Clyde is a member of the class of royal elephants, which are typically elephants, which are in turn typically grey.
Reference: [ Feiner et al., 1993 ] <author> Steven K. Feiner, Diane J. Litman, Kathleen R. McKeown, and Rebecca J. Passonneau. </author> <title> Towards coordinated temporal multimedia presentation. </title> <editor> In M. Maybury, editor, </editor> <title> Intelligent Multi Media. </title> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1993. </year> <month> Forthcoming. </month>
Reference-contexts: The set B of the thirteen basic relations. plaining the performance of the system : : : " Consequently, for natural language understanding [ Allen, 1984; Song and Cohen, 1988 ] , general planning [ Allen, 1991; Allen and Koomen, 1983 ] , and presentation planning in a multimedia context <ref> [ Feiner et al., 1993 ] </ref> , the representation of qualitative temporal relations and reasoning about them is essential. Allen [ 1983 ] introduces a calculus of binary relations on intervals for representing qualitative temporal information and addresses the problem of reasoning about such information.
Reference: [ Fikes and Nilsson, 1971 ] <author> Richard E. Fikes and Nils Nilsson. </author> <title> STRIPS: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: Finally, Donini et al. [ 1991a ] were able to provide an exhaustive classification of all description logics that contain FL as a sublanguage. Similar classifications were carried out in the area of plan generation. By-lander [ 1994 ] analyzed the computational complexity of propositional strips planning <ref> [ Fikes and Nilsson, 1971 ] </ref> according to different restrictions on operators and Backstrom and Nebel [ 1993 ] did a similar analysis for a slightly more expressive planning formalism taking local and global restrictions into account. <p> temporal projection problem is the problem of determining for a given initial state and a given partially ordered set of actions the conditions that hold necessarily (i.e., in all linearizations) before a given event. 9 This problem is analyzed by Dean and Boddy in the context of propositional strips planning <ref> [ Fikes and Nilsson, 1971 ] </ref> . Formally, Dean and Boddy [ 1988 ] define a (propositional) state S to be a set of propositional atoms.
Reference: [ Garey and Johnson, 1979 ] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability|A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman, </publisher> <address> San Francisco, CA, </address> <year> 1979. </year>
Reference: [ Gent and Walsh, 1994 ] <author> Ian P. Gent and Toby Walsh. </author> <title> The hardest random sat problems. </title> <editor> In B. Nebel and L. Dreschler-Fischer, editors, </editor> <booktitle> KI-94: Advances in Artificial Intelligence, </booktitle> <pages> pages 355-366, </pages> <address> Saarucken, Germany, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference: [ Ginsberg, 1986 ] <author> Matthew L. Ginsberg. </author> <title> Counterfactuals. </title> <journal> Artificial Intelligence, </journal> <volume> 30(1) </volume> <pages> 35-79, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: This is the problem of changing a belief state, knowledge base, or database in order to accommodate new information that is possibly inconsistent with the belief state. This problem has been studied in philosophical logic [ Alchourron et al., 1985 ] , as well as in AI <ref> [ Ginsberg, 1986 ] </ref> and the field of logical databases [ Fagin et al., 1983 ] .
Reference: [ Golumbic and Shamir, 1992 ] <author> Martin C. Golumbic and Ron Shamir. </author> <title> Algorithms and complexity for reasoning about time. </title> <booktitle> In Proceedings of the 10th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 741-747. </pages> <publisher> MIT Press, </publisher> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year> <note> A long version will appear in JACM. </note>
Reference-contexts: The fundamental reasoning problem in this framework is the problem of deciding the satisfiability of a set of interval formulae, also called ISAT in the following. This problem is fundamental because all other interesting problems can be polynomially reduced to it <ref> [ Golumbic and Shamir, 1992 ] </ref> . Unfortunately however, ISAT is NP-hard, as has been shown by Vilain and Kautz [ 1986 ] .
Reference: [ Gottlob, 1992 ] <author> Georg Gottlob. </author> <title> Complexity results for nonmonotonic logics. </title> <journal> Journal for Logic and Computation, </journal> <volume> 2(3), </volume> <year> 1992. </year>
Reference-contexts: Interestingly, a number of related reasoning problems can be classified as belonging to the same complexity class. For instance the cautious reasoning problems in nonmonotonic logics, such as default logic, autoepistemic logic, and circumscription are coNP NP -complete <ref> [ Gottlob, 1992; Stillman, 1992; Eiter and Gottlob, 1993b ] </ref> , as well as a number of abduction problems [ Eiter and Gottlob, 1993a ] . 10 This implies that all these problems are equivalent on an abstract algorithmic level and that all these problems can be more or less directly <p> Furthermore, these results also enable us to check the plausibility of certain conjectures. Ben-Eliyahu and Dechter [ 1991 ] conjectured, for example, that their (polynomial-time) method of translating certain restricted default theories into classical propositional logic could also be applied to general default theories. As pointed out by Gottlob <ref> [ Gottlob, 1992 ] </ref> , this seems rather unlikely since it would mean that there exist a method of reducing a coNP NP - complete problem to a coNP-complete problem implying that the two classes are identical, which is believed to be quite unlikely [ Johnson, 1990 ] . 10 Cadoli
Reference: [ Gottlob, 1994 ] <author> Georg Gottlob. </author> <title> The power of beliefs or translating default logic into standard autoepistemic logic. </title> <editor> In Gerhard Lakemeyer and Bern-hard Nebel, editors, </editor> <booktitle> Foundations of Knowledge Representation, volume 810 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidel-berg, New York, </address> <year> 1994. </year> <note> Also appeared in Proc. 13th IJCAI. </note>
Reference-contexts: and Gottlob, 1993b ] , as well as a number of abduction problems [ Eiter and Gottlob, 1993a ] . 10 This implies that all these problems are equivalent on an abstract algorithmic level and that all these problems can be more or less directly translated into each other (e.g. <ref> [ Gottlob, 1994 ] </ref> ). Furthermore, these results also enable us to check the plausibility of certain conjectures. Ben-Eliyahu and Dechter [ 1991 ] conjectured, for example, that their (polynomial-time) method of translating certain restricted default theories into classical propositional logic could also be applied to general default theories.
Reference: [ Gupta and Nau, 1992 ] <author> Naresh Gupta and Dana S. Nau. </author> <title> On the complexity of blocks-world planning. </title> <journal> Artificial Intelligence, </journal> <volume> 56(2) </volume> <pages> 223-254, </pages> <year> 1992. </year> <month> 33 </month>
Reference-contexts: Such an observation is interesting only if there are different opinions about the difficulty, as in the case of the blocks-world planning problem <ref> [ Gupta and Nau, 1992 ] </ref> , or if it is claimed that a problem can be solved efficiently [ Nebel, 1988 ] . Further, an NP-hardness result alone does not provide many insights about the problem. <p> In particular, it is often possible to give up on the optimality of solutions. For example, generating arbitrary plans in the blocks world is polynomial while requiring optimal plans leads to NP-hardness <ref> [ Gupta and Nau, 1992 ] </ref> . However, often AI problems cannot be weakened by giving up on optimality. In particular logic-based problems such as subsumption checking, diagnosis, or temporal projection are not optimization problems and it seems to be difficult to define what an approximation to a solution is.
Reference: [ Heinsohn et al., 1994 ] <author> Jochen Heinsohn, Daniel Kudenko, Bernhard Nebel, and Hans-Jurgen Profitlich. </author> <title> An empirical analysis of terminological representation systems. </title> <journal> Artificial Intelligence, </journal> <volume> 68(2) </volume> <pages> 367-397, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: In most papers on the complexity of this problem, it is assumed that the interpretation of atomic concepts is unconstrained. However, in all existing knowledge representation systems that use description logics it is possible to define the meaning of an atomic concept <ref> [ Heinsohn et al., 1994 ] </ref> . <p> In fact, an empirical analysis of different description logic systems <ref> [ Heinsohn et al., 1994 ] </ref> confirmed that the structure of the knowledge base can have indeed a severe influence on the runtime behavior.
Reference: [ Hollunder et al., 1990 ] <author> Bernhard Hollunder, Werner Nutt, and Manfred Schmidt-Schau. </author> <title> Subsumption algorithms for concept description languages. </title> <booktitle> In Proceedings of the 9th European Conference on Artificial Intelligence, </booktitle> <pages> pages 348-353, </pages> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1990. </year> <pages> Pitman. </pages>
Reference: [ Horty et al., 1987 ] <author> John F. Horty, Richmond H. Thomason, and David S. Touretzky. </author> <title> A skeptical theory of inheritance in nonmonotonic semantic networks. </title> <booktitle> In Proceedings of the 6th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 358-363, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1987. </year>
Reference-contexts: As shown by Selman and Levesque [ 1993 ] , one source of complexity is the way how argument chains are concatenated. In the case of Touretzky's coupled concatenation, it is NP-hard to generate a conclusion set. If one uses decoupled inheritance <ref> [ Horty et al., 1987 ] </ref> , however, conclusion sets can be generated in polynomial time. Unfortunately, this positive result does not carry over to ideally skeptical and credulous reasoning, because in this case all extensions must be considered|and there may be exponentially many extensions.
Reference: [ Johnson, 1990 ] <author> David S. Johnson. </author> <title> A catalog of complexity classes. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Vol. A, </booktitle> <pages> pages 67-161. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: As pointed out by Gottlob [ Gottlob, 1992 ] , this seems rather unlikely since it would mean that there exist a method of reducing a coNP NP - complete problem to a coNP-complete problem implying that the two classes are identical, which is believed to be quite unlikely <ref> [ Johnson, 1990 ] </ref> . 10 Cadoli and Schaerf [ 1993 ] gave an extensive survey of complexity results for nonmo-notonic propositional logics. 22 4 Coping with NP-hardness Having shown that a problem is NP-hard does not imply that it is impossible to tackle this problem computationally.
Reference: [ Kambhampati and Hendler, 1992 ] <author> Subbarao Kambhampati and James A. Hendler. </author> <title> A validation-structure-based theory of plan modification and reuse. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 193-258, </pages> <year> 1992. </year>
Reference-contexts: In fact, if we take the suggestion from the literature seriously that the new solution should be generated by minimally modifying the old solution <ref> [ Kambhampati and Hendler, 1992 ] </ref> , there are cases when it is easier to generate a plan from scratch.
Reference: [ Kasif and Delcher, 1994 ] <author> Simon Kasif and Arthur L. Delcher. </author> <title> Local consistency in parallel constraint networks. </title> <journal> Artificial Intelligence, </journal> <volume> 69(1-2):307-327, </volume> <year> 1994. </year>
Reference-contexts: This is a topic, however, that has not been extensively studied in AI (but cf. <ref> [ Kasif, 1990; Kasif and Delcher, 1994 ] </ref> ).
Reference: [ Kasif, 1990 ] <author> Simon Kasif. </author> <title> On the parallel complexity of discrete relaxation in constraint networks. </title> <journal> Artificial Intelligence, </journal> <volume> 45(3) </volume> <pages> 275-286, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: This is a topic, however, that has not been extensively studied in AI (but cf. <ref> [ Kasif, 1990; Kasif and Delcher, 1994 ] </ref> ).
Reference: [ Kautz and Selman, 1994 ] <author> Henry Kautz and Bart Selman. </author> <title> An empirical evaluation of knowledge compilation. </title> <booktitle> In Proceedings of the 12th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 155-161, </pages> <address> Seattle, WA, July 1994. </address> <publisher> MIT Press. </publisher>
Reference-contexts: In this approach, it is assumed, however, that the approximating theories are computed "off-line" since their computation can be quite involved. First experiments using this approach show that this might be indeed an interesting way to go <ref> [ Kautz and Selman, 1994 ] </ref> . While the above approaches provide us with incompleteness in a principled way that can be characterized declaratively, there is also a way of approximating the solution of a problem by methods that seem to work empirically well|without giving any guarantees at all.
Reference: [ Koskenniemi and Church, 1988 ] <author> Kimmo Koskenniemi and Kenneth Ward Church. </author> <title> Complexity, two-level morphology and Finish. </title> <booktitle> In Proceedings of the 12th International Conference on Computational Linguistics, </booktitle> <pages> pages 335-340, </pages> <address> Budapest, Hungary, </address> <year> 1988. </year> <month> 34 </month>
Reference-contexts: A closer look reveals, however, that only a certain kind of rules, rules that realize co-called harmony processes, are responsible for the NP-completeness result. Furthermore, for all known languages it happens that only a very limited number of these 23 rules is necessary, namely at most two <ref> [ Koskenniemi and Church, 1988 ] </ref> . In other words, for all practical cases, word-form recognition using two-level morphology is computationally feasible. As a second example, we will consider subsumption checking in description logics.
Reference: [ Koskenniemi, 1983 ] <author> Kimmo Koskenniemi. </author> <title> Two-level morphology: A general computational model for word-form recognition and production. </title> <type> Technical Report No. 11, </type> <institution> University of Helsinki, Department of General Linguistics, Helsinki, Finland, </institution> <year> 1983. </year>
Reference-contexts: In this case, it may turn out that (almost) all instances occurring in practice have this parameter bounded by a constant. As an example, let us consider the two-level morphology that has been used for word-form recognition in natural language processing <ref> [ Koskenniemi, 1983 ] </ref> . Barton [ Barton, 1986 ] showed that this approach is NP-complete in the number of morphological rules, which means that the processing time can grow exponentially with the number of rules.
Reference: [ Lakemeyer, 1987 ] <author> G. Lakemeyer. </author> <title> Tractable meta-reasoning in propositional logics of belief. </title> <editor> In J. McDermott, editor, </editor> <booktitle> Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 402-408, </pages> <address> Milan, Italy, August 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, Patel-Schneider [ 1989a ] uses a four--valued semantics for characterizing a subsumption relation that is polynomial but weaker than the standard one. Levesque and Lakemeyer use a similar technique to weaken the general logical implication for propositional belief logics <ref> [ Levesque, 1984; Lakemeyer, 1987 ] </ref> . Other approaches try to approximate the right answer from above and below by using correct but incomplete and incorrect but complete methods.
Reference: [ Levesque and Brachman, 1987 ] <author> Hector J. Levesque and Ronald J. Brach-man. </author> <title> Expressiveness and tractability in knowledge representation and reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 3 </volume> <pages> 78-93, </pages> <year> 1987. </year>
Reference-contexts: In both cases, polynomiality is achieved only for very severe restrictions on the planning formalism, however. 8 The proof appears in <ref> [ Levesque and Brachman, 1987 ] </ref> . 13 In both cases described above, it was possible to analyze the complexity of the subproblems manually. Sometimes, the space of subproblems may be so large, however, that a manual analysis is impossible.
Reference: [ Levesque, 1984 ] <author> Hector J. Levesque. </author> <title> A logic of implicit and explicit belief. </title> <booktitle> In Proceedings of the 4th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 198-202, </pages> <address> Austin, TX, </address> <year> 1984. </year>
Reference-contexts: For example, Patel-Schneider [ 1989a ] uses a four--valued semantics for characterizing a subsumption relation that is polynomial but weaker than the standard one. Levesque and Lakemeyer use a similar technique to weaken the general logical implication for propositional belief logics <ref> [ Levesque, 1984; Lakemeyer, 1987 ] </ref> . Other approaches try to approximate the right answer from above and below by using correct but incomplete and incorrect but complete methods.
Reference: [ Levesque, 1986 ] <author> Hector J. Levesque. </author> <title> Making believers out of computers. </title> <journal> Artificial Intelligence, </journal> <volume> 30(1) </volume> <pages> 81-108, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: They postulate that a knowledge representation system should have a status similar to a database system in conventional Computer Science applications. It should give a correct answer in a time bound that is predictable and reasonable <ref> [ Levesque, 1986 ] </ref> . Based on their observation that a particular description-forming object can make subsumption NP-hard, they propose to restrict the expressiveness of representation languages as one way to achieve guaranteed efficiency.
Reference: [ Mitchell et al., 1992 ] <author> David Mitchell, Bart Selman, and Hector Levesque. </author> <title> Hard and easy distributions of SAT problems. </title> <booktitle> In Proceedings of the 10th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 459-465, </pages> <address> San Jose, CA, July 1992. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Empirical investigations of the graph coloring problem [ Cheeseman et al., 1991 ] and of the satisfiability problem for constant clause length <ref> [ Mitchell et al., 1992 ] </ref> seem to suggest that certain problem parameters determine easy and hard problem ranges. Basically, for one range of parameter values the problem is underconstrained, in which case it is easily solvable by almost all methods.
Reference: [ Nebel and Backstrom, 1994 ] <author> Bernhard Nebel and Christer Backstrom. </author> <title> On the computational complexity of temporal projection, planning, and plan validation. </title> <journal> Artificial Intelligence, </journal> <volume> 66(1) </volume> <pages> 125-160, </pages> <year> 1994. </year>
Reference-contexts: However, even in this case, temporal projection remains NP-hard <ref> [ Nebel and Backstrom, 1994 ] </ref> . Interestingly, if we apply the same restrictions to the plan generation problem|which is an NP-hard problem in the general case|it becomes polynomial. For this reason, there is the question whether temporal projection is indeed a subproblem of planning. <p> While this sounds like a clever divide and conquer strategy, it turns out to be the opposite if one takes a computational point of view. Deciding the validity for partial-order plans consisting of unconditional actions can be shown to a be polynomial time problem <ref> [ Nebel and Backstrom, 1994 ] </ref> using the technique developed by Chapman [ Chapman, 1987 ] . Deciding the corresponding temporal projection problems is NP-hard, however.
Reference: [ Nebel and Burckert, 1994 ] <author> Bernhard Nebel and Hans-Jurgen Burckert. </author> <title> Reasoning about temporal relations: A maximal tractable subclass of Allen's interval algebra. </title> <booktitle> In Proceedings of the 12th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 356-361, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year> <note> MIT Press. Extended version to appear in JACM. </note>
Reference-contexts: By analyzing the structure of the space of subproblems and using a computer-based case analysis, it was nevertheless possible to identify the precise boundary between polynomial and NP-hard subproblems also in this case <ref> [ Nebel and Burckert, 1994 ] </ref> . 3.3 Differential Analyses Often a reasoning problem is only informally specified and there exist different formalizations, which may lead to different complexity classes for the formalized reasoning problems.
Reference: [ Nebel and Koehler, 1995 ] <author> Bernhard Nebel and Jana Koehler. </author> <title> Plan reuse versus plan generation: A theoretical and empirical analysis. </title> <journal> Artificial Intelligence, </journal> <note> 1995. To appear. A preliminary version is available as DFKI Research Report RR-93-33. 35 </note>
Reference-contexts: Not very surprisingly, reusing old solutions never leads to a provable speed up in terms of computational complexity, even if we assume that the new situation differs only minimally from the old situation we already have a solution for <ref> [ Nebel and Koehler, 1995 ] </ref> . <p> Even worse, finding a solution in a case library that is appropriate to be used as the starting point for generating a new solution can already be computationally expensive <ref> [ Nebel and Koehler, 1995 ] </ref> . While all these results concern "only" the worst-case complexity, empirical tests of different plan reuse techniques also seem to suggest that it is not obvious that plan reuse techniques pay off under all circumstances [ Nebel and Koehler, 1995 ] . 19 3.5 Classifying <p> generating a new solution can already be computationally expensive <ref> [ Nebel and Koehler, 1995 ] </ref> . While all these results concern "only" the worst-case complexity, empirical tests of different plan reuse techniques also seem to suggest that it is not obvious that plan reuse techniques pay off under all circumstances [ Nebel and Koehler, 1995 ] . 19 3.5 Classifying NP-hard Problems Although the classification "polynomial vs. NP-hard" seems to be sufficient in most cases, it is sometimes worthwhile to determine the precise complexity of a problem.
Reference: [ Nebel and Smolka, 1991 ] <author> Bernhard Nebel and Gert Smolka. </author> <title> Attributive de-scription formalisms : : : and the rest of the world. </title> <editor> In Otthein Herzog and Claus-Rainer Rollinger, editors, </editor> <booktitle> Text Understanding in LILOG, volume 546 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 439-452. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1991. </year>
Reference: [ Nebel, 1988 ] <author> Bernhard Nebel. </author> <title> Computational complexity of terminological reasoning in BACK. </title> <journal> Artificial Intelligence, </journal> <volume> 34(3) </volume> <pages> 371-383, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Such an observation is interesting only if there are different opinions about the difficulty, as in the case of the blocks-world planning problem [ Gupta and Nau, 1992 ] , or if it is claimed that a problem can be solved efficiently <ref> [ Nebel, 1988 ] </ref> . Further, an NP-hardness result alone does not provide many insights about the problem.
Reference: [ Nebel, 1990a ] <author> Bernhard Nebel. </author> <title> Reasoning and Revision in Hybrid Representation Systems, </title> <booktitle> volume 422 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1990. </year>
Reference-contexts: Note that one can think of quite different terminological logics employing, for instance, different role-forming operators, cardinality restrictions on roles, concept disjunction, concept negation and so on. Indeed, quite a number of different representation systems have been built using a variety of terminological logics (for a survey, see <ref> [ Nebel, 1990a ] </ref> ). Brachman and Levesque started with the description logic specified above and showed that the subsumption problem can be solved in polynomial time.
Reference: [ Nebel, 1990b ] <author> Bernhard Nebel. </author> <title> Terminological reasoning is inherently intractable. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 235-249, </pages> <year> 1990. </year>
Reference-contexts: One only has to replace each defined atomic concept by its definition. While this reduction appears to be conceptually easy, it turns out that it may imply considerable computational costs. In the worst case, it can happen that the expansion process leads to an exponentially larger concept description <ref> [ Nebel, 1990b ] </ref> . Furthermore, this is not an artifact of the algorithmic technique, but the problem of subsumption checking relative to a set of equations seems to be inherently difficult. <p> While subsumption checking in 11 See [ Baader, 1990; Nebel, 1991b ] for an analysis of cyclic concept definitions. 24 FL is polynomial, as noted in Section 3.2, it becomes NP-hard if subsump--tion checking has to be done relative to a set of equations <ref> [ Nebel, 1990b ] </ref> . Interestingly, in all practically occurring situations the worst case never seems to happen. The reason is that definitions occurring in practice are somehow well-structured. They always have only limited depth which leads to a reasonable runtime behavior [ Nebel, 1990b ] ; and this does not <p> be done relative to a set of equations <ref> [ Nebel, 1990b ] </ref> . Interestingly, in all practically occurring situations the worst case never seems to happen. The reason is that definitions occurring in practice are somehow well-structured. They always have only limited depth which leads to a reasonable runtime behavior [ Nebel, 1990b ] ; and this does not only hold for knowledge representation systems based on description logic but also for object-oriented database systems [ Bergamaschi and Nebel, 1994 ] .
Reference: [ Nebel, 1991a ] <author> Bernhard Nebel. </author> <title> Belief revision and default reasoning: Syntax-based approaches. </title> <editor> In J. A. Allen, R. Fikes, and E. Sandewall, editors, </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the 2nd International Conference, </booktitle> <pages> pages 417-428, </pages> <address> Cambridge, MA, April 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There are many different ways of revising a belief state rationally according to the rationality postulates for revision that have been developed by Alchourron, Makinson, and Gardenfors [ Alchourron et al., 1985 ] . In Computer Science, the preferred way seems to be to use a syntax-based revision scheme <ref> [ Nebel, 1991a ] </ref> that can be sketched as follows. Given a belief state K represented by a finite set of propositions and a new proposition ', the revised belief state, symbolically K + ', is the disjunction of all maximal subsets of K consistent with ' plus ' itself. <p> In fact, the problem turns out to be complete for a complexity class that lies between NP and PSPACE, namely the class coNP NP (also denoted by p 2 ) <ref> [ Nebel, 1991a; Eiter and Gottlob, 21 1992 ] </ref> .
Reference: [ Nebel, 1991b ] <author> Bernhard Nebel. </author> <title> Terminological cycles: Semantics and computational properties. </title> <editor> In John F. Sowa, editor, </editor> <booktitle> Principles of Semantic Networks, </booktitle> <pages> pages 331-362. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Furthermore, this is not an artifact of the algorithmic technique, but the problem of subsumption checking relative to a set of equations seems to be inherently difficult. While subsumption checking in 11 See <ref> [ Baader, 1990; Nebel, 1991b ] </ref> for an analysis of cyclic concept definitions. 24 FL is polynomial, as noted in Section 3.2, it becomes NP-hard if subsump--tion checking has to be done relative to a set of equations [ Nebel, 1990b ] .
Reference: [ Nebel, 1994 ] <author> Bernhard Nebel. </author> <title> Base revision operations and schemes: Semantics, representation, and complexity. </title> <booktitle> In Proceedings of the 11th Eu-ropean Conference on Artificial Intelligence, </booktitle> <pages> pages 341-345, </pages> <address> Amsterdam, The Netherlands, </address> <month> August </month> <year> 1994. </year> <pages> Wiley. </pages>
Reference-contexts: This implies that in order to achieve polynomi-ality, both sources of complexity have to be eliminated. It is not enough to restrict the base logic to, say, propositional Horn logic [ Eiter and Gottlob, 1992 ] , but one also has to restrict the number of maximal, consistent subsets <ref> [ Nebel, 1994 ] </ref> . Interestingly, a number of related reasoning problems can be classified as belonging to the same complexity class.
Reference: [ Nokel, 1991 ] <author> Klaus Nokel. </author> <title> Temporally Distributed Symptoms in Technical Diagnosis, </title> <booktitle> volume 517 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1991. </year>
Reference-contexts: sg, which can be characterized by the conjunction (X &lt; X + ) ^ (Y &lt; Y + ) ^ (X &lt; Y + ) ^ (Y &lt; X + ) ^ (X + &lt; Y + ) The same subset of relations is sufficient for diagnosis of technical artifacts <ref> [ Nokel, 1991 ] </ref> . Interestingly, the reasoning problems for the restricted set of relations are efficiently solvable [ Vilain et al., 1989; van Beek and Cohen, 1990 ] .
Reference: [ Orponen et al., 1986 ] <author> P. Orponen, D. A. Russo, and U. Schoning. </author> <title> Optimal approximations and polynomially levelable sets. </title> <journal> SIAM Journal on Computing, </journal> <volume> 15(2) </volume> <pages> 399-408, </pages> <year> 1986. </year> <month> 36 </month>
Reference-contexts: From concept and role 6 One should note that it is in general impossible to identify a greatest polynomial sub-problem of an NP-complete problem (provided P 6= NP) <ref> [ Orponen et al., 1986 ] </ref> .
Reference: [ Papadimitriou, 1994 ] <author> Christos H. Papadimitriou. </author> <title> Computational Comple--xity. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1994. </year>
Reference: [ Patel-Schneider, 1989a ] <author> Peter F. Patel-Schneider. </author> <title> A four-valued semantics for terminological logics. </title> <journal> Artificial Intelligence, </journal> <volume> 38(3) </volume> <pages> 319-351, </pages> <month> April </month> <year> 1989. </year>
Reference: [ Patel-Schneider, 1989b ] <author> Peter F. Patel-Schneider. </author> <title> Undecidability of sub-sumption in NIKL. </title> <journal> Artificial Intelligence, </journal> <volume> 39(2) </volume> <pages> 263-272, </pages> <month> June </month> <year> 1989. </year>
Reference: [ Reiter and Mackworth, 1989 ] <author> Ray Reiter and Alan K. Mackworth. </author> <title> A logical framework for depiction and image interpretation. </title> <journal> Artificial Intelligence, </journal> <volume> 41 </volume> <pages> 125-155, </pages> <year> 1989. </year>
Reference-contexts: Another problem that is NP-hard in general but seems to be easily solvable in practice is the problem of interpreting "sketch maps" as they can be derived from aerial photographs <ref> [ Reiter and Mackworth, 1989 ] </ref> . Interpretation means here to assign a meaning such as "shore," "river," "street," etc. 10 to the lines on the sketch map.
Reference: [ Reiter, 1987 ] <author> Raymond Reiter. </author> <title> A theory of diagnosis from first principles. </title> <journal> Artificial Intelligence, </journal> <volume> 32(1) </volume> <pages> 57-95, </pages> <month> April </month> <year> 1987. </year>
Reference: [ Rich and Knight, 1991 ] <author> Elaine Rich and Kevin Knight. </author> <booktitle> Artificial Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, NY, 2nd edition edition, </address> <year> 1991. </year>
Reference-contexts: It provides us with insights about the possible sources of complexity and can be used to direct the search for efficient methods to solve the problem, perhaps giving up on generality or the quality of the solution. In an AI textbook <ref> [ Rich and Knight, 1991, p. 44 ] </ref> , AI is defined as the study of techniques for solving exponentially hard problems in polynomial time by exploiting knowledge about the problem domain.
Reference: [ Riesbeck and Schank, 1989 ] <author> Christopher K. Riesbeck and Roger C. Schank. </author> <title> Inside Case-Based Reasoning. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1989. </year>
Reference-contexts: Enzinger et al [ 1994 ] , for instance, consider a number of AI problems, which are formulated as NP-hard search problems, and contrast them with 27 simplifications and reformulations based on arguments from cognitive science. For example, they compare case-based diagnosis (see <ref> [ Riesbeck and Schank, 1989 ] </ref> ) with consistency-based diagnosis as formulated by Reiter [ 1987 ] and conclude that the former is much better behaved from a computational complexity point of view. However, one should note that the application areas of the two approaches are very different.
Reference: [ Schild, 1991 ] <author> Klaus Schild. </author> <title> A correspondence theory for terminological logics: Preliminary report. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 466-471, </pages> <address> Sydney, Australia, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Schmidt-Schau and Smolka, 1991 ] <author> Manfred Schmidt-Schau and Gert Smolka. </author> <title> Attributive concept descriptions with complements. </title> <journal> Artificial Intelligence, </journal> <volume> 48 </volume> <pages> 1-26, </pages> <year> 1991. </year>
Reference: [ Schmidt-Schau, 1989 ] <author> Manfred Schmidt-Schau. </author> <title> Subsumption in KL-ONE is undecidable. </title> <editor> In R. Brachman, H. J. Levesque, and R. Reiter, editors, </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the 1st International Conference, </booktitle> <pages> pages 421-431, </pages> <address> Toronto, ON, May 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Schmolze and Woods, 1992 ] <author> James G. Schmolze and William A. Woods. </author> <title> The KL-ONE family. </title> <editor> In F. Lehmann, editor, </editor> <booktitle> Semantic Networks in Artificial Intelligence. </booktitle> <publisher> Pergamon Press, </publisher> <year> 1992. </year> <month> 37 </month>
Reference: [ Selman and Kautz, 1991 ] <author> Bart Selman and Henry Kautz. </author> <title> Knowledge com-pilation using Horn approximations. </title> <booktitle> In Proceedings of the 9th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 904-909, </pages> <address> Anaheim, CA, July 1991. </address> <publisher> MIT Press. </publisher>
Reference: [ Selman and Levesque, 1989 ] <author> Bart Selman and Hector J. Levesque. </author> <title> The tractability of path-based inheritance. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <month> August </month> <year> 1989. </year> <note> Mor-gan Kaufmann. </note>
Reference: [ Selman and Levesque, 1993 ] <author> Bart Selman and Hector J. Levesque. </author> <title> The complexity of path-based defeasible inheritance. </title> <journal> Artificial Intelligence, </journal> <volume> 62 </volume> <pages> 303-339, </pages> <year> 1993. </year>
Reference: [ Selman et al., 1992 ] <author> Bart Selman, Hector Levesque, and David Mitchell. </author> <title> A new method for solving hard satisfiability problems. </title> <booktitle> In Proceedings of the 10th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 440-446, </pages> <address> San Jose, CA, July 1992. </address> <publisher> MIT Press. </publisher>
Reference-contexts: One such approach is the "greedy satisfiability" method, also called GSAT <ref> [ Selman et al., 1992 ] </ref> , that tries to solve the satisfiability problem by local search attempting to maximize the number of satisfied clauses of a formula in conjunctive normal form. Local search is a well-known technique to solve optimization problems.
Reference: [ Selman, 1994 ] <author> Bart Selman. </author> <title> Domain-specific complexity tradeoffs. </title> <booktitle> In Proceedings of the 11th European Conference on Artificial Intelligence, </booktitle> <pages> pages 416-420, </pages> <address> Amsterdam, The Netherlands, </address> <month> August </month> <year> 1994. </year> <pages> Wiley. </pages>
Reference: [ Song and Cohen, 1988 ] <author> Fei Song and Robin Cohen. </author> <title> The interpretation of temporal relations in narrative. </title> <booktitle> In Proceedings of the 7th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 745-750, </pages> <address> Saint Paul, MI, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Y + X equals Y j X = Y , X &lt; Y + , X + &gt; Y , X + = Y + Table 1: The set B of the thirteen basic relations. plaining the performance of the system : : : " Consequently, for natural language understanding <ref> [ Allen, 1984; Song and Cohen, 1988 ] </ref> , general planning [ Allen, 1991; Allen and Koomen, 1983 ] , and presentation planning in a multimedia context [ Feiner et al., 1993 ] , the representation of qualitative temporal relations and reasoning about them is essential. <p> If one uses Allen's calculus in the context of natural language understanding for reasoning about the relative positions of events mentioned in a story, however, it turns out that only a subset of all possible relations is necessary <ref> [ Song and Cohen, 1988 ] </ref> . This subset has the property that each relation can be described as a conjunction of arithmetic comparisons between the interval endpoints.
Reference: [ Stillman, 1992 ] <author> J. Stillman. </author> <title> The complexity of propositional default logics. </title> <booktitle> In Proceedings of the 10th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 794-799, </pages> <address> San Jose, CA, July 1992. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Interestingly, a number of related reasoning problems can be classified as belonging to the same complexity class. For instance the cautious reasoning problems in nonmonotonic logics, such as default logic, autoepistemic logic, and circumscription are coNP NP -complete <ref> [ Gottlob, 1992; Stillman, 1992; Eiter and Gottlob, 1993b ] </ref> , as well as a number of abduction problems [ Eiter and Gottlob, 1993a ] . 10 This implies that all these problems are equivalent on an abstract algorithmic level and that all these problems can be more or less directly
Reference: [ Touretzky et al., 1987 ] <author> David S. Touretzky, John F. Horty, and Rich-mond H. Thomason. </author> <title> A clash of intuitions: The current state of nonmono-tonic multiple inheritance systems. </title> <editor> In J. McDermott, editor, </editor> <booktitle> Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 476-482, </pages> <address> Milan, Italy, </address> <month> August </month> <year> 1987. </year> <note> Morgan Kaufmann. 38 </note>
Reference-contexts: Although Touretzky's formalization of defeasible reasoning leads to more intuitive results than Fahlman's method, it is possible to have different opinions about whether Touretzky's proposal is the most reasonable one. In fact, there exist a number of different proposals for the definition of what an extension is <ref> [ Touretzky et al., 1987 ] </ref> . There exist alternative proposals 15 for the concatenation of argument chains, for conflict resolution, and for the notion of specificity. All in all, there are (at least) eight different ways of interpreting inheritance networks.
Reference: [ Touretzky, 1986 ] <author> David S. Touretzky. </author> <title> The Mathematics of Inheritance Sy--stems. </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: Inheritance networks are composed out of nodes denoting individuals, concepts, or properties, and positive and negative links between nodes standing for "X is typically Y ," symbolically X ! Y , and "X is typically not a Y ," symbolically X 6! Y <ref> [ Fahlman, 1979; Touretzky, 1986 ] </ref> . A typical example of an inheritance network is given in From the network in Figure 1 we may conclude, for instance, that Clyde is a member of the class of royal elephants, which are typically elephants, which are in turn typically grey.
Reference: [ van Beek and Cohen, 1990 ] <author> Peter van Beek and Robin Cohen. </author> <title> Exact and approximate reasoning about temporal relations. </title> <journal> Computational Intelligence, </journal> <volume> 6 </volume> <pages> 132-144, </pages> <year> 1990. </year>
Reference-contexts: Interestingly, the reasoning problems for the restricted set of relations are efficiently solvable <ref> [ Vilain et al., 1989; van Beek and Cohen, 1990 ] </ref> . Another problem that is NP-hard in general but seems to be easily solvable in practice is the problem of interpreting "sketch maps" as they can be derived from aerial photographs [ Reiter and Mackworth, 1989 ] .
Reference: [ Vilain and Kautz, 1986 ] <author> Marc B. Vilain and Henry A. Kautz. </author> <title> Constraint propagation algorithms for temporal reasoning. </title> <booktitle> In Proceedings of the 5th National Conference of the American Association for Artificial Intelligence, </booktitle> <pages> pages 377-382, </pages> <address> Philadelphia, PA, </address> <month> August </month> <year> 1986. </year>
Reference: [ Vilain et al., 1989 ] <author> Marc B. Vilain, Henry A. Kautz, and Peter G. van Beek. </author> <title> Constraint propagation algorithms for temporal reasoning: A revised report. </title> <editor> In D. S. Weld and J. de Kleer, editors, </editor> <booktitle> Readings in Qualitative Reasoning about Physical Systems, </booktitle> <pages> pages 373-381. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference-contexts: Interestingly, the reasoning problems for the restricted set of relations are efficiently solvable <ref> [ Vilain et al., 1989; van Beek and Cohen, 1990 ] </ref> . Another problem that is NP-hard in general but seems to be easily solvable in practice is the problem of interpreting "sketch maps" as they can be derived from aerial photographs [ Reiter and Mackworth, 1989 ] .
Reference: [ Wright et al., 1993 ] <author> Jon R. Wright, Elia S. Weixelbaum, Gregg T. Vesonder, Karen E. Brown, Stephen R. Palmer, Jay I. Berman, and Harry H. Moore. </author> <title> A knowledge-based configurator that supports sales, </title> <journal> engineering, and manufacturing at AT&T network systems. The AI Magazine, </journal> <volume> 14(3) </volume> <pages> 69-80, </pages> <year> 1993. </year>
Reference-contexts: While this is true in the general case, one should consider two points. First of all, Brachman and Levesque proposed this way of achieving efficiency only as one way. Secondly, there are indeed applications that can benefit from restricted knowledge representation languages <ref> [ Wright et al., 1993 ] </ref> . In any case, the discussion of this issue demonstrates that the idea of developing and analyzing knowledge representation systems independently from applications seems to be arguable.
References-found: 93

