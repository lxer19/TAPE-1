URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-271.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: (e-mail: shers@media.mit.edu)  (e-mail: picard@media.mit.edu),  
Title: Efficiency Of The Orthogonal Least Squares Training Method For Radial Basis Function Networks  
Author: Alex Sherstinsky yz Rosalind W. Picard Alex Sherstinsky Rosalind W. Picard 
Keyword: approximation networks, Radial Basis Functions, interpolation, approximation, Linear Least Squares Estimation, Gram-Schmidt, Orthogonal Least Squares, Karhunen-Loeve Transform, Gaussians, energy compaction.  
Address: E15-383, 20 Ames Street, Cambridge, MA 02139.  
Date: February, 1992 (revised October, 1993)  
Affiliation: Department of Electrical Engineering and Computer Science The Media Laboratory  MIT Media Laboratory,  
Note: On The  Inquiries can be addressed to:  or to  
Abstract: MIT Media Laboratory Perceptual Computing Group Technical Report #271 To appear in: IEEE Transactions on Neural Networks. Abstract The efficiency of the Orthogonal Least Squares (OLS) method for training approximation networks is examined using the criterion of energy compaction. We show that the selection of basis vectors produced by the procedure is not the most compact when the approximation is performed using a non-orthogonal basis. Hence, the algorithm does not produce the smallest possible networks for a given approximation error. Specific examples are given using the Gaussian Radial Basis Functions (RBF) type of approximation networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Cybenko, </author> <title> "Approximations by superpositions of a sigmoidal function," </title> <journal> Math. Control Signals Systems, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 303-314, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction A number of feed-forward networks with one hidden layer of processing units have been proven to possess the ability to approximate any continuous function arbitrarily well <ref> [1] </ref>, [2]. One such approximation scheme, the Radial Basis Function (RBF) network, has been used as a classifier [3], [4], [5], [6], [7]. The training problem for an RBF network can be viewed as interpolation and solved by inverting a matrix.
Reference: [2] <author> T. Poggio and F. Girosi, </author> <title> "Networks and the best approximation property," A.I. </title> <type> Memo #1164, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1989. </year>
Reference-contexts: 1 Introduction A number of feed-forward networks with one hidden layer of processing units have been proven to possess the ability to approximate any continuous function arbitrarily well [1], <ref> [2] </ref>. One such approximation scheme, the Radial Basis Function (RBF) network, has been used as a classifier [3], [4], [5], [6], [7]. The training problem for an RBF network can be viewed as interpolation and solved by inverting a matrix. <p> The RBF method is one of the possible solutions to the real multivariate interpolation problem, stated as follows [18], [8], [19], <ref> [2] </ref>, [20], [21]: Interpolation Problem: Given N different points f ~x i 2 R d j i = 1; : : : ; N g, where d is the number of dimensions, and N real numbers fy i 2 R j i = 1; : : : ; N g, find
Reference: [3] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction A number of feed-forward networks with one hidden layer of processing units have been proven to possess the ability to approximate any continuous function arbitrarily well [1], [2]. One such approximation scheme, the Radial Basis Function (RBF) network, has been used as a classifier <ref> [3] </ref>, [4], [5], [6], [7]. The training problem for an RBF network can be viewed as interpolation and solved by inverting a matrix. But this approach often causes numerical problems, because the matrices involved are typically large.
Reference: [4] <author> S. Renals and R. Rohwer, </author> <title> "Phoneme classification experiments using radial basis functions," </title> <booktitle> in Proceedings Of IEEE International Joint Conference on Neural Networks, </booktitle> <address> (Washington DC), </address> <pages> pp. </pages> <address> I:461-467, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: 1 Introduction A number of feed-forward networks with one hidden layer of processing units have been proven to possess the ability to approximate any continuous function arbitrarily well [1], [2]. One such approximation scheme, the Radial Basis Function (RBF) network, has been used as a classifier [3], <ref> [4] </ref>, [5], [6], [7]. The training problem for an RBF network can be viewed as interpolation and solved by inverting a matrix. But this approach often causes numerical problems, because the matrices involved are typically large.
Reference: [5] <author> K. Ng, </author> <title> "A comparative study of the practical characteristics of neural network and conventional pattern classifiers," </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction A number of feed-forward networks with one hidden layer of processing units have been proven to possess the ability to approximate any continuous function arbitrarily well [1], [2]. One such approximation scheme, the Radial Basis Function (RBF) network, has been used as a classifier [3], [4], <ref> [5] </ref>, [6], [7]. The training problem for an RBF network can be viewed as interpolation and solved by inverting a matrix. But this approach often causes numerical problems, because the matrices involved are typically large.
Reference: [6] <author> R. P. Lippmann, </author> <title> "An introduction to computing with neural nets," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 4, </volume> <pages> pp. 4-22, </pages> <month> Apr. </month> <year> 1987. </year>
Reference-contexts: 1 Introduction A number of feed-forward networks with one hidden layer of processing units have been proven to possess the ability to approximate any continuous function arbitrarily well [1], [2]. One such approximation scheme, the Radial Basis Function (RBF) network, has been used as a classifier [3], [4], [5], <ref> [6] </ref>, [7]. The training problem for an RBF network can be viewed as interpolation and solved by inverting a matrix. But this approach often causes numerical problems, because the matrices involved are typically large.
Reference: [7] <author> R. P. Lippmann, </author> <title> "Pattern classification using neural networks," </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 27, no. 11, </volume> <pages> pp. 47-64, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction A number of feed-forward networks with one hidden layer of processing units have been proven to possess the ability to approximate any continuous function arbitrarily well [1], [2]. One such approximation scheme, the Radial Basis Function (RBF) network, has been used as a classifier [3], [4], [5], [6], <ref> [7] </ref>. The training problem for an RBF network can be viewed as interpolation and solved by inverting a matrix. But this approach often causes numerical problems, because the matrices involved are typically large.
Reference: [8] <author> D. S. Broomhead and D. Lowe, </author> <title> "Multivariable functional interpolation and adaptive networks," </title> <journal> Complex Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 321-355, </pages> <year> 1988. </year>
Reference-contexts: But this approach often causes numerical problems, because the matrices involved are typically large. This problem has led to several alternatives aimed at reducing the training complexity without significant losses in approximation accuracy <ref> [8] </ref>, [9], [10]. This report analyzes the efficiency of one such method, Orthogonal Least Squares (OLS), proposed by Chen et al [11], [10]. Since its original publication, the OLS technique has found use in several applications, such as automatic control [12], [13], [14], fuzzy logic networks [15], [16], and others. <p> The RBF method is one of the possible solutions to the real multivariate interpolation problem, stated as follows [18], <ref> [8] </ref>, [19], [2], [20], [21]: Interpolation Problem: Given N different points f ~x i 2 R d j i = 1; : : : ; N g, where d is the number of dimensions, and N real numbers fy i 2 R j i = 1; : : : ; N <p> Then the product ( ^ H T ^ H) 2 R MfiM is an invertible matrix and thus a basis in R M [22]. Using this result, an approximation to (4) can be formulated and solved by the method of Linear Least Squares Estimation (LLSE) <ref> [8] </ref>: Approximation Problem: Given ^ H 2 R NfiM and ~y 2 R N , related by ~y = ^ H ~w + ~e; (6) find an optimal coefficient vector ~w 2 R M such that the error energy ~e T ~e is minimized.
Reference: [9] <author> J. Moody and C. Darken, </author> <title> "Fast-learning in networks of locally-tuned processing units," </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 281-294, </pages> <year> 1989. </year> <month> 6 </month>
Reference-contexts: But this approach often causes numerical problems, because the matrices involved are typically large. This problem has led to several alternatives aimed at reducing the training complexity without significant losses in approximation accuracy [8], <ref> [9] </ref>, [10]. This report analyzes the efficiency of one such method, Orthogonal Least Squares (OLS), proposed by Chen et al [11], [10]. Since its original publication, the OLS technique has found use in several applications, such as automatic control [12], [13], [14], fuzzy logic networks [15], [16], and others.
Reference: [10] <author> S. Chen, C. F. N. Cowan, and P. M. Grant, </author> <title> "Or--thogonal least squares learning algorithm for radial basis function networks," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 2, </volume> <pages> pp. 302-309, </pages> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: But this approach often causes numerical problems, because the matrices involved are typically large. This problem has led to several alternatives aimed at reducing the training complexity without significant losses in approximation accuracy [8], [9], <ref> [10] </ref>. This report analyzes the efficiency of one such method, Orthogonal Least Squares (OLS), proposed by Chen et al [11], [10]. Since its original publication, the OLS technique has found use in several applications, such as automatic control [12], [13], [14], fuzzy logic networks [15], [16], and others. <p> This problem has led to several alternatives aimed at reducing the training complexity without significant losses in approximation accuracy [8], [9], <ref> [10] </ref>. This report analyzes the efficiency of one such method, Orthogonal Least Squares (OLS), proposed by Chen et al [11], [10]. Since its original publication, the OLS technique has found use in several applications, such as automatic control [12], [13], [14], fuzzy logic networks [15], [16], and others. However, none of these papers discuss the method's efficiency. <p> The RBF matrix will be invertible if the column vectors of H form a basis in R N . This condition is satisfied for a number of RBFs [19]. network with one layer of hidden units <ref> [10] </ref>. Since each radial hidden unit defines a (d + 1)-dimensional hyper-surface, the RBF network interpolates by reconstructing the data with scaled hypersurfaces. <p> How ever, the centers are commonly chosen to be a subset of data points <ref> [10] </ref>. <p> However, arbitrarily selecting the centers from data points often results in poor perfor mance in a sense that the networks end up with more nodes than necessary for a desired accuracy of approximation <ref> [10] </ref>. 2.3 Orthogonal Least Squares In order to improve the performance of an RBF network trained by solving the approximation problem, a judicious selection of centers is needed. It has been reported in [10] that the approximation problem, stated in (6), lends itself to the Orthogonal Least Squares (OLS) method, which <p> sense that the networks end up with more nodes than necessary for a desired accuracy of approximation <ref> [10] </ref>. 2.3 Orthogonal Least Squares In order to improve the performance of an RBF network trained by solving the approximation problem, a judicious selection of centers is needed. It has been reported in [10] that the approximation problem, stated in (6), lends itself to the Orthogonal Least Squares (OLS) method, which is a recursive algorithm for selecting a suitable subset of data points as centers. <p> Therefore, it is interesting to find out whether or not the selection performed by the OLS procedure is the best in terms of energy packing. It has been stated in <ref> [10] </ref> that the OLS algorithm can be used to select centers so that adequate and parsimonious RBF networks can be obtained. However, the OLS method is not "optimally-parsimonious". <p> In other words, the sorted regressors cannot be used to train the RBF approximation network. 5 Conclusions While the OLS method has been believed to find a more efficient selection of RBF centers than a random-based approach <ref> [10] </ref>, it does not produce the smallest RBF network for a given approximation accuracy. Simple examples were constructed to provide intuition about the sources of inefficiency. Acknowledgements The authors would like to thank Federico Girosi and Terence Sanger for their comments on the manuscript.
Reference: [11] <author> S. Chen, S. A. Billings, and W. Luo, </author> <title> "Orthogonal least squares methods and their application to nonlinear system identification," </title> <journal> International Journal of Control, </journal> <volume> vol. 50, no. 5, </volume> <pages> pp. 1873-1896, </pages> <year> 1989. </year>
Reference-contexts: This problem has led to several alternatives aimed at reducing the training complexity without significant losses in approximation accuracy [8], [9], [10]. This report analyzes the efficiency of one such method, Orthogonal Least Squares (OLS), proposed by Chen et al <ref> [11] </ref>, [10]. Since its original publication, the OLS technique has found use in several applications, such as automatic control [12], [13], [14], fuzzy logic networks [15], [16], and others. However, none of these papers discuss the method's efficiency.
Reference: [12] <author> S. Chen and S. A. Billings, </author> <title> "Neural networks for nonlinear dynamic system modelling and identification," </title> <journal> International Journal of Control, </journal> <volume> vol. 56, no. 2, </volume> <pages> pp. 319-346, </pages> <year> 1992. </year>
Reference-contexts: This report analyzes the efficiency of one such method, Orthogonal Least Squares (OLS), proposed by Chen et al [11], [10]. Since its original publication, the OLS technique has found use in several applications, such as automatic control <ref> [12] </ref>, [13], [14], fuzzy logic networks [15], [16], and others. However, none of these papers discuss the method's efficiency. The present study suggests that the OLS algorithm is inefficient in its selection of significant basis functions. Section 2 reviews the RBF approximation problem and the OLS algorithm for solving it.
Reference: [13] <author> S. Chen, S. A. Billings, and P. M. Grant, </author> <title> "Recursive hybrid algorithm for non-linear system identification using radial basis function networks," </title> <journal> International Journal of Control, </journal> <volume> vol. 55, no. 5, </volume> <pages> pp. 1051-1070, </pages> <year> 1992. </year>
Reference-contexts: This report analyzes the efficiency of one such method, Orthogonal Least Squares (OLS), proposed by Chen et al [11], [10]. Since its original publication, the OLS technique has found use in several applications, such as automatic control [12], <ref> [13] </ref>, [14], fuzzy logic networks [15], [16], and others. However, none of these papers discuss the method's efficiency. The present study suggests that the OLS algorithm is inefficient in its selection of significant basis functions. Section 2 reviews the RBF approximation problem and the OLS algorithm for solving it.
Reference: [14] <author> S. Mukhopadhyay and K. S. Narendra, </author> <title> "Disturbance rejection in nonlinear systems using neural networks," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, </volume> <pages> pp. 63-72, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: This report analyzes the efficiency of one such method, Orthogonal Least Squares (OLS), proposed by Chen et al [11], [10]. Since its original publication, the OLS technique has found use in several applications, such as automatic control [12], [13], <ref> [14] </ref>, fuzzy logic networks [15], [16], and others. However, none of these papers discuss the method's efficiency. The present study suggests that the OLS algorithm is inefficient in its selection of significant basis functions. Section 2 reviews the RBF approximation problem and the OLS algorithm for solving it.
Reference: [15] <author> L.-X. Wang and J. M. Mendel, </author> <title> "Fuzzy basis functions, universal approximation, and orthogonal least squares learning," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, </volume> <pages> pp. 807-814, </pages> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: This report analyzes the efficiency of one such method, Orthogonal Least Squares (OLS), proposed by Chen et al [11], [10]. Since its original publication, the OLS technique has found use in several applications, such as automatic control [12], [13], [14], fuzzy logic networks <ref> [15] </ref>, [16], and others. However, none of these papers discuss the method's efficiency. The present study suggests that the OLS algorithm is inefficient in its selection of significant basis functions. Section 2 reviews the RBF approximation problem and the OLS algorithm for solving it.
Reference: [16] <author> J.-S. R. Jang and C.-T. Sun, </author> <title> "Functional equivalence between radial basis function networks and fuzzy inference systems," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, </volume> <pages> pp. 156-159, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: This report analyzes the efficiency of one such method, Orthogonal Least Squares (OLS), proposed by Chen et al [11], [10]. Since its original publication, the OLS technique has found use in several applications, such as automatic control [12], [13], [14], fuzzy logic networks [15], <ref> [16] </ref>, and others. However, none of these papers discuss the method's efficiency. The present study suggests that the OLS algorithm is inefficient in its selection of significant basis functions. Section 2 reviews the RBF approximation problem and the OLS algorithm for solving it.
Reference: [17] <author> A. Sherstinsky and R. W. </author> <title> Picard, "On training gaussian radial basis functions for image coding," </title> <type> Tech. Rep. 188, </type> <institution> M.I.T. Media Lab Vision and Modeling Group, </institution> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: Examples using Gaussian RBFs are also given in Section 4. Finally, Section 5 summarizes the present study. In a more detailed report, we address the issue of using the OLS method in order to judge the overall efficiency of the RBF expansion for image coding <ref> [17] </ref>. 2 Background 2.1 Radial Basis Functions A non-linear function h (~x; ~c), where ~x is the independent variable and ~c is the constant parameter, is called a Radial Basis Function (RBF) when it depends only on the radial distance r = k~x ~ck, where ~c is its "center".
Reference: [18] <author> M. J. D. Powell, </author> <title> "Radial basis functions for mul-tivariable interpolation: A review," in Algorithms for Approximation (J. </title> <editor> C. Mason and M. G. Cox, eds.), </editor> <publisher> (Oxford), Clarendon Press, </publisher> <year> 1987. </year>
Reference-contexts: The RBF method is one of the possible solutions to the real multivariate interpolation problem, stated as follows <ref> [18] </ref>, [8], [19], [2], [20], [21]: Interpolation Problem: Given N different points f ~x i 2 R d j i = 1; : : : ; N g, where d is the number of dimensions, and N real numbers fy i 2 R j i = 1; : : : ;
Reference: [19] <author> T. Poggio and F. Girosi, </author> <title> "A theory of networks for approximation and learning," A.I. </title> <type> Memo #1140, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1989. </year>
Reference-contexts: The RBF method is one of the possible solutions to the real multivariate interpolation problem, stated as follows [18], [8], <ref> [19] </ref>, [2], [20], [21]: Interpolation Problem: Given N different points f ~x i 2 R d j i = 1; : : : ; N g, where d is the number of dimensions, and N real numbers fy i 2 R j i = 1; : : : ; N g, <p> The RBF matrix will be invertible if the column vectors of H form a basis in R N . This condition is satisfied for a number of RBFs <ref> [19] </ref>. network with one layer of hidden units [10]. Since each radial hidden unit defines a (d + 1)-dimensional hyper-surface, the RBF network interpolates by reconstructing the data with scaled hypersurfaces.
Reference: [20] <author> T. Poggio and F. Girosi, </author> <title> "Extension of a theory of networks for approximation and learning: Dimensionality reduction and clustering," A.I. </title> <type> Memo #1167, </type> <institution> Artificial Intelligence Laboratory, Mas-sachusetts Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: The RBF method is one of the possible solutions to the real multivariate interpolation problem, stated as follows [18], [8], [19], [2], <ref> [20] </ref>, [21]: Interpolation Problem: Given N different points f ~x i 2 R d j i = 1; : : : ; N g, where d is the number of dimensions, and N real numbers fy i 2 R j i = 1; : : : ; N g, find a
Reference: [21] <author> T. Poggio and F. Girosi, </author> <title> "Networks for approximation and learning," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 78, </volume> <pages> pp. 1481-1497, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: The RBF method is one of the possible solutions to the real multivariate interpolation problem, stated as follows [18], [8], [19], [2], [20], <ref> [21] </ref>: Interpolation Problem: Given N different points f ~x i 2 R d j i = 1; : : : ; N g, where d is the number of dimensions, and N real numbers fy i 2 R j i = 1; : : : ; N g, find a function <p> In contrast, the non-variational approach starts with fixed basis vectors and searches for a combination that best approximates the data. In the context of the approximation problem, the criterion is typically the minimization of the mean-squared error, and "smoothness" of the solution is a possible choice for the constraint <ref> [21] </ref>.
Reference: [22] <author> G. Strang, </author> <title> Linear Algebra and Its Applications. </title> <publisher> Academic Press, </publisher> <year> 1980. </year>
Reference-contexts: Then the product ( ^ H T ^ H) 2 R MfiM is an invertible matrix and thus a basis in R M <ref> [22] </ref>. <p> process finds A = ^ QR, where the matrix ^ Q 2 R NfiM consists of orthonormal column vectors, and the right-triangular matrix R 2 R MfiM contains projection and normalization coefficients computed by GS 1 . 1 For a detailed treatment, consult a standard linear algebra text, such as <ref> [22] </ref>, [23], etc. 2 Using the selection matrix notation, the approxima- tion problem, stated in (6), takes the following form: ~y = HS ~w + ~e; (12) ~y = ^ QR ~w + ~e; (14) where ~w 2 R M is the coefficient vector and ~e 2 R N is the
Reference: [23] <author> G. Strang, </author> <title> Introduction to Applied Mathematics. </title> <address> Cambridge, MA: </address> <publisher> Wellesley-Cambridge Press, </publisher> <year> 1986. </year>
Reference-contexts: finds A = ^ QR, where the matrix ^ Q 2 R NfiM consists of orthonormal column vectors, and the right-triangular matrix R 2 R MfiM contains projection and normalization coefficients computed by GS 1 . 1 For a detailed treatment, consult a standard linear algebra text, such as [22], <ref> [23] </ref>, etc. 2 Using the selection matrix notation, the approxima- tion problem, stated in (6), takes the following form: ~y = HS ~w + ~e; (12) ~y = ^ QR ~w + ~e; (14) where ~w 2 R M is the coefficient vector and ~e 2 R N is the error
Reference: [24] <author> A. Papoulis, </author> <title> Probability, Random Variables, and Stochastic Processes. </title> <publisher> McGraw-Hill, </publisher> <year> 1965. </year>
Reference-contexts: matrix to be orthogonal, and applying the variational approach leads to the method of "principal components". 3.1 Principal Components Analysis is Variational It is well-known that the eigenvectors of the covari-ance matrix of the data are the "principal components", which form the basis that possesses the best energy compaction properties <ref> [24] </ref>, [25]. This basis constitutes the Karhunen-Loeve Transform (KLT), which decorrelates the data and maximizes the incremental energy (or variance, in the statistical sense) explained by each regres sor.
Reference: [25] <author> R. J. Clarke, </author> <title> Transform Coding of Images. </title> <publisher> Or-lando: Academic Press, Inc., </publisher> <year> 1985. </year>
Reference-contexts: to be orthogonal, and applying the variational approach leads to the method of "principal components". 3.1 Principal Components Analysis is Variational It is well-known that the eigenvectors of the covari-ance matrix of the data are the "principal components", which form the basis that possesses the best energy compaction properties [24], <ref> [25] </ref>. This basis constitutes the Karhunen-Loeve Transform (KLT), which decorrelates the data and maximizes the incremental energy (or variance, in the statistical sense) explained by each regres sor.
Reference: [26] <author> T. D. Sanger, </author> <title> "Optimal unsupervised learning in a single-layer linear feedforward neural network," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 459-473, </pages> <year> 1989. </year> <month> 7 </month>
Reference-contexts: The significance of the KLT is in its energy efficiency, and networks that "learn" the principal components of the data need the smallest number of processing units for a given amount of error <ref> [26] </ref>. 3.2 OLS is Non-Variational The objective of the OLS method is to find the smallest subset of a fixed original basis (while not exceeding the allowed approximation error); therefore, the choices available to the procedure are restricted to various combinations of the original basis vectors.
References-found: 26

