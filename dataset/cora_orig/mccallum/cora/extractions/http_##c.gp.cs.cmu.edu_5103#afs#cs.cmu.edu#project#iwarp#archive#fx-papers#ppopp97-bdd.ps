URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/iwarp/archive/fx-papers/ppopp97-bdd.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/bwolen/Web/publications.html
Root-URL: http://www.cs.cmu.edu
Email: fbwolen,drohg@cs.cmu.edu  
Title: Parallel Breadth-First BDD Construction  
Author: Bwolen Yang and David R. O'Hallaron 
Address: Pittsburgh, PA 15213  
Affiliation: Carnegie Mellon University  
Abstract: With the increasing complexity of protocol and circuit designs, formal verification has become an important research area and binary decision diagrams (BDDs) have been shown to be a powerful tool in formal verification. This paper presents a parallel algorithm for BDD construction targeted at shared memory multiprocessors and distributed shared memory systems. This algorithm focuses on improving memory access locality through specialized memory managers and partial breadth-first expansion, and on improving processor utilization through dynamic load balancing. The results on a shared memory system show speedups of over two on four processors and speedups of up to four on eight processors. The measured results clearly identify the main source of bottlenecks and point out some interesting directions for further improvements. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Amza, C., Cox, A., Dwarkadas, S., Hyams, C., Li, Z., and Zwaenepoel, W. Treadmarks: </author> <title> Shared memory computing on networks of workstations. </title> <booktitle> IEEE Computer 29, </booktitle> <month> 2 (Feb </month> <year> 1996), </year> <pages> 18-28. </pages>
Reference-contexts: When a processor becomes idle, work loads are redistributed to keep the load balanced. The target architectures of this algorithm are shared memory multiprocessors [15] and DSM systems <ref> [1] </ref>. The rest of this section will describe each part of the algorithm in more detail. 3.1 Partial Breadth-First Construction Since BDD construction involves a large number of accesses of many small data structures, localizing the memory access pattern to bound the working set size is critical. <p> However, these numbers also show that this algorithm will be effective in pooling memory together for a DSM system (e.g., cluster of workstations running Treadmarks <ref> [1] </ref>) to avoid page faults; e.g., on a DSM with 8 processors (i.e., 8 times the memory), this memory usage would be equivalent to having 4 times the memory on the 1 processor case.
Reference: [2] <author> Ashar, R., and Cheong, M. </author> <title> Efficient breadth-first manipulation of binary decision diagrams. </title> <booktitle> In Proceedings of the International Conference on Computer-Aided Design (November 1994), </booktitle> <pages> pp. 622-627. </pages>
Reference-contexts: The performance im-pact for the depth-first algorithm's poor memory locality is especially severe for BDDs larger than the physical memory. Recently, there has been much interest in BDD construction based on breadth-first traversal <ref> [16, 17, 2, 11, 20, 7] </ref>. In a breadth-first traversal, the expansion phase expands operations one variable at a time with all the operations of the same variable expanded together. Furthermore, during the reduction phase, all the new BDD nodes of the same variable are created together. <p> The BDDs are constructed through building and minimizing finite automata. This work does not consider dynamic load balancing. In 1991, [16] describes a breadth-first algorithm for BDD construction for vector machines. Later in 1993, [17] shows how to use the breadth-first approach to construct very large BDDs. In 1994, <ref> [2] </ref> shows an efficient implementation of the breadth-first approach by building a block index table of size proportional to the address space. In 1994, [18] parallelizes Bryant's original 1986 BDD algorithm on the CM5 using DSM. The data is arbitrarily distributed by the DSM system.
Reference: [3] <author> Brace, K., Rudell, R., and Bryant, R. E. </author> <title> Efficient implementation of a BDD package. </title> <booktitle> In Proceedings of the 27th ACM/IEEE Design Automation Conference (June 1990), </booktitle> <pages> pp. 40-45. </pages>
Reference-contexts: It not only requires a lot of memory, it also requires frequent accesses to a lot of small data structures (the node size is typically 16 bytes on 32-bit machines). Conventional BDD construction algorithms are based on depth-first traversal of the BDDs <ref> [3] </ref>. This approach has poor memory behavior because of irregular control flows and memory access pattern. The control flow is irregular because the recursive expansion can terminate at any time when a terminal case is detected or when the operation is cached by the computed cache. <p> In this section we will briefly describe each implementation. We will also briefly note related advances in sequential BDD techniques to place each parallel implementation in its proper context. In 1986, [5] first proposes the use BDDs to manipulate Boolean functions. In 1990, <ref> [3] </ref> describes some techniques which can lead to an order of magnitude improvement in execution time and memory usage. This is the basis of most of the depth-first BDD packages today. In 1990, [14] explores parallelism in operation sequences on shared memory systems.
Reference: [4] <author> Brglez, F., and Fujiwara, H. </author> <title> A neutral netlist of 10 combinational benchmark circuits and a target translator in Fortran. </title> <booktitle> In 1985 International Symposium on Circuits And Systems (1985). </booktitle>
Reference-contexts: Each processor is a 195 MHz MIPS R10000. On this machine, we have studied results up to 8 processors, which is sufficient to identify the strengths and the weak-nesses of our parallel algorithm. The test cases used are from the ISCAS85 benchmarks <ref> [4] </ref> which contains netlists of ten circuits used in industry. The variable ordering used is generated by order dfs in SIS [22].
Reference: [5] <author> Bryant, R. E. </author> <title> Graph-based algorithms for Boolean function manipulation. </title> <journal> In IEEE Transactions on Computers (1986), </journal> <volume> vol. C-35, </volume> <pages> pp. 8 677-691. </pages>
Reference-contexts: As an example, in 1994, Intel's famous Pentium division bug (which caused a pretax charge of $475 million to Intel's revenue) clearly demonstrated the demands for more powerful verification tools. Binary decision diagrams (BDDs) have been shown to be a powerful tool in formal verification <ref> [5] </ref>. BDDs have proven to be so useful because they provide a unique and compact symbolic representation of a Boolean function. Compactness is important because it allows us to represent large functions. Uniqueness is important because it allows us to easily test two Boolean functions for equivalence. <p> Even though many functions have compact BDD representations, some functions can have very large BDDs. For example, BDD representations for integer multiplication have been shown to be exponential in the number of input bits <ref> [5] </ref>. To address this issue, there are many BDD related research efforts on reducing the size of the graph with techniques like new compact representations for specific classes of functions (KFDD [9] and K*BMD [8]), divide-and-conquer (POBDD [12] and ACV [6]), function abstraction (aBdd [13]), and variable ordering [21]. <p> In this section we will briefly describe each implementation. We will also briefly note related advances in sequential BDD techniques to place each parallel implementation in its proper context. In 1986, <ref> [5] </ref> first proposes the use BDDs to manipulate Boolean functions. In 1990, [3] describes some techniques which can lead to an order of magnitude improvement in execution time and memory usage. This is the basis of most of the depth-first BDD packages today.
Reference: [6] <author> Chen, Y.-A., and Bryant, R. E. Acv: </author> <title> An arithmetic circuit verifier. </title> <booktitle> In Proceedings of the International Conference on Computer-Aided Design (November 1996), </booktitle> <pages> pp. 361-365. </pages>
Reference-contexts: To address this issue, there are many BDD related research efforts on reducing the size of the graph with techniques like new compact representations for specific classes of functions (KFDD [9] and K*BMD [8]), divide-and-conquer (POBDD [12] and ACV <ref> [6] </ref>), function abstraction (aBdd [13]), and variable ordering [21]. Despite these efforts, large graphs can still naturally arise for more irregular functions or for incorrect implementation of specification. Incorrect implementation can break the structure of a function and thus can greatly increase the graph size.
Reference: [7] <author> Chen, Y.-A., Yang, B., and Bryant, R. E. </author> <title> Breadth-first with depth-first BDD construction: A hybrid approach. </title> <type> Tech. Rep. </type> <institution> CMU-CS-97-120, School of Computer Science, Carnegie Mellon University, </institution> <year> 1997. </year>
Reference-contexts: The implementation of the partial breadth-first algorithm is based on extending a hybrid implementation introduced in <ref> [7] </ref>, which has comparable or better performance than other sequential BDD packages. Results on a shared memory system show speedups of over two on four processors and speedups of up to four on eight processors. These results also point out some directions for future improvements. <p> The performance im-pact for the depth-first algorithm's poor memory locality is especially severe for BDDs larger than the physical memory. Recently, there has been much interest in BDD construction based on breadth-first traversal <ref> [16, 17, 2, 11, 20, 7] </ref>. In a breadth-first traversal, the expansion phase expands operations one variable at a time with all the operations of the same variable expanded together. Furthermore, during the reduction phase, all the new BDD nodes of the same variable are created together. <p> Thus, on some applications where the depth-first construction fits in the physical memory while the breadth-first construction does not, the performance of the breadth-first construction can degrade significantly due to page faults. To limit memory overhead, <ref> [7] </ref> introduces a hybrid approach which performs breadth-first expansion until a fixed threshold is reached. This threshold is set based on the amount of available physical memory. After reaching the threshold, the algorithm switches to the depth-first construction to limit memory overhead. <p> However, as BDD nodes tend to cluster on a very small number of variables (as our experiments have shown in Section 4), this approach is very ineffective as both the expansion and the reduction phase will be stalled by processing operations on these handful of variables. In 1997, <ref> [7] </ref> describes a hybrid BDD algorithm which also incorporates the computed cache and depth-first approach into the breath-first algorithm to bound memory overhead. Its performance is generally better than other leading BDD packages.
Reference: [8] <author> Drechsler, R., Becker, B., and Ruppertz, S. K*BMDs: </author> <title> a new data structure for verification. </title> <booktitle> In Proceedings of European Design and Test Conference (March 1996), </booktitle> <pages> pp. 2-8. </pages>
Reference-contexts: To address this issue, there are many BDD related research efforts on reducing the size of the graph with techniques like new compact representations for specific classes of functions (KFDD [9] and K*BMD <ref> [8] </ref>), divide-and-conquer (POBDD [12] and ACV [6]), function abstraction (aBdd [13]), and variable ordering [21]. Despite these efforts, large graphs can still naturally arise for more irregular functions or for incorrect implementation of specification.
Reference: [9] <author> Drechsler, R., Sarabi, A., Theobald, M., Becker, B., and Perkowski, M. A. </author> <title> Efficient representation and manipulation of switching functions based on ordered kronecker functional decision diagrams. </title> <booktitle> In Proceedings of the 31st ACM/IEEE Design Automation Conference (June 1994), </booktitle> <pages> pp. 415-419. </pages>
Reference-contexts: To address this issue, there are many BDD related research efforts on reducing the size of the graph with techniques like new compact representations for specific classes of functions (KFDD <ref> [9] </ref> and K*BMD [8]), divide-and-conquer (POBDD [12] and ACV [6]), function abstraction (aBdd [13]), and variable ordering [21]. Despite these efforts, large graphs can still naturally arise for more irregular functions or for incorrect implementation of specification.
Reference: [10] <author> Gai, S., Rebaudengo, M., and Reorda, M. S. </author> <title> A data parallel algorithm for Boolean function manipulation. </title> <booktitle> In Proceedings of Fifth Symposium on Frontiers of Massively Parallel Computation (February 1995), </booktitle> <pages> pp. 28-34. </pages>
Reference-contexts: With parallelization, BDD construction not only can benefit from more computation power, but more importantly, it can also benefit from pooling the memory of multiple machines together. There is a great deal of interest in parallel BDD construction algorithms <ref> [14, 18, 10, 23] </ref>. However, it is nontrivial to parallelize BDD construction efficiently, primarily because the construction process involves numerous memory references to small data structures with little computational work to amortize the cost of each reference. <p> This approach does not consider memory access locality and will require a good DSM system in order to perform well. In 1995, <ref> [10] </ref> introduces a data parallel BDD package for massively parallel SIMD machine. On a 16K-node MasPar, they reported a speedup of around 10. In 1996, [23] parallelizes depth-first BDD construction on a network of workstations with a specialized network and a communication co-processor on each workstation.
Reference: [11] <author> Hett, A., Frechsler, R., and Becker, B. </author> <title> MORE: Alternative Implementation of BDD-Packages by Multi-Operand Synthesis. </title> <booktitle> In Proceedings of the Eu-ropean Design Automation Conference (1996). </booktitle>
Reference-contexts: The performance im-pact for the depth-first algorithm's poor memory locality is especially severe for BDDs larger than the physical memory. Recently, there has been much interest in BDD construction based on breadth-first traversal <ref> [16, 17, 2, 11, 20, 7] </ref>. In a breadth-first traversal, the expansion phase expands operations one variable at a time with all the operations of the same variable expanded together. Furthermore, during the reduction phase, all the new BDD nodes of the same variable are created together.
Reference: [12] <author> Jain, J., Narayan, A., Coelho, C., Khatri, S., Sangiovanni-Vincentelli, A., and R.K. Brayton, M. F. </author> <title> Decomposition techniques for efficient ROBDD construction. </title> <booktitle> In Proceedings of the Formal Methods on Computer-Aided Design (November 1996), </booktitle> <pages> pp. 419-434. </pages>
Reference-contexts: To address this issue, there are many BDD related research efforts on reducing the size of the graph with techniques like new compact representations for specific classes of functions (KFDD [9] and K*BMD [8]), divide-and-conquer (POBDD <ref> [12] </ref> and ACV [6]), function abstraction (aBdd [13]), and variable ordering [21]. Despite these efforts, large graphs can still naturally arise for more irregular functions or for incorrect implementation of specification. Incorrect implementation can break the structure of a function and thus can greatly increase the graph size.
Reference: [13] <author> Jha, S., Lu, Y., Minea, M., and Clarke, E. M. </author> <title> Equivalence checking using abstract BDDs. </title> <booktitle> Submitted to 1997 IEEE International Conference on Computer Design, </booktitle> <year> 1997. </year>
Reference-contexts: To address this issue, there are many BDD related research efforts on reducing the size of the graph with techniques like new compact representations for specific classes of functions (KFDD [9] and K*BMD [8]), divide-and-conquer (POBDD [12] and ACV [6]), function abstraction (aBdd <ref> [13] </ref>), and variable ordering [21]. Despite these efforts, large graphs can still naturally arise for more irregular functions or for incorrect implementation of specification. Incorrect implementation can break the structure of a function and thus can greatly increase the graph size.
Reference: [14] <author> Kimura, S., and Clarke, E. M. </author> <title> A parallel algorithm for constructing binary decision diagrams. </title> <booktitle> In 1990 IEEE Proceedings of the International Conference on Computer Design (Sept 1990), </booktitle> <pages> pp. 220-223. </pages>
Reference-contexts: With parallelization, BDD construction not only can benefit from more computation power, but more importantly, it can also benefit from pooling the memory of multiple machines together. There is a great deal of interest in parallel BDD construction algorithms <ref> [14, 18, 10, 23] </ref>. However, it is nontrivial to parallelize BDD construction efficiently, primarily because the construction process involves numerous memory references to small data structures with little computational work to amortize the cost of each reference. <p> In 1986, [5] first proposes the use BDDs to manipulate Boolean functions. In 1990, [3] describes some techniques which can lead to an order of magnitude improvement in execution time and memory usage. This is the basis of most of the depth-first BDD packages today. In 1990, <ref> [14] </ref> explores parallelism in operation sequences on shared memory systems. Their experimental results on a 10-bit multiplier show a speedup of 10 on a 16-processors Encore-Multimax shared memory machine. The BDDs are constructed through building and minimizing finite automata. This work does not consider dynamic load balancing.
Reference: [15] <author> Lenoski, D., Laudon, J., Gharachorloo, K., Weber, W., Gupta, A., Hennessy, J., Horowitz, M., and Lam, M. </author> <title> The Stanford Dash multiprocessor. </title> <booktitle> IEEE Computer 25, </booktitle> <month> 3 (Mar. </month> <year> 1992), </year> <pages> 63-79. </pages>
Reference-contexts: When a processor becomes idle, work loads are redistributed to keep the load balanced. The target architectures of this algorithm are shared memory multiprocessors <ref> [15] </ref> and DSM systems [1].
Reference: [16] <author> Ochi, H., Ishiura, N., and Yajima, S. </author> <title> Breadth-first manipulation of SBDD of Boolean functions for vector processing. </title> <booktitle> In Proceedings of the 28th ACM/IEEE Design Automation Conference (June 1991), </booktitle> <pages> pp. 413-416. </pages>
Reference-contexts: The performance im-pact for the depth-first algorithm's poor memory locality is especially severe for BDDs larger than the physical memory. Recently, there has been much interest in BDD construction based on breadth-first traversal <ref> [16, 17, 2, 11, 20, 7] </ref>. In a breadth-first traversal, the expansion phase expands operations one variable at a time with all the operations of the same variable expanded together. Furthermore, during the reduction phase, all the new BDD nodes of the same variable are created together. <p> Their experimental results on a 10-bit multiplier show a speedup of 10 on a 16-processors Encore-Multimax shared memory machine. The BDDs are constructed through building and minimizing finite automata. This work does not consider dynamic load balancing. In 1991, <ref> [16] </ref> describes a breadth-first algorithm for BDD construction for vector machines. Later in 1993, [17] shows how to use the breadth-first approach to construct very large BDDs.
Reference: [17] <author> Ochi, H., Yasuoka, K., and Yajima, S. </author> <title> Breadth-first manipulation of very large binary-decision diagrams. </title> <booktitle> In Proceedings of the International Conference on Computer-Aided Design (November 1993), </booktitle> <pages> pp. 48-55. </pages>
Reference-contexts: The performance im-pact for the depth-first algorithm's poor memory locality is especially severe for BDDs larger than the physical memory. Recently, there has been much interest in BDD construction based on breadth-first traversal <ref> [16, 17, 2, 11, 20, 7] </ref>. In a breadth-first traversal, the expansion phase expands operations one variable at a time with all the operations of the same variable expanded together. Furthermore, during the reduction phase, all the new BDD nodes of the same variable are created together. <p> The BDDs are constructed through building and minimizing finite automata. This work does not consider dynamic load balancing. In 1991, [16] describes a breadth-first algorithm for BDD construction for vector machines. Later in 1993, <ref> [17] </ref> shows how to use the breadth-first approach to construct very large BDDs. In 1994, [2] shows an efficient implementation of the breadth-first approach by building a block index table of size proportional to the address space.
Reference: [18] <author> Parasuram, Y., Stabler, E., and Chin, S.-K. </author> <title> Parallel implementation of BDD algorithms using a distributed shared memory. </title> <booktitle> In Proceedings of 27th Hawaii International Conference on Systems Sciences (Jan-uary 1994), </booktitle> <pages> pp. 16-25. </pages>
Reference-contexts: With parallelization, BDD construction not only can benefit from more computation power, but more importantly, it can also benefit from pooling the memory of multiple machines together. There is a great deal of interest in parallel BDD construction algorithms <ref> [14, 18, 10, 23] </ref>. However, it is nontrivial to parallelize BDD construction efficiently, primarily because the construction process involves numerous memory references to small data structures with little computational work to amortize the cost of each reference. <p> Later in 1993, [17] shows how to use the breadth-first approach to construct very large BDDs. In 1994, [2] shows an efficient implementation of the breadth-first approach by building a block index table of size proportional to the address space. In 1994, <ref> [18] </ref> parallelizes Bryant's original 1986 BDD algorithm on the CM5 using DSM. The data is arbitrarily distributed by the DSM system. The computation is distributed using a distributed stack. During the expansion phase, new operations are pushed onto the distributed stack.
Reference: [19] <author> Ranjan, R. K., Sanghavi, J. V., Brayton, R. K., and Sangiovanni-Vincentelli, A. </author> <title> Decision diagrams on network of workstations. </title> <booktitle> In 1996 IEEE Proceedings of the International Conference on Computer Design (October 1996). </booktitle>
Reference-contexts: In the cases where the problem does not fit into the memory of one machine, a superlinear speedup is observed. In 1996, [20] describes a breadth-first BDD algorithm and introduces the concept of issuing superscalarity and pipelining in expanding multiple top level operations. Later in the same year, <ref> [19] </ref> extends this work and describes a sequential algorithm with the speedup obtained by pooling memory of a network of workstations. In their approach, each workstation owns a disjoint set of consecutive variables and each BDD node is assigned to the owner the corresponding variable.
Reference: [20] <author> Ranjan, R. K., Sanghavi, J. V., Brayton, R. K., and Sangiovanni-Vincentelli, A. </author> <title> High performance BDD package based on exploiting memory hierarchy. </title> <booktitle> In Proceedings of the 33rd ACM/IEEE Design Automation Conference (June 1996), </booktitle> <pages> pp. 635-640. </pages>
Reference-contexts: The performance im-pact for the depth-first algorithm's poor memory locality is especially severe for BDDs larger than the physical memory. Recently, there has been much interest in BDD construction based on breadth-first traversal <ref> [16, 17, 2, 11, 20, 7] </ref>. In a breadth-first traversal, the expansion phase expands operations one variable at a time with all the operations of the same variable expanded together. Furthermore, during the reduction phase, all the new BDD nodes of the same variable are created together. <p> lookup (unique table, b) 17 if BDD node b does not exist in the unique table, 18 insert b into the unique table 19 opNode.result b BDD Algorithm To take advantage of structured access in the partial breadth-first approach, we associate a specialized BDD-node manager for each variable as in <ref> [20] </ref>. Each variable's BDD-node manager clusters BDD nodes of the same variable by allocating memory in terms of blocks and allocates BDD nodes contiguously within each block. <p> This work shows that pooling together the memory of several workstations can be a main factor in achieving speedup. In the cases where the problem does not fit into the memory of one machine, a superlinear speedup is observed. In 1996, <ref> [20] </ref> describes a breadth-first BDD algorithm and introduces the concept of issuing superscalarity and pipelining in expanding multiple top level operations. Later in the same year, [19] extends this work and describes a sequential algorithm with the speedup obtained by pooling memory of a network of workstations.
Reference: [21] <author> Rudell, R. </author> <title> Dynamic variable ordering for ordered binary decision diagrams. </title> <booktitle> In Proceedings of the International Conference on Computer-Aided Design (Novem-ber 1993), </booktitle> <pages> pp. 139-144. </pages>
Reference-contexts: To address this issue, there are many BDD related research efforts on reducing the size of the graph with techniques like new compact representations for specific classes of functions (KFDD [9] and K*BMD [8]), divide-and-conquer (POBDD [12] and ACV [6]), function abstraction (aBdd [13]), and variable ordering <ref> [21] </ref>. Despite these efforts, large graphs can still naturally arise for more irregular functions or for incorrect implementation of specification. Incorrect implementation can break the structure of a function and thus can greatly increase the graph size.
Reference: [22] <author> Sentovich, E. M., Singh, K. J., Lavagno, L., Moon, C., Murgai, R., Saldanha, A., Savoj, H., Stephan, P. R., Brayton, R. K., and Sangiovanni-Vincentelli., A. L. </author> <title> SIS: A system for sequential circuit synthesis. </title> <type> Tech. Rep. </type> <institution> UCB/ERL M92/41, Electronics Research Lab, University of California, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The test cases used are from the ISCAS85 benchmarks [4] which contains netlists of ten circuits used in industry. The variable ordering used is generated by order dfs in SIS <ref> [22] </ref>. Using these variable orderings, only four out of these ten circuits take more than 5 seconds to construct and only two (C2670 and C3540) complete in less than one hour on this machine. One of the very long running circuits is a 16-bit multiplier (C6288).
Reference: [23] <author> Sotrnetta, T., and Brewer, F. </author> <title> Implementation of an efficient parallel BDD package. </title> <booktitle> In Proceedings of the 33rd ACM/IEEE Design Automation Conference (June 1996), </booktitle> <pages> pp. 641-644. </pages>
Reference-contexts: With parallelization, BDD construction not only can benefit from more computation power, but more importantly, it can also benefit from pooling the memory of multiple machines together. There is a great deal of interest in parallel BDD construction algorithms <ref> [14, 18, 10, 23] </ref>. However, it is nontrivial to parallelize BDD construction efficiently, primarily because the construction process involves numerous memory references to small data structures with little computational work to amortize the cost of each reference. <p> This approach does not consider memory access locality and will require a good DSM system in order to perform well. In 1995, [10] introduces a data parallel BDD package for massively parallel SIMD machine. On a 16K-node MasPar, they reported a speedup of around 10. In 1996, <ref> [23] </ref> parallelizes depth-first BDD construction on a network of workstations with a specialized network and a communication co-processor on each workstation. The computational model is "owner-computes" with BDD nodes evenly distributed. This method has the advantage that load is perfectly balanced.
Reference: [24] <author> Subhlok, J., and Yang, B. </author> <title> A new model for integrated nested task and data parallel programming. </title> <booktitle> In Ninth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (June 1997). To Appear. </booktitle>
References-found: 24

