URL: ftp://ftp.speech.sri.com/pub/people/julia/papers/cmc98-2.ps.gz
Refering-URL: http://www.speech.sri.com/people/julia/mmap.html
Root-URL: 
Email: martin@limsi.fr  cheyer@ai.sri.com, julia@speech.sri.com  
Phone: 2  
Title: A Theoretical Framework for Multimodal User Studies  
Author: Jean-Claude Martin ; Luc Julia Adam Cheyer 
Keyword: Multimodality, user modeling, theoretical framework  
Address: BP 133, 91403 Orsay Cedex, France,  333 Ravenswood Avenue, Menlo Park, CA 94025, U.S.A.  
Affiliation: 1 Laboratoire d'Informatique pour la Mecanique et les Sciences de l'Ingenieur (LIMSI-CNRS),  SRI International,  
Abstract: Researchers have conducted studies in various application areas to model human behavior during multimodal interactions with a real or simulated system. We propose a theoretical framework based on "types" and "goals" of cooperation between modalities. Using a survey of current multimodal research, we analyze several multimodal experiments and systems, examining which aspects of our framework were already considered by the researchers, and what elements the proposed framework could have added to these investigations. It is our hope that the framework will make it easier to link user observation on one side and software design on the other side.
Abstract-found: 1
Intro-found: 1
Reference: <institution> ACL (1997) Proceedings of the workshop "Referring phenomena in a multimedia context and their computational treatment", ACL/EACL'97, July 11th, Madrid. </institution> <note> http://www.dfki.uni-sb.de/imedia/workshops/mm-references.html Ando, </note> <author> H., Kitahara, Y., Hataoka, N. </author> <title> (1994) Evaluating multimodal interface using spoken language and pointing gesture on interior design system. </title> <booktitle> Proceedings of International Conference on Spoken Language Processing. </booktitle> <pages> 567-570. </pages>
Reference: <author> Bearne, M., Jones, S., Sapsford-Francis, J. </author> <title> (1994) Towards usability guidelines for multimedia systems. </title> <booktitle> Proceedings of the second ACM International Conference on Multimedia (MULTIMEDIA'94), </booktitle> <address> San Francisco, </address> <month> 15-20 October. </month> <pages> 105-110. </pages>
Reference-contexts: User studies can also help in finding out whether a concurrent use of several modalities by the system is helpful and appreciated by the subjects <ref> (Bearne et al. 94) </ref>.
Reference: <author> Cassell, J., Pelachaud, C., Badler, N., Steedman, M., Achorn, B., Becket, T., Douville, B., Prevost, S., Stone, M. </author> <title> (1994) Animated conversation: rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. </title> <booktitle> SIGGRAPH'94. Computer Graphics Proceedings, Annual Conference Series. </booktitle> <pages> 413-420. </pages>
Reference-contexts: multimodal conversations between human (and animated agents) seem to involve several types of cooperation: facial expressions can replace sequences of words (equivalence) or accompany them (redundancy and complementarity), for example, gazing at the other person to see how she follows (transfer between gaze and speech) such as is described in <ref> (Cassell et al. 94) </ref>. We think that TYCOON could be a useful theoretical framework for analyzing multimodal corpuses. We have provided examples on how some existing studies fit into it.
Reference: <author> Catinis, L., Caelen, J. </author> <title> (1995) Analyse du comportement multimodal de l'usager humain dans une tache de dessin. </title> <booktitle> Actes des 7emes Journees sur l'Ingenierie de l'Interaction Homme-Machine (IHM'95). </booktitle> <pages> 123-129. </pages> <note> In French. </note>
Reference-contexts: When addressing similar questions, different experiments sometimes get different results. Some experiments observed temporal relationships between speech and gestures, such as temporal coincidence <ref> (Catinis and Caelen 95) </ref> or temporal sequence (Oviatt et al. 97); others did not observe any temporal relationship at all (Mignot and Carbonell 96). <p> A subset of our ideas initially introduced in (Martin and Beroule 93) was renamed "Care" properties by (Coutaz and Nigay 94) and was also used in a Wizard of Oz experiment where the wizard is seen by the subject <ref> (Catinis and Caelen 95) </ref>. 5.3 Conclusion We have described how a theoretical framework that we originally used for building multimodal interfaces could be useful for evaluating multimodal user studies. We have provided a list of questions that should be considered during the analysis phase of these user studies.
Reference: <author> Cheyer, A., Julia, L., Martin, </author> <title> J.C. (1998) A unified framework for constructing multimodal experiments and applications. </title> <booktitle> Proceedings of the Second International Conference on Cooperative Multimodal Communication (CMC'98). </booktitle> <address> Tillburg, the Netherlands, </address> <month> 28-30 January. </month>
Reference-contexts: Furthermore, these types of cooperation between modalities may also provide information about dialogue or the subject's intentions. What we intend to do in the near future is to apply the TYCOON framework to provide data analysis for the multimodal Wizard of Oz experiment described in <ref> (Cheyer et al. 1998) </ref>.
Reference: <author> Coutaz, J., Nigay, L. </author> <title> (1994) Les proprietes CARE dans les interfaces multimodales. </title> <institution> Actes des 6emes Journees sur l'Ingenierie de l'Interaction Homme-Machine (IHM'94), Lille. </institution> <note> 7-14. In French. </note>
Reference-contexts: As we believe that there should be a particular user study for each system, aiming at open theoretical tools for multimodal user studies can be of interest. A subset of our ideas initially introduced in (Martin and Beroule 93) was renamed "Care" properties by <ref> (Coutaz and Nigay 94) </ref> and was also used in a Wizard of Oz experiment where the wizard is seen by the subject (Catinis and Caelen 95). 5.3 Conclusion We have described how a theoretical framework that we originally used for building multimodal interfaces could be useful for evaluating multimodal user studies.
Reference: <author> Denda, A., Itoh, T., Nakagawa, S. </author> <title> (1997) Evaluation of spoken dialogue system for a sightseeing guidance with multi-modal interface. </title> <booktitle> In IJCAI-IMS (1997). </booktitle> <pages> 41-48. </pages>
Reference: <author> Fais, L. </author> <title> (1997) Effects of interface on linguistic behavior in task-oriented, multimodal dialogues. </title> <booktitle> In IJCAI-IMS (1997). </booktitle> <pages> 35-40. </pages>
Reference-contexts: In (Wang et al. 93), it was observed that redundancy between speech output and text display enabled the user to shorten learning time for use of a graphical interface. In (Huls and Bos 95), the efficiency of such a redundant combination was also studied. In <ref> (Fais 97) </ref>, speech output was used along with a graphical synthetic speaking face (although the content may be redundant, some low-level events like phonemes are better transmitted using visual cues than simply audio speech). <p> What looks like a complementarity may in fact be seen as a redundancy when considering the pragmatic knowledge the subject has of the system. Moreover, each type of cooperation may be involved at several levels of abstraction, in either the content or the form. Studies like <ref> (Fais 97, Oviatt et al. 97) </ref> have evaluated the influence (and transfer of information) of the combination of output modalities on the subject's behavior. Transfer means that a chunk of information produced by one modality is used by another modality.
Reference: <author> Gapp, </author> <title> K.P. (1994) From vision to language: a cognitive approach to the computation of spatial relations in 3D space. </title> <booktitle> Proceedings of the First European Conference on Cognitive Science in Industry, Luxembourg. </booktitle> <pages> 339-357. </pages> <note> http://www.dfki.uni-sb.de/vitra/index/node70.html#absb110 Guyomard, </note> <author> M., Le Meur D., Poignonnec, S., Siroux, J. </author> <title> (1995) Experimental work for the dual usage of voice and touch screen for a cartographic application. </title> <booktitle> Proceedings of the ESCA Tutorial and Research Workshop on Spoken Dialogue Systems, </booktitle> <address> Vigso, Denmark. </address> <month> 30 May - 2 June </month> <year> 1995. </year> <pages> 153-156. </pages>
Reference-contexts: In fact, the relationship between the modality used by the subject and the modality used in response by the system can be seen as a transfer. Transfer is also studied when people use speech to describe images, transferring information from visual perception to verbal production <ref> (Gapp 94) </ref>.
Reference: <author> Hare, M., Doubleday, A., Ryan, M., Bennet, I. </author> <title> (1995) Intelligent presentation of information retrieved from heterogeneous multimedia databases. </title> <booktitle> Pre-Proceedings of the First International Workshop on Intelligence and Multimodality in Multimedia Interfaces (IMMI-1), </booktitle> <address> Edinburgh, Scotland, </address> <month> July 13-14. </month>
Reference-contexts: Finally, the spatial overlay of the different visual modalities like text, graphics, or pictures seems to have an impact on the way subjects integrate the chunks of information they receive <ref> (Hare et al. 95) </ref>. 5.2 Using TYCOON in multimodal user studies The distinction between complementarity and redundancy is not always mentioned in user studies. Some authors refer to "combined situations" without giving more details on the relationship between the content of the information transmitted on the different modalities.
Reference: <author> Huls, C., Bos, E. </author> <title> (1995) Studies into full integration of language and action. </title> <booktitle> Proceedings of the International Conference on Cooperative Multimodal Communication(CMC/95), Eindhoven. Part II: </booktitle> <pages> 161-174. </pages> <booktitle> IJCAI-IMS (1997) Proceedings of the IJCAI-97 Workshop on "Intelligent Multimodal Systems", </booktitle> <address> Nagoya, Japan, </address> <note> August 24. http://www.miv.t.u-tokyo.ac.jp/ijcai97-IMS/ Martin, J.C., </note> <author> Beroule, D. </author> <title> (1993) Types et buts de cooperations entre modalites dans les interfaces multimodales. </title> <booktitle> Actes des 5e Journees sur l'Ingenierie de l'Interaction Homme-Machine. </booktitle> <pages> 17-22. </pages> <month> 19-20 October. </month> <note> Lyon. In French. </note>
Reference-contexts: Regarding the goals of cooperation, complementarity may be useful in improving speech recognition (Oviatt et al. 97) or making interaction faster <ref> (Huls and Bos 95) </ref>. 4.3 Does the subject use redundancy? Why? Which criteria should be used by the system to merge the chunks of information? Redundancy means that the subject sends to the computer the same chunks of information by using two (or more) modalities. <p> In (Wang et al. 93), it was observed that redundancy between speech output and text display enabled the user to shorten learning time for use of a graphical interface. In <ref> (Huls and Bos 95) </ref>, the efficiency of such a redundant combination was also studied. In (Fais 97), speech output was used along with a graphical synthetic speaking face (although the content may be redundant, some low-level events like phonemes are better transmitted using visual cues than simply audio speech).
Reference: <author> Martin, J.C., Veldman, R., Beroule, D. </author> <title> (1995) Towards adequate representations technologies for multimodal interfaces. </title> <booktitle> Proceedings of the International Conference on Cooperative Multimodal Communication (CMC'95). Part II: </booktitle> <pages> 207-223. </pages>
Reference: <author> Martin, </author> <title> J.C. (1997) Towards "intelligent" cooperation between modalities. The example of a system enabling multimodal interaction with a map. </title> <booktitle> In IJCAI-IMS (1997). </booktitle> <pages> 63-69. </pages>
Reference-contexts: As our primary aim was the building of multimodal systems, our conceptual framework produced formal notations and two software tools: a specification language and a multimodal module. We have applied these tools as a configuration system for a prototype enabling multimodal interaction with a map <ref> (Martin 97) </ref>. The second dimension of TYCOON is called "goals of cooperation".
Reference: <author> Mignot, C., Carbonell, N. (1996) Commandes orales et gestuelles: </author> <title> Une etude empirique. </title> <journal> Techniques et Sciences Informatiques. </journal> <volume> Vol 15, No 10. </volume> <pages> 1399-1428. </pages> <note> In French. </note>
Reference-contexts: When addressing similar questions, different experiments sometimes get different results. Some experiments observed temporal relationships between speech and gestures, such as temporal coincidence (Catinis and Caelen 95) or temporal sequence (Oviatt et al. 97); others did not observe any temporal relationship at all <ref> (Mignot and Carbonell 96) </ref>. It appears that the type of microphone (phone handset, head microphone, or table microphone) and the medium used for gestures (tactile screen or pen) might modify the multimodal behavior of the subjects, including the role of time. <p> On the other hand, redundancy was very rarely observed in (Oviatt et al. 97). In only 2% of the commands, speech provided duplicate but less precise information about location. Symmetrically, drawn graphics provided partially duplicated information about the type of object followed with more precise speech. In <ref> (Mignot and Carbonel 96) </ref>, continuous gestures were combined with a direct-manipulation spoken style (and high redundancy between both), but discrete gestures were combined with a communication spoken style (and low redundancy between both). <p> User studies can be useful in finding out how much it happens in a user's behavior. In fact, it is seldom observed in a multimodal corpus. Yet, in <ref> (Mignot and Carbonel 96) </ref>, one example was observed: the user was speaking a command when he realized that he had to move a piece of furniture before executing the command, so he used gesture to move the furniture while continuing to speak. 4.6 Does the subject use specialization? Why? Specialization means <p> In (Oviatt et al. 97), 19% of the corpus is multimodal, 17.5% pen-only and 63.5% speech-only. In (Petrelli et al. 97), users with preliminary computer experience performed 84% multimodal input (with shorter written input and transferring part of the reference meaning on the pointing) and nonexperts only 30%. In <ref> (Mignot and Carbonel 96) </ref>, knowledge about the type of command can be used to infer the probability of use of multimodality (58% of rotation commands are expressed multimodally). <p> In <ref> (Mignot and Carbonel 96) </ref>, high interindividual differences between users were observed: two subjects specialized in each modality according to the information such as adding new furniture (speech and discrete gesture) or moving furniture (continuous gesture only) and two other subjects preferred only speech. <p> For instance, a specialization of speech in one type of command may evolve toward equivalence between speech and gesture for this command type. In <ref> (Mignot and Carbonel 96) </ref>, the percentage of multimodal observations increased between the first session and the third one. 5 Discussion 5.1 Multimodal output Although we have focused in this paper on multimodal input to the computer, some multimodal user studies also deal with multimodal output. <p> Some authors refer to "combined situations" without giving more details on the relationship between the content of the information transmitted on the different modalities. This difference is sometimes lost in statistics opposing monomodal and multimodal behaviors. In fact, as mentioned in <ref> (Mignot and Carbonel 96) </ref>, the difference is not always obvious. What looks like a complementarity may in fact be seen as a redundancy when considering the pragmatic knowledge the subject has of the system.
Reference: <author> Oviatt, S., De Angeli, A., Kuhn, K. </author> <title> (1997) Integration and synchronization of input modes during multimodal human-computer interaction. </title> <booktitle> In ACL (1997). </booktitle> <pages> 1-13. </pages>
Reference-contexts: When addressing similar questions, different experiments sometimes get different results. Some experiments observed temporal relationships between speech and gestures, such as temporal coincidence (Catinis and Caelen 95) or temporal sequence <ref> (Oviatt et al. 97) </ref>; others did not observe any temporal relationship at all (Mignot and Carbonell 96). <p> Regarding the goals of cooperation, complementarity may be useful in improving speech recognition <ref> (Oviatt et al. 97) </ref> or making interaction faster (Huls and Bos 95). 4.3 Does the subject use redundancy? Why? Which criteria should be used by the system to merge the chunks of information? Redundancy means that the subject sends to the computer the same chunks of information by using two (or <p> Redundancy was observed in (Guyomard et al. 95, Siroux et al. 97): a confirmative relationship for which the oral syntagma was sufficient was accompanied by a tactile designation that was redundant with the linguistic reference. On the other hand, redundancy was very rarely observed in <ref> (Oviatt et al. 97) </ref>. In only 2% of the commands, speech provided duplicate but less precise information about location. Symmetrically, drawn graphics provided partially duplicated information about the type of object followed with more precise speech. <p> This may help the system to decide whether the type of cooperation brought into play is multimodal or monomodal. In <ref> (Oviatt et al. 97) </ref>, 19% of the corpus is multimodal, 17.5% pen-only and 63.5% speech-only. In (Petrelli et al. 97), users with preliminary computer experience performed 84% multimodal input (with shorter written input and transferring part of the reference meaning on the pointing) and nonexperts only 30%. <p> What looks like a complementarity may in fact be seen as a redundancy when considering the pragmatic knowledge the subject has of the system. Moreover, each type of cooperation may be involved at several levels of abstraction, in either the content or the form. Studies like <ref> (Fais 97, Oviatt et al. 97) </ref> have evaluated the influence (and transfer of information) of the combination of output modalities on the subject's behavior. Transfer means that a chunk of information produced by one modality is used by another modality.
Reference: <author> Petrelli, D., De Angeli, A., Gerbino, W., Cassano G. </author> <title> (1997) Referring in multimodal systems: The importance of user expertise and system features. </title> <booktitle> In ACL (1997). </booktitle> <pages> 14-19. </pages>
Reference-contexts: In (Mignot and Carbonel 96), continuous gestures were combined with a direct-manipulation spoken style (and high redundancy between both), but discrete gestures were combined with a communication spoken style (and low redundancy between both). In <ref> (Petrelli et al. 97) </ref>, when very short labels (one character) were available, users strongly adopted a redundant strategy (they referred to the object in a linguistic way and used pointing, too). There may be a reason that redundant behavior is seldom considered in user studies. <p> This may help the system to decide whether the type of cooperation brought into play is multimodal or monomodal. In (Oviatt et al. 97), 19% of the corpus is multimodal, 17.5% pen-only and 63.5% speech-only. In <ref> (Petrelli et al. 97) </ref>, users with preliminary computer experience performed 84% multimodal input (with shorter written input and transferring part of the reference meaning on the pointing) and nonexperts only 30%.
Reference: <author> Siroux, J., Guyomard, M., Multon, F., Remondeau, C. </author> <title> (1997) Multimodal references in GEORAL TACTILE. </title> <booktitle> In ACL (1997). </booktitle> <pages> 39-43. </pages>
Reference-contexts: Yet, details contrasting possible subtypes of multimodal (either redundant or complementary) and monomodal behavior (equivalence, specialization, concurrency) are not always provided. 4.8 Are there strong differences between individuals? In <ref> (Siroux et al. 97) </ref>, the rate of use of the tactile screen differs a lot from one subject to another (from 2% to 95%).
Reference: <author> Trafton, J. G., Wauchope, K., Stroup J. </author> <title> (1997) Errors and usability of natural language in a multimodal system. </title> <booktitle> In IJCAI-IMS (1997). </booktitle> <pages> 49-53. </pages>
Reference: <author> Wang, E., Shahnvaz, H., Hedman, L., Papadopoulos, K., and Watkinson, N. </author> <title> (1993) A usability evaluation of text and speech redundant help messages on a reader interface. </title> <editor> In G. Salvendy and M. Smith (eds.). </editor> <title> Human-Computer Interaction: Software and Hardware Interfaces. </title> <address> 724 -729. </address>
Reference-contexts: In (Mignot and Carbonel 96), the percentage of multimodal observations increased between the first session and the third one. 5 Discussion 5.1 Multimodal output Although we have focused in this paper on multimodal input to the computer, some multimodal user studies also deal with multimodal output. In <ref> (Wang et al. 93) </ref>, it was observed that redundancy between speech output and text display enabled the user to shorten learning time for use of a graphical interface. In (Huls and Bos 95), the efficiency of such a redundant combination was also studied.
References-found: 19

