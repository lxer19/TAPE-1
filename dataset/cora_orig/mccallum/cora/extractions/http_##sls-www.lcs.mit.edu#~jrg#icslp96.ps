URL: http://sls-www.lcs.mit.edu/~jrg/icslp96.ps
Refering-URL: http://www.sls.lcs.mit.edu/SUMMIT.html
Root-URL: 
Title: A PROBABILISTIC FRAMEWORK FOR FEATURE-BASED SPEECH RECOGNITION 1  
Author: James Glass, Jane Chang, and Michael McCandless 
Address: Cambridge, Massachusetts 02139 USA  
Affiliation: Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Most current speech recognizers use an observation space which is based on a temporal sequence of frames (e.g., Mel-cepstra). There is another class of recognizer which further processes these frames to produce a segment-based network, and represents each segment by fixed-dimensional features. In such feature-based rec-ognizers the observation space takes the form of a temporal network of feature vectors, so that a single segmentation of an utterance will use a subset of all possible feature vectors. In this work we examine a maximum a posteriori decoding strategy for feature-based recognizers and develop a normalization criterion useful for a segment-based Viterbi or A fl search. We report experimental results for the task of phonetic recognition on the TIMIT corpus where we achieved context-independent and context-dependent (using di-phones) results on the core test set of 64.1% and 69.5% respectively. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. Cohen. </author> <title> Segmenting speech using dynamic programming. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 69(5):14301438, </volume> <month> May </month> <year> 1981. </year>
Reference-contexts: In this way, it has similarities with techniques being used in word-spotting, which compare acoustic likelihoods with those of filler models [17, 18, 20]. The likelihood or odds ratio was also used by Cohen to use HMMs for segmenting speech <ref> [1] </ref>. The independence assumption between X and Y made to enable efficient decoding is somewhat suspect since overlapping segments are likely correlated with each other. It would therfore be worth examining alternative methods for modelling the joint XY space.
Reference: 2. <author> V. Digilakis, J. Rohlicek, and M. Ostendorf. </author> <title> ML estimation of a stochastic linear system with the EM algorithm and its application to speech recognition. </title> <journal> IEEE Trans. Speech and Audio Processing, </journal> <volume> 1(4):431442, </volume> <month> October </month> <year> 1993. </year>
Reference-contexts: Marcus for example, predetermines a set of acoustic-phonetic sub-segments, represents each by an observation vector, which is then modelled with an HMM [11]. Other segment-based techniques hypothesize segments, but compute likelihoods on a set of observation frames <ref> [2, 6, 10, 19] </ref>. 2.2. Feature-based Observations In contrast to frame-based approaches, in a feature-based framework, each segment s i is represented by a single fixed-dimensional feature vector x i . Typically, there is an extra stage of processing to convert the frame sequence O to corresponding features.
Reference: 3. <author> J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallet, and N. Dahlgren. </author> <title> The DARPA TIMIT acoustic-phonetic continuous speech corpus CDROM. NTIS order number PB91-505065, </title> <month> October </month> <year> 1990. </year>
Reference-contexts: Again, there is no assumption about whether context-independent or context-dependent (diphone) boundary models are used. 5. EXPERIMENTS Our initial evaluations of this framework were based on phonetic recognition experiments using the TIMIT corpus <ref> [3] </ref>. Models were built using the TIMIT 61 label set and collapsed down to the 39 labels used by others to report recognition results [4, 7, 8, 14, 15, 21].
Reference: 4. <author> W. Goldenthal. </author> <title> Statistical trajectory models for phonetic recognition. </title> <type> Technical report MIT/LCS/TR-642, </type> <institution> MIT Lab. for Computer Science, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Note that since S implies X we can say P (XY jSW ) = P (XY jW ). In practice, most feature-based recognition systems have not estimated a probability for P (XY jW ) but have only estimated the likelihood of X, P (XjW ) <ref> [4, 9, 13, 22] </ref>. The following section discusses one method for estimating P (XY jW ) in an efficient manner. 3. <p> EXPERIMENTS Our initial evaluations of this framework were based on phonetic recognition experiments using the TIMIT corpus [3]. Models were built using the TIMIT 61 label set and collapsed down to the 39 labels used by others to report recognition results <ref> [4, 7, 8, 14, 15, 21] </ref>. Models were trained on the designated training set of 462 speakers, and results are reported on the 24 speaker core test set. <p> Internally, both the CI and CD results (64.1 and 69.5%) represent a significant improvement over our previously reported results of 55.3 and 68.5%, respectively [14]. Our previous CD results were achieved by hypothesizing segment boundaries at every frame and performing an exhaustive segment-based search. Group Description % Accuracy Goldenthal <ref> [4] </ref> Trigram, Triphone STM 69.5 Lamel et al. [7] Bigram, Triphone CDHMM 69.1 Mari et al. [12] Bigram, 2nd order HMM 68.8 Robinson [15] Bigram, Recurrent Network 73.4 SUMMIT Bigram, Diphone 69.5 Table 1: Reported recognition accuracies on the TIMIT core test set.
Reference: 5. <author> L. Hetherington and M. McCandless. SAPPHIRE: </author> <title> An extensible speech analysis and recognition tool based on Tcl/Tk. </title> <booktitle> In these proceedings. </booktitle>
Reference-contexts: Our research was greatly facilitated by SAPPHIRE, a graphical speech analysis and recognition tool based on Tcl/Tk that is being developed in our group <ref> [5] </ref>. SAPPHIRE's flexibility and expressiveness allows us to quickly test novel ideas and frameworks. 5.1. Context-Independent Recognition The first set of experiments we performed used 62 labels (61 TIMIT labels plus the anti-phone not) to explore context-independent (CI) phonetic recognition using segment-based information only.
Reference: 6. <author> W. Holmes and M. Russell. </author> <title> Modeling speech variability with segmental HMMs. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 447450, </pages> <address> Atlanta, GA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Marcus for example, predetermines a set of acoustic-phonetic sub-segments, represents each by an observation vector, which is then modelled with an HMM [11]. Other segment-based techniques hypothesize segments, but compute likelihoods on a set of observation frames <ref> [2, 6, 10, 19] </ref>. 2.2. Feature-based Observations In contrast to frame-based approaches, in a feature-based framework, each segment s i is represented by a single fixed-dimensional feature vector x i . Typically, there is an extra stage of processing to convert the frame sequence O to corresponding features.
Reference: 7. <author> L. Lamel and J.L. Gauvain. </author> <title> High performance speaker-independent phone recognition using CDHMM. </title> <booktitle> In Proc. Eurospeech, </booktitle> <pages> pages 121 124, </pages> <address> Berlin, Germany, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Note that by definition A includes all observations so the denominator term P (A) can be ignored. As mentioned previously, most recognizers use frame-based observations for input to the decoder. Thus all discrete and continuous HMMs, including those using artificial neural networks for classification, fit under this framework <ref> [7, 12, 15, 16, 21] </ref>. Many segment-based techniques also use a common set of fixed observation vectors as well. Marcus for example, predetermines a set of acoustic-phonetic sub-segments, represents each by an observation vector, which is then modelled with an HMM [11]. <p> EXPERIMENTS Our initial evaluations of this framework were based on phonetic recognition experiments using the TIMIT corpus [3]. Models were built using the TIMIT 61 label set and collapsed down to the 39 labels used by others to report recognition results <ref> [4, 7, 8, 14, 15, 21] </ref>. Models were trained on the designated training set of 462 speakers, and results are reported on the 24 speaker core test set. <p> Our previous CD results were achieved by hypothesizing segment boundaries at every frame and performing an exhaustive segment-based search. Group Description % Accuracy Goldenthal [4] Trigram, Triphone STM 69.5 Lamel et al. <ref> [7] </ref> Bigram, Triphone CDHMM 69.1 Mari et al. [12] Bigram, 2nd order HMM 68.8 Robinson [15] Bigram, Recurrent Network 73.4 SUMMIT Bigram, Diphone 69.5 Table 1: Reported recognition accuracies on the TIMIT core test set.
Reference: 8. <author> K.F. Lee and H.W. Hon. </author> <title> Speaker-independent phone recognition us ing hidden Markov models. </title> <journal> IEEE Trans. ASSP, </journal> <volume> 37(11):16411648, </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: EXPERIMENTS Our initial evaluations of this framework were based on phonetic recognition experiments using the TIMIT corpus [3]. Models were built using the TIMIT 61 label set and collapsed down to the 39 labels used by others to report recognition results <ref> [4, 7, 8, 14, 15, 21] </ref>. Models were trained on the designated training set of 462 speakers, and results are reported on the 24 speaker core test set.
Reference: 9. <author> H. Leung, I. Hetherington, and V. Zue. </author> <title> Speech recognition using stochastic segment neural networks. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 613616, </pages> <address> San Francisco, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Note that since S implies X we can say P (XY jSW ) = P (XY jW ). In practice, most feature-based recognition systems have not estimated a probability for P (XY jW ) but have only estimated the likelihood of X, P (XjW ) <ref> [4, 9, 13, 22] </ref>. The following section discusses one method for estimating P (XY jW ) in an efficient manner. 3.
Reference: 10. <author> A. Ljolje. </author> <title> High accuracy phone recognition using context clustering and quasi-triphone models. </title> <booktitle> Computer Speech and Language, </booktitle> <address> 8(2):129151, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Marcus for example, predetermines a set of acoustic-phonetic sub-segments, represents each by an observation vector, which is then modelled with an HMM [11]. Other segment-based techniques hypothesize segments, but compute likelihoods on a set of observation frames <ref> [2, 6, 10, 19] </ref>. 2.2. Feature-based Observations In contrast to frame-based approaches, in a feature-based framework, each segment s i is represented by a single fixed-dimensional feature vector x i . Typically, there is an extra stage of processing to convert the frame sequence O to corresponding features.
Reference: 11. <author> J. Marcus. </author> <title> Phonetic recognition in a segment-based HMM. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 479482, </pages> <address> Minneapolis, MN, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Many segment-based techniques also use a common set of fixed observation vectors as well. Marcus for example, predetermines a set of acoustic-phonetic sub-segments, represents each by an observation vector, which is then modelled with an HMM <ref> [11] </ref>. Other segment-based techniques hypothesize segments, but compute likelihoods on a set of observation frames [2, 6, 10, 19]. 2.2. Feature-based Observations In contrast to frame-based approaches, in a feature-based framework, each segment s i is represented by a single fixed-dimensional feature vector x i .
Reference: 12. <author> J.F. Mari, D. Fohr, and J.C. Junqua. </author> <title> A second-order HMM for high performance word and phoneme-based continuous speech recognition. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 435438, </pages> <address> Atlanta, GA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Note that by definition A includes all observations so the denominator term P (A) can be ignored. As mentioned previously, most recognizers use frame-based observations for input to the decoder. Thus all discrete and continuous HMMs, including those using artificial neural networks for classification, fit under this framework <ref> [7, 12, 15, 16, 21] </ref>. Many segment-based techniques also use a common set of fixed observation vectors as well. Marcus for example, predetermines a set of acoustic-phonetic sub-segments, represents each by an observation vector, which is then modelled with an HMM [11]. <p> Our previous CD results were achieved by hypothesizing segment boundaries at every frame and performing an exhaustive segment-based search. Group Description % Accuracy Goldenthal [4] Trigram, Triphone STM 69.5 Lamel et al. [7] Bigram, Triphone CDHMM 69.1 Mari et al. <ref> [12] </ref> Bigram, 2nd order HMM 68.8 Robinson [15] Bigram, Recurrent Network 73.4 SUMMIT Bigram, Diphone 69.5 Table 1: Reported recognition accuracies on the TIMIT core test set. The word recognition experiments we have performed to date have shown a consistent increase in word accuracy as well.
Reference: 13. <author> M. Ostendorf and S. Roucos. </author> <title> A stochastic segment model for phoneme based continuous speech recognition. </title> <journal> IEEE Trans. ASSP, </journal> <volume> 37(12):1857 1869, </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: Note that since S implies X we can say P (XY jSW ) = P (XY jW ). In practice, most feature-based recognition systems have not estimated a probability for P (XY jW ) but have only estimated the likelihood of X, P (XjW ) <ref> [4, 9, 13, 22] </ref>. The following section discusses one method for estimating P (XY jW ) in an efficient manner. 3.
Reference: 14. <author> M. Phillips and J. Glass. </author> <title> Phonetic transition modelling for continuous speech recognition. </title> <journal> J. Acoust. Soc. Amer., </journal> <volume> 95(5):2877, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: EXPERIMENTS Our initial evaluations of this framework were based on phonetic recognition experiments using the TIMIT corpus [3]. Models were built using the TIMIT 61 label set and collapsed down to the 39 labels used by others to report recognition results <ref> [4, 7, 8, 14, 15, 21] </ref>. Models were trained on the designated training set of 462 speakers, and results are reported on the 24 speaker core test set. <p> Context-Dependent Recognition The second set of experiments we performed used a set of context-dependent (CD) diphone models based on feature vectors extracted at hypothesized landmarks. The feature vector consisted of eight averages of MFCC and energy resulting in a 120 dimensional feature vector <ref> [14] </ref>. PCA was used to normalize the feature space and reduce the dimensionality to 50. A set of 1000 diphone classes (tran-sition and internal) was created based on frequency of occurrence in the training data and simple similarity measures. <p> Internally, both the CI and CD results (64.1 and 69.5%) represent a significant improvement over our previously reported results of 55.3 and 68.5%, respectively <ref> [14] </ref>. Our previous CD results were achieved by hypothesizing segment boundaries at every frame and performing an exhaustive segment-based search.
Reference: 15. <author> A. Robinson. </author> <title> An application of recurrent nets to phone probability estimation. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 5(2):298305, </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: Note that by definition A includes all observations so the denominator term P (A) can be ignored. As mentioned previously, most recognizers use frame-based observations for input to the decoder. Thus all discrete and continuous HMMs, including those using artificial neural networks for classification, fit under this framework <ref> [7, 12, 15, 16, 21] </ref>. Many segment-based techniques also use a common set of fixed observation vectors as well. Marcus for example, predetermines a set of acoustic-phonetic sub-segments, represents each by an observation vector, which is then modelled with an HMM [11]. <p> EXPERIMENTS Our initial evaluations of this framework were based on phonetic recognition experiments using the TIMIT corpus [3]. Models were built using the TIMIT 61 label set and collapsed down to the 39 labels used by others to report recognition results <ref> [4, 7, 8, 14, 15, 21] </ref>. Models were trained on the designated training set of 462 speakers, and results are reported on the 24 speaker core test set. <p> Our previous CD results were achieved by hypothesizing segment boundaries at every frame and performing an exhaustive segment-based search. Group Description % Accuracy Goldenthal [4] Trigram, Triphone STM 69.5 Lamel et al. [7] Bigram, Triphone CDHMM 69.1 Mari et al. [12] Bigram, 2nd order HMM 68.8 Robinson <ref> [15] </ref> Bigram, Recurrent Network 73.4 SUMMIT Bigram, Diphone 69.5 Table 1: Reported recognition accuracies on the TIMIT core test set. The word recognition experiments we have performed to date have shown a consistent increase in word accuracy as well.
Reference: 16. <author> T. Robinson, M. Hochberg, and S. Renals. </author> <title> IPA: Improved phone mod elling with recurrent neural networks. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 3740, </pages> <address> Adelaide, Australia, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Note that by definition A includes all observations so the denominator term P (A) can be ignored. As mentioned previously, most recognizers use frame-based observations for input to the decoder. Thus all discrete and continuous HMMs, including those using artificial neural networks for classification, fit under this framework <ref> [7, 12, 15, 16, 21] </ref>. Many segment-based techniques also use a common set of fixed observation vectors as well. Marcus for example, predetermines a set of acoustic-phonetic sub-segments, represents each by an observation vector, which is then modelled with an HMM [11].
Reference: 17. <author> J. Rohlicek, W. Russell, S. Roucos, and H. Gish. </author> <title> Continuous hidden Markov modelling for speaker-independent word spotting. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 627630, </pages> <address> Glasgow, Scotland, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Note that the anti-phone is not used during lexical access. Its only role is to serve as a form of normalization for the segment scoring. In this way, it has similarities with techniques being used in word-spotting, which compare acoustic likelihoods with those of filler models <ref> [17, 18, 20] </ref>. The likelihood or odds ratio was also used by Cohen to use HMMs for segmenting speech [1]. The independence assumption between X and Y made to enable efficient decoding is somewhat suspect since overlapping segments are likely correlated with each other.
Reference: 18. <author> R. Rose and D. Paul. </author> <title> A hidden Markov model based keyword recog nition system. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 129132, </pages> <address> Albuquerque, NM, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Note that the anti-phone is not used during lexical access. Its only role is to serve as a form of normalization for the segment scoring. In this way, it has similarities with techniques being used in word-spotting, which compare acoustic likelihoods with those of filler models <ref> [17, 18, 20] </ref>. The likelihood or odds ratio was also used by Cohen to use HMMs for segmenting speech [1]. The independence assumption between X and Y made to enable efficient decoding is somewhat suspect since overlapping segments are likely correlated with each other.
Reference: 19. <author> S. Roucos, M. Ostendorf, H. Gish, and A. Derr. </author> <title> Stochastic segment modelling using the Estimate-Maximize algorithm. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 127130, </pages> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: Marcus for example, predetermines a set of acoustic-phonetic sub-segments, represents each by an observation vector, which is then modelled with an HMM [11]. Other segment-based techniques hypothesize segments, but compute likelihoods on a set of observation frames <ref> [2, 6, 10, 19] </ref>. 2.2. Feature-based Observations In contrast to frame-based approaches, in a feature-based framework, each segment s i is represented by a single fixed-dimensional feature vector x i . Typically, there is an extra stage of processing to convert the frame sequence O to corresponding features.
Reference: 20. <author> J. Wilpon, L. Rabiner, C.H. Lee, and E. Goldman. </author> <title> Automatic recogni tion of keywords in unconstrained speech using hidden Markov models. </title> <journal> IEEE Trans. ASSP, </journal> <volume> 38(11):18701878, </volume> <month> November </month> <year> 1990. </year>
Reference-contexts: Note that the anti-phone is not used during lexical access. Its only role is to serve as a form of normalization for the segment scoring. In this way, it has similarities with techniques being used in word-spotting, which compare acoustic likelihoods with those of filler models <ref> [17, 18, 20] </ref>. The likelihood or odds ratio was also used by Cohen to use HMMs for segmenting speech [1]. The independence assumption between X and Y made to enable efficient decoding is somewhat suspect since overlapping segments are likely correlated with each other.
Reference: 21. <author> S. Young and P. Woodland. </author> <title> State clustering in hidden Markov model based continuous speech recognition. </title> <booktitle> Computer Speech and Language, </booktitle> <address> 8(4):369383, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Note that by definition A includes all observations so the denominator term P (A) can be ignored. As mentioned previously, most recognizers use frame-based observations for input to the decoder. Thus all discrete and continuous HMMs, including those using artificial neural networks for classification, fit under this framework <ref> [7, 12, 15, 16, 21] </ref>. Many segment-based techniques also use a common set of fixed observation vectors as well. Marcus for example, predetermines a set of acoustic-phonetic sub-segments, represents each by an observation vector, which is then modelled with an HMM [11]. <p> EXPERIMENTS Our initial evaluations of this framework were based on phonetic recognition experiments using the TIMIT corpus [3]. Models were built using the TIMIT 61 label set and collapsed down to the 39 labels used by others to report recognition results <ref> [4, 7, 8, 14, 15, 21] </ref>. Models were trained on the designated training set of 462 speakers, and results are reported on the 24 speaker core test set.
Reference: 22. <author> V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips, J. Polifroni, and S. Seneff. </author> <title> Recent progress on the SUMMIT system. </title> <booktitle> In Proc. Speech and Natural Language Workshop, </booktitle> <pages> pages 380384, </pages> <address> Hidden Valley, PA, June 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1. INTRODUCTION The SUMMIT speech recognizer developed by our group uses a segment-based framework for its acoustic-phonetic representation of the speech signal <ref> [22] </ref>. Feature vectors are extracted both over hypothesized segments and at their boundaries for phonetic analysis. The resulting observation space (the set of all feature vectors) takes the form of an acoustic-phonetic network, whereby different paths through the network are associated with different sets of feature vectors. <p> Note that since S implies X we can say P (XY jSW ) = P (XY jW ). In practice, most feature-based recognition systems have not estimated a probability for P (XY jW ) but have only estimated the likelihood of X, P (XjW ) <ref> [4, 9, 13, 22] </ref>. The following section discusses one method for estimating P (XY jW ) in an efficient manner. 3. <p> A single parameter (optimized on the development set) controlled the trade-off between insertions and deletions. All utterances were represented by 14 Mel-scale cepstral coefficients (MFCCs) and log energy, computed at 5 msec intervals. Acoustic landmarks were determined by looking for local maxima in spectral change in the MFCCs <ref> [22] </ref>. Segment networks were created by fully connecting landmarks within acoustically stable regions. An analysis of the networks showed that on the development set there were 2.4 boundaries per transcription boundary and 7.0 segments per transcription segment on average.
References-found: 22

