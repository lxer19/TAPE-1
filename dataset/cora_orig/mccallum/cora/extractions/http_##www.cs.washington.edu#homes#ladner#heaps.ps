URL: http://www.cs.washington.edu/homes/ladner/heaps.ps
Refering-URL: http://www.cs.washington.edu/homes/ladner/papers.html
Root-URL: 
Title: The Influence of Caches on the Performance of Heaps  
Author: Anthony LaMarca Richard E. Ladner 
Date: January 6, 1997  
Address: Seattle, WA 98195  
Affiliation: Xerox PARC  University of Washington  
Abstract: As memory access times grow larger relative to processor cycle times, the cache performance of algorithms has an increasingly large impact on overall performance. Unfortunately, most commonly used algorithms were not designed with cache performance in mind. This paper investigates the cache performance of implicit heaps. We present optimizations which significantly reduce the cache misses that heaps incur and improve their overall performance. We present an analytical model called collective analysis that allows cache performance to be predicted as a function of both cache configuration and algorithm configuration. As part of our investigation, we perform an approximate analysis of the cache performance of both traditional heaps and our improved heaps in our model. In addition empirical data is given for five architectures to show the impact our optimizations have on overall performance. We also revisit a priority queue study originally performed by Jones [25]. Due to the increases in cache miss penalties, the relative performance results we obtain on today's machines differ greatly from the machines of only ten years ago. We compare the performance of implicit heaps, skew heaps and splay trees and discuss the difference between our results and Jones's. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, M. Horowitz, and J. Hennessy. </author> <title> An analytical cache model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7:2:184-215, </volume> <year> 1989. </year>
Reference-contexts: An analytical cache model also has the inherent advantage that it helps a designer understand the algorithm and helps uncover possible optimizations. A number of researchers have employed hybrid modeling techniques in which a combination of trace-driven simulation and analytical models is used <ref> [1, 35] </ref>. These techniques compress an address trace into a few key parameters describing an application's behavior, and these are then used to drive an analytical cache model. The difference between the collective analysis framework we present and these techniques is that collective analysis does not involve any trace data.
Reference: [2] <author> R. Agarwal, F. Gustavson, and M. Zubair. </author> <title> Exploiting functional parallelism of POWER2 to design high-performance numerical algorithms. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38:5:563-576, </volume> <month> Sep </month> <year> 1994. </year>
Reference-contexts: Typically, this investigation leads to optimizations that speed up the application by a factor of two or more. Agarwal et al. optimized a library of linear algebra routines by tuning its memory behavior to the Power2 architecture <ref> [2] </ref>. Lebeck and Wood improved the performance of the SPEC benchmarks with cache optimizations [30]. LaMarca and Ladner use empirical and analytical techniques to investigate and improve the performance of sorting algorithms [28, 29].
Reference: [3] <author> A. Aggarwal, K. Chandra, and M. Snir. </author> <title> A model for hierarchical memory. </title> <booktitle> In 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 305-314, </pages> <year> 1987. </year>
Reference-contexts: As well as the computation that is to occur, an algorithm for the uniform memory hierarchy describes when blocks of data should be moved between modules. Another example is the hierarchical memory model <ref> [3] </ref> with a single address space in which an access to location i takes time dlog ie. Efficient programming in this model requires commonly used data items to be copied from high addresses to low addresses during their periods of high use.
Reference: [4] <author> A. Aho, J. Hopcroft, and J. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1974. </year>
Reference-contexts: While execution time is the final measure of performance, it is too coarse-grained a metric to use in the intermediate stages of analysis and optimization of algorithms. The majority of researchers in the algorithm community compare algorithm performance using analyses in a unit-cost model. The RAM model <ref> [4, 12] </ref> is used most commonly, and in this abstract architecture all basic operations, including reads and writes to memory, have unit cost. Unit-cost models have the advantage that they are simple, easy to use, and produce results that are easily compared.
Reference: [5] <author> B. Alpern, L. Carter, E. Feig, and T. Selker. </author> <title> The uniform memory hierarchy model of computation. </title> <journal> Algorithmica, </journal> <volume> 12 2-3:72-109, </volume> <year> 1994. </year>
Reference-contexts: LaMarca and Ladner use empirical and analytical techniques to investigate and improve the performance of sorting algorithms [28, 29]. A number of abstract architectures for sequential machines have been introduced that take into account the memory hierarchies present in modern machines. The uniform memory hierarchy <ref> [5] </ref> models the memory system as a series of increasingly large memory modules. As well as the computation that is to occur, an algorithm for the uniform memory hierarchy describes when blocks of data should be moved between modules.
Reference: [6] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the 1993 ACM Symposium on Programming Languages Design and Implementation, </booktitle> <pages> pages 112-125. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: Efficient programming in this model requires commonly used data items to be copied from high addresses to low addresses during their periods of high use. The compiler community has produced many optimizations for improving the cache locality of code <ref> [9, 6, 43] </ref> and algorithms for deciding when these optimizations should be applied [26, 8, 20]. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
Reference: [7] <author> S. Carlsson. </author> <title> An optimal algorithm for deleting the root of a heap. </title> <journal> Information Processing Letters, </journal> <volume> 37:2:117-120, </volume> <year> 1991. </year>
Reference-contexts: The reference patterns of the priority queue algorithms we examine in the paper are complicated enough that these compiler optimizations offer no benefit. Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps <ref> [27, 13, 7, 21, 19] </ref>. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps [36], have been developed since the heap was introduced. <p> In this paper, we do not consider the implication nor the optimization of other priority queue operations such as reduce-min or the merging of two priority queues. Heaps have been well studied and there are numerous extensions and more sophisticated algorithms for adding and removing <ref> [27, 13, 7, 21, 19] </ref>. 4.1 Optimizing Remove-min For a stream of operations on a heap where the number of adds and the number of removes are roughly equal, the work performed will be dominated by the cost of the removes.
Reference: [8] <author> S. Carr, K. McKinley, and C. W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 252-262, </pages> <year> 1994. </year>
Reference-contexts: The compiler community has produced many optimizations for improving the cache locality of code [9, 6, 43] and algorithms for deciding when these optimizations should be applied <ref> [26, 8, 20] </ref>. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
Reference: [9] <author> M. Cierniak and Wei Li. </author> <title> Unifying data and control transformations for distributed shared-memory machines. </title> <booktitle> In Proceedings of the 1995 ACM Symposium on Programming Languages Design and Implementation, </booktitle> <pages> pages 205-217. </pages> <publisher> ACM, </publisher> <year> 1995. </year>
Reference-contexts: Efficient programming in this model requires commonly used data items to be copied from high addresses to low addresses during their periods of high use. The compiler community has produced many optimizations for improving the cache locality of code <ref> [9, 6, 43] </ref> and algorithms for deciding when these optimizations should be applied [26, 8, 20]. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
Reference: [10] <author> D. Clark. </author> <title> Cache performance of the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1:1:24-37, </volume> <year> 1983. </year>
Reference-contexts: 1 Introduction The time to service a cache miss to memory has grown from 6 cycles for the Vax 11/780 to 120 for the AlphaServer 8400 <ref> [10, 18] </ref>. Cache miss penalties have grown to the point where good overall performance cannot be achieved without good cache performance. Unfortunately, many fundamental algorithms were developed without considering caching. In this paper, we investigate and optimize the cache performance of implicit heaps.
Reference: [11] <author> E. Coffman and P. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1973. </year>
Reference-contexts: Systems with similar features include include Cprof [30] and SHMAP [16]. A study by Rao [33] examines the performance of page replacement policies using the independent reference model <ref> [11] </ref> to predict miss behavior. In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions. The result is that our formulas for cache performance are very similar to Rao's. There are many examples of case studies of the cache performance of specific applications. <p> Collectively, all of the processes represent the accesses to the entire virtual address space and hence represent the system's overall memory behavior. 6 For purposes of the analysis we assume that the references to memory satisfy the independent reference assumption <ref> [11] </ref>. In this model each access is independent of all previous accesses, that is, the system is memoryless. Algorithms which exhibit very regular access patterns such as sequential traversals will not be accurately modeled in our framework because we make the independent reference assumption.
Reference: [12] <author> T. Cormen, C. Leiserson, and R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: While execution time is the final measure of performance, it is too coarse-grained a metric to use in the intermediate stages of analysis and optimization of algorithms. The majority of researchers in the algorithm community compare algorithm performance using analyses in a unit-cost model. The RAM model <ref> [4, 12] </ref> is used most commonly, and in this abstract architecture all basic operations, including reads and writes to memory, have unit cost. Unit-cost models have the advantage that they are simple, easy to use, and produce results that are easily compared. <p> The general consensus is that, due to its low operation count and linear worst case behavior, Floyd's method is the preferred algorithm for building a heap from a set of keys <ref> [34, 40, 12] </ref>. In our experiments, we executed both build-heap algorithms on a set of uniformly distributed keys. As the literature suggests, Floyd's method executes far fewer instructions per key than does Repeated-Adds.
Reference: [13] <author> J. De Graffe and W. Kosters. </author> <title> Expected heights in heaps. BIT, </title> <address> 32:4:570-579, </address> <year> 1992. </year>
Reference-contexts: The reference patterns of the priority queue algorithms we examine in the paper are complicated enough that these compiler optimizations offer no benefit. Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps <ref> [27, 13, 7, 21, 19] </ref>. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps [36], have been developed since the heap was introduced. <p> In this paper, we do not consider the implication nor the optimization of other priority queue operations such as reduce-min or the merging of two priority queues. Heaps have been well studied and there are numerous extensions and more sophisticated algorithms for adding and removing <ref> [27, 13, 7, 21, 19] </ref>. 4.1 Optimizing Remove-min For a stream of operations on a heap where the number of adds and the number of removes are roughly equal, the work performed will be dominated by the cost of the removes.
Reference: [14] <author> E. Doberkat. </author> <title> Inserting a new element into a heap. </title> <journal> BIT, </journal> <volume> 21 </volume> <pages> 225-269, </pages> <year> 1981. </year>
Reference-contexts: Doberkat <ref> [14] </ref> showed that independent of N , if the keys are uniformly distributed, add operations percolate the new item up only 1.6 levels on average. Doberkat [15] also studied the cost of the remove-min operation on a heap chosen at random from the set of legal heaps.
Reference: [15] <author> E. Doberkat. </author> <title> Deleting the root of a heap. </title> <journal> Acta Informatica, </journal> <volume> 17 </volume> <pages> 245-265, </pages> <year> 1982. </year>
Reference-contexts: Doberkat [14] showed that independent of N , if the keys are uniformly distributed, add operations percolate the new item up only 1.6 levels on average. Doberkat <ref> [15] </ref> also studied the cost of the remove-min operation on a heap chosen at random from the set of legal heaps. He showed that after remove-min swaps the last element to the root, the swapped element is percolated down more that (depth 1) levels on average.
Reference: [16] <author> J. Dongarra, O. Brewer, J. Kohl, and S. Fineberg. </author> <title> A tool to aid in the design, implementation, and understanding of matrix algorithms for parallel processors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9:2:185-202, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: Martonosi et al. augment trace-driven cache simulation by categorizing the cache misses by the type of miss and by the name of the data structure incurring the miss [31]. Systems with similar features include include Cprof [30] and SHMAP <ref> [16] </ref>. A study by Rao [33] examines the performance of page replacement policies using the independent reference model [11] to predict miss behavior. In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions.
Reference: [17] <author> M. Farrens, G. Tyson, and A. Pleszkun. </author> <title> A study of single-chip processor/cache organizations for large numbers of transistors. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 338-347, </pages> <year> 1994. </year>
Reference-contexts: Most cache performance analysis is currently done with hardware monitoring [39] or with trace-driven simulation <ref> [17, 41] </ref>. Neither of these solutions offers the benefits of an analytical cache model, namely the ability to quickly obtain estimates of cache performance for varying cache and algorithm configurations.
Reference: [18] <author> D. Fenwick, D. Foley, W. Gist, S. VanDoren, and D. Wissell. </author> <title> The AlphaServer 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal, </journal> <volume> 7:1:43-65, </volume> <year> 1995. </year> <month> 24 </month>
Reference-contexts: 1 Introduction The time to service a cache miss to memory has grown from 6 cycles for the Vax 11/780 to 120 for the AlphaServer 8400 <ref> [10, 18] </ref>. Cache miss penalties have grown to the point where good overall performance cannot be achieved without good cache performance. Unfortunately, many fundamental algorithms were developed without considering caching. In this paper, we investigate and optimize the cache performance of implicit heaps. <p> The third-level cache in the DEC AlphaServer 8400, for example, has a four megabyte capacity and and a miss penalty of 120 cycles <ref> [18] </ref>. Accordingly, the techniques we present in this paper are primarily targeted for algorithms running on architectures with large direct-mapped caches. 2.1 Why Look at Cache Performance? Traditional algorithm design and analysis has, for the most part, ignored caches.
Reference: [19] <author> Robert W. Floyd. </author> <title> Treesort 3. </title> <journal> Communications of the ACM, </journal> <volume> 7:12:701, </volume> <year> 1964. </year>
Reference-contexts: The reference patterns of the priority queue algorithms we examine in the paper are complicated enough that these compiler optimizations offer no benefit. Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps <ref> [27, 13, 7, 21, 19] </ref>. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps [36], have been developed since the heap was introduced. <p> The first algorithm is the obvious and naive way, namely to start with an empty heap and repeatedly perform adds until the heap is built. We call this the Repeated-Adds algorithm. The second algorithm for building a heap is due to Floyd <ref> [19] </ref> and builds a heap in fewer instructions than Repeated-Adds. Floyd's method begins by treating the array of unordered keys as if it were a heap. It then starts half way into the heap and re-heapifies subtrees from the middle up until the entire heap is valid. <p> In this paper, we do not consider the implication nor the optimization of other priority queue operations such as reduce-min or the merging of two priority queues. Heaps have been well studied and there are numerous extensions and more sophisticated algorithms for adding and removing <ref> [27, 13, 7, 21, 19] </ref>. 4.1 Optimizing Remove-min For a stream of operations on a heap where the number of adds and the number of removes are roughly equal, the work performed will be dominated by the cost of the removes.
Reference: [20] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5:5:587-616, </volume> <month> Oct </month> <year> 1988. </year>
Reference-contexts: The compiler community has produced many optimizations for improving the cache locality of code [9, 6, 43] and algorithms for deciding when these optimizations should be applied <ref> [26, 8, 20] </ref>. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
Reference: [21] <author> G. Gonnet and J. Munro. </author> <title> Heaps on heaps. </title> <journal> SIAM Journal of Computing, </journal> <volume> 15:4:964-971, </volume> <year> 1986. </year>
Reference-contexts: The reference patterns of the priority queue algorithms we examine in the paper are complicated enough that these compiler optimizations offer no benefit. Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps <ref> [27, 13, 7, 21, 19] </ref>. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps [36], have been developed since the heap was introduced. <p> In this paper, we do not consider the implication nor the optimization of other priority queue operations such as reduce-min or the merging of two priority queues. Heaps have been well studied and there are numerous extensions and more sophisticated algorithms for adding and removing <ref> [27, 13, 7, 21, 19] </ref>. 4.1 Optimizing Remove-min For a stream of operations on a heap where the number of adds and the number of removes are roughly equal, the work performed will be dominated by the cost of the removes.
Reference: [22] <author> D. Grunwald, B. Zorn, and R. Henderson. </author> <title> Improving the cache locality of memory allocation. </title> <booktitle> In Proceedings of the 1993 ACM Symposium on Programming Languages Design and Implementation, </booktitle> <pages> pages 177-186. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: Jones and the Alphastation we used, our results strongly suggest that cache performance is responsible for the bulk of the difference in execution times. that when cache misses increase significantly execution times also follow suit. 2 There are memory pool implementations that do not incur per-allocation overhead for small objects <ref> [22] </ref>. In OSF/1, however, the default malloc incurs an 8 byte overhead per allocation. 22 Another interesting difference between our results and Jones's is that in our study, the skew heap outperformed the bottom-up splay tree.
Reference: [23] <author> J. Hennesey and D. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: A safe generalization is that most new machines have a multi-level cache architecture where the cache closest to memory is typically direct-mapped and has a high miss penalty <ref> [23] </ref>. The third-level cache in the DEC AlphaServer 8400, for example, has a four megabyte capacity and and a miss penalty of 120 cycles [18]. <p> In our model, the cache is divided into blocks of size B bytes each, where B C and is also a power of two. In order to simplify analysis, we only model direct mapped caches <ref> [23] </ref>, that is to say, we assume that any given block of memory can be loaded into only one location in the cache. Another simplification in our model is that we do not distinguish reads from writes. <p> Another simplification in our model is that we do not distinguish reads from writes. We also assume that items that are contiguous in the virtual address space map to contiguous cache locations, which means that we are modeling a virtually indexed cache <ref> [23] </ref>. Our model does not include a TLB, nor does it attempt to capture page faults due to physical memory limitations. 3.2 Applying the Model The goal of collective analysis is to approximate the memory behavior of an algorithm and predict its cache performance characteristics from this approximation.
Reference: [24] <author> D. B. Johnson. </author> <title> Priority queues with update and finding minimum spanning trees. </title> <journal> Information Processing Letters, </journal> <volume> 4, </volume> <year> 1975. </year>
Reference-contexts: The remove-min operation will no longer load elements into the cache and not look at them. A d-heap is the generalization of a heap with fanout d rather than two. The d-heap was first introduced by Johnson as a way to reduce the cost of the Reduce-Min operation <ref> [24] </ref>. Naor et al. suggest using d-heaps with large d as a way to reduce the number of page faults that heaps incur in a virtual memory environment [32]. Figure 5 shows a 4-heap and the way it lays in the cache when four elements fit per cache block.
Reference: [25] <author> D. Jones. </author> <title> An emperical comparison of priority-queue and event-set implementations. </title> <journal> Communications of the ACM, </journal> <volume> 29:4:300-311, </volume> <year> 1986. </year>
Reference-contexts: Using this performance data as a guide, we develop an improved implicit heap which has better memory system and overall performance. In our experiments, our improved heap incurs as much as 65% fewer cache misses running in the hold model <ref> [25] </ref>, a benchmark that measures how well a priority queue serves as the event set for a simulation. <p> high as 75% for a heap running in the hold model, and performing heapsort with our optimized heaps improved performance by as much as a factor of two. 2 In this paper we also reproduce a subset of the experiments performed by Jones comparing the performance of various priority queues <ref> [25] </ref>. In our experiments, we compare the performance of traditional heaps, our improved heaps, top-down skew heaps and both top-down and bottom-up splay trees. Our comparison of priority queues yielded very different results from Jones's original experiments. <p> This different approach raises opportunities not previously considered and allows us to improve performance beyond existing results. Evaluations of priority queue performance include Jones's study using various architectures and data distributions <ref> [25] </ref>, and a study by Naor et al. which examines instruction counts as well as page fault behavior [32]. 1.4 Overview In Section 2 we describe a typical memory system and motivate the analysis of the cache performance of algorithms. <p> In Section 5 we present data from trace-driven simulations and executions on various architectures to show the impact of our optimizations on both cache performance and overall performance of heaps. In Section 6 we revisit Jones's priority queue study <ref> [25] </ref>. We compare the performance of a traditional heap, an improved heap, skew heaps and splay trees in the hold model using exponentially distributed keys. In Section 7 we present our conclusions. 2 Motivation While machines have varying memory system configurations, almost all share some characteristics. <p> Heaps are often used in discrete event simulations as a priority queue to store the events. In order to measure the performance of heaps operating as an event queue, we analyze our heaps in the hold model <ref> [25] </ref>. In the hold model, 1 It has been noticed previously that increasing a heap's fanout can reduce the instruction count of its operations [27, Ex. 28 Pg. 158][12, Ex. 7-2 Pg. 152]. 10 the heap is initially seeded with some number of keys. <p> Our experiments are intended to be a reproduction of a subset of the priority queue study comparison performed by Jones in 1986 <ref> [25] </ref>. The results of Jones's experiments indicated that for the architecture of that time, pointer-based, self-balancing priority queues such as splay trees and skew heaps performed better than the simple implicit heaps. <p> In these experiments we use an exponential random increment rather than a uniform increment in order to match the distribution used by Jones <ref> [25] </ref>. As before, the mean of the increment distribution was set to 40 times the number of events in the queue (see Section 4.5). In our experiments we compared a traditional heap, a cache-aligned 4-heap, a top-down skew heap and both a top-down and a bottom-up splay tree. <p> In our framework, we performed a cache analysis of d-heaps, and our performance predictions closely match the results of a trace-driven simulation. Finally, we reproduced a subset of Jones's experiments <ref> [25] </ref> examining the performance of a number of priority queues. Our experiments showed that the low memory overhead of implicit heaps makes them an excellent choice as a priority queue, somewhat contradicting Jones's results.
Reference: [26] <author> K. Kennedy and K. McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 323-334, </pages> <year> 1992. </year>
Reference-contexts: The compiler community has produced many optimizations for improving the cache locality of code [9, 6, 43] and algorithms for deciding when these optimizations should be applied <ref> [26, 8, 20] </ref>. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
Reference: [27] <author> D. E. Knuth. </author> <title> The Art of Computer Programming, vol III Sorting and Searching. </title> <address> Addison-Wesely, Reading, MA, </address> <year> 1973. </year>
Reference-contexts: The reference patterns of the priority queue algorithms we examine in the paper are complicated enough that these compiler optimizations offer no benefit. Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps <ref> [27, 13, 7, 21, 19] </ref>. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps [36], have been developed since the heap was introduced. <p> In this paper, we do not consider the implication nor the optimization of other priority queue operations such as reduce-min or the merging of two priority queues. Heaps have been well studied and there are numerous extensions and more sophisticated algorithms for adding and removing <ref> [27, 13, 7, 21, 19] </ref>. 4.1 Optimizing Remove-min For a stream of operations on a heap where the number of adds and the number of removes are roughly equal, the work performed will be dominated by the cost of the removes. <p> In order to measure the performance of heaps operating as an event queue, we analyze our heaps in the hold model [25]. In the hold model, 1 It has been noticed previously that increasing a heap's fanout can reduce the instruction count of its operations <ref> [27, Ex. 28 Pg. 158] </ref>[12, Ex. 7-2 Pg. 152]. 10 the heap is initially seeded with some number of keys.
Reference: [28] <author> A. LaMarca. </author> <title> Caches and algorithms. </title> <type> Ph.D. Dissertation, </type> <institution> University of Washington, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Lebeck and Wood improved the performance of the SPEC benchmarks with cache optimizations [30]. LaMarca and Ladner use empirical and analytical techniques to investigate and improve the performance of sorting algorithms <ref> [28, 29] </ref>. A number of abstract architectures for sequential machines have been introduced that take into account the memory hierarchies present in modern machines. The uniform memory hierarchy [5] models the memory system as a series of increasingly large memory modules.
Reference: [29] <author> A. LaMarca and R.E. Ladner. </author> <title> The influence of caches on the performance of sorting. </title> <type> Technical Report 96-10-01, </type> <institution> University of Washington, Department of Computer Science and Engineering, </institution> <year> 1992. </year> <booktitle> Also appears in the Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: Lebeck and Wood improved the performance of the SPEC benchmarks with cache optimizations [30]. LaMarca and Ladner use empirical and analytical techniques to investigate and improve the performance of sorting algorithms <ref> [28, 29] </ref>. A number of abstract architectures for sequential machines have been introduced that take into account the memory hierarchies present in modern machines. The uniform memory hierarchy [5] models the memory system as a series of increasingly large memory modules.
Reference: [30] <author> A. Lebeck and D. Wood. </author> <title> Cache profiling and the spec benchmarks: a case study. </title> <booktitle> Computer, </booktitle> <address> 27:10:15-26, </address> <month> Oct </month> <year> 1994. </year>
Reference-contexts: Martonosi et al. augment trace-driven cache simulation by categorizing the cache misses by the type of miss and by the name of the data structure incurring the miss [31]. Systems with similar features include include Cprof <ref> [30] </ref> and SHMAP [16]. A study by Rao [33] examines the performance of page replacement policies using the independent reference model [11] to predict miss behavior. In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions. <p> Agarwal et al. optimized a library of linear algebra routines by tuning its memory behavior to the Power2 architecture [2]. Lebeck and Wood improved the performance of the SPEC benchmarks with cache optimizations <ref> [30] </ref>. LaMarca and Ladner use empirical and analytical techniques to investigate and improve the performance of sorting algorithms [28, 29]. A number of abstract architectures for sequential machines have been introduced that take into account the memory hierarchies present in modern machines.
Reference: [31] <author> M. Martonosi, A. Gupta, and T. Anderson. Memspy: </author> <title> analyzing memory system bottlenecks in programs. </title> <booktitle> In Proceedings of the 1992 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 1-12, </pages> <year> 1992. </year>
Reference-contexts: Martonosi et al. augment trace-driven cache simulation by categorizing the cache misses by the type of miss and by the name of the data structure incurring the miss <ref> [31] </ref>. Systems with similar features include include Cprof [30] and SHMAP [16]. A study by Rao [33] examines the performance of page replacement policies using the independent reference model [11] to predict miss behavior.
Reference: [32] <author> D. Naor, C. Martel, and N. Matloff. </author> <title> Performance of priority queue structures in a virtual memory environment. </title> <journal> Computer Journal, </journal> <volume> 34:5:428-437, </volume> <month> Oct </month> <year> 1991. </year>
Reference-contexts: This different approach raises opportunities not previously considered and allows us to improve performance beyond existing results. Evaluations of priority queue performance include Jones's study using various architectures and data distributions [25], and a study by Naor et al. which examines instruction counts as well as page fault behavior <ref> [32] </ref>. 1.4 Overview In Section 2 we describe a typical memory system and motivate the analysis of the cache performance of algorithms. In Section 3 we introduce collective analysis, a framework for predicting the cache performance of algorithms. <p> The d-heap was first introduced by Johnson as a way to reduce the cost of the Reduce-Min operation [24]. Naor et al. suggest using d-heaps with large d as a way to reduce the number of page faults that heaps incur in a virtual memory environment <ref> [32] </ref>. Figure 5 shows a 4-heap and the way it lays in the cache when four elements fit per cache block. Unlike our previous optimization, this change will definitely have an impact on the dynamic instruction count of our heap operations.
Reference: [33] <author> G. Rao. </author> <title> Performance analysis of cache memories. </title> <journal> Journal of the ACM, </journal> <volume> 25:3:378-395, </volume> <year> 1978. </year>
Reference-contexts: Martonosi et al. augment trace-driven cache simulation by categorizing the cache misses by the type of miss and by the name of the data structure incurring the miss [31]. Systems with similar features include include Cprof [30] and SHMAP [16]. A study by Rao <ref> [33] </ref> examines the performance of page replacement policies using the independent reference model [11] to predict miss behavior. In collective analysis, we make assumptions similar to Rao's about the distribution of references within cache regions. The result is that our formulas for cache performance are very similar to Rao's. <p> An access is a miss if it is not a hit. The following theorem is a reformulation of the results of Rao <ref> [33] </ref>. Our formulation differs from Rao's only in that we group together blocks into a region if the accesses are uniformly distributed in the region. This, however, simplifies the analysis considerably.
Reference: [34] <author> R. Sedgewick. </author> <title> Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: The general consensus is that, due to its low operation count and linear worst case behavior, Floyd's method is the preferred algorithm for building a heap from a set of keys <ref> [34, 40, 12] </ref>. In our experiments, we executed both build-heap algorithms on a set of uniformly distributed keys. As the literature suggests, Floyd's method executes far fewer instructions per key than does Repeated-Adds.
Reference: [35] <author> J.P. Singh, H.S. Stone, and D.F. Thiebaut. </author> <title> A model of workloads and its use in miss-rate prediction for fully associative caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41:7:811-825, </volume> <year> 1992. </year>
Reference-contexts: An analytical cache model also has the inherent advantage that it helps a designer understand the algorithm and helps uncover possible optimizations. A number of researchers have employed hybrid modeling techniques in which a combination of trace-driven simulation and analytical models is used <ref> [1, 35] </ref>. These techniques compress an address trace into a few key parameters describing an application's behavior, and these are then used to drive an analytical cache model. The difference between the collective analysis framework we present and these techniques is that collective analysis does not involve any trace data.
Reference: [36] <author> D. Sleator and R. Tarjan. </author> <title> Self-adjusting binary search trees. </title> <journal> Journal of the ACM, </journal> <volume> 32:3:652-686, </volume> <year> 1985. </year>
Reference-contexts: Priority queues and heaps in particular have been well studied. Improved algorithms have been developed for building, adding and removing from heaps [27, 13, 7, 21, 19]. A number of pointer-based, self-balancing priority queues, including splay trees and skew heaps <ref> [36] </ref>, have been developed since the heap was introduced. The main difference 3 between the work in this paper and previous work is that we focus on cache performance as well as instruction cost. <p> The implementations of the skew heap and the bottom-up splay tree were taken from an archive of the code used in Jones's study. The top-down splay tree implementation is an adoption of Sleator and Tarjan's code <ref> [36] </ref>. As Jones did in his study, the queues were run in the hold model, and no work was performed between the remove-min and the add of each iteration (w = 0). In our experiments, a queue element consisted of an 8 byte key and no data.
Reference: [37] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the 1994 ACM Symposium on Programming Languages Design and Implementation, </booktitle> <pages> pages 196-205. </pages> <publisher> ACM, </publisher> <year> 1994. </year> <month> 25 </month>
Reference-contexts: B ) + w Similar to the derivation of Equation 6, an expression for can be derived by substituting these intensities into Equation 1 and reducing. 4.4 Experimental Apparatus Throughout this paper, both our dynamic instruction counts and our cache simulation results were measured using Atom <ref> [37] </ref>. Atom is a flexible toolkit developed by DEC for instrumenting codes on Alpha workstations. Dynamic instruction counts were obtained by using Atom to insert an increment to an instruction counter after each instruction executed by the algorithm. <p> Our executions were run on a DEC Alphastation 250. Dynamic instruction counts and cache performance data was collected using Atom <ref> [37] </ref> (see Section 4.4). <p> The queue was then measured for 200,000 iterations. As before, executions were run on a DEC Alphastation 250, and cache simulations were configured for a two megabyte direct-mapped cache and a 32 byte block size. Dynamic instruction counts and cache performance data was collected using Atom <ref> [37] </ref> (see Section 4.4). Execution times were collected with the Alpha's cycle timer and represent the median of 15 executions.
Reference: [38] <author> O. Temam, C. Fricker, and W. Jalby. </author> <title> Cache interference phenomena. </title> <booktitle> In Proceedings of the 1994 ACM SIGMET--RICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 261-271, </pages> <year> 1994. </year>
Reference-contexts: The behavior of an algorithm is explicitly stated, and this serves as input to our model. While this makes the analysis more difficult to perform, it offers the advantage that our model can provide fast predictions for a variety of algorithm configurations and cache configurations. Temam, Fricker and Jalby <ref> [38] </ref> provide a purely analytical approach for predicting conflict misses in loop nests of numerical codes. In their model, the memory reference patterns are examined, and cache performance is predicted by determining when each data item is reused and how often this reuse is disrupted.
Reference: [39] <author> R. Uhlig, D. Nagle, T. Stanley, T. Mudge, S. Sechrest, and R. Brown. </author> <title> Design tradeoffs for software-managed TLBs. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12:3:175-205, </volume> <year> 1994. </year>
Reference-contexts: Most cache performance analysis is currently done with hardware monitoring <ref> [39] </ref> or with trace-driven simulation [17, 41]. Neither of these solutions offers the benefits of an analytical cache model, namely the ability to quickly obtain estimates of cache performance for varying cache and algorithm configurations.
Reference: [40] <author> M. Weiss. </author> <title> Data structures and algorithm analysis. </title> <publisher> Benjamin/Cummings Pub. Co., </publisher> <address> Redwood City, CA, </address> <year> 1995. </year>
Reference-contexts: The general consensus is that, due to its low operation count and linear worst case behavior, Floyd's method is the preferred algorithm for building a heap from a set of keys <ref> [34, 40, 12] </ref>. In our experiments, we executed both build-heap algorithms on a set of uniformly distributed keys. As the literature suggests, Floyd's method executes far fewer instructions per key than does Repeated-Adds.
Reference: [41] <author> H. Wen and J. L. Baer. </author> <title> Efficient trace-driven simulation methods for cache performance analysis. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9:3:222-241, </volume> <year> 1991. </year>
Reference-contexts: Most cache performance analysis is currently done with hardware monitoring [39] or with trace-driven simulation <ref> [17, 41] </ref>. Neither of these solutions offers the benefits of an analytical cache model, namely the ability to quickly obtain estimates of cache performance for varying cache and algorithm configurations.
Reference: [42] <author> J. W. Williams. </author> <title> Heapsort. </title> <journal> Communications of the ACM, </journal> <volume> 7:6:347-348, </volume> <year> 1964. </year>
Reference-contexts: When both analytical predictions and simulation results are available, we compare them to validate the accuracy of the predictions. 1.2 Implicit Heaps In this paper, we perform a study of the cache performance of implicit heaps <ref> [42] </ref>. The main goal of our study is to understand and improve the memory system performance of heaps using both experimental and analytical tools. We evaluate the performance of heaps as both a tool for sorting as well as the event queue in a discrete event simulation. <p> The 8-heap executes more instructions that the 4-heap and consequently executes slower initially. Eventually, however, the reduction in cache misses overcomes the difference in instruction count, and the 8-heap performs best for large data sets. 5.2 Heapsort We next examine the effects our improvements have on heapsort <ref> [42] </ref>. We compare heapsort built from traditional heaps and our aligned d-heaps. The traditional heaps were built using Floyd's method.
Reference: [43] <author> M. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the 1991 ACM Symposium on Programming Languages Design and Implementation, </booktitle> <pages> pages 30-44. </pages> <publisher> ACM, </publisher> <year> 1991. </year> <month> 26 </month>
Reference-contexts: Efficient programming in this model requires commonly used data items to be copied from high addresses to low addresses during their periods of high use. The compiler community has produced many optimizations for improving the cache locality of code <ref> [9, 6, 43] </ref> and algorithms for deciding when these optimizations should be applied [26, 8, 20]. The vast majority of these compiler optimizations focus on loop nests and offer little improvement to pointer-based structures and algorithms whose reference patterns are difficult to predict.
References-found: 43

