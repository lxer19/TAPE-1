URL: http://www.cs.toronto.edu/~chow/papers/apo94.ps.gz
Refering-URL: http://www.cs.toronto.edu/~chow/interests.html
Root-URL: 
Email: chow@cs.toronto.edu  
Title: Asynchronous Parallel Optimization  
Author: Kwok L. Chow 
Address: Ontario, Canada M5S 1A4  
Affiliation: Department of Computer Science University of Toronto, Toronto  
Abstract: It is generally believed that synchronization and communication delays are the major sources of performance degradation of synchronous iterative algorithms. It is rather controversial that asynchronous implementations have the potential to reduce the overhead to minimum and to accelerate the convergence. This paper seeks to examine the effect of removing synchronization points from a parallel implementation of a simple iterative algorithm, namely Jacobi's method for linear systems, on the KSR1 multiprocessor machine. Computational results showed that the asynchronous implementation outperforms its synchronous counterpart. Since parallel unconstrained optimization algorithms are also iterative in character, one would expect to take the same advantage by incorporating asynchronism in parallel implementations. Thus, based on the same principle, we propose an asynchronous algorithmic model for parallel unconstrained optimization. We will discuss some features of this conceptual model and its use in our future research works on parallel unconstrained optimization. 
Abstract-found: 1
Intro-found: 1
Reference: [BF92] <author> J.M. Bull and T.L. Freeman. </author> <title> Numerical performance of asynchronous Jacobi iteration. </title> <type> Technical Report UMIST NA report No. 210, </type> <institution> Department of Mathematics, University of Manchester, </institution> <month> Febuary </month> <year> 1992. </year>
Reference-contexts: Compared with the results obtained from both synchronous and partially asynchronous Jacobi algorithms, it is clearly that totally asynchronous implementation out performs its synchronous counterpart and is as good as the partially asynchronous version. Similar experiments for testing total asynchronism have been conducted by Bull and Freeman <ref> [BF92] </ref> on the Intel iPSC/2 and iPSC/860 hypercubes. Unlike KSR1, both iPSC/2 and iPSC/860 hypercubes are distributed memory multiprocessors. Hence, their implementation required message passing primitives.
Reference: [BT89] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and distributed computation : Numerical methods. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: The discussion in the remainder of this paper is mainly oriented towards parallel computation on multiprocessors in which the specific type of architecture is relatively unimportant. Moreover, two types of parallel algorithms for multiprocessors <ref> [Kun76, BT89] </ref>, i.e. synchronous and asynchronous algorithms, will be discussed. Iterative algorithms are suitable for implementation on a parallel computer. A simple way of parallelizing iterative algorithms is to assign a single component or a block of components of the iterate to each processor. <p> Gauss-Seidel algorithms are often preferable because they incorporate the newest available information, and for this reason, they sometimes converge faster than the corresponding Jacobi algorithms. This idea is supported by the Stein-Rosenberg theorem <ref> [BT89] </ref>. The theorem refers to linear systems and states that, under appropriate assumptions, if the Jacobi algorithm converges, then the corresponding Gauss-Seidel algorithm also converges and its convergence rate is no worse than that of the Jacobi algorithm. <p> Moreover, the detection of termination tends to be somewhat more difficult for asynchronous than for synchronous algorithms. Some concrete examples could be found in the references <ref> [BT89, LHM93] </ref>. An interesting fact is that some asynchronous algorithms, called totally asynchronous or chaotic, can tolerate arbitrarily large communication and computation delays, while other called partially asynchronous, are not guaranteed to work unless there is an upper bound on these delays. <p> Sufficient conditions for the totally asynchronous algorithms to converge are described by the following convergence theorem. Theorem 2.1 (Totally asynchronous convergence theorem <ref> [BT89] </ref>) Suppose that, for k = 1; ; p, there is a sequence fX j k j j = 0; g of nonempty subsets of X k , such that * X k X k for all j 0. * The sets X j = Q p j k have the <p> Theorem 2.2 (Partially asynchronous convergence theorem 2 <ref> [BT89] </ref>) Suppose that F is continuous, the partial asynchronism assumption holds, and there is a positive integer t fl and a continuous function d : Z ! [0; 1) with the following properties * For every z 0 62 Z fl , we have d (z t fl ) &lt; d
Reference: [Cho93] <author> K. L. Chow. </author> <title> Parallel unconstrained optimization. </title> <type> Depth Paper, </type> <institution> Department of Computer Sciences, University of Toronto, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: synchronous Byrd-Schnabel-Shultz 1988 synchronous Still 1991 synchronous Second derivative Pierre 1973 synchronous Fischer-Ritter 1988 asynchronous Lootsma 1991 asynchronous Hybrid Ferris-Mangasarian 1993 synchronous/asynchronous Table 16: Summary of the existing parallel algorithms 23 be found in the Table 16 and the detail descriptions of each algorithm can be found in the reference <ref> [Cho93] </ref>. However, the challenge in designing parallel algorithms for solving the optimization problem is limited by the common approach, i.e. parallelization of a single iteration. So, the end of a single iteration acts as a synchronization point. This type of parallel iterative algorithm is referred to as synchronous iterative algorithm. <p> It can be shown that this inequality holds by the convexity of f (see also p.7 of [Man93]). 2 By comparing Theorem 3.3 with the parallel convergence theorem of the PGD model (c.f. p.6 of [Man93] or p.30 of <ref> [Cho93] </ref>), we observe that the latter is just a special case of the former when p = 1.
Reference: [GLL89] <author> L. Grippo, F. Lampariello, and S. Lucidi. </author> <title> A truncated Newton method with nonmonotone line search for unconstrained optimization. </title> <journal> Journal of optimization theory and applications, </journal> <volume> 60(3) </volume> <pages> 401-419, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: From the topological properties of a compact and convex set, we know that the number of stationary points of f in is finite. This implies the sequence fx j j j = 0; g converges (c.f. Theorem 2.1 of <ref> [GLL89] </ref>). Thus, the corresponding triple sequence f (x j ; s j ; j ) j j = 0; g converges. This completes the proof. 2 We now have the parallel convergence theorem described as follows.
Reference: [GMW81] <author> P.E. Gill, W. Murray, and M.H. Wright. </author> <title> Practical optimization. </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: Gill et al. <ref> [GMW81] </ref>).
Reference: [Kun76] <author> H.T. Kung. </author> <title> Synchronized and asynchronous parallel algorithm for multiprocessors. Algorithms and Complexity : New directions and Recent Results (J.F. Traub, </title> <publisher> ed.), </publisher> <pages> pages 153-200, </pages> <year> 1976. </year>
Reference-contexts: The discussion in the remainder of this paper is mainly oriented towards parallel computation on multiprocessors in which the specific type of architecture is relatively unimportant. Moreover, two types of parallel algorithms for multiprocessors <ref> [Kun76, BT89] </ref>, i.e. synchronous and asynchronous algorithms, will be discussed. Iterative algorithms are suitable for implementation on a parallel computer. A simple way of parallelizing iterative algorithms is to assign a single component or a block of components of the iterate to each processor.
Reference: [LHM93] <author> E.J. Lu, M.G. Hilgers, and B. McMillin. </author> <title> Asynchronous parallel schemes : A survey. </title> <type> Technical Report CSC 93-19, </type> <institution> Department of Computer Science, University of Missouri-Rolla, </institution> <month> Novermber </month> <year> 1993. </year>
Reference-contexts: Moreover, the detection of termination tends to be somewhat more difficult for asynchronous than for synchronous algorithms. Some concrete examples could be found in the references <ref> [BT89, LHM93] </ref>. An interesting fact is that some asynchronous algorithms, called totally asynchronous or chaotic, can tolerate arbitrarily large communication and computation delays, while other called partially asynchronous, are not guaranteed to work unless there is an upper bound on these delays.
Reference: [Man93] <author> O. L. Mangasarian. </author> <title> Parallel gradient distribution in unconstrained optimization. </title> <type> Technical Report Technical Report #1145, </type> <institution> Department of Computer Sciences, University of Wisconsin-Madison, WI, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Theorem 3.1 (Serial convergence theorem <ref> [Man93] </ref>) Let f 2 C 1 (R n ) and x 0 be the initial estimate of the minimizer. <p> The better model should provide a more feasible way to design, synthesize, and analyze new parallel algorithms for unconstrained optimization. For this purpose, we try to simulate the parallel gradient distribution model, or simply PGD model, proposed recently by Mangasarian <ref> [Man93] </ref>. The basic idea behind the PGD model is that, for each iteration, the model enables each of the available processors to activate concurrently and independently of the others a serial unconstrained optimization algorithm. <p> Lemma 3.2 Every PGD action A P GD is a descent action. Proof : It suffices to show that the PGD action satisfies the descent action property f ( (x)) f (x). It can be shown that this inequality holds by the convexity of f (see also p.7 of <ref> [Man93] </ref>). 2 By comparing Theorem 3.3 with the parallel convergence theorem of the PGD model (c.f. p.6 of [Man93] or p.30 of [Cho93]), we observe that the latter is just a special case of the former when p = 1. <p> It can be shown that this inequality holds by the convexity of f (see also p.7 of <ref> [Man93] </ref>). 2 By comparing Theorem 3.3 with the parallel convergence theorem of the PGD model (c.f. p.6 of [Man93] or p.30 of [Cho93]), we observe that the latter is just a special case of the former when p = 1.
Reference: [Roc72] <author> R.T. Rockafellar. </author> <title> Convex analysis. </title> <publisher> Princeton University Press, </publisher> <year> 1972. </year> <note> i </note>
Reference-contexts: This implies that all the elements of in sequence fx j j j = 0; g remain in . Since the function f is a convex function, the level set defined by f for any x 0 is a convex set (c.f. p.28-29 of <ref> [Roc72] </ref>). From the topological properties of a compact and convex set, we know that the number of stationary points of f in is finite. This implies the sequence fx j j j = 0; g converges (c.f. Theorem 2.1 of [GLL89]).
References-found: 9

