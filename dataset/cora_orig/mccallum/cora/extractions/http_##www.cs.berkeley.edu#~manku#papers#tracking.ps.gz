URL: http://www.cs.berkeley.edu/~manku/papers/tracking.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~manku/papers-abstracts.html
Root-URL: 
Email: email: suban@cse.iitd.ernet.in  
Title: Object Tracking using Affine Multiple Views Geometry  
Author: Gurmeet Singh Manku Himanshu Nautiyal Subhashis Banerjee 
Keyword: Object tracking, Kalman filter, affine camera, affine structure.  
Date: February 22, 1996  
Address: New Delhi 110016  
Affiliation: Department of Computer Science and Engineering Indian Institute of Technology  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Bar-Shalom and T. E. Fortmann. </author> <title> Tracking and Data Association. </title> <publisher> Academic Press., </publisher> <year> 1988. </year>
Reference-contexts: The overall algorithm for matching is: For all matched corners: * Predict the position of each corner in the new frame, using the standard Kalman filter prediction equations <ref> [1] </ref> and define a search area, whose size depends upon the predicted error covariance matrix P . * Search the area for corners and make a list of candidates. * For each candidate, carry out a cross-correlation with the corresponding neighborhood (size is user-defined, typically 5 fi 5) of the original <p> Update its Kalman filter's state vector and state covariance using the standard equations <ref> [1] </ref>. * If no candidate was finally selected, delete this corner from the list of matched corners.
Reference: [2] <author> S. T. Barnard and W. B. Thompson. </author> <title> Disparity Analysis of Images. </title> <journal> In IEEE Trans. on Pattern Anal. and Machine Intell., </journal> <pages> pages 333-340, </pages> <year> 1980. </year>
Reference-contexts: In this paper we present a robust and computationally efficient corner tracking algorithm. Tracking and computation of structure and motion have been treated in the past as two separate problems. On one hand, traditional approaches to tracking have been based primarily on intensity correlation matching <ref> [2] </ref>, with the more sophisticated approaches employing recursive Kalman filtering [11] for temporal consistency. Little attempt has been made to impose structural constraints on the features being tracked. On the other hand most approaches to structure and motion parameter estimation have assumed that reliable correspondences are already available [7, 15].
Reference: [3] <author> Adi Ben-Israel and Thomas N. E. Greville. </author> <title> Generalized Inverses: Theory and Applications. </title> <publisher> John Wiley & Sons., </publisher> <year> 1974. </year>
Reference-contexts: The value of * = i;j w 2 2P F measures the average pixel-error in each observation. In fact, W T is the best rank-3 approximation to W that minimizes * <ref> [3] </ref>. Thus, the affine structure we compute is the best in a least squares sense. 4 Overall Algorithm 1. Find matches for corners in the previous frame by using the Kalman filter algorithm described in Section 2.1.
Reference: [4] <author> J. L. Carter and M. N. Wegman. </author> <title> Universal Classes of Hash Functions. </title> <journal> In J. Comput. System Sci., </journal> <pages> pages 143-154, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: To search a given neighborhood for possible candidates, we need to access the corners in the least possible time. The fastest method is to use hashing, which we have implemented using a Universal class of hash functions, as proposed by Wegman and Carter <ref> [4] </ref>, which ensure that the expected time for k searches is a linear function of k, averaged over all the functions belonging to that class. 5 Results We have implemented an off-line version of our algorithm using the HORATIO image-processing library [9] and carried out extensive off-line experimentation on image sequences
Reference: [5] <author> O. D. Faugeras. </author> <title> What can be seen in Three Dimensions with an Uncalibrated Stereo Ring? In Proc. </title> <booktitle> European Conf. on Computer Vision, </booktitle> <address> Santa Margherita, Italy, </address> <pages> pages 563-576, </pages> <year> 1992. </year>
Reference-contexts: We are presently experimenting with such a method. Finally, we would like to point out that most of the ideas presented in the paper can be extended to deal with perspective cases using projective structure (which requires a 5 point basis) and projective epipolar geometry <ref> [5] </ref>.
Reference: [6] <author> C. Harris and M. Stephens. </author> <title> A Combined Corner and Edge Detector. </title> <booktitle> In Proc. 4th Alvey Vision Conf., </booktitle> <pages> pages 153-158, </pages> <year> 1988. </year> <month> 15 </month>
Reference-contexts: Corners (feature points) are easily computed, and are small in number compared to the overall image size, making corner tracking algorithms feasible for real time implementations [11]. However, the primary problem in point tracking arises due to the localization and detection errors of most corner detectors <ref> [16, 6] </ref>, making it necessary to design tracking algorithms which are insensitive to such errors. In this paper we present a robust and computationally efficient corner tracking algorithm. Tracking and computation of structure and motion have been treated in the past as two separate problems.
Reference: [7] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> Affine Structure from Motion. </title> <journal> In Journal of the Optical Society of America., </journal> <pages> pages 377-385, </pages> <year> 1991. </year>
Reference-contexts: Little attempt has been made to impose structural constraints on the features being tracked. On the other hand most approaches to structure and motion parameter estimation have assumed that reliable correspondences are already available <ref> [7, 15] </ref>. The successes of these approaches are crucially dependent on the correctness of the assumed correspondences. In contrast, our work attempts to unify feature tracking and structure computation and provide solutions to both, each of which aids the other.
Reference: [8] <author> C.H. Lee and T. Huang. </author> <title> Finding Point Correspondences and Determining Motion of a Rigid Object from Two Weak Perspective Views. </title> <booktitle> Computer Vision Graphics and Image Processing, </booktitle> <volume> 52 </volume> <pages> 309-327, </pages> <year> 1990. </year>
Reference-contexts: In contrast, our work attempts to unify feature tracking and structure computation and provide solutions to both, each of which aids the other. Such an approach considerably enhances the quality of correspondences making the subsequent structure and motion computations more accurate. See <ref> [8, 14] </ref> for related approaches. fl Contact Author 1 We assume that the object in view is undergoing affine motion, which is more general than the customary assumption of rigid transformations. We use the affine camera projective model [10] and recover structure up to an arbitrary affine transformation.
Reference: [9] <author> P. F. McLauchlan. </author> <title> HORATIO : Libraries for Vision Applications. </title> <type> Technical report, </type> <institution> Robotics Research Group, University of Oxford, WP3/OXFORD/930112/HORATIO, </institution> <year> 1993. </year>
Reference-contexts: of hash functions, as proposed by Wegman and Carter [4], which ensure that the expected time for k searches is a linear function of k, averaged over all the functions belonging to that class. 5 Results We have implemented an off-line version of our algorithm using the HORATIO image-processing library <ref> [9] </ref> and carried out extensive off-line experimentation on image sequences of a head, undergoing motion of varying nature. In the first image sequence of 43 frames (taken at 7.0 Hz), the motion is primarily in the vertical direction, with the person nodding his head up and down.
Reference: [10] <author> J. L. Mundy and A. Zisserman. </author> <title> Geometric Invariance in Computer Vision. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: See [8, 14] for related approaches. fl Contact Author 1 We assume that the object in view is undergoing affine motion, which is more general than the customary assumption of rigid transformations. We use the affine camera projective model <ref> [10] </ref> and recover structure up to an arbitrary affine transformation. The affine camera model is valid when the field of view is small and the depth of the object is small compared to the viewing distance, which is expected in any foveal tracking application [10, 11]. <p> The affine camera model is valid when the field of view is small and the depth of the object is small compared to the viewing distance, which is expected in any foveal tracking application <ref> [10, 11] </ref>. When the perspective effects are small, the problem of locating the epipolar lines [13] becomes ill-conditioned in the traditional perspective camera model, leading to error in subsequent computations.
Reference: [11] <author> I. D. Reid and D. W. Murray. </author> <title> Tracking Foveated Corner Clusters using Affine Structure. </title> <booktitle> In Proc. Intl. Conf. Computer Vision, </booktitle> <pages> pages 76-83, </pages> <year> 1993. </year>
Reference-contexts: Corners (feature points) are easily computed, and are small in number compared to the overall image size, making corner tracking algorithms feasible for real time implementations <ref> [11] </ref>. However, the primary problem in point tracking arises due to the localization and detection errors of most corner detectors [16, 6], making it necessary to design tracking algorithms which are insensitive to such errors. In this paper we present a robust and computationally efficient corner tracking algorithm. <p> Tracking and computation of structure and motion have been treated in the past as two separate problems. On one hand, traditional approaches to tracking have been based primarily on intensity correlation matching [2], with the more sophisticated approaches employing recursive Kalman filtering <ref> [11] </ref> for temporal consistency. Little attempt has been made to impose structural constraints on the features being tracked. On the other hand most approaches to structure and motion parameter estimation have assumed that reliable correspondences are already available [7, 15]. <p> The affine camera model is valid when the field of view is small and the depth of the object is small compared to the viewing distance, which is expected in any foveal tracking application <ref> [10, 11] </ref>. When the perspective effects are small, the problem of locating the epipolar lines [13] becomes ill-conditioned in the traditional perspective camera model, leading to error in subsequent computations. <p> In what follows we describe our new approach to feature tracking. In Sec. 2, we describe our real time (25 frames/sec) implementation of a state of the art Kalman filter based corner tracking algorithm <ref> [11] </ref> and discuss its limitations. In Sec. 3 we describe the affine multiple views geometry and derive the geometric and structure based constraints for point tracking. In Sec. 4, we describe our new algorithm which is based on the above constraints. <p> In Sec. 4, we describe our new algorithm which is based on the above constraints. In Sec. 5 we present tracking results on long real image sequences and compare the performance with the Kalman filter based tracking algorithm of <ref> [11] </ref>. We also compare two different methods for fixation of gaze in foveal tracking. Finally, in Sec. 6, we conclude the paper. 2 Kalman Filtering based Tracking Kalman filters comprise a class of linear unbiased minimum-error covariance sequential state estimation algorithms. <p> Finally, in Sec. 6, we conclude the paper. 2 Kalman Filtering based Tracking Kalman filters comprise a class of linear unbiased minimum-error covariance sequential state estimation algorithms. We have implemented a constant image-velocity Kalman filter for each corner <ref> [11] </ref>. The state vector for any corner, x (k) is given by a 4 x 1 matrix [x (k); y (k); _x (k); _y (k)] T .
Reference: [12] <author> I.D. Reid, P.M. Sharkey, P.F. Mclauchlan, and Murray D.W. </author> <title> A modular head/eye platform for real-time reactive vision. </title> <type> Technical report, </type> <institution> Dept. of Engineering Science, University of Oxford, </institution> <year> 1992. </year>
Reference-contexts: (x 0 ; y 0 ) is the predicted corner position and (x 1 ; y 1 ) is the actual position of the match in the new frame. 2.2 Real-Time Implementation We have implemented the simple Kalman filter tracking using a CCD camera mounted on a Yorick robot head <ref> [12] </ref>. An array of transputers has been used to provide spatial and temporal parallelism. The system runs at 25 Hz, as shown in Figure 1. Tracking is carried out in a fovea of 64 x 64 pixels. <p> The gaze demand is sent to the Yorick head controller for a corresponding head movement. The Yorick controller carries out a 12:5 Hz Nyquist rate filtering of the gaze demand to remove jitter <ref> [12] </ref>. 2.3 Performance Kalman filtering yields several false matches across frames. This is because matching is based on the correlation of intensities in a small window around the corner. <p> the gaze point using the centroid of the corners, weighted by their age, exhibits a lot of jitter (Figures 10 and 11), leading to instabilities in real-time tracking with an active head-eye platform, even with the 12:5 Hz Nyquist rate filtering of the gaze demand as carried out by Yorick <ref> [12] </ref>. However, gaze point fixation by tracking a specific corner is very smooth in our method (Figure 10). In the case of Kalman filtering alone, tracking a fixed corner does not reduce jitter to that extent (Figure 11).
Reference: [13] <author> L. S. Shapiro, Zisserman A., and J. M. Brady. </author> <title> What Can One See With an Affine Camera? Technical report, </title> <institution> Robotics Research Group, University of Oxford, </institution> <year> 1993. </year> <title> Also see Motion from Point Matches using Affine Epipolar Geaometry, </title> <note> to appear in IJCV. </note>
Reference-contexts: The affine camera model is valid when the field of view is small and the depth of the object is small compared to the viewing distance, which is expected in any foveal tracking application [10, 11]. When the perspective effects are small, the problem of locating the epipolar lines <ref> [13] </ref> becomes ill-conditioned in the traditional perspective camera model, leading to error in subsequent computations. In such cases, it is convenient to assume the parallel projection model of the affine camera which explicitly models the ambiguities. <p> As explained in <ref> [13] </ref>, the perspective camera gives epipolar lines which are concurrent (at a point called the epipole) and the affine camera gives parallel epipolar lines. 3.2 The Affine Camera In our approach, we have used the affine camera model instead of the more familiar perspective model. <p> Thus, the conclusion about the invariance of the affine coordinates holds true in the image too. 3.2.2 Determination of epipolar line As described in <ref> [13] </ref>, the epipolar constraint may be expressed, using the affine fundamental matrix Q as i Qp 0 that is h i y O i 6 0 0 a c d e 7 2 4 i i 3 5 = 0 (5) where 1 i P and p 0 i = (x <p> Also, the inevitability of noise and corner detection errors means that the points do not lie exactly on the epipolar lines. This necessitates a least squares solution of the system of conditions given by Equation 5. As given in <ref> [13] </ref>, we choose to minimize the cost function E = i=1 i + by O i + dy N a 2 + b 2 + c 2 + d 2 subject to a 2 + b 2 + c 2 + d 2 = 1. <p> ; x N N ; y N N ): i ; y O i ; y N i ) are the coordinates of a matched pair of corners in successive frames. 3.3 Affine structure from motion: Tomasi and Kanade As proposed by Tomasi and Kanade [15] and later modified in <ref> [13] </ref>, we compute the affine structure of the corners which have a match history of at least F frames by constructing a P fi 2F measurement matrix W = 6 x 1 1 x 2 1 : : : 2 y 1 2 y 2 . . . . . . <p> If w has rank 3, * Compute (a; b; c; d) T = e w , where e w is the eigenvector of w corresponding to its eigenvalue with the least magnitude <ref> [13] </ref>. * Compute e = e w :r. * Compute 2 P X (ax O i + cx N i + e) 2 * The best estimate of the variance of errors is given by epi = 2 P 5 * Remove outliers using the affine epipolar constraint i.e. any pair
Reference: [14] <author> G. Sudhir, S. Banerjee, and A. Zisserman. </author> <title> Finding Point Correspondences in Motion Sequences Preserving Affine Structure. </title> <booktitle> In Proc. British Machine Vision Conf., </booktitle> <year> 1993. </year> <note> Also revised for CVGIP: Image Understanding. </note>
Reference-contexts: In contrast, our work attempts to unify feature tracking and structure computation and provide solutions to both, each of which aids the other. Such an approach considerably enhances the quality of correspondences making the subsequent structure and motion computations more accurate. See <ref> [8, 14] </ref> for related approaches. fl Contact Author 1 We assume that the object in view is undergoing affine motion, which is more general than the customary assumption of rigid transformations. We use the affine camera projective model [10] and recover structure up to an arbitrary affine transformation.
Reference: [15] <author> Carlo Tomasi and Takeo Kanade. </author> <title> Shape and Motion from Image Streams under Orthography: a Factorization Method. </title> <booktitle> In Intl. J. Computer Vision, </booktitle> <pages> pages 137-154, </pages> <year> 1992. </year>
Reference-contexts: Little attempt has been made to impose structural constraints on the features being tracked. On the other hand most approaches to structure and motion parameter estimation have assumed that reliable correspondences are already available <ref> [7, 15] </ref>. The successes of these approaches are crucially dependent on the correctness of the assumed correspondences. In contrast, our work attempts to unify feature tracking and structure computation and provide solutions to both, each of which aids the other. <p> It also develops the mathematics of the affine camera model and of affine structure in terms of affine coordinates. A recapitulation of the method for determining affine structure from motion proposed by Tomasi and Kanade <ref> [15] </ref> concludes the section. 4 3.1 Epipolar Geometry Given two views of an object, obtained by variation in the position of one or both of the camera and the object, and the image of a world point in one frame, the epipolar line is the locus of all possible points in <p> O ; y O O ; x N N ; y N N ): i ; y O i ; y N i ) are the coordinates of a matched pair of corners in successive frames. 3.3 Affine structure from motion: Tomasi and Kanade As proposed by Tomasi and Kanade <ref> [15] </ref> and later modified in [13], we compute the affine structure of the corners which have a match history of at least F frames by constructing a P fi 2F measurement matrix W = 6 x 1 1 x 2 1 : : : 2 y 1 2 y 2 .
Reference: [16] <author> H. Wang and J. M. Brady. </author> <title> Corner Detection for 3D Vision using Array Processors. </title> <booktitle> In Proc. </booktitle> <address> BARNAIMAGE-91, Barcelona, Spain. </address> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Corners (feature points) are easily computed, and are small in number compared to the overall image size, making corner tracking algorithms feasible for real time implementations [11]. However, the primary problem in point tracking arises due to the localization and detection errors of most corner detectors <ref> [16, 6] </ref>, making it necessary to design tracking algorithms which are insensitive to such errors. In this paper we present a robust and computationally efficient corner tracking algorithm. Tracking and computation of structure and motion have been treated in the past as two separate problems. <p> 5 Measurement is noisy and given by a 2 x 1 vector z (k) = Hx (k) + w (k), where w (k) are iid Gaussian, zero-mean, temporally uncorrelated noises with covariance R, and H = 1 0 0 0 # In our implementation, we use the Wang-Brady corner detector <ref> [16] </ref>, which uses Gaussian smoothing prior to detection of corners. Therefore, the covariance matrix R can be constructed by noting that the localization error in corner detection has the same variance as that of the Gaussian mask.
Reference: [17] <author> Saul A. Teukolsky William T. Vellerling, William H. Press and Brain P. Fleming. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press., </publisher> <month> May </month> <year> 1992. </year> <month> 16 </month>
Reference-contexts: Compute v i = (x O i ; x N i ) = (x O O ; y O O ; x N N ; y N N ): w = i=1 i 3. Compute the eigenvalues of w by Jacobi's method <ref> [17] </ref>, (since w is real symmetric) and decide whether its rank is 4. If the rank is 4, Stop, because this indicates that either or both of the assumptions that the camera model is affine and that the object is undergoing affine motion, are invalid. 4. <p> If the rank is 4, Stop, because this indicates that either or both of the assumptions that the camera model is affine and that the object is undergoing affine motion, are invalid. 4. Compute a 2 x 2 matrix g by the Least Squares Method <ref> [17] </ref> satisfying 2 6 6 x O 1 2 y O . . . x O P 7 7 5 g 11 g 12 # 2 6 6 x N 1 2 y N . . . x N P 7 7 5 The least squares solution also yields g =
References-found: 17

