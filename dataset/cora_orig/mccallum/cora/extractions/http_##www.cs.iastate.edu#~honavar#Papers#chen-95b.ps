URL: http://www.cs.iastate.edu/~honavar/Papers/chen-95b.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/honavar.html
Root-URL: 
Email: chen@cs.iastate.edu, honavar@cs.iastate.edu  
Title: A Neural Network Architecture for High-Speed Database Query Processing  
Author: Chun-Hsien Chen Vasant Honavar 
Note: This research was partially supported by the National Science Foundation through the grant IRI-9409580 to Vasant Honavar.  
Address: 226 Atanasoff Hall,  Ames, IA 50011. U.S.A.  
Affiliation: Department of Computer Science  Iowa State University,  
Abstract: Artificial neural networks (ANN), due to their inherent parallelism and potential fault tolerance, offer an attractive paradigm for robust and efficient implementations of large modern database and knowledge base systems. This paper explores a neural network model for efficient implementation of a database query system. The application of the proposed model to a high-speed library query system for retrieval of multiple items is based on partial match of the specified query criteria with the stored records. The performance of the ANN realization of the database query module is analyzed and compared with other techniques commonly in current computer systems. The results of this analysis suggest that the proposed ANN design offers an attractive approach for the realization of query modules in large database and knowledge base systems, especially for retrieval based on partial matches. 
Abstract-found: 1
Intro-found: 1
Reference: <author> J. L. Bently, </author> <title> Multidimensional Binary Search Trees Used for Associative Searching, </title> <journal> Communications of the ACM, </journal> <volume> vol. 18, no. 9, </volume> <pages> pp. 507-517, </pages> <year> 1975. </year>
Reference: <author> C. Chen & V. Honavar, </author> <title> Neural Network Automata, </title> <booktitle> Proc. of World Congress on Neural Networks, </booktitle> <volume> vol. 4, </volume> <pages> pp. 470-477, </pages> <address> San Diego, </address> <year> 1994. </year>
Reference: <author> C. Chen and V. Honavar, </author> <title> Neural Associative Memories for Content as well as Address-Based Storage and Recall: </title> <journal> Theory and Applications. </journal> <note> Submitted. Preliminary version available as Iowa State University Dept. </note> <institution> of Computer Science Tech. </institution> <type> Rep. </type> <address> ISU-CS-TR 95-03, </address> <year> 1995a. </year>
Reference-contexts: The rest of the paper is organized as follows: * Section 2 reviews the mathematical model of multiple recall from partially specified input pattern in a neural network proposed in <ref> (Chen & Honavar, 1995a) </ref>. * Section 3 develops an ANN design for a high-speed library query system in detail based on the model described in section 2. * Section 4 compares the performance of the proposed ANN-based query processing system with that of several currently used techniques. * Section 5 concludes <p> Thus, query processing in a database can be viewed as an instance of the task of recall of multiple stored patterns given a partial specification of the patterns to be recalled. A neural associative memory for performing this task was proposed in <ref> (Chen & Honavar, 1995a) </ref>. This section summarizes the relevant properties of the neural memory developed in (Chen & Honavar, 1995a). 2.1 Associative Memory with Bipolar Input and Bi nary Output The associative memory used is based on 2-layer perceptron (with a layer of input neurons, a layer of hidden neurons, and <p> A neural associative memory for performing this task was proposed in <ref> (Chen & Honavar, 1995a) </ref>. This section summarizes the relevant properties of the neural memory developed in (Chen & Honavar, 1995a). 2.1 Associative Memory with Bipolar Input and Bi nary Output The associative memory used is based on 2-layer perceptron (with a layer of input neurons, a layer of hidden neurons, and a layer of output neurons; and hence two layers of connection weights). <p> With the addition of appropriate control circuitry, this behavior can be modified to yield sequential recall of more than one stored pattern. Multiple recalls are possible if some of the associative partitions real-ized in the associative ANN memory are not isolated (see <ref> (Chen & Honavar, 1995a) </ref> for details). An input pattern (a vertex of an n-dimensional bipolar hypercube) located in a region of overlap between several associative partitions is close enough to the corresponding partition centers (stored memory patterns) and hence can turn on more than one hidden neuron as explained below. <p> of the ASCII input is converted into a bipolar bit x p by expression x p = 2x b 1 before it is fed into the ANN memory module for database queries. (This is motivated by the relative efficiency of the hardware implementations of binary and bipolar associative memories see <ref> (Chen & Honavar, 1995a) </ref> for details). Let output be a M -dimensional binary vector pointing to a record in the library database that contains information about a volume (or the binary vector can encode information about a volume directly).
Reference: <author> C. Chen and V. Honavar, </author> <title> Neural Network Architecture for Parallel Set Operations In preparation, </title> <year> 1995b. </year>
Reference: <author> C. Chen and V. Honavar, </author> <title> A Neural Network Architecture for Syntax Analysis. </title> <note> Submitted. Preliminary version available as Iowa State University Dept. </note> <institution> of Computer Science Tech. </institution> <type> Rep. </type> <address> ISU-CS-TR 95-18, </address> <year> 1995c. </year>
Reference: <editor> Goonatilake, S. and Khebbal, S. (Ed.) </editor> <title> Intelligent Hybrid Systems. </title> <publisher> Wiley, </publisher> <address> London, </address> <year> 1995. </year>
Reference: <author> S. M. Gowda et al., </author> <title> Design and Characterization of Analog VLSI Neural Network Modules, </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 28, no. 3, </volume> <pages> pp. 301-313, </pages> <year> 1993. </year>
Reference-contexts: ANNs are implemented using mostly CMOS-based analog, digital, and hybrid electronic circuits. The analog circuit which mainly consists of processing elements for multiplication, summation and thresholding is popular for the realization of ANNs since compact circuits capable of high-speed asynchronous operation can be achieved <ref> (Gowda et al., 1993) </ref>. (Uchimura et al., 1992) reports a measured propagation delay of 104 ns from the START signal until the result is latched by using digital synapse circuit containing an 8-bit memory, an 8-bit subtractor and an 8-bit adder. (Graf & Hender-son, 1990) adopts a hybrid analog-digital design with
Reference: <author> H. P. Graf and D. Henderson, </author> <title> A Reconfigurable CMOS Neural Network, </title> <booktitle> ISSCC Dig. Tech. Papers, </booktitle> <pages> pp. 144-145, </pages> <address> San Francisco, CA, </address> <year> 1990. </year>
Reference-contexts: circuits capable of high-speed asynchronous operation can be achieved (Gowda et al., 1993). (Uchimura et al., 1992) reports a measured propagation delay of 104 ns from the START signal until the result is latched by using digital synapse circuit containing an 8-bit memory, an 8-bit subtractor and an 8-bit adder. <ref> (Graf & Hender-son, 1990) </ref> adopts a hybrid analog-digital design with 4-bit binary synapse weight values and current-summing circuits to achieve a network computation time of less than 100 ns between the loading of the input and the latching of the result in a comparator. (Masa et al., 1994) adopts a hybrid
Reference: <author> D. Grant et al., </author> <title> Design, Implementation and Evaluation of a High-Speed Integrated Hamming Neural Classifier, </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 29, no. 9, </volume> <pages> pp. 1154-1157, </pages> <year> 1994. </year>
Reference-contexts: of the result in a comparator. (Masa et al., 1994) adopts a hybrid analog--digital design with 5-bit (4 bits + sign) binary synapse weight values and current-summing circuits to implement a feed-forward neural network with 2 connection layers, and a network computation time of less than 20 ns is reported. <ref> (Grant et al., 1994) </ref> achieves the throughput at the rate of 10MHz (delay = 100 ns) in a Hamming Net pattern classifier using analog circuits.
Reference: <author> R. L. Greene, </author> <title> Connectionist Hashed Associative Memory, </title> <booktitle> Artificial Intelligence 48, </booktitle> <pages> pp. 87-98, </pages> <year> 1991. </year>
Reference: <author> A. Hamilton et al., </author> <title> Integrated Pulse Stream Neural Networks: Results, Issues, and Pointers, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 385-393, </pages> <year> 1992. </year>
Reference: <author> V. Honavar and L. Uhr (Ed.) </author> <title> Artificial Intelligence and Neural Networks: Steps Toward Principled Integration. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1994. </year>
Reference: <editor> T. Kohonen, Content-Addressable Memories, 2nd ed., </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987. </year>
Reference: <author> R. Kumar, NCMOS: </author> <title> A High Performance CMOS Logic, </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 29, no. 5, </volume> <pages> pp. 631-633, </pages> <year> 1994. </year>
Reference-contexts: The development of specialized hardware for implementation of ANNs is still in its early stages. Conventional CMOS technology that is currently the main technology for VLSI implementation of ANN is known to be slow (kumar, 1994; Lu & Samuleli, 1993). Other technologies, such as BiCMOS, NCMOS <ref> (kumar, 1994) </ref>, pseudo-NMOS logic, standard N-P domino logic, and quasi N-P domino logic (Lu & Samuleli, 1993), may provide better performance for the realization of ANNs.
Reference: <author> D. Levine and M. Aparicioiv (Ed.) </author> <title> Neural Networks for Knowledge Representation and Inference. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1994. </year>
Reference: <author> J. B. Lont and W. Guggenbuhl, </author> <title> Analog CMOS Implementation of a Multilayer Perceptron with Nonlinear Synapses, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 457-465, </pages> <year> 1992. </year>
Reference: <author> F. Lu and H. Samueli, </author> <title> A 200-MHz CMOS Pipelined Multiplier-Accumulator Using a Quasi-Domino Dynamic Full-Adder Cell Design, </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 28, no. 2, </volume> <pages> pp. 123-132, </pages> <year> 1993. </year>
Reference-contexts: Conventional CMOS technology that is currently the main technology for VLSI implementation of ANN is known to be slow (kumar, 1994; Lu & Samuleli, 1993). Other technologies, such as BiCMOS, NCMOS (kumar, 1994), pseudo-NMOS logic, standard N-P domino logic, and quasi N-P domino logic <ref> (Lu & Samuleli, 1993) </ref>, may provide better performance for the realization of ANNs.
Reference: <author> P. Masa , K. Hoen and H. Wallinga, </author> <title> 70 Input, 20 Nanosecond Pattern Classifier, </title> <booktitle> IEEE International Joint Conference on Neural Networks, </booktitle> <volume> vol. </volume> <pages> 3, </pages> <address> Orlando, FL, </address> <year> 1994. </year>
Reference-contexts: 8-bit subtractor and an 8-bit adder. (Graf & Hender-son, 1990) adopts a hybrid analog-digital design with 4-bit binary synapse weight values and current-summing circuits to achieve a network computation time of less than 100 ns between the loading of the input and the latching of the result in a comparator. <ref> (Masa et al., 1994) </ref> adopts a hybrid analog--digital design with 5-bit (4 bits + sign) binary synapse weight values and current-summing circuits to implement a feed-forward neural network with 2 connection layers, and a network computation time of less than 20 ns is reported. (Grant et al., 1994) achieves the throughput <p> The 1st-layer and 2nd-layer subnetworks of the proposed neural architecture for database query processing are very similar to the 1st-layer subnetwork of a Hamming Net respectively, and the neural architecture with 2 connection layers in the proposed ANN is exactly same as that implemented by <ref> (Masa et al., 1994) </ref> except (Masa et al., 1994) uses discretized inputs, 5-bit synaptic weights, and sigmoid-like activation function. The proposed ANN uses bipolar inputs, weights in f1; 0; 1g and binary hardlimiter as activation function. <p> The 1st-layer and 2nd-layer subnetworks of the proposed neural architecture for database query processing are very similar to the 1st-layer subnetwork of a Hamming Net respectively, and the neural architecture with 2 connection layers in the proposed ANN is exactly same as that implemented by <ref> (Masa et al., 1994) </ref> except (Masa et al., 1994) uses discretized inputs, 5-bit synaptic weights, and sigmoid-like activation function. The proposed ANN uses bipolar inputs, weights in f1; 0; 1g and binary hardlimiter as activation function.
Reference: <author> L. W. Massengill and D. B. Mundie, </author> <title> An Analog Neural Network Hardware Implementation Using Charge-Injection Multipliers and Neuron-Specific Gain Control, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 354-362, </pages> <year> 1992. </year>
Reference: <author> M. Minsky and S. Papert, </author> <title> Perceptrons: An Introduction to Computational Geometry, </title> <publisher> MIT Press, </publisher> <address> Massachusetts, </address> <year> 1969. </year>
Reference: <author> G. Moon et al., </author> <title> VLSI Implementation of Synaptic Weighting and Summing in Pulse Coded Neural-Type Cells, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 394-403, </pages> <year> 1992. </year>
Reference: <author> M. E. Robinson et al., </author> <title> A Modular CMOS Design of a Hamming Network, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 444-456, </pages> <year> 1992. </year>
Reference: <author> R. Sedgewick, </author> <title> Algorithms, 2nd ed., </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference: <author> R. Sun and L. Bookman (Ed.) </author> <title> Computational Architectures Integrating Symbolic and Neural Processes. </title> <publisher> Kluwer, </publisher> <address> New York, </address> <year> 1994. </year>
Reference: <author> K. Uchimura et al., </author> <title> An 8G Connection-per-second 54mW Digital Neural Network with Low-power Chain-Reaction Architecture, </title> <booktitle> ISSCC Dig. Tech. Papers, </booktitle> <pages> pp. 134-135, </pages> <address> San Francisco, CA, </address> <year> 1992. </year>
Reference-contexts: ANNs are implemented using mostly CMOS-based analog, digital, and hybrid electronic circuits. The analog circuit which mainly consists of processing elements for multiplication, summation and thresholding is popular for the realization of ANNs since compact circuits capable of high-speed asynchronous operation can be achieved (Gowda et al., 1993). <ref> (Uchimura et al., 1992) </ref> reports a measured propagation delay of 104 ns from the START signal until the result is latched by using digital synapse circuit containing an 8-bit memory, an 8-bit subtractor and an 8-bit adder. (Graf & Hender-son, 1990) adopts a hybrid analog-digital design with 4-bit binary synapse weight
Reference: <author> J. D. Ullman, </author> <title> Principles of Databases and Knowledge-base Systems, vol. I, </title> <type> Chapter 6, </type> <institution> Computer Science Press, Maryland, </institution> <year> 1988. </year>
Reference-contexts: of ANNs is likely to improve with technological advances in VLSI. 4.2 Analysis of Query Processing in Conventional Com puter Systems In database systems implemented on conventional computer systems, given the values for a key, a record is located quickly by using key-based organizations including hashing, index-sequential access and B-trees <ref> (Ullman, 1988) </ref>. For a very large database, the data is usually stored in secondary storage devices like hard disks. Conventionally, estimated cost of locating a record is based on the number of physical block accesses of secondary storage devices (Ullman, 1988). <p> quickly by using key-based organizations including hashing, index-sequential access and B-trees <ref> (Ullman, 1988) </ref>. For a very large database, the data is usually stored in secondary storage devices like hard disks. Conventionally, estimated cost of locating a record is based on the number of physical block accesses of secondary storage devices (Ullman, 1988). The cost-effective access time with current disk systems appears to be around 10 ms. In comparison, the proposed ANN-based im-plementation is clearly far superior when speed of access is the prime consideration.
Reference: <author> T. Watanabe et al., </author> <title> A Single 1.5-V Digital Chip for a 10 6 Synapse Neural Network, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 387-393, </pages> <year> 1993. </year>
References-found: 27

