URL: http://www.cs.umn.edu/Users/dept/users/kumar/sort.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Scalability of Parallel Sorting on Mesh Multicomputers  
Author: V. Singh, V. Kumar, G. Agha, and C. Tomlinson 
Date: 90-07195).  
Note: Kumar's work was partially supported by Army Research Office grant 28408-MA-SDI to the University of Minnesota and by the Army High Performance Computing Research Center at the University of Minnesota. Agha's work has been supported in part by a Young Investigator Award from the Office of Naval Research (ONR contract number N00014-90-J-1899), by an Incentives for Excellence Award from the Digital Equipment Corporation Faculty Program, and by joint support from the Defense Advanced Research Projects Agency and the National Science Foundation (NSF CCR  
Address: 3500 West Balcones Center Drive Austin, TX 78759  Minneapolis, MN 55455  Urbana, IL 61801  
Affiliation: MCC  Computer Science Department University of Minnesota  Department of Computer Science University of Illinois  
Abstract: This paper presents two new parallel algorithms QSP1 and QSP2 based on sequential quicksort for sorting data on a mesh multicomputer, and analyzes their scalability using the isoefficiency metric. We show that QSP2 matches the lower bound on the isoeffi-ciency function for mesh multicomputers. The isoef-ficiency of QSP1 is also fairly close to optimal. Lang et al. and Schnorr et al. have developed parallel sorting algorithms for the mesh architecture that have either optimal (Schnorr) or close to optimal (Lang) run-time complexity for the one-element-per-processor case. Both QSP1 and QSP2 have worse performance than these algorithms for the one-element-per-processor case. But QSP1 and QSP2 have better scalability than the scaled-down variants of these algorithms (for the case in which there are more elements than processors). As a result, our new parallel formulations are better than these scaled-down variants in terms of speedup w.r.t the best sequential algorithms. We also present a different variant of Lang's sort which is asymptotically as scalable as QSP2 (for the multiple-element-per-processor case). We briefly discuss another metric called "resource consumption metric". According to this metric, both QSP1 and QSP2 are strictly superior to Lang's sort and its variations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ull man. </author> <title> Data Structures and Algorithms. Computer 6 They believe that random mapping should work well in a lot of cases. In all cases, they feel that an algorithm-independent strategy will suffice. </title> <booktitle> Science and Information Processing, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: Therefore, for the sake of convenience in the rest of the paper, we will refer to isoefficiency functions and isoefficiency curves without reference to a specific efficiency. 4 A Naive Parallel Formulation of Quicksort Quicksort <ref> [1] </ref> is a recursive algorithm that repeatedly partitions an unsorted list into two smaller sublists using one of the members of the original list as a pivot. One of the sublists contains elements less than or equal to the pivot and the other contains elements greater than the pivot.
Reference: [2] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: If context-switching costs are 1 ignored, such a technique will result in a slow down of at most a factor of N p over the original parallel algorithm. If the original parallel algorithm is cost-optimal (i.e., if the processor-time product <ref> [2] </ref> of the original parallel algorithm is the same as the sequential time complexity of the best sequential algorithm), then this technique works well; the resulting scaled-down parallel algorithm still has the same processor-time product. <p> The reader should note that there are other parallel formulations of quicksort on PRAM <ref> [2, 10, 5] </ref> which can be shown to have much better scalability. 5 More Scalable Formulations of Quicksort Here we present a new parallel quicksort algorithm and some of its variations and show that all of them are more scalable on a mesh than the naive parallel quick-sort (on a PRAM).
Reference: [3] <author> Bill Athas. </author> <title> Fine Grain Concurrent Computations. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Califor-nia Institute of Technology, </institution> <year> 1987. </year> <note> Also published as technical report 5242:TR:87. </note>
Reference-contexts: Even the scaled-down variant of Schnorr and Shamir's optimal one-element-per-processor algorithm has no isoefficiency (i.e., is not scalable). The methodology of first uncovering all the inherent parallelism in the problem, and then mapping the concurrent activities to the parallel architectures has been promoted by a number of researchers <ref> [3, 24, 4] </ref>. This is one of the motivations behind developing the parallel algorithms that are in the NC class [6].
Reference: [4] <author> K. M. Chandy and J. Misra. </author> <title> Parallel Program Design: A Foundation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1988. </year>
Reference-contexts: Even the scaled-down variant of Schnorr and Shamir's optimal one-element-per-processor algorithm has no isoefficiency (i.e., is not scalable). The methodology of first uncovering all the inherent parallelism in the problem, and then mapping the concurrent activities to the parallel architectures has been promoted by a number of researchers <ref> [3, 24, 4] </ref>. This is one of the motivations behind developing the parallel algorithms that are in the NC class [6].
Reference: [5] <author> Bogdan S. Chlebus and Imrich Vrto. </author> <title> Parallel quick-sort. </title> <journal> Journal of Parallel and Distributed Processing, </journal> <note> 1991(to appear). </note>
Reference-contexts: The reader should note that there are other parallel formulations of quicksort on PRAM <ref> [2, 10, 5] </ref> which can be shown to have much better scalability. 5 More Scalable Formulations of Quicksort Here we present a new parallel quicksort algorithm and some of its variations and show that all of them are more scalable on a mesh than the naive parallel quick-sort (on a PRAM).
Reference: [6] <author> Stephen A. Cook. </author> <title> Towards a complexity theory of synchronous parallel computation. </title> <address> L'Enseignement Mathematique, XXVIII:99-124, </address> <year> 1981. </year>
Reference-contexts: This is one of the motivations behind developing the parallel algorithms that are in the NC class <ref> [6] </ref>. This is also similar to the idea advocated by Athas and Seitz in the context of the Actor paradigm. 6 Our results show that this methodology does not result in the best possible parallel algorithms at least for some problems.
Reference: [7] <author> L. Devroye. </author> <title> A note on the height of binary search trees. </title> <journal> Journal of Association of Computing Machinery, </journal> <volume> 33 </volume> <pages> 489-498, </pages> <year> 1986. </year>
Reference-contexts: Since the partitioning step does not cut the number of processors in exactly half each time, it turns out that the total number of iterations is more than log p. The average number of iterations is less than or equal to 3 log p + 6 <ref> [7] </ref>. We approximate this to be 3 log p. Therefore, the total time spent on all the iterations is given by the following formula: (k s fi 2 p + p p p 3 p (2k s + k b p Finally, each processor needs to mergesort its elements sequentially.
Reference: [8] <author> Anshul Gupta and Vipin Kumar. </author> <title> On the scalability of FFT on parallel computers. </title> <booktitle> In Proceedings of the Frontiers 90 Conference on Massively Parallel Computation, </booktitle> <month> October </month> <year> 1990. </year> <note> An extended version of the paper is available as a technical report from the Department of Computer Science and Army High Performance Computing Research Center, </note> <institution> University of Minnesota. </institution>
Reference-contexts: The isoefficiency metric relates the problem size to the number of processors necessary for linear speedup. Isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms <ref> [15, 22, 16, 8, 14] </ref>. An important feature of isoefficiency analysis is that it succinctly captures the behavior of a parallel algorithm in relation to the given parallel architecture. <p> From our analysis, it is clear that for asymptotic order of time complexity of sorting, mesh with cut-through routing is no more powerful than a mesh with simple routing (i.e., store-and-forward routing). It has been shown elsewhere that the same is true for FFT <ref> [8] </ref> and some parallel algorithms for shortest path [16]. Of course, for many other parallel algorithms (including some for shortest path), mesh with cut-through routing has much better scalability than mesh with store-and-forward routing.
Reference: [9] <author> R. Halstead. </author> <title> Multilisp: a language for concurrent symbolic computation. </title> <journal> ACM Trans. on Prog. Languages and Systems, </journal> <pages> 501-538, </pages> <year> 1985. </year>
Reference-contexts: Recall that the average time to quicksort a list of length N is k QS N log N , where k QS is a constant. The following naive parallel variant of quicksort [21] is frequently used to illustrate the utility of high-level parallel programming languages <ref> [9, 20] </ref>. In this algorithm, the entire unsorted list is stored initially in one processor. This processor partitions the list into two sublists. It hands one sublist to an idle processor and keeps the other sublist.
Reference: [10] <author> P. Heidelberger, A. Norton, and J.T. Robinson. </author> <title> Parallel quicksort using fetch-and-add. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(1) </volume> <pages> 133-138, </pages> <year> 1990. </year>
Reference-contexts: The reader should note that there are other parallel formulations of quicksort on PRAM <ref> [2, 10, 5] </ref> which can be shown to have much better scalability. 5 More Scalable Formulations of Quicksort Here we present a new parallel quicksort algorithm and some of its variations and show that all of them are more scalable on a mesh than the naive parallel quick-sort (on a PRAM).
Reference: [11] <author> D. Hillis and G. L. Steele. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> (12):1170-1183, 1986. 
Reference-contexts: This becomes important when the parallel processor is being shared among many different applications. It must be noted that ELS1 is a natural algorithm that one would derive by simply following the data-parallel paradigm in <ref> [11] </ref>.
Reference: [12] <author> M. A. Huang. </author> <title> Solving some graph problems with optimal or near optimal speedup on mesh-of-trees networks. </title> <booktitle> In Proceedings of 26th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 232-240, </pages> <year> 1985. </year>
Reference-contexts: This property is true of all parallel algo rithms. For a large class of parallel algorithms (e.g., parallel DFS [15], parallel 0/1 knapsack [19], parallel connected components <ref> [12] </ref>, and parallel shortest path algorithms [13]), the following additional property is also true: * For any given number p of processors, the efficiency of the parallel algorithm goes up monotonically (i.e., it never goes down, and approaches a constant e, s.t. 0 &lt; e 1) if it is used to
Reference: [13] <author> J. Jenq and S. Sahni. </author> <title> All Pairs Shortest Paths on a Hypercube Multiprocessor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 713-716, </pages> <year> 1987. </year>
Reference-contexts: This property is true of all parallel algo rithms. For a large class of parallel algorithms (e.g., parallel DFS [15], parallel 0/1 knapsack [19], parallel connected components [12], and parallel shortest path algorithms <ref> [13] </ref>), the following additional property is also true: * For any given number p of processors, the efficiency of the parallel algorithm goes up monotonically (i.e., it never goes down, and approaches a constant e, s.t. 0 &lt; e 1) if it is used to solve problem instances of increasing size.
Reference: [14] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Load balancing on the hypercube architecture. </title> <booktitle> In Proceedings of the 1989 Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pages 603-608, </pages> <year> 1989. </year>
Reference-contexts: The isoefficiency metric relates the problem size to the number of processors necessary for linear speedup. Isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms <ref> [15, 22, 16, 8, 14] </ref>. An important feature of isoefficiency analysis is that it succinctly captures the behavior of a parallel algorithm in relation to the given parallel architecture.
Reference: [15] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Parallel depth-first search, part II: analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):501-519, 1987. 
Reference-contexts: 1 Introduction In this paper, we investigate the problem of parallel sorting on a two-dimensional mesh multicomputer architecture. We characterize the scalability of various algorithms using the formally-defined isoefficiency metric <ref> [15] </ref>. As described below, this metric helps assess the performance of a given parallel algorithm on a given architecture under realistic situations. Let us first consider the disadvantages of using a metric other than isoefficiency. <p> The above run-time metric is inadequate, as it only deals with a 1-D curve on the 2-D space (of p and problem size). The isoefficiency metric initially introduced in <ref> [15] </ref> is one metric that can be used to compare the performance of parallel algorithms when both problem size and number of processors may vary. The isoef-ficiency of a parallel algorithm reflects its scalability; i.e., its ability to effectively utilize increasing number of processors. <p> The isoefficiency metric relates the problem size to the number of processors necessary for linear speedup. Isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms <ref> [15, 22, 16, 8, 14] </ref>. An important feature of isoefficiency analysis is that it succinctly captures the behavior of a parallel algorithm in relation to the given parallel architecture. <p> This property is true of all parallel algo rithms. For a large class of parallel algorithms (e.g., parallel DFS <ref> [15] </ref>, parallel 0/1 knapsack [19], parallel connected components [12], and parallel shortest path algorithms [13]), the following additional property is also true: * For any given number p of processors, the efficiency of the parallel algorithm goes up monotonically (i.e., it never goes down, and approaches a constant e, s.t. 0
Reference: [16] <author> Vipin Kumar and V. Singh. </author> <title> Scalability of Parallel Algorithms for the All-Pairs Shortest Path Problem: </title>
Reference-contexts: The isoefficiency metric relates the problem size to the number of processors necessary for linear speedup. Isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms <ref> [15, 22, 16, 8, 14] </ref>. An important feature of isoefficiency analysis is that it succinctly captures the behavior of a parallel algorithm in relation to the given parallel architecture. <p> It has been shown elsewhere that the same is true for FFT [8] and some parallel algorithms for shortest path <ref> [16] </ref>. Of course, for many other parallel algorithms (including some for shortest path), mesh with cut-through routing has much better scalability than mesh with store-and-forward routing.
References-found: 16

