URL: http://www.cs.wisc.edu/~cao/papers/TOCS96.ps.gz
Refering-URL: http://www.cs.princeton.edu/~felten/
Root-URL: 
Title: Implementation and Performance of Integrated Application-Controlled File Caching, Prefetching and Disk Scheduling  
Author: Pei Cao Edward W. Felten Anna R. Karlin Kai Li 
Abstract: This paper presents the design, implementation and performance of a file system that integrates application-controlled caching, prefetching and disk scheduling. We use a two-level cache management strategy. The kernel uses the LRU-SP (Least-Recently-Used with Swapping and Placeholders) policy to allocate blocks to processes, and each process integrates application-specific caching and prefetching based on the controlled-aggressive policy, an algorithm previously shown in a theoretical sense to be near-optimal. Each process also improves its disk access latency by submitting its prefetches in batches so that the requests can be scheduled to optimize disk access performance. Our measurements show that this combination of techniques greatly improves the performance of the file system. We measured that the running time is reduced by 3% to 49% (average 26%) for single-process workloads, and by 5% to 76% (average 32%) for multi-process workloads. 
Abstract-found: 1
Intro-found: 1
Reference: [ABLL92] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Simple schemes perform well, but do not give applications sufficient flexibility in controlling cache replacement. A more general scheme, for example, "upcalls", can give applications complete control over cache replacement, but can incur high overhead <ref> [ABLL92, MA90] </ref>. The main design challenge is to devise an interface that allows applications to exert the control they need, without introducing the overhead that would result from a totally general mechanism.
Reference: [Cao96] <author> Pei Cao. </author> <title> Application-Controlled File Caching and Prefetching. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> January </month> <year> 1996. </year> <note> Also published as technical report CS TR-522-96. </note>
Reference-contexts: The LRU-SP (LRU with Swapping and Placeholders) allocation policy is designed based on the above principles. It can be proven that, in a simplified theoretical model that describes the case of application-controlled caching without prefetching and disk scheduling, LRU-SP satisfies all three criteria <ref> [Cao96] </ref>. LRU-SP operates by maintaining an "LRU" list of all in-cache blocks. The list is in the order of the blocks' last references, except for two modifications (explained below). <p> B is accessed before A); in this case, the erroneous process will give up a block for the extra cache miss, instead of other processes giving up a block. (More details can be found in <ref> [CFL94a, Cao96] </ref>.) Early experiments with application-controlled file caching showed that LRU-SP performs quite well, and the swapping and placeholders mechanisms are crucial to performance improvement in multi-process environments [CFL94b].
Reference: [CB92] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Reducing memory latency via non blocking and prefetching caches. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-61, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they do not address the integration of caching, prefetching and disk scheduling in multi-process environments. Finally, prefetching in uni-processor and multi-processor computer architectures <ref> [RL92, CB92, CKP91, Smi78, TE93] </ref> is similar to prefetching in file systems. However, in these systems there is little flexibility in cache management, as the cache is usually direct-mapped or has very limited associativity.
Reference: [CD85] <author> Hong-Tai Chou and David J. DeWitt. </author> <title> An evaluation of buffer management strategies for relational database systems. </title> <booktitle> In Proceedings of the Eleventh International Conference on Ver y Large Databases, </booktitle> <pages> pages 127-141, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Although these fixed algorithms generally perform well, application-specific policies can often perform much better. Many researchers have pointed out the advantage of application-specific replacement policies, in the context of both file caching and physical memory management <ref> [CD85, SP91, HC92] </ref>. However, devising fair and efficient policies for allocating file cache space among multiple competing processes remains a difficult problem. Recent studies have also demonstrated the benefits of application-specified prefetch-ing [TD90, GA94, PGG + 95]. However, the question of how aggressively to prefetch remains a difficult problem. <p> We believe these principles are important to avoid the thrashing problem [WYT93]. The database community has long studied access patterns and buffer replacement <ref> [Sto81, CD85, OOW93] </ref> and prefetching [CKV93, PZ91] policies. However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they do not address the integration of caching, prefetching and disk scheduling in multi-process environments.
Reference: [CFKL95] <author> Pei Cao, Edward W. Felten, Anna R. Karlin, and Kai Li. </author> <title> A study of integrated prefetching and caching strategies. </title> <booktitle> In Proc. 1995 ACM SIG-METRICS, </booktitle> <pages> pages 188-197, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Given some amount of looka-head in the file block reference pattern, what is the optimal combined prefetching and caching strategy? In <ref> [CFKL95] </ref> we derived several theoretical results about this problem. These results are summarized here. <p> In addition, it always does optimal replacement. We proved in <ref> [CFKL95] </ref> that the running time under controlled-aggressive is never more than 1 + F=K times the optimal running time. <p> In many systems, the file cache is big enough such that F=K is typically less than 0.02. Thus, controlled-aggressive can perform very close to optimal. We also performed simulation studies in <ref> [CFKL95] </ref> comparing controlled-aggressive and six other existing prefetching approaches, including the one-block lookahead prefetching used in many file systems. <p> In practice, however, only imperfect and limited lookahead knowledge is available. Fortunately, even though the near-optimality of "controlled-aggressive" depends 1 In <ref> [CFKL95] </ref> this policy was called aggressive to denote that it is the most aggressive of all reasonable policies. <p> AC caching + prefetching: an integration of application-controlled caching and prefetching, using the controlled-aggressive cache management policy; 6. AC caching + prefetching + scheduling: the fully-integrated system which incorporates all of the techniques. 2 For those who are familiar with our early paper <ref> [CFKL95] </ref>, Global LRU + prefetching is roughly the equivalent of the LRU-sensible algorithm. 21 Our workstation has two disks, an RZ56 and an RZ26. <p> The only exception is din with a 6.4MB cache, which is due to a relatively small fetch cost compared with CPU time and is predicted in our simulation studies in <ref> [CFKL95] </ref>. We chose the batch size B in ACFS through experiments. We measured the elapsed times of several applications (cs2, cs3, gli and psel2) in the fully integrated system, varying B. <p> Thus, the locality of disk accesses is improved. Looking at the number of disk I/Os, we see that prefetching and disk scheduling have little impact except for the case of cs1+psel2 under 12MB file cache. We fed the traces of cs1+psel2 to our calibrated simulator (described in <ref> [CFKL95] </ref>), and found that because prefetching and scheduling changed the interleaving of accesses from the two applications, global LRU will not always hold the working set of cs1 in cache. <p> In [CFL94a] we introduced the LRU-SP kernel allocation policy and simulated its performance on applications. In [CFL94b] we implemented LRU-SP without prefetch-ing or disk scheduling, and showed by experiment that it works well in practice, confirming the simulation results of [CFL94a]. In <ref> [CFKL95] </ref> we presented theoretical results on policies for integrating prefetching with application-controlled caching. We introduced the controlled-aggressive policy, proved that it is theoretically close to optimal, and verified its good performance by simulation. This paper improves on our previous work in several ways.
Reference: [CFL94a] <author> Pei Cao, Edward W. Felten, and Kai Li. </author> <title> Application-controlled file caching policies. </title> <booktitle> In Proc. USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 171-182, </pages> <month> June </month> <year> 1994. </year> <month> 33 </month>
Reference-contexts: B is accessed before A); in this case, the erroneous process will give up a block for the extra cache miss, instead of other processes giving up a block. (More details can be found in <ref> [CFL94a, Cao96] </ref>.) Early experiments with application-controlled file caching showed that LRU-SP performs quite well, and the swapping and placeholders mechanisms are crucial to performance improvement in multi-process environments [CFL94b]. <p> Still, we observe that even with the variances our techniques provide substantial performance benefits. 29 running concurrently with our application, under 12MB file cache. 6 Related Work The work described in this paper builds on our previous work. In <ref> [CFL94a] </ref> we introduced the LRU-SP kernel allocation policy and simulated its performance on applications. In [CFL94b] we implemented LRU-SP without prefetch-ing or disk scheduling, and showed by experiment that it works well in practice, confirming the simulation results of [CFL94a]. <p> In <ref> [CFL94a] </ref> we introduced the LRU-SP kernel allocation policy and simulated its performance on applications. In [CFL94b] we implemented LRU-SP without prefetch-ing or disk scheduling, and showed by experiment that it works well in practice, confirming the simulation results of [CFL94a]. In [CFKL95] we presented theoretical results on policies for integrating prefetching with application-controlled caching. We introduced the controlled-aggressive policy, proved that it is theoretically close to optimal, and verified its good performance by simulation. This paper improves on our previous work in several ways.
Reference: [CFL94b] <author> Pei Cao, Edward W. Felten, and Kai Li. </author> <title> Implementation and performance of application-controlled file caching. </title> <booktitle> In Proc. First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 165-178, </pages> <month> Novem-ber </month> <year> 1994. </year>
Reference-contexts: Care must be taken to ensure that performance is not adversely affected by this interaction. ACFS addresses these concerns using the LRU-SP (Least-Recently-Used with Swapping and Placeholders) allocation policy. Previous studies of application-controlled file caching <ref> [CFL94b] </ref> showed that LRU-SP not only performs well, but also is fair and relatively robust. In addition, to reduce the effect of false prefetching, ACFS matches an application's file accesses with its prediction and only allows the application to prefetch if its predictions have been reasonably accurate. <p> up a block for the extra cache miss, instead of other processes giving up a block. (More details can be found in [CFL94a, Cao96].) Early experiments with application-controlled file caching showed that LRU-SP performs quite well, and the swapping and placeholders mechanisms are crucial to performance improvement in multi-process environments <ref> [CFL94b] </ref>. <p> In [CFL94a] we introduced the LRU-SP kernel allocation policy and simulated its performance on applications. In <ref> [CFL94b] </ref> we implemented LRU-SP without prefetch-ing or disk scheduling, and showed by experiment that it works well in practice, confirming the simulation results of [CFL94a]. In [CFKL95] we presented theoretical results on policies for integrating prefetching with application-controlled caching.
Reference: [CKP91] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software prefetch ing. </title> <booktitle> In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they do not address the integration of caching, prefetching and disk scheduling in multi-process environments. Finally, prefetching in uni-processor and multi-processor computer architectures <ref> [RL92, CB92, CKP91, Smi78, TE93] </ref> is similar to prefetching in file systems. However, in these systems there is little flexibility in cache management, as the cache is usually direct-mapped or has very limited associativity.
Reference: [CKV93] <author> Kenneth M. Curewitz, P. Krishnan, and Jeffrey Scott Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proc. 1993 ACM-SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 257-266, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Despite these differences, both projects utilize disk scheduling, employ similar application interfaces for prefetching, and arrive at similar conclusions about the benefits of prefetching and disk scheduling. There have also been many studies focusing on how to predict future accesses from past accesses <ref> [TD90, PZ91, CKV93, GA94] </ref>. In particular, Griffioen and Appleton's work [GA94] tries to predict future file accesses based on past accesses using "probability graphs," and prefetches accordingly. Few of these studies, however, considered the interaction between prefetching and caching or investigated the combined cache management problem. <p> We believe these principles are important to avoid the thrashing problem [WYT93]. The database community has long studied access patterns and buffer replacement [Sto81, CD85, OOW93] and prefetching <ref> [CKV93, PZ91] </ref> policies. However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they do not address the integration of caching, prefetching and disk scheduling in multi-process environments.
Reference: [EK89] <author> Carla Schlatter Ellis and David Kotz. </author> <title> Prefetching in file systems for MIMD multiprocessors. </title> <booktitle> In Proc. 1989 Intl. Conf. on Parallel Processing, </booktitle> <pages> pages 306-314, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: On the other hand, choosing the best writeback algorithm in the context of application-controlled file caching and prefetching is an interesting problem that has not been solved yet. 31 There are numerous studies on prefetching in parallel I/O systems <ref> [EK89, WYT93] </ref>. Although our work focuses on prefetching with a single disk or server, the "Do No Harm" and "First Opportunity" principles apply to prefetching algorithms in the parallel context as well. We believe these principles are important to avoid the thrashing problem [WYT93].
Reference: [GA94] <author> Jim Griffioen and Randy Appleton. </author> <title> Reducing file system latency using a predictive approach. </title> <booktitle> In Proc. of USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 197-208, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: However, devising fair and efficient policies for allocating file cache space among multiple competing processes remains a difficult problem. Recent studies have also demonstrated the benefits of application-specified prefetch-ing <ref> [TD90, GA94, PGG + 95] </ref>. However, the question of how aggressively to prefetch remains a difficult problem. Finally, how to integrate caching and prefetching policies for optimal performance is a difficult issue. <p> The next section will describe how to extend this integration to the multiple-process case. We focus on applications that can predict their future access patterns. Research has shown that such predictions are often possible in practice <ref> [PG94, GA94, TD90, Smi78] </ref>. 2.1 Integrating Prefetching with Caching Since both application-controlled caching and prefetching rely on knowledge of future access patterns, it is natural to try to integrate them. <p> Despite these differences, both projects utilize disk scheduling, employ similar application interfaces for prefetching, and arrive at similar conclusions about the benefits of prefetching and disk scheduling. There have also been many studies focusing on how to predict future accesses from past accesses <ref> [TD90, PZ91, CKV93, GA94] </ref>. In particular, Griffioen and Appleton's work [GA94] tries to predict future file accesses based on past accesses using "probability graphs," and prefetches accordingly. Few of these studies, however, considered the interaction between prefetching and caching or investigated the combined cache management problem. <p> There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93, GA94]. In particular, Griffioen and Appleton's work <ref> [GA94] </ref> tries to predict future file accesses based on past accesses using "probability graphs," and prefetches accordingly. Few of these studies, however, considered the interaction between prefetching and caching or investigated the combined cache management problem. Several detailed studies of disk scheduling have been done [SCO90, JW91, WGP94].
Reference: [Gra91] <author> Jim Gray. </author> <title> The Benchmark Handbook. </title> <publisher> Morgan Kaufman, </publisher> <year> 1991. </year>
Reference-contexts: If there is no index file, the query will scan the relation tuple file sequentially to find the matching tuples. We used a 200,000 tuple relation from a scaled-up Wisconsin benchmark <ref> [Gra91] </ref>. There is an index on attribute unique1, which is uniquely random in the range 1-200,000.
Reference: [HC92] <author> Kieran Harty and David R. Cheriton. </author> <title> Application-controlled physical memory using external page-cache management. </title> <booktitle> In The Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 187-197, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Although these fixed algorithms generally perform well, application-specific policies can often perform much better. Many researchers have pointed out the advantage of application-specific replacement policies, in the context of both file caching and physical memory management <ref> [CD85, SP91, HC92] </ref>. However, devising fair and efficient policies for allocating file cache space among multiple competing processes remains a difficult problem. Recent studies have also demonstrated the benefits of application-specified prefetch-ing [TD90, GA94, PGG + 95]. However, the question of how aggressively to prefetch remains a difficult problem.
Reference: [HKM + 88] <author> J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols, M. Satya narayanan, R. N. Sidebotham, and M. J. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 6(1) 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The cs1+cs3 and cs3+gli experiments were done with one disk; the cs1+psel2 and cs3+psel1 runs were done with two disks. We also ran our applications concurrently with a copy of the Andrew file system benchmark <ref> [HKM + 88] </ref>: din+andrew, cs2+andrew, gli+andrew and psel2+andrew. The Andrew benchmark runs on the RZ26 disk, so only psel2+andrew involves both disks. The results are summarized in figures 8 and 9.
Reference: [HP90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Only a few lines of code were needed to issue these directives to the kernel. Dinero [din] Dinero is a cache simulator written by Mark Hill and used in Hennessy and Patterson's architecture textbook <ref> [HP90] </ref>. The distribution package for the 19 course material includes the simulator and several program trace files.
Reference: [JW91] <author> David Jacobson and John Wilkes. </author> <title> Disk scheduling algorithms based on rotational positioning. </title> <type> Technical report, </type> <institution> Hewlett Packard Laboratories, HPL-CSP-91-7, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Thus, although "controlled-aggressive" is close to optimal given perfect knowledge, we do not claim that our implementation is close to optimal in real systems. 2.2 Incorporating Disk Scheduling Because of the physical attributes of disks, careful scheduling of disk accesses can provide significant improvement in performance <ref> [SCO90, JW91] </ref>. Without prefetching, scheduling opportunities only come from asynchronous I/O activities or multiple processes. Prefetching provides new opportunities for disk scheduling, because prefetch requests can be generated in groups. However, scheduling prefetching requests is different from scheduling asynchronous I/O requests or requests from multiple processes. <p> Few of these studies, however, considered the interaction between prefetching and caching or investigated the combined cache management problem. Several detailed studies of disk scheduling have been done <ref> [SCO90, JW91, WGP94] </ref>. These studies typically considered a wide variety of scheduling policies under timesharing Unix workloads. However, the policies are for requests queued from multiple processes, not for requests specified in a prefetch-list from a single process.
Reference: [MA90] <author> Dylan McNamee and Katherine Armstrong. </author> <title> Extending the Mach external pager interface to accommodate user-level page replacement policies. </title> <booktitle> In Proceedings of USENIX Mach Symposiumi '91, </booktitle> <pages> pages 17-29, </pages> <year> 1990. </year>
Reference-contexts: Simple schemes perform well, but do not give applications sufficient flexibility in controlling cache replacement. A more general scheme, for example, "upcalls", can give applications complete control over cache replacement, but can incur high overhead <ref> [ABLL92, MA90] </ref>. The main design challenge is to devise an interface that allows applications to exert the control they need, without introducing the overhead that would result from a totally general mechanism.
Reference: [MK91] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like performance from a UNIX file system. </title> <booktitle> In Proc. of 1991 Winter USENIX Symposium, </booktitle> <pages> pages 33-43, </pages> <year> 1991. </year>
Reference-contexts: We plan to continue working on this problem. Several recent research projects have tried to improve file system performance in a number of other ways, including log-structured file systems [RO91], disk block clustering <ref> [MK91, SS95] </ref>, and delayed writeback [Mog94]. Most of these papers still assume global LRU as the basic cache replacement policies and sequential one-block lookahead or large I/O units as the primary prefetch-ing techniques. They do not address how to integrate prefetching and disk scheduling with application-controlled file caching.
Reference: [Mog94] <author> Jeffrey C. Mogul. </author> <title> A better update policy. </title> <booktitle> In Proc. of 1994 Summer USENIX Technical Conference, </booktitle> <pages> pages 99-111, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: We plan to continue working on this problem. Several recent research projects have tried to improve file system performance in a number of other ways, including log-structured file systems [RO91], disk block clustering [MK91, SS95], and delayed writeback <ref> [Mog94] </ref>. Most of these papers still assume global LRU as the basic cache replacement policies and sequential one-block lookahead or large I/O units as the primary prefetch-ing techniques. They do not address how to integrate prefetching and disk scheduling with application-controlled file caching.
Reference: [MW94] <author> Udi Manber and Sun Wu. GLIMPSE: </author> <title> A tool to search through entire file systems. </title> <booktitle> In Proc. of USENIX Winter 1994 Technical Conference, </booktitle> <pages> pages 23-32, </pages> <month> January </month> <year> 1994. </year> <month> 34 </month>
Reference-contexts: Dinero reads the trace file sequentially for each simulation. Thus, the caching strategy is MRU on the trace file. For prefetching, we simply pass the trace file name to the kernel. Glimpse [gli] Glimpse is a text information retrieval system <ref> [MW94] </ref>. It builds approximate indices for words to allow relatively fast search with small index files. We took a snapshot of news articles in several comp.* newsgroups on May 22, 1994, consisting of about 40 MBytes of text in all.
Reference: [OOW93] <author> Elizabeth J. O'Neil, Patrick E. O'Neil, and Gerhard Weikum. </author> <title> The LRU-K page replacement algorithm for database disk buffer ing. </title> <booktitle> In ACM SIGMOD Conference on the Management of Data, </booktitle> <pages> pages 297-306, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We believe these principles are important to avoid the thrashing problem [WYT93]. The database community has long studied access patterns and buffer replacement <ref> [Sto81, CD85, OOW93] </ref> and prefetching [CKV93, PZ91] policies. However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they do not address the integration of caching, prefetching and disk scheduling in multi-process environments.
Reference: [PG94] <author> R. Hugo Patterson and Garth A. Gibson. </author> <title> Exposing I/O concurrency witn informed prefetching. </title> <booktitle> In Proc. Third Intl. Conf. on Parallel and Distributed Information Systems, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: The next section will describe how to extend this integration to the multiple-process case. We focus on applications that can predict their future access patterns. Research has shown that such predictions are often possible in practice <ref> [PG94, GA94, TD90, Smi78] </ref>. 2.1 Integrating Prefetching with Caching Since both application-controlled caching and prefetching rely on knowledge of future access patterns, it is natural to try to integrate them. <p> Using real applications it shows that the techniques together generate considerable synergy and provide significant improvement on file cache performance. Recently there have been a number of research projects on prefetching in file systems. Patterson and Gibson's informed prefetching and caching <ref> [PG94, PGG + 95] </ref> is similar to our study. However, there are a number of major differences. Instead of providing a mechanism for applications to control file cache replacement, informed prefetching and caching tries to "infer" good replacement decisions from the reference list passed from the application to the kernel.
Reference: [PGG + 95] <author> R. Hugo Patterson, Garth A. Gibson, Eka Ginting, Daniel Stodolsky, and Jim Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proceedings of 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: However, devising fair and efficient policies for allocating file cache space among multiple competing processes remains a difficult problem. Recent studies have also demonstrated the benefits of application-specified prefetch-ing <ref> [TD90, GA94, PGG + 95] </ref>. However, the question of how aggressively to prefetch remains a difficult problem. Finally, how to integrate caching and prefetching policies for optimal performance is a difficult issue. <p> Using real applications it shows that the techniques together generate considerable synergy and provide significant improvement on file cache performance. Recently there have been a number of research projects on prefetching in file systems. Patterson and Gibson's informed prefetching and caching <ref> [PG94, PGG + 95] </ref> is similar to our study. However, there are a number of major differences. Instead of providing a mechanism for applications to control file cache replacement, informed prefetching and caching tries to "infer" good replacement decisions from the reference list passed from the application to the kernel.
Reference: [Pos93] <author> Postgres Group. </author> <title> POSTGRES version 4.1 release notes. </title> <type> Technical re port, </type> <institution> Electronics Research Lab, University of California, Berkeley, </institution> <month> Febru-ary </month> <year> 1993. </year>
Reference-contexts: As with CScope, only five lines of code were needed to pass the file list to the kernel for prefetching. Postgres [psel1-2] Postgres is a relational database system from the University of California at Berkeley <ref> [Pos93] </ref>. Postgres uses indices whenever possible to optimize query execution. A selection query, for example, has two possible access patterns: indexed or sequential. If there is an index, the selection query will use the index to find all tuples satisfying the queried condition.
Reference: [PZ91] <author> Mark Palmer and Stanley B. Zdonik. </author> <title> Fido: A cache that learns to fetch. </title> <booktitle> In Proc. of 17th Intl. Conf. on Very Large Data Bases, </booktitle> <pages> pages 255-264, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Despite these differences, both projects utilize disk scheduling, employ similar application interfaces for prefetching, and arrive at similar conclusions about the benefits of prefetching and disk scheduling. There have also been many studies focusing on how to predict future accesses from past accesses <ref> [TD90, PZ91, CKV93, GA94] </ref>. In particular, Griffioen and Appleton's work [GA94] tries to predict future file accesses based on past accesses using "probability graphs," and prefetches accordingly. Few of these studies, however, considered the interaction between prefetching and caching or investigated the combined cache management problem. <p> We believe these principles are important to avoid the thrashing problem [WYT93]. The database community has long studied access patterns and buffer replacement [Sto81, CD85, OOW93] and prefetching <ref> [CKV93, PZ91] </ref> policies. However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they do not address the integration of caching, prefetching and disk scheduling in multi-process environments.
Reference: [RL92] <author> Anne Rogers and Kai Li. </author> <title> Software support for speculative loads. </title> <booktitle> In Pro ceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 38-50, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they do not address the integration of caching, prefetching and disk scheduling in multi-process environments. Finally, prefetching in uni-processor and multi-processor computer architectures <ref> [RL92, CB92, CKP91, Smi78, TE93] </ref> is similar to prefetching in file systems. However, in these systems there is little flexibility in cache management, as the cache is usually direct-mapped or has very limited associativity.
Reference: [RO91] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The design and implemen tation of a log-structured file system. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 1-15, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We plan to continue working on this problem. Several recent research projects have tried to improve file system performance in a number of other ways, including log-structured file systems <ref> [RO91] </ref>, disk block clustering [MK91, SS95], and delayed writeback [Mog94]. Most of these papers still assume global LRU as the basic cache replacement policies and sequential one-block lookahead or large I/O units as the primary prefetch-ing techniques.
Reference: [SCO90] <author> Margo Seltzer, Peter Chen, and John Ousterhout. </author> <title> Disk scheduling re visited. </title> <booktitle> In Proc. of USENIX Winter 1990 Technical Conference, </booktitle> <pages> pages 313-324, </pages> <year> 1990. </year>
Reference-contexts: Thus, although "controlled-aggressive" is close to optimal given perfect knowledge, we do not claim that our implementation is close to optimal in real systems. 2.2 Incorporating Disk Scheduling Because of the physical attributes of disks, careful scheduling of disk accesses can provide significant improvement in performance <ref> [SCO90, JW91] </ref>. Without prefetching, scheduling opportunities only come from asynchronous I/O activities or multiple processes. Prefetching provides new opportunities for disk scheduling, because prefetch requests can be generated in groups. However, scheduling prefetching requests is different from scheduling asynchronous I/O requests or requests from multiple processes. <p> Few of these studies, however, considered the interaction between prefetching and caching or investigated the combined cache management problem. Several detailed studies of disk scheduling have been done <ref> [SCO90, JW91, WGP94] </ref>. These studies typically considered a wide variety of scheduling policies under timesharing Unix workloads. However, the policies are for requests queued from multiple processes, not for requests specified in a prefetch-list from a single process.
Reference: [Smi78] <author> Alan Jay Smith. </author> <title> Sequential program prefetching in memory hierarchies. </title> <journal> IEEE Computer, </journal> <volume> 11(12) </volume> <pages> 7-21, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: The next section will describe how to extend this integration to the multiple-process case. We focus on applications that can predict their future access patterns. Research has shown that such predictions are often possible in practice <ref> [PG94, GA94, TD90, Smi78] </ref>. 2.1 Integrating Prefetching with Caching Since both application-controlled caching and prefetching rely on knowledge of future access patterns, it is natural to try to integrate them. <p> However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they do not address the integration of caching, prefetching and disk scheduling in multi-process environments. Finally, prefetching in uni-processor and multi-processor computer architectures <ref> [RL92, CB92, CKP91, Smi78, TE93] </ref> is similar to prefetching in file systems. However, in these systems there is little flexibility in cache management, as the cache is usually direct-mapped or has very limited associativity.
Reference: [SP91] <author> Stuart Sechrest and Yoonho Park. </author> <title> User-level physical memory manage ment for Mach. </title> <booktitle> In Proceedings of the 1991 USENIX Mach Symposium, </booktitle> <pages> pages 189-199, </pages> <year> 1991. </year>
Reference-contexts: Although these fixed algorithms generally perform well, application-specific policies can often perform much better. Many researchers have pointed out the advantage of application-specific replacement policies, in the context of both file caching and physical memory management <ref> [CD85, SP91, HC92] </ref>. However, devising fair and efficient policies for allocating file cache space among multiple competing processes remains a difficult problem. Recent studies have also demonstrated the benefits of application-specified prefetch-ing [TD90, GA94, PGG + 95]. However, the question of how aggressively to prefetch remains a difficult problem.
Reference: [SS95] <author> Margo Seltzer and Keith A. Smith. </author> <title> File system logging versus clustering: A performance comparision. </title> <booktitle> In Proceedings of 1995 Summer USENIX, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: We plan to continue working on this problem. Several recent research projects have tried to improve file system performance in a number of other ways, including log-structured file systems [RO91], disk block clustering <ref> [MK91, SS95] </ref>, and delayed writeback [Mog94]. Most of these papers still assume global LRU as the basic cache replacement policies and sequential one-block lookahead or large I/O units as the primary prefetch-ing techniques. They do not address how to integrate prefetching and disk scheduling with application-controlled file caching.
Reference: [Ste85] <author> Joseph L. Steffen. </author> <title> Interactive examination of a c program with cscope. </title> <booktitle> In USENIX Dallas 1985 Winter Conference Proceedings, </booktitle> <pages> pages 170-175, </pages> <year> 1985. </year>
Reference-contexts: We begin by describing the applications, their file access patterns, and the application-controlled caching and prefetching strategies used for each. CScope [cs1-3] CScope is an interactive C-source examination tool written by Joe Stef-fen <ref> [Ste85] </ref>. It builds a database of all source files, then uses the database to answer queries about the program. There are two kinds of queries: symbol-oriented queries and egrep-like text search. We used cscope on two operating system kernel sources of about 18 Mbytes and 10 Mbytes, respectively.
Reference: [Sto81] <author> Michael Stonebraker. </author> <title> Operating system support for database manage ment. </title> <journal> Communications of the ACM, v. </journal> <volume> 24, no. 7, </volume> <pages> pages 412-418, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: We believe these principles are important to avoid the thrashing problem [WYT93]. The database community has long studied access patterns and buffer replacement <ref> [Sto81, CD85, OOW93] </ref> and prefetching [CKV93, PZ91] policies. However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they do not address the integration of caching, prefetching and disk scheduling in multi-process environments.
Reference: [TD90] <author> Carl D. Tait and Dan Duchamp. </author> <title> Detection and exploitation of file working sets. </title> <type> Technical Report CUCS-050-90, </type> <institution> Computer Science Dept., Columbia University, </institution> <year> 1990. </year>
Reference-contexts: However, devising fair and efficient policies for allocating file cache space among multiple competing processes remains a difficult problem. Recent studies have also demonstrated the benefits of application-specified prefetch-ing <ref> [TD90, GA94, PGG + 95] </ref>. However, the question of how aggressively to prefetch remains a difficult problem. Finally, how to integrate caching and prefetching policies for optimal performance is a difficult issue. <p> The next section will describe how to extend this integration to the multiple-process case. We focus on applications that can predict their future access patterns. Research has shown that such predictions are often possible in practice <ref> [PG94, GA94, TD90, Smi78] </ref>. 2.1 Integrating Prefetching with Caching Since both application-controlled caching and prefetching rely on knowledge of future access patterns, it is natural to try to integrate them. <p> Despite these differences, both projects utilize disk scheduling, employ similar application interfaces for prefetching, and arrive at similar conclusions about the benefits of prefetching and disk scheduling. There have also been many studies focusing on how to predict future accesses from past accesses <ref> [TD90, PZ91, CKV93, GA94] </ref>. In particular, Griffioen and Appleton's work [GA94] tries to predict future file accesses based on past accesses using "probability graphs," and prefetches accordingly. Few of these studies, however, considered the interaction between prefetching and caching or investigated the combined cache management problem.
Reference: [TE93] <author> Dean M. Tullsen and Susan J. Eggers. </author> <title> Limitations of cache prefetching on a bus-based multiprocessor. </title> <booktitle> In Proceedings of 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 278-288, </pages> <month> May </month> <year> 1993. </year> <month> 35 </month>
Reference-contexts: However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they do not address the integration of caching, prefetching and disk scheduling in multi-process environments. Finally, prefetching in uni-processor and multi-processor computer architectures <ref> [RL92, CB92, CKP91, Smi78, TE93] </ref> is similar to prefetching in file systems. However, in these systems there is little flexibility in cache management, as the cache is usually direct-mapped or has very limited associativity. <p> File systems, on the other hand, can change their cache management algorithms freely and can spare more cycles for calculating a good replacement or prefetching decision, as the potential savings are substantial. On the other hand, Tullsen and Eggers <ref> [TE93] </ref> showed that thrashing is a problem when prefetching in bus-based multiprocessor caches, suggesting that the "Do No Harm" rule applies in those systems as well. 7 Conclusions We have presented the design, implementation and performance of a file system incorporating integrated application-controlled file caching, prefetching and disk scheduling.
Reference: [WGP94] <author> Bruck L. Worthington, Gregory R. Ganger, and Yale N. Patt. </author> <title> Schedul ing for modern disk drives and non-random workloads. </title> <type> Technical Report CSE-TR-194-94, </type> <institution> Dept. of Electrical Engineering and Computer Science, University of Michigan, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Nevertheless, studies show that fetching in order of logical block numbers tends to work well <ref> [WGP94] </ref>. The mechanisms we have described so far work well in the single-process case. We now consider what to do when multiple processes are running at the same time. 3 Integration for Multi-Process Case The situation is more complicated in the multi-process case than in the single-process case. <p> Few of these studies, however, considered the interaction between prefetching and caching or investigated the combined cache management problem. Several detailed studies of disk scheduling have been done <ref> [SCO90, JW91, WGP94] </ref>. These studies typically considered a wide variety of scheduling policies under timesharing Unix workloads. However, the policies are for requests queued from multiple processes, not for requests specified in a prefetch-list from a single process.
Reference: [WYT93] <author> Kun-Ling Wu, Philip S. Yu, and James Z. Teng. </author> <title> Performance compar ison of thrashing control policies for concurrent mergesorts with parallel prefetching. </title> <booktitle> In Proc. of 1993 ACM SIGMETRICS, </booktitle> <pages> pages 171-182, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: On the other hand, choosing the best writeback algorithm in the context of application-controlled file caching and prefetching is an interesting problem that has not been solved yet. 31 There are numerous studies on prefetching in parallel I/O systems <ref> [EK89, WYT93] </ref>. Although our work focuses on prefetching with a single disk or server, the "Do No Harm" and "First Opportunity" principles apply to prefetching algorithms in the parallel context as well. We believe these principles are important to avoid the thrashing problem [WYT93]. <p> Although our work focuses on prefetching with a single disk or server, the "Do No Harm" and "First Opportunity" principles apply to prefetching algorithms in the parallel context as well. We believe these principles are important to avoid the thrashing problem <ref> [WYT93] </ref>. The database community has long studied access patterns and buffer replacement [Sto81, CD85, OOW93] and prefetching [CKV93, PZ91] policies.
References-found: 37

