URL: http://www.ri.cmu.edu/afs/cs/project/iwarp/archive/iWarp-papers/super91-blocked-lalgebra-parallel.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/jass/www/papers.html
Root-URL: 
Date: Nov 1991  
Note: This research was supported in part by the Defense Advanced Research Projects Agency (DOD) monitored by DARPA/CMO under Contract MDA972-90-C-0035, and in part by the Office of Naval Research un der Contract N00014-90-J-1939. To appear in Proceedings of Supercomputing 91, Albuquerque, NM,  
Abstract: This paper describes a new approach for automatic generation of efficient parallel programs from sequential blocked linear algebra programs. By exploiting recent progress in fine-grain parallel architectures such as iWarp, and in libraries based on matrix-matrix block operations such as LAPACK, the approach is expected to be effective in parallelizing a large class of linear algebra computations. An implementation of LAPACK on iWarp is under development. In the implementation, block routines are executed on the iWarp processor array using highly parallel systolic algorithms. Matrices are distributed over the array in a way that allows parallel block routines to be used wherever the original program calls a sequential block routine. This data distribution scheme significantly simplifies the process of parallelization, and as a result, efficient parallel versions of programs can be generated automatically. We discuss experiences and performance results from our preliminary implementation, and present the design of a fully automatic system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Annaratone, E. Arnould, T. Gross, H. T. Kung, M. Lam, O. Menzilcioglu, and J. A. Webb. </author> <title> The Warp Computer: Architecture, Implementation, and Performance. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36 (12): </volume> <pages> 1523-1538, </pages> <month> De-cember </month> <year> 1987. </year>
Reference-contexts: On iWarp, the computation and communication steps discussed above can be overlapped, giving near peak performance even for small sub-block sizes. This has also been demonstrated on Warp <ref> [1] </ref>, a prototype systolic array machine that led to iWarp.
Reference: [2] <author> S. Borkar, R. Cohn, G. Cox, S. Gleason, T. Gross, H. T. Kung, M. Lam, B. Moore, C. Peterson, J. Pieper, L. Rankin, P. S. Tseng, J. Sutton, J. Urbanski, and J.Webb. </author> <title> iWarp: An integrated solution to high-speed parallel computing. </title> <booktitle> In Proceedings of the Supercomputing Conference, </booktitle> <pages> pages 330--339, </pages> <month> November </month> <year> 1988. </year>
Reference: [3] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J.Webb, </author> <title> Supporting systolic and memory communication in iWarp, </title> <booktitle> in Proceedings of the 17th Annual International Symposium on Computer Architecture, pages 70--81, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference: [4] <author> M. Dayde and I. Duff. </author> <title> Level 3 BLAS in LU Factorization on the CRAY-2, ETA-10P, </title> <journal> and IBM3090-200/VF. The International Journal of Supercomputer Applications, </journal> <volume> 3(2):40--70, </volume> <year> 1989. </year>
Reference-contexts: In the next section, we will discuss the results in more detail. 5.1 Algorithm Overview A high level description of the program is outlined in Figure 5. For a detailed description of this and other block algorithms for LU decomposition, see <ref> [4] </ref>. The exact amount of computation that is done in the different steps in the program depends on the block size. However, for reasonable block sizes, maximum computation is done in the matrix multiplication routine. 5.2 Implementation We discuss the implementation of the main steps in the program above.
Reference: [5] <author> M. Dayde and I. Duff. </author> <title> Use of parallel level 3 BLAS in LU Factorization on three vector multiprocessors the Alliant FX/80, the CRAY2, and the IBM 3090 VF. </title> <booktitle> In Proceedings of the 1990 International Conference on SUPERCOMPUTING, pages 82--95, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference: [6] <author> J. Dongarra, J. Du Croz, I. Duff, and S.Hammarling,. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16 </volume> (1):1-17, March 1990. 
Reference: [7] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> An extended set of fortran basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14 </volume> (1):1-17, March 1988. 
Reference: [8] <author> J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, and D. Sorensen. </author> <title> Prospectus for the development of a linear algebra library for high-performance computers. </title> <type> Technical Report ANL-MCS-TM-97, </type> <institution> Argonne National Lab, </institution> <month> September </month> <year> 1987. </year>
Reference: [9] <author> J. Dongarra and S. Ostrouchov. </author> <title> Lapack block factorization algorithms on the Intel IPSC/860. </title> <type> Technical Report CS-90-115, </type> <institution> Computer Science Department, University of Tennes-see, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: Note that the performance approaches the peak of the 64-processor iWarp, which is about 1.2 GFLOPS, even for moderate matrix sizes. In the figure this is contrasted to the performance curve <ref> [9] </ref> for a coarse-grain parallel machine which has a peak performance larger than 1.2 GFLOPS 4 Implementation on iWarp The approach described in this paper is being implementat-ed on a 8 8 iWarp array at Carnegie Mellon University. <p> However, for large problem sizes, the computation to communication ratio is much higher, and performance is mainly determined by the peak oating point performance of the processors. For instance, comparing iWarp to a conventional hypercube with the same number of processors, we expect performance characteristics plotted in Figure 4 <ref> [9] </ref>. We see that iWarp is expected to perform much better for small matrix sizes. 7 Automatic Parallelization The key to achieving good performance using our methodology is to make calls to parallel block routines whenever possible.
Reference: [10] <author> W. Gentleman and H. T. Kung. </author> <title> Matrix triangularization by systolic arrays. </title> <booktitle> In Proceedings of SPIE Symposium, </booktitle> <volume> Vol. 298, </volume> <booktitle> Real-Time Signal Processing IV, </booktitle> <pages> pages 19-26, </pages> <month> August </month> <year> 1981. </year>
Reference-contexts: We believe that this approach can be applied to building practically useful tools for parallelizing linear algebra programs using block routines. Based on automatic parallelization of block operations, the approach is effective for three reasons: (1) Efficient systolic algorithms exist for parallelizing block operations such as matrix multiplication <ref> [10] </ref>, [13], [14]. (2) Fine-grain distributed memory parallel machines capable of efficient execution of systolic algorithms have become available such as iWarp [2],[3]. (iWarp is commercially available from Intel.) (3) Libraries written using block routines for linear algebra computations have been developed such as LAPACK [6],[8].
Reference: [11] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> Impact of hierarchical memory systems on linear algebra algorithm design. </title> <journal> The International Journal of Supercomputer Applications,3(2):40--70, </journal> <year> 1989. </year>
Reference: [12] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine independent parallel programming in Fortran D. </title> <type> Technical Report TR90-149, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> February </month> <year> 1991. </year>
Reference: [13] <author> H. T. Kung,. </author> <title> Why Systolic Architectures?. </title> <journal> Computer Magazine, </journal> <volume> 15 (1): </volume> <pages> 37-46, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: We believe that this approach can be applied to building practically useful tools for parallelizing linear algebra programs using block routines. Based on automatic parallelization of block operations, the approach is effective for three reasons: (1) Efficient systolic algorithms exist for parallelizing block operations such as matrix multiplication [10], <ref> [13] </ref>, [14]. (2) Fine-grain distributed memory parallel machines capable of efficient execution of systolic algorithms have become available such as iWarp [2],[3]. (iWarp is commercially available from Intel.) (3) Libraries written using block routines for linear algebra computations have been developed such as LAPACK [6],[8].
Reference: [14] <author> H.T. Kung and C. Leiserson. </author> <title> Systolic Arrays (for VLSI). In Sparse Matrix Proceedings 1978, </title> <note> edited by I. </note> <author> S. Duff and G. W. Stewart. </author> <note> A slightly different version appears in Introduction to VLSI Systems by C. </note> <editor> A. Mead and L. A. Conway, </editor> <publisher> Addison-Wesley, </publisher> <year> 1980, </year> <journal> Section 8.3, </journal> <pages> pp. 37-46. </pages>
Reference-contexts: Based on automatic parallelization of block operations, the approach is effective for three reasons: (1) Efficient systolic algorithms exist for parallelizing block operations such as matrix multiplication [10], [13], <ref> [14] </ref>. (2) Fine-grain distributed memory parallel machines capable of efficient execution of systolic algorithms have become available such as iWarp [2],[3]. (iWarp is commercially available from Intel.) (3) Libraries written using block routines for linear algebra computations have been developed such as LAPACK [6],[8].
Reference: [15] <author> C. Lawson, R. Hanson, R. Kincaid, and F. Krogh. </author> <title> Basic linear algebra subprograms for fortran usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16: 308--323, </volume> <year> 1979. </year>
Reference: [16] <author> H. Ribas. </author> <title> Automatic Generation of Systolic Programs from Nested Loops. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical and Computer Engineering, Carnegie Mellon University, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: This has also been demonstrated on Warp [1], a prototype systolic array machine that led to iWarp. More precisely, it has been shown that for many matrix operations, systolic algorithms can achieve near linear speedups on Warp <ref> [16] </ref>. 3 Theoretical Performance Analysis This section shows that with the approach outlined above, we can expect to achieve large speedups even for matrices whose size is not much larger than the size of the processor array.
Reference: [17] <author> P.S. Tseng. </author> <title> A Parallelizing Compiler for Distributed Memory Parallel Computers, </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical and Computer Engineering, Carnegie Mellon University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: This requires developing a conventional parallelizing compiler to handle code outside of block routines. We already have a parallelizing compiler available to us for experimentation <ref> [17] </ref> and it has been successful in getting good speedups on many linear algebra programs. We are currently in the process of transferring and enhancing those ideas in a par-allelizing Fortran compiler. This system would be the other main component of a fully automatic parallelizing system.
References-found: 17

