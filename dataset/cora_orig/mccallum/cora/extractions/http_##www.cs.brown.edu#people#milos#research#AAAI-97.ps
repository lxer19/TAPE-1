URL: http://www.cs.brown.edu/people/milos/research/AAAI-97.ps
Refering-URL: http://www.cs.brown.edu/people/milos/papers.html
Root-URL: http://www.cs.brown.edu/
Email: milos@medg.lcs.mit.edu  
Title: Incremental methods for computing bounds in partially observable Markov decision processes  
Author: Milos Hauskrecht 
Address: NE43-421 545 Technology Square Cambridge, MA 02139  
Affiliation: MIT Laboratory for Computer Science,  
Abstract: Partially observable Markov decision processes (POMDPs) allow one to model complex dynamic decision or control problems that include both action outcome uncertainty and imperfect observabil-ity. The control problem is formulated as a dynamic optimization problem with a value function combining costs or rewards from multiple steps. In this paper we propose, analyse and test various incremental methods for computing bounds on the value function for control problems with infinite discounted horizon criteria. The methods described and tested include novel incremental versions of grid-based linear interpolation method and simple lower bound method with Sondik's updates. Both of these can work with arbitrary points of the belief space and can be enhanced by various heuristic point selection strategies. Also introduced is a new method for computing an initial upper bound the fast informed bound method. This method is able to improve significantly on the standard and commonly used upper bound computed by the MDP-based method. The quality of resulting bounds are tested on a maze navigation problem with 20 states, 6 actions and 8 observations. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Astrom, K.J. </author> <year> 1965. </year> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> Journal of Mathematical Analysis and Applications 10: </journal> <pages> 174-205. </pages>
Reference: <author> Cassandra, A.R. </author> <year> 1994. </year> <title> Optimal policies for partially observable Markov decision processes. </title> <type> Technical report CS-94-14, </type> <institution> Brown University. </institution>
Reference: <author> Hauskrecht, M. </author> <year> 1996. </year> <title> Planning and control in stochastic domains with imperfect information. </title> <type> PhD thesis proposal, </type> <institution> EECS, MIT. </institution>
Reference-contexts: Thus in order to decide about the best action one needs to consider costs and benefits associated with both control and investigative actions. Problems with such characteristics include robot navigation (Littman, Cassandra and Kaelbling 95a), disease treatment <ref> (Hauskrecht 96) </ref> and various fault repair problems. A framework that can model both sources of uncertainty and can represent both investigative and 1 Copyright c fl1997, American Association for Artificial Intelligence (www.aaai.org). <p> For example bounds can be used to speed up the on-line decision (control) methods that compute an optimal or *-optimal control action for a specific belief state via forward expansion of the decision tree <ref> (Hauskrecht 96) </ref>. Good value function bounds often reduce the size of the tree explored via branch and bound strategies and allow one to cut the suboptimal action branches from the active tree. Finally value function bounds can be used as good approximations of an optimal value function. <p> As it holds that b V i (b) s b (s)ff b (s) V fl (b), one can easily construct a new value function b V i+1 b V i by simply updating i to: i+1 = i [ ff b <ref> (Hauskrecht 96) </ref>. Such a value function update guarantees that the new lower bound is improved. Note that after adding new linear vector to i some of the previous linear vectors can become redundant. To fix that the update step can be combined with various redundancy check procedures. <p> set defining current bound b V to init repeat until the stopping criterion is met select a belief point b compute new update ff b for b add the ff b to return b V The initial lower bound b V init can be computed via the blind policy method <ref> (Hauskrecht 96) </ref>. The main idea of the method is to compute value functions for all "one-action" (or blind) policies. These correspond to simple linear value functions that lower bound the optimal one and can be easily computed within the fully observable MDP framework.
Reference: <author> Hauskrecht, M. </author> <year> 1997a. </year> <title> Dynamic decision making in stochastic partially observable medical domains: Ischemic heart disease example. </title> <booktitle> In Procedings of AIME-97. </booktitle>
Reference-contexts: Especially important with regard to control are suboptimal bounds that can guarantee minimal expected reward. However, the evaluation of the quality of various bound functions for approximating control and their comparison to alternative methods is a subject of another paper <ref> (Hauskrecht 97b) </ref>. Test example: Maze20 For the purpose of testing and illustrating results, we have built a toy robot maze navigation problem with 20 states, 6 actions and 8 observations. The maze (figure 1) consists of 20 partially connected rooms (states) in which a robot functions and collects rewards.
Reference: <author> Hauskrecht, M. </author> <year> 1997b. </year> <title> Approximation methods for solving control problems in partially observable Markov decision processes. </title> <type> Technical Memo. </type> <institution> MIT-LCS-TM-565. </institution>
Reference-contexts: Especially important with regard to control are suboptimal bounds that can guarantee minimal expected reward. However, the evaluation of the quality of various bound functions for approximating control and their comparison to alternative methods is a subject of another paper <ref> (Hauskrecht 97b) </ref>. Test example: Maze20 For the purpose of testing and illustrating results, we have built a toy robot maze navigation problem with 20 states, 6 actions and 8 observations. The maze (figure 1) consists of 20 partially connected rooms (states) in which a robot functions and collects rewards.
Reference: <author> Littman, </author> <title> M.L.; Cassandra, A.R.; Kaelbling, L.P. 1995a. Learning policies for partially observable environmets: scaling up. </title> <booktitle> In Proceedings of the 12-th international conference on Machine Learning. </booktitle>
Reference-contexts: These can be more or less informative and can come with different costs. Thus in order to decide about the best action one needs to consider costs and benefits associated with both control and investigative actions. Problems with such characteristics include robot navigation <ref> (Littman, Cassandra and Kaelbling 95a) </ref>, disease treatment (Hauskrecht 96) and various fault repair problems. A framework that can model both sources of uncertainty and can represent both investigative and 1 Copyright c fl1997, American Association for Artificial Intelligence (www.aaai.org). <p> Moreover, the computation of a set of all useful linear segments of V i+1 can be solved efficiently only when RP = N P <ref> (Littman, Cassandra and Kaelbling 95b) </ref>. The role of value function bounds The computational inefficiency of exact value function updates as well as general *-optimal solutions leads naturally to the exploration of various approximations and shortcuts that can speed up exact methods or provide good control solutions with less computation.
Reference: <author> Littman, </author> <title> M.L.; Cassandra, A.R.; Kaelbling, L.P. 1995b. Efficient dynamic programming updates in partially observable Markov decision processes. </title> <note> submitted to Operations Research. </note>
Reference-contexts: These can be more or less informative and can come with different costs. Thus in order to decide about the best action one needs to consider costs and benefits associated with both control and investigative actions. Problems with such characteristics include robot navigation <ref> (Littman, Cassandra and Kaelbling 95a) </ref>, disease treatment (Hauskrecht 96) and various fault repair problems. A framework that can model both sources of uncertainty and can represent both investigative and 1 Copyright c fl1997, American Association for Artificial Intelligence (www.aaai.org). <p> Moreover, the computation of a set of all useful linear segments of V i+1 can be solved efficiently only when RP = N P <ref> (Littman, Cassandra and Kaelbling 95b) </ref>. The role of value function bounds The computational inefficiency of exact value function updates as well as general *-optimal solutions leads naturally to the exploration of various approximations and shortcuts that can speed up exact methods or provide good control solutions with less computation.
Reference: <author> Lovejoy, W.S. </author> <year> 1991. </year> <title> Computationally feasible bounds for partially observed Markov decision processes. </title> <journal> Operations Research 39(1) </journal> <pages> 192-175. </pages>
Reference-contexts: After reaching the goal state, the robot is placed with some probability into one of the `initial" rooms. Computing upper bound A standard method for computing an upper bound combines point interpolation techniques and value iteration strategy (see <ref> (Lovejoy 91) </ref> (Lovejoy 93)). In this method a value function is represented nonparametri-cally using a set of grid points together with their val ues and an interpolation rule that estimates the value at non-grid points with a convex combination of jSj grid points. <p> It can be shown (see <ref> (Lovejoy 91) </ref>) that H inter is a contraction mapping that preserves the upper bound, i.e. starting from an initial upper bound every new value function in the iteration method is guaranteed to be an upper bound.
Reference: <author> Lovejoy, W.S. </author> <year> 1993. </year> <title> Suboptimal policies with bounds for parameter adaptive decision processes. </title> <journal> Operations Research 41(3) </journal> <pages> 583-599. </pages>
Reference-contexts: After reaching the goal state, the robot is placed with some probability into one of the `initial" rooms. Computing upper bound A standard method for computing an upper bound combines point interpolation techniques and value iteration strategy (see (Lovejoy 91) <ref> (Lovejoy 93) </ref>). In this method a value function is represented nonparametri-cally using a set of grid points together with their val ues and an interpolation rule that estimates the value at non-grid points with a convex combination of jSj grid points.
Reference: <author> Parr, R.; Russell, S. </author> <year> 1995. </year> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of IJCAI-97, </booktitle> <pages> 1088-1094. </pages>
Reference: <author> Smallwood, R.D.; Sondik, E.J. </author> <year> 1973. </year> <title> The optimal control of Partially observable processes over a finite horizon. </title> <journal> Operations Research 21 </journal> <pages> 1071-1088. </pages>
Reference-contexts: This is based on the result in <ref> (Smallwood and Sondik 73) </ref> showing that the value function for a belief state can be computed as: V i+1 (b) = a2A s2S X X P (s 0 ; ojs; a)ff (b;a;o) where (b; a; o) indexes a linear vector ff i in a set of linear vectors i that maximizes: <p> Computing lower bounds A lower bound on the optimal value function for infinite horizon discounted problem can be acquired using a simple method that updates derivatives (linear vectors) for belief points using equation 3 <ref> (Smallwood and Sondik 73) </ref>.
References-found: 11

