URL: http://www.pdl.cs.cmu.edu/PDL-FTP/NASD/CMU-CS-97-118.ps
Refering-URL: http://www.pdl.cs.cmu.edu/Publications/publications.html
Root-URL: 
Title: Secure Disks  
Author: Garth A. Gibson, David F. Nagle, Khalil Amiri, Fay W. Chang, Howard Gobioff, Erik Riedel, David Rochberg, and Jim Zelenka 
Note: Network-Attached  
Address: Pittsburgh, Pennsylvania 15213-3890  
Affiliation: School of Computer Science Carnegie Mellon University  
Pubnum: July1997 CMU-CS-97-118  
Email: garth+nasd@cs.cmu.edu  
Web: http://www.cs.cmu.edu/Web/Groups/NASD  
Abstract: Network-attached storage enables network-striped data transfers directly between client and storage to provide clients with scalable bandwidth on large transfers. Network-attached storage also decouples policy and enforcement of access control, avoiding unnecessary reverification of protection checks, reducing file manager work and increasing scalability. It eliminates the expense of a server computer devoted to copying data between peripheral network and client network. This architecture better matches storage technologys sustained data rates, now 80 Mb/s and growing at 40% per year. Finally, it enables self-managing storage to counter the increasing cost of data management. The availability of cost-effective network-attached storage depends on it becoming a storage commodity, which in turn depends on its utility to a broad segment of the storage market. Specifically, multiple distributed and parallel filesystems must benefit from network-attached storages requirement for secure, direct access between client and storage, for reusable, asynchronous access protection checks, and for increased license to efficiently manage underlying storage media. In this paper, we describe a prototype network-attached secure disk interface and filesystems adapted to network-attached storage implementing Suns NFS, Transarcs AFS, a network-striped NFS variant, and an informed prefetching NFS variant. Our experimental implementations demonstrate bandwidth and workload scaling and aggressive optimization of application access patterns. Our experience with applications and filesystems adapted to run on network-attached secure disks emphasizes the much greater cost of client network messaging relative to peripheral bus messaging, which offsets some of the expected scaling results. Filesystems for This research is sponsored by DARPA/ITO through ARPA Order D306, and issued by Indian Head Division, NSWC under contract N00174-96-0002. Additional support was provided by NSF and ONR graduate fellowships. The project team is indebted to generous contributions from the member companies of the Parallel Data Consortium. At the time of this writing, these companies include Hewlett-Packard Laboratories, Symbios Logic Inc., Data General, Compaq, IBM Corporation, Seagate Technology, and Storage Technology Corporation. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of any supporting organization or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [Anderson95] <author> Anderson, D., Seagate Technology Inc., </author> <type> Personal communication, </type> <year> 1995. </year>
Reference-contexts: Fast packetized storage interconnects such as Fibrechannel [Benner96] promise that storage devices, including individual disk drives <ref> [Anderson95] </ref>, can be cost-effectively attached to networks, and the proposed requirement for cryptographic support in version 6 of the Internet Protocol promises cost-effective network support for security [Deering95].
Reference: [Baker91] <author> Baker, M.G. et al., </author> <title> Measurements of a Distributed File System, </title> <booktitle> 13th SOSP, </booktitle> <month> Oct. </month> <year> 1991, </year> <pages> pp. 198-212. </pages>
Reference-contexts: In these simulations, the SAD file server achieved a hit rate ranging from 46% to 54%, which is consistent with past file server caching studies <ref> [Baker91] </ref>. However, this comparison is unfair to NASD since on-disk caches in current SAD servers are not used effectively (all data except read-ahead blocks are redundant because they are also cached in the file server), but would be effectively used in NASD drives.
Reference: [Bellare96] <author> Bellare, M., Canetti, R., and Krawczyk, H., </author> <title> Keying Hash Functions for Message Authentication, </title> <booktitle> Advances in Cryptol ogy: Crypto 96 Proceedings, 1996. 17 of 18 </booktitle>
Reference-contexts: NASD/NFS with Protected Communication Our NASD prototype drive implements NO_PROTECTION, INTEGRITY_ARGS, and INTEGRITY_DATA protection options and uses timestamp-nonces. For the keyed hash function, we use HMAC-MD5 because of the wide acceptance of MD5 [Rivest92] and the theoretical strengths of the HMAC construction <ref> [Bellare96] </ref>. We use the reference implementation of HMAC-MD5 with MD5 code from the cryptolib library compiled by AT&T Bell Labs. We also utilize a capability cache on the drive to avoid unnecessary recalculations of capability keys.
Reference: [Benner96] <author> Benner, </author> <title> A.F., Fibre Channel: Gigabit Communications and I/O for Computer Networks, </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: Direct transfer of data between storage subsystem and client machine eliminates the filesystem server machine as a bottleneck [Miller88, Long94, Drapeau94] and enables filesystems to stripe data over multiple storage subsystems to increase per-client and aggregate bandwidth [Berdahl95, Hartman93]. Fast packetized storage interconnects such as Fibrechannel <ref> [Benner96] </ref> promise that storage devices, including individual disk drives [Anderson95], can be cost-effectively attached to networks, and the proposed requirement for cryptographic support in version 6 of the Internet Protocol promises cost-effective network support for security [Deering95].
Reference: [Berdahl95] <author> Berdahl, L., </author> <title> Draft of Parallel Transport Protocol Proposal, </title> <institution> Lawrence Livermore National Labs, </institution> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: Direct transfer of data between storage subsystem and client machine eliminates the filesystem server machine as a bottleneck [Miller88, Long94, Drapeau94] and enables filesystems to stripe data over multiple storage subsystems to increase per-client and aggregate bandwidth <ref> [Berdahl95, Hartman93] </ref>. Fast packetized storage interconnects such as Fibrechannel [Benner96] promise that storage devices, including individual disk drives [Anderson95], can be cost-effectively attached to networks, and the proposed requirement for cryptographic support in version 6 of the Internet Protocol promises cost-effective network support for security [Deering95].
Reference: [Birrell80] <author> Birell, A.D. and Needham, </author> <title> R.M., A Universal File Server, </title> <journal> IEEE Transactions on Software Engineering 6,5, </journal> <month> Sept. </month> <year> 1980. </year>
Reference-contexts: Related Work The idea of a simple, disk-like network-attached storage server whose functions are employed by high-level distributed filesystems has been around for a long time. Cambridges Universal File Server used a simple memory abstraction, like SCSI today, but advocated that the server do all allocation and reclamation <ref> [Birrell80] </ref>. Accordingly, a directory-like index structure had to be understood by the server so it could detect no reference to a range of storage and free it. Cambridge UFS used capabilities primarily to distinguish one high-level filesystem from another.
Reference: [Birrell93] <author> Birrell, A. et al., </author> <title> The Echo Distributed File System, </title> <type> Research Report 111, </type> <institution> DEC SRC, </institution> <address> Palo Alto, CA, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: The follow-on filesystem at Cambridge, CFS, used unrevokable capabilities to authorize clients access to files, continued UFSs automatic reclamation through storage-under-stood indices, and added undoable (for a period of time after initiation) transactions into the filesystem interface [Mitchell82]. With the well-understood journalling technology available in higher-level file systems today <ref> [Chutani92, Birrell93] </ref>, we chose to avoid the cost of automatic reclamation of objects and arbitrarily larger storage-level transactions in NASD. Recently, there has been renewed interest in the logical interface to storage, driven primarily by the huge cost of managing rapidly growing storage repositories.
Reference: [Burrows92] <author> Burrows, M. et al., </author> <title> On-line Data Compression in a Log-structured File System, </title> <booktitle> 5th ASPLOS, </booktitle> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Written data need not consume as many storage bytes as are written; storage may compress written data <ref> [Burrows92] </ref>, replace long sequences of unspecified data with holes that read as zeros, or link one storage unit into two storage groupings sharing the same contents with copy-on-write semantics. More commonly, the storage consumed is larger than the written data because of indexing information, preallocation, or failure tolerating encodings [Patterson88].
Reference: [Cabrera91] <author> Cabrera, L. and Long, D., Swift: </author> <title> Using Distributed Disk Striping to Provide High I/O Data Rates, </title> <booktitle> Computing Systems 4:4, </booktitle> <month> Fall </month> <year> 1991. </year>
Reference-contexts: Direct transfer from storage to client is a prominent feature in many storage designs. Most rely on synchronous file manager oversight and offer network striping <ref> [IEEE94, Drapeau94, Cabrera91] </ref>. However, Berkeleys Zebra network-striped LFS post-processes authorization for client actions [Hartman93] and ISIs Derived Virtual Devices [VanMeter96a] binds file definition and access authorization into the initialization of communication state and exploits authenticated RPC to verify a clients binding to the authorized communication state.
Reference: [Cao95] <author> Cao, P. et al., </author> <title> A Study of Integrated Prefetching and Caching Strategies, </title> <booktitle> SIGMETRICS 95, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Notably, application hints describing client access patterns can dramatically improve storage performance by prefetching from disk to cache or from tape to disk <ref> [Cao95] </ref>. Extension interfaces can also enable groups of NASD drives to provide more pow-erful storage self-management such as tolerance of single drive failures [Long94, Lee96]. 2.2. <p> Informed Prefetching NASD/NFS To demonstrate the value of NASD drive extensions, we have added an informed prefetching system <ref> [Cao95, Patterson95] </ref> to a NASD drive and the NASD/NFS client. Applications issue hints to the drives to describe their future read accesses. Drives use this knowledge to prefetch data so that they will be able to satisfy client requests from their caches.
Reference: [Carey94] <author> Carey, M.J. et al., </author> <title> Shoring Up Persistent Applications, </title> <booktitle> SIGMOD 94, </booktitle> <year> 1994, </year> <pages> pp. 383-394. </pages>
Reference: [Chutani92] <author> Chutani, S. et al., </author> <title> The Episode File System, </title> <booktitle> Winter 1992 USENIX, </booktitle> <year> 1992, </year> <pages> pp. 43-60. </pages>
Reference-contexts: The follow-on filesystem at Cambridge, CFS, used unrevokable capabilities to authorize clients access to files, continued UFSs automatic reclamation through storage-under-stood indices, and added undoable (for a period of time after initiation) transactions into the filesystem interface [Mitchell82]. With the well-understood journalling technology available in higher-level file systems today <ref> [Chutani92, Birrell93] </ref>, we chose to avoid the cost of automatic reclamation of objects and arbitrarily larger storage-level transactions in NASD. Recently, there has been renewed interest in the logical interface to storage, driven primarily by the huge cost of managing rapidly growing storage repositories.
Reference: [Deering95] <author> Deering, S. and Hinden, R., </author> <title> Internet Protocol Version 6 Specification, </title> <type> RFC 1883, </type> <note> Dec. 1995. </note> <author> [deJonge93] de Jonge, W., Kaashoek, M.F. and Hsieh, W.C., </author> <title> The Logical Disk: A New Approach to Improving File Systems, </title> <booktitle> 14th SOSP, </booktitle> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Fast packetized storage interconnects such as Fibrechannel [Benner96] promise that storage devices, including individual disk drives [Anderson95], can be cost-effectively attached to networks, and the proposed requirement for cryptographic support in version 6 of the Internet Protocol promises cost-effective network support for security <ref> [Deering95] </ref>. Exploiting the indirection provided by interface abstractions such as SCSI, storage subsystems attack self-management by virtualizing the storage media that filesystems appear to manage [Lee96, deJonge93, Wilkes95, StorageTek94]. <p> With a single network, NASD drives must be capable of computing keyed digests on command and status fields without bandwidth penalty. This requirement is essentially the same as the security requirement in current proposals for IPv6 <ref> [Deering95] </ref>. With hardware support for cryptography in a drive, high-performance data integrity, data privacy and command privacy are also feasible. Storage self-management: With asynchronous filesystem oversight, NASD drives have knowledge of logical relationships between storage units.
Reference: [Dennis66] <author> Dennis, J.B. and Van Horn, </author> <title> E.C., Programming Semantics for Multiprogrammed Computations, </title> <journal> CACM 9,3, </journal> <year> 1966, </year> <pages> pp. 143-155. </pages>
Reference-contexts: Several projects have also investigated the security issues raised in network-attached storage environments. Derived Virtual Devices [VanMeter96a] present a mechanism, based on Kerberos [Neuman94], to secure shared access to network-attached peripherals. Capabilities are a well established concept <ref> [Dennis66] </ref> for regulating access to resources. In the past, many systems have used capability systems that rely on hardware support or trusted operating system kernels to protect system integrity [Karger88, Wilkes79, Wulf74]. Within NASD, we do not make assumptions about the integrity of the client which maintains capabilities.
Reference: [Drapeau94] <editor> Drapeau, A.L. et al., </editor> <title> RAID-II: A High-Bandwidth Network File Server, </title> <booktitle> 21st ISCA, </booktitle> <year> 1994, </year> <month> pp.234-244. </month>
Reference-contexts: A variety of approaches for improving distributed filesystem scalability and storage management have been proposed. Direct transfer of data between storage subsystem and client machine eliminates the filesystem server machine as a bottleneck <ref> [Miller88, Long94, Drapeau94] </ref> and enables filesystems to stripe data over multiple storage subsystems to increase per-client and aggregate bandwidth [Berdahl95, Hartman93]. <p> Direct transfer from storage to client is a prominent feature in many storage designs. Most rely on synchronous file manager oversight and offer network striping <ref> [IEEE94, Drapeau94, Cabrera91] </ref>. However, Berkeleys Zebra network-striped LFS post-processes authorization for client actions [Hartman93] and ISIs Derived Virtual Devices [VanMeter96a] binds file definition and access authorization into the initialization of communication state and exploits authenticated RPC to verify a clients binding to the authorized communication state.
Reference: [English92] <author> English, R.M. and Stepanov, A.A., Loge: </author> <title> a Self-Organizing Disk Controller, </title> <booktitle> Winter 1992 USENIX, </booktitle> <month> Jan. </month> <year> 1992, </year> <pages> pp. 237 251. </pages>
Reference-contexts: Others have simply resorted to transparent manipulation of clustering by either writing to the closest free block <ref> [English92] </ref> or reorganizing according to observed access activity [StorageTek94, Wilkes95]. Focusing on the artificial partition exhausted problem of filesystems whose logical space management is implemented as immutable commitment of disk space, the concept of virtual volumes or virtual disks has been advocated [Lee96, IEEE94].
Reference: [Gibson97] <author> Gibson, G. et al., </author> <title> File Server Scaling with Network-Attached Secure Disks, </title> <booktitle> 1997 SIGMETRICS, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Decoupling filesystem policy decisions, such as access protection or client cache consistency, from the datapath of individual accesses may reduce the work on a filesystem server, enabling increased scalability for existing, small-fetch workloads <ref> [Gibson97] </ref>. Taken collectively, these approaches define the range of network-attached storage. To deliver their promise to the majority of cost-conscious computing environments, network-attached storage devices must adhere to a standard interface and exhibit large volume manufacturing; that is, network-attached storage must evolve from and inherit the SCSI market.
Reference: [Gobioff97] <author> Gobioff, H. Gibson, G., Tygar, J.D., </author> <title> Security for Network Attached Storage Devices, </title> <booktitle> Work in Progress. </booktitle>
Reference-contexts: Having shown NASD-based adaptations for existing filesystems, we proceed to describe and demonstrate systems extended to support network striping [Hartman93] and informed prefetching [Patterson95]. Our experiments show a cryptographically sealed capability scheme <ref> [Gobioff97] </ref> for securing communications and asynchronous authorization that has little impact on performance when securing only the integrity of command, status and authorization. Securing data integrity using software cryptography reduces bandwidth by 55%, encouraging the inclusion of hardware support for message digest to support this level of protection.
Reference: [Golding95] <author> Golding, R., Shriver, E., Sullivan, T., and Wilkes, J., </author> <title> Attribute-managed storage, </title> <booktitle> Workshop on Modeling and Specifica tion of I/O, </booktitle> <address> San Antonio, TX, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: Another significant trend in storage management is the exploitation of access pattern and workload attributes <ref> [Golding95, IEEE94] </ref>. While we have not extensively explored this area, we believe that NASD implementations can offer extended interfaces to interpret filesystem-specific attribute fields in this way. Direct transfer from storage to client is a prominent feature in many storage designs.
Reference: [Gong89] <author> Gong, L., </author> <title> A Secure Identity-Based Capability System IEEE Symposium on Security and Privacy, </title> <month> May </month> <year> 1989, </year> <pages> pp. 56-64. </pages>
Reference-contexts: An alternative that avoids dependence on the local environments authentication system and the synchronous rights-granting message from the file manager, is to employ capabilities based on one-way functions, similar to the ICAP <ref> [Gong89] </ref> or Amoeba [Tanenbaum86] distributed environments, transported to the device via a client but only computable/mutable by file manager and NASD drive. Operation semantics: There is wide variation in the semantics of operations as simple as read and write. <p> Within NASD, we do not make assumptions about the integrity of the client which maintains capabilities. Therefore, we utilize cryptographic techniques similar to ICAP <ref> [Gong89] </ref> and Amoeba [Tanenbaum86]. In these systems, the act of issuing a capability and validating a capability must have access to a large amount 16 of 18 of private information about all the issued capabilities.
Reference: [Hartman93] <author> Hartman, J.H. and Ousterhout, J.K., </author> <title> The Zebra Striped Network File System, </title> <booktitle> 14th SOSP, </booktitle> <month> Dec. </month> <year> 1993, </year> <pages> pp. 29-43. </pages>
Reference-contexts: Direct transfer of data between storage subsystem and client machine eliminates the filesystem server machine as a bottleneck [Miller88, Long94, Drapeau94] and enables filesystems to stripe data over multiple storage subsystems to increase per-client and aggregate bandwidth <ref> [Berdahl95, Hartman93] </ref>. Fast packetized storage interconnects such as Fibrechannel [Benner96] promise that storage devices, including individual disk drives [Anderson95], can be cost-effectively attached to networks, and the proposed requirement for cryptographic support in version 6 of the Internet Protocol promises cost-effective network support for security [Deering95]. <p> We then describe and demonstrate NASD-based adaptations of two typical and significantly different, distributed filesystems, Suns Network File System (NFS) [Sandberg85] and Transarcs Andrew File System (AFS) [Howard88]. Having shown NASD-based adaptations for existing filesystems, we proceed to describe and demonstrate systems extended to support network striping <ref> [Hartman93] </ref> and informed prefetching [Patterson95]. Our experiments show a cryptographically sealed capability scheme [Gobioff97] for securing communications and asynchronous authorization that has little impact on performance when securing only the integrity of command, status and authorization. <p> Direct transfer from storage to client is a prominent feature in many storage designs. Most rely on synchronous file manager oversight and offer network striping [IEEE94, Drapeau94, Cabrera91]. However, Berkeleys Zebra network-striped LFS post-processes authorization for client actions <ref> [Hartman93] </ref> and ISIs Derived Virtual Devices [VanMeter96a] binds file definition and access authorization into the initialization of communication state and exploits authenticated RPC to verify a clients binding to the authorized communication state. Several projects have also investigated the security issues raised in network-attached storage environments.
Reference: [Holland94] <author> Holland, M. et al., </author> <title> Architectures and Algorithms for On-Line Failure Recovery in Redundant Disk Arrays, </title> <booktitle> Parallel and Distributed Databases 2,3, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: By locating blocks with sequential addresses at the location closest in positioning time (adjacent where possible), SCSI supports the locality-based optimizations being computed by the filesystems obsolete disk model. More advanced SCSI devices exploit this virtual interface to transparently implement RAID, data compression, dynamic block remapping, and representation-migration <ref> [Patterson88, StorageTek94, Wilkes95, Holland94] </ref>. In our NASD interface we abandon the notion that file managers understand and directly control storage layout. Instead, NASD drives store variable-length, logical byte streams called objects. Filesystems wanting to allocate storage for a new file request one or more objects to hold the files data.
Reference: [Howard88] <author> Howard, J.H. et al., </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM TOCS 6, </journal> <volume> 1, </volume> <month> Feb. </month> <year> 1988, </year> <pages> pp. 51-81. </pages>
Reference-contexts: We then describe and demonstrate NASD-based adaptations of two typical and significantly different, distributed filesystems, Suns Network File System (NFS) [Sandberg85] and Transarcs Andrew File System (AFS) <ref> [Howard88] </ref>. Having shown NASD-based adaptations for existing filesystems, we proceed to describe and demonstrate systems extended to support network striping [Hartman93] and informed prefetching [Patterson95]. <p> Additionally, we measure CPU utilization by recording time spent in the operat-ing system idle loop. All experiments were executed five times, and means and standard deviations are reported. We use several benchmarks to evaluate our prototype filesystems: the Andrew benchmark <ref> [Howard88] </ref>, a raw read bandwidth test, agrep full-text search over a large number of small files or a small number of large files, and gnuld linking several hundred object modules into an 8 MB Digital UNIX kernel.
Reference: [IEEE94] <author> IEEE P1244. </author> <title> Reference Model for Open Storage Systems Interconnection-Mass Storage System Reference Model Ver sion 5, </title> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Focusing on the artificial partition exhausted problem of filesystems whose logical space management is implemented as immutable commitment of disk space, the concept of virtual volumes or virtual disks has been advocated <ref> [Lee96, IEEE94] </ref>. NASD achieves much of this by regarding partitions as a grouping of objects whose total size is constrained but mutable rather than a region of storage; however, striped NASD/NFS also constructs virtual objects and partitions. <p> Another significant trend in storage management is the exploitation of access pattern and workload attributes <ref> [Golding95, IEEE94] </ref>. While we have not extensively explored this area, we believe that NASD implementations can offer extended interfaces to interpret filesystem-specific attribute fields in this way. Direct transfer from storage to client is a prominent feature in many storage designs. <p> Direct transfer from storage to client is a prominent feature in many storage designs. Most rely on synchronous file manager oversight and offer network striping <ref> [IEEE94, Drapeau94, Cabrera91] </ref>. However, Berkeleys Zebra network-striped LFS post-processes authorization for client actions [Hartman93] and ISIs Derived Virtual Devices [VanMeter96a] binds file definition and access authorization into the initialization of communication state and exploits authenticated RPC to verify a clients binding to the authorized communication state.
Reference: [Karger88] <author> Karger, </author> <title> P.A., Improving Security and Performance for Capability Systems, </title> <institution> University of Cambridge Computer Labora tory Technical Report No. </institution> <month> 149, Oct. </month> <year> 1988. </year>
Reference-contexts: Capabilities are a well established concept [Dennis66] for regulating access to resources. In the past, many systems have used capability systems that rely on hardware support or trusted operating system kernels to protect system integrity <ref> [Karger88, Wilkes79, Wulf74] </ref>. Within NASD, we do not make assumptions about the integrity of the client which maintains capabilities. Therefore, we utilize cryptographic techniques similar to ICAP [Gong89] and Amoeba [Tanenbaum86].
Reference: [Katz92] <author> Katz, </author> <title> R.H., High-Performance Network- and Channel-Attached Storage, </title> <booktitle> Proceedings of the IEEE 80, </booktitle> <volume> 8, </volume> <month> Aug. </month> <year> 1992. </year>
Reference: [Kim86] <author> Kim, M.Y., </author> <title> Synchronized disk interleaving, </title> <journal> IEEE Transactions on Computers C-35, </journal> <volume> 11, </volume> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: In a NASD environment, data requests go directly to NASD drives, so data caching should be performed at the drives. The aggregate caching resources of the filesystem need to be partitioned amongst multiple entities, which amplifies the problem of disk hotspots <ref> [Kim86] </ref> if client data requests are not load-balanced across drives. However, striping with a fine granularity distributes localized accesses [Livny87]. Using AFS workload traces, we performed a simple cache simulation to estimate the impact of splitting the data cache.
Reference: [Lampen91] <author> Lampen, A., </author> <title> Advancing Files to Attributed Software Objects, </title> <booktitle> 1991 Winter USENIX, </booktitle> <year> 1991, </year> <pages> pp. 219-229. </pages>
Reference-contexts: For instance, a UNIX filesystem representing files as individual NASD objects could store owner, group, and permission bits in the fs-spe-cific attribute. The idea of binding arbitrary attributes to storage objects is not new, for instance, AtFS has a similar notion <ref> [Lampen91] </ref>.
Reference: [Lee95] <author> Lee, E.K., </author> <title> Highly-Available, Scalable Network Storage, </title> <booktitle> 1995 Spring COMPCON, </booktitle> <month> Mar. </month> <year> 1995. </year>
Reference: [Lee96] <author> Lee, E.K. and Thekkath, </author> <title> C.A., Petal: Distributed Virtual Disks, </title> <booktitle> 7th ASPLOS, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Exploiting the indirection provided by interface abstractions such as SCSI, storage subsystems attack self-management by virtualizing the storage media that filesystems appear to manage <ref> [Lee96, deJonge93, Wilkes95, StorageTek94] </ref>. Decoupling filesystem policy decisions, such as access protection or client cache consistency, from the datapath of individual accesses may reduce the work on a filesystem server, enabling increased scalability for existing, small-fetch workloads [Gibson97]. Taken collectively, these approaches define the range of network-attached storage. <p> Through such intelligence, NASD drives target reducing the cost of storage management, sometimes estimated to be three or more times larger than the total acquisition cost of storage devices <ref> [Lee96] </ref>. Extensibility: With asynchronous filesystem oversight and direct communication between clients and storage, NASD drives are uniquely qualified to offer extensions to be used by client applications with and without file manager involvement. <p> Notably, application hints describing client access patterns can dramatically improve storage performance by prefetching from disk to cache or from tape to disk [Cao95]. Extension interfaces can also enable groups of NASD drives to provide more pow-erful storage self-management such as tolerance of single drive failures <ref> [Long94, Lee96] </ref>. 2.2. NASD Interface Design Grouping of storage: The set of storage units accessible to a client as the result of an asynchronous access control decision must be named and navigated by client and NASD. <p> Focusing on the artificial partition exhausted problem of filesystems whose logical space management is implemented as immutable commitment of disk space, the concept of virtual volumes or virtual disks has been advocated <ref> [Lee96, IEEE94] </ref>. NASD achieves much of this by regarding partitions as a grouping of objects whose total size is constrained but mutable rather than a region of storage; however, striped NASD/NFS also constructs virtual objects and partitions.
Reference: [Livny87] <author> Livny, M., </author> <title> Multi-disk management algorithms, </title> <booktitle> 1987 SIGMETRICS, </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: The aggregate caching resources of the filesystem need to be partitioned amongst multiple entities, which amplifies the problem of disk hotspots [Kim86] if client data requests are not load-balanced across drives. However, striping with a fine granularity distributes localized accesses <ref> [Livny87] </ref>. Using AFS workload traces, we performed a simple cache simulation to estimate the impact of splitting the data cache.
Reference: [Long94] <author> Long, D.D.E., Montague, B.R., and Cabrera, L., Swift/RAID: </author> <title> A Distributed RAID System, </title> <booktitle> Computing Systems 7,3, </booktitle> <month> Summer </month> <year> 1994. </year>
Reference-contexts: A variety of approaches for improving distributed filesystem scalability and storage management have been proposed. Direct transfer of data between storage subsystem and client machine eliminates the filesystem server machine as a bottleneck <ref> [Miller88, Long94, Drapeau94] </ref> and enables filesystems to stripe data over multiple storage subsystems to increase per-client and aggregate bandwidth [Berdahl95, Hartman93]. <p> Notably, application hints describing client access patterns can dramatically improve storage performance by prefetching from disk to cache or from tape to disk [Cao95]. Extension interfaces can also enable groups of NASD drives to provide more pow-erful storage self-management such as tolerance of single drive failures <ref> [Long94, Lee96] </ref>. 2.2. NASD Interface Design Grouping of storage: The set of storage units accessible to a client as the result of an asynchronous access control decision must be named and navigated by client and NASD.
Reference: [McKusick84] <editor> McKusick, M.K. et al., </editor> <title> A Fast File System for UNIX, </title> <journal> ACM TOCS 2, </journal> <month> Aug. </month> <year> 1984, </year> <pages> pp. 181-197. </pages>
Reference-contexts: Using notions of the disk drives physical parameters and geometry, filesystems maintain information about which blocks are in-use, how blocks are grouped together into logical objects, and how these objects are distributed across the device <ref> [McKusick84, McVoy91] </ref>. Current SCSI disks offer virtual or logical fixed-sized blocks named in a linear address space. This is virtual because modern disks already transparently remap storage sectors to hide defective media and the variation in track densities across the disk. <p> We have borrowed representation of storage clustering hints as a list of lists from deJonges logical disk model to escape the SCSI block address space and the heroics that filesystems do to achieve contiguity in these linear address spaces <ref> [deJonge93, McKusick84, McVoy91, Rosenblum91] </ref>. Others have simply resorted to transparent manipulation of clustering by either writing to the closest free block [English92] or reorganizing according to observed access activity [StorageTek94, Wilkes95].
Reference: [McVoy91] <author> McVoy, L.W. and Kleiman, </author> <title> S.R., Extent-Like Performance from a UNIX File System, </title> <booktitle> Winter 1991 USENIX, </booktitle> <year> 1991. </year>
Reference-contexts: By forcing contiguous allocation and sequential access, Bullet can achieve high data rates, but similar benefits can be derived from other clustering schemes such as lists of lists [deJonge93], extents <ref> [McVoy91] </ref>, and logs [Rosenblum91]. Written data need not consume as many storage bytes as are written; storage may compress written data [Burrows92], replace long sequences of unspecified data with holes that read as zeros, or link one storage unit into two storage groupings sharing the same contents with copy-on-write semantics. <p> Using notions of the disk drives physical parameters and geometry, filesystems maintain information about which blocks are in-use, how blocks are grouped together into logical objects, and how these objects are distributed across the device <ref> [McKusick84, McVoy91] </ref>. Current SCSI disks offer virtual or logical fixed-sized blocks named in a linear address space. This is virtual because modern disks already transparently remap storage sectors to hide defective media and the variation in track densities across the disk. <p> We have borrowed representation of storage clustering hints as a list of lists from deJonges logical disk model to escape the SCSI block address space and the heroics that filesystems do to achieve contiguity in these linear address spaces <ref> [deJonge93, McKusick84, McVoy91, Rosenblum91] </ref>. Others have simply resorted to transparent manipulation of clustering by either writing to the closest free block [English92] or reorganizing according to observed access activity [StorageTek94, Wilkes95].
Reference: [Miller88] <author> Miller, </author> <title> S.W., A Reference Model for Mass Storage Systems, </title> <booktitle> Advances in Computers 27, </booktitle> <year> 1988, </year> <pages> pp. 157-210. </pages>
Reference-contexts: A variety of approaches for improving distributed filesystem scalability and storage management have been proposed. Direct transfer of data between storage subsystem and client machine eliminates the filesystem server machine as a bottleneck <ref> [Miller88, Long94, Drapeau94] </ref> and enables filesystems to stripe data over multiple storage subsystems to increase per-client and aggregate bandwidth [Berdahl95, Hartman93].
Reference: [Mills88] <author> Mills, D., </author> <title> Network Time Protocol Version 1 Specification and Implementation, </title> <type> RFC 1059, </type> <month> July </month> <year> 1988. </year>
Reference-contexts: Although all that is needed in a NASD drive is a readable, high-resolution, monotonically increasing counter, we further assume that this rate is controlled by a network time protocol such as NTP <ref> [Mills88] </ref> to ensure that all NASD drives possess a loosely synchronized real-time clock. Extensions: NASD drives must provide interfaces that enable clients and filesystems to discover available extended functions. Implementors are responsible for ensuring that extensions do not circumvent the security policies specified by the file manager.
Reference: [Mitchell82] <author> Mitchell, J. and Dion, J., </author> <title> A Comparison of Two Network-Based File Servers, </title> <booktitle> 8th SOSP, </booktitle> <month> Dec. </month> <year> 1981. </year>
Reference-contexts: Because information sharing is often channelled through storage, lock synchronization is sometimes provided as a storage primitive. Moreover, some implementations provide atomic operations in the faces of failures, or allow transactions composed of multiple operations to be entirely undone <ref> [Mitchell82] </ref>. While NASD should leave as many of these optimizations to be developed by device designers as possible, most must be pinned down by a particular interface specification. The operations of our prototype NASD interface are specified in the next sections. <p> Cambridge UFS used capabilities primarily to distinguish one high-level filesystem from another. The follow-on filesystem at Cambridge, CFS, used unrevokable capabilities to authorize clients access to files, continued UFSs automatic reclamation through storage-under-stood indices, and added undoable (for a period of time after initiation) transactions into the filesystem interface <ref> [Mitchell82] </ref>. With the well-understood journalling technology available in higher-level file systems today [Chutani92, Birrell93], we chose to avoid the cost of automatic reclamation of objects and arbitrarily larger storage-level transactions in NASD.
Reference: [NCSA89] <institution> National Center for Supercomputing Applications "XDataSlice for the X Window System" UIUC, </institution> <year> 1989. </year>
Reference-contexts: The drives use a slightly-modified version of the [Patterson95] system to translate the hints into prefetching requests and caching decisions. To evaluate the performance benefit of our NASD prefetching extension, we used a hinting version of the XDataSlice 3D scientific visualization application <ref> [NCSA89] </ref> to render 25 random planar slices through a 256 x 256 x 256 cube of 32-bit data values.
Reference: [Neuman93] <author> Neuman, B.C. and Stubblebine, S.G., </author> <title> A Note on the Use of Timestamps and Nonces, OS Review 27,2, </title> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Clocks: Efficient, secure communications defeats message replay attacks by uniquely timestamping messages with loosely synchronized clocks and enforcing uniqueness of all received timestamps within a skew threshold of each nodes current time <ref> [Neuman93] </ref>. Although all that is needed in a NASD drive is a readable, high-resolution, monotonically increasing counter, we further assume that this rate is controlled by a network time protocol such as NTP [Mills88] to ensure that all NASD drives possess a loosely synchronized real-time clock.
Reference: [Neuman94] <author> Neuman, B.C. and Tso, T., </author> <title> Kerberos: An Authentication Service for Computer Networks, </title> <journal> IEEE Communications 32,9, </journal> <month> Sept. </month> <year> 1994, </year> <pages> pp. 33-38. </pages>
Reference-contexts: The authenticated decision authorizes particular operations on particular groupings of storage. Because this authorization is asynchronous, a NASD device may be required to record an audit trail of operations performed, or to revoke authorization at the file managers discretion. For a small number of a popular authentication systems, Kerberos <ref> [Neuman94] </ref> for example, NASD drives could be built to directly participate, synchronously receiving a client identity and rights in an authenticated message from a file manager during its access control processing. <p> Several projects have also investigated the security issues raised in network-attached storage environments. Derived Virtual Devices [VanMeter96a] present a mechanism, based on Kerberos <ref> [Neuman94] </ref>, to secure shared access to network-attached peripherals. Capabilities are a well established concept [Dennis66] for regulating access to resources. In the past, many systems have used capability systems that rely on hardware support or trusted operating system kernels to protect system integrity [Karger88, Wilkes79, Wulf74].
Reference: [Patterson88] <author> Patterson, D.A., Gibson, G. and Katz, </author> <title> R.H., A Case for Redundant Arrays of Inexpensive Disks (RAID), 1988 SIG MOD, </title> <month> June </month> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: More commonly, the storage consumed is larger than the written data because of indexing information, preallocation, or failure tolerating encodings <ref> [Patterson88] </ref>. Beyond simple read and write, the seman 5 of 18 tics of filenames and directories are particularly critical [Rosenblum91]. Because information sharing is often channelled through storage, lock synchronization is sometimes provided as a storage primitive. <p> By locating blocks with sequential addresses at the location closest in positioning time (adjacent where possible), SCSI supports the locality-based optimizations being computed by the filesystems obsolete disk model. More advanced SCSI devices exploit this virtual interface to transparently implement RAID, data compression, dynamic block remapping, and representation-migration <ref> [Patterson88, StorageTek94, Wilkes95, Holland94] </ref>. In our NASD interface we abandon the notion that file managers understand and directly control storage layout. Instead, NASD drives store variable-length, logical byte streams called objects. Filesystems wanting to allocate storage for a new file request one or more objects to hold the files data.
Reference: [Patterson95] <author> Patterson, R.H. et al., </author> <title> Informed Prefetching and Caching, </title> <booktitle> 15th SOSP, </booktitle> <year> 1995. </year> <editor> [vanRenesse89] van Renesse, R. et al. </editor> <title> The Design of a High-Performance File Server, </title> <booktitle> 9th International Conference on Distributed Com puter Systems, </booktitle> <year> 1989, </year> <pages> pp. 22-27. </pages>
Reference-contexts: Having shown NASD-based adaptations for existing filesystems, we proceed to describe and demonstrate systems extended to support network striping [Hartman93] and informed prefetching <ref> [Patterson95] </ref>. Our experiments show a cryptographically sealed capability scheme [Gobioff97] for securing communications and asynchronous authorization that has little impact on performance when securing only the integrity of command, status and authorization. <p> Informed Prefetching NASD/NFS To demonstrate the value of NASD drive extensions, we have added an informed prefetching system <ref> [Cao95, Patterson95] </ref> to a NASD drive and the NASD/NFS client. Applications issue hints to the drives to describe their future read accesses. Drives use this knowledge to prefetch data so that they will be able to satisfy client requests from their caches. <p> To resolve this confusion, the client periodically notifies the drive of the reads it has issued. The drives use a slightly-modified version of the <ref> [Patterson95] </ref> system to translate the hints into prefetching requests and caching decisions.
Reference: [Rivest92] <author> Rivest, R., </author> <title> The MD5 Message-Digest Algorithm, </title> <type> RFC 1321, </type> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: NASD/NFS with Protected Communication Our NASD prototype drive implements NO_PROTECTION, INTEGRITY_ARGS, and INTEGRITY_DATA protection options and uses timestamp-nonces. For the keyed hash function, we use HMAC-MD5 because of the wide acceptance of MD5 <ref> [Rivest92] </ref> and the theoretical strengths of the HMAC construction [Bellare96]. We use the reference implementation of HMAC-MD5 with MD5 code from the cryptolib library compiled by AT&T Bell Labs. We also utilize a capability cache on the drive to avoid unnecessary recalculations of capability keys.
Reference: [Rosenblum91] <author> Rosenblum, M. and Ousterhout, J.K., </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> 13th SOSP, </booktitle> <year> 1991. </year>
Reference-contexts: By forcing contiguous allocation and sequential access, Bullet can achieve high data rates, but similar benefits can be derived from other clustering schemes such as lists of lists [deJonge93], extents [McVoy91], and logs <ref> [Rosenblum91] </ref>. Written data need not consume as many storage bytes as are written; storage may compress written data [Burrows92], replace long sequences of unspecified data with holes that read as zeros, or link one storage unit into two storage groupings sharing the same contents with copy-on-write semantics. <p> More commonly, the storage consumed is larger than the written data because of indexing information, preallocation, or failure tolerating encodings [Patterson88]. Beyond simple read and write, the seman 5 of 18 tics of filenames and directories are particularly critical <ref> [Rosenblum91] </ref>. Because information sharing is often channelled through storage, lock synchronization is sometimes provided as a storage primitive. Moreover, some implementations provide atomic operations in the faces of failures, or allow transactions composed of multiple operations to be entirely undone [Mitchell82]. <p> We have borrowed representation of storage clustering hints as a list of lists from deJonges logical disk model to escape the SCSI block address space and the heroics that filesystems do to achieve contiguity in these linear address spaces <ref> [deJonge93, McKusick84, McVoy91, Rosenblum91] </ref>. Others have simply resorted to transparent manipulation of clustering by either writing to the closest free block [English92] or reorganizing according to observed access activity [StorageTek94, Wilkes95].
Reference: [Sandberg85] <author> Sandberg, R. et al., </author> <title> Design and Implementation of the Sun Network Filesystem, </title> <booktitle> Summer 1985 USENIX, </booktitle> <month> June </month> <year> 1985, </year> <pages> pp. 119-130. </pages>
Reference-contexts: In this paper we present a prototype for a network-attached storage interface, which we call network-attached secure disks (NASD), that exhibits most of the promise of this technology. We then describe and demonstrate NASD-based adaptations of two typical and significantly different, distributed filesystems, Suns Network File System (NFS) <ref> [Sandberg85] </ref> and Transarcs Andrew File System (AFS) [Howard88]. Having shown NASD-based adaptations for existing filesystems, we proceed to describe and demonstrate systems extended to support network striping [Hartman93] and informed prefetching [Patterson95].
Reference: [Satya89] <author> Satyanarayanan, M., </author> <title> Integrating Security in a Large Distributed Environment, </title> <journal> ACM TOCS 7,3, </journal> <month> Aug. </month> <year> 1989, </year> <pages> pp. 247-280. </pages>
Reference-contexts: DVD devices use this approach, which is simplified by the availability of authenticated RPC packages, also used by filesystems such as AFS <ref> [Satya89] </ref>.
Reference: [Seagate96] <author> Seagate Technology Inc., </author> <title> Barracuda Family Product Brief (ST19171), </title> <year> 1996. </year>
Reference: [Seagate96a] <author> Seagate Technology Inc., </author> <title> Elite Family Product Brief (ST423451FC), </title> <year> 1996. </year>
Reference-contexts: If we include the 1 MB to 2 MB on-disk caches available in current high-end disk drives <ref> [Seagate96a] </ref> when measuring the data cache of a SAD file server, the hit rate difference between SAD and NASD configurations would decrease. 3.1.
Reference: [Shekita90] <author> Shekita, E. and Zwilling, M., Cricket: </author> <title> A Mapped Persistent Object Store, </title> <booktitle> Persistent Object Ssytems Workshop, 18 of 18 Marthas Vineyard, </booktitle> <address> MA, </address> <month> Sept. </month> <year> 1990. </year>
Reference: [Shirriff92] <author> Shirriff, K. and Ousterhout, J., Sawmill: </author> <title> A High Bandwidth Logging File System, </title> <booktitle> Summer 1994 USENIX, </booktitle> <month> June </month> <year> 1994. </year>
Reference: [StorageTek94] <author> Storage Technology Corporation, </author> <title> Iceberg 9200 Storage System: Introduction, </title> <booktitle> STK Part Number 307406101, </booktitle> <year> 1994. </year>
Reference-contexts: Exploiting the indirection provided by interface abstractions such as SCSI, storage subsystems attack self-management by virtualizing the storage media that filesystems appear to manage <ref> [Lee96, deJonge93, Wilkes95, StorageTek94] </ref>. Decoupling filesystem policy decisions, such as access protection or client cache consistency, from the datapath of individual accesses may reduce the work on a filesystem server, enabling increased scalability for existing, small-fetch workloads [Gibson97]. Taken collectively, these approaches define the range of network-attached storage. <p> By locating blocks with sequential addresses at the location closest in positioning time (adjacent where possible), SCSI supports the locality-based optimizations being computed by the filesystems obsolete disk model. More advanced SCSI devices exploit this virtual interface to transparently implement RAID, data compression, dynamic block remapping, and representation-migration <ref> [Patterson88, StorageTek94, Wilkes95, Holland94] </ref>. In our NASD interface we abandon the notion that file managers understand and directly control storage layout. Instead, NASD drives store variable-length, logical byte streams called objects. Filesystems wanting to allocate storage for a new file request one or more objects to hold the files data. <p> Others have simply resorted to transparent manipulation of clustering by either writing to the closest free block [English92] or reorganizing according to observed access activity <ref> [StorageTek94, Wilkes95] </ref>. Focusing on the artificial partition exhausted problem of filesystems whose logical space management is implemented as immutable commitment of disk space, the concept of virtual volumes or virtual disks has been advocated [Lee96, IEEE94].
Reference: [Tanenbaum86] <author> Tanenbaum, </author> <title> A.S. </title> <editor> et al., </editor> <title> Using Sparse Capabilities in a Distributed System, </title> <booktitle> 6th International Conference on Distributed Computing, </booktitle> <year> 1986, </year> <pages> pp. 558-563. </pages>
Reference-contexts: An alternative that avoids dependence on the local environments authentication system and the synchronous rights-granting message from the file manager, is to employ capabilities based on one-way functions, similar to the ICAP [Gong89] or Amoeba <ref> [Tanenbaum86] </ref> distributed environments, transported to the device via a client but only computable/mutable by file manager and NASD drive. Operation semantics: There is wide variation in the semantics of operations as simple as read and write. <p> Within NASD, we do not make assumptions about the integrity of the client which maintains capabilities. Therefore, we utilize cryptographic techniques similar to ICAP [Gong89] and Amoeba <ref> [Tanenbaum86] </ref>. In these systems, the act of issuing a capability and validating a capability must have access to a large amount 16 of 18 of private information about all the issued capabilities.
Reference: [Tygar95] <author> Tygar, J.D. and Yee, B.S., </author> <title> Secure Coprocessors in Electronic Commerce Applications, </title> <booktitle> Proceedings 1995 USENIX Electronic Commerce Workshop, 1995, </booktitle> <address> New York. </address>
Reference-contexts: A NASD drive can employ a wide spectrum of measures to protect the integrity of its critical state. For instance, a drive could utilize a tamper resistant memory to store the secrets or a secure coprocessor to protect computation <ref> [Tygar95] </ref>. Ultimately, the security of the system will rest on how well both the drive and the file manager protect the key hierarchy. 3. NASD-Adapted Filesystems The primary purpose of a distributed filesystem is to provide distributed access to shared storage organized under the abstraction of a shared name space.
Reference: [VanMeter96] <author> Van Meter, R., </author> <title> A Brief Survey Of Current Work on Network Attached Peripherals (Extended Abstract), OS Review 30,1, </title> <month> Jan. </month> <year> 1996. </year>
Reference: [VanMeter96a] <author> Van Meter, R., Holtz, S., and Finn G., </author> <title> Derived Virtual Devices: A Secure Distributed File System Mechanism, </title> <booktitle> Fifth NASA Goddard Conference on Mass Storage Systems and Technologies, </booktitle> <address> College Park, MD. </address> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: Such names may be temporary. For example, Derived Virtual Devices (DVD) use the communications port identifier established by the file managers decision to grant access to a group of storage units <ref> [VanMeter96a] </ref>. In our prototype interface, a NASD drive parti-tions its allocated storage into containers which we call objects. Access control enforcement: Access control decisions made by a file manager must be enforced by a NASD drive. This enforcement implies authentication of the file managers decisions. <p> Direct transfer from storage to client is a prominent feature in many storage designs. Most rely on synchronous file manager oversight and offer network striping [IEEE94, Drapeau94, Cabrera91]. However, Berkeleys Zebra network-striped LFS post-processes authorization for client actions [Hartman93] and ISIs Derived Virtual Devices <ref> [VanMeter96a] </ref> binds file definition and access authorization into the initialization of communication state and exploits authenticated RPC to verify a clients binding to the authorized communication state. Several projects have also investigated the security issues raised in network-attached storage environments. Derived Virtual Devices [VanMeter96a] present a mechanism, based on Kerberos [Neuman94], <p> actions [Hartman93] and ISIs Derived Virtual Devices <ref> [VanMeter96a] </ref> binds file definition and access authorization into the initialization of communication state and exploits authenticated RPC to verify a clients binding to the authorized communication state. Several projects have also investigated the security issues raised in network-attached storage environments. Derived Virtual Devices [VanMeter96a] present a mechanism, based on Kerberos [Neuman94], to secure shared access to network-attached peripherals. Capabilities are a well established concept [Dennis66] for regulating access to resources.
Reference: [Wilkes79] <author> Wilkes, </author> <title> M.V. and Needham, R.M., </title> <booktitle> The Cambridge CAP Computer and Its Operating System, </booktitle> <year> 1979. </year>
Reference-contexts: Capabilities are a well established concept [Dennis66] for regulating access to resources. In the past, many systems have used capability systems that rely on hardware support or trusted operating system kernels to protect system integrity <ref> [Karger88, Wilkes79, Wulf74] </ref>. Within NASD, we do not make assumptions about the integrity of the client which maintains capabilities. Therefore, we utilize cryptographic techniques similar to ICAP [Gong89] and Amoeba [Tanenbaum86].
Reference: [Wilkes95] <author> Wilkes, J. et al., </author> <title> The HP AutoRAID Hierarchical Storage System, </title> <booktitle> 15th SOSP, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Exploiting the indirection provided by interface abstractions such as SCSI, storage subsystems attack self-management by virtualizing the storage media that filesystems appear to manage <ref> [Lee96, deJonge93, Wilkes95, StorageTek94] </ref>. Decoupling filesystem policy decisions, such as access protection or client cache consistency, from the datapath of individual accesses may reduce the work on a filesystem server, enabling increased scalability for existing, small-fetch workloads [Gibson97]. Taken collectively, these approaches define the range of network-attached storage. <p> By locating blocks with sequential addresses at the location closest in positioning time (adjacent where possible), SCSI supports the locality-based optimizations being computed by the filesystems obsolete disk model. More advanced SCSI devices exploit this virtual interface to transparently implement RAID, data compression, dynamic block remapping, and representation-migration <ref> [Patterson88, StorageTek94, Wilkes95, Holland94] </ref>. In our NASD interface we abandon the notion that file managers understand and directly control storage layout. Instead, NASD drives store variable-length, logical byte streams called objects. Filesystems wanting to allocate storage for a new file request one or more objects to hold the files data. <p> Others have simply resorted to transparent manipulation of clustering by either writing to the closest free block [English92] or reorganizing according to observed access activity <ref> [StorageTek94, Wilkes95] </ref>. Focusing on the artificial partition exhausted problem of filesystems whose logical space management is implemented as immutable commitment of disk space, the concept of virtual volumes or virtual disks has been advocated [Lee96, IEEE94].
Reference: [Wulf74] <author> Wulf, </author> <title> W.A. </title> <editor> et al., HYDRA: </editor> <title> The Kernel of a Multiprocessor Operating System, </title> <journal> CACM 17,6, </journal> <month> June </month> <year> 1974, </year> <pages> pp. 337-345. </pages>
Reference-contexts: Capabilities are a well established concept [Dennis66] for regulating access to resources. In the past, many systems have used capability systems that rely on hardware support or trusted operating system kernels to protect system integrity <ref> [Karger88, Wilkes79, Wulf74] </ref>. Within NASD, we do not make assumptions about the integrity of the client which maintains capabilities. Therefore, we utilize cryptographic techniques similar to ICAP [Gong89] and Amoeba [Tanenbaum86].
References-found: 58

