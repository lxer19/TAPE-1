URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92203.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Compiling Fortran 77D and 90D for MIMD Distributed-Memory Machines  
Author: Alok Choudhary Geoffrey Fox Seema Hiranandani Ken Kennedy Charles Koelbel Sanjay Ranka Chau-Wen Tseng 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  In Proceedings of the 4th Symposium on the Frontiers of Massively Parallel Computation, McLean, VA,  
Date: March 1992  October 1992.  
Pubnum: CRPC-TR92203  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 5, 6, 26, 28, 33] </ref>. For instance, consider the following example. <p> Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37]. A number of researchers have studied techniques to reduce storage and promote memory reuse <ref> [1, 5, 6, 24, 26, 28, 33, 35] </ref>. These optimizations have proved useful for both scalar and parallel machines.
Reference: [2] <author> I. Ahmad, A. Choudhary, G. Fox, K. Parasuram, R. Pon-nusamy, S. Ranka, and R. Thakur. </author> <title> Implementation and scalability of Fortran 90D intrinsic functions on distributed memory machines. </title> <type> Technical Report SCCS-256, </type> <institution> NPAC, Syracuse University, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: The run-time library is built on top of the Express communication package to ensure portability across different architectures [30]. Table 2 presents some sample performance numbers for a subset of the intrinsic functions on an iPSC/860, details are presented elsewhere <ref> [2] </ref>. The times in the table include both the computation and communication times for each function. For large problem sizes, we were able to obtain almost linear speedups. In the case of transpose function, going from one processor to two or four degrades execution time due to increased communication.
Reference: [3] <author> E. Albert, K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Compiling Fortran 8x array features for the Connection Machine computer system. </title> <booktitle> In Proceedings of the ACM SIG-PLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Adapt proposes to scalarize and partition Fortran 90 programs using a run-time library for Fortran 90 intrinsics [29]. The CM Fortran compiler compiles Fortran 90 with alignment and layout specifications directly to the physical machine, and can optimize floating point register usage <ref> [3] </ref>. The Fortran-90-Y compiler uses formal specification techniques to generate efficient code for the CM-2 and CM-5 [15]. Paragon is a version of C extended with array syntax, operations, reductions, permutations, and distribution specifications [14].
Reference: [4] <author> E. Albert, J. Lukas, and G. Steele, Jr. </author> <title> Data parallel computers and the forall statement. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(4) </volume> <pages> 185-192, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Because the alignment and distribution statements are executable, dynamic data decomposition is possible. 2.2 Forall Fortran D provides forall loops to permit the user to specify difficult parallel loops in a deterministic manner <ref> [4] </ref>. In a forall loop, each iteration uses only values defined before the loop or within the current iteration. When a statement in an iteration of the forall loop accesses a memory location, it will not get any value written by a different iteration of the loop.
Reference: [5] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: Fortran 90 array operations allow the programmer to access and modify entire arrays atomically, even if the underlying machine lacks this capability. The Fortran D compiler must divide array operations into sections that fit the hardware of the target machine <ref> [5, 6] </ref>. We defer both loop fusion and sectioning to the Fortran D back end. Loop fusion is deferred because even hand-written Fortran 77 programs can benefit significantly [24, 28]. Sectioning is needed in the back end because forall loops may also be present in Fortran 77D. <p> This enhances the clarity and conciseness of the program, and has the advantage of making parallelism explicit. It is the responsibility of the compiler to efficiently implement array constructs for scalar machines. Previous research has shown that this is a difficult problem <ref> [5, 6] </ref>. One problem is that when Fortran 90 array constructs are used in assignment statements, the entire right-hand side (rhs) must be evaluated before storing the results in the left-hand side (lhs). <p> A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests <ref> [5, 35, 36] </ref>. The current Fortran D back end fuses all adjacent loop nests where legal, if no loop-carried true dependences are introduced. This heuristic does not adversely affect the parallelism or communication overhead of the resulting program, and should perform well for the simple cases found in practice. <p> Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 5, 6, 26, 28, 33] </ref>. For instance, consider the following example. <p> Parameters are added where necessary to provide necessary data partitioning information. 4.3.3 Sectioning The final phase of the Fortran D back end completes the scalarization process. After partitioning is performed, the compiler applies sectioning to convert forall loops into do loops <ref> [5, 6] </ref> in the node program. The Fortran D back end detects cases where temporary storage may be needed using data dependence analysis. <p> Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37]. A number of researchers have studied techniques to reduce storage and promote memory reuse <ref> [1, 5, 6, 24, 26, 28, 33, 35] </ref>. These optimizations have proved useful for both scalar and parallel machines.
Reference: [6] <author> J. R. Allen and K. Kennedy. </author> <title> Vector register allocation. </title> <type> Technical Report TR86-45, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: Fortran 90 array operations allow the programmer to access and modify entire arrays atomically, even if the underlying machine lacks this capability. The Fortran D compiler must divide array operations into sections that fit the hardware of the target machine <ref> [5, 6] </ref>. We defer both loop fusion and sectioning to the Fortran D back end. Loop fusion is deferred because even hand-written Fortran 77 programs can benefit significantly [24, 28]. Sectioning is needed in the back end because forall loops may also be present in Fortran 77D. <p> This enhances the clarity and conciseness of the program, and has the advantage of making parallelism explicit. It is the responsibility of the compiler to efficiently implement array constructs for scalar machines. Previous research has shown that this is a difficult problem <ref> [5, 6] </ref>. One problem is that when Fortran 90 array constructs are used in assignment statements, the entire right-hand side (rhs) must be evaluated before storing the results in the left-hand side (lhs). <p> Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 5, 6, 26, 28, 33] </ref>. For instance, consider the following example. <p> Parameters are added where necessary to provide necessary data partitioning information. 4.3.3 Sectioning The final phase of the Fortran D back end completes the scalarization process. After partitioning is performed, the compiler applies sectioning to convert forall loops into do loops <ref> [5, 6] </ref> in the node program. The Fortran D back end detects cases where temporary storage may be needed using data dependence analysis. <p> The previous example is problematic because temporary storage is required for the values of A (i1). In some cases, the Fortran D compiler can eliminate buffering through program transformations such as loop reversal. In other cases, the compiler can reduce the amount of temporary storage required through data prefetching <ref> [6] </ref>. For instance, in the Jacobi example a more efficient translation would result in: X = A (1) Y = 0.5 * (X + A (i+1)) A (i) = Y ENDFOR This reduces the temporary memory required significantly, from an entire array to two scalars. <p> Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37]. A number of researchers have studied techniques to reduce storage and promote memory reuse <ref> [1, 5, 6, 24, 26, 28, 33, 35] </ref>. These optimizations have proved useful for both scalar and parallel machines.
Reference: [7] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: We find that the desired order for compilation phases is to apply loop fusion first, followed by partitioning and sectioning. Loop fusion is performed first because it simplifies partitioning by reducing the need to consider inter-loop interactions. It also enables optimizations such as strip-mining and loop interchange <ref> [7, 36] </ref>. In addition, loop fusion does not increase the difficulty of later compiler phases. <p> Fusing such loops simplifies the partitioning process and enables additional optimizations. Data dependence is a concept developed for vectoriz-ing and parallelizing compilers to characterize memory access patterns at compile time <ref> [7, 26, 36] </ref>. A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests [5, 35, 36].
Reference: [8] <author> ANSI X3J3/S8.115. </author> <title> Fortran 90, </title> <month> June </month> <year> 1990. </year>
Reference-contexts: Our goal is to establish a machine-independent programming model for data-parallel programs that is easy to use, yet performs with acceptable efficiency on different parallel architectures. Fortran D provides data decomposition specifications that can be applied to Fortran 77 and Fortran 90 <ref> [8] </ref> to produce Fortran 77D and Fortran 90D, respectively. In this paper, we describe a unified strategy for compiling both Fortran 77D and Fortran 90D into efficient SPMD (Single Program Multiple Data) message-passing programs. <p> We assign to the Fortran 90D front end the remaining task, scalarizing Fortran 90 constructs that have no equivalent in the Fortran 77D intermediate form. There are three principal Fortran 90 language features that must be scalarized: array constructs, where statements, and intrinsic functions <ref> [8] </ref>.
Reference: [9] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Based on the work partition, references that result in nonlocal accesses are marked. * Optimize communication. Nonlocal references are examined to determine optimization opportunities. The key optimization, message vectorization, uses the level of loop-carried true dependences to com bine element messages into vectors <ref> [9, 38] </ref>. * Manage storage. "Overlaps" [38] or buffers are al located to store nonlocal data. * Generate code. Information gathered previously is used to generate the SPMD program with explicit message-passing that executes directly on the nodes of the distributed-memory machine. <p> Compiling for MIMD distributed-memory machines is only a part of the Fortran D project. We also are working on Fortran 77D and Fortran 90D compilers for SIMD machines, translations between the two Fortran dialects, support for irregular computations, and environmental support for static performance estimation and automatic data decomposition <ref> [9, 10, 19, 23] </ref>. 9 Acknowledgements We are grateful to the ParaScope and Fortran D research groups for their assistance, and to Parasoft for providing the Fortran 90 parser and Express.
Reference: [10] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Compiling for MIMD distributed-memory machines is only a part of the Fortran D project. We also are working on Fortran 77D and Fortran 90D compilers for SIMD machines, translations between the two Fortran dialects, support for irregular computations, and environmental support for static performance estimation and automatic data decomposition <ref> [9, 10, 19, 23] </ref>. 9 Acknowledgements We are grateful to the ParaScope and Fortran D research groups for their assistance, and to Parasoft for providing the Fortran 90 parser and Express.
Reference: [11] <author> S. Benkner, B. Chapman, and H. Zima. </author> <title> Vienna Fortran 90. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: The Fortran-90-Y compiler uses formal specification techniques to generate efficient code for the CM-2 and CM-5 [15]. Paragon is a version of C extended with array syntax, operations, reductions, permutations, and distribution specifications [14]. Our compiler resembles the Vienna Fortran 90 compiler derived from Superb <ref> [11, 38] </ref> and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37]. A number of researchers have studied techniques to reduce storage and promote memory reuse [1, 5, 6, 24, 26, 28, 33, 35].
Reference: [12] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Tor-czon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: The Fortran 90D and 77D front ends process input programs into the common intermediate form. The Fortran D back end then compiles this to the SPMD message-passing node program. The Fortran D compiler is implemented in the context of the ParaScope programming environment <ref> [12] </ref>. 4.1 Fortran 90D Front End The function of the Fortran 90D front end is to scalar-ize the Fortran 90D program, translating it to an equivalent Fortran 77D program.
Reference: [13] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Fortran D data decomposition specifications are analyzed to determine the decom position of each array in a program. * Partition computation. The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes val ues of data it owns <ref> [13, 31, 38] </ref>. * Analyze communication. Based on the work partition, references that result in nonlocal accesses are marked. * Optimize communication. Nonlocal references are examined to determine optimization opportunities.
Reference: [14] <author> C. Chase, A. Cheung, A. Reeves, and M. Smith. </author> <title> Paragon: A parallel programming environment for scientific applications using communication structures. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The Fortran-90-Y compiler uses formal specification techniques to generate efficient code for the CM-2 and CM-5 [15]. Paragon is a version of C extended with array syntax, operations, reductions, permutations, and distribution specifications <ref> [14] </ref>. Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37].
Reference: [15] <author> M. Chen and J. Cowie. </author> <title> Prototyping Fortran-90 compilers for massively parallel machines. </title> <booktitle> In Proceedings of the SIG-PLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The CM Fortran compiler compiles Fortran 90 with alignment and layout specifications directly to the physical machine, and can optimize floating point register usage [3]. The Fortran-90-Y compiler uses formal specification techniques to generate efficient code for the CM-2 and CM-5 <ref> [15] </ref>. Paragon is a version of C extended with array syntax, operations, reductions, permutations, and distribution specifications [14]. Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37].
Reference: [16] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kre-mer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: We conclude with a discussion of related work. 2 Fortran D Language We briefly overview aspects of Fortran D relevant to this paper. These extensions can be added to either Fortran 77 or Fortran 90. The complete language is described elsewhere <ref> [16] </ref>. 2.1 Data Alignment and Distribution In Fortran D, the decomposition statement declares an abstract problem or index domain. The align statement maps each array element onto one or more elements of the decomposition.
Reference: [17] <author> A. Goldberg and R. Paige. </author> <title> Stream processing. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 228-234, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: This heuristic does not adversely affect the parallelism or communication overhead of the resulting program, and should perform well for the simple cases found in practice. More sophisticated algorithms are discussed elsewhere <ref> [17, 28, 35] </ref>. Loop fusion also has the added advantage of being able to improve memory reuse in the resulting program. Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs.
Reference: [18] <author> P. Hatcher, M. Quinn, A. Lapadula, B. Seevers, R. Ander-son, and R. Jones. </author> <title> Data-parallel programming on MIMD computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 377-383, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Many distributed-memory compilers reduce communication overhead by aggregating messages outside of parallel loops [22, 25] or parallel procedures <ref> [18, 32] </ref>, while others rely on functional language [27] or single assignment semantics [31]. In comparison, the Fortran D compiler uses dependence analysis to automatically exploit parallelism and extract communication even from sequential loops such as those found in ADI integration.
Reference: [19] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: Compiling for MIMD distributed-memory machines is only a part of the Fortran D project. We also are working on Fortran 77D and Fortran 90D compilers for SIMD machines, translations between the two Fortran dialects, support for irregular computations, and environmental support for static performance estimation and automatic data decomposition <ref> [9, 10, 19, 23] </ref>. 9 Acknowledgements We are grateful to the ParaScope and Fortran D research groups for their assistance, and to Parasoft for providing the Fortran 90 parser and Express.
Reference: [20] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albu-querque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: The principal issues involved in compiling Fortran 90D are partitioning the program across multiple nodes and scalarizing it for execution on each individual node. Previous work has described the partitioning process <ref> [20, 21] </ref>. In this paper we demonstrate how to integrate partitioning with scalarization, and show that an efficient portable run-time library can ease the task of compiling Fortran D. <p> m r v m n Array Size (double precision) 8 16 32 64 128 256 512 1K 2K 4K 8K 16K -25 25 ffi ffi ffi ffi ffi ffi ffi ffi ? ? ? ? Loop Fusion ffi ffi Data Prefetching Fortran D compilation process below, details are discussed elsewhere <ref> [20, 21] </ref>. * Analyze Program. Symbolic and data dependence analysis is performed. * Partition data. Fortran D data decomposition specifications are analyzed to determine the decom position of each array in a program. * Partition computation. <p> Analysis determines that both I and J are cross-processor loops|loops carrying true dependences that se-quentialize the computation across processors. To exploit pipeline parallelism, the Fortran D compiler interchanges such loops inward. We call this technique fine-grain pipelining <ref> [20, 21] </ref>. For this version of ADI integration, data dependences permit the Fortran D compiler to interchange the J loop inwards. However, if loop fusion is not performed, the imperfectly nested K loops inhibit loop interchange for loop I, forcing it to remain in place. <p> For simplicity, only the first loop is shown. The remaining loops are compiled in a similar manner as before. To reduce communication overhead, we can also apply strip-mining in conjunction with loop interchange to adjust the granularity of pipelining. We call this technique coarse-grain pipelining <ref> [20, 21] </ref>. In the ADI example, we strip-mine the K loop by four (an empirically derived value), then interchange the resulting loop outside the I loop. Messages inserted outside the K loop allow each processor to reduce communication costs at the expense of some parallelism, resulting in Figure 7.
Reference: [21] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Wash-ington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The principal issues involved in compiling Fortran 90D are partitioning the program across multiple nodes and scalarizing it for execution on each individual node. Previous work has described the partitioning process <ref> [20, 21] </ref>. In this paper we demonstrate how to integrate partitioning with scalarization, and show that an efficient portable run-time library can ease the task of compiling Fortran D. <p> m r v m n Array Size (double precision) 8 16 32 64 128 256 512 1K 2K 4K 8K 16K -25 25 ffi ffi ffi ffi ffi ffi ffi ffi ? ? ? ? Loop Fusion ffi ffi Data Prefetching Fortran D compilation process below, details are discussed elsewhere <ref> [20, 21] </ref>. * Analyze Program. Symbolic and data dependence analysis is performed. * Partition data. Fortran D data decomposition specifications are analyzed to determine the decom position of each array in a program. * Partition computation. <p> Analysis determines that both I and J are cross-processor loops|loops carrying true dependences that se-quentialize the computation across processors. To exploit pipeline parallelism, the Fortran D compiler interchanges such loops inward. We call this technique fine-grain pipelining <ref> [20, 21] </ref>. For this version of ADI integration, data dependences permit the Fortran D compiler to interchange the J loop inwards. However, if loop fusion is not performed, the imperfectly nested K loops inhibit loop interchange for loop I, forcing it to remain in place. <p> For simplicity, only the first loop is shown. The remaining loops are compiled in a similar manner as before. To reduce communication overhead, we can also apply strip-mining in conjunction with loop interchange to adjust the granularity of pipelining. We call this technique coarse-grain pipelining <ref> [20, 21] </ref>. In the ADI example, we strip-mine the K loop by four (an empirically derived value), then interchange the resulting loop outside the I loop. Messages inserted outside the K loop allow each processor to reduce communication costs at the expense of some parallelism, resulting in Figure 7.
Reference: [22] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Many distributed-memory compilers reduce communication overhead by aggregating messages outside of parallel loops <ref> [22, 25] </ref> or parallel procedures [18, 32], while others rely on functional language [27] or single assignment semantics [31]. In comparison, the Fortran D compiler uses dependence analysis to automatically exploit parallelism and extract communication even from sequential loops such as those found in ADI integration.
Reference: [23] <author> K. Kennedy and U. Kremer. </author> <title> Automatic data alignment and distribution for loosely synchronous problems in an interactive programming environment. </title> <type> Technical Report TR91-155, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Compiling for MIMD distributed-memory machines is only a part of the Fortran D project. We also are working on Fortran 77D and Fortran 90D compilers for SIMD machines, translations between the two Fortran dialects, support for irregular computations, and environmental support for static performance estimation and automatic data decomposition <ref> [9, 10, 19, 23] </ref>. 9 Acknowledgements We are grateful to the ParaScope and Fortran D research groups for their assistance, and to Parasoft for providing the Fortran 90 parser and Express.
Reference: [24] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Wash-ington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The Fortran D compiler must divide array operations into sections that fit the hardware of the target machine [5, 6]. We defer both loop fusion and sectioning to the Fortran D back end. Loop fusion is deferred because even hand-written Fortran 77 programs can benefit significantly <ref> [24, 28] </ref>. Sectioning is needed in the back end because forall loops may also be present in Fortran 77D. We assign to the Fortran 90D front end the remaining task, scalarizing Fortran 90 constructs that have no equivalent in the Fortran 77D intermediate form. <p> For this example, we measured improvements of up to 30% for some problem sizes on an Intel i860, as shown in Figure 2. Additional transformations to enhance memory reuse and increase unit-stride memory accesses are also quite important; they are described elsewhere <ref> [24, 28] </ref>. 4.3.2 Program Partitioning The major step in compiling Fortran D for MIMD distributed-memory machines is to partition the data and computation across processors, introducing communication where needed. <p> Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37]. A number of researchers have studied techniques to reduce storage and promote memory reuse <ref> [1, 5, 6, 24, 26, 28, 33, 35] </ref>. These optimizations have proved useful for both scalar and parallel machines.
Reference: [25] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Many distributed-memory compilers reduce communication overhead by aggregating messages outside of parallel loops <ref> [22, 25] </ref> or parallel procedures [18, 32], while others rely on functional language [27] or single assignment semantics [31]. In comparison, the Fortran D compiler uses dependence analysis to automatically exploit parallelism and extract communication even from sequential loops such as those found in ADI integration.
Reference: [26] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: Fusing such loops simplifies the partitioning process and enables additional optimizations. Data dependence is a concept developed for vectoriz-ing and parallelizing compilers to characterize memory access patterns at compile time <ref> [7, 26, 36] </ref>. A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests [5, 35, 36]. <p> Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 5, 6, 26, 28, 33] </ref>. For instance, consider the following example. <p> Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37]. A number of researchers have studied techniques to reduce storage and promote memory reuse <ref> [1, 5, 6, 24, 26, 28, 33, 35] </ref>. These optimizations have proved useful for both scalar and parallel machines.
Reference: [27] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Many distributed-memory compilers reduce communication overhead by aggregating messages outside of parallel loops [22, 25] or parallel procedures [18, 32], while others rely on functional language <ref> [27] </ref> or single assignment semantics [31]. In comparison, the Fortran D compiler uses dependence analysis to automatically exploit parallelism and extract communication even from sequential loops such as those found in ADI integration. Several other projects are also developing Fortran 90 compilers for MIMD distributed-memory machines.
Reference: [28] <author> K. S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: The Fortran D compiler must divide array operations into sections that fit the hardware of the target machine [5, 6]. We defer both loop fusion and sectioning to the Fortran D back end. Loop fusion is deferred because even hand-written Fortran 77 programs can benefit significantly <ref> [24, 28] </ref>. Sectioning is needed in the back end because forall loops may also be present in Fortran 77D. We assign to the Fortran 90D front end the remaining task, scalarizing Fortran 90 constructs that have no equivalent in the Fortran 77D intermediate form. <p> This heuristic does not adversely affect the parallelism or communication overhead of the resulting program, and should perform well for the simple cases found in practice. More sophisticated algorithms are discussed elsewhere <ref> [17, 28, 35] </ref>. Loop fusion also has the added advantage of being able to improve memory reuse in the resulting program. Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. <p> Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 5, 6, 26, 28, 33] </ref>. For instance, consider the following example. <p> For this example, we measured improvements of up to 30% for some problem sizes on an Intel i860, as shown in Figure 2. Additional transformations to enhance memory reuse and increase unit-stride memory accesses are also quite important; they are described elsewhere <ref> [24, 28] </ref>. 4.3.2 Program Partitioning The major step in compiling Fortran D for MIMD distributed-memory machines is to partition the data and computation across processors, introducing communication where needed. <p> Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37]. A number of researchers have studied techniques to reduce storage and promote memory reuse <ref> [1, 5, 6, 24, 26, 28, 33, 35] </ref>. These optimizations have proved useful for both scalar and parallel machines.
Reference: [29] <author> J. Merlin. </author> <title> ADAPTing Fortran-90 array programs for distributed memory architectures. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Several other projects are also developing Fortran 90 compilers for MIMD distributed-memory machines. Adapt proposes to scalarize and partition Fortran 90 programs using a run-time library for Fortran 90 intrinsics <ref> [29] </ref>. The CM Fortran compiler compiles Fortran 90 with alignment and layout specifications directly to the physical machine, and can optimize floating point register usage [3]. The Fortran-90-Y compiler uses formal specification techniques to generate efficient code for the CM-2 and CM-5 [15].
Reference: [30] <institution> Parasoft Corporation. </institution> <note> Express User's Manual, </note> <year> 1989. </year>
Reference-contexts: We have selected Fortran 77 with calls to communication and run-time libraries based on Express, a collection of portable message-passing primitives <ref> [30] </ref>. Evaluating our experiences with this node interface is the first step towards defining an "optimal" level of support for programming individual nodes of a parallel machine. 4 Fortran D Compiler The Fortran D compiler thus consists of three parts. <p> The Fortran D compiler translates intrinsics into calls to run-time library routines using a standard interface. Additional information is passed describing bounds, overlaps, and partitioning for each array dimension. The run-time library is built on top of the Express communication package to ensure portability across different architectures <ref> [30] </ref>. Table 2 presents some sample performance numbers for a subset of the intrinsic functions on an iPSC/860, details are presented elsewhere [2]. The times in the table include both the computation and communication times for each function. For large problem sizes, we were able to obtain almost linear speedups.
Reference: [31] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Fortran D data decomposition specifications are analyzed to determine the decom position of each array in a program. * Partition computation. The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes val ues of data it owns <ref> [13, 31, 38] </ref>. * Analyze communication. Based on the work partition, references that result in nonlocal accesses are marked. * Optimize communication. Nonlocal references are examined to determine optimization opportunities. <p> Many distributed-memory compilers reduce communication overhead by aggregating messages outside of parallel loops [22, 25] or parallel procedures [18, 32], while others rely on functional language [27] or single assignment semantics <ref> [31] </ref>. In comparison, the Fortran D compiler uses dependence analysis to automatically exploit parallelism and extract communication even from sequential loops such as those found in ADI integration. Several other projects are also developing Fortran 90 compilers for MIMD distributed-memory machines.
Reference: [32] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Many distributed-memory compilers reduce communication overhead by aggregating messages outside of parallel loops [22, 25] or parallel procedures <ref> [18, 32] </ref>, while others rely on functional language [27] or single assignment semantics [31]. In comparison, the Fortran D compiler uses dependence analysis to automatically exploit parallelism and extract communication even from sequential loops such as those found in ADI integration.
Reference: [33] <author> V. Sarkar and G. Gao. </author> <title> Optimization of array accesses by collective loop transformations. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 5, 6, 26, 28, 33] </ref>. For instance, consider the following example. <p> Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37]. A number of researchers have studied techniques to reduce storage and promote memory reuse <ref> [1, 5, 6, 24, 26, 28, 33, 35] </ref>. These optimizations have proved useful for both scalar and parallel machines.
Reference: [34] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, version 5.2-0.6 edition, Septem-ber 1989. </note>
Reference-contexts: However, communication may still be required before the loop to acquire nonlocal values, and after the loop to update or merge nonlocal values. Single-statement Fortran D forall loops are identical to those supported in CM Fortran <ref> [34] </ref>. 3 Fortran D Compilation Strategy 3.1 Overall Strategy Our strategy for parallelizing Fortran D programs for distributed-memory MIMD computers is illustrated in Figure 1.
Reference: [35] <author> J. Warren. </author> <title> A hierachical basis for reordering transformations. </title> <booktitle> In Conference Record of the Eleventh Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <month> January </month> <year> 1984. </year>
Reference-contexts: A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests <ref> [5, 35, 36] </ref>. The current Fortran D back end fuses all adjacent loop nests where legal, if no loop-carried true dependences are introduced. This heuristic does not adversely affect the parallelism or communication overhead of the resulting program, and should perform well for the simple cases found in practice. <p> This heuristic does not adversely affect the parallelism or communication overhead of the resulting program, and should perform well for the simple cases found in practice. More sophisticated algorithms are discussed elsewhere <ref> [17, 28, 35] </ref>. Loop fusion also has the added advantage of being able to improve memory reuse in the resulting program. Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. <p> Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37]. A number of researchers have studied techniques to reduce storage and promote memory reuse <ref> [1, 5, 6, 24, 26, 28, 33, 35] </ref>. These optimizations have proved useful for both scalar and parallel machines.
Reference: [36] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: We find that the desired order for compilation phases is to apply loop fusion first, followed by partitioning and sectioning. Loop fusion is performed first because it simplifies partitioning by reducing the need to consider inter-loop interactions. It also enables optimizations such as strip-mining and loop interchange <ref> [7, 36] </ref>. In addition, loop fusion does not increase the difficulty of later compiler phases. <p> Fusing such loops simplifies the partitioning process and enables additional optimizations. Data dependence is a concept developed for vectoriz-ing and parallelizing compilers to characterize memory access patterns at compile time <ref> [7, 26, 36] </ref>. A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests [5, 35, 36]. <p> A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests <ref> [5, 35, 36] </ref>. The current Fortran D back end fuses all adjacent loop nests where legal, if no loop-carried true dependences are introduced. This heuristic does not adversely affect the parallelism or communication overhead of the resulting program, and should perform well for the simple cases found in practice.
Reference: [37] <author> M. Wu and G. Fox. </author> <title> Compiling Fortran90 programs for distributed memory MIMD parallel computers. </title> <type> CRPC Report CRPC-TR91126, </type> <institution> Center for Research on Parallel Computation, Syracuse University, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: Paragon is a version of C extended with array syntax, operations, reductions, permutations, and distribution specifications [14]. Our compiler resembles the Vienna Fortran 90 compiler derived from Superb [11, 38] and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization <ref> [37] </ref>. A number of researchers have studied techniques to reduce storage and promote memory reuse [1, 5, 6, 24, 26, 28, 33, 35]. These optimizations have proved useful for both scalar and parallel machines.
Reference: [38] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: Fortran D data decomposition specifications are analyzed to determine the decom position of each array in a program. * Partition computation. The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes val ues of data it owns <ref> [13, 31, 38] </ref>. * Analyze communication. Based on the work partition, references that result in nonlocal accesses are marked. * Optimize communication. Nonlocal references are examined to determine optimization opportunities. <p> Based on the work partition, references that result in nonlocal accesses are marked. * Optimize communication. Nonlocal references are examined to determine optimization opportunities. The key optimization, message vectorization, uses the level of loop-carried true dependences to com bine element messages into vectors <ref> [9, 38] </ref>. * Manage storage. "Overlaps" [38] or buffers are al located to store nonlocal data. * Generate code. Information gathered previously is used to generate the SPMD program with explicit message-passing that executes directly on the nodes of the distributed-memory machine. <p> Based on the work partition, references that result in nonlocal accesses are marked. * Optimize communication. Nonlocal references are examined to determine optimization opportunities. The key optimization, message vectorization, uses the level of loop-carried true dependences to com bine element messages into vectors [9, 38]. * Manage storage. "Overlaps" <ref> [38] </ref> or buffers are al located to store nonlocal data. * Generate code. Information gathered previously is used to generate the SPMD program with explicit message-passing that executes directly on the nodes of the distributed-memory machine. <p> The Fortran-90-Y compiler uses formal specification techniques to generate efficient code for the CM-2 and CM-5 [15]. Paragon is a version of C extended with array syntax, operations, reductions, permutations, and distribution specifications [14]. Our compiler resembles the Vienna Fortran 90 compiler derived from Superb <ref> [11, 38] </ref> and has also been influenced by a proposal by Wu & Fox that discussed program generation and optimization [37]. A number of researchers have studied techniques to reduce storage and promote memory reuse [1, 5, 6, 24, 26, 28, 33, 35].
References-found: 38

