URL: http://www.demo.cs.brandeis.edu/papers/sur97.ps.Z
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Email: blair@cs.brandeis.edu  
Title: Scaling-up RAAMs  
Author: Alan D. Blair 
Date: January 6, 1997  
Address: Waltham, MA 02254-9110  
Affiliation: Dept. of Computer Science Volen Center for Complex Systems Brandeis University  
Abstract: Modifications to Recursive Auto-Associative Memory are presented, which allow it to store deeper and more complex data structures than previously reported. These modifications include adding extra layers to the compressor and reconstructor networks, employing integer rather than real-valued representations, pre-conditioning the weights and pre-setting the representations to be compatible with them. The resulting system is tested on a data set of syntactic trees extracted from the Penn Treebank. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D.H., G.E. Hinton & T.J. Sejnowski, </author> <year> 1985. </year> <title> A learning algorithm for Boltzman Machines, </title> <booktitle> Cognitive Science 9, </booktitle> <pages> 147-169. </pages>
Reference-contexts: Recursive Auto-Associative Memory or RAAM (Pollack, 1990) is a method for storing tree structures in a feed-forward neural network. It's architecture is very similar to that of encoder networks <ref> (Ackley et al., 1985, Cottrell et al., 1987) </ref>, consisting of a compressor unit and a reconstructor unit.
Reference: <author> Angeline, P.J. </author> <year> 1992. </year> <title> Avoiding fusion in floating symbol systems, </title> <type> Tech. Report 92-PA-FUSION, </type> <institution> Computer Science Dept., Ohio State University. </institution>
Reference-contexts: This approach has the disadvantage that two or more representations may become fused in the course of the training <ref> (Angeline, 1992) </ref>. The fusion problem gets more pronounced as the number of nodes increases, and is even more prevalent when the representations become digital. We circumvent this difficulty by assigning the representations at the outset, in a way that is compatible with the initial weights.
Reference: <author> Blank, D.S., L.A. Meeden & J.B. Marshall, </author> <year> 1992. </year> <title> Exploring the Symbolic/Subsymbolic Continuum: A Case Study of RAAM, in: Closing the Gap: Symbolism vs. Connectionism, </title> <editor> J. Dinsmore, ed. </editor> <publisher> (Lawrence Erlbaum Associates). </publisher>
Reference: <author> Blelloch, G. & C.R. Rosenberg, </author> <year> 1987. </year> <title> Network learning on the Connection Machine, </title> <booktitle> Proceedings Tenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Milan, Italy. </address>
Reference-contexts: The learning rate must be very small in order to ensure successful training. After some preliminary trials, we settled on a learning rate of (nd) 1 for the reconstructors and (2nd) 1 for the compressor. Parallelization of the training set provides a significant speed-up to backpropagation <ref> (Blelloch & Rosenberg, 1987) </ref>.
Reference: <author> Chalmers, D.J. </author> <year> 1990. </year> <title> Syntactic transformations on distributed representations, </title> <booktitle> Connection Science 2(1-2), </booktitle> <pages> 53-62. </pages>
Reference: <author> Chrisman, L. </author> <year> 1991. </year> <title> Learning Recursive Distributed Representations for Holistic Computation, </title> <booktitle> Connection Science 3, </booktitle> <pages> 345-366. </pages>
Reference: <author> Cottrell, G., P. Munro & D. Zipser, </author> <year> 1987. </year> <title> Learning internal representations from gray-scale images: An example of extensional programming, </title> <booktitle> Proceedings Ninth Annual Conference of the Cognitive Science Society, </booktitle> <address> Seattle, WA, </address> <pages> 461-473. </pages>
Reference: <author> Cowper, E.A. </author> <year> 1992. </year> <title> A Concise Introduction to Syntactic Theory (University of Chicago Press, </title> <address> Chicago, IL). </address>
Reference: <author> Elman, J.L. </author> <year> 1990. </year> <title> Finding Structure in Time, </title> <booktitle> Cognitive Science 14, </booktitle> <pages> 179-211. </pages>
Reference: <author> Fahlman, S.E. </author> <year> 1989. </year> <title> Fast-learning variations on back-propagation: an empirical study. </title> <editor> In D. Touretzky, G. Hinton & T. Sejnowski, eds. </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> Pittsburgh, PA, </address> <publisher> 38-51 (Morgan Kaufman, </publisher> <address> San Mateo). 10 Fodor, J.A. & Z.W. Pylyshyn, </address> <year> 1988. </year> <title> Connectionism and cognitive architec-ture: a critical analysis, </title> <journal> Cognition 28, </journal> <pages> 3-71. </pages>
Reference-contexts: We trained them using back-propagation (Rumelhart et al., 1986), with a modification similar to Quickprop <ref> (Fahlman, 1989) </ref> 1 .
Reference: <author> Hinton, G.E., J.L. McClelland & D.E. Rumelhart, </author> <year> 1986. </year> <title> Distributed Representations. </title> <editor> In D.E. Rumelhart, J.L. McClelland and the PDP Research Group, eds. </editor> <title> Parallel Distributed Processing: Experiments in the Microstructure of Cognition 1: </title> <publisher> Foundations (MIT Press, </publisher> <address> Cambridge, MA). </address>
Reference-contexts: Fodor & Pylyshyn, 1988) that connectionism lacked the flexibility and representational adequacy needed for higher level cognitive tasks. Chief among these were coarse coding (Touretzky, 1986), tensor based representations (Smolensky, 1990), reduced representations <ref> (Hinton et al., 1986) </ref>, and RAAM (Pollack, 1990).
Reference: <author> Kolen, J. & J.B. Pollack, </author> <year> 1990. </year> <title> Back propagation is sensitive to initial conditions, </title> <booktitle> Complex Systems 4, </booktitle> <pages> 269-280. </pages>
Reference-contexts: X P jk x k ) z i = fi R i0 + j=1 R ij tanh (Q j0 + d X Q jk x k ) 3.3 Pre-conditioned weights It is well known that the success of neural network training using backpropagation is sensitive to the initial weight configuration <ref> (Kolen & Pollack, 1990) </ref>, and indeed can be enhanced by pre-setting some or all of the weights (Pratt et al., 1991). The complete randomness of the initial representations and weights becomes a significant problem as RAAM s are scaled up.
Reference: <author> Marcus, M., B. Santorini, M.A. Marcinkiewicz, </author> <year> 1993. </year> <title> Building a large annotated corpus of English: the Penn Treebank, </title> <note> Computational Linguistics 19 (also at ftp.cis.upenn.edu/pub/treebank/doc/). </note>
Reference-contexts: Our aim was to see whether one of these architectures could be adapted to handle linguistic data of `real world' complexity. To find such data, we took a small fragment of the Penn Treebank <ref> (Marcus et al., 1993) </ref> - a large corpus of text from the Wall Street Journal and used a filter to strip out the text, leaving only the syntactic structures.
Reference: <author> Niklasson, L.F. & T. van Gelder, </author> <year> 1994. </year> <title> Can Connectionist Models Exhibit Non-Classical Structure Sensitivity, </title> <booktitle> Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 664-669. </pages>
Reference: <author> Plate, T.A. </author> <year> 1994. </year> <title> Distributed Representations and Nested Compositional Structure, </title> <type> Ph.D. Thesis, </type> <institution> University of Toronto. </institution>
Reference: <author> Pollack, J.B. </author> <year> 1990. </year> <title> Recursive Distributed Representations, </title> <booktitle> Artificial Intelligence 46(1), </booktitle> <pages> 77-105. </pages>
Reference-contexts: Fodor & Pylyshyn, 1988) that connectionism lacked the flexibility and representational adequacy needed for higher level cognitive tasks. Chief among these were coarse coding (Touretzky, 1986), tensor based representations (Smolensky, 1990), reduced representations (Hinton et al., 1986), and RAAM <ref> (Pollack, 1990) </ref>. Compared to earlier systems, they had the advantage of compositionality built more explicitly into their design, and they have shown a great deal of promise in a number of areas (Chalmers, 1990, Elman, 1990, Chrisman, 1991, Blank et al., 1992, Plate, 1994, Niklas-son & van Gelder, 1994). <p> Recursive Auto-Associative Memory or RAAM <ref> (Pollack, 1990) </ref> is a method for storing tree structures in a feed-forward neural network. It's architecture is very similar to that of encoder networks (Ackley et al., 1985, Cottrell et al., 1987), consisting of a compressor unit and a reconstructor unit. <p> X P jk x k ) z i = fi R i0 + j=1 R ij tanh (Q j0 + d X Q jk x k ) 3.3 Pre-conditioned weights It is well known that the success of neural network training using backpropagation is sensitive to the initial weight configuration <ref> (Kolen & Pollack, 1990) </ref>, and indeed can be enhanced by pre-setting some or all of the weights (Pratt et al., 1991). The complete randomness of the initial representations and weights becomes a significant problem as RAAM s are scaled up.
Reference: <author> Pratt, L.Y., J.A. Mostow & C.A. Kamm, </author> <year> 1991. </year> <title> Direct Transfer of Learned Information Among Neural Networks, </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> 584-589. </pages>
Reference-contexts: (Q j0 + d X Q jk x k ) 3.3 Pre-conditioned weights It is well known that the success of neural network training using backpropagation is sensitive to the initial weight configuration (Kolen & Pollack, 1990), and indeed can be enhanced by pre-setting some or all of the weights <ref> (Pratt et al., 1991) </ref>. The complete randomness of the initial representations and weights becomes a significant problem as RAAM s are scaled up.
Reference: <author> Rumelhart, D.E., G.E. Hinton, G.E. & R.J. Williams, </author> <year> 1986. </year> <title> Learning representation by back-propagating errors, </title> <booktitle> Nature 323, </booktitle> <pages> 533-536. </pages>
Reference-contexts: We trained them using back-propagation <ref> (Rumelhart et al., 1986) </ref>, with a modification similar to Quickprop (Fahlman, 1989) 1 .
Reference: <author> Smolensky, P. </author> <year> 1990. </year> <title> Tensor product variable binding and the representation of symbolic structures in a connectionist system, </title> <booktitle> Artificial Intelligence 46(1-2), </booktitle> <pages> 159-216. </pages>
Reference-contexts: 1 Introduction In the late 1980's a number of new connectionist models were developed in response to criticisms (e.g. Fodor & Pylyshyn, 1988) that connectionism lacked the flexibility and representational adequacy needed for higher level cognitive tasks. Chief among these were coarse coding (Touretzky, 1986), tensor based representations <ref> (Smolensky, 1990) </ref>, reduced representations (Hinton et al., 1986), and RAAM (Pollack, 1990).
Reference: <author> Touretzky, D.S. </author> <year> 1986. </year> <title> BoltzCONS: Reconciling connectionism with the recursive nature of stacks and trees, </title> <booktitle> Proceedings Eighth Annual conference of the Cognitive Science Society (Erlbaum, </booktitle> <address> Hillsdale, NJ). </address> <month> 11 </month>
Reference-contexts: 1 Introduction In the late 1980's a number of new connectionist models were developed in response to criticisms (e.g. Fodor & Pylyshyn, 1988) that connectionism lacked the flexibility and representational adequacy needed for higher level cognitive tasks. Chief among these were coarse coding <ref> (Touretzky, 1986) </ref>, tensor based representations (Smolensky, 1990), reduced representations (Hinton et al., 1986), and RAAM (Pollack, 1990).
References-found: 20

