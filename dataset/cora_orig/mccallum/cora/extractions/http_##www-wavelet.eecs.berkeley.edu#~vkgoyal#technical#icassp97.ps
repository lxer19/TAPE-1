URL: http://www-wavelet.eecs.berkeley.edu/~vkgoyal/technical/icassp97.ps
Refering-URL: http://www-wavelet.eecs.berkeley.edu/~vkgoyal/technical/icassp97.html
Root-URL: 
Email: E-mail: v.goyal@ieee.org, Martin.Vetterli@de.epfl.ch  
Title: COMPUTATION-DISTORTION CHARACTERISTICS OF BLOCK TRANSFORM CODING  
Author: Vivek K Goyal Martin Vetterli 
Address: Switzerland  
Affiliation: Dept. of Electrical Engineering and Computer Sciences University of California, Berkeley Laboratoire de Communications Audiovisuelles Ecole Polytechnique Federale de Lausanne,  
Abstract: A distortion-computation function D(C) is defined as the minimum expected distortion in computing some quantity while using no more than C computational units. In a communication framework, where the computational problem is to determine a representation that can be transmitted with expected rate not exceeding R, this gives slices of a rate-distortion-computation surface. The convexity of distortion-computation functions and rate-distortion-computation surfaces is asserted. Transform coding is studied as a particular instance of this theory. Explicit comparisons between the efficacies of the Karhunen-Loeve Transform and the Discrete Cosine Transform for coding of a Gauss-Markov source are given. Results are also given on joint optimization of the block length and the computational precision. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Berger. </author> <title> Rate Distortion Theory. </title> <publisher> Prentice-Hall, </publisher> <year> 1971. </year>
Reference-contexts: Theorem 2: The D-R-C surface is convex, i.e. given C 1 C 2 and R 1 R 2 , for all 2 <ref> [0; 1] </ref>, where R = R 1 + (1 )R 2 . As in rate-distortion theory, a system that is described by parameters chosen from discrete sets will not necessar ily have a convex operational distortion-computation func tion (or rate-distortion-computation surface). <p> For large n we can make the approximation k k S X ( 2k 2 Multiplications in which one or more multiplicands are ra tional are not counted. 3 The latter quantities are R n (D) points as defined in <ref> [1] </ref>. 4 This is the "original" DCT first reported in [2] and classified as DCT-II in [3]. where S X (!) = 1ff 2 12ff cos !+ff 2 is the power spectral density of X [4, 5].
Reference: [2] <author> N. Ahmed, T. Natarajan, and K. R. Rao. </author> <title> Discrete cosine transform. </title> <journal> IEEE Trans. Comp., </journal> <volume> 23 </volume> <pages> 90-93, </pages> <month> Jan. </month> <year> 1974. </year>
Reference-contexts: For large n we can make the approximation k k S X ( 2k 2 Multiplications in which one or more multiplicands are ra tional are not counted. 3 The latter quantities are R n (D) points as defined in [1]. 4 This is the "original" DCT first reported in <ref> [2] </ref> and classified as DCT-II in [3]. where S X (!) = 1ff 2 12ff cos !+ff 2 is the power spectral density of X [4, 5].
Reference: [3] <author> K. R. Rao and P. Yip. </author> <title> Discrete Cosine Transform: Algorithms, Advantages, Applications. </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: the approximation k k S X ( 2k 2 Multiplications in which one or more multiplicands are ra tional are not counted. 3 The latter quantities are R n (D) points as defined in [1]. 4 This is the "original" DCT first reported in [2] and classified as DCT-II in <ref> [3] </ref>. where S X (!) = 1ff 2 12ff cos !+ff 2 is the power spectral density of X [4, 5]. <p> For n a power of two, one possible implementation of the DCT (which does not have an inordinate number of additions) has 1 2 n log 2 n multiplications <ref> [3] </ref>. To maintain an analogy with the multiplication count for the KLT, we should consider pruned computations that determine only the DCT coefficients with positive bit allocations.
Reference: [4] <author> U. Grenander and G. </author> <title> Szego. Toeplitz Forms and Their Applications. </title> <publisher> Univ. California Press, </publisher> <year> 1958. </year>
Reference-contexts: are not counted. 3 The latter quantities are R n (D) points as defined in [1]. 4 This is the "original" DCT first reported in [2] and classified as DCT-II in [3]. where S X (!) = 1ff 2 12ff cos !+ff 2 is the power spectral density of X <ref> [4, 5] </ref>. This approximation, however, dismisses the coding gain difference between the KLT and the DCT and obscures the dependence on n, so in the remainder of the paper we will use the exact values for the k 's and k 's.
Reference: [5] <author> R. M. Gray. </author> <title> On the asymptotic eigenvalue distribution of Toeplitz matrices. </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> 18(6) </volume> <pages> 725-730, </pages> <month> Nov. </month> <year> 1972. </year>
Reference-contexts: are not counted. 3 The latter quantities are R n (D) points as defined in [1]. 4 This is the "original" DCT first reported in [2] and classified as DCT-II in [3]. where S X (!) = 1ff 2 12ff cos !+ff 2 is the power spectral density of X <ref> [4, 5] </ref>. This approximation, however, dismisses the coding gain difference between the KLT and the DCT and obscures the dependence on n, so in the remainder of the paper we will use the exact values for the k 's and k 's.
Reference: [6] <author> A. Gersho and R. M. Gray. </author> <title> Vector Quantization and Signal Compression. </title> <publisher> Kluwer Acad. Pub., </publisher> <year> 1992. </year>
Reference-contexts: high rate approximations and assuming optimal scalar quantization leads to the following optimal (arbitrary real) bit allocation for coding at rate R bits/sample: b i = R + 2 i 2 ; where 2 = i=1 ! 1=n Under these assumptions, the distortion is given by the fol lowing expressions <ref> [6] </ref>: D KLT = 3 i=1 ! 1=n D DCT = 3 i=1 ! 1=n Using the true component variances we obtain the dotted D (n) curves shown in Fig. 1. By abandoning high rate approximations one can obtain more precise results. <p> By abandoning high rate approximations one can obtain more precise results. In particular, instead of using approximate expressions for optimal companding, we computed the performance using nonneg-ative integer bit allocation according to a greedy algorithm <ref> [6, x8.4] </ref> and uniform quantization with optimal loading. These results are also shown in Fig. 1, along with the performance obtained with no transform and D n (R). Note that for ff = 0:9, the performance of KLT coding is virtually indistinguishable from that of DCT coding.
Reference: [7] <author> R. E. Blahut. </author> <title> Fast Algorithms for Digital Signal Processing. Addison-Wesley, 1985 (with corrections 1987). </title>
Reference-contexts: The block sizes are powers of two. 3.1.2. Estimating computational load Consider first the computational complexity measure given by the number of multiplications between arbitrary real numbers per input sample. This model is familiar because of its connection to Winograd convolution algorithms and provable complexity lower bounds <ref> [7, 8] </ref>. The computation of any square linear transform of size n can be viewed as a multiplication between an n fi n matrix and an n fi 1 vector. <p> For convenience, we will assume that n = 2 s , s 2 Z + . To code n vectors at a time would entail multiplying pairs of n fi n matrices, which can be done with n log 2 7 multiplications using Strassen's method <ref> [7] </ref>.
Reference: [8] <author> M. T. Heideman. </author> <title> Multiplicative Complexity, Convolution, and the DFT. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: The block sizes are powers of two. 3.1.2. Estimating computational load Consider first the computational complexity measure given by the number of multiplications between arbitrary real numbers per input sample. This model is familiar because of its connection to Winograd convolution algorithms and provable complexity lower bounds <ref> [7, 8] </ref>. The computation of any square linear transform of size n can be viewed as a multiplication between an n fi n matrix and an n fi 1 vector. <p> The minimum number of multiplications to compute a length-n DCT is 2n log 2 n 2 <ref> [8] </ref>. Normalizing gives C DCT = 2 1 n (log 2 n 2) multiplies/sample. (6) When using moderate block sizes and usual computer architectures, algorithms that minimize the number of multiplications are generally not efficient.
Reference: [9] <author> K. Lengwehasatit and A. Ortega. </author> <title> DCT computation with minimal average number of operations. </title> <booktitle> In Proc. SPIE Conf. on Vis. Commun. and Image Proc., </booktitle> <address> San Jose, California, </address> <month> Feb. </month> <year> 1997. </year> <month> 4 </month>
Reference-contexts: The design of such input-dependent algorithms is generally done in an ad hoc fashion. A notable exception is work presented in <ref> [9] </ref>, where the following approach to input-dependent inverse DCT computation is proposed: classify each input block based on its sparsity structure and use a pruned DCT algorithm optimized for that sparsity structure. The classes are designed to minimize the average computational load over a training set.
References-found: 9

