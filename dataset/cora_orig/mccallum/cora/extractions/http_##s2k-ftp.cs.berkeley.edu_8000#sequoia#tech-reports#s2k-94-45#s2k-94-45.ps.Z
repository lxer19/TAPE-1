URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-94-45/s2k-94-45.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-94-45/
Root-URL: http://www.cs.berkeley.edu
Title: Single Query Optimization for Tertiary Memory  
Author: Sunita Sarawagi Michael Stonebraker 
Address: Berkeley  
Affiliation: Computer Science Division University of California at  
Abstract: We present query execution strategies that are optimized for the characteristics of tertiary memory devices. Traditional query execution methods are oriented to magnetic disk or main memory and perform poorly on tertiary memory. Our methods use ordering and batching techniques on the I/O requests to reduce the media switch cost and seek cost on these devices. Some of our methods are provably optimal and others are shown to be superior by simulation and cost formula analysis.
Abstract-found: 1
Intro-found: 1
Reference: [CAR93] <author> M.J. Carey, L.M. Haas, and M. Livny. </author> <title> Tapes hold data, too: challenges of tuples on tertiary store. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 22(2) </volume> <pages> 413-417, </pages> <year> 1993. </year>
Reference-contexts: Tertiary memory, if used at all, functioned only as an archival storage system to be written once and rarely read. With the inclusion of tertiary memory as an active part of the storage hierarchy it is necessary to rethink the optimization decisions made by a DBMS [STO91a] <ref> [CAR93] </ref>. In this paper, we propose improvements to existing query execution strategies to adapt them to tertiary storage devices. Tertiary memory devices have very different performance characteristics than magnetic disks. A typical device consists of a large number of storage media (tapes or disk platters) and a few read-write drives.
Reference: [DOZ91] <author> J. Dozier and H.K. Ramapriyan. </author> <title> Planning for the EOS data and information system. In Global Environment Change, volume 1. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Large capacity storage systems are essential for an increasing number of scientific and commercial applications. For example, research on global change effects requires the storage and analysis of massive amounts of satellite data [STO91]. The Earth Observation System (EOS) <ref> [DOZ91] </ref> alone is expected to provide one terabyte per day of raw data to global change scientists. Such volumes of data require high capacity tertiary memory devices [KAT91] for storage and smart data base systems for efficient query support.
Reference: [KAT91] <author> R.H. Katz et al. </author> <title> Robo-line storage: High capacity storage systems over geographically distributed networks. </title> <type> Sequoia 2000 Technical Report 91/3, </type> <institution> University of California at Berkeley, </institution> <year> 1991. </year>
Reference-contexts: The Earth Observation System (EOS) [DOZ91] alone is expected to provide one terabyte per day of raw data to global change scientists. Such volumes of data require high capacity tertiary memory devices <ref> [KAT91] </ref> for storage and smart data base systems for efficient query support. Traditional DBMSs have assumed that all data reside on magnetic disk or main memory. Therefore all optimization decisions were oriented towards this technology.
Reference: [STO91] <author> M. Stonebraker and J. Dozier. </author> <title> Large capacity object servers to support global change research. </title> <type> Sequoia 2000 Technical Report 91/1, </type> <institution> University of California at Berkeley, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction Large capacity storage systems are essential for an increasing number of scientific and commercial applications. For example, research on global change effects requires the storage and analysis of massive amounts of satellite data <ref> [STO91] </ref>. The Earth Observation System (EOS) [DOZ91] alone is expected to provide one terabyte per day of raw data to global change scientists. Such volumes of data require high capacity tertiary memory devices [KAT91] for storage and smart data base systems for efficient query support.
Reference: [SHA86] <author> L.D. Shapiro. </author> <title> Join processing in database systems with large main memory. </title> <journal> ACM Transactions on database systems, </journal> <volume> 11(3) </volume> <pages> 239-264, </pages> <year> 1986. </year>
Reference-contexts: list involves an index tree search, we need to remember the index key of the last tuple in the previous partition and start the new TID list formation from that key. 3.2 Two-level Hash Join If the smaller relation, R, fits in the disk cache, any conventional hash join method <ref> [SHA86] </ref> will work well for tertiary memory databases. Otherwise, the conventional methods of partitioning the two relations into buckets that fit in the cache and doing the join between the partitioned 9 buckets can be inefficient for some tertiary memory devices. <p> For joining tuples within a bucket we suggest using classic hash <ref> [SHA86] </ref> because it requires only one scan for tuples of the outer relation and hence does not require any cache space for the probing relation.
Reference: [STO91a] <author> M. Stonebraker. </author> <title> Managing persistent objects in a multi-level store. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 20(2) </volume> <pages> 2-11, </pages> <year> 1991. </year>
Reference-contexts: Tertiary memory, if used at all, functioned only as an archival storage system to be written once and rarely read. With the inclusion of tertiary memory as an active part of the storage hierarchy it is necessary to rethink the optimization decisions made by a DBMS <ref> [STO91a] </ref> [CAR93]. In this paper, we propose improvements to existing query execution strategies to adapt them to tertiary storage devices. Tertiary memory devices have very different performance characteristics than magnetic disks.
Reference: [STO91b] <author> M. Stonebraker. </author> <title> An overview of the Sequoia 2000 project. </title> <type> Sequoia 2000 Technical Report 91/5, </type> <institution> University of California at Berkeley, </institution> <year> 1991. </year> <month> 12 </month>
Reference-contexts: For example, the Sequoia 2000 project <ref> [STO91b] </ref> has a 14 gigabyte magnetic disk cache attached to 9 terabyte tape robot. As such, cache size is 0.16% of storage size and few cache "hits" can be expected.
References-found: 7

