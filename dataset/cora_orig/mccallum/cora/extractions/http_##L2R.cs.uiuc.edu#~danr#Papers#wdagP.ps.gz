URL: http://L2R.cs.uiuc.edu/~danr/Papers/wdagP.ps.gz
Refering-URL: http://L2R.cs.uiuc.edu/~danr/publications.html
Root-URL: http://www.cs.uiuc.edu
Title: Efficient, Strongly Consistent Implementations of Shared Memory (Extended Abstract)  
Author: Marios Mavronicolas Dan Roth 
Address: Cambridge, MA 02138, USA  
Affiliation: Aiken Computation Laboratory, Harvard University,  
Note: Appeared in the 6th International Workshop on Distributed Algorithms, WDAG '92  
Abstract: We present linearizable implementations for two distributed organizations of multiprocessor shared memory. For the full caching organization, where each process keeps a local copy of the whole memory, we present a linearizable implementations of read/write memory objects that achieves essentially optimal efficiency and allows quantitative degradation of the less frequently employed operation. For the single ownership organization, where each memory object is "owned" by a single process which is most likely to access it frequently, our linearizable implementation allows local operations to be performed much faster (almost instantaneously) than remote ones. We suggest to combine these organizations in a "hybrid" memory structure that allows processes to access local and remote information in a transparent manner, while at a lower level of the memory consistency system, different portions of the memory are allocated to employ the suitable implementation based on their typical usage and sharing pat tern.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Y. Afek, G. Brown and M. Merritt, </author> <title> "A Lazy Cache Algorithm," </title> <booktitle> in Proceedings of the 1st ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 209-222, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Many authors (e.g., <ref> [1, 2, 5, 6, 8, 14] </ref>) have argued that supporting strong consistency conditions is difficult to implement efficiently.
Reference: 2. <author> M. Ahamad, P. Hutto and R. John, </author> <title> Implementing and Programming Causal Distributed Shared Memory, </title> <type> TR GIT-CC-90-49, </type> <institution> Georgia Institute of Technology, </institution> <month> De-cember </month> <year> 1990. </year>
Reference-contexts: Many authors (e.g., <ref> [1, 2, 5, 6, 8, 14] </ref>) have argued that supporting strong consistency conditions is difficult to implement efficiently. <p> Capturing this intuitive tradeoff between the degree of "correctness" and concurrency, different memory systems have been suggested in order to resolve these problems. In most cases, researchers investigated different types of weaker consistency conditions, placing some of the correctness under the control of the programmer (e.g., <ref> [2, 8] </ref>). Other researchers explored the possibility of structuring the memory system as a hierarchy of caches, counting on some sort of isolation between processes that access shared data and those that do not. <p> Other researchers explored the possibility of structuring the memory system as a hierarchy of caches, counting on some sort of isolation between processes that access shared data and those that do not. In some cases (e.g., <ref> [2] </ref>), a distributed shared memory was implemented by separating it at a higher level to memory accessed locally and memory accessed through ordered message passing between processes, thus losing an important advantage of shared memory abstraction in programming distributed applications, that of uniformity of accessing, without gaining anything as far as <p> It is notable, though, that our implementation allows for efficient local operations, a feature sought after by other researchers as well (e.g., <ref> [2, 6] </ref>), but is still linearizable. In the single ownership organization the owner of an object (also called the master process for that object) is the only process which keeps a local copy of it, and coordinates operations performed by other processes 7 on it.
Reference: 3. <author> H. Attiya, </author> <title> "Implementing FIFO Queues and Stacks," </title> <booktitle> in Proceedings of the 5th Workshop on Distributed Algorithms, </booktitle> <pages> pp. 80-94, </pages> <booktitle> Lecture Notes in Computer Science (Vol. </booktitle> <volume> 579), </volume> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: object X that can be accessed by at least two processes, jR (X)j u 2 . 6 Discussion and Future Research This paper continues the complexity-theoretic study of the costs of implementing memory objects under various correctness conditions for shared-memory multiprocessor systems, initiated in [4, 10] and further pursued in <ref> [3, 12] </ref>. Furthermore, we suggested a new kind of memory organization, which combines all the advantages of a global strong consistency condition with the ability to efficiently perform local operations.
Reference: 4. <author> H. Attiya and J. Welch, </author> <title> "Sequential Consistency versus Linearizability," </title> <booktitle> in Proceedings of the 3rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 304-315, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The first complexity-theoretic analysis of this tradeoff appears in <ref> [4] </ref>. <p> Our next impossibility result is a lower bound of u 2 on the worst-case response time for a read operation in any linearizable implementation. The proof makes use of the "shifting" technique to improve upon a lower bound given in <ref> [4] </ref>. Theorem 7. <p> any linearizable implementation of a read/write object X that can be accessed by at least two processes, jR (X)j u 2 . 6 Discussion and Future Research This paper continues the complexity-theoretic study of the costs of implementing memory objects under various correctness conditions for shared-memory multiprocessor systems, initiated in <ref> [4, 10] </ref> and further pursued in [3, 12]. Furthermore, we suggested a new kind of memory organization, which combines all the advantages of a global strong consistency condition with the ability to efficiently perform local operations.
Reference: 5. <author> Jean-Loup Baer and Weu-Hann Wang, </author> <title> "Multilevel Cache Hierarchies: Organizations, Protocols and Performance," </title> <journal> in Journal of Parallel and Distributed Computing, </journal> <volume> 6, </volume> <pages> pp. 451-476, </pages> <year> 1989. </year>
Reference-contexts: Many authors (e.g., <ref> [1, 2, 5, 6, 8, 14] </ref>) have argued that supporting strong consistency conditions is difficult to implement efficiently.
Reference: 6. <author> G. Brown and M. Merritt, </author> <title> "Hierarchical Lazy Caching," </title> <booktitle> in Proceedings of the 28th Annual Allerton Conference on Communication, Control and Computing, </booktitle> <pages> pp. 548-557, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Many authors (e.g., <ref> [1, 2, 5, 6, 8, 14] </ref>) have argued that supporting strong consistency conditions is difficult to implement efficiently. <p> It is notable, though, that our implementation allows for efficient local operations, a feature sought after by other researchers as well (e.g., <ref> [2, 6] </ref>), but is still linearizable. In the single ownership organization the owner of an object (also called the master process for that object) is the only process which keeps a local copy of it, and coordinates operations performed by other processes 7 on it.
Reference: 7. <author> M. Herlihy and J. Wing, </author> <title> "Linearizability: A Correctness Condition for Concurrent Objects," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol 12, No. 3, </volume> <pages> pp. 463-492, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: We say that ff is linearizable (cf. <ref> [7] </ref>) if there exists such an admissible t which respects ff and, in addition, is ff-linearizable: whenever the response for operation op 1 precedes the call for operation op 2 in ops (ff), then op 1 precedes op 2 in t ; that is, the order of any two non-overlapping operations
Reference: 8. <author> P. Hutto and M. Ahamad, </author> <title> Slow Memory: Weakening Consistency to Enhance Con-currency in Distributed Shared Memories, </title> <type> TR GIT-ICS-89/39, </type> <institution> Georgia Institute of Technology, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Many authors (e.g., <ref> [1, 2, 5, 6, 8, 14] </ref>) have argued that supporting strong consistency conditions is difficult to implement efficiently. <p> Capturing this intuitive tradeoff between the degree of "correctness" and concurrency, different memory systems have been suggested in order to resolve these problems. In most cases, researchers investigated different types of weaker consistency conditions, placing some of the correctness under the control of the programmer (e.g., <ref> [2, 8] </ref>). Other researchers explored the possibility of structuring the memory system as a hierarchy of caches, counting on some sort of isolation between processes that access shared data and those that do not.
Reference: 9. <author> L. Lamport, </author> <title> "How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs," </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-28, No.9, </volume> <pages> pp. 690-691, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: The definitions of the correctness conditions involve, for an execution ff, the existence of an operation sequence t which is a permutation of the operations in ff and possesses certain properties. We say that ff is sequentially consistent (cf. <ref> [9] </ref>) if there exists such an admissible t which also respects ff: for each process p i , the restriction of ops (ff) (ops (ff) denotes the sequence of call and response events appearing in ff in real-time order) to operations of p i is equal to the restriction of t
Reference: 10. <author> R. Lipton and J. Sandberg, </author> <title> A Scalable Shared Memory, </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Two technical claims facilitate our main construction. Our first claim (inspired by a result in <ref> [10] </ref>) shows that by carefully choosing message delays, and using the shifting technique, a non-symmetric configuration "looks" totally symmetric to the processes. <p> any linearizable implementation of a read/write object X that can be accessed by at least two processes, jR (X)j u 2 . 6 Discussion and Future Research This paper continues the complexity-theoretic study of the costs of implementing memory objects under various correctness conditions for shared-memory multiprocessor systems, initiated in <ref> [4, 10] </ref> and further pursued in [3, 12]. Furthermore, we suggested a new kind of memory organization, which combines all the advantages of a global strong consistency condition with the ability to efficiently perform local operations.
Reference: 11. <author> J. Lundelius and N. Lynch, </author> <title> "An Upper and Lower Bound for Clock Synchronization," </title> <journal> Information and Control, </journal> <volume> Vol. 62, No. 2/3, </volume> <pages> pp. 190-204, </pages> <month> August/September </month> <year> 1984. </year>
Reference-contexts: Upon a W rite i (X; v) event and when in the 6 Although, by the results of Lundelius and Lynch in <ref> [11] </ref>, an accuracy of u is not optimal, our synchronization algorithm is extremely simple and an accuracy of u suffices here. appropriate time interval, p i broadcasts an update (X; v) message and waits for an additional (1 fi)d time to set X i to v and issue Ack i (X). <p> The lower bound proof combines the use of various methods already common in the theory of distributed computing, namely, symmetry arguments and the technique of "shifting" executions (originally introduced in <ref> [11] </ref>), with a novel technique of augmenting executions to "causally link" them. The known lower bound of d u on message delay time is used in constructing the executions, to achieve non-availability of knowledge to processes.
Reference: 12. <author> M. Mavronicolas and D. Roth, </author> <title> "Sequential Consistency and Linearizability: Read/Write Objects," </title> <booktitle> in Proceedings of the 29th Annual Allerton Conference on Communication, Control and Computing, </booktitle> <month> October </month> <year> 1991. </year> <note> Expanded version: "Lin-earizable Read/Write Objects,", Technical Report TR-28-91, </note> <institution> Aiken Computation Laboratory, Harvard University, </institution> <year> 1991. </year> <note> Submitted for publication. </note>
Reference-contexts: We develop a family of efficient implementations of read/write objects that support linearizability. This is the first linearizable implementation, known for read/write objects 4 . 4 Earlier results of this research were reported in <ref> [12] </ref>. We describe a family of implementations, parameterized by fi, and allow for the selection of a member of that family, in order to degrade the less frequently employed (read or write) operation. These implementations, C-Linear, make heavy use of a novel time slicing technique which is of independent interest. <p> This motivates our linearizable implementations. Our implementation uses a simple synchronization procedure, Synch, originally introduced and used in <ref> [12] </ref>, which is run during an initialization phase of 5 As we observe later, in a caching linearizable implementation, the decision on which value to return in a read operation may not depend solely on timing information available to the process. the implementation, to enable the processes achieve a certain amount <p> Each process p i sets its local time to 0 on either the first receipt of some (synch) message from any other process or on expiration of T , whichever happens first. In <ref> [12] </ref>, we show the following property of Synch 6 : Fact 1. Synch synchronizes the system within accuracy u, i.e., the maximum difference between the local times of any two processes at any real time after all processes have completed Synch is at most u. <p> object X that can be accessed by at least two processes, jR (X)j u 2 . 6 Discussion and Future Research This paper continues the complexity-theoretic study of the costs of implementing memory objects under various correctness conditions for shared-memory multiprocessor systems, initiated in [4, 10] and further pursued in <ref> [3, 12] </ref>. Furthermore, we suggested a new kind of memory organization, which combines all the advantages of a global strong consistency condition with the ability to efficiently perform local operations.
Reference: 13. <author> M. Mavronicolas and D. Roth, </author> <title> Efficient, Strongly Consistent Implementations of Shared Memory, </title> <type> Technical Report TR-05-92, </type> <institution> Aiken Computation Laboratory, Har-vard </institution>
Reference-contexts: Due to lack of space, the details of some of our definitions, constructions and proofs are omitted here. We refer the reader to <ref> [13] </ref> for an expanded version of this paper. 2 The Model The model we consider consists of a collection of application programs running concurrently and communicating through virtual shared memory which consists of read/write objects.
Reference: 14. <author> Weu-Hann Wang, Jean-Loup Baer and Henry M. Levy, </author> <title> "Organization and Performance of a Two-level Virtual-Real Cache Hierarchy," </title> <booktitle> in Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <pages> pp. 140-148, </pages> <month> June </month> <year> 1989. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Many authors (e.g., <ref> [1, 2, 5, 6, 8, 14] </ref>) have argued that supporting strong consistency conditions is difficult to implement efficiently.
References-found: 14

