URL: http://www.cs.umn.edu/Research/Agassiz/Paper/li.icpp94.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Title: An Empirical Study of the Workload Distribution Under Static Scheduling  
Author: Zhiyuan Li Trung N. Nguyen 
Address: 200 Union St. SE, Minneapolis, MN 55455  
Affiliation: Department of Computer Science University of Minnesota  
Abstract: In the decision regarding static scheduling vs. dynamic scheduling, the only argument against the former is the potential imbalance of the workload. However, it has never been clear how the workload distributes in the iterations of Fortran parallel loops. This work examines a set of Perfect benchmarking programs [2] and report two striking results. First, when using operation counts as the measure, almost all the parallel loops identified by the Parafrase 2 compiler [11] in those programs have an equal workload among different iterations. Second, simulation shows that, although run time events on the memory system can lead to considerable variation in the execution time of different iterations which are scheduled statically, the finish time of the slowest processor normally does not exceed the average processor by the mean execution time of one loop iteration. Such workload distribution is as good as any known dynamic scheduling policy can guarantee. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. G. Bell. </author> <title> Multi: a new class of multiprocessor computers. </title> <journal> Science, </journal> <volume> 228, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: Thus, memory allocation has no significant effect on the memory access time. A cache hit, however, results in a much shorter access latency. This model seems to represent both the multi type multiprocessors (including the Sequent, Concurrent, and Encore machines <ref> [1] </ref>) and the COMA architectures [12], [3] reasonably well, although they do not match precisely. This model does not represent other architectures as well. For example, the Cray XMP and YMP has a shared memory but each processor has a large private register file instead of a private cache.
Reference: [2] <author> M. Berry, D. Chen, P. Koss, D. Kuck, and S. Lo. </author> <title> The perfect club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report CSRD-827, </type> <institution> University of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: However, it is difficult to do the same with dynamic scheduling. This work performs an empirical study of the workload variation among the iterations of DOALL loops. We examine a set of Perfect benchmarking programs <ref> [2] </ref> and use the Parafrase 2 parallelizing compiler [11] to identify the DOALL loops. We obtain two striking results. The first result shows that, when using operation counts as the workload measure, almost all those DOALL loops identified by the Parafrase 2 have an equal workload among different iterations.
Reference: [3] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM a cache-only memory architecture. </title> <journal> Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Thus, memory allocation has no significant effect on the memory access time. A cache hit, however, results in a much shorter access latency. This model seems to represent both the multi type multiprocessors (including the Sequent, Concurrent, and Encore machines [1]) and the COMA architectures [12], <ref> [3] </ref> reasonably well, although they do not match precisely. This model does not represent other architectures as well. For example, the Cray XMP and YMP has a shared memory but each processor has a large private register file instead of a private cache.
Reference: [4] <author> S. F. Hummel, E. Schonberg, and L. E. Flynn. </author> <title> Factoring: A method for scheduling parallel loops. </title> <journal> CACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Authors of the affinity dynamic scheduling algorithm report similar results for small kernels [8]. Although these kernels are small, their results indicate that our conclusion may well extend to the NUMA architectures. The LDS authors also compare their results with previous results obtained on the RP3 machine <ref> [4] </ref> and suggest that the memory allocation on NUMA architectures may considerably affect the comparison between static scheduling and dynamic scheduling. 2.3 Variants of scheduling policies Under static scheduling, the compiler assigns a set of DOALL loop iterations to each processor by manipulating the limits of the main DO loop in <p> Dynamic scheduling has several variants whose main difference is in the number of iterations each processor may request. The guided self scheduling (GSS) [10] and some of its close variants <ref> [4] </ref>, [6] exhibit the following optimality: assuming each iteration takes exactly the same amount of time to execute, then all processors finish executing the parallel loop within the time difference of one iteration from each other.
Reference: [5] <author> D. Lenoski, J. Laudon, K. Gharachrloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The stanford dash multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The NUMA architectures present yet another case to consider (see <ref> [5] </ref>, [13] for examples). Each processor in such machines has fast access to its local memory which is part of the common address space but has much slower access to any remote memory (which is the local memory of another processor).
Reference: [6] <author> H. Li, S. Tandri, M. Stumm, and K. C. Sevcik. </author> <title> Locality and loop scheduling on NUMA multiprocessors. </title> <booktitle> In Proc. 1993 International Conference on Parallel Processing, </booktitle> <pages> pages II:140-147, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Memory allocation may have a significant effect on the access time on NUMA. The memory allocation is a difficult problem when we are dealing with real programs such as the Perfect benchmarks. Nonetheless, the authors of a recent algorithm, the LDS, for dynamic scheduling <ref> [6] </ref> have good news for static scheduling after they study the performance of a few numerical kernels. <p> Currently, there is no satisfactory mechanism to allow dynamic scheduling to take advantage of the two kinds of opportunities we mentioned above. (New possibilities are explored in [8], <ref> [6] </ref>.) Nonetheless, it is commonly believed that dynamic scheduling tends to produce more balanced workload than static scheduling where the operation counts of individual iterations vary significantly. Dynamic scheduling has several variants whose main difference is in the number of iterations each processor may request. <p> Dynamic scheduling has several variants whose main difference is in the number of iterations each processor may request. The guided self scheduling (GSS) [10] and some of its close variants [4], <ref> [6] </ref> exhibit the following optimality: assuming each iteration takes exactly the same amount of time to execute, then all processors finish executing the parallel loop within the time difference of one iteration from each other.
Reference: [7] <author> Z. Li and T. N. Nguyen. </author> <title> An empirical study of the workload distribution under static scheduling. </title> <type> Technical Report CS-TR-94-03, </type> <institution> University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year>
Reference-contexts: The iterations are assigned to processors using the cyclic static scheduling. 4.2 The variance Next, we examine the simulation results. As one can see, only three DOALL loops belong to the second category, one of which is executed only once. The details of these loops can be found in <ref> [7] </ref>. Here, we focus our discussion on DOALL loops that have zero variance in the operation counts, because we are mainly interested in whether the operation counts are a good measure.
Reference: [8] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <booktitle> In Supercomputing 92, </booktitle> <pages> pages 104-113, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: They suggest that, on NUMA architectures, static scheduling is superior to all known dynamic scheduling algorithms when the processors have roughly an equal workload (which apparently is measured in terms of the operation counts). Authors of the affinity dynamic scheduling algorithm report similar results for small kernels <ref> [8] </ref>. Although these kernels are small, their results indicate that our conclusion may well extend to the NUMA architectures. <p> Currently, there is no satisfactory mechanism to allow dynamic scheduling to take advantage of the two kinds of opportunities we mentioned above. (New possibilities are explored in <ref> [8] </ref>, [6].) Nonetheless, it is commonly believed that dynamic scheduling tends to produce more balanced workload than static scheduling where the operation counts of individual iterations vary significantly. Dynamic scheduling has several variants whose main difference is in the number of iterations each processor may request.
Reference: [9] <author> T. N. Nguyen, Z. Li, and D. J. Lilja. </author> <title> Efficient use of dynamically tagged directories through compiler analysis. </title> <booktitle> In Proc. 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <year> 1993. </year>
Reference-contexts: The memory trace generating statements are instrumented in the Fortran source programs by a special pass we add in the Parafrase 2 compiler <ref> [9] </ref>. Table 1 lists data for these two categories (labeled "Equal-load loops" and "Unequal-load loops", respectively). The first column of the table lists the names of the examined programs, nine out of the thirteen total Perfect benchmarks.
Reference: [10] <author> C. Polychronopoulos and D. J. Kuck. </author> <title> Guided self scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Dynamic scheduling has several variants whose main difference is in the number of iterations each processor may request. The guided self scheduling (GSS) <ref> [10] </ref> and some of its close variants [4], [6] exhibit the following optimality: assuming each iteration takes exactly the same amount of time to execute, then all processors finish executing the parallel loop within the time difference of one iteration from each other. <p> This figure shows that in the majority of the instances of the DOALL loops, the ratio of the difference over the iteration length is less than 1. A theorem in <ref> [10] </ref> implies that, assuming the execution time of different iterations is the same, the guided self scheduling (GSS) can guarantee that the slowest processor finishes execution no later than within the iteration length. This is also true for certain variants of the GSS.
Reference: [11] <author> C. D. Polychronopoulos, M. B. Girkar, M. R. Haghighat, C. L. Lee, B. P. Leung, and D. A. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <booktitle> In Proc. 1989 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: However, it is difficult to do the same with dynamic scheduling. This work performs an empirical study of the workload variation among the iterations of DOALL loops. We examine a set of Perfect benchmarking programs [2] and use the Parafrase 2 parallelizing compiler <ref> [11] </ref> to identify the DOALL loops. We obtain two striking results. The first result shows that, when using operation counts as the workload measure, almost all those DOALL loops identified by the Parafrase 2 have an equal workload among different iterations.
Reference: [12] <institution> Kendall Square Research. Technical Summary. </institution> <year> 1992. </year>
Reference-contexts: Thus, memory allocation has no significant effect on the memory access time. A cache hit, however, results in a much shorter access latency. This model seems to represent both the multi type multiprocessors (including the Sequent, Concurrent, and Encore machines [1]) and the COMA architectures <ref> [12] </ref>, [3] reasonably well, although they do not match precisely. This model does not represent other architectures as well. For example, the Cray XMP and YMP has a shared memory but each processor has a large private register file instead of a private cache.
Reference: [13] <author> R. D. Rettberg, W. R. Crowther, P. P. Carvey, and R. S. Tomlinson. </author> <title> The monarch parallel processor hardware design. </title> <journal> Computer, </journal> <volume> 23(4) </volume> <pages> 18-30, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The NUMA architectures present yet another case to consider (see [5], <ref> [13] </ref> for examples). Each processor in such machines has fast access to its local memory which is part of the common address space but has much slower access to any remote memory (which is the local memory of another processor).
Reference: [14] <author> P. Stenstrom, J. Truman, and A. Gupta. </author> <title> Comparative performance evaluation of cache-coherent numa and coma architectures. </title> <booktitle> In Proc. 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <year> 1992. </year>
Reference-contexts: Table 2: Memory reference timing Events Cycles read hit 1 Read miss 79 Read miss w/ write-back 99 Write hit (no inv) 1 Write hit (w/ inv) 73 Write miss (no inv) 41 Write miss (w/ inv) 80 Write-back penalty 20 and memory latency parameters from the Stanford DASH multiprocessor <ref> [14] </ref>, we set the timing values for the memory references as shown in Table 2. (DASH is a cache-coherent NUMA machine.
References-found: 14

