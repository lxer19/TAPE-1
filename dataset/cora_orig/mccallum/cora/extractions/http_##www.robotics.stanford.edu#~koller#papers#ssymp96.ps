URL: http://www.robotics.stanford.edu/~koller/papers/ssymp96.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/ssymp96.html
Root-URL: http://www.robotics.stanford.edu
Title: Approximate probabilistic inference in dynamic processes  
Author: Daphne Koller 
Address: Gates Building 1A, Room 142  Stanford, CA 94305  
Affiliation: Computer Science Department  Stanford University  
Abstract: When dealing with a dynamic process, we are typically interested in keeping track of where we are, and in predicting where we will be in the future. When the dynamic process is stochastic and partially observable, as it invariably is, the best we can hope for is to have accurate beliefs about the state of the system. Thus, we want to maintain a belief state: a probability distribution over the possible states the system can be in. When the state space is large (or infinite), it is typically impossible to maintain a completely accurate representation of the belief state. In this paper, we investigate the possibility of maintaining an approximate belief state. We argue that it can be very useful to use the (approximate) belief state at time t to focus attention on the relevant aspects of the situation at time t +1, thereby providing guidance to the algorithm that computes the next belief state. Thus, our belief state should guide not just our "real world" actions, but also our computational actions. We present some preliminary results supporting this claim in the context of stochastic simulation algorithms, and suggest ways in which this idea can be extended.
Abstract-found: 1
Intro-found: 1
Reference: [ Ast65 ] <author> K. J. Astrom. </author> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> J. Math. Anal. Applic., </journal> <volume> 10 </volume> <pages> 174-205, </pages> <year> 1965. </year>
Reference-contexts: Since the domain is stochastic and partially observable, the true state of the process is almost never known with certainty. At best, we can maintain an accurate belief state, which is a probability distribution over the current state. It is well known <ref> [ Ast65 ] </ref> that the belief state at time t completely captures all of the information implicit in our observations up to that time (this is a consequence of the Markov assumption). <p> The main idea is to use the belief state at time t to dynamically refocus our attention on the relevant aspects of the situation at time t + 1. We are accustomed to using our belief state to guide our "real-world" actions <ref> [ Ast65 ] </ref> . Our results suggest ways in which the belief state can be used to guide our computational actions as well. In Section 3, we describe one approach (recently suggested in [ KKR95 ] ) to doing this in the context of stochastic sampling.
Reference: [ DK89 ] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: Situations of this type invariably change in unpredictable (nondeterministic) ways, and always involve aspects that are not fully accessible to the observer. A number of formal models have been developed for describing situations of this type, including Hidden Markov Models, Kalman Filters, and Dynamic Belief Networks (DBNs) <ref> [ DK89 ] </ref> . All of them (at least in the version most often used) can be characterized using the same basic model, described in point in time t, the situation can be described using some state s 2 S.
Reference: [ DL93 ] <author> P. Dagum and M. Luby. </author> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence, </journal> <volume> 60(1) </volume> <pages> 141-153, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Let ~ + t be the distribution obtained from marginalizing ~ + on S t . If we could base ^ t+1 on the posterior ~ + t , stochastic simulation provides us with a polynomial time approximation scheme for probabilistic inference <ref> [ DL93 ] </ref> . Unfortunately, it is infeasible to sample from a distribution conditioned on evidence that we have not yet even seen.
Reference: [ FC89 ] <author> R. Fung and K. C. Chang. </author> <title> Weighting and integrating evidence for stochastic simulation in Bayesian networks. </title> <booktitle> In Proceedings 5th Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1989. </year>
Reference-contexts: The belief state at time t + 1 is generated from the belief state at time t using a standard sampling process. However, unlike standard stochastic simulation algorithms (e.g., likelihood weighting <ref> [ FC89; SP89 ] </ref> ), the way in which samples are generated for the belief state at time t + 1 depends on our belief state at time t. <p> In Figure 1 (b), for example, we have a variable representing the vehicle's position and one representing its velocity. We typically also have variables representing the intentions of the driver, e.g., the target-lane variable. In standard inference algorithms for DBNs (whether exact [ Kja92 ] or approximate <ref> [ FC89; SP89 ] </ref> ), the same amount of computational resources are expended on each of the variables. However, we would often like to focus more attention on some aspects of the situation. For example, we might want to maintain a better estimate for vehicle locations than for weather status. <p> We are currently investigating the properties of this process, with the goal of understanding how the error changes as a function of t. In particular, we hope to find conditions under which this error is bounded over time. 3 Stochastic simulation algorithms Likelihood weighting (LW) <ref> [ SP89; FC89 ] </ref> is one of the most effective of the stochastic simulation algorithms for belief networks.
Reference: [ HKM + 94 ] <author> T. Huang, D. Koller, J. Malik, G. Oga-sawara, B. Rao, S. J. Russell, and J. Weber. </author> <title> Automatic symbolic traffic scene analysis using belief networks. </title> <booktitle> In Proceedings 12th National Conference on Artificial Intelligence, </booktitle> <pages> pp. 966-972, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction In many real-world domains, we are interested in monitoring a complex situation that changes over time. For example, we may be observing a freeway traffic scene via a video camera mounted on a bridge, with the goal of understanding the current traffic situation <ref> [ HKM + 94 ] </ref> . We may be monitoring a patient's vital signs in an intensive care unit via sensors, with the goal of alerting a physician in case of sudden change.
Reference: [ Kja92 ] <author> U. Kjaerulff. </author> <title> A computational scheme for reasoning in dynamic probabilistic networks. </title> <booktitle> In Proceedings 8th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. 121-129, </pages> <year> 1992. </year>
Reference-contexts: In Figure 1 (b), for example, we have a variable representing the vehicle's position and one representing its velocity. We typically also have variables representing the intentions of the driver, e.g., the target-lane variable. In standard inference algorithms for DBNs (whether exact <ref> [ Kja92 ] </ref> or approximate [ FC89; SP89 ] ), the same amount of computational resources are expended on each of the variables. However, we would often like to focus more attention on some aspects of the situation. <p> This does not mesh very well with the sampling process in standard stochastic simulation algorithms. However, in SOF, we often want to propagate some samples (the more likely ones) multiple times. How do we adapt exact inference algorithms (such as <ref> [ Kja92 ] </ref> )? These return the correct distribution over all of the state variables at time t + 1, so there seems to be no way in which their attention can be dynamically "focused".
Reference: [ KKR95 ] <author> K. Kanazawa, D. Koller, and S. J. Rus-sell. </author> <title> Stochastic simulation algorithms for dynamic probabilistic networks. </title> <booktitle> In Proceedings 11th Annual Conference on Uncertainty in AI, </booktitle> <pages> pp. 346-351, </pages> <year> 1995. </year>
Reference-contexts: We are accustomed to using our belief state to guide our "real-world" actions [ Ast65 ] . Our results suggest ways in which the belief state can be used to guide our computational actions as well. In Section 3, we describe one approach (recently suggested in <ref> [ KKR95 ] </ref> ) to doing this in the context of stochastic sampling. Intuitively, at each time t, we use a set of randomly sampled states, each with an associated weight, to represent the approximate belief state at t. <p> In fact, we are generating all of our samples at time t from the prior distribution on S t . In our traffic surveillance example, our samples represent completely imaginary tracks for our vehicle, most of which will have very low likelihood. One solution to that, as described in <ref> [ KKR95 ] </ref> , is to allocate our samples more wisely, using the information gathered so far. <p> Therefore, SOF uses our approximate belief state ^ t to guide the choice of samples for ^ t+1 . The algorithm, as described in Figure 2 (b) (see also <ref> [ KKR95 ] </ref> for the details), samples a state s at random from ^ t ; s is used to generate a weighted sample at time t+1 by choosing s 0 with probability Pr (s ; s 0 ), and giving it weight Pr (s ,! o) (this phase is precisely
Reference: [ KL51 ] <author> S. Kullback and R. A. Leibler. </author> <title> On information and sufficiency. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22 </volume> <pages> 76-86, </pages> <year> 1951. </year>
Reference-contexts: The operator C o T tends to reduce the error between t and ^ t , while using d C o T instead of C o T tends to increase it. More precisely, for two distributions and over some probability space , the cross-entropy (or KL-distance <ref> [ KL51 ] </ref> ) of to is D (; ) = P (x) (x) . For many (well-known) reasons, cross-entropy is perhaps the most appropriate way to determine the distance between two distributions.
Reference: [ LS88 ] <author> S. L. Lauritzen and D. J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal Royal Statistical Society, </journal> <pages> pp. 157-224, </pages> <year> 1988. </year>
Reference-contexts: While details remain to be worked out, in principle this should not pose a problem to the standard belief network inference algorithms <ref> [ LS88 ] </ref> used to compute the belief state at the next time point. We are currently exploring this issue. How do we decide which are the more important variables to keep track of? One issue, of course, is the effect of the variable on our value function.
Reference: [ Pro93 ] <author> G. Provan. </author> <title> Tradeoffs in constructing and evaluating temporal influence diagrams. </title> <booktitle> In Proceedings 9th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. 40-47, </pages> <year> 1993. </year>
Reference-contexts: How do we adapt exact inference algorithms (such as [ Kja92 ] )? These return the correct distribution over all of the state variables at time t + 1, so there seems to be no way in which their attention can be dynamically "focused". Provan <ref> [ Pro93 ] </ref> presents an approach where the designer specifies that certain variables be eliminated from the process entirely. We suggest a more refined approach, where we dynamically choose to eliminate variables from some time slices but not from others.
Reference: [ SP89 ] <author> R. D. Shachter and M. A. Peot. </author> <title> Simulation approaches to general probabilistic inference on belief networks. </title> <booktitle> In Proceedings Fifth Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1989. </year>
Reference-contexts: The belief state at time t + 1 is generated from the belief state at time t using a standard sampling process. However, unlike standard stochastic simulation algorithms (e.g., likelihood weighting <ref> [ FC89; SP89 ] </ref> ), the way in which samples are generated for the belief state at time t + 1 depends on our belief state at time t. <p> In Figure 1 (b), for example, we have a variable representing the vehicle's position and one representing its velocity. We typically also have variables representing the intentions of the driver, e.g., the target-lane variable. In standard inference algorithms for DBNs (whether exact [ Kja92 ] or approximate <ref> [ FC89; SP89 ] </ref> ), the same amount of computational resources are expended on each of the variables. However, we would often like to focus more attention on some aspects of the situation. For example, we might want to maintain a better estimate for vehicle locations than for weather status. <p> We are currently investigating the properties of this process, with the goal of understanding how the error changes as a function of t. In particular, we hope to find conditions under which this error is bounded over time. 3 Stochastic simulation algorithms Likelihood weighting (LW) <ref> [ SP89; FC89 ] </ref> is one of the most effective of the stochastic simulation algorithms for belief networks.
References-found: 11

