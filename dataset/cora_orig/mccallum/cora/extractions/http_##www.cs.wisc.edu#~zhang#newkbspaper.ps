URL: http://www.cs.wisc.edu/~zhang/newkbspaper.ps
Refering-URL: http://www.cs.wisc.edu/~zhang/
Root-URL: 
Email: zhang,raghu,miron@cs.wisc.edu  Email: tian zhang@vnet.ibm.com  
Phone: Phone: 408-463-4106 Fax: 408-463-3834  
Title: BIRCH: A New Data Clustering Algorithm and Its Applications  
Author: TIAN ZHANG, RAGHU RAMAKRISHNAN, MIRON LIVNY Tian Zhang 
Address: Wisconsin, Madison, WI 53706,U.S.A.  Address: 555 Bailey Avenue, IBM-Santa Teresa Lab., J15/C265, San Jose, CA95141, U.S.A.  
Affiliation: Computer Sciences Department, University of  Postal  
Note: Small Journal Name, 1-40 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  Corresponding Author:  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Norbert. Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger, </author> <title> The R fl -tree: An Efficient and Robust Access Method for Points and Rectangles, </title> <booktitle> Proc. of ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <address> 322-331,1990. </address>
Reference-contexts: CLARANS suffers from the same drawbacks as the KMEDOIDS method with respect to efficiency. In addition, it may not find a real local minimum due to the random search trimming controlled by maxneighbor. The R-tree [13] (or variants such as R fl -tree <ref> [1] </ref>) is a popular dynamic multidimensional spatial index structure that has existed in the database community for more than a decade.
Reference: 2. <author> Peter Cheeseman, James Kelly, Matthew Self, et al., </author> <title> AutoClass : A Bayesian Classification System, </title> <booktitle> Proc. of the 5th Int. Conf. on Machine Learning, </booktitle> <publisher> Morgan Kaufman, </publisher> <month> Jun. </month> <year> 1988. </year>
Reference-contexts: Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning <ref> [2, 9, 10, 12, 21] </ref>, Statistics [4, 5, 22, 23], and Database [6, 7, 24] communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21]. <p> Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning <ref> [2, 9, 12, 21] </ref>. They concentrate on incremental approaches that accept instances one at a time, and do not extensively reprocess previously encountered instances while incorporating a new concept.
Reference: 3. <author> Michael Cheng, Miron Livny, and Raghu Ramakrishnan, </author> <title> Visual Analysis of Stream Data, </title> <booktitle> Proc. of IS&T/SPIE Conf. on Visual Data Exploration and Analysis, </booktitle> <address> San Jose, CA, </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: This is important to many fields including ecology, forestry, meteorology, and other agricultural sciences. The main use of BIRCH is to help classify pixels in the MVI images by performing clustering, and experimenting with different feature selection and weighting choices. To do that, we integrated BIRCH with the DEVISE <ref> [3] </ref> data visualization system, as shown in Figure 19, to form a user-friendly, interactive and iterative pixel classification tool.
Reference: 4. <author> Richard Duda, and Peter E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning [2, 9, 10, 12, 21], Statistics <ref> [4, 5, 22, 23] </ref>, and Database [6, 7, 24] communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21]. <p> There are two categories of clustering algorithms [16]: Partitioning Clustering and Hierarchical Clustering. Partitioning Clustering (PC) <ref> [4, 16] </ref> starts with an initial partition, then tries all possible moving or swapping of data points from one group to another iteratively to optimize the objective measurement function. <p> It guarantees convergence BIRCH: A NEW DATA CLUSTERING ALGORITHM AND ITS APPLICATIONS 5 to a local minimum, but the quality of the local minimum is very sensitive to the initial partition, and the worst case time complexity is exponential. Hierarchical Clustering (HC) <ref> [4, 23] </ref> does not try to find the `best' clusters, instead it keeps merging (agglomerative HC) the closest pair, or splitting (divisive HC) the farthest pair, of objects to form clusters. With a reasonable distance measurement, the best time complexity of a practical HC algorithm is O (N 2 ).
Reference: 5. <author> R. Dubes, and A.K. Jain, </author> <title> Clustering Methodologies in Exploratory Data Analysis, Advances in Computers, Edited by M.C. </title> <journal> Yovits, </journal> <volume> Vol. 19, </volume> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: This information may be used to guide the application of more rigorous data analysis procedures. It is a problem with many practical applications and has been studied for many years. Many clustering methods have been developed and applied to various domains, including data classification and image compression <ref> [22, 5] </ref>. However, it is also a very difficult subject because theoretically, it is a nonconvex, discrete optimization [16] problem. Due to an abundance of local minima, there is typically no way to find a globally minimal solution without trying all possible partitions. <p> Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning [2, 9, 10, 12, 21], Statistics <ref> [4, 5, 22, 23] </ref>, and Database [6, 7, 24] communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21].
Reference: 6. <author> Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu, </author> <title> A Database Interface for Clustering in Large Spatial Databases, </title> <booktitle> Proc. of 1st Int. Conf. on Knowledge Discovery and Data Mining, </booktitle> <year> 1995. </year>
Reference-contexts: Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning [2, 9, 10, 12, 21], Statistics [4, 5, 22, 23], and Database <ref> [6, 7, 24] </ref> communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21]. <p> The R-tree [13] (or variants such as R fl -tree [1]) is a popular dynamic multidimensional spatial index structure that has existed in the database community for more than a decade. Based on spatial locality in R fl -trees (a variation of R- trees), <ref> [6] </ref> and [7] propose focusing techniques to improve CLARANS's ability to deal with very large datasets that may reside on disks by (1) clustering a sample of the dataset that is drawn from each R fl -tree data page; and (2) focusing on relevant data points for distance and quality updates.
Reference: 7. <author> Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu, </author> <title> Knowledge Discovery in Large Spatial Databases: Focusing Techniques for Efficient Class Identification, </title> <booktitle> Proc. of 4th Int. Symposium on Large Spatial Databases, </booktitle> <address> Portland, Maine, U.S.A., </address> <year> 1995. </year>
Reference-contexts: Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning [2, 9, 10, 12, 21], Statistics [4, 5, 22, 23], and Database <ref> [6, 7, 24] </ref> communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21]. <p> The R-tree [13] (or variants such as R fl -tree [1]) is a popular dynamic multidimensional spatial index structure that has existed in the database community for more than a decade. Based on spatial locality in R fl -trees (a variation of R- trees), [6] and <ref> [7] </ref> propose focusing techniques to improve CLARANS's ability to deal with very large datasets that may reside on disks by (1) clustering a sample of the dataset that is drawn from each R fl -tree data page; and (2) focusing on relevant data points for distance and quality updates.
Reference: 8. <author> E. A. Feigenbaum, and H. Simon, </author> <title> EPAM-like models of recognition and learning, </title> <journal> Cognitive Science, </journal> <volume> vol. 8, </volume> <year> 1984, </year> <pages> 305-336. </pages>
Reference: 9. <author> Douglas H. Fisher, </author> <title> Knowledge Acquisition via Incremental Conceptual Clustering, </title> <journal> Machine Learning, </journal> <volume> 2(2), </volume> <year> 1987 </year>
Reference-contexts: Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning <ref> [2, 9, 10, 12, 21] </ref>, Statistics [4, 5, 22, 23], and Database [6, 7, 24] communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21]. <p> Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning <ref> [2, 9, 12, 21] </ref>. They concentrate on incremental approaches that accept instances one at a time, and do not extensively reprocess previously encountered instances while incorporating a new concept. <p> Concept (or cluster) formation is accomplished by top-down `sorting', with each new instance directed through a hierarchy whose nodes are formed gradually, and represent concepts. They are usually probability-based approaches, i.e., (1) they use probabilistic measurements (e.g., category utility as discussed in <ref> [9, 12] </ref>) for making decisions; and (2) they represent concepts (or clusters) with probabilistic descriptions. For example, COBWEB [9] proceeds as follows. <p> They are usually probability-based approaches, i.e., (1) they use probabilistic measurements (e.g., category utility as discussed in [9, 12]) for making decisions; and (2) they represent concepts (or clusters) with probabilistic descriptions. For example, COBWEB <ref> [9] </ref> proceeds as follows.
Reference: 10. <author> Douglas H. Fisher, </author> <title> Iterative Optimization and Simplification of Hierarchical Clusterings, </title> <type> Technical Report CS-95-01, </type> <institution> Dept. of Computer Science, Vanderbilt University, </institution> <address> Nashville, TN 37235. </address>
Reference-contexts: Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning <ref> [2, 9, 10, 12, 21] </ref>, Statistics [4, 5, 22, 23], and Database [6, 7, 24] communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21].
Reference: 11. <author> A. Gersho, and R. Gray, </author> <title> Vector quantization and signal compression, </title> <address> Boston, Ma.: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Phase 4 can be extended with additional passes if desired by the user, and it has been proved to converge to a minimum <ref> [11] </ref>. As a bonus, during this pass, each data point can be labeled with the cluster that it belongs to, if we wish to identify the data points in each cluster. Phase 4 also provides us with the option of discarding outliers. <p> Codebook Generalization in Image Compression Digital image compression [19] is the technology of reducing image data to save storage space and transmission bandwidth. Vector quantization <ref> [11] </ref> is a widely used image compression/decompression technique which operates on blocks of pixels instead of pixels for better efficiency. In vector quantization, the original image is first decomposed into small rectangular blocks, and each block is represented as a vector. <p> In a little more detail: 1. LBG uses the GLA (or KMEANS with empty cell filling strategy) algorithm <ref> [11] </ref> to find the `optimal' codebook of current size. 2. If it reaches the desired codebook size, then it stops; otherwise it doubles the current codebook size, perturbs and splits the current `optimal' codebook and goes to the previous step.
Reference: 12. <author> John H. Gennari, Pat Langley, and Douglas Fisher, </author> <title> Models of Incremental Concept Formation, </title> <journal> Artificial Intelligence, </journal> <volume> vol. 40, </volume> <year> 1989, </year> <pages> 11-61. </pages>
Reference-contexts: Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning <ref> [2, 9, 10, 12, 21] </ref>, Statistics [4, 5, 22, 23], and Database [6, 7, 24] communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21]. <p> Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning <ref> [2, 9, 12, 21] </ref>. They concentrate on incremental approaches that accept instances one at a time, and do not extensively reprocess previously encountered instances while incorporating a new concept. <p> Concept (or cluster) formation is accomplished by top-down `sorting', with each new instance directed through a hierarchy whose nodes are formed gradually, and represent concepts. They are usually probability-based approaches, i.e., (1) they use probabilistic measurements (e.g., category utility as discussed in <ref> [9, 12] </ref>) for making decisions; and (2) they represent concepts (or clusters) with probabilistic descriptions. For example, COBWEB [9] proceeds as follows. <p> It has also been shown that this kind of large hierarchy tends to `overfit' the data. A related problem is that this hierarchy is not kept width- balanced or height-balanced. So in the case of skewed input data, this may cause performance to degrade. Another system called CLASSIT <ref> [12] </ref> is very similar to COBWEB, with the following main differences. (1) It only deals with continuous (or real-valued) attributes (in contrast to discrete attributes in COBWEB). (2) It stores a continuous normal distribution (i.e., mean and standard deviation) for each individual attribute in a node, in contrast to a discrete
Reference: 13. <author> A. Guttman, R-trees: </author> <title> a dynamic index structure for spatial searching, </title> <booktitle> Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> 47-57, </pages> <year> 1984. </year>
Reference-contexts: CLARANS suffers from the same drawbacks as the KMEDOIDS method with respect to efficiency. In addition, it may not find a real local minimum due to the random search trimming controlled by maxneighbor. The R-tree <ref> [13] </ref> (or variants such as R fl -tree [1]) is a popular dynamic multidimensional spatial index structure that has existed in the database community for more than a decade.
Reference: 14. <author> C. Huang, Q. Bi, G. Stiles, R. Harris, </author> <title> Fast Full Search Equivalent Encoding Algorithms for Image Compression Using Vector Quantization, </title> <journal> IEEE Trans. on Image Processing, </journal> <volume> vol. 1, no. 3, </volume> <month> July, </month> <year> 1992. </year>
Reference-contexts: Phase 4 scans the dataset again and puts each data point into the proper cluster; the time taken is proportional to N flK. However using the newest nearest neighbor techniques proposed in <ref> [14] </ref>, for each of the N data point, instead of looking all K cluster centers to find the nearest one, it only looks those cluster centers that are around the data point. This way, Phase 4 can be improved quite a bit. 6.2. <p> Although we have tried to improve the Phase 4 refining algorithm using the `nearest neighbor' techniques proposed in <ref> [14] </ref>, and this improvement performs very well and brings the time complexity O (N fl K) down to be almost linear with respect to N , the linear slope is sensitive to the distribution patterns of data.
Reference: 15. <author> J. A. Hartigan, and M. A. Wong, </author> <title> A K-Means Clustering Algorithm, </title> <journal> Appl. Statist., </journal> <volume> vol. 28, no. 1, </volume> <year> 1979. </year>
Reference-contexts: 33.8 1.97 197 3 32.9 3.66 187 3o 36.0 4.35 241 does. (Clearly, this assumption greatly favors these two algorithms in terms of the running time comparison!) Second, the CLARANS implementation was provided by Raymond Ng, and the KMEANS implementation was done by us based on the algorithm presented in <ref> [15] </ref>, with the initial seeds selected randomly. We have observed that the performances of CLARANS and KMEANS are very sensitive to the random number generator used.
Reference: 16. <author> Leonard Kaufman, and Peter J. Rousseeuw, </author> <title> Finding Groups in Data An Introduction to Cluster Analysis, </title> <journal> Wiley Series in Probability and Mathematical Statistics, </journal> <year> 1990. </year>
Reference-contexts: Many clustering methods have been developed and applied to various domains, including data classification and image compression [22, 5]. However, it is also a very difficult subject because theoretically, it is a nonconvex, discrete optimization <ref> [16] </ref> problem. Due to an abundance of local minima, there is typically no way to find a globally minimal solution without trying all possible partitions. Usually, this is infeasible except when N and K are extremely small. <p> That is, (1) they assume that there is a distance measurement between any two instances (or data points), and that this measurement can be used for making similarity decisions; and (2) they represent clusters by some kind of `center' measure. There are two categories of clustering algorithms <ref> [16] </ref>: Partitioning Clustering and Hierarchical Clustering. Partitioning Clustering (PC) [4, 16] starts with an initial partition, then tries all possible moving or swapping of data points from one group to another iteratively to optimize the objective measurement function. <p> There are two categories of clustering algorithms [16]: Partitioning Clustering and Hierarchical Clustering. Partitioning Clustering (PC) <ref> [4, 16] </ref> starts with an initial partition, then tries all possible moving or swapping of data points from one group to another iteratively to optimize the objective measurement function.
Reference: 17. <author> C.J. Kucharik, and J.M. Norman, </author> <title> Measuring Canopy Architecture with a Multiband Vegetation Imager (MVI) Proc. </title> <booktitle> of the 22nd conf. on Agricultural and Forest Meteorology, American Meteorological Society annual meeting, </booktitle> <address> Atlanta, GA, Jan 28-Feb 2, </address> <year> 1996. </year>
Reference-contexts: Interactive and Iterative Pixel Classification The first application is motivated by the MVI (Multiband Vegetation Imager) technique developed in <ref> [17, 18] </ref>. The MVI is the combination of a charge-coupled device 32 TIAN ZHANG, RAGHU RAMAKRISHNAN, MIRON LIVNY (CCD) camera, a filter exchange mechanism, and laptop computer used to capture rapid, successive images of plant canopies in two wavelength bands.
Reference: 18. <author> C.J. Kucharik, J.M. Norman, L.M. Murdock, and S.T. Gower, </author> <title> Characterizing Canopy non-randomness with a Multiband Vegetation Imager (MVI), </title> <note> Submitted to Journal of Geophysical Research, to appear in the Boreal Ecosystem-Atmosphere Study (BOREAS) special issue. </note>
Reference-contexts: Interactive and Iterative Pixel Classification The first application is motivated by the MVI (Multiband Vegetation Imager) technique developed in <ref> [17, 18] </ref>. The MVI is the combination of a charge-coupled device 32 TIAN ZHANG, RAGHU RAMAKRISHNAN, MIRON LIVNY (CCD) camera, a filter exchange mechanism, and laptop computer used to capture rapid, successive images of plant canopies in two wavelength bands.
Reference: 19. <author> Weidong Kou, </author> <title> Digital Image Compression Algorithms and Standards, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: BIRCH's running time is not sensitive to the different MVI image pairs whereas for CLARANS and KMEANS, their running time and data scan numbers are very sensitive to the datasets themselves. 7.2. Codebook Generalization in Image Compression Digital image compression <ref> [19] </ref> is the technology of reducing image data to save storage space and transmission bandwidth. Vector quantization [11] is a widely used image compression/decompression technique which operates on blocks of pixels instead of pixels for better efficiency. <p> We have used two different images: Lena and Baboon (each with 512x512 pixels) as examples to compare BIRCH, CLARANS and LBG's performances in terms of the running time, number of scans of the dataset, distortion and entropy <ref> [19, 26] </ref> 38 TIAN ZHANG, RAGHU RAMAKRISHNAN, MIRON LIVNY on high dimensional (d=16) real datasets. First the training vectors are derived by blocking the image into 4x4 blocks (d=16), and the desired codebook size is set to 256.
Reference: 20. <author> Y. Linde, A. Buzo, and R. M. Gray, </author> <title> An Algorithm for Vector Quantization Design, </title> <journal> IEEE Trans. on Communications, </journal> <volume> vol. 28, no. 1, </volume> <year> 1980. </year>
Reference-contexts: Given the training vectors (from the training image) and the desired codebook size (i.e., number of codewords), the main problem of vector quantization is how to generate the codebook. A commonly used codebook generating algorithm is the LBG algorithm <ref> [20] </ref>.
Reference: 21. <author> Michael Lebowitz, </author> <title> Experiments with Incremental Concept Formation : UNIMEM, </title> <booktitle> Machine Learning, </booktitle> <year> 1987. </year>
Reference-contexts: Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning <ref> [2, 9, 10, 12, 21] </ref>, Statistics [4, 5, 22, 23], and Database [6, 7, 24] communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21]. <p> Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning <ref> [2, 9, 12, 21] </ref>. They concentrate on incremental approaches that accept instances one at a time, and do not extensively reprocess previously encountered instances while incorporating a new concept.
Reference: 22. <author> R.C.T.Lee, </author> <title> Clustering analysis and its applications, </title> <booktitle> Advances in Information Systems Science, Edited by J.T.Toum, </booktitle> <volume> Vol. 8, </volume> <pages> pp. 169-292, </pages> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: This information may be used to guide the application of more rigorous data analysis procedures. It is a problem with many practical applications and has been studied for many years. Many clustering methods have been developed and applied to various domains, including data classification and image compression <ref> [22, 5] </ref>. However, it is also a very difficult subject because theoretically, it is a nonconvex, discrete optimization [16] problem. Due to an abundance of local minima, there is typically no way to find a globally minimal solution without trying all possible partitions. <p> Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning [2, 9, 10, 12, 21], Statistics <ref> [4, 5, 22, 23] </ref>, and Database [6, 7, 24] communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21].
Reference: 23. <author> F. Murtagh, </author> <title> A Survey of Recent Advances in Hierarchical Clustering Algorithms, </title> <journal> The Computer Journal, </journal> <year> 1983. </year> <type> 40 TIAN ZHANG, RAGHU RAMAKRISHNAN, MIRON LIVNY </type>
Reference-contexts: Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning [2, 9, 10, 12, 21], Statistics <ref> [4, 5, 22, 23] </ref>, and Database [6, 7, 24] communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21]. <p> It guarantees convergence BIRCH: A NEW DATA CLUSTERING ALGORITHM AND ITS APPLICATIONS 5 to a local minimum, but the quality of the local minimum is very sensitive to the initial partition, and the worst case time complexity is exponential. Hierarchical Clustering (HC) <ref> [4, 23] </ref> does not try to find the `best' clusters, instead it keeps merging (agglomerative HC) the closest pair, or splitting (divisive HC) the farthest pair, of objects to form clusters. With a reasonable distance measurement, the best time complexity of a practical HC algorithm is O (N 2 ).
Reference: 24. <author> Raymond T. Ng and Jiawei Han, </author> <title> Efficient and Effective Clustering Methods for Spatial Data Mining, </title> <booktitle> Proc. of VLDB, </booktitle> <year> 1994. </year>
Reference-contexts: Finally our conclusions and directions for future research are presented in Section 8. 2. Previous Work and BIRCH Data clustering has been studied in the Machine Learning [2, 9, 10, 12, 21], Statistics [4, 5, 22, 23], and Database <ref> [6, 7, 24] </ref> communities with different methods and different emphases. 2.1. Probability-Based Clustering Previous data clustering work in Machine Learning is usually referred to as unsupervised conceptual learning [2, 9, 12, 21]. <p> Hence none of them can scale up linearly with stable quality. Data clustering has been recognized as a useful spatial data mining method recently. <ref> [24] </ref> presents CLARANS, which is a KMEDOIDS algorithm but with randomized partial search strategy, and suggests that CLARANS out-performs the traditional KMEDOIDS algorithms. <p> Its numlocal value is still 2, as in <ref> [24] </ref>. ing them with the intended clusters for DS1, we observe that: (1) The pattern of the location of the cluster centers is distorted. (2) The number of data points in a CLARANS or KMEANS cluster can be as many as 40% different from the number in the intended cluster. (3)
Reference: 25. <author> Clark F. Olson, </author> <title> Parallel Algorithms for Hierarchical Clustering, </title> <type> Technical Report, </type> <institution> Computer Science Division, Univ. of California at Berkeley, Dec.,1993. </institution>
Reference-contexts: We observe that existing clustering algorithms (e.g., HC, KMEANS and CLARANS) that work with a set of data points can be readily adapted to work with a set of subclusters, each described by its CF entry. We adapted an agglomerative hierarchical clustering algorithm based on the description in <ref> [25] </ref>. It is applied to the subclusters represented by their CF entries. It has a complexity of O (m 2 ), where m is the number of subclusters. If the distance metric satisfies the reducibility property [25] 1 , it produces exact results; otherwise it still provides a very good approximate <p> We adapted an agglomerative hierarchical clustering algorithm based on the description in <ref> [25] </ref>. It is applied to the subclusters represented by their CF entries. It has a complexity of O (m 2 ), where m is the number of subclusters. If the distance metric satisfies the reducibility property [25] 1 , it produces exact results; otherwise it still provides a very good approximate algorithm. In our case, D2 and D4 satisfy the reducibility property and they are the ideal metrics for using this algorithm.
Reference: 26. <author> Majid Rabbani, and Paul W. Jones, </author> <title> Digital Image Compression Techniques, </title> <publisher> SPIE Optical Engineering Press, </publisher> <year> 1991. </year>
Reference-contexts: We have used two different images: Lena and Baboon (each with 512x512 pixels) as examples to compare BIRCH, CLARANS and LBG's performances in terms of the running time, number of scans of the dataset, distortion and entropy <ref> [19, 26] </ref> 38 TIAN ZHANG, RAGHU RAMAKRISHNAN, MIRON LIVNY on high dimensional (d=16) real datasets. First the training vectors are derived by blocking the image into 4x4 blocks (d=16), and the desired codebook size is set to 256.
Reference: 27. <author> Tian Zhang, Raghu Ramakrishnan, and Miron Livny, </author> <title> BIRCH: An Efficient Data Clustering Method for Very Large Databases, </title> <type> Technical Report, </type> <institution> Computer Sciences Dept., Univ. of Wisconsin-Madison, </institution> <year> 1995. </year>
Reference: 28. <author> Tian Zhang, </author> <title> Data Clustering for Very Large Datasets Plus Applications, </title> <type> Dissertation, </type> <institution> Computer Sciences Dept. at Univ. of Wisconsin-Madison, </institution> <year> 1996. </year> <title> Received Date Accepted Date Final Manuscript Date 41 BIRCH Codebook CLARANS Codebook Codebook BIRCH Codebook CLARANS Codebook LBG Codebook </title>
Reference-contexts: two disjoint subclusters is: CF 1 + CF 2 = (N 1 + N 2 ; ~ LS 1 + ~ LS 2 ; SS 1 + SS 2 ) (11) BIRCH: A NEW DATA CLUSTERING ALGORITHM AND ITS APPLICATIONS 9 The theorem's proof consists of conventional vector space algebra <ref> [28] </ref>. Accord- ing to the CF definition and the CF representativity theorem, one can think of a subcluster as a set of data points, and the CF entry stored as a summary.
References-found: 28

