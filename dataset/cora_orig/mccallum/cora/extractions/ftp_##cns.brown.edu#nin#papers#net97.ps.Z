URL: ftp://cns.brown.edu/nin/papers/net97.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Email: nin@cns.brown.edu  edelman@ai.mit.edu  
Title: Learning low dimensional representations via the usage of multiple class labels Learning low dimensional representations
Author: Nathan Intratory and Shimon Edelmanz labels 
Date: April 11, 1997  
Note: Short title:  On leave:  
Address: Providence, RI 02192  E25-201 Cambridge, MA 02142  
Affiliation: Institute for Brain and Neural Systems Brown University  Center for Biological and Computational Learning MIT  School of Mathematical Sciences, Tel-Aviv University.  
Abstract-found: 0
Intro-found: 1
Reference: <author> Atick, J. J., Griffin, P. A., and Redlich, A. N. </author> <year> (1996). </year> <title> The vocabulary of shape: principal shapes for probing perception and neural response. </title> <journal> Network, </journal> <volume> 7 </volume> <pages> 1-5. </pages> <note> 32 REFERENCES Baldi, </note> <author> P. and Hornik, K. </author> <year> (1989). </year> <title> Neural networks and principal component analysis: Learning from examples without local minima. </title> <booktitle> Neural Networks, </booktitle> 2:53-58. 
Reference-contexts: As with the Fractals data set, we quantified the degree of nonlinearity by subjecting the image data to PCA and to MDS analysis. For y A similar approach to the generation of parametrically controlled head stimuli has been recently proposed in <ref> (Atick et al., 1996) </ref>. 11 shown in Figure 5.
Reference: <author> Baxt, W. G. and White, H. </author> <year> (1995). </year> <title> Bootstrapping confidence intervals for clinical input variable effects in network trained to identify the presence of acute myocardial infraction. </title> <journal> Neural Computation, </journal> <volume> 7(3) </volume> <pages> 624-638. </pages>
Reference-contexts: More generally, innovative use of the training data is needed. For example, methods for data reuse such as cross-validation (Stone, 1974) and bootstrap (Efron and Tibshirani, 1993) can help in obtaining confidence intervals <ref> (Baxt and White, 1995) </ref> and improved performance (Breiman, 1992; Breiman, 1994; LeBlanc and Tibshirani, 1994) of learning networks. Smooth bootstrap (Efron and Tibshirani, 1993) can also increase the independence among predictors for the purpose of ensemble averaging (Raviv and Intrator, 1996).
Reference: <author> Bellman, R. E. </author> <year> (1961). </year> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference-contexts: Thus, it is clear that neural network learning methods require special care when applied to problems arising in vision. 3 1.1. General approaches to the facilitation of learning A fundamental assumption frequently made in an attempt to alleviate the problem of sparse data, also known as curse of dimensionality <ref> (Bellman, 1961) </ref>, is the existence of a low dimensional representation (LDR) of the problem space. Yet, postulating that an LDR exists does not provide an efficient way to find it. To do so, one may for instance, further assume that the data are clustered.
Reference: <author> Borg, I. and Lingoes, J. </author> <year> (1987). </year> <title> Multidimensional Similarity Structure Analysis. </title> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference-contexts: Note that in this case the results of repeated runs cannot be simply averaged (unless they are first converted into the distance-table format, which would necessitate subsequent application of MDS). To combine the results of a number of runs we used Procrustes transformations <ref> (Borg and Lingoes, 1987) </ref> to normalize (scale, rotate, and translate) all the LDR configurations to the configuration obtained in the first run.y As a direct control of the outcome of MDS-based visualization, we also ran the 3-HL bottleneck MLP on the LDRs obtained by other methods, as a kind of post-processing.
Reference: <author> Breiman, L. </author> <year> (1992). </year> <title> Stacked regression. </title> <type> Technical Report TR-367, </type> <institution> Department of Statistics, University of California, Berkeley. </institution>
Reference: <author> Breiman, L. </author> <year> (1994). </year> <title> Bagging predictors. </title> <type> Technical Report TR-421, </type> <institution> Department of Statistics, University of California, Berkeley. </institution>
Reference: <author> Buckheit, J. and Donoho, D. L. </author> <year> (1995). </year> <title> Improved linear discrimination using time-frequency dictionaries. </title> <type> Technical Report, </type> <institution> Stanford University. </institution>
Reference-contexts: Even simple and widely used methods such as linear discriminant analysis (Fisher, 1936) should be adjusted to reflect the low ratio of training samples to the number of parameters <ref> (Buckheit and Donoho, 1995) </ref>. Thus, it is clear that neural network learning methods require special care when applied to problems arising in vision. 3 1.1.
Reference: <author> Busey, T. A., Brady, N. P., and Cutting, J. E. </author> <year> (1990). </year> <title> Compensation is unnecessary for the perception of faces in slanted pictures. </title> <journal> Perception and Psychophysics, </journal> <volume> 48 </volume> <pages> 1-11. </pages>
Reference-contexts: It should be noted that orientation differences of up to 20 ffi go unnoticed by human viewers <ref> (Busey et al., 1990) </ref>, while presenting nontrivial problems for neural networks, which must be trained explicitly to compensate for or to tolerate the misorientation.
Reference: <author> Caruana, R. </author> <year> (1993). </year> <title> Multitask connectionist learning. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 372-379, </pages> <address> San Mateo, CA. </address>
Reference-contexts: It has been observed in the past that training a classifier on multiple tasks (using the same data) may be an efficient way to introduce desirable bias into the solution <ref> (Caruana, 1993) </ref> and to improve generalization (Caruana, 1995).
Reference: <author> Caruana, R. </author> <year> (1995). </year> <title> Learning many related tasks at the same time with backpropagation. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 657-664. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: It has been observed in the past that training a classifier on multiple tasks (using the same data) may be an efficient way to introduce desirable bias into the solution (Caruana, 1993) and to improve generalization <ref> (Caruana, 1995) </ref>.
Reference: <author> Caruana, R. and de Sa, V. R. </author> <year> (1997). </year> <title> Promoting poor features to supervisors: Some inputs work better as outputs. </title> <editor> In Mozer, M. C., Jordan, M. I., and Petsche, T., </editor> <title> REFERENCES 33 editors, </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 9. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: This approach can be compared with a recent suggestion to convert poor features into supervisors <ref> (Caruana and de Sa, 1997) </ref>; in contrast to that idea, we show that there is a limit to the utility of class labels, and that invariance constraints we impose are equally important.
Reference: <author> Cortese, J. M. and Dyre, B. P. </author> <year> (1996). </year> <title> Perceptual similarity of shapes generated from Fourier Descriptors. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 22 </volume> <pages> 133-143. </pages>
Reference: <author> Cottrell, G. W., Munro, P., and Zipser, D. </author> <year> (1987). </year> <title> Learning internal representations from gray-scale images: An example of extensional programming. </title> <booktitle> In Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 462-473, </pages> <address> Hillsdale. </address> <publisher> Erlbaum. </publisher>
Reference: <author> Cutzu, F. and Edelman, S. </author> <year> (1996). </year> <title> Faithful representation of similarities among three-dimensional shapes in human vision. </title> <booktitle> Proceedings of the National Academy of Science, </booktitle> <volume> 93 </volume> <pages> 12046-12050. </pages>
Reference: <author> Demartines, P. and Herault, J. </author> <year> (1996). </year> <title> Curvilinear component analysis: a self-organizing neural network for non linear mapping of data sets. Submitted to IEEE Transaction on Neural Networks. </title>
Reference: <author> DeMers, D. and Cottrell, G. </author> <year> (1993). </year> <title> Nonlinear dimensionality reduction. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 580-587. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: moreover, was nonlinearly related to the original parametric representation of the data, made the extraction of LDR a highly nontrivial task. 3.1.1. 1-HL MLP classifier Researchers have explored in the past the ability of multilayer perceptrons, trained as autoencoders, to form low-dimensional representations of the data in the hidden layer <ref> (DeMers and Cottrell, 1993) </ref>. In the autoencoder approach, the data are forced to pass through a low-dimensional "bottleneck" | the hidden layer of an MLP. The network is taught to reconstruct the input patterns from the LDR formed at the hidden layer.
Reference: <author> Edelman, S. </author> <year> (1995a). </year> <title> Representation of similarity in 3D object discrimination. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 407-422. </pages>
Reference: <author> Edelman, S. </author> <year> (1995b). </year> <title> Representation, Similarity, and the Chorus of Prototypes. </title> <journal> Minds and Machines, </journal> <volume> 5 </volume> <pages> 45-68. </pages>
Reference: <author> Edelman, S. </author> <year> (1997). </year> <title> Representation is representation of similarity. </title> <journal> Behavioral and Brain Sciences, </journal> <note> to appear. </note>
Reference-contexts: The conditions on the LDR extraction process that makes such recovery possible, and the wider philosophical implications of this phenomenon, are discussed in <ref> (Edelman, 1997) </ref>. y In the context of Figure 16, this operation corresponds to the definition of a novel face (say, face B, which was not included in the training set) in terms of its similarities to familiar faces (e.g., defining B as the stimulus that is halfway between A and C,
Reference: <author> Edelman, S., Cutzu, F., and Duvdevani-Bar, S. </author> <year> (1996). </year> <title> Similarity to reference shapes as a basis for shape representation. </title> <editor> In Cottrell, G. W., editor, </editor> <booktitle> Proceedings of 18th 34 REFERENCES Annual Conf. of the Cognitive Science Society, </booktitle> <pages> pages 260-265, </pages> <address> San Diego, CA. </address>
Reference-contexts: This approach naturally facilitates generalization across tasks, also known as transfer of skill | a hallmark of human cognitive prowess (see section 1.1 in <ref> (Intrator and Edelman, 1996) </ref> for review). It has been observed in the past that training a classifier on multiple tasks (using the same data) may be an efficient way to introduce desirable bias into the solution (Caruana, 1993) and to improve generalization (Caruana, 1995). <p> Information regarding other experiments with the Fractals data (namely, a study of the effect of the number of bases on the quality of the LDR extracted by an RBF network, and a comparison of LDRs derived from several versions of Fractals data of varying difficulty) can be found in <ref> (Intrator and Edelman, 1996) </ref>. MDS visualization (section 3.2.2). Left: MLP trained on 18 classes; here and in the subsequent plots, the three symbol kinds (ffi, +, and fl) correspond to points belonging to the different rows of Figure 1. <p> A parametric study of the performance of this method on the number of centers of the RBF network can be found in <ref> (Intrator and Edelman, 1996) </ref>. level label indicated the row to which it belonged (see Figure 1). In the resulting configuration, the 18 clusters were separated, on a coarser level, into three groups, corresponding to the three higher-level class labels (see Figure 11, left).
Reference: <author> Edelman, S. and Duvdevani-Bar, S. </author> <year> (1997a). </year> <title> A model of visual recognition and categorization. </title> <journal> Phil. Trans. R. Soc. Lond. (B), </journal> <note> 352:-. to appear. </note>
Reference-contexts: the following two operations can be carried out safely: (1) a new instance of a class can be categorized by finding, in the representation space, the cluster to which the current stimulus is the closest; (2) a new class can be defined by its representation-space distances to the familiar classes <ref> (Edelman and Duvdevani-Bar, 1997a) </ref>.y We remark that topology preservation appears to be true of representation formed by human subjects in a variety of perceptual tasks.
Reference: <author> Edelman, S. and Duvdevani-Bar, S. </author> <year> (1997b). </year> <title> Similarity, connectionism, and the problem of representation in vision. </title> <journal> Neural Computation, </journal> <volume> 9 </volume> <pages> 701-720. </pages>
Reference-contexts: inputs, and if its response drops off monotonically with increasing distance between the input and the optimal stimulus (the prototype for that module), a collection of modules tuned to different classes forms a distributed representation of the input that is likely to capture the low-dimensional structure of the input space <ref> (Edelman and Duvdevani-Bar, 1997b) </ref>. For the present purpose, we trained a single RBF network to output a unary representation of the class membership, as we did with the MLP classifiers described above (this is equivalent, of course, to training 18 separate RBFs, sharing the same "hidden" or basis function layer). <p> Topology preservation is useful because it allows the representational system to 30 categorize novel instances of familiar object classes, as well as make sense of novel classes <ref> (Edelman and Duvdevani-Bar, 1997b) </ref>.
Reference: <author> Efron, B. and Tibshirani, R. </author> <year> (1993). </year> <title> An introduction to the bootstrap. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: More generally, innovative use of the training data is needed. For example, methods for data reuse such as cross-validation (Stone, 1974) and bootstrap <ref> (Efron and Tibshirani, 1993) </ref> can help in obtaining confidence intervals (Baxt and White, 1995) and improved performance (Breiman, 1992; Breiman, 1994; LeBlanc and Tibshirani, 1994) of learning networks. Smooth bootstrap (Efron and Tibshirani, 1993) can also increase the independence among predictors for the purpose of ensemble averaging (Raviv and Intrator, 1996). <p> For example, methods for data reuse such as cross-validation (Stone, 1974) and bootstrap <ref> (Efron and Tibshirani, 1993) </ref> can help in obtaining confidence intervals (Baxt and White, 1995) and improved performance (Breiman, 1992; Breiman, 1994; LeBlanc and Tibshirani, 1994) of learning networks. Smooth bootstrap (Efron and Tibshirani, 1993) can also increase the independence among predictors for the purpose of ensemble averaging (Raviv and Intrator, 1996). Such methods lead to a reduction in the variance portion of the error, with little or no effect on the bias of the predictor.
Reference: <author> Elman, J. L. and Zipser, D. </author> <year> (1988). </year> <title> Learning the hidden structure of speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 4(83) </volume> <pages> 1615-1626. </pages>
Reference-contexts: First, we asked whether a self-supervised 3-layer MLP autoencoder, which aims at the best reconstruction of the inputs, can reveal the correct low-dimensional structure in our data. Although in the linear case such networks do quite well, essentially by extracting the principal components of the data <ref> (Elman and Zipser, 1988) </ref>, the performance on the Faces data was poor. Specifically, the autoencoder network consistently converged to the mean of the data, presumably due to the nonlinearity introduced by the imaging step. Second, we experimented with a 5-layer nonlinear bottleneck autoencoder (Leen and Kambhatla, 1994).
Reference: <author> Fisher, R. A. </author> <year> (1936). </year> <title> The use of multiple measurements in taxonomic problems. </title> <journal> Annals of Eugenics, </journal> <volume> 7 </volume> <pages> 179-188. </pages>
Reference-contexts: Even simple and widely used methods such as linear discriminant analysis <ref> (Fisher, 1936) </ref> should be adjusted to reflect the low ratio of training samples to the number of parameters (Buckheit and Donoho, 1995). Thus, it is clear that neural network learning methods require special care when applied to problems arising in vision. 3 1.1.
Reference: <author> Grossman, T. and Lapedes, A. </author> <year> (1993). </year> <title> Use of bad training data for better prediction. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 342-350. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A general framework for imposing such constraints is presented in (Intrator, 1993). 4 1.2. Specific assumptions and prior knowledge Unlike data, class labels are not often reused to facilitate learning (see, however, <ref> (Grossman and Lapedes, 1993) </ref>). In particular, few learning algorithms can accommodate multiple-class labels, which are likely to contain useful information regarding the structure of the data. Furthermore, humans make natural use of the knowledge that objects may have several class associations (say, at different category levels).
Reference: <author> Intrator, N. </author> <year> (1993). </year> <title> Combining exploratory projection pursuit and projection pursuit regression with application to neural networks. </title> <journal> Neural Computation, </journal> <volume> 5(3) </volume> <pages> 443-455. </pages>
Reference-contexts: To do so, one may for instance, further assume that the data are clustered. If indeed data points belonging to the same class are clustered in the high dimensional space, a useful LDR can be found by looking for projections that emphasize the cluster structure <ref> (Intrator, 1993) </ref>. More generally, innovative use of the training data is needed. <p> A general framework for imposing such constraints is presented in <ref> (Intrator, 1993) </ref>. 4 1.2. Specific assumptions and prior knowledge Unlike data, class labels are not often reused to facilitate learning (see, however, (Grossman and Lapedes, 1993)). In particular, few learning algorithms can accommodate multiple-class labels, which are likely to contain useful information regarding the structure of the data.
Reference: <author> Intrator, N. and Edelman, S. </author> <year> (1996). </year> <title> How to make a low-dimensional representation suitable for diverse tasks. </title> <journal> Connection Science, </journal> <volume> 8 </volume> <pages> 205-224. </pages>
Reference-contexts: Smooth bootstrap (Efron and Tibshirani, 1993) can also increase the independence among predictors for the purpose of ensemble averaging <ref> (Raviv and Intrator, 1996) </ref>. Such methods lead to a reduction in the variance portion of the error, with little or no effect on the bias of the predictor. <p> This approach naturally facilitates generalization across tasks, also known as transfer of skill | a hallmark of human cognitive prowess (see section 1.1 in <ref> (Intrator and Edelman, 1996) </ref> for review). It has been observed in the past that training a classifier on multiple tasks (using the same data) may be an efficient way to introduce desirable bias into the solution (Caruana, 1993) and to improve generalization (Caruana, 1995). <p> Information regarding other experiments with the Fractals data (namely, a study of the effect of the number of bases on the quality of the LDR extracted by an RBF network, and a comparison of LDRs derived from several versions of Fractals data of varying difficulty) can be found in <ref> (Intrator and Edelman, 1996) </ref>. MDS visualization (section 3.2.2). Left: MLP trained on 18 classes; here and in the subsequent plots, the three symbol kinds (ffi, +, and fl) correspond to points belonging to the different rows of Figure 1. <p> A parametric study of the performance of this method on the number of centers of the RBF network can be found in <ref> (Intrator and Edelman, 1996) </ref>. level label indicated the row to which it belonged (see Figure 1). In the resulting configuration, the 18 clusters were separated, on a coarser level, into three groups, corresponding to the three higher-level class labels (see Figure 11, left).
Reference: <author> Kambhatla, N. and Leen, T. K. </author> <year> (1994). </year> <title> Fast non-linear dimension reduction. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: A natural modification of the 1-HL MLP architecture, which may be better suited to nonlinear LDR extraction, is the three hidden layer (3-HL) MLP <ref> (Leen and Kambhatla, 1994) </ref>. We have, therefore, chosen to explore this approach, but, following the considerations stated above, the 3-HL MLP was to be trained as a classifier, and not as an autoencoder. <p> Specifically, the autoencoder network consistently converged to the mean of the data, presumably due to the nonlinearity introduced by the imaging step. Second, we experimented with a 5-layer nonlinear bottleneck autoencoder <ref> (Leen and Kambhatla, 1994) </ref>. This training scheme, likewise, performed poorly on our data set. The outcome of this experiment showed that self-supervised dimensionality reduction cannot recover a good LDR in the present case, illustrating the importance of guidance provided by the class labels.
Reference: <author> Koontz, W. L. G. and Fukunaga, K. </author> <year> (1972). </year> <title> A nonlinear feature extraction algorithm using distance information. </title> <journal> IEEE Trans. Comput., </journal> <volume> 21 </volume> <pages> 56-63. </pages>
Reference: <author> Kruskal, J. B. </author> <year> (1964). </year> <title> Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. </title> <journal> Psychometrika, </journal> <volume> 29(1) </volume> <pages> 1-27. </pages> <note> REFERENCES 35 Lando, </note> <author> M. and Edelman, S. </author> <year> (1995). </year> <title> Receptive field spaces and class-based generalization from a single view in face recognition. </title> <journal> Network, </journal> <volume> 6 </volume> <pages> 551-576. </pages>
Reference-contexts: This discovery led to the development of the nonmetric MDS algorithm <ref> (Kruskal, 1964) </ref>, which employs gradient descent to seek a monotonic transformation between measured distances and distances computed from the hypothesized point configuration, which would minimize stress (defined as the discrepancy between the ranks of the measured and the computed distances).
Reference: <author> LeBlanc, M. and Tibshirani, R. </author> <year> (1994). </year> <title> Combining estimates in regression and classification. </title> <type> Preprint. </type>
Reference: <author> Leen, T. K. and Kambhatla, N. </author> <year> (1994). </year> <title> Fast non-linear dimension reduction. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 152-159. </pages> <publisher> Morgan Kauffman, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: A natural modification of the 1-HL MLP architecture, which may be better suited to nonlinear LDR extraction, is the three hidden layer (3-HL) MLP <ref> (Leen and Kambhatla, 1994) </ref>. We have, therefore, chosen to explore this approach, but, following the considerations stated above, the 3-HL MLP was to be trained as a classifier, and not as an autoencoder. <p> Specifically, the autoencoder network consistently converged to the mean of the data, presumably due to the nonlinearity introduced by the imaging step. Second, we experimented with a 5-layer nonlinear bottleneck autoencoder <ref> (Leen and Kambhatla, 1994) </ref>. This training scheme, likewise, performed poorly on our data set. The outcome of this experiment showed that self-supervised dimensionality reduction cannot recover a good LDR in the present case, illustrating the importance of guidance provided by the class labels.
Reference: <author> Lowe, D. </author> <year> (1993). </year> <title> Novel 'topographic' nonlinear feature extraction using radial basis functions for concentration codeing in the 'artificial nose'. </title> <booktitle> In Proceedings of the 3rd IEE International Conference on Artificial Neural Networks. </booktitle> <publisher> IEE Conference Publications. </publisher>
Reference: <author> Lowe, D. and Tipping, M. E. </author> <year> (1996). </year> <title> Feed-forward neural networks and topographic mappings for exploratory data analysis. </title> <journal> Neural Computing and Applications, </journal> <volume> 4 </volume> <pages> 83-95. </pages>
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 473-493. </pages>
Reference-contexts: These include smoothness (Wahba, 1990; Poggio and Girosi, 1990), as well as assumptions about the distribution of the parameters, e.g., favoring small weights via a weight decay process, or favoring particular distributions such as mixtures of Gaussians <ref> (Nowlan and Hinton, 1992) </ref>. A general framework for imposing such constraints is presented in (Intrator, 1993). 4 1.2. Specific assumptions and prior knowledge Unlike data, class labels are not often reused to facilitate learning (see, however, (Grossman and Lapedes, 1993)).
Reference: <author> Oja, E. </author> <year> (1989). </year> <title> Neural networks, principal components, and subspaces. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 61-68. </pages>
Reference: <author> Pickover, C. </author> <year> (1990). </year> <title> Computers, Pattern, Chaos, and Beauty. </title> <address> St. </address> <publisher> Martin's Press. </publisher>
Reference-contexts: iteration formula is q (0) = (xpixel; ypixel; z j ; z k ) q (n + 1) = q (n) fl q (n) + c; where both q and c = (c 1 ; c i ; c j ; c k ) are quaternions (for further details, see <ref> (Pickover, 1990) </ref>, chapter 10). The three dimensions shown in Figure 2 correspond to the variation of parameters c 1 ; c j , and c k , respectively. Note that the transformation from the 3D parameter space to the image space, implied by the above formula, is highly nonlinear.
Reference: <author> Poggio, T. and Girosi, F. </author> <year> (1990). </year> <title> Regularization algorithms for learning that are equivalent to multilayer networks. </title> <journal> Science, </journal> <volume> 247 </volume> <pages> 978-982. </pages>
Reference-contexts: Training was confined to the computation of the optimal hidden to output weights (the basis functions being centered on a subset of the input examples), and could be carried out, therefore, by simple pseudoinverse <ref> (Poggio and Girosi, 1990) </ref>. As a result, this LDR extraction method was much faster than the two methods involving MLPs (which were trained by back-propagation). As we shall see, this rapid LDR extraction produced representations nearly as good as those obtained by MLP. 3.2.
Reference: <author> Price, D., Knerr, S., Personnaz, L., and Dreyfus, G. </author> <year> (1995). </year> <title> Pairwise neural network classifiers with probabilistic outputs. </title> <editor> In G. Tesauro, D. S. T. and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing 7, </booktitle> <pages> pages 1109-1116. </pages> <publisher> MIT Press. 36 REFERENCES Raviv, </publisher> <editor> Y. and Intrator, N. </editor> <year> (1996). </year> <title> Bootstrapping with noise: An effective regularization technique. </title> <journal> Connection Science, Special issue on Combining Estimators, </journal> <volume> 8 </volume> <pages> 356-372. </pages>
Reference-contexts: A natural extrapolation of this strategy would be to teach the network many possible dichotomies, in the hope that the structure of the underlying LDR can be recovered from the multiple two-way classifications <ref> (Price et al., 1995) </ref>. The advantage of operating at the level of 18 classes (or of three classes, with six subclasses each) 23 illustrates the incorporation of prior hierarchical category knowledge into the LDR extraction process.
Reference: <author> Sammon, J. W. </author> <year> (1969). </year> <title> A nonlinear mapping for data structure analysis. </title> <journal> IEEE Trans. Comput., </journal> <volume> 18 </volume> <pages> 401-409. </pages> <note> Sas (1989). SAS/STAT User's Guide, Version 6. </note> <institution> SAS Institute Inc., Cary, NC. </institution>
Reference: <author> Shepard, R. N. </author> <year> (1966). </year> <title> Metric structures in ordinal data. </title> <journal> J. Math. Psychology, </journal> <volume> 3 </volume> <pages> 287-315. </pages>
Reference-contexts: The power of MDS as a tool for the study of internal representations (of human subjects) was revealed when Shepard discovered in 1962 that fixing the relative distances of a set of points effectively determines their coordinates <ref> (Shepard, 1966) </ref>.
Reference: <author> Shepard, R. N. </author> <year> (1980). </year> <title> Multidimensional scaling, tree-fitting, and clustering. </title> <journal> Science, </journal> <volume> 210 </volume> <pages> 390-397. </pages>
Reference: <author> Shepard, R. N. and Cermak, G. W. </author> <year> (1973). </year> <title> Perceptual-cognitive explorations of a toroidal set of free-form stimuli. </title> <journal> Cognitive Psychology, </journal> <volume> 4 </volume> <pages> 351-377. </pages>
Reference: <author> Siedlecki, W., Siedlecka, K., and Sklansky, J. </author> <year> (1988). </year> <title> An overview of mapping techniques for exploratory pattern analysis. </title> <journal> Pattern Recognition, </journal> <volume> 21 </volume> <pages> 411-429. </pages>
Reference: <author> Simard, P., Victorri, B., LeCun, Y., and Denker, J. </author> <year> (1992). </year> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In Moody, J., Lippman, R., and Hanson, S. J., editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 895-903. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Stone, M. </author> <year> (1974). </year> <title> Cross-validatory choice and assessment of statistical predictions (with discussion). </title> <journal> J. Royal Statistics Society B, </journal> <volume> 36 </volume> <pages> 111-147. </pages>
Reference-contexts: More generally, innovative use of the training data is needed. For example, methods for data reuse such as cross-validation <ref> (Stone, 1974) </ref> and bootstrap (Efron and Tibshirani, 1993) can help in obtaining confidence intervals (Baxt and White, 1995) and improved performance (Breiman, 1992; Breiman, 1994; LeBlanc and Tibshirani, 1994) of learning networks.
Reference: <author> Thrun, S. and Mitchell, T. </author> <year> (1995). </year> <title> Learning one more thing. </title> <editor> In Mellish, C., editor, </editor> <booktitle> Proc. 14th IJCAI, </booktitle> <volume> volume 2, </volume> <pages> pages 1217-1223, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wahba, G. </author> <year> (1990). </year> <title> Splines Models for Observational Data. </title> <booktitle> Series in Applied Mathematics, </booktitle> <volume> Vol. 59, </volume> <publisher> SIAM, </publisher> <address> Philadelphia. </address> <note> REFERENCES 37 Webb, </note> <author> A. R. </author> <year> (1995). </year> <title> Multidimensional-scaling by iterative majorization using radial basis functions. </title> <journal> Pattern Recognition, </journal> <volume> 28 </volume> <pages> 753-759. </pages>
Reference: <author> Young, G. and Householder, A. S. </author> <year> (1938). </year> <title> Discussion of a set of points in terms of their mutual distances. </title> <journal> Psychometrika, </journal> <volume> 3 </volume> <pages> 19-22. </pages>
Reference-contexts: As topology-preserving mapping is the ultimate goal of a number of methods of dimensionality reduction, it is appropriate to mention here the typical approaches taken by these methods. The oldest among these is multidimensional scaling (MDS) <ref> (Young and Householder, 1938) </ref>. It is discussed at some length in the appendix. <p> REFERENCES 31 Acknowledgments We thank P. Dayan and J. Tenenbaum for useful suggestions. Appendix: Multidimensional scaling (MDS) MDS has been originally developed in psychometrics, as a method for the recovery of the coordinates of a set of points from measurements of the pairwise distances between those points <ref> (Young and Householder, 1938) </ref>. In a typical application, the experimenter would attempt to characterize a subject's performance by placing a point corresponding to each stimulus perceived by the subject in a coordinate space, derived from subjective similarity ratings of pairs of stimuli.
References-found: 50

