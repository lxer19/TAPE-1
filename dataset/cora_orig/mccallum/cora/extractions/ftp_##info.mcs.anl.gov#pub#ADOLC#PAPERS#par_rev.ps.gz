URL: ftp://info.mcs.anl.gov/pub/ADOLC/PAPERS/par_rev.ps.gz
Refering-URL: http://www.mcs.anl.gov/Projects/autodiff/AD_Tools/adolc.anl/adolc.html
Root-URL: http://www.mcs.anl.gov
Title: Parallelism in the Reverse Mode  
Author: Jochen Benary 
Abstract: In the basic form of the reverse mode for calculating derivatives, the amount of memory needed to record the intermediate values can become excessively large for problems of practical interest. If sequential checkpointing schemes are used, the memory requirement can be dramatically reduced, but the run time may be significantly increased. Implementing suitable checkpointing schemes on multiprocessor systems can decrease the run time to its theoretical minimum. Among the many possible scheduling strategies, we develop one that minimizes resource requirements. We present different communication structures that depend on the memory architecture of the multiprocessor system and the available resources. We also estimate the limits of the complexity and the memory requirements of the problem function. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Benary: </author> <title> DAP - Dresdener Adjungierten Parallelisierungsprojekt, </title> <type> Preprint IOKOMO-05-1995, TU Dresden, Dresden, </type> <year> 1995 </year>
Reference-contexts: Producing snapshots in a distributed-memory architecture right the communication thread is shown, while in the middle the computational part is presented. The copy operations are the synchronization points. 4 First Results and Further Work DAP <ref> [1] </ref> is a first version of a software system that implements the strategy on a homogeneous multiprocessor system using PVM [4]. The use of PVM implies the asynchronous communication as mentioned above. DAP is implemented as a preprocessing system.
Reference: [2] <author> A. Griewank: </author> <title> Achieving Logarithmic Growth of Temporal and Spatial Complexity in Reverse Automatic Differentiation, Optimization Methods and Software, </title> <booktitle> 1992, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 35-54 </pages>
Reference-contexts: To avoid starting from state s 0 every time, intermediate states s j = (x j ; r j ) are stored at suitably selected indices j as temporary starting points. Such states s j are called snapshots. Griewank <ref> [2] </ref> has proposed a modification of the reverse mode based on this idea, which limits the growth in both spatial and temporal complexity to a multiple of log 4 n compared with the original function evaluation. <p> Therefore, the formula (2) implies that the number of processors grows with O (log n) as a function of the length of the iteration n. As we will see, this holds for the memory needed as well. Comparing this result with the symmetrical scheduling strategy of <ref> [2] </ref>, we may conclude that the logarithmic growth of the run time in the sequential scheduling method is replaced by the logarithmic growth of the processor requirement in our parallel scheduling scheme. In the underlying concept we have assumed that each processor is permanently assigned to one level. <p> Ratio of the run time of the multiprocessor scheduling to the simple function evaluation Fig. 7. Ratio of the run time of the single-processor scheduling to the multiproces sor scheduling In addition, the scheduling method of <ref> [2] </ref> in its symmetrical form was implemented to compare the results of the time measuring with the single-processor scheduling. Figure 7 depicts this ratio in dependency on the dimension and the number of iteration steps n.
Reference: [3] <author> A. Griewank, D. Juedes, J. Utke: ADOL-C, </author> <title> A Package for the Automatic differentiation of Algorithms Written in C/C++, </title> <journal> ACM TOMS, </journal> <note> to appear 1996 </note>
Reference-contexts: In addition, the modules M and M 0 must be worked by hand. In the future, a Fortran version should be implemented. Furthermore, automatic generation of the modules M and M 0 should be possible. Initially, ADOL-C <ref> [3] </ref> will be used for this purpose. Acknowledgments This project benefited from discussions with Bertram Bracher and Andreas Griewank, who originally suggested the approach. Furthermore, Bertram Bracher transferred DAP to the Parsytec-PowerXplorer and carried out the timings.
Reference: [4] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, V. Sunderam: </author> <title> PVM Parallel Virtual Machine, A User's Guide and Tutorial for Networked Parallel Computing, </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1994 </year>
Reference-contexts: The copy operations are the synchronization points. 4 First Results and Further Work DAP [1] is a first version of a software system that implements the strategy on a homogeneous multiprocessor system using PVM <ref> [4] </ref>. The use of PVM implies the asynchronous communication as mentioned above. DAP is implemented as a preprocessing system. Hence, the user need specify only these parts that are specific to the given problem.
Reference: [5] <author> Th. Frach, G. Viehover: </author> <note> PowerPVM for PARIX 1.2-PPC, Version 1.0 beta, </note> <institution> PowerStone GmbH, Herzogenrath, </institution> <year> 1994 </year>
Reference-contexts: To clarify the influence of the communication, we generated records of about (8 + 13) bytes. The tasks were executed on a Parsytec-PowerXplorer with 16 nodes and 32 megabytes of local memory. The PVM version used was PowerPVM <ref> [5] </ref>. The time needed to compute the gradient was measured for various dimensions and numbers of iteration steps.
References-found: 5

