URL: http://www-ai.ijs.si/AramKaralic/bibliography/1995f.ps
Refering-URL: http://www-ai.ijs.si/AramKaralic/bibliography/1995f.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Aram Karalic First Order Regression  
Degree: Dissertation Dissertation Supervisor: prof. dr. Ivan Bratko  
Date: Ljubljana, 1995  
Affiliation: University of Ljubljana Faculty of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Ivan Bratko. </author> <title> Prolog Programming for Artificial Intelligence. </title> <publisher> Addison-Wesley, </publisher> <address> 2 nd edition, </address> <year> 1990. </year>
Reference-contexts: More about theoretical foundations of logic programming and the relation between Prolog and logic can be read in [32], while introduction to Prolog as a programming language and its use in artificial intelligence programming can be found in <ref> [1] </ref>. The rest of the Chapter is organized as follows. In Section 1.1 a motivation for our work is elaborated and the goals of the work are specified. <p> We will now briefly describe the way Prolog programs are executed while a more elaborate description of Prolog interpreter's operation can be studied in <ref> [1] </ref>. <p> Interpretation proceeds as described in Figure 2.5 (taken from Chapter 2 of <ref> [1] </ref>). 28 Chapter 2. Fors Algorithm procedure ProveGoalList (P,G,Success) begin if G 6= ; then begin G f := Head (G) : : : First goal. G o := Tail (G) : : : Other goals.
Reference: [2] <author> L. Breiman, J. H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Int. Group, </publisher> <address> Belmont, California, USA, </address> <year> 1984. </year>
Reference-contexts: Examples of algorithms of this kind are: ID3 [41], Assistant [23, 3], AQ15 [33], Ginesys [8, 16]. Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart <ref> [2] </ref>, Retis [17, 18], M5 [43], Lagrange [4], GoldHorn [27]. In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34].
Reference: [3] <author> Bojan Cestnik, Igor Kononenko, and Ivan Bratko. </author> <title> Assistant 86: A knowledge-elicitation tool for sophisticated users. </title> <editor> In Ivan Bratko and Nada Lavrac, editors, </editor> <booktitle> Progress in Machine Learning, Wilmslow, 1987. </booktitle> <publisher> Sigma Press. </publisher>
Reference-contexts: The algorithms can be divided into two categories: * Algorithms that learn from examples with discrete class. * Algorithms that learn from examples with continuous class. Algorithms learning discrete class induce a classification rule. Examples of algorithms of this kind are: ID3 [41], Assistant <ref> [23, 3] </ref>, AQ15 [33], Ginesys [8, 16]. Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis [17, 18], M5 [43], Lagrange [4], GoldHorn [27].
Reference: [4] <author> Saso Dzeroski and Ljupco Todorovski. </author> <title> Discovering dynamics. </title> <booktitle> In Proceedings of Tenth Machine Learning Conference, </booktitle> <pages> pages 97-103, </pages> <address> Amherst, Massachusetts, USA, </address> <month> June 27-29 </month> <year> 1993. </year>
Reference-contexts: Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis [17, 18], M5 [43], Lagrange <ref> [4] </ref>, GoldHorn [27]. In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34].
Reference: [5] <author> Saso Dzeroski, Ljupco Todorovski, and Tanja Urbancic. </author> <title> Handling real numbers in ilp: A step towards successful behavioral cloning (extended abstract). </title> <booktitle> In Proceedings of Eighth European Conference on Machine Learning ECML'95, </booktitle> <pages> pages 283-286, </pages> <address> Berlin, Germany, 1995. </address> <publisher> Springer. </publisher>
Reference-contexts: The need for algorithms capable 1.2. Organisation of the Thesis 17 of handling continuous variables is therefore obvious. Dzeroski and Todorovski have lately introduced an ILP system Dinus <ref> [5] </ref>, which can handle real numbers. Dinus, a successor of Linus [30], transforms ILP problems into propositional form and then applies propositional learning systems that have the desired capability. Currently, Dinus employs Retis [18] to perform the induction when the treatment of real-valued attributes and class is required.
Reference: [6] <author> B. Falkenheiner and Ryszard Michalski. </author> <title> Integrating quantitative and qualitative discovery in the ABACUS system. </title> <editor> In Yves Kodratoff and Ryszard Michalski, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume III, </booktitle> <pages> pages 153-190, </pages> <address> San Mateo, CA, USA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Examples of algorithms of this kind are: ID3 [41], Assistant [23, 3], AQ15 [33], Ginesys [8, 16]. Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus <ref> [6] </ref>, Bacon [29] Fahrenheit [49], Cart [2], Retis [17, 18], M5 [43], Lagrange [4], GoldHorn [27]. In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34].
Reference: [7] <author> Bogdan Filipic, Miha Junkar, Ivan Bratko, and Aram Karalic. </author> <title> An application of machine learning to a metal-working process. </title> <booktitle> In Proceedings of ITI-91, </booktitle> <address> Cavtat, Croatia, </address> <year> 1991. </year>
Reference-contexts: An example of a control action is terminating the process when its performance reaches an unacceptable degree <ref> [7] </ref>. Since the control action is easily deducible from the workpiece roughness, the subproblem of the workpiece roughness estimation was addressed first. Deeper insight in steel grinding process from the mechanical engineering point of view can be obtained from [11]. <p> Deeper insight in steel grinding process from the mechanical engineering point of view can be obtained from [11]. Several machine learning techniques were already applied to the problem, yielding encouraging results [14], [12], <ref> [7] </ref>, [18], and showing that machine learning tools can produce quite adequate models of the system behavior. 5.3.2 Data Acquisition Data were obtained during an experiment in which vibration signals generated by the grinding wheel and the workpiece were detected by an accelerometer sensor and processed by a spectrum analyser.
Reference: [8] <author> Matjaz Gams. </author> <title> Simplification principles in machine learning systems. </title> <type> PhD thesis, </type> <institution> University of Ljubljana, Faculty for Electrical Engineering and Computer Science, Ljubljana, Slovenia, </institution> <year> 1989. </year>
Reference-contexts: The algorithms can be divided into two categories: * Algorithms that learn from examples with discrete class. * Algorithms that learn from examples with continuous class. Algorithms learning discrete class induce a classification rule. Examples of algorithms of this kind are: ID3 [41], Assistant [23, 3], AQ15 [33], Ginesys <ref> [8, 16] </ref>. Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis [17, 18], M5 [43], Lagrange [4], GoldHorn [27].
Reference: [9] <author> Marko Grobelnik. </author> <title> Induction of prolog programs with markus. </title> <booktitle> In Proceedings of the Second Workshop in Logic Program Synthesis and Program Transformation (LOP-STR'93), </booktitle> <address> Lovain la Neuve, Belgium, </address> <year> 1993. </year> <note> 121 122 Bibliography </note>
Reference-contexts: In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34]. Some algorithms of this kind are: Foil [42], Cigol [36], Golem [37], Linus [30], Markus <ref> [9] </ref>, Progol [35]. ILP algorithms are usually not especially adapted to continuous data, although such variables are rather common when dealing with real-life domains [44]. The need for algorithms capable 1.2. Organisation of the Thesis 17 of handling continuous variables is therefore obvious.
Reference: [10] <author> R. W. </author> <title> Hamming. Digital Filters. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: The reason that this method is preferred to methods which draw a polynomial through all the points is that this technique also acts as a simple high-pass filter <ref> [10] </ref>. The technique was already successfully employed for deriving results of measurements in [24]. 2.7 Recursive Definitions The usage of recursive literals is regulated with the parameter R . When R is true, recursive literals are added as follows.
Reference: [11] <author> Miha Junkar. </author> <title> Identification of Grinding Process With Correlative Functions. </title> <type> PhD thesis, </type> <institution> Faculty for Mechanical Engineering, University of Ljubljana, Ljubljana, Slove-nia, </institution> <year> 1986. </year> <note> In Slovene. </note>
Reference-contexts: Since the control action is easily deducible from the workpiece roughness, the subproblem of the workpiece roughness estimation was addressed first. Deeper insight in steel grinding process from the mechanical engineering point of view can be obtained from <ref> [11] </ref>.
Reference: [12] <author> Miha Junkar, Bogdan Filipic, and Ivan Bratko. </author> <title> Identifying the grinding process by means of inductive machine learning. </title> <booktitle> In Preprints of the first CIRP Workshop of the Intelligent Manufacturing Systems, </booktitle> <address> Budapest, Hungary, </address> <year> 1991. </year>
Reference-contexts: Deeper insight in steel grinding process from the mechanical engineering point of view can be obtained from [11]. Several machine learning techniques were already applied to the problem, yielding encouraging results [14], <ref> [12] </ref>, [7], [18], and showing that machine learning tools can produce quite adequate models of the system behavior. 5.3.2 Data Acquisition Data were obtained during an experiment in which vibration signals generated by the grinding wheel and the workpiece were detected by an accelerometer sensor and processed by a spectrum analyser.
Reference: [13] <institution> Mihael Junkar. University of Ljubljana, Faculty for Mechanical Engineering, Laboratory for Alternative Technologies. </institution> <type> Personal communication. </type>
Reference-contexts: Comparison with the models induced in [17] revealed that no significant improvement in 5.3. Steel Grinding 77 model quality in terms of RE occurred. However, newly induced models frequently contained background knowledge literals. It is interesting that according to domain experts <ref> [13, 39] </ref>, this can be considered as a significant improvement, because newly induced models are more general than models without background knowledge. <p> Usage of background knowledge didn't bring any significant improvement to model quality in terms of RE. However, newly induced models frequently contained background knowledge literals. It is interesting that according to domain experts <ref> [13, 39] </ref>, this can be considered as a significant achievement, because newly induced models are more general than models without background knowledge.
Reference: [14] <author> Mihael Junkar, Bogdan Filipic, and Ivan Bratko. </author> <title> Identifying the grinding process by means of inductive machine learning. Computers in Industry, </title> <address> 17(2-3):147-153, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Deeper insight in steel grinding process from the mechanical engineering point of view can be obtained from [11]. Several machine learning techniques were already applied to the problem, yielding encouraging results <ref> [14] </ref>, [12], [7], [18], and showing that machine learning tools can produce quite adequate models of the system behavior. 5.3.2 Data Acquisition Data were obtained during an experiment in which vibration signals generated by the grinding wheel and the workpiece were detected by an accelerometer sensor and processed by a spectrum <p> Simultaneously, workpiece surface roughness was measured. During the grinding experiment 123 measurements were performed corresponding to 1275 laps of the wheel <ref> [14] </ref>. 5.3.3 Background Knowledge Two background knowledge literals were defined: =&lt; and &gt;=, enabling Fors to compare the frequencies. An experiment was then carried out to evaluate the contribution of so defined background knowledge. Domain grv was split in ten pairs of learning/testing example sets.
Reference: [15] <author> Mihael Junkar, Bogdan Filipic, and Matjaz Znidarsic. </author> <title> An AI approach to the selection of dielectricum in electrical discharge machining. </title> <booktitle> In Proceedings of Third International Conference on Advanced Manufacturing Systems and Technology AMST'93, </booktitle> <address> Udine, Italy, </address> <year> 1993. </year>
Reference-contexts: The gap is continuously flushed by the third element, the dielectricum <ref> [15] </ref>. The process consists of numerous randomly ignited monodischarges generating crater-textured surface.
Reference: [16] <author> Aram Karalic. </author> <title> Implementation of a Machine Learning System GINESYS PC. </title> <type> BSc thesis, </type> <institution> University of Ljubljana, Faculty of Electrical Engineering and Computer Science, Ljubljana, Slovenia, </institution> <year> 1988. </year>
Reference-contexts: The algorithms can be divided into two categories: * Algorithms that learn from examples with discrete class. * Algorithms that learn from examples with continuous class. Algorithms learning discrete class induce a classification rule. Examples of algorithms of this kind are: ID3 [41], Assistant [23, 3], AQ15 [33], Ginesys <ref> [8, 16] </ref>. Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis [17, 18], M5 [43], Lagrange [4], GoldHorn [27].
Reference: [17] <author> Aram Karalic. </author> <title> Induction of Regression Trees From Incomplete Data. </title> <type> Master's thesis, </type> <institution> University of Ljubljana, Faculty for Electrical Engineering and Computer Science, Ljubljana, Slovenia, </institution> <year> 1991. </year>
Reference-contexts: Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis <ref> [17, 18] </ref>, M5 [43], Lagrange [4], GoldHorn [27]. In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34]. <p> Domain grv was split in ten pairs of learning/testing example sets. Each learning set contained 70 % of all the examples and testing set contained the rest of them. Pairs of example sets were used as new domains, named grv0 through grv9. Comparison with the models induced in <ref> [17] </ref> revealed that no significant improvement in 5.3. Steel Grinding 77 model quality in terms of RE occurred. However, newly induced models frequently contained background knowledge literals.
Reference: [18] <author> Aram Karalic. </author> <title> Employing linear regression in regression tree leaves. </title> <booktitle> In Proceedings of ECAI'92 (European Conference on Artificial Intelligence), </booktitle> <pages> pages 440-441, </pages> <address> Wienna, Austria, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis <ref> [17, 18] </ref>, M5 [43], Lagrange [4], GoldHorn [27]. In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34]. <p> Dzeroski and Todorovski have lately introduced an ILP system Dinus [5], which can handle real numbers. Dinus, a successor of Linus [30], transforms ILP problems into propositional form and then applies propositional learning systems that have the desired capability. Currently, Dinus employs Retis <ref> [18] </ref> to perform the induction when the treatment of real-valued attributes and class is required. In the thesis, we tried to develop a system capable of inducing concepts in first order logic, utilizing some useful ILP concepts (such as background knowledge). <p> Deeper insight in steel grinding process from the mechanical engineering point of view can be obtained from [11]. Several machine learning techniques were already applied to the problem, yielding encouraging results [14], [12], [7], <ref> [18] </ref>, and showing that machine learning tools can produce quite adequate models of the system behavior. 5.3.2 Data Acquisition Data were obtained during an experiment in which vibration signals generated by the grinding wheel and the workpiece were detected by an accelerometer sensor and processed by a spectrum analyser.
Reference: [19] <institution> Igor Komel. University of Ljubljana, Faculty for Mechanical Engineering, Laboratory for Alternative Technologies. </institution> <type> Personal communication. </type>
Reference-contexts: Experiments in Real Domains - Gap Flow G min G opt G max F opt fl fl o r c s n p o e s no process arcs stable process 5.4.6 Expert's Evaluation of the Selected Model Domain expert <ref> [19] </ref> qualitatively defined the relation between the effectiveness of the process and the control parameters | gap and flow. The state of the process is presented in two dimensions on Figure 5.16. The figure defines two states of the process: stable process and arcing process. <p> However, there were some comments primarily concerned with the understandability of the models. To improve model comprehensibility the two induced submodels were combined together to reveal combined strategy for simultaneous control of gap and flow. Domain expert <ref> [19] </ref> qualitatively defined the relation between effectiveness of the process 104 Chapter 5. Experiments in Real Domains and the control parameters | gap and flow. The state diagram was further divided into six regions, representing qualitatively different process behaviors.
Reference: [20] <institution> Boris Kompare. Faculty for Civil Engineering and Geodesy, Department for Hydro-engineering, Institute for Sanitary Engineering. </institution> <type> Personal communication. </type>
Reference-contexts: Comparison between all the discussed models is presented in Table 5.1. The results show that Fors, GoldHorn, and the theoretical model perform approximately the same, the theoretical model being the best and f being the best of induced models. Expert's opinion According to the domain expert <ref> [20] </ref>, the most important criterion of the model quality is : : , so he regards our induced model as a success.
Reference: [21] <author> Boris Kompare and Saso Dzeroski. </author> <title> Two artificial intelligence methods for knowledge synthesis from environmental data. </title> <editor> In P. Zanetti, editor, </editor> <booktitle> Proceedings of Envirosoft'94, Computer Techniques in Environmental Studies V, Vol. II: Environmental Systems, </booktitle> <pages> pages 265-272, </pages> <address> San Francisco, USA, </address> <year> 1994. </year>
Reference-contexts: Eutroph-ication of the Lake of Bled progressed in big steps this century, endangering the tourist economy of the region. Several restoration measures have been undertaken to avoid the disturbing algal blooms [22], <ref> [21] </ref>. Modelling of the algal biomass quantity could help understanding the mechanisms which influence the algal blooms and choosing the measures to prevent them. 5.2.1 Data Acquisition and Domain Description Measurements were provided by the National Institute of Biology, University of Ljubljana. <p> It should be noted, however, that our work did not substantially improve (in terms of clarification of the underlying processes) the results of previous work in this domain, described in <ref> [21] </ref>. The expert's opinion is that not much more can be done without additional more precise and frequent measurements. 5.2.4 Section Summary The task was to model algal biomass quantity in the Lake of Bled, Slovenia. During six years (1987-1992) several quantities were measured in approximately monthly intervals. <p> The impression is, that our work did not substantially improve the results of previous work in this domain, described in <ref> [21] </ref>. The expert's opinion is that not much more can be done in this domain without additional more precise and more frequent measurements. 76 Chapter 5. <p> It should be noted, however, that our work did not substantially improve the results of previous work in this domain, described in <ref> [21] </ref>. The expert's opinion is that not much more can be achieved in this domain without additional 5.7. Chapter Summary 113 more precise and more frequent measurements.
Reference: [22] <author> Boris Kompare and Mitja Rismal. </author> <title> Modelling the lake of bled. </title> <booktitle> In ISEM's 8 th International Conference on the Stat-Of-The-Art in Ecological Modelling, </booktitle> <address> Kiel, Germany, </address> <year> 1992. </year> <note> Bibliography 123 </note>
Reference-contexts: Eutroph-ication of the Lake of Bled progressed in big steps this century, endangering the tourist economy of the region. Several restoration measures have been undertaken to avoid the disturbing algal blooms <ref> [22] </ref>, [21]. Modelling of the algal biomass quantity could help understanding the mechanisms which influence the algal blooms and choosing the measures to prevent them. 5.2.1 Data Acquisition and Domain Description Measurements were provided by the National Institute of Biology, University of Ljubljana.
Reference: [23] <author> Igor Kononenko. </author> <title> Development of inductive learning system Assistant. </title> <type> Master's thesis, </type> <institution> University of Ljubljana, Faculty for Electrical Engineering and Computer Science, Ljubljana, Slovenia, </institution> <year> 1985. </year>
Reference-contexts: The algorithms can be divided into two categories: * Algorithms that learn from examples with discrete class. * Algorithms that learn from examples with continuous class. Algorithms learning discrete class induce a classification rule. Examples of algorithms of this kind are: ID3 [41], Assistant <ref> [23, 3] </ref>, AQ15 [33], Ginesys [8, 16]. Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis [17, 18], M5 [43], Lagrange [4], GoldHorn [27].
Reference: [24] <author> Borut Korenjak. </author> <title> Experimental Environment for Modelling of Dynamic Systems Using Artificial Intelligence Methods. </title> <type> Master's thesis, </type> <institution> University of Ljubljana, Faculty for Electrical Engineering and Computer Science, Ljubljana, Slovenia, </institution> <year> 1994. </year> <note> In Slovene. </note>
Reference-contexts: The reason that this method is preferred to methods which draw a polynomial through all the points is that this technique also acts as a simple high-pass filter [10]. The technique was already successfully employed for deriving results of measurements in <ref> [24] </ref>. 2.7 Recursive Definitions The usage of recursive literals is regulated with the parameter R . When R is true, recursive literals are added as follows. All combinations of input variables (obeying type specifications) are considered, and the output variable (that is: class variable) is treated as a new variable.
Reference: [25] <author> Matevz Kovacic. </author> <title> Stochastic Inductive Loginc Programming. </title> <type> PhD thesis, </type> <institution> Faculty of electrical Engineering and Computer Science, Ljubljana, Slovenia, </institution> <year> 1995. </year>
Reference-contexts: It is an application of the Rissa-nen's minimal description length (MDL) principle [46], which recently became a successful technique for estimating the point at which a theory starts overfitting the data, thus fitting the noise present in the data [45], [38], <ref> [25] </ref>. The MDL principle states that the best theory to explain the data is the one which minimizes the sum of the length (in bits) of the theory description, and the length (in bits) of the data when coded with the help of the theory [31]. <p> Applications of the Rissanen's minimal description length (MDL) principle [46] recently emerged as successful techniques for estimating the point at which a model starts overfit 35 36 Chapter 3. MDL Principle for Continuous Variables ting the data, thus fitting the noise present in the data [45], [38], <ref> [25] </ref>. <p> They also devise a coding schema that enables efficient coding of theory, proofs, and examples and explore relationship between the compressibility of a clause and its significance and accuracy. * Kovacic <ref> [25] </ref> uses the MDL principle in the area of stochastic inductive logic program ming. He suggests, implements and tests an improvement of Muggleton's approach. Each of the implementations is characterized by a particular coding technique, chosen to estimate length (in bits) of the theory and examples. <p> if jBj + jAj + jHj + jProofsj &lt; jBj + jE j: (3.6) During the search, only such hypotheses are considered promising, while the hypothesis not satisfying (3.6) are pruned immediately. 3.2.2 Coding Schema The main difference of our coding schema with respect to schemata used in [45], [38], <ref> [25] </ref> emerges from the fact that we deal also with continuous variables. Therefore, we must 3.2. Adaptation of the MDL principle to Continuous Variables 39 drop the requirement of the exact reconstruction of a class value from its attributes, if we want to avoid overfitting the noise. <p> Usage of MDL pruning produced slightly, but not significantly, worse models. Table 5.4 compares achieved results with the results of the FOIL, mFOIL, GOLEM, and MILP. Results for other systems were taken from <ref> [25] </ref>. Fors performed better than FOIL and mFOIL and roughly similar to GOLEM and MILP. However, only the differences between FOIL and the rest of the algorithms are significant. 5.5.3 Conclusions The achieved results are comparable with the results of other algorithms in this domain.
Reference: [26] <institution> Vane Kralj. University of Ljubljana, Faculty for Mechanical Engineering, Laboratory for Alternative Technologies. </institution> <type> Personal communication. </type>
Reference-contexts: Formation of Attributes From I/O parameters described in the previous sections, the attributes for machine learning had to be defined. The attributes were established through interaction with the domain expert and the process operator. Main patterns of behavior, as explained by the operator <ref> [26] </ref>, were the following: "1. First I regulate the gap. 2. If the share of C pulses (arcs) is too high (more than 15 %), then I increase the gap. 3.
Reference: [27] <author> Viljem Krizman. </author> <title> Handling Noisy Data in Automatic Modelling of Dynamical Systems. </title> <type> Master's thesis, </type> <institution> University of Ljubljana, Faculty for Electrical Engineering and Computer Science, Ljubljana, Slovenia, </institution> <year> 1993. </year> <note> In Slovene. </note>
Reference-contexts: Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis [17, 18], M5 [43], Lagrange [4], GoldHorn <ref> [27] </ref>. In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34]. <p> Surge Tank 67 We will now evaluate the induced model and compare its performance with the model induced with GoldHorn <ref> [27] </ref> and with the performance of the best known model (the theoretically derived model).
Reference: [28] <author> Viljem Krizman, Saso Dzeroski, and Boris Kompare. </author> <title> Discovering dynamics from measured data. </title> <booktitle> In Working Notes on the MLNet Workshop on Statistics, Machine Learning, and Knowledge Discovery in Databases. </booktitle> <institution> Institute of Computer Science, </institution> <address> Heraklion, Greece, </address> <year> 1995. </year>
Reference-contexts: Surge Tank 65 5.1 Surge Tank Modelling of water behavior in surge tank is described in this section. First, we give brief description of the domain, taken from <ref> [28] </ref>, then we proceed to our experiments in inducing the model of water behavior. Results are evaluated by comparing them to: * the measured behavior, * the best known model so far, and * models induced with GoldHorn. <p> Surge pressure is transformed to water movement in the surge tank, resulting in an increase or decrease of the steady-state water level in the tank. The rest of the pipeline is thus not exposed to the pressure shocks. <ref> [28] </ref>. Our experiments were conducted on measurements of water movement in a laboratory replica of a surge tank. Only one variable was measured: the water level h. 5.1.2 Experiments The data consist of time series describing water level. Level is measured in equal time intervals dt.
Reference: [29] <author> Pat Langley, Herbert A. Simon, and Gary L. Bradshaw. </author> <title> Heuristics for empirical discovery. </title> <editor> In L. Bolc, editor, </editor> <booktitle> Computational Models of Learning, </booktitle> <pages> pages 355-372, </pages> <address> Berlin, 1987. </address> <publisher> Springer. </publisher>
Reference-contexts: Examples of algorithms of this kind are: ID3 [41], Assistant [23, 3], AQ15 [33], Ginesys [8, 16]. Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon <ref> [29] </ref> Fahrenheit [49], Cart [2], Retis [17, 18], M5 [43], Lagrange [4], GoldHorn [27]. In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34]. <p> The law states that the ratio between the cube of the major semi-axis of the planets and the square of its period is constant for all the planets. In our experiment the same data was used as in experiments with Bacon <ref> [29] </ref>. Fors was presented with data for six planets known at Kepler's time (Figure 4.12), both variables having precision 0.001. Background knowledge consisted of predicates enabling square root of a number and multiplication of two numbers (Figure 4.13).
Reference: [30] <author> Nada Lavrac, Saso Dzeroski, and Marko Grobelnik. </author> <title> Learning nonrecursive definitions of relation s with linus. </title> <booktitle> In Proceedings of Fifth European Working Session on Learning, </booktitle> <pages> pages 265-281, </pages> <address> Berlin, 1991. </address> <publisher> Springer. </publisher>
Reference-contexts: In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34]. Some algorithms of this kind are: Foil [42], Cigol [36], Golem [37], Linus <ref> [30] </ref>, Markus [9], Progol [35]. ILP algorithms are usually not especially adapted to continuous data, although such variables are rather common when dealing with real-life domains [44]. The need for algorithms capable 1.2. Organisation of the Thesis 17 of handling continuous variables is therefore obvious. <p> The need for algorithms capable 1.2. Organisation of the Thesis 17 of handling continuous variables is therefore obvious. Dzeroski and Todorovski have lately introduced an ILP system Dinus [5], which can handle real numbers. Dinus, a successor of Linus <ref> [30] </ref>, transforms ILP problems into propositional form and then applies propositional learning systems that have the desired capability. Currently, Dinus employs Retis [18] to perform the induction when the treatment of real-valued attributes and class is required.
Reference: [31] <author> Ming Li and Paul Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and its Applications. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: The MDL principle states that the best theory to explain the data is the one which minimizes the sum of the length (in bits) of the theory description, and the length (in bits) of the data when coded with the help of the theory <ref> [31] </ref>. The MDL principle balances two factors: coding of the hypothesis and coding of the data, given the hypothesis. If the hypothesis is too general, it will have small coding, but will tend to misclassify the examples, therefore more information would be needed to correctly describe the examples. <p> MDL Principle for Continuous Variables ting the data, thus fitting the noise present in the data [45], [38], [25]. The MDL principle can be informally stated as <ref> [31] </ref>: Minimum Description Length (MDL) principle: The best theory to explain a set of data is the one which minimizes the sum of: * the length, in bits, of the description of the theory; and * the length, in bits, of data when coded with the help of the theory. <p> If, on the other hand, the hypothesis describes the examples very accurately, it will be very specialized, therefore requiring more information for its coding. A balance between the two is required. MDL principle can be derived from Fischer's maximum likelihood (ML) principle 1 . We reproduce the derivation from <ref> [31] </ref>. Let H be a hypothesis from a set of possible hypotheses H = fH i g and let D be a set of observed data.
Reference: [32] <author> John Wylie Lloyd. </author> <title> Foundations of Logic Programming. </title> <publisher> Springer-Verlag, </publisher> <address> Germany, </address> <year> 1984. </year>
Reference-contexts: Prolog is a programming language, based on the syntax of the first order predicate logic formulas. Formulas are restricted to Horn clauses and written in the clause form. More about theoretical foundations of logic programming and the relation between Prolog and logic can be read in <ref> [32] </ref>, while introduction to Prolog as a programming language and its use in artificial intelligence programming can be found in [1]. The rest of the Chapter is organized as follows. In Section 1.1 a motivation for our work is elaborated and the goals of the work are specified.
Reference: [33] <author> Ryszard Michalski, Igor Mozetic, J. R. Hong, and Nada Lavrac. </author> <title> The AQ15 inductive learning system. </title> <type> Technical Report UIUCDCS-R-86-1260, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1986. </year>
Reference-contexts: The algorithms can be divided into two categories: * Algorithms that learn from examples with discrete class. * Algorithms that learn from examples with continuous class. Algorithms learning discrete class induce a classification rule. Examples of algorithms of this kind are: ID3 [41], Assistant [23, 3], AQ15 <ref> [33] </ref>, Ginesys [8, 16]. Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis [17, 18], M5 [43], Lagrange [4], GoldHorn [27].
Reference: [34] <author> Stephen Muggleton. </author> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <address> London, Great Britain, </address> <year> 1992. </year>
Reference-contexts: In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) <ref> [34] </ref>. Some algorithms of this kind are: Foil [42], Cigol [36], Golem [37], Linus [30], Markus [9], Progol [35]. ILP algorithms are usually not especially adapted to continuous data, although such variables are rather common when dealing with real-life domains [44]. The need for algorithms capable 1.2. <p> Detailed description of objects, as well as some previous work in the domain can be found in <ref> [34] </ref>. In the experiments, four structures were used for learning and the remaining structure was used for testing.
Reference: [35] <author> Stephen Muggleton. </author> <title> Inductive logic programming: derivations, successes and shortcomings. </title> <journal> SIGART Bulletin, </journal> <volume> 1(6) </volume> <pages> 127-130, </pages> <year> 1994. </year> <note> 124 Bibliography </note>
Reference-contexts: The area is named Inductive Logic Programming (ILP) [34]. Some algorithms of this kind are: Foil [42], Cigol [36], Golem [37], Linus [30], Markus [9], Progol <ref> [35] </ref>. ILP algorithms are usually not especially adapted to continuous data, although such variables are rather common when dealing with real-life domains [44]. The need for algorithms capable 1.2. Organisation of the Thesis 17 of handling continuous variables is therefore obvious.
Reference: [36] <author> Stephen Muggleton and Wray Buntine. </author> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of 5 th Machine Learning Conference. </booktitle> <publisher> Morgan Kaufman, </publisher> <year> 1988. </year>
Reference-contexts: In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34]. Some algorithms of this kind are: Foil [42], Cigol <ref> [36] </ref>, Golem [37], Linus [30], Markus [9], Progol [35]. ILP algorithms are usually not especially adapted to continuous data, although such variables are rather common when dealing with real-life domains [44]. The need for algorithms capable 1.2. Organisation of the Thesis 17 of handling continuous variables is therefore obvious.
Reference: [37] <author> Stephen Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <address> Tokyo, Japan, </address> <year> 1990. </year>
Reference-contexts: In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34]. Some algorithms of this kind are: Foil [42], Cigol [36], Golem <ref> [37] </ref>, Linus [30], Markus [9], Progol [35]. ILP algorithms are usually not especially adapted to continuous data, although such variables are rather common when dealing with real-life domains [44]. The need for algorithms capable 1.2. Organisation of the Thesis 17 of handling continuous variables is therefore obvious.
Reference: [38] <author> Stephen Muggleton, Ashwin Srinivasan, and Michael Bain. </author> <title> Compression, significance and accuracy. </title> <booktitle> In Proceedings of Machine Learning Conference 1992, </booktitle> <address> Aberdeen, </address> <year> 1992. </year>
Reference-contexts: It is an application of the Rissa-nen's minimal description length (MDL) principle [46], which recently became a successful technique for estimating the point at which a theory starts overfitting the data, thus fitting the noise present in the data [45], <ref> [38] </ref>, [25]. The MDL principle states that the best theory to explain the data is the one which minimizes the sum of the length (in bits) of the theory description, and the length (in bits) of the data when coded with the help of the theory [31]. <p> Applications of the Rissanen's minimal description length (MDL) principle [46] recently emerged as successful techniques for estimating the point at which a model starts overfit 35 36 Chapter 3. MDL Principle for Continuous Variables ting the data, thus fitting the noise present in the data [45], <ref> [38] </ref>, [25]. <p> I wish to send you an exact description of the missing column using as few bits as possible. Quinlan and Rivest devise and explore particular coding schema that enables effi cient sending of the class column. * Muggleton, Srinivasan and Bain <ref> [38] </ref> apply the MDL principle in the area of Inductive Logic Programming. They use a reference Turing machine T with theory and proofs on the input tape and positive and negative examples on the output tape. <p> MDL Principle for Continuous Variables 3.2.1 Global Schema of the MDL Pruning The global idea is, as in <ref> [38] </ref> that in our search of the hypothesis space, only compressive hypotheses will be regarded as promising candidates. <p> compressive if jBj + jAj + jHj + jProofsj &lt; jBj + jE j: (3.6) During the search, only such hypotheses are considered promising, while the hypothesis not satisfying (3.6) are pruned immediately. 3.2.2 Coding Schema The main difference of our coding schema with respect to schemata used in [45], <ref> [38] </ref>, [25] emerges from the fact that we deal also with continuous variables. Therefore, we must 3.2. Adaptation of the MDL principle to Continuous Variables 39 drop the requirement of the exact reconstruction of a class value from its attributes, if we want to avoid overfitting the noise.
Reference: [39] <institution> Robert Posel. University of Ljubljana, Faculty for Mechanical Engineering, Laboratory for Alternative Technologies. </institution> <type> Personal communication. </type>
Reference-contexts: Comparison with the models induced in [17] revealed that no significant improvement in 5.3. Steel Grinding 77 model quality in terms of RE occurred. However, newly induced models frequently contained background knowledge literals. It is interesting that according to domain experts <ref> [13, 39] </ref>, this can be considered as a significant improvement, because newly induced models are more general than models without background knowledge. <p> Usage of background knowledge didn't bring any significant improvement to model quality in terms of RE. However, newly induced models frequently contained background knowledge literals. It is interesting that according to domain experts <ref> [13, 39] </ref>, this can be considered as a significant achievement, because newly induced models are more general than models without background knowledge.
Reference: [40] <author> William H. Press, Saul A. Teulkosky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: A singular value decomposition method was used to construct the regression plane since it is not subject to certain singularities occurring in the process of matrix decomposition when two or more variables (or their combinations) are correlated <ref> [40] </ref>. So, when Fors during the clause construction estimates a clause that does not instantiate the class, it constructs the best linear regression function F using at most ~ jLRVj continuous clause variables for independent variables in the function. <p> The behaviors of the models were simulated using classical fourth order Runge-Kutta differential equation numerical integration method <ref> [40] </ref>. From the calculated difference measures, displayed in Table 5.1 we can see that theoretical behavior almost perfectly fits the measured behavior while GoldHorn's and Fors' behaviors differ by approximately the same amount.
Reference: [41] <author> Ross Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: The algorithms can be divided into two categories: * Algorithms that learn from examples with discrete class. * Algorithms that learn from examples with continuous class. Algorithms learning discrete class induce a classification rule. Examples of algorithms of this kind are: ID3 <ref> [41] </ref>, Assistant [23, 3], AQ15 [33], Ginesys [8, 16]. Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis [17, 18], M5 [43], Lagrange [4], GoldHorn [27].
Reference: [42] <author> Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 3(5), </volume> <year> 1990. </year>
Reference-contexts: In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34]. Some algorithms of this kind are: Foil <ref> [42] </ref>, Cigol [36], Golem [37], Linus [30], Markus [9], Progol [35]. ILP algorithms are usually not especially adapted to continuous data, although such variables are rather common when dealing with real-life domains [44]. The need for algorithms capable 1.2. <p> Training examples are specified with values of all their variables (the class and the attributes). 2.2 Top Levels of the Algorithm Fors uses the covering approach similar to the one used in Foil <ref> [42] </ref>. The algorithm repeatedly constructs a clause. When a clause is found, all examples covered by the clause are removed from the learning set. The procedure is repeated while there are enough examples left (parameter jE j ). At the end of learning, a default clause is added if necessary.
Reference: [43] <author> Ross Quinlan. </author> <title> Learning with continuous classes. </title> <booktitle> In Proceedings if AI'92, </booktitle> <address> Singapore, 1992. </address> <publisher> World Scientific. </publisher>
Reference-contexts: Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit [49], Cart [2], Retis [17, 18], M5 <ref> [43] </ref>, Lagrange [4], GoldHorn [27]. In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34].
Reference: [44] <author> Ross Quinlan and M. Cameron-Jones. </author> <title> Foil: A midterm report. </title> <editor> In Pavel Brazdil, editor, </editor> <booktitle> Proceedings of Sixth European Conference on Machine Learning, </booktitle> <pages> pages 3-20, </pages> <address> Vienna, Austria, </address> <year> 1993. </year>
Reference-contexts: The area is named Inductive Logic Programming (ILP) [34]. Some algorithms of this kind are: Foil [42], Cigol [36], Golem [37], Linus [30], Markus [9], Progol [35]. ILP algorithms are usually not especially adapted to continuous data, although such variables are rather common when dealing with real-life domains <ref> [44] </ref>. The need for algorithms capable 1.2. Organisation of the Thesis 17 of handling continuous variables is therefore obvious. Dzeroski and Todorovski have lately introduced an ILP system Dinus [5], which can handle real numbers.
Reference: [45] <author> Ross Quinlan and Ronald L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: It is an application of the Rissa-nen's minimal description length (MDL) principle [46], which recently became a successful technique for estimating the point at which a theory starts overfitting the data, thus fitting the noise present in the data <ref> [45] </ref>, [38], [25]. The MDL principle states that the best theory to explain the data is the one which minimizes the sum of the length (in bits) of the theory description, and the length (in bits) of the data when coded with the help of the theory [31]. <p> Applications of the Rissanen's minimal description length (MDL) principle [46] recently emerged as successful techniques for estimating the point at which a model starts overfit 35 36 Chapter 3. MDL Principle for Continuous Variables ting the data, thus fitting the noise present in the data <ref> [45] </ref>, [38], [25]. <p> Several implementations of MDL principle have been investigated in machine learning community: * Quinlan and Rivest <ref> [45] </ref> explore the use of the Rissanen's MDL principle for the construction of decision trees. They base their approach on the communication problem: Communication Problem: You and I have copies of the data, but in your copy the last column | giving the class of each object | is missing. <p> considered compressive if jBj + jAj + jHj + jProofsj &lt; jBj + jE j: (3.6) During the search, only such hypotheses are considered promising, while the hypothesis not satisfying (3.6) are pruned immediately. 3.2.2 Coding Schema The main difference of our coding schema with respect to schemata used in <ref> [45] </ref>, [38], [25] emerges from the fact that we deal also with continuous variables. Therefore, we must 3.2. Adaptation of the MDL principle to Continuous Variables 39 drop the requirement of the exact reconstruction of a class value from its attributes, if we want to avoid overfitting the noise.
Reference: [46] <author> Jorma Rissanen. </author> <title> Modelling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: When a clause with an estimated error smaller than " R is encountered, it is considered as "perfectly accurate" and the search is stopped. 2.4.6 MDL Pruning Another noise-fighting mechanism is the MDL-pruning. It is an application of the Rissa-nen's minimal description length (MDL) principle <ref> [46] </ref>, which recently became a successful technique for estimating the point at which a theory starts overfitting the data, thus fitting the noise present in the data [45], [38], [25]. <p> An algorithm must therefore avoid fitting the learning data too strongly. Specialization of the clause with the intention to correctly describe learning data usually leads to increase of its size. Applications of the Rissanen's minimal description length (MDL) principle <ref> [46] </ref> recently emerged as successful techniques for estimating the point at which a model starts overfit 35 36 Chapter 3. MDL Principle for Continuous Variables ting the data, thus fitting the noise present in the data [45], [38], [25].
Reference: [47] <author> Jorma Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: want the correction to be as small as possible, its coding should be such, that the code length monotonically increases with the absolute value of the correction: 8x; y 2 &lt; : jxj jyj ) jxj jyj (3.7) Coding of Natural Numbers The coding of natural numbers suggested by Rissanen <ref> [47] </ref> was used. The code length is computed according to the following rules: j0j = 1 (3.8) where the sum only includes the terms that are positive and where C 1 = 2:865064 : : :.
Reference: [48] <author> Ashwin Srinivasan, Muggleton Stephen H. Sternberg M.J.E. </author> <title> and King Ross D._The-ories for mutagenicity: a study in first-order and feature-based induction. </title> <type> Technical Report PRG-TR-8-95, </type> <institution> Oxford Univerity Computing Laboratory, Oxford, </institution> <year> 1995. </year>
Reference-contexts: Experiments in Real Domains 5.6 Mutagenicity of Nitroaromatic Compounds 5.6.1 Introduction In this subsection we describe the problem. Description is largely based on <ref> [48] </ref>, where also additional details can be found. The problem of discovering rules for mutagenicity in nitroaromatic compounds comes from the field of organic chemistry. Such compounds occur in automobile exhaust fumes and in the development of industrial compounds. <p> Mutagenic activity has often been found to be linked to carcinogenesis and damage to DNA. In normal discourse, chemists typically use graph-based constructs such as atom/bound connectivities or Cartesian coordinates of atoms to describe properties of molecules. 5.6.2 Data We have studied 188 chemical compounds, studied in <ref> [48] </ref>. Although mutagenicity is a real number, we have confined our task to discriminating compounds with positive log mutagenicity from those which have zero or negative log mutagenicity. This simplified approach was used to enable comparison with the results of the previous work in the domain. <p> Comparison with the performance of other algorithms, studied in <ref> [48] </ref> is given in Table 5.6. <p> The results are compared with the results obtained with Progol, linear regression model, back-propagation neural network, and a version of CART. <ref> [48] </ref> gives more details about the algorithms. For linear regression, neural network, and CART two experiments were performed: one with only the two non-structural attributes log P and * LUMO , used also 110 Chapter 5.
Reference: [49] <author> R. Zembowitz and J _ Zytkov. </author> <title> Discovery of equations: Experimental evaluation of convergence. </title> <booktitle> In Proceedings of 10 th National Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA, USA, 1992. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Examples of algorithms of this kind are: ID3 [41], Assistant [23, 3], AQ15 [33], Ginesys [8, 16]. Algorithms learning continuous class induce functional dependence between discrete and continuous independent variables and a continuous dependent variable. Some algorithms of this kind are: Abacus [6], Bacon [29] Fahrenheit <ref> [49] </ref>, Cart [2], Retis [17, 18], M5 [43], Lagrange [4], GoldHorn [27]. In recent years the attention of machine learning researchers largely turned to the algorithms capable of inducting concepts describable in the first order language. The area is named Inductive Logic Programming (ILP) [34].
References-found: 49

