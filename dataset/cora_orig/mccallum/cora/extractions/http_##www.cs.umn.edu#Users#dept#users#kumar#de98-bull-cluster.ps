URL: http://www.cs.umn.edu/Users/dept/users/kumar/de98-bull-cluster.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: fhan,karypis,kumar,mobasherg@cs.umn.edu  
Title: Hypergraph Based Clustering in High-Dimensional Data  
Affiliation: Department of Computer Science and Engineering/Army HPC Research Center University of Minnesota  
Abstract: Sets: A Summary of Results fl Abstract Clustering of data in a large dimension space is of a great interest in many data mining applications. In this paper, we propose a method for clustering of data in a high dimensional space based on a hypergraph model. In this method, the relationship present in the original data in high dimensional space are mapped into a hypergraph. A hyperedge represents a relationship (affinity) among subsets of data and the weight of the hyperedge reflects the strength of this affinity. A hypergraph partitioning algorithm is used to find a partitioning of the vertices such that the corresponding data items in each partition are highly related and the weight of the hyperedges cut by the partitioning is minimized. We present results of experiments on two different data sets: S&P500 stock data for the period of 1994-1996 and protein coding data. These experiments demonstrate that our approach is applicable and effective in high dimensional datasets. 
Abstract-found: 1
Intro-found: 1
Reference: [AS94] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules. </title> <booktitle> In Proc. of the 20th VLDB Conference, </booktitle> <pages> pages 487-499, </pages> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: In our current implementation, we use frequent item sets found by the association rule algorithm <ref> [AS94] </ref> as hyperedges. 2.1 Hypergraph Modeling A hypergraph [Ber76] H = (V; E) consists of a set of vertices (V ) and a set of hyperedges (E). A hypergraph is an extension of a graph in the sense that each hyperedge can connect more than two vertices. <p> The results of these experiments are described in the following subsections. In all of our experiments, we used a locally implemented version of Apriori algorithm <ref> [AS94] </ref> to find the association rules and construct the association-rule hypergraph. 3.1 S&P 500 Stock Data Our first data-set consists of the daily price movement of the stocks that belong to the S&P500 index.
Reference: [BDO95] <author> M.W. Berry, S.T. Dumais, and G.W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: These experiments demonstrate that our approach is applicable and effective in a wide range of domains. More specifically, our approach performed much better than traditional schemes for dimensionality reduction <ref> [Jac91, BDO95] </ref> in terms of quality of clusters and runtime. The rest of this paper is organized as follows. Section 2 presents our clustering method based on hypergraph models. Section 3 presents the experimental results.
Reference: [Ber76] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> American Elsevier, </publisher> <year> 1976. </year>
Reference-contexts: In our current implementation, we use frequent item sets found by the association rule algorithm [AS94] as hyperedges. 2.1 Hypergraph Modeling A hypergraph <ref> [Ber76] </ref> H = (V; E) consists of a set of vertices (V ) and a set of hyperedges (E). A hypergraph is an extension of a graph in the sense that each hyperedge can connect more than two vertices.
Reference: [BMS97] <author> S. Brin, R. Motwani, and C. Silversteim. </author> <title> Beyond market baskets: Generalizing association rules to correlations. </title> <booktitle> In Proc. of 1997 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: Another, more natural, possibility is to define weight as a function of the support and confidence of the rules that are made of a group of items in a frequent item set. Other options include correlation <ref> [BMS97] </ref>, distance or similarity measure.
Reference: [CHY96] <author> M.S. Chen, J. Han, and P.S. Yu. </author> <title> Data mining: An overview from database perspective. </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> 8(6) </volume> <pages> 866-883, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Clustering in data mining is a discovery process that groups a set of data such that the intracluster similarity is maximized and the intercluster similarity is minimized <ref> [CHY96] </ref>. These discovered clusters are used to explain the characteristics of the data distribution.
Reference: [CS96] <author> P. Cheeseman and J. Stutz. </author> <title> Baysian classification (autoclass): Theory and results. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Additional support was provided by the IBM Partnership Award, and by the IBM SUR equipment grant. Access to computing facilities was provided by AHPCRC, Minnesota Supercomputer Institute. See http://www.cs.umn.edu/han for other related papers. 1 Given a set of n data items with m variables, traditional clustering techniques <ref> [CS96, JD88] </ref> group the data based on some measure of similarity or distance between data points. Most of these clustering algorithms are able to effectively cluster data when the dimensionality of the space (i.e., the number of variables) is relatively small. <p> Our experiments indicate that the hypergraph-based clustering holds great promise for clustering data in large dimensional spaces. Traditional clustering schemes such as Autoclass <ref> [CS96] </ref> and K-means [JD88] cannot be directly used in such large dimensionality data sets, as they tend to produce extremely poor results [HKKM97b]. These methods perform much better when the dimensionality of the data is reduced using methods such as Principal Component Analysis [Jac91].
Reference: [Han98] <author> E.H. Han. </author> <title> Knowledge discovery in a large dimensional space using hypergraph models. </title> <type> Technical Report Thesis In Progress, </type> <institution> Department of Computer Science, University of Minnesota, M inneapolis, </institution> <year> 1998. </year>
Reference-contexts: However, such a discretization can lead to a distortion in the relations among items, especially in cases in which a higher value indicates a stronger relation. For this type of domains, in which continuous variables with higher values imply greater importance, we have developed a new algorithm called Min-Apriori <ref> [Han98] </ref> that operates directly on these continuous variables without discretizing them. Our current clustering algorithm relies on the hypergraph partitioning algorithm HMETIS to find a good k-way partitioning. As discussed in Section 2, even though HMETIS produces high quality partitions, it has some limitations.
Reference: [HBG + 98] <author> E.H. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore. Webace: </author> <title> A web agent for document categorization and exploartion. </title> <booktitle> In Proc. of the 2nd International Conference on Autonomous Agents, </booktitle> <month> May </month> <year> 1998. </year>
Reference-contexts: To test the applicability and robustness of our scheme, we evaluated it on a wide variety of data sets <ref> [HKKM97a, MHB + 97, HBG + 98, HKKM97b] </ref>. We present a summary of results on two different data sets: S&P500 stock data for the period of 1994-1996 and protein coding data. These experiments demonstrate that our approach is applicable and effective in a wide range of domains.
Reference: [HHS92] <author> N. Harris, L. Hunter, and D. States. Mega-classification: </author> <title> Discovering motifs in massive datastreams. </title> <booktitle> In Proceedings of the Tenth International Conference on Artificial Intelligence (AAAI), </booktitle> <year> 1992. </year>
Reference-contexts: At this point, the biologists can focus on experimentally verifying the functions of the new EST represented by the matching EST clusters. Hence finding clusters of related ESTs is an important problem. Related work in this area can be found in <ref> [HHS92] </ref>, which reports clustering of the sequence-level building blocks of proteins by finding transitive closure of the pairwise probabilistic similarity judgments. To assess the utility of hypergraph-based clustering in this domain, we performed experiments with data provided by the authors of [NRS + 95].
Reference: [HKKM97a] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hy-pergraphs (position paper). </title> <booktitle> In Proc. of the Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <pages> pages 9-13, </pages> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: To test the applicability and robustness of our scheme, we evaluated it on a wide variety of data sets <ref> [HKKM97a, MHB + 97, HBG + 98, HKKM97b] </ref>. We present a summary of results on two different data sets: S&P500 stock data for the period of 1994-1996 and protein coding data. These experiments demonstrate that our approach is applicable and effective in a wide range of domains.
Reference: [HKKM97b] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering in a high-dimensional space using hypergraph models. </title> <type> Technical Report TR-97-063, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: To test the applicability and robustness of our scheme, we evaluated it on a wide variety of data sets <ref> [HKKM97a, MHB + 97, HBG + 98, HKKM97b] </ref>. We present a summary of results on two different data sets: S&P500 stock data for the period of 1994-1996 and protein coding data. These experiments demonstrate that our approach is applicable and effective in a wide range of domains. <p> Out of 20 clusters, 16 clusters were clean clusters as they contain stocks primarily from one industry group. Some of the clean clusters found from this data are shown in Table 1 and the complete list of clusters is available in <ref> [HKKM97b] </ref>. Looking at these clusters we can see that our item-clustering algorithm was very successful in grouping together stocks that belong to the same industry group. For example, our algorithm was able to find technology-, financial-, oil-, gold-, and metal-related stock-clusters. <p> Our experiments indicate that the hypergraph-based clustering holds great promise for clustering data in large dimensional spaces. Traditional clustering schemes such as Autoclass [CS96] and K-means [JD88] cannot be directly used in such large dimensionality data sets, as they tend to produce extremely poor results <ref> [HKKM97b] </ref>. These methods perform much better when the dimensionality of the data is reduced using methods such as Principal Component Analysis [Jac91]. However, as shown by our experiments in [HKKM97b], the hypergraph-based scheme produces clusters that are at least as good or better than those produced by AutoClass or K-means algorithm <p> and K-means [JD88] cannot be directly used in such large dimensionality data sets, as they tend to produce extremely poor results <ref> [HKKM97b] </ref>. These methods perform much better when the dimensionality of the data is reduced using methods such as Principal Component Analysis [Jac91]. However, as shown by our experiments in [HKKM97b], the hypergraph-based scheme produces clusters that are at least as good or better than those produced by AutoClass or K-means algorithm on the reduced dimensionality data sets.
Reference: [Jac91] <author> J. E. Jackson. </author> <title> A User's Guide To Principal Components. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: These experiments demonstrate that our approach is applicable and effective in a wide range of domains. More specifically, our approach performed much better than traditional schemes for dimensionality reduction <ref> [Jac91, BDO95] </ref> in terms of quality of clusters and runtime. The rest of this paper is organized as follows. Section 2 presents our clustering method based on hypergraph models. Section 3 presents the experimental results. <p> Traditional clustering schemes such as Autoclass [CS96] and K-means [JD88] cannot be directly used in such large dimensionality data sets, as they tend to produce extremely poor results [HKKM97b]. These methods perform much better when the dimensionality of the data is reduced using methods such as Principal Component Analysis <ref> [Jac91] </ref>. However, as shown by our experiments in [HKKM97b], the hypergraph-based scheme produces clusters that are at least as good or better than those produced by AutoClass or K-means algorithm on the reduced dimensionality data sets.
Reference: [JD88] <author> A.K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Additional support was provided by the IBM Partnership Award, and by the IBM SUR equipment grant. Access to computing facilities was provided by AHPCRC, Minnesota Supercomputer Institute. See http://www.cs.umn.edu/han for other related papers. 1 Given a set of n data items with m variables, traditional clustering techniques <ref> [CS96, JD88] </ref> group the data based on some measure of similarity or distance between data points. Most of these clustering algorithms are able to effectively cluster data when the dimensionality of the space (i.e., the number of variables) is relatively small. <p> Our experiments indicate that the hypergraph-based clustering holds great promise for clustering data in large dimensional spaces. Traditional clustering schemes such as Autoclass [CS96] and K-means <ref> [JD88] </ref> cannot be directly used in such large dimensionality data sets, as they tend to produce extremely poor results [HKKM97b]. These methods perform much better when the dimensionality of the data is reduced using methods such as Principal Component Analysis [Jac91].
Reference: [KAKS97] <author> G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. </author> <title> Multilevel hypergraph partitioning: Application in VLSI domain. </title> <booktitle> In Proceedings ACM/IEEE Design Automation Conference, </booktitle> <year> 1997. </year>
Reference-contexts: Note that by minimizing the hyperedge-cut we essentially minimize the relations that are violated by splitting the items into two groups. Now each of these two parts can be further bisected recursively, until each partition is highly connected. HMETIS <ref> [KAKS97, Kar98] </ref> is a multi-level hypergraph partitioning algorithm that has been shown to produce high quality bi-sections on a wide range of problems arising in scientific and VLSI applications.
Reference: [Kar98] <author> G. Karypis. hMETIS 1.0.0. </author> <note> http://www.cs.umn.edu/karypis/metis/hmetis/main.html, 1998. </note>
Reference-contexts: Note that by minimizing the hyperedge-cut we essentially minimize the relations that are violated by splitting the items into two groups. Now each of these two parts can be further bisected recursively, until each partition is highly connected. HMETIS <ref> [KAKS97, Kar98] </ref> is a multi-level hypergraph partitioning algorithm that has been shown to produce high quality bi-sections on a wide range of problems arising in scientific and VLSI applications.
Reference: [MHB + 97] <author> J. Moore, E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Web page categorization and feature selection using association rule and principal component clustering. </title> <booktitle> In 7th Workshop on Information Technologies and Systems, </booktitle> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: To test the applicability and robustness of our scheme, we evaluated it on a wide variety of data sets <ref> [HKKM97a, MHB + 97, HBG + 98, HKKM97b] </ref>. We present a summary of results on two different data sets: S&P500 stock data for the period of 1994-1996 and protein coding data. These experiments demonstrate that our approach is applicable and effective in a wide range of domains.
Reference: [NRS + 95] <author> T. Newman, E.F. Retzel, E. Shoop, E. Chi, and C. Somerville. </author> <title> Arabidopsis thaliana expressed sequence tags: Generation, analysis and dissemination. In Plant Genome III: </title> <booktitle> International Conference on the Status of Plant Genome Research, </booktitle> <address> San Diego, CA, </address> <year> 1995. </year> <month> 8 </month>
Reference-contexts: To rapidly determine the function of many previously unknown genes, biologists generate short segments of protein-coding sequences (called 5 Table 1: Clustering of S&P 500 Stock Data expressed sequence tags, or ESTs) and match each EST against the sequences of known proteins, using similarity matching algorithms <ref> [NRS + 95] </ref>. The result of this matching is a table showing similarities between ESTs and known proteins. <p> To assess the utility of hypergraph-based clustering in this domain, we performed experiments with data provided by the authors of <ref> [NRS + 95] </ref>. Our data set consists of a binary table of size 662 fi 11986. Each row of this table corresponds to an EST, and each column corresponds to a protein.
References-found: 17

