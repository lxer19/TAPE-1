URL: http://www.cs.monash.edu.au/~rohan/PAPERS/alt96.ps
Refering-URL: http://www.cs.monash.edu.au/~rohan/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: rohan,jono@cs.monash.edu.au  
Title: The Kindest Cut: Minimum Message Length Segmentation  
Author: Rohan A. Baxter and Jonathan J. Oliver 
Date: April 15, 1996  
Address: Australia  
Affiliation: Dept. of Computer Science Monash University, Clayton, 3168  
Abstract: We consider some particular instances of the segmentation problem. We derive minimum message length (MML) expressions for stating the region boundaries for some one and two dimensional examples. It is the found the message length cost of stating region boundaries is dependent on the noise of the data in the separated regions and also the `degree of separation' of the two regions. The framework given here can be extended to different shaped cuts and also non-constant fits for the regions. Possible applications for the work presented here include its use in tree (i.e. CART) regression and in image segmentation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman et al. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: Is there a cut-point or not? * maximum likelihood (MaxLik). This is the case where no penalty term is applied, such as in CART <ref> [1] </ref>. * MML, using Equations (2) and (15) of this paper. * AIC, using log f (xj) + k [5]. * BIC, using log f (xj) + k 2 log n [4]. * QUI, using log f (xj) + log n.
Reference: [2] <author> J.H. Conway and N.J.A. Sloane. </author> <title> Sphere Packings, Lattices and Groups. </title> <address> Springer-Verlag,New York, </address> <year> 1988. </year>
Reference-contexts: Since the likelihood is Gaussian N (c 0 ; ), the Fisher Information matrix in this case has two diagonal entries and is , n 2 4 . There are two parameters so d = 2 and 2 = 5 36 3 <ref> [2] </ref>.

Reference: [4] <author> Mengxiang Li. </author> <title> Minimum description length based 2-d shape description. </title> <booktitle> In IEEE 4th Int. Conf. on Computer Vision, </booktitle> <pages> pages 512-517, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This is the case where no penalty term is applied, such as in CART [1]. * MML, using Equations (2) and (15) of this paper. * AIC, using log f (xj) + k [5]. * BIC, using log f (xj) + k 2 log n <ref> [4] </ref>. * QUI, using log f (xj) + log n. We note that its use by Quinlan is in a slightly different context to here, but believe the rationale provided for its use still holds [7]. * DOM, using log f (xj) + log d .
Reference: [5] <author> Z. Liang et al. </author> <title> Parameter estimation of finite mixtures using the EM algorithm and information criteria with applications to medical image processing. </title> <journal> IEEE Trans. on Nuclear Science, </journal> <volume> 39(4) </volume> <pages> 1126-1133, </pages> <year> 1992. </year>
Reference-contexts: Is there a cut-point or not? * maximum likelihood (MaxLik). This is the case where no penalty term is applied, such as in CART [1]. * MML, using Equations (2) and (15) of this paper. * AIC, using log f (xj) + k <ref> [5] </ref>. * BIC, using log f (xj) + k 2 log n [4]. * QUI, using log f (xj) + log n.
Reference: [6] <author> J.J. Oliver, Baxter R.A., and Wallace C.S. </author> <title> Unsupervised Learning using MML. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: We choose a noninformative prior on these, based on the population variance of y i <ref> [6] </ref>. Since the likelihood is Gaussian N (c 0 ; ), the Fisher Information matrix in this case has two diagonal entries and is , n 2 4 . There are two parameters so d = 2 and 2 = 5 36 3 [2].
Reference: [7] <author> J.R. Quinlan. </author> <title> Improved use of continuous attributes in c4.5. </title> <journal> Journal of Artificial Intelligence, </journal> <volume> 4 </volume> <pages> 77-90, </pages> <year> 1996. </year>
Reference-contexts: We note that its use by Quinlan is in a slightly different context to here, but believe the rationale provided for its use still holds <ref> [7] </ref>. * DOM, using log f (xj) + log d . We note that Dom used this penalty measure in the different, but related, context of segmenting binary strings and that our use of it here is not meant to imply that Dom would advocate its use here [3].

References-found: 6

