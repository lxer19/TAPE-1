URL: http://www.medg.lcs.mit.edu/ftp/doyle/aaai91.ps
Refering-URL: http://www.medg.lcs.mit.edu/ftp/doyle/
Root-URL: 
Email: wellman@wrdc.af.mil  doyle@zermatt.lcs.mit.edu  
Title: Preferential Semantics for Goals When both a perfectly predictable or deterministic environment, the situation in
Author: Michael P. Wellman Jon Doyle and 
Keyword: Preferences and utility  
Address: WL/AAA-1 Wright-Patterson AFB, OH 45433  545 Technology Square Cambridge, MA 02139  
Affiliation: Wright Laboratory AI Office  MIT Lab for Computer Science  
Note: Reprinted from Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), July, 1991, pp. 698-703.  Planning to achieve goals Jon Doyle is supported by the USAF Rome Laboratory and DARPA under contract F30602-91-C-0018.  over called the preference order. When that is, 0 is preferred to 0 if and only if (iff) 0 but 0 6  In  
Abstract: Goals, as typically conceived in AI planning, provide an insufficient basis for choice of action, and hence are deficient as the sole expression of an agent's objectives. Decision-theoretic utilities offer a more adequate basis, yet lack many of the computational advantages of goals. We provide a preferential semantics for goals that grounds them in decision theory and preserves the validity of some, but not all, common goal operations performed in planning. This semantic account provides a criterion for verifying the design of goal-based planning strategies, thus providing a new framework for knowledge-level analysis of planning systems. In the predominant AI planning paradigm, planners construct plans designed to produce states satisfying particular conditions called goals. Each goal represents a partition of possible states of the world into those satisfying and those not satisfying the goal. Though planners use goals to guide their reasoning, the crude binary distinctions defined by goals provide no basis for choosing among alternative plans that ensure achievement of goals, and no guidance whatever when no such plans can be found. These lacunae pose significant problems for planning in all realistic situations, where actions have uncertain effects or objectives can be partially satisfied. To overcome these widely-recognized expressive limitations of goals, many AI planners make ad hoc use of heuristic evaluation functions. These augment the guidance provided by goals, but lack the semantic justification needed to evaluate their true efficacy. We believe that heuristic evaluation functions should not be viewed as mere second-order refinements on the primary goal-based representation of objectives, supporting a separate "optimizing" phase of planning. Our thesis is that relative preference over the possible results of a plan constitutes the fundamental concept underlying the objectives of planning, with goals serv ing as a computationally useful heuristic approximation to these preferences (Doyle, 1990). Our purpose here is to provide a formal semantics for goals in terms of decision-theoretic preferences that supports rational justifications for planning principles. The grounding in decision theory enables designers to determine whether their planning systems act rationally in accord with their goals, and provides a principled basis for integrating goals with other types of preference information. We begin by summarizing some basic concepts of preference. We then develop formal decision-theoretic semantics for goals and examine some standard planning operations in light of the semantics. We conclude by discussing some related work and offering some directions for future investigation. Decision theory starts with the notion of preferences over outcomes (Keeney and Raiffa, 1976; Savage, 1972). Outcomes represent the possible consequences of the agent's decisions. In the planning context, an outcome might be taken to be the state resulting from execution of a plan, or perhaps the entire history of instantaneous states over the lifetime of the agent. To provide an adequate basis for decision, the set of possible outcomes must distinguish all consequences that the agent cares about and are possibly affected by its actions. We represent the agent's preferences by a total preorder (a complete, reflexive, and transitive relation) ! 0 we say that ! is weakly preferred to ! 0 , which means that the former outcome is at least as desirable as the latter. The strict preference order consists of the irreflexive part of ~ !, we say the two outcomes are indifferent, and write ! ~ ! 0 . Decision theory postulates that rational agents make choices so that the chosen alternatives are maximally preferred among those available. In planning, agents choose among courses of action, or plans.
Abstract-found: 1
Intro-found: 0
Reference: <author> Cohen, Philip R. and Levesque, Hector J. </author> <year> 1990. </year> <title> Intention is choice with commitment. </title> <booktitle> Artificial Intelligence 42 </booktitle> <pages> 213-261. </pages>
Reference: <author> Dean, Thomas and Wellman, Michael P. </author> <year> 1989. </year> <title> On the value of goals. In Proceedings from the Rochester Planning Workshop. </title> <note> Published as University of Rochester TR 284. </note>
Reference: <author> Doyle, </author> <title> Jon 1980. A model for deliberation, action, and introspection. </title> <type> AI-TR 581, </type> <institution> Massachusetts Institute of Technology, Artificial Intelligence Laboratory, 545 Technology Square, </institution> <address> Cambridge, MA, </address> <month> 02139. </month>
Reference: <author> Doyle, </author> <title> Jon 1990. Rationality and its roles in reasoning (extended abstract). </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> Menlo Park, CA. </address> <publisher> AAAI Press. </publisher> <pages> 1093-1100. </pages>
Reference: <author> Farquhar, Peter H. </author> <year> 1987. </year> <note> Applications of utility theory in artificial intelligence research. In Sawaragi, </note> <author> Y.; Inoue, K.; and Nakayama, H., </author> <title> editors, Toward Interactive and Intelligent Decision Support Systems, </title> <booktitle> Volume 2, Volume 286 of Lecture notes in economics and mathematical systems. </booktitle> <publisher> Springer-Verlag. </publisher> <pages> 155-161. </pages>
Reference: <author> Feldman, Jerome A. and Sproull, Robert F. </author> <year> 1977. </year> <booktitle> Decision Theory and Artificial Intelligence II: The hungry monkey. Cognitive Science 1 </booktitle> <pages> 158-192. </pages>
Reference: <author> Fishburn, Peter C. and Vickson, Raymond G. </author> <year> 1978. </year> <title> Theoretical foundations of stochastic dominance. </title> <editor> In Whitmore, G. A. and Findlay, M. C., editors, </editor> <title> Stochastic Dominance: An Approach to Decision Making Under Risk. </title> <editor> D. C. Heath and Company, </editor> <address> Lexington, MA. </address>
Reference: <author> Good, I. J. </author> <year> 1962. </year> <title> A five-year plan for automatic chess. </title> <editor> In Dale, E. and Michie, D., editors, </editor> <booktitle> Machine Intelligence 2. </booktitle> <publisher> Oliver and Boyd, London. </publisher> <pages> 89-118. </pages>
Reference: <author> Gorman, W. M. </author> <year> 1968. </year> <title> The structure of utility functions. </title> <journal> Review of Economic Studies 35 </journal> <pages> 367-390. </pages>
Reference: <author> Haddawy, Peter and Hanks, </author> <title> Steve 1990. Issues in decision-theoretic planning: Symbolic goals and numeric utilities. </title> <booktitle> In Proceedings of the DARPA Workshop on Innovative Approaches to Planning, Scheduling, and Control. </booktitle> <pages> 48-58. </pages>
Reference: <author> Keeney, Ralph L. </author> <year> 1981. </year> <title> Analysis of preference dependencies among objectives. </title> <journal> Operations Research 29 </journal> <pages> 1105-1120. </pages>
Reference: <author> Keeney, Ralph L. and Raiffa, </author> <title> Howard 1976. Decisions with Multiple Objectives: Preferences and Value Tradeoffs. </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference: <author> Loui, </author> <title> Ronald 1990. Defeasible specification of utilities. </title> <editor> In Kyburg, Henry E. Jr.; Loui, Ronald P.; and Carlson, Greg N., editors, </editor> <title> Knowledge Representation and Defeasible Reasoning. </title> <publisher> Kluwer Academic Publishers. </publisher> <pages> 345-359. </pages>
Reference: <author> Newell, </author> <title> Allen 1982. The knowledge level. </title> <booktitle> Artificial Intelligence 18(1) </booktitle> <pages> 87-127. </pages>
Reference: <author> Savage, Leonard J. </author> <year> 1972. </year> <title> The Foundations of Statistics. </title> <publisher> Dover Publications, </publisher> <address> New York, </address> <note> second edition. </note>
Reference: <author> Simon, Herbert A. </author> <year> 1955. </year> <title> A behavioral model of rational choice. </title> <journal> Quarterly Journal of Economics 69 </journal> <pages> 99-118. </pages>
Reference: <author> Wellman, Michael P. </author> <year> 1990a. </year> <title> Formulation of Tradeoffs in Planning Under Uncertainty. </title> <publisher> Pitman and Morgan Kaufmann. </publisher>
Reference: <author> Wellman, Michael P. </author> <year> 1990b. </year> <title> The strips assumption for planning under uncertainty. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA. </address> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference: <author> Wellman, Michael P.; Doyle, Jon; and Dean, </author> <title> Thomas 1991. Goals, preferences, and utilities: A reconciliation. </title> <note> In preparation. 703 </note>
References-found: 19

