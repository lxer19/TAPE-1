URL: http://www.cs.iastate.edu/~honavar/Papers/ijcnn98.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/aigroup2.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fparekhjhonavarg@cs.iastate.edu  
Title: Constructive Theory Refinement in Knowledge Based Neural Networks  
Author: Rajesh Parekh Vasant Honavar 
Address: 226 Atanasoff Hall  Ames IA 50011. U.S.A.  
Affiliation: Artificial Intelligence Research Group Department of Computer Science  Iowa State University  
Abstract: Knowledge based artificial neural networks offer an approach for connectionist theory refinement. We present an algorithm for refining and extending the domain theory incorporated in a knowledge based neural network using constructive neural network learning algorithms. The initial domain theory comprising of propositional rules is translated into a knowledge based network of threshold logic units (TLU). The domain theory is modified by dynamically adding neurons to the existing network. A constructive neural network learning algorithm is used to add and train these additional neurons using a sequence of labeled examples. We propose a novel hybrid constructive learning algorithm based on the Tiling and Pyramid constructive learning algorithms that allows knowledge based neural network to handle patterns with continuous valued attributes. Results of experiments on two non-trivial tasks (the ribosome binding site prediction and the financial advisor) show that our algorithm compares favorably with other algorithms for connectionist theory refinement both in terms of generalization accuracy and network size. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Ourston and R. J. Mooney, </author> <title> "Theory refinement: Combining analytical and empirical methods," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 66, </volume> <pages> pp. 273-310, </pages> <year> 1994. </year>
Reference: [2] <author> K. Thompson, P. Langley, and W. Iba, </author> <title> "Using background knowledge in concept formation," </title> <booktitle> in Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <year> 1991, </year> <pages> pp. 554-558. </pages>
Reference: [3] <author> S. Muggleton, </author> <title> Inductive Logic Programming, </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1992. </year>
Reference: [4] <author> G. G. Towell, J. W. Shavlik, and M. O. Noordwier, </author> <title> "Refinement of approximate domain theories by knowledge-based neural networks," </title> <booktitle> in Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA, </address> <year> 1990, </year> <pages> pp. 861-866. </pages>
Reference-contexts: Constructive Knowledge based Neural Network Learning Algorithm A. Embedding the Domain Theory in the Neural Network The symbolic knowledge encoding procedure translates a domain theory in the form of propositional rules into a network of TLU <ref> [4] </ref>, [12]. The weights of the individual TLU are chosen to satisfy the rules.
Reference: [5] <author> L. M. Fu, </author> <title> "Integration of neural heuristics into knowledge-based inference," </title> <journal> Connection Science, </journal> <volume> vol. 1, </volume> <pages> pp. 325-340, </pages> <year> 1989. </year>
Reference: [6] <author> L. M. Fu, </author> <title> "Knowledge based connectionism for refining domain theories," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 23, no. 1, </volume> <year> 1993. </year>
Reference-contexts: Further, since the original rules are left uncorrupted in our approach, the comprehensibility of rules extracted from the trained network is likely to improve significantly. Some results in rule extraction from trained neural networks are reported in [20], <ref> [6] </ref>. TopGen heuristically finds effective places to add nodes to the knowledge bases. The idea of training new neurons to compensate for errors made by existing ones is presented in the Upstart learning algorithm [21].
Reference: [7] <author> G. Towell and J. Shavlik, </author> <title> "Knowledge-based artificial neural networks," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 70, no. </volume> <pages> 1-2, pp. 119-165, </pages> <year> 1994. </year>
Reference: [8] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Processing: Explorations into the Microstructure of Cognition, </title> <booktitle> vol. 1 (Foundations). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1986. </year>
Reference: [9] <author> V. Honavar, </author> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks, </title> <type> Ph.D. thesis, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1990. </year> <month> 6 </month>
Reference: [10] <author> V. Honavar and L. Uhr, </author> <title> "Generative learning structures and processes for connectionist networks," </title> <journal> Information Sciences, </journal> <volume> vol. 70, </volume> <pages> pp. 75-108, </pages> <year> 1993. </year>
Reference: [11] <author> R. G. Parekh, J. Yang, and V. G. Honavar, </author> <title> "Constructive neural network learning algorithms for multi-category real-valued pattern classification," </title> <type> Tech. Rep. </type> <institution> ISU-CS-TR97-06, Department of Computer Science, Iowa State University, </institution> <year> 1997, </year> <note> (Submitted for review to the IEEE Transactions on Neural Networks). </note>
Reference-contexts: B. Constructive Neural Network Learning Algorithm A constructive neural network learning algorithm is used to augment the initial network topology. Our approach allows the use of any of the commonly used constructive neural network learning algorithms <ref> [11] </ref>. For the purpose of these experiments we have used a novel hybrid algorithm that combines the features of the Tiling [17] and the Pyramid [13] learning algorithms. The Tiling algorithm constructs a strictly layered network of TLUs. <p> The new layer is similarly trained until it achieves a faithful representation of the patterns. The convergence of this algorithm is demonstrated by showing that each successive master neuron reduces the number of mis-classifications in the training set by at least one. The interested reader is referred to [18], <ref> [11] </ref> for a detailed description of the Tiling algorithm and its extensions to handle pattern sets with real valued attributes and multiple output categories. The Pyramid algorithm successively adds new layers of TLUs to the network. Each newly added layer becomes the network's new output layer. <p> The extension of the Pyramid algorithm to handle real valued attributes requires a pre-processing of the pattern set. The individual patterns could either be normalized or be projected to a parabolic surface to guarantee convergence <ref> [11] </ref>. Alternatively, the continuous valued features could be dis-cretized.
Reference: [12] <author> J. Fletcher and Z. Obradovic, </author> <title> "Combining prior symbolic knowledge and constructive neural netwo rk learning," </title> <journal> Connection Science, </journal> <volume> vol. 5, no. 3,4, </volume> <pages> pp. 365-375, </pages> <year> 1993. </year>
Reference-contexts: Constructive Knowledge based Neural Network Learning Algorithm A. Embedding the Domain Theory in the Neural Network The symbolic knowledge encoding procedure translates a domain theory in the form of propositional rules into a network of TLU [4], <ref> [12] </ref>. The weights of the individual TLU are chosen to satisfy the rules. <p> The financial advisor rule base is shown in Table I. A set of 5500 patterns (500 for training and 5000 for testing) that correctly match the financial advisor rule base are randomly generated as is the case for the experiments performed by Fletcher and Obradovic <ref> [12] </ref>. We used the above hybrid Tiling-Pyramid constructive learning algorithm to augment the initial domain knowledge. The hybrid network was trained using the Thermal Perceptron learning rule [19]. <p> The incomplete domain theory was then augmented using constructive learning. Our experiments follow those performed by Fletcher and Obradovic <ref> [12] </ref>. In Table III we summarize the average generalization (on the 5000 test patterns) and the average network size over 25 runs for 6 different pruning points. The generalization accuracy of the corresponding network prior to the theory refinement (i.e., based on rules alone) is also reported. <p> In Table IV we present the results for the experiments with HDE 3 <ref> [12] </ref> 3 Note that the standard deviations for the results with these experiments were not available. 5 Pruning point Tiling-Pyramid Rules alone Test % Size Test % dep sav adeq 92.1 2.03 23.3 3.71 52.4 assets hi 98.7 0.42 10 0.0 99.4 dep inc adeq 87 2.39 30.2 3.57 67.4 debt
Reference: [13] <author> S. Gallant, </author> <title> "Perceptron based learning algorithms," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 179-191, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Our approach allows the use of any of the commonly used constructive neural network learning algorithms [11]. For the purpose of these experiments we have used a novel hybrid algorithm that combines the features of the Tiling [17] and the Pyramid <ref> [13] </ref> learning algorithms. The Tiling algorithm constructs a strictly layered network of TLUs. Each layer maintains a master neuron that is responsible for classifying the training patterns.
Reference: [14] <author> D. W. Opitz and J. W. Shavlik, </author> <title> "Dynamically adding symbolically meaningful nodes to knowledge-based neural networks," </title> <journal> Knowledge-Based Systems, </journal> <volume> vol. 8, no. 6, </volume> <pages> pp. 301-311, </pages> <year> 1995. </year>
Reference: [15] <author> D. W. Opitz and J. W. Shavlik, </author> <title> "Connectionist theory refinement: Genetically searching the space of network topologies," </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> vol. 6, </volume> <pages> pp. 177-209, </pages> <year> 1997. </year>
Reference-contexts: As can be seen, theory refinement does provide an improvement in generalization accuracy. We compare these results with the results for TopGen and REGENT reported in <ref> [15] </ref>. 2 Note that the standard deviations were not available for experiments with TopGen and REGENT.
Reference: [16] <author> G. F. Luger and W. A. Stubblefield, </author> <booktitle> Artificial Intelligence and the Design of Expert Systems, </booktitle> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1989. </year>
Reference-contexts: The weights of the individual TLU are chosen to satisfy the rules. For example, consider the simple financial advisor rule base (due to <ref> [16] </ref>) in Table I. if (sav adeq and inc adeq) then invest stocks if dep sav adeq then sav adeq if assets hi then sav adeq if (dep inc adeq and earn steady) then inc adeq if debt lo then inc adeq if (sav dep * 5000) then dep sav adeq
Reference: [17] <author> M. Mezard and J. Nadal, </author> <title> "Learning feed-forward networks: The tiling algorithm," </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> vol. 22, </volume> <pages> pp. 2191-2203, </pages> <year> 1989. </year>
Reference-contexts: Our approach allows the use of any of the commonly used constructive neural network learning algorithms [11]. For the purpose of these experiments we have used a novel hybrid algorithm that combines the features of the Tiling <ref> [17] </ref> and the Pyramid [13] learning algorithms. The Tiling algorithm constructs a strictly layered network of TLUs. Each layer maintains a master neuron that is responsible for classifying the training patterns.
Reference: [18] <author> J. Yang, R. Parekh, and V. Honavar, </author> <title> "MTiling a constructive neural network learning algorithm for multi-category pattern classification," </title> <booktitle> in Proceedings of the World Congress on Neural Networks'96, </booktitle> <address> San Diego, </address> <year> 1996, </year> <pages> pp. 182-187. </pages>
Reference-contexts: The new layer is similarly trained until it achieves a faithful representation of the patterns. The convergence of this algorithm is demonstrated by showing that each successive master neuron reduces the number of mis-classifications in the training set by at least one. The interested reader is referred to <ref> [18] </ref>, [11] for a detailed description of the Tiling algorithm and its extensions to handle pattern sets with real valued attributes and multiple output categories. The Pyramid algorithm successively adds new layers of TLUs to the network. Each newly added layer becomes the network's new output layer.
Reference: [19] <author> M. Frean, </author> <title> "A thermal perceptron learning rule," </title> <journal> Neural Computation, </journal> <volume> vol. 4, </volume> <pages> pp. 946-957, </pages> <year> 1992. </year>
Reference-contexts: We used the above hybrid Tiling-Pyramid constructive learning algorithm to augment the initial domain knowledge. The hybrid network was trained using the Thermal Perceptron learning rule <ref> [19] </ref>. Each TLU was trained for 1000 epochs with the initial weights chosen randomly between 1 and 1 and the initial temperature for the thermal perceptron (T 0 ) set to 10.0. Ribosome Binding Sites: The ribosome dataset contains an imperfect domain theory.
Reference: [20] <author> G. Towell and J. Shavlik, </author> <title> "Extracting rules from knowledge-based neural networks," </title> <journal> Machine Learning, </journal> <volume> vol. 13, </volume> <pages> pp. 71-101, </pages> <year> 1993. </year>
Reference-contexts: Further, since the original rules are left uncorrupted in our approach, the comprehensibility of rules extracted from the trained network is likely to improve significantly. Some results in rule extraction from trained neural networks are reported in <ref> [20] </ref>, [6]. TopGen heuristically finds effective places to add nodes to the knowledge bases. The idea of training new neurons to compensate for errors made by existing ones is presented in the Upstart learning algorithm [21].
Reference: [21] <author> M. Frean, </author> <title> "The upstart algorithm: A method for constructing and training feedforward neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 2, </volume> <pages> pp. 198-209, </pages> <year> 1990. </year>
Reference-contexts: Some results in rule extraction from trained neural networks are reported in [20], [6]. TopGen heuristically finds effective places to add nodes to the knowledge bases. The idea of training new neurons to compensate for errors made by existing ones is presented in the Upstart learning algorithm <ref> [21] </ref>. It is of interest to use a similar constructive learning approach in conjunction with our current approach. This would serve the twin objectives of refining inaccuracies in the domain theory and also adding new rules to the original theory.
References-found: 21

