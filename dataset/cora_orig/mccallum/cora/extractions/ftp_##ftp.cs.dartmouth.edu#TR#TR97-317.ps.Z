URL: ftp://ftp.cs.dartmouth.edu/TR/TR97-317.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR97-317/
Root-URL: http://www.cs.dartmouth.edu
Email: thc@cs.dartmouth.edu  
Title: Performing BMMC Permutations Efficiently on Distributed-Memory Multiprocessors with MPI  
Author: Thomas H. Cormen 
Affiliation: Dartmouth College Computer Science  Dartmouth College Department of Computer Science  
Pubnum: Technical Report PCS-TR97-317  
Abstract: This paper presents an architecture-independent method for performing BMMC permutations on multiprocessors with distributed memory. All interprocessor communication uses the MPI function MPI_Sendrecv_replace(). The number of elements and number of processors must be powers of 2, with at least one element per processor, and there is no inherent upper bound on the ratio of elements per processor. Our method transmits only data without transmitting any source or target indices, which conserves network bandwidth. When data is transmitted, the source and target processors implicitly agree on each other's identity and the indices of the elements being transmitted. A C-callable implementation of our method is available from Netlib. The implementation allows preprocessing (which incurs a modest cost) to be factored out for multiple runs of the same permutation, even if on different data. Data may be laid out in any one of several ways: processor-major, processor-minor, or anything in between.
Abstract-found: 1
Intro-found: 1
Reference: [BR90] <author> Rajendra Boppana and C. S. Raghavendra. </author> <title> Optimal self routing of linear-complement permutations in hypercubes. </title> <booktitle> In Proceedings of the 5th Distributed Memory Conference, </booktitle> <pages> pages 800-808, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: To our knowledge, this paper represents the first BMMC-permutation algorithm that is independent of the network architecture. Other authors have shown how to perform BMMC permutations (which are also known as affine transformations) on specific networks such as hypercubes <ref> [BR90, EHJ94] </ref>, meshes [Sib92], Omega networks 1 [KS88], and expanded delta networks [WCS96]. Many of the techniques used in the present paper are adapted from earlier work in performing BMMC permutations on parallel disk systems [CSW94]. The remainder of this paper is organized as follows.
Reference: [CB95] <author> Thomas H. Cormen and Kristin Bruhl. </author> <title> Don't be too clever: Routing BMMC permutations on the MasPar MP-2. </title> <booktitle> In Proceedings of the 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 288-297, </pages> <month> July </month> <year> 1995. </year> <note> Revised version to appear in Theory of Computing Systems. </note>
Reference: [CLR90] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year> <note> 4 Available at http://www.mcs.anl.gov/mpi/mpich/. 14 </note>
Reference: [Cor97] <author> Thomas H. Cormen. libbmmc_mpi.a: </author> <title> a library to perform fast BMMC permutations for any multiprocessor system that supports MPI. </title> <note> Available from Netlib at http:// www.netlib.org/mpi/contrib/bmmc.tar.gz, 1997. </note>
Reference-contexts: By default, the method assumes that data is laid out in processor-major order, but we shall see that we can compensate for other orders by performing a modified BMMC permutation. A C-callable implementation of our method is available from Netlib <ref> [Cor97] </ref>. The implementation allows the computation of the decomposition (which incurs a modest cost) to be factored out for multiple runs of the same permutation, even if on different data. To our knowledge, this paper represents the first BMMC-permutation algorithm that is independent of the network architecture. <p> Applying Lemma 1, instead of performing these three permutations consecutively, we perform their composition. That is, we perform the BMMC permutation with characteristic matrix Q A Q 1 and complement vector Q c. 7 Conclusion This paper has detailed the algorithm behind the software package libbmmc_mpi.a <ref> [Cor97] </ref>, which is available from Netlib. Although the factoring techniques in Section 4 are borrowed from the out-of-core BMMC permutation algorithm [CSW94], the application of these techniques in Section 5 is new. There are several ways to invoke the C-callable implementation in libbmmc_mpi.a.
Reference: [CSW94] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1994. </year> <note> Preliminary version appeared in Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures. Revised version to appear in SIAM Journal on Computing. </note>
Reference-contexts: Many of the techniques used in the present paper are adapted from earlier work in performing BMMC permutations on parallel disk systems <ref> [CSW94] </ref>. The remainder of this paper is organized as follows. Section 2 defines the class of BMMC permutations, shows how we represent them, and gives the technical conditions required to use our method. <p> Each layout can be viewed as consisting of bands of 2 f P elements. 3 Matrix forms When factoring a characteristic matrix in Section 4, we will want certain submatrices to have particular forms. This section reviews a technique used in <ref> [CSW94] </ref> to produce such desired forms. Column-addition matrices A column-addition matrix is a matrix M such that the product A 0 = A M is a modified form of A in which specified columns of A have been added (elementwise) into others. <p> In other words, if m ij = 1, then m jk = 0 for all k 6= j. With the dependency restriction, any column-addition matrix is nonsingular <ref> [CSW94] </ref>. In the remainder of this section, we present three matrix forms that we will use for factoring in Section 4. The matrices that we will use are of block forms. <p> Consider a given processor k, where 0 k P 1. How many different processors do the elements that start on k map to when performing the permutation characterized by the original matrix A? The answer is given by the following lemma, whose proof appears in <ref> [CSW94] </ref>. 3 Lemma 3 Let fl be the lower left p fi (n p) submatrix of A, and consider any processor k. <p> There are exactly 2 rank fl target processors that the elements of k map to, and for each such target processor, exactly N=(2 rank fl P ) elements from processor k map to it. 3 The lemma in <ref> [CSW94] </ref> is couched in terms of blocks on a parallel disk system, but it translates easily to the context of the present paper. 9 Since W characterizes an intraprocessor permutation, any movement among processors must occur during the permutation characterized by V . <p> Although the factoring techniques in Section 4 are borrowed from the out-of-core BMMC permutation algorithm <ref> [CSW94] </ref>, the application of these techniques in Section 5 is new. There are several ways to invoke the C-callable implementation in libbmmc_mpi.a.
Reference: [EHJ94] <author> Alan Edelman, Steve Heller, and S. Lennart Johnsson. </author> <title> Index transformation algorithms in a linear algebra framework. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(12) </volume> <pages> 1302-1309, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: To our knowledge, this paper represents the first BMMC-permutation algorithm that is independent of the network architecture. Other authors have shown how to perform BMMC permutations (which are also known as affine transformations) on specific networks such as hypercubes <ref> [BR90, EHJ94] </ref>, meshes [Sib92], Omega networks 1 [KS88], and expanded delta networks [WCS96]. Many of the techniques used in the present paper are adapted from earlier work in performing BMMC permutations on parallel disk systems [CSW94]. The remainder of this paper is organized as follows.
Reference: [GLS94] <author> William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction Suppose you were writing a multiprocessor application using MPI <ref> [GLS94, SOHL + 96] </ref>, and you needed to transpose a fairly large matrix distributed over several processors. You could write a specialized matrix-transpose function. You might parameterize it enough to work on non-square matrices.
Reference: [KS88] <author> John Keohane and Richard E. Stearns. </author> <title> Routing linear permutations though the Omega network in two passes. </title> <booktitle> In Proceedings of the 2nd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 479-482, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: To our knowledge, this paper represents the first BMMC-permutation algorithm that is independent of the network architecture. Other authors have shown how to perform BMMC permutations (which are also known as affine transformations) on specific networks such as hypercubes [BR90, EHJ94], meshes [Sib92], Omega networks 1 <ref> [KS88] </ref>, and expanded delta networks [WCS96]. Many of the techniques used in the present paper are adapted from earlier work in performing BMMC permutations on parallel disk systems [CSW94]. The remainder of this paper is organized as follows. <p> Section 6 presents how to compensate for non-processor-major data layouts. Section 7 contains some concluding remarks, focusing on the implementation in Netlib. There are three appendices containing background material. Appendix A shows how to compute a 1 The algorithm in <ref> [KS88] </ref> is for BPC (bit-permute/complement) permutations, which are a large subclass of BMMC permutations. 2 column basis for a matrix, which we need to do in Section 4. Appendix B presents the Gray-code technique of calculating source and target indices, which we use in Section 5.
Reference: [Sib92] <author> Jop Frederik Sibeyn. </author> <title> Algorithms for Routing on Meshes. </title> <type> PhD thesis, </type> <institution> Utrecht University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: To our knowledge, this paper represents the first BMMC-permutation algorithm that is independent of the network architecture. Other authors have shown how to perform BMMC permutations (which are also known as affine transformations) on specific networks such as hypercubes [BR90, EHJ94], meshes <ref> [Sib92] </ref>, Omega networks 1 [KS88], and expanded delta networks [WCS96]. Many of the techniques used in the present paper are adapted from earlier work in performing BMMC permutations on parallel disk systems [CSW94]. The remainder of this paper is organized as follows.
Reference: [SOHL + 96] <author> Marc Snir, Steve W. Otto, Steven Huss-Lederman, David W. Walker, and Jack Don-garra. </author> <title> MPI: The Complete Reference. </title> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Suppose you were writing a multiprocessor application using MPI <ref> [GLS94, SOHL + 96] </ref>, and you needed to transpose a fairly large matrix distributed over several processors. You could write a specialized matrix-transpose function. You might parameterize it enough to work on non-square matrices.
Reference: [WCS96] <author> Leonard F. Wisniewski, Thomas H. Cormen, and Thomas Sundquist. </author> <title> Performing BMMC permutations in two passes through the expanded delta network and MasPar MP-2. </title> <booktitle> In Proceedings of the Sixth Symposium on The Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 282-289, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Other authors have shown how to perform BMMC permutations (which are also known as affine transformations) on specific networks such as hypercubes [BR90, EHJ94], meshes [Sib92], Omega networks 1 [KS88], and expanded delta networks <ref> [WCS96] </ref>. Many of the techniques used in the present paper are adapted from earlier work in performing BMMC permutations on parallel disk systems [CSW94]. The remainder of this paper is organized as follows.
References-found: 11

