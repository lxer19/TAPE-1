URL: ftp://ftp.cs.concordia.ca/pub/laks/papers/ilps94.ps.gz
Refering-URL: http://www.cs.concordia.ca/~faculty/laks/papers.html
Root-URL: http://www.cs.concordia.ca
Email: email: laks@cs.concordia.ca  email: sadri@uncg.edu  
Title: Probabilistic Deductive Databases  
Author: Laks V.S. Lakshmanan and Fereidoon Sadri 
Address: Montreal, Canada,  Greensboro Greensboro, NC, USA,  
Affiliation: Dept. of Computer Science, Concordia University  Dept. of Mathematics, University of North Carolina at  
Note: Appears in: ILPS'94.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Abiteboul, P. Kanellakis, and G. Grahne. </author> <title> On the representation and querying of sets of possible worlds. </title> <journal> Theoretical Computer Science, </journal> <volume> 78 </volume> <pages> 159-187, </pages> <year> 1991. </year>
Reference-contexts: In this paper, we are interested in an extension capable of handling probabilistic knowledge. Previous Work: Recent developments in DDBs (and logics in general) have led to frameworks capable of handling various forms of imperfection in knowledge. Abiteboul, et. al. <ref> [1] </ref>, Liu [16], and Dong and Lakshmanan [5] dealt with DDBs with incomplete information in the form of null values. Kifer and Lozinskii [13] have developed a logic for reasoning with inconsistency. <p> Contributions: We associate a confidence level with facts and rules (of a deductive database). A confidence level comes with both a belief and a doubt (in what is being asserted) [see Section 2 for a motivation]. Belief and doubt are subintervals of <ref> [0; 1] </ref> representing probability ranges. Confidence levels have an interesting algebraic structure called trilattices as their basis (Section 3). In addition to providing an algebraic footing for our framework, trilattices also shed light on the relationship between our work and earlier works and offer useful insights. <p> As usual, each rule is implicitly universally quantified outside the entire rule. Each rule is expressed in the form A h [ff; fi]; [fl; ffi]i &lt; Body, where ff; fi; fl; ffi 2 <ref> [0; 1] </ref>. We usually require that ff fi and fl ffi. With each rule, we have associated two intervals. [ff; fi] ([fl; ffi]) is the belief (doubt) the expert has in the rule. <p> In this section, we shall see that a notion of trilattices naturally arises with confidence levels. We shall establish the structure and properties of trilattices here, which will be used in later sections. Definition 3.1 Denote by C <ref> [0; 1] </ref> the set of all closed subintervals over [0; 1]. Consider the set L c = def C [0; 1] fi C [0; 1]. We denote the elements of L c as h [ff; fi]; [fl; ffi]i. Define the following orders on this set. <p> In this section, we shall see that a notion of trilattices naturally arises with confidence levels. We shall establish the structure and properties of trilattices here, which will be used in later sections. Definition 3.1 Denote by C <ref> [0; 1] </ref> the set of all closed subintervals over [0; 1]. Consider the set L c = def C [0; 1] fi C [0; 1]. We denote the elements of L c as h [ff; fi]; [fl; ffi]i. Define the following orders on this set. <p> We shall establish the structure and properties of trilattices here, which will be used in later sections. Definition 3.1 Denote by C <ref> [0; 1] </ref> the set of all closed subintervals over [0; 1]. Consider the set L c = def C [0; 1] fi C [0; 1]. We denote the elements of L c as h [ff; fi]; [fl; ffi]i. Define the following orders on this set. <p> We shall establish the structure and properties of trilattices here, which will be used in later sections. Definition 3.1 Denote by C <ref> [0; 1] </ref> the set of all closed subintervals over [0; 1]. Consider the set L c = def C [0; 1] fi C [0; 1]. We denote the elements of L c as h [ff; fi]; [fl; ffi]i. Define the following orders on this set. <p> The top and bottom elements w.r.t. the various orders are as follows. The subscripts indicate the associated orders, as usual. &gt; t = h <ref> [1; 1] </ref>; [0; 0]i, ? t = h [0; 0]; [1; 1]i, &gt; p = h [1; 0]; [1; 0]i, ? p = h [0; 1]; [0; 1]i. &gt; t corresponds to total belief and no doubt; ? t is the opposite. &gt; k represents maximal information (total belief and doubt), <p> The top and bottom elements w.r.t. the various orders are as follows. The subscripts indicate the associated orders, as usual. &gt; t = h <ref> [1; 1] </ref>; [0; 0]i, ? t = h [0; 0]; [1; 1]i, &gt; p = h [1; 0]; [1; 0]i, ? p = h [0; 1]; [0; 1]i. &gt; t corresponds to total belief and no doubt; ? t is the opposite. &gt; k represents maximal information (total belief and doubt), to the point of being probabilistically inconsistent: belief and doubt <p> The top and bottom elements w.r.t. the various orders are as follows. The subscripts indicate the associated orders, as usual. &gt; t = h [1; 1]; [0; 0]i, ? t = h [0; 0]; [1; 1]i, &gt; p = h <ref> [1; 0] </ref>; [1; 0]i, ? p = h [0; 1]; [0; 1]i. &gt; t corresponds to total belief and no doubt; ? t is the opposite. &gt; k represents maximal information (total belief and doubt), to the point of being probabilistically inconsistent: belief and doubt probabilities sum to more than 1; <p> The top and bottom elements w.r.t. the various orders are as follows. The subscripts indicate the associated orders, as usual. &gt; t = h [1; 1]; [0; 0]i, ? t = h [0; 0]; [1; 1]i, &gt; p = h <ref> [1; 0] </ref>; [1; 0]i, ? p = h [0; 1]; [0; 1]i. &gt; t corresponds to total belief and no doubt; ? t is the opposite. &gt; k represents maximal information (total belief and doubt), to the point of being probabilistically inconsistent: belief and doubt probabilities sum to more than 1; ? k <p> The top and bottom elements w.r.t. the various orders are as follows. The subscripts indicate the associated orders, as usual. &gt; t = h [1; 1]; [0; 0]i, ? t = h [0; 0]; [1; 1]i, &gt; p = h [1; 0]; [1; 0]i, ? p = h <ref> [0; 1] </ref>; [0; 1]i. &gt; t corresponds to total belief and no doubt; ? t is the opposite. &gt; k represents maximal information (total belief and doubt), to the point of being probabilistically inconsistent: belief and doubt probabilities sum to more than 1; ? k gives the least information: no basis <p> The subscripts indicate the associated orders, as usual. &gt; t = h [1; 1]; [0; 0]i, ? t = h [0; 0]; [1; 1]i, &gt; p = h [1; 0]; [1; 0]i, ? p = h <ref> [0; 1] </ref>; [0; 1]i. &gt; t corresponds to total belief and no doubt; ? t is the opposite. &gt; k represents maximal information (total belief and doubt), to the point of being probabilistically inconsistent: belief and doubt probabilities sum to more than 1; ? k gives the least information: no basis for belief <p> In fact, v 3 is the least valuation satisfying P . val B C A v 2 h <ref> [0:9; 1] </ref>; [0; 0]i h [0:9; 1]; [0; 0]i h [0:5; 0:7]; [0:1; 0:4]i Fixpoint Semantics: We associate an "immediate consequence" operator T P with a p-program P , defined as follows. Definition 5.2 Let P be a p-program and P fl its Herbrand instantiation. <p> In fact, v 3 is the least valuation satisfying P . val B C A v 2 h <ref> [0:9; 1] </ref>; [0; 0]i h [0:9; 1]; [0; 0]i h [0:5; 0:7]; [0:1; 0:4]i Fixpoint Semantics: We associate an "immediate consequence" operator T P with a p-program P , defined as follows. Definition 5.2 Let P be a p-program and P fl its Herbrand instantiation. <p> Confidences are associated with (finite) DPTs as follows. Definition 6.2 Let P be a p-program, G a goal, and T any finite DPT for G w.r.t. P . We associate confidences with the nodes of T as follows. 1. Each failure nodes gets the confidence h [0; 0]; <ref> [1; 1] </ref>i (see Section 3). Each success node labeled (r 0 ; ), where r 0 is an instance of rule r 2 P , and c is the confidence of rule r, gets the confidence c. 2. <p> First, let us examine the (only) "mode" for disjunction used by them 7 . They combine the confidences of an atom A coming from different derivations by taking their intersection. Indeed, the bottom of their lattice is a valuation (called "formula function" there) that assigns the interval <ref> [0; 1] </ref> to every atom. From the trilattice structure, it is clear that (i) their bottom corresponds to ? p , and (ii) their disjunctive mode corresponds to p . <p> This is quite unintuitive. Indeed, there is a definite path (with probability 1) corresponding to the edge e (1; 2). Suppose now rule r 3 is replaced by r 0 3 : e (1; 2) : <ref> [0; 1] </ref>. In this case, the least fixpoint of T N S P is only attained at ! and it assigns the range [0; 0] to p (1; 2). Again, the result is unintuitive for this example. <p> can easily write programs such that no reasonable approximation to lf p (T N S P ) can be obtained by iterating T N S P an arbitrary (finite) number of times. (E.g. , consider the program obtained by adding the rule r 5 : q (X; Y ) : <ref> [1; 1] </ref> p (X; Y ) : [0; 0] to fr 1 ; r 2 ; r 0 3 ; r 4 g.) Notice that as long as one uses any arithmetic annotation function such that the probability of the head is less than the probability of the subgoals of r <p> Also, associate the confidence level h <ref> [1; 1] </ref>; [0; 0]i with r 1 ; r 2 . For uniformity and ease of comparison, assume the doubt ranges are all [0; 0]. <p> As an example, let the conjunctive mode used in r 1 ; r 2 be independence and let the disjunctive mode used be positive correlation (or, in this case, even ignorance!). Then lf p (T P ) would assign the confidence h <ref> [1; 1] </ref>; [0; 0]i to p (1; 2), which agrees with our intuition. Consider the p-program corresponding to the rules fr 1 ; r 2 ; r 0 3 ; r 4 g, obtained as suggested above. <p> Suppose positive correlation (ignorance) is used for disjunction and independence for conjunction, as before. Then lf p (T P ) would assign the confidence level h <ref> [0; 1] </ref> [0; 0]i to p (1; 2). This again agrees with our intuition. As a last example, suppose we start with the confidence h [0; 0:1]; [0; 0]i for e (1; 2) instead. <p> Then under positive correlation (for disjunction) lf p (T P )(p (1; 2)) = h [0; 0:1]; [0; 0]i while ignorance leads to lf p (T P )(p (1; 2)) = h <ref> [0; 1] </ref>; [0; 0]i. The former makes more intuitive sense, although the latter (more conservative under p ) is obviously not wrong. Also, in the latter case, the lf p is reached only at !.
Reference: [2] <author> J.F. Baldwin. </author> <title> Evidential support logic programming. </title> <journal> Journal of Fuzzy Sets and Systems, </journal> <volume> 24 </volume> <pages> 1-26, </pages> <year> 1987. </year>
Reference-contexts: uncertainty in knowledge-bases employ one of the following formalisms: (1) a form of fuzzy logic (programming) (e.g. van Emden [20], Steger et. al. [19], and Fitting [8]), (2) annotated logic programming (e.g. see Kifer and Li [12] and Kifer and Subrahmanian [14]), (3) evidence theoretic logic programming (e.g. see Baldwin <ref> [2] </ref>), and (4) probabilistic logic programming (see below). Ng and Subrahmanian [17, 18] have recently proposed an interesting scheme for logic programming with uncertainty modeled using probabilities. <p> This shows we cannot regard the expert's doubt as the complement (w.r.t. 1) of his belief. Thus, if we have to model what necessarily follows according to the expert's knowledge, then we must carry both the belief and the doubt explicitly. Kifer and Li [12] and Baldwin <ref> [2] </ref> have argued that incorporating both belief and doubt (called disbelief there) is useful in dealing with incomplete knowledge, where different evidences may contradict each other. However, in their frameworks, doubt need not be maintained explicitly.
Reference: [3] <author> C. Barbara, H. Garcia-Molina, and D. Porter. </author> <title> A probabilistic relational data model. </title> <booktitle> In Proc. Advancing Database Technology EDBT 90, </booktitle> <pages> pages 60-64, </pages> <year> 1990. </year>
Reference-contexts: We need a model of our knowledge about the interaction between events. A simplistic model studied in the literature (e.g. , see Barbara et. al. <ref> [3] </ref>) assumes independence between all pairs of events. This is highly restrictive and is of limited applicability. A general model, studied by Ng and Subrahmanian [17, 18] is that of ignorance: assume no knowledge about event interaction.
Reference: [4] <author> R. Carnap. </author> <title> The Logical Foundations of Probability. </title> <publisher> University of Chicago Press, </publisher> <year> 1962. </year> <title> 2nd. </title> <type> Edn. </type>
Reference-contexts: The motivation for using probability theory as opposed to other formalisms for representing uncertainty has been discussed at length in the literature <ref> [4, 17] </ref>. Probability theory is perhaps the best understood and mathematically well-founded paradigm in which uncertainty can be modeled and reasoned about.
Reference: [5] <author> Fangqing Dong and Laks V. S. Lakshmanan. </author> <title> Deductive databases with incomplete information. </title> <booktitle> In Joint Int. Conf. and Symp. on Logic Programming, </booktitle> <address> Washington, D.C., </address> <month> Nov </month> <year> 1992. </year>
Reference-contexts: In this paper, we are interested in an extension capable of handling probabilistic knowledge. Previous Work: Recent developments in DDBs (and logics in general) have led to frameworks capable of handling various forms of imperfection in knowledge. Abiteboul, et. al. [1], Liu [16], and Dong and Lakshmanan <ref> [5] </ref> dealt with DDBs with incomplete information in the form of null values. Kifer and Lozinskii [13] have developed a logic for reasoning with inconsistency.
Reference: [6] <author> R. Fagin, J. Halpern, and N. Meggido. </author> <title> A logic for reasoning about probabilities. </title> <booktitle> Information and Computation, </booktitle> <year> 1992. </year>
Reference-contexts: Although they make use of a datalog-based interface to implement this calculus, they do not extend DDBs with the ability to handle uncertain facts and rules. Besides these works, a wealth of literature is available on probabilistic logic alone (e.g. see Carnap [4]and Fagin, Halpern, and Meggido <ref> [6] </ref>). As pointed out in [18] it is unclear how these can form a basis for probabilistic logic programming. An interested reader is referred to [18] for a detailed account of these and other works. Contributions: We associate a confidence level with facts and rules (of a deductive database).
Reference: [7] <author> J. E. Fenstad. </author> <title> The structure of probabilities defined on first-order languages. </title> <editor> In R. C. Jeffrey, editor, </editor> <booktitle> Studies in inductive logic and probabilities, </booktitle> <volume> volume 2, </volume> <pages> pages 251-262. </pages> <institution> University of California Press, </institution> <year> 1980. </year>
Reference-contexts: A second issue is whether we should insist on precise probabilities or allow intervals (or ranges). Firstly, probabilities derived from any sources may have tolerances associated with them. Even experts may feel more comfortable with specifying a range rather than a precise probability. Secondly, Fenstad <ref> [7] </ref> has shown (also see [17]) that when enough information is not available about the interaction between events, the probability of compound events cannot be determined precisely: one can only give (tight) bounds. Thus, we associate ranges of probabilities with facts and rules. A last issue is the following.
Reference: [8] <author> M. C. </author> <title> Fitting. Logic programming on a topological bilattice. </title> <journal> Fundamenta Informaticae, </journal> <volume> 11 </volume> <pages> 209-218, </pages> <year> 1988. </year>
Reference-contexts: Kifer and Lozinskii [13] have developed a logic for reasoning with inconsistency. Most of the works dealing with uncertainty in knowledge-bases employ one of the following formalisms: (1) a form of fuzzy logic (programming) (e.g. van Emden [20], Steger et. al. [19], and Fitting <ref> [8] </ref>), (2) annotated logic programming (e.g. see Kifer and Li [12] and Kifer and Subrahmanian [14]), (3) evidence theoretic logic programming (e.g. see Baldwin [2]), and (4) probabilistic logic programming (see below).
Reference: [9] <author> M. C. </author> <title> Fitting. Bilattices and the semantics of logic programming. </title> <journal> Journal of Logic Programming, </journal> <volume> 11 </volume> <pages> 91-116, </pages> <year> 1991. </year>
Reference-contexts: The difference with our framework, however, is that we model what is known definitely, as opposed to what is possible. This makes (in our case) an explicit treatment of belief and doubt mandatory. 3 The Algebra of Confidence Levels Fitting <ref> [9] </ref> has shown that bilattices (introduced by Ginsburg [10]) lead to an elegant framework for quantified logic programming involving both belief and doubt. In this section, we shall see that a notion of trilattices naturally arises with confidence levels. <p> Fitting <ref> [9] </ref> defines a bilattice to be interlaced whenever the meet and join w.r.t. any order of the bilattice are monotone w.r.t. the other order. He shows that it is the interlaced property of bilattices that makes them most useful and attractive.
Reference: [10] <author> M. Ginsburg. </author> <title> Multivalued logics: A uniform approach to reasoning in artificial intelligence. </title> <journal> Computational Intelligence, </journal> <volume> 4 </volume> <pages> 265-316, </pages> <year> 1988. </year>
Reference-contexts: The difference with our framework, however, is that we model what is known definitely, as opposed to what is possible. This makes (in our case) an explicit treatment of belief and doubt mandatory. 3 The Algebra of Confidence Levels Fitting [9] has shown that bilattices (introduced by Ginsburg <ref> [10] </ref>) lead to an elegant framework for quantified logic programming involving both belief and doubt. In this section, we shall see that a notion of trilattices naturally arises with confidence levels. We shall establish the structure and properties of trilattices here, which will be used in later sections.
Reference: [11] <author> U. Guntzer, W. Kieling, and H. Thone. </author> <title> New directions for uncertainty reasoning in deductive databases. </title> <booktitle> In Proc. ACM SIGMOD International Conf. on Management of Data, </booktitle> <pages> pages 178-187, </pages> <year> 1991. </year>
Reference-contexts: In [17] only annotations involving constant probability ranges were allowed. This was extended in [18] by allowing annotation variables and functions. They developed fixpoint and model-theoretic semantics, and provided a sound and weakly complete proof procedure. Guntzer et. al. <ref> [11] </ref> have proposed a sound (propositional) probabilistic calculus based on conditional probabilities, for reasoning in the presence of incomplete information. Although they make use of a datalog-based interface to implement this calculus, they do not extend DDBs with the ability to handle uncertain facts and rules.
Reference: [12] <author> M. Kifer and A. Li. </author> <title> On the semantics of rule-based expert systems with uncertainty. </title> <booktitle> In 2nd Intl. Conf. on Database Theory, </booktitle> <pages> pages 102-117, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Most of the works dealing with uncertainty in knowledge-bases employ one of the following formalisms: (1) a form of fuzzy logic (programming) (e.g. van Emden [20], Steger et. al. [19], and Fitting [8]), (2) annotated logic programming (e.g. see Kifer and Li <ref> [12] </ref> and Kifer and Subrahmanian [14]), (3) evidence theoretic logic programming (e.g. see Baldwin [2]), and (4) probabilistic logic programming (see below). Ng and Subrahmanian [17, 18] have recently proposed an interesting scheme for logic programming with uncertainty modeled using probabilities. <p> This shows we cannot regard the expert's doubt as the complement (w.r.t. 1) of his belief. Thus, if we have to model what necessarily follows according to the expert's knowledge, then we must carry both the belief and the doubt explicitly. Kifer and Li <ref> [12] </ref> and Baldwin [2] have argued that incorporating both belief and doubt (called disbelief there) is useful in dealing with incomplete knowledge, where different evidences may contradict each other. However, in their frameworks, doubt need not be maintained explicitly. <p> Finally, if we know that the data does not contain cycles, we can use any mode even with a recursive predicate and still have a polynomial time data complexity. In the full paper [15], we make a detailed comparison with non-probabilistic frameworks such as Kifer and Li <ref> [12] </ref> and Kifer and Sub-rahmanian [14]. In particular, the framework of annotation functions used 9 It is straightforward to show that the data complexity for the framework of [17] is polynomial, although the paper does not address this issue.
Reference: [13] <author> M. Kifer and E.L. Lozinskii. </author> <title> A logic for reasoning with inconsistency. </title> <booktitle> In Proc. 4th IEEE Symp. on Logic in Computer Science, </booktitle> <pages> pages 253-262, </pages> <address> Asilomar, CA, </address> <year> 1989. </year>
Reference-contexts: Previous Work: Recent developments in DDBs (and logics in general) have led to frameworks capable of handling various forms of imperfection in knowledge. Abiteboul, et. al. [1], Liu [16], and Dong and Lakshmanan [5] dealt with DDBs with incomplete information in the form of null values. Kifer and Lozinskii <ref> [13] </ref> have developed a logic for reasoning with inconsistency.
Reference: [14] <author> Michael Kifer and V. S. Subrahmanian. </author> <title> Theory of generalized annotated logic programming and its applications. </title> <journal> Journal of Logic Programming, </journal> <volume> 12 </volume> <pages> 335-367, </pages> <year> 1992. </year>
Reference-contexts: Most of the works dealing with uncertainty in knowledge-bases employ one of the following formalisms: (1) a form of fuzzy logic (programming) (e.g. van Emden [20], Steger et. al. [19], and Fitting [8]), (2) annotated logic programming (e.g. see Kifer and Li [12] and Kifer and Subrahmanian <ref> [14] </ref>), (3) evidence theoretic logic programming (e.g. see Baldwin [2]), and (4) probabilistic logic programming (see below). Ng and Subrahmanian [17, 18] have recently proposed an interesting scheme for logic programming with uncertainty modeled using probabilities. <p> rules in a DDB are van Emden's style of associating confidences with rules as a whole [20], or the annotation style of Kifer and Subrahmanian <ref> [14] </ref>.The first approach is more suited for truth-functional derivation of probabilities whereas the second is ideal for subjective derivations of probabilities. (It is shown in [14] that the second approach subsumes the first.) In this paper, our interest is in truth-functional 1 Other modes can be used (for conjunction/disjunction) in the "non-recursive part" of the program. derivation of probabilities, and we choose the first option. <p> In the full paper [15], we make a detailed comparison with non-probabilistic frameworks such as Kifer and Li [12] and Kifer and Sub-rahmanian <ref> [14] </ref>. In particular, the framework of annotation functions used 9 It is straightforward to show that the data complexity for the framework of [17] is polynomial, although the paper does not address this issue. However, that framework only allows constant annotations and is of limited expressive power. in [14] enables an <p> and Sub-rahmanian <ref> [14] </ref>. In particular, the framework of annotation functions used 9 It is straightforward to show that the data complexity for the framework of [17] is polynomial, although the paper does not address this issue. However, that framework only allows constant annotations and is of limited expressive power. in [14] enables an infinite family of modes to be used in propagating confi-dences from rule bodies to heads. The major differences with our work are (i) in [14] a fixed "mode" for disjunction is imposed unlike our framework, and (ii) they do not study the complexity of query answering, whereas we <p> However, that framework only allows constant annotations and is of limited expressive power. in <ref> [14] </ref> enables an infinite family of modes to be used in propagating confi-dences from rule bodies to heads. The major differences with our work are (i) in [14] a fixed "mode" for disjunction is imposed unlike our framework, and (ii) they do not study the complexity of query answering, whereas we establish the conditions under which the important advantage of polynomial time data complexity of classical datalog can be retained.
Reference: [15] <author> Laks V.S. Lakshmanan and Fereidoon Sadri. </author> <title> A framework for modeling uncertainty in databases and knowledge-bases. </title> <type> Technical report, </type> <institution> Concordia University, </institution> <address> Montreal, Quebec, </address> <year> 1993. </year>
Reference-contexts: We also compare our work with related work and bring out the advantages and generality of our approach (Section 7). Section 8 presents our conclusions and future research directions. Complete details of the framework presented here and the proofs can be found in <ref> [15] </ref>. 2 Motivation In this section, we discuss the motivation for our work as well as comment on our design decisions for this framework. The motivation for using probability theory as opposed to other formalisms for representing uncertainty has been discussed at length in the literature [4, 17]. <p> For a formula F , conf (F ) will denote its confidence level. In the following, we describe the various modes informally and state our results on the confidence levels of conjunction and disjunction under these modes. The complete details are discussed in <ref> [15] </ref>. 1. Ignorance: This is the most general situation possible: nothing is assumed/known about event interaction between F and G. The extent of the interaction between F and G could range from maximum overlap to minimum overlap. 2. Independence: This is a well-known mode. <p> The following theorem establishes the confidence levels of compound formulas as a function of those of the constituent formulas, under various modes. For brevity, we give the result for ignorance and positive correlation only. The reader is refereed to <ref> [15] </ref> for the complete details. Theorem 4.2 Let F and G be any events and let conf (F ) = h [ff 1 ; fi 1 ]; [fl 1 ; ffi 1 ]i and conf (G) = h [ff 2 ; fi 2 ]; [fl 2 ; ffi 2 ]i. <p> Then u gets the confidence c 1 _ p _ p c k . We have the following theorems. For technical details, the reader is referred to <ref> [15] </ref>. Theorem 6.1 (Soundness) Let P be a p-program and G a goal. If there is a DPT for G w.r.t. P with an associated confidence c at its root, then c t lf p (T P )(G). <p> Finally, if we know that the data does not contain cycles, we can use any mode even with a recursive predicate and still have a polynomial time data complexity. In the full paper <ref> [15] </ref>, we make a detailed comparison with non-probabilistic frameworks such as Kifer and Li [12] and Kifer and Sub-rahmanian [14].
Reference: [16] <author> Y. Liu. </author> <title> Null values in definite programs. </title> <booktitle> In Proc. North American Conf. on Logic Programming, </booktitle> <pages> pages 273-288, </pages> <address> Austin, TX, </address> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: In this paper, we are interested in an extension capable of handling probabilistic knowledge. Previous Work: Recent developments in DDBs (and logics in general) have led to frameworks capable of handling various forms of imperfection in knowledge. Abiteboul, et. al. [1], Liu <ref> [16] </ref>, and Dong and Lakshmanan [5] dealt with DDBs with incomplete information in the form of null values. Kifer and Lozinskii [13] have developed a logic for reasoning with inconsistency.
Reference: [17] <author> R. Ng and V. S. Subrahmanian. </author> <title> Probabilistic logic programming. </title> <journal> Information and Computation, </journal> <volume> 101(2) </volume> <pages> 150-201, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Ng and Subrahmanian <ref> [17, 18] </ref> have recently proposed an interesting scheme for logic programming with uncertainty modeled using probabilities. <p> Syntactically, the framework shares the notation with annotated logic program fl This research was supported in part by grants from the NSERC (Canada) and FCAR (Quebec), and was performed while the second author was at Concordia University. 1 ming. In <ref> [17] </ref> only annotations involving constant probability ranges were allowed. This was extended in [18] by allowing annotation variables and functions. They developed fixpoint and model-theoretic semantics, and provided a sound and weakly complete proof procedure. <p> The motivation for using probability theory as opposed to other formalisms for representing uncertainty has been discussed at length in the literature <ref> [4, 17] </ref>. Probability theory is perhaps the best understood and mathematically well-founded paradigm in which uncertainty can be modeled and reasoned about. <p> Firstly, probabilities derived from any sources may have tolerances associated with them. Even experts may feel more comfortable with specifying a range rather than a precise probability. Secondly, Fenstad [7] has shown (also see <ref> [17] </ref>) that when enough information is not available about the interaction between events, the probability of compound events cannot be determined precisely: one can only give (tight) bounds. Thus, we associate ranges of probabilities with facts and rules. A last issue is the following. <p> A simplistic model studied in the literature (e.g. , see Barbara et. al. [3]) assumes independence between all pairs of events. This is highly restrictive and is of limited applicability. A general model, studied by Ng and Subrahmanian <ref> [17, 18] </ref> is that of ignorance: assume no knowledge about event interaction. Although this is the most general possible situation, it can be overly conservative when some knowledge is available, concerning some of the events. We argue that for "real-life" applications, no single model of event interaction would suffice. <p> Even when these modes are used indiscriminately, we can still obtain the confidence associated with the goal with an arbitrarily high degree of accuracy, by constructing DPTs of appropriate height. 7 Termination and Complexity In this section, we first compare our work with that of Ng and Subrahma-nian <ref> [17, 18] </ref> (see Section 1 for a general comparison with non-probabilistic frameworks). First, let us examine the (only) "mode" for disjunction used by them 7 . They combine the confidences of an atom A coming from different derivations by taking their intersection. <p> In the full paper [15], we make a detailed comparison with non-probabilistic frameworks such as Kifer and Li [12] and Kifer and Sub-rahmanian [14]. In particular, the framework of annotation functions used 9 It is straightforward to show that the data complexity for the framework of <ref> [17] </ref> is polynomial, although the paper does not address this issue. However, that framework only allows constant annotations and is of limited expressive power. in [14] enables an infinite family of modes to be used in propagating confi-dences from rule bodies to heads. <p> The parametric nature of modes in p-programs is shown to be a significant advantage w.r.t. these aspects. Also, the analysis of trilattices shows insightful relationships between previous work (e.g. , Ng and Subrahnmanian <ref> [17, 18] </ref>) and ours. Interesting open issues which merit further research include (1) semantics of p-programs under various trilattice orders and various modes, including new ones, (2) query optimization, (3) handling inconsistency in a framework handling uncertainty, such as the one studied here.
Reference: [18] <author> R. T. Ng and V. S. Subrahmanian. </author> <title> A semantical framework for supporting subjective and conditional probabilities in deductive databases. </title> <type> Technical Report CS-TR-2563, </type> <institution> University of Maryland, College Park, MD, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Ng and Subrahmanian <ref> [17, 18] </ref> have recently proposed an interesting scheme for logic programming with uncertainty modeled using probabilities. <p> In [17] only annotations involving constant probability ranges were allowed. This was extended in <ref> [18] </ref> by allowing annotation variables and functions. They developed fixpoint and model-theoretic semantics, and provided a sound and weakly complete proof procedure. Guntzer et. al. [11] have proposed a sound (propositional) probabilistic calculus based on conditional probabilities, for reasoning in the presence of incomplete information. <p> Besides these works, a wealth of literature is available on probabilistic logic alone (e.g. see Carnap [4]and Fagin, Halpern, and Meggido [6]). As pointed out in <ref> [18] </ref> it is unclear how these can form a basis for probabilistic logic programming. An interested reader is referred to [18] for a detailed account of these and other works. Contributions: We associate a confidence level with facts and rules (of a deductive database). <p> Besides these works, a wealth of literature is available on probabilistic logic alone (e.g. see Carnap [4]and Fagin, Halpern, and Meggido [6]). As pointed out in <ref> [18] </ref> it is unclear how these can form a basis for probabilistic logic programming. An interested reader is referred to [18] for a detailed account of these and other works. Contributions: We associate a confidence level with facts and rules (of a deductive database). A confidence level comes with both a belief and a doubt (in what is being asserted) [see Section 2 for a motivation]. <p> A simplistic model studied in the literature (e.g. , see Barbara et. al. [3]) assumes independence between all pairs of events. This is highly restrictive and is of limited applicability. A general model, studied by Ng and Subrahmanian <ref> [17, 18] </ref> is that of ignorance: assume no knowledge about event interaction. Although this is the most general possible situation, it can be overly conservative when some knowledge is available, concerning some of the events. We argue that for "real-life" applications, no single model of event interaction would suffice. <p> Even when these modes are used indiscriminately, we can still obtain the confidence associated with the goal with an arbitrarily high degree of accuracy, by constructing DPTs of appropriate height. 7 Termination and Complexity In this section, we first compare our work with that of Ng and Subrahma-nian <ref> [17, 18] </ref> (see Section 1 for a general comparison with non-probabilistic frameworks). First, let us examine the (only) "mode" for disjunction used by them 7 . They combine the confidences of an atom A coming from different derivations by taking their intersection. <p> This is a pf-program in the framework of Ng and Subrahmanian <ref> [18] </ref>. Let us denote the operator T P defined by them as T NS P for distinguishing it from ours. It is not hard to see that lf p (T N S P ) would assign an empty probability range for p (1; 2). This is quite unintuitive. <p> The parametric nature of modes in p-programs is shown to be a significant advantage w.r.t. these aspects. Also, the analysis of trilattices shows insightful relationships between previous work (e.g. , Ng and Subrahnmanian <ref> [17, 18] </ref>) and ours. Interesting open issues which merit further research include (1) semantics of p-programs under various trilattice orders and various modes, including new ones, (2) query optimization, (3) handling inconsistency in a framework handling uncertainty, such as the one studied here.
Reference: [19] <author> N. Steger, H. Schmidt, U. Guntzer, and W. Kieling. </author> <title> Semantics and efficient compilation for quantitative deductive databases. </title> <booktitle> In Proc. IEEE Int. Conf. on Data Engineering, </booktitle> <pages> pages 660-669, </pages> <year> 1989. </year>
Reference-contexts: Kifer and Lozinskii [13] have developed a logic for reasoning with inconsistency. Most of the works dealing with uncertainty in knowledge-bases employ one of the following formalisms: (1) a form of fuzzy logic (programming) (e.g. van Emden [20], Steger et. al. <ref> [19] </ref>, and Fitting [8]), (2) annotated logic programming (e.g. see Kifer and Li [12] and Kifer and Subrahmanian [14]), (3) evidence theoretic logic programming (e.g. see Baldwin [2]), and (4) probabilistic logic programming (see below).
Reference: [20] <author> M. H. van Emden. </author> <title> Quantitative deduction and its fixpoint theory. </title> <journal> Journal of Logic Programming, </journal> <volume> 4(1) </volume> <pages> 37-53, </pages> <year> 1986. </year>
Reference-contexts: Kifer and Lozinskii [13] have developed a logic for reasoning with inconsistency. Most of the works dealing with uncertainty in knowledge-bases employ one of the following formalisms: (1) a form of fuzzy logic (programming) (e.g. van Emden <ref> [20] </ref>, Steger et. al. [19], and Fitting [8]), (2) annotated logic programming (e.g. see Kifer and Li [12] and Kifer and Subrahmanian [14]), (3) evidence theoretic logic programming (e.g. see Baldwin [2]), and (4) probabilistic logic programming (see below). <p> Probability theory is perhaps the best understood and mathematically well-founded paradigm in which uncertainty can be modeled and reasoned about. Two possibilities for associating probabilities with facts and rules in a DDB are van Emden's style of associating confidences with rules as a whole <ref> [20] </ref>, or the annotation style of Kifer and Subrahmanian [14].The first approach is more suited for truth-functional derivation of probabilities whereas the second is ideal for subjective derivations of probabilities. (It is shown in [14] that the second approach subsumes the first.) In this paper, our interest is in truth-functional 1
Reference: [21] <author> M. Y. Vardi. </author> <title> Querying logical databases. </title> <booktitle> In Proceedings of the 4th ACM Symp. on Principles of Database Systems, </booktitle> <pages> pages 57-65, </pages> <year> 1985. </year>
Reference-contexts: The former makes more intuitive sense, although the latter (more conservative under p ) is obviously not wrong. Also, in the latter case, the lf p is reached only at !. We define the data complexity <ref> [21] </ref> of a p-program P as the time complexity of computing the least fixpoint of T P as a function of the size of the database, i.e. the number of constants occurring in P 8 . It is well known that the data complexity for datalog programs is polynomial.
References-found: 21

