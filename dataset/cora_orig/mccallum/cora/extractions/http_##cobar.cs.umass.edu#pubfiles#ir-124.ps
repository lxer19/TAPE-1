URL: http://cobar.cs.umass.edu/pubfiles/ir-124.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Title: Effective Retrieval with Distributed Collections  
Author: Jinxi Xu and Jamie Callan 
Web: www.cs.umass.edu/~xu www.cs.umass.edu/~callan  
Address: Amherst, MA 01003-4610, USA  
Affiliation: Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts, Amherst  
Abstract: This paper evaluates the retrieval effectiveness of distributed information retrieval systems in realistic environments. We find that when a large number of collections are available, the retrieval effectiveness is significantly worse than that of centralized systems, mainly because typical queries are not adequate for the purpose of choosing the right collections. We propose two techniques to address the problem. One is to use phrase information in the collection selection index and the other is query expansion. Both techniques enhance the discriminatory power of typical queries for choosing the right collections and hence significantly improve retrieval results. Query expansion, in particular, brings the effectiveness of searching a large set of distributed collections close to that of searching a centralized collection. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Baumgarten. </author> <title> A probabilistic model for dis tributed information retrieval. </title> <booktitle> In Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 258-266, </pages> <year> 1997. </year>
Reference-contexts: The retrieval performance is highly dependent on the block size. Severe degradation in performance was observed when each block contains 1000 documents. Despite the difference in terminology, the general concept of a collection selection index has occurred in many studies <ref> [15, 11, 10, 4, 6, 1] </ref>. A collection selection index typically contains a list of textual objects, each of which is a concise description about the content of a collection. In some studies the collection selection index assumes a more complex hierarchical structure [10, 1]. <p> A collection selection index typically contains a list of textual objects, each of which is a concise description about the content of a collection. In some studies the collection selection index assumes a more complex hierarchical structure <ref> [10, 1] </ref>. A common technique to describe the content of a collection is to use the words that occur in the collection and their frequency of occurrences.
Reference: [2] <author> J. Broglio, J. P. Callan, and W.B. Croft. </author> <title> An overview of the INQUERY system as used for the TIPSTER project. </title> <booktitle> In Proceedings of the TIPSTER Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: For example, 12 collections in TREC4 are of U.S. Patents articles. 6 of them are in TRAIN and the other 6 are in TREC4-TEST. Indices for the collections are created and searched by INQUERY <ref> [2] </ref>. So are the collection selection indices. We think that in a realistic environment a typical user can only afford searching a small number of collections. Therefore, we only search 10 collections for a query in our experiments.
Reference: [3] <author> Chris Buckley, Amit Singhal, Mandar Mitra, and Gerard Salton. </author> <title> New retrieval approaches using SMART : TREC 4. </title> <booktitle> In Proceedings of the TREC 4 Conference, </booktitle> <year> 1996. </year>
Reference-contexts: But for longer queries and queries which do not require query words to be adjacent, the loss of document boundary information is still a problem. The other technique is query expansion. Query expansion has been extensively studied to address the vocabulary mismatch between queries and documents <ref> [17, 14, 3, 22] </ref>. Our use of query expansion in this paper has a different motivation. As we said before, queries using common words may be adequate for document retrieval but do not provide a good basis for collection selection.
Reference: [4] <author> J. P. Callan, Z. Lu, and W.B. Croft. </author> <title> Searching dis tributed collections with inference networks. </title> <booktitle> In Proceedings of 18th ACM SIGIR International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 21-28, </pages> <year> 1995. </year>
Reference-contexts: When a query is posted, the system first compares it with the virtual documents to decide which collections are most likely to contain relevant documents for the query. Document retrieval will only take place at such collections. A complete discussion of the system can be found in <ref> [4] </ref>. The virtual documents are a very concise representation, requiring less than 1.0% of space taken by the underlying document collections [4]. Furthermore, the process of creating the virtual documents is completely automatic and hence scales well in a realistic environment. <p> Document retrieval will only take place at such collections. A complete discussion of the system can be found in <ref> [4] </ref>. The virtual documents are a very concise representation, requiring less than 1.0% of space taken by the underlying document collections [4]. Furthermore, the process of creating the virtual documents is completely automatic and hence scales well in a realistic environment. The primary concern of this paper is the retrieval effectiveness of distributed information retrieval. Two things make this work different from previous work in this area. <p> Firstly, the sets of distributed collections used in this paper are far more realistic than those used in previous studies. Our sets of collections are large, each consisting of up to 107 collections. In contrast, those used in previous studies usually consisted of a very small number of collections <ref> [4, 11, 20] </ref>. As we will see later in this paper, the number of collections in a distributed environment can significantly affect the retrieval effectiveness. Secondly, our evaluation is based on actual relevance judgment data (from the TREC conferences [12]). <p> Section 10 draws conclusions and points out future work. 2 Related Work There have been a number of studies concerning retrieval effectiveness in a distributed environment. Callan, Lu and Croft compared the retrieval effectiveness of searching a set of distributed collections with that of searching a centralized collection <ref> [4] </ref>. They found no significant difference in retrieval performance between distributed and centralized searching when about half of the collections were searched for a query on average. <p> The retrieval performance is highly dependent on the block size. Severe degradation in performance was observed when each block contains 1000 documents. Despite the difference in terminology, the general concept of a collection selection index has occurred in many studies <ref> [15, 11, 10, 4, 6, 1] </ref>. A collection selection index typically contains a list of textual objects, each of which is a concise description about the content of a collection. In some studies the collection selection index assumes a more complex hierarchical structure [10, 1]. <p> The major advantage of this technique is that it is cheap to obtain and scales well in a realistic distributed environment. This technique was also used by Callan, et al, in <ref> [4] </ref>. The primary difference from GlOSS was that collection selection in [4] is based on a probabilistic model, the Inference Network model [18]. Other techniques for describing the content of a collection are more expensive. <p> The major advantage of this technique is that it is cheap to obtain and scales well in a realistic distributed environment. This technique was also used by Callan, et al, in <ref> [4] </ref>. The primary difference from GlOSS was that collection selection in [4] is based on a probabilistic model, the Inference Network model [18]. Other techniques for describing the content of a collection are more expensive. In WAIS, a piece of natural language text had to be manually written to describe the content of a collection [15]. <p> Given a query Q, the virtual documents V D (C i )'s are treated as normal documents and are ranked for Q based on a probabilistic model. The top ranked m collections are chosen for retrieval. The algorithm for ranking the collections is fully described in <ref> [4, 18] </ref>. To understand the arguments in this paper, it is sufficient to know that the process is analogous to that of ranking the documents in an ordinary document collection. We conjecture that the above approach will do a poor job of ranking the collections for typical queries. <p> The collection selection index is word-based. Callan, Lu and Croft did similar experiments in <ref> [4] </ref>. They observed that the effectiveness of searching a set of distributed collections was close to that of searching a centralized collection. But their results were obtained on a small number of collections (17). <p> This means that the amount of work to manually edit the expanded query is reasonable. Finally, our technique for merging retrieval results needs to be improved. It merges documents from different collections based on locally computed scores. As pointed out in <ref> [4] </ref>, this simple technique has certain drawbacks. How to effectively merge retrieval results from different collections is an active research topic by itself. Any progress in that area will benefit us as well. Acknowledgements Thanks to Bruce Croft for his guidance throughout this study.
Reference: [5] <author> J.P. Callan, W.B. Croft, and J. Broglio. </author> <title> TREC and TIPSTER experiments with INQUERY. </title> <booktitle> Information Processing and Management, </booktitle> <pages> pages 327-343, </pages> <year> 1995. </year>
Reference-contexts: These are opposed to long elaborate queries typical in routing environments. The reason for the poor ranking of the collections is not that the ranking algorithm is bad. In fact, the quality of the algorithm for document ranking is well documented <ref> [18, 5] </ref>. The reason is that typical queries do not provide enough information for collection ranking under the chosen representation of the collections (as virtual documents), no matter how good the ranking algorithm is. Take the query "the White House" for an example.
Reference: [6] <author> Anil Chakravarthy and Kenneth Hasse. NetSerf: </author> <title> Us ing semantic knowledge to find Internet information archives. </title> <booktitle> In Proceedings of the 18 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 4-11, </pages> <year> 1995. </year>
Reference-contexts: The retrieval performance is highly dependent on the block size. Severe degradation in performance was observed when each block contains 1000 documents. Despite the difference in terminology, the general concept of a collection selection index has occurred in many studies <ref> [15, 11, 10, 4, 6, 1] </ref>. A collection selection index typically contains a list of textual objects, each of which is a concise description about the content of a collection. In some studies the collection selection index assumes a more complex hierarchical structure [10, 1]. <p> Other techniques for describing the content of a collection are more expensive. In WAIS, a piece of natural language text had to be manually written to describe the content of a collection [15]. In NertSerf, the descriptions of the collections were also created manually and represented as frames <ref> [6] </ref>. Chamis used switching vocabularies for selection of online text databases [7]. In that study, manual thesauri were used to reconcile the vocabulary difference among the databases. Fuhr proposed a decision-theoretic approach to collection selection [8].
Reference: [7] <author> Alice Chamis. </author> <title> Selection of online databases using switching vocabularies. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 39(3), </volume> <year> 1988. </year>
Reference-contexts: In WAIS, a piece of natural language text had to be manually written to describe the content of a collection [15]. In NertSerf, the descriptions of the collections were also created manually and represented as frames [6]. Chamis used switching vocabularies for selection of online text databases <ref> [7] </ref>. In that study, manual thesauri were used to reconcile the vocabulary difference among the databases. Fuhr proposed a decision-theoretic approach to collection selection [8].
Reference: [8] <author> Norbert Fuhr. </author> <title> A decision-theoretic approach to database selection in networked IR. </title> <type> Technical report, </type> <institution> Computer Science Department, University of Dortmund, </institution> <year> 1996. </year>
Reference-contexts: Chamis used switching vocabularies for selection of online text databases [7]. In that study, manual thesauri were used to reconcile the vocabulary difference among the databases. Fuhr proposed a decision-theoretic approach to collection selection <ref> [8] </ref>.
Reference: [9] <author> L. Gravano, Kevin Chang, H. Garc ia-Molina, and Andreas Paepcke. </author> <title> STARTS Stanford protocol proposal for Internet retrieval and search. </title> <type> Technical report, </type> <institution> Computer Science Department, Stanford University, </institution> <year> 1996. </year>
Reference-contexts: A common technique to describe the content of a collection is to use the words that occur in the collection and their frequency of occurrences. This technique was used by Gravano, et al, in the GlOSS project [11, 10] and later proposed as part of the STARTS standard <ref> [9] </ref>. The major advantage of this technique is that it is cheap to obtain and scales well in a realistic distributed environment. This technique was also used by Callan, et al, in [4].
Reference: [10] <author> L. Gravano and H. Garc ia-Molina. </author> <title> Generalizing GlOSS to vector-space databases and broker hierarchies. </title> <booktitle> In Proceedings of the 21th VLDB Conference, </booktitle> <year> 1995. </year>
Reference-contexts: It is hard to gauge the true value of their system because of this flaw in evaluation. The work was later extended to the Vector Space Model, but the lack of relevance judgments was still a problem <ref> [10] </ref>. Voorhees, et al, evaluated the effectiveness of searching a set of distributed collections from the perspective of collection fusions [20]. <p> The retrieval performance is highly dependent on the block size. Severe degradation in performance was observed when each block contains 1000 documents. Despite the difference in terminology, the general concept of a collection selection index has occurred in many studies <ref> [15, 11, 10, 4, 6, 1] </ref>. A collection selection index typically contains a list of textual objects, each of which is a concise description about the content of a collection. In some studies the collection selection index assumes a more complex hierarchical structure [10, 1]. <p> A collection selection index typically contains a list of textual objects, each of which is a concise description about the content of a collection. In some studies the collection selection index assumes a more complex hierarchical structure <ref> [10, 1] </ref>. A common technique to describe the content of a collection is to use the words that occur in the collection and their frequency of occurrences. <p> A common technique to describe the content of a collection is to use the words that occur in the collection and their frequency of occurrences. This technique was used by Gravano, et al, in the GlOSS project <ref> [11, 10] </ref> and later proposed as part of the STARTS standard [9]. The major advantage of this technique is that it is cheap to obtain and scales well in a realistic distributed environment. This technique was also used by Callan, et al, in [4].
Reference: [11] <author> L. Gravano, H. Garc ia-Molina, and A. Tomasic. </author> <title> The effectiveness of GlOSS for the text database discovery problem. </title> <booktitle> In Proceedings of SIGMOD 94, </booktitle> <pages> pages 126-137. </pages> <publisher> ACM, </publisher> <month> September </month> <year> 1994. </year>
Reference-contexts: Firstly, the sets of distributed collections used in this paper are far more realistic than those used in previous studies. Our sets of collections are large, each consisting of up to 107 collections. In contrast, those used in previous studies usually consisted of a very small number of collections <ref> [4, 11, 20] </ref>. As we will see later in this paper, the number of collections in a distributed environment can significantly affect the retrieval effectiveness. Secondly, our evaluation is based on actual relevance judgment data (from the TREC conferences [12]). <p> Secondly, our evaluation is based on actual relevance judgment data (from the TREC conferences [12]). This is in contrast to some studies in which evaluation was performed in absence of actual relevance data. Gra-vano, for example, used Boolean satisfaction of a query by a document as "relevance" <ref> [11] </ref>. We find that for typical queries, the effectiveness of searching a large set of distributed collections is significantly (about 30%) worse than that of searching a single centralized collection. The primary cause is that typical queries, though adequate for document retrieval, are not very suitable for collection selection. <p> Since the total number of collections was small (17) and the percentage of collections searched was high, their results may not reflect the true retrieval performance in a realistic distributed environment. Gravano, et al, evaluated the capability of the GlOSS system for choosing the right collections for a query <ref> [11] </ref>. Their experiments were carried out on a set of 6 collections [11]. Without human judgments of relevance, their evaluation was based on how many documents in a collection satisfy a query in the Boolean sense. <p> Gravano, et al, evaluated the capability of the GlOSS system for choosing the right collections for a query <ref> [11] </ref>. Their experiments were carried out on a set of 6 collections [11]. Without human judgments of relevance, their evaluation was based on how many documents in a collection satisfy a query in the Boolean sense. <p> The retrieval performance is highly dependent on the block size. Severe degradation in performance was observed when each block contains 1000 documents. Despite the difference in terminology, the general concept of a collection selection index has occurred in many studies <ref> [15, 11, 10, 4, 6, 1] </ref>. A collection selection index typically contains a list of textual objects, each of which is a concise description about the content of a collection. In some studies the collection selection index assumes a more complex hierarchical structure [10, 1]. <p> A common technique to describe the content of a collection is to use the words that occur in the collection and their frequency of occurrences. This technique was used by Gravano, et al, in the GlOSS project <ref> [11, 10] </ref> and later proposed as part of the STARTS standard [9]. The major advantage of this technique is that it is cheap to obtain and scales well in a realistic distributed environment. This technique was also used by Callan, et al, in [4].
Reference: [12] <author> D. Harman. </author> <title> Overview of the Third Text REtrieval Conference (TREC-3). </title> <editor> In D. Harman, editor, </editor> <booktitle> Proceedings of the Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 1-20. </pages> <note> NIST Special Publication 500-225, </note> <year> 1995. </year>
Reference-contexts: As we will see later in this paper, the number of collections in a distributed environment can significantly affect the retrieval effectiveness. Secondly, our evaluation is based on actual relevance judgment data (from the TREC conferences <ref> [12] </ref>). This is in contrast to some studies in which evaluation was performed in absence of actual relevance data. Gra-vano, for example, used Boolean satisfaction of a query by a document as "relevance" [11].
Reference: [13] <author> David Hull. </author> <title> Using statistical testing in the evalua tion of retrieval experiments. </title> <booktitle> In Proceedings of the 16 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 329-338, </pages> <year> 1993. </year>
Reference-contexts: Therefore, in this paper we will only consider precision when 5, 10, 15, 20 and 30 documents are read, even though 300 documents are returned for each query. In addition to precision figures at the above document cut-offs, the t-test <ref> [13] </ref> is used to decide whether the performance difference between two methods is statistically significant. To decide whether the improvement by method A over method B is significant, the t-test calculates a p value based on the performance data of A and B.
Reference: [14] <author> Y. Jing and W. Bruce Croft. </author> <title> An association the saurus for information retrieval. </title> <booktitle> In Proceedings of RIAO 94, </booktitle> <pages> pages 146-160, </pages> <year> 1994. </year>
Reference-contexts: But for longer queries and queries which do not require query words to be adjacent, the loss of document boundary information is still a problem. The other technique is query expansion. Query expansion has been extensively studied to address the vocabulary mismatch between queries and documents <ref> [17, 14, 3, 22] </ref>. Our use of query expansion in this paper has a different motivation. As we said before, queries using common words may be adequate for document retrieval but do not provide a good basis for collection selection.
Reference: [15] <author> Brewster Kahle and Art Medlar. </author> <title> An information system for corporate users: Wide Area Information Servers. </title> <type> Technical Report TMC199, </type> <institution> Thinking Machines Corporation, </institution> <year> 1991. </year>
Reference-contexts: The retrieval performance is highly dependent on the block size. Severe degradation in performance was observed when each block contains 1000 documents. Despite the difference in terminology, the general concept of a collection selection index has occurred in many studies <ref> [15, 11, 10, 4, 6, 1] </ref>. A collection selection index typically contains a list of textual objects, each of which is a concise description about the content of a collection. In some studies the collection selection index assumes a more complex hierarchical structure [10, 1]. <p> Other techniques for describing the content of a collection are more expensive. In WAIS, a piece of natural language text had to be manually written to describe the content of a collection <ref> [15] </ref>. In NertSerf, the descriptions of the collections were also created manually and represented as frames [6]. Chamis used switching vocabularies for selection of online text databases [7]. In that study, manual thesauri were used to reconcile the vocabulary difference among the databases.
Reference: [16] <author> Alistair Moffat and Justin Zobel. </author> <title> Information re trieval systems for large document collections. </title> <editor> In D. Harman, editor, </editor> <booktitle> The TREC3 Proceedings, </booktitle> <year> 1995. </year>
Reference-contexts: Moffat, et al, used a centralized index on blocks of documents <ref> [16] </ref>. In that study, an initial search on the centralized index returns the highly ranked blocks for a query. A second retrieval returns the highly ranked documents from the highly ranked blocks. The retrieval performance is highly dependent on the block size.
Reference: [17] <author> Karen Sparck Jones. </author> <title> Automatic Keyword Classifi cation for Information Retrieval. Butterworth, </title> <address> Lon-don, </address> <year> 1971. </year>
Reference-contexts: But for longer queries and queries which do not require query words to be adjacent, the loss of document boundary information is still a problem. The other technique is query expansion. Query expansion has been extensively studied to address the vocabulary mismatch between queries and documents <ref> [17, 14, 3, 22] </ref>. Our use of query expansion in this paper has a different motivation. As we said before, queries using common words may be adequate for document retrieval but do not provide a good basis for collection selection.
Reference: [18] <author> Howard R. </author> <title> Turtle. Inference Networks for Document Retrieval. </title> <type> PhD thesis, </type> <institution> University of Massachusetts at Amherst, </institution> <year> 1990. </year>
Reference-contexts: This technique was also used by Callan, et al, in [4]. The primary difference from GlOSS was that collection selection in [4] is based on a probabilistic model, the Inference Network model <ref> [18] </ref>. Other techniques for describing the content of a collection are more expensive. In WAIS, a piece of natural language text had to be manually written to describe the content of a collection [15]. In NertSerf, the descriptions of the collections were also created manually and represented as frames [6]. <p> Given a query Q, the virtual documents V D (C i )'s are treated as normal documents and are ranked for Q based on a probabilistic model. The top ranked m collections are chosen for retrieval. The algorithm for ranking the collections is fully described in <ref> [4, 18] </ref>. To understand the arguments in this paper, it is sufficient to know that the process is analogous to that of ranking the documents in an ordinary document collection. We conjecture that the above approach will do a poor job of ranking the collections for typical queries. <p> These are opposed to long elaborate queries typical in routing environments. The reason for the poor ranking of the collections is not that the ranking algorithm is bad. In fact, the quality of the algorithm for document ranking is well documented <ref> [18, 5] </ref>. The reason is that typical queries do not provide enough information for collection ranking under the chosen representation of the collections (as virtual documents), no matter how good the ranking algorithm is. Take the query "the White House" for an example.
Reference: [19] <author> Charles Viles and James French. </author> <title> Dissemination of collection wide information in a distributed information retrieval system. </title> <booktitle> In Proceedings of the 18 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 12-20, </pages> <year> 1995. </year>
Reference-contexts: Viles and French studied the impact of inverse document frequency (idf) estimation on the performance of distributed retrieval and proposed a method to disseminate collection wide information periodically to achieve better estimation of idf statistics in a dynamic distributed environment <ref> [19] </ref>. Moffat, et al, used a centralized index on blocks of documents [16]. In that study, an initial search on the centralized index returns the highly ranked blocks for a query. A second retrieval returns the highly ranked documents from the highly ranked blocks.
Reference: [20] <author> E. M. Voorhees, N. K. Gupta, and B. Johnson-Laird. </author> <title> The collection fusion problem. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Third Text REtrieval Conference (TREC-3), </booktitle> <address> Gaithersburg, MD, </address> <year> 1995. </year> <institution> National Institute of Standards and Technology, </institution> <note> Special Publication 500-225. </note>
Reference-contexts: Firstly, the sets of distributed collections used in this paper are far more realistic than those used in previous studies. Our sets of collections are large, each consisting of up to 107 collections. In contrast, those used in previous studies usually consisted of a very small number of collections <ref> [4, 11, 20] </ref>. As we will see later in this paper, the number of collections in a distributed environment can significantly affect the retrieval effectiveness. Secondly, our evaluation is based on actual relevance judgment data (from the TREC conferences [12]). <p> The work was later extended to the Vector Space Model, but the lack of relevance judgments was still a problem [10]. Voorhees, et al, evaluated the effectiveness of searching a set of distributed collections from the perspective of collection fusions <ref> [20] </ref>.
Reference: [21] <author> Jinxi Xu, John Broglio, and Bruce Croft. </author> <title> The design and implementation of a part of speech tagger for English. </title> <type> Technical Report IR52, </type> <institution> CIIR, Computer and Information Science Department, University of Massachusetts, </institution> <address> Amherst, MA 01003, </address> <year> 1994. </year>
Reference-contexts: That is, the virtual document for a collection stores not only single words and their frequencies, but also phrases and their frequencies in the collection. Phrases are defined as noun phrases in the documents and queries, and are recognized by a part of speech tag-ger, JTAG <ref> [21] </ref>. Including phrases in the collection selection index will at least partially compensate for the loss of document boundary information. Simple queries such as "the White House" and "George Washington" are properly handled.
Reference: [22] <author> Jinxi Xu and Bruce Croft. </author> <title> Query expansion using lo cal and global document analysis. </title> <booktitle> In Proceedings of the 19 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 4-11, </pages> <year> 1996. </year>
Reference-contexts: The primary cause is that typical queries, though adequate for document retrieval, are not very suitable for collection selection. Fortunately, this problem can be largely solved through query expansion. When typical queries are expanded by local context analysis <ref> [22] </ref>, the retrieval effectiveness of searching a large set of distributed collections can rival that of searching a centralized collection even when a small number of collections are searched. <p> But for longer queries and queries which do not require query words to be adjacent, the loss of document boundary information is still a problem. The other technique is query expansion. Query expansion has been extensively studied to address the vocabulary mismatch between queries and documents <ref> [17, 14, 3, 22] </ref>. Our use of query expansion in this paper has a different motivation. As we said before, queries using common words may be adequate for document retrieval but do not provide a good basis for collection selection. <p> Topic words are ideal for collection selection because the loss of document boundary information in the collection selection index is a much less serious problem for them. The query expansion technique used in this paper is local context analysis (LCA) <ref> [22] </ref>. There are two reasons we choose LCA. Firstly, LCA has shown to be an effective query expansion technique and is completely automatic. Secondly, it requires expansion terms to co-occur with all words in the original query. <p> In this section we describe experiments that tested the conjuncture. 7.1 Local Context Analysis (LCA) The query expansion technique used in this paper is local context analysis (LCA) <ref> [22] </ref>. LCA requires a document collection in order to do query expansion. To expand a query, a retrieval system (INQUERY in our case) retrieves a number of top ranked documents or document portions (passages) and presents them to LCA. <p> As we will see in Section 8, it is possible to expand a query using a separate training collection and achieve the same effect. The expansion parameters were based on those in <ref> [22] </ref>: 70 LCA concepts (including words and phrases) were added to a query, with decreasing weights assigned to the concepts according to the order in which they were returned by LCA. The collection selection indices in the experiments in this section were word-based.
References-found: 22

