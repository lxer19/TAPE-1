URL: http://charm.cs.uiuc.edu/version2/papers/PrioLoadECOOP92.ps
Refering-URL: http://charm.cs.uiuc.edu/version2/papers/PrioLoadECOOP92.html
Root-URL: http://www.cs.uiuc.edu
Title: A Load Balancing Strategy For Prioritized Execution of Tasks  
Author: Amitabh B. Sinha Laxmikant V. Kale 
Address: Urbana, IL 61801  
Affiliation: Department of Computer Science University Of Illinois  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Edward W. Reingold, Jurg Nievergelt, and Narsingh Deo. </author> <title> Combinatorial Algorithms: Theory and Practice. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1977. </year>
Reference-contexts: We have used this analysis to develop a more efficient prioritized load balancing strategy, which is described in Section 4, alongwith the results of the performance evaluation experiments. 2 Application & Programming Environment The Traveling Salesman Problem (TSP) <ref> [1] </ref> is a typical example of an optimization problem solved using branch&bound techniques. In this problem the salesman must visit n cities, returning to the starting point, and is required to minimize the total cost of the trip.
Reference: [2] <author> J. D. C. Little, K. G. Murty, D. W. Sweeney, and C. Karel. </author> <title> An algorithm for the traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 11 </volume> <pages> 972-989, </pages> <year> 1963. </year>
Reference-contexts: The upper bound is updated whenever a solution is reached. In our implementation of TSP, we have chosen to implement the branch&bound scheme proposed by Little, et.al <ref> [2] </ref>. Much better branch&bound schemes are available; since our focus is not on the best branch&bound scheme, but rather on an efficient prioritized load balancing strategy, Little's scheme is sufficient for our purposes.
Reference: [3] <author> M. Bellmore and G. Nemhauser. </author> <title> The traveling salesman problem: a survey. </title> <journal> Operations Research, </journal> <volume> 16 </volume> <pages> 538-558, </pages> <year> 1968. </year>
Reference-contexts: Much better branch&bound schemes are available; since our focus is not on the best branch&bound scheme, but rather on an efficient prioritized load balancing strategy, Little's scheme is sufficient for our purposes. For a thorough discussion on branch&bound schemes and their parallelizations we refer you to <ref> [3, 4, 5, 6] </ref>. The application was programmed using Charm.
Reference: [4] <author> M. Held and R. Karp. </author> <title> The traveling salesman problem and minimum spanning trees. </title> <journal> Operations Research, </journal> <volume> 18 </volume> <pages> 1138-1162, </pages> <year> 1970. </year> <month> 6 </month>
Reference-contexts: Much better branch&bound schemes are available; since our focus is not on the best branch&bound scheme, but rather on an efficient prioritized load balancing strategy, Little's scheme is sufficient for our purposes. For a thorough discussion on branch&bound schemes and their parallelizations we refer you to <ref> [3, 4, 5, 6] </ref>. The application was programmed using Charm.
Reference: [5] <author> B. W. Wah, G. Li, and C. Yu. </author> <title> Multiprocessing of combinatorial search problems. </title> <editor> In V. Kumar, P. S. Gopalakrishnan, and L. N. Kamal, editors, </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Much better branch&bound schemes are available; since our focus is not on the best branch&bound scheme, but rather on an efficient prioritized load balancing strategy, Little's scheme is sufficient for our purposes. For a thorough discussion on branch&bound schemes and their parallelizations we refer you to <ref> [3, 4, 5, 6] </ref>. The application was programmed using Charm.
Reference: [6] <author> B. Monien and O. Vornberger. </author> <title> Parallel processing of combinatorial search trees. </title> <booktitle> Proceedings International Workshop on Parallel Algorithms and Architectures, </booktitle> <publisher> Math. Research Nr. </publisher> <address> 38, Akadmie-Verlag, Berlin, </address> <year> 1987. </year>
Reference-contexts: Much better branch&bound schemes are available; since our focus is not on the best branch&bound scheme, but rather on an efficient prioritized load balancing strategy, Little's scheme is sufficient for our purposes. For a thorough discussion on branch&bound schemes and their parallelizations we refer you to <ref> [3, 4, 5, 6] </ref>. The application was programmed using Charm.
Reference: [7] <author> L. V. Kale. </author> <title> The Chare Kernel Parallel Programming System. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: For a thorough discussion on branch&bound schemes and their parallelizations we refer you to [3, 4, 5, 6]. The application was programmed using Charm. Charm <ref> [7] </ref> is a machine independent parallel programming language that currently runs on shared memory machines including Encore Multimax and Sequent Symmetry, nonshared memory machines including Intel i860 and NCUBE/2, UNIX based networks of workstations including a network of IBM RISC workstations, and any UNIX based uniprocessor machine. <p> Messages can be addressed to existing chares using the SendMsg system call. This call generates for-chare messages. Charm also provides a type of replicated process called a branch-office chare, and five efficient data sharing abstractions read-only, write-once, accumulator, monotonic and dynamic tables. We refer the interested user to <ref> [7, 8] </ref> for details of Charm. New work messages (new-chare and for-chare) are queued up before being picked up for execution. New-chare messages are the only messages having no fixed destination, and therefore are the only messages which can be load balanced.
Reference: [8] <author> L. V. Kale et al. </author> <title> The Chare Kernel Programming Language Manual (Internal Report). </title>
Reference-contexts: Messages can be addressed to existing chares using the SendMsg system call. This call generates for-chare messages. Charm also provides a type of replicated process called a branch-office chare, and five efficient data sharing abstractions read-only, write-once, accumulator, monotonic and dynamic tables. We refer the interested user to <ref> [7, 8] </ref> for details of Charm. New work messages (new-chare and for-chare) are queued up before being picked up for execution. New-chare messages are the only messages having no fixed destination, and therefore are the only messages which can be load balanced.
Reference: [9] <author> W. Shu and L. V. Kale. </author> <title> Dynamic scheduling of medium-grained processes on multicomputers. </title> <type> Technical report, </type> <institution> University of Illinois, Urbana, </institution> <year> 1989. </year>
Reference-contexts: We have examined many existing load balancing strategies, e.g., ACWN and Random, to measure and understand performance of these strategies for prioritized execution of tasks. We shall summarize our observations and conclusions for the ACWN strategy. In the adaptive contracting within a neighborhood (ACWM) <ref> [9] </ref>, newly generated work is required to travel between a minimum distance, minHops, and a maximum distance, maxHops. Work always travels to topologically adjacent neighbors with the least load, but only if the difference in loads between the two neighbors is less than some predefined quantity, loadDelta.
Reference: [10] <author> V. </author> <title> Saletore . Machine Independent Parallel Execution of Speculative Computations. </title> <type> PhD thesis, </type> <institution> University of Illinois, Urbana, </institution> <month> September, </month> <year> 1990. </year>
Reference-contexts: A good load balancing strategy for prioritized task execution would need to maintain as closely as possible the global order of priorities | this, then is the criterion required of a good prioritized load balancing strategy. Although prioritized ACWM schemes have been discussed in <ref> [10] </ref>, the results are available only till 16 processors, and hence a comparison with our strategy was not possible. 4 Development of a Prioritized Load Balancing Strategy The first step towards the development of a good prioritized load balancing scheme was a centralized load manager strategy.
Reference: [11] <author> M. Furuichi, K. Taki, and N. Ichiyoshi. </author> <title> A multi-level load balancing scheme for or-parallel exhaustive search programs on the multi-psi. </title> <booktitle> Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1990. </year>
Reference-contexts: This motivated the next stage in the development of a prioritized load balancing strategy: the hierarchical strategy. Hierarchical strategies for load balancing have been discussed in <ref> [11, 12] </ref>, but these strategies have focussed on load balancing for non-prioritized tasks. 4.2 Second Step: Hierarchical Strategy In the hierarchical strategy, the processors in the system are partitioned into clusters, each cluster having its own load manager.
Reference: [12] <author> I. Ahmad and A. Ghafoor. </author> <title> A semi distributed allocation strategy for large hypercube supercomputers. </title> <booktitle> Supercomputing, </booktitle> <year> 1990. </year> <month> 7 </month>
Reference-contexts: This motivated the next stage in the development of a prioritized load balancing strategy: the hierarchical strategy. Hierarchical strategies for load balancing have been discussed in <ref> [11, 12] </ref>, but these strategies have focussed on load balancing for non-prioritized tasks. 4.2 Second Step: Hierarchical Strategy In the hierarchical strategy, the processors in the system are partitioned into clusters, each cluster having its own load manager.
References-found: 12

