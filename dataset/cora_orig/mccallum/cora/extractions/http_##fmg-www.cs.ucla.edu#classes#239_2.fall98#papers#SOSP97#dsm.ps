URL: http://fmg-www.cs.ucla.edu/classes/239_2.fall98/papers/SOSP97/dsm.ps
Refering-URL: http://fmg-www.cs.ucla.edu/classes/239_2.fall98/weekly.html
Root-URL: http://www.cs.ucla.edu
Email: fscales,kouroshg@pa.dec.com  
Title: Towards Transparent and Efficient Software Distributed Shared Memory  
Author: Daniel J. Scales and Kourosh Gharachorloo 
Affiliation: Western Research Laboratory Digital Equipment Corporation  
Date: October, 1997  
Note: To Appear in the 16th ACM Symposium on Operating System Principles,  
Abstract: Despite a large research effort, software distributed shared memory systems have not been widely used to run parallel applications across clusters of computers. The higher performance of hardware multiprocessors makes them the preferred platform for developing and executing applications. In addition, most applications are distributed only in binary format for a handful of popular hardware systems. Due to their limited functionality, software systems cannot directly execute the applications developed for hardware platforms. We have developed a system called Shasta that attempts to address the issues of efficiency and transparency that have hindered wider acceptance of software systems. Shasta is a distributed shared memory system that supports coherence at a fine granularity in software and can efficiently exploit small-scale SMP nodes by allowing processes on the same node to share data at hardware speeds. This paper focuses on our goal of tapping into large classes of commercially available applications by transparently executing the same binaries that run on hardware platforms. We discuss the issues involved in achieving transparent execution of binaries, which include supporting the full instruction set architecture, implementing an appropriate memory consistency model, and extending OS services across separate nodes. We also describe the techniques used in Shasta to solve the above problems. The Shasta system is fully functional on a prototype cluster of Alpha multiprocessors connected through Digital's Memory Channel network and can transparently run parallel applications on the cluster that were compiled to run on a single shared-memory multiprocessor. As an example of Shasta's flexibility, it can execute Oracle 7.3, a commercial database engine, across the cluster, including workloads modeled after the TPC-B and TPC-D database benchmarks. To characterize the performance of the system and the cost of providing complete transparency, we present performance results for microbenchmarks and applications running on the cluster, include preliminary results for Oracle runs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bilas, L. Iftode, D. Martin, and J. P. Singh. </author> <title> Shared Virtual Memory Across SMP Nodes Using Automatic Update: Protocols and Performance. </title> <type> Technical Report TR-517-96, </type> <institution> Department of Computer Science, Princeton University, </institution> <year> 1996. </year>
Reference-contexts: Even architectures that support aggressive relaxed models (i.e., Alpha, PowerPC, and Sparc) fail to provide sufficient information (in the executable) to allow many of the optimizations based on release consistency that are exploited by several of the software systems <ref> [1, 7] </ref>. Furthermore, the above issues related to hardware memory consistency models are unlikely to change in the foreseeable future. Transparently executing applications that use OS services leads to another set of challenging problems. <p> The primary advantage of SMP nodes is that processors within an SMP can share memory via the hardware, thus eliminating software intervention for intra-node data sharing. The widespread availability of SMP servers has led researchers to consider their use in page-based systems <ref> [1, 3, 4, 19, 22] </ref>, with SoftFLASH [4] and Cashmere [19] being the only actual implementations based on commercial multiprocessor nodes. Exploiting SMP nodes efficiently in the context of a fine-grain system like Shasta is a bit more complex. <p> issue to consider for software DSM systems, especially when the goal is to transparently support binaries for commercial architectures. 3.2.1 Current Software DSM Systems Much of the recent research in software DSM systems has been dedicated to relaxing memory consistency models further and developing protocols that aggressively exploit such models <ref> [1, 2, 7, 19] </ref>. In general, the use of a relaxed memory model allows a system to delay protocol actions, since the ordering requirements on memory operations are relaxed. <p> Finally, explicit knowledge of acquire and release synchronization allows for further optimizations such as lazily following synchronization chains across different processors to avoid communicating unnecessary data (as in implementations of lazy release consistency <ref> [1, 7] </ref>). 3.2.2 Commercial Architectures Many of the above optimizations and simplifications lead to incorrect behavior if applied to binaries from commercial architectures, either because the corresponding memory model is more strict or because the binary does not provide sufficient information about memory operations. <p> This property fundamentally disallows aggressive optimizations, such as implementations of lazy release consistency <ref> [1, 7] </ref>, that require exact knowledge about the synchronization chain across multiple processors. <p> Finally, we have developed methods to transparently execute unmodified multiprocessor executables. There are a variety of other software DSM systems that use the virtual memory hardware to detect access to data that is not available locally in the correct state <ref> [1, 2, 4, 7, 9, 19] </ref>. None of these systems have focused on transparently executing SMP binaries. Most of these page-based systems make use of aggressive protocol optimizations in order to minimize the false sharing problems that can arise because of the large coherence granularity.
Reference: [2] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: A variety of such distributed shared memory (DSM) systems have been developed, using various techniques to minimize the software overhead for supporting the shared address space. The most common approach uses the virtual memory hardware to detect access to data that is not available locally <ref> [2, 7, 9, 19] </ref>. These systems communicate data and maintain coherence at a fixed granularity equal to the size of a virtual page. Despite all the research on software DSM systems, software platforms have yet to make an impact on mainstream computing. <p> issue to consider for software DSM systems, especially when the goal is to transparently support binaries for commercial architectures. 3.2.1 Current Software DSM Systems Much of the recent research in software DSM systems has been dedicated to relaxing memory consistency models further and developing protocols that aggressively exploit such models <ref> [1, 2, 7, 19] </ref>. In general, the use of a relaxed memory model allows a system to delay protocol actions, since the ordering requirements on memory operations are relaxed. <p> Finally, we have developed methods to transparently execute unmodified multiprocessor executables. There are a variety of other software DSM systems that use the virtual memory hardware to detect access to data that is not available locally in the correct state <ref> [1, 2, 4, 7, 9, 19] </ref>. None of these systems have focused on transparently executing SMP binaries. Most of these page-based systems make use of aggressive protocol optimizations in order to minimize the false sharing problems that can arise because of the large coherence granularity.
Reference: [3] <author> A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The primary advantage of SMP nodes is that processors within an SMP can share memory via the hardware, thus eliminating software intervention for intra-node data sharing. The widespread availability of SMP servers has led researchers to consider their use in page-based systems <ref> [1, 3, 4, 19, 22] </ref>, with SoftFLASH [4] and Cashmere [19] being the only actual implementations based on commercial multiprocessor nodes. Exploiting SMP nodes efficiently in the context of a fine-grain system like Shasta is a bit more complex.
Reference: [4] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy. SoftFLASH: </author> <title> Analyzing the Performance of Clustered Distributed Virtual Shared Memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 210-220, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: The primary advantage of SMP nodes is that processors within an SMP can share memory via the hardware, thus eliminating software intervention for intra-node data sharing. The widespread availability of SMP servers has led researchers to consider their use in page-based systems <ref> [1, 3, 4, 19, 22] </ref>, with SoftFLASH [4] and Cashmere [19] being the only actual implementations based on commercial multiprocessor nodes. Exploiting SMP nodes efficiently in the context of a fine-grain system like Shasta is a bit more complex. <p> The primary advantage of SMP nodes is that processors within an SMP can share memory via the hardware, thus eliminating software intervention for intra-node data sharing. The widespread availability of SMP servers has led researchers to consider their use in page-based systems [1, 3, 4, 19, 22], with SoftFLASH <ref> [4] </ref> and Cashmere [19] being the only actual implementations based on commercial multiprocessor nodes. Exploiting SMP nodes efficiently in the context of a fine-grain system like Shasta is a bit more complex. <p> Finally, we have developed methods to transparently execute unmodified multiprocessor executables. There are a variety of other software DSM systems that use the virtual memory hardware to detect access to data that is not available locally in the correct state <ref> [1, 2, 4, 7, 9, 19] </ref>. None of these systems have focused on transparently executing SMP binaries. Most of these page-based systems make use of aggressive protocol optimizations in order to minimize the false sharing problems that can arise because of the large coherence granularity.
Reference: [5] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The use of a relaxed model also allows a system to reduce communication overhead by coalescing outgoing requests. In order to exploit especially aggressive optimizations based on relaxed memory models, a large number of page-based DSM systems heavily depend on programs being properly labeled <ref> [5] </ref>. They also typically require that applications synchronize via a few high-level constructs, such as locks and barriers, that are directly implemented through message passing as opposed to on top of the shared-memory abstraction. <p> Several important commercial architectures support relatively strict memory consistency models. The MIPS/SGI architecture requires the system to support sequential consistency, while the popular Intel x86 architecture supports processor consistency <ref> [5] </ref> which is a little less strict. The requirement to support either model would virtually disallow all the key performance optimizations exploited in page-based systems. A number of commercial architectures, including Alpha, PowerPC, and Sparc, support more relaxed models, which allow aggressive reordering of read and write operations.
Reference: [6] <author> R. B. Gillett. </author> <title> Memory Channel Network for PCI. </title> <journal> IEEE Micro, </journal> <volume> 16(1) </volume> <pages> 12-18, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The Shasta system is fully functional on our prototype cluster which consists of a total of sixteen 300 MHz Alpha processors connected through the Memory Channel <ref> [6] </ref>. We can currently run the Oracle 7.3 executable across the cluster using Shasta and execute applications that are similar to the TPC-B and TPC-D benchmarks. We present performance results for various mi-crobenchmarks and applications, including Oracle, running on the above cluster. <p> The Memory Channel is a memory-mapped network that allows a process to transmit data to a remote process without any operating system overhead via a simple store to a mapped page <ref> [6] </ref>. The one-way latency from user process to user process over Memory Channel is about 4 microseconds, and each network link can support a bandwidth of 60 Mbytes/sec. Each node in our cluster is connected to a single network link.
Reference: [7] <author> P. Keleher, A. L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: A variety of such distributed shared memory (DSM) systems have been developed, using various techniques to minimize the software overhead for supporting the shared address space. The most common approach uses the virtual memory hardware to detect access to data that is not available locally <ref> [2, 7, 9, 19] </ref>. These systems communicate data and maintain coherence at a fixed granularity equal to the size of a virtual page. Despite all the research on software DSM systems, software platforms have yet to make an impact on mainstream computing. <p> Even architectures that support aggressive relaxed models (i.e., Alpha, PowerPC, and Sparc) fail to provide sufficient information (in the executable) to allow many of the optimizations based on release consistency that are exploited by several of the software systems <ref> [1, 7] </ref>. Furthermore, the above issues related to hardware memory consistency models are unlikely to change in the foreseeable future. Transparently executing applications that use OS services leads to another set of challenging problems. <p> issue to consider for software DSM systems, especially when the goal is to transparently support binaries for commercial architectures. 3.2.1 Current Software DSM Systems Much of the recent research in software DSM systems has been dedicated to relaxing memory consistency models further and developing protocols that aggressively exploit such models <ref> [1, 2, 7, 19] </ref>. In general, the use of a relaxed memory model allows a system to delay protocol actions, since the ordering requirements on memory operations are relaxed. <p> Finally, explicit knowledge of acquire and release synchronization allows for further optimizations such as lazily following synchronization chains across different processors to avoid communicating unnecessary data (as in implementations of lazy release consistency <ref> [1, 7] </ref>). 3.2.2 Commercial Architectures Many of the above optimizations and simplifications lead to incorrect behavior if applied to binaries from commercial architectures, either because the corresponding memory model is more strict or because the binary does not provide sufficient information about memory operations. <p> This property fundamentally disallows aggressive optimizations, such as implementations of lazy release consistency <ref> [1, 7] </ref>, that require exact knowledge about the synchronization chain across multiple processors. <p> Finally, we have developed methods to transparently execute unmodified multiprocessor executables. There are a variety of other software DSM systems that use the virtual memory hardware to detect access to data that is not available locally in the correct state <ref> [1, 2, 4, 7, 9, 19] </ref>. None of these systems have focused on transparently executing SMP binaries. Most of these page-based systems make use of aggressive protocol optimizations in order to minimize the false sharing problems that can arise because of the large coherence granularity.
Reference: [8] <author> Y. A. Khalidi, J. M. Bernabeu, V. Matena, K. Shirriff, and M. Thadani. </author> <title> Solaris MC: A MultiComputer OS. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference, </booktitle> <pages> pages 191-204, </pages> <address> San Diego, CA, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Transparently executing applications that use OS services leads to another set of challenging problems. Some of the issues are similar to those faced by cluster operating systems such as Locus [11], Sprite [10], and Solaris-MC <ref> [8] </ref>. The functionality of the OS must be extended so that system calls work transparently across the cluster as if all processes were on the same machine. However, there are a number of additional issues when software is also used to support shared memory across the cluster. <p> done at the time of the next entry into the protocol code (due to explicit polls or misses) after the batch code is complete. 7 4.2 Extending System Calls across the Cluster The issues of extending OS functionality across a cluster have been addressed extensively by a number of systems <ref> [8, 10, 11] </ref>. These systems typically attempt to provide almost complete transparency by reimplementing all system calls so that they work across the cluster. However, these systems are usually not concerned with efficient software support for shared memory across the cluster. <p> There are a number of distributed operating systems that have been developed to make a cluster of machines appear as a single machine with a single operating system <ref> [8, 10, 11, 23] </ref>. These systems have concentrated on extending nearly all operating system services to achieve identical functionality regardless of where they are invoked in the cluster.
Reference: [9] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: A variety of such distributed shared memory (DSM) systems have been developed, using various techniques to minimize the software overhead for supporting the shared address space. The most common approach uses the virtual memory hardware to detect access to data that is not available locally <ref> [2, 7, 9, 19] </ref>. These systems communicate data and maintain coherence at a fixed granularity equal to the size of a virtual page. Despite all the research on software DSM systems, software platforms have yet to make an impact on mainstream computing. <p> Finally, we have developed methods to transparently execute unmodified multiprocessor executables. There are a variety of other software DSM systems that use the virtual memory hardware to detect access to data that is not available locally in the correct state <ref> [1, 2, 4, 7, 9, 19] </ref>. None of these systems have focused on transparently executing SMP binaries. Most of these page-based systems make use of aggressive protocol optimizations in order to minimize the false sharing problems that can arise because of the large coherence granularity. <p> The above systems do not typically attempt to support shared memory between processes on different nodes, though a few systems do support sharing via virtual memory mechanisms using a simple protocol similar to Ivy <ref> [9] </ref>. We have not attempted to support a full distributed operating system in Shasta. We have instead focused on supporting shared memory efficiently between processes, and on supporting the necessary operating system services to run interesting applications such as databases.
Reference: [10] <author> J. Ousterhout, A. Cherenson, F. Douglis, M. Nelson, and B. Welch. </author> <title> The Sprite Network Operating System. </title> <booktitle> IEEE Computer, </booktitle> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: Transparently executing applications that use OS services leads to another set of challenging problems. Some of the issues are similar to those faced by cluster operating systems such as Locus [11], Sprite <ref> [10] </ref>, and Solaris-MC [8]. The functionality of the OS must be extended so that system calls work transparently across the cluster as if all processes were on the same machine. However, there are a number of additional issues when software is also used to support shared memory across the cluster. <p> done at the time of the next entry into the protocol code (due to explicit polls or misses) after the batch code is complete. 7 4.2 Extending System Calls across the Cluster The issues of extending OS functionality across a cluster have been addressed extensively by a number of systems <ref> [8, 10, 11] </ref>. These systems typically attempt to provide almost complete transparency by reimplementing all system calls so that they work across the cluster. However, these systems are usually not concerned with efficient software support for shared memory across the cluster. <p> There are a number of distributed operating systems that have been developed to make a cluster of machines appear as a single machine with a single operating system <ref> [8, 10, 11, 23] </ref>. These systems have concentrated on extending nearly all operating system services to achieve identical functionality regardless of where they are invoked in the cluster.
Reference: [11] <author> G. Popek and B. Walker. </author> <title> The LOCUS Distributed System Architecture. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: Furthermore, the above issues related to hardware memory consistency models are unlikely to change in the foreseeable future. Transparently executing applications that use OS services leads to another set of challenging problems. Some of the issues are similar to those faced by cluster operating systems such as Locus <ref> [11] </ref>, Sprite [10], and Solaris-MC [8]. The functionality of the OS must be extended so that system calls work transparently across the cluster as if all processes were on the same machine. <p> done at the time of the next entry into the protocol code (due to explicit polls or misses) after the batch code is complete. 7 4.2 Extending System Calls across the Cluster The issues of extending OS functionality across a cluster have been addressed extensively by a number of systems <ref> [8, 10, 11] </ref>. These systems typically attempt to provide almost complete transparency by reimplementing all system calls so that they work across the cluster. However, these systems are usually not concerned with efficient software support for shared memory across the cluster. <p> There are a number of distributed operating systems that have been developed to make a cluster of machines appear as a single machine with a single operating system <ref> [8, 10, 11, 23] </ref>. These systems have concentrated on extending nearly all operating system services to achieve identical functionality regardless of where they are invoked in the cluster.
Reference: [12] <author> D. J. Scales and K. Gharachorloo. </author> <title> Design and Performance of the Shasta Distributed Shared Memory Protocol. </title> <booktitle> In Proceedings of the 11th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1997. </year> <note> Extended version available as Western Research Laboratory technical report 97/2 (Feb. </note> <year> 1997). </year>
Reference-contexts: We present detailed performance results in Section 6. Finally, we describe related work and conclude. 2 Basic Design of Shasta This section presents an overview of the Shasta system, which is described more fully in previous papers on support for fine-grain coherence [14], the base cache coherence protocol <ref> [12] </ref>, and protocol extensions to exploit SMP nodes [13]. 2.1 Cache Coherence Protocol Shasta divides the virtual address space of each processor into private and shared regions. <p> However, in contrast to page-based systems, the performance of Shasta is quite good even when we support a strict model <ref> [12] </ref>. This effect is primarily because Shasta supports coherence at a fine granularity, and therefore does not depend heavily on relaxed memory models for alleviating problems associated with larger coherence granularities. <p> The last three rows of the table gives the overheads for 6 The running times of LU and LU-Contig are much shorter than those in previous papers <ref> [12, 13] </ref> because of much better optimization by the compiler in Digital Unix 4.0. The more efficient sequential runs also reduce the parallel speedups attainable with Shasta for a given input size. <p> Our previous work on Shasta has explored a large number of optimizations to achieve better performance, including efficient support for fine-grain coherence [14], effective protocol optimizations <ref> [12] </ref>, and effectively exploiting a cluster of SMP nodes [13]. With respect to increasing the application base, we believe that the most promising way of addressing this issue is to support transparent execution of the increasing number of application binaries available for hardware DSM systems.
Reference: [13] <author> D. J. Scales, K. Gharachorloo, and A. Aggarwal. </author> <title> Fine-Grain Software Distributed Shared Memory on SMP Clusters. </title> <type> Technical Report 97/3, </type> <institution> Western Research Laboratory, Digital Equipment Corporation, </institution> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: Finally, we describe related work and conclude. 2 Basic Design of Shasta This section presents an overview of the Shasta system, which is described more fully in previous papers on support for fine-grain coherence [14], the base cache coherence protocol [12], and protocol extensions to exploit SMP nodes <ref> [13] </ref>. 2.1 Cache Coherence Protocol Shasta divides the virtual address space of each processor into private and shared regions. Data in the shared region may be cached by multiple processors at the same time, with copies residing at the same virtual address on each processor. <p> We have implemented a solution that allows sharing of memory among processors on the same node and avoids the race conditions described above without the use of costly synchronization in the inline checking code <ref> [13] </ref>. The overall solution depends on the selective use of explicit messages between processors on the same node for protocol operations that can lead to the race conditions involving the inline checks. <p> When using SMP-Shasta on a cluster of SMP nodes, we have observed significant performance improvements (of as high as two times) in applications over Base-Shasta because of the reduced number of remote misses and software protocol messages <ref> [13] </ref>. <p> The last three rows of the table gives the overheads for 6 The running times of LU and LU-Contig are much shorter than those in previous papers <ref> [12, 13] </ref> because of much better optimization by the compiler in Digital Unix 4.0. The more efficient sequential runs also reduce the parallel speedups attainable with Shasta for a given input size. <p> Our previous work on Shasta has explored a large number of optimizations to achieve better performance, including efficient support for fine-grain coherence [14], effective protocol optimizations [12], and effectively exploiting a cluster of SMP nodes <ref> [13] </ref>. With respect to increasing the application base, we believe that the most promising way of addressing this issue is to support transparent execution of the increasing number of application binaries available for hardware DSM systems.
Reference: [14] <author> D. J. Scales, K. Gharachorloo, and C. A. Thekkath. </author> <title> Shasta: A Low-Overhead Software-Only Approach to Fine-Grain Shared Memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 174-185, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: For example, software systems typically require the use of special constructs for synchronization and task creation and severely limit the use of system calls across the cluster. We have attempted to address some of the above issues of efficiency and transparency in the Shasta system <ref> [14] </ref>. Shasta is a software DSM system that supports sharing of data at a fine granularity by inserting code in an application executable that checks if data being 1 accessed by a load or store is available locally in the appro-priate state. <p> We present detailed performance results in Section 6. Finally, we describe related work and conclude. 2 Basic Design of Shasta This section presents an overview of the Shasta system, which is described more fully in previous papers on support for fine-grain coherence <ref> [14] </ref>, the base cache coherence protocol [12], and protocol extensions to exploit SMP nodes [13]. 2.1 Cache Coherence Protocol Shasta divides the virtual address space of each processor into private and shared regions. <p> Shasta also uses a number of other optimizations that reduce the checking overhead to an average of about 20% (including polling overhead) across the SPLASH-2 applications <ref> [14] </ref>. The two most important optimizations are described below. Whenever a block on a processor becomes invalid, the Shasta protocol stores a particular flag value in each 4-byte word of the block. <p> Our previous work on Shasta has explored a large number of optimizations to achieve better performance, including efficient support for fine-grain coherence <ref> [14] </ref>, effective protocol optimizations [12], and effectively exploiting a cluster of SMP nodes [13]. With respect to increasing the application base, we believe that the most promising way of addressing this issue is to support transparent execution of the increasing number of application binaries available for hardware DSM systems.
Reference: [15] <author> I. Schoinas, B. Falsafi, A. R. Lebeck, S. K. Reinhardt, J. R. Larus, and D. A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: The main point of these results is to illustrate that speedup is possible for database code running on top of Shasta. 7 Related Work Shasta's basic approach to checking loads and stores is derived from the Blizzard-S work <ref> [15] </ref>. However, we have substantially extended the previous work in this area by developing several techniques for reducing the otherwise excessive checking overheads. We have also designed an efficient protocol that exploits a relaxed consistency model, supports multiple coherence granularities in a single application, and executes efficiently on SMP clusters.
Reference: [16] <author> R. L. Sites and R. T. Witek, </author> <title> editors. Alpha AXP Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1995. </year> <note> Second Edition. </note>
Reference-contexts: execution of binaries which achieve synchronization through either normal loads and stores or through specialized atomic read-modify-write instructions. 3.1.1 Semantics of Load-Locked and Store-Conditional The Alpha architecture provides a pair of instructions, load-locked and store-conditional, that can be used together to to implement a binary lock. support atomic read-modify-write functionality <ref> [16] </ref>. Similar instruction pairs are provided by the MIPS and IBM PowerPC architectures. Figure 1 shows an example of how acquisition of a lock can be implemented using this pair of instructions. The current value for the lock location is read by the load-locked (LL). <p> The following discussion focuses on the Alpha memory model, but the issues raised also apply to the PowerPC and Sparc models due to the similarity among these models. The Alpha memory model <ref> [16] </ref> provides special fence instructions, called memory-barrier (MB) instructions, for enforcing program order among memory operations where necessary. A memory barrier instruction ensures that all read and write operations preceding the MB are performed prior to any reads and writes following the MB.
Reference: [17] <author> P. Sobalvarro. </author> <title> Demand-based Coscheduling of Parallel Jobs on Mul-tiprogrammed Multiprocessors. </title> <type> PhD thesis, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: We assume here that context switches are caused only by an application's own processes. Problems resulting from switches between processes of different applications can potentially be dealt with through techniques such as dynamic coscheduling <ref> [17] </ref>. Even if there is only one process per processor, a process can be suspended due to a system call. For instance, a process in a database application may be frequently suspended waiting for I/O system calls to complete or for a signal from another process.
Reference: [18] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A System for Building Customized Program Analysis Tools. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 196-205, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Polling is inexpensive (three instructions) in our Memory Channel cluster because the implementation arranges for a single cachable location that can be tested to determine if a message has arrived. 2.2 Shared Miss Checks Shasta inserts the shared miss checks in the application executable using a modified version of ATOM <ref> [18] </ref>. An efficient shared miss check requires about seven instructions. Since the static and stack data areas are not shared, Shasta does not insert checks for any loads or stores that are clearly to these areas.
Reference: [19] <author> R. Stets, S. Dwarkadas, N. Hardavellas, G. Hunt, L. Kontothanas-sis, S. Parthasarathy, and M. Scott. Cashmere-2L: </author> <title> Software Coherent Shared Memory on a Clustered Remote-Write Network. </title> <booktitle> In Proceedings of the 16th ACM Symposium on Operating Systems Principles, </booktitle> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: A variety of such distributed shared memory (DSM) systems have been developed, using various techniques to minimize the software overhead for supporting the shared address space. The most common approach uses the virtual memory hardware to detect access to data that is not available locally <ref> [2, 7, 9, 19] </ref>. These systems communicate data and maintain coherence at a fixed granularity equal to the size of a virtual page. Despite all the research on software DSM systems, software platforms have yet to make an impact on mainstream computing. <p> The primary advantage of SMP nodes is that processors within an SMP can share memory via the hardware, thus eliminating software intervention for intra-node data sharing. The widespread availability of SMP servers has led researchers to consider their use in page-based systems <ref> [1, 3, 4, 19, 22] </ref>, with SoftFLASH [4] and Cashmere [19] being the only actual implementations based on commercial multiprocessor nodes. Exploiting SMP nodes efficiently in the context of a fine-grain system like Shasta is a bit more complex. <p> The widespread availability of SMP servers has led researchers to consider their use in page-based systems [1, 3, 4, 19, 22], with SoftFLASH [4] and Cashmere <ref> [19] </ref> being the only actual implementations based on commercial multiprocessor nodes. Exploiting SMP nodes efficiently in the context of a fine-grain system like Shasta is a bit more complex. <p> issue to consider for software DSM systems, especially when the goal is to transparently support binaries for commercial architectures. 3.2.1 Current Software DSM Systems Much of the recent research in software DSM systems has been dedicated to relaxing memory consistency models further and developing protocols that aggressively exploit such models <ref> [1, 2, 7, 19] </ref>. In general, the use of a relaxed memory model allows a system to delay protocol actions, since the ordering requirements on memory operations are relaxed. <p> These requirements disallow many of the optimizations even in systems that do not exploit lazy release consistency (e.g., Cashmere <ref> [19] </ref>). Figure 2 presents a contrived example to illustrate why a number of these optimizations are not correct under the Alpha memory model. <p> Finally, we have developed methods to transparently execute unmodified multiprocessor executables. There are a variety of other software DSM systems that use the virtual memory hardware to detect access to data that is not available locally in the correct state <ref> [1, 2, 4, 7, 9, 19] </ref>. None of these systems have focused on transparently executing SMP binaries. Most of these page-based systems make use of aggressive protocol optimizations in order to minimize the false sharing problems that can arise because of the large coherence granularity.
Reference: [20] <author> C. A. Thekkath, T. Mann, and E. K. Lee. Frangipani: </author> <title> A Scalable Distributed File System. </title> <booktitle> In Proceedings of the 16th ACM Symposium on Operating Systems Principles, </booktitle> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: To support the use of system calls that access files across the cluster, in general we need to implement a distributed file system such as Frangipani <ref> [20] </ref>. Frangipani provides all nodes with coherent access to a shared set of files and is highly available despite component failures.
Reference: [21] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In addition, these results are for files that have been recently accessed, so no disk operations are involved. The relative overhead of validation would be much less for system calls that accessed the disk. 6.3 Applications and Overhead Measure ments We report results for nine of the SPLASH-2 applications <ref> [21] </ref>. Table 3 shows the input sizes used in our experiments along with the sequential running times. We have increased some of the standard input sizes in order to make sure that the applications run for at least a few seconds on our cluster.
Reference: [22] <author> D. Yeung, J. Kubiatowicz, and A. Agarwal. MGS: </author> <title> A Multigrain Shared Memory System. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 44-56, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The primary advantage of SMP nodes is that processors within an SMP can share memory via the hardware, thus eliminating software intervention for intra-node data sharing. The widespread availability of SMP servers has led researchers to consider their use in page-based systems <ref> [1, 3, 4, 19, 22] </ref>, with SoftFLASH [4] and Cashmere [19] being the only actual implementations based on commercial multiprocessor nodes. Exploiting SMP nodes efficiently in the context of a fine-grain system like Shasta is a bit more complex.
Reference: [23] <author> R. Zajcew et al. </author> <title> An OSF/1 UNIX for Massively Parallel Multicom-puters. </title> <booktitle> In Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pages 449-468, </pages> <month> Jan. </month> <year> 1993. </year> <month> 16 </month>
Reference-contexts: There are a number of distributed operating systems that have been developed to make a cluster of machines appear as a single machine with a single operating system <ref> [8, 10, 11, 23] </ref>. These systems have concentrated on extending nearly all operating system services to achieve identical functionality regardless of where they are invoked in the cluster.
References-found: 23

