URL: http://www.cs.duke.edu/~mlittman/docs/ml97-sarsa.ps
Refering-URL: 
Root-URL: 
Email: baveja@cs.colorado.edu  TOMMI JAAKKOLA tommi@cse.ucsc.edu  mlittman@cs.duke.edu  szepes@sol.cc.u-szeged.hu  
Title: Algorithms  
Author: SATINDER SINGH MICHAEL L. LITTMAN CSABA SZEPESV ARI 
Keyword: reinforcement-learning, on-policy, convergence, Markov decision processes  
Address: Boulder, CO 80309-0430  Santa Cruz, CA 95064  Durham, NC 27708-0129  Szeged 6720, Aradi vrt tere 1. Hungary  
Affiliation: Department of Computer Science University of Colorado  Computer Science Department University of California  Department of Computer Science Duke University  Bolyai Institute of Mathematics "Jozsef Attila" University of Szeged  
Note: Reinforcement-Learning  
Abstract: Convergence Results for Single-Step On-Policy Abstract. An important application of reinforcement learning (RL) is to finite-state control problems and one of the most difficult problems in learning for control is balancing the exploration/exploitation tradeoff. Existing theoretical results for RL give very little guidance on reasonable ways to perform exploration. In this paper, we examine the convergence of single-step on-policy RL algorithms for control. On-policy algorithms cannot separate exploration from learning and therefore must confront the exploration problem directly. We prove convergence results for several related on-policy algorithms with both decaying exploration and persistent exploration. We also provide examples of exploration strategies that can be followed during learning that result in convergence to both optimal values and optimal policies. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1) </volume> <pages> 81-138. </pages>
Reference: <author> Bellman, R. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference-contexts: Clearly, fl (s) = argmax a Q fl (s; a), and V fl (s) = max a Q fl (s; a). The optimal Q values satisfy the recursive Bellman optimality equations <ref> (Bellman, 1957) </ref>, 8s; a: X P a b In reinforcement learning, the quantities that define the mdp, P and R, are not known in advance.
Reference: <author> Bertsekas, D. P. </author> <year> (1995). </year> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Massachusetts. </address> <note> Volumes 1 and 2. </note>
Reference: <author> Boyan, J. A. and Moore, A. W. </author> <year> (1995). </year> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 369-376, </pages> <address> Cambridge, MA. </address> <publisher> The MIT Press. </publisher>
Reference: <author> Breiman, L. </author> <year> (1992). </year> <title> Probability. </title> <institution> Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania. </institution> <note> CONVERGENCE OF ON-POLICY RL ALGORITHMS 21 Crites, </note> <author> R. H. and Barto, A. G. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: In a communicating mdp, every state gets visited infinitely often as long as each action is chosen infinitely often in each state (this is a consequence of the Borel-Cantelli Lemma <ref> (Breiman, 1992) </ref>; all we have to ensure is that in each state each action gets chosen infinitely often in the limit. Consider some state s. Let t s (i) represent the timestep at which the i th visit to state s occurs. Consider some action a.
Reference: <author> Dayan, P. </author> <year> (1992). </year> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8(3) </booktitle> <pages> 341-362. </pages>
Reference-contexts: Following Sutton and Barto (1997), we distinguish between two types of RL algorithms: on-policy and off-policy. Off-policy algorithms may update estimated value functions on the basis of hypothetical actions, i.e., actions other than those actually executed|in this sense Q-learning <ref> (Watkins & Dayan, 1992) </ref> is an off-policy algorithm. On-policy algorithms, on the other hand, update value functions strictly on the basis of the experience gained from executing some (possibly nonstationary) policy. This distinction is important because off-policy algorithms can (at least conceptually) separate exploration from control while on-policy algorithms cannot.
Reference: <author> Dayan, P. and Sejnowski, T. J. </author> <year> (1994). </year> <title> TD() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14(3). </volume>
Reference: <author> Dayan, P. and Sejnowski, T. J. </author> <year> (1996). </year> <title> Exploration bonuses and dual control. </title> <journal> Machine Learning, </journal> <volume> 25 </volume> <pages> 5-22. </pages>
Reference: <author> Gullapalli, V. and Barto, A. G. </author> <year> (1994). </year> <title> Convergence of indirect adaptive asynchronous value iteration algorithms. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 695-702, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Jaakkola, T., Jordan, M. I., and Singh, S. P. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1185-1201. </pages>
Reference: <author> John, G. H. </author> <year> (1994). </year> <title> When the best move isn't optimal: Q-learning with exploration. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> page 1464, </pages> <address> Seattle, WA. </address>
Reference: <author> John, G. H. </author> <year> (1995). </year> <title> When the best move isn't optimal: Q-learning with exploration. </title> <type> Unpublished manuscript, </type> <note> available through URL ftp://starry.stanford.edu/pub/gjohn/papers/rein-nips.ps. </note>
Reference: <author> Kaelbling, L. P., Littman, M. L., and Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285. </pages>
Reference: <author> Kumar, P. R. and Varaiya, P. P. </author> <year> (1986). </year> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Littman, M. L. </author> <year> (1996). </year> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University. </institution> <note> Also Technical Report CS-96-09. </note>
Reference-contexts: The only change between Equation 6 and Equation 7 is that the latter uses an assignment of ranks that is based upon the recursively defined Q-value function Q, whereas the former uses a fixed assignment of ranks. Using the theory of generalized mdps <ref> (Szepesvari & Littman, 1996) </ref>, we can show that this difference is not important from the perspective of proving the existence and uniqueness of the solution of Equation 7. <p> As long as N satisfies the non-expansion property that fi fi fi a O Q 0 (s; a) fi fi max jQ (s; a) Q 0 (s; a)j for all Q-value functions Q and Q 0 and all states s, then Equation 9 has a solution and it is unique <ref> (Szepesvari & Littman, 1996) </ref>; this is proven in Appendix C. The non-expansion property of N can be verified by the following argument. * Consider a family of operators N i Q (s; a) = ith largest value of Q (s; a) for each 1 i m. <p> We conjecture that the same result does not hold for persistent Boltzmann exploration because related synchronous algorithms do not have a unique target of convergence <ref> (Littman, 1996) </ref>. The results of this section show that RRR learning policies with the sarsa (0) update rule converge to optimal restricted policies.
Reference: <author> Littman, M. L. and Szepesvari, C. </author> <year> (1996). </year> <title> A generalized reinforcement-learning model: Convergence and applications. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 310-318. </pages>
Reference-contexts: The only change between Equation 6 and Equation 7 is that the latter uses an assignment of ranks that is based upon the recursively defined Q-value function Q, whereas the former uses a fixed assignment of ranks. Using the theory of generalized mdps <ref> (Szepesvari & Littman, 1996) </ref>, we can show that this difference is not important from the perspective of proving the existence and uniqueness of the solution of Equation 7. <p> As long as N satisfies the non-expansion property that fi fi fi a O Q 0 (s; a) fi fi max jQ (s; a) Q 0 (s; a)j for all Q-value functions Q and Q 0 and all states s, then Equation 9 has a solution and it is unique <ref> (Szepesvari & Littman, 1996) </ref>; this is proven in Appendix C. The non-expansion property of N can be verified by the following argument. * Consider a family of operators N i Q (s; a) = ith largest value of Q (s; a) for each 1 i m. <p> We conjecture that the same result does not hold for persistent Boltzmann exploration because related synchronous algorithms do not have a unique target of convergence <ref> (Littman, 1996) </ref>. The results of this section show that RRR learning policies with the sarsa (0) update rule converge to optimal restricted policies.
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes|Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY. </address>
Reference: <author> Rummery, G. A. </author> <year> (1994). </year> <title> Problem solving with reinforcement learning. </title> <type> PhD thesis, </type> <institution> Cambridge University Engineering Department. </institution>
Reference: <author> Rummery, G. A. and Niranjan, M. </author> <year> (1994). </year> <title> On-line Q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFENG/TR 166, </type> <institution> Cambridge University Engineering Department. </institution>
Reference: <author> Singh, S. P. and Bertsekas, D. </author> <year> (1997). </year> <title> Reinforcement learning for dynamic channel allocation in cellular telephone systems. </title> <editor> In Mozer, M. C., Jordan, M. I., and Petsche, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, pages zzz-zzz, </booktitle> <address> Cambridge, MA. </address> <publisher> The MIT Press. </publisher>
Reference: <author> Singh, S. P. and Sutton, R. S. </author> <year> (1996). </year> <title> Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> 22(1/2/3):123-158. 
Reference: <author> Singh, S. P. and Yee, R. C. </author> <year> (1994). </year> <title> An upper bound on the loss from approximate optimal-value functions. Machine Learning, </title> <publisher> 16:227. </publisher>
Reference: <author> Sutton, R. and Barto, A. </author> <title> (forthcoming). An Introduction to Reinforcement Learning. </title> <publisher> The MIT Press. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44. </pages>
Reference-contexts: Trajectory-based algorithms appear superior to trajectory-free algorithms for prediction when parameterized function approxima-tors are used (Tsitsiklis & Van Roy, 1996). These results carry over empirically to the control case as well (Boyan & Moore, 1995; Sutton, 1996). In addition, multi-step prediction algorithms such as TD () <ref> (Sutton, 1988) </ref> are more flexible and data efficient than single-step algorithms (TD (0)), and most natural multi-step algorithms for control are on-policy. These observations suggest that on-policy control algorithms are important and worthy of study.
Reference: <author> Sutton, R. S. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Another example of a GLIE learning policy is some forms of *-greedy exploration <ref> (Sutton, 1996) </ref>, which at timestep t in state s picks a random exploration action with probability * t (s) and the greedy action with probability 1 * t (s).
Reference: <author> Szepesvari, C. and Littman, M. L. </author> <year> (1996). </year> <title> Generalized Markov decision processes: Dynamic-programming and reinforcement-learning algorithms. </title> <type> Technical Report CS-96-11, </type> <institution> Brown University, Providence, RI. </institution>
Reference-contexts: The only change between Equation 6 and Equation 7 is that the latter uses an assignment of ranks that is based upon the recursively defined Q-value function Q, whereas the former uses a fixed assignment of ranks. Using the theory of generalized mdps <ref> (Szepesvari & Littman, 1996) </ref>, we can show that this difference is not important from the perspective of proving the existence and uniqueness of the solution of Equation 7. <p> As long as N satisfies the non-expansion property that fi fi fi a O Q 0 (s; a) fi fi max jQ (s; a) Q 0 (s; a)j for all Q-value functions Q and Q 0 and all states s, then Equation 9 has a solution and it is unique <ref> (Szepesvari & Littman, 1996) </ref>; this is proven in Appendix C. The non-expansion property of N can be verified by the following argument. * Consider a family of operators N i Q (s; a) = ith largest value of Q (s; a) for each 1 i m.
Reference: <author> Tesauro, G. </author> <year> (1995). </year> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <pages> pages 58-67. </pages>
Reference: <author> Thrun, S. B. </author> <year> (1992). </year> <title> The role of exploration in learning control. In White, </title> <editor> D. A. and Sofge, D. A., editors, </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY. </address> <note> 22 S. </note> <author> SINGH, T. JAAKKOLA, M.L. LITTMAN AND C. SZEPESV ARI Tsitsiklis, J. N. </author> <year> (1994). </year> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3) </volume> <pages> 185-202. </pages>
Reference: <author> Tsitsiklis, J. N. and Van Roy, B. </author> <year> (1996). </year> <title> An analysis of temporal-difference learning with function approximation. </title> <type> Technical Report LIDS-P-2322, </type> <institution> Massachusetts Institute of Technology. </institution> <note> Available through URL http://web.mit.edu/bvr/www/td.ps. To appear in IEEE Transactions on Automatic Control. </note>
Reference-contexts: On-policy algorithms may prove to be important for several reasons. The analogue of the on-policy/off-policy distinction for RL prediction problems is the trajectory-based/trajectory-free distinction. Trajectory-based algorithms appear superior to trajectory-free algorithms for prediction when parameterized function approxima-tors are used <ref> (Tsitsiklis & Van Roy, 1996) </ref>. These results carry over empirically to the control case as well (Boyan & Moore, 1995; Sutton, 1996).
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, UK. </address>
Reference-contexts: Bertsekas (1995)). Hereafter, unless explicitly noted, all policies are assumed to be stationary. The value function associated with fl is denoted V fl . Often it is convenient to associate values not with states but with state-action pairs, called Q values as in Watkins' Q-learning <ref> (Watkins, 1989) </ref>: Q (s; a) = R (s; a) + flEfV (s 0 )g, and Q fl (s; a) = R (s; a) + flEfV fl (s 0 )g, where s 0 is the random next state on executing action a in state s, and R (s; a) is expected value <p> The update rule is how the algorithm uses experience to change its estimate of the optimal value function. In an off-policy algorithm, the update rule need not have any relation to the learning policy. Q-learning <ref> (Watkins, 1989) </ref> is an off-policy algorithm that estimates the optimal Q-value function as follows: Q t+1 (s t ; a t ) = (1 ff t (s t ; a t ))Q t (s t ; a t ) b where Q t is the estimate at the beginning of the
Reference: <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292. </pages>
Reference-contexts: Following Sutton and Barto (1997), we distinguish between two types of RL algorithms: on-policy and off-policy. Off-policy algorithms may update estimated value functions on the basis of hypothetical actions, i.e., actions other than those actually executed|in this sense Q-learning <ref> (Watkins & Dayan, 1992) </ref> is an off-policy algorithm. On-policy algorithms, on the other hand, update value functions strictly on the basis of the experience gained from executing some (possibly nonstationary) policy. This distinction is important because off-policy algorithms can (at least conceptually) separate exploration from control while on-policy algorithms cannot.
Reference: <author> Williams, R. J. and Baird, III, L. C. </author> <year> (1993). </year> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-14, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA. </address>
Reference: <author> Zhang, W. and Dietterich, T. G. </author> <year> (1995). </year> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intellience, </booktitle> <pages> pages 1114-1120. </pages>
References-found: 33

