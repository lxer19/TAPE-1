URL: ftp://cse.ogi.edu/pub/dsrg/synthetix/icdcs96.ps.gz
Refering-URL: http://www.cse.ogi.edu/~crispin/
Root-URL: http://www.cse.ogi.edu
Email: crispin@cse.ogi.edu  hanan@csd.uwo.ca  
Title: A Wait-free Algorithm for Optimistic Programming: HOPE Realized  
Author: Crispin Cowan Hanan L. Lutfiyya 
Keyword: optimism, concurrency, parallelism, distributed, rollback, wait-free, implementation.  
Address: P.O. Box 91000 Portland, OR 97291-1000  Ontario London, Ontario N6A 5B7  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute  Computer Science Department University of Western  
Abstract: Optimism is a powerful technique for avoiding latency by increasing concurrency. Optimistically assuming the results of one computation allows other computations to execute in parallel, even when they depend on the assumed result. Optimistic techniques can particularly benefit distributed systems because of the critical impact of communications latency. This paper reviews HOPE: our model of optimistic programming, and describes how optimism can enhance distributed program performance by avoiding remote communications delay. We then present the wait-free algorithm used to implement HOPE in a distributed environment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David F. Bacon and Robert E. Strom. </author> <title> Optimistic Paralleliza-tion of Communicating Sequential Processes. </title> <booktitle> In Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: The classic example is optimistic concurrency control: assume that locks will be granted, process the transaction, and post hoc verify that the locks were granted [17]. This paper reviews how to avoid the latency of a remote procedure call by optimistically assuming that the call behaved as expected <ref> [1, 10, 11] </ref>, illustrating why distributed systems particularly benefit from optimism: moving a computation to a remote node increases latency, but does not change the predictability of the computation. fl Supported in part by the National Sciences and EngineeringResearch Council of Canada (NSERC) and ARPA. <p> S 2 takes the line number and checks to see if the line number now exceeds page size. If it does, then S 2 creates a new page; otherwise execution can immediately proceed to S 3 . Bacon and Strom's Call Streaming algorithm <ref> [1] </ref> optimistically parallelizes two such statements. We can parallelize S 1 and S 2 (and hence the statements after S 2 ) by making the optimistic assumption that the report does not end exactly at the bottom of the page, i.e., line &lt; PageSize.
Reference: [2] <author> Rajive L. Bagrodia and Wen-Toh Liao. Maisie: </author> <title> A Language for the Design of Efficient Discrete-Event Simulations. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 20(4) </volume> <pages> 225-238, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Section 6 presents our conclusions and future research. 2 RELATED WORK Optimism has been used in a variety of applications such as fault tolerance [20, 15], replication [5, 16, 21], concurrency control [17], and discrete event simulation <ref> [2, 14] </ref>. However, optimism has not been generally exploited because of the difficulty in writing optimistic algorithms. Optimistic programming is difficult, time-consuming, and ad hoc, for the following reasons. First, checkpointing and rollback is difficult and non-portable.
Reference: [3] <author> Philip A. Bernstein, Vassos Hadzilacos, and Nathan Good-man. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: A direct implementation of these semantics in a distributed system requires updating variables in remote processes. Because HOPE primitives may be executed by concurrent processes, they are subject to concurrency errors due to interference <ref> [3, page 11] </ref>, as detailed in [7]. One way to avoid interference problems is to prevent the interleaved execution of HOPE primitives by serializing execution of HOPE primitives, with unfortunate consequences. <p> A more scalable approach would be to use some form of concurrency control on the HOPE data structures, producing a serializable execution of the HOPE primitives <ref> [3] </ref>. However, the time and space requirements of incorporating a general concurrency control system within the HOPE run-time are prohibitive.
Reference: [4] <author> R.G. Bubenik. </author> <title> Optimistic Computation. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: Previous work in supporting optimistic programming includes <ref> [4, 14] </ref>. However, previous work has either restricted the type of optimistic assumption that can be made, or restricted the scope of optimistic computation [7]. In [4] computation based on an assumption is limited to the scope of a statically defined encapsulation. <p> Previous work in supporting optimistic programming includes [4, 14]. However, previous work has either restricted the type of optimistic assumption that can be made, or restricted the scope of optimistic computation [7]. In <ref> [4] </ref> computation based on an assumption is limited to the scope of a statically defined encapsulation.
Reference: [5] <author> Crispin Cowan. </author> <title> Optimistic Replication in HOPE. </title> <booktitle> In Proceedings of the 1992 CAS Conference, </booktitle> <pages> pages 269-282, </pages> <address> Toronto, Ontario, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: If the assumption is discovered to be correct, then latency has been avoided and performance has improved. However, if the assumption is incorrect, then all computations that used the assumption must be rolled back and re-executed using correct data. Optimism has been used to enhance performance in various areas <ref> [5, 20, 21] </ref>, but mostly embedded in systems, and not exposed to the programmer. Optimism is rarely used in applications because optimistic programs are difficult to write and require ad hoc techniques to implement. <p> This paper presents HOPE (Hopefully Optimistic Programming Environment): a programming model for expressing optimism. Previously, we defined the HOPE programming model and its applicability <ref> [5, 10] </ref>. We constructed a prototype HOPE system [6], defined a formal semantics for HOPE [9], and measured the performance of the prototype [11]. This paper presents the algorithm used to build the prototype. The algorithm is wait-free in that no user process ever blocks when executing a HOPE primitive. <p> Section 6 presents our conclusions and future research. 2 RELATED WORK Optimism has been used in a variety of applications such as fault tolerance [20, 15], replication <ref> [5, 16, 21] </ref>, concurrency control [17], and discrete event simulation [2, 14]. However, optimism has not been generally exploited because of the difficulty in writing optimistic algorithms. Optimistic programming is difficult, time-consuming, and ad hoc, for the following reasons. First, checkpointing and rollback is difficult and non-portable. <p> Elsewhere we have defined a formal semantics for HOPE [9]. The prototype implementation is freely available [8] and we have shown that the prototype can improve RPC performance by of up to 70%. We have also done preliminary investigations into the application of HOPE to replication <ref> [5] </ref> scientific programming [6], and software fault tolerance [18].
Reference: [6] <author> Crispin Cowan. </author> <title> Optimistic Programming in PVM. </title> <booktitle> In Proceedings of the 2nd PVM User's Group Meeting, </booktitle> <institution> Oak Ridge, TN, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: This paper presents HOPE (Hopefully Optimistic Programming Environment): a programming model for expressing optimism. Previously, we defined the HOPE programming model and its applicability [5, 10]. We constructed a prototype HOPE system <ref> [6] </ref>, defined a formal semantics for HOPE [9], and measured the performance of the prototype [11]. This paper presents the algorithm used to build the prototype. The algorithm is wait-free in that no user process ever blocks when executing a HOPE primitive. <p> Elsewhere we have defined a formal semantics for HOPE [9]. The prototype implementation is freely available [8] and we have shown that the prototype can improve RPC performance by of up to 70%. We have also done preliminary investigations into the application of HOPE to replication [5] scientific programming <ref> [6] </ref>, and software fault tolerance [18].
Reference: [7] <author> Crispin Cowan. </author> <title> A Programming Model for Optimism. </title> <type> PhD thesis, </type> <institution> University of Western Ontario, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Previous work in supporting optimistic programming includes [4, 14]. However, previous work has either restricted the type of optimistic assumption that can be made, or restricted the scope of optimistic computation <ref> [7] </ref>. In [4] computation based on an assumption is limited to the scope of a statically defined encapsulation. <p> Assumption identifiers are implemented as AID processes, which are spawned in the course of executing the HOPE guess function. AID processes track the set of processes that depend on the associated assumption identifier. <ref> [7] </ref> details the implementation, including techniques for checkpoint and rollback of a UNIX process. 5 THE HOPE ALGORITHM The HOPE algorithm operates in an environment abstracted from the implementation in Section 4: concurrent processes that communicate via messages. <p> A direct implementation of these semantics in a distributed system requires updating variables in remote processes. Because HOPE primitives may be executed by concurrent processes, they are subject to concurrency errors due to interference [3, page 11], as detailed in <ref> [7] </ref>. One way to avoid interference problems is to prevent the interleaved execution of HOPE primitives by serializing execution of HOPE primitives, with unfortunate consequences. <p> The conflicting operations commute to produce the same re sult, 2. Algorithm 1 corrects for the conflict, or 3. A cyclic dependency is formed. Proof: A construction that shows that for all possible forms of conflict, Algorithm 1 meets one of the criteria <ref> [7, pages 67-73] </ref>. First the set of atomic read and write operations resulting from affirm executions is identified. Then the construction exhaustively shows that for all of the potential conflicts between these read and write operations, they satisfy one of the three conditions in the lemma. <p> Lemma 5.2 For any conflicting concurrentexecution of an affirm primitive and a guess primitive, either: 1. The conflicting operations commute to produce the same re sult, or 2. Algorithm 1 corrects for the conflict. Proof: A similar construction to Lemma 5.1 <ref> [7, pages 73-75] </ref>. Lemma 5.3 Affirm Transitivity: Let B be an interval in process Q that depends on an AID X, i.e., X 2 B:I DO, and let all dependency graphs produced by this execution be acyclic. <p> However, if the dependency graph becomes cyclic, as in Figure 13 when mutual affirm primitives are executed simultaneously, then Algorithm 1 will fail to detect the cycle, and the participating intervals will bounce their way around the cyclic of dependent AIDs forever <ref> [7] </ref>. Algorithm 2 extends Algorithm 1 to detect and remove dependencies from intervals to AIDs that are members of a cycle. Figure 14 shows the dependency graph progression from the cyclic dependency in Figure 13 to a state in which the intervals no longer depend on the cycle.
Reference: [8] <author> Crispin Cowan. </author> <title> HOPE: Hopefully Optimistic Programming Environment. Prototype implementation avialable via FTP from ftp://ftp.csd.uwo.ca/pub/src/hope.tar.gz, </title> <month> August </month> <year> 1995. </year>
Reference-contexts: The model has been implemented, and the algorithm has been shown to be both wait-free, and consistent with the formal semantics of the HOPE primitives with respect to finalizing speculative computation. Elsewhere we have defined a formal semantics for HOPE [9]. The prototype implementation is freely available <ref> [8] </ref> and we have shown that the prototype can improve RPC performance by of up to 70%. We have also done preliminary investigations into the application of HOPE to replication [5] scientific programming [6], and software fault tolerance [18].
Reference: [9] <author> Crispin Cowan and Hanan Lutfiyya. </author> <title> Formal Semantics for Expressing Optimism: The Meaning of HOPE. </title> <booktitle> In 1995 Symposium on the Principles of Distributed Computing (PODC), </booktitle> <address> Ottawa, Ontario, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: This paper presents HOPE (Hopefully Optimistic Programming Environment): a programming model for expressing optimism. Previously, we defined the HOPE programming model and its applicability [5, 10]. We constructed a prototype HOPE system [6], defined a formal semantics for HOPE <ref> [9] </ref>, and measured the performance of the prototype [11]. This paper presents the algorithm used to build the prototype. The algorithm is wait-free in that no user process ever blocks when executing a HOPE primitive. <p> As in Section 4, dependency tracking is implemented using a combination of AID processes and library functions attached to each user process. User process execution is recorded as an execution history of process states composed of intervals, as detailed in the formal semantics of HOPE <ref> [9] </ref>. An interval is a subsequence of an execution history between two executions of the guess primitive, and constitutes the smallest granularity of rollback that may occur. An interval is said to be speculative if that interval can be rolled back; otherwise, the interval is said to be definite. <p> The messages sent to user processes are intercepted by the message passing system and given to the HOPElib attached to each user process for processing. 5.1 THE PROBLEM: INTERFERENCE The HOPE operational semantics <ref> [9] </ref> specify the execution of a HOPE primitive as a sequence of operations. A direct implementation of these semantics in a distributed system requires updating variables in remote processes. <p> Like con-currency control, this produces only serializable executions, but does so at a lower cost by exploiting specific knowledge of the HOPE primitives. We show that such an algorithm is consistent with the HOPE semantics by showing that the algorithm satisfies the following Theorem <ref> [9, page 8] </ref>, which essentially states that the HOPE algorithms finalize precisely those intervals which have been definitely affirmed: Theorem 5.1 For all intervals B, finalize (B) occurs iff affirm (X) is applied to all of the AIDs X 2 B:IDO by intervals that eventually become definite. <p> If the resultant A:IDO set is empty, then interval A is finalized. If the interval is not finalized, then it must update its dependencies. <ref> [9] </ref> specifies that speculative execution of affirm (X) in interval A should add all intervals listed in X:DOM to the DOM set control (message M) target := M.iid switch M.type case Rollback: if target 2 history then rollback (target) end if case Replace: if M.IDO = ; then target.IDO := target.IDO <p> The model has been implemented, and the algorithm has been shown to be both wait-free, and consistent with the formal semantics of the HOPE primitives with respect to finalizing speculative computation. Elsewhere we have defined a formal semantics for HOPE <ref> [9] </ref>. The prototype implementation is freely available [8] and we have shown that the prototype can improve RPC performance by of up to 70%. We have also done preliminary investigations into the application of HOPE to replication [5] scientific programming [6], and software fault tolerance [18].
Reference: [10] <author> Crispin Cowan, Hanan Lutfiyya, and Mike Bauer. </author> <title> Increasing Concurrency Through Optimism: A Reason for HOPE. </title> <booktitle> In Proceedings of the 1994 ACM Computer Science Conference, </booktitle> <pages> pages 218-225, </pages> <address> Phoenix, Arizona, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: The classic example is optimistic concurrency control: assume that locks will be granted, process the transaction, and post hoc verify that the locks were granted [17]. This paper reviews how to avoid the latency of a remote procedure call by optimistically assuming that the call behaved as expected <ref> [1, 10, 11] </ref>, illustrating why distributed systems particularly benefit from optimism: moving a computation to a remote node increases latency, but does not change the predictability of the computation. fl Supported in part by the National Sciences and EngineeringResearch Council of Canada (NSERC) and ARPA. <p> This paper presents HOPE (Hopefully Optimistic Programming Environment): a programming model for expressing optimism. Previously, we defined the HOPE programming model and its applicability <ref> [5, 10] </ref>. We constructed a prototype HOPE system [6], defined a formal semantics for HOPE [9], and measured the performance of the prototype [11]. This paper presents the algorithm used to build the prototype. The algorithm is wait-free in that no user process ever blocks when executing a HOPE primitive.
Reference: [11] <author> Crispin Cowan, Hanan Lutfiyya, and Mike Bauer. </author> <title> Performance Benefits of Optimistic Programming: A Measure of HOPE. </title> <booktitle> In Fourth IEEE International Symposium on High-Performance Distributed Computing (HPDC-4), </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: The classic example is optimistic concurrency control: assume that locks will be granted, process the transaction, and post hoc verify that the locks were granted [17]. This paper reviews how to avoid the latency of a remote procedure call by optimistically assuming that the call behaved as expected <ref> [1, 10, 11] </ref>, illustrating why distributed systems particularly benefit from optimism: moving a computation to a remote node increases latency, but does not change the predictability of the computation. fl Supported in part by the National Sciences and EngineeringResearch Council of Canada (NSERC) and ARPA. <p> This paper presents HOPE (Hopefully Optimistic Programming Environment): a programming model for expressing optimism. Previously, we defined the HOPE programming model and its applicability [5, 10]. We constructed a prototype HOPE system [6], defined a formal semantics for HOPE [9], and measured the performance of the prototype <ref> [11] </ref>. This paper presents the algorithm used to build the prototype. The algorithm is wait-free in that no user process ever blocks when executing a HOPE primitive. We also prove that some important properties of the prototype are consistent with HOPE's formal semantics. Section 2 presents related work. <p> Despite the overhead required for these features, we have shown that HOPE can decrease the latency of a remote procedure call <ref> [11] </ref>. 3 THE HOPE PROGRAMMING MODEL The HOPE programming model is a set of primitives designed to be embedded in some other system. HOPE can be embedded in any message-based concurrent system.
Reference: [12] <author> J. Doyle. </author> <title> A Truth Maintenance System. </title> <journal> Artificial Intelligence, </journal> <volume> 12 </volume> <pages> 231-272, </pages> <year> 1979. </year>
Reference-contexts: In future work, we will show that these algorithms are quadratic in the number of intervals and AIDs associated with an affirm 2 , and we will also extend the application of optimism beyond its traditional domains into new areas such as optimistic specialization [19] and truth maintenance systems <ref> [12] </ref>. 7 ACKNOWLEDGMENTS HOPE was inspired by optimism studies at the IBM T.J. Watson Research Center by Rob Strom et al. Thanks go to Andy Lowry, Jim Russell and Ajei Gopal of IBM for their early comments on 2 We expect the N to be small. this work.
Reference: [13] <author> Al Geist, Adam Geguelin, Jack Dongarra, Wicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM: Parallel Virtual Machine, a Users' Guide and Tutorial for Networked Parallel Computing. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1995. </year>
Reference-contexts: The free of (Order) primitive is used to detect this causality violation and force rollbacks to solve the problem. 4 PROTOTYPE SYSTEM HOPE was built on top of a pre-existing message passing system to make optimistic techniques easily accessible. PVM <ref> [13] </ref> was selected because the source code was freely available, it is well supported by its authors, and has a broad user base, providing a potentially large audience for optimism. PVM is implemented as a library of message passing and administrative functions callable.
Reference: [14] <author> D. Jefferson. </author> <title> Virtual Time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 3(7) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Section 6 presents our conclusions and future research. 2 RELATED WORK Optimism has been used in a variety of applications such as fault tolerance [20, 15], replication [5, 16, 21], concurrency control [17], and discrete event simulation <ref> [2, 14] </ref>. However, optimism has not been generally exploited because of the difficulty in writing optimistic algorithms. Optimistic programming is difficult, time-consuming, and ad hoc, for the following reasons. First, checkpointing and rollback is difficult and non-portable. <p> Previous work in supporting optimistic programming includes <ref> [4, 14] </ref>. However, previous work has either restricted the type of optimistic assumption that can be made, or restricted the scope of optimistic computation [7]. In [4] computation based on an assumption is limited to the scope of a statically defined encapsulation. <p> However, previous work has either restricted the type of optimistic assumption that can be made, or restricted the scope of optimistic computation [7]. In [4] computation based on an assumption is limited to the scope of a statically defined encapsulation. In Time Warp <ref> [14] </ref>, the amount of computation based on a optimistic assumption is not statically bound, but only one kind of optimistic assumption can be made: that messages arrive in time-stamp order. 2.1 HOPE FEATURES Any optimistic assumption can be made, and any method can be used to verify an optimistic assumption, including
Reference: [15] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> Recovery in Distributed Systems using Optimistic Message Logging and Checkpoint-ing. </title> <journal> J. Algorithms, </journal> <volume> 11(3) </volume> <pages> 462-491, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Section 6 presents our conclusions and future research. 2 RELATED WORK Optimism has been used in a variety of applications such as fault tolerance <ref> [20, 15] </ref>, replication [5, 16, 21], concurrency control [17], and discrete event simulation [2, 14]. However, optimism has not been generally exploited because of the difficulty in writing optimistic algorithms. Optimistic programming is difficult, time-consuming, and ad hoc, for the following reasons. First, checkpointing and rollback is difficult and non-portable.
Reference: [16] <author> Puneet Kumar. </author> <title> Coping with Conflicts in an Optimistically Replicated File System. </title> <booktitle> In 1990 Workshop on the Management of Replicated Data, </booktitle> <pages> pages 60-64, </pages> <address> Houston, TX, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Section 6 presents our conclusions and future research. 2 RELATED WORK Optimism has been used in a variety of applications such as fault tolerance [20, 15], replication <ref> [5, 16, 21] </ref>, concurrency control [17], and discrete event simulation [2, 14]. However, optimism has not been generally exploited because of the difficulty in writing optimistic algorithms. Optimistic programming is difficult, time-consuming, and ad hoc, for the following reasons. First, checkpointing and rollback is difficult and non-portable.
Reference: [17] <author> H.T. Kung and John T. Robinson. </author> <title> On Optimistic Methods for Concurrency Control. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(2) </volume> <pages> 213-226, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Optimism increases concurrency by making an assumption about a future state, and verifying the assumption in parallel with computations based on the optimistic assumption. The classic example is optimistic concurrency control: assume that locks will be granted, process the transaction, and post hoc verify that the locks were granted <ref> [17] </ref>. <p> Section 6 presents our conclusions and future research. 2 RELATED WORK Optimism has been used in a variety of applications such as fault tolerance [20, 15], replication [5, 16, 21], concurrency control <ref> [17] </ref>, and discrete event simulation [2, 14]. However, optimism has not been generally exploited because of the difficulty in writing optimistic algorithms. Optimistic programming is difficult, time-consuming, and ad hoc, for the following reasons. First, checkpointing and rollback is difficult and non-portable.
Reference: [18] <author> Hanan Lutfiyya and Crispin Cowan. </author> <title> Language Support for the Application-Oriented Fault Tolerance Paradigm. </title> <note> Submitted for review, </note> <year> 1995. </year>
Reference-contexts: The prototype implementation is freely available [8] and we have shown that the prototype can improve RPC performance by of up to 70%. We have also done preliminary investigations into the application of HOPE to replication [5] scientific programming [6], and software fault tolerance <ref> [18] </ref>.
Reference: [19] <author> Calton Pu, Tito Autrey, Andrew Black, Charles Consel, Crispin Cowan, Jon Inouye, Lakshmi Kethana, Jonathan Walpole, and Ke Zhang. </author> <title> Optimistic Incremental Specialization: Streamlining a Commercial Operating System. </title> <booktitle> In Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Copper Mountain, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: In future work, we will show that these algorithms are quadratic in the number of intervals and AIDs associated with an affirm 2 , and we will also extend the application of optimism beyond its traditional domains into new areas such as optimistic specialization <ref> [19] </ref> and truth maintenance systems [12]. 7 ACKNOWLEDGMENTS HOPE was inspired by optimism studies at the IBM T.J. Watson Research Center by Rob Strom et al.
Reference: [20] <author> R.E. Strom and S. Yemini. </author> <title> Optimistic Recovery in Distributed Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: If the assumption is discovered to be correct, then latency has been avoided and performance has improved. However, if the assumption is incorrect, then all computations that used the assumption must be rolled back and re-executed using correct data. Optimism has been used to enhance performance in various areas <ref> [5, 20, 21] </ref>, but mostly embedded in systems, and not exposed to the programmer. Optimism is rarely used in applications because optimistic programs are difficult to write and require ad hoc techniques to implement. <p> Section 6 presents our conclusions and future research. 2 RELATED WORK Optimism has been used in a variety of applications such as fault tolerance <ref> [20, 15] </ref>, replication [5, 16, 21], concurrency control [17], and discrete event simulation [2, 14]. However, optimism has not been generally exploited because of the difficulty in writing optimistic algorithms. Optimistic programming is difficult, time-consuming, and ad hoc, for the following reasons. First, checkpointing and rollback is difficult and non-portable.
Reference: [21] <author> P. Triantafillou and D.J. Taylor. </author> <title> A New Paradigm for High Availability and Efficiency in Replicated and Distributed Databases. </title> <booktitle> In 2nd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 136-143, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: If the assumption is discovered to be correct, then latency has been avoided and performance has improved. However, if the assumption is incorrect, then all computations that used the assumption must be rolled back and re-executed using correct data. Optimism has been used to enhance performance in various areas <ref> [5, 20, 21] </ref>, but mostly embedded in systems, and not exposed to the programmer. Optimism is rarely used in applications because optimistic programs are difficult to write and require ad hoc techniques to implement. <p> Section 6 presents our conclusions and future research. 2 RELATED WORK Optimism has been used in a variety of applications such as fault tolerance [20, 15], replication <ref> [5, 16, 21] </ref>, concurrency control [17], and discrete event simulation [2, 14]. However, optimism has not been generally exploited because of the difficulty in writing optimistic algorithms. Optimistic programming is difficult, time-consuming, and ad hoc, for the following reasons. First, checkpointing and rollback is difficult and non-portable.
References-found: 21

