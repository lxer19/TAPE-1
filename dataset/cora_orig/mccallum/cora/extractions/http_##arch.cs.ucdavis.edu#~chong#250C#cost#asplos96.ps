URL: http://arch.cs.ucdavis.edu/~chong/250C/cost/asplos96.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/cost/
Root-URL: http://www.cs.ucdavis.edu
Title: Abstract  
Abstract: One potentially attractive way to build large-scale shared-memory machines is to use small-scale to medium-scale shared-memory machines as clusters that are interconnected with an off-the-shelf network. To create a shared-memory programming environment across the clusters, it is possible to use a virtual shared-memory software layer. Because of the low latency and high bandwidth of the interconnect available within each cluster, there are clear advantages in making the clusters as large as possible. The critical question then becomes whether the latency and bandwidth of the top-level network and the software system are sufficient to support the communication demands generated by the clusters. To explore these questions, we have built an aggressive kernel implementation of a virtual shared-memory system using SGI multiprocessors and 100Mbyte/sec HIPPI interconnects. The system obtains speedups on 32 processors (four nodes, eight processors per node plus additional reserved protocol processors) that range from 6.9 on the communication-intensive FFT program to 21.6 on Ocean (both from the SPLASH 2 suite). In general, clustering is effective in reducing internode miss rates, but as the cluster size increases, increases in the remote latency, mostly due to increased TLB synchronization cost, offset the advantages. For communication-intensive applications, such as FFT, the overhead of sending out network requests, the limited network bandwidth, and the long network latency prevent the achievement of good performance. Overall, this approach still appears promising, but our results indicate that large low latency networks may be needed to make cluster-based virtual shared-memory machines broadly useful as large-scale shared-memory multiprocessors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, R. Bianchini, D. Chaiken, K. Johnson, D Kranz, J. Kubiatowicz, Beng-Hong Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance, </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: MGS by Yeung et al. is the closest to our system. Their proposal for a mixed page and cache line-based system is similar to ours but their investigation differs in many ways. Because their experimental test bed, the MIT Alewife <ref> [1] </ref> machine, lacks TLB translation hardware, their experiments simulate the operation of the TLB using compiler generated software translation instructions and software handlers. The issue of TLB synchronization is discussed in the design section, but it is unclear whether it was accurately modeled in the software handlers.
Reference: [2] <author> Brian Bershad and Matthew J. Zekauskas. Midway: </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors, </title> <note> Carnegie Mellon University Technical Report No. CMU-CS 91-170, </note> <month> September </month> <year> 1991. </year>
Reference: [3] <author> J.B. Carter. </author> <title> Design of the Munin Distributed Shared Memory System, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 29(2) </volume> <pages> 219-27, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Delayed invalidate release consistency was proposed by Dubois [8] as a way of reducing false sharing, but we use it to limit the overhead of TLB synchronization. Several other fully operational software shared-memory systems exist, including Munin <ref> [3] </ref>, Treadmarks [17] and MGS [28]. Neither Munin nor Treadmarks have been studied in clustered environments outside of simulation. MGS by Yeung et al. is the closest to our system.
Reference: [4] <author> A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software versus Hardware Shared-memory Implementation: a Case Study, </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 106-17, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Clustering effects in hardware cache coherent shared-memory multiprocessors have been investigated in [7] and [9]. Those investigations do not consider page-based systems. Hybrid hardware software page-based systems, similar to what we look at, are studied in simulation by Cox et al <ref> [4] </ref> and Karlsson et al [14]. Cox looks at different applications then we do, making direct comparisons difficult. Karlsson looks at one of the same applications, FFT and their results are similar to our findings in terms of overall speedup obtained for comparable degrees of clustering.
Reference: [5] <author> Rohit Chandra, K. Gharachorloo, V. Soundararajan, and A. Gupta. </author> <title> Performance Evaluation of Hybrid Hardware and Software Distributed Shared Memory Protocols, </title> <booktitle> In Proceedings of International Conference on Supercomputing 94, </booktitle> <pages> pp. 274-288, </pages> <month> July </month> <year> 1994. </year>
Reference: [6] <author> Jeffery Chase, F. Amador, E. Lazowska, H. Levy, and R. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors, </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating System Principles, </booktitle> <pages> pp. 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Karlsson looks at one of the same applications, FFT and their results are similar to our findings in terms of overall speedup obtained for comparable degrees of clustering. Hybrid hardware software parallel programming has been studied by Chase using the Amber system <ref> [6] </ref>. The system does not provide a global coherent address space, instead providing a shared object space that requires explicit annotations.
Reference: [7] <author> D. R. Cheriton, H. Goosen and P. Boyle. </author> <title> Multi-level Shared Caching Techniques for Scalability in VMP-MC, </title> <booktitle> In Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <pages> pp. 16-24, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Most importantly, since they partition a hardware shared-memory machine to build a software shared-memory machine, they scale the internode bandwidth as they increase the cluster size, which we do not. Clustering effects in hardware cache coherent shared-memory multiprocessors have been investigated in <ref> [7] </ref> and [9]. Those investigations do not consider page-based systems. Hybrid hardware software page-based systems, similar to what we look at, are studied in simulation by Cox et al [4] and Karlsson et al [14]. Cox looks at different applications then we do, making direct comparisons difficult.
Reference: [8] <author> M. Dubois, J. C. Wang, L. A. Barroso, K. L. Lee, and Y. Chen. </author> <title> Delayed Consistency and its Effect on the Miss Rate of Parallel Programs, </title> <booktitle> Proceedings of SuperComputing 95, </booktitle> <pages> pp. 197-206, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Our system assumes that programs are written in a synchronized style where all references to shared memory are protected by locks or barriers. The prototype provides limited extensions to permit application programs to synchronize on memory flags without mutual exclusion locks under Delayed Invalidate Release Consistency <ref> [8] </ref> (DIRC). The application is expected to fork into parallel threads which work together to solve a problem, communicating via shared memory and synchronizing using locks and barriers. SoftFLASH supports prefetching of page data through application function calls. <p> This constraint is sufficient to guarantee correctness. While a process waits for an epoch to clear, other processes can continue to execute writes, which only affect subsequent epochs. In addition to the FLASH modes, we implement a modified form of Delayed Invalidate Release Consistency <ref> [8] </ref> (DIRC) that postpones the processing of invalidation messages on a remote node until the next synchronization acquire point. We implement DIRC partially to reduce false sharing, but mainly to reduce the impact of invalidation traffic on performance. <p> The use of the virtual to physical translation mechanism to enforce coherency was originally suggested by Kai Li [20]. The directory-based invalidation protocol used between the nodes was developed by Kuskin et al [18]. Delayed invalidate release consistency was proposed by Dubois <ref> [8] </ref> as a way of reducing false sharing, but we use it to limit the overhead of TLB synchronization. Several other fully operational software shared-memory systems exist, including Munin [3], Treadmarks [17] and MGS [28]. Neither Munin nor Treadmarks have been studied in clustered environments outside of simulation.
Reference: [9] <author> Andrew Erlichson, Basem Nayfeh, Jaswinder P. Singh and Kunle Olukotun. </author> <title> The Benefits of Clustering in Shared Address Space Multiprocessors: An Applications Driven Investigation, </title> <booktitle> Proceedings of SuperComputing 95, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The miss latency per miss, however, shown in Figure 5b, increases when clustered by nearly a corresponding amount, countering the effects of the reduced miss rate. The overall result, shown in Figure 5c, is that the execution time goes down by only about 2%. Analytically, FFT exhibits all-to-all sharing <ref> [9] </ref>. Hence, in the 4x2p case, each process communicates with six off-node processes. When more highly clustered, in a 2x4p configuration, each process communicates with four off-node processes. Hence the per-process communication miss rate should go down by a factor of 4/6. <p> Most importantly, since they partition a hardware shared-memory machine to build a software shared-memory machine, they scale the internode bandwidth as they increase the cluster size, which we do not. Clustering effects in hardware cache coherent shared-memory multiprocessors have been investigated in [7] and <ref> [9] </ref>. Those investigations do not consider page-based systems. Hybrid hardware software page-based systems, similar to what we look at, are studied in simulation by Cox et al [4] and Karlsson et al [14]. Cox looks at different applications then we do, making direct comparisons difficult.
Reference: [10] <author> Ewing Lusk. </author> <title> Portable Programs for Parallel Processors, </title> <publisher> Holt, Rinehart, and Winston, </publisher> <address> New York, </address> <year> 1987 </year>
Reference-contexts: This facility permits us to run applications written to the fork model unchanged. Most of the SPLASH II [27] programs can be run unmodified on the system, requiring only recompilation and a different set of Argonne National Laboratory (ANL) macros <ref> [10] </ref>. (Some applications did require moving the ANL initialization functions.) The ability to run unmodified applications eliminates tedious porting and permits users to program to a single interface that is already familiar to them. Relaxed Memory Consistency We implement the memory consistency modes that are used by FLASH.
Reference: [11] <author> K. Gharachorloo, Dan Lenoski, James Laudon, P. Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors, </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pp. 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In Delayed mode a writer must wait until all invalidates for a coherency object have been acknowledged before continuing. In Eager mode, the writer may continue as soon as ownership is transferred and the data has returned. Eager mode, a form of release consistency <ref> [11] </ref>, permits the faulting cache to retire writes before all invalidation messages have been acknowledged. At the next release operation, the process must wait until all previous writes have been acknowledged before proceeding.
Reference: [12] <author> Chris Holt and Jaswinder Pal Singh. </author> <title> Hierarchical N-Body Methods on Shared Address Space Multiprocessors, </title> <booktitle> In Proceedings of the Seventh SIAM International Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pp. 313-18, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: FFT [25] performs a fast Fourier transform on a 1D array of complex points. It is known to have very high communication demands and typically does not run well on software-based systems [15]. Barnes <ref> [12] </ref> simulates a system of particles using the Barnes-Hut hierachical N-body method. LU [26] is a linear algebra kernel that factors a matrix into two matrices; one lower triangular, the other upper triangular. It is known to have a relatively small communication to computation ratio.
Reference: [13] <author> Kirk Johnson, M. F. Kaashoek and D. Wallach. </author> <title> CRL: High-performance All-software Distributed Shared Memory, </title> <booktitle> In Fifteenth AC Symposium on Operating Systems Principles, </booktitle> <pages> pp. 213-28, </pages> <month> December </month> <year> 1995. </year>
Reference: [14] <author> Magnus Karlsson and Per Stenstrom. </author> <title> Performance Evaluation of a Cluster-Based Multiprocessor Built from ATM Switches and Bus-Based Multiprocessor Servers, </title> <booktitle> In Proceedings of the Second International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pp. 4-13, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Clustering effects in hardware cache coherent shared-memory multiprocessors have been investigated in [7] and [9]. Those investigations do not consider page-based systems. Hybrid hardware software page-based systems, similar to what we look at, are studied in simulation by Cox et al [4] and Karlsson et al <ref> [14] </ref>. Cox looks at different applications then we do, making direct comparisons difficult. Karlsson looks at one of the same applications, FFT and their results are similar to our findings in terms of overall speedup obtained for comparable degrees of clustering.
Reference: [15] <author> Peter Keleher. </author> <title> Lazy Release Consistency for Distributed Shared Memory, </title> <type> PhD Thesis, </type> <institution> Rice University, Houston, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: With good data distribution, the last writer is often the home. This latency, 1164ms for a 16kbyte page, compares favorably with latencies recorded on other systems. For example, TreadMarks, a user-level implementation on a uniprocessor system, reports a latency of 1956ms to fault a 4kbyte page over ATM <ref> [15] </ref>. A user-level implementation of SoftFLASH (done for debugging), which runs the same protocol code, moves a 16kbyte page in 3216ms over HIPPI (150 Mhz R4400-based systems). SoftFLASH (kernel level) moves 4kbyte pages on four-cpu R4400-based systems in about 1100ms. <p> We ran a subset of applications from the SPLASH-2 suite of applications: FFT, Barnes, LU, and Ocean. FFT [25] performs a fast Fourier transform on a 1D array of complex points. It is known to have very high communication demands and typically does not run well on software-based systems <ref> [15] </ref>. Barnes [12] simulates a system of particles using the Barnes-Hut hierachical N-body method. LU [26] is a linear algebra kernel that factors a matrix into two matrices; one lower triangular, the other upper triangular. It is known to have a relatively small communication to computation ratio.
Reference: [16] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory, </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference: [17] <author> P. Keleher, Alan Cox, S. Dwarkadas and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems, </title> <booktitle> In Proceedings of USENIX Winter 1994 Conference, </booktitle> <pages> pp. 115-32, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: It has been proposed [4,28] that large multiprocessors be built by connecting hardware-based cache-coherent multiprocessors using software shared memory. Since software shared memory has been shown to be scalable to a small number of processing nodes <ref> [17] </ref>, it would seem well suited to connecting nodes of multiprocessors. In this paper, we investigate the viability of building multiprocessors using a hybrid scheme whereby software shared memory is used to connect hardware-based shared-memory multiprocessors. <p> Delayed invalidate release consistency was proposed by Dubois [8] as a way of reducing false sharing, but we use it to limit the overhead of TLB synchronization. Several other fully operational software shared-memory systems exist, including Munin [3], Treadmarks <ref> [17] </ref> and MGS [28]. Neither Munin nor Treadmarks have been studied in clustered environments outside of simulation. MGS by Yeung et al. is the closest to our system. Their proposal for a mixed page and cache line-based system is similar to ours but their investigation differs in many ways.
Reference: [18] <author> Jeff Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, K, Gharachorloo, J. Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum and John Hennessy, </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pp. 18-21, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The use of the virtual to physical translation mechanism to enforce coherency was originally suggested by Kai Li [20]. The directory-based invalidation protocol used between the nodes was developed by Kuskin et al <ref> [18] </ref>. Delayed invalidate release consistency was proposed by Dubois [8] as a way of reducing false sharing, but we use it to limit the overhead of TLB synchronization. Several other fully operational software shared-memory systems exist, including Munin [3], Treadmarks [17] and MGS [28].
Reference: [19] <author> W. Leler. </author> <title> System-level Parallel Programming Based on Linda, </title> <booktitle> In Proceedings of the Third North American Transputer Users Group, </booktitle> <pages> pp. 175-9, </pages> <month> April </month> <year> 1990. </year>
Reference: [20] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The use of the virtual to physical translation mechanism to enforce coherency was originally suggested by Kai Li <ref> [20] </ref>. The directory-based invalidation protocol used between the nodes was developed by Kuskin et al [18]. Delayed invalidate release consistency was proposed by Dubois [8] as a way of reducing false sharing, but we use it to limit the overhead of TLB synchronization.
Reference: [21] <author> Ron Minnich. Mether-NFS: </author> <title> A Modified NFS which supports Virtual Shared Memory, </title> <booktitle> In Proceedings of Symposium on Experiences with Distributed and Multiprocessor Systems IV, </booktitle> <pages> pp. 89-107, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: DMA I/O in the SGI machines is cache coherent, making it a good candidate for virtual shared memory. From a software standpoint, our shared-memory space resides within the virtual memory system as a mapped file system, an idea taken from Minnichs Mether-NFS system <ref> [21] </ref>. Memory coherency on a page is enforced by removing a processors ability to translate virtual addresses into physical addresses. Globally shared pages maintained by SoftFLASH are kept within a shared software page cache in main memory on each node. The FLASH protocol requires each page to have a home.
Reference: [22] <author> Bryan S. Rosenburg. </author> <title> Low-Synchronization Translation Lookaside Buffer Consistency in Large-Scale Shared Memory Multiprocessors, </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating System Principles, </booktitle> <pages> pp. 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: TLB Synchronization costs in multiprocessor systems have been studied by Rosenburg <ref> [22] </ref>. He suggests a scheme for reducing the cost of keeping the TLBs synchronized that is similar to the current scheme used in Irix 6.2.
Reference: [23] <author> Dan Scales and Monica Lam. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines, </title> <booktitle> In Proceedings of 1st Symposium on Operation Systems Design and Implementation, </booktitle> <pages> pp. 101-14, </pages> <month> November </month> <year> 1994. </year>
Reference: [24] <author> Michael Y. Thompson, J. M. Barton, T. Jermoluk, and J. Wagner. </author> <title> Translation Lookaside Buffer Synchronization in a Multiprocssor System, </title> <booktitle> In Proceeding of USENIX Association Winter Conference, </booktitle> <pages> pp. 297-302, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: For large number of processors the time it takes to get through a TLB synchronization operation climbs to above 350ms. The process used to synchronize the TLBs scales poorly. The calling processor interrupts all other processors and waits for them to invalidate the TLB entry and rendezvous <ref> [24] </ref>. With larger numbers of processors the probability of finding a processor in an interruptible state, and having to wait, increases. The results shown in this paper run with a modified unix TLB invalidate routine that only interrupts processors that have had DVSM processes scheduled on them.
Reference: [25] <author> Steven Cameron Woo, Jaswinder Pal Singh, and John L. Hennessy. </author> <title> The Performance Advantages of Integrating Block Data Transfer in Cache-Coherent Multiprocessors, </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pp. 219-229, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: We ran a subset of applications from the SPLASH-2 suite of applications: FFT, Barnes, LU, and Ocean. FFT <ref> [25] </ref> performs a fast Fourier transform on a 1D array of complex points. It is known to have very high communication demands and typically does not run well on software-based systems [15]. Barnes [12] simulates a system of particles using the Barnes-Hut hierachical N-body method.
Reference: [26] <author> Steven Cameron Woo, Jaswinder Pal Singh, and John L. Hennessy. </author> <title> The Performance Advantages of Integrating Block Data Transfer in Cache-Coherent Multiprocessors, </title> <institution> Stanford University Technical Report No. CSL-TR-93-593, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: FFT [25] performs a fast Fourier transform on a 1D array of complex points. It is known to have very high communication demands and typically does not run well on software-based systems [15]. Barnes [12] simulates a system of particles using the Barnes-Hut hierachical N-body method. LU <ref> [26] </ref> is a linear algebra kernel that factors a matrix into two matrices; one lower triangular, the other upper triangular. It is known to have a relatively small communication to computation ratio. Ocean [26] simulates large ocean currents. <p> Barnes [12] simulates a system of particles using the Barnes-Hut hierachical N-body method. LU <ref> [26] </ref> is a linear algebra kernel that factors a matrix into two matrices; one lower triangular, the other upper triangular. It is known to have a relatively small communication to computation ratio. Ocean [26] simulates large ocean currents. It is interesting because it exhibits nearest neighbor sharing, and should therefore be an excellent candidate for a clustered system. For an overview of the SPLASH applications and their characteristics see [27]. The applications were chosen for a variety of reasons.
Reference: [27] <author> Steven Cameron Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations, </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The prototype supports a limited soft fork that permits applications to inherit the memory state of the non-shared portions of the address space at the time of fork. This facility permits us to run applications written to the fork model unchanged. Most of the SPLASH II <ref> [27] </ref> programs can be run unmodified on the system, requiring only recompilation and a different set of Argonne National Laboratory (ANL) macros [10]. (Some applications did require moving the ANL initialization functions.) The ability to run unmodified applications eliminates tedious porting and permits users to program to a single interface that <p> It is known to have a relatively small communication to computation ratio. Ocean [26] simulates large ocean currents. It is interesting because it exhibits nearest neighbor sharing, and should therefore be an excellent candidate for a clustered system. For an overview of the SPLASH applications and their characteristics see <ref> [27] </ref>. The applications were chosen for a variety of reasons. First, they represent a wide spectrum in terms of communication to computation, with FFT known to have high communication demands, that drop off slowly as problem size is increased, and LU known to have relatively light communication demands. <p> Third, they do not exhibit any pathological false sharing problems with a single writer protocol. One SPLASH application, radix, can be easily scaled and would be interesting to run, but is known to suffer from bad false sharing at large coherency block sizes <ref> [27] </ref>. All runs were performed on one to four (16 to 18)-processor R8000 machines in various clustered configurations of up to eight Table 3: Applications Code Problem Size Growth Rate of Comm. to Comp.
Reference: [28] <author> Donald Yeung, John Kubiatowicz, and Anant Agarwal. MGS: </author> <title> A Multi-Grain Shared Memory System, </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 44-55, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Delayed invalidate release consistency was proposed by Dubois [8] as a way of reducing false sharing, but we use it to limit the overhead of TLB synchronization. Several other fully operational software shared-memory systems exist, including Munin [3], Treadmarks [17] and MGS <ref> [28] </ref>. Neither Munin nor Treadmarks have been studied in clustered environments outside of simulation. MGS by Yeung et al. is the closest to our system. Their proposal for a mixed page and cache line-based system is similar to ours but their investigation differs in many ways.
References-found: 28

