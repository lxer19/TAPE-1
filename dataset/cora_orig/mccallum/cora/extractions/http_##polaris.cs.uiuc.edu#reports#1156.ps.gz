URL: http://polaris.cs.uiuc.edu/reports/1156.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Abstract-found: 0
Intro-found: 1
Reference: <institution> -25 </institution>
Reference: [AbPa90] <author> S. Abraham and K. Padmanabhan, </author> <title> ``Constraint Based Evaluation of Multicomputer Networks,'' </title> <booktitle> 1990 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: Meaningful predictions of network performance must take into account limited channel widths and wiring costs. In [Dall90], k-ary n-cubes with constant wiring costs were compared. Bisection width [Thom79] was used as an estimate of wiring area. In <ref> [AbPa90] </ref>, both switch pinout and bisection width were used as cost factors in analytical studies of k-ary n-cubes. <p> This avoids the problems discussed in [FrWT82]. The FH and CH have been studied in detail in [HsYe91]. We will concentrate on the bidirectional mesh and MSN in this paper. 3. Performance analysis of meshes We will adapt the analyses in <ref> [AbPa90] </ref> for message switching and partial cut-through, in clustered systems. In message switching for multiple-nibble messages, an entire message has to arrive at the output port of a switch before it can be forwarded. <p> Partial cut-through (similar to wormhole routing [Dall90]) allows a message with enough routing information to ``cut-through'' to the next switch, even if part of the message has not arrived <ref> [AbPa90] </ref>. <p> As with the hypercube, the mesh performs better with -11 LR routing. <ref> [AbPa90] </ref> presented analysis of the mesh with random routing. We will present queueing analysis of the mesh with LR routing in the next section. As in [HsYe91], for the clustered systems, we assume random routing for convenience of analysis. Hence, our performance numbers for CMs and CHs are conservative estimates. <p> We will use the terms ``bus'' and ``queue'' interchangeably for the cluster/global connections. We assume that all the channels of the global switch have the same bandwidth. The assumption of infinite buffering in <ref> [AbPa90] </ref> is unrealistic. However, we expect network operation to be in regions of moderate link utilization (&lt;80%), where the message delay curve is relatively flat and few buffers are necessary. Our analysis may be considered an upper bound on performance. <p> The above analysis can be adapted to cluster local traffic. The process is relatively straightforward but laborious [Hsu92]. We performed detailed simulations of some of the networks (see [Hsu92] for data). Our simulation model largely followed standard network queueing models with cut-through routing. such as that of <ref> [AbPa90] </ref>. For link utilizations of up to about 75% the analysis predicts delays which are within 1% to 15% of the simulation results. Discrepancies between analysis and simulation results increase for heavier loads, because the assumption of independent arrivals [Abra90] results in inaccuracies. <p> For partial cut-through, the differences between analysis and simulation results are larger, percentage-wise. Most of the inaccuracy came from the message switching analysis. While our approximation for the savings from cut-through is much simpler than that of <ref> [AbPa90] </ref>, it is a fairly standard procedure for pipelined switches (see, for example, [Agar91]). The term Dt m -16 predicts fairly accurately the savings from cut-through in numerous simulation runs (we have checked data both from our own runs and from Abraham's experiments). <p> Hence, in system design, task granularity and the size of messages are important considerations. Wide network channels are not efficiently utilized if the average message size is two or three nibbles. -21 In comparison with the results of <ref> [AbPa90] </ref> for constant pinout, it appears that the saturation performance of the FM relative to the FH is better than the results in [AbPa90]. <p> Wide network channels are not efficiently utilized if the average message size is two or three nibbles. -21 In comparison with the results of <ref> [AbPa90] </ref> for constant pinout, it appears that the saturation performance of the FM relative to the FH is better than the results in [AbPa90]. This is because, when processors are grouped on the same board, the FM is more wire-efficient than the FH: only the processors on the perimeter of the submesh have channels that route off-board, while every processor in a subcube has off-board channels. <p> The systems with constant pinout in Figures 4 and 5 also have constant bisection width. In been omitted for conciseness. They are compared with the various mesh-based systems, but under the constant bisection width constraint. As observed in <ref> [AbPa90] </ref>, FHs and FMs with constant bisection width have similar saturation performance. The FM has better queueing performance than the FH. The CH has similar saturation performance as the FH, and hence the FM.
Reference: [Abra90] <author> S. Abraham, </author> <title> Issues in the Architecture of Direct Interconnection Schemes for Multiprocessors, </title> <type> PhD Thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: Let P t be the probability of termination, i.e., the probability that a message received on a link will terminate at that node. For the flat systems, m is also the traffic rate on a local link. For an FM, from <ref> [Abra90] </ref>, P t =2 (N 1 -1)/( dd N 1 N 1 ). t m = R L/C H , and m=t m m g /4P t . <p> Let P y,x be the probability that a message received on a y link will continue on an x link. Let P x,x be the probability that a message received on an x link will continue on an x link. -14 We follow the method used in <ref> [Abra90] </ref>, and count the number of source-destination (S-D) pairs that use specific links. Consider an incoming y link to processor i, and the messages that terminate at i. <p> For link utilizations of up to about 75% the analysis predicts delays which are within 1% to 15% of the simulation results. Discrepancies between analysis and simulation results increase for heavier loads, because the assumption of independent arrivals <ref> [Abra90] </ref> results in inaccuracies. For partial cut-through, the differences between analysis and simulation results are larger, percentage-wise. Most of the inaccuracy came from the message switching analysis.
Reference: [Agar91] <author> A. Agarwal, </author> <title> ``Limits on interconnection network performance,'' </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 2, No. 4, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: Most of the inaccuracy came from the message switching analysis. While our approximation for the savings from cut-through is much simpler than that of [AbPa90], it is a fairly standard procedure for pipelined switches (see, for example, <ref> [Agar91] </ref>). The term Dt m -16 predicts fairly accurately the savings from cut-through in numerous simulation runs (we have checked data both from our own runs and from Abraham's experiments). Our analytical predictions are adequate for the system sizes and loads of interest.
Reference: [AgMa85] <author> D. Agrawal and I. Mahgoub, </author> <title> ``Performance analysis of cluster-based supersystems,'' </title> <booktitle> 1st Int. Conf. on Supercomputer Systems, IEEE Comp. </booktitle> <publisher> Soc. Press, </publisher> <year> 1985. </year>
Reference-contexts: Experience with parallel applications has shown that reorganizing a parallel program to exploit two levels of architectural hierarchy is a non-trivial problem. A partial bibliography of hierarchical system organizations would include [WuLi81] (clusters of processors using shared buses), <ref> [AgMa85] </ref> (clusters of processors with crossbar switches), [Carl85] (local meshes of processors, with a global mesh connecting the clusters), [DaEa90] (two-level systems with many network topologies), [Padm91] (combinations of Omega networks and composite cube networks), [HsYe91] and [ChHo91] (systems with buses within a cluster and a global hypercube network). <p> The biggest argument against this approach is it is simply not realistic to build huge crossbars (say, beyond 20x20) and expect -7 them to have the same cycle time as small crossbars, without an overwhelming amount of control logic. The processors in <ref> [AgMa85] </ref> have equitable access to the global crossbar, but are also connected to local crossbars; the system in [IrNa88] is also similar. TICNET [GiMo91] also uses local crossbars and global crossbars. This means that intercluster traffic takes only one hop through intermediate clusters.
Reference: [Carl85] <author> D. Carlson, </author> <title> ``The mesh with a global mesh: a flexible, high-speed organization for parallel computation,'' </title> <booktitle> 1st Int. Conf. on Supercomputer Systems, IEEE Comp. </booktitle> <publisher> Soc. Press, </publisher> <year> 1985. </year>
Reference-contexts: Experience with parallel applications has shown that reorganizing a parallel program to exploit two levels of architectural hierarchy is a non-trivial problem. A partial bibliography of hierarchical system organizations would include [WuLi81] (clusters of processors using shared buses), [AgMa85] (clusters of processors with crossbar switches), <ref> [Carl85] </ref> (local meshes of processors, with a global mesh connecting the clusters), [DaEa90] (two-level systems with many network topologies), [Padm91] (combinations of Omega networks and composite cube networks), [HsYe91] and [ChHo91] (systems with buses within a cluster and a global hypercube network). <p> We call this the shared global channels approach. The processors within the cluster are connected to this switch either directly or through some intermediate network. Thus, the remaining issue is how these processors access the global switch. The approach taken by <ref> [Carl85] </ref> and [DaEa90] is to have a single processor as the interface processor that directly accesses the global switch. All other processors are connected to the interface processor through a local network (which may be a mesh, hypercube etc).
Reference: [ChAg90] <author> T. Chung and D. Agrawal, </author> <title> ``On network characterization of and optimal broadcasting in the Manhattan Street network,'' </title> <booktitle> Proc. of INFOCOM '90. </booktitle>
Reference-contexts: Messages are broken into large nibbles, which means there is more ``wasted space'' in wide channels. For an FMSN, P t =(N 1 -1)/(N 1 dd N 1 /2+N 1 -4) (see <ref> [ChAg90] </ref> or [Hsu92] for independent derivations of average message distance in an FMSN). t m = R L/C H , and m=t m m g /(2P t ).
Reference: [ChHo91] <author> S. Chowdhury and M. Holliday, </author> <title> ``Stability and performance of alternative two-level interconnection networks,'' </title> <booktitle> 1991 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: of hierarchical system organizations would include [WuLi81] (clusters of processors using shared buses), [AgMa85] (clusters of processors with crossbar switches), [Carl85] (local meshes of processors, with a global mesh connecting the clusters), [DaEa90] (two-level systems with many network topologies), [Padm91] (combinations of Omega networks and composite cube networks), [HsYe91] and <ref> [ChHo91] </ref> (systems with buses within a cluster and a global hypercube network). Except for [HsYe91], these studies have not taken into account the effects of packaging constraints on system performance, and have often restricted the discussion to traffic patterns with high locality. <p> With the fat tree approach, depending on the local network channels close to the interface processor may require more bandwidth than the global channels, and additional buffering and rate-matching hardware is necessary. Our approach is very similar to that of <ref> [ChHo91] </ref>; detailed definitions are in Section 2.2. 2.1.2 Partitioned Global Channels It is possible to have fixed the topology of the global network, but not build a true clustered mesh, hypercube etc with a large global switch per cluster.
Reference: [Dall90] <author> W. Dally, </author> <title> ``Performance Analysis of k-ary n-cube Interconnection Networks,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-39, No. 6, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: NASA NCC 2-559, and Sun Microsystems. -1 -2 construction of narrow channels. Long messages have to be broken into multiple nibbles and transmitted in many cycles. Meaningful predictions of network performance must take into account limited channel widths and wiring costs. In <ref> [Dall90] </ref>, k-ary n-cubes with constant wiring costs were compared. Bisection width [Thom79] was used as an estimate of wiring area. In [AbPa90], both switch pinout and bisection width were used as cost factors in analytical studies of k-ary n-cubes. <p> In message switching for multiple-nibble messages, an entire message has to arrive at the output port of a switch before it can be forwarded. Partial cut-through (similar to wormhole routing <ref> [Dall90] </ref>) allows a message with enough routing information to ``cut-through'' to the next switch, even if part of the message has not arrived [AbPa90]. <p> Our analysis may be considered an upper bound on performance. We do not expect the relative performance of our systems to change if only a small number of buffers are available per switch (as in <ref> [Dall90] </ref>). 3.1 Analysis of bidirectional mesh (LR routing) Consider a flat mesh. Call the high-order dimension y, and the low-order dimension x. The layout of the processors is arbitrary, but for convenience of discussion, we assume that the x dimension corresponds to rows, and the y dimension to columns. <p> Some of the queueing performance at low traffic will be lost, but the system will saturate at higher values of m g . 4.2 Systems with constant bisection width As in <ref> [Dall90] </ref>, we will use bisection width as a measure of wiring area, and compare flat and clustered systems with constant bisection width. It is generally straightforward to calculate -22 the bisection width of a network. We can adapt the terms from [Dall90] making appropriate adjustments for bidirectional channels. <p> . 4.2 Systems with constant bisection width As in <ref> [Dall90] </ref>, we will use bisection width as a measure of wiring area, and compare flat and clustered systems with constant bisection width. It is generally straightforward to calculate -22 the bisection width of a network. We can adapt the terms from [Dall90] making appropriate adjustments for bidirectional channels. For a hypercube with N 1 processors (and channel width C), the bisection width is CN 1 . For FMs and CMs, it is 4C dd N 1 . For FMSNs and CMSNs, it is 2C dd N 1 .
Reference: [DaEa90] <author> S. Dandamudi and D. Eager, </author> <title> ``Hierarchical Interconnection Networks for Multicomputer Systems,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-39, No. 6, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: A partial bibliography of hierarchical system organizations would include [WuLi81] (clusters of processors using shared buses), [AgMa85] (clusters of processors with crossbar switches), [Carl85] (local meshes of processors, with a global mesh connecting the clusters), <ref> [DaEa90] </ref> (two-level systems with many network topologies), [Padm91] (combinations of Omega networks and composite cube networks), [HsYe91] and [ChHo91] (systems with buses within a cluster and a global hypercube network). <p> We call this the shared global channels approach. The processors within the cluster are connected to this switch either directly or through some intermediate network. Thus, the remaining issue is how these processors access the global switch. The approach taken by [Carl85] and <ref> [DaEa90] </ref> is to have a single processor as the interface processor that directly accesses the global switch. All other processors are connected to the interface processor through a local network (which may be a mesh, hypercube etc). <p> Hence, the interface processor has a large fanout (since it connects to both local and global channels), but all other processors have small fanouts. There is a major disadvantage of this scheme, if we assume that all channels have the same bandwidth. As observed in <ref> [DaEa90] </ref>, under traffic that does not have an extremely high degree of locality, the local channels attached to the interface processor are carrying most of the global traffic for the cluster.
Reference: [FrWT82] <author> M. Franklin, D. Wann, and W. Thomas, </author> <title> ``Pin Limitations and Partitioning of VLSI Interconnection Networks,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-31, No. 11, </volume> <month> Nov. </month> <year> 1982. </year>
Reference-contexts: One slice of the data path determines the setting of the switch, and broadcasts the state information to all the other slices. This avoids the problems discussed in <ref> [FrWT82] </ref>. The FH and CH have been studied in detail in [HsYe91]. We will concentrate on the bidirectional mesh and MSN in this paper. 3. Performance analysis of meshes We will adapt the analyses in [AbPa90] for message switching and partial cut-through, in clustered systems.
Reference: [GiMo91] <author> W. Giloi and S. Montenegro, </author> <title> ``Choosing the interconnect of distributed memory systems by cost and blocking behavior,'' </title> <booktitle> 1991 Int. Parallel Processing Symp., </booktitle> <address> Anaheim CA, </address> <pages> 4/30-5/2. </pages>
Reference-contexts: The processors in [AgMa85] have equitable access to the global crossbar, but are also connected to local crossbars; the system in [IrNa88] is also similar. TICNET <ref> [GiMo91] </ref> also uses local crossbars and global crossbars. This means that intercluster traffic takes only one hop through intermediate clusters. These organizations are reasonable options for small systems.
Reference: [HaMS86] <author> J.P. Hayes, T.N. Mudge, and Q.F. Stout, </author> <title> ``Architecture of a Hypercube Supercomputer", </title> <booktitle> 1986 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1986. </year>
Reference: [Hsu92] <author> W. Hsu, </author> <title> Multiprocessor communications: design and technology, </title> <type> PhD dissertation, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <note> in preparation. </note>
Reference-contexts: See <ref> [Hsu92] </ref> for details.) -12 Again we apply the approximations made with hypercubes in [HsYe91], to get some idea of saturation performance. By saturation, we refer to the situation in which the average message delay becomes unbounded. <p> Messages are broken into large nibbles, which means there is more ``wasted space'' in wide channels. For an FMSN, P t =(N 1 -1)/(N 1 dd N 1 /2+N 1 -4) (see [ChAg90] or <ref> [Hsu92] </ref> for independent derivations of average message distance in an FMSN). t m = R L/C H , and m=t m m g /(2P t ). <p> It is straightforward to derive a i,y , the probability of i arrivals at a y link, and a i,x , the probability of i arrivals at an x link (see <ref> [Hsu92] </ref>). The probability of having i items in a y/x queue is b i,y/x =a i,y/x b 0,y/x + k=0 i a k,y/x b i-k+1,y/x . For uniform traffic, the average message distance is D= dd N 1 hhhhh N 1 -1 N 1 hhhhh . <p> The average queueing delay is T=(1+ 2 m y i m x i With partial cut-through, the delay is T-Dt m . The above analysis can be adapted to cluster local traffic. The process is relatively straightforward but laborious <ref> [Hsu92] </ref>. We performed detailed simulations of some of the networks (see [Hsu92] for data). Our simulation model largely followed standard network queueing models with cut-through routing. such as that of [AbPa90]. <p> The above analysis can be adapted to cluster local traffic. The process is relatively straightforward but laborious <ref> [Hsu92] </ref>. We performed detailed simulations of some of the networks (see [Hsu92] for data). Our simulation model largely followed standard network queueing models with cut-through routing. such as that of [AbPa90]. For link utilizations of up to about 75% the analysis predicts delays which are within 1% to 15% of the simulation results. <p> Network queueing equations are adjusted for multiple arrivals. Details of this procedure for the mesh can be found in <ref> [Hsu92] </ref>. Figures 3 to 6 show the performance of various FMs and CMs. The delays predicted by our analysis match with small errors the numbers obtained through simulations, up to link utilizations of at least 80% (see [Hsu92] for data). <p> Details of this procedure for the mesh can be found in <ref> [Hsu92] </ref>. Figures 3 to 6 show the performance of various FMs and CMs. The delays predicted by our analysis match with small errors the numbers obtained through simulations, up to link utilizations of at least 80% (see [Hsu92] for data). We also find that CMs have better queueing delays than FMs for most of the traffic range before saturation. This is because the CM has wider channels and messages can be routed in fewer clock cycles. <p> Each cluster-to-global bus and global-to-cluster bus is modeled by a single queue. Delays for the buses are added to T to obtain -18 the total message delay. Details of this procedure can be found in <ref> [Hsu92] </ref>. We simulated some of these networks to verify our analyses. The delays predicted by our analysis match the numbers obtained through simulations (with small errors), up to link utilizations of at least 80% (see [Hsu92] for details). <p> Details of this procedure can be found in <ref> [Hsu92] </ref>. We simulated some of these networks to verify our analyses. The delays predicted by our analysis match the numbers obtained through simulations (with small errors), up to link utilizations of at least 80% (see [Hsu92] for details). The correction factor for partial cut-through is fairly accurate, and the inaccuracies in the cut-through analysis mostly come from the message switching analysis. 3.3 Analysis of Manhattan Street Network (MSN) We can adapt Abraham's analysis for MSNs. <p> Hence, d 0 =(1-m (1-P t )/2)(1-m (1-P t )/2) d 2 =(m (1-P t )/2)(m (1-P t )/2) We can go through the analysis and come up with similar expressions for message delays. Details of the analysis can be found in <ref> [Hsu92] </ref>. As in the FM, the message switching analysis predicts average message delays with reasonable accuracy, for low to medium network loads (up to at least 75% link utilization). We can adapt the queueing analysis for MSNs to clustered MSNs [Hsu92]. <p> Details of the analysis can be found in <ref> [Hsu92] </ref>. As in the FM, the message switching analysis predicts average message delays with reasonable accuracy, for low to medium network loads (up to at least 75% link utilization). We can adapt the queueing analysis for MSNs to clustered MSNs [Hsu92]. The performance of some MSNs are plotted in Figures 3-6. -19 For flat systems, the MSN has wider channels than the FM, and better performance at a large range of traffic.
Reference: [HsYe91] <author> W. Hsu and P. Yew, </author> <title> ``The performance of hierarchical systems with wiring constraints,'' </title> <booktitle> 1991 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: partial bibliography of hierarchical system organizations would include [WuLi81] (clusters of processors using shared buses), [AgMa85] (clusters of processors with crossbar switches), [Carl85] (local meshes of processors, with a global mesh connecting the clusters), [DaEa90] (two-level systems with many network topologies), [Padm91] (combinations of Omega networks and composite cube networks), <ref> [HsYe91] </ref> and [ChHo91] (systems with buses within a cluster and a global hypercube network). Except for [HsYe91], these studies have not taken into account the effects of packaging constraints on system performance, and have often restricted the discussion to traffic patterns with high locality. <p> (clusters of processors with crossbar switches), [Carl85] (local meshes of processors, with a global mesh connecting the clusters), [DaEa90] (two-level systems with many network topologies), [Padm91] (combinations of Omega networks and composite cube networks), <ref> [HsYe91] </ref> and [ChHo91] (systems with buses within a cluster and a global hypercube network). Except for [HsYe91], these studies have not taken into account the effects of packaging constraints on system performance, and have often restricted the discussion to traffic patterns with high locality. <p> We will incorporate packaging considerations into the study of hierarchical systems based on hypercube and mesh connections, using wiring cost as a basis for cost comparisons. Some of this analysis has appeared in <ref> [HsYe91] </ref> and [HsYe92]. In Section 2, we survey different approaches to hierarchical organization, and describe the simple strategy of [HsYe91]. In Section 3, we present queueing analyses of the hypercube, bidirectional mesh and MSN. <p> Some of this analysis has appeared in <ref> [HsYe91] </ref> and [HsYe92]. In Section 2, we survey different approaches to hierarchical organization, and describe the simple strategy of [HsYe91]. In Section 3, we present queueing analyses of the hypercube, bidirectional mesh and MSN. In Section 4, comparisons based on wiring costs are made for a variety of systems, including hypercubes, bidirectional meshes and MSNs, in both clustered and flat configurations. Conclusions are drawn in Section 5. 2. <p> In Figure 1a, if W=512, C=32. Since each processor has a 5x5 crossbar, the number of crosspoints in the system is 25CN 1 . In <ref> [HsYe91] </ref>, we examined a specific approach to clustering. This is easily applied to mesh-like structures. In a clustered mesh (CM), each board or cluster of N 0 processors has a global crossbar switch. <p> Flat hypercubes (FHs) and clustered hypercubes (CHs) are similar, except all processors on a board in an FH have off-board channels, and the off-board channels correspond to the dimensions of the cube that are on other boards <ref> [HsYe91] </ref>. Hence, for an FH, C= Q J ; for a CH, C= Q J . Figure 1c and 1d show an FH and a CH for 64 processors, 4 per board. In the discussion of CHs in [HsYe91], we emphasized that k &gt; 1. <p> correspond to the dimensions of the cube that are on other boards <ref> [HsYe91] </ref>. Hence, for an FH, C= Q J ; for a CH, C= Q J . Figure 1c and 1d show an FH and a CH for 64 processors, 4 per board. In the discussion of CHs in [HsYe91], we emphasized that k &gt; 1. This is because the network links in the CH are capable of handling high message generation rates which may saturate a single cluster/global bus. <p> One slice of the data path determines the setting of the switch, and broadcasts the state information to all the other slices. This avoids the problems discussed in [FrWT82]. The FH and CH have been studied in detail in <ref> [HsYe91] </ref>. We will concentrate on the bidirectional mesh and MSN in this paper. 3. Performance analysis of meshes We will adapt the analyses in [AbPa90] for message switching and partial cut-through, in clustered systems. <p> As with the hypercube, the mesh performs better with -11 LR routing. [AbPa90] presented analysis of the mesh with random routing. We will present queueing analysis of the mesh with LR routing in the next section. As in <ref> [HsYe91] </ref>, for the clustered systems, we assume random routing for convenience of analysis. Hence, our performance numbers for CMs and CHs are conservative estimates. <p> See [Hsu92] for details.) -12 Again we apply the approximations made with hypercubes in <ref> [HsYe91] </ref>, to get some idea of saturation performance. By saturation, we refer to the situation in which the average message delay becomes unbounded. <p> But the difference in P t s is significant when the number of clusters is small. The one or two extra hops required by the CMSN is very large compared to an average message distance of, say, 3 or 4. -13 As in <ref> [HsYe91] </ref>, we will model each bus as a single queue, and queueing delays approximate closely bus contention delays. We will use the terms ``bus'' and ``queue'' interchangeably for the cluster/global connections. We assume that all the channels of the global switch have the same bandwidth. <p> Our analytical predictions are adequate for the system sizes and loads of interest. Only curves for partial cut-through are shown, because of limited space. 3.2 Analysis of clustered systems The rest of the analysis follows Section 3.1 in <ref> [HsYe91] </ref>. As mentioned before, each cluster-to-global bus and global-to-cluster bus is modeled as a N 0 /kx1 or 1xN 0 /k switch. <p> For partial cut through, we save P t t m hhh because of the pipelining, and the delay is T P t The rest of the analysis follows Section 3.1 in <ref> [HsYe91] </ref>. Each cluster-to-global bus and global-to-cluster bus is modeled by a single queue. Delays for the buses are added to T to obtain -18 the total message delay. Details of this procedure can be found in [Hsu92]. We simulated some of these networks to verify our analyses. <p> For a hypercube with N 1 processors (and channel width C), the bisection width is CN 1 . For FMs and CMs, it is 4C dd N 1 . For FMSNs and CMSNs, it is 2C dd N 1 . As observed in <ref> [HsYe91] </ref>, a flat network and its clustered analogue have the same bisection width (if cluster pinout is constant), because clustering changes the way channels are used, and not the way they are physically connected on the system level. <p> Data for the MSN has been omitted; trends are similar to the uniform traffic environments. The performance of FHs and CHs under local traffic have been discussed in detail in <ref> [HsYe91] </ref>. In summary, the saturation load of an FH does not change if locality of traffic increases. However, CHs saturate at higher loads, especially if four cluster buses are used. We mentioned before that two cluster buses are sufficient for the CM, except for extremely high rates of local traffic. <p> We found that the FMSN and CMSN, despite the -24 possibility of wider channels, has limitations which make them less desirable than the FM and CM. We compared the FM and CM with the flat and clustered hypercubes (FH and CH) proposed in <ref> [HsYe91] </ref>. We used both pinout and bisection width as wiring cost constraints, and assumed uniform traffic. The relative performance of the networks is dependent on the constraint chosen. If we assumed constant bisection width, all systems saturate at roughly the same load, if channel efficiency is ignored.
Reference: [Maxe87] <author> N. Maxemchuk, </author> <title> ``Routing in the Manhattan Street network,'' </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. COM-35, No. 5, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: Each supernode is a cluster of N 0 =4 processors. Networks with unidirectional links have half the number of channels per node when compared to their bidirectional counterparts. The Manhattan street network (MSN) <ref> [Maxe87] </ref> has channels organized like a mesh that alternate in direction. For example, nodes in the -9 even/odd numbered rows and columns would have channels routing to increasing/decreasing node numbers, with the appropriate moduli.
Reference: [Padm91] <author> K. Padmanabhan, </author> <title> ``Effective architectures for data access in a shared memory hierarchy,'' </title> <booktitle> Journ. of Parallel and Distributed Computing 11. </booktitle>
Reference-contexts: A partial bibliography of hierarchical system organizations would include [WuLi81] (clusters of processors using shared buses), [AgMa85] (clusters of processors with crossbar switches), [Carl85] (local meshes of processors, with a global mesh connecting the clusters), [DaEa90] (two-level systems with many network topologies), <ref> [Padm91] </ref> (combinations of Omega networks and composite cube networks), [HsYe91] and [ChHo91] (systems with buses within a cluster and a global hypercube network).
Reference: [PeKr87] <author> W. Pence and J. Krusius, </author> <title> ``The fundamental limits for electronic packaging and systems,'' </title> <journal> IEEE Trans. on Components, Hybrids, and Manufacturing Tech., </journal> <volume> Vol. CHMT-10, </volume> <pages> No.2, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: 1. Introduction Many researchers have observed that wiring and packaging constraints are important determinants of performance in large-scale computer systems. The problem is well-summarized in <ref> [PeKr87] </ref>: ``... many critical packaging problems are not independent, and ... different systems levels cannot be effectively designed independently from each other.'' In particular, in the study of multiprocessor interconnects, packaging constraints should be incorporated into traditional network analysis. Classical network studies assumed constant channel widths and single-nibble messages.
Reference: [Thom79] <author> C.D. Thompson, </author> <title> ``Area-Time Complexity for VLSI,'' Ann. </title> <booktitle> Symp. on Theory of Computing, </booktitle> <month> May </month> <year> 1979. </year> <month> -26 </month>
Reference-contexts: Long messages have to be broken into multiple nibbles and transmitted in many cycles. Meaningful predictions of network performance must take into account limited channel widths and wiring costs. In [Dall90], k-ary n-cubes with constant wiring costs were compared. Bisection width <ref> [Thom79] </ref> was used as an estimate of wiring area. In [AbPa90], both switch pinout and bisection width were used as cost factors in analytical studies of k-ary n-cubes.
Reference: [WuLi81] <author> S. Wu and M. Liu, </author> <title> ``A cluster structure as an interconnection network for large multiprocessor systems,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-30, No. 4, </volume> <month> Apr. </month> <year> 1981. </year>
Reference-contexts: As in most previous studies, we will restrict our discussion to a two-level hierarchy. Experience with parallel applications has shown that reorganizing a parallel program to exploit two levels of architectural hierarchy is a non-trivial problem. A partial bibliography of hierarchical system organizations would include <ref> [WuLi81] </ref> (clusters of processors using shared buses), [AgMa85] (clusters of processors with crossbar switches), [Carl85] (local meshes of processors, with a global mesh connecting the clusters), [DaEa90] (two-level systems with many network topologies), [Padm91] (combinations of Omega networks and composite cube networks), [HsYe91] and [ChHo91] (systems with buses within a cluster
References-found: 20

