URL: ftp://dis.cs.umass.edu/pub/pscheduler.ps
Refering-URL: http://dis.cs.umass.edu/research/parallel.html
Root-URL: 
Title: A Heuristic Real-Time Parallel Scheduler Based on Task Structures  
Abstract: Qiegang Long and Victor Lesser Department of Computer Science University of Massachusetts Technical Report 95-92 Abstract The development of networks and multi-processor computers has allowed us to solve problems in parallel. The task of efficiently coordinating parallel processors is formidable. This paper presents a heuristic parallel real-time scheduler that analyzes the interactions among the tasks, and builds a paral lel schedule that tends to take advantage of those interactions. fl This work is supported by NSF grant IRI9321324.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Wesley W. Chu and Lance M-T. </author> <title> Lan. Task allocation and precedence relations for distributed real-time systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-36(6):667-679, </volume> <month> June </month> <year> 1987. </year>
Reference-contexts: 1 Introduction The development of networks and multi-processor computers has allowed us to solve problems in parallel. The task of efficiently coordinating parallel processors is formidable. It requires a scheduler to specify which processor to allocate for what problem and when. Existing parallel schedulers can be divided into three categories <ref> [1] </ref>: graph-theoretic [2] [3], integer 0-1 programming approach [4] and the heuristic approach [5] [6]. <p> We are undergoing more experiments to analyze the performance of the scheduler with or without the repairing stage. 7 Related work Our scheduling problem would be quite like the ones that are widely studied if there are no task relationships that would affect task execution durations or their result qualities <ref> [1, 5, 6] </ref>. The presence of these task relationships imply that not only all the methods need to be scheduled so that they can be executed before their deadlines, they should also be ordered in a way to achieve shorter schedule duration and better overall result qualities.
Reference: [2] <author> S. H. Bokhari. </author> <title> Dual processor scheduling with dynamic reassignment. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> SE-8:401-412, </volume> <month> July </month> <year> 1979. </year>
Reference-contexts: The task of efficiently coordinating parallel processors is formidable. It requires a scheduler to specify which processor to allocate for what problem and when. Existing parallel schedulers can be divided into three categories [1]: graph-theoretic <ref> [2] </ref> [3], integer 0-1 programming approach [4] and the heuristic approach [5] [6]. Many of these methods use the cost of computation and communications as the schedule objective function, assuming that the duration of executing a task and its result quality are independent of the order of task execution.
Reference: [3] <author> G. S. Rao, H. S. Stone, and T. C. Hu. </author> <title> Assignment of tasks in a distributed processing system with limited memory. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-28:291-299, </volume> <month> April </month> <year> 1979. </year>
Reference-contexts: The task of efficiently coordinating parallel processors is formidable. It requires a scheduler to specify which processor to allocate for what problem and when. Existing parallel schedulers can be divided into three categories [1]: graph-theoretic [2] <ref> [3] </ref>, integer 0-1 programming approach [4] and the heuristic approach [5] [6]. Many of these methods use the cost of computation and communications as the schedule objective function, assuming that the duration of executing a task and its result quality are independent of the order of task execution.
Reference: [4] <author> P. Y. R. Ma, E. Y. S. Lee, and M. Tsuchiya. </author> <title> A task allocation model for distributed computing systems. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-31:41-47, </volume> <month> January </month> <year> 1982. </year>
Reference-contexts: The task of efficiently coordinating parallel processors is formidable. It requires a scheduler to specify which processor to allocate for what problem and when. Existing parallel schedulers can be divided into three categories [1]: graph-theoretic [2] [3], integer 0-1 programming approach <ref> [4] </ref> and the heuristic approach [5] [6]. Many of these methods use the cost of computation and communications as the schedule objective function, assuming that the duration of executing a task and its result quality are independent of the order of task execution.
Reference: [5] <author> Gilbert C. Sih and Edward A. Lee. </author> <title> Declustering: A new multiprocessor scheduling technique. </title> <journal> IEEE transactions on Parallel And Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 625-637, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The task of efficiently coordinating parallel processors is formidable. It requires a scheduler to specify which processor to allocate for what problem and when. Existing parallel schedulers can be divided into three categories [1]: graph-theoretic [2] [3], integer 0-1 programming approach [4] and the heuristic approach <ref> [5] </ref> [6]. Many of these methods use the cost of computation and communications as the schedule objective function, assuming that the duration of executing a task and its result quality are independent of the order of task execution. <p> We are undergoing more experiments to analyze the performance of the scheduler with or without the repairing stage. 7 Related work Our scheduling problem would be quite like the ones that are widely studied if there are no task relationships that would affect task execution durations or their result qualities <ref> [1, 5, 6] </ref>. The presence of these task relationships imply that not only all the methods need to be scheduled so that they can be executed before their deadlines, they should also be ordered in a way to achieve shorter schedule duration and better overall result qualities.
Reference: [6] <author> Gilbert C. Sih and Edward A. Lee. </author> <title> A compile-time scheduling heuristic for interconnection-constrained heterogeneous processor architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 175-186, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The task of efficiently coordinating parallel processors is formidable. It requires a scheduler to specify which processor to allocate for what problem and when. Existing parallel schedulers can be divided into three categories [1]: graph-theoretic [2] [3], integer 0-1 programming approach [4] and the heuristic approach [5] <ref> [6] </ref>. Many of these methods use the cost of computation and communications as the schedule objective function, assuming that the duration of executing a task and its result quality are independent of the order of task execution. However, this assumption does not hold if there are complex interactions among tasks. <p> We are undergoing more experiments to analyze the performance of the scheduler with or without the repairing stage. 7 Related work Our scheduling problem would be quite like the ones that are widely studied if there are no task relationships that would affect task execution durations or their result qualities <ref> [1, 5, 6] </ref>. The presence of these task relationships imply that not only all the methods need to be scheduled so that they can be executed before their deadlines, they should also be ordered in a way to achieve shorter schedule duration and better overall result qualities.
Reference: [7] <author> Keith S. Decker and Victor R. Lesser. </author> <title> Quantitative modeling of complex computational task environments. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 217-224, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: A reasonable parallel schedule can be generated only if the scheduler has taken into account these relationship. For example, if there is a dependency relationship between two tasks, the dependent task cannot be executed before the other is done. Decker and Lesser <ref> [7] </ref> have identified several kinds of other task relationships which also affect the quality of a schedule. <p> Throughout the paper, we will use examples to explain how each algorithm works. Section 7 reviews related work, and section 8 analyzes our approach and offers suggestions for future research. 2 Environment and Assumptions Our scheduler works in a modified TMS environment <ref> [7] </ref>. The scheduler is given a set of task groups that are represented in TMS task structure, as shown in Fig. 3 1. The leaves in a task structure are the executable methods.
Reference: [8] <author> Alan Garvey and Victor Lesser. </author> <title> Design-to-time scheduling with uncertainty. </title> <type> Technical Report TR95-03, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Thus, the scheduler has to decide which task relationship can be exploited in view of the overall schedule quality. In this paper, we present a heuristic parallel real-time scheduler which is based on the design-to-time algorithm developed by Garvey and Lesser <ref> [8] </ref>. Our sched-uler builds schedules incrementally. It uses a near-greedy algorithm to construct a draft schedule first, and then employs a iterative repairing procedure to focus on the task relationships that have not been exploited. <p> The detailed description of these heuristics can be found in <ref> [8] </ref>. With these heuristics, the scheduler uses a simple evaluate-choose loop: rate each method in the schedule against the heuristics, select the one with the highest nonnegative rating and then add it to the existing schedule. <p> We did not specify what to do if a given set of methods (alternative) cannot be scheduled to meet the desired quality. It is expected that the repairing approach 18 developed by Garvey and Lesser in <ref> [8] </ref> can be used in this parallel scheduler. Their approach allows the scheduler to regenerate the alternative by switching some methods that causes the failure or the low quality of the schedule.
Reference: [9] <author> Mark S. Fox and Stephen F. Smith. </author> <title> Isisa knowledge-based system for factory scheduling. </title> <journal> Expert Systems, </journal> <volume> 1(1) </volume> <pages> 25-49, </pages> <year> 1984. </year>
Reference-contexts: Below we describe the related work that does consider some relationships between tasks, but none as realistic and the ones we use. Fox and Smith treats scheduling as a constraint-directed search. Their system ISIS <ref> [9] </ref> implements a hierarcial scheduling approach. Orders (tasks) are selected one by one to have their operations (methods) scheduled according to their priority.
Reference: [10] <author> David W. Hildum. </author> <title> Flexibility in a knowledge-based system for solving dynamic resource-constrained scheduling problems. </title> <type> Technical Report Umass CMPSCI TR94-77, </type> <institution> University of Massachusetts, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: If not all the constraints can be satisfied, the flexibility from task relationships thus requires the scheduler to decide what constraints should pursue. Hildum implements a knowledge-based system for solving dynamic resource-constrained scheduling problems in <ref> [10] </ref>. His scheduling algorithm expliots the flexibility properties (like earliest-start-time) of tasks to allow the schedule to be adaptable to the changing environment (like the arrival of some new tasks). It uses 17 least-commitment decision making technique to preserve maneuverability by ex-plicitly incorporating slack time into the developing schedule.
Reference: [11] <author> Monte Zweben, Brian Daun, and Michael Deale. </author> <title> Scheduling and Rescheduling with iterative repair, </title> <address> pages 241-256. </address> <publisher> Morgan Kauffman, </publisher> <year> 1994. </year>
Reference-contexts: The preserved slack time can be used (e.g., shifting task) later to adjust the schedule. Our scheduler does not preserve slack time in the schedule, it inserts them only when necessary. GERRY <ref> [11] </ref> developed by Zweben et al. uses constraint-based iterative repair to schedule and reschedule the tasks of a plan according to temporal constraints, milestones, resource constraints and state constraints. Some of their constraints are similar to or can be represented as our task relationships.
Reference: [12] <author> Krithi Ramamritham and John A. Stankovic. </author> <title> Efficient scheduling algorithms for real-time multiprocessor systems. </title> <journal> IEEE Transactions of Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 184-194, </pages> <month> April </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: Their scheduling and repairing algorithm iteratively modifies the schedule via some basic actions like inserting an achiever, shifting a task forward or backward. Our work employs a constructive method to build draft schedule. This reduces the amount of iterative modifications needed at the repairing process. Ramamritham and Stankovic in <ref> [12] </ref> uses a heuristic incremental approach to build schedule for multiprocessor systems. Their work focuses on how to guarantee that all the tasks meet their deadlines.
References-found: 12

