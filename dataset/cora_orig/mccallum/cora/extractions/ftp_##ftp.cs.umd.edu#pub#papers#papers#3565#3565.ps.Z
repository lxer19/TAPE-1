URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3565/3565.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/authors/Anurag_Acharya-no-abs.html
Root-URL: 
Email: franga,acha,edjlali,als,saltzg@cs.umd.edu  
Title: Runtime Coupling of Data-parallel Programs  
Author: M. Ranganathan, A. Acharya, G. Edjlali, A. Sussman and J. Saltz 
Address: College Park MD 20742  
Affiliation: Dept. of Computer Science and UMIACS University of Maryland,  
Abstract: We consider the problem of efficiently coupling multiple data-parallel programs at runtime. We propose an approach that establishes a mapping between data structures in different data-parallel programs and implements a user specified consistency model. Mappings are established at runtime and new mappings between programs can be added and deleted while the programs are in execution. Mappings, or the identity of the processors involved, do not have to be known at compile-time or even link-time. Programs can be made to interact with different granularities of interaction without requiring any re-coding. A priori knowledge of data movement requirements allows for buffering of data and overlap of computations between coupled applications. Efficient data movement is achieved by pre-computing an optimized schedule. We describe our prototype implementation and evaluate its performance for a set of synthetic benchmarks that examine the variation of performance with coupling parameters. We demonstrate that the cost of the added flexibility gained by our coupling method is not prohibitive when compared with a monolithic code that does the same computation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Francois Bodin, Peter Beckman, Dennis Gannon, Srinivas Narayana, and Shelby X. Yang. </author> <title> Distributed pC++: Basic ideas for an object parallel language. </title> <journal> Scientific Programming, </journal> <volume> 2(3), </volume> <month> Fall </month> <year> 1993. </year>
Reference-contexts: Efficient data movement is achieved by pre-computing an optimized plan (schedule) for data movement. Our prototype implementation uses a generalized data movement library called Meta-Chaos [5] and is able to couple data-parallel programs written in different languages (including High Performance Fortran (HPF) [4], C and pC++ <ref> [1] </ref>) and using different communication libraries (including Multiblock PARTI [19] and CHAOS [12]). By coupling multiple concurrently executing data parallel applications, we gain the added benefit of combining task and data parallelism.
Reference: [2] <author> K. M. Chandy, I. Foster, K. Kennedy, C. Koelbel, and C.-W. Tseng. </author> <title> Integrated Support for Task and Data Parallelism. </title> <journal> Journal of Supercomputing Applications, </journal> <volume> 8(2), </volume> <year> 1994. </year> <note> Also available as CRPC Technical Report CRPC-TR93430. </note>
Reference-contexts: Fx [18] uses additional HPF directives to specify task parallelism. Opus [10] is a set of HPF extensions that provides mechanism for communication and synchronization through a shared data abstraction (SDA). Fortran M <ref> [7, 2] </ref> makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements. Braid [6] introduces data parallel extensions to the Mentat Programming Language [9].
Reference: [3] <author> Jefferey S. Chase, Henry M. Levy, Michael J. Feeley, and Edward D. Lazowska. </author> <title> Sharing and protection in a single address space operating system. </title> <type> Technical Report 93-04-2, </type> <institution> University of Washington at Seattle, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Polylith has also been extended to allow for dynamic re-configuration while the system is executing [11]. Single address space operating systems such as Opal <ref> [3] </ref> offer a uniform view of memory across all processes. All processes in Opal share the same address space and hence this facility could be used as a means of sharing objects between applications. Linda [15] offers a tuple space oriented programming model which could be used to couple programs.
Reference: [4] <author> C.Koebel, D.Loveman, R.Schreiber, G.Steele Jr., and M.Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year> <month> 14 </month>
Reference-contexts: Efficient data movement is achieved by pre-computing an optimized plan (schedule) for data movement. Our prototype implementation uses a generalized data movement library called Meta-Chaos [5] and is able to couple data-parallel programs written in different languages (including High Performance Fortran (HPF) <ref> [4] </ref>, C and pC++ [1]) and using different communication libraries (including Multiblock PARTI [19] and CHAOS [12]). By coupling multiple concurrently executing data parallel applications, we gain the added benefit of combining task and data parallelism.
Reference: [5] <author> Guy Edjlali et. al. </author> <title> Meta-chaos An Inter-operability layer for Data-Parallel programs. </title> <note> Technical Report In Preparation., Center For Research on Parallel Computation. </note>
Reference-contexts: A priori knowledge of the mapping specification at run-time allows for overlapping execution of the interacting programs by buffering the data. Efficient data movement is achieved by pre-computing an optimized plan (schedule) for data movement. Our prototype implementation uses a generalized data movement library called Meta-Chaos <ref> [5] </ref> and is able to couple data-parallel programs written in different languages (including High Performance Fortran (HPF) [4], C and pC++ [1]) and using different communication libraries (including Multiblock PARTI [19] and CHAOS [12]). <p> In contrast to other approaches that require language extensions to achieve this [7, 17, 18], our approach can work with off the shelf compiler implementations as long as the implementations provide certain query functions about the distributions of data structures <ref> [5] </ref>. 1 This research was supported by NASA under grant NASA #NAG-1-1485 (ARPA Project Number 8874), by ARPA under grant #F19628-94-C-0057 and by NSF under grant #ASC9318183. <p> The nodes are connected by an FDDI network 5 . The primary goals of our implementation were language independence, flexibility and efficiency. The concern for language independence prompted the use of the Meta-Chaos library <ref> [5] </ref>, which we introduce below. There are several points to be made about our implementation. <p> In addition, functions need to be provided to convert between global and local indices. The Meta-Chaos library, including descriptions of the necessary support functions that it requires from the data-parallel run-time library being linked, is described in greater detail in <ref> [5] </ref>. We have successfully implemented Meta-Chaos to test inter-operability between data-parallel programs written using HPF, CHAOS, Multiblock Parti and pC++. For the underlying messaging layer between applications, we used PVM [8]. Each data parallel program is assigned a distinct PVM group.
Reference: [6] <author> E.West and A. Grishaw. </author> <title> Braid: Integrating Task and Data Parallelism. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 211-219. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1995. </year>
Reference-contexts: Opus [10] is a set of HPF extensions that provides mechanism for communication and synchronization through a shared data abstraction (SDA). Fortran M [7, 2] makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements. Braid <ref> [6] </ref> introduces data parallel extensions to the Mentat Programming Language [9]. Communication libraries like PVM and MPI [13] may be used by the programmer to directly transfer messages from one data parallel task to another.
Reference: [7] <author> I. Foster, M. Xu, B. Avalani, and A. Choudhary. </author> <title> A Compilation System that Integrates High Performance Fortran and Fortran M. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: By coupling multiple concurrently executing data parallel applications, we gain the added benefit of combining task and data parallelism. In contrast to other approaches that require language extensions to achieve this <ref> [7, 17, 18] </ref>, our approach can work with off the shelf compiler implementations as long as the implementations provide certain query functions about the distributions of data structures [5]. 1 This research was supported by NASA under grant NASA #NAG-1-1485 (ARPA Project Number 8874), by ARPA under grant #F19628-94-C-0057 and by <p> Fx [18] uses additional HPF directives to specify task parallelism. Opus [10] is a set of HPF extensions that provides mechanism for communication and synchronization through a shared data abstraction (SDA). Fortran M <ref> [7, 2] </ref> makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements. Braid [6] introduces data parallel extensions to the Mentat Programming Language [9].
Reference: [8] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: We have successfully implemented Meta-Chaos to test inter-operability between data-parallel programs written using HPF, CHAOS, Multiblock Parti and pC++. For the underlying messaging layer between applications, we used PVM <ref> [8] </ref>. Each data parallel program is assigned a distinct PVM group. Asynchronous data transfer is achieved by using a dedicated thread for receiving messages.
Reference: [9] <author> Andrew S. Grimshaw. </author> <title> The Mentat computation model data-driven support for object-oriented parallel processing. </title> <type> Technical Report CS-93-30, </type> <institution> University of Virginia, </institution> <month> May 93. </month>
Reference-contexts: Fortran M [7, 2] makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements. Braid [6] introduces data parallel extensions to the Mentat Programming Language <ref> [9] </ref>. Communication libraries like PVM and MPI [13] may be used by the programmer to directly transfer messages from one data parallel task to another. However, such an approach burdens the programmer with having to understand low level details about data distributions and message passing.
Reference: [10] <author> M. Haines, B. Hess, P. Mehrotra, J. Van Rosendale, and H. Zima. </author> <title> Runtime Support for Data Parallel Tasks. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 432-439. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1995. </year>
Reference-contexts: Some research efforts are working on enhancements of data parallel languages or task parallel languages in order to integrate data parallelism and task parallelism. Fx [18] uses additional HPF directives to specify task parallelism. Opus <ref> [10] </ref> is a set of HPF extensions that provides mechanism for communication and synchronization through a shared data abstraction (SDA). Fortran M [7, 2] makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements.
Reference: [11] <author> Christine R. Hofmiester and James M. Purtilo. </author> <title> A framework for dynamic re-configuration of distributed programs. </title> <type> Technical Report CS-TR 3119, </type> <institution> University of Maryland, Department of Computer Science and UMIACS, </institution> <year> 1993. </year>
Reference-contexts: The other alternative would be to use PVM to start tasks remotely, and then use our approach to connect the data streams of the executing tasks. Polylith has also been extended to allow for dynamic re-configuration while the system is executing <ref> [11] </ref>. Single address space operating systems such as Opal [3] offer a uniform view of memory across all processes. All processes in Opal share the same address space and hence this facility could be used as a means of sharing objects between applications.
Reference: [12] <author> Yuan-Shin Hwang, Bongki Moon, Shamik D. Sharma, Ravi Ponnusamy, Raja Das, and Joel H. Saltz. </author> <title> Runtime and language support for compiling adaptive irregular programs. </title> <journal> Software-Practice and Experience, </journal> <volume> 25(6) </volume> <pages> 597-621, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Our prototype implementation uses a generalized data movement library called Meta-Chaos [5] and is able to couple data-parallel programs written in different languages (including High Performance Fortran (HPF) [4], C and pC++ [1]) and using different communication libraries (including Multiblock PARTI [19] and CHAOS <ref> [12] </ref>). By coupling multiple concurrently executing data parallel applications, we gain the added benefit of combining task and data parallelism.
Reference: [13] <author> Message Passing Interface Forum. </author> <title> Document for a Standard Message-Passing Interface. </title> <type> Technical Report CS-93-214, </type> <institution> University of Tennessee, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Fortran M [7, 2] makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements. Braid [6] introduces data parallel extensions to the Mentat Programming Language [9]. Communication libraries like PVM and MPI <ref> [13] </ref> may be used by the programmer to directly transfer messages from one data parallel task to another. However, such an approach burdens the programmer with having to understand low level details about data distributions and message passing.
Reference: [14] <author> M.Ranganathan, A.Acharya, G.Edjlali, A.Sussman, and J.Saltz. </author> <title> Run-time coupling of Data-Parallel programs. </title> <note> Technical Report In Preparation., Center For Research on Parallel Computation. </note>
Reference-contexts: These are not always detectable by looking at the mappings alone. In our implementation, we expect the programmer to be aware of these situations when building an inter connection of applications. We address these issues in greater detail in a technical report <ref> [14] </ref>. 4 Implementation We have implemented our system on a network of four-processor SMP Digital Alpha Server 4/2100 workstations. The nodes are connected by an FDDI network 5 . The primary goals of our implementation were language independence, flexibility and efficiency. <p> Some coordination between the producer peers is required to ensure that this situation does not happen. A simple distributed protocol that guarantees that the consumer sees a consistent version of the source array has been implemented and is described in greater detail in <ref> [14] </ref>. Next we consider dynamic addition of mappings. A mapping is safe for the producer iff all the versions of the array it would transfer are available (either buffered or yet to be generated).
Reference: [15] <author> N.Carriero and D.Gelertner. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4), </volume> <year> 1989. </year>
Reference-contexts: Single address space operating systems such as Opal [3] offer a uniform view of memory across all processes. All processes in Opal share the same address space and hence this facility could be used as a means of sharing objects between applications. Linda <ref> [15] </ref> offers a tuple space oriented programming model which could be used to couple programs. A stream oriented model such as ours could be implemented on top of Linda.
Reference: [16] <author> James Purtillo. </author> <title> The Polylith software toolbus. </title> <type> Technical Report CS-TR-2469, </type> <institution> University of Maryland, Department of Computer Science and UMIACS, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The experiments indicate that performance is not degraded much by coupling data-parallel applications in a fully constrained manner. 6 Related Work Our approach is similar in some respects to the software bus approach used in Polylith <ref> [16] </ref>. A software bus isolates and encapsulates run time interfacing concerns for applications. The software bus allows modules to export interfaces and be invoked by remote processes. Our approach is data stream driven rather than remote procedure call driven.
Reference: [17] <author> J. Subhlok, D. O'Hallaron, and T. Gross. </author> <title> Task Parallel Programming in Fx. </title> <type> Technical Report CMU-CS-94-112, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: By coupling multiple concurrently executing data parallel applications, we gain the added benefit of combining task and data parallelism. In contrast to other approaches that require language extensions to achieve this <ref> [7, 17, 18] </ref>, our approach can work with off the shelf compiler implementations as long as the implementations provide certain query functions about the distributions of data structures [5]. 1 This research was supported by NASA under grant NASA #NAG-1-1485 (ARPA Project Number 8874), by ARPA under grant #F19628-94-C-0057 and by
Reference: [18] <author> Jaspal Subhlok, James M. Stichnoth, David R. O'Hallaron, and Thomas Gross. </author> <title> Exploiting task and data parallelism on a multicomputer. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 13-22, </pages> <month> May </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 28, No. </volume> <pages> 7. </pages>
Reference-contexts: By coupling multiple concurrently executing data parallel applications, we gain the added benefit of combining task and data parallelism. In contrast to other approaches that require language extensions to achieve this <ref> [7, 17, 18] </ref>, our approach can work with off the shelf compiler implementations as long as the implementations provide certain query functions about the distributions of data structures [5]. 1 This research was supported by NASA under grant NASA #NAG-1-1485 (ARPA Project Number 8874), by ARPA under grant #F19628-94-C-0057 and by <p> The general topic of integrating task and data parallelism has received a lot of attention. Some research efforts are working on enhancements of data parallel languages or task parallel languages in order to integrate data parallelism and task parallelism. Fx <ref> [18] </ref> uses additional HPF directives to specify task parallelism. Opus [10] is a set of HPF extensions that provides mechanism for communication and synchronization through a shared data abstraction (SDA). Fortran M [7, 2] makes extensions to Fortran77 for task parallel computations, and also allows some data distribution statements.
Reference: [19] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year> <month> 15 </month>
Reference-contexts: Our prototype implementation uses a generalized data movement library called Meta-Chaos [5] and is able to couple data-parallel programs written in different languages (including High Performance Fortran (HPF) [4], C and pC++ [1]) and using different communication libraries (including Multiblock PARTI <ref> [19] </ref> and CHAOS [12]). By coupling multiple concurrently executing data parallel applications, we gain the added benefit of combining task and data parallelism.
References-found: 19

