URL: http://www.ai.mit.edu/~viola/ICCV-95-viola-wells.ps.gz
Refering-URL: http://www.ai.mit.edu/~viola/pub.html
Root-URL: 
Email: viola@ai.mit.edu  sw@ai.mit.edu  
Title: Alignment by Maximization of Mutual Information approach registering MR images, aligning a smooth 3D object
Author: Paul Viola William M. Wells III 
Date: 95  
Note: To appear in International Conference on Computer Vision  Experiments are presented that demonstrate the  supported by USAF ASSERT, Parent Grant#:F49620-93-1-0263 supported by ARPA IU program  
Address: 545 Technology Square Cambridge, MA 02139  
Affiliation: Massachusetts Institute of Technology Artificial Intelligence Laboratory  Massachusetts Institute of Technology Artificial Intelligence Laboratory  Harvard Medical School and Brigham and Women's Hospital Department of Radiology  
Abstract: A new approach is presented for finding the pose of an object model in an image. The technique does not require information about the surface properties of the object, besides its shape, and is robust with respect to variations of illumination. In our derivation, few assumptions are made about the nature of the imaging process. As a result the algorithms are quite general and can forseeably be used in a wide variety of imaging situations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Athanasios Papoulis. </author> <title> Probability, Random Variables, and Stochastic Processes. </title> <publisher> McGraw-Hill, Inc., </publisher> <address> third edition, </address> <year> 1991. </year>
Reference-contexts: It is not a function of T . The second term is the entropy of the part of the image into which the model projects. It encourages transformations that project 1 We follow the definitional conventions of Papoulis <ref> [1] </ref> for mutual information and entropy. 2 u into complex parts of v. The third term, the (negative) joint entropy of u and v, takes on large values if u and v are functionally related. It encourages transformations where u explains v well.
Reference: [2] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1973. </year>
Reference-contexts: A good discussion of density estimation can be found in the textbook by Duda and Hart <ref> [2] </ref>. <p> It will approach one if z i is signifi cantly closer to z j than any other element of A. It will be near zero if some other element of A is significantly closer to z i . Distance is interpreted with respect to the squared Mahalonobis distance (see <ref> [2] </ref>) D (z) z T 1 z : Thus, W z (z i ; z j ) is an indicator of the degree of match between its arguments, in a "soft" sense.
Reference: [3] <author> John S. Bridle. </author> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In David S. Touret-zky, editor, </editor> <booktitle> Advances in Neural Information Processing 2, </booktitle> <pages> pages 211-217. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1989. </year>
Reference-contexts: It is equivalent to using the "softmax" function of neural networks <ref> [3] </ref> on the negative of the Mahalonobis distance to indicate correspondence between z i and elements of A.
Reference: [4] <author> B. Widrow and M.E. Hoff. </author> <title> Adaptive switching circuits. </title> <booktitle> In 1960 IRE WESCON Convention Record, </booktitle> <volume> volume 4, </volume> <pages> pages 96-104. </pages> <address> IRE, New York, </address> <year> 1960. </year>
Reference-contexts: Non-linear stochastic approximation is commonly used in the neural network literature, where it is often called stochastic gradient descent. It was introduced there by Widrow and Hoff <ref> [4] </ref> and has been used extensively with good results. It has been observed that the noise introduced by the sampling can effectively penetrate small local minima. Such local minima are often characteristic of continuous alignment schemes, and we too have found that local minima can be overcome in this manner.
Reference: [5] <author> Simon Haykin. </author> <title> Neural Networks: A comprehensive foundation. </title> <publisher> Macmillan College Publishing, </publisher> <year> 1994. </year>
Reference-contexts: It has been observed that the noise introduced by the sampling can effectively penetrate small local minima. Such local minima are often characteristic of continuous alignment schemes, and we too have found that local minima can be overcome in this manner. The textbook by Haykin <ref> [5] </ref> discusses the use of such algorithms by the neural network community. An excellent discussion of stochastic approximation appears in the textbook by Ljung and Soderstrom [6]. Simulated annealing [7] is another method that has been used in high-dimensional optimization problems having numerous local minima.
Reference: [6] <author> Lennart Ljung and Torsten Soderstrom. </author> <title> Theory and Practice of Recursive Identification. </title> <publisher> MIT Press, </publisher> <year> 1983. </year>
Reference-contexts: The textbook by Haykin [5] discusses the use of such algorithms by the neural network community. An excellent discussion of stochastic approximation appears in the textbook by Ljung and Soderstrom <ref> [6] </ref>. Simulated annealing [7] is another method that has been used in high-dimensional optimization problems having numerous local minima. It too is a descent technique where noise is explicitly added to the current solution.
Reference: [7] <author> S. Kirkpatrick, </author> <title> C.D. Gelatt, and M.P. Vecchi. Optimization by Simulated Annealing. </title> <journal> Science, </journal> <volume> 220(4598) </volume> <pages> 671-680, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: The textbook by Haykin [5] discusses the use of such algorithms by the neural network community. An excellent discussion of stochastic approximation appears in the textbook by Ljung and Soderstrom [6]. Simulated annealing <ref> [7] </ref> is another method that has been used in high-dimensional optimization problems having numerous local minima. It too is a descent technique where noise is explicitly added to the current solution.
Reference: [8] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, </publisher> <year> 1991. </year> <month> 19 </month>
Reference-contexts: These parameters may be chosen so that they are optimal in the maximum likelihood sense with respect to samples drawn from the random variables. This approach is equivalent to minimizing the cross entropy of the estimated distribution with the true distribution <ref> [8] </ref>. Such techniques for density estimation are described in [9].
Reference: [9] <author> Keinosuke Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Harcourt Brace Jovanovich, </publisher> <year> 1990. </year>
Reference-contexts: This approach is equivalent to minimizing the cross entropy of the estimated distribution with the true distribution [8]. Such techniques for density estimation are described in <ref> [9] </ref>.
Reference: [10] <author> J.F. Canny. </author> <title> A Computational Approach to Edge Detection. </title> <journal> IEEE Transactions PAMI, </journal> <volume> PAMI-8(6):679-698, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: In the second row the 11 light moves from top to bottom. These 10 images demonstrate that even for a simple Lambertian surface, image variation can be significant. Below this we show the output of a Canny edge detector run on the 10 different images <ref> [10] </ref>. The variation between the different edge images of the same object is quite striking. Our model consists of 7500 three dimensional points that are uniformly distributed on the surface of the 3 bumps. Each point has an associated surface normal.
Reference: [11] <author> B.K.P. Horn. </author> <title> Robot Vision. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: 25 30 5.21 5.82 19.97 15.41 .56 .45 9.73 5.68 100 20 20 25 20 11.72 11.07 13.85 9.22 .65 .38 8.35 3.12 80 Table 1: Curved Surface Alignment Data represent T , the imaging transformation from model to image coordinates, as a double quaternion followed by a perspective operation <ref> [11] </ref>. We use a dot product metric for the normals where the component density has = 0.7. We use = 0.5 for the image intensities for both the joint and marginal distributions. The size of the random sample used is 50 points. <p> A typical situation is shown in Figure 6 which contains two images of the same car under different lighting. It is well known from photometric stereo research <ref> [11] </ref> that three images under different illumination are sufficient to build a three dimensional model of a surface. The three images and knowledge of the surface properties of the object are enough to constrain the missing parameters of the model: the normal and the albedo.
Reference: [12] <author> Derek LG Hill, Colin Studholme, and David J. Hawkes. </author> <title> Voxel Similarity Measures for Automated Image Registration. </title> <booktitle> In Proceedings of the Third Conference on Visualization in Biomedical Computing, pages 205 - 216. SPIE, </booktitle> <year> 1994. </year>
Reference-contexts: While our technique works well using only shading, it also works well in domains having surface property discontinuities and silhouette information (see Section 3.3). Alignment by extremizing properties of the joint signal has been used by Hill and Hawkes <ref> [12] </ref> to align MRI, CT, and other medical image modalities. They use third order moments to characterize the clustering of the joint data.
Reference: [13] <author> G. Borgefors. </author> <title> Hierarchical Chamfer Matching: A Parametric Edge Matching Algorithm. </title> <journal> IEEE Transactions PAMI, </journal> <volume> 10(6) </volume> <pages> 849-865, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: The approximation of this analogy is due to the dissimilarity between max and softmax. Equation 12 is essentially the measure used in chamfer matching techniques, such as the method described by Borgefors <ref> [13] </ref>. Huttenlocher [14] has used a related measure in feature matching applications, the Hausdorff distance, which uses maximum instead of the sum that appears in Equation 12. The similarity between geometrical matching and entropy becomes even stronger if one uses softmax rather than min, as Wells has [15] [16].
Reference: [14] <author> D.P. Huttenlocher, K. Kedem, K. Sharir, and M. Sharir. </author> <title> The Upper Envelope of Voronoi Surfaces and its Applications. </title> <booktitle> In Proceedings of the Seventh ACM Symposium on Computational Geometry, </booktitle> <pages> pages 194-293, </pages> <year> 1991. </year>
Reference-contexts: The approximation of this analogy is due to the dissimilarity between max and softmax. Equation 12 is essentially the measure used in chamfer matching techniques, such as the method described by Borgefors [13]. Huttenlocher <ref> [14] </ref> has used a related measure in feature matching applications, the Hausdorff distance, which uses maximum instead of the sum that appears in Equation 12. The similarity between geometrical matching and entropy becomes even stronger if one uses softmax rather than min, as Wells has [15] [16].
Reference: [15] <author> W.M. Wells III. </author> <title> Statistical Object Recognition. </title> <type> PhD thesis, </type> <institution> MIT Department Electrical Engineering and Computer Science, </institution> <address> Cambridge, Mass., </address> <year> 1992. </year> <institution> MIT AI Laboratory TR 1398. </institution>
Reference-contexts: Huttenlocher [14] has used a related measure in feature matching applications, the Hausdorff distance, which uses maximum instead of the sum that appears in Equation 12. The similarity between geometrical matching and entropy becomes even stronger if one uses softmax rather than min, as Wells has <ref> [15] </ref> [16]. We reiterate that in vision applications, these methods have typically been used to measure aggregate geometrical distance, while here we are measuring aggregate distances among signal values (typically intensities, brightnesses, or surface properties). <p> A smooth, optimizable version of this metric can be defined by introducing a penalty both for unmatched edges and for the distance between those that are matched [19] <ref> [15] </ref>. This metric can then be used both for image/model comparison and for pose refinement.
Reference: [16] <author> W.M. Wells III. </author> <title> Posterior Marginal Pose Estimation. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 745 - 751. </pages> <publisher> Morgan Kaufmann, </publisher> <month> January </month> <year> 1992. </year>
Reference-contexts: Huttenlocher [14] has used a related measure in feature matching applications, the Hausdorff distance, which uses maximum instead of the sum that appears in Equation 12. The similarity between geometrical matching and entropy becomes even stronger if one uses softmax rather than min, as Wells has [15] <ref> [16] </ref>. We reiterate that in vision applications, these methods have typically been used to measure aggregate geometrical distance, while here we are measuring aggregate distances among signal values (typically intensities, brightnesses, or surface properties).
Reference: [17] <author> P.J. Besl and R.C. Jain. </author> <title> Three-Dimensional Object Recognition. </title> <journal> Computing Surveys, </journal> <volume> 17 </volume> <pages> 75-145, </pages> <year> 1985. </year>
Reference: [18] <author> R.T. Chin and C.R. Dyer. </author> <title> Model-Based Recognition in Robot Vision. </title> <journal> Computing Surveys, </journal> <volume> 18 </volume> <pages> 67-108, </pages> <year> 1986. </year>
Reference: [19] <author> D.G. Lowe. </author> <title> Perceptual Organization and Visual Recognition. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1985. </year>
Reference-contexts: A smooth, optimizable version of this metric can be defined by introducing a penalty both for unmatched edges and for the distance between those that are matched <ref> [19] </ref> [15]. This metric can then be used both for image/model comparison and for pose refinement.
Reference: [20] <author> M.A. Turk and A.P. Pentland. </author> <title> Face Recognition using Eigenfaces. </title> <booktitle> In Proceedings of the Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 586-591, </pages> <address> Lahaina, Maui, Hawaii, </address> <month> June </month> <year> 1991. </year> <note> IEEE. </note>
Reference-contexts: Turk and Pentland have used a large collection of face images to train a system to construct representations that are invariant to some changes in lighting and pose <ref> [20] </ref>. These representations are a projection onto the largest eigenvectors of the distribution of images within the collection. Their system addresses the problem of recognition rather than alignment, and as a result much of the emphasis and many of the results are different.
Reference: [21] <author> A. Shashua. </author> <title> Geometry and Photometry in 3D Visual Recognition. </title> <type> PhD thesis, </type> <institution> M.I.T Artificial Intelligence Laboratory, AI-TR-1401, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: We do not see a straightforward extension of this or similar eigenspace work to the problem of pose refinement. On a related note Shashua has shown that all of the images, under different lighting, of a Lambertian surface are a linear combination of any three of the images <ref> [21] </ref>. This bears a clear relation to the work of Pentland in that the eigenvectors of a set of images of same object should span this three dimensional space. Entropy is playing an ever increasing role within the field of neural networks.
Reference: [22] <author> R. Linsker. </author> <title> From basic network principles to neural architecture. </title> <booktitle> Proceedings of the National Academy of Sciences, USA, </booktitle> <volume> 83 </volume> <pages> 7508-7512, 8390-8394, 8779-8783, </pages> <year> 1986. </year>
Reference-contexts: In most cases the distributions are assumed to be either binomial or Gaussian. This both simplifies and limits such approaches. Linsker has used the concept of information maximization to motivate a theory of development in the primary visual cortex <ref> [22] </ref>. He has been able to predict the development of receptive fields that are very reminiscent of the ones found in primate visual cortex. He uses a Gaussian model both for the signal and the noise.
Reference: [23] <author> Suzanna Becker and Geoffrey E. Hinton. </author> <title> Learning to make coherent predictions in domains with discontinuities. </title> <editor> In John E. Moody, Steven J. Hanson, and Richard P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 4, </booktitle> <address> Denver 1991, 1992. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address> <month> 20 </month>
Reference-contexts: He uses a Gaussian model both for the signal and the noise. Becker and Hinton have used the maximization of mutual information as a framework for learning different low-level processing algorithms such as disparity estimation and curvature estimation <ref> [23] </ref>. They assume that the signals whose mutual information is to be maximized are Gaussian. In addition, they assume that the only joint information between images is the information that they wish to extract (i.e. the train their disparity detectors on random dot stereograms).
Reference: [24] <author> Anthony J. Bell. </author> <title> An information-maximisation approach to blind separation. </title> <booktitle> In Advances in Neural Information Processing Systems, volume 7, </booktitle> <address> Denver 1994, 1995. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco. </address> <note> To appear. 21 Figure: Skull Alignment </note>
Reference-contexts: In addition, they assume that the only joint information between images is the information that they wish to extract (i.e. the train their disparity detectors on random dot stereograms). Finally, Bell has used a measure of information to separate signals that have been linearly mixed together <ref> [24] </ref>. His technique assumes that the different mixed signals carry little mutual information. While he does not assume that the distribution has a particular functional form, he does assume that the distribution is well matched to a pre-selected transfer function.
References-found: 24

