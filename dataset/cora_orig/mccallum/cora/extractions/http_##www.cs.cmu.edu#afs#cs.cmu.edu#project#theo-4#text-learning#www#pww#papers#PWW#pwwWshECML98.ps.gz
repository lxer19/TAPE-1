URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-4/text-learning/www/pww/papers/PWW/pwwWshECML98.ps.gz
Refering-URL: 
Root-URL: 
Email: E-mail: Marko.Grobelnik@ijs.si, Dunja.Mladenic@ijs.si  
Phone: Phone: (+38)(61) 1773 272, Fax: (+38)(61) 1258-158  
Title: Efficient text categorization  
Author: Marko Grobelnik and Dunja Mladenic 
Date: February 17, 1998  
Address: Jamova 39, 1111 Ljubljana, Slovenia  
Affiliation: Department for Intelligent Systems, J.Stefan Institute,  
Abstract: We present an approach to text categorization using machine learning techniques. The approach is developed and tested on large text hierarchy named Yahoo that is available on the Web. We handle the large number of features and training examples by taking into account hierarchical structure of examples and using feature subset selection for large text data. The large number of categories is handled separately for each testing example by pruning unpromising categories. In this way, the number of categories to be considered is cut to less than a half without degrading the system performance. Our experiments are performed using naive Bayesian classifier on text data using feature-vector document representation that includes n-grams instead of just single words (unigrams). Experimental evaluation on three domains constructed from Yahoo hierarchy shows that among several hundred categories the correct category is assigned probability over 0.99 when rather small number of features used.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., Verkamo, A.I., </author> <year> 1996. </year> <title> Fast Discovery of Association Rules, </title> <editor> In Fayyad, U.M., Piatetsky-Shapiro, G., Smyth, </editor> <title> 8 Education (percent of testing examples with pruned correct category, average number of considered categories.) Computers and Internet (percent of testing examples with pruned correct category, average number of considered categories.) </title> <editor> P., Uthurusamy, R. (eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining AAAI Press/The MIT Press, </booktitle> <pages> pp. 307|328. </pages>
Reference-contexts: Each new pass generates features of length i only from the candidate features (of length i1) generated in the previous pass. This process is similar to the large k-itemset generation used in association rules algorithm <ref> [1] </ref>. 3 Learning text classifier In order to handle large number of examples and features we divide the whole problem into subproblems as suggested in [5] for hierarchical document classification. For a new document, the classifier returns probability distribution over categories included in the hierarchy.
Reference: [2] <author> Domingos, P., Pazzani, M., </author> <year> 1997. </year> <title> On the Optimality of the Simple Bayesian Classifier under Zero-One Loss, Machine Learning 29, </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 103|130. </pages>
Reference-contexts: The assumption about feature independence used in naive Bayesian classifier is here incorrect, especially when features representing several words are added. According to <ref> [2] </ref> this does not necessary mean that classifier will have poor performance because of that.
Reference: [3] <author> Filo, D., Yang, J., </author> <year> 1997. </year> <institution> Yahoo! Inc., </institution> <year> 1997. </year> <note> http://www.yahoo.com/docs/pr/ </note>
Reference: [4] <author> Grobelnik, M., Mladenic, D., </author> <title> Learning Machine: design and impelmentation, </title> <type> Technical Report IJS-DP-7824, </type> <institution> Department for Intelligent Systems, J.Stefan Institute, </institution> <year> 1998. </year>
Reference-contexts: This property of used classifier enables pruning of unpromising categories. 4 Experimental results Experimental evaluation of developed approach is performed using our recently developed machine learning system Learning Machine <ref> [4] </ref> that supports usage of different machine learning techniques on large data sets with especially designed modules for learning on text and collecting data from the Web.
Reference: [5] <author> Koller, D., Sahami, M., </author> <title> Hierarchically classifying documents using very few words, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 170|178, </pages> <year> 1997. </year>
Reference-contexts: This process is similar to the large k-itemset generation used in association rules algorithm [1]. 3 Learning text classifier In order to handle large number of examples and features we divide the whole problem into subproblems as suggested in <ref> [5] </ref> for hierarchical document classification. For a new document, the classifier returns probability distribution over categories included in the hierarchy. For each of the subproblems, a classifier is constructed that predicts probability that a document is a member of a corresponding category.
Reference: [6] <author> Mitchell, </author> <title> T.M., Machine Learning, </title> <publisher> The McGraw-Hill Companies, Inc., </publisher> <year> 1997. </year> <month> 9 </month>
Reference-contexts: The last subtree `Videos' is a pointer to category `Business and Economy: Companies: Arts and Craft: Performing Arts: Dance: Videos: Modern'. Learning algorithm used in our experiments is naive Bayesian classifier on feature-vector as described in <ref> [6] </ref>. In this approach documents are represented with frequency vectors where an attribute is defined for each word position in the document hav 4 ing word at that position as its value.
Reference: [7] <author> Mladenic, D., </author> <title> Feature subset selection in text-learning, </title> <booktitle> Proc. of the 10th European Conference on Machine Learning ECML98, </booktitle> <year> 1998. </year> <editor> [8] van Rijsbergen, C.J,. Harper, D.J., Porter, M.F., </editor> <title> The selection of good search terms, </title> <booktitle> Information Processing & Management, 17, </booktitle> <address> pp.77|91, </address> <year> 1981. </year>
Reference-contexts: In this way we can capture some characteristic word combinations but also increase the number of feature (eg. in whole Yahoo hierarchy from 69,280 features for 1-grams to 255,602 features for 5-grams). We additionally apply feature subset selection as commonly used on text data eg. <ref> [7] </ref>, [10]. In this approach a score is assigned to each feature independently, features are sorted according to the assigned score and a predefined number of the best features is taken to form the solution feature subset.
Reference: [9] <author> Shaw Jr, W.M., </author> <title> Term-relevance computations and perfect retrieval performance, </title> <booktitle> Information Processing & Management, 31(4), </booktitle> <address> pp.491|498, </address> <year> 1995. </year>
Reference-contexts: Where P (W jpos) is the conditional probability of word W occurring given the class value `positive', P (W jneg) is the conditional probability of word W occurring given the class value `negative'. We handle singularities as proposed in <ref> [9] </ref>. The process of feature generation is performed in n passes over documents, where i-grams are generated in the i-th pass. At the end of each pass over documents all infrequent features are deleted (here we check for frequency &lt; 4).
Reference: [10] <author> Yang, Y., Pedersen, J.O., </author> <title> A Comparative Study on Feature Selection in Text Categorization, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 412|420, </pages> <year> 1997. </year> <month> 10 </month>
Reference-contexts: In this way we can capture some characteristic word combinations but also increase the number of feature (eg. in whole Yahoo hierarchy from 69,280 features for 1-grams to 255,602 features for 5-grams). We additionally apply feature subset selection as commonly used on text data eg. [7], <ref> [10] </ref>. In this approach a score is assigned to each feature independently, features are sorted according to the assigned score and a predefined number of the best features is taken to form the solution feature subset.
References-found: 9

