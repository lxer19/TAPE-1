URL: http://www.cs.wisc.edu/~tamches/cs736.ps
Refering-URL: 
Root-URL: 
Title: The Performance of Non-Blocking and Wait-Free Highly Concurrent Objects in Asynchronous Shared-Memory Multiprocessors  
Author: Ariel Tamches William Donaldson James Jason 
Date: May 12, 1994  
Abstract: Traditional shared-memory mutual exclusion techniques (locks or semaphores) allow only a single process access to a shared object at a time. Such techniques are blocking; no other process may proceed to enter a "critical section" until the first process leaves it, no matter how long that takes. Any unexpected delay (process preemption, page fault, etc.) suffered by the first process is unnecessarily shared by each blocked process. In contrast, a non-blocking (also called lock free) approach guarantees that at least one process will progress after a finite number of system steps. A wait-free approach guarantees that all processes progress after a finite number of steps. Maurice Herlihy has recently implemented a methodology for non-blocking and wait-free mutual exclusion on MIMD shared-memory multiprocessors [8], which includes some performance measurements based on an insufficient benchmark suite. In this experimental research project, we have implemented and extensively measured the performance of blocking, non-blocking, and wait-free objects on a more sufficient benchmark suite. Comparing performance aspects of the various methodologies on four realistic, fully asynchronous benchmarks shows that blocking mutual exclusion outperforms non-blocking and wait-free methodologies for several reasons, including, unfortunately, the extra overhead of the measuring code itself. Among the legitimate factors contributing to this disparity, the cost of copying in non-blocking and wait-free is determined to be the key; up to 50% of CPU traffic is spent making local copies of the shared object in our non-blocking and wait-free implementation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Juan Alemany and Edward Felten. </author> <title> Performance issues in non-blocking synchronization on shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 11th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 125-134, </pages> <year> 1992. </year>
Reference-contexts: Wait traffic is the total traffic minus the service traffic. The ratio of service to total traffic gives the percentage of work that is useful, as opposed to overhead, blocking, copying, or otherwise. An incentive for calculating the total traffic was to challenge the core assumption made in <ref> [1] </ref>, which presents a technique (requiring operation system support) to entirely eliminate copying in the (supposedly) common case of 1 process working on an object at a time 7 , paying a penalty when this is not so. <p> A recent paper concentrated on operating system support to reduce the cost of copying <ref> [1] </ref> with an optimistic protocol that does not have to make a copy in the case of no competition.
Reference: [2] <author> T. E. Anderson. </author> <title> The performance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: A backoff amount is maintained for each process; this amount doubles for each unsuccessful store-conditional, up to a fixed limit which should roughly be n fi t, where n is the number of processors and t is the response time for an operation <ref> [2] </ref>. 5 Each time an operation begins, the backoff amount is reduced by half. Exponential backoff not only eliminated starvation in Herlihy's benchmark (the average number of attempts per operation became 1.00, and the maximum was reduced by an order of magnitude), but dramatically increased throughput as well.
Reference: [3] <author> Dimitri Bertsekas and Jogn Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: This example was selected because of the availability of a verifiably correct asynchronous version of the original algorithm <ref> [3] </ref>; no iteration barriers were required. The calculations in this algorithm are not iteration dependent. The shortest distances are relaxed as long as desired; we simply set the stop condition to a time limit. In our implementation, each processor is assigned to a set of vertices.
Reference: [4] <author> Richard Burden and J. Douglas Fairnes. </author> <title> Numerical Analysis. </title> <publisher> PWS-Kent Publishing Company, </publisher> <address> fourth edition, </address> <year> 1988. </year>
Reference-contexts: use, while an object that is in high demand lends itself well to the waitfree object. 5.4 Poisson Equation Solver The Poisson equation is an elliptic partial-differential equation that often arises in the study of numerous time-independent physical problems such as the steady-state distribution of heat in a plane region <ref> [4] </ref>. The algorithm divides up the rectangular region described by the equation into a set of smaller rectangles, whose vertices are called mesh points.
Reference: [5] <author> M. Herlihy and J. Wing. </author> <title> Axioms for concurrent objects. </title> <booktitle> In 14th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 13-26, </pages> <year> 1987. </year>
Reference-contexts: Even stronger, a methodology is wait-free if it can guarantee that every process always progresses after a finite number of steps. Although theoretical issues of non-blocking and wait-free mutual exclusion are well-known <ref> [11, 5, 7] </ref>, no practical implementation existed until Maurice Herlihy presented a methodology [6, 8] which alleviated the need to explicitly reason about concurrency. This methodology achieves non-blocking mutual exclusion by having each process work on a copy of the shared object in the process' local memory space. <p> Put another way, a linearlizable object (appears to) operate at the granularity of its member functions. Herlihy has shown <ref> [5, 7] </ref> that linearlizability is the basis of the correctness proof for a non-blocking or wait-free methodology.
Reference: [6] <author> Maurice Herlihy. </author> <title> A methodology for implementing highly concurrent data structures. </title> <booktitle> In Proceedings of the 2nd ACM SIGPLAN Symposium on Principles And Practice of Parallel Programming, </booktitle> <pages> pages 197-206, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Even stronger, a methodology is wait-free if it can guarantee that every process always progresses after a finite number of steps. Although theoretical issues of non-blocking and wait-free mutual exclusion are well-known [11, 5, 7], no practical implementation existed until Maurice Herlihy presented a methodology <ref> [6, 8] </ref> which alleviated the need to explicitly reason about concurrency. This methodology achieves non-blocking mutual exclusion by having each process work on a copy of the shared object in the process' local memory space.
Reference: [7] <author> Maurice Herlihy. </author> <title> Wait-free synchronization. </title> <journal> acm Transactions on Programming Languages and Systems, </journal> <volume> 13(1) </volume> <pages> 123-149, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Even stronger, a methodology is wait-free if it can guarantee that every process always progresses after a finite number of steps. Although theoretical issues of non-blocking and wait-free mutual exclusion are well-known <ref> [11, 5, 7] </ref>, no practical implementation existed until Maurice Herlihy presented a methodology [6, 8] which alleviated the need to explicitly reason about concurrency. This methodology achieves non-blocking mutual exclusion by having each process work on a copy of the shared object in the process' local memory space. <p> Put another way, a linearlizable object (appears to) operate at the granularity of its member functions. Herlihy has shown <ref> [5, 7] </ref> that linearlizability is the basis of the correctness proof for a non-blocking or wait-free methodology.
Reference: [8] <author> Maurice Herlihy. </author> <title> A methodology for implementing highly concurrent data objects. </title> <journal> acm Transactions on Programming Languages and Systems, </journal> <volume> 15(5) </volume> <pages> 745-770, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Even stronger, a methodology is wait-free if it can guarantee that every process always progresses after a finite number of steps. Although theoretical issues of non-blocking and wait-free mutual exclusion are well-known [11, 5, 7], no practical implementation existed until Maurice Herlihy presented a methodology <ref> [6, 8] </ref> which alleviated the need to explicitly reason about concurrency. This methodology achieves non-blocking mutual exclusion by having each process work on a copy of the shared object in the process' local memory space. <p> But note that in such an implementation, there can be spurious failures of store--conditional, since cache lines can be invalidated for many reasons (e.g. a process preemption usually flushes a cache). 1 2.1 Validate A validate instruction <ref> [8] </ref> indicates whether a store-conditional would have succeeded; that is, whether the shared word has been modified by a store-conditional from a "competing" process. <p> Herlihy quips, "It is no secret that reasoning about concurrent programs is difficult. A practical methodology should permit a programmer to design, say, a correct lock-free priority queue without ending up with a publishable result." <ref> [8, page 746] </ref> When reasoning about the correctness of Herlihy's methodology and ours, note that the member functions of its objects are linearlizable|operations appear to take effect instantly at some time. Put another way, a linearlizable object (appears to) operate at the granularity of its member functions. <p> The enqueue operation was just 10% slower than dequeue, yet as many as 5000 attempts were required, with an average of about 80 <ref> [8, page 757] </ref>. This seems overwhelming evidence that starvation is a major problem, yet it is important to realize that such measurements are highly dependent on the call frequency in the benchmark used. <p> Measuring the total traffic on our benchmarks will test the validity of this assumption. Copying is obviously the bottleneck in Herlihy's non-blocking and wait-free methodologies, yet no measurements related to copying were reported in <ref> [8] </ref>. We measure three such statistics. The first measures the number of copies required in a Load-locked operation.
Reference: [9] <author> E. Jensen, G. Hagensen, and J. Broughton. </author> <title> A new approach to exclusive data access in shared memory multiprocessors. </title> <type> Technical Report UCRL-97663, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: The paper ends with a conclusion (Section 6) and appendices. 2 Instruction Set Support for Non-Blocking Objects The load-locked and store-conditional primitives, pioneered in <ref> [9] </ref> and currently present in the MIPS-II [10] and Alpha AXP [14] architectures, are the key to Herlihy's methodology. Load-locked loads a word of shared memory into a processor's register.
Reference: [10] <author> G. Kane. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N. J., </address> <year> 1992. </year>
Reference-contexts: Two special machine instructions, Load-locked and Store-conditional , present in the Alpha AXP [14] and MIPS-II <ref> [10] </ref> architectures, are the key to the simplicity of Herlihy's methodology. Although Herlihy's theoretical treatment is complete, there remains much room for improvement in the experimental area. <p> The paper ends with a conclusion (Section 6) and appendices. 2 Instruction Set Support for Non-Blocking Objects The load-locked and store-conditional primitives, pioneered in [9] and currently present in the MIPS-II <ref> [10] </ref> and Alpha AXP [14] architectures, are the key to Herlihy's methodology. Load-locked loads a word of shared memory into a processor's register. Store-conditional stores the register's value back to the shared memory location, but only if no other process has executed a store-conditional before it.
Reference: [11] <author> Leslie Lamport. </author> <title> Concurrent reading and writing. </title> <journal> Communications of the ACM, </journal> 20(11) 806-811, November 1977. 
Reference-contexts: Even stronger, a methodology is wait-free if it can guarantee that every process always progresses after a finite number of steps. Although theoretical issues of non-blocking and wait-free mutual exclusion are well-known <ref> [11, 5, 7] </ref>, no practical implementation existed until Maurice Herlihy presented a methodology [6, 8] which alleviated the need to explicitly reason about concurrency. This methodology achieves non-blocking mutual exclusion by having each process work on a copy of the shared object in the process' local memory space.
Reference: [12] <author> Robert Metcalfe and David Boggs. </author> <title> Ethernet: Distributed packet switching for local computer networks. </title> <journal> Communications of the ACM, </journal> <volume> 19(7) </volume> <pages> 395-404, </pages> <year> 1976. </year>
Reference-contexts: Herlihy used a toy program in which the processes spend essentially all of their time calling enqueue and dequeue (no processing). Read world programs do not spend all of their time operating on shared objects. Exponential Backoff <ref> [12] </ref> is a probabalistic guarantee against starvation. A process whose store-conditional fails backs off (delays) before retrying.
Reference: [13] <author> Subbarao Palacharla, </author> <year> 1994. </year> <type> Personal Communication. </type>
Reference-contexts: Number of Processes Response time for member get () in Bellman-Ford benchmark blocking + + + non-blocking fi fi fi fi non-blocking w/backoff 4 4 4 wait-free 8 Simply put, we can statically tell vt, the virtual time measurer, that a certain section of code (our measurements) takes 0 cycles. <ref> [13] </ref> 12 0:0s 200:0s 400:0s 600:0s Number of Processes Response time for member set () in Bellman-Ford benchmark blocking + + + + non-blocking fi fi fi fi non-blocking w/backoff 4 4 4 4 wait-free 0 0:4 0:8 4 6 8 10 12 14 16 Frac procs copying at a time
Reference: [14] <author> Richard Sites. </author> <title> Alpha axp architecture. </title> <journal> Communications of the ACM, </journal> <volume> 36(2) </volume> <pages> 33-44, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The first process to execute this pointer swap succeeds in updating the object; later processes will fail and have to repeat the entire operation, beginning with making a local copy of the shared object. Two special machine instructions, Load-locked and Store-conditional , present in the Alpha AXP <ref> [14] </ref> and MIPS-II [10] architectures, are the key to the simplicity of Herlihy's methodology. Although Herlihy's theoretical treatment is complete, there remains much room for improvement in the experimental area. <p> The paper ends with a conclusion (Section 6) and appendices. 2 Instruction Set Support for Non-Blocking Objects The load-locked and store-conditional primitives, pioneered in [9] and currently present in the MIPS-II [10] and Alpha AXP <ref> [14] </ref> architectures, are the key to Herlihy's methodology. Load-locked loads a word of shared memory into a processor's register. Store-conditional stores the register's value back to the shared memory location, but only if no other process has executed a store-conditional before it.
References-found: 14

