URL: http://www.cs.toronto.edu/~chow/papers/depth93.ps.gz
Refering-URL: http://www.cs.toronto.edu/~chow/interests.html
Root-URL: 
Email: chow@cs.toronto.edu  
Title: Parallel Unconstrained Optimization  
Author: Kwok L. Chow 
Address: Ontario, Canada M5S 1A4  
Affiliation: Department of Computer Science University of Toronto, Toronto  
Abstract: We deal with the parallel solution of the nonlinear, unconstrained, optimization problem minff(x) j x 2 R n g where the objective function f : R n ! R is continuous and differentiable. For the most part, we assume that f has continuous second derivatives and is pseudo-convex. Most of the parallel methods for optimization are based on the fact that the parallelism can be exploited in the levels of function evaluations, and search direction and stepsize computations. In this article, we discuss these parallel numerical methods and the motivation which led to their development. We also outline the possible future research directions. 
Abstract-found: 1
Intro-found: 1
Reference: [BDKS90] <author> R.H. Byrd, C.L. Dert, A.H.G. Rinnooy Kan, and R.B. Schnabel. </author> <title> Concurrent stochastic methods for global optimization. </title> <journal> Mathematical Programming, </journal> <volume> 46 </volume> <pages> 1-29, </pages> <year> 1990. </year>
Reference-contexts: Due to the expensive nature of global optimization and the practical need to solve such problems, there is ample incentive to develop parallel global optimization methods. The references <ref> [ES89, TZ89, BDKS90] </ref> give the recent advances in this research area. Parallel automatic differentiation If the leading expense of solving unconstrained optimization problems is the evaluation of the objective function and its derivatives, one possibility to improve the performance is simply to parallelize each of these computations.
Reference: [Bro70] <author> C.G. </author> <title> Broyden. The convergence of a class of double-rank minimization algorithms. </title> <journal> Journal of Institute Mathematical Application, </journal> <volume> 6 </volume> <pages> 222-231, </pages> <year> 1970. </year>
Reference: [BSS88a] <author> R.H. Byrd, R.B. Schnabel, and G.A. Shultz. </author> <title> Parallel quasi-newton methods for unconstrained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 42 </volume> <pages> 273-306, </pages> <year> 1988. </year>
Reference-contexts: The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods <ref> [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91] </ref>, and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. <p> However Straeter method does not have this property. Hence van Laarhoven claims that this parallel method should exhibit better convergence results than the Straeter method. 3.3 Byrd-Schnabel-Shultz methods Byrd, Schnabel and Shultz <ref> [BSS88a, BSS88b] </ref> have investigated parallel quasi-Newton methods for solving the unconstrained optimization problem. They emphasize on the new methods that effectively utilize multiple processors to perform multiple function and derivative evaluations simultaneously. <p> The details of the convergence proofs and computational results can be found in the reference. 3.4 Still's methods The investigations by Byrd, Schnabel, and Schultz <ref> [BSS88a] </ref> into the development of a parallel secant method focus almost entirely on performing the linear algebra computations and function calculations in parallel. Still [Sti90, Sti91] claims that their formulations are only good for the use on vector computers. The inherent parallelism of the unconstrained optimization problem is left untapped. <p> This is the idea of speculative finite difference (inverse) Hessian tech-nique borrowed from the method by Byrd, Schnabel, and Schultz <ref> [BSS88a, BSS88b] </ref>. Hence this results in another parallel variant of Newton's method. Lootsma claims that there is no particular difference between the two parallel variants if every iteration makes a full scan with the search vectors s k j in all coordinates directions.
Reference: [BSS88b] <author> R.H. Byrd, R.B. Schnabel, and G.A. Shultz. </author> <title> Using parallel function evaluations to improve hessian approximation for unconstrained optimization. </title> <journal> Annals of Operations Research, </journal> <volume> 14 </volume> <pages> 167-193, </pages> <year> 1988. </year>
Reference-contexts: The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods <ref> [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91] </ref>, and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. <p> However Straeter method does not have this property. Hence van Laarhoven claims that this parallel method should exhibit better convergence results than the Straeter method. 3.3 Byrd-Schnabel-Shultz methods Byrd, Schnabel and Shultz <ref> [BSS88a, BSS88b] </ref> have investigated parallel quasi-Newton methods for solving the unconstrained optimization problem. They emphasize on the new methods that effectively utilize multiple processors to perform multiple function and derivative evaluations simultaneously. <p> The second assumption is motivated by the computational experience from Byrd et al.. They show that the iterations saved by using a finite difference Newton's method usually do not offset the extra cost per iteration in function evaluations. Byrd, Schnabel, and Shultz <ref> [BSS88b] </ref> consider the parallel methods that use part of the finite difference Hessian whenever there are not enough processors to evaluate the function, gradient, and Hessian in one concurrent function evaluation step, but more than to evaluate just the function and gradient. <p> This is the idea of speculative finite difference (inverse) Hessian tech-nique borrowed from the method by Byrd, Schnabel, and Schultz <ref> [BSS88a, BSS88b] </ref>. Hence this results in another parallel variant of Newton's method. Lootsma claims that there is no particular difference between the two parallel variants if every iteration makes a full scan with the search vectors s k j in all coordinates directions.
Reference: [BT89] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and distributed computation : Numerical methods. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: Therefore, the discussion in the remainder of this paper is mainly oriented 5 towards parallel computation on multiprocessors in which the specific type of architecture is relatively unimportant. Moreover, two types of parallel algorithms for multiprocessors <ref> [Kun76, BT89] </ref>, i.e. synchronous and asynchronous algorithms, will be discussed. The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information.
Reference: [CM70] <author> D. Chazan and W.L. Miranker. </author> <title> A nongradient and parallel algorithm for unconstrained minimization. </title> <journal> SIAM Journal of Control, </journal> <volume> 8 </volume> <pages> 207-217, </pages> <year> 1970. </year>
Reference-contexts: Moreover, two types of parallel algorithms for multiprocessors [Kun76, BT89], i.e. synchronous and asynchronous algorithms, will be discussed. The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods <ref> [CM70, Slo75, Sut83, DT91] </ref>, the first derivative parallel methods [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91], and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. <p> Hence the methods make no use of any of the derivatives of the function. Two techniques are used to construct these methods and they are direct search and conjugate direction. We will discuss both parallel methods and their serial counterparts. 2.1 Chazan-Miranker method The Chazan-Miranker method <ref> [CM70] </ref> was motivated by the conjugate direction methods of Powell [Pow64] and of Zangwill [Zan67] which require no costly computation of gradient and Hessian of a given objective function. <p> We note that if s n j turns out to be zero, s 1 j would eventually become zero and the algorithm would simply imply that x j+1 = x j . The convergence proof of this algorithm can be found in the references <ref> [Zan67, CM70] </ref>. It is seen that the Chazan-Miranker method uses features found in Powell's method as well as a feature resembling the modification 2 introduced by Zangwill. However, it is quite different from either of these methods and has as its principal objective to increase the degree of parallelism.
Reference: [CM93] <author> D. Conforti and R. Musmanno. </author> <title> A parallel asynchronous newton algorithm for unconstrained optimization. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 77(2) </volume> <pages> 305-322, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91], and the second derivative parallel methods <ref> [Pie73, FR88, CM93, Loo89, Loo91] </ref>. Besides, we also discuss a general framework, namely parallel gradient distribution [Man93], that was recently proposed. 2 Non-derivative parallel methods In this section, we outline the parallel methods which are based on the comparison of values of the objective function only. <p> In this case x j is the minimizer of the given objective function f . Moreover, x j is a global minimizer if the function is uniformly convex. The details of the proof can be found in the reference [FR88]. Computation experience has been conducted by Conforti and Musmanno <ref> [CM93] </ref> on different parallel computing systems. Numerical results indicate that the proposed asynchronous algorithm guarantees to converge to the solution of a wide class of objective functions.
Reference: [Dav59] <author> W.C. Davidon. </author> <title> Variable metric method for minimization. </title> <type> Technical Report ANL-5990, </type> <institution> AEC Research and Development Report, </institution> <month> November </month> <year> 1959. </year>
Reference: [Dix88] <author> L.C.W. Dixon. </author> <title> Automatic differentiation and parallel processing in optimization. Optimization, </title> <booktitle> parallel processing and applications, </booktitle> <pages> pages 86-93, </pages> <year> 1988. </year>
Reference-contexts: One of the possible ways to parallelize the function and its derivatives evaluations is by means of automatic differentiation [Ral81]. The parallel automatic differentiation was investigated by Dixon <ref> [Dix88] </ref> and Fischer [Fis90]. Parallel automatic differentiation can be considered in conjunction with the design and analysis of parallel numerical algorithms for unconstrained optimization problems. Parallel gradient distribution In principle, this framework can be applied to enhance the 32 degree of parallelism even for the existing parallel unconstrained optimization algorithms.
Reference: [DT91] <author> Jr J.E. Dennis and V. Torczon. </author> <title> Direct search methods in parallel machines. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 448-474, </pages> <month> 11 </month> <year> 1991. </year>
Reference-contexts: Moreover, two types of parallel algorithms for multiprocessors [Kun76, BT89], i.e. synchronous and asynchronous algorithms, will be discussed. The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods <ref> [CM70, Slo75, Sut83, DT91] </ref>, the first derivative parallel methods [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91], and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. <p> Hence the sequence of points built by the parallel method also converges for the same class of functions and terminates in a finite number of iterations for a quadratic function. 10 2.4 Dennis-Torczon methods Dennis and Torczon <ref> [DT91] </ref> describe an approach to construct non-derivative methods for unconstrained optimization that can be easily implemented on parallel computers. These methods are based on the direct search technique, namely the simplex method, proposed by Nelder and Mead [NM65] in the mid-sixties.
Reference: [ES89] <author> E. Eskow and R.B. Schnabel. </author> <title> Mathematical modeling of a parallel global optimization algorithm. </title> <journal> Parallel Computing, </journal> <volume> 12 </volume> <pages> 315-325, </pages> <year> 1989. </year>
Reference-contexts: Due to the expensive nature of global optimization and the practical need to solve such problems, there is ample incentive to develop parallel global optimization methods. The references <ref> [ES89, TZ89, BDKS90] </ref> give the recent advances in this research area. Parallel automatic differentiation If the leading expense of solving unconstrained optimization problems is the evaluation of the objective function and its derivatives, one possibility to improve the performance is simply to parallelize each of these computations.
Reference: [FGK73] <author> L. Fratta, M. Gerla, and L. Kleinrock. </author> <title> The flow derivation method : An approach to store anmd forward communication network design. </title> <journal> Networks, </journal> <volume> 3 </volume> <pages> 97-133, </pages> <year> 1973. </year>
Reference-contexts: Practical applications of this problem include mathematical programming [FM68, Wri92], machine learning [Rea86, Man92], nonlinear regression [JDS83], and engineering design <ref> [FGK73] </ref>. The mathematical programming problem of optimizing a nonlinear function subject to nonlinear constraints also has many practical applications. Fiacco and McCormick [FM68] show how this problem can be solved by barrier methods which involve the solution of a sequence of unconstrained optimization problems. <p> The nonlinear regression problem is important to fields such as physics, chemistry, and medicine, etc. Dennis and Schnabel [JDS83] describe an unconstrained optimization problem arising in physics that involves fitting a bell shaped curve to twenty pieces of solar spectroscopy data. In Fratta et al. <ref> [FGK73] </ref>, a nonlinear, unconstrained, multicommodity flow problem is described. This problem has applications to the design of computer networks. Other interesting problems such as the problem of solving nonlinear equations and the integer programming problem can also be reduced to unconstrained optimization problems (cf.
Reference: [Fis90] <author> H. Fisher. </author> <title> Automatic differentiation : parallel computation of function, gradient, and hessian matrix. </title> <journal> Parallel Computing, </journal> <volume> 13 </volume> <pages> 101-110, </pages> <year> 1990. </year>
Reference-contexts: One of the possible ways to parallelize the function and its derivatives evaluations is by means of automatic differentiation [Ral81]. The parallel automatic differentiation was investigated by Dixon [Dix88] and Fischer <ref> [Fis90] </ref>. Parallel automatic differentiation can be considered in conjunction with the design and analysis of parallel numerical algorithms for unconstrained optimization problems. Parallel gradient distribution In principle, this framework can be applied to enhance the 32 degree of parallelism even for the existing parallel unconstrained optimization algorithms.
Reference: [FM68] <author> A.V. </author> <title> Fiacco and G.P. McCormick. Nonlinear programming : Sequential unconstrained minimization techniques. </title> <publisher> John Wiley & Sons, </publisher> <year> 1968. </year>
Reference-contexts: Practical applications of this problem include mathematical programming <ref> [FM68, Wri92] </ref>, machine learning [Rea86, Man92], nonlinear regression [JDS83], and engineering design [FGK73]. The mathematical programming problem of optimizing a nonlinear function subject to nonlinear constraints also has many practical applications. <p> Practical applications of this problem include mathematical programming [FM68, Wri92], machine learning [Rea86, Man92], nonlinear regression [JDS83], and engineering design [FGK73]. The mathematical programming problem of optimizing a nonlinear function subject to nonlinear constraints also has many practical applications. Fiacco and McCormick <ref> [FM68] </ref> show how this problem can be solved by barrier methods which involve the solution of a sequence of unconstrained optimization problems. In 1984, Karmarkar's announcement of a fast polynomial time method for linear programming caused tremendous excitement in the field of optimization.
Reference: [FM93] <author> M. C. Ferris and O. L. Mangasarian. </author> <title> Parallel variable distribution. </title> <type> Technical Report Technical Report #1175, </type> <institution> Department of Computer Sciences, University of Wisconsin-Madison, WI, </institution> <month> August </month> <year> 1993. </year> <month> 37 </month>
Reference-contexts: Hopefully this will not just enhance the degree of parallelism of the problems but also the performance of the parallel algorithms used to solve them. Parallel variable distribution Ferris and Mangasarian <ref> [FM93] </ref> recently developed a parallel unconstrained optimization method called parallel variable distribution. It is a variant of the parallel gradient distribution and further highlights the advantage of the use of parallel gradient distribution framework. The principle idea of parallel variable distribution is given as follows.
Reference: [FP63] <author> R. Fletcher and M.J.D. Powell. </author> <title> A rapidly convergent descent method for mini-mization. </title> <journal> Computer journal, </journal> <volume> 6 </volume> <pages> 163-168, </pages> <year> 1963. </year>
Reference: [FR88] <author> H. Fischer and K. Ritter. </author> <title> An asynchronous parallel newton method. </title> <journal> Mathematical Programming, </journal> <volume> 42 </volume> <pages> 363-374, </pages> <year> 1988. </year>
Reference-contexts: This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91], and the second derivative parallel methods <ref> [Pie73, FR88, CM93, Loo89, Loo91] </ref>. Besides, we also discuss a general framework, namely parallel gradient distribution [Man93], that was recently proposed. 2 Non-derivative parallel methods In this section, we outline the parallel methods which are based on the comparison of values of the objective function only. <p> The number of processors will not be prohibitive in this parallel line search technique. However, the parallel implementation and its results are left unexplored. 26 4.2 Fischer-Ritter method Fischer and Ritter <ref> [FR88] </ref> described a parallel Newton method for minimizing twice continuously differentiable and uniformly convex functions. The parallel algorithm has the super-linear convergence property. The basic steps in an iteration of Newton's method consist of calculating the gradient vector, the Hessian matrix, the search direction, and the stepsize. <p> In this case x j is the minimizer of the given objective function f . Moreover, x j is a global minimizer if the function is uniformly convex. The details of the proof can be found in the reference <ref> [FR88] </ref>. Computation experience has been conducted by Conforti and Musmanno [CM93] on different parallel computing systems. Numerical results indicate that the proposed asynchronous algorithm guarantees to converge to the solution of a wide class of objective functions. <p> The first approach is to simply replace the kth row or column of the Hessian matrix itself by the vector d k j . This approach can be incorporated into the asynchronous Newton method by Fischer and Ritter <ref> [FR88] </ref>. This results in another asynchronous Newton's method.
Reference: [GL80] <author> R. Gerber and F. Luk. </author> <title> A generalized broyden's method for solving simultaneous linear systems. </title> <type> Technical Report TR-80-438, </type> <institution> Department of Computer Sciences, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1980. </year>
Reference-contexts: Also we are free to choose the update vector v j . Hence we yield the inverse Hessian update H j+1 = H j + v T v T Still referred to a generalized secant method by Gerber and Luk <ref> [GL80] </ref> for solving linear systems and applied the results to minimization of quadratic functions. But there is a need to change the inverse Hessian update before the Gerber and Luk method can be applied.
Reference: [GMW81] <author> P.E. Gill, W. Murray, and M.H. Wright. </author> <title> Practical optimization. </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: Most of the unconstrained optimization methods have the form of the following algorithmic model (cf. Gill et al. <ref> [GMW81] </ref>).
Reference: [HM89] <author> M.E. Henderson and W.L. Miranker. </author> <title> Synergy in parallel algorithms. </title> <journal> Parallel Computing, </journal> <volume> 11 </volume> <pages> 17-35, </pages> <year> 1989. </year>
Reference-contexts: That is, to identify a class of unconstrained optimization algorithms for which its asynchronous parallel version will converge more rapidly than its synchronous counterpart. Synergy in parallel iterative algorithms A property of iterative algorithms called synergy was introduced by Henderson and Miranker <ref> [HM89] </ref> . When synergism is used, both parallel or serial iterative algorithms will run faster than normal. Henderson and Mirkanker showed that synergy is a useful concept in the design of parallel algorithms.
Reference: [Hua70] <author> H.Y. Huang. </author> <title> Unified approach to quadratically convergent algorithm for function minimization. </title> <journal> Journal of Optimization Theory and Application, </journal> <volume> 5 </volume> <pages> 405-423, </pages> <year> 1970. </year>
Reference-contexts: A further reduction in computing time can be achieved by parallelizing the linear search itself. 3.2 van Laarhoven's methods Van Laarhoven [vL85] classified Straeter's ideas for parallel unconstrained optimization and applied the same formulation technique to the Huang's class <ref> [Hua70] </ref> of update formulas. Huang's class update formulas contributes to the most important class of quasi-Newton 14 methods such as DFP and BFGS methods.
Reference: [JDM77] <author> Jr J.E. Dennis and J.J. </author> <title> More. Quasi-newton methods : Motivation and theory. </title> <journal> SIAM Preview, </journal> <volume> 19(1) </volume> <pages> 46-89, </pages> <year> 1977. </year>
Reference: [JDS83] <author> Jr J.E. Dennis and R.B. Schnabel. </author> <title> Numerical methods for unconstrained opti-miziation and nonlinear equations. </title> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference-contexts: Practical applications of this problem include mathematical programming [FM68, Wri92], machine learning [Rea86, Man92], nonlinear regression <ref> [JDS83] </ref>, and engineering design [FGK73]. The mathematical programming problem of optimizing a nonlinear function subject to nonlinear constraints also has many practical applications. Fiacco and McCormick [FM68] show how this problem can be solved by barrier methods which involve the solution of a sequence of unconstrained optimization problems. <p> The nonlinear regression problem is important to fields such as physics, chemistry, and medicine, etc. Dennis and Schnabel <ref> [JDS83] </ref> describe an unconstrained optimization problem arising in physics that involves fitting a bell shaped curve to twenty pieces of solar spectroscopy data. In Fratta et al. [FGK73], a nonlinear, unconstrained, multicommodity flow problem is described. This problem has applications to the design of computer networks. <p> This problem has applications to the design of computer networks. Other interesting problems such as the problem of solving nonlinear equations and the integer programming problem can also be reduced to unconstrained optimization problems (cf. Dennis and Schnabel <ref> [JDS83] </ref>). 2 1.2 The unconstrained optimization problem In order to present numerical techniques for solving the unconstrained optimization problem, we write it in the following standard form : find a point x fl in the space R n such that f (x fl ) f (x) for all x 2 R
Reference: [Kun76] <author> H.T. Kung. </author> <title> Synchronized and asynchronous parallel algorithm for multiprocessors. Algorithms and Complexity : New directions and Recent Results (J.F. Traub, </title> <publisher> ed.), </publisher> <pages> pages 153-200, </pages> <year> 1976. </year>
Reference-contexts: Therefore, the discussion in the remainder of this paper is mainly oriented 5 towards parallel computation on multiprocessors in which the specific type of architecture is relatively unimportant. Moreover, two types of parallel algorithms for multiprocessors <ref> [Kun76, BT89] </ref>, i.e. synchronous and asynchronous algorithms, will be discussed. The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information.
Reference: [Loo89] <author> F.A. Lootsma. </author> <title> An asynchronous newton-raphson method. </title> <booktitle> NATO ASI Series : Proceedings of the NATO Advanced Research Workshop on Supercomputing, </booktitle> <address> F62:367-376, </address> <year> 1989. </year>
Reference-contexts: This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91], and the second derivative parallel methods <ref> [Pie73, FR88, CM93, Loo89, Loo91] </ref>. Besides, we also discuss a general framework, namely parallel gradient distribution [Man93], that was recently proposed. 2 Non-derivative parallel methods In this section, we outline the parallel methods which are based on the comparison of values of the objective function only. <p> Numerical results indicate that the proposed asynchronous algorithm guarantees to converge to the solution of a wide class of objective functions. Also, the asynchronous Newton algorithm is preferable for medium and large scale problems. 27 4.3 Lootsma's methods Lootsma <ref> [Loo89, Loo91] </ref> has proposed two parallel variants of Newton's method for unconstrained optimization. One variant is Newton's method with asynchronous updates of the Hessian itself. The other variant is Newton's method with asynchronous updates of the inverse Hessian.
Reference: [Loo91] <author> F.A. Lootsma. </author> <title> Parallel newton-raphson methods for unconstrained minimization with asynchronous updates of the hessian matrix or its inverse. </title> <journal> Parallel Computing and Mathematical Optimization, </journal> <pages> pages 1-18, </pages> <year> 1991. </year>
Reference-contexts: This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91], and the second derivative parallel methods <ref> [Pie73, FR88, CM93, Loo89, Loo91] </ref>. Besides, we also discuss a general framework, namely parallel gradient distribution [Man93], that was recently proposed. 2 Non-derivative parallel methods In this section, we outline the parallel methods which are based on the comparison of values of the objective function only. <p> A further improvement in Pierre's method is to employ the deferred updating strategy proposed by Mukai [Muk79, Muk81]. This strategy is to keep the approximation to the Hessian matrix unchanged during a number of iterations. However Lootsma <ref> [Loo91] </ref> claims that the advantages of deferred updating in, at least in the sequential form of Newton's method for unconstrained optimization are doubtful. The benefits of deferred updating on parallel computers are still unexplored. Also, Lootsma suggests a parallel line search technique that can improve Pierre's method. <p> Numerical results indicate that the proposed asynchronous algorithm guarantees to converge to the solution of a wide class of objective functions. Also, the asynchronous Newton algorithm is preferable for medium and large scale problems. 27 4.3 Lootsma's methods Lootsma <ref> [Loo89, Loo91] </ref> has proposed two parallel variants of Newton's method for unconstrained optimization. One variant is Newton's method with asynchronous updates of the Hessian itself. The other variant is Newton's method with asynchronous updates of the inverse Hessian.
Reference: [Man92] <author> O.L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <type> Technical Report CSTR# 1129, </type> <institution> University of Wisconsin-Madison, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Practical applications of this problem include mathematical programming [FM68, Wri92], machine learning <ref> [Rea86, Man92] </ref>, nonlinear regression [JDS83], and engineering design [FGK73]. The mathematical programming problem of optimizing a nonlinear function subject to nonlinear constraints also has many practical applications. <p> The recent developments related to this subject can be found in the survey by Wright [Wri92]. The machine learning problem can be handled by means of neural networks. A neural network consists of neuron-like units with threshold values and those units are connected by weight arcs. Mangasarian <ref> [Man92] </ref> casts the problem of determining the weights and thresholds of a feed-forward neural network with a single hidden layer as an unconstrained optimization problem and relates this problem to the standard backpropagation algorithm [Rea86] for training such a neural network.
Reference: [Man93] <author> O. L. Mangasarian. </author> <title> Parallel gradient distribution in unconstrained optimization. </title> <type> Technical Report Technical Report #1145, </type> <institution> Department of Computer Sciences, University of Wisconsin-Madison, WI, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91], and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. Besides, we also discuss a general framework, namely parallel gradient distribution <ref> [Man93] </ref>, that was recently proposed. 2 Non-derivative parallel methods In this section, we outline the parallel methods which are based on the comparison of values of the objective function only. Hence the methods make no use of any of the derivatives of the function. <p> Finally, there is no actual parallel implementation for these two approaches and the correctness of these algorithms rely highly upon the Fischer-Ritter method and Byrd-Schnabel-Schultz method. 5 Parallel gradient distribution Recently, Mangasarian <ref> [Man93] </ref> has proposed a general framework, called parallel gradient distribution, in conjunction with the design of parallel unconstrained optimization methods. This general framework enables each of the processors to activate concurrently and independently of the others a serial unconstrained optimization algorithm.
Reference: [Muk79] <author> H. Mukai. </author> <title> Parallel algorithms for unconstrained optimization. </title> <booktitle> Proceeding of 18th IEEE Conference on Decision and Control, </booktitle> <volume> Vol. 1 </volume> <pages> 451-454, </pages> <year> 1979. </year>
Reference-contexts: Similarly, we can construct a parallel Newton method for the case of a limited number of processors available. A further improvement in Pierre's method is to employ the deferred updating strategy proposed by Mukai <ref> [Muk79, Muk81] </ref>. This strategy is to keep the approximation to the Hessian matrix unchanged during a number of iterations. However Lootsma [Loo91] claims that the advantages of deferred updating in, at least in the sequential form of Newton's method for unconstrained optimization are doubtful.
Reference: [Muk81] <author> H. Mukai. </author> <title> Parallel algorithms for solving systems of nonlinear equations. </title> <journal> Computer, Mathematics with Applications, </journal> <volume> 7 </volume> <pages> 235-250, </pages> <year> 1981. </year> <month> 38 </month>
Reference-contexts: Similarly, we can construct a parallel Newton method for the case of a limited number of processors available. A further improvement in Pierre's method is to employ the deferred updating strategy proposed by Mukai <ref> [Muk79, Muk81] </ref>. This strategy is to keep the approximation to the Hessian matrix unchanged during a number of iterations. However Lootsma [Loo91] claims that the advantages of deferred updating in, at least in the sequential form of Newton's method for unconstrained optimization are doubtful.
Reference: [NM65] <author> J.A. Nelder and R. Mead. </author> <title> A simplex method for function minimization. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 308-313, </pages> <year> 1965. </year>
Reference-contexts: These methods are based on the direct search technique, namely the simplex method, proposed by Nelder and Mead <ref> [NM65] </ref> in the mid-sixties. A convergence theorem has been proved to support their approach. A very special feature of their approach is the ease with which algorithms can be developed to use any number of processors and to adapt to any cost ratio of communication over function evaluation.
Reference: [Pie73] <author> D. Pierre. </author> <title> A nongradient minimization algorithm having parallel structure, with implementation for array processor. </title> <journal> Comput. Elect. Engrg., </journal> <volume> 1 </volume> <pages> 3-21, </pages> <year> 1973. </year>
Reference-contexts: This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91], and the second derivative parallel methods <ref> [Pie73, FR88, CM93, Loo89, Loo91] </ref>. Besides, we also discuss a general framework, namely parallel gradient distribution [Man93], that was recently proposed. 2 Non-derivative parallel methods In this section, we outline the parallel methods which are based on the comparison of values of the objective function only. <p> Unconstrained optimization methods with this property are usually fast and accurate, at least when they operate in the vicinity of a local minimum. 4.1 Pierre's method The suggestion to employ the parallel processors for numerical differentiation is due to Pierre <ref> [Pie73] </ref>. This results in a parallel Newton method for solving unconstrained optimization problems. Providing the analytic gradient and Hessian of the objective function is a major problem in the actual implementation of the Newton method.
Reference: [Pow64] <author> M.J.D. Powell. </author> <title> An efficient method for finding the minimum of a function of several variables without calculating derivatives. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 155-162, </pages> <year> 1964. </year>
Reference-contexts: Two techniques are used to construct these methods and they are direct search and conjugate direction. We will discuss both parallel methods and their serial counterparts. 2.1 Chazan-Miranker method The Chazan-Miranker method [CM70] was motivated by the conjugate direction methods of Powell <ref> [Pow64] </ref> and of Zangwill [Zan67] which require no costly computation of gradient and Hessian of a given objective function. The methods of Powell and of Zangwill, as well as most other minimization methods, proceed by means of a sequence of univariate minimizations.
Reference: [Ral81] <author> L.B. Rall. </author> <title> Automatic differentiation : Techniques and applications. </title> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: The effectiveness of this approach depends on how readily a parallel routine for computing the objective function and its derivatives is available and how fully it parallelizes the computation. One of the possible ways to parallelize the function and its derivatives evaluations is by means of automatic differentiation <ref> [Ral81] </ref>. The parallel automatic differentiation was investigated by Dixon [Dix88] and Fischer [Fis90]. Parallel automatic differentiation can be considered in conjunction with the design and analysis of parallel numerical algorithms for unconstrained optimization problems.
Reference: [Rea86] <editor> D.E. Rumelhart and et al. </editor> <booktitle> Parallel distributed processing : Volumn 1. </booktitle> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Practical applications of this problem include mathematical programming [FM68, Wri92], machine learning <ref> [Rea86, Man92] </ref>, nonlinear regression [JDS83], and engineering design [FGK73]. The mathematical programming problem of optimizing a nonlinear function subject to nonlinear constraints also has many practical applications. <p> Mangasarian [Man92] casts the problem of determining the weights and thresholds of a feed-forward neural network with a single hidden layer as an unconstrained optimization problem and relates this problem to the standard backpropagation algorithm <ref> [Rea86] </ref> for training such a neural network. The nonlinear regression problem is important to fields such as physics, chemistry, and medicine, etc. Dennis and Schnabel [JDS83] describe an unconstrained optimization problem arising in physics that involves fitting a bell shaped curve to twenty pieces of solar spectroscopy data.
Reference: [Rit93] <author> K. Ritter. </author> <note> Private communications in symposium on parallel optimization 3. </note> <month> July </month> <year> 1993. </year>
Reference-contexts: Steps are performed asynchronously in the sense of independent tasks defined in each basic iteration. Fischer and Ritter claim that the steps of the parallel algorithm never fail. More specifically, the Cholesky factor L is always non-singular. Ritter <ref> [Rit93] </ref> points out that this asynchronous algorithm is easy to be implemented in multiprocessor systems. There are two disadvan tages of this algorithm. One is that it might never get the actual Hessian even it is positive definite.
Reference: [Slo75] <author> F. Sloboda. </author> <title> Parallel method of conjugate directions for minimization. </title> <journal> Aplikace Matematiky, </journal> <volume> 20 </volume> <pages> 436-446, </pages> <year> 1975. </year>
Reference-contexts: Moreover, two types of parallel algorithms for multiprocessors [Kun76, BT89], i.e. synchronous and asynchronous algorithms, will be discussed. The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods <ref> [CM70, Slo75, Sut83, DT91] </ref>, the first derivative parallel methods [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91], and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. <p> be more time consuming than the rest of n 1 minimizations. 2 Zangwill's algorithm is identical to Powell's except that each iterate of the minimizations is augmented by an additional minimization in a coordinate direction and the coordinate directions are chosen in cyclic order. 7 2.2 Sloboda's method Sloboda's method <ref> [Slo75, Slo77] </ref> is based on the projection method for solving linear algebraic systems. Sloboda showed that the projection method can be modified by a suitable choice of an argument to become a new method of conjugate directions for the minimization of strictly convex functions. <p> Hence we can construct an equivalent recurrence relation based on the suitable choice of k j1 defined by k j1 + d j1 ): The convergence results can be found in the reference <ref> [Slo75] </ref>. This is an early version of Sloboda's method for minimizing quadratic functions. This version was further extended to become a more powerful method to determine the minimizer of any continuously differentiable and strictly convex. The details of the latter modification will be given next.
Reference: [Slo77] <author> F. Sloboda. </author> <title> A conjugate direction method and its application. </title> <booktitle> Proceedings of the 8th IFIP conference on Optimization Techniques, </booktitle> <year> 1977. </year>
Reference-contexts: be more time consuming than the rest of n 1 minimizations. 2 Zangwill's algorithm is identical to Powell's except that each iterate of the minimizations is augmented by an additional minimization in a coordinate direction and the coordinate directions are chosen in cyclic order. 7 2.2 Sloboda's method Sloboda's method <ref> [Slo75, Slo77] </ref> is based on the projection method for solving linear algebraic systems. Sloboda showed that the projection method can be modified by a suitable choice of an argument to become a new method of conjugate directions for the minimization of strictly convex functions. <p> It has been proved that the sequence of iterates generated by this algorithm will converge to the unique minimizer of f (x) ( see <ref> [Slo77] </ref>). Hence this method is guaranteed to converge for twice continuously differentiable strictly convex functions and has finite termination on quadratic objective function.
Reference: [Sti90] <author> C.H. </author> <title> Still. Parallel quasi-newton methods for unconstrained optimization. </title> <booktitle> Proceedings of the fifth distributed memory computing conference, </booktitle> <volume> 1 </volume> <pages> 263-271, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods <ref> [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91] </ref>, and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. <p> The details of the convergence proofs and computational results can be found in the reference. 3.4 Still's methods The investigations by Byrd, Schnabel, and Schultz [BSS88a] into the development of a parallel secant method focus almost entirely on performing the linear algebra computations and function calculations in parallel. Still <ref> [Sti90, Sti91] </ref> claims that their formulations are only good for the use on vector computers. The inherent parallelism of the unconstrained optimization problem is left untapped. Hence, Still developed other effective parallel methods for the use on multiprocessor systems. Still presented two parallel formulations of the well-known quasi-Newton methods.
Reference: [Sti91] <author> C.H. </author> <title> Still. The parallel bfgs method for unconstrained optimization. </title> <booktitle> Proceedings of the sixth distributed memory computing conference, </booktitle> <pages> pages 347-354, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods <ref> [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91] </ref>, and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. <p> The details of the convergence proofs and computational results can be found in the reference. 3.4 Still's methods The investigations by Byrd, Schnabel, and Schultz [BSS88a] into the development of a parallel secant method focus almost entirely on performing the linear algebra computations and function calculations in parallel. Still <ref> [Sti90, Sti91] </ref> claims that their formulations are only good for the use on vector computers. The inherent parallelism of the unconstrained optimization problem is left untapped. Hence, Still developed other effective parallel methods for the use on multiprocessor systems. Still presented two parallel formulations of the well-known quasi-Newton methods.
Reference: [Str73] <author> T. Straeter. </author> <title> A parallel variable metric optimization algorithm. </title> <type> Technical Report D-7329, </type> <institution> NASA Langley Research Center, Hampton, VA, </institution> <year> 1973. </year>
Reference-contexts: The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods <ref> [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91] </ref>, and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. <p> There are many possible choices of the correction matrix D j (cf.[Dav59, FP63, Hua70, Bro70]). 3.1 Straeter's method An attempt to introduce parallelism into quasi-Newton methods is suggested by Straeter <ref> [Str73] </ref>. Straeter's method was motivated by the possibility of modification suitable for parallel computation of the symmetric rank one update formula for approximating the inverse Hessian. The basic idea of Straeter's method is to modify the inverse Hessian by a number of symmetric rank one corrections along independent directions. <p> These parallel methods use as many finite differences of gradients as possible to update the Hessian matrix or its inverse. The motivation of these methods is based on the Gauss-Seidel type of updating for quasi-Newton methods originally proposed by Straeter <ref> [Str73] </ref> and the incorporation of the finite difference approximations via the symmetric rank one or two updates analyzed by van Laarhoven [vL85]. Lootsma assumes that, in practical problems, the function and gradient evaluations are much more expensive than the overhead of the algorithm such as the linear algebra costs.
Reference: [Sut75] <author> C. Sutti. </author> <title> A new method for unconstrained minimization without derivatives. </title> <booktitle> Towards global optimization, </booktitle> <pages> pages 277-289, </pages> <year> 1975. </year>
Reference-contexts: In this case, if cooperating processes are synchronous, the sequence fx j j j j 1g coincides with the sequence built according to the sequential Sloboda's method. All properties are still preserved. 2.3 Sutti's method In the mid-seventies, Sutti proposed a method <ref> [Sut75] </ref> for minimizing continuously differentiable strictly convex functions without using derivative information. The sequential form of Sutti method belongs to the class of conjugate direction methods.
Reference: [Sut77] <author> C. Sutti. </author> <title> Convergence proof of minimization algorithms for nonconvex functions. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 23(2), </volume> <year> 1977. </year>
Reference-contexts: The sequential Sutti's method has been proved to have the properties of finite termination for a quadratic objective function and convergence for continuously differentiable strictly convex function (cf. [Sut78]). Under additional conditions <ref> [Sut77] </ref>, this method also converges for a non-convex function. Now let us consider the parallelism in the sequential Sutti's method.
Reference: [Sut78] <author> C. Sutti. </author> <title> Numerical experiences on a minimization algorithm without derivatives. </title> <booktitle> Towards global optimization 2, </booktitle> <pages> pages 255-267, </pages> <year> 1978. </year>
Reference-contexts: The sequential Sutti's method has been proved to have the properties of finite termination for a quadratic objective function and convergence for continuously differentiable strictly convex function (cf. <ref> [Sut78] </ref>). Under additional conditions [Sut77], this method also converges for a non-convex function. Now let us consider the parallelism in the sequential Sutti's method.
Reference: [Sut83] <author> C. Sutti. </author> <title> Nongradient minimization methods for parallel processing computers. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 39 </volume> <pages> 465-488, </pages> <year> 1983. </year>
Reference-contexts: Moreover, two types of parallel algorithms for multiprocessors [Kun76, BT89], i.e. synchronous and asynchronous algorithms, will be discussed. The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods <ref> [CM70, Slo75, Sut83, DT91] </ref>, the first derivative parallel methods [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91], and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. <p> Comparing this method with other non-derivative methods, it is computationally efficient in the sense that it does not always require linear search along all conjugate directions. This reduces the number of function evaluations required to locate the minimizer. Also, this method was found to be suitable for parallel computation <ref> [Sut83] </ref>. Suppose x 1;1 is the initial approximation to the minimizer of the objective function f (x). Let a set of direction vectors be fs k 1;1 j k = 1; ; ng linearly independent and normalized. These can be chosen as the n elementary vectors in R n .
Reference: [Tor90] <author> V. Torczon. </author> <title> Multi-directional search : a direct search algorithm for parallel machines. </title> <type> Technical Report 90-7, </type> <institution> Department of Mathematical Sciences, Rice university, Houston, TX, </institution> <year> 1990. </year> <month> 39 </month>
Reference-contexts: Therefore, unsuccessful sequences of iterations will generate a so-called backtracking line search with alternating orientations. This backtracking forms an important part of the convergence proof of Dennis and Torczon methods found in <ref> [Tor90, Tor91] </ref>. 11 We note that as long as the backtracking line search flavor of the basic method is preserved, the convergence theorem can be extended to the convergence proof of a parallel multidirec-tional search method which follows the strategy to simply look ahead to subsequent iterates of the basic algorithm
Reference: [Tor91] <author> V. Torczon. </author> <title> On the convergence of the multidirectional search algorithm. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(1) </volume> <pages> 123-145, </pages> <month> 2 </month> <year> 1991. </year>
Reference-contexts: Therefore, unsuccessful sequences of iterations will generate a so-called backtracking line search with alternating orientations. This backtracking forms an important part of the convergence proof of Dennis and Torczon methods found in <ref> [Tor90, Tor91] </ref>. 11 We note that as long as the backtracking line search flavor of the basic method is preserved, the convergence theorem can be extended to the convergence proof of a parallel multidirec-tional search method which follows the strategy to simply look ahead to subsequent iterates of the basic algorithm
Reference: [TZ89] <author> A. Torn and A. Zilinskas. </author> <title> Global optimization. </title> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Due to the expensive nature of global optimization and the practical need to solve such problems, there is ample incentive to develop parallel global optimization methods. The references <ref> [ES89, TZ89, BDKS90] </ref> give the recent advances in this research area. Parallel automatic differentiation If the leading expense of solving unconstrained optimization problems is the evaluation of the objective function and its derivatives, one possibility to improve the performance is simply to parallelize each of these computations.
Reference: [vL85] <author> P.J.M. van Laarhoven. </author> <title> Parallel variable metric methods for unconstrained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 33 </volume> <pages> 68-81, </pages> <year> 1985. </year>
Reference-contexts: The parallel unconstrained optimization algorithms discussed below are classified into three categories. This classification is based on the use of the derivatives information. They are referred as the non-derivative parallel methods [CM70, Slo75, Sut83, DT91], the first derivative parallel methods <ref> [Str73, vL85, BSS88a, BSS88b, Sti90, Sti91] </ref>, and the second derivative parallel methods [Pie73, FR88, CM93, Loo89, Loo91]. <p> This is extremely important since linear searches take a large amount of function evaluations. A further reduction in computing time can be achieved by parallelizing the linear search itself. 3.2 van Laarhoven's methods Van Laarhoven <ref> [vL85] </ref> classified Straeter's ideas for parallel unconstrained optimization and applied the same formulation technique to the Huang's class [Hua70] of update formulas. Huang's class update formulas contributes to the most important class of quasi-Newton 14 methods such as DFP and BFGS methods. <p> The motivation of these methods is based on the Gauss-Seidel type of updating for quasi-Newton methods originally proposed by Straeter [Str73] and the incorporation of the finite difference approximations via the symmetric rank one or two updates analyzed by van Laarhoven <ref> [vL85] </ref>. Lootsma assumes that, in practical problems, the function and gradient evaluations are much more expensive than the overhead of the algorithm such as the linear algebra costs.
Reference: [Wri92] <author> M.H. Wright. </author> <title> Interior methods for constrained optimization. </title> <booktitle> Acta Numerica 1992, </booktitle> <pages> pages 341-407, </pages> <year> 1992. </year>
Reference-contexts: Practical applications of this problem include mathematical programming <ref> [FM68, Wri92] </ref>, machine learning [Rea86, Man92], nonlinear regression [JDS83], and engineering design [FGK73]. The mathematical programming problem of optimizing a nonlinear function subject to nonlinear constraints also has many practical applications. <p> A formal connection can be shown between his method and classical barrier methods which have consequently undergone a renaissance in interest and popularity. The recent developments related to this subject can be found in the survey by Wright <ref> [Wri92] </ref>. The machine learning problem can be handled by means of neural networks. A neural network consists of neuron-like units with threshold values and those units are connected by weight arcs.
Reference: [Zan67] <author> W.I. Zangwill. </author> <title> Minimizing a function without calculating derivatives. </title> <journal> Computer journal, </journal> <volume> 10 </volume> <pages> 293-296, </pages> <year> 1967. </year> <month> 40 </month>
Reference-contexts: Two techniques are used to construct these methods and they are direct search and conjugate direction. We will discuss both parallel methods and their serial counterparts. 2.1 Chazan-Miranker method The Chazan-Miranker method [CM70] was motivated by the conjugate direction methods of Powell [Pow64] and of Zangwill <ref> [Zan67] </ref> which require no costly computation of gradient and Hessian of a given objective function. The methods of Powell and of Zangwill, as well as most other minimization methods, proceed by means of a sequence of univariate minimizations. We now give a brief description of these methods. <p> We note that if s n j turns out to be zero, s 1 j would eventually become zero and the algorithm would simply imply that x j+1 = x j . The convergence proof of this algorithm can be found in the references <ref> [Zan67, CM70] </ref>. It is seen that the Chazan-Miranker method uses features found in Powell's method as well as a feature resembling the modification 2 introduced by Zangwill. However, it is quite different from either of these methods and has as its principal objective to increase the degree of parallelism.
References-found: 51

