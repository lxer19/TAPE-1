URL: http://euler.mcs.utulsa.edu/~sandip/jetai.ps
Refering-URL: http://euler.mcs.utulsa.edu/~sandip/GAGP.html
Root-URL: 
Email: e-mail:sandip@kolkata.mcs.utulsa.edu  
Phone: Phone: +918-631-2985  
Title: Individual learning of coordination knowledge  
Author: Sandip Sen Mahendra Sekaran 
Address: 600 South College Avenue Tulsa, OK 74104-3189  
Affiliation: Department of Mathematical Computer Sciences University of Tulsa  
Abstract-found: 0
Intro-found: 1
Reference: <author> Barto, A. B., Sutton, R. S., and Watkins, C. </author> <year> (1989). </year> <title> Sequential decision problems and neural networks. </title> <booktitle> In Proceedings of 1989 Conference on Neural Information Processing. </booktitle>
Reference: <author> Bond, A. H. and Gasser, L. </author> <year> (1988). </year> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Booker, L., Goldberg, D., and Holland, J. </author> <year> (1989). </year> <title> Classifier systems and genetic algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 235-282. </pages>
Reference: <author> Booker, L. B. </author> <year> (1988). </year> <title> Classifier systems that learn internal world models. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 161-192. </pages>
Reference-contexts: Other planned experiments include using world models within classifier systems <ref> (Booker, 1988) </ref> and combining features of BBA and PSP (Grefenstette, 1988) that would be useful for learning multiagent coordination strategies. Acknowledgments This research has been sponsored, in part, by the National Science Foundation under a Research Initiation Award IRI-9410180 and a CAREER award IRI-9702672.
Reference: <author> Brazdil, P., Gams, M., Sian, S., Torgo, L., and van de Velde, W. </author> <year> (1991). </year> <booktitle> Learning in distributed systems and multi-agent environments. In European Working Session on Learning, Lecture Notes in AI, 482, </booktitle> <address> Berlin. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: At this point, we would like to emphasize the difference between this work and other recent publications in the nascent area of multiagent learning (MAL) research. Previous proposals for using learning techniques to coordinate multiple agents have mostly relied on using prior knowledge <ref> (Brazdil et al., 1991) </ref>, or on cooperative domains with unrestricted information sharing (Sian, 1991). A significant percentage of this research have concentrated on cooperative learning between communicating agents where agents share their knowledge or experiences (Garland and Alterman, 1996; Provost and Hennessy, 1996; Prasad et al., 1996; Tan, 1993).
Reference: <author> Bui, H. H., Kieronska, D., and Venkatesh, S. </author> <year> (1996). </year> <title> Negotiating agents that learn about others' preferences. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 114-119, </pages> <address> Menlo Park, CA. </address> <publisher> AAAI Press. 31 Carmel, </publisher> <editor> D. and Markovitch, S. </editor> <year> (1996). </year> <title> Incorporating opponent models into adversary search. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 62-67, </pages> <address> Menlo Park, CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Cohen, P. R. and Perrault, C. R. </author> <year> (1979). </year> <title> Elements of a plan-based theory of speech acts. </title> <journal> Cognitive Science, </journal> <volume> 3(3) </volume> <pages> 177-212. </pages>
Reference-contexts: Almost all of the coordination schemes developed to date assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results (Durfee and Lesser, 1991), speech acts <ref> (Cohen and Perrault, 1979) </ref>, resource availabilities (Smith, 1980), etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents (Fox, 1981; Genesereth et al., 1986; Zlotkin and Rosenschein, 1990) to aid local decision-making.
Reference: <author> Conry, S. E., Meyer, R. A., and Lesser, V. R. </author> <year> (1988). </year> <title> Multistage negotiation in distributed planning. </title> <editor> In Bond, A. H. and Gasser, L., editors, </editor> <booktitle> Readings in Distributed Artificial Intelligence, </booktitle> <pages> pages 367-384. </pages> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Dorigo, M. and Bersini, H. </author> <year> (1994). </year> <title> A comparison of Q-learning and classifier systems. </title> <booktitle> In Proceedings of From Animals to Animats, Third International Conference on Simulation of Adaptive Behavior. </booktitle>
Reference-contexts: The similarities of Q-learning and classifier systems have been analyzed in <ref> (Dorigo and Bersini, 1994) </ref>. Classifier systems are rule based systems that learn by adjusting rule strengths from feedback and by discovering better rules using genetic algorithms. <p> In this paper, we will use simplified classifier systems where all possible message action pairs are explicitly stored and classifiers have one condition and one action. These assumptions are similar to those made by Dorigo and Bersini <ref> (Dorigo and Bersini, 1994) </ref>; we also use their notation to describe a classifier i by (c i ; a i ), where c i and a i are respectively the condition and action parts of the classifier.
Reference: <author> Durfee, E. H. and Lesser, V. R. </author> <year> (1991). </year> <title> Partial global planning: A coordination framework for distributed hypothesis formation. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(5). </volume>
Reference-contexts: Almost all of the coordination schemes developed to date assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results <ref> (Durfee and Lesser, 1991) </ref>, speech acts (Cohen and Perrault, 1979), resource availabilities (Smith, 1980), etc. to other agents to facilitate the process of coordination.
Reference: <author> Durfee, E. H., Lesser, V. R., and Corkill, D. D. </author> <year> (1987). </year> <title> Coherent cooperation among communicating problem solvers. </title> <journal> IEEE Transactions on Computers, C-36(11):1275-1291. </journal>
Reference-contexts: Though communication is often helpful and indispensable as an aid to group activity, it does not guarantee coordinated behavior (Halpern and Moses, 1990), is time-consuming and can detract from other problem solving activity if not carefully controlled <ref> (Durfee et al., 1987) </ref>. Also, agents overly reliant on communication will be severely affected if the quality of communication is compromised (broken communication channels, incorrect or deliberately misleading information, etc.).
Reference: <author> Durfee, E. H., Lesser, V. R., and Corkill, D. D. </author> <year> (1989). </year> <title> Trends in cooperative distributed problem solving. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(1) </volume> <pages> 63-83. </pages>
Reference-contexts: Multiagent systems are a particular type of distributed AI system (Bond and Gasser, 1988; Lesser, 1995), in which autonomous intelligent agents inhabit a world with no global control or globally consistent knowledge. In contrast to cooperative problem solvers <ref> (Durfee et al., 1989) </ref>, agents in multiagent systems are not pre-disposed to help each other out with all the resources and capabilities that they possess. These agents may still need to coordinate their activities with others to achieve their own local goals.
Reference: <author> Durfee, E. H. and Montgomery, T. A. </author> <year> (1991). </year> <title> Coordination as distributed search in a hierarchical behavior space. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(6) </volume> <pages> 1363-1378. </pages>
Reference-contexts: Almost all of the coordination schemes developed to date assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results <ref> (Durfee and Lesser, 1991) </ref>, speech acts (Cohen and Perrault, 1979), resource availabilities (Smith, 1980), etc. to other agents to facilitate the process of coordination.
Reference: <author> Fox, M. S. </author> <year> (1981). </year> <title> An organizational view of distributed systems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 11(1) </volume> <pages> 70-80. </pages>
Reference: <author> Garland, A. and Alterman, R. </author> <year> (1996). </year> <title> Multiagent learning through collective memory. </title> <editor> In Sen, S., editor, </editor> <booktitle> Working Notes for the AAAI Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems, </booktitle> <pages> pages 33-38, </pages> <address> Stanford University, CA. </address> <note> 32 Gasser, </note> <editor> L. and Huhns, M. N., editors (1989). </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence. </booktitle> <publisher> Pitman. </publisher>
Reference: <author> Genesereth, M., Ginsberg, M., and Rosenschein, J. </author> <year> (1986). </year> <title> Cooperation without communications. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 51-57, </pages> <address> Philadelphia, Pennsylvania. </address>
Reference: <author> Gmytrasiewicz, P. J., Durfee, E. H., and Wehe, D. K. </author> <year> (1991). </year> <title> A decision-theoretic approach to coordinating multiagent interactions. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 62-68. </pages>
Reference: <author> Grefenstette, J. </author> <year> (1988). </year> <title> Credit assignment in rule discovery systems. </title> <journal> Machine Learning, </journal> 3(2/3):225-246. 
Reference-contexts: gives the strength update of classifier i: S t+1 (c i ; a i ) = (1 ff) fl S t (c i ; a i ) + ff fl (R + S t+1 (c j ; a j )): We now describe the profit sharing plan (PSP) strength-updating scheme <ref> (Grefenstette, 1988) </ref> used in classifier systems. In this method, problem solving is divided into episodes in between receipts of external reward. A rule is said to be active in a period if it fired in at least one of the cycles in that episode. <p> Other planned experiments include using world models within classifier systems (Booker, 1988) and combining features of BBA and PSP <ref> (Grefenstette, 1988) </ref> that would be useful for learning multiagent coordination strategies. Acknowledgments This research has been sponsored, in part, by the National Science Foundation under a Research Initiation Award IRI-9410180 and a CAREER award IRI-9702672.
Reference: <author> Gu, P. and Maddox, A. B. </author> <year> (1996). </year> <title> A framework for distributed reinforcement learning. </title> <editor> In Wei, G. and Sen, S., editors, </editor> <booktitle> Adaptation and Learning in Multi-Agent Systems, Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 97-112. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Halpern, J. and Moses, Y. </author> <year> (1990). </year> <title> Knowledge and common knowledge in a distributed environment. </title> <journal> Journal of the ACM, </journal> <volume> 37(3) </volume> <pages> 549-587. </pages> <note> A preliminary version appeared in Proc. 3rd ACM Symposium on Principles of Distributed Computing, </note> <year> 1984. </year>
Reference-contexts: At times communication is the most effective and even perhaps the only mechanism to guarantee coordinated behavior. Though communication is often helpful and indispensable as an aid to group activity, it does not guarantee coordinated behavior <ref> (Halpern and Moses, 1990) </ref>, is time-consuming and can detract from other problem solving activity if not carefully controlled (Durfee et al., 1987). Also, agents overly reliant on communication will be severely affected if the quality of communication is compromised (broken communication channels, incorrect or deliberately misleading information, etc.).
Reference: <author> Haynes, T. and Sen, S. </author> <title> Learning cases to compliment rules for conflict resolution in multia-gent systems. </title> <note> International Journal of Human-Computer Studies (to appear). </note>
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adpatation in natural and artificial systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference-contexts: This is a quintessential example of the exploration-exploitation tradeoff <ref> (Holland, 1975) </ref>. Also, this is more likely to happen when the same action can generate more number of distinct feedbacks (the same action for the stronger agent can produce more distinct feedbacks when the discretizations for weaker agent is increased).
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: the possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In Michalski, R., Carbonell, J., and Mitchell, T. M., editors, </editor> <booktitle> Machine Learning, an artificial intelligence approach: Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Alamos, CA. </address>
Reference-contexts: of R and a transition to state s 0 , then the corresponding Q value is modified as follows: Q (s; a) (1 fi) Q (s; a) + fi (R + fl max Q (s 0 ; a 0 )): (1) The above update rule is similar to Holland's bucket-brigade <ref> (Holland, 1986) </ref> algorithm in classifier systems and Sutton's temporal-difference (Sutton, 1984) learning scheme. The similarities of Q-learning and classifier systems have been analyzed in (Dorigo and Bersini, 1994). Classifier systems are rule based systems that learn by adjusting rule strengths from feedback and by discovering better rules using genetic algorithms.
Reference: <author> Huhns, M., </author> <title> editor (1987). </title> <journal> Distributed Artificial Intelligence. </journal> <note> Morgan Kaufmann. 33 Kaelbling, </note> <author> L., Littman, M. L., and Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of AI Research, </journal> <volume> 4 </volume> <pages> 237-285. </pages>
Reference: <author> Kirkpatrick, S., Gelatt, C., and Vecchi, M. </author> <year> (1983). </year> <title> Optimization by simulated reannealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680. </pages>
Reference-contexts: Each of these options, however, results in an exponential increase of the trials to convergence. Currently we are developing a simulated annealing <ref> (Kirkpatrick et al., 1983) </ref> based procedure which results in a decrease in the proportion of random choices as the policy matrix converges to its steady state. 6 Robot navigation problem We designed a problem in which four agents, A, B, C, and D, are to find the optimal path in a
Reference: <author> Lesser, V. R. </author> <year> (1995). </year> <title> Multiagent systems: An emerging subdiscipline of AI. </title> <journal> ACM Computing Surveys, </journal> <volume> 27(3) </volume> <pages> 340-342. </pages>
Reference: <author> Littman, M. L. </author> <year> (1994). </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163. </pages>
Reference: <author> Mahadevan, S. </author> <year> (1993). </year> <title> To discount or not to discount in reinforcement learning: A case study comparing R learning and Q learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 205-211. </pages>
Reference-contexts: In the other method of action choice, the classifier with the highest fitness in M is chosen 90% of the time, and a random classifier from M is chosen in the rest 10% cases (Mahadevan uses such an action choosing mechanism for Q-learning in <ref> (Mahadevan, 1993) </ref>). We call this a semi-random PSP or PSP (SR). For the Q-learning algorithm, we stop a run when the algebraic difference of the policies at the end of neighboring trials is below a threshold for 10 consecutive trials.
Reference: <author> Malone, T. W. </author> <year> (1987). </year> <title> Modeling coordination in organizations and markets. </title> <journal> Management Science, </journal> <volume> 33(10) </volume> <pages> 1317-1332. </pages> <note> (Also published in Readings in Distributed Artificial Intelligence, </note> <editor> Alan H. Bond and Les Gasser, editors, </editor> <address> pages 151-158, </address> <publisher> Morgan Kaufmann, 1988.). </publisher>
Reference: <author> March, J. G. and Simon, H. A. </author> <year> (1958). </year> <title> Organizations. </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Mataric, M. J. </author> <year> (1996). </year> <title> Learning in multi-robot systems. </title> <editor> In Wei, G. and Sen, S., editors, </editor> <booktitle> Adaptation and Learning in Multi-Agent Systems, Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 152-163. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: A major difference from our work is that the simulated robots in this work build explicit models of other robots. Other researchers have used reinforcement learning for developing effective groups of physical robots (Mataric, 1996; Yanco and Stein, 1993). Mataric <ref> (Mataric, 1996) </ref> concentrates on using intermediate feedback for subgoal fulfillment to accelerate learning. In contrast with our work, the evaluation of learning effectiveness under varying degrees of interaction between the agents is not the focus of this work.
Reference: <author> Narendra, K. and Thatachar, M. </author> <year> (1989). </year> <title> Learning Automata: An Introduction. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: This is complementary to our approach of learning to coordinate in the absence of communication. Though our agents can be viewed as a learning automaton, the scalar feedback received from the environment prevents the use of results directly from the field of learning automata <ref> (Narendra and Thatachar, 1989) </ref>.
Reference: <author> Parker, L. E. </author> <year> (1992). </year> <title> Adaptive action selection for cooperative robot teams. </title> <editor> In Meyer, J., Roitblat, H., and Wilson, S., editors, </editor> <booktitle> Proc. of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 442-450, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher> <address> 34 Prasad, </address> <note> M. </note> <author> V. N., Lander, S. E., and Lesser, V. R. </author> <year> (1996). </year> <title> Cooperative learning over composite search spaces: Experiences with a multi-agent design system. </title> <booktitle> In Proceedings of Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 68-73. </pages>
Reference-contexts: Schaerf et al. have studied the use of reinforcement learning based agents for load balancing in distributed systems (Schaerf et al., 1995). In this work, a comprehensive history of past performance is used to make informed decisions about choice of resources to submit jobs to. Parker <ref> (Parker, 1992) </ref> has studied the emergence of coordination in simulated robot 9 groups by using simple adaptive schemes that alter robot motivations. A major difference from our work is that the simulated robots in this work build explicit models of other robots.
Reference: <author> Provost, F. J. and Hennessy, D. N. </author> <year> (1996). </year> <title> Scaling up: Distributed machine learning with cooperation. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 74-79, </pages> <address> Menlo Park, CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Rosenschein, J. S. </author> <year> (1982). </year> <title> Synchronization of multi-agent plans. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 115-119, </pages> <address> Pittsburgh, Pennsylvania. </address>
Reference: <author> Sandholm, T. W. and Crites, R. H. </author> <year> (1996). </year> <title> On multiagent Q-learning in a semi-competitive domain. </title> <editor> In Wei, G. and Sen, S., editors, </editor> <booktitle> Adaptation and Learning in Multi-Agent Systems, Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 191-205. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference: <author> A.Schaerf, Y.Shoham, </author> <title> and M.Tennenholtz (1995). Adaptive load balancing: A study in multiagent learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 475-500. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1996). </year> <title> A general method for multi-agent reinforcement learning in unrestricted environments. </title> <editor> In Sen, S., editor, </editor> <booktitle> Working Notes for the AAAI Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems, </booktitle> <pages> pages 84-87, </pages> <address> Stanford University, CA. </address>
Reference: <author> Sekaran, M. and Sen, S. </author> <year> (1994). </year> <title> Learning with friends and foes. </title> <booktitle> In Sixteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 800-805. </pages>
Reference-contexts: Researchers have investigated two approaches to learning without explicit communication in non-cooperative domains: (a) treating opponents as part of the environment without explicit modeling <ref> (Sekaran and Sen, 1994) </ref>, (b) learning explicit competitor models (Littman, 1994; Sandholm and Crites, 1996). Little work has been done to date in learning to compete with explicit communication.
Reference: <author> Sen, S. </author> <year> (1996). </year> <title> IJCAI-95 workshop on adaptation and learning in multiagent systems. </title> <journal> AI Magazine, </journal> <volume> 17(1) </volume> <pages> 87-89. </pages>
Reference: <author> Sen, S., Sekaran, M., and Hale, J. </author> <year> (1994). </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In National Conference on Artificial Intelligence, </booktitle> <pages> pages 426-431. </pages>
Reference-contexts: Others have looked at each agent learning individually but using communication to share pertinent information (Bui et al., 1996; Provost and Hennessy, 1996; Tan, 1993). Learning to cooperate without explicit information sharing can be based on primarily environmental feedback <ref> (Sen et al., 1994) </ref> or from observation of other agents in action (Haynes and Sen, ). <p> Researchers have investigated two approaches to learning without explicit communication in non-cooperative domains: (a) treating opponents as part of the environment without explicit modeling <ref> (Sekaran and Sen, 1994) </ref>, (b) learning explicit competitor models (Littman, 1994; Sandholm and Crites, 1996). Little work has been done to date in learning to compete with explicit communication.
Reference: <author> Shoham, Y. and Tennenholtz, M. </author> <year> (1992). </year> <title> On the synthesis of useful social laws for artificial agent societies (preliminary report). </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 276-281, </pages> <address> San Jose, California. 35 Sian, S. </address> <year> (1991). </year> <title> Adaptation based on cooperative learning in multi-agent systems. </title> <editor> In De--mazeau, Y. and Muller, J.-P., editors, </editor> <booktitle> Decentralized AI, </booktitle> <volume> volume 2, </volume> <pages> pages 257-272. </pages> <publisher> Else-vier Science Publications. </publisher>
Reference: <author> Smith, R. G. </author> <year> (1980). </year> <title> The contract net protocol: High-level communication and control in a distributed problem solver. </title> <journal> IEEE Transactions on Computers, C-29(12):1104-1113. </journal>
Reference-contexts: Almost all of the coordination schemes developed to date assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results (Durfee and Lesser, 1991), speech acts (Cohen and Perrault, 1979), resource availabilities <ref> (Smith, 1980) </ref>, etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents (Fox, 1981; Genesereth et al., 1986; Zlotkin and Rosenschein, 1990) to aid local decision-making.
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts at Amherst. </institution>
Reference-contexts: , then the corresponding Q value is modified as follows: Q (s; a) (1 fi) Q (s; a) + fi (R + fl max Q (s 0 ; a 0 )): (1) The above update rule is similar to Holland's bucket-brigade (Holland, 1986) algorithm in classifier systems and Sutton's temporal-difference <ref> (Sutton, 1984) </ref> learning scheme. The similarities of Q-learning and classifier systems have been analyzed in (Dorigo and Bersini, 1994). Classifier systems are rule based systems that learn by adjusting rule strengths from feedback and by discovering better rules using genetic algorithms.
Reference: <author> Tan, M. </author> <year> (1993). </year> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 330-337. </pages>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, Cambridge University. </institution>
Reference-contexts: To verify our intuitions more rigorously, we decided to investigate two well-known reinforcement learning schemes: the Q-learning algorithm developed by Watkins <ref> (Watkins, 1989) </ref>, and the classifier systems method developed by Holland. Whereas the Q-learning algorithm was inspired by the theory of dynamic programming for optimization, classifier systems arose from an interesting blend of rule-based reasoning and computational mech 4 anisms inspired by natural genetics (Booker et al., 1989; Holland, 1986). <p> For evaluating the classifier system paradigm for multiagent reinforcement learning, we compare it with the Q-learning <ref> (Watkins, 1989) </ref> algorithm, which is designed to find a policy fl that maximizes V fl (s) for all states s 2 S. The decision policy is represented by a function, Q : S fi A 7! &lt;, which estimates long-term discounted rewards for each state-action pair.
Reference: <author> Wei, G. </author> <year> (1993). </year> <title> Learning to coordinate actions in multi-agent systems. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 311-316. </pages>
Reference-contexts: Some researchers have used communication to aid agent groups jointly decide on their course of actions <ref> (Wei, 1993) </ref>. The isolated instances of research in MAL that do not use explicit communication have concentrated on competitive, rather than cooperative, domains (Carmel and Markovitch, 1996; Littman, 1994; Sandholm and Crites, 1996).
Reference: <author> Wei, G., </author> <title> editor (1997). </title> <booktitle> Distributed Artificial Intelligence Meets Machine Learning: Learning in Multi-Agent Environments. Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Wei, G. and Sen, S., </author> <title> editors (1996). Adaptation and Learning in Multi-Agent Systems. </title> <booktitle> Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Werner, E. </author> <year> (1989). </year> <title> Cooperating agents: A unified theory of communication and social structure. </title> <editor> In Gasser, L. and Huhns, M. N., editors, </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence, </booktitle> <pages> pages 3-36. </pages> <publisher> Pitman. </publisher>

Reference: <author> Yokoo, M., Durfee, E., Ishida, T., and Kuwabara, K. </author> <year> (1992). </year> <title> Distributed constraint satisfaction for formalizing distributed problem solving. </title> <booktitle> In Proceedings of the Twelfth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 614-621. </pages>
Reference: <author> Zlotkin, G. and Rosenschein, J. S. </author> <year> (1990). </year> <title> Negotiation and conflict resolution in noncooperative domains. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 100-105. 37 </pages>
References-found: 52

