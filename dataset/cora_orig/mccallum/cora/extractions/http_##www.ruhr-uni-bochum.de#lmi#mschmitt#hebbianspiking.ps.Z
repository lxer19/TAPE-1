URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/hebbianspiking.ps.Z
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Email: E-mail: fbruf, mschmittg@igi.tu-graz.ac.at  
Title: Hebbian Learning in Networks of Spiking Neurons Using Temporal Coding  
Author: Berthold Ruf and Michael Schmitt 
Address: Klosterwiesgasse 32/2, A-8010 Graz, Austria  
Affiliation: Institute for Theoretical Computer Science, Technische Universitat Graz  
Abstract: Computational tasks in biological systems that require short response times can be implemented in a straightforward way by networks of spiking neurons that encode analogue values in temporal coding. We investigate the question how spiking neurons can learn on the basis of differences between firing times. In particular, we provide learning rules of the Hebbian type in terms of single spiking events of the pre- and post-synaptic neuron and show that the weights approach some value given by the difference between pre- and postsynaptic firing times with arbitrary high precision. Our learning rules give rise to a straightforward possibility for realizing very fast pattern analysis tasks with spiking neurons. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Abeles, M., Bergman, H., Margalit, E., and Vaadia, E.: </author> <title> Spatiotemporal firing patterns in the frontal cortex of behaving monkeys. </title> <journal> J. </journal> <note> of Neurophysiology 70 (1993) 1629-1638. </note>
Reference-contexts: However, biological neural systems can perform certain computations within a time window being too small for sampling firing rates. It seems more likely that these computations are based on single firing events (see e.g. <ref> [1, 10, 13, 12] </ref>). For example, Thorpe and Imbert [13] were able to show that humans can analyze and classify visual patterns in 100 msec, although at least 10 synaptic stages are involved. <p> It is not hard to see then that w (n) ew &lt; w (n+1) ew holds for w (n) 6= ew, which implies the convergence. 2 It is easy to verify that by some proper transformation of the normalization not only weights from <ref> [0; 1] </ref> but from some arbitrary interval [w min ; w max ] R + can be learned. We finally observe that the type of learning in this section is not based on an error signal.
Reference: 2. <author> Blum, E.K.: </author> <title> Numerical Analysis and Computation: Theory and Practice. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass. </address> <year> (1972). </year>
Reference-contexts: Hence t v depends also on t 0 . We show that in both cases the value of w u;v stays on the same side of ew and approaches ew successively. This will be done using a fixed point theorem from numerical analysis <ref> [2] </ref>. Case 1: w &gt; ew. <p> We will show that f maps [ ew; w max ] into itself and is contractive on [ ew; w max ]. Then it follows by a well known fixed point theorem (see, e.g., <ref> [2, Theorem 5.1.1] </ref>) that f has a unique fixed point in [ ew; w max ], which is then ew, and that the sequence w (n+1) = f (w (n) ) converges to that fixed point for any choice of w (0) 2 [ ew; w max ]. <p> the proof of the theorem. 2 With regard to the convergence rate we can make the following observation: It is well known that for contractive iterative mappings the rate of convergence is governed by jw (n) ewj L n jw (0) ewj where L &lt; 1 is a Lipschitz constant <ref> [2] </ref>. Thus the sequence of weights generated according to the learning rule (2) satisfies jw (n) ewj 1 w 2 n where = j (fi P rest v ) if w (0) &gt; ew and = j (fi P rest v )=2 if w (0) &lt; ew.
Reference: 3. <author> Brown, T.H. and Chattarji, S.: </author> <title> Hebbian synaptic plasticity. In: Handbook of brain theory and neural networks, Arbib, </title> <editor> M., ed., </editor> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> (1995) </year> <month> 454-459. </month>
Reference-contexts: Since the traditional formulation of the Hebb-rule allows only an increase of the weights, one frequently assumes in addition that after each weight change all weights of the neuron are normalized (see e.g. <ref> [3] </ref>).
Reference: 4. <author> Gerstner, W. and van Hemmen, </author> <title> L.H.: How to describe neuronal activity: spikes, rates, </title> <booktitle> or assemblies? Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo (1994) 463-470. </address>
Reference-contexts: In this article we consider spiking neuron networks (SNN's), as introduced by Maass [6], where each neuron is basically a leaky integrate-and-fire neuron and can be considered as a noise free version of the spike response model by Gerstner and van Hemmen <ref> [4] </ref>. These SNN's are besides their biological realism also because of their computational power of great interest. In [7, 5] it has been shown that SNN's are computationally more powerful than McCulloch-Pitts neurons (i.e. threshold gates) and also than sigmoidal gates.
Reference: 5. <author> Maass, W.: </author> <title> Fast sigmoidal networks via spiking neurons. </title> <note> To appear in Neural Computation. </note>
Reference-contexts: These SNN's are besides their biological realism also because of their computational power of great interest. In <ref> [7, 5] </ref> it has been shown that SNN's are computationally more powerful than McCulloch-Pitts neurons (i.e. threshold gates) and also than sigmoidal gates. <p> It has turned out that especially neurons receiving their input as the time difference between firing times can be used to compute in a biologically plausible way the product between its weight vector and its input vector. In <ref> [5] </ref> it has been shown how this approach can be used for simulating arbitrary feedforward sigmoidal neural nets in a way which is much faster and more consistent with experimental results about fast information processing in biological neural systems than the usual method using average firing rates. <p> Hence these SNN's can approximate any continuous function of several variables in temporal coding. This approach gives also rise to a very simple and straightforward way of realizing pattern analysis tasks with SNN's <ref> [5, 9] </ref>. Within this model one considers neurons with given fixed weights that compute functions where the inputs and output are temporally encoded. <p> Furthermore we show in Section 4 how several synapses of some neuron can learn given weight values in parallel. 2 Basic Assumptions and Definitions We consider the common model of leaky integrate-and-fire neurons, without noise. As in <ref> [5, 6, 7] </ref> we make only one additional assumption for realizing computations in such a model: The postsynaptic potential is assumed to grow linearly during an initial segment of fixed but arbitrarily small length.
Reference: 6. <author> Maass, W.: </author> <title> Lower bounds for the computational power of networks of spiking neurons. </title> <booktitle> Neural Computation 8 (1996) 1-40. </booktitle>
Reference-contexts: Results like these motivated the interest in investigating networks of spiking neurons, which can base their computation on single firing events. In this article we consider spiking neuron networks (SNN's), as introduced by Maass <ref> [6] </ref>, where each neuron is basically a leaky integrate-and-fire neuron and can be considered as a noise free version of the spike response model by Gerstner and van Hemmen [4]. These SNN's are besides their biological realism also because of their computational power of great interest. <p> Furthermore we show in Section 4 how several synapses of some neuron can learn given weight values in parallel. 2 Basic Assumptions and Definitions We consider the common model of leaky integrate-and-fire neurons, without noise. As in <ref> [5, 6, 7] </ref> we make only one additional assumption for realizing computations in such a model: The postsynaptic potential is assumed to grow linearly during an initial segment of fixed but arbitrarily small length.
Reference: 7. <author> Maass, W.: </author> <title> Networks of spiking neurons: </title> <booktitle> the third generation of neural network models. Proc. of the 7th Australian Conference on Neural Networks, </booktitle> <address> Canberra, Australia (1996) 1-10. </address>
Reference-contexts: These SNN's are besides their biological realism also because of their computational power of great interest. In <ref> [7, 5] </ref> it has been shown that SNN's are computationally more powerful than McCulloch-Pitts neurons (i.e. threshold gates) and also than sigmoidal gates. <p> Furthermore we show in Section 4 how several synapses of some neuron can learn given weight values in parallel. 2 Basic Assumptions and Definitions We consider the common model of leaky integrate-and-fire neurons, without noise. As in <ref> [5, 6, 7] </ref> we make only one additional assumption for realizing computations in such a model: The postsynaptic potential is assumed to grow linearly during an initial segment of fixed but arbitrarily small length.
Reference: 8. <author> Markram, H.: </author> <title> Neocortical pyramidal neurons scale the efficacy of synaptic input according to arrival time: a proposed selection principle of the most appropriate synaptic information. </title> <booktitle> Proc. of Cortical Dynamics in Jerusalem (1995) 10-11. </booktitle>
Reference-contexts: that the action potential in neocortical pyramidal cells is propagated backwards from the soma to the dendritic tree, such that information about the pre- and postsynaptic firing times is available at the synapse [11] and that synaptic efficacy in those neurons can be actually changed due to this time difference <ref> [8] </ref>. In this article we focus on supervised learning, where the weight is supposed to assume a certain value and where the weight changes during the learning process are only based on differences between pre- and postsynaptic firing times.
Reference: 9. <author> Ruf, B.: </author> <title> Pattern analysis with networks of spiking neurons. </title> <note> Submitted for publi-cation. </note>
Reference-contexts: Hence these SNN's can approximate any continuous function of several variables in temporal coding. This approach gives also rise to a very simple and straightforward way of realizing pattern analysis tasks with SNN's <ref> [5, 9] </ref>. Within this model one considers neurons with given fixed weights that compute functions where the inputs and output are temporally encoded.
Reference: 10. <author> Sejnowski, T. J.: </author> <title> Time for a new neural code? Nature 376 (1995) 21-22. </title>
Reference-contexts: However, biological neural systems can perform certain computations within a time window being too small for sampling firing rates. It seems more likely that these computations are based on single firing events (see e.g. <ref> [1, 10, 13, 12] </ref>). For example, Thorpe and Imbert [13] were able to show that humans can analyze and classify visual patterns in 100 msec, although at least 10 synaptic stages are involved.
Reference: 11. <author> Stuart, G. J. and Sakmann, B.: </author> <title> Active propagation of somatic action potentials into neocortical pyramidal cell dendrites. </title> <note> Nature 367 (1994) 69-72. </note>
Reference-contexts: This type of learning is motivated by recent neu-robiological results, where it was shown that the action potential in neocortical pyramidal cells is propagated backwards from the soma to the dendritic tree, such that information about the pre- and postsynaptic firing times is available at the synapse <ref> [11] </ref> and that synaptic efficacy in those neurons can be actually changed due to this time difference [8].
Reference: 12. <author> Thorpe, S. J. and Gautrais, J.: </author> <title> Rapid visual processing using spike asynchrony. </title> <booktitle> To appear in Advances in Neural Information Processing Systems 9, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1997. </year>
Reference-contexts: However, biological neural systems can perform certain computations within a time window being too small for sampling firing rates. It seems more likely that these computations are based on single firing events (see e.g. <ref> [1, 10, 13, 12] </ref>). For example, Thorpe and Imbert [13] were able to show that humans can analyze and classify visual patterns in 100 msec, although at least 10 synaptic stages are involved. <p> Generally for realizing neural computations based on single firing events it seems to be important to understand the relation between weights and temporally encoded inputs. Thorpe and Gautrais have recently shown that exactly this question could be crucial for fast information processing of neurons in the visual system <ref> [12] </ref>. They demonstrated that feature extraction might be achieved by the visual system using the order of firing and a proper tuning of synaptic strenghts.
Reference: 13. <author> Thorpe, S. J. and Imbert, M.: </author> <title> Biological constraints on connectionist modelling. In: Connectionism in Perspective, </title> <editor> Pfeifer, R., Schreter, Z., Fogelman-Soulie, F., and Steels, L., eds., </editor> <publisher> Elsevier, </publisher> <month> North-Holland </month> <year> (1989). </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: However, biological neural systems can perform certain computations within a time window being too small for sampling firing rates. It seems more likely that these computations are based on single firing events (see e.g. <ref> [1, 10, 13, 12] </ref>). For example, Thorpe and Imbert [13] were able to show that humans can analyze and classify visual patterns in 100 msec, although at least 10 synaptic stages are involved. <p> However, biological neural systems can perform certain computations within a time window being too small for sampling firing rates. It seems more likely that these computations are based on single firing events (see e.g. [1, 10, 13, 12]). For example, Thorpe and Imbert <ref> [13] </ref> were able to show that humans can analyze and classify visual patterns in 100 msec, although at least 10 synaptic stages are involved.
References-found: 13

