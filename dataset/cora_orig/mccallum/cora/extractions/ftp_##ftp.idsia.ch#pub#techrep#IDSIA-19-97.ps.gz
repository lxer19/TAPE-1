URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-19-97.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Email: nic@idsia.ch  
Title: Centering Neural Network Gradient Factors  
Author: Nicol N. Schraudolph 
Web: http://www.idsia.ch/  
Address: Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Date: 19, 1997 revised August 21, 1998  
Note: April  
Abstract: Technical Report IDSIA-19-97 Abstract. It has long been known that neural networks can learn faster when their input and hidden unit activities are centered about zero; recently we have extended this approach to also encompass the centering of error signals [2]. Here we generalize this notion to all factors involved in the network's gradient, leading us to propose centering the slope of hidden unit activation functions as well. Slope centering removes the linear component of backpropagated error; this improves credit assignment in networks with shortcut connections. Benchmark results show that this can speed up learning significantly without adversely affecting the trained network's generalization ability. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Genevieve B. Orr and Klaus-Robert Muller, </author> <title> editors. Neural Networks: Tricks of the Trade, </title> <booktitle> volume 1524 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: Although Sejnowski [5] proposed a variant of Hebbian learning in which both ? Reprinted from Orr and Muller (eds.), Neural Networks: Tricks of the Trade <ref> [1] </ref>. the pre- and postsynaptic factors of the weight update are centered, the idea was not taken up when backpropagation became popular.
Reference: 2. <author> Nicol N. Schraudolph and Terrence J. Sejnowski. </author> <title> Tempering backpropagation networks: Not all weights are created equal. </title> <editor> In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 563-569. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: The benefits of centering error signals in multi-layer networks were thus reported only recently <ref> [2] </ref>; here we finally suggest centering as a general methodology, and present backpropagation equations in which all factors are centered. Independence of architecture. Although centering is introduced here in the context of feedforward networks with sigmoid activation functions, the approach itself has a far wider reach. <p> With a local step size j ij for each weight, this results in the weight update equation w ij = j ij ffi j x i ; where ffi j = @E=@y j : (3) Centered. We have recently proposed <ref> [2] </ref> that the error signals ffi j should be centered as well to achieve even faster convergence. <p> Since this means that the average error hffi j i is given exclusively to the bias weight w 0 j , we have previously called this technique d.c. error shunting <ref> [2] </ref>. 2.3 Error Backpropagation Conventional. <p> This includes algorithms such as BCM learning [8, 9] and binary information gain optimization [10]. 4 Empirical Results While activity centering has long been part of backpropagation lore, and empirical results for error centering have been reported previously <ref> [2] </ref>, slope centering is being proposed for the first time here.
Reference: 3. <author> Bernard Widrow, John M. McCool, Michael G. Larimore, and C. Richard Johnson, Jr. </author> <title> Stationary and nonstationary learning characteristics of the LMS adaptive filter. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 64(8) </volume> <pages> 1151-1162, </pages> <year> 1976. </year>
Reference-contexts: Its basic tenet is: All pattern-dependent factors entering the update equation for a neural network weight should be centered, i.e., have their average over patterns subtracted out. Prior work. It is well-known that the inputs to an LMS adaptive filter should be centered to permit rapid yet stable adaptation <ref> [3] </ref>, and it has been argued [4] that the same applies to input and hidden unit activity in a multi-layer network.
Reference: 4. <author> Yann LeCun, Ido Kanter, and Sara A. Solla. </author> <title> Eigenvalues of covariance matrices: Application to neural-network learning. </title> <journal> Physical Review Letters, </journal> <volume> 66(18) </volume> <pages> 2396-2399, </pages> <year> 1991. </year>
Reference-contexts: Prior work. It is well-known that the inputs to an LMS adaptive filter should be centered to permit rapid yet stable adaptation [3], and it has been argued <ref> [4] </ref> that the same applies to input and hidden unit activity in a multi-layer network. <p> In particular, we posit a bias input x 0 j 1 and require that all nodes are connected to it: (8j &gt; 0) 0 2 A j . Centered. As suggested by LeCun et al. <ref> [4] </ref>, the activity of the network's input and hidden units should be centered to permit faster learning. <p> For instance, the hyperbolic tangent (tanh) function with its symmetric range from -1 to 1 will typically produce better-centered output than the commonly used logistic sigmoid f (y) = 1=(1 + e y ) ranging from 0 to 1, and is therefore the preferred activation function for hidden units <ref> [4] </ref>. Similarly, the input representation can (and should) be chosen such that inputs will be roughly centered. When using shortcuts, one may even choose a priori to subtract a constant (say, half their maximum) from hidden unit slopes to improve their centering. <p> For a single linear node y = w T x with squared loss function, the Hessian is simply the covariance matrix of the inputs: H = . Its largest eigenvalue is typically caused by the d.c. component of x <ref> [4] </ref>. Centering the inputs removes that eigenvalue, thus conditioning the Hessian and permitting larger step sizes.
Reference: 5. <author> Terrence J. Sejnowski. </author> <title> Storing covariance with nonlinearly interacting neurons. </title> <journal> Journal of Mathematical Biology, </journal> <volume> 4 </volume> <pages> 303-321, </pages> <year> 1977. </year>
Reference-contexts: Prior work. It is well-known that the inputs to an LMS adaptive filter should be centered to permit rapid yet stable adaptation [3], and it has been argued [4] that the same applies to input and hidden unit activity in a multi-layer network. Although Sejnowski <ref> [5] </ref> proposed a variant of Hebbian learning in which both ? Reprinted from Orr and Muller (eds.), Neural Networks: Tricks of the Trade [1]. the pre- and postsynaptic factors of the weight update are centered, the idea was not taken up when backpropagation became popular.
Reference: 6. <author> Nicol N. Schraudolph. </author> <title> Slope centering: Making shortcut weights effective. </title> <editor> In Lars Niklasson, Mikael Boden, and Tom Ziemke, editors, </editor> <booktitle> Proceedings of the 8th International Conference on Artificial Neural Networks, Perspectives in Neural Computing, </booktitle> <pages> pages 523-528, </pages> <address> Skovde, Sweden, 1998. </address> <publisher> Springer Verlag, </publisher> <address> Berlin. ftp://ftp.idsia.ch/pub/nic/slope.ps.gz fl. </address>
Reference-contexts: It has been noted before that neural network learning sometimes improves with the addition of shortcut weights. In our own experiments (see Section 4), however, we find that it is slope centering that makes shortcut weights genuinely useful <ref> [6] </ref>. A complementary approach? Van der Smagt and Hirzinger [7] also advocate shortcuts as a means for accelerating neural network learning.
Reference: 7. <author> Patrick van der Smagt and Gerd Hirzinger. </author> <title> Solving the ill-conditioning in neural network learning. </title> <booktitle> In Neural Networks: Tricks of the Trade [1], </booktitle> <pages> pages 193-206. </pages>
Reference-contexts: It has been noted before that neural network learning sometimes improves with the addition of shortcut weights. In our own experiments (see Section 4), however, we find that it is slope centering that makes shortcut weights genuinely useful [6]. A complementary approach? Van der Smagt and Hirzinger <ref> [7] </ref> also advocate shortcuts as a means for accelerating neural network learning. Note, however, that their use of shortcuts is quite different from ours: in order to improve the conditioning of a neural network, they add shortcut connections whose weights are coupled to (shared with) existing weights.
Reference: 8. <author> E.L. Bienenstock, L.N. Cooper, and P.W. Munro. </author> <title> Theory for the development of neuron selectivity: Orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal of Neuroscience, </journal> <volume> 2, </volume> <year> 1982. </year> <note> Reprinted in [29]. </note>
Reference-contexts: Note that the expansion technique shown here may be used to derive an exact single-pass batch method for any weight update that involves the addition (or subtraction) of some quantity that must be computed from the entire batch of training patterns. This includes algorithms such as BCM learning <ref> [8, 9] </ref> and binary information gain optimization [10]. 4 Empirical Results While activity centering has long been part of backpropagation lore, and empirical results for error centering have been reported previously [2], slope centering is being proposed for the first time here.
Reference: 9. <author> N. Intrator. </author> <title> Feature extraction using an unsupervised neural network. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 98-107, </pages> <year> 1992. </year>
Reference-contexts: Note that the expansion technique shown here may be used to derive an exact single-pass batch method for any weight update that involves the addition (or subtraction) of some quantity that must be computed from the entire batch of training patterns. This includes algorithms such as BCM learning <ref> [8, 9] </ref> and binary information gain optimization [10]. 4 Empirical Results While activity centering has long been part of backpropagation lore, and empirical results for error centering have been reported previously [2], slope centering is being proposed for the first time here.
Reference: 10. <author> Nicol N. Schraudolph and Terrence J. Sejnowski. </author> <title> Unsupervised discrimination of clustered data via optimization of binary information gain. </title> <editor> In Stephen Jose Han-son, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 499-506. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: This includes algorithms such as BCM learning [8, 9] and binary information gain optimization <ref> [10] </ref>. 4 Empirical Results While activity centering has long been part of backpropagation lore, and empirical results for error centering have been reported previously [2], slope centering is being proposed for the first time here.
Reference: 11. <author> Samir Shah, Francesco Palmieri, and Michael Datum. </author> <title> Optimal filtering algorithms for fast learning in feedforward neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 779-787, </pages> <year> 1992. </year>
Reference-contexts: Where not done a priori, centering was then implemented with the exact two-pass batch method. In addition, we always updated the hidden-to-output weights of the network before backpropagating error through them. This is known to sometimes improve convergence behavior <ref> [11] </ref>, and we have found it to increase stability at the large step sizes we desire. Competitive controls. The ordinary backpropagation (plain gradient descent) algorithm has many known defects, and a large number of acceleration techniques has been proposed for it.
Reference: 12. <author> Ralph Neuneier and Hans Georg Zimmermann. </author> <title> How to train neural networks. </title> <booktitle> In Neural Networks: Tricks of the Trade [1], </booktitle> <pages> pages 373-423. </pages>
Reference-contexts: This combination | vario-j and bold driver | was then used for all experiments reported here. Thus any performance advantage for centering reported thereafter has been realized on top of a state-of-the-art accelerated gradient method as control. Vario-j <ref> [12, 13, page 48] </ref>. This interesting technique sets the local learning rate for each weight inversely proportional to the standard deviation of its stochastic gradient.
Reference: 13. <editor> Hans Georg Zimmermann. Neuronale Netze als Entscheidungskalkul. In Heinz Rehkugler and Hans Georg Zimmermann, editors, Neuronale Netze in der Oko-nomie: </editor> <booktitle> Grundlagen und finanzwirtschaftliche Anwendungen, </booktitle> <pages> pages 1-87. </pages> <publisher> Vahlen Verlag, </publisher> <address> Munich, </address> <year> 1994. </year>
Reference-contexts: This combination | vario-j and bold driver | was then used for all experiments reported here. Thus any performance advantage for centering reported thereafter has been realized on top of a state-of-the-art accelerated gradient method as control. Vario-j <ref> [12, 13, page 48] </ref>. This interesting technique sets the local learning rate for each weight inversely proportional to the standard deviation of its stochastic gradient.
Reference: 14. <author> A. Lapedes and R. Farber. </author> <title> A self-optimizing, nonsymmetrical neural net for content addressable memory and pattern recognition. </title> <journal> Physica, </journal> <volume> D 22 </volume> <pages> 247-259, </pages> <year> 1986. </year>
Reference-contexts: We used vario-j for all experiments reported here, with % = 0:1. In a batch implementation this leaves only one free parameter to be determined: the global learning rate j. Bold driver <ref> [14, 15, 16, 17] </ref>. This algorithm for adapting the global learning rate j is simple and effective, but only works for batch learning.
Reference: 15. <author> T. P. Vogl, J. K. Mangis, A. K. Rigler, W. T. Zink, and D. L. Alkon. </author> <title> Accelerating the convergence of the back-propagation method. </title> <journal> Biological Cybernetics, </journal> <volume> 59 </volume> <pages> 257-263, </pages> <year> 1988. </year>
Reference-contexts: We used vario-j for all experiments reported here, with % = 0:1. In a batch implementation this leaves only one free parameter to be determined: the global learning rate j. Bold driver <ref> [14, 15, 16, 17] </ref>. This algorithm for adapting the global learning rate j is simple and effective, but only works for batch learning.
Reference: 16. <author> Roberto Battiti. </author> <title> Accelerated back-propagation learning: Two optimization methods. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 331-342, </pages> <year> 1989. </year>
Reference-contexts: We used vario-j for all experiments reported here, with % = 0:1. In a batch implementation this leaves only one free parameter to be determined: the global learning rate j. Bold driver <ref> [14, 15, 16, 17] </ref>. This algorithm for adapting the global learning rate j is simple and effective, but only works for batch learning.
Reference: 17. <author> R. Battiti. </author> <title> First- and second-order methods for learning: Between steepest descent and Newton's method. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 141-166, </pages> <year> 1992. </year>
Reference-contexts: We used vario-j for all experiments reported here, with % = 0:1. In a batch implementation this leaves only one free parameter to be determined: the global learning rate j. Bold driver <ref> [14, 15, 16, 17] </ref>. This algorithm for adapting the global learning rate j is simple and effective, but only works for batch learning.
Reference: 18. <author> David H. Deterding. </author> <title> Speaker Normalisation for Automatic Speech Recognition. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
Reference-contexts: questions: 1) will these results transfer to more challenging, realistic problems, and 2) is the gain in learning speed | as often happens | bought at the expense of generalization ability? In order to address these questions, we conducted further experiments with the speaker-independent vowel recognition data due to Deterding <ref> [18] </ref>, a popular benchmark for which good generalization performance is rather difficult to achieve. The task. The network's task is to recognize the eleven steady-state vowels of British English in a speaker-independent fashion, given 10 spectral features (specifically: LPC-derived log area ratios) of the speech signal.
Reference: 19. <author> Anthony J. Robinson. </author> <title> Dynamic Error Propagation Networks. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
Reference-contexts: We follow the convention of splitting it into a training set containing the data from the first 8 (4 male, 4 female) speakers, and a test set containing those of the remaining 7 (4 male, 3 female). Note that there is no separate validation set available. Prior work. Robinson <ref> [19] </ref> pioneered the use of Deterding's data as a benchmark by comparing the performance of a number of neural network architectures on it.
Reference: 20. <author> Michael Finke and Klaus-Robert Muller. </author> <title> Estimating a-posteriori probabilities using stochastic network models. </title> <editor> In Michael C. Mozer, Paul Smolensky, David S. Touretzky, Jeffrey L. Elman, and Andreas S. Weigend, editors, </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School,, </booktitle> <address> Boulder, CO, 1994. </address> <publisher> Lawrence Erl-baum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Trained on the task as formulated above, conventional backpropagation networks in fact appear to reach their limits at error rates of around 42% <ref> [20, 21] </ref>, while an adaptive nearest neighbor technique can achieve 38% [22]. Flake [23] reports comparably favorable results for RBF networks as well as his own hybrid architectures.
Reference: 21. <author> Sepp Hochreiter and Jurgen Schmidhuber. </author> <title> Feature extraction through lococode. </title> <note> To appear in Neural Computation, </note> <year> 1998. </year>
Reference-contexts: Trained on the task as formulated above, conventional backpropagation networks in fact appear to reach their limits at error rates of around 42% <ref> [20, 21] </ref>, while an adaptive nearest neighbor technique can achieve 38% [22]. Flake [23] reports comparably favorable results for RBF networks as well as his own hybrid architectures.
Reference: 22. <author> Trevor J. Hastie and Robert J. Tibshirani. </author> <title> Discriminant adaptive nearest neighbor classification. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 18(6) </volume> <pages> 607-616, </pages> <year> 1996. </year>
Reference-contexts: Trained on the task as formulated above, conventional backpropagation networks in fact appear to reach their limits at error rates of around 42% [20, 21], while an adaptive nearest neighbor technique can achieve 38% <ref> [22] </ref>. Flake [23] reports comparably favorable results for RBF networks as well as his own hybrid architectures. Even better performance can be obtained by using speaker sex/identity information [24, 25], or by training a separate model for each vowel [26].
Reference: 23. <author> Gary William Flake. </author> <title> Square unit augmented, radially extended, multilayer perceptrons. </title> <booktitle> In Neural Networks: Tricks of the Trade [1], </booktitle> <pages> pages 145-163. </pages>
Reference-contexts: Trained on the task as formulated above, conventional backpropagation networks in fact appear to reach their limits at error rates of around 42% [20, 21], while an adaptive nearest neighbor technique can achieve 38% [22]. Flake <ref> [23] </ref> reports comparably favorable results for RBF networks as well as his own hybrid architectures. Even better performance can be obtained by using speaker sex/identity information [24, 25], or by training a separate model for each vowel [26].
Reference: 24. <author> Peter D. Turney. </author> <title> Exploiting context when learning to classify. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 402-407, </pages> <year> 1993. </year>
Reference-contexts: Flake [23] reports comparably favorable results for RBF networks as well as his own hybrid architectures. Even better performance can be obtained by using speaker sex/identity information <ref> [24, 25] </ref>, or by training a separate model for each vowel [26]. By combining these two approaches, a test set error of 23% has been reached [27], the lowest we are aware of to date. Training and testing.
Reference: 25. <author> Peter D. Turney. </author> <title> Robust classification with context-sensitive features. </title> <booktitle> In Proceedings of the Sixth International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, </booktitle> <pages> pages 268-276, </pages> <year> 1993. </year>
Reference-contexts: Flake [23] reports comparably favorable results for RBF networks as well as his own hybrid architectures. Even better performance can be obtained by using speaker sex/identity information <ref> [24, 25] </ref>, or by training a separate model for each vowel [26]. By combining these two approaches, a test set error of 23% has been reached [27], the lowest we are aware of to date. Training and testing.
Reference: 26. <author> Michael Herrmann. </author> <title> On the merits of topography in neural maps. </title> <editor> In Teuvo Koho-nen, editor, </editor> <booktitle> Proceedings of the Workshop on Self-Organizing Maps, </booktitle> <pages> pages 112-117. </pages> <institution> Helsinki University of Technology, </institution> <year> 1997. </year>
Reference-contexts: Flake [23] reports comparably favorable results for RBF networks as well as his own hybrid architectures. Even better performance can be obtained by using speaker sex/identity information [24, 25], or by training a separate model for each vowel <ref> [26] </ref>. By combining these two approaches, a test set error of 23% has been reached [27], the lowest we are aware of to date. Training and testing. We trained fully connected feedforward networks with 10 inputs, 22 hidden units, and 11 logistic output units by minimization of cross-entropy loss.
Reference: 27. <author> Joshua B. Tenenbaum and William T. Freeman. </author> <title> Separating style and content. </title> <editor> In Michael C. Mozer, Michael I. Jordan, and Thomas Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 9, </volume> <pages> pages 662-668. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1997. </year>
Reference-contexts: Even better performance can be obtained by using speaker sex/identity information [24, 25], or by training a separate model for each vowel [26]. By combining these two approaches, a test set error of 23% has been reached <ref> [27] </ref>, the lowest we are aware of to date. Training and testing. We trained fully connected feedforward networks with 10 inputs, 22 hidden units, and 11 logistic output units by minimization of cross-entropy loss. The target was 1 for the output corresponding to the correct vowel, 0 for all others.
Reference: 28. <author> Lutz Prechelt. </author> <title> Early stopping | but when? In Neural Networks: </title> <booktitle> Tricks of the Trade [1], </booktitle> <pages> pages 55-69. </pages>
Reference-contexts: If the best value found so far is not improved upon within a certain period of time, we pick it as the minimum of that run for the purpose of quantitative analysis. The appropriate length of waiting period before giving up on further improvement is a difficult issue <ref> [28] </ref>. For a fair comparison between faster and slower optimization methods, it should be proportional to the time it took to reach the minimum in question: a slow run then has correspondingly more time to improve its solution than a fast one.
Reference: 29. <editor> J.A. Anderson and E. Rosenfeld, editors. Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
References-found: 29

