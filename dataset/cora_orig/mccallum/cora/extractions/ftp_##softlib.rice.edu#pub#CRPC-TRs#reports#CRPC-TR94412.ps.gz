URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94412.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A Highly Parallel Algorithm for the Reduction of a Nonsymmetric Matrix to Block Upper-Hessenberg Form  
Author: Michael W. Berry Jack J. Dongarra Youngbae Kim 
Date: February 6, 1994  
Abstract: In this paper, we present an algorithm for the reduction to block upper-Hessenberg form which can be used to solve the nonsymmetric eigenvalue problem on message-passing multicom-puters. On such multicomputers, a nonsymmetric matrix can be distributed across processing nodes configured into a network of two-dimensional mesh processor array using a block-scattered decomposition. Based on the matrix partitioning and mapping, the algorithm employs both Householder reflectors and Givens rotations within each reduction step. We analyze the arithmetic and communication complexities and describe the implementation details of the algorithm on message-passing multicomputers. We discuss two different implementations|synchronous and asynchronous|and present performance results on the In-tel iPSC/860 and DELTA. We conclude with an evaluation of the algorithm's communication cost, and suggest areas for further improvement.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Barnett, D. G. Payne, and R. van de Geijn. </author> <title> Optimal broadcasting in mesh-connected architectures. </title> <type> Technical Report TR-91-38, </type> <institution> University of Texas, </institution> <year> 1991. </year>
Reference-contexts: If a message is delivered h hops away, the message transfer time can be roughly estimated as h (ff+mfi). In the wormhole routing mechanism used in the Intel DELTA, the message transfer time is almost independent of the distance (number of hops) between processors <ref> [1] </ref>. In this case, if network contention is not considered, the message transfer time can be represented as ff+mfi regardless of the distance that a message has to traverse. In our analysis, for convenience, we assume that the message transfer time is ff+mfi for m floating-point numbers.
Reference: [2] <author> Eleanor Chu and Alan George. </author> <title> QR factorization of a dense matrix on a hypercube multiprocessor. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11(5) </volume> <pages> 990-1028, </pages> <year> 1990. </year>
Reference-contexts: On a parallel machine where memory access times may dominate flop times, Givens algorithms may be preferable. In [4], for example, it was shown that on the Denelcor HEP (a shared-memory multiprocessor), Givens algorithms were twice as fast as Householder algorithms (see also <ref> [2] </ref>). 2 Dongarra and Ostrouchov have also developed a parallel algorithm for reducing a matrix to upper--Hessenberg form on distributed-memory multiprocessors [3]; the implementation in LAPACK is discussed by Dongarra and van de Geijn in [7].
Reference: [3] <author> J. J. Dongarra and S. Ostrouchov. </author> <title> LAPACK block factorization algorithms on the intel ipsc/860. </title> <type> Technical Report CS-90-115, </type> <institution> University of Tennessee, </institution> <year> 1990. </year> <note> LAPACK Working Note 24. </note>
Reference-contexts: In [4], for example, it was shown that on the Denelcor HEP (a shared-memory multiprocessor), Givens algorithms were twice as fast as Householder algorithms (see also [2]). 2 Dongarra and Ostrouchov have also developed a parallel algorithm for reducing a matrix to upper--Hessenberg form on distributed-memory multiprocessors <ref> [3] </ref>; the implementation in LAPACK is discussed by Dongarra and van de Geijn in [7]. In 1989, Pothen and Raghavan [10] showed that a hybrid algorithm could take advantage of both low-cost arithmetic operations of Householder algorithm and low-cost communication costs of Givens algorithms.
Reference: [4] <author> J. J. Dongarra, A. H. Sameh, and D. C. Sorensen. </author> <title> Implementation of some concurrent algorithms for matrix factorizations. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 25-34, </pages> <year> 1986. </year>
Reference-contexts: For example, one can use DGEHRD from LAPACK [5]. Since block methods on high-performance computers improve processing efficiency by grouping memory references, DGEHRD is implemented using block Householder reflectors. On a parallel machine where memory access times may dominate flop times, Givens algorithms may be preferable. In <ref> [4] </ref>, for example, it was shown that on the Denelcor HEP (a shared-memory multiprocessor), Givens algorithms were twice as fast as Householder algorithms (see also [2]). 2 Dongarra and Ostrouchov have also developed a parallel algorithm for reducing a matrix to upper--Hessenberg form on distributed-memory multiprocessors [3]; the implementation in LAPACK
Reference: [5] <author> J. J. Dongarra, D. C. Sorensen, and S. Hammarling. </author> <title> Block reduction of matrices to condensed forms for eigenvalue computations. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 27 </volume> <pages> 215-227, </pages> <year> 1989. </year>
Reference-contexts: On traditional sequential machines, therefore, where the cost of a floating-point operation dominates the cost of a memory reference, Householder-based algorithms are generally preferred. Several sequential algorithms have been developed for the reduction of a general dense matrix to upper-Hessenberg form. For example, one can use DGEHRD from LAPACK <ref> [5] </ref>. Since block methods on high-performance computers improve processing efficiency by grouping memory references, DGEHRD is implemented using block Householder reflectors. On a parallel machine where memory access times may dominate flop times, Givens algorithms may be preferable.
Reference: [6] <author> J. J. Dongarra, R. van de Geijn, and R. C. Whaley. </author> <title> A users' guide to the BLACS. </title> <type> Technical Report CS-93-187, </type> <institution> University of Tennessee, </institution> <year> 1993. </year> <note> LAPACK Working Note 57. </note>
Reference-contexts: The set for nonblocking calls comprises isend () and irecv (); the msgdone () call is used to check whether an asynchronous operation has completed. 4.2 Synchronous Implementation For our synchronous implementation, we use the BLACS (Basic Linear Algebra Communication Subprograms) communication library <ref> [6] </ref>. The BLACS are a linear algebra communication library written using communication primitives of message-passing multicomputers. The library provides portable, efficient, and modular high-level routines for manipulating and communicating data structures that are distributed among the memories of message-passing multicomputers.
Reference: [7] <author> J. J. Dongarra and R. A. van de Geijn. </author> <title> Reduction to condensed form for the eigenvalue problem on distributed memory architectures. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 973-982, </pages> <year> 1992. </year>
Reference-contexts: HEP (a shared-memory multiprocessor), Givens algorithms were twice as fast as Householder algorithms (see also [2]). 2 Dongarra and Ostrouchov have also developed a parallel algorithm for reducing a matrix to upper--Hessenberg form on distributed-memory multiprocessors [3]; the implementation in LAPACK is discussed by Dongarra and van de Geijn in <ref> [7] </ref>. In 1989, Pothen and Raghavan [10] showed that a hybrid algorithm could take advantage of both low-cost arithmetic operations of Householder algorithm and low-cost communication costs of Givens algorithms.
Reference: [8] <author> J.J. Dongarra, J. DuCroz, and S. Hammerling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans.on Math. Soft., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: These blocking and nonblocking mechanisms are often referred to as "synchronous" and "asynchronous" message passing, respectively. We have used the BLAS <ref> [8] </ref> and LAPACK routines for doing all basic block computations.
Reference: [9] <author> G. H. Golub and C. V. Van Loan. </author> <title> Matrix Computations, 2nd ed. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> lBaltimore, Mayland, </address> <year> 1989. </year>
Reference-contexts: Suppose that we have computed the QR factorization A 21 = ~ Q 1 R 1 and that ~ Q 1 = I + W 1 Y T 1 , an orthogonal matrix of the WY form, where W and Y are (n b) fi n matrices <ref> [9] </ref>. The block representation of the reduction algorithm is then given by Q T 0 B @ R 1 ~ Q T 1 C A ; where Q 1 = 0 B @ 0 ~ Q 1 C C ; and I b is an b fi b identity matrix. <p> In computing the nonsymmetric eigensystem associated with A, the original matrix A must also be postmultiplied by the orthogonal matrix, Q k . Two schemes are available for orthogonal factorizations: Householder reflectors and Givens rotations. Householder reflectors involve half the number of arithmetic operations as do Givens rotations <ref> [9] </ref>. On traditional sequential machines, therefore, where the cost of a floating-point operation dominates the cost of a memory reference, Householder-based algorithms are generally preferred. Several sequential algorithms have been developed for the reduction of a general dense matrix to upper-Hessenberg form.
Reference: [10] <author> Alex Pothen and Padma Raghavan. </author> <title> Distributed orthogonal factorization: Givens and Householder algorithms. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10(6) </volume> <pages> 1113-1134, </pages> <year> 1989. </year>
Reference-contexts: In 1989, Pothen and Raghavan <ref> [10] </ref> showed that a hybrid algorithm could take advantage of both low-cost arithmetic operations of Householder algorithm and low-cost communication costs of Givens algorithms.
Reference: [11] <author> Steven R. Seidel. </author> <title> Broadcasting on linear arrays and meshes. </title> <type> Technical Report ORNL/TM-12356, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1990. </year> <month> 17 </month>
Reference-contexts: IRING, DRING, and STREE, were implemented using broadcasting schemes that the BLACS supports: increasing ring, decreasing ring, and minimum spanning tree, respectively. Those broadcasting schemes require unidirectional ring topologies or linear arrays. Seidel <ref> [11] </ref> has shown that in ring broadcasts such as increasing ring and decreasing ring, while the original sending processor is required to spend only the amount of time to send a message to the next processor, the last processor must waste p times as much as that of receiving and sending
References-found: 11

