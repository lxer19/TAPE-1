URL: http://www.cs.nyu.edu/phd_students/wyckoff/plinda-man.ps
Refering-URL: http://www.cs.nyu.edu/phd_students/wyckoff/index.html
Root-URL: http://www.cs.nyu.edu
Title: PLinda User Manual  
Author: Thomas Brown Karpjoo Jeong Bin Li Suren Talla Peter Wyckoff Dennis Shasha 
Note: Contents  
Date: January 8, 1997  
Address: New York University 251 Mercer Street New York, NY 10012  
Affiliation: Courant Institute of Mathematical Sciences  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Yeshayahu Artsy and Raphael Finkel. </author> <title> Designing a process migration facility: </title> <booktitle> The Charlotte experience. IEEE Computer, </booktitle> <pages> pages 47-56, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: This is mainly because it minimizes the work of the server which is especially important in the current centralized design. 5 These criteria have already been used by other work-stealing systems and demonstrated to be effective <ref> [1, 6, 3, 7, 9, 17, 19, 20, 21] </ref>. 6 The current PLinda translator is only a preprocessor written in Perl and requires the programmer to annotate PLinda operations with variable type information explicitly. 10 6 Tuning the execution of PLinda programs Recall that PLinda allows the programmer to control the
Reference: [2] <author> Robert Bjornson. </author> <title> Linda on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1992. </year>
Reference-contexts: There are also two asynchronous operators rdp/inp which return true if a matching tuple was found in tuple space and false otherwise. There use is discouraged due to the phantom tuple problem <ref> [2] </ref>. Here is a brief description of the four basic operations: 1. out. Takes a sequence of typed expressions as arguments. It evaluates them, constructs a tuple from them, and inserts the tuple into tuple space. 2. eval. <p> The difference is that in is destructive (i.e. removes the tuple) while rd is not. function creates two processes using the eval operation: "producer" and "consumer". Then, the two processes communicate and synchronize using tuples. For a more detailed description of Linda, refer to <ref> [2, 4, 5] </ref>. 4 The PLinda model Processes cooperating to solve a problem by communicating through shared memory is the essence of the Linda and PLinda models.
Reference: [3] <author> Clemens H. Cap and Volker Strumpen. </author> <title> Efficient parallel computing in distributed workstation environments. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1221-1234, </pages> <year> 1993. </year>
Reference-contexts: This is mainly because it minimizes the work of the server which is especially important in the current centralized design. 5 These criteria have already been used by other work-stealing systems and demonstrated to be effective <ref> [1, 6, 3, 7, 9, 17, 19, 20, 21] </ref>. 6 The current PLinda translator is only a preprocessor written in Perl and requires the programmer to annotate PLinda operations with variable type information explicitly. 10 6 Tuning the execution of PLinda programs Recall that PLinda allows the programmer to control the
Reference: [4] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communication of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The difference is that in is destructive (i.e. removes the tuple) while rd is not. function creates two processes using the eval operation: "producer" and "consumer". Then, the two processes communicate and synchronize using tuples. For a more detailed description of Linda, refer to <ref> [2, 4, 5] </ref>. 4 The PLinda model Processes cooperating to solve a problem by communicating through shared memory is the essence of the Linda and PLinda models.
Reference: [5] <author> Nicholas Carriero and David Gelernter. </author> <title> How to write parallel programs : a first course. </title> <publisher> MIT Press Cambridge, </publisher> <year> 1992. </year>
Reference-contexts: This is a very brief description of parallel programming; the interested reader may refer to <ref> [5] </ref> for a more comprehensive treatment. 2.1 Process creation In the PLinda model parallel tasks are created at the file executable level. The programmer may create an instance of an executable by making a PLinda system call with the file name and its arguments. <p> The difference is that in is destructive (i.e. removes the tuple) while rd is not. function creates two processes using the eval operation: "producer" and "consumer". Then, the two processes communicate and synchronize using tuples. For a more detailed description of Linda, refer to <ref> [2, 4, 5] </ref>. 4 The PLinda model Processes cooperating to solve a problem by communicating through shared memory is the essence of the Linda and PLinda models.
Reference: [6] <author> David Cheriton. </author> <title> The V distributed system. </title> <booktitle> Communication of the ACM, </booktitle> <pages> pages 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: This is mainly because it minimizes the work of the server which is especially important in the current centralized design. 5 These criteria have already been used by other work-stealing systems and demonstrated to be effective <ref> [1, 6, 3, 7, 9, 17, 19, 20, 21] </ref>. 6 The current PLinda translator is only a preprocessor written in Perl and requires the programmer to annotate PLinda operations with variable type information explicitly. 10 6 Tuning the execution of PLinda programs Recall that PLinda allows the programmer to control the
Reference: [7] <author> P. Dasgupta, R.J. LeBlanc, M. Ahamad, and U. Ramachandran. </author> <title> The Clouds distributed operating system. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 34-44, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: This is mainly because it minimizes the work of the server which is especially important in the current centralized design. 5 These criteria have already been used by other work-stealing systems and demonstrated to be effective <ref> [1, 6, 3, 7, 9, 17, 19, 20, 21] </ref>. 6 The current PLinda translator is only a preprocessor written in Perl and requires the programmer to annotate PLinda operations with variable type information explicitly. 10 6 Tuning the execution of PLinda programs Recall that PLinda allows the programmer to control the
Reference: [8] <author> J. Dongarra, G. A. Geist, R. Mancheck, and V. S. Sunderam. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <journal> Computers in Physics, </journal> <volume> 7(2) </volume> <pages> 166-175, </pages> <year> 1993. </year>
Reference-contexts: In the case of no common file systems, we could just keep all the executables in the same directory path. This kind of a scheme has already been proposed and used in PVM <ref> [18, 8] </ref>. Data conversion between machines of different types can be done either by using a common external data representation, as in Sun's XDR routines, or by converting data in place. We have implemented both schemes.
Reference: [9] <author> Fred Douglis and John Ousterhout. </author> <title> Transparent process migration: Design alternatives and the Sprite implementation. </title> <journal> Software-Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-785, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: This is mainly because it minimizes the work of the server which is especially important in the current centralized design. 5 These criteria have already been used by other work-stealing systems and demonstrated to be effective <ref> [1, 6, 3, 7, 9, 17, 19, 20, 21] </ref>. 6 The current PLinda translator is only a preprocessor written in Perl and requires the programmer to annotate PLinda operations with variable type information explicitly. 10 6 Tuning the execution of PLinda programs Recall that PLinda allows the programmer to control the
Reference: [10] <author> High Performance Fortran Forum. </author> <title> High performance fortran language specification version 1.0. Crpc-tr92225, Center for Research on Parallel Computation, </title> <institution> Rice University, </institution> <year> 1993. </year>
Reference-contexts: The programmer may create an instance of an executable by making a PLinda system call with the file name and its arguments. This is very similar to the method used in PVM [14]. This type of parallelism is know as task parallelism. In High Performance Fortran (HPF) <ref> [10] </ref> parallelism is achieved by writing parallel loops that operate on vectors. This is known as data parallelism. 2.2 Communication In the PLinda model communication is facilitated through shared memory.
Reference: [11] <author> Vincent W. Freeh. </author> <title> A comparison of implicit and explicit parallel programming. </title> <type> Technical Report TR-93-30, </type> <institution> Dept of Computer Science, University of Arizona, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Mandelbrot set <ref> [11] </ref> computes operations in a complex plane and displays the results graphically. All samples may be found in $PLINDA HOME/code/samples. 15 Acknowledgments The following people were instrumental in creating, maintaining, and enhancing the PLinda system: Brian Anderson, Hansoo Kim, Jihai Qiu, and Zasha Weinberg.
Reference: [12] <author> Leslie Greengard. </author> <title> Rapid Evaluation of Potential Fields in Particle Systems. </title> <type> PhD thesis, </type> <address> M.I.T. </address> <publisher> Press, </publisher> <year> 1987. </year>
Reference-contexts: The sample programs included solve problems from physics and mathematics. PLinda/C++ samples include ping-pong, matrix multiply, particle simulation, and mandelbrot. The PLinda/Fortran77 sample is matrix multiply. Particle simulation is a parallelization of the Greengard sequential algorithm <ref> [12] </ref> which models a square computational box containing N point charges, with given charge values, initial positions, and velocities to compute the state of the system (i.e. position and velocities of all particles) after a given time interval.
Reference: [13] <author> W. G. Griswold and D. Notkin. </author> <title> Architectural tradeoffs for a meaning-preserving program restructuring tool. </title> <journal> IEEE Transactions of Software Engineering, </journal> <volume> 21(4) </volume> <pages> 275-287, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Adjusting arrays to fit a notion of correctness is in fact quite simple. In this case, the alternative would be to alter and possibly obfuscate the simple matrix multiplication algorithm. As in all application programming, there may exist a tradeoff between performance and structural intelligibility <ref> [13] </ref>. The PLinda/Fortran77 implementation allows the programmer to make that decision. 11 Compiling a PLinda Program Compiling a PLinda program written in C++ or Fortran77 follows similar guidelines. This section outlines how to successfully compile a PLinda program into an executable.
Reference: [14] <author> Jack Dongarra and others. </author> <title> A Users' Guide to PVM Parallel Virtual Machine. </title> <institution> Oak Ridge National Laboratory, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: The programmer may create an instance of an executable by making a PLinda system call with the file name and its arguments. This is very similar to the method used in PVM <ref> [14] </ref>. This type of parallelism is know as task parallelism. In High Performance Fortran (HPF) [10] parallelism is achieved by writing parallel loops that operate on vectors. This is known as data parallelism. 2.2 Communication In the PLinda model communication is facilitated through shared memory. <p> That is, plinda-2.x, lib, and include are all in the directory PLINDA HOME. PLinda source files need to include the PLinda client header file. This file defines the PLinda interface just as the PVM header files define the PVM message passing interface <ref> [14] </ref>. The source 23 files should include the file plinda.h. Make sure the include directory directive is used when compiling, -I$(PLINDA HOME)/include. The suffix of a PLinda/C++ source file should be .plc. Compilation of a PLinda program foo.plc consists of three steps: 1.
Reference: [15] <author> Karpjoo Jeong. </author> <title> Fault-Tolerant Parallel Processing Combining Linda, Checkpointing, and Transactions. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, </institution> <address> New York University, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: That is, a single failure leads to massive rollback. This method is fastest in the failure-free case. * Message Logging and Replay. This method is not supported in the current release of PLinda. All three methods are guaranteed to be correct. The interested reader may see <ref> [15] </ref> for a proof. The default in PLinda is to use the commit consistent execution method. This method is currently called Process Private snapshot in the user interface (this will be changed to continuation committing in the near future).
Reference: [16] <author> Karpjoo Jeong and Dennis Shasha. </author> <title> Persistent linda 2: A transactional/checkpointing approach to fault-tolerant linda. </title> <booktitle> In Proceedings of the 13th Symposium on Fault-Tolerant Distributed Systems. IEEE, </booktitle> <month> October </month> <year> 1994. </year> <month> 27 </month>
Reference-contexts: 1 Introduction Persistent Linda (PLinda) is a programming environment for writing fault-tolerant distributed/parallel programs that may be run on networks of workstations (NOWs). PLinda is a set of extensions to the Linda parallel programming model <ref> [16] </ref> and PLinda/C++ (Fortran77 respectively) is an implementation combined with the sequential language C++ (Fortran77 respectively). For the reader who is not familiar with parallel programming and/or the Linda model we introduce these in sections 2 and 3 respectively.
Reference: [17] <author> D. Kaminsky. </author> <title> Adaptive Parallelism with Piranha. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1994. </year>
Reference-contexts: This is mainly because it minimizes the work of the server which is especially important in the current centralized design. 5 These criteria have already been used by other work-stealing systems and demonstrated to be effective <ref> [1, 6, 3, 7, 9, 17, 19, 20, 21] </ref>. 6 The current PLinda translator is only a preprocessor written in Perl and requires the programmer to annotate PLinda operations with variable type information explicitly. 10 6 Tuning the execution of PLinda programs Recall that PLinda allows the programmer to control the <p> Common idleness criterion are: * Keyboard, mouse and console idle times. * Load average. * The number of users logged on. * Remote-login-session idle time. Besides these, there are other criterion such as time of day which are used relatively less often. See <ref> [17] </ref> for a comprehensive description of idleness criterion. Currently, the PLinda system uses keyboard, mouse, and console idle times, but we plan to add other criterion in the near future. There are two approaches to scheduling on distributed systems: centralized and decentralized. <p> Therefore, scheduling multiple parallel applications is crucial to effectively utilizing idle workstations. Also, both throughput and turnaround time can benefit because most parallel applications do not exhibit linear speedup; that is, they are more efficient on a smaller number of processors <ref> [17] </ref>. The PLinda system is designed to support multiple applications. Process migration is intended to move processes from overloaded or busy machines or to under-utilized or idle machines. In PLinda, process migration and process failure-resiliency are treated uniformly.
Reference: [18] <author> V.S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: In the case of no common file systems, we could just keep all the executables in the same directory path. This kind of a scheme has already been proposed and used in PVM <ref> [18, 8] </ref>. Data conversion between machines of different types can be done either by using a common external data representation, as in Sun's XDR routines, or by converting data in place. We have implemented both schemes.
Reference: [19] <author> C.A. Waldspurger, T. Hogg, B.A. Huberman, J.O. Kephart, and W.S. Stornetta. Spawn: </author> <title> A distributed computational economy. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(2) </volume> <pages> 103-117, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This is mainly because it minimizes the work of the server which is especially important in the current centralized design. 5 These criteria have already been used by other work-stealing systems and demonstrated to be effective <ref> [1, 6, 3, 7, 9, 17, 19, 20, 21] </ref>. 6 The current PLinda translator is only a preprocessor written in Perl and requires the programmer to annotate PLinda operations with variable type information explicitly. 10 6 Tuning the execution of PLinda programs Recall that PLinda allows the programmer to control the
Reference: [20] <author> Jingwen Wang, Songnian Zhou, Khalid Ahmed, and Weihong Long. LSBATCH: </author> <title> A distributed load sharing batch system. </title> <type> Technical Report CSRI-286, </type> <institution> University of Toronto, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: This is mainly because it minimizes the work of the server which is especially important in the current centralized design. 5 These criteria have already been used by other work-stealing systems and demonstrated to be effective <ref> [1, 6, 3, 7, 9, 17, 19, 20, 21] </ref>. 6 The current PLinda translator is only a preprocessor written in Perl and requires the programmer to annotate PLinda operations with variable type information explicitly. 10 6 Tuning the execution of PLinda programs Recall that PLinda allows the programmer to control the
Reference: [21] <author> Songnian Zhou, Jingwen Wang, Xiaohu Zheng, and Pierre Delisle. </author> <title> Utopia: A load sharing facility for large, heterogeneous distributed computer systems. </title> <type> Technical Report CSRI-257, </type> <institution> University of Toronto, </institution> <month> April </month> <year> 1992. </year> <month> 28 </month>
Reference-contexts: This is mainly because it minimizes the work of the server which is especially important in the current centralized design. 5 These criteria have already been used by other work-stealing systems and demonstrated to be effective <ref> [1, 6, 3, 7, 9, 17, 19, 20, 21] </ref>. 6 The current PLinda translator is only a preprocessor written in Perl and requires the programmer to annotate PLinda operations with variable type information explicitly. 10 6 Tuning the execution of PLinda programs Recall that PLinda allows the programmer to control the
References-found: 21

