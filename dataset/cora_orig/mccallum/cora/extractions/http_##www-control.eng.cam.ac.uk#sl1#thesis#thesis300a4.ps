URL: http://www-control.eng.cam.ac.uk/sl1/thesis/thesis300a4.ps
Refering-URL: http://www-control.eng.cam.ac.uk/Homepage/Papers.html
Root-URL: 
Title: ROBUST CONTROL SYNTHESIS IN THE TIME DOMAIN  
Author: Sanjay Lall Jesus 
Affiliation: College, Cambridge  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> B. D. O. Anderson. </author> <title> Internal and external stability of linear time varying systems. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 20 </volume> <pages> 408-413, </pages> <year> 1982. </year>
Reference-contexts: Note that this definition depends on the homogeneous part of the system only, and we will use the phrase "A is stable" also to mean that G is stable. The following lemma is well known. Related results are given in <ref> [74, 1, 65] </ref>. Lemma 2.2. <p> The system here is taken from [29, p. 299]. The finite horizon is <ref> [0; 1] </ref>, and the final conditions are all multiples of the infinite horizon stabilizing solution.
Reference: [2] <author> B. D. O. Anderson and J. B. Moore. </author> <title> Optimal Control Linear Quadratic Methods. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: There are also games which have neither value nor saddle point. For example, the zero sum game with cost function J (u; w) = (u w) 2 defined for u; w 2 <ref> [0; 2] </ref> R has J = 1 and J = 0, and hence does not have a value. An example of such a game in the dynamic case is given by Stoorvogel [63, p. 123]. The following result will be useful in the sequel. Theorem 3.6. <p> The problem when F = 1, that is when the terminal state is constrained to be zero was considered by Tadmor [68]. The techniques we will use are similar to those in [45]. We will first need the following Lemma. Related results can be found in <ref> [45, 2, 59, 17, 57, 7] </ref>. Lemma 6.3. Suppose F satisfies _ F + (A B 2 D 0 12 C 1 ) 2 fl 2 B 1 B 0 1 (I D 12 D 0 and F (t) 0 for all t 0.
Reference: [3] <author> B. A. Bamieh and J. B. Pearson. </author> <title> A general framework for linear periodic systems with applications to h-infinity sampled-data control. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 37(4) </volume> <pages> 418-435, </pages> <year> 1992. </year>
Reference-contexts: We would like a synthesis procedure which, using a continuous time performance and robustness measure, gives us the achievable performance level using a discrete controller with a given sampling rate. Hara and Kabamba [31], and Bamieh and Pearson <ref> [3] </ref>, have given H 1 synthesis procedures for sampled-data systems using the technique of lifting. <p> Several authors have given synthesis techniques for construction of H 1 suboptimal controllers for sampled-data systems, in particular solutions to the infinite horizon time invariant problem are given by Hara and Kabamba [31] and Bamieh and Pearson <ref> [3] </ref>, by constructing equivalent discrete time systems. Further simplifications to this solution are given by Chen and Francis in [19]. These results are derived using lifting techniques, where the original hybrid system is transformed to one on a discrete operator-valued space.
Reference: [4] <author> T. Ba~sar. </author> <title> Disturbance attenuation in LTI plants with finite horizon: optimality of nonlinear controllers. </title> <journal> Systems and Control Letters, </journal> <volume> 13 </volume> <pages> 183-191, </pages> <year> 1989. </year>
Reference: [5] <author> T. Ba~sar. </author> <title> Generalized riccati equations in dynamic games. </title> <editor> In S. Bittanti, A. Laub, and J. Willems, editors, </editor> <title> The Riccati Equation. </title> <publisher> Springer Verlag, </publisher> <year> 1991. </year>
Reference: [6] <author> T. Ba~sar. </author> <title> Optimum H 1 designs under sampled state measurements. </title> <journal> Systems and Control Letters, </journal> <volume> 16 </volume> <pages> 399-409, </pages> <year> 1991. </year>
Reference-contexts: These results are derived using lifting techniques, where the original hybrid system is transformed to one on a discrete operator-valued space. The finite horizon time varying problem has also been considered, notably by Ba~sar and Bernhard [7]. Further solutions to this problem are considered in <ref> [6, 67, 64, 11] </ref>. The above results all consider systems with single rate periodic sampling and hold devices synchronized with each other. In this chapter we will be considering sampled-data systems which are both multi-rate and asynchronous.
Reference: [7] <author> T. Ba~sar and P. Bernhard. </author> <title> H 1 Optimal Control and Related Minimax Design Problems. A Dynamic Game Approach. Systems and Control: Foundations and Applications. </title> <publisher> Birkhauser, </publisher> <year> 1991. </year>
Reference-contexts: We show that the H 1 problem is equivalent to a dynamic game problem over certain strategy spaces. In the output feedback case, we consider 1.1 Organisation of the thesis 5 the separation theories of Whittle [78] and Ba~sar and Bernhard <ref> [7] </ref>, and also the use of information state for separation as developed by James, Baras and Elliott [40]. We give a new simple proof of the separation principle for the continuous time output feedback problem, and an explicit characterization of the discrete systems for which separation can be applied. <p> We will concentrate on the game theoretic formulations, and will use both the nonlinear concept of information state, first used in H 1 theory by James and Baras [40], and the idea of worst case disturbances, as developed by Ba~sar and Bernhard <ref> [7] </ref> and Whittle [78]. Throughout this thesis we will consider what has become known as the "standard" H 1 synthesis problem. That is, we are given a causal linear system G mapping inputs w; u to outputs z; y. <p> The following theorem can be found in <ref> [7, 8] </ref>. Theorem 3.13. Suppose there exists a continuously differentiable function V (t; x (t)) satisfying (3.10) for t 2 [0; t f ], and at each t there exists a saddle point solution to the corresponding static game in equation (3.10). <p> The certainty equivalence principles that have been derived by various authors <ref> [78, 38, 7] </ref> do not depend on the linear character of the problem, and can be easily described in the nonlinear framework. We will then, in the next section, go on to specialize these results to the linear case. <p> = sup (x 0 ;w t )2 (t;x) ff (x 0 ) + t 0 = sup sup Z t g t; x (t); u t (t); w t (t) dt + V t; x (t ) (x 0 ;w t )2 0 (t) The following theorem is stated in <ref> [7] </ref>. Theorem 3.16 (Ba~sar and Bernhard [7]). Suppose V is a smooth solution of the Isaacs equation (3.16), and that 0 is the infimising state feedback control law for the u player. <p> )2 (t;x) ff (x 0 ) + t 0 = sup sup Z t g t; x (t); u t (t); w t (t) dt + V t; x (t ) (x 0 ;w t )2 0 (t) The following theorem is stated in <ref> [7] </ref>. Theorem 3.16 (Ba~sar and Bernhard [7]). Suppose V is a smooth solution of the Isaacs equation (3.16), and that 0 is the infimising state feedback control law for the u player. <p> This worst case disturbance defines a unique worst case state trajectory, which should be used in the state feedback law as if it were the actual state. In <ref> [7] </ref> the theorem is proved using techniques from dynamic programming. It is stated that the result holds not only for the linear quadratic case but also for nonlinear and nonquadratic problems. <p> Ba~sar and Bernhard <ref> [7] </ref> claim that, for general linear systems, W is a decreasing function of t for all y given uniqueness of the maximum of G for each t . <p> The problem is that in general p (j) is described by an infinite dimensional evolution equation. Then V is a difference equation defined on a space of functions, which appears extremely difficult to solve at present. However, Ba~sar and Bernhard <ref> [7] </ref>, give a separation principle for this problem, which replaces the infinite dimensional recursion (3.38) for V by the finite dimensional recursion for V , the value of the game. Bernhardsson [12] gives a counterexample to this principle in the general nonlinear case. <p> C 1 (k) 0 C 1 (k) with boundary condition ~ X 1 (k f + 1) = Q f : satisfies fl 2 I B 1 (k) 0 ~ X 1 (k)B 1 (k) &gt; 0 for all k 2 [k 0 ; k f ] (see for example <ref> [7] </ref>). <p> Note that this is also a necessary condition for the existence of a saddle point in the state feedback problem also, see for example <ref> [7, Theorem 3.2] </ref>. <p> We have given an overview of the certainty equivalence principle derived by Whittle [78] and Ba~sar and Bernhard <ref> [7] </ref>, and given a new explicit derivation of this for the linear-quadratic continuous time finite horizon output feedback H 1 problem. In the discrete problem, we have used the information state ideas of James, Baras and Elliott [40] to construct a solution to the one-step-delayed output feedback problem. <p> Further simplifications to this solution are given by Chen and Francis in [19]. These results are derived using lifting techniques, where the original hybrid system is transformed to one on a discrete operator-valued space. The finite horizon time varying problem has also been considered, notably by Ba~sar and Bernhard <ref> [7] </ref>. Further solutions to this problem are considered in [6, 67, 64, 11]. The above results all consider systems with single rate periodic sampling and hold devices synchronized with each other. In this chapter we will be considering sampled-data systems which are both multi-rate and asynchronous. <p> We now derive the necessary conditions for existence of a bounded upper value for the dynamic game, and hence those for existence of a fl-feasible state feedback controller. 4.2.3 Necessary conditions for state feedback control We will make use of the following result, from Ba~sar and Bernhard <ref> [7, Theorem 8.3] </ref>. Theorem 4.2. Let A, B 1 and Q be bounded matrix functions on the interval [t 0 ; t f ], and suppose Q (t) &gt; 0 for all t 2 [t 0 ; t f ]. <p> The problem when F = 1, that is when the terminal state is constrained to be zero was considered by Tadmor [68]. The techniques we will use are similar to those in [45]. We will first need the following Lemma. Related results can be found in <ref> [45, 2, 59, 17, 57, 7] </ref>. Lemma 6.3. Suppose F satisfies _ F + (A B 2 D 0 12 C 1 ) 2 fl 2 B 1 B 0 1 (I D 12 D 0 and F (t) 0 for all t 0. <p> Proof. This is proved in <ref> [7, Proposition 8.5] </ref>. Lemma 6.23. <p> Proof. A proof of continuity for general nonlinear equations satisfying a local Lipschitz condition is in Sontag [61]. For specific details for finite horizon Riccati equations, see also Ba~sar and Bernhard <ref> [7, Chapter 8] </ref>. Lemma 6.24. <p> CONCLUSIONS By way of conclusion we summarise the main contributions of this thesis. * In chapter 3, we have given an explicit derivation of the certainty equivalence principle developed by Whittle [78] and Ba~sar and Bernhard <ref> [7] </ref> for the finite horizon continuous time linear differential game with a quadratic cost function. * We have given necessary and sufficient conditions for the existence of state feedback controllers which attenuate the induced L 2 norm of a multi-rate system with jumps to less than some prespecified level fl on
Reference: [8] <author> T. Ba~sar and G. J. Olsder. </author> <title> Dynamic Noncooperative Game Theory. </title> <booktitle> Mathematics in Science and Engineering. </booktitle> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: We then solve the finite horizon time varying problem in the state and output feedback cases. 3.2 Zero-sum games We will now describe a broad class of problems called zero-sum games. Most of the following material is standard and can be found in <ref> [8] </ref>, for example. Let U and W be vector spaces, and U 1 U , W 1 W be subsets. Let J : U 1 fi W 1 ! R. <p> Equation (3.5) was derived in the early 1950's by Rufus Isaacs [35] in continuous time. The following theorem can be found in <ref> [8] </ref>. Theorem 3.12. Suppose there exists a function V satisfying (3.5) for 0 k K, and at each k there exists a unique saddle point solution to the static game described by equation (3.6). <p> The following theorem can be found in <ref> [7, 8] </ref>. Theorem 3.13. Suppose there exists a continuously differentiable function V (t; x (t)) satisfying (3.10) for t 2 [0; t f ], and at each t there exists a saddle point solution to the corresponding static game in equation (3.10).
Reference: [9] <author> G. Becker, A. Packard, D. Philbrick, and G. Balas. </author> <title> Control of parametrically dependent linear systems: A single quadratic lyapunov approach. </title> <booktitle> In Proceedings of the 1993 American Control Conference, Baltimore, </booktitle> <pages> pages 2795-2799, </pages> <year> 1993. </year>
Reference: [10] <author> P. Bernhard. </author> <title> Linear-quadratic, two-person, zero-sum differential games: necessary and sufficient conditions. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 27(1) </volume> <pages> 51-69, </pages> <month> January </month> <year> 1979. </year>
Reference: [11] <author> P. Bernhard. </author> <title> Application of the min-max certainty equivalence principle to the sampled data output feedback H 1 control problem. </title> <journal> Systems and Control Letters, </journal> <volume> 16 </volume> <pages> 229-234, </pages> <year> 1991. </year>
Reference-contexts: These results are derived using lifting techniques, where the original hybrid system is transformed to one on a discrete operator-valued space. The finite horizon time varying problem has also been considered, notably by Ba~sar and Bernhard [7]. Further solutions to this problem are considered in <ref> [6, 67, 64, 11] </ref>. The above results all consider systems with single rate periodic sampling and hold devices synchronized with each other. In this chapter we will be considering sampled-data systems which are both multi-rate and asynchronous.
Reference: [12] <author> Bo Bernhardsson. </author> <title> Topics in Digital and Robust Control of Linear Systems. </title> <type> PhD thesis, </type> <institution> Lund Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: It is stated that the result holds not only for the linear quadratic case but also for nonlinear and nonquadratic problems. Note that in discrete time, the corresponding results to Theorem 3.15 and Theorem 3.16 are not equivalent, and the certainty equivalence controller may not be optimal. Bernhardsson <ref> [12, p. 66] </ref> gives a counterexample to the discrete time nonlinear-nonquadratic case certainty equivalence principle. Also James [40] gives conditions for when the certainty equivalence principle is optimal in discrete time. The certainty equivalence principle above can be proved in the following way: Theorem 3.17. <p> However, Ba~sar and Bernhard [7], give a separation principle for this problem, which replaces the infinite dimensional recursion (3.38) for V by the finite dimensional recursion for V , the value of the game. Bernhardsson <ref> [12] </ref> gives a counterexample to this principle in the general nonlinear case. Following James [37], we shall now derive this separation principle from the information state formulation, and see that a particular saddle point condition is necessary for it to hold. Theorem 3.31.
Reference: [13] <author> R. R. Bitmead, M. Gevers, and V. Wertz. </author> <title> Adaptive Optimal Control. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Counterexamples to this, in the discrete time case for the regulator problem, are given in <ref> [13] </ref>, and it is straightforward to construct similar examples in the continuous time case. fl fl 0 0:4 0:8 1:2 Interval length A = 4 0 0:0100 15:0000 3 2 9:3969 0 0 0 5 2 6:9500 0 5 C 1 = 0 0 0 0 against horizon length for a
Reference: [14] <author> B. Bollobas. </author> <title> Linear Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1990. </year> <note> 131 Bibliography 132 </note>
Reference-contexts: PRELIMINARIES 2.1 Signals and Systems We begin with this section which describes the basic continuous and discrete time systems we will encounter throughout this thesis. The following material is standard and can be found, for example, in <ref> [14] </ref>. Let R n be n-dimensional Euclidean space, with norm jj defined by jxj 2 = P n where x 2 R n is given by x = (x 1 ; : : : ; x n ).
Reference: [15] <author> S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. </author> <title> Linear Matrix inequalities in system and control theory. </title> <publisher> SIAM, </publisher> <year> 1994. </year>
Reference-contexts: In fact, it is easy to see <ref> [15, p. 28] </ref> that S 0 R 0 if and only if R 0 Q SR y S 0 0 S (I RR y ) = 0 and so in general the nonstrict LMI is a stronger condition than the nonstrict Riccati inequality.
Reference: [16] <author> R. W. Brockett. </author> <title> Finite Dimensional Linear Systems. </title> <publisher> Wiley, </publisher> <year> 1970. </year>
Reference-contexts: Since A is bounded, the right hand side of this ordinary differential equation is a globally Lipschitz function of x (t), and hence a solution always exists for all t t 0 . Proofs of these results can be found in, for example <ref> [61, 16] </ref>. Then define , the transition matrix of A by @t The solution of (2.1) is then given by x (t) = (t; t 0 )x (t 0 ) 2.2 Linear Operators 8 Useful properties of . <p> Note that although R = X 1 1 , we cannot simply apply Lemma 6.2, since we specifically want to apply this to the case Q 2 6&gt; 0. In order to do this, we use the results of Brockett <ref> [16, p. 130] </ref> on linear quadratic regulation with an indefinite cost on the state.
Reference: [17] <author> R. S. Bucy. </author> <title> Global theory of the riccati equation. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 1 </volume> <pages> 349-361, </pages> <year> 1967. </year>
Reference-contexts: The problem when F = 1, that is when the terminal state is constrained to be zero was considered by Tadmor [68]. The techniques we will use are similar to those in [45]. We will first need the following Lemma. Related results can be found in <ref> [45, 2, 59, 17, 57, 7] </ref>. Lemma 6.3. Suppose F satisfies _ F + (A B 2 D 0 12 C 1 ) 2 fl 2 B 1 B 0 1 (I D 12 D 0 and F (t) 0 for all t 0.
Reference: [18] <author> C. C. Chen and L. Shaw. </author> <title> On receding horizon feedback control. </title> <journal> Automatica, </journal> <volume> 18(3) </volume> <pages> 349-352, </pages> <year> 1982. </year>
Reference-contexts: The techniques in his paper were later reformulated and generalized by various authors, see for example <ref> [18, 46, 45] </ref>. Kwon and Pearson [46] proved stability for linear time varying systems using a controller which optimized a quadratic cost function integrated over a time interval from the current time t to a fixed distance ahead t + T .
Reference: [19] <author> T. Chen and B. A. Francis. </author> <title> H 1 optimal sampled-data control: computation and design. </title> <booktitle> In Proceedings of the 1994 American Control Conference, Baltimore, </booktitle> <pages> pages 2767-2771, </pages> <year> 1994. </year>
Reference-contexts: Further simplifications to this solution are given by Chen and Francis in <ref> [19] </ref>. These results are derived using lifting techniques, where the original hybrid system is transformed to one on a discrete operator-valued space. The finite horizon time varying problem has also been considered, notably by Ba~sar and Bernhard [7]. Further solutions to this problem are considered in [6, 67, 64, 11].
Reference: [20] <author> T. Chen and L. Qiu. </author> <title> H 1 design of general multirate sampled-data control systems. </title> <type> Technical Report 1090, </type> <institution> Institute for mathematics and its applications, University of Minnesota, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Again we will use as performance measure an induced 2-norm. It will be natural for us to consider some disturbances as discrete and others as continuous. Multi-rate systems have been considered by several authors, notably Chen and Qiu <ref> [20] </ref> and Voulgaris [76]. In these papers, the sampling and hold devices are assumed to have rational rates, and to be synchronized with each other, so that the system can be lifted up to either the fastest or the slowest commensurate period.
Reference: [21] <author> M. Dahleh and I. J. Diaz-Bobillo. </author> <title> Control of Uncertain Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: A good discussion of the techniques associated with this methodology can be found in, for example, Dahleh and Diaz-Bobillo <ref> [21] </ref>. This stated H 1 system methodology, when transferred to the moving horizon context, brings problems.
Reference: [22] <author> J. C. Doyle, K. Glover, P. P. Khargonekar, and B. A. Francis. </author> <title> State-space solutions to standard H 2 and H 1 control problems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34(8) </volume> <pages> 831-847, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Chapter 5: Riccati differential inequalities In this chapter we present a solution of the continuous time infinite horizon linear time varying H 1 synthesis problem, using the separation theory of Doyle, Glover, Khar-gonekar and Francis <ref> [22] </ref>. We extend the results of Khargonekar, Ravi and Nagpal [43] to replace the Riccati differential equations with Riccati differential inequalities, and remove a particular stability assumption on the solutions to these inequalities. We then obtain similar results using game theoretic techniques, by perturbing the past and future cost functions. <p> Further, if the original system matrices are continuous functions of time, then the transformed matrices will also be continuous. For details of these loopshifting transformations, see for example [50, 29]. Also, following <ref> [22] </ref> we will make the additional standing assumptions that D 21 (t)B 1 (t) 0 = 0 D 12 (t) 0 C 1 (t) = 0 (3.20) for all t 2 [t 0 ; t f ]. <p> Indeed, we know that, in the continuous time case, the controller which minimizes the upper value of the differential game is far from the only fl-suboptimal H 1 controller. Indeed, the "central" controller of <ref> [22] </ref> is not this controller. One viewpoint on this nonuniqueness is that, for the induced 2-norm problem, we do not really need to minimize the upper value of the differential game. All we need to do is ensure boundedness of the upper value. <p> We make use of the separation theory of Doyle, Glover, Khargonekar and Francis <ref> [22] </ref>, also known as DGKF, and we also separately derive a solution in terms of inequalities using the game theory of Chapter 3. <p> matrix function Q satisfies, for all t 0, b) There exists fi &gt; 0 such that Q (t) fiI c) Q (t) 0 then the system _x (t) = A (t)x (t) is exponentially stable. 5.2 Inequalities via DGKF separation In this section we use the separation theory of DGKF <ref> [22] </ref> to give a solution to the infinite horizon time varying H 1 problem. In particular, our solution technique is essentially the same as that of Khargonekar, Ravi and Nagpal [43]. <p> In the proof, we will use the separation theory of <ref> [22] </ref>, and essentially follow [43] for the time varying case. Theorem 5.15. Let G be a system of the form (5.6) satisfying the above assumptions. <p> We use the by now standard change of variables, first introduced in <ref> [22] </ref>, setting v = u C k x and r = w fl 2 B 0 1 Xx. <p> ~ D 12 u where ~ C 1 = C 1 1 0 : then X satisfies the `fake' algebraic Riccati equation (A B 2 12 ~ D 0 ~ C 1 ) 2 fl 2 B 1 B 0 1 (I ~ D 12 ~ D 0 Then, from <ref> [22] </ref>, since X is a stabilising solution to this equation, with the controller given by equation (6.9) the closed loop norm from w to ~z must be strictly less than fl.
Reference: [23] <author> G. E. Dullerud. </author> <title> Control of uncertain sampled-data systems. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1994. </year>
Reference: [24] <author> P. Gahinet. </author> <title> Explicit controller formulas for LMI-based H 1 synthesis. </title> <booktitle> In Proceedings of the 1994 American Control Conference, Baltimore, </booktitle> <pages> pages 2396-2400, </pages> <year> 1994. </year>
Reference: [25] <author> P. Gahinet. </author> <title> A new parametrization of H 1 suboptimal controllers. </title> <journal> International Journal of Control, </journal> <volume> 59(4) </volume> <pages> 1031-1051, </pages> <year> 1994. </year>
Reference: [26] <author> P. Gahinet and P. Apkarian. </author> <title> Linear matrix inequality approach to H 1 control. </title> <journal> International Journal of Robust and Nonlinear Control, </journal> <volume> 4(4) </volume> <pages> 421-448, </pages> <year> 1994. </year>
Reference-contexts: See <ref> [26] </ref> for more details on this approach.
Reference: [27] <author> I. M. Gelfand and S. V. Fomin. </author> <title> Calculus of Variations. </title> <publisher> Prentice Hall, </publisher> <year> 1963. </year>
Reference: [28] <author> K. Glover and J. C. Doyle. </author> <title> State-space formulae for all stabilizing controllers that satisfy an H 1 -norm bound and relations to risk sensitivity. </title> <journal> Systems and Control Letters, </journal> <volume> 11 </volume> <pages> 167-172, </pages> <year> 1988. </year>
Reference-contexts: We will not pursue this further here, but suggest it as an opportunity for future research. 5. RICCATI DIFFERENTIAL INEQUALITIES The solution to the H 1 problem for the time invariant infinite horizon case by Glover and Doyle <ref> [28] </ref> was characterised in terms of two algebraic Riccati equations. The subsequent solution of the time varying problem was similarly characterised by Khar-gonekar, Ravi, Nagpal [43] in terms of two Riccati differential equations. However, in the time varying case these solutions are not so straightforward to apply.
Reference: [29] <author> M. Green and D. J. N. Limebeer. </author> <title> Linear Robust Control. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: Further, if the original system matrices are continuous functions of time, then the transformed matrices will also be continuous. For details of these loopshifting transformations, see for example <ref> [50, 29] </ref>. Also, following [22] we will make the additional standing assumptions that D 21 (t)B 1 (t) 0 = 0 D 12 (t) 0 C 1 (t) = 0 (3.20) for all t 2 [t 0 ; t f ]. <p> The open loop poles of this system are 0:0050 + 6:7082i, 0:0050 6:7082i, and 0:0100, and the open loop frequency response is shown in Figure 6.2. The system is taken from a spring damper example in <ref> [29] </ref>. The resonance shown has a period of about 0:93 seconds. <p> Further, since we are integrating backwards, given any specific final value Q f , any trajectory of X which increases sufficiently fast backwards in time will be a solution of the inequality, and hence will provide a fl-feasible H 1 controller. system. The system here is taken from <ref> [29, p. 299] </ref>. The finite horizon is [0; 1], and the final conditions are all multiples of the infinite horizon stabilizing solution.
Reference: [30] <author> J. K. Hale. </author> <title> Ordinary differential equations. </title> <publisher> Wiley, </publisher> <year> 1969. </year>
Reference-contexts: The first of these equations is simply equation (4.21), and by assumption this has a bounded solution. The next two are then linear equations with bounded time varying coefficients, and hence have well defined solutions on any finite interval, see for example <ref> [30] </ref>. Therefore, existence of a solution to (4.21) is sufficient for existence of a solution to (4.22). We now need to apply Theorem 4.5 to the special case of the system specified by equations (4.8).
Reference: [31] <author> S. Hara and P. T. Kabamba. </author> <title> Worst case analysis and design of sampled-data control systems. </title> <booktitle> In Proceedings of the 29th IEEE CDC, </booktitle> <year> 1990. </year> <note> Bibliography 133 </note>
Reference-contexts: We would like a synthesis procedure which, using a continuous time performance and robustness measure, gives us the achievable performance level using a discrete controller with a given sampling rate. Hara and Kabamba <ref> [31] </ref>, and Bamieh and Pearson [3], have given H 1 synthesis procedures for sampled-data systems using the technique of lifting. <p> Again, for such a system we are looking for a systematic method to design robust controllers. Several authors have given synthesis techniques for construction of H 1 suboptimal controllers for sampled-data systems, in particular solutions to the infinite horizon time invariant problem are given by Hara and Kabamba <ref> [31] </ref> and Bamieh and Pearson [3], by constructing equivalent discrete time systems. Further simplifications to this solution are given by Chen and Francis in [19]. These results are derived using lifting techniques, where the original hybrid system is transformed to one on a discrete operator-valued space.
Reference: [32] <author> Y. C. Ho. </author> <title> Differential games, dynamic optimization and generalized control theory. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 6(3) </volume> <pages> 179-209, </pages> <year> 1970. </year>
Reference: [33] <author> Y. C. Ho. </author> <title> On the minimax principle and zero-sum stochastic differential games. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 13(3) </volume> <pages> 343-361, </pages> <year> 1974. </year>
Reference: [34] <author> R. A. Horn and C. R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference: [35] <author> R. Isaacs. </author> <title> Differential Games. </title> <publisher> Kruger Publishing Company, </publisher> <address> Huntington, NY, </address> <year> 1965. </year>
Reference-contexts: Hence, at time K, we know that the future cost is zero, and so we can calculate exactly the optimal current inputs as a function of the current state. Equation (3.5) was derived in the early 1950's by Rufus Isaacs <ref> [35] </ref> in continuous time. The following theorem can be found in [8]. Theorem 3.12. Suppose there exists a function V satisfying (3.5) for 0 k K, and at each k there exists a unique saddle point solution to the static game described by equation (3.6).
Reference: [36] <author> T. Iwasaki and R. E. Skelton. </author> <title> All controllers for the general H 1 control problem: LMI existence conditions and state space formulas. </title> <journal> Automatica, </journal> <volume> 30(8) </volume> <pages> 1307-1317, </pages> <year> 1994. </year>
Reference: [37] <author> M. R. James. </author> <title> On the certainty equivalence principle and the optimal control of partially observed dynamic games. </title> <type> Preprint, </type> <year> 1994. </year>
Reference-contexts: For each t , the information state is a functional on R n . Note that, for linear systems, the information state is a nonhomogeneous quadratic function of the ~, see <ref> [37, 78] </ref>. Whittle [78] calls this function the past stress. The certainty equivalence principle. <p> However, Ba~sar and Bernhard [7], give a separation principle for this problem, which replaces the infinite dimensional recursion (3.38) for V by the finite dimensional recursion for V , the value of the game. Bernhardsson [12] gives a counterexample to this principle in the general nonlinear case. Following James <ref> [37] </ref>, we shall now derive this separation principle from the information state formulation, and see that a particular saddle point condition is necessary for it to hold. Theorem 3.31.
Reference: [38] <author> M. R. James and J. S. Baras. </author> <title> Partially observed differential games, infinite dimensional HJI equations, and nonlinear H 1 control. </title> <type> Preprint, </type> <month> August </month> <year> 1994. </year>
Reference-contexts: The certainty equivalence principles that have been derived by various authors <ref> [78, 38, 7] </ref> do not depend on the linear character of the problem, and can be easily described in the nonlinear framework. We will then, in the next section, go on to specialize these results to the linear case. <p> The quantity P (t; ~) is the worst possible cost that could have been incurred so far if the current state were equal to ~, consistent with the information given by knowledge of u t and y t . It is shown in <ref> [38] </ref> that the function P satisfies the Hamilton-Jacobi equation @ P (t; ~) = sup y t (t)=h (t;~;w t (t)) g t; ~; u (t ); w (t ) @~ (3.17) 3.5 Continuous time games with imperfect measurement 24 with the boundary condition P (t 0 ; x) = ff <p> P (t; ~) = sup y t (t)=h (t;~;w t (t)) g t; ~; u (t ); w (t ) @~ (3.17) 3.5 Continuous time games with imperfect measurement 24 with the boundary condition P (t 0 ; x) = ff (x) 8x 2 R n : James and Baras <ref> [38] </ref> describe P (t; ) as an infinite dimensional state, and the above partial differential equation can then be viewed as the state evolution equation for this new state. For each t , the information state is a functional on R n . <p> Whittle [78] calls this function the past stress. The certainty equivalence principle. The following theorem, the discrete version of which was first formulated by Whittle [78], and which was proved in this form for the continuous time problem by James and Baras <ref> [38] </ref> gives us the desired certainty equivalence principle. Theorem 3.15. Suppose V is a smooth solution of the Isaacs equation (3.16) with the infimising saddle point solution for u given by 0 , and P is a smooth solution of the Hamilton-Jacobi equation (3.17). <p> This theorem is similar to one first introduced by Whittle [78] for the discrete time linear-quadratic problem. It is proved for the continuous time nonlinear problem in <ref> [38, 40] </ref> using arguments from dynamic programming, and there are several stringent technical assumptions regarding smoothness that must be made. <p> In this sense this is a certainty equivalence principle; we are using a worst case state as if it were the actual state. It is also shown in <ref> [38] </ref> that the problem can be expressed in the following way.
Reference: [39] <author> M. R. James and J. S. Baras. </author> <title> Robust H 1 output feedback control for nonlinear systems. </title> <type> Preprint, </type> <year> 1994. </year>
Reference: [40] <author> M. R. James, J. S. Baras, and R. J. Elliott. </author> <title> Risk-sensitive control and dynamic games for partially observed discrete time nonlinear systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 39(4) </volume> <pages> 780-792, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In the output feedback case, we consider 1.1 Organisation of the thesis 5 the separation theories of Whittle [78] and Ba~sar and Bernhard [7], and also the use of information state for separation as developed by James, Baras and Elliott <ref> [40] </ref>. We give a new simple proof of the separation principle for the continuous time output feedback problem, and an explicit characterization of the discrete systems for which separation can be applied. <p> We will concentrate on the game theoretic formulations, and will use both the nonlinear concept of information state, first used in H 1 theory by James and Baras <ref> [40] </ref>, and the idea of worst case disturbances, as developed by Ba~sar and Bernhard [7] and Whittle [78]. Throughout this thesis we will consider what has become known as the "standard" H 1 synthesis problem. <p> This theorem is similar to one first introduced by Whittle [78] for the discrete time linear-quadratic problem. It is proved for the continuous time nonlinear problem in <ref> [38, 40] </ref> using arguments from dynamic programming, and there are several stringent technical assumptions regarding smoothness that must be made. <p> Note that in discrete time, the corresponding results to Theorem 3.15 and Theorem 3.16 are not equivalent, and the certainty equivalence controller may not be optimal. Bernhardsson [12, p. 66] gives a counterexample to the discrete time nonlinear-nonquadratic case certainty equivalence principle. Also James <ref> [40] </ref> gives conditions for when the certainty equivalence principle is optimal in discrete time. The certainty equivalence principle above can be proved in the following way: Theorem 3.17. <p> We will now give a review of the work done by James, Baras and Elliott <ref> [40] </ref> on the information state approach to constructing solutions to the problem inf sup x 0 2R n where the set of strictly causal discrete time output feedback strategies, U of , is defined by p;m p 2 [k 0 ; k 1 ] fi Definition 3.24. <p> In the discrete problem, we have used the information state ideas of James, Baras and Elliott <ref> [40] </ref> to construct a solution to the one-step-delayed output feedback problem. This derivation is not new, but the ideas involved will be used heavily in Chapter 4 to construct solutions to the multi-rate sampled-data problem. 4.
Reference: [41] <author> J. P. Keller and B. D. O. Anderson. </author> <title> A new approach to the discretisation of continuous-time controllers. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 37(2) </volume> <pages> 214-223, </pages> <year> 1992. </year>
Reference-contexts: Often, the assumption is made that by sampling at a fast enough rate it is possible to simply approximate a desired continuous controller with such a discrete controller. This natural assumption has been shown to be justified in practice, and is also given some theoretical justification by <ref> [41] </ref>. However, we aim in this chapter to give a systematic design procedure for such systems. That is, we wish to know exactly how fast it is necessary to sample in order to achieve some specified level of performance.
Reference: [42] <author> P. P. Khargonekar, K. M. Nagpal, and K. R. Poolla. </author> <title> H 1 control with transients. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 29(6) </volume> <pages> 1373-1393, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: T : (w; x 0 ) 7! (z; x (t f )), giving kT k fl () J (x 0 ; ; w) 0 8 w 2 L 2 ; x 0 2 R n This gives a formulation of H 1 control taking transients into account, as described in <ref> [42] </ref>. Note that if kT k fl, then kT zw k fl also, since equation (3.21) holds for all x 0 , including x 0 = 0. <p> For the finite horizon problem, typically the assumptions made are that the initial state at the beginning of each optimization interval is completely known, or is completely unknown. In the latter case it is treated as part of the disturbance and subject to a quadratic weighting [72] <ref> [42] </ref>. In the receding horizon problem, though, we have observations from before the optimization interval. We first consider the state feedback problem, with the above quadratic cost function replaced by the game theoretic cost function of previous chapters.
Reference: [43] <author> P. P. Khargonekar, R. Ravi, and K. M. Nagpal. </author> <title> H 1 control of linear time varying systems: A state space approach. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 29(6) </volume> <pages> 1394-1413, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Chapter 5: Riccati differential inequalities In this chapter we present a solution of the continuous time infinite horizon linear time varying H 1 synthesis problem, using the separation theory of Doyle, Glover, Khar-gonekar and Francis [22]. We extend the results of Khargonekar, Ravi and Nagpal <ref> [43] </ref> to replace the Riccati differential equations with Riccati differential inequalities, and remove a particular stability assumption on the solutions to these inequalities. We then obtain similar results using game theoretic techniques, by perturbing the past and future cost functions. <p> The following definitions can be found in <ref> [58, 43] </ref>. Definition 2.3. The system G, or the pair (A; B), is said to be stabilizable if there exists a bounded matrix function F such that the system _x (t) = A (t) B (t)F (t) x (t) is exponentially stable. Definition 2.4. <p> RICCATI DIFFERENTIAL INEQUALITIES The solution to the H 1 problem for the time invariant infinite horizon case by Glover and Doyle [28] was characterised in terms of two algebraic Riccati equations. The subsequent solution of the time varying problem was similarly characterised by Khar-gonekar, Ravi, Nagpal <ref> [43] </ref> in terms of two Riccati differential equations. However, in the time varying case these solutions are not so straightforward to apply. <p> In particular, our solution technique is essentially the same as that of Khargonekar, Ravi and Nagpal <ref> [43] </ref>. However, we show that the controller can be constructed using any solution to a pair of Riccati differential inequalities, and remove particular stability requirements on their solutions. 5.2.1 The bounded real lemma Lemma 5.3. <p> In the proof, we will use the separation theory of [22], and essentially follow <ref> [43] </ref> for the time varying case. Theorem 5.15. Let G be a system of the form (5.6) satisfying the above assumptions. <p> To show necessity, we need only show that existence of solutions to the Riccati differential equations is necessary, since these will also satisfy the inequalities. This is proved in the case when B 1 D 0 1 D 12 = 0 by Ravi, Nagpal, and Khargonekar in <ref> [43] </ref>, and generalization to the case with cross terms is not difficult. We would now like to remove the assumptions in Theorem 5.15 that A B 2 D 0 2 fl 2 B 1 B 0 z C z fl 2 C 0 are exponentially stable. <p> We have now derived independent inequalities for Y and Z, rather than for X and Z. Also, the standard controller state description of Section 5.2 depends on the parameters 1 and 2 in this case. In this chapter we have extended the solution of <ref> [43] </ref> to remove the assumptions that the solution to the Riccati differential equations are stabilizing. We have also shown 5.3 Connections to differential games 93 that these equations can be replaced by inequalities. <p> The other is to design fully for time varying systems. In the case of both linear quadratic and H 1 controllers, this requires the backwards integration from infinity of a Riccati equation (see for example <ref> [69, 43] </ref>), and somewhat optimistically assumes knowledge of the system throughout future time. The moving horizon method can be viewed as a compromise between these two methods. <p> that the closed loop system has kT zw k &lt; fl, then there exists a solution to the Riccati differential equation _ F + (A B 2 D 0 12 C 1 ) 2 fl 2 B 1 B 0 1 (I D 12 D 0 This is proved in <ref> [43] </ref>. This implies that, for any given time varying system, if there exists a fl-feasible stabilizing state feedback controller, then there exists a fl-feasible state feedback moving horizon controller. Of course, this statement is in fact not very useful, since the moving horizon controller would be the infinite horizon controller.
Reference: [44] <author> D. L. Kleinman. </author> <title> An easy way to stabilize a linear constant system. </title> <journal> IEEE Transactions on Automatic Control, </journal> <pages> page 692, </pages> <month> December </month> <year> 1970. </year>
Reference-contexts: In this methodology, at each time t the controller is designed for the finite intervals [t; t + T ]. The controller is then used on the possibly infinitesimal interval [t; t + ffi] before being recomputed. This technique was motivated by Kleinman <ref> [44] </ref> and Kwon and Pearson [46] in the early 1970s for use with linear quadratic optimization problems. It was shown that the resulting controllers were stable when applied on an infinite horizon. This allowed the synthesis for linear time varying systems without full future knowledge of the plant parameters. <p> MOVING HORIZON HHHH H 1 CONTROL 6.1 Motivation The moving horizon control technique was developed using the linear-quadratic regulator in the 1970's, with one of the first papers being that by Kleinman <ref> [44] </ref> in 1970. The techniques in his paper were later reformulated and generalized by various authors, see for example [18, 46, 45].
Reference: [45] <author> W. H. Kwon, A. M. Bruckstein, and T. Kailath. </author> <title> Stabilizing state-feedback design via the moving horizon method. </title> <journal> International Journal of Control, </journal> <volume> 37(3) </volume> <pages> 631-643, </pages> <year> 1983. </year>
Reference-contexts: The techniques in his paper were later reformulated and generalized by various authors, see for example <ref> [18, 46, 45] </ref>. Kwon and Pearson [46] proved stability for linear time varying systems using a controller which optimized a quadratic cost function integrated over a time interval from the current time t to a fixed distance ahead t + T . <p> The solution to this problem was given in terms of a Riccati differential equation, integrated backwards from time t + T at each time t. Kwon et. al. <ref> [45] </ref> also formulated a general procedure for the forwards integration of such solutions. This allows a continuous update of the time varying controller. <p> from f (u; w) g (u; w) =) inf sup f (u; w) inf sup g (u; w) 6.2.2 Stability of the receding horizon controller For the linear quadratic problem with a terminal weight, the closed loop was shown to be stable under certain conditions by Kwon, Bruckstein and Kailath <ref> [45] </ref>. The problem when F = 1, that is when the terminal state is constrained to be zero was considered by Tadmor [68]. The techniques we will use are similar to those in [45]. We will first need the following Lemma. <p> weight, the closed loop was shown to be stable under certain conditions by Kwon, Bruckstein and Kailath <ref> [45] </ref>. The problem when F = 1, that is when the terminal state is constrained to be zero was considered by Tadmor [68]. The techniques we will use are similar to those in [45]. We will first need the following Lemma. Related results can be found in [45, 2, 59, 17, 57, 7]. Lemma 6.3. <p> The problem when F = 1, that is when the terminal state is constrained to be zero was considered by Tadmor [68]. The techniques we will use are similar to those in [45]. We will first need the following Lemma. Related results can be found in <ref> [45, 2, 59, 17, 57, 7] </ref>. Lemma 6.3. Suppose F satisfies _ F + (A B 2 D 0 12 C 1 ) 2 fl 2 B 1 B 0 1 (I D 12 D 0 and F (t) 0 for all t 0. <p> Kwon, Bruckstein and Kailath <ref> [45] </ref> applied results from scattering theory in their solution to the quadratic problem, to give a forwards differential equation for X 1 (t; t + T; F (t + T )). Here we state the same solution for the indefinite Riccati equation.
Reference: [46] <author> W. H. Kwon and A. E. Pearson. </author> <title> A modified quadratic cost problem and feedback stabilization of a linear system. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 22(5) </volume> <pages> 838-842, </pages> <month> October </month> <year> 1977. </year> <note> Bibliography 134 </note>
Reference-contexts: In this methodology, at each time t the controller is designed for the finite intervals [t; t + T ]. The controller is then used on the possibly infinitesimal interval [t; t + ffi] before being recomputed. This technique was motivated by Kleinman [44] and Kwon and Pearson <ref> [46] </ref> in the early 1970s for use with linear quadratic optimization problems. It was shown that the resulting controllers were stable when applied on an infinite horizon. This allowed the synthesis for linear time varying systems without full future knowledge of the plant parameters. <p> The techniques in his paper were later reformulated and generalized by various authors, see for example <ref> [18, 46, 45] </ref>. Kwon and Pearson [46] proved stability for linear time varying systems using a controller which optimized a quadratic cost function integrated over a time interval from the current time t to a fixed distance ahead t + T . <p> The techniques in his paper were later reformulated and generalized by various authors, see for example [18, 46, 45]. Kwon and Pearson <ref> [46] </ref> proved stability for linear time varying systems using a controller which optimized a quadratic cost function integrated over a time interval from the current time t to a fixed distance ahead t + T .
Reference: [47] <author> S. Lall and K. Glover. </author> <title> A game theoretic approach to moving horizon control. </title> <editor> In D. Clarke, editor, </editor> <title> Advances in Model-Based Predictive Control. </title> <publisher> Oxford University Press, </publisher> <year> 1994. </year>
Reference: [48] <author> S. Lall and K. Glover. </author> <title> Robust performance and adaptation using receding horizon H 1 control of time varying systems. </title> <note> To appear at the 1995 American Control Conference, </note> <year> 1995. </year>
Reference: [49] <author> S. Lall, K. Glover, and G. Dullerud. </author> <title> A solution to the asynchronous multi-rate sampled-data H 1 problem. </title> <note> Submitted to Automatica, </note> <year> 1995. </year>
Reference: [50] <author> D. J. N. Limebeer, B. D. O. Anderson, P. P. Khargonekar, and M. Green. </author> <title> A game theoretic approach to H 1 control for time varying systems. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 30(2) </volume> <pages> 262-283, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Further, if the original system matrices are continuous functions of time, then the transformed matrices will also be continuous. For details of these loopshifting transformations, see for example <ref> [50, 29] </ref>. Also, following [22] we will make the additional standing assumptions that D 21 (t)B 1 (t) 0 = 0 D 12 (t) 0 C 1 (t) = 0 (3.20) for all t 2 [t 0 ; t f ]. <p> This result can also be found in <ref> [50] </ref>. For the moving horizon problem, we will write the solution X 1 to this Riccati differential equation on the interval [t 1 ; t 2 ] with boundary condition X 1 (t 2 ) = F as X 1 (t 1 ; t 2 ; F ). <p> A variant on this theorem is proved in Chapter 3. See also Limebeer, Anderson, Khargonekhar and Green <ref> [50] </ref>.
Reference: [51] <author> E. F. Mageirou. </author> <title> Values and strategies for infinite time linear quadratic games. </title> <journal> IEEE Transactions on Automatic Control, </journal> <pages> pages 547-550, </pages> <month> August </month> <year> 1976. </year>
Reference: [52] <author> D. Q. Mayne and H. Michalska. </author> <title> Receding horizon control of nonlinear systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 35(7) </volume> <pages> 814-824, </pages> <month> July </month> <year> 1990. </year>
Reference: [53] <author> K. L. Moore, S. P. Bhattacharyya, and M. Dahleh. </author> <title> Capabilities and limitations of multirate control schemes. </title> <journal> Automatica, </journal> <volume> 29(4) </volume> <pages> 941-951, </pages> <year> 1993. </year>
Reference: [54] <author> K. M. Nagpal and R. Ravi. </author> <title> H 1 control and estimation problems with delayed measurements: state space solutions. </title> <booktitle> In Proceedings of the 1994 American Control Conference, Baltimore, </booktitle> <pages> pages 2379-2383, </pages> <year> 1994. </year>
Reference-contexts: This might be expected, since during the `one step' w has the opportunity to maximize the cost function with u only having open loop control. Similar conditions occur also in the H 1 problem with delayed measurements, see for example <ref> [54] </ref>. Theorem 4.13. Suppose, for each i = 0; : : : ; f , there exists a bounded solution to each of the Riccati differential equations (4.34) on the interval [t i ; t i+1 ], with Y 1 (t 0 ) = Q 0 .
Reference: [55] <author> A. Packard. </author> <title> Gain scheduling via linear fractional transformations. </title> <journal> Systems and Control Letters, </journal> <volume> 22 </volume> <pages> 79-92, </pages> <year> 1994. </year>
Reference-contexts: Therefore, it is possible to have the controller vary according to variations in the plant, rather than design in advance for all possible future variations in the plant. There is an important difference between this moving horizon approach and the parameter varying approaches of <ref> [55, 79] </ref>. The parameter varying approach is to consider the system matrices A, B, C, D as functions of a time varying parameter (t). Then _ X can be replaced by @X @ _. <p> This is usually approached by gridding the parameter space. In this case, the controller achieves an induced 2-norm of fl for all trajectories of the parameter which satisfy the rate limitation. The controller may either be independent of the parameter as in <ref> [55] </ref> or, less conservatively, may depend on the parameter and its rate of change as in [79]. Note that the parameter varying approach gives only sufficient conditions for existence of a controller.
Reference: [56] <author> M. A. Poubelle, R. R. Bitmead, and M. Gevers. </author> <title> Fake algebraic riccati techniques and stability. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 33(4) </volume> <pages> 379-381, </pages> <year> 1988. </year>
Reference: [57] <author> A. C. M. Ran and R. Vreugdenhil. </author> <title> Existence and comparison theorems for algebraic riccati equations for continuous and discrete time systems. </title> <journal> Linear Algebra and its applications, </journal> <volume> 99 </volume> <pages> 63-83, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The problem when F = 1, that is when the terminal state is constrained to be zero was considered by Tadmor [68]. The techniques we will use are similar to those in [45]. We will first need the following Lemma. Related results can be found in <ref> [45, 2, 59, 17, 57, 7] </ref>. Lemma 6.3. Suppose F satisfies _ F + (A B 2 D 0 12 C 1 ) 2 fl 2 B 1 B 0 1 (I D 12 D 0 and F (t) 0 for all t 0.
Reference: [58] <author> R. Ravi, A. M. Pascoal, and P. P. Khargonekar. </author> <title> Normalized coprime factorizations and the graph metric for linear time varying systems. </title> <booktitle> In Proceedings of the 29th IEEE CDC, </booktitle> <pages> pages 1241-1246, </pages> <year> 1990. </year>
Reference-contexts: The following definitions can be found in <ref> [58, 43] </ref>. Definition 2.3. The system G, or the pair (A; B), is said to be stabilizable if there exists a bounded matrix function F such that the system _x (t) = A (t) B (t)F (t) x (t) is exponentially stable. Definition 2.4. <p> The system matrices A, B, C and D are bounded functions of time. We will write (t 1 ; t 2 ) for the transition matrix of A. The following lemmas are proved in <ref> [58] </ref>. Lemma 5.1.
Reference: [59] <author> W. T. Reid. </author> <title> Riccati Differential Equations. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: + I Z i 22 (t i+1 ) = R ?0 [t i+1 ]X 12 R ? [t i+1 ] 1 Z i 12 [t i+1 ] Z i 12 (t i+1 ) = X 12 [t i+1 ]R ? [t i+1 ] using the standard fact (see for example <ref> [59] </ref>) that solutions to this form of Riccati differential equation are symmetric. The first of these equations is simply equation (4.21), and by assumption this has a bounded solution. <p> The problem when F = 1, that is when the terminal state is constrained to be zero was considered by Tadmor [68]. The techniques we will use are similar to those in [45]. We will first need the following Lemma. Related results can be found in <ref> [45, 2, 59, 17, 57, 7] </ref>. Lemma 6.3. Suppose F satisfies _ F + (A B 2 D 0 12 C 1 ) 2 fl 2 B 1 B 0 1 (I D 12 D 0 and F (t) 0 for all t 0. <p> Here we state the same solution for the indefinite Riccati equation. These results are derived in [73] and <ref> [59] </ref> for the case when the Riccati differential equation has a positive definite quadratic term. In this section, P (t; ) is used to mean X 1 (t; ; 0).
Reference: [60] <author> M. Sampei, T. Mita, and M. Nakamichi. </author> <title> An algebraic approach to H 1 output feedback control problems. </title> <journal> Systems and Control Letters, </journal> <volume> 14 </volume> <pages> 13-24, </pages> <year> 1990. </year>
Reference: [61] <author> E. D. Sontag. </author> <title> Mathematical Control Theory. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Since A is bounded, the right hand side of this ordinary differential equation is a globally Lipschitz function of x (t), and hence a solution always exists for all t t 0 . Proofs of these results can be found in, for example <ref> [61, 16] </ref>. Then define , the transition matrix of A by @t The solution of (2.1) is then given by x (t) = (t; t 0 )x (t 0 ) 2.2 Linear Operators 8 Useful properties of . <p> Proof. A proof of continuity for general nonlinear equations satisfying a local Lipschitz condition is in Sontag <ref> [61] </ref>. For specific details for finite horizon Riccati equations, see also Ba~sar and Bernhard [7, Chapter 8]. Lemma 6.24.
Reference: [62] <author> A. A. Stoorvogel. </author> <title> The discrete time H 1 control problem with measurement feedback. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 30(1) </volume> <pages> 182-202, </pages> <year> 1992. </year> <note> Bibliography 135 </note>
Reference: [63] <author> A. A. Stoorvogel. </author> <title> The H 1 control problem. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: An example of such a game in the dynamic case is given by Stoorvogel <ref> [63, p. 123] </ref>. The following result will be useful in the sequel. Theorem 3.6.
Reference: [64] <author> W. Sun, K. M. Nagpal, and P. P. Khargonekar. </author> <title> H 1 control and filtering for sampled-data systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 38(8) </volume> <pages> 1162-1175, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: These results are derived using lifting techniques, where the original hybrid system is transformed to one on a discrete operator-valued space. The finite horizon time varying problem has also been considered, notably by Ba~sar and Bernhard [7]. Further solutions to this problem are considered in <ref> [6, 67, 64, 11] </ref>. The above results all consider systems with single rate periodic sampling and hold devices synchronized with each other. In this chapter we will be considering sampled-data systems which are both multi-rate and asynchronous. <p> We will show that, if we wish to synthesize a controller for a sampled-data system, it is possible to express the continuous system combined with generalized sample and hold operators in this form. This class of systems was developed by Sun, Nagpal and Khargonekar <ref> [64] </ref>, who used quite different techniques to construct solutions to H 1 synthesis problems for particular special cases of this general form of system.
Reference: [65] <author> G. Tadmor. </author> <title> Input/output norms in general linear systems. </title> <journal> International Journal of Control, </journal> <volume> 51(4) </volume> <pages> 911-921, </pages> <year> 1990. </year>
Reference-contexts: Note that this definition depends on the homogeneous part of the system only, and we will use the phrase "A is stable" also to mean that G is stable. The following lemma is well known. Related results are given in <ref> [74, 1, 65] </ref>. Lemma 2.2. <p> Then if kGk &lt; fl, there exists X 2 L nfin 1 , with X 0, such that _ X + A 0 X + XA + fl 2 XBB 0 X + C 0 C 0 8 t 2 R + Proof. See Tadmor <ref> [65] </ref>. Remark 5.5. Note that the above results can be shown without assuming differentiabil ity of X, or even finite dimensionality of the system matrices. In this case, the Riccati differential equations are replaced by integral equations. In the sequel, we will write the Riccati equations in the differential form.
Reference: [66] <author> G. Tadmor. </author> <title> Worst case design in the time domain: the maximum principle and the standard H 1 problem. Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> 3 </volume> <pages> 301-324, </pages> <year> 1990. </year>
Reference: [67] <author> G. Tadmor. </author> <title> H 1 optimal sampled-data control in continuous time systems. </title> <journal> International Journal of Control, </journal> <volume> 56(1) </volume> <pages> 99-141, </pages> <year> 1992. </year>
Reference-contexts: These results are derived using lifting techniques, where the original hybrid system is transformed to one on a discrete operator-valued space. The finite horizon time varying problem has also been considered, notably by Ba~sar and Bernhard [7]. Further solutions to this problem are considered in <ref> [6, 67, 64, 11] </ref>. The above results all consider systems with single rate periodic sampling and hold devices synchronized with each other. In this chapter we will be considering sampled-data systems which are both multi-rate and asynchronous.
Reference: [68] <author> G. Tadmor. </author> <title> Receding horizon revisited: An easy way to robustly stabilize an LTV system. </title> <journal> Systems and Control Letters, </journal> <volume> 18 </volume> <pages> 285-294, </pages> <year> 1992. </year>
Reference-contexts: In this chapter, a receding horizon controller is formulated with each finite horizon optimization based upon an H 1 optimization. It is hoped that the advantages of receding horizon control might be combined with the robustness advantages of H 1 control. Tadmor <ref> [68] </ref> proved, under certain assumptions and with a terminal constraint, that a similar controller was stable and satisfied an infinite horizon norm bound. We generalize these results in the state feedback case and construct a new controller for the output feedback case. <p> The problem when F = 1, that is when the terminal state is constrained to be zero was considered by Tadmor <ref> [68] </ref>. The techniques we will use are similar to those in [45]. We will first need the following Lemma. Related results can be found in [45, 2, 59, 17, 57, 7]. Lemma 6.3.
Reference: [69] <author> G. Tadmor. </author> <title> The standard H 1 problem and the maximum principle: the general linear case. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 31(4) </volume> <pages> 813-846, </pages> <year> 1993. </year>
Reference-contexts: The other is to design fully for time varying systems. In the case of both linear quadratic and H 1 controllers, this requires the backwards integration from infinity of a Riccati equation (see for example <ref> [69, 43] </ref>), and somewhat optimistically assumes knowledge of the system throughout future time. The moving horizon method can be viewed as a compromise between these two methods. <p> In this case, the differential equations for X are replaced by integral equations. This technique is used by Tadmor <ref> [69] </ref> to solve the general linear H 1 problem. 6.2 State feedback 100 Remark 6.6. Clearly the assumption that (A; B 2 ) is stabilizable is necessary for the existence of a stabilizing state feedback linear time varying controller. However, we have not used it in the above argument.
Reference: [70] <author> H. T. Toivonen. </author> <title> Sampled-data H 1 optimal control of time varying systems. </title> <journal> Au-tomatica, </journal> <volume> 28(4) </volume> <pages> 823-826, </pages> <year> 1992. </year>
Reference: [71] <author> K. Uchida and M. Fujita. </author> <title> On the central controller: Chracterizations via differential games and leqg control problems. </title> <journal> Systems and Control Letters, </journal> <volume> 13 </volume> <pages> 9-13, </pages> <year> 1989. </year>
Reference: [72] <author> K. Uchida and M. Fujita. </author> <title> Finite horizon H 1 control problems with terminal penalties. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 37(11) </volume> <pages> 1762-1767, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: For the finite horizon problem, typically the assumptions made are that the initial state at the beginning of each optimization interval is completely known, or is completely unknown. In the latter case it is treated as part of the disturbance and subject to a quadratic weighting <ref> [72] </ref> [42]. In the receding horizon problem, though, we have observations from before the optimization interval. We first consider the state feedback problem, with the above quadratic cost function replaced by the game theoretic cost function of previous chapters.
Reference: [73] <author> G. Verghese, B. Friedlander, and T. Kailath. </author> <title> Scattering theory and linear least squares estimation, part III: The estimates. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 25(4) </volume> <pages> 794-802, </pages> <month> August </month> <year> 1980. </year>
Reference-contexts: Here we state the same solution for the indefinite Riccati equation. These results are derived in <ref> [73] </ref> and [59] for the case when the Riccati differential equation has a positive definite quadratic term. In this section, P (t; ) is used to mean X 1 (t; ; 0).
Reference: [74] <author> M. Vidyasagar. </author> <title> Nonlinear Systems Analysis. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: Note that this definition depends on the homogeneous part of the system only, and we will use the phrase "A is stable" also to mean that G is stable. The following lemma is well known. Related results are given in <ref> [74, 1, 65] </ref>. Lemma 2.2.
Reference: [75] <author> P. G. Voulgaris. </author> <title> Control of asynchronous sampled data systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 39(7) </volume> <pages> 1451-1455, </pages> <year> 1994. </year>
Reference: [76] <author> P. G. Voulgaris and B. Bamieh. </author> <title> Optimal H 1 and H 2 control of hybrid multirate systems. </title> <journal> Systems and Control Letters, </journal> <volume> 20 </volume> <pages> 249-261, </pages> <year> 1993. </year>
Reference-contexts: Again we will use as performance measure an induced 2-norm. It will be natural for us to consider some disturbances as discrete and others as continuous. Multi-rate systems have been considered by several authors, notably Chen and Qiu [20] and Voulgaris <ref> [76] </ref>. In these papers, the sampling and hold devices are assumed to have rational rates, and to be synchronized with each other, so that the system can be lifted up to either the fastest or the slowest commensurate period.
Reference: [77] <author> P. Whittle. </author> <title> Optimization over time. </title> <publisher> Wiley, </publisher> <year> 1982. </year>
Reference: [78] <author> P. Whittle. </author> <title> Risk-sensitive optimal control. Wiley Interscience series in Systems and Optimization. </title> <publisher> Wiley, </publisher> <year> 1990. </year>
Reference-contexts: We show that the H 1 problem is equivalent to a dynamic game problem over certain strategy spaces. In the output feedback case, we consider 1.1 Organisation of the thesis 5 the separation theories of Whittle <ref> [78] </ref> and Ba~sar and Bernhard [7], and also the use of information state for separation as developed by James, Baras and Elliott [40]. <p> We will concentrate on the game theoretic formulations, and will use both the nonlinear concept of information state, first used in H 1 theory by James and Baras [40], and the idea of worst case disturbances, as developed by Ba~sar and Bernhard [7] and Whittle <ref> [78] </ref>. Throughout this thesis we will consider what has become known as the "standard" H 1 synthesis problem. That is, we are given a causal linear system G mapping inputs w; u to outputs z; y. <p> The certainty equivalence principles that have been derived by various authors <ref> [78, 38, 7] </ref> do not depend on the linear character of the problem, and can be easily described in the nonlinear framework. We will then, in the next section, go on to specialize these results to the linear case. <p> Then, subject to certain technical assumptions, V will satisfy @t u (t) w (t) g t; ~; u (t ); w (t ) + @~ (3.16) with boundary condition V (t f ; x) = fi (x) 8x 2 R n Whittle <ref> [78] </ref> describes the quantity V (t; ~) as the (partially extremized) future stress. This is exactly the Isaacs equation for the state feedback differential game problem. Let u (t) = 0 (t; x (t)) be the optimal state feedback controller. <p> For each t , the information state is a functional on R n . Note that, for linear systems, the information state is a nonhomogeneous quadratic function of the ~, see <ref> [37, 78] </ref>. Whittle [78] calls this function the past stress. The certainty equivalence principle. <p> For each t , the information state is a functional on R n . Note that, for linear systems, the information state is a nonhomogeneous quadratic function of the ~, see [37, 78]. Whittle <ref> [78] </ref> calls this function the past stress. The certainty equivalence principle. The following theorem, the discrete version of which was first formulated by Whittle [78], and which was proved in this form for the continuous time problem by James and Baras [38] gives us the desired certainty equivalence principle. <p> Note that, for linear systems, the information state is a nonhomogeneous quadratic function of the ~, see [37, 78]. Whittle <ref> [78] </ref> calls this function the past stress. The certainty equivalence principle. The following theorem, the discrete version of which was first formulated by Whittle [78], and which was proved in this form for the continuous time problem by James and Baras [38] gives us the desired certainty equivalence principle. Theorem 3.15. <p> This theorem is similar to one first introduced by Whittle <ref> [78] </ref> for the discrete time linear-quadratic problem. It is proved for the continuous time nonlinear problem in [38, 40] using arguments from dynamic programming, and there are several stringent technical assumptions regarding smoothness that must be made. <p> We have given an overview of the certainty equivalence principle derived by Whittle <ref> [78] </ref> and Ba~sar and Bernhard [7], and given a new explicit derivation of this for the linear-quadratic continuous time finite horizon output feedback H 1 problem. <p> Provided the terminal state weights on each subinterval satisfy certain inequalities, the resulting controller will be fl-feasible. 8. CONCLUSIONS By way of conclusion we summarise the main contributions of this thesis. * In chapter 3, we have given an explicit derivation of the certainty equivalence principle developed by Whittle <ref> [78] </ref> and Ba~sar and Bernhard [7] for the finite horizon continuous time linear differential game with a quadratic cost function. * We have given necessary and sufficient conditions for the existence of state feedback controllers which attenuate the induced L 2 norm of a multi-rate system with jumps to less than
Reference: [79] <author> F. Wu, X. H. Yang, A. Packard, and G. Becker. </author> <title> Induced L 2 -norm control for LPV system with bounded parameter variation rates. </title> <type> Preprint, </type> <year> 1995. </year> <note> Bibliography 136 </note>
Reference-contexts: The solution to the time varying problem has also been expressed in terms of Riccati differential inequalities, or linear matrix inequalities with a differential term, in the parameter varying case by Wu, Yang, Packard and Becker <ref> [79] </ref>. However, in this solution the resulting controller depends explicitly on _ X 1 and hence also on the rate of change of the parameter. <p> Hence we can now construct controllers for the finite horizon time varying induced 2-norm problem by solving Riccati differential inequalities. The solution presented here has the advantage over the simpler solution based upon the bounded real lemma by Wu et. al. <ref> [79] </ref> in that the controllers do not depend on _ X 1 and _ Y 1 . <p> This in turn implies that, if we solve this problem for parameter varying systems as a special case of the time varying problem, the resulting controller does not depend on the rate of change of the parameter. However, we lose an important feature of the solutions of <ref> [79] </ref>, and that is that the constructive procedure here is not convex. <p> Therefore, it is possible to have the controller vary according to variations in the plant, rather than design in advance for all possible future variations in the plant. There is an important difference between this moving horizon approach and the parameter varying approaches of <ref> [55, 79] </ref>. The parameter varying approach is to consider the system matrices A, B, C, D as functions of a time varying parameter (t). Then _ X can be replaced by @X @ _. <p> In this case, the controller achieves an induced 2-norm of fl for all trajectories of the parameter which satisfy the rate limitation. The controller may either be independent of the parameter as in [55] or, less conservatively, may depend on the parameter and its rate of change as in <ref> [79] </ref>. Note that the parameter varying approach gives only sufficient conditions for existence of a controller. However, in the moving horizon approach, we do not synthesize for all possible trajectories of a parameter, or equivalently for all possible variations of the system matrices within some set. <p> That is, we can choose to integrate both control and filtering equations either forwards or backwards in time. Unfortunately, the controller formulas in terms of X 1 and Y 1 also depend on _ X 1 , see for example Wu. et. al. <ref> [79] </ref>. Since we are looking for solutions to the Riccati differential equations with no specified boundary conditions, one possible solution might be to pick an initial boundary condition and integrate forwards. However, the problem is how to choose the initial value.
Reference: [80] <author> I. Yaesh and U. Shaked. </author> <title> H 1 optimal one-step-ahead output feedback control of discrete time systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 37(8) </volume> <pages> 1245-1250, </pages> <month> August </month> <year> 1992. </year>
Reference: [81] <author> G. Zames. </author> <title> Feedback and optimal sensitivity: Model reference transformations, multiplicative seminorms, and approximate inverses. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 26 </volume> <pages> 301-320, </pages> <year> 1981. </year>
References-found: 81

