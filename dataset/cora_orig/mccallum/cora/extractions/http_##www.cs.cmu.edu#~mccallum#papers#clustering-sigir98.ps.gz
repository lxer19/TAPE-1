URL: http://www.cs.cmu.edu/~mccallum/papers/clustering-sigir98.ps.gz
Refering-URL: http://www.cs.cmu.edu/~mccallum/
Root-URL: http://www.cs.cmu.edu/~jr6b
Email: www.cs.cmu.edu/~ldbapp  www.cs.cmu.edu/~mccallum  
Title: Distributional Clustering of Words for Text Classification  
Author: L. Douglas Baker yz Andrew Kachites McCallum zy 
Address: Pittsburgh, PA 15213  4616 Henry Street Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  Just Research  
Abstract: This paper describes the application of Distributional Clustering [20] to document classification. This approach clusters words into groups based on the distribution of class labels associated with each word. Thus, unlike some other unsupervised dimensionality-reduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy. Experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2% accuracy|significantly better than Latent Semantic Indexing [6], class-based clustering [1], feature selection by mutual information [23], or Markov-blanket-based feature selection [13]. We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra, and J. C. Lai. </author> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 467-479, </pages> <year> 1992. </year>
Reference-contexts: In this paper we cluster words into groups specifically for the benefit of document classification. While much study has been devoted to word clustering for language modeling and word co-occurrence <ref> [1, 20] </ref>, little work has been done on word clustering for document classification. The underlying clustering method we apply is Distributional Clustering [20]|an information-theoretic approach that has shown good performance in language modeling. Word clustering methods create new, reduced-size event spaces by joining similar words into groups. <p> We present experimental results on three real-world text corpora, including newswire stories, UseNet articles and Web pages. Results show that Distributional Clustering can reduce the feature dimensionality by three orders of magnitude, and lose only 2% accuracy. This performance is significantly better than class-based clustering using mutual information <ref> [1] </ref>, clustering by Latent Semantic Indexing [6], feature selection by information gain [23] and feature selection by Markov-blanket [13]. On one of the data sets we show that clustering increases classification accuracy. <p> Chi2 [18] is an extension of ChiMerge for use as a feature selector of numeric attributes. Liu and Setiono observe that if all the values of any attribute are clustered together, then that value is irrelevant to the classification task and can be removed. Class-based clustering <ref> [1] </ref> uses an agglomerative, hard clustering algorithm where the clustering criterion is designed to maximize the overall average mutual information between clusters and the class variable. <p> At equal feature dimensionality, it achieves significantly higher accuracy than four other feature clustering and feature selection algorithms: supervised Latent Semantic Indexing [8], class-based clustering <ref> [1] </ref>, feature selection by mutual information with the class variable [23] and feature selection by a Markov-blanket method [13]. The 20 Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20 UseNet discussion groups [10].
Reference: [2] <author> Thomas Cover and Peter Hart. </author> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13(1) </volume> <pages> 21-27, </pages> <year> 1967. </year>
Reference-contexts: the weighted average of the individual distributions combination P (Cjw t _ w s ) = P (w t ) + P (w s ) + P (w t ) + P (w s ) Distributional Clustering differs from some other machine learning approaches to similarity metrics (e.g. k-nearest neighbor <ref> [2] </ref>) in that it measures similarity based on the target variable that it is trying to estimate for the task at hand, not the other "input" attributes.
Reference: [3] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley, </publisher> <year> 1991. </year>
Reference-contexts: This results in t=1 which is precisely the expression of the cross entropy between the distribution of words in the document, P (W jd i ) and the distribution of words in the class P (W jc j ) <ref> [3] </ref>, where W is a random variable over words.
Reference: [4] <author> Mark Craven, Daniel DiPasquo, Dayne Freitag, An-drew McCallum, Tom Mitchell, Kamal Nigam, and Sean Slattery. </author> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <booktitle> In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98), </booktitle> <year> 1998. </year>
Reference-contexts: As the amount of documents and number of users rise, automatic document categorization becomes an increasingly important tool for helping people organize this vast amount of data. Statistical document classification algorithms have been applied to categorizing newsfeeds [10], classifying Web pages <ref> [4] </ref>, sorting electronic mail [17] and learning the interests of users [14]. In this paper we cluster words into groups specifically for the benefit of document classification. <p> ) = P (d i j ^ ) = Q jd i j P jCj Q jd i j Both the mixture model and word independence assumptions are violated in practice with real-world data; however, there is empirical evidence that naive Bayes often performs well in spite of these violations <ref> [16, 23, 10, 4] </ref>.
Reference: [5] <author> Ido Dagan, Fernando Pereira, and Lillian Lee. </author> <title> Similarity-based estimation of word cooccurrence probabilities. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1994. </year>
Reference-contexts: However, we avoid the costly EM-style update procedure that must be used to find a stable configuration of the cluster centroids and the cluster membership probabilities. 3 Related Work Distributional Clustering has been used <ref> [20, 5, 15] </ref> to address the problem of sparse data in building statistical language models for natural language processing, but it has not previously been applied to document classification. We have used larger data sets with more prevalent sparseness and fewer class labels.
Reference: [6] <author> S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: Results show that Distributional Clustering can reduce the feature dimensionality by three orders of magnitude, and lose only 2% accuracy. This performance is significantly better than class-based clustering using mutual information [1], clustering by Latent Semantic Indexing <ref> [6] </ref>, feature selection by information gain [23] and feature selection by Markov-blanket [13]. On one of the data sets we show that clustering increases classification accuracy. <p> Koller and Sahami present a Markov-blanket-based feature selection algorithm that aims to address exactly this [13]. Their technique is based on the same principles as Distributional Clustering|it examines P (Cjw t ), and tries to preserve the proper C distribution. Latent Semantic Indexing <ref> [6] </ref> is an unsupervised dimensionality reduction technique for information retrieval that explicitly accounts for the dependencies between words. In brief, it applies Principle Component Analysis (PCA) to documents represented as word vectors.
Reference: [7] <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: Conditions for the optimality of the simple bayesian classifier. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 103-130, </pages> <year> 1997. </year>
Reference-contexts: Friedman and Domingos and Pazzani discuss why the violation of the word independence assumption some times does little damage to classification accuracy <ref> [9, 7] </ref>. 2.2 Measuring Word Similarity for Distribu tional Clustering Now we address the question of how to cluster words in the context of our generative model and naive Bayes.
Reference: [8] <author> Susan T. Dumais. </author> <title> Using LSI for information filtering: TREC-3 experiments. </title> <type> Technical Report 500-225, </type> <institution> National Institute of Standards and Technology, </institution> <year> 1995. </year>
Reference-contexts: Latent Semantic Indexing [6] is an unsupervised dimensionality reduction technique for information retrieval that explicitly accounts for the dependencies between words. In brief, it applies Principle Component Analysis (PCA) to documents represented as word vectors. Du-mais applies it to text classification <ref> [8] </ref> by representing each class as a centroid, which is the vector sum of all the feature vectors of all the documents in that class. <p> At equal feature dimensionality, it achieves significantly higher accuracy than four other feature clustering and feature selection algorithms: supervised Latent Semantic Indexing <ref> [8] </ref>, class-based clustering [1], feature selection by mutual information with the class variable [23] and feature selection by a Markov-blanket method [13]. The 20 Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20 UseNet discussion groups [10].
Reference: [9] <author> Jerome H. Friedman. </author> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery, </title> <booktitle> 1 </booktitle> <pages> 55-77, </pages> <year> 1997. </year>
Reference-contexts: Friedman and Domingos and Pazzani discuss why the violation of the word independence assumption some times does little damage to classification accuracy <ref> [9, 7] </ref>. 2.2 Measuring Word Similarity for Distribu tional Clustering Now we address the question of how to cluster words in the context of our generative model and naive Bayes.
Reference: [10] <author> Thorsten Joachims. </author> <title> A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <year> 1997. </year>
Reference-contexts: As the amount of documents and number of users rise, automatic document categorization becomes an increasingly important tool for helping people organize this vast amount of data. Statistical document classification algorithms have been applied to categorizing newsfeeds <ref> [10] </ref>, classifying Web pages [4], sorting electronic mail [17] and learning the interests of users [14]. In this paper we cluster words into groups specifically for the benefit of document classification. <p> ) = P (d i j ^ ) = Q jd i j P jCj Q jd i j Both the mixture model and word independence assumptions are violated in practice with real-world data; however, there is empirical evidence that naive Bayes often performs well in spite of these violations <ref> [16, 23, 10, 4] </ref>. <p> The 20 Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20 UseNet discussion groups <ref> [10] </ref>. Several of the topic classes are quite confusable: four of them are about computers; three discuss religion. In tokenizing the data we skipped all UseNet headers, used a stoplist, but did not stem because we found it to hurt accuracy.
Reference: [11] <author> J. D. Jobson. </author> <title> Applied Multivariate Data Analysis Volume II: Categorical and Multivariate Methods. </title> <publisher> Springer Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Supervised techniques can take advantage of the class labels in order to concentrate their efforts on the specific task at hand. We believe this difference explains the accuracy increase of Distributional Clustering over LSI in the top graph. Linear Discriminant Analysis <ref> [11] </ref> is a supervised technique similar to LSI which we feel may work well for text classification, although we have not yet experimented with this technique. <p> As previously mentioned, LSI is an unsupervised dimensionality reduction technique based on the Singular Value Decomposition of a term-document matrix. The underlying technique in LSI is to find an orthonormal basis for the term-document space for which the axes lie along the dimensions of maximum variance. Linear Discriminant Analysis <ref> [11] </ref> is a related technique which instead attempts to find a basis such that the distance between the means of the members of each class is maximized while the variance within each class is minimized. We plan to investigate its use for text classification.
Reference: [12] <author> R. Kerber. Chimerge: </author> <title> Discretization of numeric attributes. </title> <booktitle> In Proceedings of Tenth National Conference on Artificial Intelligence (AAAI-92), </booktitle> <year> 1992. </year>
Reference-contexts: We have used larger data sets with more prevalent sparseness and fewer class labels. ChiMerge <ref> [12] </ref> uses a form of Distributional Clustering to discretize numeric attributes for subsequent classification. It is an agglomerative, hard clustering algorithm that uses the 2 statistic as the the similarity metric. We have also tried 2 in our experiments and found that the KL divergence average yields better performance.
Reference: [13] <author> D. Koller and M. Sahami. </author> <title> Toward optimal feature selection. </title> <booktitle> In Proceedings of Thirteenth International Conference on Machine Learning (ICML-96), </booktitle> <year> 1996. </year>
Reference-contexts: Results show that Distributional Clustering can reduce the feature dimensionality by three orders of magnitude, and lose only 2% accuracy. This performance is significantly better than class-based clustering using mutual information [1], clustering by Latent Semantic Indexing [6], feature selection by information gain [23] and feature selection by Markov-blanket <ref> [13] </ref>. On one of the data sets we show that clustering increases classification accuracy. We hypothesize why this did not happen in more cases, and discuss possible future improvements. 2 Clustering Words by Class Distributions This section introduces our probabilistic framework, derives the naive Bayes classifier and explains Distributional Clustering. <p> However, mutual information between words and classes does not capture dependencies between words. Koller and Sahami present a Markov-blanket-based feature selection algorithm that aims to address exactly this <ref> [13] </ref>. Their technique is based on the same principles as Distributional Clustering|it examines P (Cjw t ), and tries to preserve the proper C distribution. Latent Semantic Indexing [6] is an unsupervised dimensionality reduction technique for information retrieval that explicitly accounts for the dependencies between words. <p> At equal feature dimensionality, it achieves significantly higher accuracy than four other feature clustering and feature selection algorithms: supervised Latent Semantic Indexing [8], class-based clustering [1], feature selection by mutual information with the class variable [23] and feature selection by a Markov-blanket method <ref> [13] </ref>. The 20 Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20 UseNet discussion groups [10]. Several of the topic classes are quite confusable: four of them are about computers; three discuss religion.
Reference: [14] <author> Ken Lang. Newsweeder: </author> <title> Learning to filter netnews. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <pages> pages 331-339, </pages> <year> 1995. </year>
Reference-contexts: Statistical document classification algorithms have been applied to categorizing newsfeeds [10], classifying Web pages [4], sorting electronic mail [17] and learning the interests of users <ref> [14] </ref>. In this paper we cluster words into groups specifically for the benefit of document classification. While much study has been devoted to word clustering for language modeling and word co-occurrence [1, 20], little work has been done on word clustering for document classification.
Reference: [15] <author> Lillian Lee. </author> <title> Similarity-Based Approaches to Natural Language Processing. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1997. </year> <note> (also Technical Report TR-11-97). </note>
Reference-contexts: The most similar two clusters are joined, then the next word is added as a singleton cluster to bring the total number of clusters back up to M . Table 2 contains an outline of our algorithm. In contrast, probabilistic "soft" clustering, as used in previous Distributional Clustering work <ref> [20, 15] </ref>, is more formally rigorous, and allows the clustering to be less greedy. <p> However, we avoid the costly EM-style update procedure that must be used to find a stable configuration of the cluster centroids and the cluster membership probabilities. 3 Related Work Distributional Clustering has been used <ref> [20, 5, 15] </ref> to address the problem of sparse data in building statistical language models for natural language processing, but it has not previously been applied to document classification. We have used larger data sets with more prevalent sparseness and fewer class labels. <p> We found that Distributional Clustering is better than feature selection at preserving the information contained in redundant features. It allows the size of the model to be reduced much more aggressively while maintaining good performance. However, it is still susceptible to detrimental features. Earlier work with Distributional Clustering <ref> [20, 15] </ref> shows that Distributional Clustering addresses the sparse data problem (improving what were previously detrimen tal features). We also observed a small increase in classification accuracy, but this happened only on the one data set with the most, and most evenly distributed, data.
Reference: [16] <author> David Lewis and Marc Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 81-93, </pages> <year> 1994. </year>
Reference-contexts: ) = P (d i j ^ ) = Q jd i j P jCj Q jd i j Both the mixture model and word independence assumptions are violated in practice with real-world data; however, there is empirical evidence that naive Bayes often performs well in spite of these violations <ref> [16, 23, 10, 4] </ref>.
Reference: [17] <author> David D. Lewis and Kimberly A. Knowles. </author> <title> Threading electronic mail: A preliminary study. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 33(2) </volume> <pages> 209-217, </pages> <year> 1997. </year>
Reference-contexts: As the amount of documents and number of users rise, automatic document categorization becomes an increasingly important tool for helping people organize this vast amount of data. Statistical document classification algorithms have been applied to categorizing newsfeeds [10], classifying Web pages [4], sorting electronic mail <ref> [17] </ref> and learning the interests of users [14]. In this paper we cluster words into groups specifically for the benefit of document classification.
Reference: [18] <author> H. Liu and R. Setiono. Chi2: </author> <title> Feature selection and discretization of numeric attributes. </title> <booktitle> In Proceedings of 7th IEEE Int'l Conference on Tools with Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: It is an agglomerative, hard clustering algorithm that uses the 2 statistic as the the similarity metric. We have also tried 2 in our experiments and found that the KL divergence average yields better performance. Chi2 <ref> [18] </ref> is an extension of ChiMerge for use as a feature selector of numeric attributes. Liu and Setiono observe that if all the values of any attribute are clustered together, then that value is irrelevant to the classification task and can be removed.
Reference: [19] <author> Andrew McCallum and Kamal Nigam. </author> <title> A comparison of event models for naive Bayes text classification. </title> <booktitle> In AAAI-98 Workshop on Learning for Text Categorization, </booktitle> <year> 1998. </year> <note> http://www.cs.cmu.edu/~mccallum. </note>
Reference-contexts: In this paper we approach document generation as language modeling. Thus, unlike some notions of naive Bayes in which documents are `events' and the words in the document are `attributes' of that event (a multi-variate Bernoulli model), we instead consider words to be `events' (a multinomial model) <ref> [19] </ref>. Multinomial naive Bayes has been shown to out-perform the multi-variate Bernoulli on many real-world corpora [19]. We say a document is comprised of an ordered sequence of word events, and write d ik for the word in position k of document d i . <p> notions of naive Bayes in which documents are `events' and the words in the document are `attributes' of that event (a multi-variate Bernoulli model), we instead consider words to be `events' (a multinomial model) <ref> [19] </ref>. Multinomial naive Bayes has been shown to out-perform the multi-variate Bernoulli on many real-world corpora [19]. We say a document is comprised of an ordered sequence of word events, and write d ik for the word in position k of document d i .
Reference: [20] <author> Fernando Pereira, Naftali Tishby, and Lillian Lee. </author> <title> Distributional clustering of english words. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 183-90, </pages> <year> 1993. </year>
Reference-contexts: In this paper we cluster words into groups specifically for the benefit of document classification. While much study has been devoted to word clustering for language modeling and word co-occurrence <ref> [1, 20] </ref>, little work has been done on word clustering for document classification. The underlying clustering method we apply is Distributional Clustering [20]|an information-theoretic approach that has shown good performance in language modeling. Word clustering methods create new, reduced-size event spaces by joining similar words into groups. <p> We hypothesize why this did not happen in more cases, and discuss possible future improvements. 2 Clustering Words by Class Distributions This section introduces our probabilistic framework, derives the naive Bayes classifier and explains Distributional Clustering. Like previous work in Distributional Clustering <ref> [20] </ref> we use a form of "Kullback-Leibler divergence to the mean." Unlike their work, we use a weighted average instead of a simple average, we use hard clustering instead of soft, and we use a greedy agglomerative method instead of a divisive entropy-based method. 2.1 Probabilistic Framework and Naive Bayes We <p> Thus, in Distributional Clustering we use a related measure that does not have these problems. It is the average of the KL divergence of each distribution to their mean distribution, called "KL divergence to the mean." Unlike earlier work <ref> [20] </ref> we use a weighted average instead of a simple average. <p> The most similar two clusters are joined, then the next word is added as a singleton cluster to bring the total number of clusters back up to M . Table 2 contains an outline of our algorithm. In contrast, probabilistic "soft" clustering, as used in previous Distributional Clustering work <ref> [20, 15] </ref>, is more formally rigorous, and allows the clustering to be less greedy. <p> However, we avoid the costly EM-style update procedure that must be used to find a stable configuration of the cluster centroids and the cluster membership probabilities. 3 Related Work Distributional Clustering has been used <ref> [20, 5, 15] </ref> to address the problem of sparse data in building statistical language models for natural language processing, but it has not previously been applied to document classification. We have used larger data sets with more prevalent sparseness and fewer class labels. <p> We found that Distributional Clustering is better than feature selection at preserving the information contained in redundant features. It allows the size of the model to be reduced much more aggressively while maintaining good performance. However, it is still susceptible to detrimental features. Earlier work with Distributional Clustering <ref> [20, 15] </ref> shows that Distributional Clustering addresses the sparse data problem (improving what were previously detrimen tal features). We also observed a small increase in classification accuracy, but this happened only on the one data set with the most, and most evenly distributed, data.
Reference: [21] <author> WiseWire. </author> <note> http://www.wisewire.com. </note>
Reference-contexts: We argue that successful use of small-footprint text classification models becomes increasingly important with the wide-spread and popular use of text classification. For example, large population, high-volume routing tasks, as required by companies such as WiseWire <ref> [21] </ref>, can involve text categorization with hundreds of thousands of class labels on a stream of documents arriving a rate of hundreds per second|the use of word clustering can avoid the need for machines with many gigabytes of memory.
Reference: [22] <author> Yiming Yang. </author> <title> Noise reduction in a statistical approach to text categorization. </title> <booktitle> In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'95), </booktitle> <pages> pages 256-263, </pages> <year> 1995. </year>
Reference-contexts: A new document is labelled with the class of the centroid to which its feature vector is closest, as measured by the cosine-similarity between the two vectors. The Linear Least Squares Fit (LLSF) method <ref> [22] </ref> is another classification algorithm based on PCA, which is equivalent to Dumais' use of LSI for classification except that LLSF uses the dot-product to compute similarity instead of the cosine and is thus sensitive to the length of the two vectors being compared. 4 Experimental Results This section provides empirical
Reference: [23] <author> Yiming Yang and Jan Pederson. </author> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In ICML-97, </booktitle> <pages> pages 412-420, </pages> <year> 1997. </year>
Reference-contexts: Results show that Distributional Clustering can reduce the feature dimensionality by three orders of magnitude, and lose only 2% accuracy. This performance is significantly better than class-based clustering using mutual information [1], clustering by Latent Semantic Indexing [6], feature selection by information gain <ref> [23] </ref> and feature selection by Markov-blanket [13]. On one of the data sets we show that clustering increases classification accuracy. <p> ) = P (d i j ^ ) = Q jd i j P jCj Q jd i j Both the mixture model and word independence assumptions are violated in practice with real-world data; however, there is empirical evidence that naive Bayes often performs well in spite of these violations <ref> [16, 23, 10, 4] </ref>. <p> A previous study found feature selection by mutual information with the class label to be the best for text, among several common, time/space-efficient methods <ref> [23] </ref>. However, mutual information between words and classes does not capture dependencies between words. Koller and Sahami present a Markov-blanket-based feature selection algorithm that aims to address exactly this [13]. <p> At equal feature dimensionality, it achieves significantly higher accuracy than four other feature clustering and feature selection algorithms: supervised Latent Semantic Indexing [8], class-based clustering [1], feature selection by mutual information with the class variable <ref> [23] </ref> and feature selection by a Markov-blanket method [13]. The 20 Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20 UseNet discussion groups [10]. Several of the topic classes are quite confusable: four of them are about computers; three discuss religion.
References-found: 23

