URL: http://goliat.upc.es/~mmartin/animat.ps.Z
Refering-URL: http://www-lsi.upc.es/~mmartin/publicacions.html
Root-URL: 
Email: mmartin@lsi.upc.es, marius@lsi.upc.es, ia@lsi.upc.es  
Title: Animats Adaptation to Complex Environments as Learning Guided by Evolution  
Author: Mario Martin, Marius Garcia, Ulises Cortes 
Address: Pau Gargallo 5, 08028, Barcelona (Catalunya), SPAIN  
Affiliation: Dept. Llenguatges i Sistemes Informatics Universitat Politecnica de Catalunya  
Abstract: In this paper, a new approach to adapt ani-mats to complex environments is proposed. It is based on the advantages and drawbacks of two known strategies: learning and evolution. The proposed approach uses a new learning by reinforcement mechanism guided by an innate and general knowledge -obtained by an evolutionary mechanism- that triggers off a developmental process of learning general behavior. This developmental process increases the animats set of behaviors, characterized by sequences of actions, and facilitates the solving of more complex tasks. This approach is studied, described and finally illustrated with a set of experiments in a complex en vironment.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Ballard. </author> <title> Reference frames for animate vision. </title> <booktitle> In Proceedings of IJCAI89, </booktitle> <year> 1989. </year>
Reference-contexts: This limited perception implies that the information the animat has of its environment is always partial, incomplete and ambiguous. To solve a problem, the system must learn to search and perceive the required information. This kind of activity has been called animated perception <ref> [1] </ref>. In many other learning mechanisms it is supposed that the animat is able to perceive all the information of its environment. When these methods have to deal with ambiguities, the problem of "perceptual aliasing" [17] appears, disabling the learning process.
Reference: [2] <author> R.A. Brooks. </author> <title> Intelligence without representation. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 139-159, </pages> <year> 1991. </year>
Reference-contexts: There are different kinds of reactive systems, ranging from systems based on simple architectures as look-up tables or stimulus-action rules, to structured and complex systems as "Sonja" [6] or those designed with the "subsumption architecture" <ref> [2] </ref>. Unfortunately, one of the main drawbacks of this kind of systems is their inability to adapt autonomously to complex environments 1 . Usually, reactive systems must be hand-designed in order to obtain adequate results. In this case, we should say that they are born adapted to their environment.
Reference: [3] <editor> R.A. Brooks. </editor> <booktitle> Artificial life and real robots. In Proceedings of ECAL91. </booktitle> <publisher> MIT Press/Bradford Books, </publisher> <year> 1992. </year>
Reference-contexts: The other strategy is the evolution by means of selection and variation of the best adapted animats, proposed for instance in <ref> [3] </ref>, [12] or [11]. This approach usually states that animats are born completely hard-wired (adapted), not by the design made by an engineer, but by the natural process of evolution simulated by genetic algorithms [10].
Reference: [4] <author> S. Carey and R. Gelman, </author> <title> editors. </title> <booktitle> The Epigenesis of Mind: Essays on Biology and Cognition. </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1991. </year>
Reference-contexts: This makes a robust animat. On the other, a simplified genetic process, drives the animat to achieve enough development letting it successfully accomplish complex tasks. This new approach is supported by animal learning research <ref> [4] </ref>, where we can clearly observe the influence of innate mechanisms to guide and ease the learning process. It is also supported by cognitive science research when we speak about a developmental process in an environment, instead of only a learning process.
Reference: [5] <author> D. Chapman. </author> <title> Penguins can make cake. </title> <journal> AI Magazine, </journal> <volume> 10 </volume> <pages> 45-50, </pages> <month> winter </month> <year> 1989. </year>
Reference-contexts: When these methods have to deal with ambiguities, the problem of "perceptual aliasing" [17] appears, disabling the learning process. The limited perception is a realistic constraint, but it is not a negative one. It is widely known that a limited environmental perception can produce a very profitable generalization process <ref> [5] </ref>. The problem resolution for an animat in the environment is characterized by a sequence of actions which will receive a reinforcement signal when the entire sequence has been executed. Without any initial knowledge, the animat has only the trial and error procedure to solve any problem. <p> The environment selected, where the animat will solve problems, is the "blocks world". This environment has been used by Chapman <ref> [5] </ref> and Whitehead and Ballard [17] to show the possibilities and drawbacks of reactive systems in complex domains. Chapman demonstrates, solving the "fruitcake" problem 2 that reactive systems can accomplish complex goals in this domain without the combinatorial explosion of stimulus-action rules predicted by Ginsberg [9].
Reference: [6] <author> D. Chapman. </author> <title> Vision, Instruction and Action. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: One of the most successful has been the reactive approach, which states that stimulus-driven behaviors can accomplish complex tasks. There are different kinds of reactive systems, ranging from systems based on simple architectures as look-up tables or stimulus-action rules, to structured and complex systems as "Sonja" <ref> [6] </ref> or those designed with the "subsumption architecture" [2]. Unfortunately, one of the main drawbacks of this kind of systems is their inability to adapt autonomously to complex environments 1 . Usually, reactive systems must be hand-designed in order to obtain adequate results.
Reference: [7] <author> D. Chapman and L. Kaelbling. </author> <title> Learning from delayed reinforcement in a complex domain. </title> <type> Technical Report 90-11, </type> <institution> Teleos Research, </institution> <year> 1990. </year>
Reference-contexts: The first one considers the adaptation of the animat to the environment as a learning process. This idea has been exposed by several authors [15] <ref> [7] </ref> [13]. For them, the adaptation process consists in learning (usually by means of a reinforcement signal) a mapping between the set of actions and the set of perceptions that reveals an adequate behavior to the environment.
Reference: [8] <author> R. Dawkins. </author> <title> The Selfish Gene. </title> <publisher> Oxford University Press, </publisher> <year> 1989. </year>
Reference-contexts: There are objective reasons that support this fact. First of all, genes act in this case as predictors of the actions the animat has to do in any situation to survive in its environment <ref> [8] </ref>. A codification of the architecture that always indicates the animat what action has to do, in complex environments, is really expensive, and needs a big genoma and a very long evolutionary process.
Reference: [9] <author> M.L. Ginsberg. </author> <title> Universal planning: An (almost) universally bad idea. </title> <journal> AI Magazine, </journal> <volume> 10 </volume> <pages> 41-44, </pages> <month> winter </month> <year> 1989. </year>
Reference-contexts: Chapman demonstrates, solving the "fruitcake" problem 2 that reactive systems can accomplish complex goals in this domain without the combinatorial explosion of stimulus-action rules predicted by Ginsberg <ref> [9] </ref>. The problem is solved using a "deictic" perceptual system that reduces enormously the number of states of the system. Starting from this work, Whitehead and Ballard developed a mechanism to enable learning in reactive systems with deictic perception.
Reference: [10] <author> D.E. Goldberg. </author> <title> Genetic Algorithms. </title> <publisher> Addison Wes--ley, </publisher> <year> 1989. </year>
Reference-contexts: This approach usually states that animats are born completely hard-wired (adapted), not by the design made by an engineer, but by the natural process of evolution simulated by genetic algorithms <ref> [10] </ref>. This approach is borrowed from nature and reflects the fact that animals are born with a high level of behavioral information. Nevertheless, this proposal presents several drawbacks. It implies that for every situation, the behavior of the animat is "innate".
Reference: [11] <author> I. Harvey, P. Husbands, and D. Cliff. </author> <booktitle> Issues in evolutionary robotics. In Proceedings of SAB92. </booktitle> <publisher> MIT Press/Bradford Books, </publisher> <year> 1993. </year>
Reference-contexts: The other strategy is the evolution by means of selection and variation of the best adapted animats, proposed for instance in [3], [12] or <ref> [11] </ref>. This approach usually states that animats are born completely hard-wired (adapted), not by the design made by an engineer, but by the natural process of evolution simulated by genetic algorithms [10].
Reference: [12] <author> J.R. Koza. </author> <title> Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems. </title> <type> Technical Report STAN-CS-90-1314, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <year> 1990. </year>
Reference-contexts: The other strategy is the evolution by means of selection and variation of the best adapted animats, proposed for instance in [3], <ref> [12] </ref> or [11]. This approach usually states that animats are born completely hard-wired (adapted), not by the design made by an engineer, but by the natural process of evolution simulated by genetic algorithms [10].
Reference: [13] <author> S. Mahadevan and J. Connell. </author> <title> Automatic programming of behaviour-based robots using reinforcement learning. </title> <booktitle> In Proceedings of AAAI91, </booktitle> <year> 1991. </year>
Reference-contexts: The first one considers the adaptation of the animat to the environment as a learning process. This idea has been exposed by several authors [15] [7] <ref> [13] </ref>. For them, the adaptation process consists in learning (usually by means of a reinforcement signal) a mapping between the set of actions and the set of perceptions that reveals an adequate behavior to the environment.
Reference: [14] <author> M. Martin and U. Cortes. </author> <title> A developmental approach for integrated architectures. </title> <booktitle> In ILA Workshop notes, </booktitle> <address> ECML-93, </address> <year> 1993. </year>
Reference: [15] <author> R.S. Sutton. </author> <title> Reinforcement learning architectures for animats. </title> <booktitle> In Proceedings of SAB91, </booktitle> <year> 1992. </year>
Reference-contexts: The first one considers the adaptation of the animat to the environment as a learning process. This idea has been exposed by several authors <ref> [15] </ref> [7] [13]. For them, the adaptation process consists in learning (usually by means of a reinforcement signal) a mapping between the set of actions and the set of perceptions that reveals an adequate behavior to the environment.
Reference: [16] <author> C. Watkins. </author> <title> Learning from delayed rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: Chapman has shown that this kind of systems can solve complex tasks, but only if they can operate with an incomplete description of the world. The mechanism they developed, tries to overcome the incapacity of the Q-learning <ref> [16] </ref> method to solve problems when it is fed with ambiguous information. Their results are positive, but far from learning to solve the "fruitcake" problem.
Reference: [17] <author> S.D. Whitehead and D.H. Ballard. </author> <title> Learning to per-cive and act by trial and error. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 45-83, </pages> <year> 1991. </year>
Reference-contexts: This kind of activity has been called animated perception [1]. In many other learning mechanisms it is supposed that the animat is able to perceive all the information of its environment. When these methods have to deal with ambiguities, the problem of "perceptual aliasing" <ref> [17] </ref> appears, disabling the learning process. The limited perception is a realistic constraint, but it is not a negative one. It is widely known that a limited environmental perception can produce a very profitable generalization process [5]. <p> The environment selected, where the animat will solve problems, is the "blocks world". This environment has been used by Chapman [5] and Whitehead and Ballard <ref> [17] </ref> to show the possibilities and drawbacks of reactive systems in complex domains. Chapman demonstrates, solving the "fruitcake" problem 2 that reactive systems can accomplish complex goals in this domain without the combinatorial explosion of stimulus-action rules predicted by Ginsberg [9].
References-found: 17

