URL: http://www.csc.calpoly.edu/~tpederse/aistat97-tr.ps.gz
Refering-URL: http://www.csc.calpoly.edu/~tpederse/pubs.html
Root-URL: http://www.csc.calpoly.edu
Email: fpedersen,rbruce,kayaalpg@seas.smu.edu  
Title: Model Selection using Sparse Data and Backward Sequential Search  
Author: Ted Pedersen Rebecca Bruce Mehmet Kayaalp 
Date: September 17, 1996  
Address: Dallas, TX 75275-0122  
Affiliation: Department of Computer Science and Engineering Southern Methodist University  
Abstract: This paper presents an empirical comparison of a variety of model evaluation criteria used in backwards sequential search (BSS). Both information criteria (IC) and significance tests are compared when applied to the problem of word sense disambiguation. We present experimental results that show the evaluation criteria that selects the most accurate models for classification. When the degrees of freedom of the model are adjusted the Log-likelihood ratio G 2 and Akaike's Information Criteria (AIC) select the most accurate models. If the degrees of freedom are not adjusted then the most accurate evaluation criteria for word sense disambiguation is IC = 1. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Akaike. </author> <title> A new look at the statistical model identification. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-19(6):716-723, </volume> <month> December </month> <year> 1974. </year>
Reference-contexts: AIC <ref> [1] </ref> corresponds to = 2. BIC [13] corresponds to = log (N ), where N is the sample size. Other values have also been suggested [8] and are evaluated in this study.
Reference: [2] <author> H. Akaike. </author> <title> Information measures and model selection. </title> <journal> Bulletin of the International Statistical Institute, </journal> <volume> 50 </volume> <pages> 277-290, </pages> <year> 1983. </year>
Reference: [3] <author> J. Badsberg. </author> <title> An Environment for Graphical Models. </title> <type> PhD thesis, </type> <institution> Aalborg University, </institution> <year> 1995. </year>
Reference-contexts: The significance of G 2 based on the exact conditional distribution is accurate for small and sparse data samples. The exact conditional test can be performed using the freely available software package CoCo <ref> [3] </ref>. In effect, the exact conditional test approximates Fisher's Exact test [6] for large contingency tables. The use of Fisher's Exact test for a small contingency table problem is described in [9].
Reference: [4] <author> R. Bruce. </author> <title> A Statistical Method for Word-Sense Disambiguation. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, New Mexico State University, </institution> <year> 1995. </year>
Reference-contexts: There are four nouns (bill, concern, drug, interest), four verbs (agree, close, help, include) and four adjectives (chief, common, last, public). This is the same set of words as was studied in <ref> [4] </ref> and [5]. A variety of different model evaluation criteria are used to develop models for the words to be disambiguated. Each criterion guides a model selection process that generates a series of models concluding in the most parsimonious model that adequately characterizes the data.
Reference: [5] <author> R. Bruce, J. Wiebe, and T. Pedersen. </author> <title> The measure of a model. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 101-112, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The models developed using each of the criteria are applied to the classification of words with respect to their meanings. This task is commonly referred to as word-sense disambiguation. We use a variety of existing quantitative measures to evaluate model performance. The measure of model form <ref> [5] </ref> is used to analyze the effectiveness of the evaluation criterion in characterizing the structure of the data. Recall and precision are used to gauge the quality of the parameter estimates, and accuracy is used to look at the overall performance of the model in classifying new data. <p> There are four nouns (bill, concern, drug, interest), four verbs (agree, close, help, include) and four adjectives (chief, common, last, public). This is the same set of words as was studied in [4] and <ref> [5] </ref>. A variety of different model evaluation criteria are used to develop models for the words to be disambiguated. Each criterion guides a model selection process that generates a series of models concluding in the most parsimonious model that adequately characterizes the data.
Reference: [6] <author> R. Fisher. </author> <title> The Design of Experiments. </title> <publisher> Oliver and Boyd, </publisher> <address> London, </address> <year> 1935. </year>
Reference-contexts: The significance of G 2 based on the exact conditional distribution is accurate for small and sparse data samples. The exact conditional test can be performed using the freely available software package CoCo [3]. In effect, the exact conditional test approximates Fisher's Exact test <ref> [6] </ref> for large contingency tables. The use of Fisher's Exact test for a small contingency table problem is described in [9].
Reference: [7] <author> S. Lauritzen, B. Thiesson, and D. Spiegelhalter. </author> <title> Diagnoistic systems by model selection: a case study. </title> <editor> In P. Cheeseman and R. Oldford, editors, </editor> <title> Selecting Models from Data:AI and Statistics IV. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Also, it must be decided whether or not to adjust the degrees of freedom. We examine the impact of each of these decisions on the accuracy of the resultant classifier. As in <ref> [7] </ref>, the IC can be thought of as: IC = deviance fi complexity where low deviance (as measured by G 2 ) rewards goodness of fit and high complexity (as measured by dof) is assessed a penalty. The weight of that penalty is controlled by the value of . <p> When the accuracy reaches a high point and then begins to decline as edges are removed the search has gone "too far" and removed edges that are important to classification accuracy. 5 Related Work In <ref> [7] </ref> a comparison was made of the exact conditional test and AIC when using backwards sequential search to select models to diagnose congenital heart disease.
Reference: [8] <author> H. Linhart and W. Zucchini. </author> <title> Model Selection. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: AIC [1] corresponds to = 2. BIC [13] corresponds to = log (N ), where N is the sample size. Other values have also been suggested <ref> [8] </ref> and are evaluated in this study. The various information based criteria were developed as an alternative to using predefined significance levels to judge the acceptability of a model.
Reference: [9] <author> T. Pedersen. </author> <title> Fishing for exactness. </title> <booktitle> In Proceedings of the South Central SAS User's Group (SCSUG-96) Conference, </booktitle> <address> Austin, TX, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: The exact conditional test can be performed using the freely available software package CoCo [3]. In effect, the exact conditional test approximates Fisher's Exact test [6] for large contingency tables. The use of Fisher's Exact test for a small contingency table problem is described in <ref> [9] </ref>.
Reference: [10] <author> T. Pedersen, M. Kayaalp, and R. Bruce. </author> <title> Significant lexical relationships. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pages 455-460, </pages> <address> Portland, OR, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Word sense disambiguation is a well studied but still open problem in natural language processing (NLP). Like other NLP tasks, it is both interesting and challenging because of, among other things, the inherent sparseness of the data <ref> [10] </ref> and the large number of diverse and potentially relevant features. An additional problem is that manually tagged data is difficult to acquire so that samples of tagged data are typically small. <p> Again this difference can be accounted for due to the fact that the survey data utilized was not nearly as sparse as the word sense data. In <ref> [10] </ref> the use of the exact conditional test is proposed as a means of choosing between models of independence and dependence for two variable problems.
Reference: [11] <author> A. Raftery. </author> <title> Bayesian model selection in social research (with discussion by Andrew Gelman and Donald B. Rubin, </title> <editor> and Robert M. Hauser, and a rejoinder). In P Marsden, editor, </editor> <booktitle> Sociological Methodology 1995, </booktitle> <pages> pages 111-196. </pages> <address> Blackwells, Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: Unfortunately determining a proper ff when using a significance test in model selection is difficult <ref> [11] </ref>. When working with very sparse data the traditional ff values of .01 and .05 result in overfitting. For our data we determined experimentally that ff values of .00001 for G 2 and .0001 for the exact conditional test were appropriate. <p> The word sense data has 8 or 9 variables and is represented in contingency tables of 10 to 20 million cells. Thus the word sense data is orders of magnitude more sparse. In <ref> [11] </ref> BIC is suggested as a reasonable criteria when selecting models from sociological survey data. They find that BIC leads to simpler, more accurate models than significance tests.
Reference: [12] <author> T. Read and N. Cressie. </author> <title> Goodness-of-fit Statistics for Discrete Multivariate Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: While it is standard to use a 2 distribution to assess the significance of G 2 , it is well known that this approximation may not hold when the data is sparse <ref> [12] </ref>. An alternative to using a 2 approximation is to define the exact conditional distribution of G 2 . The exact conditional distribution of G 2 is the distribution of G 2 values that would be observed for comparable data samples randomly generated from the model being tested.
Reference: [13] <author> G. Schwarz. </author> <title> Esimating the dimension of a model. </title> <journal> The Annals of Statistics, </journal> <volume> 6(2) </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: AIC [1] corresponds to = 2. BIC <ref> [13] </ref> corresponds to = log (N ), where N is the sample size. Other values have also been suggested [8] and are evaluated in this study. The various information based criteria were developed as an alternative to using predefined significance levels to judge the acceptability of a model. <p> The various information based criteria were developed as an alternative to using predefined significance levels to judge the acceptability of a model. Each formulation expresses a balance between model fit and model complexity that is asymptotically optimum when the model parameters have certain prior distributions ([2], <ref> [13] </ref>). Values close to zero represent a good model fit while more extreme values (either positive or negative) indicate a poor fit. The model selection process removes edges that result in models with very high negative values for IC . 1 We notate this as G 2 .
Reference: [14] <author> N. Wermuth. </author> <title> Model search among multiplicative models. </title> <journal> Biometrics, </journal> <volume> 32 </volume> <pages> 253-263, </pages> <month> June </month> <year> 1976. </year>
Reference-contexts: Model search is conducted using a backwards sequential search as described in <ref> [14] </ref> and implemented in CoCo. This procedure is specific to decomposable models and can be best explained using the dependency graph 3 of a decomposable model.
References-found: 14

