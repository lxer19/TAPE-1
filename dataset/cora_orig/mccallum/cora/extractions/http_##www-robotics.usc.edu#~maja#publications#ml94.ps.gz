URL: http://www-robotics.usc.edu/~maja/publications/ml94.ps.gz
Refering-URL: http://www-robotics.usc.edu/~maja/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: maja@ai.mit.edu  
Title: Reward Functions for Accelerated Learning  
Author: Maja J Mataric 
Address: 545 Technology Square #721 Cambridge, MA 02139  
Affiliation: MIT Artificial Intelligence Laboratory  
Abstract: This paper discusses why traditional reinforcement learning methods, and algorithms applied to those models, result in poor performance in situated domains characterized by multiple goals, noisy state, and inconsistent reinforcement. We propose a methodology for designing reinforcement functions that take advantage of implicit domain knowledge in order to accelerate learning in such domains. The methodology involves the use of heterogeneous reinforcement functions and progress estimators, and applies to learning in domains with a single agent or with multiple agents. The methodology is experimentally validated on a group of mobile robots learning a foraging task.
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E. & Chapman, D. </author> <year> (1990), </year> <title> What are plans for?, </title> <editor> in P. Maes, ed., </editor> <title> `Designing Autonomous Agents', </title> <publisher> MIT Press, </publisher> <pages> pp. 17-34. </pages>
Reference: <author> Atkeson, C. G. </author> <year> (1990), </year> <title> Memory-Based Approaches to Approximating Continuous Functions, </title> <booktitle> in `Proceedings, Sixth Yale Workshop on Adaptive and Learning Systems'. </booktitle>
Reference: <author> Barto, A. G., Bradtke, S. J. & Singh, S. P. </author> <year> (1993), </year> <title> `Learning to Act using Real-Time Dynamic Programming', </title> <journal> AI Journal. </journal>
Reference: <author> Brooks, R. A. </author> <year> (1990), </year> <title> The Behavior Language; User's Guide, </title> <type> Technical Report AIM-1227, </type> <institution> MIT Artificial Intelligence Lab. </institution>
Reference-contexts: FEEDBACK Design of reinforcement functions is not often discussed, although it is perhaps the most difficult aspect of setting up an RL system. A variation of RL with immediate reinforcement has been successfully applied to a six-legged robot learning to walk <ref> (Maes & Brooks 1990) </ref>. The approach was appropriate given the small size of the search space and the immediate and accurate reinforcement. <p> The robots are also equipped with radio transceivers, used for determining absolute position. Position information is obtained by triangulating the distance computed from synchronized ultrasound pulses from two fixed beacons. The robots are programmed in the Behavior Language, a parallel programming language based on the Sub-sumption Architecture <ref> (Brooks 1990) </ref>.
Reference: <author> Brooks, R. A. & Mataric, M. J. </author> <year> (1993), </year> <title> Real Robots, Real Learning Problems, in `Robot Learning', </title> <publisher> Kluwer Academic Press, </publisher> <pages> pp. 193-213. </pages>
Reference-contexts: Much of simulation work attempts to hide continuous state, such as the inputs from complex sensors, by presuming higher-level filters. (e.g., "I see a chair in front of me."). These assumptions have proven unrealistic in physical systems <ref> (Brooks & Mataric 1993, Agre & Chapman 1990) </ref>. In general, in situated domains, which are dynamic and noisy, there is no guarantee that the agent can sense its own state correctly. Furthermore, the agent can usually only perceive local, not global, external state, and do so with inherent limitations.
Reference: <author> Chapman, D. & Kaelbling, L. P. </author> <year> (1991), </year> <title> Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons, </title> <booktitle> in `Proceedings, IJCAI-91', </booktitle> <address> Sydney, Australia. </address>
Reference: <author> Jaakkola, T. & Jordan, M. I. </author> <year> (1993), </year> <title> `On the Convergence of Stochastic Iterative Dynamic Programming Algorithms', </title> <note> Submitted to Neural Computation. </note>
Reference: <author> Jordan, M. I. & Rumelhart, D. E. </author> <year> (1992), </year> <title> `Forward Models: Supervised Learning with a Distal Teacher', </title> <booktitle> Cognitive Science 16, </booktitle> <pages> 307-354. </pages>
Reference-contexts: Domain knowledge can be utilized through a reward-rich and complex reinforcement function, but the process of embedding semantics is usually ad hoc. A direct way to utilize implicit domain knowledge is to convert reward functions into error signals, akin to those used in learning control <ref> (Jordan & Rumelhart 1992, Atkeson 1990, Schaal & Atkeson 1994) </ref>. Immediate reinforcement in RL is a weak version of error signals, using only the sign of the error but not the magnitude.
Reference: <author> Kaelbling, L. P. </author> <year> (1990), </year> <title> Learning in Embedded Systems, </title> <type> PhD thesis, </type> <institution> Stanford University. </institution>
Reference-contexts: Whitehead (1992) eloquently describes why assumptions about world models commonly held in RL do not apply to real-world tasks. Thus, insightful work on building world models for more intelligent exploration <ref> (Sutton 1990, Kaelbling 1990) </ref> is yet to be generalized to situated domains. 2.3 LEARNING TRIALS Traditional RL models allow for proving convergence properties of various forms of temporal differencing (TD) applied to deterministic MDP environments (Watkins & Dayan 1992, Barto, Bradtke & Singh 1993, Jaakkola & Jordan 1993).
Reference: <author> Maes, P. & Brooks, R. A. </author> <year> (1990), </year> <title> Learning to Coordinate Behaviors, </title> <booktitle> in `Proceedings, AAAI-90', </booktitle> <address> Boston, MA, </address> <pages> pp. 796-802. </pages>
Reference-contexts: FEEDBACK Design of reinforcement functions is not often discussed, although it is perhaps the most difficult aspect of setting up an RL system. A variation of RL with immediate reinforcement has been successfully applied to a six-legged robot learning to walk <ref> (Maes & Brooks 1990) </ref>. The approach was appropriate given the small size of the search space and the immediate and accurate reinforcement.
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1991), </year> <title> Automatic Programming of Behavior-Based Robots Using Reinforcement Learning, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pittsburgh, PA, </address> <pages> pp. 8-14. </pages>
Reference: <author> Mataric, M. J. </author> <year> (1992), </year> <title> Designing Emergent Behaviors: From Local Interactions to Collective Intelligence, </title> <editor> in J.-A. Meyer, H. Roitblat & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats: International Conference on Simulation of Adaptive Behavior'. </booktitle>
Reference-contexts: Individually, each robot learns to select the best behavior for each condition, in order to find and take home the most pucks. Foraging was chosen because it is a nontrivial and biologically in spired task, and because our previous group behavior work <ref> (Mataric 1992, Mataric 1993) </ref> provided the basic behavior repertoire from which to learn behavior selection. The fixed repertoire consisted of the following behaviors: * avoiding * dispersing * searching * homing * resting Utility behaviors for grasping and dropping objects were included in the robot's capabilities, but were not learned.
Reference: <author> Mataric, M. J. </author> <year> (1993), </year> <title> Kin Recognition, Similarity, and Group Behavior, </title> <booktitle> in `Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society', </booktitle> <address> Boulder, Colorado, </address> <pages> pp. 705-710. </pages>
Reference-contexts: Much of simulation work attempts to hide continuous state, such as the inputs from complex sensors, by presuming higher-level filters. (e.g., "I see a chair in front of me."). These assumptions have proven unrealistic in physical systems <ref> (Brooks & Mataric 1993, Agre & Chapman 1990) </ref>. In general, in situated domains, which are dynamic and noisy, there is no guarantee that the agent can sense its own state correctly. Furthermore, the agent can usually only perceive local, not global, external state, and do so with inherent limitations.
Reference: <author> Mataric, M. J. </author> <year> (1994), </year> <title> Interaction and Intelligent Behavior, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference-contexts: Our previous work has described a reformulation of states and actions into conditions and behaviors in order to significantly diminish the state space <ref> (Mataric 1994) </ref>. In this paper we propose a method for accelerated learning by extending and structuring reward functions to take advantage of domain knowledge. Rather than encode knowledge explicitly, RL methods hide it in the reinforcement.
Reference: <author> Schaal, S. & Atkeson, C. C. </author> <year> (1994), </year> <title> `Robot Juggling: An Implementation of Memory-Bassed Learning', </title> <journal> Control Systems Magazine 14, </journal> <pages> 57-71. </pages>
Reference: <author> Singh, S. P. </author> <year> (1991), </year> <title> Transfer of Learning Across Compositions of Sequential Tasks, </title> <booktitle> in `Proceedings, Eighth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Evanston, </address> <publisher> Illinois, </publisher> <pages> pp. 348-352. </pages>
Reference: <author> Sutton, R. </author> <year> (1988), </year> <title> `Learning to Predict by Method of Temporal Differences', </title> <booktitle> Machine Learning 3(1), </booktitle> <pages> 9-44. </pages>
Reference-contexts: The approach applies to single agent and multi-agent learning, and is experimentally validated on a group of mobile robots learning to forage. Its performance is compared to standard alternatives. 2 LEARNING IN SITUATED DOMAINS Reinforcement learning has been studied extensively and its properties are well known <ref> (Sutton 1988, Watkins 1989, Kaelbling 1990) </ref>. Successful applications of RL methodologies to well-behaved domains have encouraged researchers to hypothesize about its value for learning on situated agents such as mobile robots. However, while simulation results are encouraging, work on physical robots has not repeated that success.
Reference: <author> Sutton, R. S. </author> <year> (1990), </year> <title> Integrated Architectures for Learning, Planning and Reacting Based on Approximating Dynamic Programming, </title> <booktitle> in `Proceedings, Seventh International Conference on Machine Learning', </booktitle> <address> Austin, Texas. </address>
Reference-contexts: 1 INTRODUCTION Reinforcement learning (RL) has become the methodology of choice for learning in a variety of different domains. Its convergence properties and potential biological relevance make it an approach worth studying. RL has been shown to perform well in Markovian domains, such as games (Tesauro 1992) and simulations <ref> (Sutton 1990) </ref>. However, it has not yet been proven useful in situated agent domains, in particular when applied to physical robots. In this paper we discuss why traditional reinforcement learning methods and associated algorithms perform poorly in situated domains with multiple goals, noisy state, and inconsistent reinforcement. <p> Whitehead (1992) eloquently describes why assumptions about world models commonly held in RL do not apply to real-world tasks. Thus, insightful work on building world models for more intelligent exploration <ref> (Sutton 1990, Kaelbling 1990) </ref> is yet to be generalized to situated domains. 2.3 LEARNING TRIALS Traditional RL models allow for proving convergence properties of various forms of temporal differencing (TD) applied to deterministic MDP environments (Watkins & Dayan 1992, Barto, Bradtke & Singh 1993, Jaakkola & Jordan 1993).
Reference: <author> Tan, M. </author> <year> (1993), </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents, in `Proceedings, Tenth International Conference on Machine Learning', </booktitle> <address> Amherst, MA, </address> <pages> pp. 330-337. </pages>
Reference: <author> Tesauro, G. </author> <year> (1992), </year> <title> Practical Issues in Temporal Dif--ference Learning, </title> <editor> in J. E. Moody, S. J. Hanson & R. P. Lippmann, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems 4', </booktitle> <publisher> Morgan Kauf-mann, </publisher> <pages> pp. 259-267. </pages>
Reference-contexts: 1 INTRODUCTION Reinforcement learning (RL) has become the methodology of choice for learning in a variety of different domains. Its convergence properties and potential biological relevance make it an approach worth studying. RL has been shown to perform well in Markovian domains, such as games <ref> (Tesauro 1992) </ref> and simulations (Sutton 1990). However, it has not yet been proven useful in situated agent domains, in particular when applied to physical robots.
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989), </year> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference-contexts: Asymptotic convergence of TD and related learning strategies based on dynamic programming requires infinite trials <ref> (Watkins 1989) </ref>. Simply generating a complete policy requires time exponential in the size of the state space, and the policy approaches optimality as the number of trials approaches infinity.
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992), </year> <title> `Q-Learning', </title> <booktitle> Machine Learning 8, </booktitle> <pages> 279-292. </pages>
Reference: <author> Whitehead, S. D. </author> <year> (1992), </year> <title> Reinforcement Learning for the Adaptive Control of Perception and Action, </title> <type> PhD thesis, </type> <institution> University of Rochester. </institution>
Reference-contexts: In such cases some progress metric is also necessary. Progress estimators are partial internal critics. They are associated with specific goals and, when active, provide a metric of improvement relative to those goals. Unlike external critics <ref> (Whitehead 1992) </ref>, progress estimators do not provide a complete oracle but only partial, goal-specific "advice." Progress estimators are important in noisy worlds because they decrease the learner's sensitivity to intermittent errors by associating a continuous metric with the behavior being executed. Progress estimators also encourage exploration. <p> The proposed subgoals are directly tied to behaviors which are used as the basis of learning. Similarly, progress estimators are also mapped to one or more behaviors, and expedite learning of the associated goals, unlike a single complete external critic used with a monolithic reinforcement function <ref> (Whitehead 1992) </ref>. The presented work is, to the best of our knowledge, the first attempt at applying reinforcement learning to a collection of physical robots learning a complex task consisting of multiple goals. Tan (1993) has applied traditional RL to a simulated multi-agent domain.
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990), </year> <title> Active Perception and Reinforcement Learning, </title> <booktitle> in `Proceedings, Seventh International Conference on Machine Learning', </booktitle> <address> Austin, Texas. </address>
Reference: <author> Whitehead, S. D., Karlsson, J. & Tenenberg, J. </author> <year> (1993), </year> <title> Learning Multiple Goal Behavior via Task Decomposition and Dynamic Policy Merging, </title> <editor> in J. H. Connell & S. Mahadevan, eds, </editor> <title> `Robot Learning', </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 45-78. </pages>
References-found: 25

