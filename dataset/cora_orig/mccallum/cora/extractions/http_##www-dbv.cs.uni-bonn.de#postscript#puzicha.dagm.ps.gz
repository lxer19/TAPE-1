URL: http://www-dbv.cs.uni-bonn.de/postscript/puzicha.dagm.ps.gz
Refering-URL: http://www-dbv.cs.uni-bonn.de/abstracts/puzicha.dagm.html
Root-URL: http://cs.uni-bonn.de
Email: fjan,jbg@cs.uni-bonn.de hofmann@ai.mit.edu  
Title: Discrete Mixture Models for Unsupervised Image Segmentation  
Author: Jan Puzicha Thomas Hofmann and Joachim M. Buhmann 
Affiliation: Institut fur Informatik III Artificial Intelligence Laboratory University of Bonn, Germany Massachusetts Institute of Technology  
Note: To appear in: Tagungsband Deutsche Arbeitsgemeinschaft fur Mustererkennung (DAGM), 1998.  
Abstract: This paper introduces a novel statistical mixture model for probabilistic clustering of histogram data and, more generally, for the analysis of discrete co-occurrence data. Adopting the maximum likelihood framework, an alternating maximization algorithm is derived which is combined with annealing techniques to overcome the inherent locality of alternating optimization schemes. We demonstrate an application of this method to the unsupervised segmentation of textured images based on local empirical distributions of Gabor coefficients. In order to accelerate the optimization process an efficient multiscale formulation is utilized. We present benchmark results on a representative set of Brodatz mondrians and real-world images. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Geman, S. Geman, C. Graffigne, and P. Dong. </author> <title> Boundary detection by constrained optimization. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 12(7) </volume> <pages> 609-628, </pages> <year> 1990. </year>
Reference-contexts: Since the Gaussian mixture assumption turns out to be inadequate in many cases, several alternative approaches have utilized pairwise proximity data, usually obtained by applying statistical tests to the local feature distribution at two image sites <ref> [1, 4, 7] </ref>. As a major advantage, these methods do not require the specification of a suitable vector-space metric. Instead, similarity is defined by the similarity of the respective feature distributions. <p> As a major advantage, these methods do not require the specification of a suitable vector-space metric. Instead, similarity is defined by the similarity of the respective feature distributions. For pairwise similarity data agglomerative clustering [7] and, more rigorously, optimization approaches to graph partitioning <ref> [1, 4, 12] </ref> have been proposed in the texture segmentation context, which we refer to as pairwise dissimilarity clustering (PDC). <p> The distributions n jji then represent a histogram of features occurring in an image neighborhood or window around some location x i <ref> [1, 4, 7] </ref>. The framework is, however, general enough to cover distinctive application domains like information retrieval [3] and natural language modeling [8]. <p> Typically, an identical number of features is observed for all sites, simplifying the equations to p i = n i = 1=N . Algorithms based on distributions n jji of features have been successfully used in texture analysis <ref> [1, 7, 4] </ref>. In the experiments, we have adopted the framework of [5, 4, 9] and utilized an image representation based on the modulus of complex Gabor filters. For each site the empirical distribution of coefficients in a surrounding (filter-specific) window is determined.
Reference: 2. <author> F. Heitz, P. Perez, and P. Bouthemy. </author> <title> Multiscale minimization of global energy functions in some visual recovery problems. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <volume> 59(1) </volume> <pages> 125-134, </pages> <year> 1994. </year>
Reference-contexts: This strategy is formalized by the concept of multiscale optimization <ref> [2] </ref> and it essentially leads to cost functions redefined on a coarsened 7 Median 20% quantile AM for ACM 8.9% 18% annealed ACM 6.7% 6% annealed PDC 6.0% 6% annealed K-means 11.7% 28% Table 1.
Reference: 3. <author> T. Hofmann and J. Puzicha. </author> <title> Statistical models for co-occurrence data. AI-Memo 1625, </title> <publisher> MIT, </publisher> <year> 1998. </year>
Reference-contexts: The distributions n jji then represent a histogram of features occurring in an image neighborhood or window around some location x i [1, 4, 7]. The framework is, however, general enough to cover distinctive application domains like information retrieval <ref> [3] </ref> and natural language modeling [8]. In the context of texture segmentation, each class C ff corresponds to a different texture which is characterized by a specific distribution q jjff of features y j . <p> The ACM is the most suitable model for image segmentation out of a family of novel mixture models developed for general co-occurrence data <ref> [3] </ref>. 4 3 Maximum Likelihood Estimation for the ACM To fit the model specified by (1) we apply the maximum likelihood principle and determine the parameter values with the highest probability to generate the observed data. <p> For ACM-based segmentation, cost-functions of identical algebraic structure are obtained at all levels. Deterministic annealing and multiscale optimization are combined in the concept of multiscale annealing. The resulting algorithm provides an acceleration factor of 5 - 500 compared to single scale optimization. For details we refer to <ref> [9, 3] </ref>. The question examined in detail is concerned with the benefits of the ACM in comparison to other clustering schemes. A typical example with K = 5 clusters is given in Fig. 1. It can be seen that the segmentations achieved by ACM and PDC are highly similar.
Reference: 4. <author> T. Hofmann, J. Puzicha, and J. Buhmann. </author> <title> Deterministic annealing for unsupervised texture segmentation. </title> <booktitle> In Proc. EMMCVPR'97, </booktitle> <volume> LNCS 1223, </volume> <pages> pages 213-228, </pages> <year> 1997. </year>
Reference-contexts: Since the Gaussian mixture assumption turns out to be inadequate in many cases, several alternative approaches have utilized pairwise proximity data, usually obtained by applying statistical tests to the local feature distribution at two image sites <ref> [1, 4, 7] </ref>. As a major advantage, these methods do not require the specification of a suitable vector-space metric. Instead, similarity is defined by the similarity of the respective feature distributions. <p> As a major advantage, these methods do not require the specification of a suitable vector-space metric. Instead, similarity is defined by the similarity of the respective feature distributions. For pairwise similarity data agglomerative clustering [7] and, more rigorously, optimization approaches to graph partitioning <ref> [1, 4, 12] </ref> have been proposed in the texture segmentation context, which we refer to as pairwise dissimilarity clustering (PDC). <p> The distributions n jji then represent a histogram of features occurring in an image neighborhood or window around some location x i <ref> [1, 4, 7] </ref>. The framework is, however, general enough to cover distinctive application domains like information retrieval [3] and natural language modeling [8]. <p> The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the generalized free energy. Details on this topic in the context of data clustering can be found in <ref> [10, 8, 4] </ref>. <p> Typically, an identical number of features is observed for all sites, simplifying the equations to p i = n i = 1=N . Algorithms based on distributions n jji of features have been successfully used in texture analysis <ref> [1, 7, 4] </ref>. In the experiments, we have adopted the framework of [5, 4, 9] and utilized an image representation based on the modulus of complex Gabor filters. For each site the empirical distribution of coefficients in a surrounding (filter-specific) window is determined. <p> Algorithms based on distributions n jji of features have been successfully used in texture analysis [1, 7, 4]. In the experiments, we have adopted the framework of <ref> [5, 4, 9] </ref> and utilized an image representation based on the modulus of complex Gabor filters. For each site the empirical distribution of coefficients in a surrounding (filter-specific) window is determined. All reported segmentations are based on a filter bank of twelve Gabor filters with four orientations and three scales.
Reference: 5. <author> A. Jain and F. Farrokhnia. </author> <title> Unsupervised texture segmentation using Gabor filters. </title> <journal> Pattern Recognition, </journal> <volume> 24(12) </volume> <pages> 1167-1186, </pages> <year> 1991. </year>
Reference-contexts: Numerous approaches to unsupervised texture segmentation have been proposed over the past decades. In the classical approaches, locally extracted features are spatially smoothed and interpreted as vectors in a metric space <ref> [5, 6] </ref>, thereby characterizing each texture by a specific average feature vector or centroid. The most commonly used distortion measure is the (weighted) squared Euclidean norm which effectively models the data by a Gaussian mixture model, where each Gaussian represents exactly one texture. <p> The method of choice for clustering vectorial data is the K-means algorithm and its variants, which have been exploited for texture segmentation in <ref> [5, 6] </ref>. Since the Gaussian mixture assumption turns out to be inadequate in many cases, several alternative approaches have utilized pairwise proximity data, usually obtained by applying statistical tests to the local feature distribution at two image sites [1, 4, 7]. <p> Algorithms based on distributions n jji of features have been successfully used in texture analysis [1, 7, 4]. In the experiments, we have adopted the framework of <ref> [5, 4, 9] </ref> and utilized an image representation based on the modulus of complex Gabor filters. For each site the empirical distribution of coefficients in a surrounding (filter-specific) window is determined. All reported segmentations are based on a filter bank of twelve Gabor filters with four orientations and three scales. <p> A database of random mixtures (512 fi 512 pixels each) containing 100 entities of five textures each (as depicted in Fig. 1) was constructed. For the K-means algorithm a spatial smoothing step was applied before clustering, see <ref> [5] </ref>. For all cost functions the multiscale annealing optimization scheme was implemented, where coarse grids up to a resolution of 8x8 grid points have been used. It is a natural assumption that adjacent image sites contain identical texture with high probability.
Reference: 6. <author> J. Mao and A. Jain. </author> <title> Texture classification and segmentation using multiresolution simultaneous autoregressive models. </title> <journal> Pattern Recognition, </journal> <volume> 25 </volume> <pages> 173-188, </pages> <year> 1992. </year>
Reference-contexts: Numerous approaches to unsupervised texture segmentation have been proposed over the past decades. In the classical approaches, locally extracted features are spatially smoothed and interpreted as vectors in a metric space <ref> [5, 6] </ref>, thereby characterizing each texture by a specific average feature vector or centroid. The most commonly used distortion measure is the (weighted) squared Euclidean norm which effectively models the data by a Gaussian mixture model, where each Gaussian represents exactly one texture. <p> The method of choice for clustering vectorial data is the K-means algorithm and its variants, which have been exploited for texture segmentation in <ref> [5, 6] </ref>. Since the Gaussian mixture assumption turns out to be inadequate in many cases, several alternative approaches have utilized pairwise proximity data, usually obtained by applying statistical tests to the local feature distribution at two image sites [1, 4, 7].
Reference: 7. <author> T. Ojala and M. Pietikainen. </author> <title> Unsupervised texture segmentation using feature distributions. </title> <type> Tech. Rep. </type> <institution> CAR-TR-837, Center for Robotics and Automation, University Maryland, </institution> <year> 1996. </year>
Reference-contexts: Since the Gaussian mixture assumption turns out to be inadequate in many cases, several alternative approaches have utilized pairwise proximity data, usually obtained by applying statistical tests to the local feature distribution at two image sites <ref> [1, 4, 7] </ref>. As a major advantage, these methods do not require the specification of a suitable vector-space metric. Instead, similarity is defined by the similarity of the respective feature distributions. <p> As a major advantage, these methods do not require the specification of a suitable vector-space metric. Instead, similarity is defined by the similarity of the respective feature distributions. For pairwise similarity data agglomerative clustering <ref> [7] </ref> and, more rigorously, optimization approaches to graph partitioning [1, 4, 12] have been proposed in the texture segmentation context, which we refer to as pairwise dissimilarity clustering (PDC). <p> The distributions n jji then represent a histogram of features occurring in an image neighborhood or window around some location x i <ref> [1, 4, 7] </ref>. The framework is, however, general enough to cover distinctive application domains like information retrieval [3] and natural language modeling [8]. <p> Typically, an identical number of features is observed for all sites, simplifying the equations to p i = n i = 1=N . Algorithms based on distributions n jji of features have been successfully used in texture analysis <ref> [1, 7, 4] </ref>. In the experiments, we have adopted the framework of [5, 4, 9] and utilized an image representation based on the modulus of complex Gabor filters. For each site the empirical distribution of coefficients in a surrounding (filter-specific) window is determined.
Reference: 8. <author> F. Pereira, N. Tishby, and L. Lee. </author> <title> Distributional clustering of English words. </title> <booktitle> In Proc. Association for Computational Linguistics, </booktitle> <pages> pages 181-190, </pages> <year> 1993. </year>
Reference-contexts: The major contribution of this paper is a general approach to the problem of grouping feature distributions, extending a technique known as distributional clustering in statistical language modeling <ref> [8] </ref>. In contrast to methods based on feature vectors and pairwise dissimilarities this approach is directly applicable to histogram data and empirical distributions. In comparison to K-means clustering, distributional clustering naturally includes component distributions with multiple modes rather than fitting segments with an univariate Gaussian mode. <p> The distributions n jji then represent a histogram of features occurring in an image neighborhood or window around some location x i [1, 4, 7]. The framework is, however, general enough to cover distinctive application domains like information retrieval [3] and natural language modeling <ref> [8] </ref>. In the context of texture segmentation, each class C ff corresponds to a different texture which is characterized by a specific distribution q jjff of features y j . Since these component distributions of the mixture model are not constrained, they can virtually model any distribution of features. <p> Eq. (4) averages over feature distributions, not over feature values. The formal similarity to K-means clustering is extended by (5), which is the analogon to the nearest neighbor rule. The ACM is similar to the distributional clustering model formulated in <ref> [8] </ref> as the minimization of the cost function H = i=1 ff=1 fi n jji jq jjff fl Here D denotes the cross entropy or Kullback-Leibler (KL) divergence. <p> The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the generalized free energy. Details on this topic in the context of data clustering can be found in <ref> [10, 8, 4] </ref>.
Reference: 9. <author> J. Puzicha and J. Buhmann. </author> <title> Multiscale annealing for real-time unsupervised texture segmentation. </title> <type> Technical Report IAI-97-4, </type> <note> Institut fur Informatik III (a short version appeared in: Proc. </note> <month> ICCV'98, </month> <pages> pp. 267-273), </pages> <year> 1997. </year>
Reference-contexts: Although these methods are directly applicable to proximity data, they are only tractable in image segmentation problems if they avoid the computation of dissimilarities for all possible pairs of sites <ref> [9] </ref>. The major contribution of this paper is a general approach to the problem of grouping feature distributions, extending a technique known as distributional clustering in statistical language modeling [8]. <p> Another important consideration for a clustering approach to image segmentation are real-time constraints. Given the respective data (vectors, histograms or proximities) all algorithms require only a few seconds for optimization <ref> [9] </ref>. While vector-based methods suffer from inferior quality, it is the data extraction process of PDC which is prohibitive for real-time applications like autonomous robotics. Using the histogram data directly avoids the necessity for pairwise comparisons altogether while achieving segmentations of similar quality compared to PDC. <p> Algorithms based on distributions n jji of features have been successfully used in texture analysis [1, 7, 4]. In the experiments, we have adopted the framework of <ref> [5, 4, 9] </ref> and utilized an image representation based on the modulus of complex Gabor filters. For each site the empirical distribution of coefficients in a surrounding (filter-specific) window is determined. All reported segmentations are based on a filter bank of twelve Gabor filters with four orientations and three scales. <p> For ACM-based segmentation, cost-functions of identical algebraic structure are obtained at all levels. Deterministic annealing and multiscale optimization are combined in the concept of multiscale annealing. The resulting algorithm provides an acceleration factor of 5 - 500 compared to single scale optimization. For details we refer to <ref> [9, 3] </ref>. The question examined in detail is concerned with the benefits of the ACM in comparison to other clustering schemes. A typical example with K = 5 clusters is given in Fig. 1. It can be seen that the segmentations achieved by ACM and PDC are highly similar.
Reference: 10. <author> K. Rose, E. Gurewitz, and G. Fox. </author> <title> A deterministic annealing approach to clustering. </title> <journal> Pattern Recognition Letters, </journal> <volume> 11 </volume> <pages> 589-594, </pages> <year> 1990. </year>
Reference-contexts: The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the generalized free energy. Details on this topic in the context of data clustering can be found in <ref> [10, 8, 4] </ref>.
Reference: 11. <author> P. Schroeter and J. Bigun. </author> <title> Hierarchical image segmentation by multi-dimensional clustering and orientation-adaptive boundary refinement. </title> <journal> Pattern Recognition, </journal> <volume> 28(5) </volume> <pages> 695-709, </pages> <year> 1995. </year>
Reference-contexts: In addition, the proposed mixture model provides a generative statistical model for the observed features by defining a texture specific distribution. This can be utilized in subsequent processing steps such as boundary localization <ref> [11] </ref>. 3 2 Mixture Models for Histogram Data To stress the generality of the proposed model we temporarily detach the presentation from the specific problem of image segmentation.
Reference: 12. <author> J. Shi and J. Malik. </author> <title> Normalized cuts and image segmentation. </title> <booktitle> In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR'97), </booktitle> <pages> pages 731-737, </pages> <year> 1997. </year>
Reference-contexts: As a major advantage, these methods do not require the specification of a suitable vector-space metric. Instead, similarity is defined by the similarity of the respective feature distributions. For pairwise similarity data agglomerative clustering [7] and, more rigorously, optimization approaches to graph partitioning <ref> [1, 4, 12] </ref> have been proposed in the texture segmentation context, which we refer to as pairwise dissimilarity clustering (PDC).
References-found: 12

