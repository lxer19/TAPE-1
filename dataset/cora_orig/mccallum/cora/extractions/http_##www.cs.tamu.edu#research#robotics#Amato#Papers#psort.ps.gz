URL: http://www.cs.tamu.edu/research/robotics/Amato/Papers/psort.ps.gz
Refering-URL: http://www.cs.tamu.edu/people/ravi/
Root-URL: http://www.cs.tamu.edu
Email: amato@cs.tamu.edu ravi@cs.tamu.edu sharad@cs.tamu.edu yanwu@cs.tamu.edu  
Title: A Comparison of Parallel Sorting Algorithms on Different Architectures  
Author: Nancy M. Amato Ravishankar Iyer Sharad Sundaresan Yan Wu 
Address: College Station, TX 77843-3112  
Affiliation: Department of Computer Science, Texas A&M University,  
Abstract: In this paper, we present a comparative performance evaluation of three different parallel sorting algorithms: bitonic sort, sample sort, and parallel radix sort. In order to study the interaction between the algorithms and the architecture, we implemented all the algorithms on three different architectures: a MasPar MP1202, a mesh-connected computer with 2048 processing elements; an nCUBE 2, a message-passing hypercube with 32 processors; and a Sequent Balance, a distributed shared-memory machine with 10 processors. For each machine, we found that the choice of algorithm depends upon the number of elements to be sorted. In addition, as expected, our results show that the relative performance of the algorithms differed on the various machines. It is our hope that our results can be extrapolated to help select appropriate candidates for implementation on machines with architectures similar to those that we have studied. As evidence for this, our findings on the nCUBE 2, a 32 node hypercube, are in accordance with the results obtained by Blelloch et al. [5] on the CM-2, a hypercube with 1024 processors. In addition, preliminary results we have obtained on the SGI Power Challenge, a distributed shared-memory machine, are in accordance with our findings on the Sequent Balance. fl This research supported in part by NCSA.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> Sorting in c log n parallel steps. </title> <journal> Combinatorica, </journal> <volume> 3 </volume> <pages> 1-19, </pages> <year> 1983. </year>
Reference-contexts: Since then many different parallel sorting algorithms have been proposed (see, e.g., [4, 11, 17]), of which we mention only a few here. The first fi (log n)-depth sorting circuit was discovered by Ajtai, Komlos, and Szemeredi <ref> [1] </ref>; this result is mainly of theoretical importance however, since the constants in their construction are quite large. Around that same time, Reif and Valiant proposed a more practical O (log n) time randomized algorithm called flashsort [16].
Reference: [2] <author> K. E. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In AFIPS Springer Joing Computer Conference, </booktitle> <pages> pages 307-314, </pages> <address> Arlington,VA, </address> <month> April </month> <year> 1968. </year>
Reference-contexts: 1 Introduction Sorting is one of the fundamental problems of computer science, and parallel algorithms for sorting have been studied since the beginning of parallel computing. Batcher's fi (log 2 n)-depth bitonic sorting network <ref> [2] </ref> was one of the first methods proposed. Since then many different parallel sorting algorithms have been proposed (see, e.g., [4, 11, 17]), of which we mention only a few here. <p> In order to facilitate comparison to previous work, we selected the same algorithms used in the study on the CM-2 by Blelloch et al. [5]; refer to their paper for background on the selection of these algorithms. 3.1 Bitonic Sort Batcher's Bitonic sort <ref> [2] </ref> is a parallel sorting algorithm whose main operation is a technique for merging two bitonic sequences. A bitonic sequence is the concatenation of an ascending and a descending sequence of numbers. For example, 2; 4; 6; 8; 9; 24; 6; 3; 2; 0 is a bitonic sequence.
Reference: [3] <author> G. Bilardi and A. Nicolau. </author> <title> Adaptive bitonic sorting: An optimal parallel algorithm for shared-memory machines. </title> <journal> SIAM J. Comput., </journal> <volume> 18(2) </volume> <pages> 216-228, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: A bitonic sequence is the concatenation of an ascending and a descending sequence of numbers. For example, 2; 4; 6; 8; 9; 24; 6; 3; 2; 0 is a bitonic sequence. Various adaptations of Batcher's original algorithm have been proposed, e.g., <ref> [3, 14] </ref>. To sort a sequence of n numbers, the Batcher's algorithm proceeds as follows. The first step is to convert the n numbers into a bitonic sequence with n=2 numbers in an increasing subsequence and n=2 numbers in a decreasing subsequence.
Reference: [4] <author> G. E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Batcher's fi (log 2 n)-depth bitonic sorting network [2] was one of the first methods proposed. Since then many different parallel sorting algorithms have been proposed (see, e.g., <ref> [4, 11, 17] </ref>), of which we mention only a few here. The first fi (log n)-depth sorting circuit was discovered by Ajtai, Komlos, and Szemeredi [1]; this result is mainly of theoretical importance however, since the constants in their construction are quite large. <p> Around that same time, Reif and Valiant proposed a more practical O (log n) time randomized algorithm called flashsort [16]. Some other notable parallel sorting algorithms are parallel versions of radix sort and quicksort <ref> [4, 17] </ref>, column sort [10], and Cole's parallel merge sort [6]. Given the large number of parallel sorting algorithms and the wide variety of parallel architectures, it is a difficult task to select the best algorithm for a particular machine and problem instance.
Reference: [5] <author> G. E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J. Smith, and M. Zagha. </author> <title> A comparison of sorting algorithms for the Connection Machine CM-2. </title> <booktitle> In Annual ACM Symp. Paral. Algor. Arch., </booktitle> <pages> pages 3-16, </pages> <year> 1991. </year>
Reference-contexts: Thus, experimental studies take on an increased importance for the evaluation and selection of appropriate algorithms for multiprocessors. There have been a number of implementation studies reported in the literature in the past few years (see, e.g., <ref> [5, 12] </ref>). However, more studies are needed before we can approach the point where a certain algorithm can be recommended for a particular machine with any degree of confidence. <p> As evidence for this, our findings on the nCUBE 2, a 32 node hypercube, are in accordance with the results obtained by Blelloch et al. <ref> [5] </ref> on the CM-2, a hypercube with 1024 processors. In addition, preliminary results we have obtained on the SGI Power Challenge, a distributed shared-memory machine, are in accordance with our findings on the Sequent Balance. 1 2 Machine Descriptions The MasPar MP1202. <p> To make the analysis as general as possible, the complexity of each algorithm is reported in terms of local computation L, and interprocessor communication C. In order to facilitate comparison to previous work, we selected the same algorithms used in the study on the CM-2 by Blelloch et al. <ref> [5] </ref>; refer to their paper for background on the selection of these algorithms. 3.1 Bitonic Sort Batcher's Bitonic sort [2] is a parallel sorting algorithm whose main operation is a technique for merging two bitonic sequences. <p> sort gives f (n; r) = O (n + 2 r ) [7], the complexity of sequential radix sort is T srs (n) = O ((b=r)(n + 2 r )): (4) The simplest way to parallelize radix sort is to use a parallel version of counting sort (also done in <ref> [5] </ref>). Let p be the number of processors available. In the parallel counting sort, each processor is responsible for dn=pe input elements. First, each processor counts the number of its elements with each possible value (in [0; 2 r )), and then computes the prefix sums of these counts. <p> Our results are in agreement with those found by Blelloch et al. for larger values of n on the CM-2, a hypercube with 1024 processors <ref> [5] </ref>. The advantage of sample sort over the other algorithms is that it minimized data movement and interprocessor communication, which is an important consideration for message passing computers.
Reference: [6] <author> R. Cole. </author> <title> Parallel merge sort. </title> <journal> SIAM J. Comput., </journal> <volume> 17(4), </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: Around that same time, Reif and Valiant proposed a more practical O (log n) time randomized algorithm called flashsort [16]. Some other notable parallel sorting algorithms are parallel versions of radix sort and quicksort [4, 17], column sort [10], and Cole's parallel merge sort <ref> [6] </ref>. Given the large number of parallel sorting algorithms and the wide variety of parallel architectures, it is a difficult task to select the best algorithm for a particular machine and problem instance.
Reference: [7] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Step 3 uses expected time T srs (n=p) (by Equation 4) in local computation and O (n=p) time in communication. Therefore, the expected time of the parallel sample sort is 3.3 Parallel Radix Sort Radix sort <ref> [7] </ref> is a relatively simple algorithm that is quite easy to parallelize. Since it is not a comparison sort (i.e., it is not based only on comparisons), radix sort is not subject to the (n log n) lower bound for comparison-based sorting. <p> The sorting algorithm used by radix sort must be stable, i.e., the 5 relative order of two items with the same key must be the same in both the input and output orderings. Counting sort <ref> [7] </ref> is often used as the stable sorting algorithm in radix sort. In counting sort, the input elements must be integers in a fixed range; for radix sort the elements are integers in [0; 2 r ). <p> Since a standard implementation of counting sort gives f (n; r) = O (n + 2 r ) <ref> [7] </ref>, the complexity of sequential radix sort is T srs (n) = O ((b=r)(n + 2 r )): (4) The simplest way to parallelize radix sort is to use a parallel version of counting sort (also done in [5]). Let p be the number of processors available.
Reference: [8] <author> W. D. Frazer and A. C. McKellar. Samplesort: </author> <title> A sampling approach to minimal storage tree sorting. </title> <journal> J. ACM, </journal> <volume> 17(3) </volume> <pages> 496-507, </pages> <year> 1970. </year>
Reference-contexts: bitonic sort is T bs 2 = O (T rs ( 2 n )L + ( p 3.2 Sample Sort There have been many sorting algorithms proposed that use a random sample of the input elements to partition the input into distinct subproblems that can be sorted independently (see, e.g., <ref> [8, 9, 16, 19] </ref>). The basic version we implemented consists of the following three phases. First, a random sample of p 1 splitter elements S = fs 1 ; s 2 ; : : : ; s p1 g is selected from the n input elements.
Reference: [9] <author> J. S. Huang and Y. C. Chow. </author> <title> Parallel sorting and data partitioning by sampling. </title> <booktitle> In Proceedings of the IEEE Computer Society's Seventh International Computer Software and Applications Conference, </booktitle> <pages> pages 627-631, </pages> <year> 1983. </year> <month> 12 </month>
Reference-contexts: bitonic sort is T bs 2 = O (T rs ( 2 n )L + ( p 3.2 Sample Sort There have been many sorting algorithms proposed that use a random sample of the input elements to partition the input into distinct subproblems that can be sorted independently (see, e.g., <ref> [8, 9, 16, 19] </ref>). The basic version we implemented consists of the following three phases. First, a random sample of p 1 splitter elements S = fs 1 ; s 2 ; : : : ; s p1 g is selected from the n input elements.
Reference: [10] <author> F. T. Leighton. </author> <title> Tight bounds on the complexity of parallel sorting. </title> <journal> IEEE Trans. Comput., </journal> <volume> 34(4):344--354, </volume> <year> 1985. </year>
Reference-contexts: Around that same time, Reif and Valiant proposed a more practical O (log n) time randomized algorithm called flashsort [16]. Some other notable parallel sorting algorithms are parallel versions of radix sort and quicksort [4, 17], column sort <ref> [10] </ref>, and Cole's parallel merge sort [6]. Given the large number of parallel sorting algorithms and the wide variety of parallel architectures, it is a difficult task to select the best algorithm for a particular machine and problem instance.
Reference: [11] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Batcher's fi (log 2 n)-depth bitonic sorting network [2] was one of the first methods proposed. Since then many different parallel sorting algorithms have been proposed (see, e.g., <ref> [4, 11, 17] </ref>), of which we mention only a few here. The first fi (log n)-depth sorting circuit was discovered by Ajtai, Komlos, and Szemeredi [1]; this result is mainly of theoretical importance however, since the constants in their construction are quite large.
Reference: [12] <author> H. Li and K. C. Sevcik. </author> <title> Parallel sorting by overpartitioning. </title> <booktitle> In Annual ACM Symp. Paral. Algor. Arch., </booktitle> <pages> pages 46-56, </pages> <year> 1994. </year>
Reference-contexts: Thus, experimental studies take on an increased importance for the evaluation and selection of appropriate algorithms for multiprocessors. There have been a number of implementation studies reported in the literature in the past few years (see, e.g., <ref> [5, 12] </ref>). However, more studies are needed before we can approach the point where a certain algorithm can be recommended for a particular machine with any degree of confidence.
Reference: [13] <institution> MasPar Computer Corporation. MPPE User Guide, PN9305-0000, </institution> <year> 1992. </year>
Reference-contexts: In addition, preliminary results we have obtained on the SGI Power Challenge, a distributed shared-memory machine, are in accordance with our findings on the Sequent Balance. 1 2 Machine Descriptions The MasPar MP1202. The MasPar <ref> [13] </ref> is a Single Instruction Multiple Data (SIMD) machine. The system used in this study consists of a front-end DECstation 5000, a MP1202 Data Parallel Unit (DPU) with 2,048 processing elements (PEs), and a MasPar Parallel Disk Array (MPDA).
Reference: [14] <author> D. Nassimi and S. Sahni. </author> <title> Bitonic sort on a mesh-connected parallel computer. </title> <journal> IEEE Trans. Comput., </journal> <volume> c-27(1), </volume> <month> January </month> <year> 1979. </year> <title> [15] nCUBE Corporation 1990. nCUBE2 Processor Manual, </title> <year> 1990. </year>
Reference-contexts: A bitonic sequence is the concatenation of an ascending and a descending sequence of numbers. For example, 2; 4; 6; 8; 9; 24; 6; 3; 2; 0 is a bitonic sequence. Various adaptations of Batcher's original algorithm have been proposed, e.g., <ref> [3, 14] </ref>. To sort a sequence of n numbers, the Batcher's algorithm proceeds as follows. The first step is to convert the n numbers into a bitonic sequence with n=2 numbers in an increasing subsequence and n=2 numbers in a decreasing subsequence. <p> All experiments were repeated 25 times in single-user mode, and the results reported are averaged over the 25 runs. 4.1 The MasPar MP1202 Bitonic sort. Our implementation of bitonic sort on the MasPar was based on the mesh sort of Nassimi and Sahni <ref> [14] </ref>, which is an adaptation of bitonic sort suitable for mesh-connected machines. The mesh sort of Nassimi and Sahni could be implemented using only X-Net communication (to the 8 local neighbors). Recall that on the MasPar, communication using the X-net is much faster than the global router. <p> Of course, more studies will be needed to further substantiate the algorithm/architecture recommendations we make below. On the MasPar MP1202, essentially a mesh-connected computer, we found that the version of bitonic sort due to Nassimi and Sahni <ref> [14] </ref> was the fastest for smaller input sizes and that parallel radix sort was the fastest for larger input sizes.
Reference: [16] <author> J. H. Reif and L. G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <journal> Journal of the ACM, </journal> <volume> 34(1) </volume> <pages> 60-76, </pages> <year> 1987. </year>
Reference-contexts: Around that same time, Reif and Valiant proposed a more practical O (log n) time randomized algorithm called flashsort <ref> [16] </ref>. Some other notable parallel sorting algorithms are parallel versions of radix sort and quicksort [4, 17], column sort [10], and Cole's parallel merge sort [6]. <p> bitonic sort is T bs 2 = O (T rs ( 2 n )L + ( p 3.2 Sample Sort There have been many sorting algorithms proposed that use a random sample of the input elements to partition the input into distinct subproblems that can be sorted independently (see, e.g., <ref> [8, 9, 16, 19] </ref>). The basic version we implemented consists of the following three phases. First, a random sample of p 1 splitter elements S = fs 1 ; s 2 ; : : : ; s p1 g is selected from the n input elements.
Reference: [17] <editor> John H. Reif, editor. </editor> <title> Synthesis of Parallel Algorithms. </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, Ca, </address> <year> 1993. </year>
Reference-contexts: Batcher's fi (log 2 n)-depth bitonic sorting network [2] was one of the first methods proposed. Since then many different parallel sorting algorithms have been proposed (see, e.g., <ref> [4, 11, 17] </ref>), of which we mention only a few here. The first fi (log n)-depth sorting circuit was discovered by Ajtai, Komlos, and Szemeredi [1]; this result is mainly of theoretical importance however, since the constants in their construction are quite large. <p> Around that same time, Reif and Valiant proposed a more practical O (log n) time randomized algorithm called flashsort [16]. Some other notable parallel sorting algorithms are parallel versions of radix sort and quicksort <ref> [4, 17] </ref>, column sort [10], and Cole's parallel merge sort [6]. Given the large number of parallel sorting algorithms and the wide variety of parallel architectures, it is a difficult task to select the best algorithm for a particular machine and problem instance.
Reference: [18] <institution> Sequent Systems. </institution> <note> Guide to Parallel Programming, </note> <year> 1987. </year>
Reference-contexts: Communication synchronization, error reporting, program exceptions and software facilities are handled using vectored interrupts. Programs on the nCUBE are written in an extended version of C, and can be executed on a sub-cube of a specified dimension. The Sequent Balance. The Sequent Balance <ref> [18] </ref> is Multiple Instruction Multiple Data (MIMD) distributed shared-memory multiprocessor. It is a tightly-coupled shared-memory system in which all processors are connected to a common set of memory modules by a single high-speed bus.
Reference: [19] <author> Y. Won and S. Sahni. </author> <title> A balanced bin sort for hypercube multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 435-448, </pages> <booktitle> 1988. </booktitle> <volume> 14 15 16 17 18 </volume>
Reference-contexts: bitonic sort is T bs 2 = O (T rs ( 2 n )L + ( p 3.2 Sample Sort There have been many sorting algorithms proposed that use a random sample of the input elements to partition the input into distinct subproblems that can be sorted independently (see, e.g., <ref> [8, 9, 16, 19] </ref>). The basic version we implemented consists of the following three phases. First, a random sample of p 1 splitter elements S = fs 1 ; s 2 ; : : : ; s p1 g is selected from the n input elements.
References-found: 18

