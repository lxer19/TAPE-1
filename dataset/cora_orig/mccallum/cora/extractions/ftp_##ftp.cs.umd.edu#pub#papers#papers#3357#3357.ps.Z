URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3357/3357.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Configuration-level optimization of RPC-based distributed programs  junction with the Common Prototyping Language project.  
Author: Tae-Hyung Kim James M. Purtilo 
Note: With oversight by Office of Naval Research, this research is supported by ARPA/ISTO in con  
Address: College Park, MD 20742  
Affiliation: Computer Science Department and Institute for Advanced Computer Studies University of Maryland  
Abstract: Many strategies for improving performance of distributed programs can be described abstractly in terms of an application's overall configuration. But previously those techniques would need to be implemented manually, and the resulting programs, though yielding good performance, are more expensive to build and much less easy to reuse. This paper describes research towards an automatic system for introducing performance improvement techniques based upon an application's configuration description. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley. </publisher>
Reference-contexts: space, the translation is straight forward, because we can decide the earliest time to send a request and the latest time to receive a result based on DU C l (Definition-Use-Chain of l-value) and U DC r (Use-Definition-Chain of r-value) sets, that can be evaluated through use-def and def-use analyses <ref> [1] </ref>. The development of an application in CORD consists of a number of steps. At some point, each module used in the application must be given an implementation, each dealing with interfaces in generic RPC terms, of course.
Reference: [2] <author> A. L. Ananda, B. H. Tay, and E. K. Koh. </author> <title> Astra An asynchronous remote procedure call facility. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 172-179, </pages> <year> 1991. </year>
Reference-contexts: An asynchronous call does not block the client, and replies can be received as they are needed. To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [4] or they may be asynchronous only <ref> [2, 19, 24] </ref>), or the decision has to be made at module programming level by use of different library routines [8]. If we let this decision be separate from RPC statement, the modules will remain reusable for different calling styles.
Reference: [3] <author> H. E. Bal, J. G. Steiner, and A. S. Tanenbaum. </author> <title> Programming languages for distributed computing systems. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 21(3) </volume> <pages> 260-322, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: 1 INTRODUCTION Writing distributed programs is difficult for programmers, and even more difficult when high performance is required. Many mechanisms to achieve better performance in distributed programming have been proposed <ref> [3, 12, 13, 14, 19] </ref>; however, in practice these mechanisms are hard to utilize, and do not take into account the burden placed on programmers who already encounter difficulty in writing functionally correct programs. <p> Furthermore, most of these mechanisms are expressed by special programming language constructs for specifying the exact semantics on communication and synchronization <ref> [3] </ref>. Such languages are not good at accommodating the programming skills of those who are already accustomed to conventional programming languages like C.
Reference: [4] <author> A. D. Birrell and B. J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: The former reflects "data parallelism" and the later, "functional parallelism". RPC <ref> [4, 8] </ref> is a popular paradigm for distributed programming since it simplifies distributed program construction by abstracting away from details of communication and synchronization. <p> When these factors are separated from individual module construction, the modules themselves can be more easily programmed as well as more reusable [9]. 3.1.1 Calling Style A synchronous call is a call whereby the client blocks the call until the server completes it <ref> [4] </ref>. An asynchronous call does not block the client, and replies can be received as they are needed. To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [4] or they may be asynchronous only [2, 19, 24]), or the decision has to <p> is a call whereby the client blocks the call until the server completes it <ref> [4] </ref>. An asynchronous call does not block the client, and replies can be received as they are needed. To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [4] or they may be asynchronous only [2, 19, 24]), or the decision has to be made at module programming level by use of different library routines [8]. If we let this decision be separate from RPC statement, the modules will remain reusable for different calling styles.
Reference: [5] <author> John Callahan and James Purtilo. </author> <title> A packaging system for heterogeneous execution environments. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 17(6) </volume> <pages> 626-635, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: This builds upon the MIL (Module Interconnection Language) approach <ref> [5, 21] </ref> for distributed programming, where the original MIL specification is intended for structural presentation of interfaces between interacting processes. We append performance related specifications onto each interface specification in a MIL. <p> The configuration language chosen for expressing modules and their compositions is derived from the Polylith module interconnection language, and the distributed run time environment chosen is likewise the software bus behind Polylith. Basic tools for preparing applications to run in this environment are already available within the Polygen system <ref> [5] </ref>, although they are to be tailored to attain our source translation (rather than stub generation) principle. Therefore the principle thrust of our effort has been to add a source translator (gen trans) to the suite of Polygen tools.
Reference: [6] <author> Clemens H. Cap and Volker Strumpen. </author> <title> Efficient parallel computing in distributed workstation environments. </title> <journal> Parallel Computing, </journal> <volume> Vol. 19 </volume> <pages> 1221-1234, </pages> <year> 1993. </year>
Reference-contexts: Many dynamic load balancing algorithms have been devised for such an efficient migration <ref> [6, 10, 17, 18, 23] </ref>; they are characterized by the following parameters which distinguish them. Load balancing algorithms can be fine tuned when programmers can change those factors conveniently. * Topology: Topology determines the shape of task migration paths.
Reference: [7] <author> N. Carriero and D. Gelernter. </author> <title> How to write parallel programs: A first course. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Length of communication paths: RPC can lengthen communication paths unnecessarily if involved modules form a computation network (like the trellis model in Chapter 8 of <ref> [7] </ref>) because of its two-way communication protocol. For instance, in Figure 1 (c), an intermediate result in each stage of the compare module must go back to the client first before being delivered to the next stage. An optimization step that eliminates such unnecessary communication paths is called for. <p> Second, the scheme does not allow overlap between communication and computation because the next task can not be issued unless the current one has been finished. To alleviate these problems, watermarking can be used. It was originally used to control overload <ref> [7] </ref>, but it can also be used to avoid underload, which is caused by latency. Good watermark enables a master to send a stream of task service requests; as a result, a worker does not sit idle while demanding more tasks.
Reference: [8] <author> John R. Corbin. </author> <title> SUN RPC:The art of distributed applications. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The former reflects "data parallelism" and the later, "functional parallelism". RPC <ref> [4, 8] </ref> is a popular paradigm for distributed programming since it simplifies distributed program construction by abstracting away from details of communication and synchronization. <p> To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [4] or they may be asynchronous only [2, 19, 24]), or the decision has to be made at module programming level by use of different library routines <ref> [8] </ref>. If we let this decision be separate from RPC statement, the modules will remain reusable for different calling styles.
Reference: [9] <author> F. DeRemer and H. Kron. </author> <title> Programming-in-the-large versus programming-in-the-small. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 2(2), </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: Module interconnection activity is understood to be an essentially distinct and different intellectual activity from that of implementing individual modules, that is "programming-in-the-large" is distinct from "programming-in-the-small" <ref> [9] </ref>. Analogously, this observation applies to performance programming as well. Decisions concerning how a configuration might be adapted in order to allow use of performance improvement mechanisms are inherently different from the task of tailoring individual program units and their interfaces to execute as dictated by the abstract decision. <p> All of these factors are related in module interactions rather than functionality; thus they will be represented at the interconnection programming level. When these factors are separated from individual module construction, the modules themselves can be more easily programmed as well as more reusable <ref> [9] </ref>. 3.1.1 Calling Style A synchronous call is a call whereby the client blocks the call until the server completes it [4]. An asynchronous call does not block the client, and replies can be received as they are needed.
Reference: [10] <author> Derek L. Eager, Edward D. Lazowska, and John Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 12(5) </volume> <pages> 662-675, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Many dynamic load balancing algorithms have been devised for such an efficient migration <ref> [6, 10, 17, 18, 23] </ref>; they are characterized by the following parameters which distinguish them. Load balancing algorithms can be fine tuned when programmers can change those factors conveniently. * Topology: Topology determines the shape of task migration paths.
Reference: [11] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Figure 4 (c) is optimized because module `h' can receive `v' independently of module `g'; moreover, the value `v' can be multicast if multicasting primitives are available in the underlying message passing environments. 3.2 Load Balancing Fox et al. <ref> [11] </ref> demonstrated that the SPMD model is a natural paradigm for a large number of problems in science and engineering. This model can similarly be expressed by RPC paradigm with the aid of replication expressions in configuration programs, but load must be balanced among workers to insure good performance.
Reference: [12] <author> N. H. Gehani. </author> <title> Concurrent C. </title> <journal> Journal of Software Practice and Experience, </journal> <volume> Vol. 16 </volume> <pages> 821-844, </pages> <month> September </month> <year> 1986. </year> <month> 16 </month>
Reference-contexts: 1 INTRODUCTION Writing distributed programs is difficult for programmers, and even more difficult when high performance is required. Many mechanisms to achieve better performance in distributed programming have been proposed <ref> [3, 12, 13, 14, 19] </ref>; however, in practice these mechanisms are hard to utilize, and do not take into account the burden placed on programmers who already encounter difficulty in writing functionally correct programs.
Reference: [13] <author> N. H. Gehani. </author> <title> Message passing in concurrent C: Synchronous versus asynchronous. </title> <journal> Journal of Software Practice and Experience, </journal> <volume> Vol. 20(6) </volume> <pages> 571-592, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION Writing distributed programs is difficult for programmers, and even more difficult when high performance is required. Many mechanisms to achieve better performance in distributed programming have been proposed <ref> [3, 12, 13, 14, 19] </ref>; however, in practice these mechanisms are hard to utilize, and do not take into account the burden placed on programmers who already encounter difficulty in writing functionally correct programs.
Reference: [14] <author> W. M. Gentleman. </author> <title> Message passing between sequential processes: The reply primitive and the administrator concept. </title> <journal> Journal of Software Practice and Experience, </journal> <volume> Vol. 11 </volume> <pages> 435-466, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: 1 INTRODUCTION Writing distributed programs is difficult for programmers, and even more difficult when high performance is required. Many mechanisms to achieve better performance in distributed programming have been proposed <ref> [3, 12, 13, 14, 19] </ref>; however, in practice these mechanisms are hard to utilize, and do not take into account the burden placed on programmers who already encounter difficulty in writing functionally correct programs.
Reference: [15] <author> D. K. Gifford and N. Glasser. </author> <title> Remote pipes and procedures for efficient distributed communication. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 6(3) </volume> <pages> 258-283, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Even worse, contemporary RPC systems are optimized to transmit limited amounts of data (usually less than 10 3 bytes) per call. To support the incremental and bulk rate data transfer, wherein conventional RPC systems performance suffers severely, a new communication model called remote pipe <ref> [15] </ref> has been devised. In the framework we are motivating, these patterns may be efficiently handled with automatic communication optimization if programmers specify which communication pattern will appear.
Reference: [16] <author> J. L. Gustafson, R. E. Benner, M. P. Sears, and T. D. Sullivan. </author> <title> A radar simulation program for a 1024-processor hypercube. </title> <booktitle> In Proceedings of SuperComputing 1989, </booktitle> <pages> pages 96-105, </pages> <year> 1989. </year>
Reference-contexts: This scheme contains two problems. First, the master process can generate a bottleneck <ref> [16] </ref>. For example, if there are 1000 workers and a master needs 10 3 second to prepare and send a task, the master would create a bottleneck unless the average time for each worker to finish a task is greater than a second.
Reference: [17] <author> Philip Krueger and Niranjan G. Shivaratri. </author> <title> Adaptive location policies for global scheduling. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 20(6) </volume> <pages> 432-444, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Many dynamic load balancing algorithms have been devised for such an efficient migration <ref> [6, 10, 17, 18, 23] </ref>; they are characterized by the following parameters which distinguish them. Load balancing algorithms can be fine tuned when programmers can change those factors conveniently. * Topology: Topology determines the shape of task migration paths.
Reference: [18] <author> Frank C. H. Lin and Robert M. Keller. </author> <title> The gradient model load balancing method. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 13(1) </volume> <pages> 32-38, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Many dynamic load balancing algorithms have been devised for such an efficient migration <ref> [6, 10, 17, 18, 23] </ref>; they are characterized by the following parameters which distinguish them. Load balancing algorithms can be fine tuned when programmers can change those factors conveniently. * Topology: Topology determines the shape of task migration paths.
Reference: [19] <author> B. Liskov and L. Shrira. </author> <title> Promises: Linguistic support for efficient asynchronous procedure calls in distributed systems. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 260-267, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 INTRODUCTION Writing distributed programs is difficult for programmers, and even more difficult when high performance is required. Many mechanisms to achieve better performance in distributed programming have been proposed <ref> [3, 12, 13, 14, 19] </ref>; however, in practice these mechanisms are hard to utilize, and do not take into account the burden placed on programmers who already encounter difficulty in writing functionally correct programs. <p> An asynchronous call does not block the client, and replies can be received as they are needed. To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [4] or they may be asynchronous only <ref> [2, 19, 24] </ref>), or the decision has to be made at module programming level by use of different library routines [8]. If we let this decision be separate from RPC statement, the modules will remain reusable for different calling styles.
Reference: [20] <author> Bruce Martin, Charles Bergan, and Brian Russ. PARPC: </author> <title> A system for parallel remote procedure calls. </title> <booktitle> In Proceedings of the International Conferences on Parallel Processing, </booktitle> <pages> pages 449-452, </pages> <year> 1987. </year>
Reference-contexts: In Figure 1 (b), no workers should be idle while others are busy. So far, RPC in itself does not make any association with load balancing. Previous RPC systems for multiple servers like PARPC <ref> [20] </ref> and MultiRPC [22] have been devised, but they do not deal with load balancing since their main purpose is fault tolerance rather than good performance. 2. Scheduling: In our example, the length of each DNA sequence varies, so does comparison time.
Reference: [21] <author> James Purtilo. </author> <title> The polylith software bus. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 16(1) </volume> <pages> 151-174, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: This builds upon the MIL (Module Interconnection Language) approach <ref> [5, 21] </ref> for distributed programming, where the original MIL specification is intended for structural presentation of interfaces between interacting processes. We append performance related specifications onto each interface specification in a MIL. <p> Table 1 (a) compares the 3 The components of the MIL program are the module descriptions and the application description. See <ref> [21] </ref>. 14 performance between synchronous and asynchronous RPC where the computation is run on each of several different servers in turn. (To be concrete, `harvey' is SparcStation IPC, `rimfire' is SparcStation IPX, `thumper' is SparcStation 2, and `highpower' is SparcStation 10: the broad spectrum of computing power in these machines is
Reference: [22] <author> M. Satyanarayanan and E. H. Siegel. MultiRPC: </author> <title> A parallel remote procedure call mechanism. </title> <type> Technical Report CMU-CS-86-139, </type> <institution> Carnegie-Mellon University, </institution> <year> 1986. </year>
Reference-contexts: In Figure 1 (b), no workers should be idle while others are busy. So far, RPC in itself does not make any association with load balancing. Previous RPC systems for multiple servers like PARPC [20] and MultiRPC <ref> [22] </ref> have been devised, but they do not deal with load balancing since their main purpose is fault tolerance rather than good performance. 2. Scheduling: In our example, the length of each DNA sequence varies, so does comparison time.
Reference: [23] <author> Jianjian Song. </author> <title> A partially asynchronous and iterative algorithm for distributed load balancing. </title> <journal> Parallel Computing, </journal> <volume> Vol. 20 </volume> <pages> 853-868, </pages> <year> 1994. </year>
Reference-contexts: Many dynamic load balancing algorithms have been devised for such an efficient migration <ref> [6, 10, 17, 18, 23] </ref>; they are characterized by the following parameters which distinguish them. Load balancing algorithms can be fine tuned when programmers can change those factors conveniently. * Topology: Topology determines the shape of task migration paths.
Reference: [24] <author> E. F. Walker, R. Floyd, and P. Neves. </author> <title> Asynchronous remote operation in distributed systems. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 253-259, </pages> <year> 1990. </year>
Reference-contexts: An asynchronous call does not block the client, and replies can be received as they are needed. To date, the decision on calling style is not the programmer's (for example, calls may be synchronous only [4] or they may be asynchronous only <ref> [2, 19, 24] </ref>), or the decision has to be made at module programming level by use of different library routines [8]. If we let this decision be separate from RPC statement, the modules will remain reusable for different calling styles.
References-found: 24

