URL: http://charm.cs.uiuc.edu/version2/papers/HpccPositionHPCC94.ps
Refering-URL: http://charm.cs.uiuc.edu/version2/papers/HpccPositionHPCC94.html
Root-URL: http://www.cs.uiuc.edu
Email: E-mail: kale@cs.uiuc.edu  
Title: APPLICATION ORIENTED AND COMPUTER SCIENCE CENTERED HPCC RESEARCH  
Author: Laxmikant V. Kale 
Address: Urbana, IL 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: At this time, there is a perception of a backlash against the HPCC program, and even the idea of massively parallel computing itself. In preparation to defining an agenda for HPCC, this paper first analyzes the reasons for this backlash. Although beset with unrealistic expectations, parallel processing will be a beneficial technology with a broad impact, beyond applications in science. However, this will require significant advances and work in computer science in addition to parallel hardware and end-applications which are emphasized currently. The paper presents a possible agenda that could lead to a successful HPCC program in the future. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Branscomb, T. Belytschko, P. Bridenbaugh, T. Chay, J. Dozier, G. S. Grest, E. F. Hayes, B. Honig, N. Lane, J. W. Lester, G. J. McRae, J. A. Sethian, B. Smith, and M. Vernon. </author> <title> Recommendations to implement this goal. From Desktop to teraflop: exploiting the U.S. </title> <booktitle> lead in high performance computing, </booktitle> <pages> pages 13-14, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: The program has supported development of parallel machines, their deployment, and end-application development projects, but has not supported efforts on languages, systems, compilers, generic algorithms and their implementation with equal emphasis. For example, the recent report by the NSF Blue Ribbon Panel on HPC <ref> [1] </ref> states the following in justifying its Recommendation B-1 for a challenge program in computer science within CISE: There is a consensus that the absence of sufficient funding for systems and algorithms work which is not mission-oriented is the primary barrier to lower cost, more widely accessible, and more us able <p> This is a real danger that must be avoided. However, this can be accomplished by requiring demonstrations of the work using a few sample end-applications or their components. My suggestion here is fully consistent with recommendation B-1 of the Branscomb report <ref> [1] </ref>, which recommends establishing a number of major projects in computational science and mathematics. 4.1.3 CSE Education As parallel computers, massive or otherwise, become commonly available, there will be a need to train or retrain a large community of programmers to use them.
Reference: [2] <author> J. Dongarra, M. Snir, W. Gropp, E. Lusk, A. Geist, and et. al. </author> <title> Document for a standard message-passing interface. </title> <year> 1993. </year>
Reference-contexts: There exist other mechanisms to exchange information among parallel processes. The information sharing mecha 1 In most current message-passing models, information can be exchanged only on a point-to-point basis. However, collective communication primitives are being designed by the message passing interface (MPI) standardization committee <ref> [2] </ref>. 2 Read-only information is data that is initialized once and not altered thereafter. nisms provided by Linda and Strand suffer from the same problem: each provides only a single information exchange mechanism.
Reference: [3] <author> J. Flower, A. Kolawa, and S. Bharadwaj. </author> <title> The Express way to distributed processing. </title> <booktitle> In Supercomputing Review, </booktitle> <pages> pages 54-55, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: However, many large-scale machines available today, such as In-tel iPSC/860 and Paragon, NCUBE/2, and CM-5, include hundreds of processors. Implementing shared variables on such machines is difficult and inefficient. Messages provide another important means of exchanging information between processes in systems such as PVM [7], Express <ref> [3] </ref>, and Actors [4]. Messages containing necessary information can be sent from a "sender" process to a known "receiver" process 1 . Most commercial distributed memory machines provide hardware support for message passing, so this mechanism to exchange information can be easily implemented.
Reference: [4] <author> G. A. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT press, </publisher> <year> 1986. </year>
Reference-contexts: However, many large-scale machines available today, such as In-tel iPSC/860 and Paragon, NCUBE/2, and CM-5, include hundreds of processors. Implementing shared variables on such machines is difficult and inefficient. Messages provide another important means of exchanging information between processes in systems such as PVM [7], Express [3], and Actors <ref> [4] </ref>. Messages containing necessary information can be sent from a "sender" process to a known "receiver" process 1 . Most commercial distributed memory machines provide hardware support for message passing, so this mechanism to exchange information can be easily implemented.
Reference: [5] <author> L. V. Kale. </author> <title> Parallel programming with Charm: an overview. Parallel Programming Laboratory, </title> <type> Technical Report PPL-TR-93-8, </type> <institution> University of Illinois, Urbana-Champaign, Department of Computer Science, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Eventually, the other module will return data and control to the calling module in the form of a message. Substantial research on message driven protocols for exchange of data among modules and on development of message-driven libraries is still needed. 4.2.2 Charm Charm <ref> [5] </ref> is a portable, object-based, and message-driven parallel programming language developed at the Parallel Programming Laboratory at the University of Illinois. Charm programs run unchanged on a variety of shared and private memory parallel machines. The basic unit of computation in Charm is a chare (which is a concurrent object).
Reference: [6] <author> K. Kennedy. </author> <title> High performance computing in trouble. </title> <booktitle> Parallel computing research, </booktitle> <address> 1(4):2, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: This backlash is against the HPCC program as well as the idea of massively parallel computing itself. Ken Kennedy, a leading researcher in parallel computing, wrote an article recently, titled "High Performance Computing in Trouble" <ref> [6] </ref> in which he alluded to the funding difficulties of the HPCC program, the skepticism about its goals in the Senate and Congress, the critical and negative report by the Congressional Budget Office, etc.
Reference: [7] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2, 4 </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: However, many large-scale machines available today, such as In-tel iPSC/860 and Paragon, NCUBE/2, and CM-5, include hundreds of processors. Implementing shared variables on such machines is difficult and inefficient. Messages provide another important means of exchanging information between processes in systems such as PVM <ref> [7] </ref>, Express [3], and Actors [4]. Messages containing necessary information can be sent from a "sender" process to a known "receiver" process 1 . Most commercial distributed memory machines provide hardware support for message passing, so this mechanism to exchange information can be easily implemented.
Reference: [8] <author> F. W. </author> <title> Weingarten. </title> <journal> HPCC research questioned. Communications of the ACM, </journal> <volume> 36(11) </volume> <pages> 27-29, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: An article by Fred Wein-garten <ref> [8] </ref> discusses this report as well as the report by GAO on ARPA's management of the HPC architecture research. All of these indicate the backlash against the HPCC program.
References-found: 8

