URL: http://www.cs.iastate.edu/~honavar/Papers/generate93.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/honavar.html
Root-URL: 
Title: Generative Learning Structures and Processes for Generalized Connectionist Networks  
Author: Vasant Honavar Leonard Uhr 
Affiliation: Department of Computer Science Iowa State University  Computer Sciences Department University of Wisconsin-Madison  
Abstract: Massively parallel networks of relatively simple computing elements offer an attractive and versatile framework for exploring a variety of learning structures and processes for intelligent systems. This paper briefly summarizes some popular learning structures and processes used in such networks. It outlines a range of potentially more powerful alternatives for pattern-directed inductive learning in such systems. It motivates and develops a class of new learning algorithms for massively parallel networks of simple computing elements. We call this class of learning processes generative for they offer a set of mechanisms for constructive and adaptive determination of the network architecture the number of processing elements and the connectivity among them as a function of experience. Generative learning algorithms attempt to overcome some of the limitations of some approaches to learning in networks that rely on modification of weights on the links within an otherwise fixed network topology e.g., rather slow learning and the need for an a-priori choice of a network architecture. Several alternative designs as well as a range of control structures and processes which can be used to regulate the form and content of internal representations learned by such networks are examined. Empirical results from the study of some generative learning algorithms are briefly summarized and several extensions and refinements of such algorithms, and directions for future research are outlined. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., & Kibler, D. </author> <year> (1989). </year> <title> Noise-tolerant instance-based learning algorithms. </title> <booktitle> In Proceedings of the 1989 International Joint Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ash, T. </author> <year> (1989). </year> <title> Dynamic node creation in backpropagation networks. </title> <journal> Connection Science - Journal of Neural Computing, Artificial Intelligence and Cognitive Research 1 365-375. </journal>
Reference-contexts: W k may then be modified using one of the available weight modification strategies (e.g., error back-propagation). An example of this approach is the dynamic node creation algorithm <ref> (Ash, 1989) </ref> which adds hidden nodes one at a time to a 3-layer network of sigmoid nodes. <p> A generated node is linked to each of the nodes in the output layer of the network and initialized with random (or heuristically chosen) weights. This is the design used in recognition cones that combine generation and weight-modification (Honavar & Uhr, 1988, 1989a), the DNC algorithm <ref> (Ash, 1989) </ref>, and the cascade correlation architecture (Fahlman & Lebiere, 1990). Alternatively, a generated node is linked selectively to the network output node (s) which are indicated by the feedback provided with the training sample that was used for generation. Several variations on these basic designs are possible. 4.3. <p> In this case, one or more sufficiently novel (see above) extraction (s) is (are) obtained from a misclassified training pattern (or a sequence of such patterns) whenever generation is initiated. Somewhat different versions of this strategy are used in the DNC algorithm <ref> (Ash, 1989) </ref>, and the cascade-correlation algorithm (Fahlman & Lebiere, 1990). 4.5.
Reference: <author> Balakrishnan, K., & Honavar, V. </author> <year> (1992). </year> <title> Faster learning in multi-layer networks by eliminating flat-spots. </title> <note> Submitted for publication. </note>
Reference: <author> Bareiss, E. R. </author> <year> (1988). </year> <title> Protos: A unified approach to concept representation, classification, and learning. </title> <type> Ph.D thesis, </type> <institution> University of Texas, Austin. </institution>
Reference-contexts: Such mechanisms have obvious commonalities with processes such as chunking (Laird, Newell, & Rosenbloom, 1987) and exemplar-based knowledge acquisition <ref> (Bareiss, 1988) </ref> used in SP systems. This, combined with the ability to fine-tune the acquired knowledge structures using weight modification strategies of the sort used in CN, provides a basis for integration of symbolic and sub-symbolic approaches to machine learning in GCN.
Reference: <author> Barto, A. G., & Anandan, P. </author> <year> (1985). </year> <title> Pattern recognizing stochastic learning automata. </title> <journal> IEEE 26 Transactions on Systems, Man, and Cybernetics 15 360-375. </journal>
Reference-contexts: Alternatively, they may be obtained from not-so-specific feedback e.g., a reward/punishment signal used in reinforcement learning <ref> (Barto & Anandan, 1985) </ref>, or they may be internally derived, e.g., based on an estimate of the output necessary from a node to ___________________________________________________________ The output of the node n j is given by s j = f j (u j ) Some typical choices for u j are: u j
Reference: <author> Baum, E. B. </author> <year> (1989). </year> <title> A proposal for more powerful learning algorithms. </title> <booktitle> Neural Computation 1 201-207. </booktitle>
Reference-contexts: A learning algorithm that simply adds new units as necessary can rapidly build a network that is essentially a random-access memory or a look-up table to represent any arbitrary set of functions <ref> (Baum, 1989) </ref>. But unfortunately, such a look-up table has to store each sample pattern and is thus inefficient in its use of memory (Minsky & Papert, 1969). Furthermore, it is incapable of generalizing correctly to sample patterns that are not stored in the look-up table.
Reference: <author> Carbonell, J. G., Michalski, R. S., & Mitchell, T. M. </author> <year> (1983). </year> <title> An overview of machine learning. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.) </editor> <booktitle> Machine Learning An Artificial Intelligence Approach. </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher>
Reference: <author> Diederich, J. </author> <year> (1988). </year> <title> Knowledge-intensive recruitment learning. </title> <type> Technical report TR-88-010. </type> <institution> Berkeley, CA: International Computer Science Institute. </institution>
Reference: <author> Dietterich, T. G., & Michalski, R. S. </author> <year> (1983). </year> <title> A comparative review of selected methods for learning from examples. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.) </editor> <booktitle> Machine Learning An Artificial Intelligence Approach. </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher>
Reference: <author> Fahlman, S. E. </author> <year> (1988). </year> <title> Faster-learning variations on back-propagation. </title> <editor> In G. E. Hinton, </editor> <booktitle> T. </booktitle>
Reference: <editor> J. Sejnowski, & D. S. Touretzky (Eds.) </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fahlman, S. E., & Lebiere, C. </author> <year> (1990). </year> <booktitle> The cascade-correlation learning architecture. In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 2). </volume> <editor> D. S. Touretzky (Ed.) </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is the design used in recognition cones that combine generation and weight-modification (Honavar & Uhr, 1988, 1989a), the DNC algorithm (Ash, 1989), and the cascade correlation architecture <ref> (Fahlman & Lebiere, 1990) </ref>. Alternatively, a generated node is linked selectively to the network output node (s) which are indicated by the feedback provided with the training sample that was used for generation. Several variations on these basic designs are possible. 4.3. <p> In this case, one or more sufficiently novel (see above) extraction (s) is (are) obtained from a misclassified training pattern (or a sequence of such patterns) whenever generation is initiated. Somewhat different versions of this strategy are used in the DNC algorithm (Ash, 1989), and the cascade-correlation algorithm <ref> (Fahlman & Lebiere, 1990) </ref>. 4.5. <p> using a range of real-world data (e.g., soybean diseases, chess end games, audiology, iris): Simulations indicate that generative learning yields results comparable to those obtained by weight modification alone in a smaller number of learning cycles and with the resulting networks being relatively compact. [4] Variations of the cascade-correlation architecture <ref> (Fahlman & Lebiere, 1990) </ref> on a range of real-world data sets such as the ones listed in [3] above: Simulations suggest about an order of magnitude speedup in learning within a certain range of parameter choices; again, the resulting networks are much smaller than the ones needed to attain comparable classification
Reference: <author> Farmer, D. J., Packard, N. H., & Perelson, A. S. </author> <year> (1986). </year> <title> The immune system, adaptation, </title> <journal> and machine learning. Physica 22D 187-204. </journal>
Reference: <author> Feldman, J. A., & Ballard, D. H. </author> <year> (1982). </year> <title> Connectionist models and their properties. </title> <booktitle> Cognitive Science 6, </booktitle> <pages> 205-264. </pages>
Reference: <author> Fisher, D. H. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <note> Machine Learning 2 139-172. </note>
Reference-contexts: perceptron (Rosenblatt, 1962)), or it may be adaptively developed by the learner using some internal criteria for grouping patterns into classes - e.g., some metric of similarity between patterns as in e.g., ART networks (Grossberg, 1976a; 1976b), ID3 algorithm (Quinlan, 1986) and concept learning systems e.g., CLS (Hunt, 1962), COBWEB <ref> (Fisher, 1987) </ref>. Inductive learning is often guided by feedback from the teacher (or from the environment). Such feedback is used to modify YY as necessary.
Reference: <author> Fogel, L. J., Owens, A. J., & Walsh, M. J. </author> <year> (1966). </year> <title> Artificial Intelligence Through Simulated Evolution. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Fukunaga, K. </author> <year> (1990). </year> <title> Introduction to Statistical Pattern Recognition. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Other alternatives for selection of a sub-set of candidate nodes for addition to the network are suggested by the techniques for optimal feature sub-set selection developed for statistical pattern classification <ref> (see Fukunaga, 1990 for a discussion of such selection algorithms) </ref>. Generalizing Extracted Sub-patterns Generation by Extraction of novel sub-patterns as described above essentially fixes the weight matrix W k associated with the generated node n k using a single extracted sub-pattern. <p> The task of elimination of nodes from the is an instance of optimal feature sub-set selection problem in statistical pattern recognition <ref> (Fukunaga, 1990) </ref>.
Reference: <author> Gallant, S. I. </author> <year> (1990). </year> <title> Perceptron-based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks. </journal> <volume> 1 179-. </volume>
Reference: <author> Garey, M. R., & Johnson, D. S. </author> <year> (1979). </year> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <address> San Francisco, CA: </address> <publisher> Freeman. </publisher>
Reference: <author> Gold, E. </author> <year> (1967). </year> <title> Language identification in the limit. </title> <note> Information and Control 10 447-474. </note>
Reference-contexts: The combinatorial space which an inductive learner has to contend with is intractable in the worst case <ref> (Gold, 1967) </ref>. The characterization of mappings that are feasibly learnable has become an important area of research in machine learning (e.g., Valiant, 1984).
Reference: <author> Greenough, W. T., & Bailey, C. H. </author> <year> (1988). </year> <title> The anatomy of a memory: convergence of results across a diversity of tests Trends in Neuroscience, </title> <booktitle> 11, </booktitle> <pages> 142-147. </pages>
Reference-contexts: Neurobiological Considerations Animal learning appears to involve the growth of new synapses throughout life in addition to the tuning of synaptic strengths <ref> (Greenough & Bailey, 1988) </ref>. GCN that combine generative learning with tuning of node functions (e.g., the weight matrices that encode synaptic strengths) therefore appear to provide a potentially useful framework for simplified models of brain development (Honavar, 1989).
Reference: <author> Grossberg, S. </author> <year> (1976a). </year> <title> Adaptive pattern classification and universal recoding I: parallel development and coding of neural feature detectors. </title> <type> Biological Cybernetics 23 121-134. </type>
Reference: <author> Grossberg, S. </author> <year> (1976b). </year> <title> Adaptive pattern classification and universal recoding II: feedback, expectation, olfaction, and illusions. </title> <type> Biological Cybernetics 23 187-202. </type>
Reference: <author> Grossberg, S. </author> <year> (1980). </year> <title> How does the brain build a cognitive code? Psychological Review 1 1-51. </title>
Reference-contexts: in the environment. [2] Robustness in the presence of noisy or misleading data (e.g., by using a large number of observations or samples, and/or by being able to undo mistakes resulting from poor data). [3] Ability to construct efficient internal representations of the environment. [4] Resolution of the stability-plasticity dilemma <ref> (Grossberg, 1980) </ref>, so as to be able to modify internal representations in response to changes in its environment and the tasks to be performed with minimal interference with performance on previously learned 2 tasks. 1.2.
Reference: <author> Grossberg, S. </author> <year> (1987). </year> <title> Competitive learning: from interactive activation to adaptive 27 resonance. </title> <booktitle> Cognitive Science 11 22-63. </booktitle>
Reference-contexts: j are: f j (u j ) = e q where q is a positive integer, or, alternatively, f j (u j ) = 1 hhhhh These functions belong to the family of radial basis functions. __________________________________________________________ 9 produce the overall desired behavior from the network, e.g., in competitive learning <ref> (Grossberg, 1987) </ref>. Consider a CN node n j which receives one of it's inputs i k , from the node n k . The feedback to the node is t j , an estimate of the desired output of the node n k .
Reference: <author> Hanson, S. J., & Pratt, L. Y. </author> <year> (1989). </year> <title> Some comparisons of constraints for minimal network construction with back-propagation. </title> <editor> In D. S. Touretzky (Ed.) </editor> <booktitle> Neural Information Processing Systems (Vol. 1). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hanson, S. J. </author> <year> (1990). </year> <title> Meiosis Networks. </title> <editor> In D. S. Touretzky (Ed.) </editor> <booktitle> Neural Information Processing Systems (Vol. 2). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hebb, D. O. </author> <year> (1949). </year> <title> The Organization of Behavior. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference: <author> Hinton, G. E. </author> <year> (1989). </year> <note> Connectionist learning procedures Artificial Intelligence 40 185-234. </note>
Reference-contexts: This is accomplished by modifying the weight vectors associated with each of the nodes linked in an a-priori fixed topology <ref> (Hinton, 1989) </ref>. Some weight-modification schemes are based on correlations in the activation values associated with connected nodes based on the mechanism proposed by Hebb (1949). Others use various estimates of error between the desired and actual network outputs.
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <booktitle> Adaptation in Natural and Artificial Systems. </booktitle> <address> Ann Arbor, MI: </address> <publisher> University of Michigan Press. </publisher>
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1988). </year> <title> A network of neuron-like units that learns to perceive by generation as well as reweighting of its links. </title> <editor> In G. E. Hinton, T. J. Sejnowski, </editor> & <address> D. </address>
Reference-contexts: Some alternative designs are discussed below. A generated node is linked to each of the nodes in the output layer of the network and initialized with random (or heuristically chosen) weights. This is the design used in recognition cones that combine generation and weight-modification <ref> (Honavar & Uhr, 1988, 1989a) </ref>, the DNC algorithm (Ash, 1989), and the cascade correlation architecture (Fahlman & Lebiere, 1990). Alternatively, a generated node is linked selectively to the network output node (s) which are indicated by the feedback provided with the training sample that was used for generation. <p> Some alternative designs are considered below: The simplest strategy initiates a generation at any layer in the network whenever the criteria for generation (see above) are satisfied. This strategy is employed in recognition cone networks that learn by combined generation and weight modification <ref> (Honavar & Uhr, 1988, 1989a) </ref>. A different strategy is suggested by the following assumption: Complex features of the input should be encoded based on informative simple features.
Reference: <editor> S. Touretzky (Eds.) </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1989a). </year> <title> Brain-Structured connectionist networks that perceive and learn. </title> <journal> Connection Science Journal of Neural Computing, Artificial Intelligence and Cognitive Research 1 139-160. </journal>
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1989b). </year> <title> Generation, local receptive fields and global convergence improve perceptual learning in connectionist networks. </title> <booktitle> In Proceedings of the 1989 International Joint Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Honavar, V. </author> <year> (1989). </year> <title> Perceptual development and learning: From behavioral, neurophysiological, and morphological evidence to computational models. </title> <type> Technical report 818. </type> <institution> Madison, WI: University of Wisconsin, Computer Sciences Dept. </institution>
Reference-contexts: GCN that combine generative learning with tuning of node functions (e.g., the weight matrices that encode synaptic strengths) therefore appear to provide a potentially useful framework for simplified models of brain development <ref> (Honavar, 1989) </ref>. Integration of Symbolic and Sub-Symbolic Approaches to Learning Generative learning provides mechanisms for rapid construction of distributed look-up tables through extraction, abstraction, and generalization of potentially useful input patterns (see below). <p> This suggests biasing the system such that it tends to fill in the lower layers before generation can start filling in the higher layers of the network (This has clear parallels in the development of the visual system in mammalian brains 22 <ref> (Honavar, 1989) </ref>. Additional alternatives exist, e.g., networks that employ encodings of the input at multiple spatio-temporal resolutions (Honavar & Uhr, 1990b) may be biased so that generation at a lower resolution is preferred to one at a higher resolution for reasons of representational parsimony. <p> Several possibilities for dynamic regulation of network plasticity are suggested by the development of the mammalian visual cortex <ref> (Honavar, 1989) </ref>. [4] Representational Bias: Constraints on generation (e.g., the topological constraints on network architecture) constitute potentially powerful representational and inductive biases. Such biases can influence the space-time complexity and hence feasibility of generative learning algorithms for real-world applications.
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1990a). </year> <title> Coordination and control structures and processes: possibilities for connectionist networks. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 2 277-302. </journal>
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1990b). </year> <title> Efficient learning using multi-resolution representations of spatial, temporal, and spatio-temporal patterns. </title> <booktitle> In Proceedings of the Conference on Neural Networks and Parallel-Distributed Processing. </booktitle> <institution> Indiana University-Purdue University. </institution>
Reference-contexts: Additional alternatives exist, e.g., networks that employ encodings of the input at multiple spatio-temporal resolutions <ref> (Honavar & Uhr, 1990b) </ref> may be biased so that generation at a lower resolution is preferred to one at a higher resolution for reasons of representational parsimony. <p> 2-dimensional visual patterns (Honavar & Uhr, 1988; 1989a): Results suggest that generative learning is substantially faster than learning by weight-modification alone in a network of a-priory fixed topology, and the networks learned are generally compact. [2] Some simple extensions that facilitate efficient learning from multi-resolution spatial, temporal, and spatio-temporal patterns <ref> (Honavar & Uhr, 1990b) </ref>: Results suggest that the use of multi-resolution representations with a suitably controlled generative learning mechanism result in relatively parsimonious representations. [3] Feedback-guided minimal generation through extraction of novel sub-patterns for pattern classification using a range of real-world data (e.g., soybean diseases, chess end games, audiology, iris): Simulations <p> it is tuned for and the input. [2] Representational parsimony: Several controls that bias the network toward learning efficient internal representations have been incorporated into generative learning designs investigated so far (e.g., feedback-guided minimal generation through extraction of novel patterns, guided search through multi-resolution spatial, temporal, and spatio-temporal representational hierarchies <ref> (Honavar & Uhr, 1990b) </ref>. Parsimony requirement in some situations may conflict with hence need to be resolved with the need for damage resistance (through redundance), or rapid learning. [3] Plasticity: controls the number, the locus, and the frequency of generations initiated.
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1990c). </year> <title> Symbol processing systems, connectionist networks, and generalized connectionist networks. </title> <type> Technical Report 90-24. </type> <institution> Ames, Iowa: Iowa State University Department of Computer Science. </institution>
Reference-contexts: learning symbol processing (SP) systems (Newell, 1980) and connectionist network (CN) models (Feldman & Ballard, 1982; Rumelhart, Hinton, & Williams, 1986) and points out some potentially interesting extensions to the currently popular CN models, leading to what may be characterized, for lack of a better term, as generalized connectionist networks <ref> (Honavar & Uhr, 1990c) </ref>. [3] Section 3 motivates a new class of learning algorithms - generative learning algorithms (Honavar & Uhr, 1988; 1989a; 1989b; 1990b) which attempt to overcome some of the limitations of CN approaches that rely on the modification of weights within an a-priori chosen, fixed, network topology. [4] <p> The remainder of this section describes CN, and some extensions to CN leading to what may be called (for lack of a better term), generalized connectionist networks (GCN) <ref> (Honavar & Uhr, 1990c) </ref>. GCN offer a number of potentially powerful alternative learning mechanisms including generative learning which is discussed in detail in this paper. 2.1. <p> Several potentially powerful extensions to today's CN including more powerful structures and processes for behaving, control, and learning suggest themselves, leading up to systems that can be characterized as generalized connectionist networks (GCN) or generalized neuromorphic systems (GNS) <ref> (Honavar & Uhr, 1990c) </ref>. Our focus here is on learning structures and processes.
Reference: <author> Honavar, V. </author> <year> (1992a). </year> <title> Learning algorithms based on generalized distance measures. </title> <booktitle> In Proceedings of the 1992 SPIE Conference on Adaptive and Learning Systems (To appear). </booktitle>
Reference-contexts: Some topics currently under study include: integration of unsupervised and supervised generative mechanisms generative learning; a design for incremental generation of multi-level representations for the recognition and description of 3-dimensional objects (Honavar, 1992a; 1992b); generative learning with structured representations <ref> (Honavar, 1992a) </ref>; improved weight modification algorithms, extraction and generalization mechanisms for generation; construction of iconic representations using generative learning; and experimental comparison of generative learning designs with each other as well as with a range of other learning methods.
Reference: <author> Honavar, V. </author> <year> (1992b). </year> <title> Learning parsimonious representations of three-dimensional shapes. </title> <booktitle> In Proceedings of the NATO Advanced Research Workshop on Mathematical Descriptions of Shape (To appear). </booktitle> <address> Netherlands. </address>
Reference: <author> Hunt, E. B. </author> <year> (1962). </year> <title> Concept Formation: An Information Processing Problem. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: teacher as in perceptron (Rosenblatt, 1962)), or it may be adaptively developed by the learner using some internal criteria for grouping patterns into classes - e.g., some metric of similarity between patterns as in e.g., ART networks (Grossberg, 1976a; 1976b), ID3 algorithm (Quinlan, 1986) and concept learning systems e.g., CLS <ref> (Hunt, 1962) </ref>, COBWEB (Fisher, 1987). Inductive learning is often guided by feedback from the teacher (or from the environment). Such feedback is used to modify YY as necessary.
Reference: <author> Jerne, N. K. </author> <year> (1967). </year> <title> Antibodies and Learning: Selection Versus Instruction. </title> <editor> In G. C. </editor> <volume> 28 Quarton, </volume> <editor> T. Melnechuk, & F. O. Schmitt (Eds.) </editor> <title> The Neurosciences: A Study Program. </title> <address> New York, NY: </address> <publisher> Rockefeller University Press. </publisher> <month> Judd </month> <year> (1990). </year> <title> Neural Networks and the Complexity of Learning. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Koza, J. R. </author> <year> (1990). </year> <title> Genetic Programming: A Paradigm for Genetically Breeding Populations of Computer Programs to Solve Problems. </title> <type> Techincal report STAN-CS-90-1314. </type> <institution> Stanford, CA: Stanford University Department of Computer Science. </institution>
Reference: <author> Laird, J. E., Newell, A., & Rosenbloom, P. S. </author> <year> (1987). </year> <title> SOAR: An architecture for general intelligence. </title> <booktitle> Artificial Intelligence 33. </booktitle>
Reference-contexts: Approaches to Pattern-Directed Inductive Learning Two of the dominant research paradigms in computational approaches to learning are symbol processing systems (Newell, 1980) and connectionist networks (Rosenblatt, 1958; Feldman & Ballard, 1982; Rumelhart, Hinton, & McClelland, 1986). Learning in SP systems (e.g., SOAR <ref> (Laird, Newell, & Rosenbloom, 1987) </ref>) typically involves modification of stored data structures, rules, or procedures. Space does not permit a review of current work in machine learning within the SP paradigm (Carbonell, Michalski, & Mitchell, 1983; Dietterich & Michalski, 1983; Michalski & Kodratoff, 1990 provide such reviews). <p> Integration of Symbolic and Sub-Symbolic Approaches to Learning Generative learning provides mechanisms for rapid construction of distributed look-up tables through extraction, abstraction, and generalization of potentially useful input patterns (see below). Such mechanisms have obvious commonalities with processes such as chunking <ref> (Laird, Newell, & Rosenbloom, 1987) </ref> and exemplar-based knowledge acquisition (Bareiss, 1988) used in SP systems.
Reference: <author> Le Cun, Y., Denker, J. S., Solla, S. A. </author> <year> (1990). </year> <title> Optimal Brain Damage. </title> <editor> In D. S. Touretzky (Ed.) </editor> <booktitle> Neural Information Processing Systems (Vol. 2). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other alternatives for selection of a sub-set of candidate nodes for addition to the network are suggested by the techniques for optimal feature sub-set selection developed for statistical pattern classification <ref> (see Fukunaga, 1990 for a discussion of such selection algorithms) </ref>. Generalizing Extracted Sub-patterns Generation by Extraction of novel sub-patterns as described above essentially fixes the weight matrix W k associated with the generated node n k using a single extracted sub-pattern. <p> This is the design used in recognition cones that combine generation and weight-modification (Honavar & Uhr, 1988, 1989a), the DNC algorithm (Ash, 1989), and the cascade correlation architecture <ref> (Fahlman & Lebiere, 1990) </ref>. Alternatively, a generated node is linked selectively to the network output node (s) which are indicated by the feedback provided with the training sample that was used for generation. Several variations on these basic designs are possible. 4.3. <p> In this case, one or more sufficiently novel (see above) extraction (s) is (are) obtained from a misclassified training pattern (or a sequence of such patterns) whenever generation is initiated. Somewhat different versions of this strategy are used in the DNC algorithm (Ash, 1989), and the cascade-correlation algorithm <ref> (Fahlman & Lebiere, 1990) </ref>. 4.5. <p> using a range of real-world data (e.g., soybean diseases, chess end games, audiology, iris): Simulations indicate that generative learning yields results comparable to those obtained by weight modification alone in a smaller number of learning cycles and with the resulting networks being relatively compact. [4] Variations of the cascade-correlation architecture <ref> (Fahlman & Lebiere, 1990) </ref> on a range of real-world data sets such as the ones listed in [3] above: Simulations suggest about an order of magnitude speedup in learning within a certain range of parameter choices; again, the resulting networks are much smaller than the ones needed to attain comparable classification
Reference: <author> Michalski, R. S., & Stepp, R. E. </author> <year> (1983). </year> <title> Learning from observation: conceptual clustering. </title>
Reference-contexts: Also of interest are particular structures that might be induced over C a : the classes may be organized in a treelike hierarchy in which the classes at any given level are disjoint (as in conceptual clustering <ref> (Michalski & Stepp, 1983) </ref>); or they may form a more complex heterarchy.
Reference: <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.) </editor> <booktitle> Machine Learning - An Artificial Intelligence Approach. </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher>
Reference: <author> Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.) </author> <year> (1983). </year> <title> Machine Learning An Artificial Intelligence Approach. </title> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher>
Reference-contexts: Also of interest are particular structures that might be induced over C a : the classes may be organized in a treelike hierarchy in which the classes at any given level are disjoint (as in conceptual clustering <ref> (Michalski & Stepp, 1983) </ref>); or they may form a more complex heterarchy.
Reference: <editor> Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.) </editor> <booktitle> Machine Learning An Artificial Intelligence Approach. </booktitle> <volume> (Vol. </volume> <pages> 2). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Michalski, R. S., & Kodratoff, Y. </author> <year> (1990). </year> <title> Research in machine learning: recent progress, classification of methods, and future directions. </title> <editor> In Y. Kodratoff & R. S. Michalski (Eds.) </editor> <booktitle> Machine Learning An Artificial Intelligence Approach (Vol. 3). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Minsky, M. & Papert, S. </author> <year> (1969). </year> <title> Perceptrons: An Introduction To Computational Geometry. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: But unfortunately, such a look-up table has to store each sample pattern and is thus inefficient in its use of memory <ref> (Minsky & Papert, 1969) </ref>. Furthermore, it is incapable of generalizing correctly to sample patterns that are not stored in the look-up table. Thus, such networks have to be supplemented by mechanisms that enable the network to generalize correctly to future sample patterns.
Reference: <author> Mozer, M. C., & Smolensky, P. </author> <year> (1989). </year> <title> Skeletonization: a technique for trimming the fat from a network via relevance assessment. </title> <editor> In D. S. Touretzky (Ed.) </editor> <booktitle> Neural Information Processing Systems (Vol. 1). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Nadal, J. P. </author> <year> (1989). </year> <title> Study of a growth algorithm for a feedforward network. </title> <journal> International journal of neural systems. </journal> <volume> 1 55-. </volume>
Reference: <author> Natarajan, B. K. </author> <year> (1992). </year> <title> Machine Learning ATheoretical Approach. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Newell, A. </author> <year> (1980). </year> <title> Physical symbol systems. </title> <booktitle> Cognitive Science 4 135-183. </booktitle>
Reference-contexts: 1 enumerates some of the desiderata of learning systems to motivate the structures and processes developed in the following sections; and defines some essential terminology used in the discussion to follow. [2] Section 2 summarizes briefly the two dominant research paradigms in computational approaches to learning symbol processing (SP) systems <ref> (Newell, 1980) </ref> and connectionist network (CN) models (Feldman & Ballard, 1982; Rumelhart, Hinton, & Williams, 1986) and points out some potentially interesting extensions to the currently popular CN models, leading to what may be characterized, for lack of a better term, as generalized connectionist networks (Honavar & Uhr, 1990c). [3] Section <p> The learning structures and processes developed in this paper constitute steps in this direction. 2. Approaches to Pattern-Directed Inductive Learning Two of the dominant research paradigms in computational approaches to learning are symbol processing systems <ref> (Newell, 1980) </ref> and connectionist networks (Rosenblatt, 1958; Feldman & Ballard, 1982; Rumelhart, Hinton, & McClelland, 1986). Learning in SP systems (e.g., SOAR (Laird, Newell, & Rosenbloom, 1987)) typically involves modification of stored data structures, rules, or procedures.
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <booktitle> The Mathematical Foundations of Learning Machines. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Some examples 7 of such calculations are shown in figures 1-A and 1-B. Some form of nonlinearity is necessary for making decisions necessary for categorization <ref> (Nilsson, 1965) </ref>. The threshold function is a discrete decision-making device. It classifies all its input patterns into two sharply distinguished sets. The logistic function is a continuous version of the threshold function. <p> This is primarily due to the limited representational power of single-layer perceptrons. Multi-layer architectures can potentially overcome this limitation given an appropriate set of learning processes <ref> (Nilsson, 1965) </ref> and indeed multi-layer networks that learn using the generalized delta rule (Rumelhart, Hinton, & Williams, 1986) demonstrate this fact.
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1989). </year> <title> Two algorithms that learn DNF by discovering relevant features. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <note> Machine learning 1 81-106. </note>
Reference-contexts: be determined by an external agent (say, the teacher as in perceptron (Rosenblatt, 1962)), or it may be adaptively developed by the learner using some internal criteria for grouping patterns into classes - e.g., some metric of similarity between patterns as in e.g., ART networks (Grossberg, 1976a; 1976b), ID3 algorithm <ref> (Quinlan, 1986) </ref> and concept learning systems e.g., CLS (Hunt, 1962), COBWEB (Fisher, 1987). Inductive learning is often guided by feedback from the teacher (or from the environment). Such feedback is used to modify YY as necessary.
Reference: <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The Perceptron: a probabilistic model for information storage and organization in the brain. </title> <type> Psychological Review 65 386-408. </type>
Reference-contexts: Others use various estimates of error between the desired and actual network outputs. Error estimates may be based on extremely specific feedback e.g., the desired network output provided by the teacher for each input pattern which is used in the perceptron algorithm <ref> (Rosenblatt, 1958) </ref> and in the error backpropagation algorithm (Rumelhart, Hinton & Williams, 1986; Werbos, 1974) and some of its faster variants (Fahlman, 1988; Balakrishnan & Honavar, 1992). <p> Some of the most popular weight modification schemes of this form are the Perceptron algorithm for a network with one layer of modifiable links <ref> (Rosenblatt, 1958) </ref> and its generalization to a network with multiple layers of modifiable links (Werbos, 1974; Rumelhart, Hinton, & Williams, 1986). <p> A different form of weight-modification strategy is suggested for the links that connect the generated nodes (or, if desired, input nodes), with the output nodes. We have used variants of the simple perceptron rule <ref> (Rosenblatt, 1958) </ref> or the Widrow-Hoff rule (Widrow & Hoff, 1960) for the modification of weights between the output nodes and other nodes in the network.
Reference: <author> Rujan, P., & Marchand, M. </author> <year> (1989). </year> <title> Learning by activating neurons: a new approach to learning in neural networks. </title> <journal> Complex Systems 3 229-. </journal> <note> 29 Rumelhart, </note> <author> D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation, In Parallel Distributed Processing Explorations into the Microstructure of Cognition (Vol. </title> <booktitle> 1: Foundations). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & McClelland, J. L. </author> <year> (1986). </year> <title> A general framework for parallel distributed processing. Parallel Distributed Processing Explorations into the Microstructure of Cognition (Vol. </title> <booktitle> 1: Foundations). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Its graded response is attractive for a variety of purposes including noise tolerance, contrast enhancement, and the existence of a derivative over the entire range of the function (a property required by some learning algorithms e.g., generalized delta rule <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> for error back-propagation). Often it is necessary to compare an input pattern with a stored template. Various versions of the match function are occasionally used to accomplish this task. A particular class of such functions is shown in figure 1-B. <p> This is primarily due to the limited representational power of single-layer perceptrons. Multi-layer architectures can potentially overcome this limitation given an appropriate set of learning processes (Nilsson, 1965) and indeed multi-layer networks that learn using the generalized delta rule <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> demonstrate this fact. <p> Generative learning can potentially offer a way around this problem by constructively building up multi-layer networks. CN that learn through weight modification can potentially support generalization <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> essentially because they are function approximators.
Reference: <author> Rumelhart, D. E. </author> <year> (1988). </year> <title> Parallel Distributed Processing. </title> <booktitle> Plenary lecture given at the IEEE International Conference on Neural Networks, </booktitle> <address> San Diego, CA. </address>
Reference-contexts: However, it has been observed that CN that have far too many nodes in excess of the number actually needed for a learning task often exhibit poor generalization (in cases where they learn to overfit the training set) <ref> (Rumelhart, 1988) </ref>.
Reference: <author> Sandon, P. A. </author> <year> (1987). </year> <title> Learning Object-Centered Representations. </title> <type> Ph.D Thesis, </type> <institution> University of Wisconsin-Madison. </institution>
Reference-contexts: But as the networks get deeper, weight modification schemes such as error backpropagation perform poorly because the the error signal gets weaker as it reaches layers further from the output nodes <ref> (Sandon, 1987) </ref>. Generative learning can potentially offer a way around this problem by constructively building up multi-layer networks. CN that learn through weight modification can potentially support generalization (Rumelhart, Hinton, & Williams, 1986) essentially because they are function approximators.
Reference: <author> Shannon, C. E. </author> <year> (1963). </year> <title> The mathematical theory of communication. </title> <editor> In C. E. </editor> <publisher> Shannon & W. </publisher>
Reference-contexts: Alternatively, the evaluation of the information-content of a potential extraction can be based on a measure of mutual information <ref> (Shannon, 1963) </ref> between the output of a candidate for generation and the nodes already in the network; candidate extractions that are most informative of the desired network output are the ones encoded by the generated nodes.
Reference: <editor> Weaver (Eds.) </editor> <booktitle> The Mathematical Theory of Communication Urbana: </booktitle> <publisher> University of Illinois Press. </publisher>
Reference: <author> Tawel, R. </author> <title> Does the neuron "learn" like the synapse? In D. </title> <editor> S. Touretzky (Ed.) </editor> <booktitle> Neural Information Processing Systems (Vol. </booktitle> <address> 1) San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tsotsos, J. K. </author> <year> (1990). </year> <title> Analyzing vision at the complexity level. </title> <booktitle> Behavioral and Brain Sciences 13 423-469. </booktitle>
Reference-contexts: But in practice, the patterns in the universe of interest U a often exhibit regularities (e.g., connectedness, compactness etc.) which, if judiciously exploited by the recognizer, can potentially make the expected-case complexity of recognition of patterns in U a computationally tractable <ref> (Tsotsos, 1990) </ref>. Inductive Learning Defined The task of an inductive learner is to learn the set of mappings YY that can assign each pattern X drawn from the universe U a into one or more appropriate class (es) drawn from C a .
Reference: <author> Uhr, L. </author> <year> (1990a). </year> <title> Connectionist networks defined generally, to show where power can be increased. </title> <journal> Connection Science Journal of Neural Computing, Artificial Intelligence and Cognitive Research (In press). </journal>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM 27 1134-1142. </journal>
Reference-contexts: The combinatorial space which an inductive learner has to contend with is intractable in the worst case (Gold, 1967). The characterization of mappings that are feasibly learnable has become an important area of research in machine learning <ref> (e.g., Valiant, 1984) </ref>. <p> ease and the computational complexity of learning: e.g., Boolean concepts expressed in k -term DNF (disjunctions of at most k pure conjuncts, each with an unlimited number of literals) are not feasibly learnable whereas concepts expressed in k -CNF (conjunctive normal form with at most k literals per conjunct) are <ref> (Valiant, 1984) </ref>. Thus we need a range of learning algorithms and architectures that exploit the strengths of available representations and/or efficiently transform between representations as necessary.
Reference: <author> Werbos, P. J. </author> <year> (1974). </year> <title> Beyond Regression: New Tools for Prediction and Analysis in Behavioral Sciences. </title> <type> Ph.D Thesis, </type> <institution> Harvard University. </institution>
Reference: <author> Wolpert, D. H. </author> <year> (1989a). </year> <title> A Mathematical Theory of Generalization: Part I. </title> <institution> Los Alamos, NM: Los Alamos National Laboratory. </institution>
Reference: <author> Wolpert, D. H. </author> <year> (1989b). </year> <title> A Mathematical Theory of Generalization: Part II. </title> <institution> Los Alamos, NM: Los Alamos National Laboratory. </institution>
Reference: <author> Widrow, B., & Hoff, M. E. </author> <year> (1960). </year> <title> Adaptive switching circuits. </title> <booktitle> In IRE WESCON Convention Record (Part 4). </booktitle>
Reference-contexts: of alternative node functions (e.g., from logical OR to logical AND) by changing that threshold. 11 [2] Learning that modifies the weight matrices or local templates associated with each processing element in the GCN: This might involve the use of one of the several weight-modification strategies currently used in CN <ref> (e.g., Widrow & Hoff, 1960) </ref> or other forms of weight modification that may be appropriate for the corresponding node functions used (e.g., if a node that matches its input with a stored weight matrix and produces a measure of match - e.g., a gaussian match node is used, a suitable weight <p> A different form of weight-modification strategy is suggested for the links that connect the generated nodes (or, if desired, input nodes), with the output nodes. We have used variants of the simple perceptron rule (Rosenblatt, 1958) or the Widrow-Hoff rule <ref> (Widrow & Hoff, 1960) </ref> for the modification of weights between the output nodes and other nodes in the network.
Reference: <author> Yang, J., & Honavar, V. </author> <year> (1991). </year> <title> Experiments with the cascade-correlation algorithm. </title> <booktitle> In Proceedings of the Fourth UNB Artificial Intelligence Symposium. </booktitle> <address> Fredericton, Canada. </address>
Reference-contexts: Similar versions of this strategy are used by (Honavar & Uhr, 1989; Fahlman & Lebiere, 1990) and empirical studies suggest that learning performance generally improves with increase in size of the candidate pool <ref> (Yang & Honavar, 1991) </ref>. <p> sets such as the ones listed in [3] above: Simulations suggest about an order of magnitude speedup in learning within a certain range of parameter choices; again, the resulting networks are much smaller than the ones needed to attain comparable classification accuracy with weight modification using the generalized delta rule <ref> (Yang & Honavar, 1991) </ref>. Extensive experimental evaluation of generative learning algorithms is necessary to identify the strengths and limitations of different designs. However, in the opinion of the authors, the results obtained so far are quite encouraging. A large variety of alternative generative learning approaches remain to be explored. 6.
References-found: 74

