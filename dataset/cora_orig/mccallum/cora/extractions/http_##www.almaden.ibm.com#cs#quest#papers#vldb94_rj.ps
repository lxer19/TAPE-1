URL: http://www.almaden.ibm.com/cs/quest/papers/vldb94_rj.ps
Refering-URL: http://www.almaden.ibm.com/cs/quest/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Fast Algorithms for Mining Association Rules  
Author: Rakesh Agrawal Ramakrishnan Srikant 
Address: 650 Harry Road, San Jose, CA 95120  
Affiliation: IBM Almaden Research Center  
Abstract: We consider the problem of discovering association rules between items in a large database of sales transactions. We present two new algorithms for solving this problem that are fundamentally different from the known algorithms. Experiments with synthetic as well as real-life data show that these algorithms outperform the known algorithms by factors ranging from three for small problems to more than an order of magnitude for large problems. We also show how the best features of the two proposed algorithms can be combined into a hybrid algorithm, called AprioriHybrid. Scale-up experiments show that AprioriHybrid scales linearly with the number of transactions. AprioriHybrid also has excellent scale-up properties with respect to the transaction size and the number of items in the database.
Abstract-found: 1
Intro-found: 1
Reference: [ABN92] <author> Tarek M. Anwar, Howard W. Beck, and Shamkant B. Navathe. </author> <title> Knowledge mining by imprecise querying: A classification-based approach. </title> <booktitle> In IEEE 8th Int'l Conf. on Data Engineering, </booktitle> <address> Phoenix, Arizona, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: Experiments show that the AprioriHybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [AIS93a] <ref> [ABN92] </ref> [HS94] [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases [HCC92] [Lub89] [PS91b].
Reference: [AFS93] <author> Rakesh Agrawal, Christos Faloutsos, and Arun Swami. </author> <title> Efficient similarity search in sequence databases. </title> <booktitle> In Proc. of the Fourth International Conference on Foundations of Data Organization and Algorithms, </booktitle> <address> Chicago, </address> <month> October </month> <year> 1993. </year> <booktitle> Also in Lecture Notes in Computer Science 730, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1993, </year> <pages> 69-84. </pages>
Reference-contexts: In Quest, we are exploring the various aspects of the database mining problem. Besides the problem of discovering association rules, some other problems that we have looked into include the enhancement of the database capability with classification queries [AGI + 92] and similarity queries over time sequences <ref> [AFS93] </ref>. We believe that database mining is an important new application area for databases, combining commercial interest with intriguing research questions. Acknowledgment We wish to thank Mike Carey for his insightful comments and suggestions.
Reference: [AGI + 92] <author> Rakesh Agrawal, Sakti Ghosh, Tomasz Imielinski, Bala Iyer, and Arun Swami. </author> <title> An interval classifier for database mining applications. </title> <booktitle> In Proc. of the VLDB Conference, </booktitle> <pages> pages 560-573, </pages> <address> Vancouver, British Columbia, Canada, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: In Quest, we are exploring the various aspects of the database mining problem. Besides the problem of discovering association rules, some other problems that we have looked into include the enhancement of the database capability with classification queries <ref> [AGI + 92] </ref> and similarity queries over time sequences [AFS93]. We believe that database mining is an important new application area for databases, combining commercial interest with intriguing research questions. Acknowledgment We wish to thank Mike Carey for his insightful comments and suggestions.
Reference: [AIS93a] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year> <note> Special Issue on Learning and Discovery in Knowledge-Based Databases. </note>
Reference-contexts: Experiments show that the AprioriHybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining <ref> [AIS93a] </ref> [ABN92] [HS94] [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases [HCC92] [Lub89] [PS91b].
Reference: [AIS93b] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pages 207-216, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year> <month> 29 </month>
Reference-contexts: They are interested in instituting information-driven marketing processes, managed by database technology, that enable marketers to develop and implement customized marketing programs and strategies [Ass90]. The problem of mining association rules over basket data was introduced in <ref> [AIS93b] </ref>. An example of such a rule might be that 98% of customers that purchase tires and auto accessories also get automotive services done. Finding all such rules is valuable for cross-marketing and attached mailing applications. <p> The databases involved in these applications are very large. It is imperative, therefore, to have fast algorithms for this task. fl Visiting from the Department of Computer Science, University of Wisconsin, Madison. 1 The following is a formal statement of the problem <ref> [AIS93b] </ref>: Let I = fi 1 ; i 2 ; . . . ; i m g be a set of literals, called items. Let D be a set of transactions, where each transaction T is a set of items such that T I. <p> The rule X =) Y has support s in the transaction set D if s% of transactions in D contain X [ Y . Our rules are somewhat more general than in <ref> [AIS93b] </ref> in that we allow a consequent to have more than one item. <p> Our discussion is neutral with respect to the representation of D. For example, D could be a data file, a relational table, or the result of a relational expression. An algorithm for finding all association rules, henceforth referred to as the AIS algorithm, was presented in <ref> [AIS93b] </ref>. Another algorithm for this task, called the SETM algorithm, has been proposed in [HS93]. In this paper, we present two new algorithms, Apriori and AprioriTid, that differ fundamentally from these algorithms. <p> these issues in this paper, except to point out that these are necessary features of a rule discovery system that may use our algorithms as the engine of the discovery process. 1.1 Problem Decomposition and Paper Organization The problem of discovering all association rules can be decomposed into two subproblems <ref> [AIS93b] </ref>: 1. Find all sets of items (itemsets) that have transaction support above minimum support. The support for an itemset is the number of transactions that contain the itemset. Itemsets with minimum support are called large itemsets, and all others small itemsets. <p> If conf minconf, then the rule holds. (The rule will surely have minimum support because ABCD is large.) Unlike <ref> [AIS93b] </ref>, where rules were limited to only one item in the consequent, we allow multiple items in the consequent. <p> The algorithms in Section 3 generate such multi-consequent rules. In Section 4, we show the relative performance of the proposed Apriori and AprioriTid algorithms against the AIS <ref> [AIS93b] </ref> and SETM [HS93] algorithms. To make the paper self-contained, we include an overview of the AIS and SETM algorithms in this section. We also describe how the Apriori and AprioriTid algorithms can be combined into a hybrid algorithm, AprioriHybrid, and demonstrate the scale-up properties of this algorithm. <p> At the end of the pass, we determine which of the candidate itemsets are actually large, and they become the seed for the next pass. This process continues until no new large itemsets are found. The Apriori and AprioriTid algorithms we propose differ fundamentally from the AIS <ref> [AIS93b] </ref> and SETM [HS93] algorithms in terms of which candidate itemsets are counted in a pass and in the way that those candidates are generated. <p> When L k does not fit in memory, we need to externally sort L k as in the buffer management scheme used for Apriori. 3 Discovering Rules The association rules that we consider here are somewhat more general than in <ref> [AIS93b] </ref> in that we allow a consequent to have more than one item; rules in [AIS93b] were limited to single item 12 consequents. We first give a straightforward generalization of the algorithm in [AIS93b] and then present a faster algorithm. <p> fit in memory, we need to externally sort L k as in the buffer management scheme used for Apriori. 3 Discovering Rules The association rules that we consider here are somewhat more general than in <ref> [AIS93b] </ref> in that we allow a consequent to have more than one item; rules in [AIS93b] were limited to single item 12 consequents. We first give a straightforward generalization of the algorithm in [AIS93b] and then present a faster algorithm. To generate rules, for every large itemset l, we find all non-empty subsets of l. <p> Apriori. 3 Discovering Rules The association rules that we consider here are somewhat more general than in <ref> [AIS93b] </ref> in that we allow a consequent to have more than one item; rules in [AIS93b] were limited to single item 12 consequents. We first give a straightforward generalization of the algorithm in [AIS93b] and then present a faster algorithm. To generate rules, for every large itemset l, we find all non-empty subsets of l. <p> The data resided in the AIX file system and was stored on a 2GB SCSI 3.5" drive, with measured sequential throughput of about 2 MB/second. We first give an overview of the AIS <ref> [AIS93b] </ref> and SETM [HS93] algorithms against which we compare the performance of the Apriori and AprioriTid algorithms. We then describe the synthetic datasets used in the performance evaluation and show the performance results. <p> entry in C k else add c to C k with a count of 1; 10) end 11) L k = fc 2 C k j c:count minsupg 12) end 13) Answer = S Data Structures The data structures required for maintaining large and candidate itemsets were not specified in <ref> [AIS93b] </ref>. We store the large itemsets in a dynamic multi-level hash table to make the subset operation in step 5 fast, using the algorithm described in Section 2.1.2. <p> This reclamation procedure is executed as often as necessary during a pass. The large itemsets discarded in a pass are extended in the next pass. This technique is a simplified version of the buffer management scheme presented in <ref> [AIS93b] </ref>. 4.2 The SETM Algorithm The SETM algorithm [HS93] was motivated by the desire to use SQL to compute large itemsets. Our description of this algorithm in Figure 5 uses the same notation as used for the other algorithms, but is functionally identical to the SETM algorithm presented in [HS93]. <p> The timings in [HS93] were obtained on a RS/6000 350 processor, whereas our experiments have been run on a slower RS/6000 530H processor. The execution time for 1% support for AIS is lower than that reported in <ref> [AIS93b] </ref> because of improvements in the data structures for storing large and candidate itemsets. 23 4.7 Algorithm AprioriHybrid It is not necessary to use the same algorithm in all the passes over data. Figure 11 shows the execution times for Apriori and AprioriTid for different passes over the dataset T10.I4.D100K. <p> We compared these algorithms to the previously known algorithms, the AIS <ref> [AIS93b] </ref> and SETM [HS93] algorithms. We presented experimental results, using both synthetic and real-life data, showing that the proposed algorithms always outperform AIS and SETM.
Reference: [ANB92] <author> Tarek M. Anwar, Shamkant B. Navathe, and Howard W. Beck. </author> <title> Knowledge mining in databases: A unified approach through conceptual clustering. </title> <type> Technical report, </type> <institution> Georgia Institute of Technology, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering <ref> [ANB92] </ref> [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [Ass90] <author> David Shepard Associates. </author> <title> The new direct marketing. Business One Irwin, </title> <publisher> Illinois, </publisher> <year> 1990. </year>
Reference-contexts: Successful organizations view such databases as important pieces of the marketing infrastructure [Ass92]. They are interested in instituting information-driven marketing processes, managed by database technology, that enable marketers to develop and implement customized marketing programs and strategies <ref> [Ass90] </ref>. The problem of mining association rules over basket data was introduced in [AIS93b]. An example of such a rule might be that 98% of customers that purchase tires and auto accessories also get automotive services done. Finding all such rules is valuable for cross-marketing and attached mailing applications.
Reference: [Ass92] <author> Direct Marketing Association. </author> <title> Managing database marketing technology for success, </title> <year> 1992. </year>
Reference-contexts: A record in such data typically consists of the transaction date and the items bought in the transaction. Successful organizations view such databases as important pieces of the marketing infrastructure <ref> [Ass92] </ref>. They are interested in instituting information-driven marketing processes, managed by database technology, that enable marketers to develop and implement customized marketing programs and strategies [Ass90]. The problem of mining association rules over basket data was introduced in [AIS93b].
Reference: [B + 93] <editor> R.J. Brachman et al. </editor> <title> Integrated support for data archeology. </title> <booktitle> In AAAI-93 Workshop on Knowledge Discovery in Databases, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: There has been work on quantifying the "usefulness" or "interestingness" of a rule [PS91a]. What is useful or interesting is often application-dependent. The need for a human in the loop and providing tools to allow human guidance of the rule discovery process has been articulated, for example, in <ref> [B + 93] </ref> [KI91] [Tsu90].
Reference: [BFOS84] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases [HCC92] [Lub89] [PS91b]. Related, but not directly applicable, work includes the induction of classification rules <ref> [BFOS84] </ref> [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [Bit92] <author> D. Bitton. </author> <title> Bridging the gap between database theory and practice, </title> <year> 1992. </year>
Reference-contexts: If used for finding all association rules, this algorithm will make as many passes over the data as the number of combinations of items in the antecedent, which is exponentially large. Related work in the database literature is the work on inferring functional dependencies from data <ref> [Bit92] </ref> [MR87]. Functional dependencies are rules requiring strict satisfaction. Consequently, having determined a dependency X ! A, the algorithms in [Bit92] [MR87] 2 consider any other dependency of the form X + Y ! A redundant and do not generate it. The association rules we consider are probabilistic in nature. <p> Related work in the database literature is the work on inferring functional dependencies from data <ref> [Bit92] </ref> [MR87]. Functional dependencies are rules requiring strict satisfaction. Consequently, having determined a dependency X ! A, the algorithms in [Bit92] [MR87] 2 consider any other dependency of the form X + Y ! A redundant and do not generate it. The association rules we consider are probabilistic in nature.
Reference: [C + 88] <author> P. Cheeseman et al. </author> <title> AutoClass: A Bayesian classification system. </title> <booktitle> In 5th Int'l Conf. on Machine Learning. </booktitle> <publisher> Morgan Kaufman, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] <ref> [C + 88] </ref> [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a]. If used for finding all association rules, this algorithm will make as many passes over the data as the number of combinations of items in the antecedent, which is exponentially large.
Reference: [Cat91] <author> J. Catlett. </author> <title> Megainduction: A test flight. </title> <booktitle> In 8th Int'l Conf. on Machine Learning, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases [HCC92] [Lub89] [PS91b]. Related, but not directly applicable, work includes the induction of classification rules [BFOS84] <ref> [Cat91] </ref> [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [CH92] <author> G. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <booktitle> Machine Learning, </booktitle> <year> 1992. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules <ref> [CH92] </ref> [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [Fis87] <author> Douglas H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2(2), </volume> <year> 1987. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] <ref> [Fis87] </ref>. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a]. If used for finding all association rules, this algorithm will make as many passes over the data as the number of combinations of items in the antecedent, which is exponentially large.
Reference: [FWD93] <author> Usama Fayyad, Nicholas Weir, and S.G. Djorgovski. Skicat: </author> <title> A machine learning system for automated cataloging of large scale sky surveys. </title> <booktitle> In 10th Int'l Conf. on Machine Learning, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases [HCC92] [Lub89] [PS91b]. Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] <ref> [FWD93] </ref> [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [HCC92] <author> Jiawei Han, Yandong Cai, and Nick Cercone. </author> <title> Knowledge discovery in databases: An attribute oriented approach. </title> <booktitle> In Proc. of the VLDB Conference, </booktitle> <pages> pages 547-559, </pages> <address> Van-couver, British Columbia, Canada, </address> <year> 1992. </year>
Reference-contexts: Experiments show that the AprioriHybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases <ref> [HCC92] </ref> [Lub89] [PS91b]. Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87]. <p> The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases <ref> [HCC92] </ref> [Lub89] [PS91b]. Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [HS93] <author> Maurice Houtsma and Arun Swami. </author> <title> Set-oriented mining of association rules. </title> <type> Research Report RJ 9567, </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, California, </address> <month> October </month> <year> 1993. </year> <month> 30 </month>
Reference-contexts: For example, D could be a data file, a relational table, or the result of a relational expression. An algorithm for finding all association rules, henceforth referred to as the AIS algorithm, was presented in [AIS93b]. Another algorithm for this task, called the SETM algorithm, has been proposed in <ref> [HS93] </ref>. In this paper, we present two new algorithms, Apriori and AprioriTid, that differ fundamentally from these algorithms. We present experimental results, using both synthetic and real-life data, showing that the proposed algorithms always outperform the earlier algorithms. <p> The algorithms in Section 3 generate such multi-consequent rules. In Section 4, we show the relative performance of the proposed Apriori and AprioriTid algorithms against the AIS [AIS93b] and SETM <ref> [HS93] </ref> algorithms. To make the paper self-contained, we include an overview of the AIS and SETM algorithms in this section. We also describe how the Apriori and AprioriTid algorithms can be combined into a hybrid algorithm, AprioriHybrid, and demonstrate the scale-up properties of this algorithm. <p> This process continues until no new large itemsets are found. The Apriori and AprioriTid algorithms we propose differ fundamentally from the AIS [AIS93b] and SETM <ref> [HS93] </ref> algorithms in terms of which candidate itemsets are counted in a pass and in the way that those candidates are generated. In both the AIS and SETM algorithms (see Sections 4.1 and 4.2 for a review), candidate itemsets are generated on-the-fly during the pass as data is being read. <p> The data resided in the AIX file system and was stored on a 2GB SCSI 3.5" drive, with measured sequential throughput of about 2 MB/second. We first give an overview of the AIS [AIS93b] and SETM <ref> [HS93] </ref> algorithms against which we compare the performance of the Apriori and AprioriTid algorithms. We then describe the synthetic datasets used in the performance evaluation and show the performance results. Next, we show the performance results for three real-life datasets obtained from a retail and a direct mail company. <p> This reclamation procedure is executed as often as necessary during a pass. The large itemsets discarded in a pass are extended in the next pass. This technique is a simplified version of the buffer management scheme presented in [AIS93b]. 4.2 The SETM Algorithm The SETM algorithm <ref> [HS93] </ref> was motivated by the desire to use SQL to compute large itemsets. Our description of this algorithm in Figure 5 uses the same notation as used for the other algorithms, but is functionally identical to the SETM algorithm presented in [HS93]. <p> in [AIS93b]. 4.2 The SETM Algorithm The SETM algorithm <ref> [HS93] </ref> was motivated by the desire to use SQL to compute large itemsets. Our description of this algorithm in Figure 5 uses the same notation as used for the other algorithms, but is functionally identical to the SETM algorithm presented in [HS93]. C k (L k ) in Figure 5 represents the set of candidate (large) itemsets in which the TIDs of the generating transactions have been associated with the itemsets. Each member of these sets is of the form &lt; T ID; itemset &gt;. <p> In fact, it needs to visit every member of L k only once in the TID order, and the candidate generation in steps 5 through 11 can be performed using the relational merge-join operation <ref> [HS93] </ref>. The disadvantage of this approach is mainly due to the size of candidate sets C k . For each candidate itemset, the candidate set now has as many entries as the number of transactions in which the candidate itemset is present. <p> Buffer Management The performance of the SETM algorithm critically depends on the size of the set C k relative to the size of memory. If C k fits in memory, the two sorting steps can be performed using an in-memory sort. In <ref> [HS93] </ref>, C k was assumed to fit in main memory and buffer management was not discussed. <p> The largest dataset in the scale-up experiments for SETM in <ref> [HS93] </ref> was still small enough that C k could fit in memory; hence they did not encounter this jump in execution time. Note that for the same minimum support, the support count for candidate itemsets increases linearly with the number of transactions. <p> We experimented with this variant of SETM and found that, while it did better than SETM, it still performed much worse than Apriori or AprioriTid. 22 very small, only 0.65MB. Some performance results for this dataset were reported in <ref> [HS93] </ref>. AprioriTid fit in memory for this dataset. Apriori and AprioriTid are roughly three times as fast as AIS and four times faster than SETM. <p> SETM had to be aborted (after taking 20 times the time Apriori took to complete) because, even for 2% support, the set C 2 became larger than the disk capacity. 4 The execution times for SETM in this figure are a little higher compared to those reported in <ref> [HS93] </ref>. The timings in [HS93] were obtained on a RS/6000 350 processor, whereas our experiments have been run on a slower RS/6000 530H processor. <p> to be aborted (after taking 20 times the time Apriori took to complete) because, even for 2% support, the set C 2 became larger than the disk capacity. 4 The execution times for SETM in this figure are a little higher compared to those reported in <ref> [HS93] </ref>. The timings in [HS93] were obtained on a RS/6000 350 processor, whereas our experiments have been run on a slower RS/6000 530H processor. <p> We compared these algorithms to the previously known algorithms, the AIS [AIS93b] and SETM <ref> [HS93] </ref> algorithms. We presented experimental results, using both synthetic and real-life data, showing that the proposed algorithms always outperform AIS and SETM. The performance gap increased with the problem size, and ranged from a factor of three for small problems to more than an order of magnitude for large problems.
Reference: [HS94] <author> M. Holsheimer and A. Siebes. </author> <title> Data mining: The search for knowledge in databases. </title> <type> Technical Report CS-R9406, </type> <institution> CWI, Netherlands, </institution> <year> 1994. </year>
Reference-contexts: Experiments show that the AprioriHybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] <ref> [HS94] </ref> [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases [HCC92] [Lub89] [PS91b].
Reference: [KI91] <author> Ravi Krishnamurthy and Tomasz Imielinski. </author> <title> Practitioner problems in need of database research: Research directions in knowledge discovery. </title> <booktitle> SIGMOD RECORD, </booktitle> <volume> 20(3) </volume> <pages> 76-78, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: What is useful or interesting is often application-dependent. The need for a human in the loop and providing tools to allow human guidance of the rule discovery process has been articulated, for example, in [B + 93] <ref> [KI91] </ref> [Tsu90].
Reference: [LSBZ87] <author> P. Langley, H. Simon, G. Bradshaw, and J. Zytkow. </author> <title> Scientific Discovery: Computational Explorations of the Creative Process. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data <ref> [LSBZ87] </ref> [Sch90], and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [Lub89] <author> David J. Lubinsky. </author> <title> Discovery from databases: A review of AI and statistical techniques. </title> <booktitle> In IJCAI-89 Workshop on Knowledge Discovery in Databases, </booktitle> <pages> pages 204-218, </pages> <address> Detroit, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Experiments show that the AprioriHybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases [HCC92] <ref> [Lub89] </ref> [PS91b]. Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87].
Reference: [MF92] <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <editor> In Steve Muggleton, editor, </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions <ref> [MF92] </ref> [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [MKKR92] <author> R.S. Michalski, L. Kerschberg, K.A. Kaufman, and J.S. Ribeiro. </author> <title> Mining for knowledge in databases: The INLEN architecture, initial implementation, and first results. </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 1 </volume> <pages> 85-113, </pages> <year> 1992. </year>
Reference-contexts: Experiments show that the AprioriHybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] <ref> [MKKR92] </ref> [S + 93] [Tsu90], also called knowledge discovery in databases [HCC92] [Lub89] [PS91b].
Reference: [MR87] <author> Heikki Mannila and Kari-Jouku Raiha. </author> <title> Dependency inference. </title> <booktitle> In Proc. of the VLDB Conference, </booktitle> <pages> pages 155-158, </pages> <address> Brighton, England, </address> <year> 1987. </year>
Reference-contexts: If used for finding all association rules, this algorithm will make as many passes over the data as the number of combinations of items in the antecedent, which is exponentially large. Related work in the database literature is the work on inferring functional dependencies from data [Bit92] <ref> [MR87] </ref>. Functional dependencies are rules requiring strict satisfaction. Consequently, having determined a dependency X ! A, the algorithms in [Bit92] [MR87] 2 consider any other dependency of the form X + Y ! A redundant and do not generate it. The association rules we consider are probabilistic in nature. <p> Related work in the database literature is the work on inferring functional dependencies from data [Bit92] <ref> [MR87] </ref>. Functional dependencies are rules requiring strict satisfaction. Consequently, having determined a dependency X ! A, the algorithms in [Bit92] [MR87] 2 consider any other dependency of the form X + Y ! A redundant and do not generate it. The association rules we consider are probabilistic in nature.
Reference: [MTV94] <author> Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Efficient algorithms for discovering association rules. </title> <booktitle> In KDD-94: AAAI Workshop on Knowledge Discovery in Databases, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Apriori, on the other hand, generates and counts only one itemset, f1 3 4 5g, because it concludes a priori that the other combinations cannot possibly have minimum support. 1 Concurrent to our work, the following two-step candidate generation procedure has been proposed in <ref> [MTV94] </ref>: C 0 C k = fX 2 C 0 k jX contains k members of L k1 g These two steps are similar to our join and prune steps respectively. However, in general, step 1 would produce a superset of the candidates produced by our join step. <p> However, in general, step 1 would produce a superset of the candidates produced by our join step. For example, if L 2 were ff1 2g, f2, 3gg, then step 1 of <ref> [MTV94] </ref> will generate the candidate f1 2 3g, whereas our join step will not generate any candidate. 6 Correctness We need to show that C k L k . Clearly, any subset of a large itemset must also have minimum support.
Reference: [Pea92] <author> J. Pearl. </author> <title> Probabilistic reasoning in intelligent systems: Networks of plausible inference, </title> <year> 1992. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] <ref> [Pea92] </ref>, learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [PS91a] <author> G. Piatestsky-Shapiro. </author> <title> Discovery, analysis, and presentation of strong rules. </title> <editor> In G. Piatestsky-Shapiro, editor, </editor> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The closest work in the machine learning literature is the KID3 algorithm presented in <ref> [PS91a] </ref>. If used for finding all association rules, this algorithm will make as many passes over the data as the number of combinations of items in the antecedent, which is exponentially large. Related work in the database literature is the work on inferring functional dependencies from data [Bit92] [MR87]. <p> Similarly, the presence of rules X ! Y and Y ! Z does not necessarily mean that X ! Z holds because the latter may not have minimum confidence. There has been work on quantifying the "usefulness" or "interestingness" of a rule <ref> [PS91a] </ref>. What is useful or interesting is often application-dependent. The need for a human in the loop and providing tools to allow human guidance of the rule discovery process has been articulated, for example, in [B + 93] [KI91] [Tsu90].
Reference: [PS91b] <author> G. Piatestsky-Shapiro, </author> <title> editor. Knowledge Discovery in Databases. </title> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases [HCC92] [Lub89] <ref> [PS91b] </ref>. Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87].
Reference: [Qui90] <author> J. Ross Quinlan. </author> <title> Learning logical definitions from examples. </title> <journal> Machine Learning, </journal> <volume> 5(3), </volume> <year> 1990. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] <ref> [Qui90] </ref>, fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [Qui93] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] [MKKR92] [S + 93] [Tsu90], also called knowledge discovery in databases [HCC92] [Lub89] [PS91b]. Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] <ref> [Qui93] </ref>, discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] [Sch90], and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [S + 93] <author> M. Stonebraker et al. </author> <title> The DBMS research at crossroads. </title> <booktitle> In Proc. of the VLDB Conference, </booktitle> <address> Dublin, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Database mining is motivated by the decision support problem faced by most large retail organizations <ref> [S + 93] </ref>. Progress in bar-code technology has made it possible for retail organizations to collect and store massive amounts of sales data, referred to as the basket data. A record in such data typically consists of the transaction date and the items bought in the transaction. <p> Experiments show that the AprioriHybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] [MKKR92] <ref> [S + 93] </ref> [Tsu90], also called knowledge discovery in databases [HCC92] [Lub89] [PS91b].
Reference: [Sch90] <author> C. Schaffer. </author> <title> Domain-Independent Function Finding. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <year> 1990. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [BFOS84] [Cat91] [FWD93] [HCC92] [Qui93], discovery of causal rules [CH92] [Pea92], learning of logical definitions [MF92] [Qui90], fitting of functions to data [LSBZ87] <ref> [Sch90] </ref>, and clustering [ANB92] [C + 88] [Fis87]. The closest work in the machine learning literature is the KID3 algorithm presented in [PS91a].
Reference: [Tsu90] <author> S. Tsur. </author> <title> Data dredging. </title> <journal> IEEE Data Engineering Bulletin, </journal> <volume> 13(4) </volume> <pages> 58-63, </pages> <month> December </month> <year> 1990. </year> <month> 32 </month>
Reference-contexts: Experiments show that the AprioriHybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [AIS93a] [ABN92] [HS94] [MKKR92] [S + 93] <ref> [Tsu90] </ref>, also called knowledge discovery in databases [HCC92] [Lub89] [PS91b]. <p> What is useful or interesting is often application-dependent. The need for a human in the loop and providing tools to allow human guidance of the rule discovery process has been articulated, for example, in [B + 93] [KI91] <ref> [Tsu90] </ref>.
References-found: 34

