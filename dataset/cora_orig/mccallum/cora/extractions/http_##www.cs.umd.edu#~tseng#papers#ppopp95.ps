URL: http://www.cs.umd.edu/~tseng/papers/ppopp95.ps
Refering-URL: http://www.cs.umd.edu/~tseng/papers.html
Root-URL: 
Email: tseng@cs.stanford.edu  
Title: Compiler Optimizations for Eliminating Barrier Synchronization  
Author: Chau-Wen Tseng 
Address: Stanford, CA 94305-4070  
Affiliation: Computer Systems Laboratory Stanford University  
Abstract: This paper presents novel compiler optimizations for reducing synchronization overhead in compiler-parallelized scientific codes. A hybrid programming model is employed to combine the flexibility of the fork-join model with the precision and power of the single-program, multiple data (SPMD) model. By exploiting compile-time computation partitions, communication analysis can eliminate barrier synchronization or replace it with less expensive forms of synchronization. We show computation partitions and data communication can be represented as systems of symbolic linear inequalities for high flexibility and precision. These optimizations has been implemented in the Stanford SUIF compiler. We extensively evaluate their performance using standard benchmark suites. Experimental results show barrier synchronization is reduced 29% on average and by several orders of magnitude for certain programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Our compiler first eliminates potential sources of communication using scalar and array dependence analysis, then applies communication analysis. In our compiler, local definitions and nonlocal accesses are both represented by systems of symbolic linear inequalities <ref> [1] </ref>. Using systems of inequalities provides greater flexibility and precision. They are particularly suited for synchronization optimizations since we need to know 1) whether communication is present, and 2) the identity of the producer and consumer processors. <p> If a solution to the system exists, inter-processor data movement takes place and must be synchronized. The solution calculated by the scan can then be used to calculate the identities of the communication processors, the nonlocal data accessed, and the loop iterations where the data is defined <ref> [1] </ref>. 3.2.2 Elimination Algorithm Given a test for interprocessor communication, the compiler uses the following greedy algorithm to eliminate barriers in an SPMD region: 1. <p> constructing SPMD regions that are more aggressive about including control flow and guarded code, as well incorporate run-time scheduling techniques for dynamic load balancing. 5 Related Work The techniques used to eliminate or lessen synchronization in our compiler are similar to those used by distributed-memory compilers to calculate explicit communication <ref> [1, 18] </ref>. However, distributed-memory compilers do not consider synchronization explicitly. Instead, the entire program is placed in a single parallel SPMD region, and the compiler inserts calls to nonblocking send and blocking receive routines that implicitly synchronize processors. <p> Good results are reported for two programs, shallow and tred2. In comparison, we apply communication analysis to precisely identify data that must be moved between processors, based on techniques from distributed-memory compilers <ref> [1, 2, 3, 18] </ref>. This information allows us to eliminate more barriers; it also enables the use of counters to satisfy producer-consumer synchronization. Finally, a number of researchers have developed techniques for analyzing synchronization in explicitly parallel programs [19, 22].
Reference: [2] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with do loops. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williams-burg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Before attempting to solve the system of symbolic linear inequalities, we sort the variables into the following scan order: sym-bolics, processors, loop index variables, and array indices. We then determine whether the resulting system of inequalities is consistent by scanning the system using Fourier-Motzkin elimination <ref> [2, 3] </ref>. If a solution to the system exists, inter-processor data movement takes place and must be synchronized. <p> Good results are reported for two programs, shallow and tred2. In comparison, we apply communication analysis to precisely identify data that must be moved between processors, based on techniques from distributed-memory compilers <ref> [1, 2, 3, 18] </ref>. This information allows us to eliminate more barriers; it also enables the use of counters to satisfy producer-consumer synchronization. Finally, a number of researchers have developed techniques for analyzing synchronization in explicitly parallel programs [19, 22].
Reference: [3] <author> C. Ancourt and F. Irigoin. </author> <title> Automatic code distribution. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Before attempting to solve the system of symbolic linear inequalities, we sort the variables into the following scan order: sym-bolics, processors, loop index variables, and array indices. We then determine whether the resulting system of inequalities is consistent by scanning the system using Fourier-Motzkin elimination <ref> [2, 3] </ref>. If a solution to the system exists, inter-processor data movement takes place and must be synchronized. <p> Good results are reported for two programs, shallow and tred2. In comparison, we apply communication analysis to precisely identify data that must be moved between processors, based on techniques from distributed-memory compilers <ref> [1, 2, 3, 18] </ref>. This information allows us to eliminate more barriers; it also enables the use of counters to satisfy producer-consumer synchronization. Finally, a number of researchers have developed techniques for analyzing synchronization in explicitly parallel programs [19, 22].
Reference: [4] <author> J. Anderson, S. Amarasinghe, and M. Lam. </author> <title> Data and computation transformation for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: A global decomposition pass attempts to find a mapping of the computation and data that maximizes inter-processor locality [5]. Synchronization optimizations are applied next, followed by a data-reorganization pass to reduce cache conflicts <ref> [4] </ref>. A code-generation pass then generates the parallel (SPMD) program, converting bodies of parallel loops and parallel regions into parallel procedures. <p> The baseline versions of each program produced by SUIF include both data and computation transformation optimizations <ref> [4] </ref>. The only difference between the optimized and base versions of each program is whether synchronization optimizations were applied. Clock routines were used to measure elapsed time between the start and end of the computation, excluding time spent by the program to initially create threads and initialize data arrays. <p> Results show on average 29% of the barrier synchronization executed at run time are eliminated; some programs achieve dramatic reductions. We expect our synchronization optimization to complement other compiler optimizations <ref> [4] </ref>; it will become in-creasingly important as microprocessors outspeed interconnection network and multiprocessor workstations become common. 7 Acknowledgements We gratefully acknowledge Monica Lam and other members of the Stanford SUIF compiler group for providing the software infrastructure upon which our implementation is based.
Reference: [5] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The computation partition is usually determined locally through simple heuristics. Alternative data-oriented approaches are based on automatic data decomposition techniques; they strive for a global partition to co-locate data and computation. The computation partition may be computed simultaneously with the data partition <ref> [5] </ref> or derived from the data decomposition [7, 14] using the owner-computes rule [18], where each processor is assigned work that results in values for its local data. <p> Data-flow, symbolic, and reduction analysis is performed, followed by a parallelism and locality analysis phase identifies and optimizes loop-level parallelism [29]. A global decomposition pass attempts to find a mapping of the computation and data that maximizes inter-processor locality <ref> [5] </ref>. Synchronization optimizations are applied next, followed by a data-reorganization pass to reduce cache conflicts [4]. A code-generation pass then generates the parallel (SPMD) program, converting bodies of parallel loops and parallel regions into parallel procedures.
Reference: [6] <author> B. Appelbe and B. Lakshmanan. </author> <title> Optimizing parallel programs using affinity regions. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Otherwise communication analysis will simply identify sources of data movement, instead of opportunities for eliminating synchronization overhead. Computation-oriented approaches for selecting computation partitions that reduce communication are known as affinity scheduling, where loop iterations accessing the same data are assigned to the same processor where possible <ref> [6, 13, 21] </ref>. The computation partition is usually determined locally through simple heuristics. Alternative data-oriented approaches are based on automatic data decomposition techniques; they strive for a global partition to co-locate data and computation.
Reference: [7] <author> B. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <booktitle> In Proceedingsof the International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The computation partition is usually determined locally through simple heuristics. Alternative data-oriented approaches are based on automatic data decomposition techniques; they strive for a global partition to co-locate data and computation. The computation partition may be computed simultaneously with the data partition [5] or derived from the data decomposition <ref> [7, 14] </ref> using the owner-computes rule [18], where each processor is assigned work that results in values for its local data. For this paper we assume the compiler partitions computation using global automatic data decomposition techniques, since such methods have been extensively studied and are more precise.
Reference: [8] <author> W. Blume et al. </author> <title> Polaris: The next generation in parallelizing compilers,. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Unfortunately, parallelizing compilers are by no means infallible. While compilers can usually detect available loop-level parallelism for inner loops, they tend to be weak in analyzing large sections of code enclosed by outer loops. Interprocedural paral-lelization has proven successful in identifying large parallel loops for some programs <ref> [8, 15] </ref>, but in most cases compilers will identify many parallel inner loops rather than a few large parallel outer loops. When the amount of computation in a parallel loop (also known as granularity) is small, parallel speedup can be significantly limited due to barrier synchronization overhead [10]. <p> Our prototype optimizer does not include two important techniques, interprocedural analysis and array privatization. Interpro-cedural analysis can in many cases locate large parallel loops, reducing synchronization overhead significantly by itself <ref> [8, 15] </ref>. We believe that there are many programs where communication analysis can reduce synchronization overhead further, particularly as the number of processors increases. Interprocedural analysis can enhance synchronization optimizations for these programs by creating larger SPMD regions and providing more precise symbolic information for communication analysis.
Reference: [9] <author> F. Bodin, L. Kervella, and M. O'Boyle. </author> <title> Synchronization minimization in a SPMD execution model. </title> <type> Technical Report TR-94-863, </type> <institution> INRIA, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Cytron et al. were the first to explore the possibilities of exploiting SPMD code for shared-memory multiprocessors [11]. They concentrated on safety concerns and the effect on privatization. In comparison, we focus on exploiting compile-time computation partition and communication analysis. Bodin et al. <ref> [9] </ref> are exploring techniques similar to ours. They apply a classification algorithm to identify data dependences that cross processor boundaries, then apply heuristics based on max-cut to insert barrier synchronization and satisfy dependence. Good results are reported for two programs, shallow and tred2.
Reference: [10] <author> D. Chen, H. Su, and P. Yew. </author> <title> The impact of synchronization and granularity on parallel systems. </title> <booktitle> In Proceedingsof the 17th International Symposium on Computer Architecture, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: When the amount of computation in a parallel loop (also known as granularity) is small, parallel speedup can be significantly limited due to barrier synchronization overhead <ref> [10] </ref>. Barriers are expensive for two reasons. First, executing a barrier has some run-time overhead that typically grow quickly as the number of processors increases.
Reference: [11] <author> R. Cytron, J. Lipkis, and E. Schonberg. </author> <title> A compiler-assisted approach to SPMD execution. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: In the SPMD model, threads are always active and can synchronize in a number of ways, including barriers, one-to-all, nearest-neighbor, and pipelining. Our compiler attempts to blend the advantages of both approaches through a hybrid strategy first suggested by Cytron et al. <ref> [11] </ref>. Sequential parts of the program are executed by a single master thread, as in traditional shared-memory compilers. Parallel loops, however, are combined to form larger parallel regions that can be treated as small SPMD programs. <p> In practice we usually achieve optimality since SPMD regions are small and require few, if any, barriers. Cytron et al. were the first to explore the possibilities of exploiting SPMD code for shared-memory multiprocessors <ref> [11] </ref>. They concentrated on safety concerns and the effect on privatization. In comparison, we focus on exploiting compile-time computation partition and communication analysis. Bodin et al. [9] are exploring techniques similar to ours.
Reference: [12] <author> S. Dwarkadas, P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> An evaluation of software distributed shared memory for next-generation processors and networks. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: We expect the importance of reducing synchronization overhead to increase with advancing technology, since the gap between uniprocessor performance and inter-processor latency continues to widen. It is particularly important to reduce barrier synchronization when compiling programs for software distributed-shared memory (DSM) systems <ref> [12] </ref>, since software barrier costs are dramatically higher than the barriers on current multiprocessors architectures. The rest of this paper presents approaches for eliminating barrier synchronization, an overview of our algorithm, some implementation details, and experimental results.
Reference: [13] <author> J. Fang and M. Lu. </author> <title> An iteration partition approach for cache or local memory thrashing on parallel processing. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Otherwise communication analysis will simply identify sources of data movement, instead of opportunities for eliminating synchronization overhead. Computation-oriented approaches for selecting computation partitions that reduce communication are known as affinity scheduling, where loop iterations accessing the same data are assigned to the same processor where possible <ref> [6, 13, 21] </ref>. The computation partition is usually determined locally through simple heuristics. Alternative data-oriented approaches are based on automatic data decomposition techniques; they strive for a global partition to co-locate data and computation.
Reference: [14] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multi-computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The computation partition is usually determined locally through simple heuristics. Alternative data-oriented approaches are based on automatic data decomposition techniques; they strive for a global partition to co-locate data and computation. The computation partition may be computed simultaneously with the data partition [5] or derived from the data decomposition <ref> [7, 14] </ref> using the owner-computes rule [18], where each processor is assigned work that results in values for its local data. For this paper we assume the compiler partitions computation using global automatic data decomposition techniques, since such methods have been extensively studied and are more precise.
Reference: [15] <author> M. W. Hall, S. Amarasinghe, and B. Murphy. </author> <title> Interproce-dural analysis for parallelization: Design and experience. </title> <booktitle> In Proceedingsof the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Unfortunately, parallelizing compilers are by no means infallible. While compilers can usually detect available loop-level parallelism for inner loops, they tend to be weak in analyzing large sections of code enclosed by outer loops. Interprocedural paral-lelization has proven successful in identifying large parallel loops for some programs <ref> [8, 15] </ref>, but in most cases compilers will identify many parallel inner loops rather than a few large parallel outer loops. When the amount of computation in a parallel loop (also known as granularity) is small, parallel speedup can be significantly limited due to barrier synchronization overhead [10]. <p> Larger SPMD regions may be created by allowing two additional types of of statements to be included: * Replicated computationsstatements whose execution can be replicated across processors. The most common case involves assignments to privatizable variables. These assignments may need to be finalized following the SPMD region <ref> [15, 27] </ref>. * Guarded computationsstatements that must be protected by explicit guard expressions to ensure they are executed by the appropriate processor (s) [18]. Synchronization may also be needed for the guarded computation. Putting replicated computations in an SPMD region increases the amount of computation that must be performed. <p> Our prototype optimizer does not include two important techniques, interprocedural analysis and array privatization. Interpro-cedural analysis can in many cases locate large parallel loops, reducing synchronization overhead significantly by itself <ref> [8, 15] </ref>. We believe that there are many programs where communication analysis can reduce synchronization overhead further, particularly as the number of processors increases. Interprocedural analysis can enhance synchronization optimizations for these programs by creating larger SPMD regions and providing more precise symbolic information for communication analysis.
Reference: [16] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Relaxing SIMD control flow constraints using loop transformations. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In Proceedings of the 5th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP'95), Santa Barbara, CA, July 1995 while waiting for the slowest processor; this effect results in poor processor utilization when processor execution times vary <ref> [16] </ref>. Eliminating the barrier allows small perturbations in task execution time to even out, taking advantage of the loosely coupled nature of multiprocessors. Barrier synchronization overhead is particularly significant when attempting to use many processors, since the interval between barriers decreases as computation is partitioned across more processors.
Reference: [17] <author> P. Hatcher and M. Quinn. </author> <title> Data-parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation <ref> [17, 24, 25] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. In comparison, we begin with parallel loops discovered by parallelizing compilers, where the remaining barriers are significantly harder to remove. <p> In comparison, we begin with parallel loops discovered by parallelizing compilers, where the remaining barriers are significantly harder to remove. For barriers separating statements on the same loop level, Hatcher and Quinn use a two-dimensional radix sort to find the minimal number of barriers <ref> [17] </ref>. Philippsen and Heinz find the minimal number of barriers with an algorithm based on topological sort; they also attempt to minimize the amount of storage needed for intermediate results [24]. Both algorithms require first computing potential interprocessor communication between all statements, the most expensive step in our optimizer.
Reference: [18] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Barrier costs for the fork-join model can be high, especially when parallel loops are nested in outer sequential loops. In comparison, compilers for message-passing machines generate code according to a single-program, multiple-data (SPMD) model, where all threads execute the entire program <ref> [18] </ref>. Sequential computation is either replicated or explicitly guarded to limit execution to a single thread. When a parallel loop is encountered, threads compute their portion of the computation based on their processor IDs. <p> I+1,N A (J) = F (A (I)) ENDDO ENDDO + broadcast DO I = 1,N wait counter DO J = LU,UB IF (J == I+1) increment counter ENDDO ENDDO barrier distributed-memory machines in order to identify where messages must be inserted into a program to access data on other processors <ref> [18] </ref>. We discovered communication analysis to be quite useful for reducing synchronization on shared-memory machines as well. If it can identify the producers and consumers of all data shared between two regions to be identical (i.e., the same processor), then data movement is local and no synchronization is necessary. <p> Alternative data-oriented approaches are based on automatic data decomposition techniques; they strive for a global partition to co-locate data and computation. The computation partition may be computed simultaneously with the data partition [5] or derived from the data decomposition [7, 14] using the owner-computes rule <ref> [18] </ref>, where each processor is assigned work that results in values for its local data. For this paper we assume the compiler partitions computation using global automatic data decomposition techniques, since such methods have been extensively studied and are more precise. <p> The most common case involves assignments to privatizable variables. These assignments may need to be finalized following the SPMD region [15, 27]. * Guarded computationsstatements that must be protected by explicit guard expressions to ensure they are executed by the appropriate processor (s) <ref> [18] </ref>. Synchronization may also be needed for the guarded computation. Putting replicated computations in an SPMD region increases the amount of computation that must be performed. Putting guarded computations in an SPMD region incurs the cost of evaluating guard expressions and also results in lower processor utilization. <p> constructing SPMD regions that are more aggressive about including control flow and guarded code, as well incorporate run-time scheduling techniques for dynamic load balancing. 5 Related Work The techniques used to eliminate or lessen synchronization in our compiler are similar to those used by distributed-memory compilers to calculate explicit communication <ref> [1, 18] </ref>. However, distributed-memory compilers do not consider synchronization explicitly. Instead, the entire program is placed in a single parallel SPMD region, and the compiler inserts calls to nonblocking send and blocking receive routines that implicitly synchronize processors. <p> Good results are reported for two programs, shallow and tred2. In comparison, we apply communication analysis to precisely identify data that must be moved between processors, based on techniques from distributed-memory compilers <ref> [1, 2, 3, 18] </ref>. This information allows us to eliminate more barriers; it also enables the use of counters to satisfy producer-consumer synchronization. Finally, a number of researchers have developed techniques for analyzing synchronization in explicitly parallel programs [19, 22].
Reference: [19] <author> T. Jeremiassen and S. Eggers. </author> <title> Static analysis of barrier synchronization in explicitly parallel systems. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: This information allows us to eliminate more barriers; it also enables the use of counters to satisfy producer-consumer synchronization. Finally, a number of researchers have developed techniques for analyzing synchronization in explicitly parallel programs <ref> [19, 22] </ref>. By focusing on compiler-parallelized programs, our analysis is simpler and we expect more precise because it is performed on the original sequential source. 6 Conclusion We presented a new compiler optimization for reducing synchronization overhead for compiler-parallelized scientific codes.
Reference: [20] <author> Z. Li. </author> <title> Compiler algorithms for event variable synchronization. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Counters are similar to event synchronization <ref> [20] </ref> but are more flexible; they enforce synchronization constraints as follows. Processors defining (producing) values can increment a counter, and processors accessing (consuming) the values wait until the counter is incremented to the proper value. Counters are desirable alternatives for two reasons. <p> Researchers have looked at the problem of exploiting parallelism through the use of data and event synchronization, where post and wait statements are used to synchronize between data items [26] or loop iterations <ref> [20] </ref>. In comparison, we do not attempt to extract fine-grain parallelism through synchronization. Instead, we try to expand the granularity of loop-level parallelism found by parallelizing compilers; we also reduce overhead by only synchronizing once between each pairs of processors.
Reference: [21] <author> E. Markatos and T. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Otherwise communication analysis will simply identify sources of data movement, instead of opportunities for eliminating synchronization overhead. Computation-oriented approaches for selecting computation partitions that reduce communication are known as affinity scheduling, where loop iterations accessing the same data are assigned to the same processor where possible <ref> [6, 13, 21] </ref>. The computation partition is usually determined locally through simple heuristics. Alternative data-oriented approaches are based on automatic data decomposition techniques; they strive for a global partition to co-locate data and computation.
Reference: [22] <author> S. Midkiff and D. Padua. </author> <title> Compiler generated synchronization for DO loops. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: This information allows us to eliminate more barriers; it also enables the use of counters to satisfy producer-consumer synchronization. Finally, a number of researchers have developed techniques for analyzing synchronization in explicitly parallel programs <ref> [19, 22] </ref>. By focusing on compiler-parallelized programs, our analysis is simpler and we expect more precise because it is performed on the original sequential source. 6 Conclusion We presented a new compiler optimization for reducing synchronization overhead for compiler-parallelized scientific codes.
Reference: [23] <author> D. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: The number of barriers executed is unchanged after merging, but the number of parallel regions is reduced, eliminating start up overhead such as broadcasts to the worker threads. Traditional parallelizing compilers perform scalar data-flow and array data-dependence analysis to track data access patterns <ref> [23] </ref>. Scalar data-flow analysis treats each variable as a monolithic object; array data-dependence analysis is more precise in that it tries to determine which array elements are accessed. <p> If not using the owner-computes rule to partition computation, also compare the list of variables defined in the first group against those defined in the second group. These correspond to true, anti, and output dependences between the two regions <ref> [23] </ref>. 3. Test for loop-independent communication between the two groups at the current loop nesting level. If communication does not exist then the barrier may be eliminated. Combine the two group of statements into a single group, merging the lists of variable definitions and references. Repeat from beginning. 4.
Reference: [24] <author> M. Philippsen and E. Heinz. </author> <title> Automatic synchronization elimination in synchronous FORALLs. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation <ref> [17, 24, 25] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. In comparison, we begin with parallel loops discovered by parallelizing compilers, where the remaining barriers are significantly harder to remove. <p> Philippsen and Heinz find the minimal number of barriers with an algorithm based on topological sort; they also attempt to minimize the amount of storage needed for intermediate results <ref> [24] </ref>. Both algorithms require first computing potential interprocessor communication between all statements, the most expensive step in our optimizer. In comparison, our greedy algorithm incrementally calculates potential communication, inserting barriers as soon as communication is discovered between two groups.
Reference: [25] <author> S. Prakash, M. Dhagat, and R. Bagrodia. </author> <title> Synchronization issues in data-parallel languages. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation <ref> [17, 24, 25] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. In comparison, we begin with parallel loops discovered by parallelizing compilers, where the remaining barriers are significantly harder to remove.
Reference: [26] <author> P. Tang, P. Yew, and C. Zhu. </author> <title> Compiler techniques for data synchronization in nested parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Researchers have looked at the problem of exploiting parallelism through the use of data and event synchronization, where post and wait statements are used to synchronize between data items <ref> [26] </ref> or loop iterations [20]. In comparison, we do not attempt to extract fine-grain parallelism through synchronization. Instead, we try to expand the granularity of loop-level parallelism found by parallelizing compilers; we also reduce overhead by only synchronizing once between each pairs of processors.
Reference: [27] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Larger SPMD regions may be created by allowing two additional types of of statements to be included: * Replicated computationsstatements whose execution can be replicated across processors. The most common case involves assignments to privatizable variables. These assignments may need to be finalized following the SPMD region <ref> [15, 27] </ref>. * Guarded computationsstatements that must be protected by explicit guard expressions to ensure they are executed by the appropriate processor (s) [18]. Synchronization may also be needed for the guarded computation. Putting replicated computations in an SPMD region increases the amount of computation that must be performed. <p> Interprocedural analysis can enhance synchronization optimizations for these programs by creating larger SPMD regions and providing more precise symbolic information for communication analysis. A second technique, array privatization, identifies opportunities when processors may be assigned private copies of an array to eliminate storage-related dependences <ref> [27] </ref>. Communication analysis is improved because privatized arrays do not cause communication, making it more likely barriers may be eliminated. In the future we plan to integrate our synchronization optimizer into the SUIF interprocedural framework and evaluate its effectiveness.
Reference: [28] <author> R. Wilson et al. </author> <title> SUIF: An infrastructure for research on par-allelizing and optimizing compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: The contributions of this paper include: * Novel algorithms for using communication analysis to eliminate or replace barrier synchronization in compiler parallelized programs. * Lessons from implementing the optimization in the SUIF compiler <ref> [28] </ref> using systems of symbolic linear inequalities. * Extensive evaluation using a large number of standard benchmarks suites, averaging 29% reduction in number of barriers executed. <p> other processors do not have to wait for the producer processor to complete all of its work for the current iteration of the outer K loop. 4 Experimental Results To evaluate the effectiveness of the techniques described in this paper, we have implemented them in the Stanford SUIF parallelizing compiler <ref> [28] </ref>. The inputs to the compiler are sequential Fortran or C programs. Data-flow, symbolic, and reduction analysis is performed, followed by a parallelism and locality analysis phase identifies and optimizes loop-level parallelism [29].
Reference: [29] <author> M. E. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The inputs to the compiler are sequential Fortran or C programs. Data-flow, symbolic, and reduction analysis is performed, followed by a parallelism and locality analysis phase identifies and optimizes loop-level parallelism <ref> [29] </ref>. A global decomposition pass attempts to find a mapping of the computation and data that maximizes inter-processor locality [5]. Synchronization optimizations are applied next, followed by a data-reorganization pass to reduce cache conflicts [4].
References-found: 29

