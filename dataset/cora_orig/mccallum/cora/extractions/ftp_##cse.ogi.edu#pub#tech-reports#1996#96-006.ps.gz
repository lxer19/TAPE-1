URL: ftp://cse.ogi.edu/pub/tech-reports/1996/96-006.ps.gz
Refering-URL: ftp://cse.ogi.edu/pub/tech-reports/README.html
Root-URL: http://www.cse.ogi.edu
Email: moody@cse.ogi.edu  denni@cse.ogi.edu  
Phone: 2  
Title: for Projective Basis Function Networks 2m1 Global Form 2m Local Form With appropriate constant factors,
Author: John E. Moody and Thorsteinn S. R ognvaldsson R G (W; m) j= j kv j k R L (W; m) j= j kv j k S(W; m) d D xW() m W(). 
Address: P.O. Box 91000 Portland, Oregon 97291-1000, U.S.A.  
Affiliation: Dept. of Computer Science and Engineering Oregon Graduate Institute of Science and Technology  
Note: Smoothing Regularizers  These regularizers are:  weight  
Abstract: OGI CSE Technical Report 96-006 Abstract: Smoothing regularizers for radial basis functions have been studied extensively, but no general smoothing regularizers for projective basis functions (PBFs), such as the widely-used sigmoidal PBFs, have heretofore been proposed. We derive new classes of algebraically-simple m th -order smoothing regularizers for networks of projective basis functions f (W; x) = P N fi fl + u 0 ; with general transfer functions g[]. These simple algebraic forms R(W; m) enable the direct enforcement of smoothness without the need for costly Monte Carlo integrations of S(W; m). The regularizers are tested on illustrative sample problems and compared to quadratic weight decay. The new regularizers are shown to yield better generalization errors than 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. </author> <year> (1995), </year> <title> `Hints', </title> <booktitle> Neural Computation 7(4), </booktitle> <pages> 639-671. </pages>
Reference: <author> Bishop, C. </author> <year> (1991), </year> <title> `Improving the generalization properties of radial basis function neural networks', </title> <booktitle> Neural Computation 3, </booktitle> <pages> 579-588. </pages>
Reference: <author> Bishop, C. </author> <year> (1995), </year> <title> `Training with noise is equivalent to Tikhonov regularization', </title> <booktitle> Neural Computation 7(1), </booktitle> <pages> 108-116. </pages>
Reference: <author> Buntine, W. L. & Weigend, A. S. </author> <year> (1991), </year> <title> `Bayesian back-propagation', </title> <booktitle> Complex Systems 5, </booktitle> <pages> 603-643. </pages>
Reference: <author> Chauvin, Y. </author> <year> (1990), </year> <title> Dynamic behavior of constrained back-propagation networks, </title> <editor> in D. Touretzky, ed., </editor> <booktitle> `Advances in Neural Information Processing Systems 2', </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <pages> pp. 642-649. </pages>
Reference: <author> Eubank, R. L. </author> <year> (1988), </year> <title> Spline Smoothing and Nonparametric Regression, </title> <publisher> Marcel Dekker, Inc. </publisher>
Reference-contexts: W (x) is not required to be equal to the input density p (x), and will most often be different. The use of smoothing functionals S (W ) like (2) has been extensively studied for smoothing splines <ref> (Eubank 1988, Hastie & Tibshirani 1990, Wahba 1990) </ref> and for radial basis function (RBF) networks (Powell 1987, Poggio & Girosi 1990, Girosi, Jones & Poggio 1995). <p> The correlation coefficients are significantly less than those for R (W; m) shown in figure 3. It is quite clear that the weight decay cost kW k 2 is not a good estimate of S (W; m). 13 5.2 Other Approaches to Smoothing Neural Networks Smoothing splines <ref> (Eubank 1988, Hastie & Tibshirani 1990, Wahba 1990) </ref> and smoothing radial basis functions (Powell 1987, Poggio & Girosi 1990, Girosi et al. 1995) impose smoothness by requiring the forms of the hidden units g [] to be Greens functions of the smoothing operator S ().
Reference: <author> Friedman, J. H. & Stuetzle, W. </author> <year> (1981), </year> <title> `Projection pursuit regression', </title> <journal> J. Amer. Stat. Assoc. </journal> <volume> 76(376), </volume> <pages> 817-823. </pages>
Reference: <author> Geman, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> `Neural networks and the bias/variance dilemma', </title> <booktitle> Neural Computation 4(1), </booktitle> <pages> 1-58. </pages>
Reference-contexts: Regularization reduces model variance at the cost of introducing some model bias. An important question arises: What is the right bias? <ref> (Geman, Bienenstock & Doursat 1992) </ref>.
Reference: <author> Girosi, F., Jones, M. & Poggio, T. </author> <year> (1995), </year> <title> `Regularization theory and neural networks architectures', </title> <booktitle> Neural Computation 7, </booktitle> <pages> 219-269. </pages>
Reference: <author> Hastie, T. J. & Tibshirani, R. J. </author> <year> (1990), </year> <title> Generalized Additive Models, </title> <booktitle> Vol. 43 of Monographs on Statistics and Applied Probability, </booktitle> <publisher> Chapman and Hall. </publisher>
Reference: <author> Hoerl, A. & Kennard, R. </author> <year> (1970a), </year> <title> `Ridge regression: applications to nonorthogonal problems', </title> <type> Technometrics 12, </type> <pages> 69-82. </pages>
Reference: <author> Hoerl, A. & Kennard, R. </author> <year> (1970b), </year> <title> `Ridge regression: biased estimation for nonorthogonal problems', </title> <booktitle> Tech-nometrics 12, </booktitle> <pages> 55-67. </pages>
Reference: <author> Ishikawa, M. </author> <year> (1996), </year> <title> `Structural learning with forgetting', </title> <booktitle> Neural Networks 9(3), </booktitle> <pages> 509-521. </pages>
Reference: <author> Kendall, M. G. & Stuart, A. </author> <year> (1972), </year> <title> The Advanced Theory of Statistics, third edn, </title> <publisher> Hafner Publishing Co., </publisher> <address> New York. </address>
Reference-contexts: The procedure is repeated for each regularization method (local/global smoothers of different orders and weight decay with or without including bias weights). Generalization performances of the different regularization methods are then compared pairwise, using a Wilcoxon rank test 8 <ref> (Kendall & Stuart 1972) </ref>. Two methods are considered significantly different if the null hypothesis (equal average generalization performance) is rejected at the 95% confidence level. As a base-level comparison, we also fit 100 linear models and 100 unregularized neural networks to test if our regularized networks perform significantly better.
Reference: <author> Leen, T. </author> <year> (1995), </year> <title> From data distributions to regularization in invariant learning, </title> <note> To appear in Neural Computation, </note> <year> 1995. </year>
Reference: <author> Lindley, D. V. & Smith, A. F. M. </author> <year> (1972), </year> <title> `Bayes estimates for the linear model', </title> <journal> Journal of the Royal Statistical Society B 34(302), </journal> <pages> 1-41. </pages> <note> 18 Moody, </note> <author> J. E. & Yarvin, N. </author> <year> (1992), </year> <title> Networks with learned unit response functions, </title> <editor> in J. E. Moody, S. J. Hanson & R. P. Lippmann, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems 4', </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <pages> pp. 1048-55. </pages>
Reference-contexts: From a Bayesian viewpoint, weight decay imposes a zero mean, spherically symmetric, gaussian prior on the network weights <ref> (Lindley & Smith 1972, Buntine & Weigend 1991) </ref>. The appropriateness of such an ad hoc prior has been questioned in the linear regression case (Smith & Campbell 1980), and it is equally questionable for nonlinear regression (e.g. neural networks).
Reference: <author> Moody, J. & Rognvaldsson, T. </author> <year> (1996), </year> <title> Smoothing regularizers for radial basis function networks, </title> <note> Manuscript in preparation. </note>
Reference-contexts: In this paper, we focus on the ubiquitous PBF networks. We derive and test the corresponding regularizers for RBFs in the companion paper <ref> (Moody & Rognvaldsson 1996) </ref>. 3 Regularizers can be viewed as being part of a more general class of biases called hints (Abu-Mostafa 1995). <p> Monte Carlo simulation results confirm that equations (27) are effective in penalizing the smoothness functional S (W; m). Both the global and local forms are very good, with the global being slightly better. In a companion paper <ref> (Moody & Rognvaldsson 1996) </ref>, we derive and test corresponding regularizers for radial basis functions. 14 Acknowledgements Both authors thank Steve Rehfuss and Dr. Lizhong Wu for stimulating input. John Moody thanks Volker Tresp for a provocative discussion at a 1991 Neural Networks Workshop sponsored by the Deutsche Informatik Akademie.
Reference: <author> Nowlan, S. & Hinton, G. </author> <year> (1992), </year> <title> `Simplifying neural networks by soft weight-sharing', </title> <booktitle> Neural Computation 4, </booktitle> <pages> 473-493. </pages>
Reference-contexts: Variations on the theme weight decay that have been presented in the neural network literature are e.g. weight elimination (Rumelhart 1988, Scalettar & Zee 1988, Chauvin 1990, Weigend, Rumelhart & Huberman 1990), Laplacian pruning (Williams 1995, Ishikawa 1996), and soft weight sharing <ref> (Nowlan & Hinton 1992) </ref>. These all build on the concept of imposing a prior distribution on the model parameters, and their motivation vis-a-vis quadratic weight decay is that a non-gaussian prior on the parameters is more likely to be correct than a gaussian prior.
Reference: <author> Plaut, D., Nowlan, S. & Hinton, G. </author> <year> (1986), </year> <title> Experiments on learning by back propagation, </title> <type> Technical Report CMU-CS-86-126, </type> <institution> Carnegie-Mellon University. </institution>
Reference-contexts: There are, however, qualitative differences between smoothing regularization and any conventional type of weight decay cost that are worth noting. Quadratic weight decay <ref> (Plaut, Nowlan & Hinton 1986) </ref>, which is essentially Hoerl & Kennard's (1970a) (1970b) ridge regression applied to nonlinear models, is the regularization method most often used for both linear regression and neural networks.
Reference: <author> Poggio, T. & Girosi, F. </author> <year> (1990), </year> <title> `Networks for approximation and learning', </title> <booktitle> IEEE Proceedings 78(9). </booktitle>
Reference: <author> Powell, M. </author> <year> (1987), </year> <title> Radial basis functions for multivariable interpolation: a review., </title> <editor> in J. Mason & M. Cox, eds, </editor> <title> `Algorithms for Approximation', </title> <publisher> Clarendon Press, Oxford. </publisher>
Reference-contexts: The use of smoothing functionals S (W ) like (2) has been extensively studied for smoothing splines (Eubank 1988, Hastie & Tibshirani 1990, Wahba 1990) and for radial basis function (RBF) networks <ref> (Powell 1987, Poggio & Girosi 1990, Girosi, Jones & Poggio 1995) </ref>. However, no general class of smoothing regularizers that directly enforce smoothness S (W; m) for projective basis functions (PBFs), such as the widely used sigmoidal PBFs, has hitherto been proposed. <p> It is quite clear that the weight decay cost kW k 2 is not a good estimate of S (W; m). 13 5.2 Other Approaches to Smoothing Neural Networks Smoothing splines (Eubank 1988, Hastie & Tibshirani 1990, Wahba 1990) and smoothing radial basis functions <ref> (Powell 1987, Poggio & Girosi 1990, Girosi et al. 1995) </ref> impose smoothness by requiring the forms of the hidden units g [] to be Greens functions of the smoothing operator S ().
Reference: <author> Riedmiller, M. & Braun, H. </author> <year> (1993), </year> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm, </title> <editor> in H. Ruspini, ed., </editor> <booktitle> `Proc. of the IEEE Intl. Conference on Neural Networks', </booktitle> <address> San Fransisco, California, </address> <pages> pp. 586-591. </pages>
Reference-contexts: However, considering low dimensional problems enables us to do extensive simulation studies. We use standard sigmoidal PBF networks for both problems, and study the effects of additive noise, sparse data sets, and the choice of order of the smoother. We train the networks using the RPROP algorithm <ref> (Riedmiller & Braun 1993) </ref>, which was empirically found to be the learning algorithm that converged quickest and reached the lowest errors. The training data are i.i.d. for both the inputs and the additive noise in the targets.
Reference: <author> Rumelhart, D. E. </author> <year> (1988), </year> <title> Learning and generalization, </title> <booktitle> in `Proc. of the IEEE Intl. Conference on Neural Networks', </booktitle> <address> San Diego. </address> <publisher> (Plenary address). </publisher>
Reference: <author> Scalettar, R. & Zee, A. </author> <year> (1988), </year> <title> Emergence of grandmother memory in feed forward networks: learning with noise and forgetfulness, </title> <editor> in D. Waltz & J. Feldman, eds, </editor> <title> `Connectionist Models and Their Implications: </title> <booktitle> Readings from Cognitive Science', </booktitle> <publisher> Ablex Pub. Corp. </publisher>
Reference: <author> Smith, G. & Campbell, F. </author> <year> (1980), </year> <title> `A critique of some ridge regression methods', </title> <journal> Journal of the American Statistical Association 75(369), </journal> <pages> 74-103. </pages> <editor> van Vuuren, S. H. J. </editor> <year> (1994), </year> <title> Neural network correlates with generalization, </title> <type> Master's thesis, </type> <institution> University of Pretoria. </institution> <note> http://www.cse.ogi.edu/~sarelv. </note>
Reference-contexts: From a Bayesian viewpoint, weight decay imposes a zero mean, spherically symmetric, gaussian prior on the network weights (Lindley & Smith 1972, Buntine & Weigend 1991). The appropriateness of such an ad hoc prior has been questioned in the linear regression case <ref> (Smith & Campbell 1980) </ref>, and it is equally questionable for nonlinear regression (e.g. neural networks). As an alternative view, it is sometimes argued that quadratic weight decay corresponds to a smoothness constraint.
Reference: <author> Wahba, G. </author> <year> (1990), </year> <title> Spline models for observational data, </title> <booktitle> CBMS-NSF Regional Conference Series in Applied Mathematics. </booktitle>
Reference: <author> Weigend, A., Rumelhart, D. & Huberman, B. </author> <year> (1990), </year> <title> Back-propagation, weight-elimination and time series prediction, </title> <editor> in T. Sejnowski, G. Hinton & D. Touretzky, eds, </editor> <booktitle> `Proceedings of the connectionist models summer school', </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <pages> pp. 105-116. </pages>
Reference: <author> Williams, P. M. </author> <year> (1995), </year> <title> `Bayesian regularization and pruning using a Laplace prior', </title> <booktitle> Neural Computation 7, </booktitle> <pages> 117-143. </pages>
Reference-contexts: No such coupling exists in quadratic weight decay. Variations on the theme weight decay that have been presented in the neural network literature are e.g. weight elimination (Rumelhart 1988, Scalettar & Zee 1988, Chauvin 1990, Weigend, Rumelhart & Huberman 1990), Laplacian pruning <ref> (Williams 1995, Ishikawa 1996) </ref>, and soft weight sharing (Nowlan & Hinton 1992).
Reference: <author> Wu, L. & Moody, J. </author> <year> (1996), </year> <title> `A smoothing regularizer for feedforward and recurrent networks', </title> <booktitle> Neural Computation 8(2). </booktitle> <pages> 19 </pages>
References-found: 29

