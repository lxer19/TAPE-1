URL: ftp://garovix.ijs.si/pub/papers/bz/phd_thesis.ps.gz
Refering-URL: http://www-ai.ijs.si/BlazZupan/phd_abs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: MACHINE LEARNING BASED ON FUNCTION DECOMPOSITION  
Author: Blaz Zupan 
Degree: DOCTORAL DISSERTATION Supervisor: Prof. Dr. Ivan Bratko  
Date: Ljubljana, 1997  
Affiliation: UNIVERSITY OF LJUBLJANA FACULTY OF COMPUTER AND INFORMATION SCIENCE  
Abstract-found: 0
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. S. </author> <year> (1988), </year> <title> Complexity in Information Theory, </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference: <author> Almuallim, H. & Dietterich, T. G. </author> <year> (1991), </year> <title> Learning with many irrelevant features, </title> <booktitle> in `Ninth National Conference on Artificial Intelligence', </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 547-552. </pages>
Reference: <author> Ashenhurst, R. L. </author> <year> (1952), </year> <title> The decomposition of switching functions, </title> <type> Technical report, </type> <institution> Bell Laboratories BL-1(11), </institution> <address> pages 541-602. </address>
Reference-contexts: Curtis (1962) reports that in the late 1940's and 1950's several switching circuit theorists considered this subject and in 1952 Ashenhurst reported on a unified theory of decomposition of switching functions <ref> (Ashenhurst 1952) </ref>. The method proposed by Ashenhurst decomposes a truth table of a Boolean function to be realized with standard binary gates.
Reference: <author> Biermann, A. W., Fairfield, J. & Beres, T. </author> <year> (1982), </year> <title> `Signature table systems and learning', </title> <journal> IEEE Trans. Syst. Man Cybern. </journal> <volume> 12(5), </volume> <pages> 635-648. </pages>
Reference: <author> Bohanec, M., Bratko, I. & Rajkovic, V. </author> <year> (1983), </year> <title> An expert system for decision making, </title> <editor> in H. G. Sol, ed., </editor> <title> `Processes and Tools for Decision Support', </title> <publisher> North-Holland. </publisher>
Reference: <author> Bohanec, M., Cestnik, B. & Rajkovic, V. </author> <year> (1996), </year> <title> A management decision support system for allocating housing loans, </title> <editor> in P. Humpreys, L. Bannon, A. McCosh & P. Migliarese, eds, </editor> <title> `Implementing System for Supporting Management Decisions', </title> <publisher> Chapman & Hall, London, </publisher> <pages> pp. 34-43. </pages>
Reference-contexts: The final decision depended on three subproblems: profession of parents and child's nursery, family structure and financial standing, and social and health conditions of the family. HOUSING: A model to determine the priority of housing loans applications <ref> (Bohanec, Cestnik & Rajkovic 1996) </ref>. This model is a part of a management decision support system for allocating housing loans that has been used since 1991 in the Housing Found of the Republic of Slovenia. <p> To derive more complex concept structures (in the form of acyclic graphs), the extension of the method, a non-disjoint decomposition, that handles non-disjoint partitions is required. Two variants of non-disjoint decompositions are presented in (Perkowski 1995) and <ref> (Zupan 102 Conclusion & Bohanec 1996) </ref>. Both show that a non-disjoint decomposition may be a straight forward extension of the disjoint decomposition. Another focus of the further work is to enhance HINT's capabilities to assist in the interpretation of derived example sets.
Reference: <author> Bohanec, M. & Rajkovic, V. </author> <year> (1988), </year> <title> Knowledge acquisition and explanation for multi-attribute decision making, </title> <booktitle> in `8th Intl Workshop on Expert Systems and their Applications', </booktitle> <address> Avignon, France, </address> <pages> pp. 59-78. </pages>
Reference-contexts: This makes models developed by DEX ideal benchmarks for the evaluation of decomposition. For this experiment, we have chosen six existing DEX models: CAR: A model for evaluating cars based on their price and technical characteristics. This simple model was developed for educational purposes and is described in <ref> (Bohanec & Rajkovic 1988) </ref>. EMPLOY1 and EMPLOY2: These are simplified versions of the models that were developed with DEX for a common problem of personnel management: selecting the best candidate for a particular job. <p> Another focus of the further work is to enhance HINT's capabilities to assist in the interpretation of derived example sets. Any of existing machine learning tools may be used for this. A possible alternative is also to use the appropriate tools included in a DEX decision support system <ref> (Bohanec & Rajkovic 1988) </ref>. However, the idea that we are most excited about is the inclusion of decomposition within an Inductive Logic Programming environment (ILP, see (Lavrac & Dzeroski 1994)).
Reference: <author> Bohanec, M. & Rajkovic, V. </author> <year> (1990), </year> <title> `DEX: An expert system shell for decision support', </title> <booktitle> Sistemica 1(1), </booktitle> <pages> 145-157. </pages>
Reference-contexts: The dissertation thus borrows from two different research areas: it shares the motivation with structured induction while the core of the method is based on the Ashenhurst-Curtis decomposition. Other approaches that most influenced this work include DEX|a structured approach to decision support <ref> (Bohanec & Rajkovic 1990) </ref>|and the minimal-error pruning approach to noise handling by Niblett & Bratko (1986) and Cestnik & Bratko (1991). <p> Michie (1995) emphasized the important role of the structured induction in the future and listed several real problems that were solved in this way. The concept hierarchy has also been used by a multi-attribute decision support expert system shell DEX <ref> (Bohanec & Rajkovic 1990) </ref> which has its roots in DECMAK methodology (Efstathiou & Rajkovic 1979, Bohanec, Bratko & Rajkovic 1983). There, a tree-like structure of variables is defined by an expert, and several tools assist in the acquisition of decision tables. <p> Column multiplicity (CM) This is the simplest partition selection measure and is equal to -(A; B), i.e., a column multiplicity of partition matrix P AjB . The idea for this measure came from the practical experience with the DEX decision support system <ref> (Bohanec & Rajkovic 1990) </ref>. There, the hierarchical system of decision tables is constructed manually and it has been found that decision tables that describe concepts with a small number of values are easier to construct and interpret. <p> We have considered the training sets of three different types: * Artificially generated training sets using binary and multi-valued functions. The structure and intermediate concepts for these domains are anticipated. * The training sets obtained from multi-attribute decision models originally developed using DEX <ref> (Bohanec & Rajkovic 1990) </ref>. The DEX models are hierarchical, so both structure and intermediate concepts for these domains are known. * Several training sets from machine learning repository (Murphy & Aha 1994) for which the structure is not known to us. <p> However, the acquisition of functions is well supported by various methods based on interactive dialogs, graphs, and tables. DEX <ref> (Bohanec & Rajkovic 1990) </ref> is a multi-attribute decision support system that has been extensively used to solve realistic decision making problems. DEX uses categorical attributes and expects the structure and the functions to be given by the expert. <p> Actually, this is a single model from a suite of models developed for this purpose and used for the evaluation of enterprises in Pakistan and Peru <ref> (Bohanec & Rajkovic 1990) </ref>. The performance of enterprises is measured by three main groups of concepts: financial, economic, and social. The goal of this experiment was to reconstruct these models from examples.
Reference: <author> Bohanec, M., Urh, B. & Rajkovic, V. </author> <year> (1992), </year> <title> `Evaluating options by combined qualitative and quantitative methods', </title> <journal> Acta Psychologica 80, </journal> <pages> 67-89. </pages>
Reference-contexts: EMPLOY1 composes the attributes into education of the candidate, age and experience, and personal characteristics. In EMPLOY2, the latter is replaced by other characteristics. EMPLOY1 is presented in <ref> (Bohanec, Urh & Rajkovic 1992) </ref>, while EMPLOY2 is previously unpublished. NURSERY: This model was developed in 1985 (Olave, Rajkovic & Bohanec 1989) to rank applications for nursery schools.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and regression trees, Wadsworth International Group. 104 BIBLIOGRAPHY Cestnik, </title> <editor> B. </editor> <year> (1990), </year> <title> Estimating probabilities: A crucial task in machine learning, </title> <booktitle> in `Proc. 9th European Conference on Artificial Intelligence ECAI90', </booktitle> <pages> pp. 147-149. </pages>
Reference-contexts: In our implementation 56 Decomposition-based attribute selection and discovery of redundancies incorporated within program HINT, the user can choose between any of the following: information gain (Quinlan 1979), GINI index of diversity <ref> (Breiman, Friedman, Olshen & Stone 1984) </ref>, gain-ratio measure (Quinlan 1986b), and ReliefF and MDL-based measures (Kononenko 1994, Kononenko 1995). The attribute subset selection method is given as Algorithm 3.1.
Reference: <author> Cestnik, B. & Bratko, I. </author> <year> (1991), </year> <title> On estimating probabilities in tree prunning, </title> <booktitle> in `Proc. European Workshop on Symbolic Learning EWSL-91'. </booktitle>
Reference-contexts: As expected, the number of attributes that remain after preprocessing by attribute selection decreases with m. Similar is true for the overall complexity of discovered functions. This is consistent with the use of m for minimal-error pruning of decision trees <ref> (Cestnik & Bratko 1991) </ref>, where with higher value of m the resulting trees are less complex. The concept structures discovered for the medical domains from this section are given in Appendix E. Because of the unavailability of the domain experts, we could not interpret these structures.
Reference: <author> Cestnik, B., Kononenko, I. & Bratko, I. </author> <year> (1987), </year> <title> Assistant 86: A knowledge-elicitation tool for sophisticated users, </title> <editor> in I. Bratko & N. Lavrac, eds, </editor> <booktitle> `Progress in Machine Learning', </booktitle> <publisher> Sigma Press, </publisher> <pages> pp. 31-45. </pages>
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1987), </year> <title> Induction in noisy domains, </title> <editor> in I. Bratko & N. Lavrac, eds, </editor> <booktitle> `Progress in Machine Learning', </booktitle> <publisher> Sigma Press, Wilmslow, </publisher> <pages> pp. 11-30. </pages>
Reference-contexts: The real-world data, however, most often violates this constraint. Its inconsistency may be contributed to either noise or the incompleteness of the description language <ref> (Clark & Niblett 1987) </ref>. Other properties of data that the minimal-complexity decomposition could not deal with further include missing attribute values and uncertainty. This chapter proposes a different decomposition method that is based on the minimization of classification error.
Reference: <author> Curtis, H. A. </author> <year> (1962), </year> <title> A New Approach to the Design of Switching Functions, </title> <publisher> Van Nostrand, </publisher> <address> Princeton, N.J. </address>
Reference-contexts: The method proposed by Ashenhurst decomposes a truth table of a Boolean function to be realized with standard binary gates. Most of other related work of those times is reported and reprinted in <ref> (Curtis 1962) </ref>, where Curtis compares the decomposition approach to other switching circuit design approaches and further formalizes and extends the decomposition theory.
Reference: <author> Demsar, J. </author> <year> (1996), </year> <title> `Decomposition of real functions'. </title> <publisher> B. </publisher> <address> Sc. </address> <note> Thesis (in Slovene). </note>
Reference: <author> Demsar, J., Zupan, B., Bohanec, M. & Bratko, I. </author> <year> (1997), </year> <title> Constructing intermediate concepts by decomposition of real functions, </title> <booktitle> in `Proc. European Conference on Machine Learning, </booktitle> <address> ECML-96', Prague. </address>
Reference: <author> Diamantidis, N. & Giakoumakis, E. G. </author> <year> (1996), </year> <title> `Don't care values in induction', </title> <booktitle> Artificial Intelligence in Medicine 8, </booktitle> <pages> 505-514. </pages>
Reference-contexts: Noise, for example, may occur because of imperfect measuring equipment used to collect the data. The incompleteness of description language may contribute to the existence of the examples with same attribute values but different class. Another common problem, in particular with data in medical domains <ref> (Diamantidis & Giakoumakis 1996) </ref> are also missing attribute values. There, the attribute values are either unavailable (say some expensive medical test was not made for that subject) or they are irrelevant.
Reference: <author> Efstathiou, J. & Rajkovic, V. </author> <year> (1979), </year> <title> `Multiattribute decisionmaking using a fuzzy heuristic approach', </title> <journal> IEEE Trans. on Systems, Man and Cybernetics 9, </journal> <pages> 326-333. </pages>
Reference-contexts: The concept hierarchy has also been used by a multi-attribute decision support expert system shell DEX (Bohanec & Rajkovic 1990) which has its roots in DECMAK methodology <ref> (Efstathiou & Rajkovic 1979, Bohanec, Bratko & Rajkovic 1983) </ref>. There, a tree-like structure of variables is defined by an expert, and several tools assist in the acquisition of decision tables. These are, like Samuel's signature tables, used to derive the values of intermediate and output variables.
Reference: <author> Goldman, J. </author> <year> (1994a), </year> <title> Pattern theoretic knowledge discovery, </title> <type> Technical report, </type> <institution> GSRP Wright Laboratories. </institution>
Reference: <author> Goldman, J. A. </author> <year> (1994b), </year> <title> Pattern theoretic knowledge discovery, </title> <booktitle> in `6th Int'l IEEE Conference on Tools with AI'. </booktitle>
Reference: <author> Halter, J. A., Carp, J. S. & Wolpaw, J. W. </author> <year> (1995), </year> <title> `Operantly conditioned motoneuron plasticity: possible role of sodium channels', </title> <journal> J. </journal> <volume> Neurophysiology 73(2), </volume> <pages> 867-871. </pages>
Reference: <author> Halter, J. A. & Clark, J. W. </author> <year> (1991), </year> <title> `A distributed-parameter model of the myelinated nerve fiber', </title> <journal> J. Theo. Biol. </journal> <volume> 148, </volume> <pages> 345-382. </pages>
Reference-contexts: The model is given in the form of a system of multiple cross-coupled parabolic partial differential equations that are solved by an implicit numerical integration method. The model is used to predict the functional implications of neuronal structural and biophysical properties <ref> (Halter & Clark 1991, Halter, Carp & Wolpaw 1995) </ref> and has recently been employed to guide laboratory experiments and a study of pathophysiologic significance of abnormal myelination.
Reference: <author> John, G. H., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <pages> pp. 121-129. </pages> <note> BIBLIOGRAPHY 105 Kohavi, </note> <author> R. & John, G. H. </author> <year> (1997), </year> <title> `Wrappers for feature subset selection', </title> <note> Artificial Intelligence Journal . to appear. </note>
Reference-contexts: From the point of view of machine learning, the attribute subset selection attempts to find a subset of attributes under some objective function. Common objective functions are <ref> (John, Kohavi & Pfleger 1994) </ref>: prediction accuracy, the complexity of the derived classifier, and minimal use of input attributes. The induced classifier may thus be more accurate at one hand, and less complex and therefore potentially more transparent on the other hand. <p> Note that the relevancy and redundancy of attributes are re-evaluated every time some redundancy is discovered and removed. The method falls under the category of filter algorithms <ref> (John et al. 1994) </ref> of which the aim is solely the reduction of number of attributes used. It is different from other attribute subset selection algorithms in attempt not only to select a subset of attributes but also to minimize the number of attribute values.
Reference: <author> Kononenko, I. </author> <year> (1994), </year> <title> Estimating attributes, </title> <editor> in F. Bergadano & L. D. Raedt, eds, </editor> <booktitle> `Proc. of the European Conference on Machine Learning (ECML-94)', Vol. 784 of Lecture Notes in Artificial Intelligence, </booktitle> <publisher> Springer-Verlag, Catania, </publisher> <pages> pp. 171-182. </pages>
Reference-contexts: In our implementation 56 Decomposition-based attribute selection and discovery of redundancies incorporated within program HINT, the user can choose between any of the following: information gain (Quinlan 1979), GINI index of diversity (Breiman, Friedman, Olshen & Stone 1984), gain-ratio measure (Quinlan 1986b), and ReliefF and MDL-based measures <ref> (Kononenko 1994, Kononenko 1995) </ref>. The attribute subset selection method is given as Algorithm 3.1. R (a i ) is the relevance estimated for attribute a i , such that if R (a i ) &gt; R (a j ), the attribute a i is more relevant than a j .
Reference: <author> Kononenko, I. </author> <year> (1995), </year> <title> On biases in estimating the multivalued attributes, </title> <booktitle> in `Proc. Intl. Joint Conference on Artificial Intelligence (IJCAI-95)', Montreal, </booktitle> <pages> pp. 1034-1040. </pages>
Reference-contexts: Again, only the attributes within the window of 4 remained. Furthermore, it was noticed that the attributes that used higher number of values remained. That could be accounted to the fact that HINT used information measure to estimate the relevancy of attributes, which is biased toward such attributes <ref> (Kononenko 1995) </ref>. However, when an alternative and differently biased MDL-based measure was used instead, the results of attribute subset selection were not significantly different.
Reference: <author> Kononenko, I., Bratko, I. & Roskar, E. </author> <year> (1984), </year> <title> Experiments in automatic learning of medical diagnostic rules, </title> <booktitle> in `Proc. ISEEK Workshop', </booktitle> <address> Bled. </address>
Reference-contexts: Again, we handle both don't-cares and don't-knows by conversion of corresponding examples to the example set that uses class distributions. We use a similar approach to handle don't-cares and don't-knows that was used in Assistant <ref> (Kononenko et al. 1984, Cestnik, Kononenko & Bratko 1987) </ref>. For don't-cares, the conversion is as follows. Let an example use the attributes a 1 ; : : : ; a n with missing values of don't care type.
Reference: <author> Kononenko, I., Simec, E. & Robnik Sikonja, M. </author> <year> (1997), </year> <title> `Overcoming the myopia of inductive learning algorithms with ReliefF', </title> <journal> Applied Intelligence Journal 7(1), </journal> <pages> 39-56. </pages>
Reference-contexts: Basic description of the medical datasets is given in Figure 4.8. It should be further noted that these datasets include noise, and most include examples with missing values as well. The classification accuracy evaluation method was the same as in <ref> (Kononenko et al. 1997) </ref>: each set was randomly split to the learning set that included 70% of examples and to test set with 30% of examples. For each domain, the results are an average of 30 such experiments. The performance of HINT is compared to several other classifiers. <p> For each domain, the results are an average of 30 such experiments. The performance of HINT is compared to several other classifiers. The results are taken from <ref> (Kononenko et al. 1997) </ref> and were not obtained using the same learning and test sets. Assistant-I, Assistant-R, and LFC all induce decision trees. Assistant-I uses information gain and Assistant-R uses ReliefF as an attribute selection criterion. <p> Other two classification tools used are the naive Bayesian classifier and the k-nearest neighbor algorithm. For further details on these methods see <ref> (Kononenko et al. 1997) </ref>. The classification accuracy results are shown in Table 4.9.
Reference: <author> Lavrac, N. & Dzeroski, S. </author> <year> (1994), </year> <title> Inductive logic programming: techniques and applications, </title> <publisher> Ellis Horwood. </publisher>
Reference-contexts: A possible alternative is also to use the appropriate tools included in a DEX decision support system (Bohanec & Rajkovic 1988). However, the idea that we are most excited about is the inclusion of decomposition within an Inductive Logic Programming environment (ILP, see <ref> (Lavrac & Dzeroski 1994) </ref>). A decomposition may benefit from ILP's ability to efficiently use the background knowledge, while ILP can benefit from the decomposition's ability to derive smaller and less manageable example sets and from its methodological approach that may be adopted for predicate invention.
Reference: <author> Luba, T. </author> <year> (1995), </year> <title> Decomposition of multiple-valued functions, </title> <booktitle> in `25th Intl. Symposium on Multiple-Valued Logic', Bloomigton, Indiana, </booktitle> <pages> pp. 256-261. </pages>
Reference: <author> Mallach, E. G. </author> <year> (1994), </year> <title> Understanding decision support systems and expert systems, </title> <publisher> Irwin. </publisher>
Reference-contexts: The decision is made on the basis of the evaluation of options by a multi-attribute hierarchical model. In addition to evaluation, such models are used also for various analyses and simulations of options. Most common are the models that use numerical values and analytically expressed functions <ref> (Mallach 1994) </ref>. A typical example of such an approach is Analytic Hierarchy Process (Saaty 1993).
Reference: <author> Michalski, R. S. </author> <year> (1983), </year> <title> A theory and methodology of inductive learning, </title> <editor> in R. Michal-ski, J. Carbonnel & T. Mitchell, eds, </editor> <booktitle> `Machine Learning: An Artificial Intelligence Approach', </booktitle> <publisher> Kaufmann, </publisher> <address> Paolo Alto, CA, </address> <pages> pp. 83-134. </pages>
Reference-contexts: Following this idea and perhaps closest to the function decomposition are 1.2 Related work 7 the constructive induction systems that use a set of constructive operators to derive new attributes. Examples of such systems are described in <ref> (Michalski 1983, Pfahringer 1994, Ragavan & Rendell 1993) </ref>. The main limitation of these approaches is that the set of constructive operators has to be defined in advance.
Reference: <author> Michalski, R. S. </author> <year> (1986), </year> <title> Understanding the nature of learning: issues and research directions, </title> <editor> in R. Michalski, J. Carbonnel & T. Mitchell, eds, </editor> <booktitle> `Machine Learning: An Artificial Intelligence Approach', </booktitle> <publisher> Kaufmann, </publisher> <address> Los Atlos, CA, </address> <pages> pp. 3-25. </pages>
Reference: <author> Michie, D. </author> <year> (1995), </year> <title> Problem decomposition and the learning of skills, </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> `Machine Learning: ECML-95', Notes in Artificial Intelligence 912, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 17-31. </pages>
Reference: <editor> Michie, D., Spiegelhalter, D. J. & Taylor, C. C., eds (1994), </editor> <title> Machine learning, neural and statistical classification, </title> <publisher> Ellis Horwood. </publisher>
Reference-contexts: different learners, and possibly for a different learner with optimized performance for a specific domain, we have decided for C4.5 for the following reasons: (1) it is one of the most known and used learning algorithms, (2) it performs well in terms of classification accuracy when compared to other learners <ref> (Michie, Spiegelhalter & Taylor 1994) </ref>. <p> DIAB: the diagnostic data for diabetes, HEART: diagnosis of heart diseases. Originally, PRIM, BREA, LYMP, and RHEU are the datasets from University Medical Center in Ljubljana, Slovenia, HEPA was provided by Gail Gong from Carnegie-Mellon University, and DIAB and HEART were obtained from StatLog database <ref> (Michie et al. 1994) </ref>. Most of the datasets include continuously-valued attributes. We use the same attribute discretization as used in the study of Kononenko et al. (1997). Basic description of the medical datasets is given in Figure 4.8.
Reference: <author> Mladenic, D. </author> <year> (1995), </year> <title> Domain-tailored machine learning, </title> <type> Master's thesis, </type> <institution> Faculty of Electrical Engineering and Computer Science, University of Ljubljana. </institution> <note> 106 BIBLIOGRAPHY Mozetic, </note> <author> I. & Hodoscek, M. </author> <year> (1997), </year> <title> Symbolic protein data base, </title> <type> Technical report, </type> <institution> Institute Jozef Stefan, </institution> <address> IJS-DP 7505. </address>
Reference-contexts: Finally, m which yields a classifier of best performance is used to induce a classifier from the complete learning set. The implementation and a detailed study of the second approach applied for the top-down induction of decision trees is presented in <ref> (Mladenic 1995) </ref>. To estimate m, Mladenic uses an optimization technique that assesses the performance of the classifier induced using a specific value of m by the k-fold cross validation: given a learning set, this is randomly split to k mutually exclusive subsets (folds) of approximately equal size.
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <note> UCI Repository of machine learning databases [http://www.ics.uci.edu/~mlearn/mlrepository.html]. Irvine, </note> <institution> CA: University of Califor-nia, Department of Information and Computer Science. </institution>
Reference-contexts: The DEX models are hierarchical, so both structure and intermediate concepts for these domains are known. * Several training sets from machine learning repository <ref> (Murphy & Aha 1994) </ref> for which the structure is not known to us. The first group of examples is to illustrate the main characteristics of decomposition method: discovery of concepts and functions. The examples also illustrate the method's ability to learn from an incomplete set of examples. <p> In spite of the large number of training examples, the time needed for learning was small or moderate (Table 2.9). 2.7.3 Several domains from the ML repository The following set of examples was taken from UCI machine learning repository <ref> (Murphy & Aha 1994) </ref>: LENSES: A small domain that uses patient's age, spectacle prescription, astigmatism, and tear production rate and describes whether the patient should wear soft or hard contact lenses or no lenses at all. <p> MONK1 and MONK2: Well-known six-attribute binary classification problems <ref> (Murphy & Aha 1994, Thrun et al. 1991) </ref>. Attributes are 2 to 4-valued. MONK1 has an underlying concept a=b OR e=1 and MONK2 the concept x = 1 for exactly two choices of attributes x 2 fa; b; c; d; e; fg. <p> To a further extent, it is hard to evaluate the performance of minimal-error decomposition and compare it to other machine learning methods. Namely, for most of real-world domains available (for example, those at UCI Machine Learning Repository <ref> (Murphy & Aha 1994) </ref>), insufficient description is provided for someone not familiar with the domain to evaluate the appropriateness of the concept structures derived by HINT. For such domains we could only evaluate HINT's ability to handle imperfect data by means of classification accuracy.
Reference: <author> Niblett, T. & Bratko, I. </author> <year> (1986), </year> <title> Learning decision rules in noisy domains, in `Expert Systems 86', </title> <publisher> Cambridge University Press, </publisher> <pages> pp. 15-18. </pages> <booktitle> (Proc. EWSL 1986, </booktitle> <address> Brighton). </address>
Reference-contexts: He demonstrated that several machine learning classifiers (naive Bayesian classifier, decision trees) perform significantly better when the induction uses the m-probability estimate instead of other known estimates like relative frequency or the Laplace's law of succession <ref> (Niblett & Bratko 1986) </ref>. The m-probability estimate can also be used to estimate the error when classifying a new example to the class c. Given an example set as above, the m-error estimate is: " = 1 N + m The value of parameter m is domain dependent.
Reference: <author> Olave, M., Rajkovic, V. & Bohanec, M. </author> <year> (1989), </year> <title> An application for admission in public school systems, </title> <editor> in I. T. M. Snellen, W. B. H. J. van de Donk & J.-P. Baquiast, eds, </editor> <title> `Expert Systems in Public Administration', </title> <publisher> Elsevier Science Publishers (North Holland), </publisher> <pages> pp. 145-160. </pages>
Reference-contexts: EMPLOY1 composes the attributes into education of the candidate, age and experience, and personal characteristics. In EMPLOY2, the latter is replaced by other characteristics. EMPLOY1 is presented in (Bohanec, Urh & Rajkovic 1992), while EMPLOY2 is previously unpublished. NURSERY: This model was developed in 1985 <ref> (Olave, Rajkovic & Bohanec 1989) </ref> to rank applications for nursery schools. It was used during several years when there was excessive enrollment to these schools in Ljubljana, and the rejected applications frequently needed an objective explanation.
Reference: <author> Perkowski, M. A. </author> <year> (1995), </year> <title> A survey of literature on function decomposition, </title> <type> Technical report, </type> <institution> GSRP Wright Laboratories, Ohio OH. </institution>
Reference-contexts: A heuristics to propose a candidate partition without performing an exhaustive search may result in substantial reduction of time-complexity. The method may be based on some of existing partition construction approaches that were developed for Boolean function decomposition <ref> (Perkowski 1995) </ref>. * An extension of decomposition that appropriately handles continuously-valued attributes, but does not require to discretize them in advance; the work of Demsar, Zupan, Bohanec & Bratko (1997) may be a starting point for such extension. * Instead of the default rule (see section 2.5) to classify the new <p> To derive more complex concept structures (in the form of acyclic graphs), the extension of the method, a non-disjoint decomposition, that handles non-disjoint partitions is required. Two variants of non-disjoint decompositions are presented in <ref> (Perkowski 1995) </ref> and (Zupan 102 Conclusion & Bohanec 1996). Both show that a non-disjoint decomposition may be a straight forward extension of the disjoint decomposition. Another focus of the further work is to enhance HINT's capabilities to assist in the interpretation of derived example sets.
Reference: <author> Pfahringer, B. </author> <year> (1994), </year> <title> Controlling constructive induction in CiPF, </title> <editor> in F. </editor> <publisher> Bergadano & L. </publisher>
Reference: <author> D. Raedt, eds, </author> <title> `Machine Learning: </title> <publisher> ECML-94', Springer-Verlag, </publisher> <pages> pp. 242-256. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1979), </year> <title> Discovering of rules by induction from large collections of examples, </title> <editor> in D. Michie, ed., </editor> <booktitle> `Expert systems in micro-electronic age', </booktitle> <publisher> Edinburgh University Press. </publisher>
Reference-contexts: The measures that estimate the relevance of the attributes have been extensively studied within statistics and machine learning. In our implementation 56 Decomposition-based attribute selection and discovery of redundancies incorporated within program HINT, the user can choose between any of the following: information gain <ref> (Quinlan 1979) </ref>, GINI index of diversity (Breiman, Friedman, Olshen & Stone 1984), gain-ratio measure (Quinlan 1986b), and ReliefF and MDL-based measures (Kononenko 1994, Kononenko 1995). The attribute subset selection method is given as Algorithm 3.1.
Reference: <author> Quinlan, J. R. </author> <year> (1986a), </year> <title> Learning from noisy data, </title> <booktitle> in `Proc. International Machine Learning Workshop', </booktitle> <institution> University of Illinois at Urbana Champaign, </institution> <note> pp. 58-64. </note>
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The dissertation proposes the decomposition method and also evaluates it from these two perspectives. To evaluate the classification accuracy, in most of the cases the performance of the decomposition is compared to that of a state-of-the-art induction tool C4.5 <ref> (Quinlan 1993) </ref>. To assess the decomposition's ability to derive meaningful structures with useful concepts, whenever possible, the domains were chosen for which such structures are either known or anticipated. <p> HINT derived a concept hierarchy and corresponding classifier using the examples in the training set and was tested for classification accuracy on the test set. For each p, the results are the average of 10 randomly chosen splits. The learning curve is compared to the one obtained by C4.5 <ref> (Quinlan 1993) </ref> inductive decision tree learner run on the same data.
Reference: <author> Quinlan, R. </author> <year> (1986b), </year> <title> `Induction of decision trees', </title> <booktitle> Machine Learning 1(1), </booktitle> <pages> 81-106. </pages>
Reference-contexts: In our implementation 56 Decomposition-based attribute selection and discovery of redundancies incorporated within program HINT, the user can choose between any of the following: information gain (Quinlan 1979), GINI index of diversity (Breiman, Friedman, Olshen & Stone 1984), gain-ratio measure <ref> (Quinlan 1986b) </ref>, and ReliefF and MDL-based measures (Kononenko 1994, Kononenko 1995). The attribute subset selection method is given as Algorithm 3.1.
Reference: <author> Ragavan, H. & Rendell, L. </author> <year> (1993), </year> <title> Lookahead feature construction for learning hard concepts, </title> <booktitle> in `Proc. Tenth International Machine Learning Conference', </booktitle> <publisher> Morgan Kaufman, </publisher> <pages> pp. 252-259. </pages>
Reference: <author> Rajkovic, V. & Bohanec, M. </author> <year> (1991), </year> <title> Decision support by knowledge explanation, </title> <booktitle> in H. </booktitle>
Reference-contexts: These are, like Samuel's signature tables, used to derive the values of intermediate and output variables. DEX also allows different representation of defined decision tables, including decision trees (as in Shapiro 1987) and decision rules <ref> (Rajkovic & Bohanec 1991) </ref>. DEX has been applied in more than 50 realistic decision making problems. Although developed independently, the hierarchical structure-based representations of a signature table system, DEX models, or of the systems discovered by a switching function decomposition are surprisingly similar.
Reference: <editor> G. Sol & J. Vecsenyi, eds, </editor> <title> `Environments for supporting Decision Process', </title> <publisher> Elsevier Science Publishers B.V. BIBLIOGRAPHY 107 Ross, </publisher> <editor> T. D., Noviskey, M. J., Axtell, M. L., Gadd, D. A. & Goldman, J. A. </editor> <year> (1994), </year> <title> Pattern theoretic feature extraction and constructive induction, </title> <type> Technical report, </type> <institution> Wright Laboratory, USAF, WL/AART, WPAFB, Ohio, OH. </institution>
Reference: <author> Ross, T. D., Noviskey, M. J., Gadd, D. A. & Goldman, J. A. </author> <year> (1994), </year> <title> Pattern theoretic feature extraction and constructive induction, </title> <booktitle> in `Proc. ML-COLT '94 Workshop on Constructive Induction and Change of Representation', </booktitle> <address> New Brunswick, New Jersey. </address>
Reference-contexts: The author called this measure a decomposed function cardinality (DFC) and computed it as Q Although its ability to guide the decomposition of Boolean functions has been illustrated in several references including <ref> (Ross, Noviskey, Gadd & Goldman 1994) </ref>, DFC has been recently criticized by Perkowski (1995) for deficiencies in handling some classes of functions including multi-output symmetric functions.
Reference: <author> Saaty, T. L. </author> <year> (1993), </year> <title> Multicriteria decision making: The analytic hierarchy process, </title> <publisher> RWS Publications. </publisher>
Reference-contexts: In addition to evaluation, such models are used also for various analyses and simulations of options. Most common are the models that use numerical values and analytically expressed functions (Mallach 1994). A typical example of such an approach is Analytic Hierarchy Process <ref> (Saaty 1993) </ref>.
Reference: <author> Samuel, A. </author> <year> (1959), </year> <title> `Some studies in machine learning using the game of checkers', </title> <journal> IBM J. Res. Develop. </journal> <volume> 3, </volume> <pages> 221-229. </pages>
Reference-contexts: Given a set of training examples and a signature table system's structure, Samuel proposed an approach to learn the corresponding signature tables by incrementally adjusting the examples describing the intermediate concepts. Compared to his previous approach that was based on the learning of a linear evaluation polynomial <ref> (Samuel 1959) </ref>, Samuel showed that using a signature table system the performance can be significantly improved. Samuel's approach was later improved by Biermann, Fairfield 6 Introduction & Beres (1982), but still did not address the problem of deriving the structure of variables.
Reference: <author> Samuel, A. </author> <year> (1967), </year> <title> `Some studies in machine learning using the game of checkers II: Recent progress', </title> <journal> IBM J. Res. Develop. </journal> <volume> 11, </volume> <pages> 601-617. </pages>
Reference-contexts: Samuel. He proposed a method based on a signature table system <ref> (Samuel 1967) </ref> and used it as an evaluation mechanism for checker playing programs. A signature table system is a tree of input, intermediate, and a single output variable (the nodes).
Reference: <author> Shapiro, A. D. </author> <year> (1987), </year> <title> Structured induction in expert systems, </title> <publisher> Turing Institute Press in association with Addison-Wesley Publishing Company. </publisher>
Reference-contexts: There, a tree-like structure of variables is defined by an expert, and several tools assist in the acquisition of decision tables. These are, like Samuel's signature tables, used to derive the values of intermediate and output variables. DEX also allows different representation of defined decision tables, including decision trees <ref> (as in Shapiro 1987) </ref> and decision rules (Rajkovic & Bohanec 1991). DEX has been applied in more than 50 realistic decision making problems. Although developed independently, the hierarchical structure-based representations of a signature table system, DEX models, or of the systems discovered by a switching function decomposition are surprisingly similar.
Reference: <author> Shapiro, A. D. & Niblett, T. </author> <year> (1982), </year> <title> Automatic induction of classificiation rules for a chess endgame, </title> <editor> in M. R. B. Clarke, ed., </editor> <booktitle> `Advances in Computer Chess 3', </booktitle> <publisher> Pergamon, Oxford, </publisher> <pages> pp. 73-92. </pages>
Reference-contexts: A possible solution to these problems is proposed by an approach called structured induction <ref> (Shapiro & Niblett 1982) </ref>. The concept structure is elicited from the domain-expert and the result is a so-called human-generated problem decomposition. The next stage involves the expert to select from the training dataset the relevant sets of examples for each of the concepts.
Reference: <author> Shortliffe, E. H. </author> <year> (1993), </year> <title> `The adolescence of AI in medicine: will field come of age in the '90s?', </title> <booktitle> Artificial Intelligence in Medicine 5(2), </booktitle> <pages> 93-106. </pages>
Reference-contexts: For example, in the domain of medicine, only four years ago one of the fathers of "Artificial Intelligence in Medicine" Edward H. Shortliffe partially blamed the underdeveloped infrastructure for the failure to fulfill the initial promise of the field <ref> (Shortliffe 1993) </ref>. Recently, however, the situation is changing rapidly: modern hospitals are well equipped with monitoring and other data collection devices, and data is gathered and shared in inter- and intra-hospital information systems.
Reference: <author> Spackman, K., Elert, J. D. & Beck, J. R. </author> <year> (1993), </year> <title> The CIO and the medical informati-cist: alliance for progress, </title> <booktitle> in `Proc. Annual Symposium on Computer Applications in Medical Care', </booktitle> <pages> pp. 525-528. </pages>
Reference-contexts: Recently, however, the situation is changing rapidly: modern hospitals are well equipped with monitoring and other data collection devices, and data is gathered and shared in inter- and intra-hospital information systems. In fact, medical informatics has become a must and an integral part of every successful medical institution <ref> (Spackman, Elert & Beck 1993) </ref>. The increase of data volume causes greater difficulties to extract useful information for decision support, discovery of underlying principles, or different analysis tasks. The traditional manual data analysis has become insufficient, and methods for efficient computer-based analysis indispensable.
Reference: <author> Spiegel, M. R. </author> <year> (1991), </year> <title> Theory and Problems of Probability and Statistics, </title> <publisher> McGraw-Hill. </publisher>
Reference: <author> Stahl, I. </author> <year> (1991), </year> <title> An overview of predicate invention techniques in ILP, </title> <booktitle> in `ESPRIT BRA 6020: Inductive Logic Programming'. </booktitle>
Reference-contexts: In first-order learning of relational concept descriptions, constructive induction is referred to as predicate invention. An overview of recent achievements in this area can be found in <ref> (Stahl 1991) </ref>. Within machine learning, there are other approaches that are based on problem decomposition, but where the problem is decomposed by the expert and not discovered by a machine. A well-known example is structured induction, a term introduced by Donald Michie and applied by Shapiro (1987).
Reference: <author> Thrun et al., S. B. </author> <year> (1991), </year> <title> A performance comparison of different learning algorithms, </title> <type> Technical report, </type> <institution> Carnegie Mellon University CMU-CS-91-197. </institution>
Reference-contexts: not possess a structure that would be recognizable by decomposition and would have a positive effect on classification accuracy. 2.7.4 Further experiments on MONK1 and MONK2 HINT was further tested on the data sets for MONK1 and MONK2 that were used in the detailed study of 25 machine learning algorithms <ref> (Thrun et al. 1991) </ref>. The summary of experimental results for this study are given in Appendix D. For both MONK1 and MONK2, the training set was the same as our original data set described above. <p> The two test sets used in the study consisted of 432 examples that completely covered the attribute space. For MONK1, the accuracy of HINT is 100%. This score was in the study <ref> (Thrun et al. 1991) </ref> achieved by 9 learners (three variants of AQ17, Assistant Professional, mFOIL, CN2, two variants of Backpropagation, and Cascade Correlation). <p> Next, the decomposition performance on the domain MONK3 is evaluated. This domain, like MONK1 and MONK2 (see section 2.7.4), was originally used in the study of classification accuracy of several machine learning algorithms <ref> (Thrun et al. 1991) </ref>. We compare the results obtained in this study to the classification accuracy of the minimal-error decomposition and also compare the structure discovered to the original definition of the problem. <p> in this section the value of m increases with noise level. domain 10% 20% 30% MM4 0.9 0.6 2.0 1.1 3.6 1.9 SHUTTLE 1.4 0.5 2.6 0.3 2.8 0.7 Table 4.6: Discovered values for m for different noise levels 4.6.2 MONK3 The detailed study of 25 machine learning algorithms in <ref> (Thrun et al. 1991) </ref> mentioned in section 2.7.4 used also a domain MONK3 with the target concept defined as: MONK3 = e=3 AND d=1 OR e6=4 AND b6=3 This binary classification problem uses two 2-valued attributes (c and f), three 3-valued attributes (a, b, and d), and a 4-valued attribute e.
Reference: <author> Zupan, B. </author> <year> (1997), </year> <title> HINT script language: Description with examples, </title> <type> Technical report, </type> <institution> J. </institution>
Reference-contexts: From the original dataset a set with 2543 examples was derived where each of the attributes was discretized using 5 intervals. The method used a genetic algorithm <ref> (Zupan, Halter & Bohanec to appear in 1997) </ref> to find the discretization intervals that minimize the classification error when new example set is used to classify the instances of original set. <p> Such a new example describes the class that is the majority class of the corresponding examples of the original set. The discretization method is further described in <ref> (Zupan, Halter & Bohanec to appear in 1997) </ref>. When used to classify the items in the dataset, the discretized example set was found to misclassify only 7 instances of the original dataset (0:23%).
Reference: <institution> Stefan Institute, Ljubljana, Slovenia. </institution> <note> 108 BIBLIOGRAPHY Zupan, </note> <author> B. & Bohanec, M. </author> <year> (1996), </year> <title> Learning concept hierarchies from examples by function decomposition, </title> <type> Technical report, </type> <institution> IJSDP-7455, J. Stefan Institute, Ljubljana. </institution> <address> URL ftp://ftp-e8.ijs.si/pub/reports/IJSDP-7455.ps. </address>
Reference: <author> Zupan, B., Demsar, J. & Bohanec, M. </author> <year> (1997), </year> <title> Structure dissimilarity coefficient: definition and experiments, </title> <type> Technical report, </type> <institution> Institute Jozef Stefan. </institution>
Reference-contexts: From the original dataset a set with 2543 examples was derived where each of the attributes was discretized using 5 intervals. The method used a genetic algorithm <ref> (Zupan, Halter & Bohanec to appear in 1997) </ref> to find the discretization intervals that minimize the classification error when new example set is used to classify the instances of original set. <p> Such a new example describes the class that is the majority class of the corresponding examples of the original set. The discretization method is further described in <ref> (Zupan, Halter & Bohanec to appear in 1997) </ref>. When used to classify the items in the dataset, the discretized example set was found to misclassify only 7 instances of the original dataset (0:23%).
Reference: <author> Zupan, B., Halter, J. A. & Bohanec, M. </author> <title> (to appear in 1997), Concept discovery by function decomposition and its application in neurophysiology, </title> <editor> in N. Lavrac, E. Keravnou & B. Zupan, eds, </editor> <booktitle> `Intelligent Data Analysis in Medicine and Pharmacology', </booktitle> <publisher> Kluwer. </publisher>
Reference-contexts: From the original dataset a set with 2543 examples was derived where each of the attributes was discretized using 5 intervals. The method used a genetic algorithm <ref> (Zupan, Halter & Bohanec to appear in 1997) </ref> to find the discretization intervals that minimize the classification error when new example set is used to classify the instances of original set. <p> Such a new example describes the class that is the majority class of the corresponding examples of the original set. The discretization method is further described in <ref> (Zupan, Halter & Bohanec to appear in 1997) </ref>. When used to classify the items in the dataset, the discretized example set was found to misclassify only 7 instances of the original dataset (0:23%).
References-found: 63

