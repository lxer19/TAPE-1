URL: http://www.cs.washington.edu/homes/soderlan/Thesis.ps
Refering-URL: http://www.cs.washington.edu/homes/soderlan/
Root-URL: http://www.cs.washington.edu
Title: LEARNING TEXT ANALYSIS RULES FOR DOMAIN-SPECIFIC NATURAL LANGUAGE PROCESSING  
Author: STEPHEN G. SODERLAND 
Degree: A Dissertation Presented by  Submitted to the Graduate School of the  in partial fulfillment of the requirements for the degree of DOCTOR OF PHILOSOPHY  
Date: February 1997  
Affiliation: University of Massachusetts Amherst  Department of Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [Aha et al. 1991] <author> Aha, D., Kilber, D., Albert, M. </author> <title> Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: It is the bottom up nature of CRYSTAL that allows it to focus on a small number of features at a time. This will also be true of other machine learning algorithms 6 that operate bottom up, such as instance based learning 6 given an appropriate implementation 47 <ref> [Aha et al. 1991, Cost and Salzberg 1993] </ref> and some covering algorithms that start from "seed" instances [Michalski 1983]. Algorithms that operate from the top down, on the other hand, typically include a step that considers features exhaustively 7 . <p> The class (the concept being learned) is already known and CRYSTAL applies the similarity metric only to instances of that class. Two IBL algorithms with different strategies for handling noisy training instances are David Aha's IB3 <ref> [Aha et al. 1991] </ref> and Scott Cost and Steven Salzberg's PEBLS [Cost and Salzberg 1993]. Each of these algorithms tabulates classification performance statistics on each exemplar to reduce the effect of noisy training instances.
Reference: [Amit et al. 1995] <author> Yamit, A., Geman, D., Wilder, K. </author> <title> Recognizing Shapes from Simple Queries about Geometry. </title> <type> Technical Report, </type> <institution> Department of Statistics, University of Chicago, Department of Mathematics and Statistics, University of Massachusetts, </institution> <year> 1995. </year>
Reference-contexts: The most similar initial concept definition is shown in Figure 7 An exception to this is a system by Yali Amit, Donald Geman, and Ken Wilder that uses decision trees for optical character recognition <ref> [Amit et al. 1995] </ref>. It handles an extremely large set of features by considering randomly selected subsets of the features. Multiple trees were built in this way and allowed to vote on the classification. 48 4.4.
Reference: [Breiman et al. 1984] <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Statistic/Probability Series, </publisher> <year> 1984. </year>
Reference-contexts: This increases the computational cost of selecting a test as each node. Many decision tree algorithms, including C4.5, base the feature selection metric on the information-theoretic measure, Shannon entropy, shown in Section 8.3.2. Another basis for a "goodness of split" metric is the Gini diversity index <ref> [Breiman et al. 1984] </ref>. Gini index = X p i p j Experiments by John Mingers suggest that the particular feature selection metric used may not be critical to performance [Mingers 1989], although the right choice of metrics can increase performance for a particular data set.
Reference: [Brill 1994] <author> Brill, E. </author> <title> Some Advances in Transformation-Based Part of Speech Tagging. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 722-727, </pages> <year> 1994. </year>
Reference-contexts: restrictions have been left as options for the CRYSTAL system, with the default being the full representation. 105 CHAPTER 7 RELATED WORK IN NATURAL LANGUAGE PROCESSING Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words <ref> [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] </ref> and in learning heuristics to guide probabilistic parsing [Charniak 1995, Magerman 1995]. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words.
Reference: [Cardie 1993] <author> Cardie, C. </author> <title> A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 798-803, </pages> <year> 1993. </year>
Reference-contexts: restrictions have been left as options for the CRYSTAL system, with the default being the full representation. 105 CHAPTER 7 RELATED WORK IN NATURAL LANGUAGE PROCESSING Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words <ref> [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] </ref> and in learning heuristics to guide probabilistic parsing [Charniak 1995, Magerman 1995]. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words.
Reference: [Charniak 1995] <author> Charniak, E. </author> <title> Parsing with Context-free Grammars and Word Statistics, </title> <type> Technical Report CS-95-28, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1995. </year>
Reference-contexts: representation. 105 CHAPTER 7 RELATED WORK IN NATURAL LANGUAGE PROCESSING Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] and in learning heuristics to guide probabilistic parsing <ref> [Charniak 1995, Magerman 1995] </ref>. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words. The work most closely related to CRYSTAL has come from participants in recent Message Understanding Conferences [MUC-4 1992, MUC-5 1993, MUC-6 1995].
Reference: [Church 1988] <author> Church, K. </author> <title> A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> 136-143, </pages> <year> 1988. </year>
Reference-contexts: restrictions have been left as options for the CRYSTAL system, with the default being the full representation. 105 CHAPTER 7 RELATED WORK IN NATURAL LANGUAGE PROCESSING Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words <ref> [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] </ref> and in learning heuristics to guide probabilistic parsing [Charniak 1995, Magerman 1995]. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words.
Reference: [Church et al. 1991] <author> Church, K., Gale, W., Hanks, P., Hindle, D. </author> <title> Using Statistics in Lexical Analysis. Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon. </title> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <pages> 115-164, </pages> <year> 1991. </year>
Reference-contexts: While this is not an insignificant investment of time, I feel that 150 I can call this a modest amount of training. The training corpus for Hospital Discharge consisted of less than 150,000 words. Statistical corpus-based techniques often require tens of millions of words of training <ref> [Church et al. 1991] </ref>. It is a combination of the limited domain and the use of semantic class information that allow CRYSTAL to work with such a comparatively small amount of training.
Reference: [Clark and Niblett 1989] <author> Clark, P. and Niblett, T. </author> <title> The CN2 Induction Algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283, </pages> <year> 1989. </year> <month> 191 </month>
Reference-contexts: Covering algorithms are discussed more fully in Section 8.3. Where CRYSTAL differs from other covering algorithms such as A q [Michalski 1983], CN2 <ref> [Clark and Niblett 1989] </ref>, and the candidate elimination algorithm [Mitchell 1978, Mitchell 1982] is in the method used to find generalized concept descriptions. To generalize a concept definition D, CRYSTAL relaxes the constraints by finding a similar initial definition D' and creating the unification U of D and D'. <p> Algorithms that operate from the top down, on the other hand, typically include a step that considers features exhaustively 7 . This is the case with top down decision tree induction [Quinlan 1993] or decision list induction [Pagallo and Haussler 1990], and some covering algorithms that operate top down <ref> [Clark and Niblett 1989] </ref>. Extremely large feature sets can have a serious impact on computation time for these algorithms. Mitchell's candidate elimination algorithm combines aspects of top down and bottom up processing [Mitchell 1978, Mitchell 1982]. <p> Each instances has 35 features with a total of 106 feature-value pairs [Michalski 1983]. Clark and Niblett report tests of CN2 on three medical data sets with eighteen, nine, and seventeen features and two artificial domains with twelve features <ref> [Clark and Niblett 1989] </ref>. Mitchell used mass spectroscopy data to test the candidate elimination algorithm. Each instance represents an molecule with four features for each of ten to twenty atoms in the molecule [Mitchell 1978]. <p> Peter Clark and Tim Niblett also offer a readable explanation of A q <ref> [Clark and Niblett 1989] </ref>. The basic methodology of A q has much in common with CRYSTAL. A q begins with a set of labeled training instances and builds a disjunctive set 126 of concept descriptions, which taken together cover all the positive instances and none of the negative. <p> The dependency on the star size s, makes A q resemble the beam search version of CRYSTAL. It seems that A q performs best with a fairly large star size. Clark and Niblett report using s = 15 for experiments with medical data sets 3 <ref> [Clark and Niblett 1989] </ref>. So long as A q is only tested on features sets with a few dozen attributes, it does not matter that computation time is proportional to a. <p> If t is treated as a constant, A q would take O (sn) computations to build a star using a representation similar to CRYSTAL's. This would give A q a time complexity that does not depend on a, similar to a beam search version of CRYSTAL. 8.3.2 CN2 CN2 <ref> [Clark and Niblett 1989] </ref>, developed by Peter Clark and Tim Niblett, combines aspects of A q with those of decision trees and decision lists.
Reference: [Cost and Salzberg 1993] <author> Cost, S. and Salzberg, S. </author> <title> A Weighted Nearest Neigh--bor Algorithm for Learning with Symbolic Features. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: It is the bottom up nature of CRYSTAL that allows it to focus on a small number of features at a time. This will also be true of other machine learning algorithms 6 that operate bottom up, such as instance based learning 6 given an appropriate implementation 47 <ref> [Aha et al. 1991, Cost and Salzberg 1993] </ref> and some covering algorithms that start from "seed" instances [Michalski 1983]. Algorithms that operate from the top down, on the other hand, typically include a step that considers features exhaustively 7 . <p> The class (the concept being learned) is already known and CRYSTAL applies the similarity metric only to instances of that class. Two IBL algorithms with different strategies for handling noisy training instances are David Aha's IB3 [Aha et al. 1991] and Scott Cost and Steven Salzberg's PEBLS <ref> [Cost and Salzberg 1993] </ref>. Each of these algorithms tabulates classification performance statistics on each exemplar to reduce the effect of noisy training instances. IB3 bases its classification of new instances only on instances that pass a statistical significance test as reliable classifiers.
Reference: [Fisher and Riloff 1992] <author> Fisher, D. and Riloff, E. </author> <title> Applying Statistical Methods to Small Corpora: Benefitting from a Limited Domain. </title> <booktitle> In Working Notes of 1992 AAAI Fall Symposium Series: Probabilistic Approaches to Natural Language, </booktitle> <year> 1992. </year>
Reference-contexts: David Fisher and Ellen Riloff have shown that statistically significant co-occurrence frequencies can be derived from a small corpus in a limited domain <ref> [Fisher and Riloff 1992] </ref>. The amount of training can be viewed as modest from another point of view. Developing a set of rules by hand also requires a set of annotated examples to guide development for all but the simplest of information extraction tasks.
Reference: [Fisher et al. 1995] <author> Fisher, D., Soderland, S., McCarthy, J., Feng, F., Lehn-ert, W. </author> <title> Description of the UMass System as Used for MUC-6. </title> <booktitle> In Proceedings of the Sixth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 221-236, </pages> <year> 1995. </year>
Reference-contexts: The present implementation of CRYSTAL, however, requires that the instances be presented as a flat list of syntactic constituents with no nested structure. Training instances for these experiments were created by the BADGER sentence analyzer of the University of Massachusetts <ref> [Fisher et al. 1995] </ref>. The Hospital Discharge domain used a version of BADGER that segments each simple clause into constituents such as SUBJ, VERB, OBJ, ADV, and PP (prepositional phrase).
Reference: [Huffman 1996] <author> Huffman, S. </author> <title> Learning Information Extraction Patterns from Examples. Connectionist, Statistical, and Symbolic approaches to Learning for Natural Language Processing. </title> <publisher> Springer, </publisher> <pages> 246-260, </pages> <year> 1996. </year>
Reference-contexts: The second system is PALKA [Kim and Moldovan 1992], developed by the University of Southern California for their MUC-5 system. A third trainable system appeared in MUC-6, the HASTEN system from SRA [Krupka 1995]. The fourth system described in this chapter is LIEP <ref> [Huffman 1996] </ref> that was developed on the MUC-6 domain. By the time of the MUC-6 conference, the University of Massachusetts had moved from AutoSlog to CRYSTAL. 7.1 AutoSlog The AutoSlog dictionary construction tool was developed by Ellen Riloff at the University of Massachusetts [Riloff 1993]. <p> and not word constraints on the sentence elements used as slot fillers. 112 The Egraph anchor had a constraint on the verb root, and the irrelevant sen-tence element had no constraints. 7.4 LIEP The last system to be discussed in this chapter is Scott Huffman's LIEP (Learning Information Extraction Patterns) <ref> [Huffman 1996] </ref>. LIEP uses a set of heuristics to create rules, called extraction patterns, from single training instances. There is also a mechanism to generalize extraction patterns slightly. LIEP learns patterns for multi-slot concept extraction, such as Management Succession events.
Reference: [Kim and Moldovan 1992] <author> Kim, J. and Moldovan, D. PALKA: </author> <title> A System for Linguistic Knowledge Acquisition. </title> <type> Technical Report PKPL 92-8, </type> <institution> USC Department of Electrical Engineering Systems, </institution> <year> 1992. </year>
Reference-contexts: The first is the AutoSlog dictionary construction tool [Riloff 1993] used by the University of Massachusetts in MUC-4 and MUC-5. AutoSlog combines machine learning with a "human in the loop" who edits the proposed rules. The second system is PALKA <ref> [Kim and Moldovan 1992] </ref>, developed by the University of Southern California for their MUC-5 system. A third trainable system appeared in MUC-6, the HASTEN system from SRA [Krupka 1995]. The fourth system described in this chapter is LIEP [Huffman 1996] that was developed on the MUC-6 domain. <p> The system presented in the next section uses a covering algorithm approach that resembles CRYSTAL more closely than AutoSlog does. 7.2 PALKA The PALKA system <ref> [Kim and Moldovan 1992] </ref> was developed by Jun-Tae Kim and Dan Moldovan for the University of Southern California MUC-5 system. PALKA (Parallel Automatic Linguistic Knowledge Acquisition) uses a method similar to the candidate elimination algorithm (See Section 8.3.4) to generate text analysis rules from training instances.
Reference: [Krupka 1995] <author> Krupka, G. </author> <title> Description of the SRA System as Used for MUC-6. </title> <booktitle> In Proceedings of the Sixth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 221-236, </pages> <year> 1995. </year>
Reference-contexts: AutoSlog combines machine learning with a "human in the loop" who edits the proposed rules. The second system is PALKA [Kim and Moldovan 1992], developed by the University of Southern California for their MUC-5 system. A third trainable system appeared in MUC-6, the HASTEN system from SRA <ref> [Krupka 1995] </ref>. The fourth system described in this chapter is LIEP [Huffman 1996] that was developed on the MUC-6 domain. <p> George Krupka developed HASTEN <ref> [Krupka 1995] </ref> for the SRA Corporation's MUC-6 text analysis system. HASTEN stores each training instances as an Egraph, which associates structural elements of the sentence with semantic classes and also with case frame slots of an extracted concept.
Reference: [Lehnert et al. 1983] <author> Lehnert, W., Dyer M., Johnson P., Yang C.J., Harley S. </author> <title> BORIS An Experiment in In-Depth Understanding of Narratives. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 15-62, </pages> <year> 1983. </year>
Reference-contexts: The person tells the computer to read the collected works of Shakespeare to gain more insight into human behavior, which of course the computer does in about sixty seconds, understanding all the nuances. Unfortunately, the amount of knowledge needed for in-depth understanding is overwhelming. The BORIS system <ref> [Lehnert et al. 1983] </ref>, developed by Michael Dyer and other researchers at Yale in the 1980's gives an indication of just how much knowledge is needed.
Reference: [Lindberg et al. 1993] <author> Lindberg, D., Humphreys, B., McCray, A. </author> <title> Unified Medical Language Systems. </title> <booktitle> Methods of Information in Medicine, </booktitle> <volume> 32(4), </volume> <pages> 281-291, </pages> <year> 1993. </year>
Reference-contexts: The Hospital Discharge domain requires a semantic hierarchy appropriate to medical texts. CRYSTAL uses a hierarchy of 133 semantic classes adapted from the Unified Medical Language Systems (UMLS) on-line medical thesaurus, developed by the National Library of Medicine <ref> [Lindberg et al. 1993] </ref>. In this hierarchy a &lt;Finding&gt; has two subclasses, &lt;Sign or Symptom&gt; and &lt;Laboratory or Test Result&gt;. A &lt;Finding&gt; is a &lt;Conceptual Entity&gt;. The semantic class &lt;Disease or Syndrome&gt; is a &lt;Pathologic Function&gt;, which is a descendant of the class &lt;Event&gt;. Appendix D shows the entire hierarchy. <p> Results with fine-tuned semantic tagging are presented later in this section. 4 These results are listed in a table in Appendix B. 5 The Unified Medical Language System thesaurus of the National Library of Medicine <ref> [Lindberg et al. 1993] </ref>. 65 10 60 40 20 80 100 Recall Precision 393 1192 2741 Symptom, Present 444 1312 3025 Symptom, Absent 208 625 1426 Diagnosis, Confirmed 43 134 310 Diagnosis, Ruled Out As in the Management Succession domain, recall increases with further training, while precision stays fairly flat 6 <p> One source of noise that I was partially able to control was the coarse fit between the semantic tagging of individual words and the information extraction task. A generic medical thesaurus, the Unified Medical Language System (UMLS) <ref> [Lindberg et al. 1993] </ref> had been used with only minor customization. Unfortunately, class assignment based on UMLS did not correlate closely with annotations of the target concepts. In particular, the class &lt;Sign or Symptom&gt; was a poor predictor of the concept Symptom,Present.
Reference: [Magerman 1995] <author> Magerman, D. </author> <title> Statistical Decision-Tree Models for Parsing. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the ACL, </booktitle> <year> 1995. </year>
Reference-contexts: representation. 105 CHAPTER 7 RELATED WORK IN NATURAL LANGUAGE PROCESSING Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] and in learning heuristics to guide probabilistic parsing <ref> [Charniak 1995, Magerman 1995] </ref>. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words. The work most closely related to CRYSTAL has come from participants in recent Message Understanding Conferences [MUC-4 1992, MUC-5 1993, MUC-6 1995].
Reference: [Michalski 1983] <author> Michalski, R. S. and Chilausky, R. L. </author> <title> Learning by Being Told and Learning from Examples. </title> <journal> Policy Analysis and Information Systems, </journal> <pages> 219-244, </pages> <year> 1980. </year>
Reference-contexts: Covering algorithms are discussed more fully in Section 8.3. Where CRYSTAL differs from other covering algorithms such as A q <ref> [Michalski 1983] </ref>, CN2 [Clark and Niblett 1989], and the candidate elimination algorithm [Mitchell 1978, Mitchell 1982] is in the method used to find generalized concept descriptions. <p> This will also be true of other machine learning algorithms 6 that operate bottom up, such as instance based learning 6 given an appropriate implementation 47 [Aha et al. 1991, Cost and Salzberg 1993] and some covering algorithms that start from "seed" instances <ref> [Michalski 1983] </ref>. Algorithms that operate from the top down, on the other hand, typically include a step that considers features exhaustively 7 . <p> A data set on diagnosing soybean diseases was used by Michalski to test the A q algorithm. Each instances has 35 features with a total of 106 feature-value pairs <ref> [Michalski 1983] </ref>. Clark and Niblett report tests of CN2 on three medical data sets with eighteen, nine, and seventeen features and two artificial domains with twelve features [Clark and Niblett 1989]. Mitchell used mass spectroscopy data to test the candidate elimination algorithm. <p> The covering algorithms described here are Michalski's A q algorithm, Clark and Niblett's CN2. I include Vere's concept induction algorithm and Mitchell's candidate elimination algorithm in this section as well. 8.3.1 A q Ryszard Michalski and his students have published several versions of the A q covering algorithm <ref> [Michalski 1983] </ref> and the INDUCE inductive learning program. Peter Clark and Tim Niblett also offer a readable explanation of A q [Clark and Niblett 1989]. The basic methodology of A q has much in common with CRYSTAL.
Reference: [Michalski 1983] <author> Michalski, R. S. </author> <title> A Theory and Methodology of Inductive Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 111-161, </pages> <year> 1983. </year> <month> 192 </month>
Reference-contexts: Covering algorithms are discussed more fully in Section 8.3. Where CRYSTAL differs from other covering algorithms such as A q <ref> [Michalski 1983] </ref>, CN2 [Clark and Niblett 1989], and the candidate elimination algorithm [Mitchell 1978, Mitchell 1982] is in the method used to find generalized concept descriptions. <p> This will also be true of other machine learning algorithms 6 that operate bottom up, such as instance based learning 6 given an appropriate implementation 47 [Aha et al. 1991, Cost and Salzberg 1993] and some covering algorithms that start from "seed" instances <ref> [Michalski 1983] </ref>. Algorithms that operate from the top down, on the other hand, typically include a step that considers features exhaustively 7 . <p> A data set on diagnosing soybean diseases was used by Michalski to test the A q algorithm. Each instances has 35 features with a total of 106 feature-value pairs <ref> [Michalski 1983] </ref>. Clark and Niblett report tests of CN2 on three medical data sets with eighteen, nine, and seventeen features and two artificial domains with twelve features [Clark and Niblett 1989]. Mitchell used mass spectroscopy data to test the candidate elimination algorithm. <p> The covering algorithms described here are Michalski's A q algorithm, Clark and Niblett's CN2. I include Vere's concept induction algorithm and Mitchell's candidate elimination algorithm in this section as well. 8.3.1 A q Ryszard Michalski and his students have published several versions of the A q covering algorithm <ref> [Michalski 1983] </ref> and the INDUCE inductive learning program. Peter Clark and Tim Niblett also offer a readable explanation of A q [Clark and Niblett 1989]. The basic methodology of A q has much in common with CRYSTAL.
Reference: [Miller et al. 1990] <author> Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D. and Miller, K. J. </author> <title> Introduction to WordNet: An On-line Lexical Data Base. </title> <journal> In the Journal of Lexicography, </journal> <volume> vol 3, no. 4, </volume> <pages> pp 235-244, </pages> <year> 1990. </year>
Reference-contexts: Any pattern that includes a word in a synonym list has the word constraint generalized to a constraint on the synonym list. This will not help LIEP generalize to words not found in training. Huffman suggested use of a knowledge source such as WordNet <ref> [Miller et al. 1990] </ref> to replace or augment the learned synonym lists. 1 The "F-measure" used in MUC evaluations to combine recall and precision 114 There does not seem to be a mechanism to discard unreliable extraction patterns.
Reference: [Mingers 1989] <author> Mingers, J. </author> <title> An Empirical Comparison of Selection Measures for Decision-Tree Induction. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 319-342, </pages> <year> 1989. </year>
Reference-contexts: Another basis for a "goodness of split" metric is the Gini diversity index [Breiman et al. 1984]. Gini index = X p i p j Experiments by John Mingers suggest that the particular feature selection metric used may not be critical to performance <ref> [Mingers 1989] </ref>, although the right choice of metrics can increase performance for a particular data set. With its recursive partitioning, top down decision tree induction tends to fragment the instance space.
Reference: [Mitchell 1978] <author> Mitchell, T. </author> <title> Version Spaces: an Approach to Concept Learning. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <year> 1978. </year>
Reference-contexts: Covering algorithms are discussed more fully in Section 8.3. Where CRYSTAL differs from other covering algorithms such as A q [Michalski 1983], CN2 [Clark and Niblett 1989], and the candidate elimination algorithm <ref> [Mitchell 1978, Mitchell 1982] </ref> is in the method used to find generalized concept descriptions. To generalize a concept definition D, CRYSTAL relaxes the constraints by finding a similar initial definition D' and creating the unification U of D and D'. <p> Extremely large feature sets can have a serious impact on computation time for these algorithms. Mitchell's candidate elimination algorithm combines aspects of top down and bottom up processing <ref> [Mitchell 1978, Mitchell 1982] </ref>. <p> Mitchell used mass spectroscopy data to test the candidate elimination algorithm. Each instance represents an molecule with four features for each of ten to twenty atoms in the molecule <ref> [Mitchell 1978] </ref>. Pagallo and Haussler tested GREEDY3 on artificial data sets with instances created by randomly selecting from among sixteen Boolean features [Pagallo and Haussler 1990] in some tests and from eighty in others. <p> Each of these is further generalized by unifying with additional positive instances. Vere gives no time complexity analysis, but acknowledges in the 1980 paper that his approach is only "potentially analytically tractable". 8.3.4 The Candidate Elimination Algorithm Tom Mitchell's candidate elimination algorithm <ref> [Mitchell 1978, Mitchell 1982] </ref> also identifies all concept descriptions that are consistent with the training data. The candidate elimination algorithm takes advantage of a partial ordering of concept descriptions to avoid enumerating the consistent concept definitions.
Reference: [Mitchell 1982] <author> Mitchell, T. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18, </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: Covering algorithms are discussed more fully in Section 8.3. Where CRYSTAL differs from other covering algorithms such as A q [Michalski 1983], CN2 [Clark and Niblett 1989], and the candidate elimination algorithm <ref> [Mitchell 1978, Mitchell 1982] </ref> is in the method used to find generalized concept descriptions. To generalize a concept definition D, CRYSTAL relaxes the constraints by finding a similar initial definition D' and creating the unification U of D and D'. <p> Extremely large feature sets can have a serious impact on computation time for these algorithms. Mitchell's candidate elimination algorithm combines aspects of top down and bottom up processing <ref> [Mitchell 1978, Mitchell 1982] </ref>. <p> Each of these is further generalized by unifying with additional positive instances. Vere gives no time complexity analysis, but acknowledges in the 1980 paper that his approach is only "potentially analytically tractable". 8.3.4 The Candidate Elimination Algorithm Tom Mitchell's candidate elimination algorithm <ref> [Mitchell 1978, Mitchell 1982] </ref> also identifies all concept descriptions that are consistent with the training data. The candidate elimination algorithm takes advantage of a partial ordering of concept descriptions to avoid enumerating the consistent concept definitions.
Reference: [MUC-3 1991] <editor> Proceedings of the Third Message Understanding Conference, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1991. </year>
Reference-contexts: One domain was a collection of news articles about Latin American terrorism, in which the target concepts to be extracted are terrorist events and the perpetrators, victims, physical targets, and weapons associate with those events. This domain was from the ARPA-sponsored Third and Fourth Message Understanding Conferences <ref> [MUC-3 1991, MUC-4 1992] </ref>. The MUC-5 conference [MUC-5 1993] used two domains. One was a Joint Ventures domain whose target concept involves companies forming joint business ventures. The other was a Microelectronics domain whose target concepts are the roles companies play in microchip fabrication technology. <p> The conference organizer, Beth Sundheim, estimated that the upper limit on human performance for the MUC-4 task was 75% recall and 85% precision [Sundheim 1992]. Not surprisingly, automated systems have somewhat lower performance. The best automated systems in MUC conferences have had recall and precision between 50% and 60% <ref> [MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref>. Despite different domains and some differences in scoring metrics, there seems to be a ceiling of about 60% recall and precision for current information processing technology.
Reference: [MUC-4 1992] <editor> Proceedings of the Fourth Message Understanding Conference, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: One domain was a collection of news articles about Latin American terrorism, in which the target concepts to be extracted are terrorist events and the perpetrators, victims, physical targets, and weapons associate with those events. This domain was from the ARPA-sponsored Third and Fourth Message Understanding Conferences <ref> [MUC-3 1991, MUC-4 1992] </ref>. The MUC-5 conference [MUC-5 1993] used two domains. One was a Joint Ventures domain whose target concept involves companies forming joint business ventures. The other was a Microelectronics domain whose target concepts are the roles companies play in microchip fabrication technology. <p> The conference organizer, Beth Sundheim, estimated that the upper limit on human performance for the MUC-4 task was 75% recall and 85% precision [Sundheim 1992]. Not surprisingly, automated systems have somewhat lower performance. The best automated systems in MUC conferences have had recall and precision between 50% and 60% <ref> [MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref>. Despite different domains and some differences in scoring metrics, there seems to be a ceiling of about 60% recall and precision for current information processing technology. <p> Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words. The work most closely related to CRYSTAL has come from participants in recent Message Understanding Conferences <ref> [MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref>. Nearly all MUC participants use some form of pattern matching rules or finite state automata to identify references to concepts of interest. Although most MUC participants build these rules by hand, the methodology looks uncannily like a hand-simulation of CRYSTAL.
Reference: [MUC-5 1993] <editor> Proceedings of the Fifth Message Understanding Conference, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: This domain was from the ARPA-sponsored Third and Fourth Message Understanding Conferences [MUC-3 1991, MUC-4 1992]. The MUC-5 conference <ref> [MUC-5 1993] </ref> used two domains. One was a Joint Ventures domain whose target concept involves companies forming joint business ventures. The other was a Microelectronics domain whose target concepts are the roles companies play in microchip fabrication technology. <p> An experiment [Will 1993] was conducted that measured the amount of inter-coder disagreement between four human analysts. These were professional analysts, each with years of experience at manual information extraction, who annotated the collection of training texts for one of the Fifth Message Understanding Conference domains <ref> [MUC-5 1993] </ref>. This was a domain of news stories about companies involved in microelectronic chip manufacturing. The experiment had pairs of analysts read the same texts and independently create output representing the relevant information. One analyst's output was evaluated by treating another analyst's output as the answer key. <p> The conference organizer, Beth Sundheim, estimated that the upper limit on human performance for the MUC-4 task was 75% recall and 85% precision [Sundheim 1992]. Not surprisingly, automated systems have somewhat lower performance. The best automated systems in MUC conferences have had recall and precision between 50% and 60% <ref> [MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref>. Despite different domains and some differences in scoring metrics, there seems to be a ceiling of about 60% recall and precision for current information processing technology. <p> Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words. The work most closely related to CRYSTAL has come from participants in recent Message Understanding Conferences <ref> [MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref>. Nearly all MUC participants use some form of pattern matching rules or finite state automata to identify references to concepts of interest. Although most MUC participants build these rules by hand, the methodology looks uncannily like a hand-simulation of CRYSTAL.
Reference: [MUC-6 1995] <editor> Proceedings of the Sixth Message Understanding Conference, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: To illustrate how an IE system works, let us consider the Management Succession domain, which was used in the MUC-6 performance evaluation <ref> [MUC-6 1995] </ref>. The task for this domain is to analyze news articles and identify persons moving into top corporate management positions and persons moving out of those positions. The only information considered relevant is the persons, positions, and corporations that are directly involved in a management succession event. <p> The conference organizer, Beth Sundheim, estimated that the upper limit on human performance for the MUC-4 task was 75% recall and 85% precision [Sundheim 1992]. Not surprisingly, automated systems have somewhat lower performance. The best automated systems in MUC conferences have had recall and precision between 50% and 60% <ref> [MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref>. Despite different domains and some differences in scoring metrics, there seems to be a ceiling of about 60% recall and precision for current information processing technology. <p> Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words. The work most closely related to CRYSTAL has come from participants in recent Message Understanding Conferences <ref> [MUC-4 1992, MUC-5 1993, MUC-6 1995] </ref>. Nearly all MUC participants use some form of pattern matching rules or finite state automata to identify references to concepts of interest. Although most MUC participants build these rules by hand, the methodology looks uncannily like a hand-simulation of CRYSTAL.
Reference: [Murthy et al. 1994] <author> Murthy, S.K., Kasif, S., and Salzberg, S. </author> <title> A System for Induction of Oblique Decision Trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 1-32, </pages> <year> 1994. </year>
Reference-contexts: The key step in top down induction of a decision tree is to select a feature at each node whose values best separate different classes into different partitions. Some decision tree algorithms such as OC1 <ref> [Murthy et al. 1994] </ref> also allow the test at a node to be a linear combination of features. This increases the computational cost of selecting a test as each node.
Reference: [Pagallo and Haussler 1990] <author> Pagallo, G. and Haussler, D. </author> <title> Boolean Feature Discovery in Empirical Learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: Algorithms that operate from the top down, on the other hand, typically include a step that considers features exhaustively 7 . This is the case with top down decision tree induction [Quinlan 1993] or decision list induction <ref> [Pagallo and Haussler 1990] </ref>, and some covering algorithms that operate top down [Clark and Niblett 1989]. Extremely large feature sets can have a serious impact on computation time for these algorithms. Mitchell's candidate elimination algorithm combines aspects of top down and bottom up processing [Mitchell 1978, Mitchell 1982]. <p> Each instance represents an molecule with four features for each of ten to twenty atoms in the molecule [Mitchell 1978]. Pagallo and Haussler tested GREEDY3 on artificial data sets with instances created by randomly selecting from among sixteen Boolean features <ref> [Pagallo and Haussler 1990] </ref> in some tests and from eighty in others. I have not seen any machine learning papers that describe a data set with more than several dozen features. <p> I will use Ross Quinlan's C4.5 [Quinlan 1993] as a representative of decision tree algorithms and use Giulia Pagallo and David Haussler's GREEDY3 algorithm <ref> [Pagallo and Haussler 1990] </ref> to represent decision lists, which are a special case of decision trees. 8.5.1 C4.5 A decision tree algorithm such as C4.5 begins with an empty tree and recursively adds tests at each tree node to partition the instance space. <p> Decision lists are able to reach a leaf node with every test by using a multivariate test, one that evaluates multiple features. This makes the decision list equivalent to a list of rules, each rule having several constraints. The GREEDY3 algorithm <ref> [Pagallo and Haussler 1990] </ref> uses a top down approach to building the test at each node of a decision list. GREEDY3 adds one feature at a time to the test, selecting the feature that has the highest probability of being found in a positive instance.
Reference: [Quinlan 1986] <author> Quinlan, J.R. </author> <title> Induction of Decision Trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Figure 8.3 shows the features occurring in a training instance with "Chest x-ray" in the subject. SUBJ-Terms-CHEST SUBJ-Terms-X-RAY SUBJ-Mod Terms-CHEST SUBJ-Head Terms-X-RAY SUBJ-Classes-Body Location SUBJ-Classes-Diagnostic Procedure SUBJ-Mod Classes-Body Location SUBJ-Head Classes-Diagnostic Procedure SUBJ-Mode-affirmative 1 I used a slightly re-implemented ID3 decision tree algorithm <ref> [Quinlan 1986] </ref> 123 These features are listed in a format that indicates the syntactic constituent (SUBJ), followed by the constraint (e.g. Terms), followed by the word or semantic class. The feature SUBJ-Terms-CHEST has the value "true" for this instance as does the feature SUBJ-Terms-X-RAY.
Reference: [Quinlan 1993] <author> Quinlan, J.R. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Algorithms that operate from the top down, on the other hand, typically include a step that considers features exhaustively 7 . This is the case with top down decision tree induction <ref> [Quinlan 1993] </ref> or decision list induction [Pagallo and Haussler 1990], and some covering algorithms that operate top down [Clark and Niblett 1989]. Extremely large feature sets can have a serious impact on computation time for these algorithms. <p> IB3 discards much of 137 the training set, but PEBLS retains all training instances, which can become expensive in terms of memory. 8.5 Decision Tree Algorithms A well known family of machine learning algorithms is top down induction of decision trees. I will use Ross Quinlan's C4.5 <ref> [Quinlan 1993] </ref> as a representative of decision tree algorithms and use Giulia Pagallo and David Haussler's GREEDY3 algorithm [Pagallo and Haussler 1990] to represent decision lists, which are a special case of decision trees. 8.5.1 C4.5 A decision tree algorithm such as C4.5 begins with an empty tree and recursively adds
Reference: [Quinlan and Cameron-Jones 1995] <author> Quinlan, J.R. and Cameron-Jones, </author> <title> R.M. Oversearching and Layered Search in Emperical Learning. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1019-1024, </pages> <year> 1995. </year> <month> 193 </month>
Reference: [Riloff 1993] <author> Riloff, E. </author> <title> Automatically Constructing a Dictionary for Informa--tion Extraction Tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 811-816, </pages> <year> 1993. </year>
Reference-contexts: Three of the MUC participants have developed systems that learn text analysis rules. The remainder of this section will compare CRYSTAL with these other systems, plus another system based on a MUC domain. The first is the AutoSlog dictionary construction tool <ref> [Riloff 1993] </ref> used by the University of Massachusetts in MUC-4 and MUC-5. AutoSlog combines machine learning with a "human in the loop" who edits the proposed rules. The second system is PALKA [Kim and Moldovan 1992], developed by the University of Southern California for their MUC-5 system. <p> By the time of the MUC-6 conference, the University of Massachusetts had moved from AutoSlog to CRYSTAL. 7.1 AutoSlog The AutoSlog dictionary construction tool was developed by Ellen Riloff at the University of Massachusetts <ref> [Riloff 1993] </ref>. AutoSlog passes through the training texts a single time and proposes concept definitions from instances of the concepts to be extracted. AutoSlog uses "one shot" learning with no generalization phase and no testing of proposed rules on the training data.
Reference: [Riloff 1996] <author> Riloff, E. </author> <title> Automatically Generating Extraction Patterns from Untagged Text. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 1044-1049, </pages> <year> 1996. </year>
Reference-contexts: Instead, it uses heuristics to craft the best concept definition it can from a single motivating example. An 107 AutoSlog concept definition assigns a fixed level of semantic constraint to the extracted phrase. A more recent version of AutoSlog <ref> [Riloff 1996] </ref> has no semantic constraints at all on the extracted phrase. This version assumes that later processing in an information extraction system will filter out extraction errors by overgeneralized AutoSlog concept definitions.
Reference: [Rivest 1987] <author> Rivest, R. </author> <title> Learning Decision Lists. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1987. </year>
Reference-contexts: This is repeated until all instances are covered. Unlike A q and CRYSTAL, which build an unordered set of concept descriptions, CN2 builds a decision 4 or ancestors of classes found in the instance, according to a semantic hierarchy 129 list <ref> [Rivest 1987] </ref> of rules that are applied in order. This handles exceptions naturally. An earlier rule can remove instances that would otherwise be errors to a later rule. Like A q , CN2 builds concept descriptions in a top down fashion, successively adding attributes to specialize a description. <p> C4.5 represents training instances as a feature vector with length O (a). All n instances must be repeatedly consulted while building a decision tree, which gives a space requirement of O (an). 8.5.2 GREEDY3 A variant of decision trees is the decision list, introduced by Ronald Rivest <ref> [Rivest 1987] </ref>. Each node of a decision list has a test that splits the instance space into exactly two partitions, at least one of which must be a leaf node. Thus each node separates off a set of training instances from the remaining instances.
Reference: [Soderland and Lehnert 1994] <author> Soderland, S. and Lehnert, W. Wrap-Up: </author> <title> a Trainable Discourse Module for Information Extraction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 131-158, </pages> <year> 1994. </year>
Reference-contexts: I had run into the problem of extremely large number of features in an earlier system called WRAP-UP <ref> [Soderland and Lehnert 1994] </ref> that learns to make inferences during the discourse processing stage of information extraction. Features are created that represent particular words in particular syntactic roles, such as being the subject of a particular verb, the object of a particular verb, or the object of a particular preposition.
Reference: [Soderland et al. 1995] <author> Soderland, S., Fisher, D., Aseltine, J., Lehnert, W. </author> <title> CRYSTAL: Inducing a Conceptual Dictionary. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1314-1321, </pages> <year> 1995. </year>
Reference-contexts: A considerable amount of knowledge must be acquired for an automated text analysis system, knowledge that is highly specific to each domain. Different domains have different information needs, and also involve considerable differences in vocabulary and writing style. This thesis presents CRYSTAL <ref> [Soderland et al. 1995] </ref>, a system that automatically learns domain-specific rules for information extraction.
Reference: [Sundheim 1992] <author> Sundheim, B. </author> <title> Overview of the Fourth Message Understanding Evaluation and Conference. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 3-21, </pages> <year> 1992. </year>
Reference-contexts: These results are consistent with human performance in previous Message Understanding Conferences. The conference organizer, Beth Sundheim, estimated that the upper limit on human performance for the MUC-4 task was 75% recall and 85% precision <ref> [Sundheim 1992] </ref>. Not surprisingly, automated systems have somewhat lower performance. The best automated systems in MUC conferences have had recall and precision between 50% and 60% [MUC-3 1991, MUC-4 1992, MUC-5 1993, MUC-6 1995].
Reference: [Vere 1975] <author> Vere, S. </author> <title> Induction of Concepts in the Predicate Calculus. </title> <booktitle> In Proceedings of the Fourth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 281-287, </pages> <year> 1975. </year>
Reference-contexts: The entire decision list requires O (asrn) computations, where r is the number of rules. The number of rules is bound by n, giving a worst-case time complexity of O (asn 2 ). 8.3.3 Vere's Induction Algorithm Steven Vere <ref> [Vere 1975] </ref> created a concept induction algorithms based on a predicate calculus representation. His main emphasis was on creating a provably correct algorithm that is guaranteed to find all concept descriptions consistent with the training data.
Reference: [Vere 1980] <author> Vere, S. </author> <title> Multilevel Counterfactuals for Generalizations of Relational Concepts and Productions. </title> <journal> Artificial Intelligence, </journal> <volume> 14, </volume> <pages> 139-164, </pages> <year> 1980. </year>
Reference-contexts: The basic step of Vere's algorithm unifies pairs of positive instances, finding the set of maximal, consistent unifying generalizations (mcg's). These are the most specific concept description that covers both instances and do not cover any negative instances. A later extension to the algorithm <ref> [Vere 1980] </ref> allows an mcg to include a list of exceptions, called counterfactuals that in turn may have counterfactuals. The unification is actually performed on a pair of product graphs that represent instances or mcg's. The resulting subgraph isomophism problem is more efficient than direct manipulation in predicate calculus.
Reference: [Will 1993] <author> Will, C. </author> <title> Comparing Human and Machine Performance for Natural Language Information Extraction: Results for English Microelectronics from the MUC-5 Evaluation. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 53-68, </pages> <year> 1993. </year>
Reference-contexts: Any differences from the answer key are counted as errors. Unfortunately, it is not always clear exactly what the "correct" output should be for a text. Texts often contain ambiguous references that are open to a variety of interpretations. An experiment <ref> [Will 1993] </ref> was conducted that measured the amount of inter-coder disagreement between four human analysts. These were professional analysts, each with years of experience at manual information extraction, who annotated the collection of training texts for one of the Fifth Message Understanding Conference domains [MUC-5 1993].
Reference: [Yarowsky 1992] <author> Yarowsky, D. </author> <title> Word Sense Disambiguation Using Statistical Models of Roget's Categories Trained on Large Corpora. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Computational Linguistics, </booktitle> <pages> 454-460, </pages> <year> 1992. </year> <month> 194 </month>
Reference-contexts: restrictions have been left as options for the CRYSTAL system, with the default being the full representation. 105 CHAPTER 7 RELATED WORK IN NATURAL LANGUAGE PROCESSING Most research in applying machine learning to natural language processing has been primarily at the level of lexical or semantic disambiguation of individual words <ref> [Brill 1994, Cardie 1993, Yarowsky 1992, Church 1988] </ref> and in learning heuristics to guide probabilistic parsing [Charniak 1995, Magerman 1995]. Little work has been done, however, in using corpus-based techniques for a higher level of inferencing that goes beyond the meaning of individual words.
References-found: 43

