URL: http://www.cs.umn.edu/Research/Agassiz/Paper/cho.tr98-020.ps.gz
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: E-mail: sycho@cs.umn.edu  
Title: Decoupling Local Variable Accesses in a Wide-Issue Superscalar Processor  
Author: Sangyeun Cho, Pen-Chung Yew, and Gyungho Lee 
Keyword: cache bandwidth, instruction-level parallelism, run-time stack, data stream partitioning, processor pipeline  
Address: Minneapolis, MN 55455 San Antonio, TX 78249  
Affiliation: Dept. of Comp. Sci. and Eng. Division of Engineering University of Minnesota University of Texas at San Antonio  
Abstract: Technical Report #98-20, May 1998 (Revised in Oct. 1998), Dept. of Computer Sci. and Eng., Univ. of Minnesota. Abstract Providing adequate data bandwidth is extremely important for a wide-issue superscalar processor to achieve its full performance potential. Adding a large number of ports to a data cache, however, becomes increasingly inefficient and can add to the hardware complexity significantly. This paper takes an alternative or complementary approach for providing more data bandwidth, called the data-decoupled architecture. The approach, with support from the compiler and hardware, partitions the memory stream into multiple independent streams early in the processor pipeline, and feeds each stream to a separate memory access queue and cache. Under this model, the paper studies the potential of decoupling memory accesses to program's local variables that are allocated on the run-time stack. Using a set of integer and floating-point programs from the SPEC 95 benchmark suite, it is shown that local variable accesses constitute a large portion of all the memory references, while their reference space is very small, averaging around 7 words per procedure. To service local variable accesses quickly, two optimizations, fast data forwarding and access combining, are proposed and studied. Some of the important design parameters, such as the cache size, the number of cache ports, and the degree of access combining, are studied based on simulations. The potential performance of the proposed scheme is measured using various configurations, and it is concluded that the scheme can become a viable alternative to building a single multi-ported data cache. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Bergner, P. Dahl, D. Engebretsen, and M. O'Keefe. </author> <title> Spill Code Minimization via Interference Region Spilling, </title> <booktitle> Proc. of the 1997 ACM SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 287 295. </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Memory accesses to these variables, usually indexed by stack pointer ($sp), can constitute a large fraction of overall memory references [9]. For example, spill codes can produce a significant number of memory references at run time, as many as 20% of all the executed instructions <ref> [1] </ref>. Even though spill codes could be eliminated or reduced by increasing the number of (architected) registers and/or by using a more sophisticated register allocation scheme, such attempts are restricted by the current technological trends; Aggressive ILP optimizations often increase register pressure and could introduce extra spill codes. <p> Chow and Hennessy [5] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 5, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [2] <author> P. Briggs, K. D. Cooper, K. Kennedy, and L. Torczon. </author> <title> Coloring Heuristics for Register Allocation, </title> <booktitle> Proc. of the 1989 ACM SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 275 284. </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Chow and Hennessy [5] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 5, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [3] <author> D. Burger and T. M. Austin. </author> <title> The SimpleScalar Tool Set, </title> <note> Version 2.0, Computer Sciences Department Technical Report, No. 1342, </note> <institution> Univ. of Wisconsin, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: cannot benefit from the unrolling optimization or software pipelining due to high register pressure, can use such program optimizations based on a new cost model. 3 Experimental Setup 3.1 Simulator and machine model We develop and use a cycle-accurate execution-driven simulator derived from the sim-outorder simulator in the SimpleScalar toolset <ref> [3] </ref>. The machine model used in the experiments is a superscalar processor that supports out-of-order issue and execution, based on the Register Update Unit (RUU) [22]. The RUU scheme uses a reorder buffer (ROB) to automatically perform register renaming and hold the results of pending instructions.
Reference: [4] <author> G. J. Chaitin. </author> <title> Register Allocation and Spilling via Graph Coloring, </title> <booktitle> Proc. of the 1982 ACM SIGPLAN Symp. on Compiler Construction, </booktitle> <pages> pp. 98 105, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: Chow and Hennessy [5] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 5, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [5] <author> F. C. Chow and J. L. Hennessy. </author> <title> The Priority-Based Coloring Approach to Register Allocation, </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 12:4, </volume> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: Among current microprocessors, Sun UltraSparc employs a special register file structure called register window to reduce the cost of a procedure call/return [27]. Chow and Hennessy <ref> [5] </ref> categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics [4, 2, 5, 1] try to efficiently assign a set of hard registers to the live ranges. <p> Chow and Hennessy [5] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 5, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [6] <author> D. Ditzel and R. McLellan. </author> <title> Register Allocation for Free: The C Machine Stack Cache, </title> <booktitle> Proc. of the Symp. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 48 56, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: To service the local variable accesses efficiently, we study two optimizations fast data forwarding and access combining. Although there have been efforts to optimize the local variable accesses with hardware support in the past <ref> [6, 10, 24] </ref>, little work has been done to study their performance impact in the context of a wide-issue superscalar processor with a multi-ported data cache. <p> The cache size to capture ten outstanding function frames is only 70 words on average. In fact, this has been the motivation for some previous work <ref> [6, 10, 24, 27] </ref>. The high frequency of local variable accesses and their spatial locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Moreover, identifying local variables in the stack frames is relatively easy for a compiler. 2.2.2 Architectural support To facilitate local variable accesses, a specialized hardware organization to simulate the run-time stack may appear attractive <ref> [6, 10, 24] </ref>. However, we use a more general cache design called the outlined in Section 3. 6 variable cache (LVC). B1, B2, and B3 are the bandwidth for each interface. local variable cache (LVC) in the framework of our data-decoupled architecture (see Figure 4). <p> Alternatively, the processor can assume those accesses indexed by $sp (or the frame pointer, $fp) as local variable accesses <ref> [6] </ref>. There are, however, local variable accesses not indexed by $sp (or $fp); For example, when the address of a local variable is taken, either to index through the data structure, or pass it as a parameter to a procedure, $sp is not used. <p> It is worth noting that the recent 21264 processor [11] has incorporated a 64 KB L1 data cache, eight times larger than that of its predecessor [7]. 5 Related Work The idea of optimizing local variables on run-time stack is not new. Ditzel and McLellan <ref> [6] </ref> studied a transparent data buffer as a close mapping of the run-time stack, called the stack cache. The stack cache is effectively a large register file to simulate the run-time stack that replaces the general register file.
Reference: [7] <author> J. Edmondson et al. </author> <title> Internal Organization of the Alpha 21164, a 300-MHz, 64-Bit, Quad-Issue, CMOS RISC Microprocessor, </title> <journal> Digital Technical Journal, </journal> <volume> Volume 7, Number 1, </volume> <year> 1995. </year>
Reference-contexts: Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. For example, DEC 21264 provides a two-ported data cache by running the cache twice as fast as the processor clock [11], DEC 21164, the predecessor of 21264, uses a replicated data cache <ref> [7] </ref>, and the MIPS R10000 implements a two-way interleaved data cache [31]. Each design, however, is either costly to implement, and/or has significant drawbacks. The time-division multiplexing does not scale beyond a certain number of ports (seemingly two ports). <p> The results under 99% percentile are shown to ease reading. Ditzel and McLellan <ref> [7] </ref> report similar results. of loads and 48% of stores in the programs studied. Over 60% of loads and 80% of stores are local variable accesses in 147.vortex. Local variable accesses occupy from 10% (129.compress) to 71% (147.vortex) among the memory references, with an average of 36%. <p> It is worth noting that the recent 21264 processor [11] has incorporated a 64 KB L1 data cache, eight times larger than that of its predecessor <ref> [7] </ref>. 5 Related Work The idea of optimizing local variables on run-time stack is not new. Ditzel and McLellan [6] studied a transparent data buffer as a close mapping of the run-time stack, called the stack cache.
Reference: [8] <author> EGCS Project. </author> <note> http://egcs.cygnus.com. </note>
Reference-contexts: Decode and commit widths are the same as the issue width. 10 3.2 Benchmark programs We use eight integer and four floating-point programs from the SPEC 95 benchmark suite [25], whose characteristics are summarized in Table 2. All the programs were compiled using EGCS 7 version 1.1b <ref> [8] </ref> at the -O3 optimization level with loop unrolling. Either train or test input is used in most cases, with some data set modification to control the simulation time.
Reference: [9] <author> J. Emer and D. Clark. </author> <title> A Characterization of Processor Performance in the VAX-11/780, </title> <booktitle> Proc. of the 11th Int'l Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: Although not visible to a programmer, additional local variables are generated by a compiler for saving/restoring registers, passing arguments, and register spilling. Memory accesses to these variables, usually indexed by stack pointer ($sp), can constitute a large fraction of overall memory references <ref> [9] </ref>. For example, spill codes can produce a significant number of memory references at run time, as many as 20% of all the executed instructions [1]. <p> These studies aimed primarily to reduce the impact of a procedure call/return on the processor performance, motivated by an observation that programs written in a high-level language tend to have many procedure calls and returns <ref> [9] </ref>, and that a function call is the most costly source language statement [18].
Reference: [10] <author> M. J. Flynn and L. W. Hoevel. </author> <title> Execution Architecture: The DELtran Experiment, </title> <journal> IEEE Trans. on Computers, C-32(2): </journal> <volume> 156 175, </volume> <month> Feb. </month> <year> 1983. </year>
Reference-contexts: To service the local variable accesses efficiently, we study two optimizations fast data forwarding and access combining. Although there have been efforts to optimize the local variable accesses with hardware support in the past <ref> [6, 10, 24] </ref>, little work has been done to study their performance impact in the context of a wide-issue superscalar processor with a multi-ported data cache. <p> The cache size to capture ten outstanding function frames is only 70 words on average. In fact, this has been the motivation for some previous work <ref> [6, 10, 24, 27] </ref>. The high frequency of local variable accesses and their spatial locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Moreover, identifying local variables in the stack frames is relatively easy for a compiler. 2.2.2 Architectural support To facilitate local variable accesses, a specialized hardware organization to simulate the run-time stack may appear attractive <ref> [6, 10, 24] </ref>. However, we use a more general cache design called the outlined in Section 3. 6 variable cache (LVC). B1, B2, and B3 are the bandwidth for each interface. local variable cache (LVC) in the framework of our data-decoupled architecture (see Figure 4). <p> Ditzel and McLellan [6] studied a transparent data buffer as a close mapping of the run-time stack, called the stack cache. The stack cache is effectively a large register file to simulate the run-time stack that replaces the general register file. The contour buffer proposed by Flynn and Hoevel <ref> [10] </ref> in their Directly Executed Languages model, is a programmer-addressable buffer that is used in conjunction with the run-time stack in memory.
Reference: [11] <author> L. Gwennap. </author> <title> Digital 21264 Sets New Standard, </title> <type> Microprocessor Report, Volume 10, </type> <note> Issue 14, </note> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. For example, DEC 21264 provides a two-ported data cache by running the cache twice as fast as the processor clock <ref> [11] </ref>, DEC 21164, the predecessor of 21264, uses a replicated data cache [7], and the MIPS R10000 implements a two-way interleaved data cache [31]. Each design, however, is either costly to implement, and/or has significant drawbacks. <p> Figure 4 shows the LVAQ and the LVC in the memory subsystem. Further optimizations are possible for the LVAQ and the LVC. Two such techniques to improve the local variable accesses are introduced: * Fast data forwarding In recent superscalar processors <ref> [31, 11] </ref>, data is forwarded from a store to a later load of the same address in the LSQ. This data forwarding enables faster loads without accessing the data cache. There is another opportunity to perform an even faster forwarding in the LVAQ. <p> The ROB and the LSQ effectively form the instruction window of the processor. The primary on-chip data cache that is 32 KB and 2-way set-associative has 2-cycle hit time, as in some of the recent machines <ref> [31, 11] </ref>. The 512 KB L2 cache, either on-chip or off-chip, has a 12-cycle hit latency. Data caches are lock-up free. <p> It is worth noting that the recent 21264 processor <ref> [11] </ref> has incorporated a 64 KB L1 data cache, eight times larger than that of its predecessor [7]. 5 Related Work The idea of optimizing local variables on run-time stack is not new.
Reference: [12] <author> M. D. Hill. </author> <title> A Case for Direct-Mapped Caches, </title> <journal> IEEE Computer, pp. </journal> <volume> 25 40, </volume> <month> Dec. </month> <year> 1988. </year>
Reference-contexts: the rest of the experiments, a 2 KB, direct-mapped LVC with a one-cycle hit latency is used. 9 We prefer this design to a 4 KB LVC or a set-associative LVC, because a small direct-mapped cache is likely to have an access time advantage when a fast clock is used <ref> [12] </ref>. Furthermore, adding additional ports to this small LVC is much cheaper than to a large data cache like the one used in our study (32 KB).
Reference: [13] <author> M. H. Lipasti and J. P. Shen. </author> <title> Superspeculative Microarchitecture for Beyond AD 2000, </title> <journal> IEEE Computer, pp. </journal> <volume> 59 66, </volume> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: For example, for a processor to sustain ten instructions per cycle (IPC), the memory subsystem should provide a minimum bandwidth of four references per cycle, or more, to prevent excessive queuing delays, assuming that about 40% of all instructions are loads and stores <ref> [13] </ref>. A straightforward approach for increasing memory bandwidth is to implement a multi-ported data cache [23]. There are a number of techniques to provide multiple cache ports: ideal multi-porting, time-division multiplexing, replicating the cache, and interleaving [19]. <p> extract and exploit more parallelism, a future superscalar processor will establish a wide instruction window that consists of a large number of reservation stations, from which instructions are steered 2 data-decoupled architecture with dual memory access queues and caches with 2 ports each. to a set of pipelined functional units <ref> [17, 13] </ref>. Building such a processor, unfortunately, poses many great challenges; Especially, the hardware complexity 1 of the logic that identifies and issues ready instructions from a large pool of reservation stations becomes an increasingly severe impediment to a faster clock rate [16]. <p> First, decoded memory access instructions should be partitioned into independent streams before they enter the instruction windows, as depicted in Figure 1 (b). Either run-time or compile-time information on per-reference access type is needed. When run-time speculation <ref> [13, 28, 15] </ref> is used for the classification, verification and recovery actions are required to 1 Hardware complexity in this paper refers to the critical path length that directly affects the clock cycle time. 2 Instruction memory accesses have been handled via a separate I-cache. 3 handle mispredictions. <p> It means that the proposed approach can have a performance advantage by providing more data bandwidth than a conventional technique at the same level of hardware complexity. This is a very critical issue for the future wide-issue processor proposals <ref> [13, 17, 21] </ref>, as they aggressively speculate on the control and use high-bandwidth instruction caches [32, 20], putting more pressure on the data cache bandwidth. <p> Even with this aggressive front-end, the obtainable IPCs are in the range of 4 6, whereas some recent proposals for wide-issue superscalar processors are targetting at 8 10 IPCs <ref> [13, 17] </ref>. Important parameters of the base machine model are summarized in Table 1. BASE MACHINE MODEL Issue width 16 Number of regs. 32 GPRs/32 FPRs ROB/LSQ size 128/64 Functional units 16 integer + 16 FP ALUs, 4 integer + 4 FP MULT/DIV units. <p> There are dynamic techniques to decouple a portion of data references and service them using sepate, specialized functional units. Lipasti introduced a notion called load stream partitioning in his Superflow processor model <ref> [13] </ref>, which simply partitions loads into multiple streams based on their run-time behavior, and sends them to disjoint functional units for processing. The functional units used include a constant verification unit, a queue for load/store folding, a stream buffer/prefetch engine, and a conventional data cache.
Reference: [14] <author> A. Moshovos, S. E. Breach, T. N. Vijaykumar, and G. S. Sohi. </author> <title> Dynamic Speculation and Synchronization of Data Dependences, </title> <booktitle> Proc. of the 24th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 181 193, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Such reduction in hardware complexity can lead to a shorter clock cycle time [16]. Second, dividing a large data stream into multiple smaller streams opens up new opportunities to optimize each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding <ref> [14, 15, 28] </ref> can be tailored to each stream for higher efficiency. This paper investigates the potential of decoupling local variable accesses in the data-decoupled architecture framework, using a small cache called the local variable cache (LVC) and an instruction queue called the local variable access queue (LVAQ).
Reference: [15] <author> A. Moshovos and G. S. Sohi. </author> <title> Streamlining Inter-operation Memory Communication via Data Dependence Prediction, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 235 245, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: Such reduction in hardware complexity can lead to a shorter clock cycle time [16]. Second, dividing a large data stream into multiple smaller streams opens up new opportunities to optimize each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding <ref> [14, 15, 28] </ref> can be tailored to each stream for higher efficiency. This paper investigates the potential of decoupling local variable accesses in the data-decoupled architecture framework, using a small cache called the local variable cache (LVC) and an instruction queue called the local variable access queue (LVAQ). <p> First, decoded memory access instructions should be partitioned into independent streams before they enter the instruction windows, as depicted in Figure 1 (b). Either run-time or compile-time information on per-reference access type is needed. When run-time speculation <ref> [13, 28, 15] </ref> is used for the classification, verification and recovery actions are required to 1 Hardware complexity in this paper refers to the critical path length that directly affects the clock cycle time. 2 Instruction memory accesses have been handled via a separate I-cache. 3 handle mispredictions. <p> The functional units used include a constant verification unit, a queue for load/store folding, a stream buffer/prefetch engine, and a conventional data cache. Techniques to detect dependent memory access instructions and explicitly synchronize and forward data between them have been proposed <ref> [28, 15] </ref>. They provide a dynamic technique to detect a producer operation and a consumer operation within the instruction window, and try to forward the data in a special buffer before the effective addresses are calculated, without accessing the cache memory.
Reference: [16] <author> S. Parlacharla, N. P. Jouppi, and J. E. Smith. </author> <title> Complexity-Effective Superscalar Processors, </title> <booktitle> Proc. of the 24th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 206 218, </pages> <month> June </month> <year> 1997. </year> <month> 21 </month>
Reference-contexts: More importantly, the network and the control logic for orchestrating memory accesses between a large number of reservation stations and cache ports become simpler. Such reduction in hardware complexity can lead to a shorter clock cycle time <ref> [16] </ref>. Second, dividing a large data stream into multiple smaller streams opens up new opportunities to optimize each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding [14, 15, 28] can be tailored to each stream for higher efficiency. <p> Building such a processor, unfortunately, poses many great challenges; Especially, the hardware complexity 1 of the logic that identifies and issues ready instructions from a large pool of reservation stations becomes an increasingly severe impediment to a faster clock rate <ref> [16] </ref>. The situation is exacerbated when there are multiple functional units of the same type, such as identical integer ALUs, because extra cycles may be needed for arbitration. <p> However, since it is increasingly difficult to add beyond two ports to a cache and to build a wide instruction window without penalizing clock cycle time, achieving a comparable performance with simpler hardware is a valid goal <ref> [16] </ref>. It means that the proposed approach can have a performance advantage by providing more data bandwidth than a conventional technique at the same level of hardware complexity.
Reference: [17] <author> Y. N. Patt, S. J. Patel, D. H. Friendly, and J. Stark. </author> <title> One Billion Transistors, One Uniprocessor, One Chip, </title> <journal> IEEE Computer, pp. </journal> <volume> 51 57, </volume> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: extract and exploit more parallelism, a future superscalar processor will establish a wide instruction window that consists of a large number of reservation stations, from which instructions are steered 2 data-decoupled architecture with dual memory access queues and caches with 2 ports each. to a set of pipelined functional units <ref> [17, 13] </ref>. Building such a processor, unfortunately, poses many great challenges; Especially, the hardware complexity 1 of the logic that identifies and issues ready instructions from a large pool of reservation stations becomes an increasingly severe impediment to a faster clock rate [16]. <p> It means that the proposed approach can have a performance advantage by providing more data bandwidth than a conventional technique at the same level of hardware complexity. This is a very critical issue for the future wide-issue processor proposals <ref> [13, 17, 21] </ref>, as they aggressively speculate on the control and use high-bandwidth instruction caches [32, 20], putting more pressure on the data cache bandwidth. <p> Even with this aggressive front-end, the obtainable IPCs are in the range of 4 6, whereas some recent proposals for wide-issue superscalar processors are targetting at 8 10 IPCs <ref> [13, 17] </ref>. Important parameters of the base machine model are summarized in Table 1. BASE MACHINE MODEL Issue width 16 Number of regs. 32 GPRs/32 FPRs ROB/LSQ size 128/64 Functional units 16 integer + 16 FP ALUs, 4 integer + 4 FP MULT/DIV units.
Reference: [18] <author> D. A. Patterson and C. H. Sequin. </author> <title> A VLSI RISC, </title> <journal> IEEE Computer, pp. </journal> <volume> 8 21, </volume> <month> Sept. </month> <year> 1982. </year>
Reference-contexts: These studies aimed primarily to reduce the impact of a procedure call/return on the processor performance, motivated by an observation that programs written in a high-level language tend to have many procedure calls and returns [9], and that a function call is the most costly source language statement <ref> [18] </ref>. Unlike the previous approaches, the technique proposed in this paper does not require processor intervention or complex algorithms to manage the buffer, which were mandated in the previous techniques to deal with buffer overflow/underflow and context switches.
Reference: [19] <author> J. A. Rivers, G. S. Tyson, E. S. Davidson, and T. M. Austin. </author> <title> On High-Bandwidth Data Cache Design for Multi-Issue Processors, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 46 56, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: A straightforward approach for increasing memory bandwidth is to implement a multi-ported data cache [23]. There are a number of techniques to provide multiple cache ports: ideal multi-porting, time-division multiplexing, replicating the cache, and interleaving <ref> [19] </ref>. Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. <p> Designing an effective multi-ported cache has been a continuing topic of active research <ref> [23, 29, 30, 19] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of the cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [20] <author> E. Rotenberg, S. Bennet, and J. E. Smith. </author> <title> Trace Cache: a Low Latency Approach to High Bandwidth Instruction Fetching, </title> <booktitle> Proc. of the 29th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 24 34, </pages> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: This is a very critical issue for the future wide-issue processor proposals [13, 17, 21], as they aggressively speculate on the control and use high-bandwidth instruction caches <ref> [32, 20] </ref>, putting more pressure on the data cache bandwidth.
Reference: [21] <author> E. Rotenberg, Q. Jacobson, and J. E. Smith. </author> <title> Trace Processors, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 138 148, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: It means that the proposed approach can have a performance advantage by providing more data bandwidth than a conventional technique at the same level of hardware complexity. This is a very critical issue for the future wide-issue processor proposals <ref> [13, 17, 21] </ref>, as they aggressively speculate on the control and use high-bandwidth instruction caches [32, 20], putting more pressure on the data cache bandwidth.
Reference: [22] <author> G. S. Sohi. </author> <title> Instruction Issue Logic for High-Performance, Interruptible, Multiple Functional Unit, Pipelined Computers, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(3):349 359, </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: The machine model used in the experiments is a superscalar processor that supports out-of-order issue and execution, based on the Register Update Unit (RUU) <ref> [22] </ref>. The RUU scheme uses a reorder buffer (ROB) to automatically perform register renaming and hold the results of pending instructions. In each cycle, the ROB retires completed instructions in program order to the architected register file.
Reference: [23] <author> G. S. Sohi and M. Franklin. </author> <title> High-Bandwidth Data Memory Systems for Superscalar Processors, </title> <booktitle> Proc. of the Fourth Int'l Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 53 62, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Furthermore, the ability to provide the execution core with adequate (cache) memory bandwidth becomes extremely critical for the next generations of wide-issue processors <ref> [23, 29] </ref>. For example, for a processor to sustain ten instructions per cycle (IPC), the memory subsystem should provide a minimum bandwidth of four references per cycle, or more, to prevent excessive queuing delays, assuming that about 40% of all instructions are loads and stores [13]. <p> A straightforward approach for increasing memory bandwidth is to implement a multi-ported data cache <ref> [23] </ref>. There are a number of techniques to provide multiple cache ports: ideal multi-porting, time-division multiplexing, replicating the cache, and interleaving [19]. Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. <p> Designing an effective multi-ported cache has been a continuing topic of active research <ref> [23, 29, 30, 19] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of the cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [24] <author> T. J. Stanley and R. G. Wedig. </author> <title> A Performance Analysis of Automatically Managed Top of Stack Buffers, </title> <booktitle> Proc. of the 14th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 272 281, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: To service the local variable accesses efficiently, we study two optimizations fast data forwarding and access combining. Although there have been efforts to optimize the local variable accesses with hardware support in the past <ref> [6, 10, 24] </ref>, little work has been done to study their performance impact in the context of a wide-issue superscalar processor with a multi-ported data cache. <p> The cache size to capture ten outstanding function frames is only 70 words on average. In fact, this has been the motivation for some previous work <ref> [6, 10, 24, 27] </ref>. The high frequency of local variable accesses and their spatial locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Moreover, identifying local variables in the stack frames is relatively easy for a compiler. 2.2.2 Architectural support To facilitate local variable accesses, a specialized hardware organization to simulate the run-time stack may appear attractive <ref> [6, 10, 24] </ref>. However, we use a more general cache design called the outlined in Section 3. 6 variable cache (LVC). B1, B2, and B3 are the bandwidth for each interface. local variable cache (LVC) in the framework of our data-decoupled architecture (see Figure 4). <p> The contour buffer proposed by Flynn and Hoevel [10] in their Directly Executed Languages model, is a programmer-addressable buffer that is used in conjunction with the run-time stack in memory. Stanley and Wedig <ref> [24] </ref> proposed three buffer management algorithms for a Top of Stack (TOS) buffer, which is a register file designed to cache the top elements of the stack.
Reference: [25] <institution> The Standard Performance Evaluation Corporation, </institution> <note> http://www.specbench.org. </note>
Reference-contexts: Ultra-fast processor clocks and considerations for instruction set architecture (ISA) compatibility, as exemplified by the Intel's x86 architectures, may disallow increasing the size of a register file. programs <ref> [25] </ref>. 5 A large fraction of memory references are to local variables, with an average of 30% 4 Another definition of a local variable, from a programming language's point of view, is a variable declared within a function. <p> Branch prediction Perfect Inst. latencies Same as those of MIPS R10000 [31]. Table 1: The base machine model. Decode and commit widths are the same as the issue width. 10 3.2 Benchmark programs We use eight integer and four floating-point programs from the SPEC 95 benchmark suite <ref> [25] </ref>, whose characteristics are summarized in Table 2. All the programs were compiled using EGCS 7 version 1.1b [8] at the -O3 optimization level with loop unrolling. Either train or test input is used in most cases, with some data set modification to control the simulation time.
Reference: [26] <author> Y. Tamir and C. H. Sequin. </author> <title> Strategies for Managing the Register File in RISC, </title> <journal> IEEE Trans. on Computers, C-32(11): </journal> <volume> 977 989, </volume> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: The major reason why a small LVC can achieve a high hit rate is that function frames tend to be very small as shown in Figure 3. Most of the programs have a call depth of four or five routines <ref> [26] </ref>. The line size of the LVC has a negligible effect on the hit rate when the LVC size is larger than or equal to 2 KB.
Reference: [27] <author> M. Tremblay, B. Joy, and K. Shin. </author> <title> A Three Dimensional Register File for Superscalar Processors, </title> <booktitle> Proc. of the 28th Annual Hawaii Int'l Conf. on Systems Sciences, </booktitle> <publisher> IEEE CS Press, </publisher> <year> 1995. </year>
Reference-contexts: The cache size to capture ten outstanding function frames is only 70 words on average. In fact, this has been the motivation for some previous work <ref> [6, 10, 24, 27] </ref>. The high frequency of local variable accesses and their spatial locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Among current microprocessors, Sun UltraSparc employs a special register file structure called register window to reduce the cost of a procedure call/return <ref> [27] </ref>. Chow and Hennessy [5] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference.
Reference: [28] <author> G. Tyson and T. M. Austin. </author> <title> Improving the Accuracy and Performance of Memory Communication Through Renaming, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 218 227, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: Such reduction in hardware complexity can lead to a shorter clock cycle time [16]. Second, dividing a large data stream into multiple smaller streams opens up new opportunities to optimize each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding <ref> [14, 15, 28] </ref> can be tailored to each stream for higher efficiency. This paper investigates the potential of decoupling local variable accesses in the data-decoupled architecture framework, using a small cache called the local variable cache (LVC) and an instruction queue called the local variable access queue (LVAQ). <p> First, decoded memory access instructions should be partitioned into independent streams before they enter the instruction windows, as depicted in Figure 1 (b). Either run-time or compile-time information on per-reference access type is needed. When run-time speculation <ref> [13, 28, 15] </ref> is used for the classification, verification and recovery actions are required to 1 Hardware complexity in this paper refers to the critical path length that directly affects the clock cycle time. 2 Instruction memory accesses have been handled via a separate I-cache. 3 handle mispredictions. <p> The functional units used include a constant verification unit, a queue for load/store folding, a stream buffer/prefetch engine, and a conventional data cache. Techniques to detect dependent memory access instructions and explicitly synchronize and forward data between them have been proposed <ref> [28, 15] </ref>. They provide a dynamic technique to detect a producer operation and a consumer operation within the instruction window, and try to forward the data in a special buffer before the effective addresses are calculated, without accessing the cache memory.
Reference: [29] <author> K. M. Wilson, K. Olukotun, and M. Rosenblum. </author> <title> Increasing Cache Port Efficiency for Dynamic Superscalar Microprocessors, </title> <booktitle> Proc. of the 23th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 147 157, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Furthermore, the ability to provide the execution core with adequate (cache) memory bandwidth becomes extremely critical for the next generations of wide-issue processors <ref> [23, 29] </ref>. For example, for a processor to sustain ten instructions per cycle (IPC), the memory subsystem should provide a minimum bandwidth of four references per cycle, or more, to prevent excessive queuing delays, assuming that about 40% of all instructions are loads and stores [13]. <p> This technique will be beneficial when there are many local store-reload pairs within a small section of code, e.g. spill codes generated by a compiler. 7 * Access combining <ref> [29] </ref> When a program or a program region contains many local variable accesses, the number of LVC ports can become a performance bottleneck. In fact, a procedure call/return generates bursty stack accesses for saving/restoring registers and passing parameters. <p> Designing an effective multi-ported cache has been a continuing topic of active research <ref> [23, 29, 30, 19] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of the cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [30] <author> K. M. Wilson and K. Olukotun. </author> <title> Designing High Bandwidth On-Chip Caches, </title> <booktitle> Proc. of the 24th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 121 132, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Designing an effective multi-ported cache has been a continuing topic of active research <ref> [23, 29, 30, 19] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of the cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [31] <author> K. C. Yeager. </author> <title> The MIPS R10000 Superscalar Microprocessor, </title> <journal> IEEE Micro, </journal> <volume> Volume 16, Number 2, </volume> <pages> pp. 28 40, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: For example, DEC 21264 provides a two-ported data cache by running the cache twice as fast as the processor clock [11], DEC 21164, the predecessor of 21264, uses a replicated data cache [7], and the MIPS R10000 implements a two-way interleaved data cache <ref> [31] </ref>. Each design, however, is either costly to implement, and/or has significant drawbacks. The time-division multiplexing does not scale beyond a certain number of ports (seemingly two ports). The replication approach broadcasts a store to each replicated cache for data coherence, and requires doubled silicon area. <p> The MIPS R10000 processor, for example, partitions the window into an integer queue, a floating-point queue, and an address queue, based on the instruction type <ref> [31] </ref>. The data-decoupled architecture further partitions the instruction window for data memory accesses, and provides a separate cache for each partitioned window. An example of a pipelined, two-way data-decoupled architecture is depicted in Figure 1 (b). <p> Figure 4 shows the LVAQ and the LVC in the memory subsystem. Further optimizations are possible for the LVAQ and the LVC. Two such techniques to improve the local variable accesses are introduced: * Fast data forwarding In recent superscalar processors <ref> [31, 11] </ref>, data is forwarded from a store to a later load of the same address in the LSQ. This data forwarding enables faster loads without accessing the data cache. There is another opportunity to perform an even faster forwarding in the LVAQ. <p> The model represents a future wide-issue processor with aggressive issue bandwidth from a large instruction window. The ROB has 128 entries and the LSQ has 64 entries, which are derived from the MIPS R10000 implementation <ref> [31] </ref>. The ROB and the LSQ effectively form the instruction window of the processor. The primary on-chip data cache that is 32 KB and 2-way set-associative has 2-cycle hit time, as in some of the recent machines [31, 11]. <p> The ROB and the LSQ effectively form the instruction window of the processor. The primary on-chip data cache that is 32 KB and 2-way set-associative has 2-cycle hit time, as in some of the recent machines <ref> [31, 11] </ref>. The 512 KB L2 cache, either on-chip or off-chip, has a 12-cycle hit latency. Data caches are lock-up free. <p> L1 data cache 2-way set-assoc. 32 KB. 2-cycle hit time. Varying number of ports. L2 data cache 4-way set-assoc. 512 KB. 12-cycle access time. Memory 50-cycle access time. Fully interleaved. Instruction cache Perfect I-cache with 1 cycle latency. Branch prediction Perfect Inst. latencies Same as those of MIPS R10000 <ref> [31] </ref>. Table 1: The base machine model. Decode and commit widths are the same as the issue width. 10 3.2 Benchmark programs We use eight integer and four floating-point programs from the SPEC 95 benchmark suite [25], whose characteristics are summarized in Table 2.
Reference: [32] <author> T.-Y. Yeh, D. T. Marr, and Y. N. Patt. </author> <title> Increasing the Instruction Fetch Rate via Multiple Branch Prediction and a Branch Address Cache, </title> <booktitle> Proc. of the 7th Int'l Conf. on Supercomputing, </booktitle> <pages> pp. 67 76, </pages> <month> July </month> <year> 1993. </year> <pages> 22 23 </pages>
Reference-contexts: This is a very critical issue for the future wide-issue processor proposals [13, 17, 21], as they aggressively speculate on the control and use high-bandwidth instruction caches <ref> [32, 20] </ref>, putting more pressure on the data cache bandwidth.
References-found: 32

