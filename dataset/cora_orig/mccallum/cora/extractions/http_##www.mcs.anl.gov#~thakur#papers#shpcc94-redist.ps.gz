URL: http://www.mcs.anl.gov/~thakur/papers/shpcc94-redist.ps.gz
Refering-URL: http://www.mcs.anl.gov/~thakur/papers.html
Root-URL: http://www.mcs.anl.gov
Email: @npac.syr.edu  
Title: Runtime Array Redistribution in HPF Programs  
Author: Rajeev Thakur Alok Choudhary Geoffrey Fox thakur, choudhar, gcf 
Address: 111 College Place, Rm 3-228 Syracuse University, Syracuse NY 13244  
Affiliation: Northeast Parallel Architectures Center  
Note: In Proc. of SHPCC '94, pp. 309-316  
Abstract: This paper describes efficient algorithms for run-time array redistribution in HPF programs. We consider block(m) to cyclic, cyclic to block(m) and the general cyclic(x) to cyclic(y) type redistributions. We initially describe algorithms for one-dimensional arrays and then extend the methodology to multidimensional arrays. The algorithms are practical enough to be easily implemented in the runtime library of an HPF compiler and can also be directly used in application programs requiring redistribution. Performance results on the Intel Paragon are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> Fortran 90D/HPF compiler for distributed memory MIMD computers: Design, implementation, and performance results. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 351-360, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: HPF extends Fortran 90 in several areas by providing data distribution features, data parallel execution features, extended intrinsic functions and standard library, and EXTRINSIC procedures. At Syracuse University, we are developing an HPF compiler which translates HPF to Fortran 77 plus message passing node programs for distributed memory computers <ref> [1] </ref>. HPF provides directives (ALIGN and DISTRIBUTE) which specify how arrays should be partitioned among the processors of a distributed memory computer. Arrays are first aligned to a template or index space. The DISTRIBUTE directive specifies how the template is to be distributed among the processors.
Reference: [2] <author> A. Carle, K. Kennedy, U. Kremer, and J. Mellor-Crummey. </author> <title> Automatic data layout for distributed memory machines in the D programming environment. </title> <type> Technical Report CRPC-TR93298, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: This approach also eliminates the need for complex inter-procedural analysis. After returning from the subroutine it is necessary to redistribute back to the original distribution. * In many applications such as 2D FFT and the ADI method for solving multidimensional PDEs, dynamic redistribution can result in significant performance improvement <ref> [2] </ref>. Array redistribution is an expensive operation as it involves data communication as well as computation of destination processors and addresses. It is necessary to do redistribution efficiently, otherwise the overhead of redistribution itself will offset the benefit of using a different distribution.
Reference: [3] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Generating local addresses and communication sets for data parallel programs. </title> <booktitle> In Proceedings of Principles and Practices of Parallel Programming (PPoPP) '93, </booktitle> <pages> pages 149-158, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Chatterjee et al <ref> [3] </ref> present an approach to calculate the sequence of local memory addresses that a given processor must access and the corresponding communication sets, using a finite state machine model which requires solving linear Diophantine equations.
Reference: [4] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction High Performance Fortran (HPF) is a language designed to support portable high performance programming on a wide variety of machines, including massively parallel SIMD and MIMD systems and vector processors <ref> [4] </ref>. It supports the data parallel programming model and uses Fortran 90 as the base language. HPF extends Fortran 90 in several areas by providing data distribution features, data parallel execution features, extended intrinsic functions and standard library, and EXTRINSIC procedures. <p> Arrays are first aligned to a template or index space. The DISTRIBUTE directive specifies how the template is to be distributed among the processors. In HPF, an array (or template) can be distributed as block (m) or cyclic (m) <ref> [4] </ref>.
Reference: [5] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> Fortran D language specifications. </title> <type> Technical Report COMP TR90-141, </type> <institution> Rice University, </institution> <year> 1990. </year>
Reference-contexts: In a cyclic distribution, array elements are distributed among processors in a round-robin fashion. In a cyclic (m) distribution, blocks of size m are distributed cyclically. The cyclic (m) distribution with 1 &lt; m &lt; dN=P e is commonly referred to as a block-cyclic distribution with block size m <ref> [5] </ref>. Block and cyclic distributions are special cases of the general cyclic (m) distribution. A cyclic (m) distribution with m = dN=P e is a block distribution and a cyclic (m) distribution with m = 1 is a cyclic distribution.
Reference: [6] <author> S. Gupta, S. Kaushik, S. Mufti, S. Sharma, C. Huang, and P. Sadayappan. </author> <title> On the generation of efficient data communication for distributed memory machines. </title> <booktitle> In Proceedings of International Computing Symposium, Taiwan, </booktitle> <pages> pages 504-513, </pages> <year> 1992. </year>
Reference-contexts: data communicated per processor is larger. 5 Cyclic (x) to Cyclic (y) Redistribution For a general cyclic (x) to cyclic (y) redistribution, there is no direct algebraic formula to calculate the set of elements to send to a destination processor and the local addresses of these elements at the destination <ref> [6] </ref>. Gupta et al [6] propose a virtual processor approach in which a block-cyclic distribution is considered to be either a virtual block or a virtual cyclic distribution using many virtual processors per physical processor. <p> is larger. 5 Cyclic (x) to Cyclic (y) Redistribution For a general cyclic (x) to cyclic (y) redistribution, there is no direct algebraic formula to calculate the set of elements to send to a destination processor and the local addresses of these elements at the destination <ref> [6] </ref>. Gupta et al [6] propose a virtual processor approach in which a block-cyclic distribution is considered to be either a virtual block or a virtual cyclic distribution using many virtual processors per physical processor.
Reference: [7] <author> R. Ponnusamy, R. Thakur, A. Choudhary, and G. Fox. </author> <title> Scheduling regular and irregular communication patterns on the CM-5. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 394-402, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The algorithms can be easily extended for the general case. Also, the algorithms do not specify how to perform data communication because the best communication algorithms are often machine dependent. These communication algorithms are described in detail in <ref> [9, 7] </ref>. We do assume that all the data to be sent from any processor i to processor j has to be collected in a packet in processor i and sent in one communication operation, so as to minimize the communication startup cost.
Reference: [8] <author> J. Stichnoth. </author> <title> Efficient compilation of array statements for private memory multicomputers. </title> <type> Technical Report CMU-CS-93-109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> Febru-ary </month> <year> 1993. </year>
Reference-contexts: Chatterjee et al [3] present an approach to calculate the sequence of local memory addresses that a given processor must access and the corresponding communication sets, using a finite state machine model which requires solving linear Diophantine equations. Stichnoth <ref> [8] </ref> defines a cyclic (m) distribution as a disjoint union of slices, where a slice is a sequence of array indices specified by a lower bound, upper bound and stride (l : h : s).
Reference: [9] <author> R. Thakur and A. Choudhary. </author> <title> All-to-all communication on meshes with wormhole routing. </title> <booktitle> In Proceedings of the 8 th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: The algorithms can be easily extended for the general case. Also, the algorithms do not specify how to perform data communication because the best communication algorithms are often machine dependent. These communication algorithms are described in detail in <ref> [9, 7] </ref>. We do assume that all the data to be sent from any processor i to processor j has to be collected in a packet in processor i and sent in one communication operation, so as to minimize the communication startup cost.
References-found: 9

