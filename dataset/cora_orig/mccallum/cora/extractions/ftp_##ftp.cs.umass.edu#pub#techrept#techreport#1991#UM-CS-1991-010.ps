URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1991/UM-CS-1991-010.ps
Refering-URL: http://www-ml.cs.umass.edu/
Root-URL: 
Title: Linear Machine Decision Trees  
Author: Paul E. Utgoff Carla E. Brodley 
Address: Amherst, Massachusetts 01003 USA  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: COINS Technical Report 91-10 January 1991 Abstract This article presents an algorithm for inducing multiclass decision trees with multivariate tests at internal decision nodes. Each test is constructed by training a linear machine and eliminating variables in a controlled manner. Empirical results demonstrate that the algorithm builds small accurate trees across a variety of tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Moreover, in domains in which the cost of misclassification is high, an intelligible tree allows a person to verify that the decision procedure is indeed correct. A tree induction algorithm is useful to the extent that it captures the structure of the underlying data <ref> (Breiman, Friedman, Olshen & Stone, 1984) </ref>. Assuming that class membership depends on some combination of properties of an instance, one would like the induction algorithm to identify that combination. Accordingly, the algorithm must be able to represent such a combination and find it in an acceptable amount of time. <p> The tree found by LMDT contains 7 linear machines, each with an average of 11.2 variables. The smallest tree produced by an ID3 variant (Utgoff, 1989) contains 62 univariate tests. A linear machine is more complex than a univariate test, so a strict comparison is unfair. The LED <ref> (Breiman, Friedman, Olshen & Stone, 1984) </ref> data set shows LMDT's performance on noisy data sets. The domain consists of seven Boolean attributes and ten classes, one for each of the decimal digits. The seven attributes are the seven light emitting diodes of a LED display.
Reference: <author> Cheng, J., Fayyad, U. M., Irani, K. B., & Qian, Z. </author> <year> (1988). </year> <title> Improved decision trees: A generalized version of ID3. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 100-106). </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Duda, R. O., & Fossum, H. </author> <year> (1966). </year> <title> Pattern classification by iteratively determined linear and piecewise linear discriminant functions. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> EC-15, </volume> <pages> 220-232. </pages>
Reference-contexts: A decision tree is an appropriate choice when the underlying class conditional probabilities are unknown and when the class distributions are multimodal <ref> (Duda & Fossum, 1966) </ref>. A secondary objective in building a decision tree is to obtain a decision procedure that is intelligible to a human, so that a person can become better at classifying instances by emulating the decision procedure. <p> For each instance, the weight vectors of the linear machine are adjusted only if the instance would be misclassified by the linear machine. One well known method is the absolute error correction rule <ref> (Duda & Fossum, 1966) </ref>, which adjusts W i , where i is the class to which the instance actually belongs, and W j , where j is the class to which the linear machine incorrectly assign the instance. <p> and one can accomplish this by c = 1 + k , where k = (W j W i ) T Y : The expression for k is the absolute error correction needed to adjust the weight vectors so that a misclassified instance becomes correctly classified by the linear machine <ref> (Duda & Fossum, 1966) </ref>. Although this will discount 6 misclassified instances that are far from a decision boundary, it does not solve the problem completely.
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> Wiley & Sons. </publisher>
Reference-contexts: If the instances are linearly separable by a linear machine, then the above training procedure, with a suitable error correction rule as described below, will find a solution machine in a finite number of steps <ref> (Duda & Hart, 1973) </ref>. A multivariate test, here represented as a linear machine, makes it possible to represent decision boundaries that are not orthogonal to the axes (Breiman, Friedman, Olshen & Stone, 1984; Utgoff & Brodley, 1990). <p> This criterion for when to reduce fi is motivated by the fact that the magnitude of the linear machine increases rapidly during the early training, stabilizing when the decision boundary is much nearer to its final location <ref> (Duda & Hart, 1973) </ref>. <p> A t-test with ff = :01, is used to measure whether the difference in the mean classification accuracies of the two LMs is significant or due to chance. The third case occurs when there are too few instances for the number of variables <ref> (Duda & Hart, 1973) </ref>. The ffi parameter is included for the sake of efficiency, and its default value is 0:10. The algorithm can stop trying to eliminate variables if the chance of finding an LM based on fewer variables with higher accuracy is remote.
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> Small nets and short paths: Optimising neural computation. </title> <type> Doctoral dissertation, </type> <institution> Center for Cognitive Science, University of Edinburgh. </institution>
Reference: <author> Gallant, S. I. </author> <year> (1988). </year> <title> Connectionist expert systems. </title> <journal> Communications of the ACM, </journal> <volume> 31, </volume> <pages> 152-169. </pages>
Reference-contexts: In general, multivariate tests are essential, so we will be seeking methods for making them understandable, rather than limiting the algorithm to weaker kinds of tests. A method similar to that employed by MACIE <ref> (Gallant, 1988) </ref> may be applicable for explaining decisions made by an individual linear machine.
Reference: <author> Hampson, S. E., & Volper, D. J. </author> <year> (1986). </year> <title> Linear function neurons: Structure and training. </title> <journal> Biological Cybernetics, </journal> <volume> 53, </volume> <pages> 203-217. </pages>
Reference-contexts: For a two-valued variable, one of the values is encoded as 1 and the other as 1. For a many-valued variable, each variable-value pair is encoded as a propositional variable, which is TRUE if and only if the variable has taken on the particular value in the instance <ref> (Hampson & Volper, 1986) </ref>. The value of each propositional variable is then encoded as 1 or 1 as before. This encoding scheme makes it possible to describe instances by any mix of variable types, numeric or symbolic.
Reference: <author> Lewis, P. M. </author> <year> (1962). </year> <title> The characteristic selection problem in recognition systems. </title> <journal> IRE Transactions on Information Theory, </journal> <volume> IT-8, </volume> <pages> 171-178. </pages> <note> 13 Michalski, </note> <author> R. S., & Chilausky, R. L. </author> <year> (1980). </year> <title> Learning by being told and learning from examples: An experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis. </title> <journal> Policy Analysis and Information Systems, </journal> <volume> 4, </volume> <pages> 125-160. </pages>
Reference-contexts: However, instead of selecting a univariate test for a decision node based on a heuristic measure, such as information gain <ref> (Lewis, 1962) </ref>, the LMDT algorithm trains a linear machine, which then serves as a multivariate test for the decision node. A linear machine (Nilsson, 1965; Duda & Hart, 1973) is a multiclass linear discriminant, which itself classifies the instance.
Reference: <author> Mtller, M. F. </author> <year> (1990). </year> <title> A scaled conjugate gradient algorithm for fast supervised learning, </title> <type> (DAIMI PB - 339), </type> <institution> Denmark: Aarhus University, Computer Science Department. </institution>
Reference-contexts: Through automatic encoding of numeric and symbolic variables, the algorithm is applicable to a wide variety of classification learning tasks. Two problems remain that we are in the process of addressing. First, there are several fast forms of conjugate gradient weight adjustment <ref> (Mtller, 1990) </ref>, and we would like to incorporate one. These methods are typically one to two orders of magnitude faster than gradient descent methods.
Reference: <author> Mooney, R., Shavlik, J., Towell, G., & Gove, A. </author> <year> (1989). </year> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 775-780). </pages> <address> Detroit, Michigan: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <title> Learning machines. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In Michalski, Carbonell & Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach. </booktitle> <address> San Ma-teo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The tree found by LMDT has two tests and three leaves and is logically equivalent to that found by FRINGE. The root node has an LM based on variables a and b and the second node has an LM based on c, d and e. The chess task <ref> (Quinlan, 1983) </ref> demonstrates that LMDT finds small trees. The tree found by LMDT contains 7 linear machines, each with an average of 11.2 variables. The smallest tree produced by an ID3 variant (Utgoff, 1989) contains 62 univariate tests.
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> Internation Journal of Man-machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference-contexts: Pessimistic pruning <ref> (Quinlan, 1987) </ref> is used to avoid over-fitting. Each measure is an average for five runs. Column "Invars" shows the number of input variables. Column "Vars/LM" indicates the average number of variables per linear machine, and column "Totvars" lists the number of unique input variables tested somewhere in the tree.
Reference: <author> Utgoff, P. E. </author> <year> (1989). </year> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 161-186. </pages>
Reference-contexts: The chess task (Quinlan, 1983) demonstrates that LMDT finds small trees. The tree found by LMDT contains 7 linear machines, each with an average of 11.2 variables. The smallest tree produced by an ID3 variant <ref> (Utgoff, 1989) </ref> contains 62 univariate tests. A linear machine is more complex than a univariate test, so a strict comparison is unfair. The LED (Breiman, Friedman, Olshen & Stone, 1984) data set shows LMDT's performance on noisy data sets.
Reference: <author> Utgoff, P. E., & Brodley, C. E. </author> <year> (1990). </year> <title> An incremental method for finding multivariate splits for decision trees. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 58-65). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 14 </pages>
References-found: 15

