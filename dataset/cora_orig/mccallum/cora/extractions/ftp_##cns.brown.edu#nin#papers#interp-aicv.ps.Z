URL: ftp://cns.brown.edu/nin/papers/interp-aicv.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Title: Interpreting Neural-Network Models  
Author: Nathan Intrator Orna Intrator 
Note: In Proceedings of the 10th Israeli Conference on AICV, (1993), pp. 257-264, R. Basri et al. (eds), Elsevier.  
Affiliation: Computer Science Department Tel-Aviv University  Statistics Department Hebrew University  
Abstract: Artificial Neural Network seem very promising for regression and classification, especially for large covariate spaces. These methods represent a non-linear function as a composition of low dimensional ridge functions and therefore appear to be less sensitive to the dimensionality of the covariate space. It is possible to show that some of these methods extend the well known logistic regression model. In this paper we propose a method for model interpretation and prediction via neural networks that takes advantage of the possible high non-linearity of the model.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. R. Barron and R. L. Barron. </author> <title> Statistical learning networks: A unifying view. </title> <editor> In Ed Wegman, editor, </editor> <booktitle> Computing Science and Statistics: Proc. 20th Symp. Interface, </booktitle> <pages> pages 192-203. </pages> <publisher> American Statistical Association, </publisher> <address> Washington, DC., </address> <year> 1988. </year>
Reference-contexts: Artificial neural networks (ANN) have been studied for some time now, and have proven to produce good prediction results in classification and regression problems. Recently, statistical aspects of neural networks have been discussed, and compared with properties of more "classical" methods <ref> [1, 2, 3] </ref>. Artificial neural networks have been especially successful in difficult real-world problems such as time series prediction [4] character recognition [5] and multi-speaker large vocabulary speech recognition [6]. <p> Several traditional statistical methods have usually been employed. A recent development has been ANN which provide an extremely flexible, almost nonparametric tool, less prone to the curse of dimensionality <ref> [1, 22] </ref>. Other reported nonlinear models have been criticized for being able to handle only a few nonlinear covariates (McCullag and Nelder, 1989, page 380). ANN models have been very successful for prediction, but their use for inference has been less practical because of problems in interpretability.
Reference: [2] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias-variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: Artificial neural networks (ANN) have been studied for some time now, and have proven to produce good prediction results in classification and regression problems. Recently, statistical aspects of neural networks have been discussed, and compared with properties of more "classical" methods <ref> [1, 2, 3] </ref>. Artificial neural networks have been especially successful in difficult real-world problems such as time series prediction [4] character recognition [5] and multi-speaker large vocabulary speech recognition [6].
Reference: [3] <author> Brian D. Ripley. </author> <title> Statistical aspects of neural networks. </title> <booktitle> In Proceedings of the Seminaire Eu-ropeen de Statistique, </booktitle> <address> Sandbjerg, Denmark, April, 1992. </address> <publisher> Chapman and Hall, </publisher> <year> 1993. </year>
Reference-contexts: Artificial neural networks (ANN) have been studied for some time now, and have proven to produce good prediction results in classification and regression problems. Recently, statistical aspects of neural networks have been discussed, and compared with properties of more "classical" methods <ref> [1, 2, 3] </ref>. Artificial neural networks have been especially successful in difficult real-world problems such as time series prediction [4] character recognition [5] and multi-speaker large vocabulary speech recognition [6].
Reference: [4] <author> A. S. Weigend and N. A. Gershenfeld, </author> <title> editors. TIME SERIES PREDICTION: Forecasting the Future and Understanding the Past. </title> <booktitle> Proceedings of the NATO Advanced Research Workshop on Comparative Time Series Analysis held in Santa Fe, </booktitle> <address> New Mexico, May 14-17, 1992. </address> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Recently, statistical aspects of neural networks have been discussed, and compared with properties of more "classical" methods [1, 2, 3]. Artificial neural networks have been especially successful in difficult real-world problems such as time series prediction <ref> [4] </ref> character recognition [5] and multi-speaker large vocabulary speech recognition [6]. This fact has motivated the use of ANN on data that relates to health outcomes such as death or disease diagnosis. One such example is the use of ANN for the diagnosis of Acute Coronary Occlusion [7].
Reference: [5] <author> Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. </author> <title> Handwritten digit recognition with a back-propagation network. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 396-404, </pages> <address> San Mateo, 1990. (Denver 1989), </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Recently, statistical aspects of neural networks have been discussed, and compared with properties of more "classical" methods [1, 2, 3]. Artificial neural networks have been especially successful in difficult real-world problems such as time series prediction [4] character recognition <ref> [5] </ref> and multi-speaker large vocabulary speech recognition [6]. This fact has motivated the use of ANN on data that relates to health outcomes such as death or disease diagnosis. One such example is the use of ANN for the diagnosis of Acute Coronary Occlusion [7].
Reference: [6] <author> R. P. Lippmann. </author> <title> Review of neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1(1) </volume> <pages> 1-38, </pages> <year> 1989. </year>
Reference-contexts: Recently, statistical aspects of neural networks have been discussed, and compared with properties of more "classical" methods [1, 2, 3]. Artificial neural networks have been especially successful in difficult real-world problems such as time series prediction [4] character recognition [5] and multi-speaker large vocabulary speech recognition <ref> [6] </ref>. This fact has motivated the use of ANN on data that relates to health outcomes such as death or disease diagnosis. One such example is the use of ANN for the diagnosis of Acute Coronary Occlusion [7].
Reference: [7] <author> W. G. Baxt. </author> <title> Use of an artificial neural network for data analysis in clinical decision-making: the diagnosis of acute coronary occlusion. </title> <journal> Neural Computation, </journal> <volume> 2(4) </volume> <pages> 480-489, </pages> <year> 1990. </year>
Reference-contexts: This fact has motivated the use of ANN on data that relates to health outcomes such as death or disease diagnosis. One such example is the use of ANN for the diagnosis of Acute Coronary Occlusion <ref> [7] </ref>. In such studies, the dependent variables of interest are class label, and the set of possible explanatory predictor variables the inputs to the ANN maybe binary or continuous.
Reference: [8] <author> A. R. Barron. </author> <title> Complexity regularization with application to artificial neural networks. </title> <editor> In G. Roussas, editor, </editor> <booktitle> Nonparametric Functional Estimation and Related Topics, </booktitle> <pages> pages 561-576. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, The Netherlands, </address> <year> 1991. </year>
Reference-contexts: In such studies, the dependent variables of interest are class label, and the set of possible explanatory predictor variables the inputs to the ANN maybe binary or continuous. Neural networks become useful in high dimensional regression by looking for low dimensional decompositions or projections <ref> [8] </ref> and are thus good candidates for analysis of multivariate clinical data. Feed-forward neural networks with simple architecture (one or two hidden layers) can approximate any L 2 function and its derivatives with any desired accuracy [9, 10]. <p> The variance of neural network estimators may be large due to this model flexibility thus contributing to overfitting. However, often the bias of such estimators is very small due to model flexibility, therefore, by careful methods for variance control and model selection <ref> [11, 8] </ref> it is often the case that the prediction error is much smaller than that of logistic regression or other small parameter models. While artificial neural networks have been extensively studied and used in classification and regression problems, their interpretability still remains vague.
Reference: [9] <author> G. Cybenko. </author> <title> Approximations by superpositions of a sigmoidal function. </title> <journal> Mathematics of Control, Signals and Systems, </journal> <volume> 2 </volume> <pages> 303-314, </pages> <year> 1989. </year>
Reference-contexts: Feed-forward neural networks with simple architecture (one or two hidden layers) can approximate any L 2 function and its derivatives with any desired accuracy <ref> [9, 10] </ref>. These two properties of ANN make them natural candidates for modelling data such as that of health outcome. The variance of neural network estimators may be large due to this model flexibility thus contributing to overfitting. <p> There are currently some theoretical results which suggest that neural network modelling is potentially very powerful for regression of various types. Such network architectures with an unbounded number of hidden units can approximate any L 2 function <ref> [9, 19] </ref>, and its derivatives [10]. Based on recent work of Jones [20, 21], Barron [22] and Hornik et al.[23] have established general results describing the degree of approximation properties of single hidden layer feedforward networks with sigmoid hidden layer activation function.
Reference: [10] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 551-560, </pages> <year> 1990. </year>
Reference-contexts: Feed-forward neural networks with simple architecture (one or two hidden layers) can approximate any L 2 function and its derivatives with any desired accuracy <ref> [9, 10] </ref>. These two properties of ANN make them natural candidates for modelling data such as that of health outcome. The variance of neural network estimators may be large due to this model flexibility thus contributing to overfitting. <p> The use of derivatives with respect to the input data for studying the interpretability of a model, sometimes called sensitivity analysis, is not new. Since a neural network architecture does not rely on totally nonparametric function estimation, the discussion about the derivatives of the function is meaningful <ref> [10] </ref>. However, there are several factors which degrade the reliability of the interpretation and that need to be addressed; First, ANN models are not identifiable; i.e. there is no unique solution to a fixed ANN architecture and learning rule. <p> There are currently some theoretical results which suggest that neural network modelling is potentially very powerful for regression of various types. Such network architectures with an unbounded number of hidden units can approximate any L 2 function [9, 19], and its derivatives <ref> [10] </ref>. Based on recent work of Jones [20, 21], Barron [22] and Hornik et al.[23] have established general results describing the degree of approximation properties of single hidden layer feedforward networks with sigmoid hidden layer activation function.
Reference: [11] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions (with discussion). </title> <journal> J. Royal Statistics Society B, </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: The variance of neural network estimators may be large due to this model flexibility thus contributing to overfitting. However, often the bias of such estimators is very small due to model flexibility, therefore, by careful methods for variance control and model selection <ref> [11, 8] </ref> it is often the case that the prediction error is much smaller than that of logistic regression or other small parameter models. While artificial neural networks have been extensively studied and used in classification and regression problems, their interpretability still remains vague.
Reference: [12] <author> P. McCullagh and J. A. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman and Hall, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: The transformation of the probability of the event that models the linear combination of the regressors is called in General Linear Models the "link" function <ref> [12] </ref>. In such models each variable (or transformed variable) has a linear effect on the transformed probability for an outcome. Another link function for binary outcomes data is the inverse function of the normal distribution.
Reference: [13] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: not constant over the space due to the greater nonlinearity of the model, so that looking at the average effect of a covariate will not always lead to a simple dependence on the parameters, and the interpretation will need further refinement. 3 Neural Network Modelling Plain back propagation training methodology <ref> [13] </ref> was used on a feed-forward neural network. Additional constrained training methods are discussed elsewhere [14]. These are aimed to smooth the regression surface and thus to avoid overfitting. The momentum method was used for acceleration of convergence time [13]. <p> further refinement. 3 Neural Network Modelling Plain back propagation training methodology <ref> [13] </ref> was used on a feed-forward neural network. Additional constrained training methods are discussed elsewhere [14]. These are aimed to smooth the regression surface and thus to avoid overfitting. The momentum method was used for acceleration of convergence time [13]. This is known to improve convergence rate with very little effect on the convergence point by allowing for larger steps of the updating rule. Either batch or stochastic training were used with no change in the results.
Reference: [14] <author> O. Intrator and N. Intrator. </author> <title> Interpreting neural-network models, 1993. </title> <type> Preprint. </type>
Reference-contexts: Additional constrained training methods are discussed elsewhere <ref> [14] </ref>. These are aimed to smooth the regression surface and thus to avoid overfitting. The momentum method was used for acceleration of convergence time [13]. This is known to improve convergence rate with very little effect on the convergence point by allowing for larger steps of the updating rule.
Reference: [15] <author> W. P. Lincoln and J. Skrzypek. </author> <title> Synergy of clustering multiple back-propagation networks. </title> <editor> In D. S. Touretzky and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 650-657. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Either batch or stochastic training were used with no change in the results. The averaging of several networks which is discussed in section 3.2 (see also <ref> [15, 16, 17, 18] </ref>) is considered to be very good for avoiding overfitting since it practically reduces the variance of the estimator without affecting its bias. Possible excessive shrinkage due to averaging can be avoided by testing for improved generalization on a validation set.
Reference: [16] <author> B. A. Pearlmutter and R. Rosenfeld. </author> <title> Chaitin-Kolmogorov complexity and generalization in neural networks. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 925-931. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Either batch or stochastic training were used with no change in the results. The averaging of several networks which is discussed in section 3.2 (see also <ref> [15, 16, 17, 18] </ref>) is considered to be very good for avoiding overfitting since it practically reduces the variance of the estimator without affecting its bias. Possible excessive shrinkage due to averaging can be avoided by testing for improved generalization on a validation set.
Reference: [17] <author> Michael P. Perrone. </author> <title> Improving Regression Estimation: Averaging Methods for Variance Reduction with Extensions to General Convex Measure Optimization. </title> <type> PhD thesis, </type> <institution> Brown University, Institute for Brain and Neural Systems, </institution> <year> 1993. </year>
Reference-contexts: Either batch or stochastic training were used with no change in the results. The averaging of several networks which is discussed in section 3.2 (see also <ref> [15, 16, 17, 18] </ref>) is considered to be very good for avoiding overfitting since it practically reduces the variance of the estimator without affecting its bias. Possible excessive shrinkage due to averaging can be avoided by testing for improved generalization on a validation set.
Reference: [18] <author> D. J. C. MacKay. </author> <title> A practical Bayesian framework for backprop networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472, </pages> <year> 1992. </year>
Reference-contexts: Either batch or stochastic training were used with no change in the results. The averaging of several networks which is discussed in section 3.2 (see also <ref> [15, 16, 17, 18] </ref>) is considered to be very good for avoiding overfitting since it practically reduces the variance of the estimator without affecting its bias. Possible excessive shrinkage due to averaging can be avoided by testing for improved generalization on a validation set.
Reference: [19] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: There are currently some theoretical results which suggest that neural network modelling is potentially very powerful for regression of various types. Such network architectures with an unbounded number of hidden units can approximate any L 2 function <ref> [9, 19] </ref>, and its derivatives [10]. Based on recent work of Jones [20, 21], Barron [22] and Hornik et al.[23] have established general results describing the degree of approximation properties of single hidden layer feedforward networks with sigmoid hidden layer activation function.
Reference: [20] <author> L. K. W. Jones. </author> <title> On a conjecture of huber concerning the convergence of projection pursuit regression. </title> <journal> Annals of Statistics, </journal> <volume> 15 </volume> <pages> 880-882, </pages> <year> 1987. </year>
Reference-contexts: There are currently some theoretical results which suggest that neural network modelling is potentially very powerful for regression of various types. Such network architectures with an unbounded number of hidden units can approximate any L 2 function [9, 19], and its derivatives [10]. Based on recent work of Jones <ref> [20, 21] </ref>, Barron [22] and Hornik et al.[23] have established general results describing the degree of approximation properties of single hidden layer feedforward networks with sigmoid hidden layer activation function.
Reference: [21] <author> L. K. W. Jones. </author> <title> A simple lemma on greedy approximation in hilbert space and convergence rates for projection pursuit regression and neural networks. </title> <journal> Annals of Statistics, </journal> <volume> 20 </volume> <pages> 608-613, </pages> <year> 1992. </year>
Reference-contexts: There are currently some theoretical results which suggest that neural network modelling is potentially very powerful for regression of various types. Such network architectures with an unbounded number of hidden units can approximate any L 2 function [9, 19], and its derivatives [10]. Based on recent work of Jones <ref> [20, 21] </ref>, Barron [22] and Hornik et al.[23] have established general results describing the degree of approximation properties of single hidden layer feedforward networks with sigmoid hidden layer activation function.
Reference: [22] <author> A. R. Barron. </author> <title> Universal approximation bounds for superpositions of a sigmoidal function. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(3) </volume> <pages> 930-945, </pages> <year> 1993. </year>
Reference-contexts: Such network architectures with an unbounded number of hidden units can approximate any L 2 function [9, 19], and its derivatives [10]. Based on recent work of Jones [20, 21], Barron <ref> [22] </ref> and Hornik et al.[23] have established general results describing the degree of approximation properties of single hidden layer feedforward networks with sigmoid hidden layer activation function. <p> Several traditional statistical methods have usually been employed. A recent development has been ANN which provide an extremely flexible, almost nonparametric tool, less prone to the curse of dimensionality <ref> [1, 22] </ref>. Other reported nonlinear models have been criticized for being able to handle only a few nonlinear covariates (McCullag and Nelder, 1989, page 380). ANN models have been very successful for prediction, but their use for inference has been less practical because of problems in interpretability.
Reference: [23] <author> K. Hornik, M. Stinchcombe, H. White, and P. Auer. </author> <title> Degree of approximation results for feedforward networks approximating unknown mappings and their derivatives. </title> <type> Discussion paper 93-15, </type> <institution> Department of Economics, UCSD, </institution> <year> 1993. </year>
References-found: 23

