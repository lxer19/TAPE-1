URL: http://www.cs.ucsb.edu/~tyang/papers/J_Numerical.ps
Refering-URL: http://www.cs.ucsb.edu/~tyang/papers/
Root-URL: http://www.cs.ucsb.edu
Email: gerasoulis@cs.rutgers.edu  tyang@cs.ucsb.edu  
Title: Performance Bounds for Column-Block Partitioning of Parallel Gaussian Elimination and Gauss-Jordan Methods  
Author: Apostolos Gerasoulis Tao Yang 
Address: New Brunswick, NJ 08903, USA  Santa Barbara, CA 93106, USA  
Affiliation: Department of Computer Science Rutgers University  Department of Computer Science University of California  
Abstract: Column-block partitioning is commonly used in the parallelization of Gaussian-Elimination(GE) and Gauss-Jordan(GJ) algorithms. It is therefore of interest to know performance bounds of such partitioning on scalable distributed-memory parallel architectures. In this paper, we use a graph-theoretic approach in deriving asymptotic performance lower bounds of column-block partitioning for both GE and GJ. The fl The work presented here was in part supported by ARPA contract DABT-63-93-C-0064, by the Office of Naval research under grant N000149310114, by a startup fund and faculty fellowship from University of California at Santa Barbara. The content of the information herein does not necessarily reflect the position of the Government and official endorsement should not be inferred. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. R. Cappello, </author> <title> Gaussian elimination on a hypercube automaton, </title> <journal> Journal of Parallel and Distributed Computing 4, </journal> <year> (1987), </year> <pages> 288-308. </pages>
Reference-contexts: 1 Introduction Gaussian-Elimination (GE) and Gauss-Jordan algorithms are widely used in the solution of linear algebraic systems and in the inversion of matrices. Parallel algorithms have been proposed for both GE and GJ <ref> [1, 3, 5, 8, 12, 14] </ref>. Given a linear algebraic system Ax = b where matrix A, where A = (a i;j ) is a non-singular n fi n dense matrix and b is the right hand side. <p> We provide experimental evidence with our scheduling and code generation system PYRROS on the nCUBE-2 parallel machine to demonstrate the sharpness of the bounds. Cappello <ref> [1] </ref> does an asymptotic analysis of GE with block partitioning. Our work studies the optimal parallel time for any method with column-block partitioning, and our analysis is more exact. 2 Background There are two fundamental steps in program parallelization: 2 1. Partitioning of program and data, and identifying the parallelism.
Reference: [2] <author> P. Chretienne, </author> <title> Task Scheduling over Distributed Memory Machines, </title> <booktitle> Proc. of the International Workshop on Parallel and Distributed Algorithms, </booktitle> <publisher> (North Holland, Ed.), </publisher> <year> 1989. </year> <month> 18 </month>
Reference-contexts: The Gantt chart completely describes the schedule since it defines both P A (n j ) and ST (n j ). The scheduling problem with communication delay has been shown to be NP-complete for a general task graph in most cases, Chretienne <ref> [2] </ref>, Papadimitriou and Yannakakis [10], Sarkar [13], Performance bounds. Let P T opt (G) be the length of an optimal schedule for task graph G.
Reference: [3] <author> M. Cosnard, B. Tourancheau, and G. Villard, </author> <title> Gaussian Elimination on Message Passing Architecture, </title> <booktitle> Lecture Notes in Computer Science 297, </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <year> 1987, </year> <pages> pp. 611-628. </pages>
Reference-contexts: 1 Introduction Gaussian-Elimination (GE) and Gauss-Jordan algorithms are widely used in the solution of linear algebraic systems and in the inversion of matrices. Parallel algorithms have been proposed for both GE and GJ <ref> [1, 3, 5, 8, 12, 14] </ref>. Given a linear algebraic system Ax = b where matrix A, where A = (a i;j ) is a non-singular n fi n dense matrix and b is the right hand side. <p> Golub and Ortega [8], Robert et. al. [12]. Deriving lower performance bounds for such partitioning is of great interests since they can provide performance information for parallel programs, e.g. Cosnard <ref> [3] </ref>, Ipsen et. al [11], Saad [14]. Most results in the literature for lower bound analysis ignore communication cost because it is quite difficult to incorporate such cost. <p> We assume that the matrix is partitioned 5 into columns data units to be consistent with the data accessing pattern of the tasks. The dependence task graph is given in Figure 3 <ref> [3, 9, 12] </ref>. Task T k+1 k is a broadcasting node, sending the same column k + 1 to all T j j = k + 2 : n + 1. This DAG has a degree of parallelism (the width of the DAG) equal to n.
Reference: [4] <author> G. Davis, </author> <title> Column LU Factorization with Pivoting on a Hypercube Multiprocessor, </title> <journal> SIAM J. Algebraic and Discrete Methods, </journal> <volume> vol. 7, </volume> <pages> pp. 538-550, </pages> <year> 1986. </year>
Reference: [5] <author> J. J. Dongarra, L. S. Duff, D. Sorensen and H. A. van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared Memory Computers, </title> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Gaussian-Elimination (GE) and Gauss-Jordan algorithms are widely used in the solution of linear algebraic systems and in the inversion of matrices. Parallel algorithms have been proposed for both GE and GJ <ref> [1, 3, 5, 8, 12, 14] </ref>. Given a linear algebraic system Ax = b where matrix A, where A = (a i;j ) is a non-singular n fi n dense matrix and b is the right hand side. <p> There are many approaches to increase the granularity of the GJ DAG and we describe one next. 3.2 Block partitioning A popular approach for increasing the granularity of GJ is BLAS-3 block partitioning, see Dongarra et. al. <ref> [5] </ref>. The matrix of n fi n is divided into N fiN submatrices and each submatrix has size of rfir where N = n=r. Each task T j k in the block GJ DAG is operating on a block of columns composed of N submatrices. <p> This graph is coarse grain in the top part but becomes fine grain in the bottom. The block LU algorithm with partial pivoting is relatively complicated <ref> [5] </ref>.
Reference: [6] <author> T.H. Dunigan, </author> <title> Performance of the INTEL iPSC/860 and nCUBE 6400 Hypercube, </title> <institution> ORNL TM-11790, Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: If ff is the startup time for processor communication and fi be the transmission speed, then the communication delay of sending a column between two neighbor processors is C = ff + nfi, see Dunigan <ref> [6] </ref>. Therefore, the granularity of the GJ DAG in g (GJ ) = C n! = ff fi where the "" assumes n is large enough. <p> The Speedup-Bound column is the speedup upper bound Seq=B using our lower bound result B in Table 2. We use ff = 160s, fi = 2:4s=word, and ! = 2:4s <ref> [6] </ref>. Table 4 lists the results for block Gauss-Jordan (GJ) algorithm without pivoting, with r = 10 and n = 1000. The sequential time is about 1150. Again we calculate speedup bound Seq=B and B is from Table 2.
Reference: [7] <author> A. Gerasoulis and T. Yang, </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, June, 1993. </journal> <volume> Vol. 4, No. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp. 686-701. </pages>
Reference-contexts: Most results in the literature for lower bound analysis ignore communication cost because it is quite difficult to incorporate such cost. In this paper, we use the graph scheduling theory and some of our recent results on the granularity of task graphs <ref> [7] </ref> to derive lower bounds that incorporate communication delays. We provide experimental evidence with our scheduling and code generation system PYRROS on the nCUBE-2 parallel machine to demonstrate the sharpness of the bounds. Cappello [1] does an asymptotic analysis of GE with block partitioning. <p> Let SU CC (n x ) be the set of tasks as the successors of task n x and P RED (n x ) be the set of tasks as the predecessors of task n x . In <ref> [7] </ref>, we define the grain of a task as follows: g (n x ) = minf min n k 2P RED (T x ) ft k g ; min n k 2SUCC (T x ) ft k g max n k 2SUCC (T x ) fc x;k g g Then the <p> When there is a sufficient number of processors, we say that a schedule uses nonlinear clustering if it assigns independent tasks in the same processor, otherwise linear clustering. In <ref> [7] </ref>, we prove the following theorem: Theorem 1 If g (G) 1, a schedule that uses nonlinear clustering can be transformed into a schedule that uses linear clustering with equal or shorter parallel time. 4 Thus there exists a schedule with linear clustering that attains the optimal solution.
Reference: [8] <author> G. Golub and J.M. Ortega, </author> <title> Scientific Computing : An Introduction to Parallel Computing, </title> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction Gaussian-Elimination (GE) and Gauss-Jordan algorithms are widely used in the solution of linear algebraic systems and in the inversion of matrices. Parallel algorithms have been proposed for both GE and GJ <ref> [1, 3, 5, 8, 12, 14] </ref>. Given a linear algebraic system Ax = b where matrix A, where A = (a i;j ) is a non-singular n fi n dense matrix and b is the right hand side. <p> A very natural partitioning for GE and GJ, when solving the above system, is a block of columns, where operations are performed between blocks of columns of size r, e.g. Golub and Ortega <ref> [8] </ref>, Robert et. al. [12]. Deriving lower performance bounds for such partitioning is of great interests since they can provide performance information for parallel programs, e.g. Cosnard [3], Ipsen et. al [11], Saad [14].
Reference: [9] <author> R.E. Lord, J.S. Kowalik, and S. P. Kumar, </author> <title> Solving Linear Algebraic Equations on an MIMD Computer, </title> <journal> Journal of the ACM, </journal> <volume> vol. 30, </volume> <pages> pp. 103-117, </pages> <year> 1983. </year>
Reference-contexts: We assume that the matrix is partitioned 5 into columns data units to be consistent with the data accessing pattern of the tasks. The dependence task graph is given in Figure 3 <ref> [3, 9, 12] </ref>. Task T k+1 k is a broadcasting node, sending the same column k + 1 to all T j j = k + 2 : n + 1. This DAG has a degree of parallelism (the width of the DAG) equal to n.
Reference: [10] <author> C. Papadimitriou and M. Yannakakis, </author> <title> Towards on an Architecture-Independent Analysis of Parallel Algorithms, </title> <journal> SIAM J. Comput., </journal> <volume> vol. 19, </volume> <pages> pp. 322-328, </pages> <year> 1990. </year>
Reference-contexts: The Gantt chart completely describes the schedule since it defines both P A (n j ) and ST (n j ). The scheduling problem with communication delay has been shown to be NP-complete for a general task graph in most cases, Chretienne [2], Papadimitriou and Yannakakis <ref> [10] </ref>, Sarkar [13], Performance bounds. Let P T opt (G) be the length of an optimal schedule for task graph G.
Reference: [11] <author> I.C.F. Ipsen, Y. Saad and M. Schultz, </author> <title> Complexity of Dense Linear System Solution on a Multiprocessor Ring, </title> <journal> Linear Algebra and Appl., </journal> <volume> vol. 77, </volume> <pages> pp. 205-239, </pages> <year> 1986. </year>
Reference-contexts: Golub and Ortega [8], Robert et. al. [12]. Deriving lower performance bounds for such partitioning is of great interests since they can provide performance information for parallel programs, e.g. Cosnard [3], Ipsen et. al <ref> [11] </ref>, Saad [14]. Most results in the literature for lower bound analysis ignore communication cost because it is quite difficult to incorporate such cost.
Reference: [12] <author> Y. Robert, B. Tourancheau and G. Villard, </author> <title> Data allocation strategies for the Gauss and Jordan Algorithms on a ring of processors. </title> <journal> Information Processing Letters 31, </journal> <year> (1989) </year> <month> 21-29. </month>
Reference-contexts: 1 Introduction Gaussian-Elimination (GE) and Gauss-Jordan algorithms are widely used in the solution of linear algebraic systems and in the inversion of matrices. Parallel algorithms have been proposed for both GE and GJ <ref> [1, 3, 5, 8, 12, 14] </ref>. Given a linear algebraic system Ax = b where matrix A, where A = (a i;j ) is a non-singular n fi n dense matrix and b is the right hand side. <p> A very natural partitioning for GE and GJ, when solving the above system, is a block of columns, where operations are performed between blocks of columns of size r, e.g. Golub and Ortega [8], Robert et. al. <ref> [12] </ref>. Deriving lower performance bounds for such partitioning is of great interests since they can provide performance information for parallel programs, e.g. Cosnard [3], Ipsen et. al [11], Saad [14]. <p> We assume that the matrix is partitioned 5 into columns data units to be consistent with the data accessing pattern of the tasks. The dependence task graph is given in Figure 3 <ref> [3, 9, 12] </ref>. Task T k+1 k is a broadcasting node, sending the same column k + 1 to all T j j = k + 2 : n + 1. This DAG has a degree of parallelism (the width of the DAG) equal to n.
Reference: [13] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> The MIT Press, </publisher> <year> 1989. </year> <month> 19 </month>
Reference-contexts: A task is an indivisible unit of computation which may be an assignment statement, a subroutine or even an entire program. We assume that tasks are convex, which means that once a task starts its execution it can run to completion without interrupting for communications, Sarkar <ref> [13] </ref>. In the task computation, a task waits to receive all data in parallel before it starts its execution. As soon as the task completes its execution it sends the output data to all successors in parallel. <p> The Gantt chart completely describes the schedule since it defines both P A (n j ) and ST (n j ). The scheduling problem with communication delay has been shown to be NP-complete for a general task graph in most cases, Chretienne [2], Papadimitriou and Yannakakis [10], Sarkar <ref> [13] </ref>, Performance bounds. Let P T opt (G) be the length of an optimal schedule for task graph G.
Reference: [14] <author> Y. Saad, </author> <title> Gaussian Elimination on Hypercubes, Parallel Algorithms and Architectures, </title> <editor> Cosnard, M. et al. Eds., </editor> <publisher> Elsevier Science Publishers, North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: 1 Introduction Gaussian-Elimination (GE) and Gauss-Jordan algorithms are widely used in the solution of linear algebraic systems and in the inversion of matrices. Parallel algorithms have been proposed for both GE and GJ <ref> [1, 3, 5, 8, 12, 14] </ref>. Given a linear algebraic system Ax = b where matrix A, where A = (a i;j ) is a non-singular n fi n dense matrix and b is the right hand side. <p> Golub and Ortega [8], Robert et. al. [12]. Deriving lower performance bounds for such partitioning is of great interests since they can provide performance information for parallel programs, e.g. Cosnard [3], Ipsen et. al [11], Saad <ref> [14] </ref>. Most results in the literature for lower bound analysis ignore communication cost because it is quite difficult to incorporate such cost.
Reference: [15] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors, </title> <booktitle> Proc. of 6th ACM Inter. Confer. on Supercomputing, </booktitle> <address> Washington D.C., </address> <year> 1992, </year> <pages> pp. 428-437. 20 </pages>
Reference-contexts: Next we compare these lower bound results with the performance of par-allelizing GJ/GE programs on nCUBE-2 using PYRROS <ref> [15] </ref>. PYRROS is an automatic scheduling and code generation tool for mapping task graphs on message-passing machines.
References-found: 15

