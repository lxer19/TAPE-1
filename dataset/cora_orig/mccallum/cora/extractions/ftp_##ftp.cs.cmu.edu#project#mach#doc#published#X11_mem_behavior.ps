URL: ftp://ftp.cs.cmu.edu/project/mach/doc/published/X11_mem_behavior.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/project/mach/public/www/doc/publications.html
Root-URL: 
Title: Memory Behavior of an X11 Window System  
Author: J. Bradley Chen 
Affiliation: School of Computer Science Carnegie Mellon University  
Note: To appear in The Proceedings of the USENIX Winter 1994 Technical Conference.  
Abstract: We used memory reference traces from a DEC Ultrix system running the X11 window system from MIT Project Athena and several freely available X11 applications to measure different aspects of memory system behavior and performance. Our measurements show that memory behavior for X11 workloads differs in several important ways from workloads more traditionally used in cache performance studies. User instruction cache behavior is a major component in overall memory system delays, with significant competition within and between address spaces. User TLB miss rates are up to a factor of two higher than other ill-behaved integer workloads. Write-buffer stalls, data cache behavior, and uncached memory reads can be problematic for microbenchmarks, but they are not an issue for the realistic applications we tested. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Michael J. Accetta, Robert V. Baron, William Bolosky, David B. Golub, Richard F. Rashid, Avadis Tevanian, Jr., and Michael W. Young. </author> <title> Mach: A New Kernel Foundation for Unix Development. </title> <booktitle> Proceedings of the Summer 1986 USENIX Conference, </booktitle> <month> July, </month> <year> 1986, </year> <pages> pp. 93-113. </pages>
Reference-contexts: Our measurements show degraded TLB behavior for X11 workloads as compared to other integer codes. Researchers at the University of Michigan [21] measured similar TLB behavior for the Mach 3.0 operating system <ref> [1, 13] </ref>, another system where a user-level server contributes significantly to overall activity. The two independent studies suggest a broader conclusion, that page behavior for user-level client/server systems induces substantially elevated TLB miss rates. As a final note, competition also affects TLB behavior.
Reference: 2. <institution> Digital Equipment Corporation. </institution> <note> Digital's 21064 Microprocessor. Data sheet. </note>
Reference-contexts: Thus, the impact on overall performance is not significant for the memory system we simulated. 9 The impact of the TLB on overall performance is dependent on the performance balance of memory system components. Processors such as the DEC Alpha 21064 <ref> [2] </ref> rely on fewer TLB entries with larger and oversized pages to achieve good TLB behavior. If software systems such as X11 don't make good use of these new features, miss rates will go up, increasing the impact of the TLB on overall performance.
Reference: 3. <author> Brian N. Bershad, Richard P. Draves, and Alessandro Forin. </author> <title> Using Microbenchmarks to Evaluate System Performance. </title> <booktitle> The Proceedings of the Third Workshop on Workstation Operating Systems, </booktitle> <month> April, </month> <year> 1992, </year> <pages> pp. 148-153. </pages>
Reference-contexts: Analysis of memory system components such as caches and write buffers is common practice for throughput benchmarks [7, 8, 10]. However, interactive programs and client-server systems have received relatively little attention in recent research. <ref> [3, 20] </ref>. This is unfortunate in that, for many computer users, quick response time for latency-critical interactive applications is more important than the throughput of batch jobs. Because of the size and complexity of server-based systems such as X11, few detailed measurements of their behavior have been made.
Reference: 4. <author> Anita Borg, R.E. Kessler, Georgia Lazana, and David Wall. </author> <title> Long Address Traces from RISC Machines: Generation and Analysis. </title> <note> WRL Research Report 89/14, </note> <institution> Digital Equipment Corporation Western Research Laboratory, </institution> <year> 1989. </year>
Reference-contexts: The paper closes with a brief review of our major conclusions. 2. Tracing and Simulation The experiments for this study ran on a DECstation 5000/200, using an address tracing system developed at Carnegie Mellon University and DEC WRL <ref> [4, 7] </ref>. The tracing system uses object code rewriting [4, 24], in which original object code is augmented with instrumentation instructions such that an address trace is generated as a side effect of program execution. Traces are accurately interleaved both within a single context and across user and system contexts. <p> The paper closes with a brief review of our major conclusions. 2. Tracing and Simulation The experiments for this study ran on a DECstation 5000/200, using an address tracing system developed at Carnegie Mellon University and DEC WRL [4, 7]. The tracing system uses object code rewriting <ref> [4, 24] </ref>, in which original object code is augmented with instrumentation instructions such that an address trace is generated as a side effect of program execution. Traces are accurately interleaved both within a single context and across user and system contexts.
Reference: 5. <author> J.R. Boyton, S.L. Chakrabarti, S.P. Hiebert, J.J. Lang, J.R. Owen, K.A. Marchington, P.R. Robinson, M.H. Stroyan, J.A. Waitz. </author> <title> "Sharing access to display resources in the Starbase/X11 Merge". </title> <journal> Hewlett Packard Journal 40, </journal> <month> 6 (December </month> <year> 1989), </year> <pages> 20-32. </pages>
Reference-contexts: They consider memory reference behavior, but strictly as related to frame buffer references; application performance is beyond the scope their work. Researchers at Hewlett Packard used a technique called Direct Hardware Accesses (DHA) in their Starbase/X11 Merge system to enable high performance when Starbase applications access the display. <ref> [6, 5] </ref> Several other projects consider memory system performance in a more general context, independent of X11 applications. MTOOL [11] compares execution time of program segments to predicted time for a perfect memory system.
Reference: 6. <author> Kenneth H. Bronstein, David J. Sweetser, and William R. Yoder. </author> <title> "System design for compatibility of a high-performance graphics library and the X Window System.". </title> <journal> Hewlett Packard Journal 40, </journal> <month> 6 (December </month> <year> 1989), </year> <pages> 6-10. </pages> . 
Reference-contexts: They consider memory reference behavior, but strictly as related to frame buffer references; application performance is beyond the scope their work. Researchers at Hewlett Packard used a technique called Direct Hardware Accesses (DHA) in their Starbase/X11 Merge system to enable high performance when Starbase applications access the display. <ref> [6, 5] </ref> Several other projects consider memory system performance in a more general context, independent of X11 applications. MTOOL [11] compares execution time of program segments to predicted time for a perfect memory system.
Reference: 7. <author> J. Bradley Chen and Brian N. Bershad. </author> <title> The Impact of Operating System Structure on Memory System Performance. </title> <booktitle> Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <month> December, </month> <year> 1993. </year>
Reference-contexts: A key distinction between interactive workloads and more traditional benchmarks is their sensitivity to latency, which is the time required for the system to respond to a given input event. Analysis of memory system components such as caches and write buffers is common practice for throughput benchmarks <ref> [7, 8, 10] </ref>. However, interactive programs and client-server systems have received relatively little attention in recent research. [3, 20]. This is unfortunate in that, for many computer users, quick response time for latency-critical interactive applications is more important than the throughput of batch jobs. <p> The paper closes with a brief review of our major conclusions. 2. Tracing and Simulation The experiments for this study ran on a DECstation 5000/200, using an address tracing system developed at Carnegie Mellon University and DEC WRL <ref> [4, 7] </ref>. The tracing system uses object code rewriting [4, 24], in which original object code is augmented with instrumentation instructions such that an address trace is generated as a side effect of program execution. Traces are accurately interleaved both within a single context and across user and system contexts. <p> Among common integer benchmarks, compress presents an above average demand on the memory system, while gcc's demands are extreme. More details on the memory system behavior of these workloads can be found in <ref> [7] </ref>. Table 3-2 gives reference counts for the experimental workloads. The low percentage of kernel instructions for the microbenchmarks demonstrates that many types of X11 operations require relatively little kernel activity. <p> Turning to user-level overhead, Figure 4-1 shows that many X11 clients have significant user instruction cache MCPI contributions, sometimes higher than system i-cache MCPI contributions. This is unusual for 3 integer workloads <ref> [7] </ref>. <p> Table 4-1 shows TLB miss data for splot, gs, and gcc. Compared to other integer workloads, X11 applications have poor TLB behavior. Both splot and gs show significantly higher miss rates than gcc, which is relatively demanding among integer workloads <ref> [7] </ref>. Three phenomena contribute to increased TLB miss rates: The X11 server needs over 200 page mappings to address the entire frame buffer. Any operation that paints a significant part of the screen will tend to flush the TLB.
Reference: 8. <author> J. Bradley Chen, Anita Borg, and Norman P. Jouppi. </author> <title> A Simulation Based Study of TLB Performance. </title> <booktitle> The Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May, </month> <year> 1992, </year> <pages> pp. 114-123. </pages>
Reference-contexts: A key distinction between interactive workloads and more traditional benchmarks is their sensitivity to latency, which is the time required for the system to respond to a given input event. Analysis of memory system components such as caches and write buffers is common practice for throughput benchmarks <ref> [7, 8, 10] </ref>. However, interactive programs and client-server systems have received relatively little attention in recent research. [3, 20]. This is unfortunate in that, for many computer users, quick response time for latency-critical interactive applications is more important than the throughput of batch jobs. <p> Also, the penalty for a single TLB miss could increase. An earlier study used 100 cycles as an estimate of the TLB miss penalty for a futuristic machine <ref> [8] </ref>. As the balance of TLB to cache resources changes, TLB performance could become an important issue. X11 workloads require more TLB resources than popular benchmarks such as gcc.
Reference: 9. <author> H. Davis, S.R. Goldschmidt, and J. Hennessy. </author> <title> Tango: A Multiprocessor Simulation and Tracing System. </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August, </month> <year> 1991, </year> <pages> pp. 99-107. </pages>
Reference-contexts: Thus, MTOOL is not appropriate for measuring X11 workloads. MTOOL has been adapted to work with shared-memory multiprocessor programs [12]. Another project, MemSpy [15], is based on the Tango <ref> [9] </ref> simulation and tracing system. To date, Tango is designed for use with parallel applications and multiprocessor systems, and has not been applied to multiprogrammed uniprocessor workloads or measurements of operating system activity. The measurements for this paper concern aggregate memory system behavior.
Reference: 10. <author> Jeffrey D. Gee, Mark D. Hill, Dionisios N. Pnevmatikatos, and Alan Jay Smith. </author> <title> Cache Performance of the SPEC Benchmark Suite. </title> <institution> University of Wisconsin-Madison, </institution> <year> 1991. </year>
Reference-contexts: A key distinction between interactive workloads and more traditional benchmarks is their sensitivity to latency, which is the time required for the system to respond to a given input event. Analysis of memory system components such as caches and write buffers is common practice for throughput benchmarks <ref> [7, 8, 10] </ref>. However, interactive programs and client-server systems have received relatively little attention in recent research. [3, 20]. This is unfortunate in that, for many computer users, quick response time for latency-critical interactive applications is more important than the throughput of batch jobs.
Reference: 11. <author> Aaron Goldberg and John Hennessy. </author> <title> MTOOL: A Method for Detecting Memory Bottlenecks. </title> <note> WRL Technical Note TN-17, </note> <institution> Digital Equipment Corporation Western Research Laboratory, </institution> <year> 1990. </year>
Reference-contexts: Researchers at Hewlett Packard used a technique called Direct Hardware Accesses (DHA) in their Starbase/X11 Merge system to enable high performance when Starbase applications access the display. [6, 5] Several other projects consider memory system performance in a more general context, independent of X11 applications. MTOOL <ref> [11] </ref> compares execution time of program segments to predicted time for a perfect memory system. A large difference between the predicted and the measured times suggests a possible memory system performance problem.
Reference: 12. <author> Aaron J. Goldberg and John L. Hennessy. </author> <title> "MTOOL: An Integrated System for Performance Debugging Shared Memory Multiprocessor Applications". </title> <journal> IEEE Transactions on Parallel Processing 4, </journal> <month> 1 (January </month> <year> 1993), </year> <pages> 28-40. </pages>
Reference-contexts: MTOOL has been applied primarily to detecting memory bottlenecks in FORTRAN programs, and is not appropriate for measuring operating system behavior. Thus, MTOOL is not appropriate for measuring X11 workloads. MTOOL has been adapted to work with shared-memory multiprocessor programs <ref> [12] </ref>. Another project, MemSpy [15], is based on the Tango [9] simulation and tracing system. To date, Tango is designed for use with parallel applications and multiprocessor systems, and has not been applied to multiprogrammed uniprocessor workloads or measurements of operating system activity.
Reference: 13. <author> David Golub, Randall Dean, Alessandro Forin and Richard Rashid. </author> <title> UNIX as an Application Program. </title> <booktitle> Proceedings of the Summer 1990 USENIX Conference, </booktitle> <month> June, </month> <year> 1990, </year> <pages> pp. 87-95. </pages>
Reference-contexts: Our measurements show degraded TLB behavior for X11 workloads as compared to other integer codes. Researchers at the University of Michigan [21] measured similar TLB behavior for the Mach 3.0 operating system <ref> [1, 13] </ref>, another system where a user-level server contributes significantly to overall activity. The two independent studies suggest a broader conclusion, that page behavior for user-level client/server systems induces substantially elevated TLB miss rates. As a final note, competition also affects TLB behavior.
Reference: 14. <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1990. </year>
Reference-contexts: MCPI is one of several components of cycles per instruction (CPI), a metric commonly used to evaluate computer systems <ref> [14] </ref>. Other components of CPI that are not reflected in MCPI include one cycle per instruction for instruction execution by the processor, cycles during interlocked multiply, divide, and floating point operations, and no-ops inserted by the compiler for load and branch delays.
Reference: 15. <author> Margaret Martonosi, Anoop Gupta, and Thomas Anderson. MemSpy, </author> <title> Analyzing Memory System Bottlenecks in Programs. </title> <booktitle> Proceedings of the 1992 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> June, </month> <year> 1992, </year> <pages> pp. 1-12. </pages>
Reference-contexts: MTOOL has been applied primarily to detecting memory bottlenecks in FORTRAN programs, and is not appropriate for measuring operating system behavior. Thus, MTOOL is not appropriate for measuring X11 workloads. MTOOL has been adapted to work with shared-memory multiprocessor programs [12]. Another project, MemSpy <ref> [15] </ref>, is based on the Tango [9] simulation and tracing system. To date, Tango is designed for use with parallel applications and multiprocessor systems, and has not been applied to multiprogrammed uniprocessor workloads or measurements of operating system activity. The measurements for this paper concern aggregate memory system behavior.
Reference: 16. <author> Joel McCormack. </author> <title> Writing Fast X Servers for Dumb Color Frame Buffers. </title> <note> WRL Research Report 91/1, </note> <institution> Digital Equipment Corporation Western Research Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Simple measures of performance, such as operations per second, are often used when characterizing new graphics hardware. Researchers at DEC WRL have done significant work in achieving good X11 performance, both with simple bit-mapped framebuffers <ref> [16] </ref> and more complicated hardware [17]. They also demonstrate software algorithms that permit effective use of the hardware. They consider memory reference behavior, but strictly as related to frame buffer references; application performance is beyond the scope their work. <p> This could potentially induce penalties for frame buffer reads. Fortunately such reads are relatively rare. Frame buffer writes pass through the write-buffer so their performance is unaffected. A discussion of effective software support of the DECstation 5000/200 frame buffer can be found in <ref> [16] </ref>. instruction cache: 64 KB, direct-mapped, physical, 16 byte line, 15 cycle miss penalty. data cache: 64 KB, direct-mapped, physical, 4 byte line, write allocate, 15 cycle read miss penalty, read miss fetches 16 aligned bytes. write buffer: six entries, page-mode writes complete in one cycle, non page-mode writes complete in
Reference: 17. <author> Joel McCormack and Bob McNamara. </author> <title> A Smart Frame Buffer. </title> <note> WRL Research Report 93/1, </note> <institution> Digital Equipment Corporation Western Research Laboratory, </institution> <year> 1993. </year>
Reference-contexts: Simple measures of performance, such as operations per second, are often used when characterizing new graphics hardware. Researchers at DEC WRL have done significant work in achieving good X11 performance, both with simple bit-mapped framebuffers [16] and more complicated hardware <ref> [17] </ref>. They also demonstrate software algorithms that permit effective use of the hardware. They consider memory reference behavior, but strictly as related to frame buffer references; application performance is beyond the scope their work.
Reference: 18. <author> Scott McFarling. </author> <title> Program Optimization for Instruction Caches. </title> <booktitle> The Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April, </month> <year> 1989, </year> <pages> pp. 183-191. </pages>
Reference-contexts: Summary For the two realistic X11 workloads we consider, both inter-context competition and self-interference misses have significant impact on memory system behavior, with the most significant effects occurring in the instruction cache. Strategies have been described <ref> [18] </ref> for avoiding text conflicts within an address space, but it is difficult to envision a practical software system to avoid competition between address spaces. The problem could potentially be addressed in hardware with cache associativity, although this could have an impact on machine cycle time.
Reference: 19. <author> Jeffrey C. Mogul and Anita Borg. </author> <title> The Effect of Context Switches on Cache Performance. </title> <booktitle> The Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April, </month> <year> 1991, </year> <pages> pp. 75-84. </pages>
Reference-contexts: Many such conflicts were identified for the splot run. The large working sets of X11 workloads, combined with frequent mandatory context switches, makes competition misses more likely. These results confirm earlier research on cache competition, which demonstrated significant penalties for warming up the cache after a context switch <ref> [19] </ref>. The earlier study found competition to be important in multitasking and compute-bound workloads, but a non-issue in the client-server workload they tested.
Reference: 20. <author> Jeffrey C. Mogul. </author> <title> SPECmarks are leading us astray. </title> <booktitle> The Third Workshop on Workstation Operating Systems, </booktitle> <month> April, </month> <year> 1992, </year> <pages> pp. 160-161. </pages>
Reference-contexts: Analysis of memory system components such as caches and write buffers is common practice for throughput benchmarks [7, 8, 10]. However, interactive programs and client-server systems have received relatively little attention in recent research. <ref> [3, 20] </ref>. This is unfortunate in that, for many computer users, quick response time for latency-critical interactive applications is more important than the throughput of batch jobs. Because of the size and complexity of server-based systems such as X11, few detailed measurements of their behavior have been made.
Reference: 21. <author> David Nagle, Richard Uhlig, Tim Stanley, Stuart Sechrest, Trevor Mudge and Richard Brown. </author> <title> Design Tradeoffs for Software-Managed TLBs. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <month> May, </month> <year> 1993, </year> <pages> pp. 27-38. 11 </pages>
Reference-contexts: Table 4-1: TLB Misses per 1000 instructions. System TLB misses include misses to both user and system segments. During the run of splot, 280000 user TLB misses occurred. There were about 680000 during gs. Estimating 20 cycles to service a TLB miss <ref> [21] </ref>, the penalty for TLB faults is less than 0.04 CPI for both workloads. Thus, the impact on overall performance is not significant for the memory system we simulated. 9 The impact of the TLB on overall performance is dependent on the performance balance of memory system components. <p> If computer systems are designed to optimize the performance of the popular benchmarks, systems such as X11 can be expected to suffer. Our measurements show degraded TLB behavior for X11 workloads as compared to other integer codes. Researchers at the University of Michigan <ref> [21] </ref> measured similar TLB behavior for the Mach 3.0 operating system [1, 13], another system where a user-level server contributes significantly to overall activity. The two independent studies suggest a broader conclusion, that page behavior for user-level client/server systems induces substantially elevated TLB miss rates.
Reference: 22. <author> J.L. Peterson. XSCOPE: </author> <title> a debugging and PERFORMANCE tool for X11. </title> <booktitle> Proceedings of the IFIP 11th World Computer Congress, </booktitle> <month> September, </month> <year> 1989, </year> <pages> pp. 49-54. </pages>
Reference-contexts: Several prior studies measure X11 behavior, although they differ substantially in that they consider behavior at higher levels of abstraction. Researchers at the Microelectronics Computation and Technology Corporation built a tool called XSCOPE to measure X11 performance and localize performance problems <ref> [22] </ref>. XSCOPE provides information about X11 request, reply, error, and event packets. Their experience in designing XSCOPE indicated some problems with the syntax of the X11 protocol. Simple measures of performance, such as operations per second, are often used when characterizing new graphics hardware.
Reference: 23. <editor> SPEC Benchmark Suite Release 1.0. </editor> <title> System Performance Evaluation Cooperative, </title> <year> 1989. </year>
Reference-contexts: This cache competition appears difficult to avoid in a direct mapped cache, suggesting that higher associativity may be required. TLB designs that do not accommodate the demands of large interactive systems may also become performance problems. X11 workloads, as compared to the SPECmarks <ref> [23] </ref> and other more traditional workloads for behavioral studies of memory systems, differ in several fundamental ways: Large program text. Even the largest SPECmarks are small compared to X11. At 688 KBytes, 1 gcc stands out among the SPECmarks for its large text segment .
Reference: 24. <author> David W. Wall. </author> <title> Systems for Late Code Modification. In Code Generation --- Concepts, Tools, Techniques, </title> <publisher> Springer-Verlag, </publisher> <year> 1992, </year> <pages> pp. 275-293. </pages> <note> J. Bradley Chen is presently completing a PhD in Computer Science at Carnegie Mellon University. </note> <editor> His interests include operating systems, </editor> <title> memory systems, and issues relating to the design and performance of large software systems. He received BS and MS degrees in 1987 from Stanford University. 12 Table of Contents </title>
Reference-contexts: The paper closes with a brief review of our major conclusions. 2. Tracing and Simulation The experiments for this study ran on a DECstation 5000/200, using an address tracing system developed at Carnegie Mellon University and DEC WRL [4, 7]. The tracing system uses object code rewriting <ref> [4, 24] </ref>, in which original object code is augmented with instrumentation instructions such that an address trace is generated as a side effect of program execution. Traces are accurately interleaved both within a single context and across user and system contexts.
Reference: 1. <author> Introduction 1 1.1. </author> <title> Related Work 2 </title>
Reference-contexts: Our measurements show degraded TLB behavior for X11 workloads as compared to other integer codes. Researchers at the University of Michigan [21] measured similar TLB behavior for the Mach 3.0 operating system <ref> [1, 13] </ref>, another system where a user-level server contributes significantly to overall activity. The two independent studies suggest a broader conclusion, that page behavior for user-level client/server systems induces substantially elevated TLB miss rates. As a final note, competition also affects TLB behavior.
Reference: 2. <institution> Tracing and Simulation 3 </institution>
Reference-contexts: Thus, the impact on overall performance is not significant for the memory system we simulated. 9 The impact of the TLB on overall performance is dependent on the performance balance of memory system components. Processors such as the DEC Alpha 21064 <ref> [2] </ref> rely on fewer TLB entries with larger and oversized pages to achieve good TLB behavior. If software systems such as X11 don't make good use of these new features, miss rates will go up, increasing the impact of the TLB on overall performance.
Reference: 3. <institution> Workloads 4 </institution>
Reference-contexts: Analysis of memory system components such as caches and write buffers is common practice for throughput benchmarks [7, 8, 10]. However, interactive programs and client-server systems have received relatively little attention in recent research. <ref> [3, 20] </ref>. This is unfortunate in that, for many computer users, quick response time for latency-critical interactive applications is more important than the throughput of batch jobs. Because of the size and complexity of server-based systems such as X11, few detailed measurements of their behavior have been made.
Reference: 4. <author> Experiments and Analysis 5 4.1. </author> <title> Memory Cycles Per Instruction 5 4.2. Cache Effects 7 4.2.1. Inter-Context Competition 7 4.2.2. Self-Interference misses 8 4.2.3. Summary 8 4.3. </title> <booktitle> TLB Behavior 9 </booktitle>
Reference-contexts: The paper closes with a brief review of our major conclusions. 2. Tracing and Simulation The experiments for this study ran on a DECstation 5000/200, using an address tracing system developed at Carnegie Mellon University and DEC WRL <ref> [4, 7] </ref>. The tracing system uses object code rewriting [4, 24], in which original object code is augmented with instrumentation instructions such that an address trace is generated as a side effect of program execution. Traces are accurately interleaved both within a single context and across user and system contexts. <p> The paper closes with a brief review of our major conclusions. 2. Tracing and Simulation The experiments for this study ran on a DECstation 5000/200, using an address tracing system developed at Carnegie Mellon University and DEC WRL [4, 7]. The tracing system uses object code rewriting <ref> [4, 24] </ref>, in which original object code is augmented with instrumentation instructions such that an address trace is generated as a side effect of program execution. Traces are accurately interleaved both within a single context and across user and system contexts.
Reference: 5. <institution> Conclusions 10 </institution>
Reference-contexts: They consider memory reference behavior, but strictly as related to frame buffer references; application performance is beyond the scope their work. Researchers at Hewlett Packard used a technique called Direct Hardware Accesses (DHA) in their Starbase/X11 Merge system to enable high performance when Starbase applications access the display. <ref> [6, 5] </ref> Several other projects consider memory system performance in a more general context, independent of X11 applications. MTOOL [11] compares execution time of program segments to predicted time for a perfect memory system.
Reference: 6. <institution> Acknowledgements 10 References 10 i </institution>
Reference-contexts: They consider memory reference behavior, but strictly as related to frame buffer references; application performance is beyond the scope their work. Researchers at Hewlett Packard used a technique called Direct Hardware Accesses (DHA) in their Starbase/X11 Merge system to enable high performance when Starbase applications access the display. <ref> [6, 5] </ref> Several other projects consider memory system performance in a more general context, independent of X11 applications. MTOOL [11] compares execution time of program segments to predicted time for a perfect memory system.
References-found: 30

