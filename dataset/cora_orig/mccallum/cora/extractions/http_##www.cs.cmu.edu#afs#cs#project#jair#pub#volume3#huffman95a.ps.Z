URL: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume3/huffman95a.ps.Z
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/huffman95a.html
Root-URL: 
Email: huffman@tc.pw.com  laird@eecs.umich.edu  
Title: Flexibly Instructable Agents  
Author: Scott B. Huffman Price Waterhouse John E. Laird 
Address: 68 Willow Road Menlo Park, CA 94025 USA  1101 Beal Ave. Ann Arbor, MI 48109-2110 USA  
Affiliation: Technology Centre,  Artificial Intelligence Laboratory The University of Michigan,  
Note: Journal of Artificial Intelligence Research 3 (1995) 271-324 Submitted 2/95; published 11/95  
Abstract: This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a flexible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this flexibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets three key requirements of flexible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks.
Abstract-found: 1
Intro-found: 1
Reference: <author> Akatsuka, N. </author> <year> (1986). </year> <title> Conditionals are discourse-bound. </title> <editor> In Traugott, E. C. (Ed.), </editor> <booktitle> On Conditionals, </booktitle> <pages> pp. 333-51. </pages> <publisher> Cambridge Univ. Press, </publisher> <address> Cambridge. </address>
Reference-contexts: General learning from specific cases. The agent is instructed in a particular situation, but is expected to learn general knowledge that will apply in sufficiently similar situations. 5. Some types of conditionals do not follow this pattern <ref> (Akatsuka, 1986) </ref>, but they are not relevant to tutorial instruction. 278 Flexibly Instructable Agents T 2 . Fast learning. An instructable agent is expected to learn new procedures quickly. Typically, a task should only have to be taught once. T 3 . Maximal use of prior knowledge.
Reference: <author> Alterman, R., Zito-Wolf, R., & Carpenter, T. </author> <year> (1991). </year> <title> Interaction, comprehension, and instruction usage. </title> <journal> Journal of the Learning Sciences, </journal> <volume> 1 (3&4), </volume> <pages> 273-318. </pages>
Reference: <author> Anderson, J. R. </author> <year> (1983). </year> <title> The architecture of cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Huffman & Laird Bergadano, F., & Giordana, A. </author> <year> (1988). </year> <title> A knowledge intensive approach to concept induction. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pp. 305-317. </pages>
Reference: <author> Birmingham, W., & Klinker, G. </author> <year> (1993). </year> <title> Knowledge acquisition tools with explicit problem-solving methods. </title> <journal> The Knowledge Engineering Review, </journal> <volume> 8 (1). </volume>
Reference: <author> Birmingham, W., & Siewiorek, D. </author> <year> (1989). </year> <title> Automated knowledge acquisition for a computer hardware synthesis system. </title> <journal> Knowledge Acquisition, </journal> <volume> 1, </volume> <pages> 321-340. </pages>
Reference: <author> Bloom, B. S. </author> <year> (1984). </year> <title> The 2 sigma problem: The search for methods of group instruction as effective as one-to-one tutoring. </title> <journal> Educational Researcher, </journal> <volume> 13 (6), </volume> <pages> 4-16. </pages>
Reference-contexts: While working on tasks, a student may receive instruction as needed to complete tasks or to understand aspects of the domain or of previous instructions. This situated, interactive form of instruction produces very strong human learning <ref> (Bloom, 1984) </ref>. Although it has c fl1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Huffman & Laird received little attention in AI, it has the potential to be a powerful knowledge source for artificial agents as well.
Reference: <author> Brachman, R. J. </author> <year> (1980). </year> <title> An introduction to KL-ONE. </title> <editor> In Brachman, R. J. (Ed.), </editor> <booktitle> Research in Natural Language Understanding, </booktitle> <pages> pp. 13-46. </pages> <institution> Bolt, Beranek and Newman Inc., </institution> <address> Cambridge, MA. </address>
Reference-contexts: Since one of our goals is to identify an agent's knowledge types, it might appear that selecting a theory of knowledge representation would be more appropriate than selecting a computational model. Such theories define the functions and structures used to represent knowledge <ref> (e.g., KL-ONE, Brachman, 1980) </ref>; some also define the possible content of those structures (e.g., conceptual dependency theory, Schank, 1975; CYC, Guha & Lenat, 1990). However, computational structure must be added to these theories to produce working agents.
Reference: <author> Carbonell, J. G., & Gil, Y. </author> <year> (1987). </year> <title> Learning by experimentation. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pp. 256-265. </pages>
Reference: <author> Carbonell, J. G., Michalski, R. S., & Mitchell, T. M. </author> <year> (1983). </year> <title> An overview of machine learning. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning: An artificial intelligence approach. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Carpenter, T., & Alterman, R. </author> <year> (1994). </year> <title> A reading agent. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, </booktitle> <address> WA. </address>
Reference-contexts: In non-interactive instruction, the content and flow of information to the student is controlled primarily by the information source. Examples include classroom lectures, instruction manuals, and textbooks. One issue in using this type of instruction is locating and extracting the information that is needed for particular problems <ref> (Carpenter & Alterman, 1994) </ref>. Non-interactive instruction can contain both situated information (e.g., worked-out example problems, Chi et al., 1989; VanLehn, 1987) and unsituated information (e.g., general expository text). Unsituated instruction conveys general or abstract knowledge that can be applied in a large number of different situations.
Reference: <author> Chapman, D. </author> <year> (1990). </year> <title> Vision, Instruction, and Action. </title> <type> Ph.D. thesis, </type> <institution> Massachusetts Institute of Technology, Artificial Intelligence Laboratory. </institution>
Reference-contexts: SHRDLU (Winograd, 1972) performed natural language commands and did a small amount of rote learning - e.g., learning new goal specifications by directly transforming sentences into state descriptions. More recent systems that act in response to language (concentrating on the mapping problem) but do only minimal learning include SONJA <ref> (Chapman, 1990) </ref>, AnimNL (DiEugenio & Webber, 1992), and Homer (Vere & Bickmore, 1990). Some recent work has focused more on learning from situated natural language instructions.
Reference: <author> Chi, M. T. H., Bassok, M., Lewis, M. W., Reimann, P., & Glaser, R. </author> <year> (1989). </year> <title> Self-explanations: How students study and use examples in learning to solve problems. </title> <journal> Cognitive Science, </journal> <volume> 13, </volume> <pages> 145-182. </pages>
Reference: <author> Cypher, A. (Ed.). </author> <year> (1993). </year> <title> Watch what I do: Programming by demonstration. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: These systems do not target learning from flexible interactive instructions or types of instructions other than imperatives, however. The bulk of work on learning from instruction-like input has been under the rubric of learning apprentice systems (LASs), and closely related programming-by-demonstration (PbD) systems <ref> (Cypher, 1993) </ref> as employed, for instance, in recent work on learning within software agents (Dent et al., 1992; Maes, 1994; Maes & Kozierok, 1993; Mitchell, Caruana, Freitag, McDermott, & Zabowski, 1994). <p> Each LAS has learned particular types of knowledge: e.g., operator implementations (Mitchell et al., 1990), goal decomposition rules (Kodratoff & Tecuci, 1987b), operational versions of functional goals (Segre, 1987), control knowledge and control features (Gruber, 1989), procedure schemas (a combination of goal decomposition and control knowledge) (VanLehn, 1987), useful macro-operations <ref> (Cypher, 1993) </ref>, heuristic classification knowledge (Porter et al., 1990; Wilkins, 1990), etc. Tutorial instruction is a more flexible type of instruction than that supported by past LASs, for three reasons.
Reference: <author> Davis, R. </author> <year> (1979). </year> <title> Interactive transfer of expertise: Acquisition of new inference rules. </title> <journal> Artificial Intelligence, </journal> <volume> 12 (2), </volume> <pages> 409-427. </pages>
Reference: <author> DeJong, G. F., & Mooney, R. J. </author> <year> (1986). </year> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1 (2), </volume> <pages> 145-176. </pages>
Reference: <author> Dent, L., Boticario, J., McDermott, J., Mitchell, T., & Zabowski, D. </author> <year> (1992). </year> <title> A personal learning apprentice. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> DiEugenio, B. </author> <year> (1993). </year> <title> Understanding natural language instructions: A computational approach to purpose clauses. </title> <type> Ph.D. thesis, </type> <institution> University of Pennsylvania. </institution> <note> IRCS Report 93-52. </note> <editor> Flexibly Instructable Agents DiEugenio, B., & Webber, B. </editor> <year> (1992). </year> <title> Plan recognition in understanding instructions. </title> <editor> In Hendler, J. (Ed.), </editor> <booktitle> Proceedings of the First International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> pp. </pages> <address> 52-61 College Park, MD. </address>
Reference-contexts: The agent is not meant to carry out these instructions immediately (as an implicitly situated instruction), but rather when a situation arises that is like the one specified. Examples include conditionals and instructions with purpose clauses <ref> (DiEugenio, 1993) </ref>, such as the following: 4 * When using chocolate chips, add them to coconut mixture just before pressing into pie pan. * To restart this, you can hit R or shift-R. * When you get to the interval that you want, you just center up the joystick again. 4. <p> any features of the current situation that are needed to carry out the instruction. 13 This hypothetical situation is used as the context for a situated explanation of the instruction. 7.1 Hypothetical Goals and Learning Effects of Operators A goal is explicitly specified in an instruction by a purpose clause <ref> (DiEugenio, 1993) </ref>: "To do X, do Y." The basic knowledge to be learned from such an instruction is an operator proposal rule for doing Y when the goal is to achieve X. Consider this example from Instructo-Soar's domain: &gt; To turn on the light, push the red button.
Reference: <author> Donoho, S. K., & Wilkins, D. C. </author> <year> (1994). </year> <title> Exploiting the ordering of observed problem-solving steps for knowledge ase refinement: An apprenticeship approach. </title> <booktitle> In Proceedings of the 12th National Conference on Artifical Intelligence Seattle, </booktitle> <address> WA. </address>
Reference: <author> Drummond, M. </author> <year> (1989). </year> <title> Situated control rules. </title> <booktitle> In Proceedings of the First International Conference on Principles of Knowledge Representation Toronto, </booktitle> <address> Canada. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Emihovich, C., & Miller, G. E. </author> <year> (1988). </year> <title> Talking to the turtle: A discourse analysis of Logo instruction. </title> <booktitle> Discourse Processes, </booktitle> <volume> 11, </volume> <pages> 183-201. </pages>
Reference-contexts: One study indicates that teacher initiation is more prevalent early in instruction; student initiation increases as the student learns more, and then drops off again as the student masters the task <ref> (Emihovich & Miller, 1988) </ref>. Instructor-initiated instruction is difficult to support because instruction events can interrupt the agent's ongoing processing. Upon interrupting the agent, an instruction event may alter the agent's knowledge in a way that could change or invalidate the reasoning in which the agent was previously engaged.
Reference: <author> Eshelman, L., Ehret, D., McDermott, J., & Tan, M. </author> <year> (1987). </year> <title> MOLE: A tenacious knowledge-acquisition tool. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 26 (1), </volume> <pages> 41-54. </pages>
Reference: <author> Fikes, R. E., Hart, P. E., & Nilsson, N. J. </author> <year> (1972). </year> <title> Learning and executing generalized robot plans. </title> <journal> Artificial Intelligence, </journal> <volume> 3, </volume> <pages> 251-288. </pages>
Reference-contexts: It progresses toward its goals by sequentially applying operators to the current state. Operators transform the state, and may produce motor commands. In PSCM, operators can be more powerful than simple STRIPS operators <ref> (Fikes, Hart, & Nilsson, 1972) </ref>, because they can perform arbitrary computation (e.g., they 282 Flexibly Instructable Agents squares, states; arrows, operators; and ovals, impasses. can include conditional effects, multiple substeps, reactivity to different situations, etc.).
Reference: <author> Ford, C. A., & Thompson, S. A. </author> <year> (1986). </year> <title> Conditionals in discourse: A text-based study from English. </title> <editor> In Traugott, E. C. (Ed.), </editor> <booktitle> On Conditionals, </booktitle> <pages> pp. 353-72. </pages> <publisher> Cambridge Univ. Press, </publisher> <address> Cambridge. </address>
Reference: <author> Frederking, R. E. </author> <year> (1988). </year> <title> Integrated natural language dialogue: A computational model. </title> <publisher> Kluwer Academic Press, </publisher> <address> Boston. </address>
Reference: <author> Golding, A., Rosenbloom, P. S., & Laird, J. E. </author> <year> (1987). </year> <title> Learning search control from outside guidance. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 334-337. </pages>
Reference: <author> Grosz, B. J. </author> <year> (1977). </year> <title> The Representation and use of focus in dialogue understanding. </title> <type> Ph.D. thesis, </type> <institution> University of California, Berkeley. </institution>
Reference-contexts: It is situated in that it applies to particular task situations that arise in the domain. It is interactive in that the agent may request instruction as needed. This type of instruction is common in task-oriented dialogues between experts and apprentices <ref> (Grosz, 1977) </ref>. An example of tutorial instruction given to Instructo-Soar in a robotic domain is shown in Figure 1. Tutorial instruction has a number of properties that make it flexible and easy for the instructor to produce: P1. Situation specificity. Instructions are given for particular tasks in particular situations. <p> In Instructo-Soar, rote learning occurs as a side effect of language comprehension. While reading each sentence, the agent learns a set of rules that encode the sentence's semantic features. The rules allow NL-Soar to resolve referents in later sentences, implementing a simple version of Grosz's focus space mechanism <ref> (Grosz, 1977) </ref>. The rules record each instruction, indexed by the goal to which it applies and its place in the instruction sequence. The result is essentially an episodic case that records the specific, lock-step sequence of the instructions given to perform the new operator.
Reference: <author> Gruber, T. </author> <year> (1989). </year> <title> Automated knowledge acquisition for strategic knowledge. </title> <booktitle> Machine Learning, </booktitle> <pages> 4 (3-4), 293-336. </pages>
Reference-contexts: Each LAS has learned particular types of knowledge: e.g., operator implementations (Mitchell et al., 1990), goal decomposition rules (Kodratoff & Tecuci, 1987b), operational versions of functional goals (Segre, 1987), control knowledge and control features <ref> (Gruber, 1989) </ref>, procedure schemas (a combination of goal decomposition and control knowledge) (VanLehn, 1987), useful macro-operations (Cypher, 1993), heuristic classification knowledge (Porter et al., 1990; Wilkins, 1990), etc. Tutorial instruction is a more flexible type of instruction than that supported by past LASs, for three reasons.
Reference: <author> Guha, R. V., & Lenat, D. B. </author> <year> (1990). </year> <title> Cyc: A mid-term report. </title> <journal> AI Magazine, </journal> <volume> 11 (3), </volume> <pages> 32-59. </pages>
Reference: <author> Haas, N., & Hendrix, G. G. </author> <year> (1983). </year> <title> Learning by being told: Acquiring knowledge for information management. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning: An artificial intelligence approach. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Haiman, J. </author> <year> (1978). </year> <title> Conditionals are topics. </title> <booktitle> Language, </booktitle> <volume> 54, </volume> <pages> 564-89. </pages>
Reference: <author> Hall, R. J. </author> <year> (1988). </year> <title> Learning by failing to explain. </title> <journal> Machine Learning, </journal> <volume> 3 (1), </volume> <pages> 45-77. </pages>
Reference: <author> Hayes-Roth, F., Klahr, P., & Mostow, D. J. </author> <year> (1981). </year> <title> Advice taking and knowledge refinement: An iterative view of skill acquisition. </title> <editor> In Anderson, J. R. (Ed.), </editor> <booktitle> Cognitive skills and their acquisition, </booktitle> <pages> pp. 231-253. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address> <note> 319 Huffman & Laird Huffman, </note> <author> S. B. </author> <year> (1994). </year> <title> Instructable autonomous agents. </title> <type> Ph.D. thesis, </type> <institution> University of Michi-gan, Dept. of Electrical Engineering and Computer Science. </institution>
Reference: <author> Huffman, S. B., & Laird, J. E. </author> <year> (1992). </year> <title> Dimensions of complexity in learning from interactive instruction. </title> <editor> In Erickson, J. (Ed.), </editor> <booktitle> Proceedings of Cooperative Intelligent Robotics in Space III, SPIE Volume 1829. </booktitle>
Reference-contexts: Instructions that apply to the current situation, such as imperative commands (e.g., "Move to the yellow table"), are called implicitly situated <ref> (Huffman & Laird, 1992) </ref>. Since the instruction itself says nothing about the situation to which it should be applied, the current situation (the task being performed and the current state) is implied. <p> Since the instruction itself says nothing about the situation to which it should be applied, the current situation (the task being performed and the current state) is implied. In contrast, instructions that specify elements of the situation to which they are meant to apply are explicitly situated <ref> (Huffman & Laird, 1992) </ref>. The agent is not meant to carry out these instructions immediately (as an implicitly situated instruction), but rather when a situation arises that is like the one specified.
Reference: <author> Huffman, S. B., & Laird, J. E. </author> <year> (1993). </year> <title> Learning procedures from interactive natural language instructions. </title> <editor> In Utgoff, P. (Ed.), </editor> <booktitle> Machine Learning: Proceedings of the Tenth International Conference. </booktitle>
Reference-contexts: It has complete, noiseless perception of its world, and can recognize a set of basic object properties (e.g., type, color, size) and relationships (e.g., robot docked-at table, 8. The techniques have also been applied in a limited way to a flight domain <ref> (Pearson, Huffman, Willis, Laird, & Jones, 1993) </ref>, in which Soar controls a flight simulator and instructions are given for taking off. 291 Huffman & Laird gripper holding object, objects above, directly-above, left-of, right-of one another). The set of properties and relations can be extended through instruction, as described below.
Reference: <author> Huffman, S. B., & Laird, J. E. </author> <year> (1994). </year> <title> Learning from highly flexible tutorial instruction. </title> <booktitle> In Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94) Seattle, </booktitle> <address> WA. </address>
Reference-contexts: A mathematical analysis <ref> (Huffman, 1994) </ref> revealed that the number of possible sequences of instructions that can be used to teach a given procedure grows exponentially with the number of actions in the procedure. <p> In addition, the agent only recalls a single instruction to internally project at a time. After the recalled 10. We also implemented a lazy/complete recall strategy, which will not be described here <ref> (see Huffman, 1994, for details) </ref>. 297 Huffman & Laird ing of errors in domain knowledge. operator is projected, the agent applies whatever general knowledge it has about the rest of the implementation of the new operator. This general knowledge, however, does not include rote memories of other past instructions. <p> If the agent knew that pushing the red button toggles the light, then in the projection, the light would come on. Thus, the explanation would succeed, and a general operator 13. See <ref> (Huffman, 1994) </ref> for details of how these features are determined. 304 Flexibly Instructable Agents proposal rule would be learned, that proposed pushing the red button when the light is off and the goal is to turn it on. <p> Rather than an experimental measurement, we performed a mathematical analysis. The analysis showed that due to command flexibility, the number of instruction sequences that can be used to teach a given procedure is very large, growing exponentially with the number of primitive steps in the procedure <ref> (Huffman, 1994) </ref>. D. Performance on a known hard problem: Since learning from tutorial instruction has not been extensively studied in machine learning, there are no standard, difficult problems. We created a comprehensive instruction scenario by crossing the command flexibility, situation flexibility, and knowledge-type flexibility requirements. <p> D. Performance on a known hard problem: Since learning from tutorial instruction has not been extensively studied in machine learning, there are no standard, difficult problems. We created a comprehensive instruction scenario by crossing the command flexibility, situation flexibility, and knowledge-type flexibility requirements. The scenario, described in detail in <ref> (Huffman, 1994) </ref>, contains 100 instructions and demonstrates 17 of Instructo-Soar's 18 instructional capabilities from Table 6 (it does not include learning indifference in selecting between two operators).
Reference: <author> Huffman, S. B., Miller, C. S., & Laird, J. E. </author> <year> (1993). </year> <title> Learning from instruction: A knowledge-level capability within a unified theory of cognition. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. 114-119. </pages>
Reference-contexts: It has complete, noiseless perception of its world, and can recognize a set of basic object properties (e.g., type, color, size) and relationships (e.g., robot docked-at table, 8. The techniques have also been applied in a limited way to a flight domain <ref> (Pearson, Huffman, Willis, Laird, & Jones, 1993) </ref>, in which Soar controls a flight simulator and instructions are given for taking off. 291 Huffman & Laird gripper holding object, objects above, directly-above, left-of, right-of one another). The set of properties and relations can be extended through instruction, as described below.
Reference: <author> Johnson-Laird, P. N. </author> <year> (1986). </year> <title> Conditionals and mental models. </title> <editor> In Traugott, E. C. (Ed.), </editor> <booktitle> On Conditionals. </booktitle> <publisher> Cambridge Univ. Press, </publisher> <address> Cambridge. </address>
Reference: <author> Jones, R. M., Tambe, M., Laird, J. E., & Rosenbloom, P. S. </author> <year> (1993). </year> <title> Intelligent automated agents for flight training simulators. </title> <booktitle> In Proceedings of the Third Conference on Computer Generated Forces, </booktitle> <pages> pp. </pages> <address> 33-42 Orlando, FL. </address>
Reference-contexts: It has complete, noiseless perception of its world, and can recognize a set of basic object properties (e.g., type, color, size) and relationships (e.g., robot docked-at table, 8. The techniques have also been applied in a limited way to a flight domain <ref> (Pearson, Huffman, Willis, Laird, & Jones, 1993) </ref>, in which Soar controls a flight simulator and instructions are given for taking off. 291 Huffman & Laird gripper holding object, objects above, directly-above, left-of, right-of one another). The set of properties and relations can be extended through instruction, as described below.
Reference: <author> Just, M. A., & Carpenter, P. A. </author> <year> (1976). </year> <title> Verbal comprehension in instructional situations. </title> <editor> In Klahr, D. (Ed.), </editor> <title> Cognition and Instruction. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Kieras, D. E., & Bovair, S. </author> <year> (1984). </year> <title> The role of a mental model in learning to operate a device. </title> <journal> Cognitive Science, </journal> <volume> 8, </volume> <pages> 255-273. </pages>
Reference-contexts: In a more complex domain, inferring implementation rules would be even less successful. Not surprisingly, psychological research shows that human subjects' learning from procedural instructions also degrades if they lack domain knowledge <ref> (Kieras & Bovair, 1984) </ref>.
Reference: <editor> Kodratoff, Y., & Tecuci, G. </editor> <year> (1987a). </year> <title> DISCIPLE-1: Interactive apprentice system in weak theory fields. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 271-273. </pages>
Reference: <editor> Kodratoff, Y., & Tecuci, G. </editor> <year> (1987b). </year> <title> Techniques of design and DISCIPLE learning apprentice. </title> <journal> International Journal of Expert Systems, </journal> <volume> 1 (1), </volume> <pages> 39-66. </pages>
Reference-contexts: Agent-initiated instruction can be directed in (at least) two possible ways: by verification or by impasses. Some learning apprentice systems, such as LEAP (Mitchell et al., 1990) and DISCIPLE <ref> (Kodratoff & Tecuci, 1987b) </ref> ask the instructor to verify or alter each reasoning step. The advantage of this approach is that each step is examined by the instructor; the disadvantage, of course, is that each step must be examined. <p> Each LAS has learned particular types of knowledge: e.g., operator implementations (Mitchell et al., 1990), goal decomposition rules <ref> (Kodratoff & Tecuci, 1987b) </ref>, operational versions of functional goals (Segre, 1987), control knowledge and control features (Gruber, 1989), procedure schemas (a combination of goal decomposition and control knowledge) (VanLehn, 1987), useful macro-operations (Cypher, 1993), heuristic classification knowledge (Porter et al., 1990; Wilkins, 1990), etc.
Reference: <author> Laird, J. E., Congdon, C. B., Altmann, E., & Doorenbos, R. </author> <year> (1993). </year> <note> Soar user's manual, version 6.. </note>
Reference-contexts: It has complete, noiseless perception of its world, and can recognize a set of basic object properties (e.g., type, color, size) and relationships (e.g., robot docked-at table, 8. The techniques have also been applied in a limited way to a flight domain <ref> (Pearson, Huffman, Willis, Laird, & Jones, 1993) </ref>, in which Soar controls a flight simulator and instructions are given for taking off. 291 Huffman & Laird gripper holding object, objects above, directly-above, left-of, right-of one another). The set of properties and relations can be extended through instruction, as described below.
Reference: <author> Laird, J. E., Hucka, M., Yager, E. S., & Tuck, C. M. </author> <year> (1990). </year> <title> Correcting and extending domain knowledge using outside guidance. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning. </booktitle>
Reference: <author> Laird, J. E., Newell, A., & Rosenbloom, P. S. </author> <year> (1987). </year> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 (1), </volume> <pages> 1-64. </pages> <note> 320 Flexibly Instructable Agents Laird, </note> <author> J. E., & Rosenbloom, P. S. </author> <year> (1990). </year> <title> Integrating execution, planning, and learning in Soar for external environments. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 1022-1029. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: It specifies an agent in terms of computation within problem spaces, without reference to the symbol level structures used for implementation. Because its components approximate the knowledge level (Newell et al., 1990), the PSCM is an apt choice for identifying an agent's knowledge types. Soar <ref> (Laird, Newell, & Rosen-bloom, 1987) </ref> is a symbol level implementation of the PSCM. A schematic of a PSCM agent is shown in Figure 2. Perception and motor modules connect the agent to the external environment.
Reference: <author> Lewis, C. </author> <year> (1988). </year> <title> Why and how to learn why: Analysis-based generalization of procedures. </title> <journal> Cognitive Science, </journal> <volume> 12, </volume> <pages> 211-256. </pages>
Reference: <author> Lewis, R. L. </author> <year> (1993). </year> <title> An Architecturally-Based Theory of Human Sentence Comprehension. </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University, School of Computer Science. </institution>
Reference-contexts: A red button toggles the light on or off; a green button toggles it dim or bright, when it is on. Instructo-Soar consists of a set of problem spaces within Soar that contain three main categories of knowledge: natural language processing knowledge, originally developed for NL-Soar <ref> (Lewis, 1993) </ref>; knowledge about obtaining and using instruction; and knowledge of the task domain itself. This task knowledge is extended through learning from instruction.
Reference: <author> Lewis, R. L., Newell, A., & Polk, T. A. </author> <year> (1989). </year> <title> Toward a Soar theory of taking instructions for immediate reasoning tasks. </title> <booktitle> In Proceedings of the Annual Conference of the Cognitive Science Society. </booktitle>
Reference: <author> Lindsay, R. K. </author> <year> (1963). </year> <title> Inferential memory as the basis of machines which understand natural language. </title> <editor> In Feigenbaum, E. A., & Feldman, J. (Eds.), </editor> <booktitle> Computers and Thought, </booktitle> <pages> pp. 217-233. </pages> <editor> R. </editor> <publisher> Oldenbourg KG. </publisher>
Reference: <author> Maes, P. </author> <year> (1994). </year> <title> Agents that reduce work and information overload. </title> <journal> Communications of the ACM, </journal> <volume> 37 (7). </volume>
Reference: <author> Maes, P., & Kozierok, R. </author> <year> (1993). </year> <title> Learning interface agents. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pp. 459-465. </pages>
Reference: <author> Mann, W. C., & Thompson, S. A. </author> <year> (1988). </year> <title> Rhetorical structure theory: Toward a functional theory of text organization. </title> <booktitle> Text, </booktitle> <volume> 8 (3), </volume> <pages> 243-281. </pages>
Reference-contexts: Similar to the notion of discourse coherence <ref> (Mann & Thompson, 1988) </ref>, a fully flexible tutorable agent needs to support any instruction event with knowledge coherence; that is, any instruction event delivering knowledge that makes sense in the current context.
Reference: <author> Marcus, S., & McDermott, J. </author> <year> (1989). </year> <title> SALT: A knowledge acquisition language for propose-and-revise systems. </title> <journal> Artificial Intelligence, </journal> <volume> 39 (1), </volume> <pages> 1-37. </pages>
Reference: <author> Martin, C. E., & Firby, R. J. </author> <year> (1991). </year> <title> Generating natural language expectations from a reactive execution system. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. 811-815. </pages>
Reference: <author> McCarthy, J. </author> <year> (1968). </year> <title> The advice taker. </title> <editor> In Minsky, M. (Ed.), </editor> <booktitle> Semantic Information Processing, </booktitle> <pages> pp. 403-410. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference: <author> Miller, C. M. </author> <year> (1993). </year> <title> A model of concept acquisition in the context of a unified theory of cognition. </title> <type> Ph.D. thesis, </type> <institution> The University of Michigan, Dept. of Computer Science and Electrical Engineering. </institution>
Reference: <author> Minton, S., Carbonell, J. G., Knoblock, C. A., Kuokka, D. R., Etzioni, O., & Gil, Y. </author> <year> (1989). </year> <title> Explanation-based learning: A problem-solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 63-118. </pages>
Reference: <author> Mitchell, T., Caruana, R., Freitag, D., McDermott, J., & Zabowski, D. </author> <year> (1994). </year> <title> Experience with a learning personal assistant. </title> <journal> Communications of the ACM, </journal> <volume> 37 (7). </volume>
Reference: <author> Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <note> 1. </note> <author> 321 Huffman & Laird Mitchell, T. M., Mahadevan, S., & Steinberg, L. I. </author> <year> (1990). </year> <title> LEAP: A learning apprentice system for VLSI design. </title> <editor> In Kodratoff, Y., & Michalski, R. S. (Eds.), </editor> <booktitle> Machine Learning: An artificial intelligence approach, Vol. III. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The operator is applied by reaching an impasse and selecting operators in a subgoal. Here, knowledge to apply the operator is selection knowledge (2, above) for the sub-operators. 4. Operator termination. An operator must be terminated when its application has been completed. The termination conditions, or goal concept <ref> (Mitchell et al., 1986) </ref>, of an operator indicate the state conditions that the operator is meant to achieve.
Reference: <author> Mooney, R. J. </author> <year> (1990). </year> <title> Learning plan schemata from observation: Explanation-based learning for plan recognition. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 483-509. </pages>
Reference: <author> Mostow, D. J. </author> <year> (1983). </year> <title> Learning by being told: Machine transformation of advice into a heuristic search procedure. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning: An artificial intelligence approach. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Musen, M. A. </author> <year> (1989). </year> <title> Automated support for building and extending expert models. </title> <booktitle> Machine Learning, </booktitle> <pages> 4 (3-4), 347-376. </pages>
Reference: <author> Newell, A. </author> <year> (1981). </year> <title> The knowledge level. </title> <journal> AI Magazine, </journal> <volume> 2 (2), </volume> <pages> 1-20. </pages>
Reference-contexts: If the agent does not know what an instructed action or subtask is, or how to perform it in the situation at hand, it will ask for further instruction. P5. Knowledge-level interaction. The instructor provides knowledge to the agent at the knowledge level <ref> (Newell, 1981) </ref>. That is, the instructions refer to objects and actions in the world, not to symbol-level structures (e.g., data structures) within the agent. <p> A computational model for a general instructable agent must meet two requirements: 1. Support of general computation/agenthood. 2. Close correspondence to the knowledge level. Because tutorial instructions provide knowledge at the knowledge level <ref> (Newell, 1981) </ref>, the further the CM com 281 Huffman & Laird ponents are from the knowledge level, the more difficult mapping and learning from instructions will be.
Reference: <author> Newell, A. </author> <year> (1990). </year> <title> Unified Theories of Cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: We first describe the computational model and then the learning framework. 5.1 The Problem Space Computational Model A computational model (CM) is a "a set of operations on entities that can be interpreted in computational terms" <ref> (Newell et al., 1990, p. 6) </ref>. A computational model for a general instructable agent must meet two requirements: 1. Support of general computation/agenthood. 2. Close correspondence to the knowledge level. <p> Thus, these models are not appropriate as the top-level CM for an instructable agent. However, because higher levels of description of a computational system are implemented by lower levels <ref> (Newell, 1990) </ref>, these CMs might be used as the implementation substrate for the higher level CM of an instructable agent. Another alternative is logic, which has entities that are well matched to the knowledge level (e.g., propositions, well-formed formulas). <p> It specifies an agent in terms of computation within problem spaces, without reference to the symbol level structures used for implementation. Because its components approximate the knowledge level <ref> (Newell et al., 1990) </ref>, the PSCM is an apt choice for identifying an agent's knowledge types. Soar (Laird, Newell, & Rosen-bloom, 1987) is a symbol level implementation of the PSCM. A schematic of a PSCM agent is shown in Figure 2.
Reference: <author> Newell, A., Yost, G., Laird, J. E., Rosenbloom, P. S., & Altmann, E. </author> <year> (1990). </year> <title> Formulating the problem space computational model. </title> <booktitle> In Proceedings of the 25th Anniversary Symposium, </booktitle> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: We first describe the computational model and then the learning framework. 5.1 The Problem Space Computational Model A computational model (CM) is a "a set of operations on entities that can be interpreted in computational terms" <ref> (Newell et al., 1990, p. 6) </ref>. A computational model for a general instructable agent must meet two requirements: 1. Support of general computation/agenthood. 2. Close correspondence to the knowledge level. <p> Thus, these models are not appropriate as the top-level CM for an instructable agent. However, because higher levels of description of a computational system are implemented by lower levels <ref> (Newell, 1990) </ref>, these CMs might be used as the implementation substrate for the higher level CM of an instructable agent. Another alternative is logic, which has entities that are well matched to the knowledge level (e.g., propositions, well-formed formulas). <p> It specifies an agent in terms of computation within problem spaces, without reference to the symbol level structures used for implementation. Because its components approximate the knowledge level <ref> (Newell et al., 1990) </ref>, the PSCM is an apt choice for identifying an agent's knowledge types. Soar (Laird, Newell, & Rosen-bloom, 1987) is a symbol level implementation of the PSCM. A schematic of a PSCM agent is shown in Figure 2.
Reference: <author> Pazzani, M. </author> <year> (1991a). </year> <title> A computational theory of learning causal relationships. </title> <journal> Cognitive Science, </journal> <volume> 15, </volume> <pages> 401-424. </pages>
Reference-contexts: The instructor is asked to verify this inference. 14 Once it is verified, Instructo-Soar heuristically guesses at the state conditions under which the effect will occur. It uses the OP-to-G-path heuristic as a very naive causality theory <ref> (Pazzani, 1991a) </ref> to guess at the causes of the inferred operator effect. Here, OP-toG-path notices that the light and the red button are both on the same table.
Reference: <author> Pazzani, M. </author> <year> (1991b). </year> <title> Learning to predict and explain: An integration of similarity-based, theory driven, and explanation-based learning. </title> <journal> Journal of the Learning Sciences, </journal> <volume> 1 (2), </volume> <pages> 153-199. </pages>
Reference-contexts: SWALE (Schank & Leake, 1989) uses an underlying theory of "anomalies" in explanations to complete incomplete explanations of events. OCCAM <ref> (Pazzani, 1991b) </ref> uses options O2 and O4 in a static order: 287 Huffman & Laird M K . It first attempts to fill in the gaps in an incomplete explanation inductively, biased by a naive theory; if that fails, it abandons explanation and falls back on correlational learning methods.
Reference: <author> Pearson, D. J., & Huffman, S. B. </author> <year> (1995). </year> <title> Combining learning from instruction with recovery from incorrect knowledge. </title> <editor> In Gordon, D., & Shavlik, J. (Eds.), </editor> <booktitle> Proceedings of the 1995 Machine Learning Workshop on Agents that Learn from Other Agents. </booktitle>
Reference-contexts: We have recently produced such an agent (Pearson & 18. We have recently added a simple interruptability capability to a new version of Instructo-Soar that incorporates recovery from incorrect knowledge <ref> (Pearson & Huffman, 1995) </ref>. 316 Flexibly Instructable Agents Huffman, 1995) that incorporates current research on incremental recovery from incorrect knowledge (Pearson & Laird, 1995). This agent learns to correct overgeneral knowledge that it infers when completing explanations of instructions. <p> We have recently added a simple interruptability capability to a new version of Instructo-Soar that incorporates recovery from incorrect knowledge (Pearson & Huffman, 1995). 316 Flexibly Instructable Agents Huffman, 1995) that incorporates current research on incremental recovery from incorrect knowledge <ref> (Pearson & Laird, 1995) </ref>. This agent learns to correct overgeneral knowledge that it infers when completing explanations of instructions. The correction process is triggered when using the overgeneral knowledge results in incorrect performance (e.g., an action that the agent expects to succeed does not).
Reference: <author> Pearson, D. J., Huffman, S. B., Willis, M. B., Laird, J. E., & Jones, R. M. </author> <year> (1993). </year> <title> A symbolic solution to intelligent real-time control. </title> <journal> IEEE Robotics and Autonomous Systems, </journal> <volume> 11, </volume> <pages> 279-291. </pages>
Reference-contexts: It has complete, noiseless perception of its world, and can recognize a set of basic object properties (e.g., type, color, size) and relationships (e.g., robot docked-at table, 8. The techniques have also been applied in a limited way to a flight domain <ref> (Pearson, Huffman, Willis, Laird, & Jones, 1993) </ref>, in which Soar controls a flight simulator and instructions are given for taking off. 291 Huffman & Laird gripper holding object, objects above, directly-above, left-of, right-of one another). The set of properties and relations can be extended through instruction, as described below. <p> This would not be sufficient for a dynamic domain such as flying an airplane, where multiple goals at multiple levels of granularity, involving both achievement and/or maintenance of conditions in the environment, may be active at once <ref> (Pearson et al., 1993) </ref>. Instructo-Soar's procedures are implemented by a series of locally decided steps, precluding instruction containing procedure-wide (i.e., nonlocal) path constraints (e.g., "Go to the other room, but don't walk on the carpeting!").
Reference: <author> Pearson, D. J., & Laird, J. E. </author> <year> (1995). </year> <title> Toward incremental knowledge correction for agents in complex environments. </title> <editor> In Muggleton, S., Michie, D., & Furukawa, K. (Eds.), </editor> <booktitle> Machine Intelligence, </booktitle> <volume> Vol. 15. </volume> <publisher> Oxford University Press. </publisher>
Reference-contexts: We have recently produced such an agent (Pearson & 18. We have recently added a simple interruptability capability to a new version of Instructo-Soar that incorporates recovery from incorrect knowledge <ref> (Pearson & Huffman, 1995) </ref>. 316 Flexibly Instructable Agents Huffman, 1995) that incorporates current research on incremental recovery from incorrect knowledge (Pearson & Laird, 1995). This agent learns to correct overgeneral knowledge that it infers when completing explanations of instructions. <p> We have recently added a simple interruptability capability to a new version of Instructo-Soar that incorporates recovery from incorrect knowledge (Pearson & Huffman, 1995). 316 Flexibly Instructable Agents Huffman, 1995) that incorporates current research on incremental recovery from incorrect knowledge <ref> (Pearson & Laird, 1995) </ref>. This agent learns to correct overgeneral knowledge that it infers when completing explanations of instructions. The correction process is triggered when using the overgeneral knowledge results in incorrect performance (e.g., an action that the agent expects to succeed does not).
Reference: <author> Porter, B. W., Bareiss, R., & Holte, R. C. </author> <year> (1990). </year> <title> Concept learning and heuristic classification in weak-theory domains. </title> <journal> Artificial Intelligence, </journal> <volume> 45 (3), </volume> <pages> 229-263. </pages>
Reference: <author> Porter, B. W., & Kibler, D. F. </author> <year> (1986). </year> <title> Experimental goal regression: A method for learning problem-solving heuristics. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 249-286. </pages>
Reference-contexts: It first attempts to fill in the gaps in an incomplete explanation inductively, biased by a naive theory; if that fails, it abandons explanation and falls back on correlational learning methods. PET <ref> (Porter & Kibler, 1986) </ref> is an example of a system that delays explanation of a reasoning step until it learns further knowledge (option O1). However, as indicated in Figure 4, an instructable agent has additional information available to it besides the incomplete explanation itself.
Reference: <author> Redmond, M. A. </author> <year> (1992). </year> <title> Learning by observing and understanding expert problem solving. </title> <type> Ph.D. thesis, </type> <institution> Georgia Institute of Technology. </institution> <note> 322 Flexibly Instructable Agents Rosenbloom, </note> <author> P. S., & Aasman, J. </author> <year> (1990). </year> <title> Knowledge level and inductive uses of chunking (EBL). </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Rosenbloom, P. S., & Laird, J. E. </author> <year> (1986). </year> <title> Mapping explanation-based generalization onto Soar. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pp. 561-567. </pages>
Reference-contexts: Depending on the type of result, chunks may correspond to any of the five types of PSCM knowledge. When similar situations arise in the future, chunks allow the impasse that caused the original subgoal to be avoided by producing their results directly. Chunking is a form of explanation-based learning <ref> (Rosenbloom & Laird, 1986) </ref>. Although it is a summarization mechanism, through taking both inductive and deductive steps in subgoals, chunking can produce both inductive and deductive learning (Miller, 1993; Rosenbloom & Aasman, 1990). Chunking occurs continuously, making learning a part of the ongoing activity of a Soar/PSCM agent. <p> This pattern continues back through the entire sequence until the full implementation is learned generally. As Figure 10 shows, the resulting learning curves closely approximate the power law of practice <ref> (Rosenbloom & Newell, 1986) </ref> (r = 0:98 for both (a) and (b)). * Effectiveness of hierarchical instruction. Due to the back-to-front effect, the agent learns a new procedure more quickly when its steps are taught using a hierarchical organization than when they are taught as a flat sequence.
Reference: <author> Rosenbloom, P. S., Laird, J. E., & Newell, A. </author> <year> (1988). </year> <title> The chunking of skill and knowledge. </title>
Reference: <editor> In Bouma, H., & Elsendoorn, A. G. (Eds.), </editor> <booktitle> Working Models of Human Perception, </booktitle> <pages> pp. 391-410. </pages> <publisher> Academic Press, </publisher> <address> London, England. </address>
Reference: <author> Rosenbloom, P. S., Laird, J. E., </author> & <title> Newell, </title> <editor> A. (Eds.). </editor> <booktitle> (1993a). The Soar Papers: Research on integrated intelligence. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference: <author> Rosenbloom, P. S., Laird, J. E., </author> & <title> Newell, </title> <editor> A. (Eds.). </editor> <booktitle> (1993b). The Soar Papers: Research on integrated intelligence. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: This section describes the system's basic performance when learning new procedures, and extending procedures to new situations, from imperative commands (implicitly situated instructions); the next describes learning other types of knowledge and handling explicitly situated instructions. 7. For an overview of Soar, and other systems built within it, see <ref> (Rosenbloom, Laird, & Newell, 1993b) </ref>. 290 Flexibly Instructable Agents 6.1 The Domain and the Agent's Initial Knowledge The primary domain to which Instructo-Soar has been applied is the simulated robotic world shown in Figure 5. 8 The agent is a simulated Hero robot, in a room with tables, buttons, blocks of
Reference: <author> Rosenbloom, P. S., & Newell, A. </author> <year> (1986). </year> <title> The chunking of goal hierarchies: A generalized model of practice. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning: An artificial intelligence approach, Volume II. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Depending on the type of result, chunks may correspond to any of the five types of PSCM knowledge. When similar situations arise in the future, chunks allow the impasse that caused the original subgoal to be avoided by producing their results directly. Chunking is a form of explanation-based learning <ref> (Rosenbloom & Laird, 1986) </ref>. Although it is a summarization mechanism, through taking both inductive and deductive steps in subgoals, chunking can produce both inductive and deductive learning (Miller, 1993; Rosenbloom & Aasman, 1990). Chunking occurs continuously, making learning a part of the ongoing activity of a Soar/PSCM agent. <p> This pattern continues back through the entire sequence until the full implementation is learned generally. As Figure 10 shows, the resulting learning curves closely approximate the power law of practice <ref> (Rosenbloom & Newell, 1986) </ref> (r = 0:98 for both (a) and (b)). * Effectiveness of hierarchical instruction. Due to the back-to-front effect, the agent learns a new procedure more quickly when its steps are taught using a hierarchical organization than when they are taught as a flat sequence.
Reference: <editor> Rumelhart, D. E., & McClelland, J. L. (Eds.). </editor> <year> (1986). </year> <title> Parallel distributed processing: Explorations in the microstructure of cognition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Standard programming languages (e.g., Lisp) and theoretical CMs like Turing machines and push-down automata support general computation, but their operations and constructs are at the symbol level, without direct correspondence to the knowledge level. Similarly, connectionist and neural network models of computation <ref> (e.g., Rumelhart & McClelland, 1986) </ref> employ (by design) computational operations and entities at a level far below the knowledge level. Thus, these models are not appropriate as the top-level CM for an instructable agent.
Reference: <author> Rychener, M. D. </author> <year> (1983). </year> <title> The instructible production system: A retrospective analysis. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning: An artificial intelligence approach, </booktitle> <pages> pp. 429-460. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sandberg, J., & Wielinga, B. </author> <year> (1991). </year> <title> How situated is cognition?. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 341-346. </pages>
Reference-contexts: For example, in physics class, students are taught that F = m a; this general equation applies in specific ways to a great variety of situations. The advantage of unsituated instruction is precisely this ability to compactly communicate abstract knowledge that is broadly applicable <ref> (Sandberg & Wielinga, 1991) </ref>. 313 Huffman & Laird However, to use such abstract knowledge, students must learn how it applies to specific situations (Singley & Anderson, 1989). 9.2 Limitations of the Agent An agent's inherent limitations constrain what it can be taught.
Reference: <author> Schank, R. C. </author> <year> (1975). </year> <title> Conceptual Information Processing. </title> <publisher> American Elsevier, </publisher> <address> New York. </address>
Reference: <author> Schank, R. C., & Leake, D. B. </author> <year> (1989). </year> <title> Creativity and learning in a case-based explainer. </title> <journal> Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 353-385. </pages>
Reference-contexts: SIERRA (VanLehn, 1987), for example, induces over multiple partially explained examples, and constrains the induction by requiring that each of the examples is unexplainable because of the same piece of missing knowledge (the same disjunct, in SIERRA's terminology). SWALE <ref> (Schank & Leake, 1989) </ref> uses an underlying theory of "anomalies" in explanations to complete incomplete explanations of events. OCCAM (Pazzani, 1991b) uses options O2 and O4 in a static order: 287 Huffman & Laird M K .
Reference: <author> Segre, A. M. </author> <year> (1987). </year> <title> A learning apprentice system for mechanical assembly. </title> <booktitle> In Third IEEE Conference on Artificial Intelligence for Applications, </booktitle> <pages> pp. 112-117. </pages>
Reference-contexts: Each LAS has learned particular types of knowledge: e.g., operator implementations (Mitchell et al., 1990), goal decomposition rules (Kodratoff & Tecuci, 1987b), operational versions of functional goals <ref> (Segre, 1987) </ref>, control knowledge and control features (Gruber, 1989), procedure schemas (a combination of goal decomposition and control knowledge) (VanLehn, 1987), useful macro-operations (Cypher, 1993), heuristic classification knowledge (Porter et al., 1990; Wilkins, 1990), etc.
Reference: <author> Shen, W. </author> <year> (1993). </year> <title> Discovery as autonomous learning from the environment. </title> <journal> Machine Learning, </journal> <volume> 12, </volume> <pages> 143-165. </pages>
Reference: <author> Simon, H. A. </author> <year> (1977). </year> <title> Artificial intelligence systems that understand. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1059-1073. </pages>
Reference: <author> Simon, H. A., & Hayes, J. R. </author> <year> (1976). </year> <title> Understanding complex task instructions. </title> <editor> In Klahr, D. (Ed.), </editor> <title> Cognition and Instruction. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Singley, M. K., & Anderson, J. R. </author> <year> (1989). </year> <title> The transfer of cognitive skill. </title> <publisher> Harvard University Press. 323 Huffman & Laird Sutton, </publisher> <editor> R. S., & Pinette, B. </editor> <year> (1985). </year> <title> The learning of world models by connectionist networks. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. 54-64. </pages>
Reference-contexts: Unsituated instruction conveys general or abstract knowledge that can be applied in a large number of different situations. Such general-purpose knowledge is often described as "declarative" <ref> (Singley & Anderson, 1989) </ref>. For example, in physics class, students are taught that F = m a; this general equation applies in specific ways to a great variety of situations. <p> The advantage of unsituated instruction is precisely this ability to compactly communicate abstract knowledge that is broadly applicable (Sandberg & Wielinga, 1991). 313 Huffman & Laird However, to use such abstract knowledge, students must learn how it applies to specific situations <ref> (Singley & Anderson, 1989) </ref>. 9.2 Limitations of the Agent An agent's inherent limitations constrain what it can be taught.
Reference: <author> Thrun, S. B., & Mitchell, T. M. </author> <year> (1993). </year> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 930-936. </pages>
Reference: <author> VanLehn, K. </author> <year> (1987). </year> <title> Learning one subprocedure per lesson. </title> <journal> Artificial Intelligence, </journal> <volume> 31 (1), </volume> <pages> 1-40. </pages>
Reference-contexts: Each LAS has learned particular types of knowledge: e.g., operator implementations (Mitchell et al., 1990), goal decomposition rules (Kodratoff & Tecuci, 1987b), operational versions of functional goals (Segre, 1987), control knowledge and control features (Gruber, 1989), procedure schemas (a combination of goal decomposition and control knowledge) <ref> (VanLehn, 1987) </ref>, useful macro-operations (Cypher, 1993), heuristic classification knowledge (Porter et al., 1990; Wilkins, 1990), etc. Tutorial instruction is a more flexible type of instruction than that supported by past LASs, for three reasons. <p> Because of the difficulty of determining missing knowledge, these systems either base their induction on multiple examples, and/or bias the induction with an underlying theory or a teacher's help. SIERRA <ref> (VanLehn, 1987) </ref>, for example, induces over multiple partially explained examples, and constrains the induction by requiring that each of the examples is unexplainable because of the same piece of missing knowledge (the same disjunct, in SIERRA's terminology).
Reference: <author> VanLehn, K., Ball, W., & Kowalski, B. </author> <year> (1990). </year> <title> Explanation-based learning of correctness: Towards a model of the self-explanation effect. </title> <booktitle> In Proceedings of the 12th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. 717-724. </pages>
Reference: <author> VanLehn, K., & Jones, R. </author> <year> (1991). </year> <title> Learning physics via explanation-based learning of correctness and analogical search control. </title> <booktitle> In Proceedings of the International Machine Learning Workshop. </booktitle>
Reference-contexts: Explaining individual instructions is also supported by psychological results on the self-explanation effect, which have shown that subjects who self-explain instructional examples do so by re-deriving individual lines of the example. "Students virtually never reflect on the overall solution and try to recognize a plan that spans all the lines" <ref> (VanLehn and Jones, 1991, p. 111) </ref>. * Endpoints of explanation. The endpoints of the explanation a state S and a goal G to be achieved correspond to the situation that the instruction applies to.
Reference: <author> VanLehn, K., Jones, R. M., & Chi, M. T. H. </author> <year> (1992). </year> <title> A model of the self-explanation effect. </title> <journal> Journal of the Learning Sciences, </journal> <volume> 2 (1), </volume> <pages> 1-59. </pages>
Reference: <author> Vere, S., & Bickmore, T. </author> <year> (1990). </year> <title> A basic agent. </title> <journal> Computational Intelligence, </journal> <volume> 6, </volume> <pages> 41-60. </pages>
Reference-contexts: More recent systems that act in response to language (concentrating on the mapping problem) but do only minimal learning include SONJA (Chapman, 1990), AnimNL (DiEugenio & Webber, 1992), and Homer <ref> (Vere & Bickmore, 1990) </ref>. Some recent work has focused more on learning from situated natural language instructions. Martin and Firby (1991) discuss an approach to interpreting and learning from elliptical instructions (e.g., "Use the shovel") by matching the instruction to expectations generated from a task execution failure.
Reference: <author> Wertsch, J. V. </author> <year> (1979). </year> <title> From social interaction to higher psychological processes: A clarification and application of Vygotsky's theory. </title> <booktitle> Human Development, </booktitle> <volume> 22, </volume> <pages> 1-22. </pages>
Reference: <author> Widmer, G. </author> <year> (1989). </year> <title> A tight integration of deductive and inductive learning. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pp. 11-13. </pages>
Reference: <author> Wilkins, D. C. </author> <year> (1990). </year> <title> Knowledge base refinement as improving an incomplete and incorrect domain theory. </title> <editor> In Kodratoff, Y., & Michalski, R. S. (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume III, </booktitle> <pages> pp. 493-514. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Winograd, T. </author> <year> (1972). </year> <title> Understanding Natural Language. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: Other work has concentrated on behaving based on interactive natural language instructions. SHRDLU <ref> (Winograd, 1972) </ref> performed natural language commands and did a small amount of rote learning - e.g., learning new goal specifications by directly transforming sentences into state descriptions.
Reference: <author> Wood, D., Bruner, J. S., & Ross, G. </author> <year> (1976). </year> <title> The role of tutoring in problem solving. </title> <journal> Journal of Child Psychology and Psychiatry, </journal> <volume> 17, </volume> <pages> 89-100. </pages>
Reference-contexts: Newly learned operators may be included in the instructions for later operators, leading to learning of operator hierarchies. One hierarchy of operators learned by Instructo-Soar is shown in Figure 13. Learning procedural hierarchies has been identified as a fundamental component of children's skill acquisition from tutorial instruction <ref> (Wood, Bruner, & Ross, 1976) </ref>. In learning the hierarchy of Figure 13, Instructo-Soar learned four new operators, an extension of a known operator (move above), and an extension of a new operator (extending "pick up" to work if the robot already is holding a block).
Reference: <author> Yost, G. R. </author> <year> (1993). </year> <title> Acquiring knowledge in Soar. </title> <journal> IEEE Expert, </journal> <volume> 8 (3), </volume> <pages> 26-34. </pages>
Reference: <author> Yost, G. R., & Newell, A. </author> <year> (1989). </year> <title> A problem space approach to expert system specification. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 621-7. </pages>
Reference-contexts: These strategies either follow a fixed global regime or require an aggregation of information from multiple problem solving states to make control decisions. It is possible to produce a global control strategy using a combination of local decisions <ref> (Yost & Newell, 1989) </ref>. However, teaching a global method by casting it purely as a sequence of local decisions may be difficult. Other types of instruction, beyond the scope of this work, are required to teach global methods in a natural way.
References-found: 103

