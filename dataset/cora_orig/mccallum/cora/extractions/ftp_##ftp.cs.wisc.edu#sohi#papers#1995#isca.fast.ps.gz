URL: ftp://ftp.cs.wisc.edu/sohi/papers/1995/isca.fast.ps.gz
Refering-URL: http://www.cs.wisc.edu/~sohi/sohi.html
Root-URL: 
Email: faustin,pnevmati,sohig@cs.wisc.edu  
Title: Streamlining Data Cache Access with Fast Address Calculation  
Author: Todd M. Austin Dionisios N. Pnevmatikatos Gurindar S. Sohi 
Address: 1210 W. Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: For many programs, especially integer codes, untolerated load instruction latencies account for a significant portion of total execution time. In this paper, we present the design and evaluation of a fast address generation mechanism capable of eliminating the delays caused by effective address calculation for many loads and stores. Our approach works by predicting early in the pipeline (part of) the effective address of a memory access and using this predicted address to speculatively access the data cache. If the prediction is correct, the cache access is overlapped with non-speculative effective address calculation. Otherwise, the cache is accessed again in the following cycle, this time using the correct effective address. The impact on the cache access critical path is minimal; the prediction circuitry adds only a single OR operation before cache access can commence. In addition, verification of the predicted effective address is completely decoupled from the cache access critical path. Analyses of program reference behavior and subsequent performance analysis of this approach shows that this design is a good one, servicing enough accesses early enough to result in speedups for all the programs we tested. Our approach also responds well to software support, which can significantly reduce the number of mispredicted effective addresses, in many cases providing better program speedups and reducing cache bandwidth requirements. 
Abstract-found: 1
Intro-found: 1
Reference: [AS92] <author> Todd M. Austin and Gurindar S. Sohi. </author> <title> Dynamic dependency analysis of ordinary programs. </title> <booktitle> In Conference Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 342-351. </pages> <institution> Association for Computing Machinery, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: There are, however, limits to the extent to which existing approaches can reduce the impact of load latencies. Tolerating techniques require independent work, which is finite and usually quite small in the basic blocks of control intensive codes, e.g., many integer codes <ref> [AS92] </ref>. The current trend to increase processor issue widths further amplifies load latencies because exploitation of instruction level parallelism decreases the amount of work between load instructions. In addition, tolerating these latencies becomes more difficult since more independent instructions are required to fill pipeline delay slots.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: For general pointer accesses, most load offsets are small. In fact, for a number of programs we analyzed, e.g., Alvinn and Espresso, zero was the most common offset used. Zero offsets are primarily the product of array subscript operations where strength reduction <ref> [ASU86] </ref> of the subscript expression succeeded, pointer dereferences to basic types (e.g., integers), and pointer dereferences to the first element of a structured (record) variable. Non-zero offsets arise from primarily three sources: structure offsets, some array accesses, and array index constants.
Reference: [CCH + 87] <author> F. Chow, S. Correll, M. Himelstein, E. Killian, and L. Weber. </author> <title> How many addressing modes are enough. </title> <booktitle> Conference Proceedings of the Second International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 117-121, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Global pointer addressing is used to access small global (static) variables. The MIPS approach to global pointer addressing uses a reserved immutable register, called the global pointer, plus a constant offset to access variables in the global region of the program's data segment <ref> [CCH + 87] </ref>. The linker constructs the global region such that all variables referenced by name are grouped together near the target address of the global pointer. As shown in Table 1, global pointer addressing is prevalent in some programs, but not all.
Reference: [Fis81] <author> Joseph A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transaction on Computers, </journal> <volume> C-30(7):478-490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: The current trend to increase processor issue widths further amplifies load latencies because exploitation of instruction level parallelism decreases the amount of work between load instructions. In addition, tolerating these latencies becomes more difficult since more independent instructions are required to fill pipeline delay slots. Global scheduling techniques <ref> [Fis81, MLC + 92, ME92] </ref> have been developed as a way to mitigate this effect. Latency reduction in the form of register allocation is limited by the size and addressability of register files, forcing many program variables into memory.
Reference: [GM93] <author> Michael Golden and Trevor Mudge. </author> <title> Hardware support for hid ing cache latency. </title> <institution> Cse-tr-152-93, University of Michigan, Dept. of Electrical Engineering and Computer, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Comparing the speedup of the integer programs (19% with hardware and software support) to the speedup with a perfect cache (only 8%) reveals a perhaps more striking result fast address calculation consistently outperforms a perfect cache with 2 cycle loads. 6 Related Work Golden and Mudge <ref> [GM93] </ref> explored the use of a load target buffer (LTB) as a means of reducing load latencies. An LTB, loosely based on a branch target buffer, uses the address of a load instruction to predict the effective address early in the pipeline.
Reference: [GM94] <author> Michael Golden and Trevor Mudge. </author> <title> A comparison of two pipeline organizations. </title> <booktitle> Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <volume> 25(1) </volume> <pages> 153-161, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Hsu argues that the nature of floating point code (the target workload of the R8000) provides more parallelism between address calculation and loads than between loads and the use of their results. In a recent paper, Golden and Mudge <ref> [GM94] </ref> compare this pipeline organization (which they name AGI) with the traditional 5-stage pipeline (which they name LUI) to determine which is more capable of tolerating load latency.
Reference: [HP90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architec ture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: The entire 32 bit effective address is not ready until late into the cache access cycle, just in time for the address tag check. The idea of exploiting the two-dimensional structure of memory is being used in several other contexts, such as paged mode DRAM access <ref> [HP90] </ref>. A strong parallel to this work can be found in [KT91]. Katevenis and Tzartzanis propose a technique for reducing pipeline branch penalties by rearranging instructions so that both possible targets of a conditional branch are stored in a single I-cache line.
Reference: [Hsu94] <author> Peter Yan-Tek Hsu. </author> <title> Designing the TFP microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 14(2) </volume> <pages> 23-33, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The address-use hazard stalls the pipeline for one cycle if the computation of the base register value is immediately followed by a dependent load or store. The R8000 (TFP) processor <ref> [Hsu94] </ref> adopts this pipeline in an attempt to assist the instruction sched-uler in tolerating load delay latencies. Hsu argues that the nature of floating point code (the target workload of the R8000) provides more parallelism between address calculation and loads than between loads and the use of their results.
Reference: [Jou89] <author> Norman P. Jouppi. </author> <title> Architecture and organizational tradeoffs in the design of the MultiTitan CPU. </title> <booktitle> Proceedings of the 16st Annual International Symposium on Computer Architecture, </booktitle> <volume> 17(3) </volume> <pages> 281-289, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Steven proposes the use of an OR function for all effective address computation. Steven's approach was only intended as a method for speeding up stack accesses, all other accesses require additional instructions to explicitly compute effective addresses. The performance of this pipeline organization was not evaluated. Jouppi <ref> [Jou89] </ref> considers a notably different pipeline organization that has a separate address generation pipeline stage, and pushes the execution of ALU instructions to the same stage as cache access.
Reference: [Jou93] <author> Norman P. Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> Pro ceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <volume> 21(2) </volume> <pages> 191-201, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For designs that speculatively execute stores, care must be taken to ensure that a misspeculated store can be undone. For designs employing a store buffer <ref> [Jou93] </ref>, this may not pose a problem, as the approach uses a two-cycle store sequence. In the first cycle the tags are probed to see if the access hits in the cache, in the second (possibly much later cycle) the store is made to the cache.
Reference: [KH92] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: The architecture is functionally identical to the MIPS-I ISA <ref> [KH92] </ref>, except for the following differences: * extended addressing modes: register+register and post increment and decrement are included * no architected delay slots The addition of register+register addressing allowed us to more fairly evaluate the benefits of fast address calculation.
Reference: [KT91] <author> Manolis Katevenis and Nestoras Tzartzanis. </author> <title> Reducing the branch penalty by rearranging instructions in a double-width memory. </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> 19(2) </volume> <pages> 15-27, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The idea of exploiting the two-dimensional structure of memory is being used in several other contexts, such as paged mode DRAM access [HP90]. A strong parallel to this work can be found in <ref> [KT91] </ref>. Katevenis and Tzartzanis propose a technique for reducing pipeline branch penalties by rearranging instructions so that both possible targets of a conditional branch are stored in a single I-cache line. The high bandwidth of the I-cache is used to fetch both targets of a branch instruction.
Reference: [ME92] <author> Soo-Mook Moon and Kemal Ebcio glu. </author> <title> An efficient resource constrained global scheduling technique for superscalar and VLIW processors. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 55-71, </pages> <month> Decem-ber </month> <year> 1992. </year>
Reference-contexts: The current trend to increase processor issue widths further amplifies load latencies because exploitation of instruction level parallelism decreases the amount of work between load instructions. In addition, tolerating these latencies becomes more difficult since more independent instructions are required to fill pipeline delay slots. Global scheduling techniques <ref> [Fis81, MLC + 92, ME92] </ref> have been developed as a way to mitigate this effect. Latency reduction in the form of register allocation is limited by the size and addressability of register files, forcing many program variables into memory.
Reference: [MLC + 92] <author> Scott A. Mahlke, David C. Lin, William Y. Chen, Richard E. Hank, and Roger A. Bringmann. </author> <title> Effective compiler support for predicated execution using the hyperblock. </title> <booktitle> In Conference Record of the 25th Annual International Symposium on Mi-croarchitecture, </booktitle> <pages> pages 45-54, </pages> <address> Portland, OR, </address> <month> December </month> <year> 1992. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: The current trend to increase processor issue widths further amplifies load latencies because exploitation of instruction level parallelism decreases the amount of work between load instructions. In addition, tolerating these latencies becomes more difficult since more independent instructions are required to fill pipeline delay slots. Global scheduling techniques <ref> [Fis81, MLC + 92, ME92] </ref> have been developed as a way to mitigate this effect. Latency reduction in the form of register allocation is limited by the size and addressability of register files, forcing many program variables into memory.
Reference: [Sla94] <author> Michael Slater. </author> <title> AMD's K5 designed to outrun Pentium. </title> <journal> Mi croprocessor Report, </journal> <volume> 8(14) </volume> <pages> 1-11, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In spite of this observation, both pipelines still suffer from many untolerated load latencies. AMD's K5 processor <ref> [Sla94] </ref> overlaps a portion of effective address computation with cache access. The lower 11 bits of the effective address is computed in the cycle prior to cache access.
Reference: [SPE91] <institution> SPEC newsletter, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Table 2 details the programs we analyzed and their inputs. Fifteen of the analyzed benchmarks are from the SPEC92 benchmark suite <ref> [SPE91] </ref>.
Reference: [Ste88] <author> Gordon Steven. </author> <title> A novel effective address calculation mecha nism for RISC microprocessors. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 150-156, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Second, our approach is more accurate at predicting effective addresses because we predict using the operands of the effective address calculation, rather than the address of the load. In addition, we employ compile-time optimization to further improve performance. An earlier paper by Steven <ref> [Ste88] </ref> goes as far as proposing a 4-stage pipeline that eliminates the address generation stage and executes both memory accesses and ALU instructions in the same stage. Steven proposes the use of an OR function for all effective address computation.
Reference: [WJ94] <author> Steven J.E. Wilton and Norman P. Jouppi. </author> <title> An enhanced access and cycle time model for on-chip caches. </title> <type> Tech report 93/5, </type> <institution> DEC Western Research Lab, </institution> <year> 1994. </year>
Reference-contexts: To accomplish this task, we exploit an organizational property of on-chip data caches. To minimize access time, on-chip caches are organized as wide two-dimensional arrays of memory cells (as shown in Figure 4). Each row of the cache array typically contains one or more data blocks <ref> [WRP92, WJ94] </ref>. To access a word in the cache, the set index portion of the effective address is used to read an entire cache row from the data array and a tag value from the tag array.
Reference: [WRP92] <author> Tomohisa Wada, Suresh Rajan, and Steven A. Pyzybylski. </author> <title> An analytical access time model for on-chip cache memories. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 27(8) </volume> <pages> 1147-1156, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: To accomplish this task, we exploit an organizational property of on-chip data caches. To minimize access time, on-chip caches are organized as wide two-dimensional arrays of memory cells (as shown in Figure 4). Each row of the cache array typically contains one or more data blocks <ref> [WRP92, WJ94] </ref>. To access a word in the cache, the set index portion of the effective address is used to read an entire cache row from the data array and a tag value from the tag array.
References-found: 19

