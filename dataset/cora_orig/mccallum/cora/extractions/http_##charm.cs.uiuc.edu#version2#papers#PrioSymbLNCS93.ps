URL: http://charm.cs.uiuc.edu/version2/papers/PrioSymbLNCS93.ps
Refering-URL: http://charm.cs.uiuc.edu/version2/papers/PrioSymbLNCS93.html
Root-URL: http://www.cs.uiuc.edu
Phone: 2  3  
Title: Prioritization in Parallel Symbolic Computing  
Author: L. V. Kale B. Ramkumar V. Saletore and A. B. Sinha 
Address: Urbana Champaign, 1304 W. Springfield Ave., Urbana IL-61801  Iowa, Iowa City, IA-52242  Corvallis, OR-97331  
Affiliation: 1 Department of Computer Science, University of Illinois at  Department of Electrical and Computer Engineering, University of  Department of Computer Science, Oregon State University,  
Abstract: It is argued that scheduling is an important determinant of performance for many parallel symbolic computations, in addition to the issues of dynamic load balancing and grain size control. We propose associating unbounded levels of priorities with tasks and messages as the mechanism of choice for specifying scheduling strategies. We demonstrate how priorities can be used in parallelizing computations in different search domains, and show how priorities can be implemented effectively in parallel systems. Priorities have been implemented in the Charm portable parallel programming system. Performance results on shared-memory machines with tens of processors and nonshared-memory machines with hundreds of processors are given. Open problems for prioritization in specific domains are given, which will constitute fertile area for future research in this field.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Burton F. W. </author> <title> Controlling Speculative Computation in a Parallel Functional Language. </title> <booktitle> In International Conference on Distributed Computer Systems, </booktitle> <pages> pages 453-458, </pages> <month> November </month> <year> 1985. </year>
Reference-contexts: Here, the order in which tasks are executed has an impact on the total amount of computations performed | by affecting the number of messages/processes that must be processed before the problem is solved. Speculative computations in parallel functional languages have also been investigated in <ref> [1] </ref>. What mechanisms can be used to specify and implement scheduling strategies? 1. Assign fractions: In this strategy, each process is given a promise of a certain fraction of the overall CPU-time available in the system.
Reference: 2. <author> Einarsson, Thorr T. </author> <title> Bidirectional Search in Parallel. </title> <type> Master's thesis, </type> <institution> Dept. of Comp. Sc., University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Secondly, to avoid backward search consuming more time than the forward one, the two should be overlapped in time. We plan to explore these issues further with other search problems. Some preliminary results on the "Peg solitaire" game are in <ref> [2] </ref>. <p> Attaching priorities to futures when they are spawned is straightforward; however, the sponsor model requires that when a process touches a future, the future assumes the priority of the touching process. Implementation of this strategy has similarities with the problem encountered in state-space searches in connection with duplication detection <ref> [2] </ref>, and also with the game tree searches, where priorities need to be dynamically propagated [21]. So some of the strategies developed in these works may be applicable in this context.
Reference: 3. <author> Fenton, Wayne. </author> <title> Additions to the chare kernel parallel programming system, and its usefulness for parallel state-space search. </title> <type> Master's thesis, </type> <institution> Dept. of Comp. Sc., University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: A similar experience was obtained for the 3-satisfiability problem <ref> [3] </ref>. As long as a solution existed, the heuristic we developed was able to find it without much search! We plan to experiment with other heuristics for NP-complete problems. 4.2 Iterative Deepening Sometimes, one is interested in an optimal solution to a search problem.
Reference: 4. <author> Furuichi, M., Taki, K. and Ichiyoshi, N. </author> . <title> A Multi-level Load Balancing Scheme for OR-parallel Exhaustive Search Programs on the Multi-PSI. </title> <booktitle> In PPOPP, </booktitle> <pages> pages 50-59, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Other work on dynamic load balancing for this problem includes that of Kumar and Rao [28, 18], who describe an idle-processor initiated load-balancing scheme which splits (i.e. divides the nodes on) the stack of the donor processors, and <ref> [4] </ref> which relies on a hierarchical load balancing scheme. When one is interested in any one solution, such parallelization techniques lead to difficulties. If we search two successors of a state (assume there are only two for simplicity), the solution may lie in the sub-tree of either node.
Reference: 5. <author> Halstead R. </author> <title> Parallel Symbolic Computing. </title> <booktitle> Computer, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: Integration of priorities with futures <ref> [5] </ref> particularly in connection with the sponsor model [23, 24] proposed for scheduling futures, represents another interesting problem. Attaching priorities to futures when they are spawned is straightforward; however, the sponsor model requires that when a process touches a future, the future assumes the priority of the touching process.
Reference: 6. <author> Hausman B. </author> <title> Pruning and Speculative Work in OR-Parallel PROLOG. </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: This added one more constraint on the possible schemes. Speculative work in OR-parallel Prolog has also been investigated by Hausman in <ref> [6] </ref>. We first worked on simply improving the first solution speedups in ROPM compared with the then prevalent scheduling scheme, which was a LIFO scheme, with each processor having its own stack. This is described below.
Reference: 7. <author> L. V. Kale and W. Shu. </author> <title> The Chare Kernel language for parallel programming: A perspective. </title> <type> Technical Report UIUCDCS-R-88-1451, </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: Due to the diversity, programs written for one parallel machine don't usually run on another machine by a different vendor. Charm <ref> [7] </ref> is a parallel programming system that we have developed to address this problem. Charm provides portability and supports features that simplify the task of parallel programming. Programs developed with Charm run efficiently on different shared-memory and nonshared-memory machines without change. <p> It currently runs on Intel iPSC/860, NCUBE, CM-5, Sequent Symmetry, Multimax, Alliant FX/8, networks of workstations, and will be ported to machines including the Intel Paragon in near future. Charm is one of the first systems to support an asynchronous, message driven, execution model <ref> [7] </ref>. This allows for maximum overlap between computation and communication and facilitates modular program organization. Recognizing that parallel programs involve distinct modes of sharing information, it supports six modes in which information may be shared between processes. These modes are implemented differently on different machines for efficiency.
Reference: 8. <author> L.V. Kale. </author> <title> The REDUCE OR process model for parallel execution of logic programs. </title> <journal> Journal of Logic Programming, </journal> <volume> 11(1) </volume> <pages> 55-84, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: REDUCE/OR Process Model: Our work on speculative computations in Parallel Logic Programming was conducted in the context of the REDUCE/OR process model (ROP M ), proposed and developed in <ref> [9, 14, 8] </ref>. The past and ongoing work related to this model in our group includes development of a binding environment [15] and a compiler [26, 25].
Reference: 9. <author> Kale L.V. </author> <title> Parallel Execution of Logic Programs: The REDUCE-OR Process Model. </title> <booktitle> In International Conference on Logic Programming, </booktitle> <pages> pages 616-632, </pages> <address> Mel-bourne, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: REDUCE/OR Process Model: Our work on speculative computations in Parallel Logic Programming was conducted in the context of the REDUCE/OR process model (ROP M ), proposed and developed in <ref> [9, 14, 8] </ref>. The past and ongoing work related to this model in our group includes development of a binding environment [15] and a compiler [26, 25]. <p> It overcomes the limitations of AND/OR trees from the point of view of parallel execution. The detailed description of the process model can be found in <ref> [9] </ref>. What concerns us here is the process structure generated by ROPM. Each invocation of a clause corresponds to a process, called a REDUCE process (with the exception of clauses and predicates explicitly marked sequential: these are used for granularity control).
Reference: 10. <author> Kale L.V. </author> <title> A Tree Representation for Parallel Problem Solving. </title> <booktitle> In National Conference on Artificial Intelligence (AAAI), </booktitle> <address> St. Paul, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: The work described in Section 4.1 on pure state-space search came later, and encouraged by those results we set a new objective of consistent and monotonic speedups. The resultant work is described subsequently. Speedups for a First Solution: The REDUCE/OR process model is based on the REDUCE/OR tree <ref> [10] </ref>, which is an alternative to the traditional AND/OR tree. It overcomes the limitations of AND/OR trees from the point of view of parallel execution. The detailed description of the process model can be found in [9]. What concerns us here is the process structure generated by ROPM.
Reference: 11. <author> Kale L.V. </author> <title> An Almost Perfect Heuristic for the N-Queens Problem. </title> <note> In Information Processing Letters, </note> <month> April </month> <year> 1990. </year>
Reference-contexts: For the N-queens problem, this led to such a good heuristics that it almost always led to a first solution, without much search <ref> [11] </ref>. This was a true heuristic, as distinct from the well-known closed form solutions to the N-queens problem, in that it can be used continually to generate multiple solutions beyond the first Table 2.
Reference: 12. <author> Kale L.V. </author> <title> The Chare-Kernel Parallel Programming Language and System. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: Indeed, if one is looking for all solutions, this is as simple as that, except for the important problems of load balancing and grain size control. Earlier, we worked on the all-solution problem using the Chare-Kernel machine independent parallel programming system <ref> [12] </ref>, which provides dynamic load balancing among other facilities. With this, we were able to obtain very good speedups for many depth-first search problems.
Reference: 13. <author> Kale L.V. and Saletore V.A. </author> <title> Parallel State-Space Search for a First Solution with Consistent Linear Speedups. </title> <journal> International Journal of Parallel Programming, </journal> <month> August </month> <year> 1990. </year>
Reference-contexts: With that objective, it is clear that all the work that is done by the sequential program is "mandatory" whereas all the other nodes not explored by the sequential algorithm are "wastage". Our scheme, described in <ref> [13, 31] </ref> is based on bit-vector priorities, and builds upon an idea in [20]. Each node in the search tree is assigned a priority. Priority bit-vectors can be of arbitrary length, and their ordering is lexicographic | the lexicographically smaller bit-vector indicates higher priority. <p> Figure 1 shows an example of how such priorities are assigned to nodes of a search tree. Fig. 1. Illustration of the assignment of bit-vector priorities to nodes. The priority of the topmost node is assumed to be X. The complete scheme, described in <ref> [13] </ref>, involves a few additional subtle points of strategy. <p> The slight superlinearity seen in the data is not surprising, and it occurs because the optimal solution is reported before all the nodes to the left of the solution node are explored | other processors may still be working on such nodes. See <ref> [13, 32] </ref> for details of this scheme, and additional performance data. Table 3. Performance of IDA* solving a 15-puzzle instance on Sequent Symmetry. The numbers shown are speedups. The sequential execution time for this instance was 116 seconds.
Reference: 14. <author> Kale L.V. and Warren D.S. </author> <title> Class of Architectures for a PROLOG Machine. </title> <booktitle> In International Conference on Logic Programming, </booktitle> <pages> pages 171-182, </pages> <address> Stockholm, Sweden, </address> <month> June </month> <year> 1985. </year>
Reference-contexts: REDUCE/OR Process Model: Our work on speculative computations in Parallel Logic Programming was conducted in the context of the REDUCE/OR process model (ROP M ), proposed and developed in <ref> [9, 14, 8] </ref>. The past and ongoing work related to this model in our group includes development of a binding environment [15] and a compiler [26, 25].
Reference: 15. <author> Kale L.V., Ramkumar B. and Shu W. </author> <title> A Memory Organization Independent Binding Environment for AND and OR Parallel Execution of Logic Programs. </title> <booktitle> In The 5th International Conference/Symposium on Logic Programming, </booktitle> <pages> pages 1223-1240, </pages> <address> Seattle, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: The past and ongoing work related to this model in our group includes development of a binding environment <ref> [15] </ref> and a compiler [26, 25]. The REDUCE/OR process model exploits AND as well as OR parallelism from Logic programs, and handles the interactions of AND and OR parallelism without losing parallelism. It is also designed so that it can use both shared and nonshared memory machines.
Reference: 16. <author> Korf R.E. </author> <title> Depth-first Iterative Deepening: An Optimal Admissible Tree Search. </title> <booktitle> In Artificial Intelligence, </booktitle> <pages> pages 97-109, </pages> <year> 1985. </year>
Reference-contexts: The promise of such a reduction makes bi-directional search very attractive. A few details must be dealt with before attempting to realize these gains. Korf et. al. <ref> [16, 17] </ref> have explored these issues in the past. The forward and backward search must intersect in time, so as to make sure the solution states don't miss each other. This rules out the space-efficient depth-first search for at least one of the two directions.
Reference: 17. <author> Korf R.E. </author> <title> Optimal Path-Finding Algorithms. </title> <booktitle> In Search in Artificial Intelligence, </booktitle> <pages> pages 223-267. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: This process is continued until a solution is found. This algorithm was defined by Korf <ref> [17] </ref>, and is called IDA*, for Iterative Deepening A*. As in A*, the first solution found is an optimal solution in IDA* too. Each successive iteration duplicates all the work done by the previous iteration. <p> The promise of such a reduction makes bi-directional search very attractive. A few details must be dealt with before attempting to realize these gains. Korf et. al. <ref> [16, 17] </ref> have explored these issues in the past. The forward and backward search must intersect in time, so as to make sure the solution states don't miss each other. This rules out the space-efficient depth-first search for at least one of the two directions.
Reference: 18. <author> Kumar Vipin and Rao V. Nageshwar. </author> <title> Parallel Depth First Search. Part 2: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <pages> pages 501-519, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: With this, we were able to obtain very good speedups for many depth-first search problems. Other work on dynamic load balancing for this problem includes that of Kumar and Rao <ref> [28, 18] </ref>, who describe an idle-processor initiated load-balancing scheme which splits (i.e. divides the nodes on) the stack of the donor processors, and [4] which relies on a hierarchical load balancing scheme. When one is interested in any one solution, such parallelization techniques lead to difficulties.
Reference: 19. <author> Lai T.H. and Sahni Sartaj. </author> <title> Anomalies in Parallel Branch-and-Bound Algorithms. </title> <booktitle> In Communications of the ACM, </booktitle> <pages> pages 594-602, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Exploring the two subtrees in parallel is thus speculative | we may not need both those sub-computations. This fact, and the resultant speedup anomalies were noted in a branch-and-bound search which is closely related to depth-first search, by Lai and Sahni <ref> [19] </ref>. One may get deceleration anomalies where adding a processor may actually slow down the search process in finding a solution. This may happen because the added processor may create some "red herring" work that other processors end up wasting their time on.
Reference: 20. <author> Li G.J. and Wah B.W. </author> <title> Coping with Anomalies in Parallel Branch-and-Bound Algorithms. </title> <journal> In IEEE Transactions on Computers, </journal> <pages> pages 568-573, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: With that objective, it is clear that all the work that is done by the sequential program is "mandatory" whereas all the other nodes not explored by the sequential algorithm are "wastage". Our scheme, described in [13, 31] is based on bit-vector priorities, and builds upon an idea in <ref> [20] </ref>. Each node in the search tree is assigned a priority. Priority bit-vectors can be of arbitrary length, and their ordering is lexicographic | the lexicographically smaller bit-vector indicates higher priority. The priority of the root is a bit-vector of length 0.
Reference: 21. <author> Low, Chin-Chau. </author> <title> Parallel game tree searching with lower and upper bounds. </title> <type> Master's thesis, </type> <institution> Dept. of Comp. Sc., University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: So, it is potentially possible that this method, under a proper prioritization strategy, may need to explore fewer nodes than the alpha-beta strategy. Finding such a strategy remains an open problem at the moment. However, we have explored some simple prioritization strategies that lead to reasonably good performances <ref> [21] </ref>. Figure 6 below shows the performance of a simple strategy from [21] on a game position for the board game Othello, with a 8-ply search. <p> Finding such a strategy remains an open problem at the moment. However, we have explored some simple prioritization strategies that lead to reasonably good performances <ref> [21] </ref>. Figure 6 below shows the performance of a simple strategy from [21] on a game position for the board game Othello, with a 8-ply search. <p> Assigning differential priorities depending on the type of messages becomes at least as important as assigning different priorities to messages of the same type. The specific strategies we employed for this purpose are described in <ref> [21] </ref>. 4.7 Bi-directional Search When the goal state of a state-space search is fixed, and the operators for transforming states are invertible, it becomes feasible to search backwards from the goal state to the start state. <p> Implementation of this strategy has similarities with the problem encountered in state-space searches in connection with duplication detection [2], and also with the game tree searches, where priorities need to be dynamically propagated <ref> [21] </ref>. So some of the strategies developed in these works may be applicable in this context. Acknowledgements: Most of the research reported in this paper was conducted by the first author with different (then) graduate students.
Reference: 22. <author> Nilsson N.J. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga Press, Inc., </publisher> <year> 1980. </year>
Reference-contexts: As long as a solution existed, the heuristic we developed was able to find it without much search! We plan to experiment with other heuristics for NP-complete problems. 4.2 Iterative Deepening Sometimes, one is interested in an optimal solution to a search problem. If an admissible heuristic is available <ref> [22] </ref> one can use the A* algorithm, which ensures that the first solution found is the optimal one. However, A* requires large memory space on the average, and degenerates to breadth-first search in the worst case.
Reference: 23. <author> R. Osborne. </author> <title> Speculative computation in multilisp. </title> <booktitle> In Lecture Notes in Computer Science, number 441. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Integration of priorities with futures [5] particularly in connection with the sponsor model <ref> [23, 24] </ref> proposed for scheduling futures, represents another interesting problem. Attaching priorities to futures when they are spawned is straightforward; however, the sponsor model requires that when a process touches a future, the future assumes the priority of the touching process.
Reference: 24. <author> R. Osborne. </author> <title> Speculative computation in multilisp: An overview. </title> <booktitle> In ACM Conference on Lisp and Functional Programming, </booktitle> <year> 1990. </year>
Reference-contexts: Integration of priorities with futures [5] particularly in connection with the sponsor model <ref> [23, 24] </ref> proposed for scheduling futures, represents another interesting problem. Attaching priorities to futures when they are spawned is straightforward; however, the sponsor model requires that when a process touches a future, the future assumes the priority of the touching process.
Reference: 25. <author> B. Ramkumar and L.V. Kale. </author> <title> Machine independent AND and OR parallel execution of logic programs: Part II compiled execution. </title> <note> To appear in IEEE Transactions on Parallel and Distributed Systems, </note> <year> 1991. </year>
Reference-contexts: The past and ongoing work related to this model in our group includes development of a binding environment [15] and a compiler <ref> [26, 25] </ref>. The REDUCE/OR process model exploits AND as well as OR parallelism from Logic programs, and handles the interactions of AND and OR parallelism without losing parallelism. It is also designed so that it can use both shared and nonshared memory machines.
Reference: 26. <author> Ramkumar B. and Kale L.V. </author> <title> Compiled Execution of the REDUCE-OR Process Model on Multiprocessors. </title> <booktitle> In North American Conference on Logic Programming, </booktitle> <pages> pages 313-331, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The past and ongoing work related to this model in our group includes development of a binding environment [15] and a compiler <ref> [26, 25] </ref>. The REDUCE/OR process model exploits AND as well as OR parallelism from Logic programs, and handles the interactions of AND and OR parallelism without losing parallelism. It is also designed so that it can use both shared and nonshared memory machines.
Reference: 27. <author> Ramkumar, B., Banerjee P. </author> <title> Portable Parallel Test Generation for Sequential Circuits. </title> <booktitle> In Proceedings of the International Conference on Computer-Aided Design, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: A sample of the performance data, taken from [29], is shown in Table 1. This strategy was also applied successfully in obtaining consistent speedups to one solution in test pattern generation for sequential circuits by Ramkumar and Banerjee <ref> [27] </ref>. In test generation for sequential circuits, we once again have a state space where the nodes in the search space represent assignments to primary inputs or inputs to flip-flops (called pseudo-inputs) in the circuit being tested. <p> If the entire search space has been explored unsuccessfully, the fault is called a redundant fault. The efficiency metric in Table 2 reports the percentage of faults which are redundant or for which test vectors have been detected. In Table 2, we quote some of the results presented in <ref> [27] </ref> for an 8-processor Intel i860 hypercube. The test generator, called ProperTEST, was developed using the Charm system and, as a result, ran unchanged on a variety of machines, including a network of Sun Sparc I workstations, a Sequent Symmetry, an Intel i860 hypercube and an Encore Multimax. <p> So some of the strategies developed in these works may be applicable in this context. Acknowledgements: Most of the research reported in this paper was conducted by the first author with different (then) graduate students. One exception is the research by Ramkumar and Banerjee <ref> [27] </ref>, who apply some of our earlier results on priorities to parallel test pattern generation. Much of the research discussed in this paper has been reported in separate papers. This paper should be seen as an overview, which brings together the applications of prioritization from different contexts.
Reference: 28. <author> Rao V. Nageshwara and Kumar Vipin. </author> <title> Parallel Depth First Search. Part 1: Implementation. </title> <journal> International Journal of Parallel Programming, </journal> <pages> pages 479-499, </pages> <month> De-cember </month> <year> 1987. </year>
Reference-contexts: With this, we were able to obtain very good speedups for many depth-first search problems. Other work on dynamic load balancing for this problem includes that of Kumar and Rao <ref> [28, 18] </ref>, who describe an idle-processor initiated load-balancing scheme which splits (i.e. divides the nodes on) the stack of the donor processors, and [4] which relies on a hierarchical load balancing scheme. When one is interested in any one solution, such parallelization techniques lead to difficulties. <p> This can happen because the added processor picked a part of the search tree that happened to contain the solution. Kumar et. al. noted this in the context of parallel depth-first search. They reported a speedup varying between 2.9 to 16 with 9 processors for a 15-puzzle problem <ref> [28] </ref>. We started with the dual objectives of (1) ensuring that speedups are consistent | i.e. do not vary from one execution to another and (2) ensuring that the speedups increase monotonically with the number of processors, preferably being as close to the number of processors as possible. <p> Even with a binary branching factor, the duplication cost is at most 100%, which is tolerable considering the significant memory savings. As each iteration of IDA* is a depth-first search, it can be parallelized us-ing the techniques described above. Kumar et. al. in <ref> [28] </ref> were the first to demonstrate parallel schemes for this problem. Their results did exhibit speedup anomalies for single solutions, and they reported speedups to all solutions (as their primary interest was to demonstrate the efficacy of their load balancing scheme).
Reference: 29. <author> Saletore V.A. </author> <title> Machine Independent Parallel Execution of Speculative Computations. </title> <type> PhD thesis, </type> <institution> Dept. Electrical and Computer Engineering, University of Illi-nois at Urbana-Champaign, Urbana, IL, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Scheduling, in contrast, deals with the question of which messages and processes, among the many available ones, to execute next. It is particularly important for speculatively parallel computations, where some of the tasks that can be carried out in parallel may turn out to be futile or unnecessary <ref> [29] </ref>. Here, the order in which tasks are executed has an impact on the total amount of computations performed | by affecting the number of messages/processes that must be processed before the problem is solved. Speculative computations in parallel functional languages have also been investigated in [1]. <p> This strategy was implemented and tested with various state-space search problems on shared-memory and small nonshared-memory machines. A sample of the performance data, taken from <ref> [29] </ref>, is shown in Table 1. This strategy was also applied successfully in obtaining consistent speedups to one solution in test pattern generation for sequential circuits by Ramkumar and Banerjee [27]. <p> With this scheme we were able to "soak up" the computing resources during the previously idle periods without increasing the wastage, and produce almost perfect speedups even for small-sized problems. The improvement obtained can be seen in Table 3, taken from the data in <ref> [29] </ref>. The slight superlinearity seen in the data is not surprising, and it occurs because the optimal solution is reported before all the nodes to the left of the solution node are explored | other processors may still be working on such nodes. <p> The complete details of this scheme can be found in <ref> [29] </ref>. We only note that consistent and excellent first solution speedups were obtained for pure OR parallel Logic Programs with this scheme. As an example, the following table shows the performance of the Prolog compiler with priorities, running a Prolog program to find a Knight's tour on a 6x6 board. <p> A synthesis of these is needed. We developed a simple scheme <ref> [29] </ref> that is sufficient to ensure consistent and linear speedups for many (but not all) AND/OR problems. We believe that schemes that involve dynamic changing of priorities are necessary to handle this class of problems. Fig. 6. Performance of a prioritization scheme for game tree search, on Sequent Sym--metry. <p> Fully distributed strategies on the other hand suffer from an inability to adhere to priorities on the global level (low priority work might be done on some processing elements, even though there exist higher priority work on other processing elements). Our earlier work <ref> [29] </ref> involved a fully distributed approach to prioritization in a parallel system. The initial distribution of the work onto various processing elements is random. Subsequently, processors periodically exchange information about load and priorities with their neighbors, and attempt to distribute priorities and load by moving work around. <p> These strategies still have the drawback (to a smaller extent) we discussed earlier | priorities are not distributed uniformly over processors, hence low priority (wasteful) work gets done. For additional variants of this strategy and their performance on various machines we refer the reader to <ref> [29] </ref>. Centralized strategies provide good priority adherence and load balancing. Their weakness is that the central pool of work becomes the bottleneck.
Reference: 30. <author> Saletore V.A. and Kale L.V. </author> <title> Obtaining First Solutions Faster in AND-OR Parallel Execution of Logic Programs. </title> <booktitle> In North American Conference on Logic Programming, </booktitle> <pages> pages 390-406, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: For further information about this scheme, we refer the reader to <ref> [30] </ref>. Table 5 below shows the performance of our strategy on a benchmark. This program involves finding a prime that can also be expressed as a sum of a Fibonacci number and a perfect number.
Reference: 31. <author> Saletore V.A. and Kale L.V. </author> <title> Consistent Linear Speedups to a First Solution in Parallel State-Space Search. </title> <booktitle> In The Eighth National Conference on Artificial Intelligence (AAAI-90), </booktitle> <address> Boston, Mass., </address> <month> July </month> <year> 1990. </year>
Reference-contexts: With that objective, it is clear that all the work that is done by the sequential program is "mandatory" whereas all the other nodes not explored by the sequential algorithm are "wastage". Our scheme, described in <ref> [13, 31] </ref> is based on bit-vector priorities, and builds upon an idea in [20]. Each node in the search tree is assigned a priority. Priority bit-vectors can be of arbitrary length, and their ordering is lexicographic | the lexicographically smaller bit-vector indicates higher priority.
Reference: 32. <author> Saletore V.A. and Kale L.V. </author> <title> Efficient Parallel Execution of IDA* on Shared and Distributed Memory Multiprocessors. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference (DMCC6), </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: However, seen from this perspective, this problem seems amenable to prioritization. Just assign higher priority to earlier iterations, and allow multiple iterations to run concurrently. This scheme is described in <ref> [32] </ref>. The fact that we do not know which iterations are necessary can be handled as follows: We start K iterations in parallel. Whenever any one of them finishes without reporting a solution, we start the next iteration. <p> The slight superlinearity seen in the data is not surprising, and it occurs because the optimal solution is reported before all the nodes to the left of the solution node are explored | other processors may still be working on such nodes. See <ref> [13, 32] </ref> for details of this scheme, and additional performance data. Table 3. Performance of IDA* solving a 15-puzzle instance on Sequent Symmetry. The numbers shown are speedups. The sequential execution time for this instance was 116 seconds.
Reference: 33. <author> Saletore V.A. and Mohammed M.A. </author> <title> A Hierarchical Load Distribution Scheme for Branch-and-Bound Computations on Distributed Memory Machines. </title> <type> Technical Report 93-80-04, </type> <institution> Dept. of Computer Science, Oregon State University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Their weakness is that the central pool of work becomes the bottleneck. We can solve the problem of a bottleneck by splitting the pool of work amongst a few processing elements | essentially creating some sort of a semi-distributed strategy as described in <ref> [36, 33] </ref>. In the strategy in citeSinhaIPPS93, the processors in the system are partitioned into clusters. One processor in each cluster is chosen as the load manager, the remaining processors in the cluster being its managees.
Reference: 34. <author> Saletore V.A., Ramkumar B., and Kale L.V. </author> <title> Consistent First Solution Speedups in OR-Parallel Execution of Logic Programs. </title> <type> Technical Report UIUCDCS-R-90-1725, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: The description of the process structure for ROPM described above should make it clear why the application is not straight-forward. The OR tree (search tree) used in state-space search is now folded into the REDUCE/OR tree. The scheme we developed in <ref> [34] </ref> to address the speedup anomaly involves tagging responses with their priorities, and using the response's priority to decide the priority of any processes fired due to it. For example, a REDUCE process with priority X may have two dependent literals p and q, with q being dependent on p.
Reference: 35. <author> Sen A.K. and Bagchi A. </author> <title> Fast Recursive Formulations for Best-First Search That Allow Controlled Use of Memory. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 297-302, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: D+x). The system will thus finish an adequate number of nodes created prior to this point in a depth-first (LIFO) fashion, but then revert back to a best-first pattern. Controlled use of memory has also been used in <ref> [35] </ref> by combining good features of A* and IDA*. Another reason for higher memory usage in a prioritized strategy is the potentially large number of low priority nodes that may "rot" in the queue. These nodes represent work that is pruned due to some solution found earlier.
Reference: 36. <author> A. B. Sinha and L. V. Kale. </author> <title> A load balancing strategy for prioritized execution of tasks. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Their weakness is that the central pool of work becomes the bottleneck. We can solve the problem of a bottleneck by splitting the pool of work amongst a few processing elements | essentially creating some sort of a semi-distributed strategy as described in <ref> [36, 33] </ref>. In the strategy in citeSinhaIPPS93, the processors in the system are partitioned into clusters. One processor in each cluster is chosen as the load manager, the remaining processors in the cluster being its managees. <p> We can balance memory requirements of processing elements using the following variant of the above strategy, developed in <ref> [36] </ref>. As in the above scheme, the processing elements in the system are split up into clusters | one processor in each cluster is chosen as the load manager, the remaining processors are its managees.
Reference: 37. <author> Stone Harold S. and Stone Janice M. </author> <title> Efficient search techniques- An empirical study of the N-Queens Problem. </title> <journal> IBM Journal of Research and Development, </journal> <pages> pages 464-474, </pages> <month> July </month> <year> 1987. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: I.e. the branching factor is not uniform, and it varies with the depth of the tree. It has an expanding phase followed by a "stagnant" phase, followed by a shrinking phase. (Stone & Stone also report similar shapes of search trees in <ref> [37] </ref>.) It turns out that the lower bounding heuristic | the Manhattan distance one | is a strong pruning device, which discards a majority of the new states being generated at deeper levels in the tree.
References-found: 37

