URL: ftp://ftp.cs.rochester.edu/pub/u/rao/papers/cvpr97.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/rao/
Root-URL: 
Email: rao@cs.rochester.edu  
Title: Dynamic Appearance-Based Recognition  
Author: Rajesh P.N. Rao 
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: We describe a hierarchical appearance-based method for learning, recognizing, and predicting arbitrary spatiotempo-ral sequences of images. The method, which implements a robust hierarchical form of the Kalman filter derived from the Minimum Description Length (MDL) principle, includes as a special case several well-known object encoding techniques including eigenspace methods for static recognition. Successive levels of the hierarchical filter implement dynamic models operating over successively larger spatial and temporal scales. Each hierarchical level predicts the recognition state at a lower level and modifies its own recognition state using the residual error between the prediction and the actual lower-level state. Simultaneously, on a longer time scale, the filter learns an internal model of input dynamics by adapting its generative and state transition matrices at each level to minimize prediction errors. The resulting prediction/learning scheme thereby implements an on-line form of the well-known Expectation-Maximization (EM) algorithm from statistics. We present experimental results demonstrating the method's efficacy in mediating robust spatiotemporal recognition in a variety of scenarios containing varying degrees of occlusions and clutter. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bell and T. Sejnowski. </author> <title> The `independent components' of natural scenes are edge filters. </title> <note> Submitted to Vision Research, </note> <year> 1996. </year>
Reference-contexts: It is therefore not surprising that recent work on modeling the response properties of neurons in the visual cortex has focused on alternative generative models using goals ranging from maximizing the sparseness of the coefficients [12] to making the coefficients statistically independent (for example, independent component analysis (ICA) <ref> [1] </ref>). In this paper, we propose a hierarchical appearance-based method for learning, recognizing, and predicting spatiotem-poral sequences of input images. As a special case of the method, one obtains the standard eigenspace and related methods for recognition of static images.
Reference: [2] <author> M. Black and A. Jepson. Eigentracking: </author> <title> Robust matching and tracking of articulated objects using a view-based representation. </title> <booktitle> In ECCV'96, </booktitle> <pages> pages 329-342, </pages> <year> 1996. </year>
Reference-contexts: The notion of hidden state is already implicit in various appearance-based methods for computer vision that have received much attention over the past few years <ref> [17, 11, 13, 9, 2, 10] </ref>. For example, in eigenspace-based methods, an input image is characterized by the vector of coefficients obtained by projecting the image along the directions given by the dominant eigenvectors of the input covariance matrix. <p> In order to handle occlusions and clutter, alternate robust methods of computing the coefficients have been proposed <ref> [2, 10] </ref>, which, in the light of the above discussion, are equivalent to re-estimating the true hidden state. Unfortunately, an eigenspace-based generative model, which uses orthogonal eigenvectors, may deviate significantly from the true generative model that characterizes the image generation process.
Reference: [3] <author> A. Blake and A. Yuille, </author> <title> editors. Active Vision. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: The proposed method is based firmly on the information-theoretic MDL principle [15] and utilizes ideas from robust statistics [8] for deriving hierarchical Kalman filter estimators that can tolerate significant occlusion and clutter. Kalman filters have previously been used extensively in computer vision <ref> [3] </ref> and image processing [5]. However, many of these approaches employ hard-wired dynamic models inferred from a priori knowledge of the task at hand. This paper describes how hierarchical dynamic models can be learned from input data, thereby avoiding the need for explicit hand-built physical models of dynamic systems.
Reference: [4] <author> A. Bryson and Y.-C. Ho. </author> <title> Applied Optimal Control. </title> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1975. </year>
Reference-contexts: These are in general non-linear functions of r. Note that to obtain the above closed-form equations, we used a first-order Taylor series expansion of f 0 about r (t). Equations 8 and 9 together implement an MDL-based Kalman filter <ref> [4] </ref> for updating the state estimate given prior estimates r and M . For static images, we may use r (t) = b r (t 1) and M (t) = P (t 1) to obtain an iterative Kalman filter for estimating the optimal state corresponding to a given static input. <p> In order to model time-varying processes, we need to describe how the state r itself varies in time. One way this can be achieved is by assuming that r is a Gauss-Markov random process <ref> [4] </ref>. <p> For the experiments in the paper, we used r (t + 1) = b r (t + 1) in Equation 19 above, although the EM algorithm prescribes the use of b r (t + 1jN ), which is the optimal temporally smoothed state estimate <ref> [4] </ref> for time t + 1 ( N ), given input data for each of the time instants 1; : : : ; N .
Reference: [5] <author> K. Chou, A. Willsky, and A. Benveniste. </author> <title> Multiscale recursive estimation, data fusion, and regularization. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> 39(3) </volume> <pages> 464-478, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Hierarchical Dynamic Models Most natural phenomena manifest themselves over a multitude of spatial and temporal scales. For example, the rich class of stochastic processes possessing a 1=f fi power spectra exhibit statistical and fractal self-similarities that can be satisfactorily captured only in a multiscale framework <ref> [5] </ref>. There has consequently been much recent interest in multi-scale signal processing methods, and techniques such as image pyramids, wavelets, and scale-space theory have found wide application in computer vision and image processing. <p> The proposed method is based firmly on the information-theoretic MDL principle [15] and utilizes ideas from robust statistics [8] for deriving hierarchical Kalman filter estimators that can tolerate significant occlusion and clutter. Kalman filters have previously been used extensively in computer vision [3] and image processing <ref> [5] </ref>. However, many of these approaches employ hard-wired dynamic models inferred from a priori knowledge of the task at hand. This paper describes how hierarchical dynamic models can be learned from input data, thereby avoiding the need for explicit hand-built physical models of dynamic systems.
Reference: [6] <author> A. Dempster, N. Laird, and D. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: An interesting question is the issue of convergence of the overall filtering/learning scheme involving r and U . Fortunately, one can appeal to the well-known Expectation-Maximization (EM) algorithm from statistics <ref> [6] </ref> and allow the overall scheme to converge by using U = b U (t 1) in Equation 8 and r (t) = b r in the matrix R (t) above, where b r is the converged state estimate for the given static input.
Reference: [7] <author> D. J. </author> <title> Field. What is the goal of sensory coding? Neural Computation, </title> <booktitle> 6 </booktitle> <pages> 559-601, </pages> <year> 1994. </year>
Reference-contexts: Recent work by Field <ref> [7] </ref> and others strongly suggest that natural images form a highly non-Gaussian distribution that cannot be described satisfactorily by orthogonal basis vectors. (d) Perhaps most importantly, eigenvector-based methods can only capture linear pairwise statistical dependencies in the input stream.
Reference: [8] <author> P. Huber. </author> <title> Robust Statistics. </title> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1981. </year>
Reference-contexts: Making S bu constant however reduces the Kalman filter estimates to be essentially ordinary least-squares estimates, and it is well-known that least-squares estimation is highly susceptible to outliers or gross errors i.e. data points that lie far away from the bulk of the observed data <ref> [8] </ref>. The problem of outliers can be tackled using robust estimation procedures [8]. <p> Kalman filter estimates to be essentially ordinary least-squares estimates, and it is well-known that least-squares estimation is highly susceptible to outliers or gross errors i.e. data points that lie far away from the bulk of the observed data <ref> [8] </ref>. The problem of outliers can be tackled using robust estimation procedures [8]. One commonly used procedure is M-estimation (Maximum likelihood type estimation), which involves minimizing a function of the form: J 0 (U; r) = i=1 where is normally taken to be a less rapidly increasing function than the square. <p> Summary and Conclusions This paper presents a new hierarchical appearance-based method for learning, recognizing, and predicting image sequences. The proposed method is based firmly on the information-theoretic MDL principle [15] and utilizes ideas from robust statistics <ref> [8] </ref> for deriving hierarchical Kalman filter estimators that can tolerate significant occlusion and clutter. Kalman filters have previously been used extensively in computer vision [3] and image processing [5]. However, many of these approaches employ hard-wired dynamic models inferred from a priori knowledge of the task at hand.
Reference: [9] <author> D. Huttenlocher, R. Lilien, and C. Olson. </author> <title> Object recognition using subspace methods. </title> <booktitle> In ECCV'96, </booktitle> <pages> pages 536-545, </pages> <year> 1996. </year>
Reference-contexts: The notion of hidden state is already implicit in various appearance-based methods for computer vision that have received much attention over the past few years <ref> [17, 11, 13, 9, 2, 10] </ref>. For example, in eigenspace-based methods, an input image is characterized by the vector of coefficients obtained by projecting the image along the directions given by the dominant eigenvectors of the input covariance matrix.
Reference: [10] <author> A. Leonardis and H. Bischof. </author> <title> Dealing with occlusions in the eigenspace approach. </title> <booktitle> In CVPR'96, </booktitle> <pages> pages 453-458, </pages> <year> 1996. </year>
Reference-contexts: The notion of hidden state is already implicit in various appearance-based methods for computer vision that have received much attention over the past few years <ref> [17, 11, 13, 9, 2, 10] </ref>. For example, in eigenspace-based methods, an input image is characterized by the vector of coefficients obtained by projecting the image along the directions given by the dominant eigenvectors of the input covariance matrix. <p> In order to handle occlusions and clutter, alternate robust methods of computing the coefficients have been proposed <ref> [2, 10] </ref>, which, in the light of the above discussion, are equivalent to re-estimating the true hidden state. Unfortunately, an eigenspace-based generative model, which uses orthogonal eigenvectors, may deviate significantly from the true generative model that characterizes the image generation process. <p> This was apparently sufficient for the 100% recognition rate that was obtained for 36 different testing views of each object, each view 5 ffi being from the nearest training view. The second to last row depicts how the effect of occlusionsspreads globally <ref> [10] </ref>, as seen in the mediocre prediction and relatively large residuals at most locations. This is handled via robust estimation (Section 2.3). Finally, a completely novel object generates an average image, and large residuals as shown in the last row.
Reference: [11] <author> H. Murase and S. Nayar. </author> <title> Visual learning and recognition of 3D objects from appearance. </title> <journal> IJCV, </journal> <volume> 14 </volume> <pages> 5-24, </pages> <year> 1995. </year>
Reference-contexts: The notion of hidden state is already implicit in various appearance-based methods for computer vision that have received much attention over the past few years <ref> [17, 11, 13, 9, 2, 10] </ref>. For example, in eigenspace-based methods, an input image is characterized by the vector of coefficients obtained by projecting the image along the directions given by the dominant eigenvectors of the input covariance matrix. <p> This is essentially the eigenspace technique of Turk and Pentland [17] and Murase and Nayar <ref> [11] </ref>. <p> u (t 1), K u = P u R (t) T S 1 bu , S (t) = P u (t 1), and g 0 and g 00 denote the first and second partial derivatives of g with respect to u. of two different 3D objects from the Columbia database <ref> [11] </ref> used for learning the generative matrix U . Each view was 10 ffi apart from the next. Only the 32 fi 32 image region demarcated by the box was used for training to preserve computational efficiency; other regions are assumed to be analyzed by neighboring modules (see Section 4).
Reference: [12] <author> B. Olshausen and D. </author> <title> Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. </title> <journal> Nature, </journal> <volume> 381 </volume> <pages> 607-609, </pages> <year> 1996. </year>
Reference-contexts: It is therefore not surprising that recent work on modeling the response properties of neurons in the visual cortex has focused on alternative generative models using goals ranging from maximizing the sparseness of the coefficients <ref> [12] </ref> to making the coefficients statistically independent (for example, independent component analysis (ICA) [1]). In this paper, we propose a hierarchical appearance-based method for learning, recognizing, and predicting spatiotem-poral sequences of input images. <p> However, natural scenes are rife with higher-order statistical structure that cannot be accounted for by linear pairwise statistics <ref> [12] </ref>. <p> For example, one could allow w to be a non-linear function of the model parameters in order to seek higher-order statistical structure than just linear, pairwise correlations <ref> [12] </ref>. <p> For example, one could use f (x) = g (x) = ffx 2 to allow regularization and avoid over-fitting [14]. Using a function such as f (x) = ff log (1 + x 2 ) causes higher-order correlations to be sought <ref> [12] </ref>. These functions are applied to all components x of a given vector x and the results are summed in the optimization function. 2.2.
Reference: [13] <author> R. Rao and D. Ballard. </author> <title> An active vision architecture based on iconic representations. </title> <journal> Artificial Intelligence, </journal> <volume> 78 </volume> <pages> 461-505, </pages> <year> 1995. </year>
Reference-contexts: The notion of hidden state is already implicit in various appearance-based methods for computer vision that have received much attention over the past few years <ref> [17, 11, 13, 9, 2, 10] </ref>. For example, in eigenspace-based methods, an input image is characterized by the vector of coefficients obtained by projecting the image along the directions given by the dominant eigenvectors of the input covariance matrix.
Reference: [14] <author> R. Rao and D. Ballard. </author> <title> Dynamic model of visual recognition predicts neural response properties in the visual cortex. </title> <journal> Neural Computation, </journal> <volume> 9 </volume> <pages> 805-847, </pages> <year> 1997. </year>
Reference-contexts: A reconstruction of the input image b I can be generated from r by using the following relation which simply inverts the transformation in Equation 1: b I = U r (2) It is well-known (see, for example, <ref> [14] </ref>) that the eigenvector matrix U minimizes the pixel-wise expected reconstruction error function: J (U ) = i=1 2 (where I i denotes the ith pixel of I and U i denotes the ith row of U ) over all inputs subject to the constraint that the columns of U are <p> For example, one could use f (x) = g (x) = ffx 2 to allow regularization and avoid over-fitting <ref> [14] </ref>. Using a function such as f (x) = ff log (1 + x 2 ) causes higher-order correlations to be sought [12]. These functions are applied to all components x of a given vector x and the results are summed in the optimization function. 2.2. <p> It follows from Equation 15 above that in 1 We describe the linear case here for simplicity but the technique readily generalizes (via Taylor series approximations)to non-linear dynamic models <ref> [14] </ref>, which yield extended Kalman filters. the dynamic case: r (t) = V b r (t 1)) + n (t 1) (16) In this dynamic form, the Kalman filter predicts one step into the future using Equation 16, corrects its prediction r using Equation 8 to obtain b r, and uses
Reference: [15] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> Sin-gapore: World Scientific, </publisher> <year> 1989. </year>
Reference-contexts: We must therefore walk the thin line between fitting but not overfitting our parameters to data in order to ensure proper generalization to novel situations. One promising way out of this bias-variance dilemma is to use the Minimum Description Length (MDL) Principle <ref> [15, 18] </ref>. Simply put, the MDL principle advocates balancing the cost of encoding the data given the use of a model with the cost of specifying the model itself (cost is defined in terms of the length of the encoding in bits). <p> Summary and Conclusions This paper presents a new hierarchical appearance-based method for learning, recognizing, and predicting image sequences. The proposed method is based firmly on the information-theoretic MDL principle <ref> [15] </ref> and utilizes ideas from robust statistics [8] for deriving hierarchical Kalman filter estimators that can tolerate significant occlusion and clutter. Kalman filters have previously been used extensively in computer vision [3] and image processing [5].
Reference: [16] <author> C. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell System Technical Journal, </journal> <volume> 27 379-423,623-656, </volume> <year> 1948. </year>
Reference-contexts: Given the true probability distribution (over discrete events) of the various terms in the above equations, the expected length of the optimal code for each term is given by Shannon's optimal coding theorem <ref> [16] </ref>: jL (x)j = log P (X = x) (6) where P (X = x) denotes the probability of the discrete event x.
Reference: [17] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: The notion of hidden state is already implicit in various appearance-based methods for computer vision that have received much attention over the past few years <ref> [17, 11, 13, 9, 2, 10] </ref>. For example, in eigenspace-based methods, an input image is characterized by the vector of coefficients obtained by projecting the image along the directions given by the dominant eigenvectors of the input covariance matrix. <p> One solution is to choose the columns of U to be the first k dominant eigenvec-tors (in terms of maximal eigenvalues) of the input covariance matrix E (II T ) computed from zero-mean samples of input data. This is essentially the eigenspace technique of Turk and Pentland <ref> [17] </ref> and Murase and Nayar [11].
Reference: [18] <author> R. Zemel. </author> <title> A Minimum Description Length Framework for Unsupervised Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1994. </year>
Reference-contexts: We must therefore walk the thin line between fitting but not overfitting our parameters to data in order to ensure proper generalization to novel situations. One promising way out of this bias-variance dilemma is to use the Minimum Description Length (MDL) Principle <ref> [15, 18] </ref>. Simply put, the MDL principle advocates balancing the cost of encoding the data given the use of a model with the cost of specifying the model itself (cost is defined in terms of the length of the encoding in bits). <p> Given a description language L, data D, and model parameters M, the MDL principle advocates minimizing the following cost function <ref> [18] </ref>: J (M; D) = jL (M; D)j = jL (DjM)j + jL (M)j (5) j j denotes length of the description. In our case, D consists of the current input image I and M consists of the parameters U and r. <p> Shannon's coding theorem relates code lengths to discrete probability distributions. Since we will be using continuous (Gaussian) distributions, we need to calculate the probability mass of a particular small interval of values around a given value x <ref> [18] </ref>. Using a trapezoidal approximation, we may estimate the mass under a continuous (in our case, Gaussian) density p in an interval of width w around a value x to be P (X = x) ~ = p (x)w.
References-found: 18

