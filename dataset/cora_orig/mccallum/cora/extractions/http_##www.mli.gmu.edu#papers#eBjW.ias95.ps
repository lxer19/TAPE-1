URL: http://www.mli.gmu.edu/papers/eBjW.ias95.ps
Refering-URL: http://www.mli.gmu.edu/kpubs.html
Root-URL: 
Email: -bloedorn, wnek-@aic.gmu.edu  
Title: Proceedings of the First International Workshop on Intelligent Adaptive Systems (IAS-95) Constructive Induction-based Learning Agents:
Author: Ibrahim F. Imam and Janusz Wnek Eric Bloedorn and Janusz Wnek George Mason 
Keyword: Key words: intelligent agents, constructive induction, multistrategy learning.  
Address: 4400 University Dr., Fairfax VA 22030, USA  
Affiliation: Center  University  
Note: (Eds.), pp. 38-51, Melbourne Beach, Florida, 1995.  
Abstract: This paper introduces a new type of intelligent agent called a constructive induction-based learning agent (CILA). This agent differs from other adaptive agents because it has the ability to not only learn how to assist a user in some task, but also to incrementally adapt its knowledge representation space to better fit the given learning task. The agents ability to autonomously make problem-oriented modifications to the originally given representation space is due to its constructive induction (CI) learning method. Selective induction (SI) learning methods, and agents based on these methods, rely on a good representation space. A good representation space has no misclassification noise, inter-correlated attributes or irrelevant attributes. Our proposed CILA has methods for overcoming all of these problems. In agent domains with poor representations, the CI-based learning agent will learn more accurate rules and be more useful than an SI-based learning agent. This paper gives an architecture for a CI-based learning agent and gives an empirical comparison of a CI and SI for a set of six abstract domains involving DNF-type (disjunctive normal form) descriptions. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bloedorn, E. and Michalski, </author> <title> R.S., Constructive Induction from Data in AQ17-DCI: Further Experiments, </title> <institution> Center for Artificial Intelligence, George Mason University, </institution> <address> MLI 91-12, </address> <year> 1991. </year>
Reference-contexts: A number of systems have been developed with this goal. These systems can be classified into data-driven, hypothesis-driven, knowledge-driven and multistrategy (Wnek and Michalski, 1994). Some representative of each of these types are: AQ17-DCI <ref> (Bloedorn and 6 Michalski, 1991) </ref>, BLIP (Wrobel, 1989), CITRE (Matheus and Rendell, 1989), Pagallo and Haussler's FRINGE, GREEDY3 and GROVE (Pagallo and Haussler, 1990), MIRO (Drastal, Czako and Raatz, 1989) and STABB (Utgoff, 1986). 3. <p> This method of hypothesis analysis as a means of constructing new attributes is detailed in a number of places including (Wnek, 1993; Wnek and Michalski, 1994). b) Data-driven (DCI) methods build new attributes based on an analysis of the training data. One such method is AQ17-DCI <ref> (Bloedorn and Michalski, 1991) </ref>. In AQ17-DCI new attributes are constructed based on a generate and test method using generic domain-independent arithmetic and boolean operators.
Reference: <author> Bloedorn, E., Michalski, R.S. and Wnek, J., </author> <title> Multistrategy Constructive Induction: </title> <booktitle> AQ17-MCI, Second International Workshop on Multistrategy Learning, </booktitle> <pages> pp. 188-203, </pages> <address> Harpers Ferry, WV, </address> <year> 1993. </year>
Reference-contexts: However, because no single method for constructive induction performed best for all the problems posed an approach for combining methods is needed. Further work which details the area of applicability of individual constructive induction methods is needed. Preliminary work in this area is described in <ref> (Bloedorn et. al, 1993) </ref>. Acknowledgements This research was conducted in the Center for Machine Learning and Inference at George Mason University. The Center's research is supported in part by the Advanced Research Projects Agency under Grant No. N00014-91-J-1854, administered by the Office of Naval Research, and the Grant No.
Reference: <author> Bloedorn, E., Michalski, R.S. and Wnek, J., </author> <title> Matching Methods with Problems: A Comparative Analysis of Constructive Induction Approaches, Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 94-2, </type> <institution> Center for AI, George Mason University, Fairfax, VA, </institution> <year> 1994. </year>
Reference-contexts: An Empirical Comparison 4.1 Descriptions of methods evaluated In order to determine the effectiveness of different CI methods, a set of experiments was performed. These experiments are described in greater detail in <ref> (Bloedorn, et. al, 1994) </ref>. This set of experiments samples a wide variety of possible learning problems including: misclassification noise, attribute-value noise, overprecision, inappropriate attributes and irrelevant attributes. In all of these experiments the AQ15c program was used as the learning algorithm (Wnek et al., 1995).
Reference: <author> Clark, P. and Niblett, T., </author> <title> The CN2 Induction Algorithm, </title> <journal> Machine Learning, </journal> <volume> Vol. 3, </volume> <pages> pp. 261-284, </pages> <year> 1989. </year>
Reference-contexts: Some tree-pruning methods include (Quinlan, 1986) and (Mingers, 1989). Pruning methods applied to learned rules include the AQ family of programs (Michalski, 1986; Zhang, 1989), and CN2 <ref> (Clark, 1989) </ref> and pruning applied to the training data based on rule-weight is presented in (Pachowicz, Bala and Zhang, 1992). The source of this inappropriateness can lie in the set of attribute values, or the attributes themselves.
Reference: <author> Dent, L., Boticario, J., McDermott, J., Mitchell, T. and Zabowski, D., </author> <title> "A Personal Learning Apprentice," </title> <booktitle> Proceedings of Tenth National Conference on AI, </booktitle> <address> San Jose, CA, </address> <month> July 12-16 </month> <year> 1992. </year>
Reference-contexts: The first approach is too difficult for most users, and the second approach is too hard for application developers, who must accurately predict the current and future needs of users (Maes, 1994). Another proposed approach is to build into the agents an ability to learn required skills from experience <ref> (Dent, 1992, Maes, 1994) </ref>. In this method, the agent gains competence by interacting 2 with the user as the user performs some tasks.
Reference: <author> Drastal, G., Czako, G. and Raatz, S., </author> <title> Induction in an Abstraction Space: A Form of Constructive Induction, </title> <booktitle> Proceedings of IJCAI-89, </booktitle> <pages> pp. 707-712, </pages> <address> Detroit, MI, </address> <year> 1989. </year>
Reference-contexts: These systems can be classified into data-driven, hypothesis-driven, knowledge-driven and multistrategy (Wnek and Michalski, 1994). Some representative of each of these types are: AQ17-DCI (Bloedorn and 6 Michalski, 1991), BLIP (Wrobel, 1989), CITRE (Matheus and Rendell, 1989), Pagallo and Haussler's FRINGE, GREEDY3 and GROVE (Pagallo and Haussler, 1990), MIRO <ref> (Drastal, Czako and Raatz, 1989) </ref> and STABB (Utgoff, 1986). 3. An Architecture for a Representation Space Adapting Agent In order to build an intelligent agent that can gain enough competence to be useful to an individual that agent must acquire a great deal of knowledge.
Reference: <author> Kerber, R., ChiMerge: </author> <title> Discretization of Numeric Attributes, </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Interlligence, </booktitle> <pages> pp. 123-128, </pages> <address> San Jose, CA, </address> <year> 1992. </year>
Reference-contexts: Various methods for automatic discretization of attribute data have been proposed. Some of these methods are quite simple such as equal-width intervals, and equal-frequency intervals. Others such as C4.5 (Quinlan, 1993) , and SCALE which implements the Chimerge algorithm <ref> (Kerber, 1992) </ref> are more complex. Inappropriate attributes are those attributes which are relevant to the problem at hand, but which pose the problem in such a way that the descriptive constructs of the language are inadequate. <p> Currently the program which performs this modification, SCALE, implements both a c 2 method and an equal-interval-size method. The c 2 method calculates the correlation between an attribute-value interval and the class. Using a c 2 correlation to quantize data was first proposed by Kerber <ref> (Kerber, 1992) </ref>. Attribute value modification (AVM) selects a set V' V (where V is the domain of A) of allowable values for attribute A.
Reference: <author> Maes, P., </author> <title> Agents that Reduce Work and Information Overload, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 37, No. 7, </volume> <pages> pp. 31-40, </pages> <year> 1994. </year>
Reference-contexts: The first approach is too difficult for most users, and the second approach is too hard for application developers, who must accurately predict the current and future needs of users <ref> (Maes, 1994) </ref>. Another proposed approach is to build into the agents an ability to learn required skills from experience (Dent, 1992, Maes, 1994). In this method, the agent gains competence by interacting 2 with the user as the user performs some tasks. <p> The ability to modify the representation space is an important element in all four types of learning. Previously reported research in building learning agents uses a predefined set of attributes to describe the learning example. For example, in describing email messages, the Maxims agent <ref> (Maes, 1994) </ref> uses features such as the sender and receiver of the message and key words in the "Subject" field. CAP, an agent for meeting scheduling (Mitchell, 1994) uses features such as event_type, time and duration.
Reference: <author> Matheus, C.J. and Rendell, L., </author> <title> Constructive Induction on Decision Trees, </title> <booktitle> Proceedings of IJCAI-89, </booktitle> <pages> pp. 645-650, </pages> <address> Detroit, MI, </address> <year> 1989. </year>
Reference-contexts: A number of systems have been developed with this goal. These systems can be classified into data-driven, hypothesis-driven, knowledge-driven and multistrategy (Wnek and Michalski, 1994). Some representative of each of these types are: AQ17-DCI (Bloedorn and 6 Michalski, 1991), BLIP (Wrobel, 1989), CITRE <ref> (Matheus and Rendell, 1989) </ref>, Pagallo and Haussler's FRINGE, GREEDY3 and GROVE (Pagallo and Haussler, 1990), MIRO (Drastal, Czako and Raatz, 1989) and STABB (Utgoff, 1986). 3.
Reference: <author> Michalski, R.S., Mozetic, I., Hong, J. and Lavrac, N., </author> <title> The MultiPurpose Incremental Learning 12 System AQ15 and its Testing Application to Three Medical Domains, </title> <booktitle> Proceedings of AAAI-86, </booktitle> <pages> pp. 1041-1045, </pages> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference: <author> Michalski, </author> <title> R.S., A Theory and Methodology of Inductive Learning, Machine Learning: An Artificial Intelligence Approach, Vol. I, R.S. </title> <editor> Michalski, J.G. Carbonell and T.M. Mitchell (Eds.), </editor> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference: <author> Mingers, J., </author> <title> An Empirical Comparison of Pruning Methods for Decision-Tree Induction, </title> <journal> Machine Learning, </journal> <volume> Vol. 2, </volume> <year> 1989. </year>
Reference-contexts: Some methods for dealing with incorrect instances or attribute values are based on identifying noisy or exceptional instances by using statistical methods applied to the distribution of attribute values, or instances or other significance measures applied to learned hypotheses. Some tree-pruning methods include (Quinlan, 1986) and <ref> (Mingers, 1989) </ref>. Pruning methods applied to learned rules include the AQ family of programs (Michalski, 1986; Zhang, 1989), and CN2 (Clark, 1989) and pruning applied to the training data based on rule-weight is presented in (Pachowicz, Bala and Zhang, 1992).
Reference: <author> Mitchell, T., Caruana, R., Freitag, D., McDermott, J. and Zabowski, D., </author> <title> "Experience with a Personal Learning Assistant," </title> <journal> Communications of the ACM, </journal> <volume> Vol 37, No. 7, </volume> <pages> pp. 81-91, </pages> <year> 1994. </year>
Reference-contexts: For example, in describing email messages, the Maxims agent (Maes, 1994) uses features such as the sender and receiver of the message and key words in the "Subject" field. CAP, an agent for meeting scheduling <ref> (Mitchell, 1994) </ref> uses features such as event_type, time and duration. CAP does automatically calculate the values of a number of features such as number-of-attendees, single-attendee, but the feature set is still predefined.
Reference: <author> Pachowicz, P.W., Bala, J. and Zhang, J., </author> <title> Iterative Rule Simplification for Noise-Tolerant Inductive Learning, </title> <booktitle> Proceedings of the Fourth International Conference on Tools for Artificial Intelligence, </booktitle> <pages> pp. 452-453, </pages> <address> Arlington, VA, </address> <year> 1992. </year>
Reference-contexts: Some tree-pruning methods include (Quinlan, 1986) and (Mingers, 1989). Pruning methods applied to learned rules include the AQ family of programs (Michalski, 1986; Zhang, 1989), and CN2 (Clark, 1989) and pruning applied to the training data based on rule-weight is presented in <ref> (Pachowicz, Bala and Zhang, 1992) </ref>. The source of this inappropriateness can lie in the set of attribute values, or the attributes themselves. An example of inappropriate attribute value set would be one in which the provided values blur the concept boundaries by being too broad or too precise.
Reference: <author> Pagallo, G. and Haussler, D., </author> <title> Boolean Feature Discovery in Empirical Learning, </title> <journal> Machine Learning, </journal> <volume> Vol. 5, </volume> <pages> pp. 71-99, </pages> <year> 1990. </year>
Reference-contexts: These systems can be classified into data-driven, hypothesis-driven, knowledge-driven and multistrategy (Wnek and Michalski, 1994). Some representative of each of these types are: AQ17-DCI (Bloedorn and 6 Michalski, 1991), BLIP (Wrobel, 1989), CITRE (Matheus and Rendell, 1989), Pagallo and Haussler's FRINGE, GREEDY3 and GROVE <ref> (Pagallo and Haussler, 1990) </ref>, MIRO (Drastal, Czako and Raatz, 1989) and STABB (Utgoff, 1986). 3. An Architecture for a Representation Space Adapting Agent In order to build an intelligent agent that can gain enough competence to be useful to an individual that agent must acquire a great deal of knowledge.
Reference: <author> Quinlan, J.R., C4.5: </author> <title> Programs for Machine Learning, </title> <address> San Mateo, </address> <publisher> Morgan Kaufmann, </publisher> <address> CA, </address> <year> 1993. </year>
Reference-contexts: Various methods for automatic discretization of attribute data have been proposed. Some of these methods are quite simple such as equal-width intervals, and equal-frequency intervals. Others such as C4.5 <ref> (Quinlan, 1993) </ref> , and SCALE which implements the Chimerge algorithm (Kerber, 1992) are more complex. Inappropriate attributes are those attributes which are relevant to the problem at hand, but which pose the problem in such a way that the descriptive constructs of the language are inadequate.
Reference: <author> Quinlan, J.R., </author> <title> The Effect of Noise on Concept Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: Some methods for dealing with incorrect instances or attribute values are based on identifying noisy or exceptional instances by using statistical methods applied to the distribution of attribute values, or instances or other significance measures applied to learned hypotheses. Some tree-pruning methods include <ref> (Quinlan, 1986) </ref> and (Mingers, 1989). Pruning methods applied to learned rules include the AQ family of programs (Michalski, 1986; Zhang, 1989), and CN2 (Clark, 1989) and pruning applied to the training data based on rule-weight is presented in (Pachowicz, Bala and Zhang, 1992).
Reference: <author> Rendell, L. and Seshu, R., </author> <title> Learning Hard Concepts Through Constructive Induction: Framework and Rationale, </title> <journal> Computational Intelligence, </journal> <volume> Vol. 6, </volume> <pages> pp. 247-270, </pages> <year> 1990. </year>
Reference-contexts: Selective induction methods assume that the given data are in an appropriate form so that examples which are close to each other in the representation space are also close to, or identical in class membership as well <ref> (Rendell and Seshu, 1990) </ref>. When any of these assumptions are violated the representation space is inadequate for selective induction and poor descriptors (low predictive accuracy and high complexity) result. 5 Problem incorrectness occurs when some attribute-values, attributes or instances are incorrectly labeled.
Reference: <author> Rendell, L., Seshu, R. and Tcheng, D., </author> <title> More Robust Concept Learning Using Dynamically-Variable Bias, </title> <booktitle> Tenth International Workshop on Machine Learning, </booktitle> <pages> pp. 66-78, </pages> <year> 1993. </year>
Reference: <author> Utgoff, P.E., </author> <title> Shift of Bias for Inductive Learning, </title> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. II, </volume> <editor> J.G. Carbonell and T.M. Mitchell (Eds.) R.S. Michalski (Eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: Some representative of each of these types are: AQ17-DCI (Bloedorn and 6 Michalski, 1991), BLIP (Wrobel, 1989), CITRE (Matheus and Rendell, 1989), Pagallo and Haussler's FRINGE, GREEDY3 and GROVE (Pagallo and Haussler, 1990), MIRO (Drastal, Czako and Raatz, 1989) and STABB <ref> (Utgoff, 1986) </ref>. 3. An Architecture for a Representation Space Adapting Agent In order to build an intelligent agent that can gain enough competence to be useful to an individual that agent must acquire a great deal of knowledge.
Reference: <author> Wnek, J., </author> <title> Hypothesis-driven Constructive Induction, </title> <type> PhD dissertation, </type> <institution> School of Information Technology and Engineering, George Mason University, Fairfax, VA, University Microfilms International, </institution> <address> Ann Arbor, MI, </address> <year> 1993. </year>
Reference: <author> Wnek, J. and Michalski, </author> <title> R.S., Discovering Representation Space Transformations for Learning Concept Descriptions Containing DNF and M-of-N Rules, </title> <booktitle> Working Notes of the ML-COLT94 Workshop on Constructive Induction, </booktitle> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: When inappropriate attributes exist attribute construction methods can be invoked which try to combine the given attributes in more problem-relevant manner. A number of systems have been developed with this goal. These systems can be classified into data-driven, hypothesis-driven, knowledge-driven and multistrategy <ref> (Wnek and Michalski, 1994) </ref>. Some representative of each of these types are: AQ17-DCI (Bloedorn and 6 Michalski, 1991), BLIP (Wrobel, 1989), CITRE (Matheus and Rendell, 1989), Pagallo and Haussler's FRINGE, GREEDY3 and GROVE (Pagallo and Haussler, 1990), MIRO (Drastal, Czako and Raatz, 1989) and STABB (Utgoff, 1986). 3. <p> An constructive induction-based learning agent is able to expand or contract the provided representation space either automatically, or based on knowledge provided by the user using one or more of the different types of CI: data-driven, hypothesis-driven, knowledge-driven and multistrategy <ref> (Wnek and Michalski, 1994) </ref>. An architecture for a constructive induction-based learning agent is shown in Figure 3. In this architecture the agent acts as an assistant to the user in dealing with the environment. The user can access the environment directly or through the agent.
Reference: <author> Wnek, J. and Michalski, </author> <title> R.S., Hypothesis-driven Constructive Induction in AQ17-HCI: A Method and Experiments, </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> pp. 139-168, </pages> <booktitle> Vol. </booktitle> <volume> 14, </volume> <pages> pp. 139-168, </pages> <year> 1994. </year>
Reference-contexts: When inappropriate attributes exist attribute construction methods can be invoked which try to combine the given attributes in more problem-relevant manner. A number of systems have been developed with this goal. These systems can be classified into data-driven, hypothesis-driven, knowledge-driven and multistrategy <ref> (Wnek and Michalski, 1994) </ref>. Some representative of each of these types are: AQ17-DCI (Bloedorn and 6 Michalski, 1991), BLIP (Wrobel, 1989), CITRE (Matheus and Rendell, 1989), Pagallo and Haussler's FRINGE, GREEDY3 and GROVE (Pagallo and Haussler, 1990), MIRO (Drastal, Czako and Raatz, 1989) and STABB (Utgoff, 1986). 3. <p> An constructive induction-based learning agent is able to expand or contract the provided representation space either automatically, or based on knowledge provided by the user using one or more of the different types of CI: data-driven, hypothesis-driven, knowledge-driven and multistrategy <ref> (Wnek and Michalski, 1994) </ref>. An architecture for a constructive induction-based learning agent is shown in Figure 3. In this architecture the agent acts as an assistant to the user in dealing with the environment. The user can access the environment directly or through the agent.
Reference: <author> Wnek, J., Kaufman, K., Bloedorn, E. and Michalski, </author> <title> R.S., Selective Induction Learning System AQ15c: The Method and Users Guide, Reports of the Machine Learning and Inference Laboratory, MLI 95-4, Center for Machine Learning and Inference, </title> <institution> George Mason University, Fairfax, VA, </institution> <year> 1995. </year>
Reference-contexts: These experiments are described in greater detail in (Bloedorn, et. al, 1994). This set of experiments samples a wide variety of possible learning problems including: misclassification noise, attribute-value noise, overprecision, inappropriate attributes and irrelevant attributes. In all of these experiments the AQ15c program was used as the learning algorithm <ref> (Wnek et al., 1995) </ref>. Each of the CI methods must transform the difficult problem into one in which AQ15c would learn simple, predictively accurate rules. A single method for hypothesis generation is used because the types of rules learned by AQ are comprehensible and efficient in decision making.
Reference: <author> Wrobel, S., </author> <title> Demand-driven Concept Formation, Knowledge Representation and Organization in Machine Learning, </title> <editor> K. Morik (Eds.), </editor> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: A number of systems have been developed with this goal. These systems can be classified into data-driven, hypothesis-driven, knowledge-driven and multistrategy (Wnek and Michalski, 1994). Some representative of each of these types are: AQ17-DCI (Bloedorn and 6 Michalski, 1991), BLIP <ref> (Wrobel, 1989) </ref>, CITRE (Matheus and Rendell, 1989), Pagallo and Haussler's FRINGE, GREEDY3 and GROVE (Pagallo and Haussler, 1990), MIRO (Drastal, Czako and Raatz, 1989) and STABB (Utgoff, 1986). 3.
Reference: <author> Zhang, J. and Michalski, </author> <title> R.S., A Preference Criterion in Constructive Learning: A Discussion of Basic Issues, </title> <booktitle> Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pp. 17-19, </pages> <address> Ithaca, NY, </address> <year> 1989. </year>
References-found: 26

