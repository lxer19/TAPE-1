URL: http://www.cs.cmu.edu/~thrun/papers/thrun.cogsci96.ps.gz
Refering-URL: http://www.cs.cmu.edu/~thrun/papers/full.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: The Role of Transfer in Learning (extended abstract)  
Author: Sebastian Thrun 
Web: http://www.cs.cmu.edu/~thrun/  
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract-found: 0
Intro-found: 1
Reference: <author> Thrun, S. </author> <year> (1996). </year> <title> Explanation-Based Neural Network Learning: A Lifelong Learning Approach. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Transfer To successfully transfer knowledge across multiple learning tasks, a learner must identify aspects that its past (and future) learning tasks have in common. Recent research has produced a variety of approaches that are capable of transferring knowledge across multiple inductive learning tasks (see the survey and references in <ref> (Thrun, 1996) </ref>). Different approaches differ * in the way they generalize when facing the first learning task, and * in the way their generalization is affected when previously learned knowledge is transferred. <p> Using object recognition from color camera images as an example, a recent study compared a variety of lifelong learning with each other, and with the corresponding conventional learning methods <ref> (Thrun, 1996) </ref>. In particular, we examined the generalization accuracy that was obtained after presenting only a single view of the target object (along with a counterexample). The approaches that were capable of transferring knowledge were also provided with views of five additional objects. <p> Learning To Act The ideas presented here are also applicable to reinforcement learning (Sutton, 1991). Reinforcement learning addresses the problem of learning to act from delayed reward. The SKILLS algorithm <ref> (Thrun & Schwartz, 1996) </ref>, a version of reinforcement learning which selectively transfers knowledge across different learning tasks, discovers partial action policies in multiple reinforcement learning tasks based upon a minimum description length argument. These partial policies can be re-used as building blocks in other reinforcement learning tasks. <p> For example, when training a mobile robot to learn to navigate to a designated target object in an in-door office environment, we also found that reinforcement learning converges significantly faster when knowledge (in this case: neural network action models) acquired in previous learning tasks is being re-used <ref> (Thrun, 1996) </ref>. Conclusion We draw three primary conclusions from this research: First, transfer, if applied correctly, is very likely to improve the results of learning, given that more than just a single learning tasks is available.
Reference: <author> Thrun, S. & O'Sullivan, J. </author> <year> (1996). </year> <title> Discovering structure in multiple learning tasks: The TC algorithm. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Transfer To successfully transfer knowledge across multiple learning tasks, a learner must identify aspects that its past (and future) learning tasks have in common. Recent research has produced a variety of approaches that are capable of transferring knowledge across multiple inductive learning tasks (see the survey and references in <ref> (Thrun, 1996) </ref>). Different approaches differ * in the way they generalize when facing the first learning task, and * in the way their generalization is affected when previously learned knowledge is transferred. <p> Using object recognition from color camera images as an example, a recent study compared a variety of lifelong learning with each other, and with the corresponding conventional learning methods <ref> (Thrun, 1996) </ref>. In particular, we examined the generalization accuracy that was obtained after presenting only a single view of the target object (along with a counterexample). The approaches that were capable of transferring knowledge were also provided with views of five additional objects. <p> Learning To Act The ideas presented here are also applicable to reinforcement learning (Sutton, 1991). Reinforcement learning addresses the problem of learning to act from delayed reward. The SKILLS algorithm <ref> (Thrun & Schwartz, 1996) </ref>, a version of reinforcement learning which selectively transfers knowledge across different learning tasks, discovers partial action policies in multiple reinforcement learning tasks based upon a minimum description length argument. These partial policies can be re-used as building blocks in other reinforcement learning tasks. <p> For example, when training a mobile robot to learn to navigate to a designated target object in an in-door office environment, we also found that reinforcement learning converges significantly faster when knowledge (in this case: neural network action models) acquired in previous learning tasks is being re-used <ref> (Thrun, 1996) </ref>. Conclusion We draw three primary conclusions from this research: First, transfer, if applied correctly, is very likely to improve the results of learning, given that more than just a single learning tasks is available.
Reference: <author> Sutton, R. S. </author> <year> (1991). </year> <title> Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming, </title> <booktitle> In Advances in Neural Information Processing Systems 3. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The results, no transfer ? selective transfer which are superior to those obtained with non-selective transfer, illustrate the role of proper task selection in the transfer of knowledge. Learning To Act The ideas presented here are also applicable to reinforcement learning <ref> (Sutton, 1991) </ref>. Reinforcement learning addresses the problem of learning to act from delayed reward. The SKILLS algorithm (Thrun & Schwartz, 1996), a version of reinforcement learning which selectively transfers knowledge across different learning tasks, discovers partial action policies in multiple reinforcement learning tasks based upon a minimum description length argument.
Reference: <author> Thrun, S. & Schwartz, A. </author> <year> (1996). </year> <title> Finding Structure in Reinforcement Learning. </title> <booktitle> In Advances in Neural Information Processing Systems 7. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Transfer To successfully transfer knowledge across multiple learning tasks, a learner must identify aspects that its past (and future) learning tasks have in common. Recent research has produced a variety of approaches that are capable of transferring knowledge across multiple inductive learning tasks (see the survey and references in <ref> (Thrun, 1996) </ref>). Different approaches differ * in the way they generalize when facing the first learning task, and * in the way their generalization is affected when previously learned knowledge is transferred. <p> Using object recognition from color camera images as an example, a recent study compared a variety of lifelong learning with each other, and with the corresponding conventional learning methods <ref> (Thrun, 1996) </ref>. In particular, we examined the generalization accuracy that was obtained after presenting only a single view of the target object (along with a counterexample). The approaches that were capable of transferring knowledge were also provided with views of five additional objects. <p> Learning To Act The ideas presented here are also applicable to reinforcement learning (Sutton, 1991). Reinforcement learning addresses the problem of learning to act from delayed reward. The SKILLS algorithm <ref> (Thrun & Schwartz, 1996) </ref>, a version of reinforcement learning which selectively transfers knowledge across different learning tasks, discovers partial action policies in multiple reinforcement learning tasks based upon a minimum description length argument. These partial policies can be re-used as building blocks in other reinforcement learning tasks. <p> For example, when training a mobile robot to learn to navigate to a designated target object in an in-door office environment, we also found that reinforcement learning converges significantly faster when knowledge (in this case: neural network action models) acquired in previous learning tasks is being re-used <ref> (Thrun, 1996) </ref>. Conclusion We draw three primary conclusions from this research: First, transfer, if applied correctly, is very likely to improve the results of learning, given that more than just a single learning tasks is available.
References-found: 4

