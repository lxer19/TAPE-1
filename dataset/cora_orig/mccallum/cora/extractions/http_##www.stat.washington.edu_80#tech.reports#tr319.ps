URL: http://www.stat.washington.edu:80/tech.reports/tr319.ps
Refering-URL: http://www.stat.washington.edu:80/tech.reports/
Root-URL: 
Title: Using the Stochastic Gradient Method to Fit Polychotomous Regression Models  
Author: Charles Kooperberg and Charles J. Stone 
Keyword: Key Words: Backpropagation; Linear splines; MARS; Multiple classification; Neural networks; Speech recognition.  
Abstract: Technical Report No. 319 April 7, 1997 University of Washington Department of Statistics Seattle, Washington 98195-4322 Abstract Kooperberg, Bose, and Stone (1997) introduced polyclass, a methodology that uses adaptively selected linear splines and their tensor products to model conditional class probabilities. The authors attempted to develop a methodology that would work well on small and moderate size problems and would scale up to large problems. However, the version of polyclass that was developed for large problems was impractical in that it required two months of cpu time to apply it to a large data set. A modification to this methodology involving the use of the stochastic gradient (on-line) method in fitting polyclass models to given sets of basis functions is developed here that makes the methodology applicable to large data sets. In particular, it is successfully applied to a phoneme recognition problem involving 45 phonemes, 81 features, 150,000 cases in the training sample, 1000 basis functions, and 44,000 unknown parameters. Comparisons with neural networks are made both on the original problem and on a three-vowel subproblem. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bourlard, H. A., and Morgan, N. </author> <year> (1994), </year> <title> Connectionist Speech Recognition, </title> <address> Boston: </address> <publisher> Kluwer. </publisher>
Reference-contexts: Such a conditional probability distribution (or, more precisely, a likelihood that is obtained by weighting the estimated probabilities by the empirically determined frequencies of the phonemes) can be used as input to train (estimate) a hidden Markov model, which in turn can be used for automatic speech recognition <ref> (Bourlard and Morgan 1994) </ref>. In the hybrid approach described by Bourlard and Morgan, a feed-forward neural network is used to estimate these probabilities. For each utterance in the database, we used the phonemes being spoken at times 12.5 ms apart to define the cases.
Reference: <author> Cheng, B., and Titterington, D. M. </author> <year> (1994), </year> <title> "Neural Networks: a review from a statistical perspective (with discussion)," </title> <journal> Statistical Science, </journal> <volume> 9, </volume> <pages> 2-54. </pages>
Reference-contexts: The quasi-Newton algorithm, which was used for this purpose, is really impractical for such a large problem. Neural networks <ref> (see, for example, Cheng and Titterington 1994 and Ripley 1994, 1996) </ref> are frequently used for large classification problems. They have several advantages, including generally good performance and ease of implementation.
Reference: <author> Cole, R., Noel, M, Burnett, D. C., Fanty, M., Lander, T., Oshika, B., and Sutton, S. </author> <year> (1994), </year> <title> "Corpus Development Activities at the Center for Spoken Language Understanding," </title> <type> Technical Report, </type> <institution> CSLU, Portland, Oregon. </institution>
Reference: <author> Cole, R. A., Roginski, K., and Fanty, M. </author> <year> (1992), </year> <title> "A telephone speech database of spelled and spoken names," </title> <booktitle> Proceedings of the International Conference on Spoken Language Processing, </booktitle> <address> Banff, Alberta, Canada, </address> <pages> 891-893. </pages>
Reference: <author> Dietterich, T. G., and Bakiri, G. </author> <year> (1995), </year> <title> "Solving multiclass learning problems via error-correcting output codes," </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 263-286. </pages>
Reference: <author> Friedman, J. H. </author> <year> (1996), </year> <title> "Another approach to polychotomous classification," </title> <type> Technical Report. </type>
Reference: <author> Gauvain, J. L., Lamel, L., Adda, G., and Adda-Decker, M. </author> <year> (1994), </year> <title> "Speaker-independent continuous speech dictation," </title> <journal> Speech Communication, </journal> <volume> 15, </volume> <pages> 21-37. </pages>
Reference-contexts: It should be noted that the person who classified the phoneme being spoken was not aware of the text of the utterance. The phoneme transcription, which we obtained from the International Computer Science Institute (ICSI) in Berkeley, California, is based on the LIMSI phonetic alphabet <ref> (Gauvain et al. 1994) </ref>. The utterances were also processed to produce perceptual linear predictive (PLP) features (Hermansky 1990; Rabiner and Juang 1993; Bourlard and Morgan 1994). Every 12.5 ms the audible spectrum is determined from a concentric 25 ms interval of sound.
Reference: <author> Hastie, T., and Tibshirani, R. </author> <year> (1997), </year> <title> "Contribution to the discussion of a paper by Stone, Hansen, </title> <journal> Kooperberg, and Truong," Annals of Statistics, </journal> <note> to appear. </note>
Reference: <author> Jordan, M. I., and Jacobs, R. A. </author> <year> (1994), </year> <title> "Hierarchical mixtures of experts and the EM algorithm," </title> <journal> Neural Computation, </journal> <volume> 6, </volume> <pages> 181-214. </pages>
Reference-contexts: Inspired by the success of stochastic (on-line) gradient versions of the backpropagation algorithm for fitting neural networks (Rumelhart and McClelland 1986) and the EM algorithm for fitting the hierarchical mixtures of experts (HME) architecture <ref> (Jordan and Jacobs 1994) </ref>, we investigate in this paper the use of the stochastic gradient method instead of the quasi-Newton algorithm in fitting polyclass models corresponding to given sets of basis functions.
Reference: <author> Kennedy, W. J., and Gentle, J. E. </author> <year> (1980), </year> <title> Statistical Computing, </title> <address> New York: </address> <publisher> Marcel Dekker. </publisher>
Reference: <author> Kooperberg, C., Bose, S., and Stone, C. J. </author> <year> (1997), </year> <title> "Polychotomous regression," </title> <journal> Journal of the American Statistical Association, </journal> <volume> 92, </volume> <pages> 117-127. </pages>
Reference: <author> Rabiner, L., and Juang, B.-H. </author> <year> (1993), </year> <title> Fundamentals of Speech Recognition, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference: <author> Ripley, B. </author> <year> (1994), </year> <title> "Neural networks and related methods for classification," </title> <journal> Journal of the Royal Statistical Society Series B, </journal> <volume> 56, </volume> <pages> 409-456. </pages>
Reference-contexts: The quasi-Newton algorithm, which was used for this purpose, is really impractical for such a large problem. Neural networks <ref> (see, for example, Cheng and Titterington 1994 and Ripley 1994, 1996) </ref> are frequently used for large classification problems. They have several advantages, including generally good performance and ease of implementation.
Reference: <author> Ripley, B. </author> <year> (1996), </year> <title> Pattern Recognition and neural networks, </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: There is a substantial literature on schemes for adjusting the learning rate <ref> (see Ripley 1996 for an overview) </ref>, but there are few theoretical results supporting these schemes. White (1989) gives conditions under which convergence should occur. In particular, an algebraic decrease of the learning rate satisfies these conditions. We tried a number of schemes for adjusting the learning rate.
Reference: <editor> Rumelhart, D. E., and McClelland, J. L. (eds) (1986), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1. Foundations, </booktitle> <address> Cambridge, MA: </address> <institution> MIT Press. The Annals of Statistics, </institution> <note> to appear. </note>
Reference-contexts: A polyclass model can be viewed as a special type of neural network, in which the inputs are not the raw features, but adaptively selected basis functions. Inspired by the success of stochastic (on-line) gradient versions of the backpropagation algorithm for fitting neural networks <ref> (Rumelhart and McClelland 1986) </ref> and the EM algorithm for fitting the hierarchical mixtures of experts (HME) architecture (Jordan and Jacobs 1994), we investigate in this paper the use of the stochastic gradient method instead of the quasi-Newton algorithm in fitting polyclass models corresponding to given sets of basis functions.
Reference: <author> White, H. </author> <year> (1989), </year> <title> "Some asymptotic results for learning in single hidden-layer feed forward networks," </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84, 1003-1013. Correction, 87, 1252. </volume> <pages> 16 </pages>
References-found: 16

