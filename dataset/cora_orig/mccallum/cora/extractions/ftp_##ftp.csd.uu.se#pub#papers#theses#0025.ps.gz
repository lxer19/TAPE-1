URL: ftp://ftp.csd.uu.se/pub/papers/theses/0025.ps.gz
Refering-URL: http://www.csd.uu.se/~thomasl/reform.html
Root-URL: 
Title: Data-parallel Implementation of Prolog  
Author: Johan Bevemyr 
Degree: Thesis for the Degree of Doctor of Philosophy  
Date: UPPSALA 1996  
Address: Uppsala University  
Affiliation: Computing Science Department  
Note: UPPSALA THESES IN COMPUTING SCIENCE 25  
Abstract-found: 0
Intro-found: 0
Reference: <institution> The numbers inside braces indicate on which pages each citation occured. </institution>
Reference: 1. <author> H. At-Kaci, </author> <title> The WAM: A (Real)Tutorial, </title> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> -4 </month>
Reference: 2. <author> K. Ali, </author> <title> Incremental Garbage Collection for OR-Parallel Prolog Based on WAM, </title> <booktitle> Gigalips Workshop, </booktitle> <year> 1989. </year> <month> -22 </month>
Reference-contexts: OR-parallel systems such as Muse [4] and Aurora [44, 129] use more or less sequential mark-sweep collectors <ref> [2, 71, 72] </ref>. The Aurora collector [220] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [215] for OR-parallel execution with its more complicated data structures. The closest we get to a parallel collector for an AND-parallel Prolog is Cram-mond's [59] mark-sweep collector for Parlog.
Reference: 3. <author> K. Ali, </author> <title> A Parallel Copying Garbage Collection Scheme for Shared-Memory Multiprocessors, </title> <journal> New Generation Computing, </journal> <volume> 14(1) </volume> <pages> 53-77, </pages> <year> 1996. </year> <journal> -12, </journal> <volume> 22, 142, </volume> <pages> 145 </pages>
Reference-contexts: The algorithm uses a mark and copy technique for handling internal pointers. The algorithm uses ideas from our sequential Prolog collector, and from a general parallel copying collection scheme described by Ali <ref> [3] </ref>. We show how the resulting collector can be made generational. An improved strategy for load balancing for Prolog is presented. 1.6. <p> Its execution time is proportional to the size of the largest heap instead of the live data. The space requirements are, at worst, as large as for a copying collector since the import stacks may be as large as the live data. Ali <ref> [3] </ref> describes a general copying collector for shared memory. It is a two space collector (to- and from-space) where each subspace consists of a number of segments. Processing elements (PEs) allocate data in the segments in from-space. Collection takes place when from-space fills up. <p> It has also been shown how to parallelise copying garbage collection for shared memory multiprocessors <ref> [3] </ref>. We present a collection scheme that combines these techniques resulting in a parallel generational copying garbage collector for Prolog. The collector is implemented and evaluated in Reform Prolog, a emulator based recursion parallel Prolog implementation. <p> However, Foster and Winsborough [78] have shown that eliminating internal variables results in approximately 28% higher memory consumption and about 15% increase in execution time due to increased dereferencing and memory allocation. 9.3 PARALLEL GARBAGE COLLECTION Khayri Ali <ref> [3] </ref> has proposed an elegant scheme for parallel copying garbage collection on shared memory multiprocessors. The memory is divided into segments. Initially all segments are stored in a common pool. The processing elements (PEs) allocate segments from the pool and link them into their private memory area (see Figure 9.2).
Reference: 4. <author> K. Ali, R. Karlsson, </author> <title> The Muse OR-Parallel Prolog Model and its Performance, </title> <booktitle> Proc. North American Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> -22 </month>
Reference-contexts: There is also a survey of collection schemes for sequential logic programming languages by Bekkers, Ridoux and Ungaro [28]. 22 Summary Parallel Collection Most parallel Prolog implementations do not include a garbage collector. OR-parallel systems such as Muse <ref> [4] </ref> and Aurora [44, 129] use more or less sequential mark-sweep collectors [2, 71, 72]. The Aurora collector [220] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [215] for OR-parallel execution with its more complicated data structures.
Reference: 5. <author> S. Anderson, P. Hudak, </author> <title> Compilation of Haskell Array Comprehensions for Scientific Computing, </title> <booktitle> Proc. SIGPLAN'90 Conf. on Programming Language Design and Implementation, </booktitle> <publisher> ACM Press, </publisher> <year> 1990. </year> <note> -20, 124 </note>
Reference-contexts: Meier also considers "backtracking" iteration, a subject we have not discussed here (compilation of bounded existential quantifications). It seems to us that the instruction set we use would be appropriate as a base also for Meier's methods. 20 Summary Array comprehensions in Haskell <ref> [5] </ref> can be used for expressing array operations, similar to our array extended bounded quantifications. The Common LISP language [184] (and some earlier LISP dialects), as well as Standard ML [91], contain iteration, mapping and reduction constructs that in some cases resemble ours. <p> Meier also considers "backtracking" iteration, a subject we have not discussed here (compilation of bounded existential quantifications). It seems to us that the instruction set we use would be appropriate as a base also for Millroth's and Meier's methods. The work on array comprehensions in Haskell and their compilation <ref> [5] </ref> also seems to be relevant for our work, although their context is lazy functional programming. The Common LISP language [184] (and some earlier LISP dialects), as well as Standard ML [91], contain iteration, mapping and reduction constructs that in some cases resemble ours. 7.12.
Reference: 6. <author> A. W. Appel, </author> <title> A Runtime System, </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 3(4) </volume> <pages> 343-380, </pages> <year> 1990. </year> <note> -21, 130 </note>
Reference-contexts: We want to avoid this situation. Their algorithm does preserve the segment-structure of the heap (but not the ordering within a segment). Hence, they can reclaim all memory by backtracking. In contrast, our algorithm only supports partial reclamation of memory by backtracking. Appel <ref> [6, 7] </ref> describes a simple generational garbage collector for Standard ML. The collector uses Cheney's [49] garbage collection algorithm, which is the basis of our algorithm as well. However, Appel's collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. <p> Our measurements indicate that this is sufficient: the copying algorithms we describe do not reclaim appreciably less memory on backtracking than the standard mark-sweep algorithm on the measured benchmarks. Appel <ref> [6, 7] </ref> describes a simple generational garbage collector for Standard ML. The collector uses Cheney's garbage collection algorithm, which is the basis of our algorithm as well. However, his collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. Our algorithm handles frequent assignments efficiently.
Reference: 7. <author> A. W. Appel, </author> <title> Simple Generational Garbage Collection and Fast Allocation, </title> <journal> Software|Practice and Experience, </journal> <volume> 19(2) </volume> <pages> 171-183, </pages> <year> 1989. </year> <journal> -20, </journal> <volume> 21, 130, 134, </volume> <pages> 156 </pages>
Reference-contexts: Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [69, 207]. This insight led to the invention of generational garbage collection <ref> [7, 121] </ref> where young objects are garbage collected more often than old. Copying collectors [76, 139] and generational copying collectors have been considered unsuitable for Prolog until recently. Prolog implementations such as SICStus Prolog [45] use a mark-sweep algorithm [131] that first marks the live data, then compacts the heap. <p> We want to avoid this situation. Their algorithm does preserve the segment-structure of the heap (but not the ordering within a segment). Hence, they can reclaim all memory by backtracking. In contrast, our algorithm only supports partial reclamation of memory by backtracking. Appel <ref> [6, 7] </ref> describes a simple generational garbage collector for Standard ML. The collector uses Cheney's [49] garbage collection algorithm, which is the basis of our algorithm as well. However, Appel's collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. <p> Our measurements indicate that this is sufficient: the copying algorithms we describe do not reclaim appreciably less memory on backtracking than the standard mark-sweep algorithm on the measured benchmarks. Appel <ref> [6, 7] </ref> describes a simple generational garbage collector for Standard ML. The collector uses Cheney's garbage collection algorithm, which is the basis of our algorithm as well. However, his collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. Our algorithm handles frequent assignments efficiently. <p> Naturally, if most of the live unbound variables have been compared in this way, the collector will have to spend more time in compacting the cv-stack. We believe this situation to be rare. 8.4 INTRODUCING GENERATIONAL GARBAGE COLLEC TION Generational garbage collection <ref> [121, 7] </ref> relies on the observation that newly created objects tend to be short-lived. Thus, garbage collection should concentrate on recently created data. The heap is split into two or more generations, and the most recent generation is collected most frequently. <p> Note that it is not necessary to clear the mark bits in from-space. This memory is overwritten by new data which have zeroed mark bits. The mark bits in to-space are restored by the pointer reversal scheme. 9.5 GENERATIONAL PARALLEL GARBAGE COLLECTION Generational garbage collection <ref> [7, 121] </ref> relies on the observation that newly created objects are short-lived. Thus, garbage collection should concentrate on recently created data. The heap is split into two or more generations. The most recent generation is collected most frequently.
Reference: 8. <author> A. W. Appel, </author> <title> Garbage Collection, Advanced Language Implementations, </title> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> -21 </month>
Reference-contexts: It was designed after our collector and use our ideas for handling internal pointers. It is not clear how their algorithm can be made generational. Cohen [55], Appel <ref> [8] </ref>, Jones and Lins [110], and Wilson [221] have written comprehensive surveys on general-purpose garbage collection algorithms. There is also a survey of collection schemes for sequential logic programming languages by Bekkers, Ridoux and Ungaro [28]. 22 Summary Parallel Collection Most parallel Prolog implementations do not include a garbage collector.
Reference: 9. <author> K. Appleby, M. Carlsson, S. Haridi, D. Sahlin, </author> <title> Garbage Collection for Prolog Based on WAM, </title> <journal> Communications of the ACM, </journal> <volume> 31(6) </volume> <pages> 719-741, </pages> <month> June </month> <year> 1988. </year> <journal> -20, </journal> <volume> 21, 128, 130, 135, </volume> <month> 142, </month> <title> 158 169 170 A Parallel Generational Copying Garbage Collector </title>
Reference-contexts: Copying collectors [76, 139] and generational copying collectors have been considered unsuitable for Prolog until recently. Prolog implementations such as SICStus Prolog [45] use a mark-sweep algorithm [131] that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. <ref> [9] </ref> as typical. It is based on the Deutsch-Schorr-Waite [55, 169] algorithm for marking and on Morris' algorithm [55, 144] for compacting. Touati and Hama [196] developed a generational copying garbage collector for Prolog. The heap is split into an old and a new generation. <p> However, Appel's collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. Our algorithm handles frequent assignments efficiently. Sahlin [162] has developed a method that makes the execution time of the Appleby et al. <ref> [9] </ref> algorithm proportional to the size of the live data. The main drawback of Sahlin's algorithm is that implementing the mark-sweep algorithm becomes more difficult, not to mention guaranteeing that there are no programming errors in its implementation. To our knowledge it has never been implemented. <p> Memory allocation can then be resumed. 8.2 RELATED WORK Prolog implementations such as SICStus Prolog use a mark-sweep algorithm that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. <ref> [9] </ref> as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite [169, 55] algorithm for marking and on Morris' algorithm [144, 55] for compacting. 1. <p> However, his collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. Our algorithm handles frequent assignments efficiently. Sahlin [162] has developed a method that makes the execution time of the Appleby et al. <ref> [9] </ref> algorithm proportional to the size of the live data. The main drawback of Sahlin's algorithm is that implementing the mark-sweep algorithm becomes more difficult, not to mention guaranteeing that there are no programming errors in its implementation. To our knowledge it has never been implemented. <p> This result in a runtime cost for using generational garbage collection. In Prolog this overhead is already present in the form of trail tests and there is no extra runtime penalty for using generational collection. 8.5 EVALUATION We have implemented a standard mark-sweep algorithm <ref> [9] </ref> and compared it to our copying algorithms. All garbage collection algorithms have been 136 A Simple and Efficient Copying Garbage Collector choice point in the new generation the trail limit is set as usual. implemented in the same system, a sequential version of Reform Prolog. <p> When two variables are unified, a pointer from one cell to the other cell is created. In general, pointer chains may arise which require dereferencing. 9.2 SEQUENTIAL COPYING GARBAGE COLLECTION A significant number Prolog garbage collectors are based on mark-slide algorithms <ref> [9] </ref>. The reason is that most Prolog implementations rely on that data maintains their relative positions throughout the execution. This has the advantages that: 1. Data allocated on the heap can be instantly reclaimed on backtracking, in a stack like fashion. 2. <p> Old choice points can be detected by marking them during garbage collection. A spare bit in the trail top pointer can be used for this purpose (this is what the mark-sweep collector in SICStus Prolog <ref> [9] </ref> does). After garbage collection, all choice points are updated such that the saved heap top pointer points to the start of the first segment in the new generation. This is necessary since we cannot deallocate memory in the old generation when backtracking.
Reference: 10. <author> K. R. Apt, </author> <title> Arrays, Bounded Quantifications and Iteration in Logic and Constraint Logic Programming, </title> <booktitle> Science of Computer Programming, 26(1-3):133-148, 1996. -19, </booktitle> <pages> 124 </pages>
Reference-contexts: Some authors have studied the use of bounded quantifiers with sets, for example in SETL [170] and flogg [70]. Barklund & Hill [23] have studied how to incorporate restricted quantifications and arrays in Godel [101], while Apt <ref> [10] </ref> has studied how bounded quantifications and arrays could be used also in constraint based languages. Lloyd & Topor [128] have studied transformation methods for running more general quantifier expressions, although their method will `flounder' for some examples that can be run using our method (Lloyd, personal communication). <p> Some authors have studied the use of bounded quantifiers with sets, for example in SETL [170] and log- [70]. Barklund & Hill [23] have studied how to incorporate restricted quantifications and arrays in Godel [101], while Apt <ref> [10] </ref> has studied how bounded quantifications and arrays could be used also in constraint based languages. Lloyd & Topor [128] have studied transformation methods for running more general quantifier expressions, although their method will `flounder' for some examples that can be run using this method (Lloyd, personal communication).
Reference: 11. <author> H. Arro, J. Barklund, J. Bevemyr, </author> <title> Parallel Bounded Quantifications | Preliminary Results, </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 28(5) </volume> <pages> 117-124, </pages> <year> 1993. </year> <journal> -20, </journal> <volume> 84, 85, 95, </volume> <pages> 100 </pages>
Reference-contexts: Hermenegildo and Carro [98] discuss how parallel execution of bounded quantifications relates to more traditional AND-parallel execution of logic programs. Arro and Barklund <ref> [11] </ref> have investigated how bounded quantifiers can be executed on the Connection Machine, a SIMD multiprocessor that directly supports data parallel computation. Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. <p> We have previously shown how bounded quantifications on a sequential computer can be given operational semantics in terms of definite iterations [20], and sketched how they can be run on a Connection Machine, a SIMD multiprocessor that directly supports data parallel computation <ref> [11] </ref>. In this paper it is shown how we can exploit the affinity between bounded quantification and recursive programs to run bounded quantifications on an existing implemented abstract machine that was developed to run a class of recursive programs efficiently on shared-memory MIMD multiprocessors [34]. <p> notions for expressing the finite sets: an integer range [26], the elements of a list, and the suffixes of a list [211, 212]. 6.3 PROLOG WITH BOUNDED QUANTIFICATIONS We will use an abstract syntax for bounded quantifications in Prolog that is slightly different from the syntax we have used before <ref> [11, 26] </ref>; an actual concrete syntax will probably have to be a compromise between several objectives. * We will write a universal quantification as all ([i], fi [i]) where [i] is a range expression and the body fi [i] is a goal. <p> % X4 &lt;- J &lt;- P0 [I] builtin array_ref X5 X0 X4 % X5 &lt;- P1 [I] builtin array_elt X3 X6 X5 % P1 [I] = P0 [J] jump L5 % next iteration 6.10 BENCHMARKS We use the same set of benchmarks that were used by Arro, Barklund and Bevemyr <ref> [11] </ref> to evaluate the parallel implementation on a SIMD multiprocessor (Thinking Machines Corp. Connection Machine model CM-200). These benchmarks have been executed on a 26 processor Sequent Symmetry. This is a cache-coherent shared memory multiprocessor equipped with Intel 80386 processors. <p> We have run programs on both SIMD computers (Connection Machine Model CM-2 by Thinking Machines Corp.) <ref> [11] </ref> and shared-memory MIMD computers (SUN 630MP and Sequent Symmetry) [19]. Much of the beauty of bounded quantifications lies in the fact that they have a clear declarative semantics, while at the same time they behave well operationally on this wide range of sequential and parallel computers.
Reference: 12. <author> Arvind, K. P. Gostelow, </author> <title> The U-Interpreter, </title> <journal> IEEE Computer, </journal> <volume> 15(2) </volume> <pages> 42-49, </pages> <year> 1982. </year> <month> -18 </month>
Reference-contexts: This in turn was inspired by other data flow models for parallel execution of Prolog proposed by Moto-oka and Fuchi [143] and Umeyama and Tamura [206]. These execution models were motivated from the research on data flow parallelism which emerged in the early 1980's <ref> [12, 68] </ref>. Nilsson and Tanaka [148, 149] designed a scheme for compiling Flat GHC into Fleng. Fleng is a primitive process-oriented language which has been implemented using a data-parallel interpreter.
Reference: 13. <author> L. Augustsson, T. Johnsson, </author> <title> Parallal Graph Reduction with the hv; Gi-Machine, </title> <booktitle> Proc. Workshop on Implementation of Lazy Functional Languages, </booktitle> <year> 1988. </year> <month> -23 </month>
Reference-contexts: Ellis, 1.6. Related Work 23 Li and Appel [73] propose a similar design with several mutators and one collector. Rojemo [161] extended the collector by Ellis et al. for the hv; Gi machine <ref> [13] </ref> (a parallel version of the G-machine [14]). Chapter 2 Reform Prolog: The Language and its Implementation Johan Bevemyr, Thomas Lindgren, H-akan Millroth In Proc. 10th Int. Conf. Logic programming, MIT Press, 1993. Reform Prolog is an (dependent) AND-parallel system based on recursion-parallelism and Reform compilation.
Reference: 14. <author> L. Augustsson, T. Johnsson, </author> <title> The Chalmers Lazy-ML Compiler, </title> <journal> The Computer Journal, </journal> <volume> 32(2) 127-141,1989. </volume> <month> -23 </month>
Reference-contexts: Ellis, 1.6. Related Work 23 Li and Appel [73] propose a similar design with several mutators and one collector. Rojemo [161] extended the collector by Ellis et al. for the hv; Gi machine [13] (a parallel version of the G-machine <ref> [14] </ref>). Chapter 2 Reform Prolog: The Language and its Implementation Johan Bevemyr, Thomas Lindgren, H-akan Millroth In Proc. 10th Int. Conf. Logic programming, MIT Press, 1993. Reform Prolog is an (dependent) AND-parallel system based on recursion-parallelism and Reform compilation.
Reference: 15. <author> H. G. Baker, </author> <title> List Processing in Real Time on a Serial Computer, </title> <journal> Communications of the ACM, </journal> <volume> 21(4) </volume> <pages> 280-294, </pages> <year> 1978. </year> <month> -22 </month>
Reference-contexts: Imai and Tick [105] describe a parallel stop-and-copy collector based on Cheney's [49] algorithm. It provides dynamic load balancing through a global work stack. Object of equal size are allocated in the same memory block. The later makes it unsuitable for Prolog. Baker <ref> [15] </ref> describes a concurrent collection scheme divided into two processes, a mutator which creates data and a collector which performs garbage collection. Execution of the two processes are interleaved. Halstead [88] describe a parallelisation of Baker's algorithm. The heap is statically divides into separate areas collected by distinct PEs.
Reference: 16. <author> J. Barklund, </author> <title> Parallel Unification, </title> <type> PhD thesis, </type> <institution> Comp. Sci. Dept., Upp-sala Univ., Uppsala, </institution> <year> 1990. </year> <month> -18 </month>
Reference-contexts: Fleng is a primitive process-oriented language which has been implemented using a data-parallel interpreter. Barklund, Hagner and Wafin [21, 22] translated a flat committed choice language into condition graphs, and proposed a data-parallel inference mechanism. Barklund <ref> [16] </ref> also proposed a data-parallel unification algorithm suitable for data-parallel logic programming implementations, e.g., Reform Prolog. Yet another approach to data-parallelism is to add parallel data structures on which certain operations can be performed in parallel.
Reference: 17. <author> J. Barklund, </author> <title> Bounded Quantifications for Iteration and Concurrency in Logic Programming, </title> <journal> New Generation Computing, </journal> <volume> 12(2) </volume> <pages> 161-182, </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <note> -9, 100 </note>
Reference-contexts: Most imperative languages provide such iteration over finite sets of integers, e.g., for loops in Pascal and DO loops in Fortran, combining the results through serialisation. Previously Barklund & Millroth <ref> [17, 26] </ref> and Voronkov [211, 212] have presented bounded quantifications as a concise way of providing definite iteration in logic programs. In paper VI we propose an extension of Prolog with bounded quantifications and argue that it then naturally follows to introduce arrays. <p> to repeat execution of a command for all integers in some range, combining the effects of the command invocations by serialization (e.g., the for loop of Pascal). 1 That is, determined before the repetition commences, not necessarily at compile time. 100 Prolog with Arrays and Bounded Quantifications Barklund & Millroth <ref> [26, 17] </ref> and Voronkov [211, 212] have proposed bounded quantifications as a computational device for repetition by iteration in logic programs.
Reference: 18. <author> J. Barklund, </author> <title> Tabulation of Functions in Definite Clause Programs, </title> <type> UPMAIL Tech. Rep. 82, </type> <institution> Comp. Sci. Dept., Uppsala Univ.,1994. -20, </institution> <month> 125 </month>
Reference-contexts: Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques [30, 36, 80] and has also been applied in logic programming <ref> [18, 218] </ref>. Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [69, 207]. This insight led to the invention of generational garbage collection [7, 121] where young objects are garbage collected more often than old. <p> Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques [30, 80, 36] and has also been applied in logic programming <ref> [218, 18] </ref>. We are interested in using bounded quantification and arrays as a target language for transformation of recursive logic programs to iterative logic programs with tabulation. 7.11 CONCLUSIONS AND FUTURE WORK An extension of Prolog with bounded quantifications and arrays has been proposed, together with a detailed implementation.
Reference: 19. <author> J. Barklund, J. Bevemyr, </author> <title> Executing Bounded Quantifications on Shared Memory Multiprocessors, </title> <booktitle> Proc. Intl. Conf. Programming Language Implementation and Logic Programming, </booktitle> <publisher> LNCS 714, Springer-Verlag, </publisher> <year> 1993. </year> <month> -100 </month>
Reference-contexts: We have run programs on both SIMD computers (Connection Machine Model CM-2 by Thinking Machines Corp.) [11] and shared-memory MIMD computers (SUN 630MP and Sequent Symmetry) <ref> [19] </ref>. Much of the beauty of bounded quantifications lies in the fact that they have a clear declarative semantics, while at the same time they behave well operationally on this wide range of sequential and parallel computers. The rest of the paper is structured as follows.
Reference: 20. <author> J. Barklund, J. Bevemyr, </author> <title> Prolog With Arrays and Bounded Quantifications, Logic Programming and Automated Reasoning, </title> <publisher> LNCS 698, Springer-Verlag, </publisher> <year> 1993 </year> <month> -84, </month> <pages> 86, 87, 100 </pages>
Reference-contexts: Bounded quantification, a limited class of quantified expressions, have been proposed as a concurrent linguistic construct for logic programs [26]. We have previously shown how bounded quantifications on a sequential computer can be given operational semantics in terms of definite iterations <ref> [20] </ref>, and sketched how they can be run on a Connection Machine, a SIMD multiprocessor that directly supports data parallel computation [11]. <p> We discuss this matter in more depth elsewhere <ref> [20] </ref>. 6.4 TRANSFORMING BOUNDED QUANTIFICATIONS TO RECURSIVE PROGRAMS It is obvious that any bounded quantification can be replaced by a call to a recursively defined predicate. In this section we will present examples of bounded quantifications and corresponding recursively defined predicates. <p> We are assuming that the limits of integer ranges, or lists of list member and list suffix range expressions, are instantiated. We could devise more complete methods that could compute these ranges, but it seems preferrable to delay execution of bounded quantifications until the range information is available. Elsewhere <ref> [20] </ref> we have shown show how bounded quantifications can be compiled on a sequential machine, both for deterministic and nondeterministic programs. <p> Section 7.11 ends the paper with some conclusions and prospects for future work. This paper is a significantly extended and slightly revised version of a paper that was presented at the Fourth International Conference on Logic Programming and Automated Reasoning and can be found in its proceedings <ref> [20] </ref>. 7.2 BOUNDED QUANTIFICATION Following Tennent [189, 190] in spirit, we define a quantification to be an expression that has three parts: a quantifier, which is a symbol from a given alphabet of quantifiers, a locally scoped iteration variable, and a body, which 7.2.
Reference: 21. <author> J. Barklund, N. Hagner, M. Wafin, </author> <title> KL1 in Condition Graphs on a Connection Machine, </title> <booktitle> Proc. Intl. Conf. Fifth Generation Computer Systems, </booktitle> <publisher> Ohmsha, </publisher> <year> 1988. </year> <month> -18 </month>
Reference-contexts: Nilsson and Tanaka [148, 149] designed a scheme for compiling Flat GHC into Fleng. Fleng is a primitive process-oriented language which has been implemented using a data-parallel interpreter. Barklund, Hagner and Wafin <ref> [21, 22] </ref> translated a flat committed choice language into condition graphs, and proposed a data-parallel inference mechanism. Barklund [16] also proposed a data-parallel unification algorithm suitable for data-parallel logic programming implementations, e.g., Reform Prolog.
Reference: 22. <author> J. Barklund, N. Hagner, M. Wafin, </author> <title> Connection Graphs, </title> <type> UPMAIL Tech. Rep. 48, </type> <institution> Comp. Sci. Dept., Uppsala Univ., Uppsala, </institution> <year> 1988. </year> <note> -18 9.8. BIBLIOGRAPHY 171 </note>
Reference-contexts: Nilsson and Tanaka [148, 149] designed a scheme for compiling Flat GHC into Fleng. Fleng is a primitive process-oriented language which has been implemented using a data-parallel interpreter. Barklund, Hagner and Wafin <ref> [21, 22] </ref> translated a flat committed choice language into condition graphs, and proposed a data-parallel inference mechanism. Barklund [16] also proposed a data-parallel unification algorithm suitable for data-parallel logic programming implementations, e.g., Reform Prolog.
Reference: 23. <author> J. Barklund, P. M. Hill, </author> <title> Extending Godel for Expressing Restricted Quantifications and Arrays, </title> <type> UPMAIL Tech. Rep. 102, </type> <institution> Comp. Sci. Dept., Uppsala Univ., Uppsala, </institution> <year> 1995. </year> <journal> -19, </journal> <volume> 101, </volume> <pages> 124 </pages>
Reference-contexts: However, the quantifications are translated essentially as by Lloyd & Topor (cf. below). Some authors have studied the use of bounded quantifiers with sets, for example in SETL [170] and flogg [70]. Barklund & Hill <ref> [23] </ref> have studied how to incorporate restricted quantifications and arrays in Godel [101], while Apt [10] has studied how bounded quantifications and arrays could be used also in constraint based languages. <p> If the range expression is true for no value of i then the bounded universal quantification succeeds trivially. The syntax of a bounded existential quantification is some ([i], fi [i]) 2 In other logic programming languages, other design choices may be appropriate <ref> [23] </ref>. 102 Prolog with Arrays and Bounded Quantifications where [i] is a range expression and fi [i] is a goal. <p> Some authors have studied the use of bounded quantifiers with sets, for example in SETL [170] and log- [70]. Barklund & Hill <ref> [23] </ref> have studied how to incorporate restricted quantifications and arrays in Godel [101], while Apt [10] has studied how bounded quantifications and arrays could be used also in constraint based languages.
Reference: 24. <author> J. Barklund, H. Millroth, </author> <title> Nova Prolog, </title> <type> UPMAIL Tech. Rep. 52, </type> <institution> Comp. Sci. Dept., Uppsala Univ., </institution> <year> 1988. </year> <month> -18 </month>
Reference-contexts: Yet another approach to data-parallelism is to add parallel data structures on which certain operations can be performed in parallel. This is the approach taken by Kacsuk in DAP Prolog [113], Barklund and Millroth in Nova Prolog <ref> [24] </ref>, and by Fagin [75]. The idea of having explicitly parallel data structures have previously been used in other languages such as APL [106], CM Lisp [102, 185], ?Lisp [191], NESL [38] among others. These languages are often designed to exploit the architecture of a specific machine, i.e.
Reference: 25. <author> J. Barklund, H. Millroth, </author> <title> Integrating Complex Data Structures in Pro-log, </title> <booktitle> Proc. 1987 Symp. Logic Programming, IEEE, </booktitle> <year> 1987. </year> <month> -105 </month>
Reference-contexts: In our earlier research, we investigated how arrays, tables and other data structures could be incorporated into Prolog by making them obey the rules above in such a way that they were denoted by terms of a fixed arity, in the same way as for lists and trees <ref> [25] </ref>. The reason was that we wanted to be able to use recursion also for traversing and constructing arrays, tables, etc. We have abandoned that effort and propose instead to use bounded quantification for this purpose, in accordance with the discussion at the beginning of this section.
Reference: 26. <author> J. Barklund, H. Millroth, </author> <title> Providing Iteration and Concurrency in Logic Programs Through Bounded Quantifications, </title> <booktitle> Intl. Conf. on Fifth Generation Computer Systems, 1992. -9, </booktitle> <volume> 84, 85, </volume> <pages> 100 </pages>
Reference-contexts: Most imperative languages provide such iteration over finite sets of integers, e.g., for loops in Pascal and DO loops in Fortran, combining the results through serialisation. Previously Barklund & Millroth <ref> [17, 26] </ref> and Voronkov [211, 212] have presented bounded quantifications as a concise way of providing definite iteration in logic programs. In paper VI we propose an extension of Prolog with bounded quantifications and argue that it then naturally follows to introduce arrays. <p> Bounded quantification, a limited class of quantified expressions, have been proposed as a concurrent linguistic construct for logic programs <ref> [26] </ref>. We have previously shown how bounded quantifications on a sequential computer can be given operational semantics in terms of definite iterations [20], and sketched how they can be run on a Connection Machine, a SIMD multiprocessor that directly supports data parallel computation [11]. <p> In this paper we shall consider only the universal quantifier and a class of arithmetic quantifiers, exemplified here with the sum, product, minimum and maximum quantifiers. We will also only consider a small number of notions for expressing the finite sets: an integer range <ref> [26] </ref>, the elements of a list, and the suffixes of a list [211, 212]. 6.3 PROLOG WITH BOUNDED QUANTIFICATIONS We will use an abstract syntax for bounded quantifications in Prolog that is slightly different from the syntax we have used before [11, 26]; an actual concrete syntax will probably have to <p> notions for expressing the finite sets: an integer range [26], the elements of a list, and the suffixes of a list [211, 212]. 6.3 PROLOG WITH BOUNDED QUANTIFICATIONS We will use an abstract syntax for bounded quantifications in Prolog that is slightly different from the syntax we have used before <ref> [11, 26] </ref>; an actual concrete syntax will probably have to be a compromise between several objectives. * We will write a universal quantification as all ([i], fi [i]) where [i] is a range expression and the body fi [i] is a goal. <p> to repeat execution of a command for all integers in some range, combining the effects of the command invocations by serialization (e.g., the for loop of Pascal). 1 That is, determined before the repetition commences, not necessarily at compile time. 100 Prolog with Arrays and Bounded Quantifications Barklund & Millroth <ref> [26, 17] </ref> and Voronkov [211, 212] have proposed bounded quantifications as a computational device for repetition by iteration in logic programs.
Reference: 27. <author> J. Barklund, H. Millroth, </author> <title> Garbage Cut for Garbage Collection of Iterative Prolog Programs, </title> <booktitle> 3rd Symp. on Logic Programming, IEEE, 1986. -20, </booktitle> <pages> 129 </pages>
Reference-contexts: For the older generation they use a mark-sweep algorithm. The technique is similar to that described by Barklund and Millroth <ref> [27] </ref> and later by Older and Rummell [150]. Bekkers, Ridoux and Ungaro [28] describe an algorithm for copying garbage collection of Prolog. They observe that it is possible to reclaim garbage 1.6. Related Work 21 collected data on backtracking if copying starts at the oldest choice point (bottom-to-top). <p> For the older generation they use a mark-sweep algorithm. The technique is similar to that described by Barklund and Millroth <ref> [27] </ref> and later by Older and Rummell [150]. We show how a simpler copying collector can be implemented, how the troublesome primitives can be accomodated better and how generational collection can be done in a simple and intuitive way. However, our view is also more radical than theirs.
Reference: 28. <author> Y. Bekkers, O. Ridoux and L. Ungaro, </author> <title> Dynamic Memory Management for Sequential Logic Programming Languages, </title> <booktitle> Proc. Intl. Workshop on Memory Management 92, </booktitle> <publisher> LNCS 637, Springer-Verlag, </publisher> <year> 1992. </year> <journal> -13, </journal> <volume> 20, 21, 129, </volume> <pages> 143 </pages>
Reference-contexts: Before this only mark-compact collectors and partially copying collectors were used. Our scheme showed how internal pointers were handled and how generational collection could be provided. Demoen, Engels and Tarau [67] have improved the scheme further using ideas from Bekkers, Ridoux and Ungaro <ref> [28] </ref>. This work also showed that instant reclaiming on backtracking can be sacrificed for data which survive garbage collection, without significant efficiency penalties. This is crucial for the parallel collector. <p> For the older generation they use a mark-sweep algorithm. The technique is similar to that described by Barklund and Millroth [27] and later by Older and Rummell [150]. Bekkers, Ridoux and Ungaro <ref> [28] </ref> describe an algorithm for copying garbage collection of Prolog. They observe that it is possible to reclaim garbage 1.6. Related Work 21 collected data on backtracking if copying starts at the oldest choice point (bottom-to-top). <p> It is not clear how their algorithm can be made generational. Cohen [55], Appel [8], Jones and Lins [110], and Wilson [221] have written comprehensive surveys on general-purpose garbage collection algorithms. There is also a survey of collection schemes for sequential logic programming languages by Bekkers, Ridoux and Ungaro <ref> [28] </ref>. 22 Summary Parallel Collection Most parallel Prolog implementations do not include a garbage collector. OR-parallel systems such as Muse [4] and Aurora [44, 129] use more or less sequential mark-sweep collectors [2, 71, 72]. <p> We show below that memory recovery by backtracking is still possible, and that the new approach in practice recovers approximately as much garbage by backtracking as the conventional approach. Bekkers, Ridoux and Ungaro <ref> [28] </ref> describe an algorithm for copying garbage collector for Prolog. They observe that it is possible to reclaim garbage collected data on backtracking if copying starts at the oldest choice point (bottom-to-top). <p> The time complexity can be made proportional to the size of the live data either by linking together and sorting the live data [162] in a mark-slide collector (actually NlogN (N = live data)), or by using a copying collector <ref> [28, 35, 67] </ref>. We use a copying collector since it is more amenable to parallelisation. Copying collectors have the disadvantage that the relative positions of the data are not preserved. We briefly discuss how instant reclaiming and trailing can be handled in a sequential copying collector. <p> A richer discussion can be found in Bevemyr and Lindgren's [35] paper. An improved method for dealing with the comparison operators is presented below. Instant Reclaiming A compacting collector preserves the heap segments (see Figure 9.1) and entire segments can be deallocated on backtracking. Bekkers, Ridoux and Ungaro <ref> [28] </ref> suggested that a reasonable approximation of the heap segments can be preserved across garbage collections. This is 144 A Parallel Generational Copying Garbage Collector achieved by copying the data in a carefully chosen order, i.e., starting at the oldest choice point.
Reference: 29. <author> G. Bell, </author> <title> Ultracomputers: A Teraflop Before Its Time, </title> <journal> Communications of the ACM, </journal> <pages> 35(8)26-47, </pages> <year> 1992. </year> <month> -45 </month>
Reference-contexts: Promising performance figures, showing high parallel efficiency and low overhead for parallelization, have been obtained on a 24 processor shared-memory mul tiprocessor. 3.1 INTRODUCTION The Single Program Multiple Data (SPMD) model of parallel computation has recently received a lot of attention (see e.g. the article by Bell <ref> [29] </ref>).
Reference: 30. <author> R. E. Bellman, </author> <title> Dynamic Programming, </title> <publisher> Princeton Univ. Press, </publisher> <address> Prince-ton, N.J., </address> <year> 1957. </year> <note> -20, 125 </note>
Reference-contexts: Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques <ref> [30, 36, 80] </ref> and has also been applied in logic programming [18, 218]. Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [69, 207]. <p> Hermenegildo & Carro [98] discuss how parallel execution of bounded quantifications relates to more traditional (AND) parallel execution of logic programs. Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques <ref> [30, 80, 36] </ref> and has also been applied in logic programming [218, 18].
Reference: 31. <author> J. Bevemyr, </author> <title> The Luther WAM Emulator, </title> <type> UPMAIL Tech. Rep. 72, </type> <institution> Comp. Sci. Dept., Uppsala Univ., </institution> <year> 1992. </year> <note> -87, 107 </note>
Reference-contexts: 2 For example, a bounded quantification and (N&lt;_I&lt;M, [I]) is compiled to code that corresponds quite directly to the following schema. for (I = N ; I &lt; M ; I++) - /* code for [I] */ - 2 Warren's Abstract Machine [214]; our base implementation was provided by Bevemyr <ref> [31] </ref>. 88 Executing Bounded Quantifications on Shared Memory For execution of arithmetical quantifiers, e.g., sums and products, an accumulator is used to hold the accumulated sum or product. 6.6 PARALLEL EXECUTION The general idea for running bounded quantification on parallel computers is analogous to loop parallelization in, e.g., Fortran. <p> the code for a body is terminated by a conditional jump back to the beginning, or some variant thereof. 7.5 THE LUTHER WAM EMULATOR Our experiments have been conducted in an implementation of Warren's abstract Prolog machine [214] which is based on an emulator written in the C programming language <ref> [31, 32] </ref>. To better understand our code sequences below one ought to know the following. 1. There are two instructions builtin and inline that do the work of various predefined relations, known to the compiler.
Reference: 32. <author> J. Bevemyr, </author> <title> A Recursion Parallel Prolog Engine, </title> <type> PhL thesis, </type> <institution> Uppsala Theses in Computer Science 16, Uppsala Univ., </institution> <year> 1993. </year> <journal> -52, </journal> <volume> 62, 77, 84, 88, 90, 107, </volume> <pages> 147 </pages>
Reference-contexts: Due to space limitations, we can only describe these by means of a simple example and refer to other sources for a full discussion <ref> [34, 32, 125] </ref>. Consider the program: map ([],[]). 3.7. <p> The design of the Reform Prolog system is an attempt to meet these challenges without making programming much harder than in the sequential case. Reform Prolog has been implemented on a varity of shared address space multiprocessors. The parallel implementation is designed as extension of a sequential Prolog machine <ref> [32] </ref>. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [136, 138], execution model [33, 34], and parallel abstract machine [33, 34] of Reform Prolog. <p> Scheduling, memory and process management are simple and efficient. Almost sequential code is executed by each worker, with the exception of some synchronisation instructions (which are only present in programs with data dependencies between recursion levels). Bevemyr <ref> [32] </ref> describes the full machinery in detail. All this result in small overheads for parallel execution. 5.3 NESTED RECURSION PARALLELISM How can this machinery be extended to support nested parallel execution? We make a few observations about the current implementation: 1. <p> In order to do so, we shall describe a quite straightforward transformation from bounded quantifications to recursive programs. The resulting recursive program can be executed efficiently using the abstract machine developed for recursion parallel execution <ref> [32] </ref>. Note, however, that the parallel implementation cannot take full advantage of the simplicity of bounded quantifications when only the transformed programs are available. Some implementation methods needed for optimal execution of bounded quantifications may not be reasonable for arbitrary recursive programs. <p> However, such a system may not exploit the data-parallel style of execution as efficiently as our specialized implementation. Due to lack of space in this paper we do not otherwise compare our implementation with other AND-parallel implementations. See Bevemyr's exposition of the Reform Prolog Engine <ref> [32] </ref> for a more detailed comparison. 6.2 BOUNDED QUANTIFICATIONS In its most general form a quantification is an expression that applies an abstraction (its body) to a set of values, combining the results in a way that depends on the quantifier. <p> Although we can certainly translate bounded quantifications directly to instruction sequences, we can understand the compilation process here as a transformation of bounded quantifications to recursive predicates. These recursive predicates are subsequently compiled using methods developed for running recursive programs <ref> [34, 32, 136] </ref>. These methods allow parallel execution of recursive programs in which binding of shared variables are deterministic. This restricts the set of bounded quantifications that can be executed in parallel using this technique. <p> schema: for (I = 0, localacc = 0 ; I &lt; N ; I++) - /* code for localacc = localacc+A [I] */ - /* code for summing all local accumulators */ 6.7 ABSTRACT MACHINE We use the abstract machine designed for recursion-parallel execution developed by Bevemyr, Lindgren and Millroth <ref> [34, 32] </ref>. This machine consists of a set of workers numbered 0, 1, . . . , n1; one per processor. Each worker is implemented as a separate process running a WAM-based Prolog engine with extensions that support parallel execution. One worker is responsible for sequential execution (the sequential worker). <p> We used the self-scheduling algorithm described by Tang and Yew [187] for dynamic scheduling. This algorithm allocates one iteration at a time. 6.8 ADDITIONS TO WAM The instructions used for executing recursion-parallelism are also used for executing bounded quantifications. Bevemyr <ref> [32] </ref> describes the extended instruction set in detail. Here we give only a brief overview of the instructions required for executing bounded quantifications. We describe each instruction in terms of its function when used for executing bounded quantifications. <p> the code for a body is terminated by a conditional jump back to the beginning, or some variant thereof. 7.5 THE LUTHER WAM EMULATOR Our experiments have been conducted in an implementation of Warren's abstract Prolog machine [214] which is based on an emulator written in the C programming language <ref> [31, 32] </ref>. To better understand our code sequences below one ought to know the following. 1. There are two instructions builtin and inline that do the work of various predefined relations, known to the compiler. <p> This problem occurs as soon as several heaps are used. One solution is to associate a choice point identifier with each variable, resulting in a slight overhead (1-4%) <ref> [32] </ref> in an emulator based implementation. Several people, including anonymous referees, have pointed out that 1-4% is a surprisingly low figure.
Reference: 33. <author> J. Bevemyr, T. Lindgren, H. Millroth, </author> <title> Exploiting Recursion-Parallelism in Prolog, </title> <publisher> PARLE-93, LNCS 694, Springer Verlag, </publisher> <year> 1993. </year> <journal> -34, </journal> <volume> 62, 63, 124, </volume> <pages> 146 </pages>
Reference-contexts: Local operations do not require suspension or locking unification; shared terms require locking unification but no suspension, while fragile terms may not be instantiated out of the sequential order. If the compiler were not to respect fragility, the system might stray from simulating sequential behavior <ref> [33] </ref>. The goal of locality analysis is to generate precisely WAM code for parallel operations on unshared data. When this is possible, there is no paralleliza-tion overhead once the parallel execution has started. <p> The parallel implementation is designed as extension of a sequential Prolog machine [32]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [136, 138], execution model <ref> [33, 34] </ref>, and parallel abstract machine [33, 34] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> The parallel implementation is designed as extension of a sequential Prolog machine [32]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [136, 138], execution model <ref> [33, 34] </ref>, and parallel abstract machine [33, 34] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> However, the processes descend through the tree in parallel, temporally suspending when encountering not-yet-created subtrees. ffi The parallel execution model of Reform Prolog restricts the nondeterministic behaviour of parallel programs so that the following properties hold <ref> [33, 34] </ref>: * Parallel programs obey the sequential semantics of Prolog. This implies that time-dependent operations (type tests, etc.) on shared, unbound variables are carried out only when leftmost is the sequential computation order. * Parallel programs do not conditionally bind shared variables. <p> Sato & Tamaki [167] have an interpreter that will run more general quantifier expressions although the method is currently not so efficient (Sato, personal communication). In comparison our approach is only applicable to range-restricted formulas, but is quite efficient for that case. The methods by Bevemyr, Lindgren & Millroth <ref> [135, 33] </ref> and Meier [133] for compiling recursive programs to iterative code are also relevant. Millroth's method is based on an analysis of variable binding patterns while Meier's method, as presented, seems somewhat more ad hoc. <p> Each worker is a full WAM with all associated memory areas. All workers have shared access to each others heaps with the restriction that they can only create new objects on their own heap. Reform Prolog <ref> [33, 34] </ref> is an example of this kind of implementation. Segmented Memory To use Ali's scheme the heaps are divided into segments.
Reference: 34. <author> J. Bevemyr, T. Lindgren, H. Millroth, </author> <title> Reform Prolog: The Language and its Implementation, </title> <booktitle> Logic Programming: Proc. Tenth Intl. Conf., </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year> <journal> -8, </journal> <volume> 52, 62, 63, 75, 84, 88, </volume> <pages> 146 </pages>
Reference-contexts: Hence, sequential compiler technology should be largely applicable to our system. 8 Summary Nested Execution (paper IV) In paper IV we propose a scheme for executing nested recursion parallelism. The scheme requires only minimal extensions to the flat execution model of Reform Prolog <ref> [34] </ref>. It is possible to transform some nested recursions into a single recursive loop. However, it is not always feasible to flatten a nested recursion in Reform Prolog, e.g., when the size of the nested recursion cannot be statically determined. <p> Due to space limitations, we can only describe these by means of a simple example and refer to other sources for a full discussion <ref> [34, 32, 125] </ref>. Consider the program: map ([],[]). 3.7. <p> The parallel implementation is designed as extension of a sequential Prolog machine [32]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [136, 138], execution model <ref> [33, 34] </ref>, and parallel abstract machine [33, 34] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> The parallel implementation is designed as extension of a sequential Prolog machine [32]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [136, 138], execution model <ref> [33, 34] </ref>, and parallel abstract machine [33, 34] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> However, the processes descend through the tree in parallel, temporally suspending when encountering not-yet-created subtrees. ffi The parallel execution model of Reform Prolog restricts the nondeterministic behaviour of parallel programs so that the following properties hold <ref> [33, 34] </ref>: * Parallel programs obey the sequential semantics of Prolog. This implies that time-dependent operations (type tests, etc.) on shared, unbound variables are carried out only when leftmost is the sequential computation order. * Parallel programs do not conditionally bind shared variables. <p> This is achieved by exploiting properties of deterministic execution and careful scheduling of work. 5.1 INTRODUCTION We propose a scheme for executing nested recursion parallelism. The scheme requires only minimal extensions to the current execution model of Reform Prolog <ref> [34] </ref>. It is possible to transform some nested recursions into a single recursive loop, as shown by Millroth [135]. Flattening nested data-parallel computations has been investigated by Blelloch [37], Palmer, Prins and Westfold [152] among others. <p> In this paper it is shown how we can exploit the affinity between bounded quantification and recursive programs to run bounded quantifications on an existing implemented abstract machine that was developed to run a class of recursive programs efficiently on shared-memory MIMD multiprocessors <ref> [34] </ref>. In order to do so, we shall describe a quite straightforward transformation from bounded quantifications to recursive programs. The resulting recursive program can be executed efficiently using the abstract machine developed for recursion parallel execution [32]. <p> Although we can certainly translate bounded quantifications directly to instruction sequences, we can understand the compilation process here as a transformation of bounded quantifications to recursive predicates. These recursive predicates are subsequently compiled using methods developed for running recursive programs <ref> [34, 32, 136] </ref>. These methods allow parallel execution of recursive programs in which binding of shared variables are deterministic. This restricts the set of bounded quantifications that can be executed in parallel using this technique. <p> schema: for (I = 0, localacc = 0 ; I &lt; N ; I++) - /* code for localacc = localacc+A [I] */ - /* code for summing all local accumulators */ 6.7 ABSTRACT MACHINE We use the abstract machine designed for recursion-parallel execution developed by Bevemyr, Lindgren and Millroth <ref> [34, 32] </ref>. This machine consists of a set of workers numbered 0, 1, . . . , n1; one per processor. Each worker is implemented as a separate process running a WAM-based Prolog engine with extensions that support parallel execution. One worker is responsible for sequential execution (the sequential worker). <p> Each worker is a full WAM with all associated memory areas. All workers have shared access to each others heaps with the restriction that they can only create new objects on their own heap. Reform Prolog <ref> [33, 34] </ref> is an example of this kind of implementation. Segmented Memory To use Ali's scheme the heaps are divided into segments.
Reference: 35. <author> J. Bevemyr and T. Lindgren, </author> <title> A Simple and Efficient Copying Garbage Collector for Prolog, </title> <booktitle> Proc. Programming Language Implementation and Logic Programming, </booktitle> <publisher> LNCS 844, Springer-Verlag, </publisher> <year> 1994. </year> <title> -142, 143, 144, 158 172 A Parallel Generational Copying Garbage Collector </title>
Reference-contexts: speedup to 1=0:15 = 6:67. 1 One of our abstract interpreters spent 70% of its execution time in garbage collection when analysing a semi-group program in SICStus. 142 A Parallel Generational Copying Garbage Collector It has been shown how a generational copying garbage collector can be used for sequential Prolog <ref> [35] </ref>. It has also been shown how to parallelise copying garbage collection for shared memory multiprocessors [3]. We present a collection scheme that combines these techniques resulting in a parallel generational copying garbage collector for Prolog. <p> The time complexity can be made proportional to the size of the live data either by linking together and sorting the live data [162] in a mark-slide collector (actually NlogN (N = live data)), or by using a copying collector <ref> [28, 35, 67] </ref>. We use a copying collector since it is more amenable to parallelisation. Copying collectors have the disadvantage that the relative positions of the data are not preserved. We briefly discuss how instant reclaiming and trailing can be handled in a sequential copying collector. <p> Copying collectors have the disadvantage that the relative positions of the data are not preserved. We briefly discuss how instant reclaiming and trailing can be handled in a sequential copying collector. A richer discussion can be found in Bevemyr and Lindgren's <ref> [35] </ref> paper. An improved method for dealing with the comparison operators is presented below. Instant Reclaiming A compacting collector preserves the heap segments (see Figure 9.1) and entire segments can be deallocated on backtracking. <p> Consequently, it is not possible to use the above technique in a parallel setting without imposing strict synchronisation between copying each choice point, severely limiting the available parallelism and introducing synchronisation overheads. Bevemyr and Lindgren <ref> [35] </ref> indicated that instant reclaiming of data that have survived a garbage collection can be sacrificed without loss of efficiency, at least for a range of benchmarks. Using their scheme, instant reclaiming of data is still possible for data allocated between collections. <p> Collapsing all segments into one will result in that all bindings of surviving variables are trailed. This will result in some extra trailing which can be removed during the next garbage collection. Bevemyr and Lindgren <ref> [35] </ref> showed that the extra trailing generated by this scheme is insignificant. An alternative to trailing all bindings in the old segment is to associate a time stamp with each variable. The time stamp may then be used for deciding whether to trail or not. <p> Quintus Prolog solve this by sorting the heap to preserve the memory (moving portions of the data) whenever a new segment is allocated. This is both slow and cumbersome, and not possible in a parallel setting. Bevemyr and Lindgren's Mark-Copy Algorithm Bevemyr and Lindgren's <ref> [35] </ref> copying collector is a straightforward adaption of Cheney's algorithm [49] and works in three phases; 1) marking all live data, 2) copying the live data, 3) updating external references. The global stack is divided into two areas. The old data reside in from-space and are evacuated into to-space. <p> The old generation. The standard solution is to use a write barrier to detect cross generational pointers as they are created, and record them for use in the gc. The write barrier is implemented by adding detection code for each potential cross generational assignment. Bevemyr and Lindgren <ref> [35] </ref> observe that this code is already present in Prolog in the form of trail tests. All cross generational bindings can be automatically recorded by carefully setting the trail condition. This results in some extra trailing. The trail. Only trail entries added since the last gc have to be examined.
Reference: 36. <author> R. S. Bird, </author> <title> Tabulation Techniques for Recursive Programs, </title> <journal> ACM Computing Surveys, </journal> <volume> 12(4) </volume> <pages> 403-417, </pages> <year> 1980. </year> <note> -20, 125 </note>
Reference-contexts: Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques <ref> [30, 36, 80] </ref> and has also been applied in logic programming [18, 218]. Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [69, 207]. <p> Hermenegildo & Carro [98] discuss how parallel execution of bounded quantifications relates to more traditional (AND) parallel execution of logic programs. Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques <ref> [30, 80, 36] </ref> and has also been applied in logic programming [218, 18].
Reference: 37. <author> G. Blelloch, </author> <title> Vector Models and Data-Parallel Computing, </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> -75 </month>
Reference-contexts: The scheme requires only minimal extensions to the current execution model of Reform Prolog [34]. It is possible to transform some nested recursions into a single recursive loop, as shown by Millroth [135]. Flattening nested data-parallel computations has been investigated by Blelloch <ref> [37] </ref>, Palmer, Prins and Westfold [152] among others. However, it is not always feasible to flatten a nested recursion in Reform Prolog, e.g. when the size of the nested recursion cannot be statically determined.
Reference: 38. <author> G. Blelloch, </author> <title> Programming Parallel Algorithms, </title> <journal> Communications of the ACM, </journal> <volume> 39(3), </volume> <year> 1996. </year> <month> -18 </month>
Reference-contexts: This is the approach taken by Kacsuk in DAP Prolog [113], Barklund and Millroth in Nova Prolog [24], and by Fagin [75]. The idea of having explicitly parallel data structures have previously been used in other languages such as APL [106], CM Lisp [102, 185], ?Lisp [191], NESL <ref> [38] </ref> among others. These languages are often designed to exploit the architecture of a specific machine, i.e. CM Lisp was designed for the Connection Machine and DAP Prolog for the Distributed Array Processor.
Reference: 39. <author> G. Blelloch, P. Gibbons, Y. Matias, </author> <title> Provably Efficient Scheduling for Languages with Fine-Grained Parallelism, </title> <booktitle> 7th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1995. </year> <month> -82 </month>
Reference-contexts: We argue that the performance and execution models for Reform Pro log are clearer than for implicitly AND-parallel systems. Possible future work include investigating to which extent the results for scheduling data-parallelism in imperative languages, e.g. Blelloch, Gibbons and Matias <ref> [39] </ref>, can be applied in Reform Prolog. 5.5 CONCLUSION This paper aims to show that there is no need for a complicated implementation technique to efficiently take advantage of nested data parallelism, and consequently a substantial part of nested dependent AND-parallelism using Lindgren's transformation [126].
Reference: 40. <author> P. Borgwardt, </author> <title> Parallel Prolog Using Stack Segments on Shared-Memory Multiprocessors, </title> <booktitle> IEEE Symp. Logic Programming, 1984. -14, </booktitle> <pages> 81 </pages>
Reference-contexts: System such as ACE [84, 85] and DDAS [178, 176, 177] are largely based on &-Prolog. Chang, Despain and DeGroot [47, 48] were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt <ref> [40, 41] </ref> proposed a stack-based implementation of the execution model developed by Chang et al. [48]. 1.6. Related Work 15 Hermenegildo and Rossi [100] classified independent and parallelism as strict independence and non-strict independence. <p> Our restriction on nested parallelism greatly simplifies the design compared with other nested AND-parallel systems like &-Prolog [95, 94, 99], DASWAM [178, 176] and ACE [85]. * No distributed stacks with markers <ref> [40, 95, 177] </ref> or multiple stack sets [94] are needed. * No intelligent backtracking, or any other non-standard sequential back tracking scheme is needed. * Very small overheads for suspension are possible. * Busy wait can be used without deadlock detection machinery. * Large number of processes can be created simultaneously.
Reference: 41. <author> P. Borgwardt, D. </author> <title> Rea, Distributed Semi-Intelligent Backtracking for a Stack Based AND-Parallel Prolog, </title> <booktitle> Proc. Symp. Logic Programming, IEEE Computer Society, </booktitle> <year> 1986. </year> <month> -14 </month>
Reference-contexts: System such as ACE [84, 85] and DDAS [178, 176, 177] are largely based on &-Prolog. Chang, Despain and DeGroot [47, 48] were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt <ref> [40, 41] </ref> proposed a stack-based implementation of the execution model developed by Chang et al. [48]. 1.6. Related Work 15 Hermenegildo and Rossi [100] classified independent and parallelism as strict independence and non-strict independence.
Reference: 42. <author> H. Burkhardt, S. Frank, B. Knobe, J. Rothnie, </author> <title> Overview of the KSR1 Computer System, </title> <type> Tech. Rep. </type> <institution> KSR-TR-9202001, Kendall Square Research, </institution> <year> 1992. </year> <month> -7 </month>
Reference-contexts: We find that the ineffectiveness of locking elimination is due to the relative rarity of locking writes, and the execution model of Reform Prolog, which results in few invalidations of shared cache lines when such writes occur. The benchmarks are evaluated on KSR-1 <ref> [42] </ref>, a cache-coherent multiprocessor with physically distributed memory, using up to 48 processors.
Reference: 43. <author> M. Burke, R. Cytron, </author> <title> Interprocedural Dependence Analysis and Par-allelization, </title> <booktitle> Proc. SIGPLAN'86 Symp. Compiler Construction, </booktitle> <year> 1986. </year> <month> -18 </month>
Reference-contexts: Loop Parallelisation in Imperative Languages Parallelising iteration in imperative languages have received much attention since it potentially may increase the speed of large sets of existing applications. Most of the research have focussed on parallelising DO-loops in Fortran. Kuck, Kuhn, Leasure, Wolfe [119], Kennedy [114], Burke and Cytron <ref> [43] </ref> among others have looked into this. The problem has proven to be difficult to solve for general loops. The best results have been obtained for loops performing simple operations on arrays.
Reference: 44. <author> M. Carlsson, </author> <title> Design and Implementation of an Or-Parallel Prolog Engine, </title> <type> PhD thesis, </type> <institution> SICS-RITA/02, </institution> <year> 1990. </year> <month> -22 </month>
Reference-contexts: There is also a survey of collection schemes for sequential logic programming languages by Bekkers, Ridoux and Ungaro [28]. 22 Summary Parallel Collection Most parallel Prolog implementations do not include a garbage collector. OR-parallel systems such as Muse [4] and Aurora <ref> [44, 129] </ref> use more or less sequential mark-sweep collectors [2, 71, 72]. The Aurora collector [220] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [215] for OR-parallel execution with its more complicated data structures.
Reference: 45. <author> M. Carlsson, </author> <title> SICStus Prolog Internals Manual, </title> <type> Internal Report, </type> <institution> Swedish Institute of Computer Science, </institution> <year> 1989. </year> <month> -20 </month>
Reference-contexts: This insight led to the invention of generational garbage collection [7, 121] where young objects are garbage collected more often than old. Copying collectors [76, 139] and generational copying collectors have been considered unsuitable for Prolog until recently. Prolog implementations such as SICStus Prolog <ref> [45] </ref> use a mark-sweep algorithm [131] that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. [9] as typical. It is based on the Deutsch-Schorr-Waite [55, 169] algorithm for marking and on Morris' algorithm [55, 144] for compacting.
Reference: 46. <author> M. Carlsson, </author> <title> Variable Shunting for the WAM, </title> <type> Tech. Rep. </type> <institution> R91-07, Swedish Institute of Computer Science, </institution> <year> 1989. </year> <note> -21, 149 </note>
Reference-contexts: We think our approach leads to better locality of reference. However, we have not found any published measurements of the effi ciency of the Bekkers-Ridoux-Ungaro algorithm. * Variable shunting <ref> [46, 120] </ref>, i.e. collapsing variable-variable chains, is used to avoid duplication of variables inside structures. However, this technique may introduce variable chains in new places. We want to avoid this situation. Their algorithm does preserve the segment-structure of the heap (but not the ordering within a segment). <p> All marking has to terminate before the data can be migrated. 9.4. Parallel Prolog Garbage Collection 149 (c) Before terminating the gc. This is to ensure that all data have been copied before any worker start accessing it. 4. Existing techniques for early reset and variable shunting <ref> [46, 120] </ref> op-timisations cannot be used. They rely on heap segments to be either marked or copied in choice point order. This requires barrier syn-chronisation after marking and copying each choice point, i.e., almost sequentialising the collector.
Reference: 47. <author> J.-H. Chang, </author> <title> High Performance Execution of Prolog Programs Based on a Static Dependency Analysis, </title> <type> PhD thesis, </type> <institution> UCB/CSD 86/263, Univ. Calif. Berkeley, </institution> <year> 1986. </year> <note> -14, 64 </note>
Reference-contexts: Hermenegildo [94, 99, 100, 145] defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE [84, 85] and DDAS [178, 176, 177] are largely based on &-Prolog. Chang, Despain and DeGroot <ref> [47, 48] </ref> were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt [40, 41] proposed a stack-based implementation of the execution model developed by Chang et al. [48]. 1.6. <p> The analysis precision is similar to that of Aquarius Prolog [210]. Aliasing and linearity The analyzer derives possible and certain aliases by maintaining equivalence classes of possibly or certainly aliased variables. This is similar to the techniques used by Chang <ref> [47] </ref>. A term is linear if no variable occurs more than once in it. To improve aliasing information, the analyzer tracks whether terms are linear [109]. Three classes of linearity are distinguished: linear, nonlinear, and indlist. The latter denotes lists where elements do not share variables.
Reference: 48. <author> J.-H. Chang, A. M. Despain, D. </author> <title> DeGroot, AND-Parallelism of Logic Programs Based on Static Data Dependency Analysis, </title> <booktitle> Proc. IEEE Symp. Logic Programming, </booktitle> <year> 1985. </year> <month> -14 </month>
Reference-contexts: Hermenegildo [94, 99, 100, 145] defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE [84, 85] and DDAS [178, 176, 177] are largely based on &-Prolog. Chang, Despain and DeGroot <ref> [47, 48] </ref> were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt [40, 41] proposed a stack-based implementation of the execution model developed by Chang et al. [48]. 1.6. <p> Chang, Despain and DeGroot [47, 48] were first to propose that scheduling of parallel execution could be done through compile time analysis. Borgwardt [40, 41] proposed a stack-based implementation of the execution model developed by Chang et al. <ref> [48] </ref>. 1.6. Related Work 15 Hermenegildo and Rossi [100] classified independent and parallelism as strict independence and non-strict independence.
Reference: 49. <author> C. J. </author> <title> Cheney, A Nonrecursive List Compacting Algorithm, </title> <journal> Communications of the ACM, </journal> <volume> 13(11) </volume> <pages> 677-678, </pages> <month> November </month> <year> 1970. </year> <journal> -21, </journal> <volume> 22, 128, 131, 144 9.8. </volume> <pages> BIBLIOGRAPHY 173 </pages>
Reference-contexts: Hence, they can reclaim all memory by backtracking. In contrast, our algorithm only supports partial reclamation of memory by backtracking. Appel [6, 7] describes a simple generational garbage collector for Standard ML. The collector uses Cheney's <ref> [49] </ref> garbage collection algorithm, which is the basis of our algorithm as well. However, Appel's collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. Our algorithm handles frequent assignments efficiently. <p> Load balancing is provided for the coping phase. Our parallel algorithm use ideas from Ali's collector and adds support for handling internal pointers and improves on the load balancing machinery. Imai and Tick [105] describe a parallel stop-and-copy collector based on Cheney's <ref> [49] </ref> algorithm. It provides dynamic load balancing through a global work stack. Object of equal size are allocated in the same memory block. The later makes it unsuitable for Prolog. <p> Garbage collection is done by starting at a set of root pointers, such as registers and the local stack, and discovering what data are reachable from these pointers, or live. Memory is reclaimed by compacting the live data [144], copying them to a new area <ref> [49] </ref> or putting the dead data on a free list. Memory allocation can then be resumed. 8.2 RELATED WORK Prolog implementations such as SICStus Prolog use a mark-sweep algorithm that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. [9] as typical. <p> The unnecessary trail entries are deleted by the next collection. 8.3. Algorithm 131 Mark-and-copy The copying collector is a straightforward adaption of Cheney's algorithm <ref> [49] </ref> and works in three phases. The algorithm allows the standard optimizations of early reset. The old data reside in fromspace and are evacuated into tospace. 1. Mark the live data. When a structure is encountered, mark the functor cell and all internal cells. <p> This is both slow and cumbersome, and not possible in a parallel setting. Bevemyr and Lindgren's Mark-Copy Algorithm Bevemyr and Lindgren's [35] copying collector is a straightforward adaption of Cheney's algorithm <ref> [49] </ref> and works in three phases; 1) marking all live data, 2) copying the live data, 3) updating external references. The global stack is divided into two areas. The old data reside in from-space and are evacuated into to-space.
Reference: 50. <author> T. Chikayama, Y. Kimura, </author> <title> Multiple Reference Management in Flat GHC, </title> <booktitle> Logic Programming|Proc. Fourth Intl. Conf., </booktitle> <publisher> MIT Press, </publisher> <year> 1987. </year> <note> -16, 104 </note>
Reference-contexts: These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [173], FCP [175], Parlog [53, 60], Strand [77], Guarded Horn Clauses (GHC) [201, 202], and Flat GHC <ref> [50, 205] </ref>. These languages are also referred to as de-evolutioned by Tick [194] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM. <p> Constant-time mechanisms with a reasonable overhead for that operation have been investigated, e.g., by Eriksson & Rayner [74]; note also the work on multiple reference management by Chikayama & Kimura <ref> [50] </ref>.
Reference: 51. <author> K. L. Clark, F. McCabe, </author> <title> The Control Facilities of IC-Prolog, Expert Systems in the Micro-Electronic World (ed. </title> <editor> D. Michie), </editor> <publisher> Edinburgh University Press, </publisher> <year> 1979. </year> <note> -14, 16 </note>
Reference-contexts: In this system only one goal, the producer, is allowed to bind a variable. All other goals are consumers. A consumer is not allowed to bind a variable, it suspends until the variable becomes bound. This form of dependent parallelism has its origin in IC-Prolog <ref> [51] </ref>. DeGroot [66] proposed a scheme for restricted AND-parallelism (RAP), i.e. only independent goals were allowed to execute in parallel. Runtime tests were added for determining variable groundness and independence. The goals were executed in parallel when the tests succeeded. <p> The current implementation of DDAS does not provide automated variable annotation, but Shen [180] mentions work in progress. 16 Summary Another approach to parallelism is to introduce explicit concurrency. This was done in IC-Prolog by Clark and McCabe <ref> [51] </ref> and in Relational Language by Clark and Gregory [52]. These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [173], FCP [175], Parlog [53, 60], Strand [77], Guarded Horn Clauses (GHC) [201, 202], and Flat GHC [50, 205].
Reference: 52. <author> K. L. Clark, S. Gregory, </author> <title> A Relational Language for Parallel Programming, </title> <booktitle> Proc. ACM Symp. Functional Programming and Computer Architecture, </booktitle> <year> 1981. </year> <month> -16 </month>
Reference-contexts: The current implementation of DDAS does not provide automated variable annotation, but Shen [180] mentions work in progress. 16 Summary Another approach to parallelism is to introduce explicit concurrency. This was done in IC-Prolog by Clark and McCabe [51] and in Relational Language by Clark and Gregory <ref> [52] </ref>. These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [173], FCP [175], Parlog [53, 60], Strand [77], Guarded Horn Clauses (GHC) [201, 202], and Flat GHC [50, 205].
Reference: 53. <author> K. L. Clark, S. Gregory, </author> <title> PARLOG: A Parallel Logic Programming Language, </title> <type> Rep. DOC 83/5, </type> <institution> Dept. Computing, Imperial College, </institution> <address> London, </address> <year> 1983. </year> <note> -5, 16 </note>
Reference-contexts: Instead of output unification, variables were allowed only one producer and, in some languages, only one consumer. 1.3. Reform Prolog 5 Examples of languages which exhibit one or more of these simplifications are: Concurrent Prolog [173, 174], FCP [175, 226], Parlog <ref> [53, 54] </ref>, (F)GHC [203, 204], Strand [77] and Janus [168]. These simplifications made it harder to write programs in these languages, the introduction of explicit concurrency being one of the main reasons. The following quote is from Tick's book on parallel logic programming [193, p. 410]. <p> This was done in IC-Prolog by Clark and McCabe [51] and in Relational Language by Clark and Gregory [52]. These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [173], FCP [175], Parlog <ref> [53, 60] </ref>, Strand [77], Guarded Horn Clauses (GHC) [201, 202], and Flat GHC [50, 205]. These languages are also referred to as de-evolutioned by Tick [194] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM.
Reference: 54. <author> K. L. Clark, S. Gregory, </author> <title> Notes on the Implementation of PARLOG, </title> <journal> Journal of Logic Programming, </journal> <volume> 2(1) </volume> <pages> 17-42, </pages> <year> 1985. </year> <month> -5 </month>
Reference-contexts: Instead of output unification, variables were allowed only one producer and, in some languages, only one consumer. 1.3. Reform Prolog 5 Examples of languages which exhibit one or more of these simplifications are: Concurrent Prolog [173, 174], FCP [175, 226], Parlog <ref> [53, 54] </ref>, (F)GHC [203, 204], Strand [77] and Janus [168]. These simplifications made it harder to write programs in these languages, the introduction of explicit concurrency being one of the main reasons. The following quote is from Tick's book on parallel logic programming [193, p. 410].
Reference: 55. <author> J. Cohen, </author> <title> Garbage Collection of Linked Data Structures, </title> <journal> Computing Surveys, </journal> <volume> 13(3) </volume> <pages> 341-367, </pages> <month> September </month> <year> 1981. </year> <journal> -20, </journal> <volume> 21, </volume> <pages> 128 </pages>
Reference-contexts: Prolog implementations such as SICStus Prolog [45] use a mark-sweep algorithm [131] that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. [9] as typical. It is based on the Deutsch-Schorr-Waite <ref> [55, 169] </ref> algorithm for marking and on Morris' algorithm [55, 144] for compacting. Touati and Hama [196] developed a generational copying garbage collector for Prolog. The heap is split into an old and a new generation. <p> Prolog implementations such as SICStus Prolog [45] use a mark-sweep algorithm [131] that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. [9] as typical. It is based on the Deutsch-Schorr-Waite [55, 169] algorithm for marking and on Morris' algorithm <ref> [55, 144] </ref> for compacting. Touati and Hama [196] developed a generational copying garbage collector for Prolog. The heap is split into an old and a new generation. <p> It was designed after our collector and use our ideas for handling internal pointers. It is not clear how their algorithm can be made generational. Cohen <ref> [55] </ref>, Appel [8], Jones and Lins [110], and Wilson [221] have written comprehensive surveys on general-purpose garbage collection algorithms. <p> We take the implementation of Appleby et al. [9] as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite <ref> [169, 55] </ref> algorithm for marking and on Morris' algorithm [144, 55] for compacting. 1. All live data are marked through roots found in registers, choice points, environments, and value trail entries (entries in the trail where the old value have been recorded, e.g., as a result of using setarg/3). <p> We take the implementation of Appleby et al. [9] as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite [169, 55] algorithm for marking and on Morris' algorithm <ref> [144, 55] </ref> for compacting. 1. All live data are marked through roots found in registers, choice points, environments, and value trail entries (entries in the trail where the old value have been recorded, e.g., as a result of using setarg/3).
Reference: 56. <author> A. Colmerauer, H. Kanoui, R. Pasero, P. Roussel, </author> <title> Un Systeme de Communication Homme-Machine en Fran~cais, </title> <institution> Groupe de Recherche en Intelligence Artificielle, Univ. de Aix-Marseille, </institution> <address> Luminy, </address> <year> 1972. </year> <note> -3, 99 </note>
Reference-contexts: have shown elsewhere that bounded quantification has a high potential for parallel implementation and we conclude in this paper that one can often run the same program efficiently on a sequential computer as well as on several kinds of parallel computers. 7.1 INTRODUCTION In logic programming languages, such as Prolog <ref> [56] </ref>, the only means for repetition has been through recursion. Recursion is well known to be theoretically sufficient (a universal Turing machine can be expressed in Horn clauses using recursion [198]) but there are many algorithms that can be expressed more concisely using definite iteration.
Reference: 57. <author> J. S. Conery, D. F. Kibler, </author> <title> Parallel Interpretation of Logic Programs, </title> <booktitle> Proc. ACM Symp. Functional Programming and Computer Architecture, </booktitle> <year> 1981. </year> <month> -14 </month>
Reference-contexts: The possibility of AND-parallel execution was noted by Kowalski [116] in 1974. Parallel Prolog can be divided into two categories: OR-parallel and AND-parallel. Our research falls into the AND-parallel category and we will not elaborate on the OR-parallel field. Conery and Kibler <ref> [57] </ref> proposed the first scheme for OR- and AND-parallel execution, where dependencies for the AND-parallel execution were determined using an ordering algorithm. The dependencies were calculated when entering a clause and updated when each body goal succeeded. The scheme has not been considered practical due to substantial overheads.
Reference: 58. <author> P. Cousot, R. Cousot, </author> <title> Abstract Interpretation: a Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints, </title> <booktitle> Conf. Record of the Fourth ACM Symp. on Principles of Programming Languages, </booktitle> <year> 1977. </year> <month> -64 </month>
Reference-contexts: In particular, the compiler can generate precisely the code for a sequential Prolog machine when data are local. 4.3 COMPILER ANALYSES The compiler analyses in the Reform Prolog compiler are based on abstract interpretation <ref> [58] </ref>. The abstract interpreter for Reform Prolog shares most of the characteristics of an abstract interpreter for sequential Prolog. This is natural, since each parallel process executes almost as a sequential Prolog machine. We have modified Debray's dataflow algorithms [61, 63] for analysis of parallel recursive predicates.
Reference: 59. <author> J. Crammond, </author> <title> A Garbage Collection Algorithm for Shared Memory Parallel Processors, </title> <journal> Intl. Journal of Parallel Programming, </journal> <volume> 17(6) </volume> <pages> 497-522, </pages> <year> 1988. </year> <note> -22, 166 </note>
Reference-contexts: The Aurora collector [220] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [215] for OR-parallel execution with its more complicated data structures. The closest we get to a parallel collector for an AND-parallel Prolog is Cram-mond's <ref> [59] </ref> mark-sweep collector for Parlog. It is parallelised by pushing all external references to each heap on a heap specific import stack. Each heap can then be collected using an almost sequential mark-sweep collector. <p> It should be noted that Reform Prolog parallelise programs in such a way that uniform amounts of data is likely to be created on each worker. However, it also use long lists when creating parallel processes, resulting in poor load balancing for those parts of the data. Crammond <ref> [59] </ref> measures his collector without taking the initial synchroni-sation overheads into account. He argues that they can be made arbitrarily small. This is not completely true. There is always an runtime overhead for either keeping the machine in a controlled state or for frequently checking if garbage collection is needed.
Reference: 60. <author> J. Crammond, </author> <title> The Abstract Machine and Implementation of Parallel Parlog, </title> <journal> New Generation Computing, </journal> <volume> 10(4) </volume> <pages> 385-422, </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <month> -16 </month>
Reference-contexts: This was done in IC-Prolog by Clark and McCabe [51] and in Relational Language by Clark and Gregory [52]. These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [173], FCP [175], Parlog <ref> [53, 60] </ref>, Strand [77], Guarded Horn Clauses (GHC) [201, 202], and Flat GHC [50, 205]. These languages are also referred to as de-evolutioned by Tick [194] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM.
Reference: 61. <author> S. K. Debray, </author> <title> Static Inference of Modes and Data Dependencies in Logic Programs, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(3) </volume> <pages> 418-450, </pages> <year> 1989. </year> <month> -64 </month>
Reference-contexts: The abstract interpreter for Reform Prolog shares most of the characteristics of an abstract interpreter for sequential Prolog. This is natural, since each parallel process executes almost as a sequential Prolog machine. We have modified Debray's dataflow algorithms <ref> [61, 63] </ref> for analysis of parallel recursive predicates. These algorithms compute call and success patterns for each procedure in the program. Call and success patterns describe the abstract values of the variables in a procedure call at procedure entry and exit, respectively.
Reference: 62. <author> S. K. Debray, </author> <title> A Simple Code Improvement Scheme for Prolog, </title> <journal> Journal of Logic Programming, </journal> <volume> 13(1) </volume> <pages> 57-88, </pages> <year> 1992. </year> <title> -34, 51 174 A Parallel Generational Copying Garbage Collector </title>
Reference-contexts: This is managed by combining three analyses: type inference, safeness analysis and locality analysis. The type inference domain is an extension of the Debray-Warren domain <ref> [62] </ref>, with the addition of support for lists and difference lists. The compiler uses both parallel and sequential types in code generation; parallel types hold at all times in the program, while sequential types hold when leftmost. Safeness analysis investigates when the computation is in a nondeterminate, parallel state. <p> The compiler then emits instructions based on this information, possibly falling back to more conservative code generation schemes when high-precision analysis results cannot be obtained. Type analysis The type inference phase employs an abstract domain based on the standard mode-analysis domain <ref> [62] </ref>, augmented with support for lists and difference lists as well as handling of certain aliases. The compiler distinguishes the parallel and sequential types of a predicate.
Reference: 63. <author> S. K. Debray, </author> <title> Efficient Dataflow Analysis of Logic Programs, </title> <journal> Journal of the ACM, </journal> <volume> 39(4) </volume> <pages> 949-984, </pages> <year> 1992. </year> <month> -64 </month>
Reference-contexts: The abstract interpreter for Reform Prolog shares most of the characteristics of an abstract interpreter for sequential Prolog. This is natural, since each parallel process executes almost as a sequential Prolog machine. We have modified Debray's dataflow algorithms <ref> [61, 63] </ref> for analysis of parallel recursive predicates. These algorithms compute call and success patterns for each procedure in the program. Call and success patterns describe the abstract values of the variables in a procedure call at procedure entry and exit, respectively.
Reference: 64. <author> S. K. Debray, M. Jain, </author> <title> A Simple Program Transformation for Parallelism, </title> <booktitle> Intl. Symp. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year> <month> -17 </month>
Reference-contexts: Only inner loops are parallelised, similarly to Fortran. The consequence is that less work can be parallelised and the exploited parallelism is fine-grained. Also, no compiler nor implementation exists. A compilation technique is outlined but never implemented. Hermenegildo and Carro [98] and Debray and Jain <ref> [64] </ref> discuss how goals can be spawned more efficiently using program transformation techniques. These have some similarities to the transformations described by Tarnlund [199] in his thesis. Hermenegildo and Carro also discuss how the &-Prolog implementation can be extended with low level primitives to support data-parallelism.
Reference: 65. <author> S. K. Debray, D. S. Warren, </author> <title> Automatic Mode Inference for Logic Programs, </title> <journal> Journal of Logic Programming, </journal> <volume> 5(3) </volume> <pages> 207-229, </pages> <year> 1988. </year> <month> -64 </month>
Reference-contexts: The compiler carries out four different analyses using the same basic algorithm. The abstract domains of these analyses are described below. Types The type domain is similar to that of Debray and Warren <ref> [65] </ref>, extended to handle difference lists [125]. For our present concerns it suffices to note that the type analysis can discover ground and nonvariable terms. The analysis precision is similar to that of Aquarius Prolog [210].
Reference: 66. <author> D. </author> <title> DeGroot, Restricted AND-parallelism, </title> <booktitle> Proc. Intl. Conf. Fifth Generation Computer Systems, </booktitle> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1984. </year> <month> -14 </month>
Reference-contexts: In this system only one goal, the producer, is allowed to bind a variable. All other goals are consumers. A consumer is not allowed to bind a variable, it suspends until the variable becomes bound. This form of dependent parallelism has its origin in IC-Prolog [51]. DeGroot <ref> [66] </ref> proposed a scheme for restricted AND-parallelism (RAP), i.e. only independent goals were allowed to execute in parallel. Runtime tests were added for determining variable groundness and independence. The goals were executed in parallel when the tests succeeded.
Reference: 67. <author> B. Demoen, G. Engels, P. Tarau, </author> <title> Segment Preserving Copying Garbage Collection for WAM based Prolog, </title> <booktitle> Proc. 1996 ACM Symp. on Applied Computing, </booktitle> <publisher> ACM Press, </publisher> <year> 1996 </year> <month> -13, </month> <pages> 21, 143, 144 </pages>
Reference-contexts: Scientific Contributions The sequential collection scheme showed how a generational copying collector could be applied to Prolog. Before this only mark-compact collectors and partially copying collectors were used. Our scheme showed how internal pointers were handled and how generational collection could be provided. Demoen, Engels and Tarau <ref> [67] </ref> have improved the scheme further using ideas from Bekkers, Ridoux and Ungaro [28]. This work also showed that instant reclaiming on backtracking can be sacrificed for data which survive garbage collection, without significant efficiency penalties. This is crucial for the parallel collector. <p> The main drawback of Sahlin's algorithm is that implementing the mark-sweep algorithm becomes more difficult, not to mention guaranteeing that there are no programming errors in its implementation. To our knowledge it has never been implemented. The collector described by Demoen et al. <ref> [67] </ref> maintains heap segments across garbage collections, and even increases the amount of memory that can be deallocated on backtracking. It was designed after our collector and use our ideas for handling internal pointers. It is not clear how their algorithm can be made generational. <p> The time complexity can be made proportional to the size of the live data either by linking together and sorting the live data [162] in a mark-slide collector (actually NlogN (N = live data)), or by using a copying collector <ref> [28, 35, 67] </ref>. We use a copying collector since it is more amenable to parallelisation. Copying collectors have the disadvantage that the relative positions of the data are not preserved. We briefly discuss how instant reclaiming and trailing can be handled in a sequential copying collector. <p> This is 144 A Parallel Generational Copying Garbage Collector achieved by copying the data in a carefully chosen order, i.e., starting at the oldest choice point. The technique has been further improved by Demoen, Engels and Tarau <ref> [67] </ref>. This technique requires barrier synchronisation after copying each choice point. It relies on that all data reachable from one choice point is copied (or processed) before the data reachable from the next is touched.
Reference: 68. <author> J. B. Dennis, </author> <title> Data Flow Supercomputers, </title> <journal> IEEE Computer, </journal> <volume> 13(11) </volume> <pages> 48-56, </pages> <year> 1980. </year> <month> -18 </month>
Reference-contexts: This in turn was inspired by other data flow models for parallel execution of Prolog proposed by Moto-oka and Fuchi [143] and Umeyama and Tamura [206]. These execution models were motivated from the research on data flow parallelism which emerged in the early 1980's <ref> [12, 68] </ref>. Nilsson and Tanaka [148, 149] designed a scheme for compiling Flat GHC into Fleng. Fleng is a primitive process-oriented language which has been implemented using a data-parallel interpreter.
Reference: 69. <author> L. P. Deutsch, D. G. Bobrow, </author> <title> An Efficient Incremental Automatic Garbage Collector, </title> <journal> Communications of the ACM, </journal> <volume> 19(9) </volume> <pages> 522-526, </pages> <year> 1976. </year> <note> -11, 20 </note>
Reference-contexts: This procedure must be fast to achieve high system performance since Prolog programs usually create large amounts of data. It has been observed in other languages, with similar allocation patters as Prolog, that most data tend to be short lived <ref> [69, 207] </ref>. This insight has led to the invention of generational garbage collection [121] where young objects are garbage collected more often than old. <p> This often involves tabulation techniques [30, 36, 80] and has also been applied in logic programming [18, 218]. Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived <ref> [69, 207] </ref>. This insight led to the invention of generational garbage collection [7, 121] where young objects are garbage collected more often than old. Copying collectors [76, 139] and generational copying collectors have been considered unsuitable for Prolog until recently.
Reference: 70. <author> A. Dovier, E. G. Omodeo, E. Pontelli, G. Rossi, log-: </author> <title> a Logic Programming Language with Finite Sets, </title> <booktitle> Proc. Intl. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <note> -19, 124 </note>
Reference-contexts: Specifications written in SPILL are executable by translating them to Pro-log, according to a set of translation rules. However, the quantifications are translated essentially as by Lloyd & Topor (cf. below). Some authors have studied the use of bounded quantifiers with sets, for example in SETL [170] and flogg <ref> [70] </ref>. Barklund & Hill [23] have studied how to incorporate restricted quantifications and arrays in Godel [101], while Apt [10] has studied how bounded quantifications and arrays could be used also in constraint based languages. <p> If the target Pro-log system had bounded quantifications, we expect that the specifications could be eexecuted more efficiently and that the "generators" for quantified variables in SPILL could be extended. Some authors have studied the use of bounded quantifiers with sets, for example in SETL [170] and log- <ref> [70] </ref>. Barklund & Hill [23] have studied how to incorporate restricted quantifications and arrays in Godel [101], while Apt [10] has studied how bounded quantifications and arrays could be used also in constraint based languages.
Reference: 71. <author> M. Doprochevsky, </author> <title> Garbage Collection in the OR-Parallel Logic Programming System ElipSys, </title> <type> ECRC Tech. Rep. </type> <institution> DPS-85, </institution> <year> 1991. </year> <month> -22 </month>
Reference-contexts: OR-parallel systems such as Muse [4] and Aurora [44, 129] use more or less sequential mark-sweep collectors <ref> [2, 71, 72] </ref>. The Aurora collector [220] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [215] for OR-parallel execution with its more complicated data structures. The closest we get to a parallel collector for an AND-parallel Prolog is Cram-mond's [59] mark-sweep collector for Parlog.
Reference: 72. <author> M. Dorochevsky, K. Schuerman, A. Veron, and J. Xu, </author> <title> Constraint Handling, Garbage Collection and Execution Model Issues in ElipSys, Parallel Execution of Logic Programs, </title> <booktitle> Proc. ICLP'91 Pre-Conf. Workshop, LNCS 569, </booktitle> <year> 1991. </year> <month> -22 </month>
Reference-contexts: OR-parallel systems such as Muse [4] and Aurora [44, 129] use more or less sequential mark-sweep collectors <ref> [2, 71, 72] </ref>. The Aurora collector [220] designed by Weemeeuw and Demoen is slightly more complicated since Aurora uses the SRI-model [215] for OR-parallel execution with its more complicated data structures. The closest we get to a parallel collector for an AND-parallel Prolog is Cram-mond's [59] mark-sweep collector for Parlog.
Reference: 73. <author> J. R. Ellis, K. Li, A. W. Appel, </author> <title> Real-time Concurrent Collection on Stock Multiprocessors, </title> <type> Tech. Rep. 25, </type> <institution> Digital Systems Research Center, Palo Alto, </institution> <year> 1988. </year> <month> -23 </month>
Reference-contexts: Our implementation instead uses the trail for pointing out references from the old to the new generation. Huelsbergen and Larus' [103] developed a concurrent copying garbage collector for shared memory with two processes; collector and mutator. Ellis, 1.6. Related Work 23 Li and Appel <ref> [73] </ref> propose a similar design with several mutators and one collector. Rojemo [161] extended the collector by Ellis et al. for the hv; Gi machine [13] (a parallel version of the G-machine [14]).
Reference: 74. <author> L.-H. Eriksson, M. Rayner, </author> <title> Incorporating Mutable Arrays into Logic Programming, </title> <booktitle> Proc. Intl. Logic Programming Conf., </booktitle> <institution> Uppsala University, Uppsala, </institution> <year> 1984. </year> <month> -104 </month>
Reference-contexts: Constant-time mechanisms with a reasonable overhead for that operation have been investigated, e.g., by Eriksson & Rayner <ref> [74] </ref>; note also the work on multiple reference management by Chikayama & Kimura [50].
Reference: 75. <author> B. Fagin, </author> <title> Data-Parallel Logic Programming Systems, </title> <type> Tech. Rep., </type> <institution> Thayer School of Engineering, Dartmouth Collage, </institution> <address> New Hampshire, </address> <year> 1990. </year> <month> -18 </month>
Reference-contexts: Yet another approach to data-parallelism is to add parallel data structures on which certain operations can be performed in parallel. This is the approach taken by Kacsuk in DAP Prolog [113], Barklund and Millroth in Nova Prolog [24], and by Fagin <ref> [75] </ref>. The idea of having explicitly parallel data structures have previously been used in other languages such as APL [106], CM Lisp [102, 185], ?Lisp [191], NESL [38] among others. These languages are often designed to exploit the architecture of a specific machine, i.e.
Reference: 76. <author> R. Fenichel, J. Yochelson, </author> <title> A LISP Garbage-collector for Virtual memory Computer Systems, </title> <journal> Communications of the ACM, </journal> <volume> 12(11) </volume> <pages> 611-612, </pages> <year> 1969. </year> <note> 9.8. BIBLIOGRAPHY 175 </note>
Reference-contexts: Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [69, 207]. This insight led to the invention of generational garbage collection [7, 121] where young objects are garbage collected more often than old. Copying collectors <ref> [76, 139] </ref> and generational copying collectors have been considered unsuitable for Prolog until recently. Prolog implementations such as SICStus Prolog [45] use a mark-sweep algorithm [131] that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. [9] as typical.
Reference: 77. <author> I. Foster, S. Taylor, Strand: </author> <title> A Practical Parallel Programming Language, </title> <booktitle> North American Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year> <note> -5, 16 </note>
Reference-contexts: Instead of output unification, variables were allowed only one producer and, in some languages, only one consumer. 1.3. Reform Prolog 5 Examples of languages which exhibit one or more of these simplifications are: Concurrent Prolog [173, 174], FCP [175, 226], Parlog [53, 54], (F)GHC [203, 204], Strand <ref> [77] </ref> and Janus [168]. These simplifications made it harder to write programs in these languages, the introduction of explicit concurrency being one of the main reasons. The following quote is from Tick's book on parallel logic programming [193, p. 410]. <p> This was done in IC-Prolog by Clark and McCabe [51] and in Relational Language by Clark and Gregory [52]. These ideas were adopted by others and resulted in the concurrent logic languages: Concurrent Prolog [173], FCP [175], Parlog [53, 60], Strand <ref> [77] </ref>, Guarded Horn Clauses (GHC) [201, 202], and Flat GHC [50, 205]. These languages are also referred to as de-evolutioned by Tick [194] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM.
Reference: 78. <author> I. Foster, W. Winsborough, </author> <title> Copy Avoidance through Compile-Time Analysis and Local Reuse, </title> <booktitle> Proc. Intl. Symp. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> -145 </month>
Reference-contexts: An alternative solution would be to eliminate internal variables altogether. This would simplify the algorithm significantly: no marking phase would be necessary, one GC bit less could be used, and one barrier synchro-nisation point would disappear. However, Foster and Winsborough <ref> [78] </ref> have shown that eliminating internal variables results in approximately 28% higher memory consumption and about 15% increase in execution time due to increased dereferencing and memory allocation. 9.3 PARALLEL GARBAGE COLLECTION Khayri Ali [3] has proposed an elegant scheme for parallel copying garbage collection on shared memory multiprocessors.
Reference: 79. <author> M. A. Friedman, </author> <title> A Characterization of Prolog Execution, </title> <type> PhD thesis, </type> <institution> Univ. of Wisconsin at Madison, </institution> <year> 1992. </year> <month> -71 </month>
Reference-contexts: Two factors contribute to this phenomenon: First, locking instructions are infrequent even in unoptimized code. Locking is spatially infrequent, since assignments to heap variables is a small fraction of the total amount of data written in Prolog implementations <ref> [192, 79] </ref>. Locking is temporally infrequent, since our Prolog implementation is based on byte-code emulation of WAM [214] instructions. In unoptimized code, 72 Compiler optimizations in Reform Prolog 2100-3700 machine instructions were executed for each locking operation on the three larger benchmarks.
Reference: 80. <author> D. P. Friedman, D. S. Wise, M. Wand, </author> <title> Recursive Programming through Table Look-Up, </title> <booktitle> Symp. on Symbolic and Algebraic Computation, ACM, 1976. -20, </booktitle> <pages> 125 </pages>
Reference-contexts: Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques <ref> [30, 36, 80] </ref> and has also been applied in logic programming [18, 218]. Garbage Collection of Prolog Sequential Collection It has been observed in other languages that most data tend to be short lived [69, 207]. <p> Hermenegildo & Carro [98] discuss how parallel execution of bounded quantifications relates to more traditional (AND) parallel execution of logic programs. Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science. This often involves tabulation techniques <ref> [30, 80, 36] </ref> and has also been applied in logic programming [218, 18].
Reference: 81. <author> T. Getzinger, </author> <title> Abstract Interpretation for the Compile-Time Analysis of Logic Programs, </title> <type> PhD thesis, Tech. Rep. </type> <institution> ACAL-TR-93-09, Univ. of South California, </institution> <year> 1993. </year> <month> -68 </month>
Reference-contexts: Furthermore, the absolute compilation times (0.6 to 14 seconds) are quite reasonable, in particular when considering that the SUN 630/MP is not a particulary fast machine by today's standards. Aquarius Prolog seems to have similar absolute analysis times on similar hardware <ref> [81] </ref>. 4.7 ANALYSIS RESULTS We measured analysis results for arguments in procedures called from parallel predicates. The following table shows the percentages of ground arguments and the locality information of non-ground arguments. The `total' percentage is weighted with respect to the total number of predicate arguments in all benchmarks. 4.8.
Reference: 82. <editor> D. Gries, </editor> <booktitle> The Science of Programming, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1981. </year> <note> -20, 125 </note>
Reference-contexts: The idea for using bounded quantification in logic programming was inspired by Tennent's proposed use of them in ALGOL-like languages [189, 190], and also by an exposition by Gries <ref> [82] </ref>. Hermenegildo and Carro [98] discuss how parallel execution of bounded quantifications relates to more traditional AND-parallel execution of logic programs. Arro and Barklund [11] have investigated how bounded quantifiers can be executed on the Connection Machine, a SIMD multiprocessor that directly supports data parallel computation. <p> Conclusions and Future Work 125 The idea for using bounded quantification in logic programming was inspired by Tennent's proposed use of them in ALGOL-like languages [189, 190], and also by an exposition of Gries <ref> [82] </ref>. Hermenegildo & Carro [98] discuss how parallel execution of bounded quantifications relates to more traditional (AND) parallel execution of logic programs. Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science.
Reference: 83. <author> G. Gupta, V. Santos Costa, R. Yang, M. V. Hermenegildo, IDIOM: </author> <title> Intergrating Dependent and-, Independent And- and Or-parallelism, </title> <booktitle> Proc. Intl. Logic Programming Symp., </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> -15 </month>
Reference-contexts: In the Extended Andorra Model [90, 163], a copy of the computation is created for each possible binding of a variable when all deterministic goals have been executed. The extended model is implemented in AKL [107, 108, 140]. Gupta, Santos Costa, Yang, and Hermenegildo <ref> [83] </ref> combine independent AND-parallelism, deterministic dependent AND-parallelism and OR-parallelism in IDIOM. This built on ideas from AO-WAM [86], which is an extension of RAP-WAM [95] with OR-parallelism. Another approach to solving the problem with conflicting bindings is to execute different branches separately and join the bindings when the branches terminate.
Reference: 84. <author> G. Gupta, M. V. Hermenegildo, </author> <title> ACE: And/Or-Parallel Copying-Based Execution of Logic Programs, Parallel Execution of Logic Programs, </title> <publisher> LNCS 569, Springer-Verlag, </publisher> <year> 1991. </year> <month> -14 </month>
Reference-contexts: Hermenegildo [95] refined DeGroot's scheme and added a backtracking semantics. Hermenegildo [94, 99, 100, 145] defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE <ref> [84, 85] </ref> and DDAS [178, 176, 177] are largely based on &-Prolog. Chang, Despain and DeGroot [47, 48] were first to propose that scheduling of parallel execution could be done through compile time analysis.
Reference: 85. <author> G. Gupta, M. V. Hermenegildo, E. Pontelli, V. Santos Costa, </author> <title> ACE: And/Or-parallel Copying-based Execution of Logic Programs, </title> <booktitle> Proc. Intl. Conf. Logic Programming, </booktitle> <publisher> MIT press, </publisher> <year> 1994. </year> <note> -14, 81 </note>
Reference-contexts: Hermenegildo [95] refined DeGroot's scheme and added a backtracking semantics. Hermenegildo [94, 99, 100, 145] defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE <ref> [84, 85] </ref> and DDAS [178, 176, 177] are largely based on &-Prolog. Chang, Despain and DeGroot [47, 48] were first to propose that scheduling of parallel execution could be done through compile time analysis. <p> These schemes reduce the performance difference between various independent AND-parallel systems and Reform Prolog, at the cost of added implementation complexity. Our restriction on nested parallelism greatly simplifies the design compared with other nested AND-parallel systems like &-Prolog [95, 94, 99], DASWAM [178, 176] and ACE <ref> [85] </ref>. * No distributed stacks with markers [40, 95, 177] or multiple stack sets [94] are needed. * No intelligent backtracking, or any other non-standard sequential back tracking scheme is needed. * Very small overheads for suspension are possible. * Busy wait can be used without deadlock detection machinery. * Large
Reference: 86. <author> G. Gupta, B. Jayaraman, </author> <title> Combined And-Or Parallelism on Shared Memory Multiprocessors, </title> <booktitle> North American Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year> <month> -15 </month>
Reference-contexts: The extended model is implemented in AKL [107, 108, 140]. Gupta, Santos Costa, Yang, and Hermenegildo [83] combine independent AND-parallelism, deterministic dependent AND-parallelism and OR-parallelism in IDIOM. This built on ideas from AO-WAM <ref> [86] </ref>, which is an extension of RAP-WAM [95] with OR-parallelism. Another approach to solving the problem with conflicting bindings is to execute different branches separately and join the bindings when the branches terminate. This is often combined with OR-parallelism.
Reference: 87. <author> G. Gupta, E. Pontelli, </author> <title> Last Alternative Optimization, </title> <booktitle> 8th IEEE Symp. on Parallel and Distributed Processing, IEEE Computer Society, 1996. -17, </booktitle> <pages> 81 </pages>
Reference-contexts: These have some similarities to the transformations described by Tarnlund [199] in his thesis. Hermenegildo and Carro also discuss how the &-Prolog implementation can be extended with low level primitives to support data-parallelism. Pontelli and Gupta <ref> [87, 156, 157, 158, 159] </ref> present a number of similar techniques for minimising the overheads for creating processes in &ACE. They then argue that data-parallelism can be efficiently exploited. However, there are still some significant differences compared to Reform Prolog. They can only exploit independent AND-parallelism. <p> This is similar to AKL [108, 107, 140] where deterministic AND-boxes are promoted. Gupta, Pontelli [156, 158] and Tang [157] use a similar technique in &ACE for reducing the overheads for independent AND-parallelism. Carro, Gupta, Hermenegildo, Pontelli, Santos Costa, Tang <ref> [95, 94, 99, 100, 159, 157, 87] </ref> and others have proposed different schemes for reducing the overheads for nested independent AND-parallelism. These schemes reduce the performance difference between various independent AND-parallel systems and Reform Prolog, at the cost of added implementation complexity.
Reference: 88. <author> R. H. Halstead, </author> <title> Implementation of Multilisp: Lisp on a Multiprocessor, </title> <booktitle> ACM Symp. LISP and Functional Programming, </booktitle> <year> 1984. </year> <month> -22 </month>
Reference-contexts: Object of equal size are allocated in the same memory block. The later makes it unsuitable for Prolog. Baker [15] describes a concurrent collection scheme divided into two processes, a mutator which creates data and a collector which performs garbage collection. Execution of the two processes are interleaved. Halstead <ref> [88] </ref> describe a parallelisation of Baker's algorithm. The heap is statically divides into separate areas collected by distinct PEs. No support for load balancing is provided. Herlihy and Moss [93] developed a concurrent lock-free version of Halstead's algorithm.
Reference: 89. <author> S. Haridi, </author> <title> A Logic Programming Language Based on the Andorra Model, New Generation Computing, </title> <address> 7(2-3):109-125, </address> <year> 1990. </year> <month> -15 </month>
Reference-contexts: Non-deterministic goals are allowed as long as no non-determinate bindings are created. Conflicting bindings result in global failure. Naish's ideas on deterministic binding and computation have been one of the major influences on Reform Prolog. Warren used ideas similar to PNU-Prolog when he formulated the Basic Andorra Model <ref> [89, 216] </ref> where deterministic goals are executed in parallel, and non-deterministic goals are suspended. The Basic Andorra Model has been implemented in Andorra-I [164, 165, 166, 225].
Reference: 90. <author> S. Haridi, S. Janson, </author> <title> Kernel Andorra Prolog and its Computation Model, </title> <booktitle> Intl. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <title> -15 176 A Parallel Generational Copying Garbage Collector </title>
Reference-contexts: Warren used ideas similar to PNU-Prolog when he formulated the Basic Andorra Model [89, 216] where deterministic goals are executed in parallel, and non-deterministic goals are suspended. The Basic Andorra Model has been implemented in Andorra-I [164, 165, 166, 225]. In the Extended Andorra Model <ref> [90, 163] </ref>, a copy of the computation is created for each possible binding of a variable when all deterministic goals have been executed. The extended model is implemented in AKL [107, 108, 140]. Gupta, Santos Costa, Yang, and Hermenegildo [83] combine independent AND-parallelism, deterministic dependent AND-parallelism and OR-parallelism in IDIOM.
Reference: 91. <author> R. Harper, D. MacQueen, R. Milner, </author> <title> Standard ML, </title> <type> Technical Report ECS-LFCS-86-2, </type> <institution> Dept. Computer Science, Edinburgh Univ., </institution> <year> 1986. </year> <note> -20, 124 </note>
Reference-contexts: The Common LISP language [184] (and some earlier LISP dialects), as well as Standard ML <ref> [91] </ref>, contain iteration, mapping and reduction constructs that in some cases resemble ours. The idea for using bounded quantification in logic programming was inspired by Tennent's proposed use of them in ALGOL-like languages [189, 190], and also by an exposition by Gries [82]. <p> The work on array comprehensions in Haskell and their compilation [5] also seems to be relevant for our work, although their context is lazy functional programming. The Common LISP language [184] (and some earlier LISP dialects), as well as Standard ML <ref> [91] </ref>, contain iteration, mapping and reduction constructs that in some cases resemble ours. 7.12. Conclusions and Future Work 125 The idea for using bounded quantification in logic programming was inspired by Tennent's proposed use of them in ALGOL-like languages [189, 190], and also by an exposition of Gries [82].
Reference: 92. <author> W. L. Harrison III, </author> <title> The Interprocedural Analysis and Parallelization of Scheme Programs, Lisp and Symbolic Computation, </title> <address> 2(3-4):176-396, </address> <year> 1989. </year> <month> -17 </month>
Reference-contexts: Warren and Hermenegildo [219] performed some early experiments with what they called MAP-parallelism. This was a limited form of independent data AND-parallelism where a procedure was applied over a set of data, similarly to mapcar in Lisp and DOALL in Fortran. Harrison <ref> [92] </ref> has developed a scheme for exploiting recursion parallelism in Scheme which is similar to Reform Prolog. The main difference is that Harrison only handles DOALL loops and recurrences which do not require synchronisation between different recursion levels. Reform Prolog handles general DO-ACROSS loops.
Reference: 93. <author> M. P. Herlihy, J. E. B. Moss, </author> <title> Lock-Free Garbage Collection for Multiprocessors, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3) </volume> <pages> 304-311, </pages> <year> 1992. </year> <month> -22 </month>
Reference-contexts: Execution of the two processes are interleaved. Halstead [88] describe a parallelisation of Baker's algorithm. The heap is statically divides into separate areas collected by distinct PEs. No support for load balancing is provided. Herlihy and Moss <ref> [93] </ref> developed a concurrent lock-free version of Halstead's algorithm. Lieberman and Hewitt [121] describe a real-time generational collector in which all pointers from older to newer generations pass through an indirection table. Our implementation instead uses the trail for pointing out references from the old to the new generation.
Reference: 94. <author> M. V. Hermenegildo, </author> <title> An Abstract Machine for Restricted AND-Parallel Execution of Logic Programs, </title> <booktitle> Third Intl. Conf. Logic Programming, </booktitle> <publisher> LNCS 225, Springer-Verlag, </publisher> <year> 1986. </year> <note> -14, 81 </note>
Reference-contexts: Runtime tests were added for determining variable groundness and independence. The goals were executed in parallel when the tests succeeded. This scheme exploited less parallelism than Conery and Kibler's but was expected to have substantially smaller runtime overheads. Hermenegildo [95] refined DeGroot's scheme and added a backtracking semantics. Hermenegildo <ref> [94, 99, 100, 145] </ref> defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE [84, 85] and DDAS [178, 176, 177] are largely based on &-Prolog. <p> This is similar to AKL [108, 107, 140] where deterministic AND-boxes are promoted. Gupta, Pontelli [156, 158] and Tang [157] use a similar technique in &ACE for reducing the overheads for independent AND-parallelism. Carro, Gupta, Hermenegildo, Pontelli, Santos Costa, Tang <ref> [95, 94, 99, 100, 159, 157, 87] </ref> and others have proposed different schemes for reducing the overheads for nested independent AND-parallelism. These schemes reduce the performance difference between various independent AND-parallel systems and Reform Prolog, at the cost of added implementation complexity. <p> These schemes reduce the performance difference between various independent AND-parallel systems and Reform Prolog, at the cost of added implementation complexity. Our restriction on nested parallelism greatly simplifies the design compared with other nested AND-parallel systems like &-Prolog <ref> [95, 94, 99] </ref>, DASWAM [178, 176] and ACE [85]. * No distributed stacks with markers [40, 95, 177] or multiple stack sets [94] are needed. * No intelligent backtracking, or any other non-standard sequential back tracking scheme is needed. * Very small overheads for suspension are possible. * Busy wait can <p> Our restriction on nested parallelism greatly simplifies the design compared with other nested AND-parallel systems like &-Prolog [95, 94, 99], DASWAM [178, 176] and ACE [85]. * No distributed stacks with markers [40, 95, 177] or multiple stack sets <ref> [94] </ref> are needed. * No intelligent backtracking, or any other non-standard sequential back tracking scheme is needed. * Very small overheads for suspension are possible. * Busy wait can be used without deadlock detection machinery. * Large number of processes can be created simultaneously.
Reference: 95. <author> M. V. Hermenegildo, </author> <title> An Abstract Machine Based Execution Model for Computer Architecture Design and Efficient Implementation of Logic Programs in Parallel, </title> <type> PhD thesis, </type> <institution> University of Texas At Austin, </institution> <year> 1986. </year> <journal> -14, </journal> <volume> 15, </volume> <pages> 81 </pages>
Reference-contexts: Runtime tests were added for determining variable groundness and independence. The goals were executed in parallel when the tests succeeded. This scheme exploited less parallelism than Conery and Kibler's but was expected to have substantially smaller runtime overheads. Hermenegildo <ref> [95] </ref> refined DeGroot's scheme and added a backtracking semantics. Hermenegildo [94, 99, 100, 145] defined and implemented the first independent AND-parallel system: &-Prolog. &-Prolog has played an important role for AND-parallel implementations. System such as ACE [84, 85] and DDAS [178, 176, 177] are largely based on &-Prolog. <p> The extended model is implemented in AKL [107, 108, 140]. Gupta, Santos Costa, Yang, and Hermenegildo [83] combine independent AND-parallelism, deterministic dependent AND-parallelism and OR-parallelism in IDIOM. This built on ideas from AO-WAM [86], which is an extension of RAP-WAM <ref> [95] </ref> with OR-parallelism. Another approach to solving the problem with conflicting bindings is to execute different branches separately and join the bindings when the branches terminate. This is often combined with OR-parallelism. Examples of such systems are Wise's EPILOG [222] and Wrench's APPNet [223]. <p> The machinery for this is cumbersome and it is questionable whether the gain is worth the added complexity. Related Work Hermenegildo <ref> [95, 96, 100] </ref> and others have noted the significance of left-biased scheduling mainly to ensure the no-slowdown criteria, meaning that less speculative work is performed first. Combined with determinism, we argue that parallel implementation overheads can be made minimal. <p> This is similar to AKL [108, 107, 140] where deterministic AND-boxes are promoted. Gupta, Pontelli [156, 158] and Tang [157] use a similar technique in &ACE for reducing the overheads for independent AND-parallelism. Carro, Gupta, Hermenegildo, Pontelli, Santos Costa, Tang <ref> [95, 94, 99, 100, 159, 157, 87] </ref> and others have proposed different schemes for reducing the overheads for nested independent AND-parallelism. These schemes reduce the performance difference between various independent AND-parallel systems and Reform Prolog, at the cost of added implementation complexity. <p> These schemes reduce the performance difference between various independent AND-parallel systems and Reform Prolog, at the cost of added implementation complexity. Our restriction on nested parallelism greatly simplifies the design compared with other nested AND-parallel systems like &-Prolog <ref> [95, 94, 99] </ref>, DASWAM [178, 176] and ACE [85]. * No distributed stacks with markers [40, 95, 177] or multiple stack sets [94] are needed. * No intelligent backtracking, or any other non-standard sequential back tracking scheme is needed. * Very small overheads for suspension are possible. * Busy wait can <p> Our restriction on nested parallelism greatly simplifies the design compared with other nested AND-parallel systems like &-Prolog [95, 94, 99], DASWAM [178, 176] and ACE [85]. * No distributed stacks with markers <ref> [40, 95, 177] </ref> or multiple stack sets [94] are needed. * No intelligent backtracking, or any other non-standard sequential back tracking scheme is needed. * Very small overheads for suspension are possible. * Busy wait can be used without deadlock detection machinery. * Large number of processes can be created simultaneously.
Reference: 96. <author> M. V. Hermenegildo, </author> <title> Relating Goal Scheduling, Precedence and Memory Management in AND-Parallel Execution of Logic Programs, </title> <booktitle> Fourth Intl. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1987. </year> <month> -81 </month>
Reference-contexts: The machinery for this is cumbersome and it is questionable whether the gain is worth the added complexity. Related Work Hermenegildo <ref> [95, 96, 100] </ref> and others have noted the significance of left-biased scheduling mainly to ensure the no-slowdown criteria, meaning that less speculative work is performed first. Combined with determinism, we argue that parallel implementation overheads can be made minimal.
Reference: 97. <author> M. V. Hermenegildo, D. Cabeza, M. Carro, </author> <title> Using Attributed Variables in the Implementation of Concurrent and Parallel Logic Programming Systems, </title> <booktitle> Proc. Intl. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year> <month> -16 </month>
Reference-contexts: These languages are also referred to as de-evolutioned by Tick [194] since they have been significantly restricted to allow efficient implementation. Most of the AND-parallel systems described above are implemented as extensions of WAM. Hermenegildo, Cabeza and Carro <ref> [97] </ref> recently proposed an elegant scheme where attributed variables [147, 120] are used for handling most of the support for parallelism. This technique makes it possible to model different forms of parallelism using the same runtime machinery, with some decrease in efficiency.
Reference: 98. <author> M. V. Hermenegildo, M. Carro, </author> <title> Relating Data-Parallelism and (And-) Parallelism in Logic Programs, </title> <journal> Journal of Computer Languages, </journal> <pages> 22(2-3), </pages> <year> 1996. </year> <journal> -17, </journal> <volume> 20, 81, </volume> <pages> 125 </pages>
Reference-contexts: However, their ideas are much less developed. Only inner loops are parallelised, similarly to Fortran. The consequence is that less work can be parallelised and the exploited parallelism is fine-grained. Also, no compiler nor implementation exists. A compilation technique is outlined but never implemented. Hermenegildo and Carro <ref> [98] </ref> and Debray and Jain [64] discuss how goals can be spawned more efficiently using program transformation techniques. These have some similarities to the transformations described by Tarnlund [199] in his thesis. <p> The idea for using bounded quantification in logic programming was inspired by Tennent's proposed use of them in ALGOL-like languages [189, 190], and also by an exposition by Gries [82]. Hermenegildo and Carro <ref> [98] </ref> discuss how parallel execution of bounded quantifications relates to more traditional AND-parallel execution of logic programs. Arro and Barklund [11] have investigated how bounded quantifiers can be executed on the Connection Machine, a SIMD multiprocessor that directly supports data parallel computation. <p> Admittedly other designs may be able to exploit more implicit parallelism. Our design relies on programmers to rewrite their programs to fit our execution model. Hermenegildo and Carro <ref> [98] </ref> observe that &-Prolog can be extended with support for data-parallel execution, similar to Reform Prolog. There are some significant differences. 82 A Scheme for Executing Nested Recursion Parallelism 1. <p> Conclusions and Future Work 125 The idea for using bounded quantification in logic programming was inspired by Tennent's proposed use of them in ALGOL-like languages [189, 190], and also by an exposition of Gries [82]. Hermenegildo & Carro <ref> [98] </ref> discuss how parallel execution of bounded quantifications relates to more traditional (AND) parallel execution of logic programs. Finally, we should mention that transforming recursive programs to iterative programs is an activity that has been studied extensively in computing science.

References-found: 99

