URL: http://www.uni-paderborn.de/fachbereich/AG/agmadh/Scripts/GENERAL/ParallelAlgorithms.ps.gz
Refering-URL: http://www.uni-paderborn.de/fachbereich/AG/agmadh/WWW/english/scripts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: guyb@cs.cmu.edu, bmm@cs.cmu.edu  
Title: Parallel Algorithms  
Author: Guy E. Blelloch and Bruce M. Maggs 
Address: 5000 Forbes Avenue Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Alok Aggarwal, Bernard Chazelle, Leo Guibas, Colm O'Dunlaing, and Chee Yap. </author> <title> Parallel computational geometry. </title> <journal> Algorithmica, </journal> <volume> 3(3) </volume> <pages> 293-327, </pages> <year> 1988. </year>
Reference-contexts: For example, suppose that, in parallel, each element of A with an even index is paired and summed with the next element of A, which has an odd index, i.e., A [0] is paired with A <ref> [1] </ref>, A [2] with A [3], and so on. The result is a new sequence of dn=2e numbers whose sum is identical to the sum that we wish to compute. This pairing and summing step can be repeated, and after dlog 2 ne steps, only the final sum remains. <p> The function dist creates a sequence of identical elements. For example, the expression dist (3; 5) creates the sequence [3; 3; 3; 3; 3]: The ++ function appends two sequences. For example <ref> [2; 1] </ref>++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. <p> The function dist creates a sequence of identical elements. For example, the expression dist (3; 5) creates the sequence [3; 3; 3; 3; 3]: The ++ function appends two sequences. For example [2; 1]++[5; 0; 3] create the sequence <ref> [2; 1; 5; 0; 3] </ref>. The flatten function converts a nested sequence into a flat sequence. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; [3; 2]; <ref> [1; 5] </ref>; [4; 6]]) creates the sequence [3; 5; 3; 2; 1; 5; 4; 6]: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; [3; 2]; [1; 5]; [4; 6]]) creates the sequence <ref> [3; 5; 3; 2; 1; 5; 4; 6] </ref>: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. The first argument is the sequence to modify and the second is a sequence of integer-value pairs that specify what to modify. <p> In the case of ++, the work is proportional to the length of the second sequence. We will use a few shorthand notations for specifying sequences. The expression [2::1] specifies the same sequence as the expression <ref> [2; 1; 0; 1] </ref>. Changing the left or right brackets surrounding a sequence, omits the first or last elements, i.e., [2::1) denotes the sequence [2; 1; 0]. The notation A [i::j] to denote the subsequence consisting of elements A [i] through A [j]. <p> We will use a few shorthand notations for specifying sequences. The expression [2::1] specifies the same sequence as the expression [2; 1; 0; 1]. Changing the left or right brackets surrounding a sequence, omits the first or last elements, i.e., <ref> [2::1) denotes the sequence [2; 1; 0] </ref>. The notation A [i::j] to denote the subsequence consisting of elements A [i] through A [j]. Similarly, A [i; j) denotes the subsequence A [i] through A [j 1]. <p> We will use a few shorthand notations for specifying sequences. The expression [2::1] specifies the same sequence as the expression [2; 1; 0; 1]. Changing the left or right brackets surrounding a sequence, omits the first or last elements, i.e., [2::1) denotes the sequence <ref> [2; 1; 0] </ref>. The notation A [i::j] to denote the subsequence consisting of elements A [i] through A [j]. Similarly, A [i; j) denotes the subsequence A [i] through A [j 1]. <p> For example, executing a plus-scan on the sequence <ref> [3; 5; 3; 1; 6] </ref> returns [0; 3; 8; 11; 12]. <p> We call the values V [j] whose indices j are successfully written into the hash table winners. In our example, the winners are V [0], V <ref> [1] </ref>, V [2], and V [3], 24 i.e., 69, 23, 91, and 18. The winners are added to the duplicate-free set that we are constructing, and then set aside. <p> Many of the sequential algorithms are based on divide-and-conquer and lead in a relatively straightforward manner to efficient parallel algorithms. Some others are based on a technique called plane sweeping, which does not parallelize well, but for which an analogous parallel technique, the plane sweep tree has been developed <ref> [1, 7] </ref>. In this section we describe parallel algorithms for two problems in two dimensions|closest pair and convex hull. For the convex hull we describe two algorithms. These algorithms are good examples of how sequential algorithms can be parallelized in a straightforward manner. <p> The patches can then be finished in constant depth by comparing all pairs between the two patches. The second technique <ref> [1, 8] </ref> uses a divide-and-conquer to separate the point set into p n regions, solves the convex hull on each region recursively and then merges all pairs of these regions using the binary search method.
Reference: [2] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: For example, suppose that, in parallel, each element of A with an even index is paired and summed with the next element of A, which has an odd index, i.e., A [0] is paired with A [1], A <ref> [2] </ref> with A [3], and so on. The result is a new sequence of dn=2e numbers whose sum is identical to the sum that we wish to compute. This pairing and summing step can be repeated, and after dlog 2 ne steps, only the final sum remains. <p> The designer of a sequential algorithm typically formulates the algorithm using an abstract model of computation called a random-access machine (RAM) <ref> [2, Chapter 1] </ref>. In this model, the machine consists of a single processor connected to a memory system. Each basic CPU operation, including arithmetic operations, logical operations, and memory accesses, requires one time step. The designer's goal is to develop an algorithm with modest time and memory requirements. <p> The function dist creates a sequence of identical elements. For example, the expression dist (3; 5) creates the sequence [3; 3; 3; 3; 3]: The ++ function appends two sequences. For example <ref> [2; 1] </ref>++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. <p> The function dist creates a sequence of identical elements. For example, the expression dist (3; 5) creates the sequence [3; 3; 3; 3; 3]: The ++ function appends two sequences. For example [2; 1]++[5; 0; 3] create the sequence <ref> [2; 1; 5; 0; 3] </ref>. The flatten function converts a nested sequence into a flat sequence. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; <ref> [3; 2] </ref>; [1; 5]; [4; 6]]) creates the sequence [3; 5; 3; 2; 1; 5; 4; 6]: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; [3; 2]; [1; 5]; [4; 6]]) creates the sequence <ref> [3; 5; 3; 2; 1; 5; 4; 6] </ref>: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. The first argument is the sequence to modify and the second is a sequence of integer-value pairs that specify what to modify. <p> For example [0; 0; 0; 0; 0; 0; 0; 0] [(4; 2); (2; 5); (5; 9)] inserts the 2, 5 and 9 into the sequence at locations 4, 2 and 5, respectively, returning <ref> [0; 0; 5; 0; 2; 9; 0; 0] </ref>: As in the PRAM model, the issue of concurrent writes arises if an index is repeated. Rather than choosing a single policy for resolving concurrent writes, we will explain the policy used for the individual algorithms. <p> In the case of ++, the work is proportional to the length of the second sequence. We will use a few shorthand notations for specifying sequences. The expression [2::1] specifies the same sequence as the expression <ref> [2; 1; 0; 1] </ref>. Changing the left or right brackets surrounding a sequence, omits the first or last elements, i.e., [2::1) denotes the sequence [2; 1; 0]. The notation A [i::j] to denote the subsequence consisting of elements A [i] through A [j]. <p> We will use a few shorthand notations for specifying sequences. The expression [2::1] specifies the same sequence as the expression [2; 1; 0; 1]. Changing the left or right brackets surrounding a sequence, omits the first or last elements, i.e., [2::1) denotes the sequence <ref> [2; 1; 0] </ref>. The notation A [i::j] to denote the subsequence consisting of elements A [i] through A [j]. Similarly, A [i; j) denotes the subsequence A [i] through A [j 1]. <p> As an example, multiprefix ([(1; 5); (0; 2); (0; 3); (1; 4); (0; 1); (2; 2)]) 21 returns the sequence <ref> [0; 0; 2; 5; 5; 0] </ref>: The fetch-and-add operation is a weaker version of the multiprefix operation, in which the order of the input elements for each scan is not necessarily the same as their order in the input sequence A. <p> We call the values V [j] whose indices j are successfully written into the hash table winners. In our example, the winners are V [0], V [1], V <ref> [2] </ref>, and V [3], 24 i.e., 69, 23, 91, and 18. The winners are added to the duplicate-free set that we are constructing, and then set aside. <p> A new frontier is generated by collecting all the neighbors of the current frontier vertices in parallel and removing any that have already been visited. This is 28 (a) Step Frontier 0 [0] 2 <ref> [2, 5, 8] </ref> 5 [7, 10, 13] 7 [15] of the BFS of G with s = 0. (c) A BFS tree. not sufficient on its own, however, since multiple vertices might collect the same unvisited vertex. For example, consider the graph in Figure 6.
Reference: [3] <author> S. G. Akl. </author> <title> Parallel Sorting Algorithms. </title> <publisher> Academic Press, </publisher> <address> Toronto, </address> <year> 1985. </year>
Reference-contexts: For example, suppose that, in parallel, each element of A with an even index is paired and summed with the next element of A, which has an odd index, i.e., A [0] is paired with A [1], A [2] with A <ref> [3] </ref>, and so on. The result is a new sequence of dn=2e numbers whose sum is identical to the sum that we wish to compute. This pairing and summing step can be repeated, and after dlog 2 ne steps, only the final sum remains. <p> Our language constructs, syntax and cost rules are based on the Nesl language [16]. The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". <p> The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; <p> This can be read: "in parallel, for each a in the sequence <ref> [3; 4; 9; 5] </ref>, square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; 4; 9; 5] such that a is greater than 0, square a". It returns the sequence [9; 25]. The elements that remain maintain their relative order. <p> <ref> [3; 4; 9; 5] </ref>, square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; 4; 9; 5] such that a is greater than 0, square a". It returns the sequence [9; 25]. The elements that remain maintain their relative order. The parallel-do construct is used to evaluate multiple statements in parallel. <p> The function dist creates a sequence of identical elements. For example, the expression dist (3; 5) creates the sequence <ref> [3; 3; 3; 3; 3] </ref>: The ++ function appends two sequences. For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. <p> The function dist creates a sequence of identical elements. For example, the expression dist (3; 5) creates the sequence [3; 3; 3; 3; 3]: The ++ function appends two sequences. For example [2; 1]++[5; 0; 3] create the sequence <ref> [2; 1; 5; 0; 3] </ref>. The flatten function converts a nested sequence into a flat sequence. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; <ref> [3; 2] </ref>; [1; 5]; [4; 6]]) creates the sequence [3; 5; 3; 2; 1; 5; 4; 6]: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; [3; 2]; [1; 5]; [4; 6]]) creates the sequence <ref> [3; 5; 3; 2; 1; 5; 4; 6] </ref>: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. The first argument is the sequence to modify and the second is a sequence of integer-value pairs that specify what to modify. <p> For example, executing a plus-scan on the sequence <ref> [3; 5; 3; 1; 6] </ref> returns [0; 3; 8; 11; 12]. <p> For example, executing a plus-scan on the sequence [3; 5; 3; 1; 6] returns <ref> [0; 3; 8; 11; 12] </ref>. <p> We call the values V [j] whose indices j are successfully written into the hash table winners. In our example, the winners are V [0], V [1], V [2], and V <ref> [3] </ref>, 24 i.e., 69, 23, 91, and 18. The winners are added to the duplicate-free set that we are constructing, and then set aside. <p> In this section we limit our discussion to two parallel sorting algorithms, QuickSort and radix sort. Both of these algorithms are easy to program, and both work well in practice. Many more sorting algorithms can be found in the literature. The interested reader is referred to <ref> [3, 39, 47] </ref> for more complete coverage. 5.1 QuickSort We begin our discussion of sorting with a parallel version of QuickSort.
Reference: [4] <author> G. S. Almasi and A Gottlieb. </author> <title> Highly Parallel Computing. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1989. </year>
Reference-contexts: Our language constructs, syntax and cost rules are based on the Nesl language [16]. The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". <p> The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; <p> This can be read: "in parallel, for each a in the sequence <ref> [3; 4; 9; 5] </ref>, square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; 4; 9; 5] such that a is greater than 0, square a". It returns the sequence [9; 25]. The elements that remain maintain their relative order. <p> <ref> [3; 4; 9; 5] </ref>, square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; 4; 9; 5] such that a is greater than 0, square a". It returns the sequence [9; 25]. The elements that remain maintain their relative order. The parallel-do construct is used to evaluate multiple statements in parallel. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; [3; 2]; [1; 5]; <ref> [4; 6] </ref>]) creates the sequence [3; 5; 3; 2; 1; 5; 4; 6]: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; [3; 2]; [1; 5]; [4; 6]]) creates the sequence <ref> [3; 5; 3; 2; 1; 5; 4; 6] </ref>: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. The first argument is the sequence to modify and the second is a sequence of integer-value pairs that specify what to modify. <p> In our example, V [5] and V [6] (23 and 18) were defeated by items with the same value, and V <ref> [4] </ref> (42) was defeated by an item with a different value. Items of the first type are set aside because they are duplicates. Items of the second type are retained, and we repeat the entire process on them using a different hash function. <p> In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages. More information on these subjects can be found in [36, 73], <ref> [4, 37] </ref>, and [17, 58], respectively. Other topics likely to interest the reader of this chapter include distributed algorithms [51] and VLSI layout theory and computation [49, 72]. 10 Defining Terms CRCW.
Reference: [5] <author> G. L. Anderson and G. L. Miller. </author> <title> A simple randomized parallel algorithm for list-ranking. </title> <journal> Information Processing Letters, </journal> <volume> 33(5) </volume> <pages> 269-273, </pages> <year> 1990. </year>
Reference-contexts: Our language constructs, syntax and cost rules are based on the Nesl language [16]. The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". <p> The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; <p> This can be read: "in parallel, for each a in the sequence <ref> [3; 4; 9; 5] </ref>, square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; 4; 9; 5] such that a is greater than 0, square a". It returns the sequence [9; 25]. The elements that remain maintain their relative order. <p> <ref> [3; 4; 9; 5] </ref>, square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; 4; 9; 5] such that a is greater than 0, square a". It returns the sequence [9; 25]. The elements that remain maintain their relative order. The parallel-do construct is used to evaluate multiple statements in parallel. <p> The function dist creates a sequence of identical elements. For example, the expression dist (3; 5) creates the sequence [3; 3; 3; 3; 3]: The ++ function appends two sequences. For example [2; 1]++[5; 0; 3] create the sequence <ref> [2; 1; 5; 0; 3] </ref>. The flatten function converts a nested sequence into a flat sequence. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; [3; 2]; <ref> [1; 5] </ref>; [4; 6]]) creates the sequence [3; 5; 3; 2; 1; 5; 4; 6]: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; [3; 2]; [1; 5]; [4; 6]]) creates the sequence <ref> [3; 5; 3; 2; 1; 5; 4; 6] </ref>: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. The first argument is the sequence to modify and the second is a sequence of integer-value pairs that specify what to modify. <p> For example [0; 0; 0; 0; 0; 0; 0; 0] [(4; 2); (2; 5); (5; 9)] inserts the 2, 5 and 9 into the sequence at locations 4, 2 and 5, respectively, returning <ref> [0; 0; 5; 0; 2; 9; 0; 0] </ref>: As in the PRAM model, the issue of concurrent writes arises if an index is repeated. Rather than choosing a single policy for resolving concurrent writes, we will explain the policy used for the individual algorithms. <p> For example, executing a plus-scan on the sequence <ref> [3; 5; 3; 1; 6] </ref> returns [0; 3; 8; 11; 12]. <p> As an example, multiprefix ([(1; 5); (0; 2); (0; 3); (1; 4); (0; 1); (2; 2)]) 21 returns the sequence <ref> [0; 0; 2; 5; 5; 0] </ref>: The fetch-and-add operation is a weaker version of the multiprefix operation, in which the order of the input elements for each scan is not necessarily the same as their order in the input sequence A. <p> If we set m = n= log n, these reduce to W (n) = O (n) and D (n) = O (log 2 n). Using a technique called contraction, it is possible to design a list ranking algorithm that runs in O (n) work and O (log n) depth <ref> [5, 6] </ref>. This technique can also be applied to trees [54, 56]. 3.6 Removing Duplicates Given a sequence of items, the remove-duplicates algorithm removes all duplicates, returning the resulting sequence. <p> Among the losers, we must distinguish between two types of items, those that were defeated by an item with the same value, and those that were defeated by an item with a different value. In our example, V <ref> [5] </ref> and V [6] (23 and 18) were defeated by items with the same value, and V [4] (42) was defeated by an item with a different value. Items of the first type are set aside because they are duplicates. <p> A new frontier is generated by collecting all the neighbors of the current frontier vertices in parallel and removing any that have already been visited. This is 28 (a) Step Frontier 0 [0] 2 <ref> [2, 5, 8] </ref> 5 [7, 10, 13] 7 [15] of the BFS of G with s = 0. (c) A BFS tree. not sufficient on its own, however, since multiple vertices might collect the same unvisited vertex. For example, consider the graph in Figure 6.
Reference: [6] <author> R. J. Anderson and G. L. Miller. </author> <title> Deterministic parallel list ranking. </title> <editor> In J. Reif, editor, </editor> <booktitle> Aegean Workshop on Computing: VLSI Algorithms and Architectures. Volume 319 of Lecture Notes in Computer Science, </booktitle> <pages> pages 81-90, </pages> <address> New York, NY, June 1988. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; [3; 2]; [1; 5]; <ref> [4; 6] </ref>]) creates the sequence [3; 5; 3; 2; 1; 5; 4; 6]: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. <p> For example [2; 1]++[5; 0; 3] create the sequence [2; 1; 5; 0; 3]. The flatten function converts a nested sequence into a flat sequence. For example, flatten ([[3; 5]; [3; 2]; [1; 5]; [4; 6]]) creates the sequence <ref> [3; 5; 3; 2; 1; 5; 4; 6] </ref>: The function is used to write multiple elements into a sequence in parallel. It takes two arguments. The first argument is the sequence to modify and the second is a sequence of integer-value pairs that specify what to modify. <p> For example, executing a plus-scan on the sequence <ref> [3; 5; 3; 1; 6] </ref> returns [0; 3; 8; 11; 12]. <p> If we set m = n= log n, these reduce to W (n) = O (n) and D (n) = O (log 2 n). Using a technique called contraction, it is possible to design a list ranking algorithm that runs in O (n) work and O (log n) depth <ref> [5, 6] </ref>. This technique can also be applied to trees [54, 56]. 3.6 Removing Duplicates Given a sequence of items, the remove-duplicates algorithm removes all duplicates, returning the resulting sequence. <p> Among the losers, we must distinguish between two types of items, those that were defeated by an item with the same value, and those that were defeated by an item with a different value. In our example, V [5] and V <ref> [6] </ref> (23 and 18) were defeated by items with the same value, and V [4] (42) was defeated by an item with a different value. Items of the first type are set aside because they are duplicates.
Reference: [7] <author> Mikhail J. Atallah, Richard Cole, and Michael T. Goodrich. </author> <title> Cascading divide-and-conquer: A technique for designing parallel algorithms. </title> <journal> SIAM Journal of Computing, </journal> <volume> 18(3) </volume> <pages> 499-532, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: A new frontier is generated by collecting all the neighbors of the current frontier vertices in parallel and removing any that have already been visited. This is 28 (a) Step Frontier 0 [0] 2 [2, 5, 8] 5 <ref> [7, 10, 13] </ref> 7 [15] of the BFS of G with s = 0. (c) A BFS tree. not sufficient on its own, however, since multiple vertices might collect the same unvisited vertex. For example, consider the graph in Figure 6. <p> Many of the sequential algorithms are based on divide-and-conquer and lead in a relatively straightforward manner to efficient parallel algorithms. Some others are based on a technique called plane sweeping, which does not parallelize well, but for which an analogous parallel technique, the plane sweep tree has been developed <ref> [1, 7] </ref>. In this section we describe parallel algorithms for two problems in two dimensions|closest pair and convex hull. For the convex hull we describe two algorithms. These algorithms are good examples of how sequential algorithms can be parallelized in a straightforward manner.
Reference: [8] <author> Mikhail J. Atallah and Michael T. Goodrich. </author> <title> Efficient parallel solutions to some geometric problems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 3(4) </volume> <pages> 492-507, </pages> <month> December </month> <year> 1986. </year> <month> 54 </month>
Reference-contexts: For example, executing a plus-scan on the sequence [3; 5; 3; 1; 6] returns <ref> [0; 3; 8; 11; 12] </ref>. <p> A new frontier is generated by collecting all the neighbors of the current frontier vertices in parallel and removing any that have already been visited. This is 28 (a) Step Frontier 0 [0] 2 <ref> [2, 5, 8] </ref> 5 [7, 10, 13] 7 [15] of the BFS of G with s = 0. (c) A BFS tree. not sufficient on its own, however, since multiple vertices might collect the same unvisited vertex. For example, consider the graph in Figure 6. <p> The patches can then be finished in constant depth by comparing all pairs between the two patches. The second technique <ref> [1, 8] </ref> uses a divide-and-conquer to separate the point set into p n regions, solves the convex hull on each region recursively and then merges all pairs of these regions using the binary search method.
Reference: [9] <author> Mikhail J. Atallah and Michael T. Goodrich. </author> <title> Parallel algorithms for some functions of two convex polygons. </title> <journal> Algorithmica, </journal> <volume> 3(4) </volume> <pages> 535-548, </pages> <year> 1988. </year>
Reference-contexts: Our language constructs, syntax and cost rules are based on the Nesl language [16]. The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". <p> The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 [3; 4; 9; 5]g squares each element of the sequence [3; 4; 9; 5] returning the sequence <ref> [9; 16; 81; 25] </ref>. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> It uses a set like notation. For example, the expression fa fl a : a 2 <ref> [3; 4; 9; 5] </ref>g squares each element of the sequence [3; 4; 9; 5] returning the sequence [9; 16; 81; 25]. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; <p> This can be read: "in parallel, for each a in the sequence <ref> [3; 4; 9; 5] </ref>, square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; 4; 9; 5] such that a is greater than 0, square a". It returns the sequence [9; 25]. The elements that remain maintain their relative order. <p> <ref> [3; 4; 9; 5] </ref>, square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; 4; 9; 5] such that a is greater than 0, square a". It returns the sequence [9; 25]. The elements that remain maintain their relative order. The parallel-do construct is used to evaluate multiple statements in parallel. <p> For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; 4; 9; 5] such that a is greater than 0, square a". It returns the sequence <ref> [9; 25] </ref>. The elements that remain maintain their relative order. The parallel-do construct is used to evaluate multiple statements in parallel. It is expressed by listing the set of statements after an in parallel do. <p> For example [0; 0; 0; 0; 0; 0; 0; 0] [(4; 2); (2; 5); (5; 9)] inserts the 2, 5 and 9 into the sequence at locations 4, 2 and 5, respectively, returning <ref> [0; 0; 5; 0; 2; 9; 0; 0] </ref>: As in the PRAM model, the issue of concurrent writes arises if an index is repeated. Rather than choosing a single policy for resolving concurrent writes, we will explain the policy used for the individual algorithms. <p> The first involves implementing the search for the bridge points such that it runs in constant depth with linear work <ref> [9] </ref>. This involves sampling every p nth point on each hull and comparing all pairs of these two samples to narrow the search region down to regions of size p n in constant depth.
Reference: [10] <author> Baruch Awerbuch and Yossi Shiloach. </author> <title> New connectivity and MSF algorithms for Ultracom-puter and PRAM. </title> <booktitle> In Proceedings International Conference on Parallel Processing, </booktitle> <pages> pages 175-179, </pages> <year> 1983. </year>
Reference-contexts: A new frontier is generated by collecting all the neighbors of the current frontier vertices in parallel and removing any that have already been visited. This is 28 (a) Step Frontier 0 [0] 2 [2, 5, 8] 5 <ref> [7, 10, 13] </ref> 7 [15] of the BFS of G with s = 0. (c) A BFS tree. not sufficient on its own, however, since multiple vertices might collect the same unvisited vertex. For example, consider the graph in Figure 6. <p> Here we mention some of them. The deterministic algorithm can be improved to run in O (log n) depth with the same work bounds <ref> [10, 68] </ref>. The basic idea is to interleave the hooking steps with the shortcutting steps. The one tricky aspect is that we must always hook in the same direction (i.e., from smaller to larger), so as not to create cycles.
Reference: [11] <author> A. Bar-Noy and S. Kipnis. </author> <title> Designing broadcasting algorithms in the postal model for message-passing systems. </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 13-22, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In this case, the bandwidth can be expressed as the minimum gap g between successive injections of messages into the network. Three models that characterize a network in terms of its latency and bandwidth are the Postal model <ref> [11] </ref>, the Bulk-Synchronous Parallel (BSP) model [74], and the LogP model [25]. In the Postal model, a network is described by a single parameter L, its latency. The Bulk-Synchronous Parallel model adds a second parameter g, the minimum ratio of computation steps to communication steps, i.e., the gap. <p> For example, executing a plus-scan on the sequence [3; 5; 3; 1; 6] returns <ref> [0; 3; 8; 11; 12] </ref>.
Reference: [12] <author> V. E. </author> <title> Benes. Mathematical Theory of Connecting Networks and Telephone Traffic. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1965. </year>
Reference-contexts: A multistage network is used to connect one set of switches called the input switches to another set called the output switches through a sequence of stages of switches. Such networks were originally designed for telephone networks <ref> [12] </ref>. The stages of a multistage network are numbered 1 through L, where L is the depth of the network. The input switches form stage 1 and the output switches form stage L. <p> For example, executing a plus-scan on the sequence [3; 5; 3; 1; 6] returns <ref> [0; 3; 8; 11; 12] </ref>.
Reference: [13] <author> Jon L. Bentley and Michael I. Shamos. </author> <title> Divide-and-conquer in multidimensional space. </title> <booktitle> In Proceedings ACM Symposium on Theory of Computing, </booktitle> <pages> pages 220-230, </pages> <year> 1976. </year>
Reference-contexts: A new frontier is generated by collecting all the neighbors of the current frontier vertices in parallel and removing any that have already been visited. This is 28 (a) Step Frontier 0 [0] 2 [2, 5, 8] 5 <ref> [7, 10, 13] </ref> 7 [15] of the BFS of G with s = 0. (c) A BFS tree. not sufficient on its own, however, since multiple vertices might collect the same unvisited vertex. For example, consider the graph in Figure 6. <p> The distance is usually defined as Euclidean distance. Here we describe a closest pair algorithm for two dimensional space, also called the planar closest-pair problem. The algorithm a parallel version of a standard sequential algorithm <ref> [13] </ref>, and for n points, it requires the same work as the sequential versions, O (n log n), and had depth O (log 2 n). The work is optimal. <p> The merge by y function merges L 0 and R 0 along the y axis and can use a standard parallel merge routine. The interesting aspect of the code is the boundary merge routine, which works on the same principle as described by Bentley and Shamos <ref> [13] </ref>, and can be computed with O (log n) depth and O (n) work. We first review the principle and then show how it is implemented in parallel.
Reference: [14] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Finding small graph separators is useful for partitioning data among processors to reduce communication [65, Chapter 14]. Hashing is useful for load balancing and mapping addresses to memory [75, 40]. Iterative techniques are useful as a replacement for direct methods for solving linear systems <ref> [14] </ref>. 3 Basic operations on sequences, lists, and trees We begin our presentation of parallel algorithms with a collection of algorithms for performing basic operations on sequences, lists, and trees. <p> Here we briefly discuss some of the problems and results. We suggest the following sources for further 49 information on parallel numerical algorithms [65, Chapters 12-14], [39, Chapter 8], [46, Chapters 5, 10 and 11] and <ref> [14] </ref>. 7.1 Matrix Operations Matrix operations form the core of many numerical algorithms and led to some of the earliest work on parallel algorithms. <p> Given a context-free grammar, deter mine if it can generate the empty string. 9 Concluding remarks In a chapter of this length, it is not possible to provide comphrensive coverage of the subject of parallel algorithms. Fortunately, there are number of excellent textbooks and surveys on parallel algorithms including <ref> [14, 26, 27, 30, 39, 41, 46, 47, 65] </ref>. In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages.
Reference: [15] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In a vector model an algorithm is expressed as a sequence of steps, each of which performs an operation on a vector (i.e., sequence) of input values, and produces a vector result <ref> [60, 15] </ref>. The work of each step is equal to the length of its input (or output) vector. The work of an algorithm is the sum of the work of its steps. The depth of an algorithm is the number of vector steps. <p> A new frontier is generated by collecting all the neighbors of the current frontier vertices in parallel and removing any that have already been visited. This is 28 (a) Step Frontier 0 [0] 2 [2, 5, 8] 5 [7, 10, 13] 7 <ref> [15] </ref> of the BFS of G with s = 0. (c) A BFS tree. not sufficient on its own, however, since multiple vertices might collect the same unvisited vertex. For example, consider the graph in Figure 6. On step 2 vertices 5 and 8 will both collect vertex 9.
Reference: [16] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 39(3) </volume> <pages> 85-97, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: The work of an algorithm is the sum of the work of its steps. The depth of an algorithm is the number of vector steps. In a language model, a work-depth cost is associated with each programming language construct <ref> [18, 16] </ref>. For example, the work for calling two functions in parallel is equal to the sum of the work of the two calls. <p> The constructs for expressing parallelism, however, may be unfamiliar. We will be using two parallel constructs|a parallel apply-to-each construct and a 13 parallel-do construct|and a small set of parallel primitives on sequences (one dimensional arrays). Our language constructs, syntax and cost rules are based on the Nesl language <ref> [16] </ref>. The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. <p> The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 [3; 4; 9; 5]g squares each element of the sequence [3; 4; 9; 5] returning the sequence <ref> [9; 16; 81; 25] </ref>. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter.
Reference: [17] <author> Guy E. Blelloch, K. Mani Chandy, and Suresh Jagannathan, </author> <title> editors. Specification of Parallel Algorithms. </title> <booktitle> Volume 18 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science. </booktitle> <publisher> American Mathematical Society, </publisher> <address> Providence, RI, </address> <year> 1994. </year>
Reference-contexts: In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages. More information on these subjects can be found in [36, 73], [4, 37], and <ref> [17, 58] </ref>, respectively. Other topics likely to interest the reader of this chapter include distributed algorithms [51] and VLSI layout theory and computation [49, 72]. 10 Defining Terms CRCW.
Reference: [18] <author> Guy E. Blelloch and John Greiner. </author> <title> Parallelism in sequential functional languages. </title> <booktitle> In Proceedings of the Symposium on Functional Programming and Computer Architecture, </booktitle> <pages> pages 226-237, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The work of an algorithm is the sum of the work of its steps. The depth of an algorithm is the number of vector steps. In a language model, a work-depth cost is associated with each programming language construct <ref> [18, 16] </ref>. For example, the work for calling two functions in parallel is equal to the sum of the work of the two calls. <p> The description we give here is somewhat informal, but should suffice for the purpose of this chapter. The language and costs can be properly formalized using a profiling semantics <ref> [18] </ref>. Most of the syntax that we use should be familiar to readers who have programmed in Algol-like languages, such as Pascal and C. The constructs for expressing parallelism, however, may be unfamiliar.
Reference: [19] <author> Guy E. Blelloch, Charles E. Leiserson, Bruce M. Maggs, C. Gregory Plaxton, Stephen J. Smith, and Marco Zagha. </author> <title> A comparison of sorting algorithms for the Connection Machine CM-2. </title> <booktitle> In Proceedings Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 3-16, </pages> <address> Hilton Head, SC, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Hence there is an optimum value of s, typically larger than one, which minimizes the total time. The sorting algorithm that selects partition elements in this fashion is called sample sort <ref> [19, 38, 64] </ref>. 5.2 Radix sort Our next sorting algorithm is radix sort, an algorithm that performs well in practice. Unlike QuickSort, radix sort is not a comparison sort, meaning that it does not compare keys directly in order to determine the relative ordering of keys. <p> In the generalized algorithm, there are b=r iterations of the for loop, each of which invokes the scan function 2 r times. When r is large, a multiprefix operation can be used for generating the ranks instead of executing a scan for each possible value <ref> [19] </ref>. In this case, and assuming the multiprefix runs in linear work, it is not hard to show that as long as b = O (log n), the total work for the radix sort is O (n), and the depth is the same order as the depth of the multiprefix.
Reference: [20] <author> Guy E. Blelloch and James J. Little. </author> <title> Parallel solutions to geometric problems in the scan model of computation. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48(1) </volume> <pages> 90-115, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: of the work before the divide step, and the other does most of the work after. 2 The depth of finding the minimum or maximum of a set of numbers can actually be improved to O (log log n) with concurrent reads [67]. 44 6.2.1 QuickHull The parallel QuickHull algorithm <ref> [20] </ref> is based on the sequential version [61], so named because of its similarity to the QuickSort algorithm. As with QuickSort, the strategy is to pick a "pivot" element, split the data based on the pivot, and recurse on each of the split sets.
Reference: [21] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <year> 1974. </year>
Reference-contexts: These translations are work-preserving in the sense that the work performed by both algorithms is the same, to within a constant factor. For example, the following theorem, known as Brent's Theorem <ref> [21] </ref>, shows that an algorithm designed for the circuit model can be translated in a work-preserving fashion to a PRAM algorithm.
Reference: [22] <author> Timothy M. Y. Chan, Jack Snoeyink, and Chee-Keng Yap. </author> <title> Output-sensitive construction of polytopes in four dimensions and clipped voronoi diagrams in three. </title> <booktitle> In Proceedings of the 6th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 282-291. ACM-SIAM, </pages> <year> 1995. </year>
Reference-contexts: Kirkpatrick and Seidel [42] have show that it is possible to modify QuickHull so that it makes provably good partitions. Although the technique is shown for a sequential algorithm, it is easy 46 to parallelize. A simplification of the technique is give by Chan et al <ref> [22] </ref>. This parallelizes even better and leads to an O (log 2 n) depth algorithm with O (n log h) work where h is the number of points on the convex hull. 6.2.2 MergeHull The MergeHull algorithm [57] is another divide-and-conquer algorithm for solving the planar convex hull problem.
Reference: [23] <author> Richard Cole. </author> <title> Parallel merge sort. </title> <journal> SIAM Journal of Computing, </journal> <volume> 17(4) </volume> <pages> 770-785, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Using this merge algorithm, the recurrence for the depth of mergesort becomes D (n) = D (n=2) + O (log n); which has solution D (n) = O (log 2 n). Using a technique called pipelined divide-and-conquer the depth of mergesort can be further reduced to O (log n) <ref> [23] </ref>. The idea is to start the merge at the top level before the recursive calls complete. Divide-and-conquer has proven to be one of the most powerful techniques for solving problems in parallel.
Reference: [24] <author> S. A. Cook. </author> <title> Towards a complexity theory of synchronous parallel computation. </title> <journal> Enseign. Math., </journal> <volume> 27 </volume> <pages> 99-124, </pages> <year> 1981. </year>
Reference-contexts: A problem is said to belong to the class NC (Nick's Class) if it can be solved in depth polylogarithmic in the size of the problem using work that is polynomial in the size of the problem <ref> [24, 59] </ref>. The class N C in parallel complexity theory plays the role of P in sequential complexity, i.e., the problems in N C are thought to be tractable in parallel. Examples of problems in N C include sorting, finding minimum-cost spanning trees, and finding convex hulls.
Reference: [25] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proceedings 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In this case, the bandwidth can be expressed as the minimum gap g between successive injections of messages into the network. Three models that characterize a network in terms of its latency and bandwidth are the Postal model [11], the Bulk-Synchronous Parallel (BSP) model [74], and the LogP model <ref> [25] </ref>. In the Postal model, a network is described by a single parameter L, its latency. The Bulk-Synchronous Parallel model adds a second parameter g, the minimum ratio of computation steps to communication steps, i.e., the gap. <p> The apply-to-each construct is used to apply an expression over a sequence of values in parallel. It uses a set like notation. For example, the expression fa fl a : a 2 [3; 4; 9; 5]g squares each element of the sequence [3; 4; 9; 5] returning the sequence <ref> [9; 16; 81; 25] </ref>. This can be read: "in parallel, for each a in the sequence [3; 4; 9; 5], square a". The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> For example fa fl a : a 2 [3; 4; 9; 5] j a &gt; 0g can be read: "in parallel, for each a in the sequence [3; 4; 9; 5] such that a is greater than 0, square a". It returns the sequence <ref> [9; 25] </ref>. The elements that remain maintain their relative order. The parallel-do construct is used to evaluate multiple statements in parallel. It is expressed by listing the set of statements after an in parallel do.
Reference: [26] <author> R. Cypher and J. L. C. Sanz. </author> <title> The SIMD Model of Parallel Computation. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Given a context-free grammar, deter mine if it can generate the empty string. 9 Concluding remarks In a chapter of this length, it is not possible to provide comphrensive coverage of the subject of parallel algorithms. Fortunately, there are number of excellent textbooks and surveys on parallel algorithms including <ref> [14, 26, 27, 30, 39, 41, 46, 47, 65] </ref>. In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages.
Reference: [27] <author> D. Eppstein and Z. Galil. </author> <title> Parallel algorithmic techniques for combinatorial computation. </title> <booktitle> Annual Review of Computer Science, </booktitle> <volume> 3 </volume> <pages> 233-83, </pages> <year> 1988. </year>
Reference-contexts: Given a context-free grammar, deter mine if it can generate the empty string. 9 Concluding remarks In a chapter of this length, it is not possible to provide comphrensive coverage of the subject of parallel algorithms. Fortunately, there are number of excellent textbooks and surveys on parallel algorithms including <ref> [14, 26, 27, 30, 39, 41, 46, 47, 65] </ref>. In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages.
Reference: [28] <author> Steven Fortune and James Wyllie. </author> <title> Parallelism in random access machines. </title> <booktitle> In Proceedings of the 10th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: These processors are attached to a common communication network. A modular memory machine consists of m memory modules and n processors all attached to a common network. A PRAM consists of a set of n processors all connected to a common shared memory <ref> [28, 32, 66] </ref>. The three types of multiprocessors differ in the way memory can be accessed. In a local memory machine, each processor can access its own local memory directly, but can access the memory in another processor only by sending a memory request through the network.
Reference: [29] <author> Hillel Gazit. </author> <title> An optimal randomized parallel algorithm for finding connected components in a graph. </title> <journal> SIAM Journal on Computing, </journal> <volume> 20(6) </volume> <pages> 1046-67, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: If it does not then it can hook to any neighbor, even if it has a larger index. This is called an unconditional hook. The randomized algorithm can be improved to run in optimal work O (n + m) <ref> [29] </ref>. The basic idea is to not use all the edges for hooking on each step, and instead use a sample of the edges.
Reference: [30] <author> Alan Gibbons and Wojciech Rytter. </author> <title> Efficient Parallel Algorithms. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1990. </year>
Reference-contexts: In particular, randomization and graph contraction will play an important role in the algorithms. In this chapter we will limit ourselves to algorithms on sparse undirected graphs. We suggest the following sources for further information on parallel graph algorithms [65, Chapters 2-8], [39, Chapter 5], <ref> [30, Chapter 2] </ref>. 4.1 Graphs and their Representation A graph G = (V; E) consists of a set of vertices V and a set of edges E in which each edge connects two vertices. <p> Full definitions of these problems and proofs that they are P-complete can be found in textbooks and surveys such as <ref> [30, 39, 41] </ref>. 1. Lexicographically-first maximal independent set and clique. Given a graph G with vertices V = 1; 2; : : :; n, and a subset S V , determine if S is the lexicographically-first maximal independent set (or maximal clique) of G. 2. Ordered depth-first search. <p> Given a context-free grammar, deter mine if it can generate the empty string. 9 Concluding remarks In a chapter of this length, it is not possible to provide comphrensive coverage of the subject of parallel algorithms. Fortunately, there are number of excellent textbooks and surveys on parallel algorithms including <ref> [14, 26, 27, 30, 39, 41, 46, 47, 65] </ref>. In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages.
Reference: [31] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> The QRQW PRAM: Accounting for contention in parallel algorithms. </title> <booktitle> In Proceedings of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 638-648, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: A final choice is to allow for queued access in which case concurrent access is permitted but the time for a step is proportional to the maximum number of accesses to any resource. A queue-read queue-write (QRQW) PRAM allows for such accesses <ref> [31] </ref>. In addition to reads and writes to non-local memory or other processors, there are other important primitives that a machine may supply. One class of such primitives support synchronization. There are a variety of different types of synchronization operations and their costs vary from model to model.
Reference: [32] <author> L. M. Goldshlager. </author> <title> A unified approach to models of synchronous parallel machines. </title> <booktitle> In Proceedings of the 10th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 89-94, </pages> <year> 1978. </year>
Reference-contexts: These processors are attached to a common communication network. A modular memory machine consists of m memory modules and n processors all attached to a common network. A PRAM consists of a set of n processors all connected to a common shared memory <ref> [28, 32, 66] </ref>. The three types of multiprocessors differ in the way memory can be accessed. In a local memory machine, each processor can access its own local memory directly, but can access the memory in another processor only by sending a memory request through the network.
Reference: [33] <author> Michael T. Goodrich. </author> <title> Parallel algorithms in geometry. In CRC Handbook of Discrete and Computational Geometry. </title> <publisher> CRC Press, </publisher> <year> 1996. </year> <note> To appear. 56 </note>
Reference-contexts: For the convex hull we describe two algorithms. These algorithms are good examples of how sequential algorithms can be parallelized in a straightforward manner. We suggest the following sources for further information on parallel algorithms for computational geometry [65, Chapters 9 and 11], [39, Chapter 6], and <ref> [33] </ref>. 6.1 Closest Pair The closest-pair problem takes a set of points in k dimensions and returns the two points that are closest to each other. The distance is usually defined as Euclidean distance.
Reference: [34] <author> John Greiner. </author> <title> A comparison of data-parallel algorithms for connected components. </title> <booktitle> In Pro--ceedings Sixth Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 16-25, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: In Section 4.3.3 we will discuss how this can be improved to lead to a work-efficient algorithm. 4.3.2 Deterministic Graph Contraction Our second algorithm for graph contraction is deterministic <ref> [34] </ref>. It is based on forming trees as subgraphs and contracting these trees into a single vertex using pointer jumping. To understand the algorithm consider the graph in Figure 8 (a). The overall goal is to contract all the vertices of the graph into a single vertex.
Reference: [35] <author> Shay Halperin and Uri Zwick. </author> <title> An optimal randomized logarithmic time connectivity algorithm for the EREW PRAM. </title> <booktitle> In Proc. ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1-10, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: This basic technique developed for parallel algorithms has since been used to improve some sequential algorithms, such as deriving the first linear work algorithm for minimum spanning trees [43]. Another improvement is to use the EREW model instead of requiring concurrent reads and writes <ref> [35] </ref>. However this comes at the cost of greatly complicating the algorithm.
Reference: [36] <author> Tim J. Harris. </author> <title> A survey of pram simulation techniques. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(2) </volume> <pages> 187-206, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: If the bandwidth of the host is smaller than the bandwidth of a comparably sized guest, however, it is usually much more difficult for the host to perform a work-preserving emulation of the guest. For more information on PRAM emulations, the reader is referred to <ref> [36, 73] </ref> 1.5 Model used in this chapter Because there are so many work-preserving translations between different parallel models of computation, we have the luxury of choosing the model that we feel most clearly illustrates the basic ideas behind the algorithms, a work-depth language model. <p> In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages. More information on these subjects can be found in <ref> [36, 73] </ref>, [4, 37], and [17, 58], respectively. Other topics likely to interest the reader of this chapter include distributed algorithms [51] and VLSI layout theory and computation [49, 72]. 10 Defining Terms CRCW.
Reference: [37] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <note> second edition, </note> <year> 1996. </year>
Reference-contexts: In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages. More information on these subjects can be found in [36, 73], <ref> [4, 37] </ref>, and [17, 58], respectively. Other topics likely to interest the reader of this chapter include distributed algorithms [51] and VLSI layout theory and computation [49, 72]. 10 Defining Terms CRCW.
Reference: [38] <author> J. S. Huang and Y. C. Chow. </author> <title> Parallel sorting and data partitioning by sampling. </title> <booktitle> In Proceedings of the IEEE Computer Society's Seventh International Computer Software and Applications Conference, </booktitle> <pages> pages 627-631, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: Hence there is an optimum value of s, typically larger than one, which minimizes the total time. The sorting algorithm that selects partition elements in this fashion is called sample sort <ref> [19, 38, 64] </ref>. 5.2 Radix sort Our next sorting algorithm is radix sort, an algorithm that performs well in practice. Unlike QuickSort, radix sort is not a comparison sort, meaning that it does not compare keys directly in order to determine the relative ordering of keys.
Reference: [39] <author> Joseph JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Mass., </address> <year> 1992. </year>
Reference-contexts: In particular, randomization and graph contraction will play an important role in the algorithms. In this chapter we will limit ourselves to algorithms on sparse undirected graphs. We suggest the following sources for further information on parallel graph algorithms [65, Chapters 2-8], <ref> [39, Chapter 5] </ref>, [30, Chapter 2]. 4.1 Graphs and their Representation A graph G = (V; E) consists of a set of vertices V and a set of edges E in which each edge connects two vertices. <p> In this section we limit our discussion to two parallel sorting algorithms, QuickSort and radix sort. Both of these algorithms are easy to program, and both work well in practice. Many more sorting algorithms can be found in the literature. The interested reader is referred to <ref> [3, 39, 47] </ref> for more complete coverage. 5.1 QuickSort We begin our discussion of sorting with a parallel version of QuickSort. <p> For the convex hull we describe two algorithms. These algorithms are good examples of how sequential algorithms can be parallelized in a straightforward manner. We suggest the following sources for further information on parallel algorithms for computational geometry [65, Chapters 9 and 11], <ref> [39, Chapter 6] </ref>, and [33]. 6.1 Closest Pair The closest-pair problem takes a set of points in k dimensions and returns the two points that are closest to each other. The distance is usually defined as Euclidean distance. <p> Here we briefly discuss some of the problems and results. We suggest the following sources for further 49 information on parallel numerical algorithms [65, Chapters 12-14], <ref> [39, Chapter 8] </ref>, [46, Chapters 5, 10 and 11] and [14]. 7.1 Matrix Operations Matrix operations form the core of many numerical algorithms and led to some of the earliest work on parallel algorithms. <p> Full definitions of these problems and proofs that they are P-complete can be found in textbooks and surveys such as <ref> [30, 39, 41] </ref>. 1. Lexicographically-first maximal independent set and clique. Given a graph G with vertices V = 1; 2; : : :; n, and a subset S V , determine if S is the lexicographically-first maximal independent set (or maximal clique) of G. 2. Ordered depth-first search. <p> Given a context-free grammar, deter mine if it can generate the empty string. 9 Concluding remarks In a chapter of this length, it is not possible to provide comphrensive coverage of the subject of parallel algorithms. Fortunately, there are number of excellent textbooks and surveys on parallel algorithms including <ref> [14, 26, 27, 30, 39, 41, 46, 47, 65] </ref>. In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages.
Reference: [40] <author> A. R. Karlin and E. Upfal. </author> <title> Parallel hashing: an efficient implementation of shared memory. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35 </volume> <pages> 876-892, </pages> <year> 1988. </year>
Reference-contexts: We assume that, in constant time, a processor can hash a virtual memory address to a physical memory bank and an address within that bank using a sufficiently powerful hash function. This scheme was first proposed by Karlin and Upfal <ref> [40] </ref> for the EREW PRAM model. Ranade [62] later presented a more general approach that allowed the butterfly to emulate a CRCW algorithms. <p> Finding small graph separators is useful for partitioning data among processors to reduce communication [65, Chapter 14]. Hashing is useful for load balancing and mapping addresses to memory <ref> [75, 40] </ref>. Iterative techniques are useful as a replacement for direct methods for solving linear systems [14]. 3 Basic operations on sequences, lists, and trees We begin our presentation of parallel algorithms with a collection of algorithms for performing basic operations on sequences, lists, and trees.
Reference: [41] <author> Richard M. Karp and Vijaya Ramachandran. </author> <title> Parallel algorithms for shared memory machines. </title> <editor> In J. Van Leeuwen, editor, </editor> <title> Handbook of Theoretical Computer Science|Volume A: Algorithms and Complexity. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: The final sum is returned at the bottom. Circuit models have been used for many years to study various theoretical aspects of parallelism, for example to prove that certain problems are hard to solve in parallel (see <ref> [41] </ref> for an overview). In a vector model an algorithm is expressed as a sequence of steps, each of which performs an operation on a vector (i.e., sequence) of input values, and produces a vector result [60, 15]. <p> Full definitions of these problems and proofs that they are P-complete can be found in textbooks and surveys such as <ref> [30, 39, 41] </ref>. 1. Lexicographically-first maximal independent set and clique. Given a graph G with vertices V = 1; 2; : : :; n, and a subset S V , determine if S is the lexicographically-first maximal independent set (or maximal clique) of G. 2. Ordered depth-first search. <p> Given a context-free grammar, deter mine if it can generate the empty string. 9 Concluding remarks In a chapter of this length, it is not possible to provide comphrensive coverage of the subject of parallel algorithms. Fortunately, there are number of excellent textbooks and surveys on parallel algorithms including <ref> [14, 26, 27, 30, 39, 41, 46, 47, 65] </ref>. In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages.
Reference: [42] <author> David G. Kirkpatrick and Raimund Seidel. </author> <title> The ultimate planar convex hull algorithm? SIAM J. </title> <journal> Comput., </journal> <volume> 15 </volume> <pages> 287-299, </pages> <year> 1986. </year>
Reference-contexts: There is one point that appears half way between x min and x max on the sphere and this point becomes the new x max . The remaining points are defined recursively. That is, the points become arbitrarily close to x min (see Figure 13). Kirkpatrick and Seidel <ref> [42] </ref> have show that it is possible to modify QuickHull so that it makes provably good partitions. Although the technique is shown for a sequential algorithm, it is easy 46 to parallelize. A simplification of the technique is give by Chan et al [22].
Reference: [43] <author> Philip N. Klein and Robert E. Tarjan. </author> <title> A randomized linear-time algorithm for finding minimum spanning trees. </title> <booktitle> In Proc. ACM Symposium on Theory of Computing, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: This basic technique developed for parallel algorithms has since been used to improve some sequential algorithms, such as deriving the first linear work algorithm for minimum spanning trees <ref> [43] </ref>. Another improvement is to use the EREW model instead of requiring concurrent reads and writes [35]. However this comes at the cost of greatly complicating the algorithm.
Reference: [44] <author> Donald E. Knuth. </author> <title> Sorting and Searching, </title> <booktitle> volume 3 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: In this case, the work and depth are given by the recurrences W (n) = 2W (n=2) + O (n) whose solutions are W (n) = O (n log n) and D (n) = O (log n). A more sophisticated analysis <ref> [44] </ref> shows that the expected work and depth are indeed W (n) = O (n log n) and D (n) = O (log n), independent of the values in the input sequence A. 39 In practice, the performance of parallel QuickSort can be improved by selecting more than one partition element.
Reference: [45] <author> Peter M. Kogge and Harold S. Stone. </author> <title> A parallel algorithm for the efficient solution of a general class of recurrence equations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22(8):786-793, </volume> <month> August </month> <year> 1973. </year> <month> 57 </month>
Reference-contexts: In fact the algorithm described can be used more generally to solve various recurrences, such as the first-order linear recurrences x i = (x i1 a i ) b i , 0 i n, where and are both associative <ref> [45] </ref>. Scans have proven so useful in the implementation of parallel algorithms that some parallel machines provide support for scan operations in hardware. 3.3 Multiprefix and fetch-and-add The multiprefix operation is a generalization of the scan operation in which multiple independent scans are performed.
Reference: [46] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin Cummings, </publisher> <year> 1994. </year>
Reference-contexts: Here we briefly discuss some of the problems and results. We suggest the following sources for further 49 information on parallel numerical algorithms [65, Chapters 12-14], [39, Chapter 8], <ref> [46, Chapters 5, 10 and 11] </ref> and [14]. 7.1 Matrix Operations Matrix operations form the core of many numerical algorithms and led to some of the earliest work on parallel algorithms. <p> Given a context-free grammar, deter mine if it can generate the empty string. 9 Concluding remarks In a chapter of this length, it is not possible to provide comphrensive coverage of the subject of parallel algorithms. Fortunately, there are number of excellent textbooks and surveys on parallel algorithms including <ref> [14, 26, 27, 30, 39, 41, 46, 47, 65] </ref>. In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages.
Reference: [47] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, and Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: Many algorithms have been designed to run efficiently on particular network topologies such as the mesh or the hypercube. For an extensive treatment such algorithms, see <ref> [47] </ref>. Although this approach can lead to very fine-tuned algorithms, it has some disadvantages. First, algorithms designed for one network may not perform well on other networks. Hence, in order to solve a problem on a new machine, it may be necessary to design a new algorithm from scratch. <p> In this section we limit our discussion to two parallel sorting algorithms, QuickSort and radix sort. Both of these algorithms are easy to program, and both work well in practice. Many more sorting algorithms can be found in the literature. The interested reader is referred to <ref> [3, 39, 47] </ref> for more complete coverage. 5.1 QuickSort We begin our discussion of sorting with a parallel version of QuickSort. <p> In fact the butterfly network topology is sometimes called the FFT network since the FFT has the same communication pattern as the network <ref> [47, Section 3.7] </ref>. <p> Given a context-free grammar, deter mine if it can generate the empty string. 9 Concluding remarks In a chapter of this length, it is not possible to provide comphrensive coverage of the subject of parallel algorithms. Fortunately, there are number of excellent textbooks and surveys on parallel algorithms including <ref> [14, 26, 27, 30, 39, 41, 46, 47, 65] </ref>. In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages.
Reference: [48] <author> Charles E. Leiserson. Fat-Trees: </author> <title> Universal networks for hardware-efficient supercomputing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-34(10):892-901, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: A fat-tree is a network whose overall structure is that of a tree <ref> [48] </ref>. Each edge of the tree, however, may represent many communication channels, and each node may represent many network switches (hence the name "fat"). Figure 2 (e) shows a fat-tree whose overall structure is that of a binary tree.
Reference: [49] <author> Th. Lengauer. </author> <title> Vlsi theory. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <pages> pages 837-868. </pages> <publisher> Elsevier Science Publishers, B. V., </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: More information on these subjects can be found in [36, 73], [4, 37], and [17, 58], respectively. Other topics likely to interest the reader of this chapter include distributed algorithms [51] and VLSI layout theory and computation <ref> [49, 72] </ref>. 10 Defining Terms CRCW. This refers to a shared memory model that allows for Concurrent reads (CR) and con current writes (CW) to the memory. 52 CREW. This refers to a shared memory model that allows for Concurrent reads (CR) but only exclusive writes (EW) to the memory.
Reference: [50] <author> Michael Luby. </author> <title> A simple parallel algorithm for the maximal independent set problem. </title> <booktitle> In Proceedings ACM Symposium on Theory of Computing, </booktitle> <pages> pages 1-10, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: As it turns out, the impasse can be resolved by using randomness to break the symmetry between the vertices <ref> [50] </ref>. Load balancing: A third use is load balancing. One way to quickly partition a large number of data items into a collection of approximately evenly sized subsets is to randomly assign each element to a subset.
Reference: [51] <author> Nancy A. Lynch. </author> <title> Distributed Algorithms. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: More information on these subjects can be found in [36, 73], [4, 37], and [17, 58], respectively. Other topics likely to interest the reader of this chapter include distributed algorithms <ref> [51] </ref> and VLSI layout theory and computation [49, 72]. 10 Defining Terms CRCW. This refers to a shared memory model that allows for Concurrent reads (CR) and con current writes (CW) to the memory. 52 CREW.
Reference: [52] <author> Y. Maon, B. Schieber, and U. Vishkin. </author> <title> Parallel ear decomposition search (eds) and st-numbering in graphs. </title> <journal> Theoretical Computer Science, </journal> <volume> 47 </volume> <pages> 277-298, </pages> <year> 1986. </year>
Reference-contexts: The end-points of each ear are anchored on previous paths. Once an ear decomposition of a graph is found, it is not difficult to determine if two edges lie on a common cycle. This information can be used in algorithms for determining biconnectivity, triconnectivity, 4-connectivity, and planarity <ref> [52, 55] </ref>. An ear decomposition can be found in parallel using linear work and logarithmic depth, independent 19 of the structure of the graph.
Reference: [53] <author> Yossi Matias and Uzi Vishkin. </author> <title> On parallel hashing and integer sorting. </title> <journal> Journal of Algorithms, </journal> <volume> 12(4) </volume> <pages> 573-606, </pages> <year> 1991. </year>
Reference-contexts: In this chapter we omit the implementation of the multiprefix operation, but it can be solved by a function that requires work O (n) and depth O (log n) using concurrent writes <ref> [53] </ref>. 3.4 Pointer jumping Pointer jumping is a technique that can be applied to both linked lists and trees [76]. The basic pointer jumping operation is simple. Each node i replaces its pointer P [i] with the pointer of the node that it points to, P [P [i]].
Reference: [54] <author> G. Miller and J. Reif. </author> <title> Parallel tree contraction part 1: Fundamentals. </title> <editor> In Silvio Micali, editor, </editor> <booktitle> Randomness and Computation. Volume 5 of Advances in Computing Research, </booktitle> <pages> pages 47-72. </pages> <publisher> JAI Press, </publisher> <address> Greenwich, CT, </address> <year> 1989. </year>
Reference-contexts: Many problems can be solved by contracting trees <ref> [54, 56] </ref>, in which case the technique is called it tree contraction. More examples of graph contraction can be found in Section 4. Ear decomposition. An ear decomposition of a graph is a partition of its edges into an ordered collection of paths. <p> Using a technique called contraction, it is possible to design a list ranking algorithm that runs in O (n) work and O (log n) depth [5, 6]. This technique can also be applied to trees <ref> [54, 56] </ref>. 3.6 Removing Duplicates Given a sequence of items, the remove-duplicates algorithm removes all duplicates, returning the resulting sequence.
Reference: [55] <author> G. L. Miller and V. Ramachandran. </author> <title> A new graph triconnectivity algorithm and its paralleliza-tion. </title> <journal> Combinatorica, </journal> <volume> 12(1) </volume> <pages> 53-76, </pages> <year> 1992. </year>
Reference-contexts: The end-points of each ear are anchored on previous paths. Once an ear decomposition of a graph is found, it is not difficult to determine if two edges lie on a common cycle. This information can be used in algorithms for determining biconnectivity, triconnectivity, 4-connectivity, and planarity <ref> [52, 55] </ref>. An ear decomposition can be found in parallel using linear work and logarithmic depth, independent 19 of the structure of the graph.
Reference: [56] <author> G. L. Miller and J. H. Reif. </author> <title> Parallel tree contraction part 2: Further applications. </title> <journal> SIAM Journal of Computing, </journal> <volume> 20(6) </volume> <pages> 1128-1147, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Many problems can be solved by contracting trees <ref> [54, 56] </ref>, in which case the technique is called it tree contraction. More examples of graph contraction can be found in Section 4. Ear decomposition. An ear decomposition of a graph is a partition of its edges into an ordered collection of paths. <p> Using a technique called contraction, it is possible to design a list ranking algorithm that runs in O (n) work and O (log n) depth [5, 6]. This technique can also be applied to trees <ref> [54, 56] </ref>. 3.6 Removing Duplicates Given a sequence of items, the remove-duplicates algorithm removes all duplicates, returning the resulting sequence.
Reference: [57] <author> Mark H. Overmars and Jan Van Leeuwen. </author> <title> Maintenance of configurations in the plane. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 23 </volume> <pages> 166-204, </pages> <year> 1981. </year>
Reference-contexts: A simplification of the technique is give by Chan et al [22]. This parallelizes even better and leads to an O (log 2 n) depth algorithm with O (n log h) work where h is the number of points on the convex hull. 6.2.2 MergeHull The MergeHull algorithm <ref> [57] </ref> is another divide-and-conquer algorithm for solving the planar convex hull problem. Unlike QuickHull, however, it does most of its work after returning from the recursive calls.
Reference: [58] <editor> David Padua, David Gelernter, and Alexandru Nicolau, editors. </editor> <booktitle> Languages and Compilers for Parallel Computing. Research Monographs in Parallel and Distributed. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year> <month> 58 </month>
Reference-contexts: In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages. More information on these subjects can be found in [36, 73], [4, 37], and <ref> [17, 58] </ref>, respectively. Other topics likely to interest the reader of this chapter include distributed algorithms [51] and VLSI layout theory and computation [49, 72]. 10 Defining Terms CRCW.
Reference: [59] <author> N. Pippenger. </author> <title> On simultaneous resource bounds. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 307-311, </pages> <year> 1979. </year>
Reference-contexts: A problem is said to belong to the class NC (Nick's Class) if it can be solved in depth polylogarithmic in the size of the problem using work that is polynomial in the size of the problem <ref> [24, 59] </ref>. The class N C in parallel complexity theory plays the role of P in sequential complexity, i.e., the problems in N C are thought to be tractable in parallel. Examples of problems in N C include sorting, finding minimum-cost spanning trees, and finding convex hulls.
Reference: [60] <author> Vaughan R. Pratt and Larry J. Stockmeyer. </author> <title> A characterization of the power of vector machines. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 12 </volume> <pages> 198-221, </pages> <year> 1976. </year>
Reference-contexts: In a vector model an algorithm is expressed as a sequence of steps, each of which performs an operation on a vector (i.e., sequence) of input values, and produces a vector result <ref> [60, 15] </ref>. The work of each step is equal to the length of its input (or output) vector. The work of an algorithm is the sum of the work of its steps. The depth of an algorithm is the number of vector steps.
Reference: [61] <author> Franco P. Preparata and Michael I. Shamos. </author> <title> Computational Geometry|An Introduction. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: and the other does most of the work after. 2 The depth of finding the minimum or maximum of a set of numbers can actually be improved to O (log log n) with concurrent reads [67]. 44 6.2.1 QuickHull The parallel QuickHull algorithm [20] is based on the sequential version <ref> [61] </ref>, so named because of its similarity to the QuickSort algorithm. As with QuickSort, the strategy is to pick a "pivot" element, split the data based on the pivot, and recurse on each of the split sets.
Reference: [62] <author> A. G. Ranade. </author> <title> How to emulate shared memory. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 42(3) </volume> <pages> 307-326, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: We assume that, in constant time, a processor can hash a virtual memory address to a physical memory bank and an address within that bank using a sufficiently powerful hash function. This scheme was first proposed by Karlin and Upfal [40] for the EREW PRAM model. Ranade <ref> [62] </ref> later presented a more general approach that allowed the butterfly to emulate a CRCW algorithms.
Reference: [63] <author> Margaret Reid-Miller. </author> <title> List ranking and list scan on the Cray C-90. </title> <booktitle> In Proceedings Sixth Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 104-113, </pages> <address> Cape May, NJ, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: There are, however, a variety of work-efficient parallel solutions to this problem. The following parallel algorithm uses the technique of random sampling to solve to construct a pointer from each node to the end of a list of n nodes in a work-efficient fashion <ref> [63] </ref>. The algorithm is easily generalized to solve the list-ranking problem. 1. Pick m list nodes at random and call them the start nodes. 2. From each start node u, follow the list until reaching the next start node v.
Reference: [64] <author> J. H. Reif and L. G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <booktitle> In Proceedings of the 15th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 10-16, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: Hence there is an optimum value of s, typically larger than one, which minimizes the total time. The sorting algorithm that selects partition elements in this fashion is called sample sort <ref> [19, 38, 64] </ref>. 5.2 Radix sort Our next sorting algorithm is radix sort, an algorithm that performs well in practice. Unlike QuickSort, radix sort is not a comparison sort, meaning that it does not compare keys directly in order to determine the relative ordering of keys.
Reference: [65] <editor> John H. Reif, editor. </editor> <title> Synthesis of Parallel Algorithms. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Cali-fornia, </address> <year> 1993. </year>
Reference-contexts: Hence, this technique can be used to replace the standard sequential technique for solving these problems, depth-first search. 2.4 Other techniques Many other techniques have proven to be useful in the design of parallel algorithms. Finding small graph separators is useful for partitioning data among processors to reduce communication <ref> [65, Chapter 14] </ref>. Hashing is useful for load balancing and mapping addresses to memory [75, 40]. <p> These algorithms use some of the general techniques. In particular, randomization and graph contraction will play an important role in the algorithms. In this chapter we will limit ourselves to algorithms on sparse undirected graphs. We suggest the following sources for further information on parallel graph algorithms <ref> [65, Chapters 2-8] </ref>, [39, Chapter 5], [30, Chapter 2]. 4.1 Graphs and their Representation A graph G = (V; E) consists of a set of vertices V and a set of edges E in which each edge connects two vertices. <p> For the convex hull we describe two algorithms. These algorithms are good examples of how sequential algorithms can be parallelized in a straightforward manner. We suggest the following sources for further information on parallel algorithms for computational geometry <ref> [65, Chapters 9 and 11] </ref>, [39, Chapter 6], and [33]. 6.1 Closest Pair The closest-pair problem takes a set of points in k dimensions and returns the two points that are closest to each other. The distance is usually defined as Euclidean distance. <p> Here we briefly discuss some of the problems and results. We suggest the following sources for further 49 information on parallel numerical algorithms <ref> [65, Chapters 12-14] </ref>, [39, Chapter 8], [46, Chapters 5, 10 and 11] and [14]. 7.1 Matrix Operations Matrix operations form the core of many numerical algorithms and led to some of the earliest work on parallel algorithms. <p> Given a context-free grammar, deter mine if it can generate the empty string. 9 Concluding remarks In a chapter of this length, it is not possible to provide comphrensive coverage of the subject of parallel algorithms. Fortunately, there are number of excellent textbooks and surveys on parallel algorithms including <ref> [14, 26, 27, 30, 39, 41, 46, 47, 65] </ref>. In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages.
Reference: [66] <author> W. J. Savitch and M. Stimson. </author> <title> Time bounded random access machines with parallel processing. </title> <journal> Journal of the Association for Computing Machinery, </journal> <pages> pages 103-118, </pages> <year> 1979. </year>
Reference-contexts: These processors are attached to a common communication network. A modular memory machine consists of m memory modules and n processors all attached to a common network. A PRAM consists of a set of n processors all connected to a common shared memory <ref> [28, 32, 66] </ref>. The three types of multiprocessors differ in the way memory can be accessed. In a local memory machine, each processor can access its own local memory directly, but can access the memory in another processor only by sending a memory request through the network.
Reference: [67] <author> Yossi Shiloach and Uzi Vishkin. </author> <title> Finding the maximum, merging and sorting in a parallel computation model. </title> <journal> Journal of Algorithms, </journal> <volume> 2(1) </volume> <pages> 88-102, </pages> <year> 1981. </year>
Reference-contexts: The problem here is that the merge step remains sequential, and is the bottleneck. As mentioned earlier, the parallelism in a divide-and-conquer algorithm can often be enhanced by parallelizing the divide step and/or the merge step. Using a parallel merge <ref> [67] </ref> two sorted sequences of n=2 keys can be merged with work O (n) and depth O (log n). Using this merge algorithm, the recurrence for the depth of mergesort becomes D (n) = D (n=2) + O (log n); which has solution D (n) = O (log 2 n). <p> both based on divide-and-conquer, but one does most of the work before the divide step, and the other does most of the work after. 2 The depth of finding the minimum or maximum of a set of numbers can actually be improved to O (log log n) with concurrent reads <ref> [67] </ref>. 44 6.2.1 QuickHull The parallel QuickHull algorithm [20] is based on the sequential version [61], so named because of its similarity to the QuickSort algorithm.
Reference: [68] <author> Yossi Shiloach and Uzi Vishkin. </author> <title> An O(log n) parallel connectivity algorithm. </title> <journal> Journal of Algorithms, </journal> <volume> 3 </volume> <pages> 57-67, </pages> <year> 1982. </year>
Reference-contexts: Here we mention some of them. The deterministic algorithm can be improved to run in O (log n) depth with the same work bounds <ref> [10, 68] </ref>. The basic idea is to interleave the hooking steps with the shortcutting steps. The one tricky aspect is that we must always hook in the same direction (i.e., from smaller to larger), so as not to create cycles.
Reference: [69] <author> Harold S. Stone. </author> <title> Parallel tridiagonal equation solvers. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 1(4) </volume> <pages> 289-307, </pages> <month> December </month> <year> 1975. </year>
Reference-contexts: For example, executing a plus-scan on the sequence [3; 5; 3; 1; 6] returns [0; 3; 8; 11; 12]. The scan operation can be implemented by the following algorithm <ref> [69] </ref>. function scan (A) 1 if jAj = 1 then return [0] 2 else 3 S = scan (fA [2i] + A [2i + 1] : i 2 [0::jAj=2)g) 4 R = fif (i mod 2) = 0 then S [i=2] else S [(i 1)=2] + A [i 1] : i
Reference: [70] <author> Volker Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numerische Mathematik, </journal> <volume> 14(3) </volume> <pages> 354-356, </pages> <year> 1969. </year>
Reference-contexts: Sequentially it is know that matrix multiplication can be done in better than O (n 3 ) work. For example, Strassen's algorithm <ref> [70] </ref> requires only O (n 2:81 ) work. Most of these more efficient algorithms are also easy to parallelize because of their recursive nature (Strassen's algorithm has O (log n) depth using a trivial parallelization). Another basic matrix operation is to invert matrices.
Reference: [71] <author> R. E. Tarjan and U. Vishkin. </author> <title> An efficient parallel biconnectivity algorithm. </title> <journal> SIAM Journal of Computing, </journal> <volume> 14(4) </volume> <pages> 862-874, </pages> <year> 1985. </year>
Reference-contexts: By keeping a linked structure that represents the Euler tour of a tree it is possible to compute many functions on the tree, such as the size of each subtree <ref> [71] </ref>. This technique uses linear work, and parallel depth that is independent of the depth of the tree. The Euler tour can often be used to replace standard traversals of a tree, such as a depth-first traversal. Graph contraction.
Reference: [72] <author> J. D. Ullman. </author> <title> Computational Aspects of VLSI. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1984. </year> <month> 59 </month>
Reference-contexts: More information on these subjects can be found in [36, 73], [4, 37], and [17, 58], respectively. Other topics likely to interest the reader of this chapter include distributed algorithms [51] and VLSI layout theory and computation <ref> [49, 72] </ref>. 10 Defining Terms CRCW. This refers to a shared memory model that allows for Concurrent reads (CR) and con current writes (CW) to the memory. 52 CREW. This refers to a shared memory model that allows for Concurrent reads (CR) but only exclusive writes (EW) to the memory.
Reference: [73] <author> L. G. Valiant. </author> <title> General purpose parallel architectures. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <pages> pages 943-971. </pages> <publisher> Elsevier Science Publishers, B. V., </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: If the bandwidth of the host is smaller than the bandwidth of a comparably sized guest, however, it is usually much more difficult for the host to perform a work-preserving emulation of the guest. For more information on PRAM emulations, the reader is referred to <ref> [36, 73] </ref> 1.5 Model used in this chapter Because there are so many work-preserving translations between different parallel models of computation, we have the luxury of choosing the model that we feel most clearly illustrates the basic ideas behind the algorithms, a work-depth language model. <p> In addition to parallel algorithms, this chapter has also touched on several related subjects, including the modeling of parallel computations, parallel computer architecture, and parallel programming languages. More information on these subjects can be found in <ref> [36, 73] </ref>, [4, 37], and [17, 58], respectively. Other topics likely to interest the reader of this chapter include distributed algorithms [51] and VLSI layout theory and computation [49, 72]. 10 Defining Terms CRCW.
Reference: [74] <author> Leslie G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In this case, the bandwidth can be expressed as the minimum gap g between successive injections of messages into the network. Three models that characterize a network in terms of its latency and bandwidth are the Postal model [11], the Bulk-Synchronous Parallel (BSP) model <ref> [74] </ref>, and the LogP model [25]. In the Postal model, a network is described by a single parameter L, its latency. The Bulk-Synchronous Parallel model adds a second parameter g, the minimum ratio of computation steps to communication steps, i.e., the gap.
Reference: [75] <author> Uzi Vishkin. </author> <title> Parallel-design distributed-implementation (PDDI) general purpose computer. </title> <journal> Theoretical Computer Science, </journal> <volume> 32 </volume> <pages> 157-172, </pages> <year> 1984. </year>
Reference-contexts: Finding small graph separators is useful for partitioning data among processors to reduce communication [65, Chapter 14]. Hashing is useful for load balancing and mapping addresses to memory <ref> [75, 40] </ref>. Iterative techniques are useful as a replacement for direct methods for solving linear systems [14]. 3 Basic operations on sequences, lists, and trees We begin our presentation of parallel algorithms with a collection of algorithms for performing basic operations on sequences, lists, and trees.
Reference: [76] <author> James C. Wyllie. </author> <title> The complexity of parallel computations. </title> <type> Technical Report TR-79-387, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <month> August </month> <year> 1979. </year> <title> 60 points M 1 and M 2 mark the middle of the remaining hulls. The dotted lines represent the part of the hull that can be eliminated from consideration. The mirror images of cases b-e are also used. In the case (e), the region to eliminate depends on which side of the separating line the intersection of the tangents appears. </title> <type> 61 </type>
Reference-contexts: Fortunately these techniques can often be replaced by parallel techniques with roughly the same power. Pointer jumping. One of the earliest parallel pointer techniques is pointer jumping <ref> [76] </ref>. This technique can be applied to either lists or trees. In each pointer jumping step, each node in parallel replaces its pointer with that of its successor (or parent). <p> this chapter we omit the implementation of the multiprefix operation, but it can be solved by a function that requires work O (n) and depth O (log n) using concurrent writes [53]. 3.4 Pointer jumping Pointer jumping is a technique that can be applied to both linked lists and trees <ref> [76] </ref>. The basic pointer jumping operation is simple. Each node i replaces its pointer P [i] with the pointer of the node that it points to, P [P [i]].
References-found: 76

