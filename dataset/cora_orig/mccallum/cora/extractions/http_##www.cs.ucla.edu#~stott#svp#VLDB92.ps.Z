URL: http://www.cs.ucla.edu/~stott/svp/VLDB92.ps.Z
Refering-URL: http://www.cs.ucla.edu/~stott/svp/
Root-URL: http://www.cs.ucla.edu
Email: Stott@cs.ucla.edu, Eric.Simon@inria.fr, Patrick.Valduriez@inria.fr  
Title: SVP a Model Capturing Sets, Streams, and Parallelism  
Author: D. Stott Parker Eric Simon and Patrick Valduriez 
Address: Los Angeles  Rocquencourt  
Affiliation: University of California,  Projet Rodin, INRIA,  
Abstract: We describe the SVP data model. The goal of SVP is to model both set and stream data, and to model parallelism in bulk data processing. SVP also shows promise for other parallel processing applications. SVP models collections, which include sets and streams as special cases. Collections are represented as ordered tree structures, and divide-and-conquer mappings are easily defined on these structures. We show that many useful database mappings (queries) have a divide-and-conquer format when specified using collections, and that this specification exposes parallelism. We formalize a class of divide-and-conquer mappings on collections called SVP-transducers. SVP-transducers generalize aggregates, set mappings, stream transductions, and scan computations. At the same time, they have a rigorous semantics based on continuity with respect to collection orderings, and permit implicit specification of both independent and pipeline parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bitton, H. Boral, D. DeWitt, W. Wilkinson, </author> <title> Parallel Algorithms for the Execution of Relational Database Operations, </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8:3, </volume> <year> 1983. </year>
Reference-contexts: A second problem is that parallel data processing requires effective data partitioning capabilities. Typically, a relational query (select-project-join expression) is translated into a low-level form of relational algebra with explicit (low-level) parallel constructs [2]. Data partitioning is used to spread the computation of relational algebra operators among parallel processors <ref> [1] </ref>. This partitioning is typically defined during the physical database design and then exploited by a compiler. Most of the time, a partitioned computation requires that processors exchange intermediate results in order to compute the final result. <p> Streams analogously use the following notation: 1. [ ] is a stream (the empty stream); 2. [x] is a stream, for any value x; 3. Finite streams are written with square braces, as with: <ref> [1; 2; 3] </ref>. 4. The concatenation S 1 * S 2 is a stream, if S 1 and S 2 are streams. (We use the symbol ` * ' for stream concatenation (`append') in this paper.) 5. <p> Here h will be a value, and T will be a stream. The constructor symbol `' (`cons') can be viewed as an operator that combines a value and a stream into a stream. The single-element stream [x] is actually a shorthand for (x [ ]), and <ref> [1; 2; 3] </ref> is a shorthand for (1 2 3 [ ]). All finite streams are terminated explicitly with [ ]. One more bit of notation will be useful. We use parentheses to set off tuples (vectors).
Reference: [2] <author> P. Borla-Salamet, C. Chachaty, B. Dageville, </author> <title> Compiling Control into Database Queries for Parallel Execution, </title> <booktitle> Proc. Int. Conf. on Parallel and Distributed Information Systems, </booktitle> <address> Miami, Florida, </address> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: Relational languages are therefore inadequate for specifying `stream processing', 2 in which ordered sequences of data are processed sequentially [13]. Pipeline parallelism is generally used, transparently to the user, in lower-level languages implementing relational algebra (e.g., PLERA <ref> [2] </ref>, or PFAD [8]). However, higher-level relational interfaces do not permit streams to be exploited, preventing specification of stream computations and also pipeline parallelism. A second problem is that parallel data processing requires effective data partitioning capabilities. <p> A second problem is that parallel data processing requires effective data partitioning capabilities. Typically, a relational query (select-project-join expression) is translated into a low-level form of relational algebra with explicit (low-level) parallel constructs <ref> [2] </ref>. Data partitioning is used to spread the computation of relational algebra operators among parallel processors [1]. This partitioning is typically defined during the physical database design and then exploited by a compiler. <p> Streams analogously use the following notation: 1. [ ] is a stream (the empty stream); 2. [x] is a stream, for any value x; 3. Finite streams are written with square braces, as with: <ref> [1; 2; 3] </ref>. 4. The concatenation S 1 * S 2 is a stream, if S 1 and S 2 are streams. (We use the symbol ` * ' for stream concatenation (`append') in this paper.) 5. <p> Here h will be a value, and T will be a stream. The constructor symbol `' (`cons') can be viewed as an operator that combines a value and a stream into a stream. The single-element stream [x] is actually a shorthand for (x [ ]), and <ref> [1; 2; 3] </ref> is a shorthand for (1 2 3 [ ]). All finite streams are terminated explicitly with [ ]. One more bit of notation will be useful. We use parentheses to set off tuples (vectors).
Reference: [3] <author> W.H. Burge, </author> <title> Recursive Programming Techniques, </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1975. </year>
Reference-contexts: Streams analogously use the following notation: 1. [ ] is a stream (the empty stream); 2. [x] is a stream, for any value x; 3. Finite streams are written with square braces, as with: <ref> [1; 2; 3] </ref>. 4. The concatenation S 1 * S 2 is a stream, if S 1 and S 2 are streams. (We use the symbol ` * ' for stream concatenation (`append') in this paper.) 5. <p> Here h will be a value, and T will be a stream. The constructor symbol `' (`cons') can be viewed as an operator that combines a value and a stream into a stream. The single-element stream [x] is actually a shorthand for (x [ ]), and <ref> [1; 2; 3] </ref> is a shorthand for (1 2 3 [ ]). All finite streams are terminated explicitly with [ ]. One more bit of notation will be useful. We use parentheses to set off tuples (vectors). <p> SVP transducer: pump (h; ; id ; hi) = id pump (h; ; id ; hxi) = h (x) pump (h; ; id ; S 1 S 2 ) = pump (h; ; id ; S 1 ) pump (h; ; id ; S 2 ): The list1 operator in <ref> [3] </ref> is similar. The APL reduction operator [10] allows non-associative, non-commutative operators.
Reference: [4] <author> N. Carriero, D. Gelertner, </author> <title> Linda in Context, </title> <journal> Communications of the ACM 32: </journal> <volume> 4, </volume> <month> April </month> <year> 1989. </year>
Reference-contexts: Although this approach can lead to high-performance, it is generally too low-level and difficult for the programmer. Furthermore, the large variety of parallel architectures result in distinct, architecture-specific extensions to the original language. 1 In order 1 Linda <ref> [4] </ref> is a notable exception of `coordination language' with simple, language-independent parallel constructs, which can mate easily with many non-parallel languages. 1 to achieve efficient program execution, the programmer must first become acquainted with the programming paradigm dictated by the architecture of the target machine.
Reference: [5] <author> C. Chase et al., </author> <title> Paragon: a Parallel Programming Environment for Scientific Applications Using Communications Structures, </title> <booktitle> Proc. Int. Conf. on Parallel Programming, </booktitle> <address> St. Charles, Illinois, </address> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: In this paper, we follow the third approach, and propose a model for parallel database programming where the primary sources for parallelism are parallel set and stream expressions. Parallel programming environments that follow this approach have recently been proposed. For example, in Paragon <ref> [5] </ref>, the primary source for parallelism is parallel array expressions. Paragon is targeted to scientific programming applications and offers the essential features of parallel Fortran languages. Our model is targeted at database applications, and bulk data processing. 1.2 Parallelism for Bulk Data Processing There are various forms of parallelism.
Reference: [6] <author> S. Danforth, P. Valduriez, </author> <title> A FAD for Data-Intensive Applications, </title> <journal> IEEE Trans. Knowledge and Data Engineering, </journal> <volume> 4: 1, </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: It seems unlikely that database systems will be able to completely automate partitioning decisions. Database models have been developed before that permit expression of both ordering among tuples and data partitioning. For example, the FAD language has operators that express various forms of fan-out and fan-in parallelism <ref> [6] </ref>. FAD is a strongly-typed set-oriented database language based on functional programming and relational algebra. It provides a fixed set of higher-order functions to aggregate functions, like the pump parametrized aggregate operator and the grouping operator. <p> In this paper these mappings are specified with recursive functional equations. They generalize other specification techniques, including restricted higher-order mappings like the reduction operator in APL [10] and the pump operator in FAD <ref> [6] </ref>, list comprehensions and elegant variants thereof [20], and series-parallel computation graphs [15]. * Parallelism in the dividing and conquering is specified using both the structure of the data, and the structure of the divide-and-conquer mapping: dividing-parallelism is specified by the data, and conquering-parallelism is specified by the mapping. <p> This can be implemented as an SVP-mapping that applies h to the x values in the input and accumulates the resulting (k; x) values into buckets [14]. 4.3 Algebraic Operators In FAD <ref> [6] </ref>, the parameterized aggregate operator pump (h,,id ,S) is defined to yield id if S = fg where is an associative, commutative binary operator, with identity id . <p> The algorithm is parallel in that all groups can be joined in parallel. * The parameterized set map operator filter (h,S 1 ; : : : ; S m ), in FAD <ref> [6] </ref> yields the value of h applied to each tuple in the cross product of the sets S 1 ; : : : ; S m (for m &gt; 0): filter (h; S 1 ; : : : ; S m ) = f h (x 1 ; : : :
Reference: [7] <author> A. Gupta, C. Forgy, A. Newell, </author> <title> High Speed Implementations of Rule-Based Systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7:2, </volume> <month> May </month> <year> 1989. </year>
Reference-contexts: For instance, experiments conducted with the OPS5 rule-based language revealed that in practice, the true speed-up achievable from parallelism was less than tenfold <ref> [7] </ref>. A related serious problem with this approach is that, in the final analysis, the serial programming paradigm does not encourage the use of parallel algorithms. The second approach enables the programmer to express parallel constructs such as task creation and inter-task synchronization, thereby providing leverage over parallelism.
Reference: [8] <author> B. Hart, S. Danforth, P. Valduriez, </author> <title> Parallelizing FAD, a Database Programming Language, </title> <booktitle> Int. Symp. on Databases in Distributed and Parallel Systems, </booktitle> <address> Austin, Texas, </address> <month> Dec. </month> <year> 1988. </year>
Reference-contexts: Relational languages are therefore inadequate for specifying `stream processing', 2 in which ordered sequences of data are processed sequentially [13]. Pipeline parallelism is generally used, transparently to the user, in lower-level languages implementing relational algebra (e.g., PLERA [2], or PFAD <ref> [8] </ref>). However, higher-level relational interfaces do not permit streams to be exploited, preventing specification of stream computations and also pipeline parallelism. A second problem is that parallel data processing requires effective data partitioning capabilities.
Reference: [9] <author> W.D. Hillis, G.L. Steele, </author> <title> Data Parallel Algorithms, </title> <journal> Communications of the ACM, </journal> <volume> 29:12, </volume> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: The regularity of the data structures available in the language permits exploitation of different forms of parallelism, such as independent and pipeline parallelism <ref> [9] </ref>. In this paper, we follow the third approach, and propose a model for parallel database programming where the primary sources for parallelism are parallel set and stream expressions. Parallel programming environments that follow this approach have recently been proposed.
Reference: [10] <author> K.E. Iverson, </author> <title> A Programming Language, </title> <publisher> NY: J. Wiley, </publisher> <year> 1962. </year>
Reference-contexts: In this paper these mappings are specified with recursive functional equations. They generalize other specification techniques, including restricted higher-order mappings like the reduction operator in APL <ref> [10] </ref> and the pump operator in FAD [6], list comprehensions and elegant variants thereof [20], and series-parallel computation graphs [15]. * Parallelism in the dividing and conquering is specified using both the structure of the data, and the structure of the divide-and-conquer mapping: dividing-parallelism is specified by the data, and conquering-parallelism <p> The APL reduction operator <ref> [10] </ref> allows non-associative, non-commutative operators.
Reference: [11] <author> G. Kahn, </author> <title> The Semantics of a Simple Language for Parallel Programming, </title> <booktitle> Proc. IFIP 74, </booktitle> <publisher> North-Holland, </publisher> <pages> 471-475, </pages> <month> August </month> <year> 1974. </year>
Reference-contexts: SVP collections allow us to model various kinds of structure important in data processing, including sort ordering and physical data partitioning. It is also possible to generalize the classic work of Kahn for continuous functions on sequences <ref> [11] </ref> to work for continuous functions on collections. The basic idea is 8 that prefix-continuous functions on sequences 3 are exactly those functions that yield pipeline parallelism. Stream-continuity gives pipeline parallelism, and set-continuity gives independent parallelism. 5. SVP is a model.
Reference: [12] <author> R.E. Ladner, M.J. Fischer, </author> <title> Parallel Prefix Computation, </title> <journal> J. ACM 27:4, </journal> <pages> 831-838, </pages> <year> 1980. </year>
Reference-contexts: For example, when is ` ', h must produce a collection. When is a complete operator with a left identity id , we call a collector. The table below gives examples of collectors. Parallel evaluation of associative collector expressions is sometimes called parallel prefix computation <ref> [12] </ref>. Other properties of collectors (such as Commutativity, Idempotency) can be exploited to obtain greater parallelism.
Reference: [13] <author> D.S. Parker, </author> <title> Stream Data Analysis in Prolog, </title> <editor> in L. Sterling, ed., </editor> <title> The Practice of Prolog, </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: A first problem is that the relational model offers no way to talk about order among data (e.g., sorted relations, or ordered tuples). Relational languages are therefore inadequate for specifying `stream processing', 2 in which ordered sequences of data are processed sequentially <ref> [13] </ref>. Pipeline parallelism is generally used, transparently to the user, in lower-level languages implementing relational algebra (e.g., PLERA [2], or PFAD [8]). However, higher-level relational interfaces do not permit streams to be exploited, preventing specification of stream computations and also pipeline parallelism.
Reference: [14] <author> D.S. Parker, E. Simon, P. Valduriez, SVP, </author> <title> a Model Capturing Sets, Streams, and Parallelism, </title> <type> Technical Report CSD-920020, </type> <institution> Computer Science Department, UCLA, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Section 4 then gives examples of SVP mappings for expressing relational algebra operators, grouping and aggregate operators. Finally, Section 5 concludes the paper and summarizes the contributions of the SVP model. A more comprehensive presentation of SVP is given in <ref> [14] </ref>. 2 Set and Stream Processing Let us clarify first what set processing and stream processing are, and then study how they might be integrated in a parallel processing model. 3 2.1 Sets and Streams For the purposes of this paper, we will rely on similar formulations of sets and streams. <p> This can be implemented as an SVP-mapping that applies h to the x values in the input and accumulates the resulting (k; x) values into buckets <ref> [14] </ref>. 4.3 Algebraic Operators In FAD [6], the parameterized aggregate operator pump (h,,id ,S) is defined to yield id if S = fg where is an associative, commutative binary operator, with identity id . <p> SVP-transducer, assuming the input is in left-linear form: APLreduction (; hi) = hi APLreduction (; hxi) = x APLreduction (; S 1 S 2 ) = APLreduction (; S 1 ) APLreduction (; S 2 ): Furthermore, any collection can be restructured to left linear form with a simple SVP-mapping <ref> [14] </ref>. 4.4 Joins Surprisingly, important n-ary operations like joins can be implemented with transducers! In fact, interesting join algorithms can be developed. Let us define a general join algorithm. <p> For example, we can implement merge scans on streams (linear collections) with SVP-transducers. Merging of two streams is accomplished by making one of the streams the initial state, and incrementally consuming this state while simultaneously consuming the other stream <ref> [14] </ref>. 5 Summary To review what is new about SVP: * SVP models information with collections. Collections include many interesting special cases, including streams, multisets, and groups, and combine sets and streams neatly in a single model. * Collections are represented as trees.
Reference: [15] <editor> I. Rival, ed., </editor> <title> Graphs and Order: the role of graphs in the theory of ordered sets and its applications, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1985. </year>
Reference-contexts: Thus, sets and streams suggest a divide-and-conquer for mat for specifying mappings which is implicitly also a format for specifying parallelism. * Divide-and-conquer computations can be represented as series-parallel graphs. Series-parallel graphs <ref> [15] </ref> are defined recursively as graphs having one input and one output that can be constructed using two combination rules: series or parallel composition of the inputs and outputs. A typical series-parallel graph is shown in Figure 2. <p> In this paper these mappings are specified with recursive functional equations. They generalize other specification techniques, including restricted higher-order mappings like the reduction operator in APL [10] and the pump operator in FAD [6], list comprehensions and elegant variants thereof [20], and series-parallel computation graphs <ref> [15] </ref>. * Parallelism in the dividing and conquering is specified using both the structure of the data, and the structure of the divide-and-conquer mapping: dividing-parallelism is specified by the data, and conquering-parallelism is specified by the mapping. Partitioning can always be used to modify data structure, and thus affect dividing-parallelism.
Reference: [16] <author> D.B. Skillikorn, </author> <title> Architecture-Independent Parallel Computation, </title> <journal> IEEE Computer 23:12, </journal> <month> 38-50 (De-cember </month> <year> 1990). </year>
Reference-contexts: The third approach can combine the advantages of the other two. It can ease the task of programming while allowing the programmer to express non-sequential computation in a high-level way <ref> [16] </ref>. Once the programmer has specified the algorithmic aspects of his program using high-level programming constructs, automatic or semi-automatic methods can be used to derive a mapping from the computational requirements of the program to parallel hardware.
Reference: [17] <institution> Thinking Machines Corporation, </institution> <note> Programming in C*, Version 5.0, </note> <institution> Cambridge, </institution> <address> Ma, </address> <year> 1989. </year>
Reference-contexts: There are essentially three ways to accomplish this: * automatically detect parallelism in programs written with a sequential language (e.g., Fortran, OPS5); * augment a language with explicit parallel constructs that exploit the computational capabilities of a parallel architecture (e.g., C* <ref> [17] </ref>, Fortran90); * create a new language in which parallelism can be expressed in an architecture-independent manner. The first approach can be practical in the short-term, but is faced by many difficult problems. Among these, development of a parallelizing compiler is a major challenge.
Reference: [18] <author> P. Valduriez, S. Khoshafian, </author> <title> Parallel Evaluation of the Transitive Closure of a Database Relation, </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17: 1, </volume> <pages> 19-42, </pages> <year> 1988. </year>
Reference: [19] <editor> P. Valduriez, ed., </editor> <booktitle> Data Management and Parallel Processing, </booktitle> <address> London: </address> <publisher> Chapman and Hall, </publisher> <year> 1991. </year>
Reference-contexts: Thus, divide-and-conquer solutions of problems often directly correspond to these four kinds of parallelism. * Database applications provide excellent opportunities for parallel processing. The set-oriented nature of the relational model makes exploitation of inde pendent parallelism natural <ref> [19] </ref>. In fact, set operators such as the relational algebra operators can often be naturally expressed as divide-and-conquer computations, as we will show in section 2. These ideas raise hope for a parallel bulk data processing system that rests upon divide-and-conquer techniques.
Reference: [20] <author> P. Wadler, </author> <title> Comprehending Monads, </title> <booktitle> Proc. 1990 ACM Conf. on LISP and Functional Programming, Nice, France, </booktitle> <pages> 61-78, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In this paper these mappings are specified with recursive functional equations. They generalize other specification techniques, including restricted higher-order mappings like the reduction operator in APL [10] and the pump operator in FAD [6], list comprehensions and elegant variants thereof <ref> [20] </ref>, and series-parallel computation graphs [15]. * Parallelism in the dividing and conquering is specified using both the structure of the data, and the structure of the divide-and-conquer mapping: dividing-parallelism is specified by the data, and conquering-parallelism is specified by the mapping.
Reference: [21] <author> O. Wolfson, A. Ozeri, </author> <title> A New Paradigm for Parallel and Distributed Rule Processing, </title> <booktitle> Proc. ACM SIGMOD Int. Conf., </booktitle> <address> Atlantic City, </address> <month> May </month> <year> 1990. </year> <month> 12 </month>
Reference-contexts: Specifying parallel computations over relations often requires specifying how data partitioning (fan-out parallelism) will be done and how distributed results will be collected (fan-in parallelism). This view is supported by recent results on data reduction for Datalog programs <ref> [21] </ref>, in which rules are replaced by their per-processor specializations. These specialized rules include appropriate hash functions that capture partitioning information. This approach is very interesting in that it incurs no communication costs between processors.
References-found: 21

