URL: http://www.cs.umn.edu/Users/dept/users/du/papers/xor.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/du/papers/
Root-URL: http://www.cs.umn.edu
Title: Efficient Implementation of RAID-5 Using Disk Based Read Modify Writes 1  
Author: Sangyup Shim, Yuewei Wang, Jenwei Hsieh, Taisheng Chang, and David H.C. Du 
Affiliation: Distributed Multimedia Research Center 2 and Computer Science Department, University of Minnesota  
Abstract: Redundant Array of Inexpensive Disks (RAID) is often used to provide a fault tolerance capability for disk failures in database systems. An efficient implementation of small writes is the most important issue to achieve high throughput because most of writes in databases are small. The traditional RAID implementation requires a RAID controller to construct parity blocks for writes (host based writes). An RAID implementation based on an exclusive-or (xor) engine in each disk may minimize link traffic and improve the response times for small writes. This is called disk based writes. However, many implementation issues have to resolved before the higher performance can be realized. We have observed that a straightforward implementation of disk based writes achieves only half of the potential throughput because of the interactions among commands. One of the implementation issues is to prevent a state of deadlock. A state of deadlock may occur when multiple disk based writes are executed simultaneously. The implementation issues addressed in this paper include how to prevent a state of deadlock, how to enhance cache replacement policy in disks, how to increase concurrency among disk based writes, and how to minimize the interactions of many commands. This paper investigates the effectiveness of disk based writes, and shows the performance comparison between a disk based write and a host based write in RAIDs. In a stress test, the aggregate throughput was increased, and the average command latency was reduced. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ANSI X3T10.1/0989D revision 10, </author> <title> "Information Technology Serial Storage Architecture Transport Layer 1 (SSA-TL1) (Draft Proposed American National Standard". </title> <publisher> American National Standard Institute, Inc., </publisher> <month> April, </month> <year> 1996. </year>
Reference-contexts: To the best of our 3 knowledge, no study has been published on the implementation issues and the performance of disk based writes. The serial interface standards have been recently developed. They include the Serial Storage Architecture (SSA) standards <ref> [1, 2] </ref>, the Fibre Channel Arbitrated Loop (FC-AL) standard [3], and the IEEE P1394 serial bus standard [13]. FC-AL was designed for high speed interconnections to provide low-cost storage attachments. FC-AL provides data bandwidth of 100 MBps and fault tolerance with an optional fairness algorithm.
Reference: [2] <author> ANSI X3T10.1/1121D revision 7, </author> <title> "Information Technology Serial Storage Architecture - SCSI-2 Protocol (SSA-S2P) (Draft Proposed American National Standard". </title> <publisher> American National Standard Institute, Inc., </publisher> <month> April, </month> <year> 1996. </year>
Reference-contexts: To the best of our 3 knowledge, no study has been published on the implementation issues and the performance of disk based writes. The serial interface standards have been recently developed. They include the Serial Storage Architecture (SSA) standards <ref> [1, 2] </ref>, the Fibre Channel Arbitrated Loop (FC-AL) standard [3], and the IEEE P1394 serial bus standard [13]. FC-AL was designed for high speed interconnections to provide low-cost storage attachments. FC-AL provides data bandwidth of 100 MBps and fault tolerance with an optional fairness algorithm.
Reference: [3] <author> ANSI X3.272-199x, </author> <title> "Fibre Channel Arbitrated Loop (FC-AL), Revision 4.5", </title> <publisher> American National Standard Institute, Inc., </publisher> <month> June 1, </month> <year> 1995. </year>
Reference-contexts: To the best of our 3 knowledge, no study has been published on the implementation issues and the performance of disk based writes. The serial interface standards have been recently developed. They include the Serial Storage Architecture (SSA) standards [1, 2], the Fibre Channel Arbitrated Loop (FC-AL) standard <ref> [3] </ref>, and the IEEE P1394 serial bus standard [13]. FC-AL was designed for high speed interconnections to provide low-cost storage attachments. FC-AL provides data bandwidth of 100 MBps and fault tolerance with an optional fairness algorithm. Serial Storage Architecture (SSA) offers high performance interconnections for storage subsystems. <p> The details of the arbitration process is described in <ref> [3, 5] </ref>. The high arbitration overhead in links for a host based RMW caused the longer loop waiting time as more disks were contending for the link simultaneously.
Reference: [4] <author> P. Chen, E. Lee, G. Gibson, R. Katz, D. Patterson, </author> <title> "RAID: High-Performance, Reliable Secondary Storage", </title> <journal> ACM Computing Surveys, </journal> <month> June </month> <year> 1994, </year> <pages> pp. 145-185. </pages>
Reference-contexts: Secondly, fault tolerance is an important issue if a database system needs to provide non-disruptable services. A good design of a storage subsystem must be capable of handling disk failures. Redundant Arrays of Inexpensive Disks (RAID) <ref> [4] </ref> provides a fault tolerant solution to allow a large number of queries to access databases on a storage subsystem. Among all the RAID levels, RAID5 [4] is widely used due to its high I/O performance in a fault-free condition and reasonable throughput under fault environment. <p> A good design of a storage subsystem must be capable of handling disk failures. Redundant Arrays of Inexpensive Disks (RAID) <ref> [4] </ref> provides a fault tolerant solution to allow a large number of queries to access databases on a storage subsystem. Among all the RAID levels, RAID5 [4] is widely used due to its high I/O performance in a fault-free condition and reasonable throughput under fault environment. For this reason, we only consider RAID-5 in this paper 3 . <p> When a write involves data blocks from more than half of a stripe, the blocks which are not written in a stripe are read into host buffers because the non-writing blocks are smaller than the updated blocks. This write is called a reconstruct write <ref> [4] </ref>. It is a common technique to improve the performance of large writes in RAID-5 5 . After the non-writing blocks are read, the new parity block is constructed, and later written back to the parity disk. Figure 4 shows a reconstruct write.
Reference: [5] <author> D.H.C. Du, T. Chang, J. Hsieh, S. Shim, and Y. Wang, </author> <title> "Emerging Serial Storage Interfaces: Serial Storage Architecture (SSA) and Fibre Channel Arbitrated Loop (FC-AL)", </title> <institution> Technical Report at Computer Science Department, University of Minnesota, </institution> <type> TR 96-073, </type> <year> 1996. </year>
Reference-contexts: Both FC-AL and SSA supports hot pluggable devices on a back plane and provides redundant link paths for fault tolerance. A detailed tutorial on the FC-AL and SSA standards are described in <ref> [5] </ref>. 3 Host Based Writes In RAID-5, data are divided into blocks and striped block-wise in disks. Data blocks in the same location at each disk and the corresponding parity block form a stripe. A write in RAID involves updating the corresponding parity block. <p> A loop port is attached to the loop for each disk on the loop. A Loop Port Sate Machine (LPSM) <ref> [5] </ref> is used to define the behavior of the loop ports. FCP simulates a protocol mapping layer that uses the services provided by LPSM. Our simulation models focus on the storage interface with attached disk drives. <p> The details of the arbitration process is described in <ref> [3, 5] </ref>. The high arbitration overhead in links for a host based RMW caused the longer loop waiting time as more disks were contending for the link simultaneously.
Reference: [6] <author> D.H.C. Du, J. Hsieh, T. Chang, Y. Wang, and S. Shim, </author> <title> "Performance Study of Serial Storage Architecture (SSA) and Fibre Channel Arbitrated Loop (FC-AL)", </title> <institution> Technical Report at Computer Science Department, University of Minnesota, </institution> <type> TR 96-074, </type> <year> 1996. </year>
Reference-contexts: In disk based RMWs, the loop was saturated at 38.2 MB/sec with 48 disks. Hence 76.4 MB/sec of the aggregate throughput in the link level was achieved out of the theoretical data bandwidth of 100 MB/sec. The results were consistent with the results found in <ref> [6] </ref>. In host based RMWs, the aggregate throughput decreased when more than 32 disks were attached on a loop because of the link saturation. However the links were saturated when more than 48 disks were attached in disk based RMWs.
Reference: [7] <author> G. Ganger, B. Worthington, R. Hou, Y. Patt, </author> <title> "Disk Arrays High-Performance, High-Reliability Storage Subsystems", </title> <booktitle> IEEE Computer, </booktitle> <month> March </month> <year> 1994, </year> <month> pp.17-28. </month>
Reference: [8] <author> ANSI X3T10-1994-111r9, G. Houlder, J. Elrod, and M. Miller, </author> <title> "XOR Commands on SCSI Disk Drives", </title>
Reference-contexts: That is why the storage industry are quite interested in a disk based RMW. The new SCSI commands to implement a disk based RMW, called XDWRITE and XPWRITE, are defined in <ref> [8] </ref>. XDWRITE is used by a host to send new data blocks to disks. XPWRITE is used by a disk to send updated information to another disk for the computation of new parity blocks. <p> Since old data blocks are not transferred to a host in a disk based RMW, data transfers in links are reduced by half compared to a host based one. We will describe the disk based write in detail in Section 4. The standard in <ref> [8] </ref> describes only the steps necessary to perform a disk based write. However, many problems arise when multiple disk based writes are intermixed with one another. <p> In RAID-5, data are striped block-wise over disks. Parity disks are selected in a circular fashion so that parity blocks are placed uniformly over all disks. Section 3 describes a host based write. Mirroring can also provide fault tolerance by doubling the storage requirement. In <ref> [8] </ref>, the new SCSI commands, an XDWRITE and XPWRITE, are defined to support a disk based write. However many problems arise when multiple disk based writes are intermixed with each other.
Reference: [9] <author> IBM Corporation, </author> <title> "Functional Specification, Ultrastar XP Models", </title> <year> 1995. </year>
Reference-contexts: The disk model is based on an IBM Ultrastar XP 4.51 GB disk. The detailed disk parameters are given in <ref> [9] </ref>. This IBM disk employs zone bit recording [12]. The disk is divided into different zones. Table 5 shows the parameters of disk cache used in the simulation. Blocks in the disk and disk cache are of the same size (i.e., 512 bytes).
Reference: [10] <author> R. Muntz and J. Lui, </author> <title> "Performance analysis of disk arrays under failure", </title> <booktitle> Proceedings of 16th VLDB, </booktitle> <address> Brisbane, Austrailia, </address> <month> August </month> <year> 1990, </year> <month> pp.162-173. </month>
Reference: [11] <author> C. Ruemmler and J. Wilkes, </author> <title> "UNIX disk access patterns", </title> <booktitle> USENIX Winter Technical Conference, </booktitle> <month> Jan. </month> <year> 1993, </year> <month> pp.313-323. </month>
Reference: [12] <author> C. Ruemmler and J. Wilkes, </author> <title> "An Introduction to Disk Drive Modeling", </title> <booktitle> IEEE Computer, </booktitle> <month> March </month> <year> 1994, </year> <month> pp.17-28. </month>
Reference-contexts: The disk model is based on an IBM Ultrastar XP 4.51 GB disk. The detailed disk parameters are given in [9]. This IBM disk employs zone bit recording <ref> [12] </ref>. The disk is divided into different zones. Table 5 shows the parameters of disk cache used in the simulation. Blocks in the disk and disk cache are of the same size (i.e., 512 bytes). In FC-AL, channel accesses are granted through arbitration.
Reference: [13] <author> A. Kunzman and A. Wetzel, </author> <title> "1394 High Performance Serial Bus: The Digital Interface for ATV", </title> <journal> IEEE Transactions on Consumer Electronics, </journal> <month> August </month> <year> 1995, </year> <journal> Vol.41, </journal> <volume> No.3, </volume> <pages> pp 893-900. </pages>
Reference-contexts: The serial interface standards have been recently developed. They include the Serial Storage Architecture (SSA) standards [1, 2], the Fibre Channel Arbitrated Loop (FC-AL) standard [3], and the IEEE P1394 serial bus standard <ref> [13] </ref>. FC-AL was designed for high speed interconnections to provide low-cost storage attachments. FC-AL provides data bandwidth of 100 MBps and fault tolerance with an optional fairness algorithm. Serial Storage Architecture (SSA) offers high performance interconnections for storage subsystems.
Reference: [14] <author> D. Patterson, G. Gibson, and R. Katz, </author> <title> "A Case of Redundant Arrays of Inexpensive Disks (RAID)" Proceedings of SIGMOD, </title> <address> Chicago IL, </address> <month> June </month> <year> 1988. </year>
Reference: [15] <author> SSA Industry Association, </author> <title> "Serial Storage Architecture: A Technology Overview", </title> <note> Version 3.0, </note> <year> 1995. </year>
Reference: [16] <author> Seagate Technology, Inc. </author> <title> "Cheetah Family Specifications", </title> <publisher> -http://www.seagate.com/disc/cheetah/cheetah.html </publisher>
Reference-contexts: For example, fast-wide SCSI provides a bandwidth of 20 MB/s, and a Cheetah disk drive offers data transfer rate up to 16.8 MB/s <ref> [16] </ref>. The emerging serial storage interfaces such as Fibre Channel Arbitrated Loop (FC-AL)[3] and Serial Storage Architecture (SSA)[1, 2] provide alternatives for high-performance storage interfaces. FC-AL offers many advantages over SCSI. The advantages include higher bandwidth, fair accesses, fault tolerance, compact connectors, and more device attachments in a single loop. <p> Since adding an XOR unit to a high performance disk is cost effective, in this paper we study the performance of disk based writes with FC-AL disk drives. The maximum number of attachments in a fast-wide SCSI bus and FC-AL is 15 and 126 devices as Cheetah disk drives <ref> [16] </ref> from Seagate offer disk rotation speed at 10,033 rpm and data transfer rates of 11.3 to 16.8 MB/s). As a disk is getting larger and faster, the cost of a high performance disk is increasing while the cost per MB is decreasing.
Reference: [17] <author> T. Sutton and D. Webb, </author> <title> "Fibre Channel: The Digital Highway Made Practical", </title> <note> Seagate technology paper, </note> <month> October </month> <year> 1994. </year> <month> 17 </month>
Reference-contexts: When the XOR capability is built into a disk, a 3 For conveniences, we shall use the terms RAID and RAID-5 interchangeably throughout the paper. 4 Recently the areal density of a hard disk increases roughly sixty percents per year <ref> [17] </ref>. The capacity of a high performance disk is approaching 20 GBytes. Disc rotation speed is also getting faster from 7,200 to 10,000 rpm (such 1 RAID controller may no longer be needed if parity computations are performed in disks.
References-found: 17

