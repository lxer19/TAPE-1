URL: ftp://ftp.aic.nrl.navy.mil/pub/papers/1994/AIC-94-017.ps
Refering-URL: http://www.aic.nrl.navy.mil/~schultz/papers.html
Root-URL: 
Email: schultz@aic.nrl.navy.mil  
Phone: (202) 767-2684 fax (202) 767-3172  
Title: EVOLVING ROBOT BEHAVIORS  
Author: ALAN C. SCHULTZ JOHN J. GREFENSTETTE 
Address: Washington, DC 20375-5337  
Affiliation: Navy Center for Applied Research in Artificial Intelligence Naval Research Laboratory  
Abstract: This paper discusses the use of evolutionary computation to evolve behaviors that exhibit emergent intelligent behavior. Genetic algorithms are used to learn navigation and collision avoidance behaviors for robots. The learning is performed under simulation, and the resulting behaviors are then used to control the actual robot. Some of the emergent behavior is described in detail. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> De Jong, K. A. and W. </author> <title> Spears (1993). </title> <booktitle> On the state of evolutionary computation. Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pp. 618-626, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: INTRODUCTION The field of robotics offers an endless supply of difficult problems, requiring an equally impressive array of methods for their solution. One class of methods that has shown its utility on a number of relevant problems is called Evolutionary Computation <ref> [1] </ref>. This term applies to computational methods that incorporate principles from biological population genetics to perform search, optimization, and machine learning, and includes a variety of specific formulations with names such as genetic algorithms, evolutionary programming, evolution strategies, and genetic programming.
Reference: 2. <author> Grefenstette, J. J., C. L. Ramsey and A. C. </author> <title> Schultz (1990). Learning sequential decision rules using simulation models and competition. </title> <booktitle> Machine Learning 5(4), </booktitle> <pages> 355-381. </pages>
Reference-contexts: ROBOTICS APPLICATIONS OF EVOLUTIONARY ALGORITHMS Evolutionary methods have found applications that span the range of architectures for intelligent robotics. For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents <ref> [2] </ref>, topologies and weights for neural nets for robotic control [3, 4], fuzzy logic control systems [5], programs for LISP-controlled robots [6], and rules for behavior-based robots [7]. <p> Both of the above paradigms may be expected to provide fundamental insights into the development of flexible and adaptive robots. The remainder of this paper will focus on the SAMUEL evolutionary learning system being developed at NRL <ref> [2] </ref> and will present the results of applying the SAMUEL system to a collision avoidance and navigation task for robots. In SAMUEL, the population is composed of candidate behaviors for solving the task. <p> Tests using a two-robot cat-and-mouse game as the task environment show that case-based methods provide significant improvement in responding to changes in the task environment [20]. The SAMUEL system has been used to learn behaviors for controlling simulated autonomous underwater vehicles [21], missile evasion <ref> [2] </ref>, and other simulated tasks. This paper reports the first tests of the learned knowledge on a real physical system. For more details of the SAMUEL system, see [19]. 5 ROBOT PLATFORM For these experiments, a Nomadic Technologies, Inc. Nomad 200 robot was used.
Reference: 3. <author> Whitley, D., S. Dominic, R. Das, C. </author> <title> Anderson (1993). Genetic reinforcement learning for neurocontrol problems. </title> <booktitle> Machine Learning 13(2/3), </booktitle> <pages> 259-284. </pages>
Reference-contexts: ROBOTICS APPLICATIONS OF EVOLUTIONARY ALGORITHMS Evolutionary methods have found applications that span the range of architectures for intelligent robotics. For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents [2], topologies and weights for neural nets for robotic control <ref> [3, 4] </ref>, fuzzy logic control systems [5], programs for LISP-controlled robots [6], and rules for behavior-based robots [7]. There are at least two different research paradigms evident in the literature of evolutionary algorithms for robotics, which might be called the artificial life (ALife) paradigm, and the genetic knowledge engineering paradigm.
Reference: 4. <author> Yamauchi, B. </author> <year> (1994). </year> <title> Dynamic neural networks for mobile robot control. </title> <address> ISRAM 94. </address>
Reference-contexts: ROBOTICS APPLICATIONS OF EVOLUTIONARY ALGORITHMS Evolutionary methods have found applications that span the range of architectures for intelligent robotics. For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents [2], topologies and weights for neural nets for robotic control <ref> [3, 4] </ref>, fuzzy logic control systems [5], programs for LISP-controlled robots [6], and rules for behavior-based robots [7]. There are at least two different research paradigms evident in the literature of evolutionary algorithms for robotics, which might be called the artificial life (ALife) paradigm, and the genetic knowledge engineering paradigm.
Reference: 5. <author> Karr, C, L. </author> <year> (1991). </year> <title> Design of an adaptive fuzzy logic controller using a genetic algorithm. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pp. 450-457, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents [2], topologies and weights for neural nets for robotic control [3, 4], fuzzy logic control systems <ref> [5] </ref>, programs for LISP-controlled robots [6], and rules for behavior-based robots [7]. There are at least two different research paradigms evident in the literature of evolutionary algorithms for robotics, which might be called the artificial life (ALife) paradigm, and the genetic knowledge engineering paradigm.
Reference: 6. <editor> Koza, J. R. </editor> <booktitle> (1992). Genetic Programming. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents [2], topologies and weights for neural nets for robotic control [3, 4], fuzzy logic control systems [5], programs for LISP-controlled robots <ref> [6] </ref>, and rules for behavior-based robots [7]. There are at least two different research paradigms evident in the literature of evolutionary algorithms for robotics, which might be called the artificial life (ALife) paradigm, and the genetic knowledge engineering paradigm. The ALife paradigm is motivated by a number of issues.
Reference: 7. <author> Dorigo, M. and U. </author> <title> Schnepf (1993). Genetics-based machine learning and behavior-based robotics: a new synthesis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, SMC-23, </journal> <volume> 1. </volume>
Reference-contexts: For example, evolutionary algorithms have been used to learn rule sets for rule-based autonomous agents [2], topologies and weights for neural nets for robotic control [3, 4], fuzzy logic control systems [5], programs for LISP-controlled robots [6], and rules for behavior-based robots <ref> [7] </ref>. There are at least two different research paradigms evident in the literature of evolutionary algorithms for robotics, which might be called the artificial life (ALife) paradigm, and the genetic knowledge engineering paradigm. The ALife paradigm is motivated by a number of issues.
Reference: 8. <author> P. Todd and S. </author> <title> Wilson (1993). Environment structure and adaptive behavior from the ground up. </title> <booktitle> From Animals to Animats 2: Proc. of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. 11-20. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The ALife paradigm is motivated by a number of issues. First, there is interest in the process of evolution itself. Simulations of robotic agents can be used to study the emergence of complex behavior from the interaction of simple components, operating under conditions of competition for resources <ref> [8] </ref>. Second, there are concerns about the utility of the traditional model-based reasoning approach to designing intelligent robots. Models used in artificial intelligence studies often fail to reflect the complexities, noise, and errors that arise in real sensors and actuators operating in the real world.
Reference: 9. <author> Brooks, R. </author> <title> Artificial Life and Real Robots. </title> <publisher> Cambridge: MIT Press. </publisher> <year> 1992. </year>
Reference-contexts: In response to such shortcomings, some researchers argue for the development of adaptive robots that evolve behaviors without using a pre-specified model of the world <ref> [9] </ref>. Finally, there is a growing interest in the possibility of physical robots based on nano-technology, that truly self-replicate and evolve in adaptation to their environment [10].
Reference: 10. <author> Higuchi, T., T. Niwa, T. Tanaka, H. Iba, H. de Garis and T. </author> <month> Furaya </month> <year> (1993). </year> <title> Evolving hardware with genetic learning: a first step towards building a Darwin machine, </title> <booktitle> From Animals to Animats 2: Proc. of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. 417-424. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <pages> 10 </pages>
Reference-contexts: Finally, there is a growing interest in the possibility of physical robots based on nano-technology, that truly self-replicate and evolve in adaptation to their environment <ref> [10] </ref>. The genetic knowledge engineering paradigm treats the knowledge acquisition task for intelligent robots as a cooperative effort between the robot designers and the robot itself [11].
Reference: 11. <author> Grefenstette, J. and C. </author> <title> Ramsey (1993). Combining experience with quantitative models. </title> <booktitle> Workshop on Learning Action Models at AAAI-93 (National Conference on Artificial Intelligence, </booktitle> <year> 1993) </year>
Reference-contexts: The genetic knowledge engineering paradigm treats the knowledge acquisition task for intelligent robots as a cooperative effort between the robot designers and the robot itself <ref> [11] </ref>. Some aspects of the robot's world will be known in great detail to the designer, for example, the size and weight of the robot, the characteristics of its sensors and effectors, and at least some of the physics of the task environment.
Reference: 12. <author> Grefenstette, J. J. and C. L. </author> <title> Ramsey (1992). An approach to anytime learning. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning pp. </booktitle> <pages> 189-195, </pages> <editor> D. Sleeman and P. Edwards (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As illustrated in Fig. 1, the current best behavior can be placed in the real, on-line system, while learning continues in the off-line system <ref> [12] </ref>. The learning algorithm was designed to learn useful behaviors from simulations of limited fidelity. The expectation is that behaviors learned in these simulations will be useful in real-world environments. <p> Therefore, a natural hybrid is to use an efficient local optimization method to improve the final solutions found by the evolutionary system [14]. Another hybrid approach is to augment the population with additional long-term memory, to deal with changing environments. We are exploring an approach called case-based anytime learning <ref> [12] </ref>. In this approach, the robot's evolutionary learning module continuously tests new strategies against a simulation model of the task environment, and dynamically updates the knowledge base used by the real robot on the basis of the results.
Reference: 13. <author> Ramsey, C. L., A. C. Schultz and J. J. </author> <title> Grefenstette (1990). Simulation-assisted learning by competition: Effects of noise differences between training model and target environment. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning pp. </booktitle> <pages> 211-215. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Previous studies have illustrated that knowledge learned under simulation is robust and might be applicable to the real world if the simulation is more general (i.e. has more noise, more varied conditions, etc.) than the real world environment <ref> [13] </ref>. Where this is not possible, it is important to identify the differences between the simulation and the world and note the effect upon the learning process. TARGET ENVIRONMENT RULE INTERPRETER ACTIVE BEHAVIOR SIMULATION ENVIRONMENT RULE INTERPRETER LEARNING MODULE TEST BEHAVIOR ON-LINE SYSTEM OFF-LINE SYSTEM Fig. 1. <p> Evolutionary 4 algorithms may also exploit unexpected features of the simulation model to maximize perfor-mance. Of course, this implies that the model should accurately reflect the conditions in the target environment. To the extent that this is not possible, our studies <ref> [13] </ref> have shown that it is still possible to learn from limited-fidelity simulations that err on the side of difficulty (e.g., have more noisy sensors that the real robots). In such cases, the learning time increases, but so does the robustness of the learned rules.
Reference: 14. <author> Grefenstette, J. </author> <title> Incorporating problem specific knowledge into genetic algorithms, in Genetic Algorithms and Simulated Annealing, </title> <editor> L. Davis (Ed.), </editor> <publisher> London: Pitman, </publisher> <year> 1987. </year>
Reference-contexts: Then we describe the simulation of the robot. The task the robot is to perform is explained, followed by a description of the experiments and the results. DESCRIPTION OF THE LEARNING ALGORITHM The evolutionary approach affords several opportunities for using background knowledge to augment the basic method <ref> [14] </ref>, producing a more efficient overall knowledge acquisition effort. Here, we outline the specifics for our robot learning problem. Choice of Search Space and Representation. The first choice the user of an evolutionary algorithm needs to make is the choice of the search space, along with an appropriate representation. <p> Evolutionary methods are less efficient at fine-tuning candidate solutions. Therefore, a natural hybrid is to use an efficient local optimization method to improve the final solutions found by the evolutionary system <ref> [14] </ref>. Another hybrid approach is to augment the population with additional long-term memory, to deal with changing environments. We are exploring an approach called case-based anytime learning [12].
Reference: 15. <author> Grefenstette, J. J. and H. C. </author> <note> Cobb (1994). User's guide for SAMUEL. Version 4.0. NRL Report, </note> <institution> Naval Research Lab, </institution> <address> Washington, DC. </address>
Reference-contexts: This cycle is repeated until the task is accomplished or failed. In SAMUEL, the user can further limit the search space by defining a set of constraints in the form of rules that specify conditions under which certain actions are either forbidden or required <ref> [15] </ref>. Constraints are intended to limit the robot's actions within physically safe parameters, but still allow freedom to explore a large set of alternative strategies [16]. Initial Population. Evolutionary algorithms often begin with candidate solutions selected at random from the search space.
Reference: 16. <author> Grefenstette, J. </author> <year> (1992). </year> <title> The evolution of strategies for multi-agent environments. </title> <booktitle> Adaptive Behavior 1(1), </booktitle> <pages> 65-90. </pages>
Reference-contexts: Constraints are intended to limit the robot's actions within physically safe parameters, but still allow freedom to explore a large set of alternative strategies <ref> [16] </ref>. Initial Population. Evolutionary algorithms often begin with candidate solutions selected at random from the search space. Often, the approach can be sped up by the use of heuristics to select the starting population.
Reference: 17. <author> Schultz, A. C. and J. J. </author> <title> Grefenstette (1990). Improving tactical plans with genetic algorithms. </title> <booktitle> Proceedings of IEEE Conference on Tools for AI 90 (pp 328-334). </booktitle> <address> Wash-ington, DC: </address> <publisher> IEEE. </publisher>
Reference-contexts: This must be done with care, however, since a lack of sufficient diversity in the initial population is almost guaranteed to produced premature convergence to suboptimal solutions. In SAMUEL, the rule representation was designed to encourage the user to include heuristic strategies in the initial population <ref> [17] </ref>. In fact, for many complex robotic tasks, it is unlikely that the system will be able to evolve solutions without some initial heuristics. <p> Each trial begins with a different configuration of the obstacles in the room. The robot was given 80 decision time steps to cross the room to the goal position. The initial heterogeneous population <ref> [17] </ref> consisted of a variety of rule sets from different sources. This included a combination of manually (human) generated rules sets and automatically generated variants of those rules.
Reference: 18. <author> Booker, L. B. </author> <year> (1988). </year> <title> Classifier systems that learn internal world models. </title> <booktitle> Machine Learning 3(3), </booktitle> <pages> 161-192. </pages>
Reference-contexts: However, some recent studies have used more heuristic operators that make more directed changes based on the learning agent's experience. For example, some genetic classifier systems use triggered operators such a creating a new rule to cover a novel situation <ref> [18] </ref>. In SAMUEL, we use generalization and specialization operators that are triggered by specific conditions relating the measured utilities of individual rules and the outcome of the task. These may be viewed as Lamarckian forms of evolution [19], and show that artificial evolution need not proceed along purely Darwinian lines.
Reference: 19. <author> Grefenstette, J. J. </author> <year> (1991). </year> <title> Lamarckian learning in multi-agent environments. </title> <booktitle> Proceedings of the Fourth International Conference of Genetic Algorithms pp. </booktitle> <pages> 303-310, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In SAMUEL, we use generalization and specialization operators that are triggered by specific conditions relating the measured utilities of individual rules and the outcome of the task. These may be viewed as Lamarckian forms of evolution <ref> [19] </ref>, and show that artificial evolution need not proceed along purely Darwinian lines. Hybrid Approaches. Finally, it is often useful to use evolutionary methods in concert with other methods that have complementary strengths. <p> The SAMUEL system has been used to learn behaviors for controlling simulated autonomous underwater vehicles [21], missile evasion [2], and other simulated tasks. This paper reports the first tests of the learned knowledge on a real physical system. For more details of the SAMUEL system, see <ref> [19] </ref>. 5 ROBOT PLATFORM For these experiments, a Nomadic Technologies, Inc. Nomad 200 robot was used.
Reference: 20. <author> Ramsey, C. L. and J. J. </author> <title> Grefenstette (1993). Case-based initialization of genetic algorithms. </title> <booktitle> Proc. Fifth Int. Conf. on Genetic Algorithms. </booktitle> <pages> pp. 84-91, </pages> <address> San Mateo, CA: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: When the simulation model is modified, the learning process is reini-tialized using previously learned strategies from similar cases. Tests using a two-robot cat-and-mouse game as the task environment show that case-based methods provide significant improvement in responding to changes in the task environment <ref> [20] </ref>. The SAMUEL system has been used to learn behaviors for controlling simulated autonomous underwater vehicles [21], missile evasion [2], and other simulated tasks. This paper reports the first tests of the learned knowledge on a real physical system.
Reference: 21. <author> Schultz, A. C., </author> <title> Using a genetic algorithm to learn strategies for collision avoidance and local navigation. </title> <booktitle> Seventh International Symposium on Unmanned, Untethered, Submersible Technology, </booktitle> <year> 1991, </year> <pages> (pp 213-225). </pages> <address> Durham, NH. </address> <month> 11 </month>
Reference-contexts: Tests using a two-robot cat-and-mouse game as the task environment show that case-based methods provide significant improvement in responding to changes in the task environment [20]. The SAMUEL system has been used to learn behaviors for controlling simulated autonomous underwater vehicles <ref> [21] </ref>, missile evasion [2], and other simulated tasks. This paper reports the first tests of the learned knowledge on a real physical system. For more details of the SAMUEL system, see [19]. 5 ROBOT PLATFORM For these experiments, a Nomadic Technologies, Inc. Nomad 200 robot was used.
References-found: 21

