URL: http://www.cs.cmu.edu/~roni/rdi-IEEE-ASR97.ps
Refering-URL: 
Root-URL: 
Title: A Whole Sentence Maximum Entropy Language Model  
Author: R. Rosenfeld 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: We introduce a new kind of language model, which models whole sentences or utterances directly using the Maximum Entropy paradigm. The new model is conceptually simpler, and more naturally suited to modeling whole-sentence phenomena, than the conditional ME models proposed to date. By avoiding the chain rule, the model treats each sentence or utterance as a "bag of features", where features are arbitrary computable properties of the sentence. The model is unnor-malizable, but this does not interfere with training (done via sampling) or with use. Using the model is computationally straightforward. The main computational cost of training the model is in generating sample sentences from a Gibbs distribution. Interestingly, this cost has different dependencies, and is potentially lower, than in the comparable conditional ME model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. T. </author> <title> Jaynes, </title> <journal> "Information Theory and Statistical Mechanics," Physics Reviews, </journal> <volume> vol. 106, </volume> <pages> pp. 620-630, </pages> <year> 1957. </year>
Reference: [2] <author> S. Della Pietra, V. Della Pietra, R. Mercer and S. Roukos, </author> <title> "Adaptive Language Modeling Using Minimum Discriminant Estimation," </title> <booktitle> In Proceedings ICASSP, </booktitle> <year> 1992, </year> <pages> pp. </pages> <month> I-633-636. </month>
Reference: [3] <author> R. Lau, R. Rosenfeld and S. Roukos, </author> <title> "Trigger-Based Language Models: a Maximum Entropy Approach," </title> <booktitle> In Proc. ICASSP, </booktitle> <year> 1993, </year> <pages> pp. II 45-48. </pages>
Reference: [4] <author> A. Berger, S. Della Pietra and V. Della Pietra, </author> <title> "A Maximum Entropy Approach to Natural Language Processing," </title> <journal> Computational Linguistics, </journal> <volume> vol. 22, no. 1, </volume> <month> March </month> <year> 1996. </year>
Reference-contexts: This is because the training procedure used for conditional ME models restricts the computation of the feature expectations to histories observed in the training data (see <ref> [4] </ref> or [5, section 4.4]). This biases the solution in an interesting and usually appropriate way. For example, consider two word trigger features: A ! Z and B ! Z. If A and B are correlated in the training data, this will affect the solution of the conditional ME model.
Reference: [5] <author> R. Rosenfeld, </author> <title> "A Maximum Entropy Approach to Adaptive Statistical Language Modeling," </title> <booktitle> Computer, Speech and Language, </booktitle> <volume> vol. 10, </volume> <pages> pp. 187-228, </pages> <year> 1996. </year> <note> Longer version: Carnegie Mellon Tech. Rep. CMU-CS-94-138. </note>
Reference-contexts: This is because the training procedure used for conditional ME models restricts the computation of the feature expectations to histories observed in the training data (see [4] or <ref> [5, section 4.4] </ref>). This biases the solution in an interesting and usually appropriate way. For example, consider two word trigger features: A ! Z and B ! Z. If A and B are correlated in the training data, this will affect the solution of the conditional ME model.
Reference: [6] <author> A. Ratnaparkhi and S. Roukos, </author> <title> "A Maximum Entropy Model for Prepositional Phrase Attachment," </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: [7] <author> S. Della Pietra, V. Della Pietra, and J. Lafferty, </author> <title> "Inducing features of random fields," </title> <journal> In IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 19, no. 4, </volume> <pages> pp. 380-393, </pages> <month> April </month> <year> 1997. </year>
Reference: [8] <author> J. N. Darroch and D. Ratcliff, </author> <title> "Generalized Iterative Scaling for Log-Linear Models," </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> vol. 43, </volume> <pages> pp. 1470-1480, </pages> <year> 1972. </year>
Reference: [9] <author> S. Geman and D. Geman, </author> <title> "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 6, </volume> <pages> pp. 721-741, </pages> <year> 1984. </year>
References-found: 9

