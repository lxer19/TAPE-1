URL: http://www.cs.umd.edu/~nau/search/CS-TR-3419.ps
Refering-URL: http://www.cs.umd.edu/~nau/publications.html
Root-URL: 
Email: Email: subrata@cs.umd.edu.  Email: nau@cs.umd.edu.  Email: kanal@cs.umd.edu.  
Title: On the Asymptotic Performance of IDA* analysis of asymptotic optimality for IDA* in [10] is
Author: A. Mahanti S. Ghosh D. S. Nau A. K. Pal L.N.Kanal Dana S. Nau 
Keyword: Search, Asymptotic Optimality, A*, IDA*  
Address: IIM, Calcutta, Diamond Harbour Road, PO. Box No. 16757, Calcutta 700 027, India.  College Park, MD 20742.  College Park, MD 20742.  Harbour Road, P. Box No. 16757, Calcutta 700 027, India.  College Park, MD 20742.  
Affiliation: India.  Dept. of Computer Science, Univ. of Maryland,  Dept. of Computer Science, Institute for Systems Research, and Institute for Advanced Computer Studies, Univ. of Maryland,  IIM, Calcutta, Diamond  -Dept. of Computer Science, Univ. of Maryland,  
Note: Address all correspondence to  The  On graphs, IDA* can perform quite poorly. In particular, there are graphs on which IDA* does (2 2N node expansions where N is the number of nodes expanded by A*.  This work was supported in part by NSF Grants IRI-8802419, NSFD CDR-88003012 and IRI 9306580 in the US, and the CMDS project (work order 019/7-148/CMDS-1039/90-91) in  
Abstract: Since best-first search algorithms such as A* require large amounts of memory, they sometimes cannot run to completion, even on problem instances of moderate size. This problem has led to the development of limited-memory search algorithms, of which the best known is IDA* [9, 10]. This paper presents the following results about IDA* and related algorithms: * To correct the above problem, we state and prove necessary and sufficient conditions for asymptotic optimality of IDA* on trees. On trees not satisfying our conditions, we show that no best-first limited-memory search algorithm can be asymptotically optimal. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bagchi and A. Mahanti. </author> <title> Search algorithms under different kinds of heuristics-a comparative study. </title> <journal> JACM, </journal> <volume> 30(1) </volume> <pages> 1-21, </pages> <year> 1983. </year> <month> 38 </month>
Reference-contexts: It is assumed that h (n) 0 for every node n, with h (n) = 0 if n is a goal node. Let P be any path from the start node s. Then the function pathmax (P ) <ref> [1] </ref> is defined as follows: pathmax (P ) = max n2P where c (P; s; n) is the cost of the subpath of P that goes from s to n. The heuristic function h is admissible if 8n 2 G, h (n) h fl (n). <p> For example, A*'s major bottleneck in practical use is its storage requirement. For most problems of non-trivial size, A* runs out of memory before any significant amount of execution time. This limitation also prevails in the case of other known variants of A* (B [15], GRAPHSEARCH [17], C <ref> [1] </ref>, PropC [2], MarkA [2], B' [16], D [14], etc.), which are not tailored to run with limited memory.
Reference: [2] <author> A. Bagchi and A. Mahanti. </author> <title> Three approaches to heuristic search in networks. </title> <journal> JACM, </journal> <volume> 32(1):1--27, </volume> <year> 1985. </year>
Reference-contexts: For most problems of non-trivial size, A* runs out of memory before any significant amount of execution time. This limitation also prevails in the case of other known variants of A* (B [15], GRAPHSEARCH [17], C [1], PropC <ref> [2] </ref>, MarkA [2], B' [16], D [14], etc.), which are not tailored to run with limited memory. <p> For most problems of non-trivial size, A* runs out of memory before any significant amount of execution time. This limitation also prevails in the case of other known variants of A* (B [15], GRAPHSEARCH [17], C [1], PropC <ref> [2] </ref>, MarkA [2], B' [16], D [14], etc.), which are not tailored to run with limited memory.
Reference: [3] <author> P. P. Chakrabarti, S. Ghosh, A. Acharya, and S. C. De Sarkar. </author> <title> Heuristic search in restricted memory. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 197-221, </pages> <year> 1989. </year>
Reference-contexts: Other Algorithms on Trees. IDA* is not the only tree search algorithm that operates in limited memory. Following IDA*, several other algorithms have been developed that run in limited memory, such as MREC [21], MA* <ref> [3] </ref>, RA* [5], SMA* [20], RBFS [8], and ITS [6]. In Section 8, we show that for trees that do not satisfy the asymptotic optimality conditions described in Section 6, no limited-memory best-first search algorithm is asymptotically optimal.
Reference: [4] <author> R. Dechter and J. Pearl. </author> <title> Generalized best-first search strategies and the optimality of A*. </title> <journal> JACM, </journal> <volume> 32(3) </volume> <pages> 505-536, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction The well known A* search algorithm [7, 17] is optimal in terms of number of node expansions (which is also a measure of its time complexity) in most cases <ref> [4] </ref>. However, since it requires exponential amount of memory to run, it runs out of memory even on problem instances of moderate size. To overcome the storage problem, a variant of A* called IDA* (Iterative Deepening A*) was introduced by Korf [9, 10]. <p> A node is surely generable if it is in E G or is a child of a node in E G . It is well known <ref> [4] </ref> that if A is any admissible best-first tree-search algorithm and G is a graph, then A must generate every surely generable node of G. <p> Admissible heuristic functions designed by the commonly used method of "relaxation" [19] automatically satisfy the monotonicity condition. One major problem with A* is the amount of memory required to store nodes in OPEN and CLOSED. As shown in <ref> [4] </ref>, every admissible search algorithm must expand all surely expandable 5 The length of a path P is the number of arcs in P . 5 nodes before finding a solution. <p> Since the admissible heuristics designed by the commonly used method of relaxation are also monotone, and A* expands each node at most once if the heuristic is monotone, one question that naturally arises is whether A* is optimal in terms of the number of node expansions. Dechter and Pearl <ref> [4] </ref> and Mero [16] have shown that out of the set of all admissible best-first search algorithms guided by the same admissible heuristic function h, no algorithm can be (in their terminology) 0-optimal; i.e., no algorithm can guarantee fewer node expansions than all other algorithms for all problem instances.
Reference: [5] <author> M. Evett, J. Hendler, A. Mahanti, and D. Nau. PRA*: </author> <title> A memory-limited heuristic search procedure for the connection machine. </title> <booktitle> In Frontiers'90: Frontiers of Massively Parallel Computation, </booktitle> <year> 1990. </year>
Reference-contexts: Other Algorithms on Trees. IDA* is not the only tree search algorithm that operates in limited memory. Following IDA*, several other algorithms have been developed that run in limited memory, such as MREC [21], MA* [3], RA* <ref> [5] </ref>, SMA* [20], RBFS [8], and ITS [6]. In Section 8, we show that for trees that do not satisfy the asymptotic optimality conditions described in Section 6, no limited-memory best-first search algorithm is asymptotically optimal.
Reference: [6] <author> S. Ghosh, A. Mahanti, and D. S. Nau. </author> <title> ITS: An efficient limited-memory heuristic tree search algorithm. </title> <booktitle> In AAAI 1994, </booktitle> <pages> pages 1353-1358, </pages> <year> 1994. </year>
Reference-contexts: Other Algorithms on Trees. IDA* is not the only tree search algorithm that operates in limited memory. Following IDA*, several other algorithms have been developed that run in limited memory, such as MREC [21], MA* [3], RA* [5], SMA* [20], RBFS [8], and ITS <ref> [6] </ref>. In Section 8, we show that for trees that do not satisfy the asymptotic optimality conditions described in Section 6, no limited-memory best-first search algorithm is asymptotically optimal.
Reference: [7] <author> P. E. Hart, N. J. Nilsson, and B. Raphael. </author> <title> A formal basis for the heuristic determination of minimum cost paths. </title> <journal> IEEE Transactions on Systems Sciences and Cybernetics, </journal> <pages> pages 1556-1562, </pages> <year> 1968. </year>
Reference-contexts: 1 Introduction The well known A* search algorithm <ref> [7, 17] </ref> is optimal in terms of number of node expansions (which is also a measure of its time complexity) in most cases [4]. However, since it requires exponential amount of memory to run, it runs out of memory even on problem instances of moderate size. <p> heuristic function, the heuristic branching factor is the average ratio of the number of nodes of each f -value to the the number of nodes at the next smaller f -value, averaged over all f -values h fl (s) [10]. 2.2 Algorithm A* The best known admissible algorithm is A* <ref> [7, 17] </ref>. A* works in a best-first manner. It maintains two lists: OPEN, which contains nodes that are to be expanded, and CLOSED, which contains nodes that have already been expanded.
Reference: [8] <author> R. Korf. </author> <title> Linear-space best-first search. </title> <journal> Artificial Intelligence, </journal> <volume> 62 </volume> <pages> 41-78, </pages> <year> 1993. </year>
Reference-contexts: Other Algorithms on Trees. IDA* is not the only tree search algorithm that operates in limited memory. Following IDA*, several other algorithms have been developed that run in limited memory, such as MREC [21], MA* [3], RA* [5], SMA* [20], RBFS <ref> [8] </ref>, and ITS [6]. In Section 8, we show that for trees that do not satisfy the asymptotic optimality conditions described in Section 6, no limited-memory best-first search algorithm is asymptotically optimal.
Reference: [9] <author> R. E. Korf. </author> <title> Depth first iterative deepening: An optimal admissible tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 97-109, </pages> <year> 1985. </year>
Reference-contexts: However, since it requires exponential amount of memory to run, it runs out of memory even on problem instances of moderate size. To overcome the storage problem, a variant of A* called IDA* (Iterative Deepening A*) was introduced by Korf <ref> [9, 10] </ref>. Basically, IDA* performs a depth-first search, backtracking whenever it finds a path whose cost exceeds a threshold value z. It repeats this search for larger and larger values of z, until it finds a solution. <p> Therefore, design of search algorithms that run with limited memory has significant practical importance. 2.4 Algorithm IDA* To overcome the storage problem, a variant of A* called IDA* (Iterative Deepening A*) was introduced by Korf <ref> [9, 10] </ref>. Basically, IDA* performs a depth-first search, backtracking whenever it finds a path whose cost exceeds a threshold value z. It repeats this search for larger and larger values of z, until it finds a solution. Figure 1 shows a pseudocode version of IDA*.
Reference: [10] <author> R. E. Korf. </author> <title> Optimal path finding algorithms. </title> <editor> In L. Kanal and V. Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence, </booktitle> <pages> pages 200-222. </pages> <publisher> Springer Verlag, </publisher> <year> 1988. </year>
Reference-contexts: However, since it requires exponential amount of memory to run, it runs out of memory even on problem instances of moderate size. To overcome the storage problem, a variant of A* called IDA* (Iterative Deepening A*) was introduced by Korf <ref> [9, 10] </ref>. Basically, IDA* performs a depth-first search, backtracking whenever it finds a path whose cost exceeds a threshold value z. It repeats this search for larger and larger values of z, until it finds a solution. <p> But since IDA*'s memory requirement is only linear in the depth of the search, this enables IDA* to solve much larger problems than A* can solve in practice. For example, IDA* can solve randomly generated instances of the 15-puzzle <ref> [10] </ref>. This paper presents a detailed analysis of the properties of IDA*. Our results are as follows: 1 1. IDA* on Trees. <p> Our results are as follows: 1 1. IDA* on Trees. One of IDA*'s most important properties is that under certain conditions it is "asymptotically optimal in time and space over the class of best-first searches that find optimal solutions on a tree" <ref> [10, p. 236] </ref>; i.e., on these trees it expands O (N ) nodes, where N is the number of nodes eligible for expansion by A*. 2 In this paper, we describe and correct some difficulties with this claim. <p> In particular, we show that: (a) IDA* is not asymptotically optimal in all of the cases where it was thought to be so. As shown in Section 4, there are trees satisfying all of asymptotic optimality conditions given in <ref> [10] </ref>, such that IDA* will expand more than O (N ) nodes. 3 (b) The above trees appear to be common enough to affect IDA*'s average-case performance. <p> there exists a tie-breaking rule such that if we run A* using h and the tie-breaking rule, A* will expand n. 3 Previous papers have described trees on which IDA* expands more than O (N ) nodes [13, 18], but the trees described in these papers do not satisfy Korf's <ref> [10] </ref> requirements of finite precision and non-exponential node costs. 2 (c) In Section 6, we present correct sets of necessary conditions and sufficient conditions for IDA* to be asymptotically optimal on trees. 2. IDA* on Graphs. <p> Given a state space and a monotone heuristic function, the heuristic branching factor is the average ratio of the number of nodes of each f -value to the the number of nodes at the next smaller f -value, averaged over all f -values h fl (s) <ref> [10] </ref>. 2.2 Algorithm A* The best known admissible algorithm is A* [7, 17]. A* works in a best-first manner. It maintains two lists: OPEN, which contains nodes that are to be expanded, and CLOSED, which contains nodes that have already been expanded. <p> Therefore, design of search algorithms that run with limited memory has significant practical importance. 2.4 Algorithm IDA* To overcome the storage problem, a variant of A* called IDA* (Iterative Deepening A*) was introduced by Korf <ref> [9, 10] </ref>. Basically, IDA* performs a depth-first search, backtracking whenever it finds a path whose cost exceeds a threshold value z. It repeats this search for larger and larger values of z, until it finds a solution. Figure 1 shows a pseudocode version of IDA*. <p> In other words, IDA*'s memory requirement grows only linearly with the depth of the search. This enables IDA* to solve much larger problems than A* can solve, such as the 15-puzzle <ref> [10] </ref>. Thus IDA* has drawn significant attention from the AI research community. 3 Definitions As we will see later, the quantities defined below correspond closely to the behavior of A* and IDA*. <p> the set of surely generable nodes is precisely V (z (I 1)), i.e., the set of nodes generated by IDA* in its second-to-last iteration and X (z (I 1)) is the set of surely expandable nodes. 4 IDA* on Trees: Examination of Optimality Conditions In his analysis of IDA*, Korf <ref> [10] </ref> introduces the following conditions that might or might not be satisfied by various search problems: Condition 1: h has a heuristic branching factor &gt; 1; Condition 2: the problem space grows exponentially with depth; Condition 3: representation of costs (f -values) is with finite precision; Condition 4: cost (f ) <p> G 2 G 1 1 1 n k1 s = n 0 n 2 goal1 1 1 <ref> [10] </ref> states that if Condition 1 holds, then IDA* is asymptotically optimal, i.e., it will do only O (N ) node expansions. <p> Therefore, x tot = (N lg N ). Example 4 In the examples above, we have shown that the conditions stated in <ref> [10] </ref> for the asymptotic optimality of IDA* are not sufficient. In this example, we show that these conditions are not necessary either. <p> To search this graph, IDA* will unfold 6 the space into the exact same tree shown in Figure 3. 5 IDA* on Trees: Experimental Studies In Section 4, we showed that there are trees satisfying the asymptotic optimality conditions stated for IDA* in <ref> [10] </ref>, for which IDA* is not actually asymptotically optimal. In this section, we examine the practical impact of the existence of such trees, by studying IDA*'s performance on randomly generated instances of the Traveling Salesman Problem (TSP). <p> the number of node generations by IDA* will grow exponentially faster than the number of node generations by A* if the cost of going from one city to another is allowed to grow in proportion to the square of the number of cities (which satisfies the asymptotic optimality conditions in <ref> [10] </ref>). To represent the search space and the lower bound heuristic for the Traveling Salesman Problem, we chose the well known method of Little et al. [11]. The search space in this formulation is a binary tree. <p> However, depth-first search will expand a node n many times, once for each path it finds from s to n. Thus, there are cases where depth-first search can do 2 N node expansions on a directed acyclic graph with N possibly expandable nodes <ref> [10] </ref>. Since IDA* does a depth-first search at each iteration, this means that on graphs it can also do exponentially many node expansions. More specifically, this section shows that in the worst case, IDA* does fi (2 2N ) node expansions. <p> In particular, we have presented the following results: 1. We have presented necessary and sufficient conditions for IDA* to be asymptotically optimal. Our conditions show that IDA* is asymptotically optimal in a somewhat different range of problems than was originally believed. For example, the conditions stated in <ref> [10] </ref> are not sufficient to guarantee asymptotic optimality of IDA*; i.e., IDA* will perform badly in some of the trees on which it was thought to be asymptotically optimal.
Reference: [11] <author> J. D. Little, K. G. Murty, D. W. Sweeney, and C. Karel. </author> <title> An algorithm for the traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 11 </volume> <pages> 972-989, </pages> <year> 1963. </year>
Reference-contexts: To represent the search space and the lower bound heuristic for the Traveling Salesman Problem, we chose the well known method of Little et al. <ref> [11] </ref>. The search space in this formulation is a binary tree. We generated two sets of data, which we will call TSP Set 1 and TSP Set 2, and ran both IDA* and A* on each set.
Reference: [12] <author> A. Mahanti, S. Ghosh, D. S. Nau, A. K. Pal, and L. N. Kanal. </author> <title> Performance of IDA* on trees and graphs. </title> <booktitle> In AAAI 1992, </booktitle> <pages> pages 539-544, </pages> <year> 1992. </year>
Reference-contexts: Our experimental results, which are presented in Section 5, suggest that for certain kinds of Traveling Salesman Problems, IDA*'s average-case performance is not asymptotically optimal. 1 Some of these results have also been summarized briefly in <ref> [12] </ref>. 2 A node n is eligible for expansion by A* under a given heuristic h, if there exists a tie-breaking rule such that if we run A* using h and the tie-breaking rule, A* will expand n. 3 Previous papers have described trees on which IDA* expands more than O
Reference: [13] <author> A. Mahanti and A. K. Pal. </author> <title> Worst-case time complexity of IDA*. </title> <booktitle> In Tenth International Conference in Computer Science, </booktitle> <address> Santiago, Chile, </address> <month> July </month> <year> 1990. </year> <month> 39 </month>
Reference-contexts: n is eligible for expansion by A* under a given heuristic h, if there exists a tie-breaking rule such that if we run A* using h and the tie-breaking rule, A* will expand n. 3 Previous papers have described trees on which IDA* expands more than O (N ) nodes <ref> [13, 18] </ref>, but the trees described in these papers do not satisfy Korf's [10] requirements of finite precision and non-exponential node costs. 2 (c) In Section 6, we present correct sets of necessary conditions and sufficient conditions for IDA* to be asymptotically optimal on trees. 2. IDA* on Graphs. <p> Since there are N 0 nodes in G 2 and G 1 also contains N 0 nodes, the total number of node expansions by IDA* is clearly (N 02 ) = (N 2 ). This result also holds for several other types of trees <ref> [13] </ref>. The example above satisfies the primary condition, and all secondary conditions except Condition 2: the search space is not exponential in the solution depth.
Reference: [14] <author> A. Mahanti and K. Ray. </author> <title> Network search algorithms with modifiable heuristics. </title> <editor> In L. Kanal and V. Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence, </booktitle> <pages> pages 200-222. </pages> <publisher> Springer Verlag, </publisher> <year> 1988. </year>
Reference-contexts: For most problems of non-trivial size, A* runs out of memory before any significant amount of execution time. This limitation also prevails in the case of other known variants of A* (B [15], GRAPHSEARCH [17], C [1], PropC [2], MarkA [2], B' [16], D <ref> [14] </ref>, etc.), which are not tailored to run with limited memory. Therefore, design of search algorithms that run with limited memory has significant practical importance. 2.4 Algorithm IDA* To overcome the storage problem, a variant of A* called IDA* (Iterative Deepening A*) was introduced by Korf [9, 10].
Reference: [15] <author> A. Martelli. </author> <title> On the complexity of admissible search algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 8 </volume> <pages> 1-13, </pages> <year> 1977. </year>
Reference-contexts: For example, A*'s major bottleneck in practical use is its storage requirement. For most problems of non-trivial size, A* runs out of memory before any significant amount of execution time. This limitation also prevails in the case of other known variants of A* (B <ref> [15] </ref>, GRAPHSEARCH [17], C [1], PropC [2], MarkA [2], B' [16], D [14], etc.), which are not tailored to run with limited memory.
Reference: [16] <author> L. Mero. </author> <title> A heuristic search algorithm with modifiable estimate. </title> <journal> Artificial Intelligence, </journal> <volume> 23 </volume> <pages> 13-27, </pages> <year> 1984. </year>
Reference-contexts: Dechter and Pearl [4] and Mero <ref> [16] </ref> have shown that out of the set of all admissible best-first search algorithms guided by the same admissible heuristic function h, no algorithm can be (in their terminology) 0-optimal; i.e., no algorithm can guarantee fewer node expansions than all other algorithms for all problem instances. <p> For most problems of non-trivial size, A* runs out of memory before any significant amount of execution time. This limitation also prevails in the case of other known variants of A* (B [15], GRAPHSEARCH [17], C [1], PropC [2], MarkA [2], B' <ref> [16] </ref>, D [14], etc.), which are not tailored to run with limited memory. Therefore, design of search algorithms that run with limited memory has significant practical importance. 2.4 Algorithm IDA* To overcome the storage problem, a variant of A* called IDA* (Iterative Deepening A*) was introduced by Korf [9, 10].
Reference: [17] <author> N. J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga, </publisher> <address> Palo Alto, </address> <year> 1980. </year>
Reference-contexts: 1 Introduction The well known A* search algorithm <ref> [7, 17] </ref> is optimal in terms of number of node expansions (which is also a measure of its time complexity) in most cases [4]. However, since it requires exponential amount of memory to run, it runs out of memory even on problem instances of moderate size. <p> heuristic function, the heuristic branching factor is the average ratio of the number of nodes of each f -value to the the number of nodes at the next smaller f -value, averaged over all f -values h fl (s) [10]. 2.2 Algorithm A* The best known admissible algorithm is A* <ref> [7, 17] </ref>. A* works in a best-first manner. It maintains two lists: OPEN, which contains nodes that are to be expanded, and CLOSED, which contains nodes that have already been expanded. <p> For example, A*'s major bottleneck in practical use is its storage requirement. For most problems of non-trivial size, A* runs out of memory before any significant amount of execution time. This limitation also prevails in the case of other known variants of A* (B [15], GRAPHSEARCH <ref> [17] </ref>, C [1], PropC [2], MarkA [2], B' [16], D [14], etc.), which are not tailored to run with limited memory.
Reference: [18] <author> B. G. Patrick, M. Almulla, and M. M. Newborn. </author> <title> An upper bound on the complexity of iterative-deepening-A*. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 5 </volume> <pages> 265-278, </pages> <year> 1992. </year>
Reference-contexts: n is eligible for expansion by A* under a given heuristic h, if there exists a tie-breaking rule such that if we run A* using h and the tie-breaking rule, A* will expand n. 3 Previous papers have described trees on which IDA* expands more than O (N ) nodes <ref> [13, 18] </ref>, but the trees described in these papers do not satisfy Korf's [10] requirements of finite precision and non-exponential node costs. 2 (c) In Section 6, we present correct sets of necessary conditions and sufficient conditions for IDA* to be asymptotically optimal on trees. 2. IDA* on Graphs. <p> For the case where G is a tree and h is an admissible heuristic function, it has been shown that IDA* will do no more than (N + N 2 )=2 node expansions in the worst case <ref> [18] </ref>.
Reference: [19] <author> J. Pearl. </author> <title> Heuristics. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: Besides its admissibility, A* has the property that if the heuristic function is monotone, then A* never expands any node more than once. Admissible heuristic functions designed by the commonly used method of "relaxation" <ref> [19] </ref> automatically satisfy the monotonicity condition. One major problem with A* is the amount of memory required to store nodes in OPEN and CLOSED.
Reference: [20] <author> S. Russell. </author> <title> Efficient memory-bounded search methods. </title> <booktitle> In ECAI-1992, </booktitle> <address> Vienna, Austria, </address> <year> 1992. </year>
Reference-contexts: Other Algorithms on Trees. IDA* is not the only tree search algorithm that operates in limited memory. Following IDA*, several other algorithms have been developed that run in limited memory, such as MREC [21], MA* [3], RA* [5], SMA* <ref> [20] </ref>, RBFS [8], and ITS [6]. In Section 8, we show that for trees that do not satisfy the asymptotic optimality conditions described in Section 6, no limited-memory best-first search algorithm is asymptotically optimal.
Reference: [21] <author> A. Sen and A. Bagchi. </author> <title> Fast recursive formulations for best-first search that allow controlled use of memory. </title> <booktitle> In IJCAI-89, </booktitle> <pages> pages 274-277, </pages> <year> 1989. </year> <month> 40 </month>
Reference-contexts: Other Algorithms on Trees. IDA* is not the only tree search algorithm that operates in limited memory. Following IDA*, several other algorithms have been developed that run in limited memory, such as MREC <ref> [21] </ref>, MA* [3], RA* [5], SMA* [20], RBFS [8], and ITS [6]. In Section 8, we show that for trees that do not satisfy the asymptotic optimality conditions described in Section 6, no limited-memory best-first search algorithm is asymptotically optimal.
References-found: 21

