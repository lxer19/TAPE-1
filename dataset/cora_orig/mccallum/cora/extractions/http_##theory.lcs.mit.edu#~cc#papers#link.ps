URL: http://theory.lcs.mit.edu/~cc/papers/link.ps
Refering-URL: http://theory.lcs.mit.edu/~cc/publications.html
Root-URL: 
Email: E-mail: fcachin,maurerg@inf.ethz.ch  
Title: Linking Information Reconciliation and Privacy Amplification  
Author: Christian Cachin and Ueli M. Maurer 
Note: Preprint, 30-Oct-95. To appear in Journal of Cryptology.  
Address: CH-8092 Zurich, Switzerland  
Affiliation: Institute for Theoretical Computer Science ETH Zurich  
Abstract: Information reconciliation allows two parties knowing correlated random variables, such as a noisy version of the partner's random bit string, to agree on a shared string. Privacy amplification allows two parties sharing a partially secret string about which an opponent has some partial information, to distill a shorter but almost completely secret key by communicating only over an insecure channel, as long as an upper bound on the opponent's knowledge about the string is known. The relation between these two techniques has not been well understood. In particular, it is important to understand the effect of side-information, obtained by the opponent through an initial reconciliation step, on the size of the secret key that can be distilled safely by subsequent privacy amplification. The purpose of this paper is to provide the missing link between these techniques by presenting bounds on the reduction of the Renyi entropy of a random variable induced by side-information. We show that, except with negligible probability, each bit of side information reduces the size of the key that can be safely distilled by at most two bits. Moreover, in the important special case of side information and raw key data generated by many independent repetitions of a random experiment, each bit of side information reduces the size of the secret key by only about one bit. The results have applications in unconditionally secure key agreement protocols and in quantum cryptography.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. H. Bennett, F. Bessette, G. Brassard, L. Salvail, and J. Smolin, </author> <title> "Experimental quantum cryptography," </title> <journal> Journal of Cryptology, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 3-28, </pages> <year> 1992. </year>
Reference-contexts: This problem can be solved by applying public-key cryptography [9], where one assumes that Eve's computing power is limited and that certain problems are computationally difficult. In the recent years, key agreement protocols have been developed that are secure against adversaries with unlimited computing power <ref> [1, 10] </ref>. The motivation for investigating such protocols is two-fold: First, one avoids having to worry about the generality of a particular computational model, which is of some concern in view of the potential realizability of quantum computers [5, 13]. <p> Unconditionally secure secret-key agreement [10, 11] takes place in a scenario where Alice, Bob and Eve know the correlated random variables X; Y and Z, respectively, distributed according to some joint probability distribution that may be under partial control of Eve (as for instance in quantum cryptography <ref> [1] </ref>). One possible scenario considered by Maurer [10] is that X; Y and Z result from a binary random string broadcast by a satellite and received by Alice, Bob and Eve over noisy channels. <p> Alice and Bob create W by exchanging messages, summarized as the random variable C, over the public channel. Information Reconciliation <ref> [1, 6] </ref>: To agree on a string T with very high probability, Alice and Bob exchange redundant error-correction information U , such as a sequence of parity checks. After this phase, Eve's (incomplete) information about T consists of Z, C and U . <p> Although this question is fundamental for any proof in the area of key agreement 2 protocols, it has previously not been well understood because the behavior of Renyi entropy is different from that of Shannon entropy with respect to side-information. Existing proofs such as the ingenious Big-Brother argument of <ref> [1] </ref> work only for particular probability distributions and reconciliation protocols. The paper is organized as follows. Section 2 reviews privacy amplification and the definition of Renyi entropy. Section 3 presents upper bounds on the reduction of Renyi entropy due to side-information for arbitrary probability distributions.
Reference: [2] <author> C. H. Bennett, G. Brassard, C. Crepeau, and U. M. Maurer, </author> <title> "Generalized privacy amplification," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 41, </volume> <month> Nov. </month> <year> 1995. </year> <note> (To appear). </note>
Reference-contexts: Information Reconciliation [1, 6]: To agree on a string T with very high probability, Alice and Bob exchange redundant error-correction information U , such as a sequence of parity checks. After this phase, Eve's (incomplete) information about T consists of Z, C and U . Privacy Amplification <ref> [2, 3] </ref>: In the final phase, Alice and Bob agree publicly on a compression function G to distill from T a shorter string S about which Eve has only a negligible amount of information. Therefore, S can subsequently be used as a secret key. <p> In particular, they know a lower bound on the Renyi entropy (see below) of the distribution P W jV =v with high probability but they do not know v. It is known <ref> [2] </ref> that the Renyi entropy after reconciliation with U = u (i.e., of the distribution P W jV =v;U=u ) is a lower bound on the size of the secret key that can be distilled safely by privacy amplification. <p> In privacy amplification, a different entropy measure, Renyi entropy, is of central importance <ref> [2] </ref>. To distinguish Renyi entropy from entropy in the sense of Shannon, we will always refer to the latter as Shannon entropy. All logarithms in this paper are to the base 2, and entropies are thus measured in bits. <p> All logarithms in this paper are to the base 2, and entropies are thus measured in bits. Privacy amplification was introduced by Bennett, Brassard and Robert [3] and investigated further in <ref> [2] </ref>, and can be described as follows. <p> Bennett, Brassard, Crepeau and Maurer <ref> [2] </ref> showed that the Renyi entropy (defined below) of Eve's distribution about W provides a lower bound on the size r of the secret key distillable from W by privacy amplification with a universal hash function. Definition 1. <p> The collision probability P c (X) of X is defined as the probability that X takes on the same value twice in two independent experiments: P c (X) = x2X The Renyi entropy of order two (or "Renyi entropy" for short) of X <ref> [12, 2] </ref> is defined as the negative logarithm of the collision probability of X: R (X) = log P c (X): For an event E, the Renyi entropy of X conditioned on E, R (XjE), is defined naturally as the Renyi entropy of the conditional distribution P XjE . <p> Similarly, we have H (XjY ) R (XjY ). Note that Renyi entropy (like Shannon entropy) is always positive. The following theorem is the main result of <ref> [2] </ref>: Theorem 1. Let X be a random variable on alphabet X with probability distribution P X and Renyi entropy R (X). <p> It should be pointed out that Theorem 1 cannot be generalized to Renyi entropy conditioned on a random variable, i.e., R (G (W )jGV ) r 2 rR (W jV ) = ln 2 is false in general <ref> [2] </ref>. 3 The Effect of Side Information on Renyi Entropy As described above, the reconciliation step consists of Alice and Bob exchanging suitable error-correction information U over the public channel. This information decreases Eve's Shannon entropy and usually also her Renyi entropy about W . <p> In contrast to Shannon entropy, the expected Renyi entropy can increase when side information is revealed, i.e., R (X) &lt; R (XjU ) is possible. This property is used in <ref> [2, 11] </ref> to prove that a key larger than suggested by Theorem 1 can be obtained by privacy amplification. The proof makes use of a conceptual oracle that is assumed to provide special side information U to Eve, called spoiling knowledge, which increases her Renyi entropy.
Reference: [3] <author> C. H. Bennett, G. Brassard, and J.-M. Robert, </author> <title> "Privacy amplification by public discussion," </title> <journal> SIAM Journal on Computing, </journal> <volume> vol. 17, </volume> <pages> pp. 210-229, </pages> <month> Apr. </month> <year> 1988. </year>
Reference-contexts: Information Reconciliation [1, 6]: To agree on a string T with very high probability, Alice and Bob exchange redundant error-correction information U , such as a sequence of parity checks. After this phase, Eve's (incomplete) information about T consists of Z, C and U . Privacy Amplification <ref> [2, 3] </ref>: In the final phase, Alice and Bob agree publicly on a compression function G to distill from T a shorter string S about which Eve has only a negligible amount of information. Therefore, S can subsequently be used as a secret key. <p> To distinguish Renyi entropy from entropy in the sense of Shannon, we will always refer to the latter as Shannon entropy. All logarithms in this paper are to the base 2, and entropies are thus measured in bits. Privacy amplification was introduced by Bennett, Brassard and Robert <ref> [3] </ref> and investigated further in [2], and can be described as follows. <p> This process transforms a partially secret n-bit string W into a highly secret but shorter r-bit string g (W ) which can be used as a secret key. The method for selecting the function g proposed in <ref> [3] </ref> is to choose it at random from a publicly-known universal class of hash functions mapping n-bit strings to r-bit strings. Universal hash functions were introduced by Carter and Wegman [7].
Reference: [4] <author> R. E. </author> <title> Blahut, </title> <booktitle> Principles and Practice of Information Theory. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: applied in Section 5 to analyze the important class of scenarios in which a given random experiment is repeated many times independently. 2 Review of Privacy Amplification and Renyi Entropy We assume that the reader is familiar with the notion of entropy and the basic concepts of Shannon's information theory <ref> [4, 8] </ref>. In privacy amplification, a different entropy measure, Renyi entropy, is of central importance [2]. To distinguish Renyi entropy from entropy in the sense of Shannon, we will always refer to the latter as Shannon entropy. <p> Furthermore, all sequences in the typical set are almost equally probable. This allows us to bound the decrease of Renyi entropy by the results of the last section. In the following we will make use of strongly typical sequences <ref> [4] </ref>. Consider a probability distribution P X over some finite set X where we assume P X (x) &gt; 0 for all x 2 X .
Reference: [5] <editor> G. Brassard, </editor> <booktitle> "A quantum jump in computer science," in Computer Science Today (J. </booktitle> <editor> van Leeuwen, ed.), </editor> <volume> vol. </volume> <booktitle> 1000 of Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: The motivation for investigating such protocols is two-fold: First, one avoids having to worry about the generality of a particular computational model, which is of some concern in view of the potential realizability of quantum computers <ref> [5, 13] </ref>. Second, no strong rigorous results on the difficulty of breaking a cryptosystem have fl This research was supported by the Swiss National Science Foundation.
Reference: [6] <author> G. Brassard and L. Salvail, </author> <title> "Secret-key reconciliation by public discussion," </title> <booktitle> in Advances in Cryptology | EUROCRYPT '93 (T. </booktitle> <editor> Helleseth, ed.), </editor> <volume> vol. </volume> <booktitle> 765 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 410-423, </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Alice and Bob create W by exchanging messages, summarized as the random variable C, over the public channel. Information Reconciliation <ref> [1, 6] </ref>: To agree on a string T with very high probability, Alice and Bob exchange redundant error-correction information U , such as a sequence of parity checks. After this phase, Eve's (incomplete) information about T consists of Z, C and U .
Reference: [7] <author> J. L. Carter and M. N. Wegman, </author> <title> "Universal classes of hash functions," </title> <journal> Journal of Computer and System Sciences, </journal> <volume> vol. 18, </volume> <pages> pp. 143-154, </pages> <year> 1979. </year> <month> 11 </month>
Reference-contexts: The method for selecting the function g proposed in [3] is to choose it at random from a publicly-known universal class of hash functions mapping n-bit strings to r-bit strings. Universal hash functions were introduced by Carter and Wegman <ref> [7] </ref>. A class G of functions A ! B is called universal if, for any distinct x 1 and x 2 in A, the probability that g (x 1 ) = g (x 2 ) is at most 1=jBj when g is chosen at random with uniform distribution from G.
Reference: [8] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: applied in Section 5 to analyze the important class of scenarios in which a given random experiment is repeated many times independently. 2 Review of Privacy Amplification and Renyi Entropy We assume that the reader is familiar with the notion of entropy and the basic concepts of Shannon's information theory <ref> [4, 8] </ref>. In privacy amplification, a different entropy measure, Renyi entropy, is of central importance [2]. To distinguish Renyi entropy from entropy in the sense of Shannon, we will always refer to the latter as Shannon entropy. <p> Shannon entropy H (X) can be expressed similarly as H (X) = E [log P X (X)]. It follows from Jensen's inequality (see <ref> [8] </ref>) that Renyi entropy is upper bounded by the Shannon entropy, a fact known to Renyi: R (X) H (X); with equality if and only if P X is the uniform distribution over X or a subset of X . Similarly, we have H (XjY ) R (XjY ). <p> c (XjU = u)] X P U (u) [log P U (u) + log P c (XjU = u)] X P U (u) log P U (u) u2U = H (U ) + u2U = H (U ) + R (XjU ); where the second inequality follows from Jensen's inequality <ref> [8] </ref> which holds with equality if and only if P U is the uniform distribution over U or a subset of U . In contrast to Shannon entropy, the expected Renyi entropy can increase when side information is revealed, i.e., R (X) &lt; R (XjU ) is possible.
Reference: [9] <author> W. Diffie and M. E. Hellman, </author> <title> "New directions in cryptography," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 22, </volume> <pages> pp. 644-654, </pages> <month> Nov. </month> <year> 1976. </year>
Reference-contexts: It is easy to see that if this public channel is not assumed to be authenticated, then such key agreement is impossible. We therefore assume that any modification or insertion of messages can be detected by Alice and Bob. This problem can be solved by applying public-key cryptography <ref> [9] </ref>, where one assumes that Eve's computing power is limited and that certain problems are computationally difficult. In the recent years, key agreement protocols have been developed that are secure against adversaries with unlimited computing power [1, 10].
Reference: [10] <author> U. M. Maurer, </author> <title> "Secret key agreement by public discussion from common information," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 39, </volume> <pages> pp. 733-742, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: This problem can be solved by applying public-key cryptography [9], where one assumes that Eve's computing power is limited and that certain problems are computationally difficult. In the recent years, key agreement protocols have been developed that are secure against adversaries with unlimited computing power <ref> [1, 10] </ref>. The motivation for investigating such protocols is two-fold: First, one avoids having to worry about the generality of a particular computational model, which is of some concern in view of the potential realizability of quantum computers [5, 13]. <p> A preliminary version of this paper was presented at Eurocrypt '94, May 9-12, Perugia, Italy. 1 been proved, and this problem continues to be among the most difficult ones in complexity theory. Unconditionally secure secret-key agreement <ref> [10, 11] </ref> takes place in a scenario where Alice, Bob and Eve know the correlated random variables X; Y and Z, respectively, distributed according to some joint probability distribution that may be under partial control of Eve (as for instance in quantum cryptography [1]). <p> One possible scenario considered by Maurer <ref> [10] </ref> is that X; Y and Z result from a binary random string broadcast by a satellite and received by Alice, Bob and Eve over noisy channels. Secret-key agreement is possible even when Eve's channel is much more reliable than Alice's and Bob's channels. <p> Secret-key agreement is possible even when Eve's channel is much more reliable than Alice's and Bob's channels. A key agreement protocol for such a scenario generally consists of three phases: Advantage Distillation: <ref> [10] </ref> The purpose of the first phase is to create a random variable W about which both Alice or Bob have more information than Eve. <p> Due to their wide-spread use linear error correcting codes are most likely to be used during the reconcilication phase. Theorem 9 can replace the spoiling knowledge argument in Maurer's proof [11] that the known results on secret key rate <ref> [10] </ref> hold also for a much stronger notion of secrecy. 6 Conclusions The described link between information reconciliation and privacy amplification for unconditionally-secure secret-key agreement can be summarized as follows.
Reference: [11] <author> U. M. Maurer, </author> <title> "The strong secret key rate of discrete random triples," in Communications and Cryptography: Two Sides of One Tapestry (R. </title> <editor> E. Blahut et al., eds.), </editor> <publisher> Kluwer, </publisher> <year> 1994. </year>
Reference-contexts: A preliminary version of this paper was presented at Eurocrypt '94, May 9-12, Perugia, Italy. 1 been proved, and this problem continues to be among the most difficult ones in complexity theory. Unconditionally secure secret-key agreement <ref> [10, 11] </ref> takes place in a scenario where Alice, Bob and Eve know the correlated random variables X; Y and Z, respectively, distributed according to some joint probability distribution that may be under partial control of Eve (as for instance in quantum cryptography [1]). <p> In contrast to Shannon entropy, the expected Renyi entropy can increase when side information is revealed, i.e., R (X) &lt; R (XjU ) is possible. This property is used in <ref> [2, 11] </ref> to prove that a key larger than suggested by Theorem 1 can be obtained by privacy amplification. The proof makes use of a conceptual oracle that is assumed to provide special side information U to Eve, called spoiling knowledge, which increases her Renyi entropy. <p> Due to their wide-spread use linear error correcting codes are most likely to be used during the reconcilication phase. Theorem 9 can replace the spoiling knowledge argument in Maurer's proof <ref> [11] </ref> that the known results on secret key rate [10] hold also for a much stronger notion of secrecy. 6 Conclusions The described link between information reconciliation and privacy amplification for unconditionally-secure secret-key agreement can be summarized as follows.
Reference: [12] <author> A. Renyi, </author> <title> "On measures of entropy and information," </title> <booktitle> in Proc. 4th Berkeley Symp. Math. Statist. Prob., </booktitle> <volume> vol. 1, </volume> <pages> (Berkeley), pp. 547-561, </pages> <institution> Univ. of Calif. Press, </institution> <year> 1961. </year>
Reference-contexts: The collision probability P c (X) of X is defined as the probability that X takes on the same value twice in two independent experiments: P c (X) = x2X The Renyi entropy of order two (or "Renyi entropy" for short) of X <ref> [12, 2] </ref> is defined as the negative logarithm of the collision probability of X: R (X) = log P c (X): For an event E, the Renyi entropy of X conditioned on E, R (XjE), is defined naturally as the Renyi entropy of the conditional distribution P XjE .
Reference: [13] <author> P. W. Shor, </author> <title> "Algorithms for quantum computation: Discrete log and factoring," </title> <booktitle> in Proc. 35th IEEE Symposium on Foundations of Computer Science (FOCS), </booktitle> <pages> pp. 124-134, </pages> <year> 1994. </year> <month> 12 </month>
Reference-contexts: The motivation for investigating such protocols is two-fold: First, one avoids having to worry about the generality of a particular computational model, which is of some concern in view of the potential realizability of quantum computers <ref> [5, 13] </ref>. Second, no strong rigorous results on the difficulty of breaking a cryptosystem have fl This research was supported by the Swiss National Science Foundation.
References-found: 13

