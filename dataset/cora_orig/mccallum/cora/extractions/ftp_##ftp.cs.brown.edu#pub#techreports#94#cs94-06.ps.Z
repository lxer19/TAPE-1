URL: ftp://ftp.cs.brown.edu/pub/techreports/94/cs94-06.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-94-06.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Boggess, L., Agarwal, R. and Davis, R. </author> <title> Disambiguation of prepositional phrases in automatically labeled technical text. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1991, </year> <pages> 155-159. </pages>
Reference: 2. <author> Brill, E. </author> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing. </booktitle> <year> 1992. </year>
Reference: 3. <author> Charniak, E. </author> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: This is surely true, but to what degree? To suggest an answer to this question, note that there is an independent measure of the quality of tag context models, their per-tag cross entropy. We do not go into detail here 8 (see <ref> [3] </ref>) but simply note that, given a well-behaved corpus of n words, the per-tag cross entropy is well approximated by n The lower the cross entropy the better the model. So let us look at how tagging improves as the cross entropy decreases.
Reference: 4. <author> Charniak, E., Hendrickson, C., Jacobson, N. and Perkowitz, M. </author> <title> Equations for part-of-speech tagging. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1993, </year> <pages> 784-789. </pages>
Reference-contexts: For our tag set, the cross entropy of this model is 3.61 bits/tag. It is common knowledge that such models give about 90% tagging accuracy. A result of 91.5% is given in <ref> [4] </ref> and that is the figure we use here. 2 Next, consider a common tag context model: P (v i j v 1;i1 ) P (v i j v i1 ).
Reference: 5. <author> Church, K. W. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Second Conference on Applied Natural Language Processing. ACL, </booktitle> <year> 1988, </year> <pages> 136-143. </pages>
Reference: 6. <author> DeRose, S. J. </author> <title> Grammatical category disambiguation by statistical optimization. </title> <booktitle> Computational Linguistics 14 (1988), </booktitle> <pages> 31-39. </pages>
Reference: 7. <author> Francis, W. N. and Ku cera, H. </author> <title> Frequency Analysis of English Usage: Lexicon and Grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1982. </year>
Reference-contexts: 95.9% 1.0 N/A Viterbi Pure 99.6% 99.5% N/A 1.0 1231 Parser Viterbi + 99.2% 99.5% 95.9% 1.0 1246 Parser All Tagger 100% 100% 96.1% 2.15 4988 + Parser 5 Results Training and testing of taggers and the PCFG were done on a 307885 word subset of the tagged Brown Corpus <ref> [7] </ref>. This includes all sentences of length greater than 1 (i.e., sentences having a symbol other than the final punctuation mark) and less than 23 that do not include foreign words, titles, or certain symbols, most notably parentheses.
Reference: 8. <author> Jelinek, F. </author> <title> Markov source modeling of text generation. In The Impact of Processing Techniques on Communications, </title> <editor> J. K. Skwirzinski, Ed. Ni-jhoff, </editor> <address> Dordrecht, </address> <year> 1985. </year>
Reference: 9. <author> Kupiec, J. and Maxwell, J. </author> <title> Training stochastic grammars from unlabeled text corpora. </title> <booktitle> In Workshop Notes, Statistically-Based NLP Techniques. AAAI, 1992, </booktitle> <volume> 14-19. 13 10. </volume> <editor> deMarcken, C. G. </editor> <booktitle> Parsing the LOB corpus. In Proceedings of the 1990 Conference of the Association for Computational Linguistics. </booktitle> <year> 1990, </year> <pages> 243-259. </pages>
Reference: 11. <author> Marcus, M. P., Santorini, B. and Marcinkiewicz, M. A. </author> <title> Building a large annotated corpus of English: the Penn treebank. </title> <booktitle> Computational Linguistics 19 (1993), </booktitle> <pages> 313-330. </pages>
Reference-contexts: However, to measure this one needs a source of agreed upon parses for the sentences. While there are now tree-banks of some size <ref> [11] </ref>, they of necessity make assumptions about grammatical formalism. As these assumptions do not fit the grammar we use, we cannot exploit these resources. On the other hand, using tagging accuracy for our performance measure has some advantages.
Reference: 12. <author> Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L. and Palmucci, J. </author> <title> Coping with ambiguity and unknown words through probabilistic models. </title> <booktitle> Computational Linguistics 19 (1993), </booktitle> <pages> 359-382. </pages>
Reference-contexts: return a single best tag for each word (we call these "single taggers"), there has been some work on asking the tagger to return a list of possible tags in those cases where a second (or even third) best might be close to the best according to the tagger's metric <ref> [10, 12] </ref> (we call these "multiple taggers"). One obvious reason to do this would be to let the parser make the final decision. <p> One obvious reason to do this would be to let the parser make the final decision. For example, the section on multiple taggers in <ref> [12] </ref> starts by observing that even with a rather low error rate of 3.7% there are cases in which the system returns the wrong tag, which can be fatal for a parsing system trying to deal with sentences averaging more than 20 words in length.([12], pp. 366) In this paper we <p> This kind of tagger returns the tag sequence v 1;n that maximizes P (v 1;n j w 1;n ), where w 1;n is a sequence of n words and v 1;n are the corresponding n tags. Second, we built a Markov-model forward-backward tagger (as in <ref> [12] </ref>). This sort of tagger computes P (v i j w 1;n ) for each tag, and it thus is capable of identifying alternative tags at a position with word probabilities close to the best.
Reference: 13. <author> Zernik, U. Shipping departments vs. shipping pacemakers: </author> <title> using thematic analysis to improve tagging accuracy. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence. </booktitle> <year> 1992, </year> <pages> 335-342. 14 </pages>
Reference-contexts: 1 Introduction Recent years have seen a spate of research on various techniques for "tagging" | assigning a part of speech to each word in a text <ref> [1,2,5,6,8,9,10,12, 13] </ref>. One justification for such research is that a tagger can serve as a front end to a parser: the tagger assigns the tags to the incoming words and thus the parser can work at the tag level, where parsers do best. <p> However, one should not read too much into this result. First, it does not preclude further improvements in tagging. Work that looks for such 11 improvements from collecting finer statistics based upon more lexical infor-mation still seems promising (e.g., <ref> [13] </ref>). Second, our result certainly does not imply that parsers are useless. One does not parse to get tags. One parses to find phrase markers. We may have ruled out multiple taggers as a route to improved parsing accuracy, but the need for parsers remains.
References-found: 12

