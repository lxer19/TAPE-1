URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR97714-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Scheduling Block-Cyclic Array Redistribution  
Author: Frederic Desprez Jack Dongarra ; Antoine Petitet Cyril Randriamaro and Yves Robert 
Keyword: Key-words: distributed arrays, redistribution, block-cyclic distribution, scheduling, MPI, HPF.  
Address: 69364 Lyon Cedex 07, France  Knoxville, TN 37996-1301, USA  Oak Ridge, TN 37831, USA  
Affiliation: 1 LIP, Ecole Normale Superieure de Lyon,  Department of Computer Science, University of Tennessee,  Mathematical Sciences Section, Oak Ridge National Laboratory,  
Email: e-mail: [desprez, crandria]@lip.ens-lyon.fr e-mail: [dongarra, petitet, yrobert]@cs.utk.edu  
Phone: 2  3  
Date: February 1997  
Abstract: This article is devoted to the run-time redistribution of arrays that are distributed in a block-cyclic fashion over a multidimensional processor grid. While previous studies have concentrated on efficiently generating the communication messages to be exchanged by the processors involved in the redistribution, we focus on the scheduling of those messages: how to organize the message exchanges into "structured" communication steps that minimize contention. We build upon results of Walker and Otto, who solved a particular instance of the problem, and we derive an optimal scheduling for the most general case, namely, moving from a CYCLIC(r) distribution on a P -processor grid to a CYCLIC(s) distribution on a Q-processor grid, for arbitrary values of the redistribution parameters P , Q, r, and s. fl This work was supported in part by the National Science Foundation Grant No. ASC-9005933; by the Defense Advanced Research Projects Agency under contract DAAH04-95-1-0077, administered by the Army Research Office; by the Department of Energy Office of Computational and Technology Research, Mathematical, Information, and Computational Sciences Division under Contract DE-AC05-84OR21400; by the National Science Foundation Science and Technology Center Cooperative Agreement No. CCR-8809615; by the CNRS-ENS Lyon-INRIA project ReMaP; and by the Eureka Project EuroTOPS. Yves Robert is on leave from Ecole Normale Superieure de Lyon and is partly supported by DRET/DGA under contract ERE 96-1104/A000/DRET/DS/SR. The authors acknowledge the use of the Intel Paragon XP/S 5 computer, located in the Oak Ridge National Laboratory Center for Computational Sciences, funded by the Department of Energy's Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ancourt, F. Coelho, F. Irigoin, and R. Keryell. </author> <title> A linear algebra framework for static HPF code distribution. </title> <journal> Scientific programming, </journal> <note> to appear. Avalaible as CRI-Ecole des Mines Technical Report A-278-CRI, and at http://www.cri.ensmp.fr. </note>
Reference-contexts: Sophisticated techniques involve finite-state machines (see Chatterjee et al. [3]), set-theoretic methods (see Gupta et al. [8]), Diophantine equations (see Kennedy et al. [11, 12]), Hermite forms and lattices (see Thirumalai and Ramanujam [18]), or linear programming (see Ancourt et al. <ref> [1] </ref>). A comparative survey of these algorithms can be found in Wang et al. [22], where it is reported that the most powerful algorithms can handle block-cyclic distributions as efficiently as the simpler case of pure cyclic or full-block mapping.
Reference: [2] <author> Claude Berge. Graphes et hypergraphes. Dunod, </author> <year> 1970. </year>
Reference-contexts: Back to Example 4 In this example, P = 12, Q = 8, r = 4 and s = 3. We have g = 24. Take p = 11 (as in the proof of Lemma 2). If q = 0, f (p; q) = 4 =2 <ref> [3; 2] </ref>, and q receives no message from p. But if q = 6, f (p; q) = 2 2 [3; 2], and q does receive a message (see Table 10 to check this). <p> We have g = 24. Take p = 11 (as in the proof of Lemma 2). If q = 0, f (p; q) = 4 =2 <ref> [3; 2] </ref>, and q receives no message from p. But if q = 6, f (p; q) = 2 2 [3; 2], and q does receive a message (see Table 10 to check this). <p> The degree of G, defined as the maximum degree of its vertices, is d G = maxfm R ; m C g. According to Konig's edge coloring theorem, the edge coloring number of a bipartite graph is equal to its degree (see [7, vol. 2, p.1666] or Berge <ref> [2, p. 238] </ref>). This means that the edges of a bipartite graph can be partitioned in d G disjoint edge matchings. A constructive proof is as follows: repeatedly extract from E a maximum matching that saturates all maximum degree nodes. <p> A constructive proof is as follows: repeatedly extract from E a maximum matching that saturates all maximum degree nodes. At each iteration, the existence of such a maximum matching is guaranteed (see Berge <ref> [2, p. 130] </ref>). To define the schedule, we simply let the matchings at each iteration represent the communication steps. Remark 2 The proof of Theorem 1 gives a bound for the complexity of determining the optimal number of steps.
Reference: [3] <author> S. Chatterjee, J. R. Gilbert, F. J. E. Long, R. Schreiber, and S.-H. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 26(1) </volume> <pages> 72-84, </pages> <year> 1995. </year>
Reference-contexts: Recently, however, several algorithms have been published that handle general block-cyclic CYCLIC (k) distributions. Sophisticated techniques involve finite-state machines (see Chatterjee et al. <ref> [3] </ref>), set-theoretic methods (see Gupta et al. [8]), Diophantine equations (see Kennedy et al. [11, 12]), Hermite forms and lattices (see Thirumalai and Ramanujam [18]), or linear programming (see Ancourt et al. [1]). <p> Back to Example 4 In this example, P = 12, Q = 8, r = 4 and s = 3. We have g = 24. Take p = 11 (as in the proof of Lemma 2). If q = 0, f (p; q) = 4 =2 <ref> [3; 2] </ref>, and q receives no message from p. But if q = 6, f (p; q) = 2 2 [3; 2], and q does receive a message (see Table 10 to check this). <p> We have g = 24. Take p = 11 (as in the proof of Lemma 2). If q = 0, f (p; q) = 4 =2 <ref> [3; 2] </ref>, and q receives no message from p. But if q = 6, f (p; q) = 2 2 [3; 2], and q does receive a message (see Table 10 to check this).
Reference: [4] <author> J. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> ScaLAPACK: A portable linear algebra library for distributed memory computers design issues and performance. </title> <journal> Computer Physics Communications, </journal> <volume> 97 </volume> <pages> 1-15, </pages> <year> 1996. </year> <note> (also LAPACK Working Note #95). 26 </note>
Reference-contexts: This approach induces a tremendous requirement in terms of buffering space, and deadlock may well happen when redistributing large arrays. The ScaLAPACK library <ref> [4] </ref> provides a set of routines to perform array redistribution. As described by Prylli and Tourancheau [15], a total exchange is organized between processors, which are arranged as a (virtual) caterpillar. The total exchange is implemented as a succession of steps. <p> Although this expression does not take hot spots and link contentions into account, it has proven useful on a variety of machines <ref> [4, 6] </ref>.
Reference: [5] <author> J. J. Dongarra and D. W. Walker. </author> <title> Software libraries for linear algebra computations on high performance computers. </title> <journal> SIAM Review, </journal> <volume> 37(2) </volume> <pages> 151-180, </pages> <year> 1995. </year>
Reference-contexts: Typically, arrays are distributed according to a CYCLIC (r) pattern along one or several dimensions of the grid. The best value of the distribution parameter r depends on the characteristics of the algorithmic kernel as well as on the communication-to-computation ratio of the target machine <ref> [5] </ref>. Because the optimal value of r changes from phase to phase and from one machine to another (think of a heterogeneous environment), run-time redistribution turns out to be a critical operation, as stated in [10, 21, 22] (among others).
Reference: [6] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix computations. </title> <publisher> Johns Hopkins, </publisher> <address> 2 edition, </address> <year> 1989. </year>
Reference-contexts: Although this expression does not take hot spots and link contentions into account, it has proven useful on a variety of machines <ref> [4, 6] </ref>.
Reference: [7] <author> R.L. Graham, M. Grotschel, and L. Lovasz. </author> <title> Handbook of combinatorics. </title> <publisher> Elsevier, </publisher> <year> 1995. </year>
Reference-contexts: The degree of G, defined as the maximum degree of its vertices, is d G = maxfm R ; m C g. According to Konig's edge coloring theorem, the edge coloring number of a bipartite graph is equal to its degree (see <ref> [7, vol. 2, p.1666] </ref> or Berge [2, p. 238]). This means that the edges of a bipartite graph can be partitioned in d G disjoint edge matchings. A constructive proof is as follows: repeatedly extract from E a maximum matching that saturates all maximum degree nodes. <p> We might end up with a schedule having more than N S opt steps but whose total cost is less. To implement both approaches, we rely on a linear programming framework (see <ref> [7, chapter 30] </ref>). Let A be the jV j fi jEj incidence matrix of G, where a ij = 1 if edge j is incident to vertex i 0 otherwise Since G is bipartite, A is totally unimodular (each square submatrix of A has determinant 0, 1 or 1).
Reference: [8] <author> S. K. S. Gupta, S. D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> Compiling array expressions for efficient execution on distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 32(2) </volume> <pages> 155-172, </pages> <year> 1996. </year>
Reference-contexts: Recently, however, several algorithms have been published that handle general block-cyclic CYCLIC (k) distributions. Sophisticated techniques involve finite-state machines (see Chatterjee et al. [3]), set-theoretic methods (see Gupta et al. <ref> [8] </ref>), Diophantine equations (see Kennedy et al. [11, 12]), Hermite forms and lattices (see Thirumalai and Ramanujam [18]), or linear programming (see Ancourt et al. [1]).
Reference: [9] <author> J.E. Hopcroft and R.M. Karp. </author> <title> An n 5=2 algorithm for maximum matching in bipartite graphs. </title> <journal> SIAM J. Computing, </journal> <volume> 2(4) </volume> <pages> 225-231, </pages> <year> 1973. </year>
Reference-contexts: Remark 2 The proof of Theorem 1 gives a bound for the complexity of determining the optimal number of steps. The best known maximum matching algorithm for bipartite graphs is due to Hopcroft and Karp <ref> [9] </ref> and has cost O (jV j 5 2 ).
Reference: [10] <author> E. T. Kalns and L. M. Ni. </author> <title> Processor mapping techniques towards efficient data redistribution. </title> <journal> IEEE Trans. Parallel Distributed Systems, </journal> <volume> 6(12) </volume> <pages> 1234-1247, </pages> <year> 1995. </year>
Reference-contexts: Because the optimal value of r changes from phase to phase and from one machine to another (think of a heterogeneous environment), run-time redistribution turns out to be a critical operation, as stated in <ref> [10, 21, 22] </ref> (among others). Basically, we can decompose the redistribution problem into the following two subproblems: Message generation The array to be redistributed should be efficiently scanned or processed in order to build up all the messages that are to be exchanged between processors. <p> Simple strategies have been advocated. For instance, Kalns and Ni <ref> [10] </ref> view the communications as a total exchange between all processors and do not further specify the operation. In their comparative survey, Wang et al. [22] use the following template for executing an array assignment statement: 1.
Reference: [11] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> Efficient address generation for block-cyclic distributions. </title> <booktitle> In 1995 ACM/IEEE Supercomputing Conference. </booktitle> <address> http://www.supercomp.org/sc95/proceedings, 1995. </address>
Reference-contexts: Recently, however, several algorithms have been published that handle general block-cyclic CYCLIC (k) distributions. Sophisticated techniques involve finite-state machines (see Chatterjee et al. [3]), set-theoretic methods (see Gupta et al. [8]), Diophantine equations (see Kennedy et al. <ref> [11, 12] </ref>), Hermite forms and lattices (see Thirumalai and Ramanujam [18]), or linear programming (see Ancourt et al. [1]).
Reference: [12] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> A linear-time algorithm for computing the memory access sequence in data-parallel programs. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 102-111. </pages> <publisher> ACM Press, </publisher> <year> 1995. </year> <month> 27 </month>
Reference-contexts: Recently, however, several algorithms have been published that handle general block-cyclic CYCLIC (k) distributions. Sophisticated techniques involve finite-state machines (see Chatterjee et al. [3]), set-theoretic methods (see Gupta et al. [8]), Diophantine equations (see Kennedy et al. <ref> [11, 12] </ref>), Hermite forms and lattices (see Thirumalai and Ramanujam [18]), or linear programming (see Ancourt et al. [1]).
Reference: [13] <author> Charles H. Koelbel, David B. Loveman, Robert S. Schreiber, Guy L. Steele Jr., and Mary E. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction Run-time redistribution of arrays that are distributed in a block-cyclic fashion over a multidimensional processor grid is a difficult problem that has recently received considerable attention. This interest is motivated largely by the HPF <ref> [13] </ref> programming style, in which scientific applications are decomposed into phases. At each phase, there is an optimal distribution of the data arrays onto the processor grid. Typically, arrays are distributed according to a CYCLIC (r) pattern along one or several dimensions of the grid.
Reference: [14] <author> Antoine Petitet. </author> <title> Algorithmic redistribution methods for block cyclic decompositions. </title> <type> PhD thesis, </type> <institution> University of Tennessee at Knoxville, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: Consider the following function f : (p; q) ! f (p; q) = pr qs mod g 2 For another proof, see Petitet <ref> [14] </ref>. 12 Table 13: Communications for P = Q = 15, r = 12, and s = 20. Message lengths are indicated for a vector X of size L = 900.
Reference: [15] <author> L. Prylli and B. Tourancheau. </author> <title> Efficient block-cyclic data redistribution. </title> <booktitle> In EuroPar'96, volume 1123 of Lectures Notes in Computer Science, </booktitle> <pages> pages 155-164. </pages> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: This approach induces a tremendous requirement in terms of buffering space, and deadlock may well happen when redistributing large arrays. The ScaLAPACK library [4] provides a set of routines to perform array redistribution. As described by Prylli and Tourancheau <ref> [15] </ref>, a total exchange is organized between processors, which are arranged as a (virtual) caterpillar. The total exchange is implemented as a succession of steps. At each step, processors are arranged into pairs that perform a send/receive operation. Then the caterpillar is shifted so that new exchange pairs are formed. <p> Schedules are composed of steps, and each step generates at most one send and/or one receive per processor. Hence we used only one-to-one communication primitives from MPI. Our main objective was a comparison of our new scheduling strategy against the current redistribution algorithm of ScaLAPACK <ref> [15] </ref>, namely, the "caterpillar" algorithm that was briefly summarized in Section 3.2. To run our scheduling algorithm, we proceed as follows: 1. Compute schedule steps using the results of Section 4. 2. Pack all the communication buffers. 3. Carry out barrier synchronization. 4. Start the timer. 5.
Reference: [16] <author> M. Snir, S. W. Otto, S. Huss-Lederman, D. W. Walker, and J. Dongarra. </author> <title> MPI the complete reference. </title> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Communication scheduling All the messages must be efficiently scheduled so as to minimize communication overhead. A given processor typically has several messages to send, to all other processors or to a subset of these. In terms of MPI collective operations <ref> [16] </ref>, we must schedule something similar to an MPI ALLTOALL communication, except that each processor may send messages only to a particular subset of receivers (the subset depending on the sender). Previous work has concentrated mainly on the first subproblem, message generation. <p> This strategy leads to a synchronized algorithm that is as efficient as the asynchronous version, as demonstrated by experiments (written in MPI <ref> [16] </ref>) on the IBM SP-1 and Intel Paragon, while requiring much less buffering space.
Reference: [17] <author> J. M. Stichnoth, D. O'Hallaron, and T. R. Gross. </author> <title> Generating communication for array statements: design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 150-159, </pages> <year> 1994. </year>
Reference: [18] <author> A. Thirumalai and J. Ramanujam. </author> <title> Fast address sequence generation for data-parallel programs using integer lattices. </title> <editor> In C.-H. Huang, P. Sadayappan, U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, volume 1033 of Lectures Notes in Computer Science, </booktitle> <pages> pages 191-208. </pages> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Recently, however, several algorithms have been published that handle general block-cyclic CYCLIC (k) distributions. Sophisticated techniques involve finite-state machines (see Chatterjee et al. [3]), set-theoretic methods (see Gupta et al. [8]), Diophantine equations (see Kennedy et al. [11, 12]), Hermite forms and lattices (see Thirumalai and Ramanujam <ref> [18] </ref>), or linear programming (see Ancourt et al. [1]). A comparative survey of these algorithms can be found in Wang et al. [22], where it is reported that the most powerful algorithms can handle block-cyclic distributions as efficiently as the simpler case of pure cyclic or full-block mapping.
Reference: [19] <author> K. van Reeuwijk, W. Denissen, H. J. Sips, and E. M.R.M. Paalvast. </author> <title> An implementation framework for HPF distributed arrays on message-passing parallel computer systems. </title> <journal> IEEE Trans. Parallel Distributed Systems, </journal> <volume> 7(9) </volume> <pages> 897-914, </pages> <year> 1996. </year>
Reference: [20] <author> A. Wakatani and M. Wolfe. </author> <title> Redistribution of block-cyclic data distributions using MPI. </title> <journal> Parallel Computing, </journal> <volume> 21(9) </volume> <pages> 1485-1490, </pages> <year> 1995. </year>
Reference-contexts: Some researchers (see Stichnoth et al.[17], van Reeuwijk et al.[19], and Wakatani and Wolfe <ref> [20] </ref>) have dealt principally with arrays distributed by using either a purely scattered or cyclic distribution (CYCLIC (1) in HPF) or a full block distribution (CYCLIC (d n p e), where n is the array size and p the number of processors).
Reference: [21] <author> David W. Walker and Steve W. Otto. </author> <title> Redistribution of block-cyclic data distributions using MPI. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 8(9) </volume> <pages> 707-728, </pages> <year> 1996. </year>
Reference-contexts: Because the optimal value of r changes from phase to phase and from one machine to another (think of a heterogeneous environment), run-time redistribution turns out to be a critical operation, as stated in <ref> [10, 21, 22] </ref> (among others). Basically, we can decompose the redistribution problem into the following two subproblems: Message generation The array to be redistributed should be efficiently scanned or processed in order to build up all the messages that are to be exchanged between processors. <p> However, the question of how to efficiently schedule the messages has received little attention. One exception is an interesting paper by Walker and Otto <ref> [21] </ref> on how to schedule messages in order to change the array distribution from CYCLIC (r) on a P - processor linear grid to CYCLIC (Kr) on the same grid. <p> In Section 2 we provide some examples of redistribution operations to expose the difficulties in scheduling the communications. In Section 3 we briefly survey the literature on the redistribution problem, with particular emphasis given to the Walker and Otto paper <ref> [21] </ref>. In Section 4 we present our main results. In Section 5 we report on some MPI experiments that demonstrate the usefulness of our results. <p> Note that the total cost is equal to the sum of the message lengths that processor q = 1 must receive; hence, it too is optimal. 3 Literature overview We briefly survey the literature on the redistribution problem, with particular emphasis given to the work of Walker and Otto <ref> [21] </ref>. 5 Table 4: Communication grid for P = Q = 16, r = 7, and s = 11. Message lengths are indicated for a vector X of size L = 1232. <p> Again, even though special care is taken in implementing the total exchange, no attempt is made to exploit the fact that some processor pairs may not need to communicate. The first paper devoted to scheduling the communications induced by a redistribution is that of Walker and Otto <ref> [21] </ref>. <p> This strategy leads to a synchronized algorithm that is as efficient as the asynchronous version, as demonstrated by experiments (written in MPI [16]) on the IBM SP-1 and Intel Paragon, while requiring much less buffering space. Walker and Otto <ref> [21] </ref> provide synchronous schedules only for some special instances of the redistribution problem, namely, to change the array distribution from CYCLIC (r) on a P -processor linear grid to CYCLIC (Kr) on a grid of same size. Their main result is to provide a schedule composed of K steps. <p> We retain their original idea: schedule the communications into steps. At each step, each participating processor neither sends nor receives more than one message, to avoid hot spots and resource contentions. As explained in <ref> [21] </ref>, this strategy is well suited to current parallel architectures. <p> Remark 1 Walker and Otto <ref> [21] </ref> deal with a redistribution with P = Q and s = Kr. We have shown that going from r to Kr can be simplified to going from r = 1 to s = K. <p> We have shown that going from r to Kr can be simplified to going from r = 1 to s = K. If gcd (K; P ) = 1, the technique described in this section enables us to retrieve the results of <ref> [21] </ref>. 4.4 The General Case When gcd (s; P ) = s 0 &gt; 1, entries of the communication grid may not be evenly distributed on the rows (senders). <p> 6 2 - 2 - 2 - 3 8 - 2 - 2 - 2 3 10 1 1 1 1 1 1 6 12 2 - 2 - 2 - 3 14 - 2 - 2 - 2 3 4.5.1 Comparison with Walker and Otto's Strategy Walker and Otto <ref> [21] </ref> deal with a redistribution where P = Q and s = Kr. We know that going from r to Kr can be simplified to going from r = 1 to s = K. <p> Because the graph is regular and all entries in the communication grid are equal, we have the following theorem, which extends Walker and Otto main result <ref> [21] </ref>. 21 Table 15: Communication steps (stepwise strategy) for P = 15, Q = 6, r = 2, and s = 3.
Reference: [22] <author> Lei Wang, James M. Stichnoth, and Siddhartha Chatterjee. </author> <title> Runtime performance of parallel array assignment: an empirical study. </title> <booktitle> In 1996 ACM/IEEE Supercomputing Conference. </booktitle> <address> http://www.supercomp.org/sc96/proceedings, 1996. </address> <month> 28 </month>
Reference-contexts: Because the optimal value of r changes from phase to phase and from one machine to another (think of a heterogeneous environment), run-time redistribution turns out to be a critical operation, as stated in <ref> [10, 21, 22] </ref> (among others). Basically, we can decompose the redistribution problem into the following two subproblems: Message generation The array to be redistributed should be efficiently scanned or processed in order to build up all the messages that are to be exchanged between processors. <p> A comparative survey of these algorithms can be found in Wang et al. <ref> [22] </ref>, where it is reported that the most powerful algorithms can handle block-cyclic distributions as efficiently as the simpler case of pure cyclic or full-block mapping. At the end of the message generation phase, each processor has computed several different messages (usually stored in temporary buffers). <p> Simple strategies have been advocated. For instance, Kalns and Ni [10] view the communications as a total exchange between all processors and do not further specify the operation. In their comparative survey, Wang et al. <ref> [22] </ref> use the following template for executing an array assignment statement: 1. Generate message tables, and post all receives in advance to minimize operating systems overhead 2. Pack all communication buffers 3.
References-found: 22

