URL: http://www.eng.warwick.ac.uk/~es2029/thesis.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00172.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Feedforward Neural Networks with Constrained Weights  
Author: Altaf Hamid Khan 
Degree: A thesis submitted in satisfaction of the requirements for the degree of Doctor of Philosophy  
Date: August 1996  
Affiliation: University of Warwick Department of Engineering  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> 1040EZ: </author> <title> Income tax return for single and joint filers with no dependents. Internal Revenue Service, </title> <institution> Department of Treasury, United States of America, </institution> <year> 1994. </year> <note> Available as ftp://ftp.fedworld.gov/pub/irs-pdf/f1040ez.pdf. </note>
Reference-contexts: Internal Revenue Service tax form 1040EZ <ref> [1] </ref>. 109 6.4 Feature reduction (256 ! 32) of the handwritten numeral data. <p> This set of functions, N 1 (sin; S 1 ), is clearly not dense on compacta in C (R), being unable, for example, to universally approximate sin (20x) on <ref> [1; 1] </ref>. An application of Theorem 2.3 provides the following very useful result: Lemma 2.2 If is the logistic function then for w j 2 S d [ f0g; N d (; B)is uniformly dense in compacta in C (R d ). <p> Feedforward networks with offsets and hyperbolic tangent activation function in both the hidden and output layer neurons were used for these simulations. The training data was scaled to the range <ref> [1; 1] </ref>. These simulations involved clean data therefore the L 1 -norm, E o m , was used as the error function: training was stopped when E o m was less than a prespecified " and all the weights had reached integer values. <p> E o was found to decrease at rate of q 1=2 , where q is the number of hidden neurons [4, 64]. 4.3 Approximating CWNs with IWNs In the work reported here, mappings of the form f : R 2 ! f1; 1g 1 , R being a closed interval <ref> [1; 1] </ref>, were used for comparing the IWN and CWN decision surfaces for a set of 10 classification problems (Figure 4.4) which were used for numerous training runs on 2:q:1 networks, with and without skip-layer synapses 2 . The results for those simulations are summarised in Table 4.1. <p> This set was then split into two equal, balanced subsets for training and testing. All eight attributes were then standardised to zero mean and unit variance [13]. x x where x is the calculated mean, and x the standard deviation. All attribute values outside the <ref> [1; 1] </ref> range were truncated to f1; 1g. <p> Twelve individuals were asked to provide 100 samples each while following a given writing style, resulting in 120 examples of each numeral. The sample style given to the writers was similar to the one required for the U.S. Internal Revenue Service's machine readable tax form 1040EZ <ref> [1] </ref> (see Figure 6.3). The raw images of those samples were normalised 20 This database has been kindly made publicly available by Isabelle Guyon at ftp://hope.caltech.- edu/pub/mackay/data/att.database 109 6.5 Generalisation Experiments and then thresholded to fit a 16 fi 16 binary pixel grid.
Reference: [2] <author> Anderson, M. </author> <title> Private communication, </title> <month> July </month> <year> 1996. </year>
Reference-contexts: The quickest way to perform a multiplication in a digital electronic implementation is with a flash multiplier. An n-bit fixed-point VLSI flash multiplier consists of n fi n full adders, each one of which is made up of 31 transistors <ref> [2] </ref>. If the classic NETtalk network of Sejnowski and Rosenberg [132] was to be constructed in this very fast incarnation, approximately 10 9 transistors will be required for n = 8.
Reference: [3] <author> Babri, H. A. and Tong, Y. </author> <title> Deep feedforward nets: Applications to pattern recognition. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 3, </volume> <pages> pages 1422-1426, </pages> <address> Washington, DC, June 1996. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference-contexts: two in poor. 15 2.1 Universal approximation in one dimension. 26 3.1 Two choices for the discretising function Q (w) [84]. 47 3.2 Frequency distribution of numbers generated by tan (RND). 51 3.3 Q prac (w). 53 3.4 Comparison of the actual error term and its approximation in the range <ref> [3; 3] </ref>. 53 3.5 The black-hole function. 54 4.1 The set of decision boundaries of an integer [-3, 3] weight 2-input perceptron with offset. <p> function Q (w) [84]. 47 3.2 Frequency distribution of numbers generated by tan (RND). 51 3.3 Q prac (w). 53 3.4 Comparison of the actual error term and its approximation in the range [3; 3]. 53 3.5 The black-hole function. 54 4.1 The set of decision boundaries of an integer <ref> [-3, 3] </ref> weight 2-input perceptron with offset. <p> a weight resolution of 0.125. w ih = 3. w io = 3. h is plotted along the horizontal axis of the contour plots, and o is along the vertical axis. w ho is the figure in the brackets. 76 5.1 A comparison of the decision boundaries of a integer <ref> [-3, 3] </ref> weight perceptron (same as Figure 4.1) and a multiplier-free perceptron, each with two input synapses and an offset. <p> Two new types of networks are proposed which lend themselves to cost-effective implementations in hardware and have a fast forward-pass capability. These two differ from the conventional model in having extra constraints on their weights: the first allows its weights to take integer values in the range <ref> [3; 3] </ref> only, whereas the second restricts its synapses to the set f1; 0; 1g while allowing unrestricted offsets. The benefits of the first configuration are in having weights which are only 3-bits deep and a multiplication operation requiring a maximum of one shift, one add, and one sign-change instruction. <p> CWNs with many hidden layers may have advantages in having more profound mapping capabilities they can implement a mapping with fewer weights as compared with a 2-layer network having a similar performance. They are not, however, as well understood as 2-layer networks and are harder to train <ref> [3] </ref>. 1.3 Application Examples It has been 10 years since the pioneering work of Rumelhart et al. [123], in which learning in multilayer feedforward networks was first introduced. <p> For example, integer weights in the range <ref> [3; 3] </ref> can be represented by just 3 bits. This property reduces the amount of memory required for weight storage in digital electronic implementations. <p> The two choices that were considered are shown in Figure 3.1. The key feature of these functions is that the zeros of (Q (w) w) have the required integer values. In practice, the application of these functions is restricted to the interval <ref> [3; 3] </ref>, since any weight values outside this interval are truncated to f3; 3g. Of the two, Q tanh (w) is computationally more expensive to generate, yet it does have the advantage in having an adjustable slope between discrete values.
Reference: [4] <author> Barron, A. R. </author> <title> Universal approximation bounds for superpositions of a sigmoidal function. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-39:930-945, </volume> <year> 1993. </year>
Reference-contexts: This logarithmic rate of decrease in error is different from the theoretically calculated degree of approximation result: E o was found to decrease at rate of q 1=2 , where q is the number of hidden neurons <ref> [4, 64] </ref>. 4.3 Approximating CWNs with IWNs In the work reported here, mappings of the form f : R 2 ! f1; 1g 1 , R being a closed interval [1; 1], were used for comparing the IWN and CWN decision surfaces for a set of 10 classification problems (Figure 4.4)
Reference: [5] <author> Battiti, R. </author> <title> First- and second-order methods for learning: Between steepest descent and Newton's method. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 141-166, </pages> <year> 1992. </year>
Reference-contexts: There is wealth of literature available on efficient E o minimisation techniques (see <ref> [5] </ref> for a survey). Many of these techniques are 2 nd -order methods they use information about the 2 nd derivative of the error function E o (W) to compute weight modifications.
Reference: [6] <author> Baum, E. B. </author> <title> On the capabilities of multilayer perceptrons. </title> <journal> Journal of Complexity, </journal> <volume> 4 </volume> <pages> 193-215, </pages> <year> 1988. </year>
Reference-contexts: Shattered. If a set of functions F includes all possible dichotomies on a set S of points, then S is said to be shattered by F . <ref> [6] </ref> Shrinkage. The difference between the training set accuracy of a network and its ac curacy on a test set. Sigmoidal functions. Definitions vary but are generally taken to be bounded, mono tone, and continuous, e.g. logistic and tanh () functions.
Reference: [7] <author> Baum, E. B. and Haussler, D. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1 </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: approximation' [29, 62, 68]? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation with the number of necessary training examples <ref> [7] </ref>? Similarly, questions can be raised about the convergence properties of a given combination of a neural architecture and a specific learning heuristic [74]. <p> about economy in explanations, was systematically applied, but never explicitly stated by the 13th century philosopher, William of Ockham [141]. 98 6.5 Generalisation Experiments however, a problem with using larger than optimal networks although smaller networks are not guaranteed to be better in general, Baum and Haussler have shown in <ref> [7] </ref> that for a given training set and training error, the worst-case error bounds on unseen data vary proportionally with the number of weights in a network. Moreover, larger-than-optimal networks are slower and demand more storage space.
Reference: [8] <author> Bennett, P. H., Burch, T. A., and Miller, M. </author> <booktitle> Diabetes mellitus in American (Pima) Indians. Lancet, </booktitle> <volume> 2 </volume> <pages> 125-128, </pages> <year> 1971. </year>
Reference: [9] <author> Bioch, J. C., van der Meer, O., and Potharst, R. </author> <title> Classification using Bayesian neural nets. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural 136 137 REFERENCES Networks, </booktitle> <volume> volume 3, </volume> <pages> pages 1488-1493, </pages> <address> Washington, DC, June 1996. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference-contexts: The second set, onset of diabetes prediction data set, has both discrete and continuous inputs, may have some irrelevant inputs, may have much noise in the inputs, may have a high degree of correlation between inputs, and has a single binary output <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> It will be interesting to compare this ranking with the weight structure of the trained networks. This database has been analysed with the help of connectionist tools in <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> Wahaba et al. [150] used 500 cases for training and 252 for testing with a smoothing spline model 13 , and report a generalisation performance of 76%. This study deleted 16 cases from the database because those cases had some attributes with impossible values. Bioch et al. <ref> [9] </ref> used the same data configuration and a Bayesian classifier 14 which resulted in a generalisation performance of 88.7%.
Reference: [10] <author> Bishop, C. M. </author> <title> Mixture density networks. </title> <type> Technical report NCRG/4288, </type> <institution> Department of Computer Science and Applied Mathematics, Aston University, </institution> <address> Birm-ingham, England, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: ARTMAP is a supervised learning procedure explicitly based on neurobiology. 17 ARTMAP with an instance counting procedure and a new match tracking algorithm. 18 Mixture representations of data use a linear combination of Gaussian distributions to represent arbitrary distributions <ref> [10] </ref>. 107 6.5 Generalisation Experiments Table 6.4 Comparison of generalisation performance on the forecasting diabetes database Network Configuration Zero Weights Effective Weights Gen. Perform. CWN 8:2:1 21 78.4% MFN 8:3:1 6 25 78.0% and the EM algorithm 19 [116]. <p> Minima, Global. The points of minimum error on an error surface. Minima, Local. The points of zero gradient on an error surface which are not global minima. Mixture representation of data use a linear combination of Gaussian distributions to represent arbitrary distributions <ref> [10] </ref>. Momentum is a training parameter used in a very common variation on standard error backpropagation learning procedure. It controls the effect of the last weight modification on the current weight update. n-layer network is a feedforward network with n 1 hidden layers.
Reference: [11] <author> Bishop, C. M. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: The deployed network constantly adapts its weights with respect to changes in the input/output behaviour of its target task. 9 1.6 Approximation Properties rigorous frameworks, such as Bayesian techniques, available in statistics <ref> [11, 113] </ref>. The feedforward network, the only neural paradigm discussed in this thesis, has a direct analogue in statistics: projection pursuit regression [66]. Projection pursuit is a generalisation of the CWN in that it allows more than one type of activation function in the hidden layer.
Reference: [12] <author> Bishop, C. M. </author> <title> Neural Networks for Pattern Recognition, </title> <note> chapter 3. In [11], </note> <year> 1995. </year>
Reference-contexts: It is however customary to use a logistic or hyperbolic tangent function for `classification tasks' (to be defined in the next section). Having a logistic function as the activation function in output neurons also helps in the probabilistic interpretation of their responses <ref> [12] </ref>. 5 Also known as regression. 7 1.4 Biological -vs- Artificial Neural Networks all of the coils generating the confinement magnetic field.
Reference: [13] <author> Bishop, C. M. </author> <title> Neural Networks for Pattern Recognition, </title> <note> chapter 8. In [11], </note> <year> 1995. </year>
Reference-contexts: Moreover, Le Cun et al. have shown analytically that standardising the inputs to zero mean improves the convergence properties of some learning procedures [82]. For discrete inputs or outputs, categorical variables should not be treated as continuous variables <ref> [13] </ref>. For instance, one of the input variables for the training data to be used in Section 6.5.2 represents shapes and can have one of three values: round, square, or octagon. <p> This variable will not be represented as a single tri-state input, as that imposes an ordering of square being somehow greater than round and less than octagon <ref> [13] </ref>, but as a triplet, having three possible states (1; 0; 0); (0; 1; 0), and (0; 0; 1) depending upon the shape being round, square, or octagon, respectively [144]. In classification problems, outputs are generally encoded as f0; 1g or f1; 1g. <p> This set was then split into two equal, balanced subsets for training and testing. All eight attributes were then standardised to zero mean and unit variance <ref> [13] </ref>. x x where x is the calculated mean, and x the standard deviation. All attribute values outside the [1; 1] range were truncated to f1; 1g.
Reference: [14] <author> Bishop, C. M. </author> <title> Neural Networks for Pattern Recognition, </title> <note> chapter 7. In [11], </note> <year> 1995. </year>
Reference-contexts: Both versions are guaranteed to converge to a solution in appropriate circumstances (see Section 2.3). The latter version is, however, faster for large training sets having some degree of information redundancy among examples <ref> [14] </ref>.
Reference: [15] <author> Bishop, C. M. </author> <title> Neural Networks for Pattern Recognition, </title> <note> chapter 1. In [11], </note> <year> 1995. </year>
Reference-contexts: procedure is guaranteed to converge to a global minimum. 1.9 Generalisation Performance Generalisation performance 18 the accuracy of a trained network on a set of data which is similar to but not the same as the training data set [53] is the key metric which determines a learning paradigm's usefulness <ref> [15] </ref>. This metric can be maximised by selecting a data set which completely represents the concept to be learned, and using that set, along with a global-minimum finding procedure, to train a network having a complexity that matches that of the concept to be learned. <p> then be presented in terms of the relationship between 23 Bayesian approach differs from the conventional `frequentist' approach to statistics in that it allows the probability of an event to be expressed as `degree of belief' in a particular outcome instead of basing it solely on a set of observations <ref> [15] </ref>. 22 1.13 Overview of the Thesis the form of the learning rate and the type of convergence that can be guaranteed for that form. This result will be used in the next chapter in support of the integer-weight learning procedure. <p> Although ease of implementation and learning time are important factors which influence the choice of a feedforward network paradigm, generalisation performance the accuracy of a trained network on unseen data 1 <ref> [15] </ref> is the key metric which determines a paradigm's usefulness. A trained network will be a good generaliser if it has learned the concept embedded in the training data [16]. <p> Bayesian statistics differs from the conventional `frequentist' approach to statistics in that it allows the probability of an event to be expressed as `degree of belief' in a particular outcome instead of basing it solely on a set of observations <ref> [15] </ref>. Bayes's theorem allows prior estimates of the probability of an event to be revised in accordance with new observations. It states that probability of an event A given another event B, P (AjB), is equal to P (BjA)P (A)/P (B).
Reference: [16] <author> Bishop, C. M. </author> <title> Neural Networks for Pattern Recognition, </title> <note> chapter 9. In [11], </note> <year> 1995. </year>
Reference-contexts: A trained network will be a good generaliser if it has learned the concept embedded in the training data <ref> [16] </ref>. Training a network to be a good generaliser is not a trivial task because most real-life applications demand training with noisy data. A good gen-eraliser usually has a smooth input to output mapping, which generally means that it will not have many large weights [16]. <p> embedded in the training data <ref> [16] </ref>. Training a network to be a good generaliser is not a trivial task because most real-life applications demand training with noisy data. A good gen-eraliser usually has a smooth input to output mapping, which generally means that it will not have many large weights [16]. More complex networks give a better fit to training data but are not good generalisers. The best generalisers are neither too complex nor too simple, but match the the complexity of the problem exactly. <p> A trained network will be a good generaliser if it has learned the concept embedded in the training data <ref> [16] </ref>. Training a network which will be a good generaliser is not a trivial task because most real-life applications demand training with noisy data. A good generaliser will have a smooth input to output mapping, which generally means that it will not have many large weights [16]. <p> in the training data <ref> [16] </ref>. Training a network which will be a good generaliser is not a trivial task because most real-life applications demand training with noisy data. A good generaliser will have a smooth input to output mapping, which generally means that it will not have many large weights [16]. Also, its complexity will match that of the `noise-free' version of the mapping embedded in the training data. The weights of IWNs and synapses of MFNs are limited in their magnitudes. They also usually have larger hidden layers, compared with their CWN counterparts. <p> with the results on the three data sets. 94 6.2 Estimation of Generalisation Performance 6.2 Estimation of Generalisation Performance Moody [97] has proposed the following relationship for estimating the error on test data, E o ts , from the sum-of-squares error on T training examples, E o tr (T ) <ref> [16] </ref>: E o ts = T T where W eff is the number of effective weights in the CWN, and 2 the variance of the noise on the training examples. <p> They result in smoother models, which in turn, results in better generalisation performance. Regularisation is achieved by enforcing smoothing and complexity minimising constraints on the model so that the resultant is the least complex model which fits a given set of training data <ref> [16] </ref>. Common regularisation methods are: 1. Weight decay is the technique in which a penalty term is added to the training cost function which penalises large weights. A common penalty term is the sum of the square of weights [57]. <p> Weight decay was used to stop weights from reaching excessively large values. This makes the comparison between IWN and MFN, and CWNs more meaningful, because IWNs and MFNs are not allowed to have large weight values. Avoiding large weights assures that the network learns smooth mappings <ref> [16] </ref>. 6.4 Optimal Network The definition of the optimal network used for the experiments presented here is based on the conjecture called Ockham's Razor 4 [141]: If two different models have similar performances on a set of data then the simpler of the two should be preferred.
Reference: [17] <author> Bishop, C. M., Haynes, P. S., Smith, M. E. U., Todd, T. N., and Trotman, D. L. </author> <title> Real-time control of a tokamak plasma using neural networks. </title> <journal> Neural Computation, </journal> <volume> 7(1) </volume> <pages> 206-217, </pages> <year> 1995. </year>
Reference-contexts: A fully parallel hardware implementation of a feedforward network is being used for the closed-loop real-time control of the shape of the magnetic confinement field of a high-temperature plasma in a Tokamak fusion reactor <ref> [17] </ref>.
Reference: [18] <author> Blum, A. L. and Rivest, R. L. </author> <title> Training a 3-node neural networks is NP-complete. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 117-127, </pages> <year> 1992. </year>
Reference-contexts: 136]? What type of neuron activation functions are permissible [27, 83, 136]? Is it capable of `universal approximation' [29, 62, 68]? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network <ref> [18] </ref>? What is the expected generalisation performance and its relation with the number of necessary training examples [7]? Similarly, questions can be raised about the convergence properties of a given combination of a neural architecture and a specific learning heuristic [74].
Reference: [19] <author> Borggaard, C., Madsen, N. T., and Thodberg, H. H. </author> <title> In-line image analysis in the slaughter industry, illustrated by beef carcass classification. Reference number 31.616, </title> <type> manuscript number 1336E, </type> <institution> Danish Meat Research Institute, Roskilde, Denmark, </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: It employs three feedforward networks to extract the class (cow, bull, or heifer) and shape of the carcasses which are then used in a linear model to determine the payment to be made to the farmer <ref> [19] </ref>. Urneco (Capenhurst) Ltd. employs a pair of feedforward networks for the closed-loop control of copper lasers, which are used in isotope separation for uranium enrichment [24].
Reference: [20] <author> Borowski, E. J. and Borwein, J. M. </author> <title> Dictionary of Mathematics. </title> <address> HarperCollins, Great Britain, </address> <year> 1989. </year>
Reference-contexts: a conditional likelihood cost-function and not optimising a quadratic cost function which is the case for linear discriminant analysis [93]. 13 Smoothing spline modelling is piecewise approximation by polynomials of degree n with the requirement that the derivatives of the polynomials are continuous up to degree n1 at the junctions <ref> [20] </ref>. 14 Bayesian classifier assigns a class to an object in such a way that the expectation value of misclas sification is minimised. <p> throughout the set; that is, for every " &gt; 0 there is a single N such that for all points in the set , jf m (x) f n (x)j &lt; " for all m; n &gt; N and similarly for uniform convergence as x tends to a value a <ref> [20] </ref>. Cost function is the quantity that is to be minimised in an optimisation experiment. In the case of feedforward networks this quantity is usually the RMS error in the output of the network. Also known as error measure. <p> Also known as short-cut synapses [78]. Known as main effects in the statistical litera ture [127]. Smoothing spline modelling is piecewise approximation by polynomials of degree n with the requirement that the derivatives of the polynomials are continuous up to degree n 1 at the junctions <ref> [20] </ref>. Softmax. The purpose of the softmax activation function is to make the sum of the output neuron responses equal to one, so that the outputs are interpretable as posterior probabilities. Also known as the multiple-logistic function. Space, Banach is a complete normed linear space.
Reference: [21] <author> Boser, B., Guyon, I., and Vapnik, V. </author> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> In Proceedings of the Computational Learning Theory ACM Workshop, </booktitle> <address> Pittsburgh, PA, </address> <year> 1992. </year> <note> 138 REFERENCES </note>
Reference-contexts: This first-order approximate rating was determined by summing of the absolute values of synapses connecting the inputs to the hidden neurons. 6.5.4 Handwritten Numeral Recognition The numeral database 20 used for these simulations was collected at Bell Labs <ref> [21, 31, 44, 47, 48] </ref>. It consists of 1200 isolated handwritten samples of numerals 0..9. Twelve individuals were asked to provide 100 samples each while following a given writing style, resulting in 120 examples of each numeral.
Reference: [22] <author> Brause, R. W. </author> <title> The error-bound descriptional complexity of approximation networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(6) </volume> <pages> 177-187, </pages> <year> 1993. </year>
Reference-contexts: mappings can it learn (linear, nonlinear, continuous, smooth, measurable) [63, 136]? What type of neuron activation functions are permissible [27, 83, 136]? Is it capable of `universal approximation' [29, 62, 68]? What is the relation of the quality of approximation with the number of neurons or the depth of weights <ref> [22, 64] </ref>? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation with the number of necessary training examples [7]? Similarly, questions can be raised about the convergence properties of a given combination of a neural architecture and a specific learning heuristic [74]. <p> Therefore, the combination of q and b w , the network complexity, is not a linear function of its components <ref> [22] </ref>. The optimal combination of these two parameters to implement a desired network complexity may or may not be unique. General methods to find the optimal combination are not currently available. <p> The quality of this approximation can be improved in one of two ways <ref> [22] </ref> either by allowing a larger number of discrete levels [40, 169], or by having more hidden neurons [58]. This chapter, while discussing both, will emphasise the latter approach, and will mainly be concerned with weights having small integer values. <p> Discrete-weight networks have a very attractive feature not found in CWNs the amount of information stored in their weights is quantifiable <ref> [22] </ref>. The CWN can store an infinite variety of information, whereas the discrete-weight networks have only a limited capability. Varying the discretisation scheme shows how much complexity is required to get a reasonable approximation to a given learning task. <p> The latter does, 78 4.5 Discussion however, have the advantage of efficient hardware implementation. Moreover, it provides an interesting way of quantifying information stored in a network <ref> [22] </ref>. Changing the discretisation scheme of the weights of a fixed-size network, or keeping a fixed dis-cretisation scheme while changing the size of the network, are alternative techniques for determining the amount of network complexity required for approximating the response of a CWN to a specified tolerance.
Reference: [23] <author> Brown, A. L. and Page, A. </author> <title> Elements of Functional Analysis. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> London, </address> <year> 1970. </year>
Reference-contexts: Then there is a real-valued function ff of bounded variation on [a; b] such that x fl (f ) = R b a f dff for all f 2 C R ([a; b]). Further, if x fl is a positive linear functional, then ff is increasing on [a; b] <ref> [23] </ref>. 132 GLOSSARY RMS error, E o rms , is computed by summing the output layer errors for all examples in a training or test set, dividing the sum by the total number of examples and the number of the output layer neurons, and taking the square root of the resultant.
Reference: [24] <author> Buckley, J. M. and Richardson, M. B. </author> <title> Control of a copper laser using neural networks. </title> <journal> IEE Computing and Control Engineering Journal, </journal> <volume> 7(3) </volume> <pages> 145-152, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Urneco (Capenhurst) Ltd. employs a pair of feedforward networks for the closed-loop control of copper lasers, which are used in isotope separation for uranium enrichment <ref> [24] </ref>.
Reference: [25] <author> Carpenter, G. A. and Markuzon, N. </author> <title> ARTMAP-IC and medical diagnosis: Instance counting and inconsistent cases. </title> <type> Technical report CAS/CNS-96-017, </type> <institution> Department of Cognitive and Neural Systems, Boston University, </institution> <address> Boston, MA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The second set, onset of diabetes prediction data set, has both discrete and continuous inputs, may have some irrelevant inputs, may have much noise in the inputs, may have a high degree of correlation between inputs, and has a single binary output <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> It will be interesting to compare this ranking with the weight structure of the trained networks. This database has been analysed with the help of connectionist tools in <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> This study deleted 16 cases from the database because those cases had some attributes with impossible values. Bioch et al. [9] used the same data configuration and a Bayesian classifier 14 which resulted in a generalisation performance of 88.7%. Carpenter and Markuzon <ref> [25] </ref> have also analysed this database and have reported a generalisation performance of 77% with k-nearest neighbours 15 , 78% with ART-EMAP 16 , and 81% with ARTMAP-IC 17 . Ripley [115] used 200 cases for training and 332 for testing, and ignored the rest because of missing values. <p> Some of the terms are not explicitly mentioned in this dissertation but are included because of their usefulness in understanding the cited literature. 122 123 GLOSSARY ART-EMAP is ARTMAP with added spatial and temporal evidence accumulation processes <ref> [25] </ref>. ARTMAP is a supervised learning procedure explicitly based on neurobiology. ARTMAP-IC is ARTMAP with an instance counting procedure and a match tracking algorithm [25]. Attribute is an element of the input vector. Also known as a feature. <p> but are included because of their usefulness in understanding the cited literature. 122 123 GLOSSARY ART-EMAP is ARTMAP with added spatial and temporal evidence accumulation processes <ref> [25] </ref>. ARTMAP is a supervised learning procedure explicitly based on neurobiology. ARTMAP-IC is ARTMAP with an instance counting procedure and a match tracking algorithm [25]. Attribute is an element of the input vector. Also known as a feature. Autoassociator A system for which the desired output response is the same as the input.
Reference: [26] <author> Carroll, S. M. and Dickinson, B. W. </author> <title> Constructing the neural nets using the Radon transform. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 607-611, </pages> <address> Washington, DC, 1989. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference-contexts: `stacked together' to form 2- (or d)-dimensional function. 2.2.1 Universal Approximation in C (R d ) Numerous results, utilising a vast array of mathematical tools and making various assumptions about the construct of the network, are available on the universal approximation in C (R d ) property of feedforward networks <ref> [26, 27, 29, 41, 55, 62, 63, 83, 136] </ref>. <p> Cybenko [29] employed Hahn-Banach and Riesz Representation Theorems, Carroll and Dickinson <ref> [26] </ref> used Radon transforms for a constructive proof, Funahashi [41] invoked Fourier analysis and Paley-Wiener theory, while Hornik et al. [63] utilised the Stone-Weierstrass theorem to show the universal approximation property with continuous activation functions on compacta 3;4 in C (R d ) with respect to the supremum 5 norm.
Reference: [27] <author> Chen, T., Chen, H., and Liu, R. </author> <title> Approximation capability in C by multilayer feedforward networks and related problems. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(6) </volume> <pages> 25-30, </pages> <year> 1995. </year>
Reference-contexts: chapters mainly consist of original work. 2 Theoretical Preliminaries 2.1 Theoretical Questions For a given artificial feedforward architecture, it is possible to theoretically explore many an interesting issue: What kinds of mappings can it learn (linear, nonlinear, continuous, smooth, measurable) [63, 136]? What type of neuron activation functions are permissible <ref> [27, 83, 136] </ref>? Is it capable of `universal approximation' [29, 62, 68]? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation <p> `stacked together' to form 2- (or d)-dimensional function. 2.2.1 Universal Approximation in C (R d ) Numerous results, utilising a vast array of mathematical tools and making various assumptions about the construct of the network, are available on the universal approximation in C (R d ) property of feedforward networks <ref> [26, 27, 29, 41, 55, 62, 63, 83, 136] </ref>. <p> Leshno et al. [83] showed that non-polynomial activation functions which are locally bounded and piecewise continuous are necessary for universal approximation. Hornik [62] proved universal approximation capability on compacta for networks having activation functions which are locally Riemann integrable and non-polynomial. Chen et al. <ref> [27] </ref> presented a constructive proof which shows that the sigmoidal activation function need not be monotone or continuous, only bounded for approximation in C (R d ) 6 . All of the above universal approximation results assume that the CWN can have weights of arbitrary precision and magnitude. <p> If a continuous function is defined in R d and lim jxj!1 f (x) exists, then f is called a continuous function in the extended R d , i.e. R d , and the set of all continuous functions on R d is C ( R d ) <ref> [27] </ref>. 28 2.2 Universal Approximation Property of CWNs and White [137] have come up with a result which trades off bounds on the magnitude of network weights with a possible increase in the number of the hidden neurons, which is highly relevant to the work presented in Chapter 5 and will
Reference: [28] <author> Chieueh, T. D. and Goodman, R. M. </author> <title> Learning algorithms for neural networks with ternary weights. </title> <booktitle> In First Annual Meeting of International Neural Networks Society, </booktitle> <address> Boston, MA, </address> <month> September </month> <year> 1988. </year> <title> Abstract in Neural Networks, </title> <address> 1:166, </address> <year> 1988. </year>
Reference-contexts: Based on this idea, a multiple-thresholding method has been proposed for generating discrete-weight networks <ref> [28, 164] </ref>. In this simple method, the continuous weights of a fully trained network are discretised using a staircase function. <p> Based on this idea, a multiple-thresholding method has been proposed for generating discrete-weight networks [28, 164]. In this simple method, the continuous weights of a fully trained network are discretised using a staircase function. This technique was used by Chieueh and Goodman for training with ternary weights <ref> [28] </ref> and it was found that a large percentage of the resulting networks failed to perform correctly when the weights were discretised. This was a rather drastic example of the truncation technique. Truncating the last bit or two of high-resolution weights should give more reasonable results. <p> The results indicate that the easier problems, signified by the smaller number of learning epochs, are more suitable for direct discretisation. The very poor performance of this direct discretisation experiment confirms the conclusions of Chieueh and Goodman <ref> [28] </ref>. The reason for testing the integer-weight learning procedure on these three simple problems was to check its functionality and to fine tune the learning parameters by monitoring the interaction of the E o minimisation and the weight-discretisation mechanisms.
Reference: [29] <author> Cybenko, G. </author> <title> Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> 6 </volume> <pages> 303-314, </pages> <year> 1989. </year>
Reference-contexts: The next big step in the continued development of the feedforward network was the development of proofs on the universal approximation capability of the 3-layer CWN [80] and later on the 2-layer CWN <ref> [29] </ref> in the late 80's. The last major achievement in this field, though still in its infancy, has been the integration of CWNs in a Bayesian framework (see, for example [86, 102, 141]). <p> 2.1 Theoretical Questions For a given artificial feedforward architecture, it is possible to theoretically explore many an interesting issue: What kinds of mappings can it learn (linear, nonlinear, continuous, smooth, measurable) [63, 136]? What type of neuron activation functions are permissible [27, 83, 136]? Is it capable of `universal approximation' <ref> [29, 62, 68] </ref>? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation with the number of necessary training examples [7]? Similarly, <p> `stacked together' to form 2- (or d)-dimensional function. 2.2.1 Universal Approximation in C (R d ) Numerous results, utilising a vast array of mathematical tools and making various assumptions about the construct of the network, are available on the universal approximation in C (R d ) property of feedforward networks <ref> [26, 27, 29, 41, 55, 62, 63, 83, 136] </ref>. <p> Cybenko <ref> [29] </ref> employed Hahn-Banach and Riesz Representation Theorems, Carroll and Dickinson [26] used Radon transforms for a constructive proof, Funahashi [41] invoked Fourier analysis and Paley-Wiener theory, while Hornik et al. [63] utilised the Stone-Weierstrass theorem to show the universal approximation property with continuous activation functions on compacta 3;4 in C (R
Reference: [30] <author> Denker, J. S. and Wittner, B. S. </author> <title> Network generality, training required, and precision required. </title> <editor> In Denker, J. S., editor, </editor> <booktitle> Neural Networks for Computing, </booktitle> <pages> pages 219-222, </pages> <address> Snowbird, UT, </address> <year> 1986. </year> <institution> American Institute of Physics, </institution> <address> New York, NY. 139 REFERENCES </address>
Reference-contexts: that numerical algorithms require temporary `guard bits' on each weight 8 They used a static and uniform discretisation (stair-case like) scheme instead. 43 3.5 Learning Heuristics during training, and these guard bits, though essential during training, do not affect the network's performance if removed after it has been fully trained <ref> [30] </ref>. Based on this idea, a multiple-thresholding method has been proposed for generating discrete-weight networks [28, 164]. In this simple method, the continuous weights of a fully trained network are discretised using a staircase function.
Reference: [31] <author> Drucker, H. and Le Cun, Y. </author> <title> Improving generalization performance using double backpropagation. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3 </volume> <pages> 991-997, </pages> <year> 1992. </year>
Reference-contexts: This particular emphasis on comparison was the main reason for the use of just two data subsets. Moreover, all of the previous results cited here on MONK's benchmark and the handwritten numeral recognition database have used the same strategy <ref> [31, 47, 143, 146] </ref>. The train-and-test method has another use in addition to the optimal stopping of training network size or complexity selection. Figure 6.1 shows how a network can can be evaluated on the test data after every training epoch for optimal stopping of training. <p> This first-order approximate rating was determined by summing of the absolute values of synapses connecting the inputs to the hidden neurons. 6.5.4 Handwritten Numeral Recognition The numeral database 20 used for these simulations was collected at Bell Labs <ref> [21, 31, 44, 47, 48] </ref>. It consists of 1200 isolated handwritten samples of numerals 0..9. Twelve individuals were asked to provide 100 samples each while following a given writing style, resulting in 120 examples of each numeral. <p> This database was then carved into two subsets of 600 samples each the first five samples of each numeral from every writer were used for training and the rest for testing (see Figures 6.5 & 6.6). Guyon et al. [47], and Druker and Le Cun <ref> [31] </ref>, both have reported a generalisation performance of 97% with a 256:20:10 network trained using conventional BP, and a 256:40:10 network trained using `double backpropagation', respectively. <p> It should be noted that in that figure, to the human eye, each of the ten numerals is clearly distinguishable from all others even after this 256 ! 32 inputs transform. Although this 8-fold reduction in input dimension did cause a 4% reduction in generalisation performance (compared with <ref> [31, 47] </ref>), it made the running of many more simulations possible due to the reduced memory and CPU requirements. The numeral classification decisions were taken according to the neuron with the maximum signal in the output layer.
Reference: [32] <author> Dugundji, J. </author> <title> Topology. </title> <publisher> Allyn and Bacon, </publisher> <address> Boston, MA, </address> <year> 1966. </year> <title> Theorem XI.10.1. </title>
Reference: [33] <author> Dundar, G. and Rose, K. </author> <title> The effect of quantization on multilayer neural nets. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 6(6) </volume> <pages> 1446-1451, </pages> <year> 1995. </year>
Reference-contexts: Most workers, however, agree that networks with weights of medium resolution are adequate to model many learning tasks to the desired accuracy <ref> [33, 169] </ref>, [58, 60, 112, 166, 167] 6 . The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two [77, 138] 6 or sum-of-powers-of-two [88, 89] 6 . <p> It does, however, have two major disadvantages: a) the effect of changes in the input layer weights on the output is relatively small and therefore difficult to measure accurately; b) the computational complexity depends more strongly on size than in standard BP <ref> [33] </ref> as E o has to be recalculated after each weight is updated. A modification of this method, the Chain Rule Perturbation (CHRP) technique [59], overcomes both of these drawbacks. This technique uses Equation 3.1 for updating output layer weights. <p> It then `jolts' the network out of this condition by incrementing weights with the larger updates to the next discrete level <ref> [33] </ref> depending upon the comparison between the sum of the last k updates and a user selectable threshold.
Reference: [34] <author> Eberhart, R. C., Dobbins, R. W., and Hutton, L. V. </author> <title> Performance metrics. </title> <editor> In Eberhart, R. C. and Dobbins, R. W., editors, </editor> <title> Neural Network PC Tools: A Practical Guide, </title> <booktitle> chapter 7, </booktitle> <pages> pages 161-176. </pages> <publisher> Academic Press, </publisher> <address> London, England, </address> <year> 1990. </year>
Reference-contexts: It is the ratio of true positives to the sum of true positives and false negatives. This metric is especially of importance when it is critical that an event be detected. Also known as True Positive Ratio <ref> [34] </ref>. 6 Specificity of a decision is the likelihood that the absence of an event is detected given that it is present. It is the ratio of the true negatives to all negatives. Also known as True Negative Ratio [34]. 100 6.5 Generalisation Experiments 6 Training A Testing A Testing B <p> Also known as True Positive Ratio <ref> [34] </ref>. 6 Specificity of a decision is the likelihood that the absence of an event is detected given that it is present. It is the ratio of the true negatives to all negatives. Also known as True Negative Ratio [34]. 100 6.5 Generalisation Experiments 6 Training A Testing A Testing B 6 Optimal stopping point Over-trained! Under-trained! Epochs Av eraged Error 0 A: Optimal network B: Under-fitted network 6 Training Testing 6 Ockham Overfit! Underfit! Complexity Av eraged Error 101 6.5 Generalisation Experiments The IWN and MFN were tested after <p> It is the ratio of true positives to the sum of true positives and false negatives. This 126 GLOSSARY metric is especially of importance when it is critical that a an event be detected. Also known as True Positive Ratio <ref> [34] </ref>. Decision specificity is the likelihood that the absence of an even is detected given that it is present. It is the ratio of the true negatives to all negatives. Also known as True Negative Ratio [34]. <p> Also known as True Positive Ratio <ref> [34] </ref>. Decision specificity is the likelihood that the absence of an even is detected given that it is present. It is the ratio of the true negatives to all negatives. Also known as True Negative Ratio [34]. Decision surface is the plot of the response of an output neuron with respect to the inputs. Denseness. A set A is dense in a set S if A S and Cl (A) = S. Denseness, Uniform.
Reference: [35] <author> Fahlman, S. E. </author> <title> Fast-learning variations on back-propagation: An empirical study. </title> <editor> In Touretzky, D., Hinton, G., and Sejnowski, T., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 38-51, </pages> <address> Pittsburgh, PA, 1989. </address> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: The encoder/decoder problems, 4:2:4 and 8:3:8, were selected because of their closeness to `real world' pattern classification tasks small changes in the input pattern cause small changes in the output pattern <ref> [35] </ref>. A d:q:d encoder/decoder, normally with q &lt; d, is an auto-associator, with the number of binary training patterns equal to d, each d-bit pattern consisting of all but one bit `on' at a time.
Reference: [36] <author> Fahlman, S. E. </author> <title> The Cascade-correlation learning algorithm on the MONK's problems. </title> <editor> In Thurn et al. </editor> <volume> [143], </volume> <pages> chapter 10. </pages>
Reference-contexts: The Alopex simulations used a slightly different input encoding resulting in 15 inputs only CWN trained with BP BP with Weight Cascade Alopex Problem [142] Decay [142] Correlation <ref> [36] </ref> [146] IWN MFN 1 17:3:1 17:2:1 17:1:1 15:3:1 17:4:1 17:3:1 3 17:4:1 17:2:1 17:3:1 15:3:1 17:1 17:2:1 Problem 1 (Attribute 1 = Attribute 2) OR (Attribute 5 = 1) This problem is in standard Disjunctive Normal Form (DNF). 124 examples were selected randomly from the data set for training, while <p> IWNs and MFNs with various 17:q:1 configurations were trained and the configurations resulting in the best generalisation performance are shown in 103 6.5 Generalisation Experiments Table 6.2 Comparison of generalisation performance on the MONK's benchmark CWN trained with BP BP with Weight Cascade Alopex Problem [142] Decay [142] Correlation <ref> [36] </ref> [146] IWN MFN 1 100% 100% 100% 100% 100% 100% 3 93.1% 97.2% 97.2% 100% 100% 100% Table 6.1. A comparison of the generalisation performance of the IWN, MFN, and CWNs on the MONK's benchmark is presented in Table 6.2. <p> It is very interesting to note that the IWN solution for problem 3 does not involve any hidden neurons, suggesting that the problem is linearly separable 9 which in fact is the case if the deliberate misclassifications are removed from the training data <ref> [36] </ref>. This result points towards the robustness of IWNs for handling noise in the training data. The IWN only captures the main features of the training data in its discrete-valued weights.
Reference: [37] <author> Fahlman, S. E. and Lebiere, C. </author> <title> The cascade-correlation learning architecture. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 524-532. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: The most frequently used technique is to start by training the simplest network and try several networks with an increasing number of hidden neurons until the required performance is achieved. Ontogenic schemes are self-constructing techniques, of which the Cascade-correlation method is the most well known example <ref> [37] </ref>. This method starts with a network without any hidden neurons and systematically increases their number during training until the required performance is achieved. Finally, one can also start with a large network and try to decrease its complexity during training to match the complexity of the concept being learned. <p> Currently, many sets of heuristics exist that dynamically modify q (e.g. Cascade-correlation <ref> [37] </ref>), but the author is unaware of any work in which just b w , or both q and b w simultaneously, were varied during training. 3.4 Approximation Capabilities It has been pointed out by Wray and Green [165] that the very fact that feedforward networks are implemented in software/hardware on
Reference: [38] <author> Fausett, L. </author> <title> Fundamentals of Neural Networks: Architectures, Algorithms and Applications, chapter 1. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1994. </year>
Reference-contexts: Artificial multilayer feedforward networks do, however, share some features with the biological brain <ref> [38] </ref>: both are layered structures formed by a number of homogeneous and simple processing elements neurons. <p> The key factor that distinguishes neural networks, both biological and artificial, from traditional computing paradigms is that processing is asynchronous and local to the individual neurons. The major emergent properties that these two classes of networks have in common are associative memory and a degree of fault tolerance <ref> [38] </ref>. The main barrier to the wide acceptance of the plausibility of artificial feedforward networks as analogues of the real ones is the learning procedures.
Reference: [39] <author> Fausett, L. </author> <title> Fundamentals of Neural Networks: Architectures, Algorithms and Applications, </title> <note> chapter 6. In [38], </note> <year> 1994. </year>
Reference-contexts: There is some agreement, however, that it is nothing like the popular procedures used for training feedforward networks. This, due to the fact that those procedures involve some global computation steps, which is in violation of the strictly local theories of learning in biological networks <ref> [39] </ref>. 1.5 The Statistical Connection Many neural network paradigms have their analogues in the statistical arena. The two fields, however, cannot be considered as being identical twins more like step-brothers.
Reference: [40] <author> Fiesler, E., Choudry, A., and Caulfield, H. J. </author> <title> A weight discretization paradigm for optical neural networks. </title> <booktitle> In Proceedings of the International Congress on Optical Science and Engineering, </booktitle> <pages> pages 164-173, </pages> <address> Bellingham, Washington, </address> <year> 1990. </year> <pages> SPIE. </pages>
Reference-contexts: This method results in weights which are non-uniformly discre-tised through a problem dependent multiple-thresholding discretisation function. This technique, sans clustering 8 , was evaluated by Fiesler et al. in <ref> [40] </ref>. The authors claimed their results to be quite encouraging. They did not, however, provide much information about the complexity of the benchmark learning tasks. The most popular learning scheme is just to use medium resolution weights throughout training [33,58,60,112,166]. <p> This was a rather drastic example of the truncation technique. Truncating the last bit or two of high-resolution weights should give more reasonable results. The Continuous-Discrete Learning Method <ref> [40] </ref> follows a more fruitful strategy. In this method, a trained CWN is discretised according to a predetermined stair-case function, and then trained again using the standard BP procedure. The discretisation-training cycle is repeated until the network converges to a satisfactory solution. <p> The quality of this approximation can be improved in one of two ways [22] either by allowing a larger number of discrete levels <ref> [40, 169] </ref>, or by having more hidden neurons [58]. This chapter, while discussing both, will emphasise the latter approach, and will mainly be concerned with weights having small integer values.
Reference: [41] <author> Funahashi, K. </author> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 183-192, </pages> <year> 1989. </year> <note> 140 REFERENCES </note>
Reference-contexts: `stacked together' to form 2- (or d)-dimensional function. 2.2.1 Universal Approximation in C (R d ) Numerous results, utilising a vast array of mathematical tools and making various assumptions about the construct of the network, are available on the universal approximation in C (R d ) property of feedforward networks <ref> [26, 27, 29, 41, 55, 62, 63, 83, 136] </ref>. <p> Cybenko [29] employed Hahn-Banach and Riesz Representation Theorems, Carroll and Dickinson [26] used Radon transforms for a constructive proof, Funahashi <ref> [41] </ref> invoked Fourier analysis and Paley-Wiener theory, while Hornik et al. [63] utilised the Stone-Weierstrass theorem to show the universal approximation property with continuous activation functions on compacta 3;4 in C (R d ) with respect to the supremum 5 norm.
Reference: [42] <author> Gallant, S. I. </author> <title> Neural Network Learning and Expert Systems, chapter 11. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: Functions that are not Borel measurable do exist but are known to mathematicians only as mathematical peculiarities. 10 1.7 Feedforward Network Training: Prerequisites which are generally accepted to have no polynomial-time solution <ref> [42] </ref>. The difficulty of solving problems belonging to this class increases exponentially with the number of inputs. Intractability of loading, however, is the case for a fixed-size CWN only if hidden neurons can be added and eliminated during training then loading is tractable. <p> Moreover, fixed-size CWNs can be trained to achieve `good enough' solutions instead of optimal ones, which, in practice, is not as time consuming as achieving optimal solutions, and is certainly better than having no solution at all <ref> [42] </ref>. 1.7 Feedforward Network Training: Prerequisites The cardinal choice in the training of a CWN is the choice of the number of neurons in the hidden layer. This number is a function of the complexity of the concept to be learned from the training data.
Reference: [43] <author> Gallant, S. I. </author> <title> Neural Network Learning and Expert Systems, </title> <note> chapter 13. In [42], </note> <year> 1993. </year>
Reference-contexts: This progress becomes painfully slow as the system becomes closer to a solution. To overcome this problem, a rounding mechanism for weights with `nearly integer' values was added 9 This `marble in a box' analogy is adapted from <ref> [43] </ref>. 52 3.5 Learning Heuristics to the discretisation process. This mechanism acts as a set of `black holes' centred at each integer value. If a weight falls within the black hole radius, , its value is forced to the centre value of the black hole.
Reference: [44] <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: They "let the data speak for itself " <ref> [44] </ref>. feedforward network. This element is conventionally known as the artificial neuron 1;2 because its function is somewhat analogous to the biological neuron. <p> This first-order approximate rating was determined by summing of the absolute values of synapses connecting the inputs to the hidden neurons. 6.5.4 Handwritten Numeral Recognition The numeral database 20 used for these simulations was collected at Bell Labs <ref> [21, 31, 44, 47, 48] </ref>. It consists of 1200 isolated handwritten samples of numerals 0..9. Twelve individuals were asked to provide 100 samples each while following a given writing style, resulting in 120 examples of each numeral.
Reference: [45] <author> Grimmett, G. R. and Stirzaker, D. R. </author> <title> Probability and Random Processes. </title> <publisher> Claren-don, Oxford, </publisher> <address> England, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: procedure is similar to simulated annealing and is guaranteed to converge with probability 1 to a global minimum. 2.4 Summary The two key results of this chapter were: 13 If a random variable X is strongly stationary then the distribution of X (t) is the same for all times t <ref> [45] </ref>. 14 A random process is ergodic if its ensemble and temporal averages are the same. 15 The cost function, e.g. the one shown in Equation 1.1, depends upon the output of the network, which, in turn, is a function of the activation function (), which transform the x w + <p> Stationary, strongly. If a random variable X is strongly stationary then the distribu tion of X (t) is independent of the time t <ref> [45] </ref>. Subset, Proper. A is a proper subset of B if A B and B 6 A.
Reference: [46] <author> Gruber, M. H. J. </author> <title> Regression Estimators: A Comparative Study. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: Ridge regression estimators are more precise in those situations and are obtained as the estimators whose distance to an ellipsoid (the `ridge'), centred at a least-squares estimate from the origin of a parameter space, is a minimum <ref> [46] </ref>. 37 3.2 Discrete-weight Networks Table 3.1 Weight resolution terminology Weights Bits Low resolution 1-4 Medium resolution 5-16 High resolution 16+ augmented by a penalty term 4 [130]. Another potential advantage of the IWN is its relatively immunity to noise in training data. <p> Ridge regression estimators are more precise in those situations and are obtained as the estimators whose distance to an ellipsoid (the `ridge') centred at a least-squares estimate from the origin of a parameter space is a minimum <ref> [46] </ref>. Ridging, Constrained. Optimisation procedure in which some norm of the weights is constrained to a specific value [130]. Ridging, Penalised. Optimisation procedure in which the cost function is augmented by a penalty term [130]. Ridging, Smoothed. Optimisation procedure in which noise is introduced in the in puts [130].
Reference: [47] <author> Guyon, I., Poujaud, I., Personnaz, L., Dreyfuss, G., Denker, J., and Le Cun, Y. </author> <title> Comparing different neural network architectures for classifying handwritten digits. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 127-132, </pages> <address> Washington, DC, 1989. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address> <note> The numeral database has been kindly made available by Isabelle at ftp://- hope.caltech.edu/pub/mackay/data/att.database. </note>
Reference-contexts: This particular emphasis on comparison was the main reason for the use of just two data subsets. Moreover, all of the previous results cited here on MONK's benchmark and the handwritten numeral recognition database have used the same strategy <ref> [31, 47, 143, 146] </ref>. The train-and-test method has another use in addition to the optimal stopping of training network size or complexity selection. Figure 6.1 shows how a network can can be evaluated on the test data after every training epoch for optimal stopping of training. <p> This first-order approximate rating was determined by summing of the absolute values of synapses connecting the inputs to the hidden neurons. 6.5.4 Handwritten Numeral Recognition The numeral database 20 used for these simulations was collected at Bell Labs <ref> [21, 31, 44, 47, 48] </ref>. It consists of 1200 isolated handwritten samples of numerals 0..9. Twelve individuals were asked to provide 100 samples each while following a given writing style, resulting in 120 examples of each numeral. <p> This database was then carved into two subsets of 600 samples each the first five samples of each numeral from every writer were used for training and the rest for testing (see Figures 6.5 & 6.6). Guyon et al. <ref> [47] </ref>, and Druker and Le Cun [31], both have reported a generalisation performance of 97% with a 256:20:10 network trained using conventional BP, and a 256:40:10 network trained using `double backpropagation', respectively. <p> It should be noted that in that figure, to the human eye, each of the ten numerals is clearly distinguishable from all others even after this 256 ! 32 inputs transform. Although this 8-fold reduction in input dimension did cause a 4% reduction in generalisation performance (compared with <ref> [31, 47] </ref>), it made the running of many more simulations possible due to the reduced memory and CPU requirements. The numeral classification decisions were taken according to the neuron with the maximum signal in the output layer.
Reference: [48] <author> Guyon, I., Vapnik, V., Boser, B., and Solla, S. </author> <title> Structural risk minimisation for character recognition. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: This first-order approximate rating was determined by summing of the absolute values of synapses connecting the inputs to the hidden neurons. 6.5.4 Handwritten Numeral Recognition The numeral database 20 used for these simulations was collected at Bell Labs <ref> [21, 31, 44, 47, 48] </ref>. It consists of 1200 isolated handwritten samples of numerals 0..9. Twelve individuals were asked to provide 100 samples each while following a given writing style, resulting in 120 examples of each numeral.
Reference: [49] <author> Hardy, G. H. and Wright, E. M. </author> <title> An Introduction to the Theory of Numbers, chapter 23. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> England, fifth edition, </address> <year> 1979. </year>
Reference-contexts: The distance between consecutive points of this chain is less than ". There is therefore a point (kn 1 ff) or (nff) within a distance " of any ff of (0; 1). Therefore, (nff) is dense in (0; 1). fl 5 This proof closely follows the approach presented in <ref> [49] </ref>. 86 5.2 Universal Approximation Proof of Theorem 5.1 Following the proof of Lemma 2.2 6 , it must be shown that sp (f (k) j (r; r) : k 0g) = C (R)j (r; r).
Reference: [50] <author> Hardy, G. H. and Wright, E. M. </author> <title> An Introduction to the Theory of Numbers, </title> <booktitle> chapter 23. In [49], fifth edition, 1979. Theorem 439. </booktitle>
Reference-contexts: i 2 R, are uniformly dense on compacta in C (R). fl Proof This follows immediately from Theorem 2.3. fl Lemma 5.2 Given any ff 2 RnQ , if m; n 2 Z then m + nff are dense in R. fl Proof This follows immediately from the following theorem <ref> [50] </ref>: Theorem 5.2 (Kronecker's Theorem in One Dimension) If ff 2 RnQ ; n 2 Z, then the set of points formed by the remainders of nff is dense in the interval (0; 1). fl Proof 5 Let y n = (nff), with n = 1; 2; 3; : : :,
Reference: [51] <author> Hassibi, B., Stork, D. G., and Wolf, G. J. </author> <title> Optimal brain surgeon and general network pruning. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 293-299, </pages> <address> San Francisco, CA, </address> <year> 1993. </year> <note> 141 REFERENCES </note>
Reference-contexts: A common penalty term is the sum of the square of weights [57]. This technique is known as penalised ridging in the statistical literature. 2. Weight elimination is the technique in which the number of non-zero weights is the penalty term <ref> [51, 81, 110, 152] </ref>. The integer-weight learning procedure presented in Chapter 3 incorporates an implicit weight elimination mechanism. 3. Weight sharing is the technique in which the number of independent weights is the penalty term [105, 122]. 4. <p> Weight depth is the number of binary bits in a weight. Weight elimination is a regularisation technique used in feedforward network training in which the cost-function is augmented with a term which penalises the number of non-zero weights <ref> [51, 81, 110, 152] </ref>. Weight perturbation is a hardware-friendly alternative to BP learning. In this method, all of the weights are perturbed in turn and the associated change in the output of the network is used to approximate local gradients [59].
Reference: [52] <author> Haykin, S. </author> <title> Neural Networks: A Comprehensive Foundation, chapter 1. </title> <publisher> Maxwell Macmillan, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference: [53] <author> Haykin, S. </author> <title> Neural Networks: A Comprehensive Foundation, </title> <note> chapter 6. In [52], </note> <year> 1994. </year>
Reference-contexts: As training examples are usually presented to a network in a random order, on-line BP does its search in the weight space in a stochastic manner, and therefore is less prone (compared with batch BP) to getting stuck in local minima 17 of the error surface <ref> [53] </ref>. Weight perturbation is an alternative to BP learning. In this method, all of the weights are perturbed in turn and the associated change in the output of the network is used to approximate local gradients [59]. <p> This procedure is guaranteed to converge to a global minimum. 1.9 Generalisation Performance Generalisation performance 18 the accuracy of a trained network on a set of data which is similar to but not the same as the training data set <ref> [53] </ref> is the key metric which determines a learning paradigm's usefulness [15]. <p> Poor generalisation results from the networks over- or under-fitting the training data. An over-fit is due to the network having a higher complexity than the concept embedded in the training data. This causes the network to essentially become a `look-up table' for the training data <ref> [53] </ref>: the network behaves very well for the training data, but gives erroneous responses to inputs which are nearby but not actual training data. This situation in the one dimensional case is shown in shown in Figure 1.4 (c).
Reference: [54] <author> Haykin, S. </author> <title> Neural Networks: A Comprehensive Foundation, </title> <note> chapter 8. In [52], </note> <year> 1994. </year>
Reference-contexts: This method is analogous to the physical process of annealing [69]. It is different from methods based upon steepest descent in that the myopic skier of the last paragraph is not always going downhill, but going downhill only `most of the time' <ref> [54] </ref>. This way the skier will not get trapped in local minima. In this method, weight modifications are made permanent if the new value of the cost function is lower or equal to the old one.
Reference: [55] <author> Hecht-Nielsen, R. </author> <title> Theory of the backpropagation neural network. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 593-606, </pages> <address> Washington, DC, 1989. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference-contexts: `stacked together' to form 2- (or d)-dimensional function. 2.2.1 Universal Approximation in C (R d ) Numerous results, utilising a vast array of mathematical tools and making various assumptions about the construct of the network, are available on the universal approximation in C (R d ) property of feedforward networks <ref> [26, 27, 29, 41, 55, 62, 63, 83, 136] </ref>.
Reference: [56] <author> Heemskerk, J. N. H. </author> <title> Neurocomputers for Brain-Style Processing. Design, Implementation and Application. </title> <type> PhD thesis, </type> <institution> Unit of Experimental and Theoretical Psychology, Leiden University, </institution> <address> The Netherlands, </address> <year> 1995. </year> <note> A draft version of Chapter 3 is available as ftp://ftp.mrc-apu.cam.ac.uk/pub/nn/murre/neurhard.ps. </note>
Reference-contexts: Electronic hardware is generally compact and cost-effective. Optical hardware has the advantage of free-space connectivity. Analogue electronic systems are usually very fast, but suffer from susceptibility to noise, manufacturing difficulties, and lack of non-volatile on-chip adjustable weights <ref> [56] </ref>. Digital electronic systems have the drawbacks of limited resolution computation and slow speed. They are less susceptible to noise and have non-volatile adjustable weights. Their main advantage is however in the ease of implementation due to the availability of a wide variety of mature VLSI tools and manufacturing facilities. <p> Their main advantage is however in the ease of implementation due to the availability of a wide variety of mature VLSI tools and manufacturing facilities. This is the reason for their popularity, as is clear from the large number of reported systems <ref> [56, 67] </ref>. This section discusses digital electronic systems along with an analogue-digital electronic hybrid that has overcome the limited resolution computation drawback of the digital systems and the lack of non-volatile on-chip adjustable weights of analogue systems.
Reference: [57] <author> Hinton, G. E. </author> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 1-12, </pages> <address> Amherst, MA, </address> <year> 1986. </year> <title> Erlbaum, Hillsdale, NJ. Reproduced in Parallel Distributed Processing: Implications for Psychology and Neurobiology, </title> <editor> Morris, R. G. M. editor. </editor> <publisher> Oxford University Press, Oxford, </publisher> <address> England, </address> <year> 1989. </year>
Reference-contexts: Common regularisation methods are: 1. Weight decay is the technique in which a penalty term is added to the training cost function which penalises large weights. A common penalty term is the sum of the square of weights <ref> [57] </ref>. This technique is known as penalised ridging in the statistical literature. 2. Weight elimination is the technique in which the number of non-zero weights is the penalty term [51, 81, 110, 152]. The integer-weight learning procedure presented in Chapter 3 incorporates an implicit weight elimination mechanism. 3.
Reference: [58] <author> Hoehfeld, M. and Fahlman, S. E. </author> <title> Learning with limited numerical precision using the cascade-correlation algorithm. </title> <type> Preprint, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1991. </year>
Reference-contexts: Most workers, however, agree that networks with weights of medium resolution are adequate to model many learning tasks to the desired accuracy [33, 169], <ref> [58, 60, 112, 166, 167] </ref> 6 . The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two [77, 138] 6 or sum-of-powers-of-two [88, 89] 6 . <p> The main problem with this method is that every single weight change caused by PRS requires the time consuming recalculation of E o for the whole network. Hoehfeld and Fahlman used the gradient-descent based Cascade-correlation onto-genic method combined with probabilistic rounding for training networks with medium-resolution weights <ref> [58] </ref>. If the w calculated by gradient-descent was smaller than the minimum discrete step then the relevant weight was incremented according to a probability proportional to the size of w. This technique showed promising results on test problems of reasonable complexities. Those results were, however, for medium-resolution weights only. <p> The quality of this approximation can be improved in one of two ways [22] either by allowing a larger number of discrete levels [40, 169], or by having more hidden neurons <ref> [58] </ref>. This chapter, while discussing both, will emphasise the latter approach, and will mainly be concerned with weights having small integer values.
Reference: [59] <author> Hollis, P. W. and Paulos, J. J. </author> <title> A neural network learning algorithm tailored for VLSI implementation. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(5) </volume> <pages> 784-791, </pages> <year> 1994. </year>
Reference-contexts: The p = 1 and p = 1 cases are mostly used in hardware implementations because they are easier to compute <ref> [59] </ref>. CWN training procedures can be divide into two major categories: those based on steepest descent methods and those influenced by stochastic optimisation techniques. Steepest descent methods attempt to minimise the cost function by taking small steps on the error surface 12 in the direction of the maximum gradient. <p> Weight perturbation is an alternative to BP learning. In this method, all of the weights are perturbed in turn and the associated change in the output of the network is used to approximate local gradients <ref> [59] </ref>. This technique requires only feedforward calculations for its operation, which simplifies its implementation in hardware [166]. It lacks the mathematical efficiency of BP however, and therefore requires a large number of epochs to reach acceptable solutions. Stochastic methods are the less common alternative to steepest descent. <p> Weight perturbation is an alternative to the BP learning procedure and can be used for discrete-weight learning. In this method, all of the weights are perturbed in turn and the associated change in the output of the network is used to approximate local 44 3.5 Learning Heuristics gradients <ref> [59] </ref>. The weight update rule for this technique is: w = E o initial E o perturbed w initial w perturbed = ffiw (3.1) This technique, although not as efficient as BP, requires only feedforward calculations for its operation, which simplifies its implementation in hardware [166]. <p> A modification of this method, the Chain Rule Perturbation (CHRP) technique <ref> [59] </ref>, overcomes both of these drawbacks. This technique uses Equation 3.1 for updating output layer weights. It then perturbs a hidden neuron output by ffiu to determine E=ffiu. All weights feeding into that hidden neuron are then simultaneously perturbed to compute u=ffiw. <p> It also has the added benefit of semi-parallel operation compared to the strictly serial updating procedure of Equation 3.1 <ref> [59] </ref>. For learning with discrete-weights, this method can be restricted to take discrete steps only. This learning technique is very suitable for on-chip implementations. It lacks the mathematical efficiency of BP however, and therefore requires a large number of epochs to reach acceptable solutions. <p> Weight perturbation is a hardware-friendly alternative to BP learning. In this method, all of the weights are perturbed in turn and the associated change in the output of the network is used to approximate local gradients <ref> [59] </ref>. Weight sharing is a regularisation technique used in feedforward network training in which the cost-function is augmented with a term which penalises the number of independent weights [105, 122].
Reference: [60] <author> Holts, J. H. and Hwang, J.-. N. </author> <title> Finite precision error analysis of neural network hardware implementations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(3) </volume> <pages> 281-290, </pages> <year> 1993. </year> <note> 142 REFERENCES </note>
Reference-contexts: Most workers, however, agree that networks with weights of medium resolution are adequate to model many learning tasks to the desired accuracy [33, 169], <ref> [58, 60, 112, 166, 167] </ref> 6 . The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two [77, 138] 6 or sum-of-powers-of-two [88, 89] 6 .
Reference: [61] <author> Horne, B. G. and Hush, D. R. </author> <title> On the node complexity of neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(9) </volume> <pages> 1413-1426, </pages> <year> 1994. </year>
Reference-contexts: Disjunctive normal form (DNF). The form of a logical expression consisting of a single conjunction () of a set of disjunctions (+). All logical expressions are expressible in this form <ref> [61] </ref>. Effective sample size (for classification learning tasks) is the number of examples rep resenting the smallest classification group [95]. EM algorithm. Expectation Maximisation algorithm calculates the probability density of observations based on parameters and not observations [117].
Reference: [62] <author> Hornik, K. </author> <title> Some new results on neural net approximation. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 1069-1072, </pages> <year> 1993. </year>
Reference-contexts: 2.1 Theoretical Questions For a given artificial feedforward architecture, it is possible to theoretically explore many an interesting issue: What kinds of mappings can it learn (linear, nonlinear, continuous, smooth, measurable) [63, 136]? What type of neuron activation functions are permissible [27, 83, 136]? Is it capable of `universal approximation' <ref> [29, 62, 68] </ref>? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation with the number of necessary training examples [7]? Similarly, <p> `stacked together' to form 2- (or d)-dimensional function. 2.2.1 Universal Approximation in C (R d ) Numerous results, utilising a vast array of mathematical tools and making various assumptions about the construct of the network, are available on the universal approximation in C (R d ) property of feedforward networks <ref> [26, 27, 29, 41, 55, 62, 63, 83, 136] </ref>. <p> Leshno et al. [83] showed that non-polynomial activation functions which are locally bounded and piecewise continuous are necessary for universal approximation. Hornik <ref> [62] </ref> proved universal approximation capability on compacta for networks having activation functions which are locally Riemann integrable and non-polynomial.
Reference: [63] <author> Hornik, K., Stinchcombe, M., and White, H. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year> <note> Reprinted in [157]. </note>
Reference-contexts: The advantage may, however, lie in its ability to construct more compact representations of arbitrary training data sets due to its freedom in having data-dependent activation functions. 1.6 Approximation Properties 2-layer CWNs are universal approximators in the space of Borel measurable functions 10 (see for example <ref> [63] </ref>). In other words, a 2-layer CWN exists that can, given enough training data and enough hidden neurons, approximate virtually any function of interest to any desired degree of accuracy [127]. <p> the next chapter largely comprise review material, whereas the remaining chapters mainly consist of original work. 2 Theoretical Preliminaries 2.1 Theoretical Questions For a given artificial feedforward architecture, it is possible to theoretically explore many an interesting issue: What kinds of mappings can it learn (linear, nonlinear, continuous, smooth, measurable) <ref> [63, 136] </ref>? What type of neuron activation functions are permissible [27, 83, 136]? Is it capable of `universal approximation' [29, 62, 68]? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network <p> `stacked together' to form 2- (or d)-dimensional function. 2.2.1 Universal Approximation in C (R d ) Numerous results, utilising a vast array of mathematical tools and making various assumptions about the construct of the network, are available on the universal approximation in C (R d ) property of feedforward networks <ref> [26, 27, 29, 41, 55, 62, 63, 83, 136] </ref>. <p> Cybenko [29] employed Hahn-Banach and Riesz Representation Theorems, Carroll and Dickinson [26] used Radon transforms for a constructive proof, Funahashi [41] invoked Fourier analysis and Paley-Wiener theory, while Hornik et al. <ref> [63] </ref> utilised the Stone-Weierstrass theorem to show the universal approximation property with continuous activation functions on compacta 3;4 in C (R d ) with respect to the supremum 5 norm.
Reference: [64] <author> Hornik, K., Stinchcombe, M., White, H., and Auer, P. </author> <title> Degree of approximation results for feedforward networks approximating unknown mappings and their derivatives. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 1262-1275, </pages> <year> 1994. </year>
Reference-contexts: mappings can it learn (linear, nonlinear, continuous, smooth, measurable) [63, 136]? What type of neuron activation functions are permissible [27, 83, 136]? Is it capable of `universal approximation' [29, 62, 68]? What is the relation of the quality of approximation with the number of neurons or the depth of weights <ref> [22, 64] </ref>? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation with the number of necessary training examples [7]? Similarly, questions can be raised about the convergence properties of a given combination of a neural architecture and a specific learning heuristic [74]. <p> This logarithmic rate of decrease in error is different from the theoretically calculated degree of approximation result: E o was found to decrease at rate of q 1=2 , where q is the number of hidden neurons <ref> [4, 64] </ref>. 4.3 Approximating CWNs with IWNs In the work reported here, mappings of the form f : R 2 ! f1; 1g 1 , R being a closed interval [1; 1], were used for comparing the IWN and CWN decision surfaces for a set of 10 classification problems (Figure 4.4)
Reference: [65] <author> Hush, D. R. and Horne, B. G. </author> <title> Progress in supervised neural networks: What's new since Lippmann. </title> <journal> IEEE Signal Processing Magazine, </journal> <volume> 10(1) </volume> <pages> 8-39, </pages> <month> January </month> <year> 1993. </year>
Reference: [66] <author> Hwang, J. N., Lay, S. R., Maechler, M., Martin, R. D., and Schimert, J. </author> <title> Regression modelling in back-propagation and projection pursuit learning. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(3) </volume> <pages> 342-353, </pages> <year> 1994. </year>
Reference-contexts: The feedforward network, the only neural paradigm discussed in this thesis, has a direct analogue in statistics: projection pursuit regression <ref> [66] </ref>. Projection pursuit is a generalisation of the CWN in that it allows more than one type of activation function in the hidden layer. These non-homogeneous activation functions are data-dependent and constructed during learning. <p> Also known as `a posteriori' probability. Projection pursuit regression is a generalisation of the feedforward network in that it allows more than one type of activation function in the hidden layer. These nonhomogeneous activation functions are data-dependent and constructed during learning <ref> [66] </ref>. Regularisation A class of methods designed to avoid overfitting to the training data by enforcing smoothness of the fit. Ridge regression. The precision of least-squares estimates gets worse with with an increase in dependence between the input variables.
Reference: [67] <author> Ienne, P. and Kuhn, G. </author> <title> Digital systems for neural networks. </title> <editor> In Papamichalis, P. and Kerwin, R., editors, </editor> <booktitle> Digital Signal Processing Technology, volume CR57 of Critical Reviews Seires, </booktitle> <pages> pages 314-345. </pages> <publisher> SPIE Optical Engineering Press, </publisher> <address> Orlando, FL, </address> <year> 1995. </year> <note> Also available as ftp://mantraftp.epfl.ch/mantra/- ienne.spie95.A4.ps.gz. </note>
Reference-contexts: Their main advantage is however in the ease of implementation due to the availability of a wide variety of mature VLSI tools and manufacturing facilities. This is the reason for their popularity, as is clear from the large number of reported systems <ref> [56, 67] </ref>. This section discusses digital electronic systems along with an analogue-digital electronic hybrid that has overcome the limited resolution computation drawback of the digital systems and the lack of non-volatile on-chip adjustable weights of analogue systems.
Reference: [68] <author> Ito, Y. </author> <title> Approximation of continuous functions on R d by linear combinations of shifted rotations of a sigmoid function with and without scaling. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 105-116, </pages> <year> 1992. </year>
Reference-contexts: 2.1 Theoretical Questions For a given artificial feedforward architecture, it is possible to theoretically explore many an interesting issue: What kinds of mappings can it learn (linear, nonlinear, continuous, smooth, measurable) [63, 136]? What type of neuron activation functions are permissible [27, 83, 136]? Is it capable of `universal approximation' <ref> [29, 62, 68] </ref>? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation with the number of necessary training examples [7]? Similarly,
Reference: [69] <author> Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680, </pages> <year> 1983. </year>
Reference-contexts: It lacks the mathematical efficiency of BP however, and therefore requires a large number of epochs to reach acceptable solutions. Stochastic methods are the less common alternative to steepest descent. A popular representative of these methods is simulated annealing. This method is analogous to the physical process of annealing <ref> [69] </ref>. It is different from methods based upon steepest descent in that the myopic skier of the last paragraph is not always going downhill, but going downhill only `most of the time' [54]. This way the skier will not get trapped in local minima. <p> Stochastic techniques like simulated annealing <ref> [69] </ref> can also be used to train CWNs or discrete-weight networks. These techniques have the advantage of global optimisa-tion they search for global minima in error surfaces, and do not get stuck in local minima, as BP can.
Reference: [70] <author> Kiselewich, S. J. and Turner, D. T. </author> <title> Using a neural network to distinguish between deployment events and non-deployment events in a supplemental inflatable 143 REFERENCES restraint system. </title> <booktitle> Automotive Engineering, </booktitle> <address> 103(6):S5, </address> <month> June </month> <year> 1995. </year> <note> Related article available as http://www.delco.com/techpapers/tech_neural.html. </note>
Reference-contexts: An example of an application in automotive safety is Delco Electronics' supplemental inflatable restraint (air bag) controller which uses a feedforward network to distinguish between deployment and nondeployment events <ref> [70] </ref>. Time series data collected from instrumented vehicle crash tests, along with the requirements of when and if air bag must be inflated for different type of situations, was used to train the feedforward network.
Reference: [71] <author> Knowler, W. C., Bennet, P. H., Hamman, R. F., and Miller, M. </author> <title> Diabetes incidence and prevalence in Pima Indians: A 19-fold greater incidence than in Rochester, Minnesota. </title> <journal> American Journal of Epidemiology, </journal> <volume> 108 </volume> <pages> 497-505, </pages> <year> 1978. </year>
Reference: [72] <author> Knowler, W. C., Pettitt, D. J., Savage, P. J., and Bennet, P. H. </author> <title> Diabetes incidence in Pima Indians: </title> <journal> Contributions of obesity and parental diabetes. American Journal of Epidemiology, </journal> <volume> 113 </volume> <pages> 144-156, </pages> <year> 1981. </year>
Reference: [73] <author> Koistinen, P. and Holmstro m, L. </author> <title> Kernel regression and backpropagation training with noise. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 1033-1039. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: Addition of noise to weights during training results in robust internal representa tions which have better generalisation performances [99, 100]. 5. Addition of noise to inputs during training yields a smoother function by providing small variations to the input <ref> [73, 131] </ref>. This technique is known as smoothed ridging in the statistical literature. 6. Optimal stopping of training uses two sets of data during training, one for training and the other for testing, to stop the training when error on the test data hits a minimum.
Reference: [74] <author> Kuan, C. M. and Hornik, K. </author> <title> Convergence of learning algorithms with constant learning rates. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(5) </volume> <pages> 484-489, </pages> <year> 1991. </year>
Reference-contexts: [22, 64]? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation with the number of necessary training examples [7]? Similarly, questions can be raised about the convergence properties of a given combination of a neural architecture and a specific learning heuristic <ref> [74] </ref>. This chapter discusses only two of these issues because of their relevance to the work presented in Chapters 3 & 5: a feedforward network's approximation properties and its convergence characteristics with respect to the error backpropagation heuristic. <p> This study concludes that on-line BP converges with probability 1 12 for successive values of the learning rate t ; t+1 ; : : : if t / t -; 0 &lt; - 1. Kuan and Hornik <ref> [74] </ref> investigated on-line BP learning with a small, constant learn 11 Error in the output of a neuron is not backpropagated if it is within a specified small margin. 12 If f^a n g is a sequence of random variables then ^a n converges with probability 1 to a, i.e. ^a <p> This convergence in probability result is weaker than the `convergence with probability 1' result, and means that convergence to optimal weights in most training runs is expected but not guaranteed in general for on-line BP with a small constant learning rate <ref> [74] </ref>. The discussion in this section has so far been about networks converging to a minimum in the error surface: this minimum can be a local one or a global minimum.
Reference: [75] <author> Kushner, H. </author> <title> Asymptotic global behavior for stochastic approximations and diffusions with slowly decreasing noise effects: Global minimisation via Monte Carlo. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 47 </volume> <pages> 169-185, </pages> <year> 1987. </year>
Reference-contexts: An example of this approach is a modified version of on-line BP with a diminishing learning rate: the modification being the addition of decreasing amounts of random noise to the weights at each weight BP modification step <ref> [75] </ref>. <p> A modified version of on-line BP with a diminishing learning rate has been proposed for finding global minima, the modification being the addition of decreasing amounts of random noise to the weights at each weight modification step <ref> [75] </ref>.
Reference: [76] <author> Kwan, H. K. and Tang, C. Z. </author> <title> Designing multilayer feedforward neural networks using simplified sigmoid activation functions and one-power-of-two weights. </title> <journal> Electronics Letters, </journal> <volume> 28(25) </volume> <pages> 2343-2344, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: very beginning to concentrate on designing heuristics that will get the job done in a reasonable number of epochs and the task of achieving the fastest convergence times was therefore not considered at all. 3.5.1 Weight Discretisation Schemes Networks having powers-of-two weights represent an interesting discretisation scheme for digital implementations <ref> [76, 77, 138] </ref>. They have the advantage in arithmetical manipulations multiplying a number with a power-of-two weight can be accomplished by a simple shift operation on that number, which is faster and requires much less hardware than the conventional digital multiplier.
Reference: [77] <author> Kwan, H. K. and Tang, C. Z. </author> <title> Multiplierless multilayer feedforward neural network design suitable for continuous input-output mapping. </title> <journal> Electronics Letters, </journal> <volume> 29(14) </volume> <pages> 1259-1260, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two <ref> [77, 138] </ref> 6 or sum-of-powers-of-two [88, 89] 6 . For such weights, multiplication operations can be replaced by much simpler shift operations or a sequence of sum-and-shift steps, respectively. <p> very beginning to concentrate on designing heuristics that will get the job done in a reasonable number of epochs and the task of achieving the fastest convergence times was therefore not considered at all. 3.5.1 Weight Discretisation Schemes Networks having powers-of-two weights represent an interesting discretisation scheme for digital implementations <ref> [76, 77, 138] </ref>. They have the advantage in arithmetical manipulations multiplying a number with a power-of-two weight can be accomplished by a simple shift operation on that number, which is faster and requires much less hardware than the conventional digital multiplier.
Reference: [78] <author> Lang, K. J. and Witbrock, M. J. </author> <title> Learning to tell two spirals apart. </title> <editor> In Touretzky, D., Hinton, G., and Sejnowski, T., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 52-59, </pages> <address> Pittsburgh 1988, 1989. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. 144 REFERENCES </address>
Reference-contexts: For all four problems, the CWN requires 2 Skip-layer synapses are the synapses connecting neurons in two non-adjacent layers. On some problems, their use has been found to result in more compact solutions. Also known as short-cut synapses <ref> [78] </ref>. <p> Definitions vary but are generally taken to be bounded, mono tone, and continuous, e.g. logistic and tanh () functions. Simulated annealing is a stochastic optimisation technique inspired by the physical process of annealing. 133 GLOSSARY Skip-layer synapses. Synapses connecting neurons in two non-adjacent layers. Also known as short-cut synapses <ref> [78] </ref>. Known as main effects in the statistical litera ture [127]. Smoothing spline modelling is piecewise approximation by polynomials of degree n with the requirement that the derivatives of the polynomials are continuous up to degree n 1 at the junctions [20]. Softmax.
Reference: [79] <author> Le Cun, Y. </author> <title> A learning scheme for asymmetric threshold networks. </title> <booktitle> In Proceedings of Cognitiva 85, </booktitle> <pages> pages 599-604, </pages> <address> Paris, France, </address> <year> 1985. </year>
Reference-contexts: The breakthrough came in 1986 with the publication of two volumes by the PDP group titled Parallel Distributed Processing: Explorations in the Microstructure of Cognition [123, 124]. Although ideas similar to the BP learning rule had been presented earlier <ref> [79, 109, 154] </ref>, it became popular only after the publication of these volumes.
Reference: [80] <author> Le Cun, Y. Modeles connexionistes de l'apprentissage. </author> <type> PhD thesis, </type> <institution> Universite Peierre et Marie Curie, </institution> <year> 1987. </year>
Reference-contexts: The next big step in the continued development of the feedforward network was the development of proofs on the universal approximation capability of the 3-layer CWN <ref> [80] </ref> and later on the 2-layer CWN [29] in the late 80's. The last major achievement in this field, though still in its infancy, has been the integration of CWNs in a Bayesian framework (see, for example [86, 102, 141]).
Reference: [81] <author> Le Cun, Y., Denker, J. S., and Solla, S. A. </author> <title> Optimal brain damage. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605, </pages> <address> Denver 1989, 1990. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: A common penalty term is the sum of the square of weights [57]. This technique is known as penalised ridging in the statistical literature. 2. Weight elimination is the technique in which the number of non-zero weights is the penalty term <ref> [51, 81, 110, 152] </ref>. The integer-weight learning procedure presented in Chapter 3 incorporates an implicit weight elimination mechanism. 3. Weight sharing is the technique in which the number of independent weights is the penalty term [105, 122]. 4. <p> Weight depth is the number of binary bits in a weight. Weight elimination is a regularisation technique used in feedforward network training in which the cost-function is augmented with a term which penalises the number of non-zero weights <ref> [51, 81, 110, 152] </ref>. Weight perturbation is a hardware-friendly alternative to BP learning. In this method, all of the weights are perturbed in turn and the associated change in the output of the network is used to approximate local gradients [59].
Reference: [82] <author> Le Cun, Y., Kanter, I., and Sola, S. A. </author> <title> Eigenvalues of covariance matrices: Application to neural network learning. </title> <journal> Physical Review Letters, </journal> <volume> 66(18) </volume> <pages> 2396-2399, </pages> <year> 1991. </year>
Reference-contexts: Moreover, Le Cun et al. have shown analytically that standardising the inputs to zero mean improves the convergence properties of some learning procedures <ref> [82] </ref>. For discrete inputs or outputs, categorical variables should not be treated as continuous variables [13]. For instance, one of the input variables for the training data to be used in Section 6.5.2 represents shapes and can have one of three values: round, square, or octagon. <p> Moreover, Le Cun et al. have analytically shown that standardising the inputs to zero mean improves the convergence properties of BP learning <ref> [82] </ref>. A comparison of the generalisation performance of the best IWN, MFN, and CWN on the diabetes database is presented in Table 6.4. The three generalisation performances are within 1.5% of each other, with CWN being the best and IWN the worst.
Reference: [83] <author> Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. </author> <title> Multilayer feedforward networks with nonpolynomial activation functions can approximate any function. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 861-867, </pages> <year> 1993. </year>
Reference-contexts: chapters mainly consist of original work. 2 Theoretical Preliminaries 2.1 Theoretical Questions For a given artificial feedforward architecture, it is possible to theoretically explore many an interesting issue: What kinds of mappings can it learn (linear, nonlinear, continuous, smooth, measurable) [63, 136]? What type of neuron activation functions are permissible <ref> [27, 83, 136] </ref>? Is it capable of `universal approximation' [29, 62, 68]? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation <p> `stacked together' to form 2- (or d)-dimensional function. 2.2.1 Universal Approximation in C (R d ) Numerous results, utilising a vast array of mathematical tools and making various assumptions about the construct of the network, are available on the universal approximation in C (R d ) property of feedforward networks <ref> [26, 27, 29, 41, 55, 62, 63, 83, 136] </ref>. <p> Leshno et al. <ref> [83] </ref> showed that non-polynomial activation functions which are locally bounded and piecewise continuous are necessary for universal approximation. Hornik [62] proved universal approximation capability on compacta for networks having activation functions which are locally Riemann integrable and non-polynomial. <p> pointed out by Wray and Green [165] that the very fact that feedforward networks are implemented in software/hardware on computers of finite word-lengths implies that the hidden-neuron activation functions are finite polynomials 7 , and are therefore not capable of universal approximation, as was shown by Leshno et al. in <ref> [83] </ref>.
Reference: [84] <author> Lisboa, P. G. J. </author> <title> Private communication, </title> <month> September </month> <year> 1996. </year>
Reference-contexts: The first case will results in good generalisation, whereas the latter two in poor. 15 2.1 Universal approximation in one dimension. 26 3.1 Two choices for the discretising function Q (w) <ref> [84] </ref>. 47 3.2 Frequency distribution of numbers generated by tan (RND). 51 3.3 Q prac (w). 53 3.4 Comparison of the actual error term and its approximation in the range [3; 3]. 53 3.5 The black-hole function. 54 4.1 The set of decision boundaries of an integer [-3, 3] weight 2-input
Reference: [85] <editor> Lisboa, P. G. J. and Taylor, J. G., editors. </editor> <booktitle> Workshop on Neural Network Applications and Tools, </booktitle> <address> Liverpool, England, </address> <month> September </month> <year> 1993, </year> <title> 1994. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference-contexts: A large number of papers on the application of CWNs have since been published, in areas of application ranging from medicine, finance, process industry, high energy physics, automotives, telecommunications, robotics, to aerospace <ref> [85, 111, 159] </ref>. These applications are generally divided into two groups: function approximation 5 and classification. This division is based on the type of the desired outputs required to accomplish the task.
Reference: [86] <author> MacKay, D. </author> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 448-472, </pages> <year> 1992. </year>
Reference-contexts: The last major achievement in this field, though still in its infancy, has been the integration of CWNs in a Bayesian framework (see, for example <ref> [86, 102, 141] </ref>).
Reference: [87] <author> MacKay, D. J. </author> <title> Probable networks and plausible predictions a review of practical Bayesian methods for supervised neural networks. </title> <type> Technical report, </type> <institution> Cavendish Laboratory, University of Cambridge, </institution> <address> Cambridge, England, </address> <year> 1995. </year> <note> Available as ftp://mraos.ra.phy.cam.ac.uk/pub/mackay/network.ps.Z. </note>
Reference-contexts: This is the most recent, and potentially the most useful, technique for training feedforward networks. The `exact' implementation of this technique suffers from the slow speed of training, the recent `approximate' implementations, however, hold much promise <ref> [87, 130, 141] </ref>. The regularisation method selected for the experiments presented here is `optimal stopping of training'. This popular technique is fast, works well with larger-than-necessary networks, and does not require the introduction of any new training parameters except the ratio of train/test data split [130].
Reference: [88] <author> Marchesi, M., Benvenuto, N., Orlandi, G., Piazza, F., and Uncini, A. </author> <title> Design of multi-layer neural networks with power-of-two weights. </title> <journal> In IEEE ISCS, </journal> <volume> volume 4, </volume> <pages> pages 2951-2954, </pages> <address> New Orleans 1-3 May 1990, 1990. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. 145 REFERENCES </address>
Reference-contexts: The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two [77, 138] 6 or sum-of-powers-of-two <ref> [88, 89] </ref> 6 . For such weights, multiplication operations can be replaced by much simpler shift operations or a sequence of sum-and-shift steps, respectively. Results on some very simple leaning tasks with feedforward networks having binary and ternary weights were presented by Perez Vincente et al. in [148]. <p> A variation on this scheme simple sums of powers-of-two weights - can provide better resolutions while somewhat maintaining the ease of multiplication: multiplying a number with a sum-of-power-of-two weight can be accomplished by a short sequence of simple shift-and-add operations <ref> [88, 89] </ref>. These two schemes and the related learning procedures were inspired by similar hardware streamlining work on FIR filter design [170]. Another possible avenue is to take a trained CWN, and analyse its weight distribution to find any clusters. <p> The fundamental assumption behind this learning scheme is that the locations of acceptable minima for discrete-weight networks and CWNs are the same. This author is, however, unaware of any evidence, theoretical or experimental, which supports this assumption. The method put forward by Marchesi et al. <ref> [88, 89] </ref> for generating networks with powers-of-two or sum of powers-of-two weights and unrestricted offsets uses a trained CWN as the starting point.
Reference: [89] <author> Marchesi, M., Orlandi, G., Piazza, F., and Uncini, A. </author> <title> Fast neural networks without multipliers. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(1) </volume> <pages> 53-62, </pages> <year> 1993. </year>
Reference-contexts: The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two [77, 138] 6 or sum-of-powers-of-two <ref> [88, 89] </ref> 6 . For such weights, multiplication operations can be replaced by much simpler shift operations or a sequence of sum-and-shift steps, respectively. Results on some very simple leaning tasks with feedforward networks having binary and ternary weights were presented by Perez Vincente et al. in [148]. <p> A variation on this scheme simple sums of powers-of-two weights - can provide better resolutions while somewhat maintaining the ease of multiplication: multiplying a number with a sum-of-power-of-two weight can be accomplished by a short sequence of simple shift-and-add operations <ref> [88, 89] </ref>. These two schemes and the related learning procedures were inspired by similar hardware streamlining work on FIR filter design [170]. Another possible avenue is to take a trained CWN, and analyse its weight distribution to find any clusters. <p> The fundamental assumption behind this learning scheme is that the locations of acceptable minima for discrete-weight networks and CWNs are the same. This author is, however, unaware of any evidence, theoretical or experimental, which supports this assumption. The method put forward by Marchesi et al. <ref> [88, 89] </ref> for generating networks with powers-of-two or sum of powers-of-two weights and unrestricted offsets uses a trained CWN as the starting point.
Reference: [90] <author> Marsden, J. E. </author> <title> Elementary Classical Analysis. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1974. </year>
Reference-contexts: Compact set. A closed and bounded subset of R d . Also known as a compact. Compact in R n . A is a compact in R n if it is a subset of R n , is a closed set, and is a bounded set <ref> [90] </ref>. Convergence, Pointwise. <p> In the case of feedforward networks this quantity is usually the RMS error in the output of the network. Also known as error measure. Cover of a set A is a collection of sets fT i g whose union contains A <ref> [90] </ref>. Cover, Open. It is an open cover of fT i g if each T i is open [90]. Cover, Sub- of a given cover is a subcollection whose union also contains A [90]. Cross-validation, n-fold. <p> Also known as error measure. Cover of a set A is a collection of sets fT i g whose union contains A <ref> [90] </ref>. Cover, Open. It is an open cover of fT i g if each T i is open [90]. Cover, Sub- of a given cover is a subcollection whose union also contains A [90]. Cross-validation, n-fold. <p> Cover of a set A is a collection of sets fT i g whose union contains A <ref> [90] </ref>. Cover, Open. It is an open cover of fT i g if each T i is open [90]. Cover, Sub- of a given cover is a subcollection whose union also contains A [90]. Cross-validation, n-fold. The data set is divided equally into k randomly selected, mutually exclusive subsets called folds. k 1 networks are trained sequentially on all combinations of k 1 folds, while the performance of the trained networks is tested on the one remaining folds. <p> Set, Closed. A subset M of metric space N is a closed set if it contains each of its limit points. Set, Finite. A is finite if all of its elements can be displayed as fa 1 ; a 2 ; :::; a n g for some integer n <ref> [90] </ref>. Set, Open is the subset G of the metric space X if each point of G is the centre of some open sphere contained in G. Shattered.
Reference: [91] <author> Masters, T. </author> <title> Signal and Image Processing with Neural Networks: A C++ Source-book, chapter 3. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: The only way they can handle it is through using more hidden neurons, which results in more expensive training. Similarly, for the outputs, if very fine tolerances are set, it may require large weights to achieve those tolerances, which again results in more expensive training <ref> [91] </ref>. This chapter was exclusively devoted to integer valued weights.
Reference: [92] <author> Mauduit, N., Duaranton, M., Gobert, J., and Sirat, J. A. L Neuro 1.0: </author> <title> A piece of hardware LEGO for building neural network systems. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(3) </volume> <pages> 414-422, </pages> <year> 1992. </year>
Reference-contexts: This section discusses digital electronic systems along with an analogue-digital electronic hybrid that has overcome the limited resolution computation drawback of the digital systems and the lack of non-volatile on-chip adjustable weights of analogue systems. The L Neuro 1.0 chip manufactured by Philips <ref> [92] </ref> is an example of a simple digital implementation of feedforward networks with adjustable weights. It can be configured as having 64 neurons with 8-bit weights or 256 neurons with 4-bit weights. Each neuron has 16 inputs.
Reference: [93] <editor> Michie, D., Spiegelhalter, D. J., and Taylor, C. C., editors. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The second set, onset of diabetes prediction data set, has both discrete and continuous inputs, may have some irrelevant inputs, may have much noise in the inputs, may have a high degree of correlation between inputs, and has a single binary output <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> It will be interesting to compare this ranking with the weight structure of the trained networks. This database has been analysed with the help of connectionist tools in <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> The best overall results of this study was a generalisation performance of 81% obtained by using a mixture representation 18 12 Logistic discriminant analysis chooses classification hyperplanes with respect to maximising a conditional likelihood cost-function and not optimising a quadratic cost function which is the case for linear discriminant analysis <ref> [93] </ref>. 13 Smoothing spline modelling is piecewise approximation by polynomials of degree n with the requirement that the derivatives of the polynomials are continuous up to degree n1 at the junctions [20]. 14 Bayesian classifier assigns a class to an object in such a way that the expectation value of misclas <p> The problem of finding the optimal weight values for a given network such that the network performs the required mapping. Logistic discriminant analysis chooses classification hyperplanes with respect to max-imising a conditional likelihood cost-function and not optimising a quadratic cost function which is the case for linear discriminant analysis <ref> [93] </ref>. 130 GLOSSARY Margin, " n . Error in the output of a neuron is not backpropagated if it is within this small margin [139]. Minima, Global. The points of minimum error on an error surface. Minima, Local.
Reference: [94] <editor> Michie, D., Spiegelhalter, D. J., and Taylor, C. C., editors. </editor> <title> Machine Learning, Neural and Statistical Classification, chapter 6. </title> <editor> In Michie et al. </editor> <volume> [93], </volume> <year> 1994. </year>
Reference-contexts: used in Chapter 5 to prove the universal approximation property of the multiplier-free net. 2.3 Convergence of Error Backpropagation The error backpropagation heuristic for training feedforward networks, the method of choice for many users, is not a completely new training procedure: the batch version is known as (total) gradient descent <ref> [94] </ref>, the on-line version is familiar to statisticians as stochastic gradient descent [156], and the momentum modification of the batch version can be recognised as the `heavy ball' method discussed in the numerical analysis 32 2.3 Convergence of Error Backpropagation literature [126].
Reference: [95] <editor> Michie, D., Spiegelhalter, D. J., and Taylor, C. C., editors. </editor> <title> Machine Learning, Neural and Statistical Classification, chapter 7. </title> <editor> In Michie et al. </editor> <volume> [93], </volume> <year> 1994. </year>
Reference-contexts: The third conclusion points towards the better suitability of the IWN and MFN as generalisers as compared with the CWN. 6.2.1 Empirical Estimation Comparative experiments for the empirical estimation of generalisation performance can be set up in one of three ways <ref> [95, 153] </ref>: 1. Train-and-test A random sample containing one half of the total number of examples is selected. This subset is used to train the network, while the remaining examples are used to test the network once it has been trained. <p> This procedure is repeated a large number of times. The average of all such test errors is an estimate of the generalisation performance metric. The train-and-test technique is recommended for large (&gt;1000 samples) data sets, cross-validation for ones with intermediate sizes, and bootstrap for small data sets <ref> [95] </ref>. For classification learning tasks, the size of data sets should be measured by the number of examples of the smallest classification group, which is termed as the effective sample size. The generalisation performances estimated by the three techniques converge as the size of the data sets grow. <p> Disjunctive normal form (DNF). The form of a logical expression consisting of a single conjunction () of a set of disjunctions (+). All logical expressions are expressible in this form [61]. Effective sample size (for classification learning tasks) is the number of examples rep resenting the smallest classification group <ref> [95] </ref>. EM algorithm. Expectation Maximisation algorithm calculates the probability density of observations based on parameters and not observations [117]. Epoch is the cycle in which all examples in the training set are presented to the net work. Ergodic process.
Reference: [96] <editor> Michie, D., Spiegelhalter, D. J., and Taylor, C. C., editors. </editor> <title> Machine Learning, Neural and Statistical Classification, chapter 9. </title> <editor> In Michie et al. </editor> <volume> [93], </volume> <year> 1994. </year>
Reference-contexts: This equality of the two figures was achieved by thresholding the continuous 0-1 output of the model at 0.448 [134]. Michie et al. used a set of 22 machine learning, neural, and statistical techniques to analyse this data <ref> [96] </ref>. They used 12-fold cross-validation to determine generalisation performance. They achieved their best results of 87.7% with logistic discriminant analysis 12 , whereas a figure of 75.2% was obtained with a CWN trained using BP.
Reference: [97] <author> Moody, J. E. </author> <title> The effective number of parameters: An analysis of generalisa-tion and regularization in nonlinear learning systems. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: The target network of the generalisation experiments, the optimal network, is then defined. After presenting the methodology of the learning experiments, this chapter concludes with the results on the three data sets. 94 6.2 Estimation of Generalisation Performance 6.2 Estimation of Generalisation Performance Moody <ref> [97] </ref> has proposed the following relationship for estimating the error on test data, E o ts , from the sum-of-squares error on T training examples, E o tr (T ) [16]: E o ts = T T where W eff is the number of effective weights in the CWN, and 2 <p> The number of effective weights is usually smaller than the total number of weights in the CWN <ref> [97] </ref> and has to be estimated. The effective number of training examples may also differ from the actual. is another parameter that needs to be estimated. The difficulty of accurately calculating these three estimates makes the prediction of E o ts using Equation 6.1 infeasible.
Reference: [98] <author> Murphy, P. M. and Aha, D. W. </author> <title> UCI Repository of Machine Learning. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA, </address> <year> 1995. </year> <note> The repository is accessible as ftp://ics.uci.edu/pub/- machine-learning-databases/. </note>
Reference-contexts: The goal is to learn the logical class description from only a subset of the description of the 432 robots that can be classified. 7 This benchmark is publicly available in the directory ftp://ics.uci.edu/pub/machine-learning-databases/monks-problems/ <ref> [98] </ref> 8 Alopex is a stochastic learning procedure which uses local correlations between changes in individual weights and changes in the cost function to update those weights. <p> make this prediction. 768 such clinical histories constitute the data set out of which 268 are for patients who tested positive for diabetes within five years of the clinical examination and the rest were found to be healthy. 10 The data set is publicly available in the directory ftp://ics.uci.edu/pub/machine-learning -databases/pima-indians-diabetes/ <ref> [98] </ref> 11 The patient has diabetes mellitus if the plasma glucose concentration after 2 hours in an oral glucose tolerance test is 200mg/dl. 105 6.5 Generalisation Experiments 49% of the 768 clinical records had zero values for attributes which cannot be zero. These are most probably missing values [114].
Reference: [99] <author> Murray, A. F. </author> <title> Multilayer perceptron learning optimised for on-chip implementation A noise robust system. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 366-381, </pages> <year> 1992. </year> <note> 146 REFERENCES </note>
Reference-contexts: Weight sharing is the technique in which the number of independent weights is the penalty term [105, 122]. 4. Addition of noise to weights during training results in robust internal representa tions which have better generalisation performances <ref> [99, 100] </ref>. 5. Addition of noise to inputs during training yields a smoother function by providing small variations to the input [73, 131]. This technique is known as smoothed ridging in the statistical literature. 6.
Reference: [100] <author> Murray, A. F. and Edwards, P. J. </author> <title> Synaptic weight noise during multilayer perceptron training: Fault tolerance and training improvements. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(4) </volume> <pages> 722-725, </pages> <year> 1993. </year>
Reference-contexts: Weight sharing is the technique in which the number of independent weights is the penalty term [105, 122]. 4. Addition of noise to weights during training results in robust internal representa tions which have better generalisation performances <ref> [99, 100] </ref>. 5. Addition of noise to inputs during training yields a smoother function by providing small variations to the input [73, 131]. This technique is known as smoothed ridging in the statistical literature. 6.
Reference: [101] <author> Mustafa, T. </author> <title> Private communication, </title> <month> June </month> <year> 1994. </year>
Reference-contexts: An above normal value is a risk factor. 4. Triceps skin fold thickness (mm). It indicates the degree of obesity. 5. 2-hour serum insulin (mu U/ml). It is below normal for insulin dependent diabetics but not for non-insulin dependent diabetics, and therefore, is not a good predictor <ref> [101] </ref>. This was the attributes missing in most of the cases with missing attribute values. 6. Body mass index (kg/m 2 ). It also indicates the degree of obesity. 7. Diabetes pedigree function. It represents the hereditary risk factor. 8. Age (years). <p> It represents the hereditary risk factor. 8. Age (years). From a clinical point of view, the glucose concentration, diabetes pedigree, age, and obesity are the major risk factors, the insulin level is of some importance, and the blood pressure and number of pregnancies are the least important indicators <ref> [101] </ref>. It will be interesting to compare this ranking with the weight structure of the trained networks. This database has been analysed with the help of connectionist tools in [9, 25, 93, 113, 134, 145, 149, 150].
Reference: [102] <author> Neal, R. M. </author> <title> Bayesian Learning in Neural Networks. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto, Canada, </institution> <year> 1995. </year>
Reference-contexts: The last major achievement in this field, though still in its infancy, has been the integration of CWNs in a Bayesian framework (see, for example <ref> [86, 102, 141] </ref>).
Reference: [103] <author> Nelson, M. C. and Illingworth, W. T. </author> <title> A Practical Guide to Neural Nets. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1991. </year>
Reference-contexts: It has been successfully tested in many experiments where the number of examples is relatively large compared with the number of adjustable parameters in the network <ref> [103, 130, 151] </ref>. 97 6.4 Optimal Network 7. Bayesian methods define a set of model regularisation parameters, e.g. the strength of the weight decay mechanism, and make the estimation of the optimal values of those parameters a part of the learning process.
Reference: [104] <author> Nilsson, N. </author> <title> Learning Machines: Foundations of Trainable Pattern Classifiers. </title> <publisher> McGraw Hill, </publisher> <address> New York, NY, </address> <year> 1965. </year> <title> Republished as The Mathematical Foundations of Learning Machines, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Such artificial layered structures were first investigated in the late 50's. Their usefulness as iterative learning machines did not manifest itself in that era due to the lack of a suitable learning algorithm <ref> [104] </ref>. The breakthrough came in 1986 with the publication of two volumes by the PDP group titled Parallel Distributed Processing: Explorations in the Microstructure of Cognition [123, 124].
Reference: [105] <author> Nowlan, S. J. and Hinton, G. E. </author> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 473-493, </pages> <year> 1992. </year>
Reference-contexts: The integer-weight learning procedure presented in Chapter 3 incorporates an implicit weight elimination mechanism. 3. Weight sharing is the technique in which the number of independent weights is the penalty term <ref> [105, 122] </ref>. 4. Addition of noise to weights during training results in robust internal representa tions which have better generalisation performances [99, 100]. 5. Addition of noise to inputs during training yields a smoother function by providing small variations to the input [73, 131]. <p> Weight sharing is a regularisation technique used in feedforward network training in which the cost-function is augmented with a term which penalises the number of independent weights <ref> [105, 122] </ref>.
Reference: [106] <author> Obradovic, Z. and Parberry, I. </author> <title> Computing with discrete multi-valued neurons. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 45 </volume> <pages> 471-492, </pages> <year> 1992. </year>
Reference-contexts: Hence: An IWN, or any feedforward network having bounded and discrete weights only, cannot be a universal approximator. 7 In the case of analogue computers, their inherent inaccuracy due to imperfections in fabrication and fluctuation in operating conditions result in similar problems <ref> [106] </ref>. 40 3.5 Learning Heuristics This means that IWNs may not be able to learn every given mapping with arbitrary accuracy. This is quite a drawback! On the other hand, it is always useful to try to model a given mapping with integer weights.
Reference: [107] <author> Oja, E. </author> <title> Position paper before panel discussion on Neural Networks and Statistical Models. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pages 20-21, </pages> <address> Washington, DC, June 1996. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference-contexts: The two fields, however, cannot be considered as being identical twins more like step-brothers. The main differences between them are perhaps more cultural than technical <ref> [107] </ref>: statistics has its roots in mathematics, neural networks in engineering, biology, and computer science. Statisticians are usually conservative and pessimistic, neural folk enthusiastic. Statisticians are often concerned with small samples, neural workers with large data sets.
Reference: [108] <author> Oteki, S., Hashimoto, A., Furuta, T., Watanabe, T., Stork, D. G., and Eguchi, H. </author> <title> A digital neural network vlsi with on-chip learning using stochastic pulse encoding. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 3, </volume> <pages> pages 3039-3045, </pages> <address> Nagoya, Japan, October 1993. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference-contexts: More than one chip can be cascaded to increase the number of neurons or the number of inputs for each neuron. The RN-200 chip by Ricoh <ref> [108] </ref> is a unique design in that it implements its input, internal, and output signals as stochastic pulse trains. The advantage of this encoding is in the ease of computation: sums can be implemented as a logical OR, products as AND, and 1 x as COMPLEMENT (x).
Reference: [109] <author> Parker, D. B. Learning-logic: </author> <title> Casting the cortex of the human brain in silicon. </title> <type> Technical report TR-47, </type> <institution> Center for Computational Research in Economics and Management Science, MIT, </institution> <address> Cambridge, MA, </address> <month> April </month> <year> 1985. </year> <note> 147 REFERENCES </note>
Reference-contexts: The breakthrough came in 1986 with the publication of two volumes by the PDP group titled Parallel Distributed Processing: Explorations in the Microstructure of Cognition [123, 124]. Although ideas similar to the BP learning rule had been presented earlier <ref> [79, 109, 154] </ref>, it became popular only after the publication of these volumes.
Reference: [110] <author> Prechelt, L. </author> <title> Adaptive parameter pruning in neural networks. </title> <type> Technical report TR-95-009, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: A common penalty term is the sum of the square of weights [57]. This technique is known as penalised ridging in the statistical literature. 2. Weight elimination is the technique in which the number of non-zero weights is the penalty term <ref> [51, 81, 110, 152] </ref>. The integer-weight learning procedure presented in Chapter 3 incorporates an implicit weight elimination mechanism. 3. Weight sharing is the technique in which the number of independent weights is the penalty term [105, 122]. 4. <p> Weight depth is the number of binary bits in a weight. Weight elimination is a regularisation technique used in feedforward network training in which the cost-function is augmented with a term which penalises the number of non-zero weights <ref> [51, 81, 110, 152] </ref>. Weight perturbation is a hardware-friendly alternative to BP learning. In this method, all of the weights are perturbed in turn and the associated change in the output of the network is used to approximate local gradients [59].
Reference: [111] <institution> Proceedings of the IEEE International Conference on Neural Networks, </institution> <address> Washing-ton, DC, June 1996. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference-contexts: A large number of papers on the application of CWNs have since been published, in areas of application ranging from medicine, finance, process industry, high energy physics, automotives, telecommunications, robotics, to aerospace <ref> [85, 111, 159] </ref>. These applications are generally divided into two groups: function approximation 5 and classification. This division is based on the type of the desired outputs required to accomplish the task.
Reference: [112] <author> Reyneri, L. M. and Filippi, E. </author> <title> An analysis on the performance of silicon implementations of backpropagation algorithms for artificial neural networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(12) </volume> <pages> 1380-1389, </pages> <year> 1991. </year>
Reference-contexts: Such weights are not possible in practice. In digital electronic hardware, a designer has to determine the amount of memory required to store the weights by considering two factors <ref> [112] </ref>: the mini 19 1.11 Feedforward Networks with Constrained Weights mum variation by which a weight can be modified, ffiw, and the difference between the maximum and minimum value of weights, the dynamic range. <p> Most workers, however, agree that networks with weights of medium resolution are adequate to model many learning tasks to the desired accuracy [33, 169], <ref> [58, 60, 112, 166, 167] </ref> 6 . The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two [77, 138] 6 or sum-of-powers-of-two [88, 89] 6 .
Reference: [113] <author> Ripley, B. D. </author> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1996. </year>
Reference-contexts: The deployed network constantly adapts its weights with respect to changes in the input/output behaviour of its target task. 9 1.6 Approximation Properties rigorous frameworks, such as Bayesian techniques, available in statistics <ref> [11, 113] </ref>. The feedforward network, the only neural paradigm discussed in this thesis, has a direct analogue in statistics: projection pursuit regression [66]. Projection pursuit is a generalisation of the CWN in that it allows more than one type of activation function in the hidden layer. <p> The second set, onset of diabetes prediction data set, has both discrete and continuous inputs, may have some irrelevant inputs, may have much noise in the inputs, may have a high degree of correlation between inputs, and has a single binary output <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> It will be interesting to compare this ranking with the weight structure of the trained networks. This database has been analysed with the help of connectionist tools in <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>.
Reference: [114] <author> Ripley, B. D. </author> <title> Pattern Recognition and Neural Networks, </title> <note> chapter 1. In [113], </note> <year> 1996. </year>
Reference-contexts: These are most probably missing values <ref> [114] </ref>. Moreover, it should be noted that this database may be very noisy: some of the attributes may be unimportant.
Reference: [115] <author> Ripley, B. D. </author> <title> Pattern Recognition and Neural Networks, </title> <note> chapter 5. In [113], </note> <year> 1996. </year>
Reference-contexts: Carpenter and Markuzon [25] have also analysed this database and have reported a generalisation performance of 77% with k-nearest neighbours 15 , 78% with ART-EMAP 16 , and 81% with ARTMAP-IC 17 . Ripley <ref> [115] </ref> used 200 cases for training and 332 for testing, and ignored the rest because of missing values. The study reports that the best CWN results were obtained without a hidden layer of neurons.
Reference: [116] <author> Ripley, B. D. </author> <title> Pattern Recognition and Neural Networks, </title> <note> chapter 6. In [113], </note> <year> 1996. </year>
Reference-contexts: Perform. CWN 8:2:1 21 78.4% MFN 8:3:1 6 25 78.0% and the EM algorithm 19 <ref> [116] </ref>. For the simulation results presented here, a balanced data set, i.e. a set having equal number of positive and negative cases, of 536 cases was randomly selected. This set was then split into two equal, balanced subsets for training and testing.
Reference: [117] <author> Ripley, B. D. </author> <title> Pattern Recognition and Neural Networks. </title> <note> In [113], 1996. Glossary. </note>
Reference-contexts: All logical expressions are expressible in this form [61]. Effective sample size (for classification learning tasks) is the number of examples rep resenting the smallest classification group [95]. EM algorithm. Expectation Maximisation algorithm calculates the probability density of observations based on parameters and not observations <ref> [117] </ref>. Epoch is the cycle in which all examples in the training set are presented to the net work. Ergodic process. A random process is ergodic if its ensemble and temporal averages are the same. <p> Learning, Unsupervised The learning process in which a system's internal parameters are modified so that similar input patterns result in similar outputs. Learning rate determines the size of the weight modification at each training step. Likelihood is the probability density of observations calculated from parameters and not observations <ref> [117] </ref>. Limit Point. A point p is a limit point of A if every neighbourhood of p contains a point q 6= p such that q 2 A. Linear separability.
Reference: [118] <author> Robbins, H. and Monro, S. </author> <title> A stochastic approximation method. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22 </volume> <pages> 400-407, </pages> <year> 1951. </year>
Reference-contexts: A diminishing learning rate is the weakest requirement for on-line BP to settle down [155]. White [155] views on-line BP as `an application of Robbins-Monro stochastic optimisation procedure <ref> [118] </ref> to solve the first order conditions for a nonlinear least squares regression problem'. He considers a sequence of independent, identically distributed, and bounded training examples, and bounded and twice continuously differentiable hidden layer activation functions.
Reference: [119] <author> Rudin, W. </author> <title> Principles of Mathematical Analysis, chapter 7. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1976. </year>
Reference: [120] <author> Rudin, W. </author> <note> Principles of Mathematical Analysis, chapter 8. In [119], 1976. Theorem 8.4. </note>
Reference: [121] <author> Rudin, W. </author> <note> Principles of Mathematical Analysis, chapter 8. In [119], 1976. Corollary to Theorem 8.1. </note>
Reference: [122] <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart et al. </editor> <volume> [123], chapter 8, </volume> <pages> pages 318-362. 148 REFERENCES </pages>
Reference-contexts: In classification problems, outputs are generally encoded as f0; 1g or f1; 1g. Due to the fact that weight modifications computed by some learning procedures are proportional to the output value of a neuron <ref> [122] </ref>, the first encoding has the disadvantage in that the weight modifications calculated by the learning procedure for class 0 are smaller relative to the ones for class 1. <p> A computationally efficient way of implementing this process in a CWN, called error backpropagation (BP), was popularised by Rumelhart et al. <ref> [122] </ref>. In the batch 13 version of BP, the weights are updated at the end of every epoch 14 , whereas in the on-line 15;16 version weights are updated after the presentation of every training example. <p> The convergence characteristics of BP can be empirically analysed by the Monte Carlo techniques <ref> [122] </ref> or theoretically by various statistical and numerical analysis tools. The theoretical results presented in this section will comment on the form of permissible cost functions, and the effect of the learning parameters on convergence to a solution. <p> This strategy did help in the initial stage of training but not during the intermediate stage. The most likely reason was the formation of new local minima due to discretisation. It is well known that the standard BP algorithm sometimes gets stuck in local minima <ref> [122] </ref>. The superposition of the weight discretisation process on BP can result in changes in the shapes of these minima [166] or even an increase in their number. At the start of discrete-weight learning, E o is large, and the output error minimisation process dominates. <p> The integer-weight learning procedure presented in Chapter 3 incorporates an implicit weight elimination mechanism. 3. Weight sharing is the technique in which the number of independent weights is the penalty term <ref> [105, 122] </ref>. 4. Addition of noise to weights during training results in robust internal representa tions which have better generalisation performances [99, 100]. 5. Addition of noise to inputs during training yields a smoother function by providing small variations to the input [73, 131]. <p> Weight sharing is a regularisation technique used in feedforward network training in which the cost-function is augmented with a term which penalises the number of independent weights <ref> [105, 122] </ref>.
Reference: [123] <author> Rumelhart, D. E., McClelland, J. L., </author> <title> and the PDP research group, </title> <editor> editors. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: They are not, however, as well understood as 2-layer networks and are harder to train [3]. 1.3 Application Examples It has been 10 years since the pioneering work of Rumelhart et al. <ref> [123] </ref>, in which learning in multilayer feedforward networks was first introduced. A large number of papers on the application of CWNs have since been published, in areas of application ranging from medicine, finance, process industry, high energy physics, automotives, telecommunications, robotics, to aerospace [85, 111, 159]. <p> Their usefulness as iterative learning machines did not manifest itself in that era due to the lack of a suitable learning algorithm [104]. The breakthrough came in 1986 with the publication of two volumes by the PDP group titled Parallel Distributed Processing: Explorations in the Microstructure of Cognition <ref> [123, 124] </ref>. Although ideas similar to the BP learning rule had been presented earlier [79, 109, 154], it became popular only after the publication of these volumes.
Reference: [124] <author> Rumelhart, D. E., McClelland, J. L., </author> <title> and the PDP research group, </title> <editor> editors. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 2. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Their usefulness as iterative learning machines did not manifest itself in that era due to the lack of a suitable learning algorithm [104]. The breakthrough came in 1986 with the publication of two volumes by the PDP group titled Parallel Distributed Processing: Explorations in the Microstructure of Cognition <ref> [123, 124] </ref>. Although ideas similar to the BP learning rule had been presented earlier [79, 109, 154], it became popular only after the publication of these volumes.
Reference: [125] <author> Sarle, W. S. </author> <title> Subject: How can generalization error be estimated? Neural Network FAQ, part 2 of 7: Learning. </title> <note> Available as ftp://ftp.sas.com/pub/neural/- FAQ2.html. </note>
Reference-contexts: The dual use of the second subset is not strictly appropriate as it has been used during training and therefore the performance metric extracted from it is not an unbiased estimate of the generalisation performance of the trained network <ref> [125] </ref>. In the case of the experiments discussed here, however, the emphasis is on comparing the generalisation ability of the three paradigms in similar circumstances, and not estimating their true generalisation performance. This particular emphasis on comparison was the main reason for the use of just two data subsets.
Reference: [126] <author> Sarle, W. S. </author> <title> Subject: What is backprop? Neural Network FAQ, part 2 of 7: Learning. </title> <note> Available as ftp://ftp.sas.com/pub/neural/FAQ2.html. </note>
Reference-contexts: the batch version is known as (total) gradient descent [94], the on-line version is familiar to statisticians as stochastic gradient descent [156], and the momentum modification of the batch version can be recognised as the `heavy ball' method discussed in the numerical analysis 32 2.3 Convergence of Error Backpropagation literature <ref> [126] </ref>. The convergence characteristics of BP can be empirically analysed by the Monte Carlo techniques [122] or theoretically by various statistical and numerical analysis tools.
Reference: [127] <author> Sarle, W. S. </author> <title> Neural network and statistical models. </title> <booktitle> In Proceedings of the 19th Annual SAS Users Group International Conference, Cary, </booktitle> <address> NC, </address> <month> April </month> <year> 1994. </year> <note> Also available as ftp://ftp.sas.com/pub/sugi19/neural/neural1.ps. </note>
Reference-contexts: The most distinctive feature of the graph of a feedforward network is its homogeneous modularity. Because of its modular architecture, the natural implementation of this network is a parallel one, whether in software or in hardware. Although most current activity is in software implementations on serial computers <ref> [127] </ref>, the unique benefit of the feedforward network, i.e. fast speed of execution, can only be achieved through its realisation in parallel hardware: electronic or optical, analogue or digital. <p> In other words, a 2-layer CWN exists that can, given enough training data and enough hidden neurons, approximate virtually any function of interest to any desired degree of accuracy <ref> [127] </ref>. This is a very powerful statement and provides great comfort to experimentalists in reinforcing their beliefs about the capabilities of CWNs. This, however, guarantees only the existence of an approximating network and does not give any clues about how to construct one. <p> The techniques for avoiding an over-fit and consequently improving generalisation performance, i.e. regularisation techniques, will be discussed in Chapter 6. 1.10 Hardware Implementation Although most feedforward networks are implemented in software on serial computers <ref> [127] </ref>, a very attractive feature of CWNs, i.e. fast speed of execution, can only be achieved through their realisation in parallel hardware: general purpose or customised, optical or electronic, analogue or digital, or any combination thereof. Custom hardware has the advantage in speed 19 and the disadvantage in cost. <p> For all four problems, the CWN requires 2 Skip-layer synapses are the synapses connecting neurons in two non-adjacent layers. On some problems, their use has been found to result in more compact solutions. Also known as short-cut synapses [78]. Known as main effects in the statistical literature <ref> [127] </ref>. 65 4.3 Approximating CWNs with IWNs 1 2 3 4 -6 -2 1 2 3 4 -6 -2 1 2 3 4 -6 -2 1 2 3 4 -6 -2 1 2 3 4 -6 -2 1 2 3 4 -6 -2 Hidden neurons `n o r ms data sets <p> Simulated annealing is a stochastic optimisation technique inspired by the physical process of annealing. 133 GLOSSARY Skip-layer synapses. Synapses connecting neurons in two non-adjacent layers. Also known as short-cut synapses [78]. Known as main effects in the statistical litera ture <ref> [127] </ref>. Smoothing spline modelling is piecewise approximation by polynomials of degree n with the requirement that the derivatives of the polynomials are continuous up to degree n 1 at the junctions [20]. Softmax.
Reference: [128] <author> Sarle, W. S. </author> <title> Neural network implementation in SAS software. </title> <booktitle> In Proceedings of the 19th Annual SAS Users Group International Conference, Cary, </booktitle> <address> NC, </address> <month> April </month> <year> 1994. </year> <note> Also available as ftp://ftp.sas.com/pub/sugi19/neural/neural2.ps. </note>
Reference-contexts: If is small and close to 1, on-line BP approximates batch BP <ref> [128] </ref>. As training examples are usually presented to a network in a random order, on-line BP does its search in the weight space in a stochastic manner, and therefore is less prone (compared with batch BP) to getting stuck in local minima 17 of the error surface [53]. <p> This can result in reduced over-fitting and therefore improved generalisation performance: it is known from shrinkage 2 estimation and ridge regression 3 analysis in linear models that generalisation can be improved by reducing the size of the weights from estimates that give best fit in the sample <ref> [128] </ref>. <p> This can result in reduced over-fitting and therefore improved generalisation performance: it is known from shrinkage estimation and ridge regression analysis in linear models that generalisation can be improved by reducing the size of the weights from estimates that give best fit in the sample <ref> [128] </ref>. Another potential advantage of the MFN is its relatively immunity to noise in training data. The MFN only captures the main features of the training data in its discrete-valued synapses.
Reference: [129] <author> Sarle, W. S. Re: </author> <title> Preprocessing data for ANN. </title> <note> Article 17388 of comp.ai.neural-nets newsgroup, </note> <month> August </month> <year> 1994. </year>
Reference-contexts: This, combined with the initialisation of the CWNs at the start of training with small randomly distributed weights which results in the start-up decision boundaries bunching up around the origin, gives the CWN the best chance of fast learning <ref> [129] </ref>. Moreover, Le Cun et al. have shown analytically that standardising the inputs to zero mean improves the convergence properties of some learning procedures [82]. For discrete inputs or outputs, categorical variables should not be treated as continuous variables [13]. <p> The standardisation of inputs to zero-means and small magnitudes along with initialising the network with small uniformly distributed weights has the advantage that most of hidden neurons start with their outputs in the linear region of the activation functions, where the learning progresses the fastest <ref> [129] </ref>. Moreover, Le Cun et al. have analytically shown that standardising the inputs to zero mean improves the convergence properties of BP learning [82]. A comparison of the generalisation performance of the best IWN, MFN, and CWN on the diabetes database is presented in Table 6.4.
Reference: [130] <author> Sarle, W. S. </author> <title> Stopped training and other remedies for overfitting. </title> <booktitle> In Proceedings of the 27th Symposium on Interface, </booktitle> <year> 1995. </year> <note> Also available as ftp://ftp.sas.com/- pub/neural/inter95.ps.Z. </note>
Reference-contexts: estimators whose distance to an ellipsoid (the `ridge'), centred at a least-squares estimate from the origin of a parameter space, is a minimum [46]. 37 3.2 Discrete-weight Networks Table 3.1 Weight resolution terminology Weights Bits Low resolution 1-4 Medium resolution 5-16 High resolution 16+ augmented by a penalty term 4 <ref> [130] </ref>. Another potential advantage of the IWN is its relatively immunity to noise in training data. The IWN only captures the main features of the training data in its discrete-valued weights. <p> There has been some work on networks with medium resolution 4 The other two types of ridging are constrained ridging, in which some norm of the weights is constrained to a specific value, and smoothed ridging, in which noise is introduced in the inputs <ref> [130] </ref>. 5 Examples indicating such a behaviour will be presented in Section 6.5.2. 38 3.3 The Hidden Neurons -vs- Weight-Depth Trade-Off weights but very little on ones with low resolution weights. <p> It has been successfully tested in many experiments where the number of examples is relatively large compared with the number of adjustable parameters in the network <ref> [103, 130, 151] </ref>. 97 6.4 Optimal Network 7. Bayesian methods define a set of model regularisation parameters, e.g. the strength of the weight decay mechanism, and make the estimation of the optimal values of those parameters a part of the learning process. <p> This is the most recent, and potentially the most useful, technique for training feedforward networks. The `exact' implementation of this technique suffers from the slow speed of training, the recent `approximate' implementations, however, hold much promise <ref> [87, 130, 141] </ref>. The regularisation method selected for the experiments presented here is `optimal stopping of training'. This popular technique is fast, works well with larger-than-necessary networks, and does not require the introduction of any new training parameters except the ratio of train/test data split [130]. <p> The regularisation method selected for the experiments presented here is `optimal stopping of training'. This popular technique is fast, works well with larger-than-necessary networks, and does not require the introduction of any new training parameters except the ratio of train/test data split <ref> [130] </ref>. The CWN experiments presented here use weight decay in addition to optimal stopping of training. Weight decay was used to stop weights from reaching excessively large values. <p> It should be noted that for CWNs, the test data performance may not degrade if the number of hidden neurons is increased beyond the optimal number provided that the `optimal stopping of training' method is used <ref> [130] </ref>. <p> For CWNs, however, this test data performance may not degrade if the network size is increased beyond the optimal provided that the train-and-test method is used for optimal stopping of training <ref> [130] </ref>. 5 Sensitivity of a decision is the likelihood that an event will be detected if it occurs. It is the ratio of true positives to the sum of true positives and false negatives. This metric is especially of importance when it is critical that an event be detected. <p> Ridging, Constrained. Optimisation procedure in which some norm of the weights is constrained to a specific value <ref> [130] </ref>. Ridging, Penalised. Optimisation procedure in which the cost function is augmented by a penalty term [130]. Ridging, Smoothed. Optimisation procedure in which noise is introduced in the in puts [130]. Riesz representation theorem. Let x fl be a bounded linear functional on the Banach space C R ([a; b]). <p> Ridging, Constrained. Optimisation procedure in which some norm of the weights is constrained to a specific value <ref> [130] </ref>. Ridging, Penalised. Optimisation procedure in which the cost function is augmented by a penalty term [130]. Ridging, Smoothed. Optimisation procedure in which noise is introduced in the in puts [130]. Riesz representation theorem. Let x fl be a bounded linear functional on the Banach space C R ([a; b]). <p> Ridging, Constrained. Optimisation procedure in which some norm of the weights is constrained to a specific value <ref> [130] </ref>. Ridging, Penalised. Optimisation procedure in which the cost function is augmented by a penalty term [130]. Ridging, Smoothed. Optimisation procedure in which noise is introduced in the in puts [130]. Riesz representation theorem. Let x fl be a bounded linear functional on the Banach space C R ([a; b]).
Reference: [131] <author> Scaletter, R. and Zee, A. </author> <title> Emergence of grandmother memory in feedforward networks: Learning with noise and forgetfulness. </title> <editor> In Waltz, D. and Feldman, J. A., editors, </editor> <title> Connectionist Models and Their Implications: </title> <booktitle> Readings from Cognitive Science, </booktitle> <pages> pages 309-332. </pages> <address> Ableex, Norwood, MA, </address> <year> 1988. </year> <note> 149 REFERENCES </note>
Reference-contexts: Addition of noise to weights during training results in robust internal representa tions which have better generalisation performances [99, 100]. 5. Addition of noise to inputs during training yields a smoother function by providing small variations to the input <ref> [73, 131] </ref>. This technique is known as smoothed ridging in the statistical literature. 6. Optimal stopping of training uses two sets of data during training, one for training and the other for testing, to stop the training when error on the test data hits a minimum.
Reference: [132] <author> Sejnowski, T. J. and Rosenberg, C. R. NETtalk: </author> <title> A parallel network that learns to read aloud. </title> <type> Technical report JHU/EECS-86/01, </type> <institution> Department of Electrical Engineering and Computer Science, John Hopkins University, Baltimore, MD, </institution> <year> 1986. </year>
Reference-contexts: An n-bit fixed-point VLSI flash multiplier consists of n fi n full adders, each one of which is made up of 31 transistors [2]. If the classic NETtalk network of Sejnowski and Rosenberg <ref> [132] </ref> was to be constructed in this very fast incarnation, approximately 10 9 transistors will be required for n = 8.
Reference: [133] <author> Simmons, G. F. </author> <title> Introduction to Topology and Modern Analysis. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: M c is a closed linear subspace of N and x 0 is a vector not in M c , then there exists a functional f 0 in the conjugate space N fl such that f 0 (M c ) = 0 and f 0 (x 0 ) 6= 0 <ref> [133] </ref>. Hebbian learning. The main idea behind Hebbian learning is that the synapse between two neurons should be strengthened if they fire simultaneously. Hidden layer is the layer of neurons which is not directly connected to the network inputs or outputs. Homogeneity property.
Reference: [134] <author> Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C., and Jo-hannes, R. S. </author> <title> Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. </title> <booktitle> In Proceedings of the Symposium on Computer Applications and Medical Care, </booktitle> <pages> pages 261-265. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1988. </year> <note> The diabetes dataset is available in the directory ftp://ics.uci.edu/pub/- machine-learning-databases/pima-indians-diabetes/. </note>
Reference-contexts: The second set, onset of diabetes prediction data set, has both discrete and continuous inputs, may have some irrelevant inputs, may have much noise in the inputs, may have a high degree of correlation between inputs, and has a single binary output <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> It will be interesting to compare this ranking with the weight structure of the trained networks. This database has been analysed with the help of connectionist tools in <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> This equality of the two figures was achieved by thresholding the continuous 0-1 output of the model at 0.448 <ref> [134] </ref>. Michie et al. used a set of 22 machine learning, neural, and statistical techniques to analyse this data [96]. They used 12-fold cross-validation to determine generalisation performance.
Reference: [135] <author> Staib, W. E. and Staib, R. B. </author> <title> The intelligent arc furnace controller: A neural network electrode position optimization system for electric arc furnaces. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks. </booktitle> <publisher> IEEE Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: The more meaningful generalisation-performance results will be discussed in Chapter 6, where a comparison with the performances of CWNs and multiplier-free networks will also be presented. IWNs require high-resolution arithmetic during the training phase. This makes them unsuitable for in-situ learning. Many commercial neural network applications (for example <ref> [135] </ref>), however, do not require in-situ training. The number of training epochs for IWNs is generally higher than for CWNs. This is not a major drawback because in many practical applications [159] the learning period occupies only a small fraction of a network's lifetime usage. <p> A multivari 91 5.6 Discussion able extension on this theorem is currently not available, but the experimental evidence of this chapter points toward its existence. MFNs require high-resolution arithmetic during the training phase. This makes them unsuitable for in-situ learning. Many commercial feedforward network applications (for example <ref> [135] </ref>), however, do not require in-situ training. The possibly large number of learning epochs required for MFNs as compared with CWNs should not be a deterrent to their use as, in many practical applications [159], the learning period occupies only a small fraction of a network's lifetime usage.
Reference: [136] <author> Stinchcombe, M. and White, H. </author> <title> Universal approximation using multilayer feed-forward networks with non-sigmoid hidden layer activation functions. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 613-617, </pages> <address> Washington, DC, 1989. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address> <note> Reprinted in [157]. </note>
Reference-contexts: the next chapter largely comprise review material, whereas the remaining chapters mainly consist of original work. 2 Theoretical Preliminaries 2.1 Theoretical Questions For a given artificial feedforward architecture, it is possible to theoretically explore many an interesting issue: What kinds of mappings can it learn (linear, nonlinear, continuous, smooth, measurable) <ref> [63, 136] </ref>? What type of neuron activation functions are permissible [27, 83, 136]? Is it capable of `universal approximation' [29, 62, 68]? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network <p> chapters mainly consist of original work. 2 Theoretical Preliminaries 2.1 Theoretical Questions For a given artificial feedforward architecture, it is possible to theoretically explore many an interesting issue: What kinds of mappings can it learn (linear, nonlinear, continuous, smooth, measurable) [63, 136]? What type of neuron activation functions are permissible <ref> [27, 83, 136] </ref>? Is it capable of `universal approximation' [29, 62, 68]? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation <p> `stacked together' to form 2- (or d)-dimensional function. 2.2.1 Universal Approximation in C (R d ) Numerous results, utilising a vast array of mathematical tools and making various assumptions about the construct of the network, are available on the universal approximation in C (R d ) property of feedforward networks <ref> [26, 27, 29, 41, 55, 62, 63, 83, 136] </ref>. <p> Separates points. A family of functions A separates points on a set S if for every x; y 2 S; x 6= y, there exists f 2 A such that f (x) 6= f (y) <ref> [136] </ref>. Set, Closed. A subset M of metric space N is a closed set if it contains each of its limit points. Set, Finite. <p> Vanishes at no point. A family of functions A vanishes at no point of the set S if for each x 2 S there exists f 2 A such that f (x) 6= 0 <ref> [136] </ref>. Weight is the value of a synapse or an offset. Weight decay is a common regularisation technique used in feedforward network training in which the cost-function is augmented with a term which penalises large weight values. Weight depth is the number of binary bits in a weight.
Reference: [137] <author> Stinchcombe, M. and White, H. </author> <title> Approximating and learning unknown mappings using multilayer feedforward networks with bounded weights. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 3, </volume> <pages> pages 7-16, </pages> <address> San Diego, CA, 1990. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address> <note> Reprinted in [157]. </note>
Reference-contexts: Stinchcombe and White <ref> [137] </ref> have shown analytically that provided the activation 20 1.11 Feedforward Networks with Constrained Weights function meets certain criteria, the bounds on a CWN's weights can be traded off with a possible increase in the number of hidden neurons without sacrificing the universal approximation property. <p> R d , and the set of all continuous functions on R d is C ( R d ) [27]. 28 2.2 Universal Approximation Property of CWNs and White <ref> [137] </ref> have come up with a result which trades off bounds on the magnitude of network weights with a possible increase in the number of the hidden neurons, which is highly relevant to the work presented in Chapter 5 and will therefore be discussed in detail in the next section. 2.2.2 <p> in the number of the hidden neurons, which is highly relevant to the work presented in Chapter 5 and will therefore be discussed in detail in the next section. 2.2.2 CWNs with Bounded Weights are Universal Approximators This section will follow the closely the approach chosen by Stinchcombe and White <ref> [137] </ref>, in which they enforced some restrictions on activation functions to achieve the universal approximation property for networks with bounded weights. The results of this section will be used in proving the multiplier-free network existence theorem of Section 5.2.1. <p> functions A is uniformly dense in C (R d ) on the compact set K R d if for all f 2 C (R d ) and every " &gt; 0 there exists ^ f 2 A such that supfjf (x) ^ f (x)j : x 2 Kg &lt; " <ref> [137] </ref>. Denseness, Uniform on compacta. A set of functions A is uniformly dense on com pacta in C (R d ) if it is uniformly dense on every compact subset of R d [137]. Disjunctive normal form (DNF). <p> ^ f 2 A such that supfjf (x) ^ f (x)j : x 2 Kg &lt; " <ref> [137] </ref>. Denseness, Uniform on compacta. A set of functions A is uniformly dense on com pacta in C (R d ) if it is uniformly dense on every compact subset of R d [137]. Disjunctive normal form (DNF). The form of a logical expression consisting of a single conjunction () of a set of disjunctions (+). All logical expressions are expressible in this form [61]. <p> if there is an infinite sequence of real numbers, fc n g; n 0, and if for every n 1; c n 6= 0, such that for jx aj &lt; r; P 1 n=0 c n (x a) n converges and f (x) = n=0 c n (x a) n <ref> [137] </ref> Function approximation is a task in which the desired output values are continuous. Also known as regression. Functional is a scalar-valued continuous linear function defined on a normed linear space. Functional, Linear on a linear space E over R is a linear transformation of E into R. <p> For any F defined on a set O, the span of F ; sp (F ), denotes the closure of the set of finite linear combinations of elements of F in the topology of uniform convergence on compact subsets of O <ref> [137] </ref>. Sphere, Open. S r (x 0 ) with centre x 0 and radius r is the subset of the metric space X with metric D defined by S r (x 0 ) = fx : d (x; x 0 ) &lt; rg. Stationary, strongly.
Reference: [138] <author> Tang, C. Z. and Kwan, H. K. </author> <title> Multilayer feedforward neural networks with single power-of-two weights. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 41(8) </volume> <pages> 2724-2727, </pages> <year> 1993. </year> <note> 150 REFERENCES </note>
Reference-contexts: The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two <ref> [77, 138] </ref> 6 or sum-of-powers-of-two [88, 89] 6 . For such weights, multiplication operations can be replaced by much simpler shift operations or a sequence of sum-and-shift steps, respectively. <p> very beginning to concentrate on designing heuristics that will get the job done in a reasonable number of epochs and the task of achieving the fastest convergence times was therefore not considered at all. 3.5.1 Weight Discretisation Schemes Networks having powers-of-two weights represent an interesting discretisation scheme for digital implementations <ref> [76, 77, 138] </ref>. They have the advantage in arithmetical manipulations multiplying a number with a power-of-two weight can be accomplished by a simple shift operation on that number, which is faster and requires much less hardware than the conventional digital multiplier.
Reference: [139] <author> Tesauro, G., He, Y., and Ahmad, S. </author> <title> Asymptotic convergence of backpropagation. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 382-391, </pages> <year> 1989. </year>
Reference-contexts: The theoretical results presented in this section will comment on the form of permissible cost functions, and the effect of the learning parameters on convergence to a solution. Tesauro et al. <ref> [139] </ref> have examined the asymptotic behaviour of the batch version of BP in terms of the decrease in error with respect to the number of epochs t. This study found the error to decrease as 1=t for large t. <p> Error in the output of a neuron is not backpropagated if it is within this small margin <ref> [139] </ref>. Minima, Global. The points of minimum error on an error surface. Minima, Local. The points of zero gradient on an error surface which are not global minima. Mixture representation of data use a linear combination of Gaussian distributions to represent arbitrary distributions [10].
Reference: [140] <author> Thodberg, H. H. </author> <title> Private communication, </title> <month> July </month> <year> 1996. </year>
Reference-contexts: The Bayesian framework has been employed to automatically determine the optimal values for weight decay parameters during training to achieve optimal generalisation performance [141]. As weight discretisation is similar to weight decay, in that it is also a constraint on weights <ref> [140] </ref>, the optimal values for weight discretisation parameters can also be determined using Bayesian techniques which will result in optimal generalisation performance for these constrained-weight networks.
Reference: [141] <author> Thodberg, H. H. </author> <title> A review of Bayesian neural network with an application to near infrared spectroscopy. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 6(1) </volume> <pages> 56-72, </pages> <year> 1996. </year>
Reference-contexts: The last major achievement in this field, though still in its infancy, has been the integration of CWNs in a Bayesian framework (see, for example <ref> [86, 102, 141] </ref>). <p> This is the most recent, and potentially the most useful, technique for training feedforward networks. The `exact' implementation of this technique suffers from the slow speed of training, the recent `approximate' implementations, however, hold much promise <ref> [87, 130, 141] </ref>. The regularisation method selected for the experiments presented here is `optimal stopping of training'. This popular technique is fast, works well with larger-than-necessary networks, and does not require the introduction of any new training parameters except the ratio of train/test data split [130]. <p> Avoiding large weights assures that the network learns smooth mappings [16]. 6.4 Optimal Network The definition of the optimal network used for the experiments presented here is based on the conjecture called Ockham's Razor 4 <ref> [141] </ref>: If two different models have similar performances on a set of data then the simpler of the two should be preferred. <p> There is, 4 This conjecture, also known as Occam's Razor, about economy in explanations, was systematically applied, but never explicitly stated by the 13th century philosopher, William of Ockham <ref> [141] </ref>. 98 6.5 Generalisation Experiments however, a problem with using larger than optimal networks although smaller networks are not guaranteed to be better in general, Baum and Haussler have shown in [7] that for a given training set and training error, the worst-case error bounds on unseen data vary proportionally with <p> A procedure that could simultaneously vary these two parameters during training will be beneficial in determining the most cost-effective combination for implementation in hardware. The Bayesian framework has been employed to automatically determine the optimal values for weight decay parameters during training to achieve optimal generalisation performance <ref> [141] </ref>. As weight discretisation is similar to weight decay, in that it is also a constraint on weights [140], the optimal values for weight discretisation parameters can also be determined using Bayesian techniques which will result in optimal generalisation performance for these constrained-weight networks.
Reference: [142] <author> Thurn, S. B. </author> <title> Backpropagation on the MONK's problems. </title> <editor> In Thurn et al. </editor> <volume> [143], </volume> <pages> chapter 9. </pages>
Reference-contexts: The Alopex simulations used a slightly different input encoding resulting in 15 inputs only CWN trained with BP BP with Weight Cascade Alopex Problem <ref> [142] </ref> Decay [142] Correlation [36] [146] IWN MFN 1 17:3:1 17:2:1 17:1:1 15:3:1 17:4:1 17:3:1 3 17:4:1 17:2:1 17:3:1 15:3:1 17:1 17:2:1 Problem 1 (Attribute 1 = Attribute 2) OR (Attribute 5 = 1) This problem is in standard Disjunctive Normal Form (DNF). 124 examples were selected randomly from the data <p> The Alopex simulations used a slightly different input encoding resulting in 15 inputs only CWN trained with BP BP with Weight Cascade Alopex Problem <ref> [142] </ref> Decay [142] Correlation [36] [146] IWN MFN 1 17:3:1 17:2:1 17:1:1 15:3:1 17:4:1 17:3:1 3 17:4:1 17:2:1 17:3:1 15:3:1 17:1 17:2:1 Problem 1 (Attribute 1 = Attribute 2) OR (Attribute 5 = 1) This problem is in standard Disjunctive Normal Form (DNF). 124 examples were selected randomly from the data set for <p> IWNs and MFNs with various 17:q:1 configurations were trained and the configurations resulting in the best generalisation performance are shown in 103 6.5 Generalisation Experiments Table 6.2 Comparison of generalisation performance on the MONK's benchmark CWN trained with BP BP with Weight Cascade Alopex Problem <ref> [142] </ref> Decay [142] Correlation [36] [146] IWN MFN 1 100% 100% 100% 100% 100% 100% 3 93.1% 97.2% 97.2% 100% 100% 100% Table 6.1. A comparison of the generalisation performance of the IWN, MFN, and CWNs on the MONK's benchmark is presented in Table 6.2. <p> IWNs and MFNs with various 17:q:1 configurations were trained and the configurations resulting in the best generalisation performance are shown in 103 6.5 Generalisation Experiments Table 6.2 Comparison of generalisation performance on the MONK's benchmark CWN trained with BP BP with Weight Cascade Alopex Problem <ref> [142] </ref> Decay [142] Correlation [36] [146] IWN MFN 1 100% 100% 100% 100% 100% 100% 3 93.1% 97.2% 97.2% 100% 100% 100% Table 6.1. A comparison of the generalisation performance of the IWN, MFN, and CWNs on the MONK's benchmark is presented in Table 6.2.
Reference: [143] <author> Thurn, S. B., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., Jong, K. D., Dzeroski, S., Fahlman, S. E., Fisher, D., Hamann, R., Kaufman, K., Keller, S., Kononenko, I., Kreuziger, J., Michalski, R. S., Mitchell, T., Pachowicz, P., Reich, Y., Vafaie, H., de Welde, W. V., Wenzel, W., Wnek, J., and Zhang, J., </author> <title> editors. The MONK's problems: A performance comparison of different learning algorithms. </title> <type> Technical reportCMU-CS-91-197. </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> December </month> <year> 1990. </year> <note> Also available as ftp://ics.uci.edu/pub/- machine-learning-databases/monks-problems/thrun.comparison.ps.Z. </note>
Reference-contexts: The first one, MONK's benchmark, is an artificial set and was designed to compare the capabilities of a number of learning algorithms, consists of three separate classification tasks, one of which is noisy, and each of which has binary-coded inputs and a single binary output <ref> [143, 146, 162] </ref>. <p> This particular emphasis on comparison was the main reason for the use of just two data subsets. Moreover, all of the previous results cited here on MONK's benchmark and the handwritten numeral recognition database have used the same strategy <ref> [31, 47, 143, 146] </ref>. The train-and-test method has another use in addition to the optimal stopping of training network size or complexity selection. Figure 6.1 shows how a network can can be evaluated on the test data after every training epoch for optimal stopping of training. <p> Test data results reported here represent the best performance of the optimal configurations. 6.5.2 MONK's Benchmark The MONK's benchmark 7 was proposed by Wnek et al. [160-162] and has been used for comparing the generalisation performances of 25 learning techniques by the creators or advocates of those techniques <ref> [143, 146] </ref>. These techniques included both machine learning and feedforward network paradigms. The feedforward network paradigms were conventional BP, BP with weight decay, Cascade-correlation [143], and Alopex 8 [146]. This benchmark is based upon an artificial robot domain. <p> These techniques included both machine learning and feedforward network paradigms. The feedforward network paradigms were conventional BP, BP with weight decay, Cascade-correlation <ref> [143] </ref>, and Alopex 8 [146]. This benchmark is based upon an artificial robot domain. Each robot in this domain can be described by specifying values for a set of six attributes. <p> The remaining 310 examples were used for testing. For meaningful comparisons, the simulations discussed here use the same training/test data subsets as were used in <ref> [143, 146] </ref>. Each possible value of every attribute was assigned a single bipolar-binary input, resulting in a total of 17 inputs.
Reference: [144] <author> Thurn, S. B., Mitchell, T., and Cheng, J. </author> <title> The MONK's comparision of learning algorithms Introduction and survey. </title> <editor> In Thurn et al. </editor> <volume> [143], </volume> <pages> chapter 1. </pages>
Reference-contexts: as a single tri-state input, as that imposes an ordering of square being somehow greater than round and less than octagon [13], but as a triplet, having three possible states (1; 0; 0); (0; 1; 0), and (0; 0; 1) depending upon the shape being round, square, or octagon, respectively <ref> [144] </ref>. In classification problems, outputs are generally encoded as f0; 1g or f1; 1g.
Reference: [145] <author> Turney, P. D. </author> <title> Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 369-409, </pages> <month> March </month> <year> 1995. </year> <note> Also available as http://www.cs.washington.edu/- research/jair/volume2/turney95a-html/title.html. </note>
Reference-contexts: The second set, onset of diabetes prediction data set, has both discrete and continuous inputs, may have some irrelevant inputs, may have much noise in the inputs, may have a high degree of correlation between inputs, and has a single binary output <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> It will be interesting to compare this ranking with the weight structure of the trained networks. This database has been analysed with the help of connectionist tools in <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>.
Reference: [146] <author> Unnikrishnan, K. P. and Venugopal, K. P. Alopex: </author> <title> A correlation-based learning algorithm for feedforward and recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 469-490, </pages> <year> 1994. </year> <note> 151 REFERENCES </note>
Reference-contexts: The first one, MONK's benchmark, is an artificial set and was designed to compare the capabilities of a number of learning algorithms, consists of three separate classification tasks, one of which is noisy, and each of which has binary-coded inputs and a single binary output <ref> [143, 146, 162] </ref>. <p> This particular emphasis on comparison was the main reason for the use of just two data subsets. Moreover, all of the previous results cited here on MONK's benchmark and the handwritten numeral recognition database have used the same strategy <ref> [31, 47, 143, 146] </ref>. The train-and-test method has another use in addition to the optimal stopping of training network size or complexity selection. Figure 6.1 shows how a network can can be evaluated on the test data after every training epoch for optimal stopping of training. <p> Test data results reported here represent the best performance of the optimal configurations. 6.5.2 MONK's Benchmark The MONK's benchmark 7 was proposed by Wnek et al. [160-162] and has been used for comparing the generalisation performances of 25 learning techniques by the creators or advocates of those techniques <ref> [143, 146] </ref>. These techniques included both machine learning and feedforward network paradigms. The feedforward network paradigms were conventional BP, BP with weight decay, Cascade-correlation [143], and Alopex 8 [146]. This benchmark is based upon an artificial robot domain. <p> These techniques included both machine learning and feedforward network paradigms. The feedforward network paradigms were conventional BP, BP with weight decay, Cascade-correlation [143], and Alopex 8 <ref> [146] </ref>. This benchmark is based upon an artificial robot domain. Each robot in this domain can be described by specifying values for a set of six attributes. <p> This procedure predates the current resurgence in neural network learning by more than a decade and was originally proposed for mapping visual receptive fields <ref> [146] </ref>. 102 6.5 Generalisation Experiments Table 6.1 Network configurations for the MONK's benchmark. The Alopex simulations used a slightly different input encoding resulting in 15 inputs only CWN trained with BP BP with Weight Cascade Alopex Problem [142] Decay [142] Correlation [36] [146] IWN MFN 1 17:3:1 17:2:1 17:1:1 15:3:1 17:4:1 <p> was originally proposed for mapping visual receptive fields <ref> [146] </ref>. 102 6.5 Generalisation Experiments Table 6.1 Network configurations for the MONK's benchmark. The Alopex simulations used a slightly different input encoding resulting in 15 inputs only CWN trained with BP BP with Weight Cascade Alopex Problem [142] Decay [142] Correlation [36] [146] IWN MFN 1 17:3:1 17:2:1 17:1:1 15:3:1 17:4:1 17:3:1 3 17:4:1 17:2:1 17:3:1 15:3:1 17:1 17:2:1 Problem 1 (Attribute 1 = Attribute 2) OR (Attribute 5 = 1) This problem is in standard Disjunctive Normal Form (DNF). 124 examples were selected randomly from the data set for training, while the <p> The remaining 310 examples were used for testing. For meaningful comparisons, the simulations discussed here use the same training/test data subsets as were used in <ref> [143, 146] </ref>. Each possible value of every attribute was assigned a single bipolar-binary input, resulting in a total of 17 inputs. <p> IWNs and MFNs with various 17:q:1 configurations were trained and the configurations resulting in the best generalisation performance are shown in 103 6.5 Generalisation Experiments Table 6.2 Comparison of generalisation performance on the MONK's benchmark CWN trained with BP BP with Weight Cascade Alopex Problem [142] Decay [142] Correlation [36] <ref> [146] </ref> IWN MFN 1 100% 100% 100% 100% 100% 100% 3 93.1% 97.2% 97.2% 100% 100% 100% Table 6.1. A comparison of the generalisation performance of the IWN, MFN, and CWNs on the MONK's benchmark is presented in Table 6.2. <p> Alopex is a stochastic learning procedure which uses local correlations between changes in individual weights and changes in the cost function to update weights. This procedure predates the current resurgence in neural network learning by more than a decade and was originally proposed for mapping visual receptive fields <ref> [146] </ref>. Approximation property, Universal, is the ability of a set functions to approximate a specific class of functions to any desired accuracy.
Reference: [147] <author> Venkatesh, S. S. </author> <title> Directed drift: A new linear threshold algorithm for learning binary weights on-line. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 46(2) </volume> <pages> 198-217, </pages> <year> 1993. </year>
Reference-contexts: Experiments with these networks, in particular with networks having small integer weights, however, show that approximation capability of these networks is not significantly affected despite having strong restriction on weights. The loading of a discrete-weight perceptron is known to be NP-complete <ref> [147] </ref>. This, combined with the fact that the loading of a 2-layer CWN is NP-complete, makes the task of loading the discrete-weight feedforward network quite formidable. <p> Results on some very simple leaning tasks with feedforward networks having binary and ternary weights were presented by Perez Vincente et al. in [148]. There is, however, a wealth of literature available on single-layer perceptrons with binary weights (e.g. <ref> [147] </ref>). 3.3 The Hidden Neurons -vs- Weight-Depth Trade-Off In selecting the architecture (i.e. complexity) of a feedforward network, the designer has to make choices about two information resources thenumber of hidden neurons, q, and weight depth, b w .
Reference: [148] <author> Vincente, C. J. P., Carrabina, J., Girrado, F., and Valderrama, E. </author> <title> Learning algorithms for feed-forward neural networks with discrete synapses. </title> <editor> In Prieto, A., editor, </editor> <booktitle> Artificial Neural Networks, Proceedings of IWANN'91, volume 540 of Lecture Notes in Computer Sciences, </booktitle> <pages> pages 144-152, </pages> <address> Granada, Spain, September 1991. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany. </address>
Reference-contexts: For such weights, multiplication operations can be replaced by much simpler shift operations or a sequence of sum-and-shift steps, respectively. Results on some very simple leaning tasks with feedforward networks having binary and ternary weights were presented by Perez Vincente et al. in <ref> [148] </ref>.
Reference: [149] <author> Wahaba, G. </author> <title> Generalization and regularization in nonlinear learning systems. </title> <editor> In Arbib, M., editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 426-430. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: The second set, onset of diabetes prediction data set, has both discrete and continuous inputs, may have some irrelevant inputs, may have much noise in the inputs, may have a high degree of correlation between inputs, and has a single binary output <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> It will be interesting to compare this ranking with the weight structure of the trained networks. This database has been analysed with the help of connectionist tools in <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>.
Reference: [150] <author> Wahba, G., Chong, G., Wang, Y., and Chappel, R. </author> <title> Soft classification, a.k.a risk estimation, via penalized log likelihood and smoothing spline analysis of variance. </title> <type> Technical report, </type> <institution> Statistics Department, University of Wisconsin, Madison, WI, </institution> <year> 1993. </year>
Reference-contexts: The second set, onset of diabetes prediction data set, has both discrete and continuous inputs, may have some irrelevant inputs, may have much noise in the inputs, may have a high degree of correlation between inputs, and has a single binary output <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> It will be interesting to compare this ranking with the weight structure of the trained networks. This database has been analysed with the help of connectionist tools in <ref> [9, 25, 93, 113, 134, 145, 149, 150] </ref>. <p> They used 12-fold cross-validation to determine generalisation performance. They achieved their best results of 87.7% with logistic discriminant analysis 12 , whereas a figure of 75.2% was obtained with a CWN trained using BP. Wahaba et al. <ref> [150] </ref> used 500 cases for training and 252 for testing with a smoothing spline model 13 , and report a generalisation performance of 76%. This study deleted 16 cases from the database because those cases had some attributes with impossible values.
Reference: [151] <author> Weigend, A. </author> <title> On overfitting and the effective number of hidden units. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 335-342, </pages> <year> 1994. </year>
Reference-contexts: It has been successfully tested in many experiments where the number of examples is relatively large compared with the number of adjustable parameters in the network <ref> [103, 130, 151] </ref>. 97 6.4 Optimal Network 7. Bayesian methods define a set of model regularisation parameters, e.g. the strength of the weight decay mechanism, and make the estimation of the optimal values of those parameters a part of the learning process.
Reference: [152] <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <title> Predicting the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-209, </pages> <year> 1990. </year>
Reference-contexts: A common penalty term is the sum of the square of weights [57]. This technique is known as penalised ridging in the statistical literature. 2. Weight elimination is the technique in which the number of non-zero weights is the penalty term <ref> [51, 81, 110, 152] </ref>. The integer-weight learning procedure presented in Chapter 3 incorporates an implicit weight elimination mechanism. 3. Weight sharing is the technique in which the number of independent weights is the penalty term [105, 122]. 4. <p> Weight depth is the number of binary bits in a weight. Weight elimination is a regularisation technique used in feedforward network training in which the cost-function is augmented with a term which penalises the number of non-zero weights <ref> [51, 81, 110, 152] </ref>. Weight perturbation is a hardware-friendly alternative to BP learning. In this method, all of the weights are perturbed in turn and the associated change in the output of the network is used to approximate local gradients [59].
Reference: [153] <author> Weiss, S. M. and Kulikowski, C. A. </author> <title> Computer Systems That Learn, chapter 2. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: The third conclusion points towards the better suitability of the IWN and MFN as generalisers as compared with the CWN. 6.2.1 Empirical Estimation Comparative experiments for the empirical estimation of generalisation performance can be set up in one of three ways <ref> [95, 153] </ref>: 1. Train-and-test A random sample containing one half of the total number of examples is selected. This subset is used to train the network, while the remaining examples are used to test the network once it has been trained.
Reference: [154] <author> Werbos, P. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference-contexts: The breakthrough came in 1986 with the publication of two volumes by the PDP group titled Parallel Distributed Processing: Explorations in the Microstructure of Cognition [123, 124]. Although ideas similar to the BP learning rule had been presented earlier <ref> [79, 109, 154] </ref>, it became popular only after the publication of these volumes.
Reference: [155] <author> White, H. </author> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1(4) </volume> <pages> 425-464, </pages> <year> 1989. </year> <note> Reprinted in [157]. 152 REFERENCES </note>
Reference-contexts: The on-line version with a fixed learning rate, however, starts to wander around the error surface as the error becomes small and does not settle into a stable solution. A diminishing learning rate is the weakest requirement for on-line BP to settle down <ref> [155] </ref>. White [155] views on-line BP as `an application of Robbins-Monro stochastic optimisation procedure [118] to solve the first order conditions for a nonlinear least squares regression problem'. He considers a sequence of independent, identically distributed, and bounded training examples, and bounded and twice continuously differentiable hidden layer activation functions. <p> The on-line version with a fixed learning rate, however, starts to wander around the error surface as the error becomes small and does not settle into a stable solution. A diminishing learning rate is the weakest requirement for on-line BP to settle down <ref> [155] </ref>. White [155] views on-line BP as `an application of Robbins-Monro stochastic optimisation procedure [118] to solve the first order conditions for a nonlinear least squares regression problem'. He considers a sequence of independent, identically distributed, and bounded training examples, and bounded and twice continuously differentiable hidden layer activation functions. <p> Also known as almost sure convergence, convergence almost everywhere, and strong convergence <ref> [155] </ref>. 33 2.4 Summary ing rate. They considered the weight update steps as an interpolation process and analysed this process by associating an ordinary differential equation (ODE) with it and by studying the asymptotic behaviour of the solutions of the ODE. <p> Also known as weak convergence <ref> [155] </ref>. 34 2.4 Summary First, a feedforward network having hidden layer activation functions that are super-analytic and these superanalytic activation functions having derivatives that form a basis for the continuous functions, can have input layer weight vectors restricted to the unit-sphere without sacrificing its ability to approximate all continuous functions to <p> real variables then a n converges to a, i.e., a n ! a as n ! 1, if there exists a real number a such that for any * &gt; 0, there exists an integer N * sufficiently large that ja n aj &lt; * for all n N * <ref> [155] </ref>. Also known as deterministic convergence. Convergence in distribution. <p> n g is a sequence of random variables having a distribution function fF : F n (a) P [^a n ag then ^a n converges to F in distribution, i.e. ^a n ! F , iff jF n (a) F (a)j ! 0 for every continuity point a of F <ref> [155] </ref>. Convergence in probability. <p> Also known as weak conver gence <ref> [155] </ref>. 125 GLOSSARY Convergence in the mean. If f^a n g is a sequence of random variables then ^a n converges to a in the mean if lim a n !1 E fj^a n ajg = 0, where Efag represents the estimated value of a. <p> Also known as almost sure convergence, convergence almost everywhere, and strong convergence. <ref> [155] </ref> Convergence, Uniform The property that all of a family of functions or series on a given set converge at the same rate throughout the set; that is, for every " &gt; 0 there is a single N such that for all points in the set , jf m (x) f
Reference: [156] <author> White, H. </author> <title> Some asymptotic results for learning in single hidden-layer feedforward networks. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84 </volume> <pages> 1003-1013, </pages> <year> 1989. </year> <note> (Correction: 87, 1252.). Reprinted in [157]. </note>
Reference-contexts: multiplier-free net. 2.3 Convergence of Error Backpropagation The error backpropagation heuristic for training feedforward networks, the method of choice for many users, is not a completely new training procedure: the batch version is known as (total) gradient descent [94], the on-line version is familiar to statisticians as stochastic gradient descent <ref> [156] </ref>, and the momentum modification of the batch version can be recognised as the `heavy ball' method discussed in the numerical analysis 32 2.3 Convergence of Error Backpropagation literature [126].
Reference: [157] <author> White, H. </author> <title> Artificial Neural Networks: Approximation and Learning Theory. </title> <publisher> Blackwell, Oxford, </publisher> <address> England, </address> <year> 1992. </year>
Reference: [158] <author> Whitehouse, D. J. and Huang, T. </author> <title> Adaptive control of electromagnetically levitated spindle using neural networks. EPSRC proposal, </title> <institution> Department of Engineering, University of Warwick, Coventry, </institution> <address> England, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: This spindle can replace the conventional ball-bearing spindle in a high speed machining process to achieve better cutting stability <ref> [158] </ref>. Glossary * Activation function is the transform applied to the weighted sum of inputs plus offset for computing the output of a neuron. Also known as the squashing function. Affine group invariance.
Reference: [159] <author> Widrow, B., Rumelhart, D. E., and Lehr, M. A. </author> <title> Neural networks: Applications in industry, </title> <journal> business and science. Communications of the ACM, </journal> <volume> 37(3) </volume> <pages> 93-105, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: A large number of papers on the application of CWNs have since been published, in areas of application ranging from medicine, finance, process industry, high energy physics, automotives, telecommunications, robotics, to aerospace <ref> [85, 111, 159] </ref>. These applications are generally divided into two groups: function approximation 5 and classification. This division is based on the type of the desired outputs required to accomplish the task. <p> This makes them unsuitable for in-situ learning. Many commercial neural network applications (for example [135]), however, do not require in-situ training. The number of training epochs for IWNs is generally higher than for CWNs. This is not a major drawback because in many practical applications <ref> [159] </ref> the learning period occupies only a small fraction of a network's lifetime usage. <p> This makes them unsuitable for in-situ learning. Many commercial feedforward network applications (for example [135]), however, do not require in-situ training. The possibly large number of learning epochs required for MFNs as compared with CWNs should not be a deterrent to their use as, in many practical applications <ref> [159] </ref>, the learning period occupies only a small fraction of a network's lifetime usage. When products incorporating such applications are mass produced, e.g. a neural network controlled microwave oven, the length of training time becomes a completely insignificant part of the combined design and manufacture process.
Reference: [160] <author> Wnek, J. </author> <title> Hypothesis-driven Constructive Induction. </title> <type> PhD thesis, </type> <institution> George Mason University, </institution> <month> March </month> <year> 1993. </year> <note> Also available as technical report MLI 93-2 of Learning and Inference Laboratory, </note> <institution> Centre for Artificial Intelligence, School of Information Technology and Engineering. </institution>
Reference: [161] <author> Wnek, J. and Michalski, R. S. </author> <title> Comparing symbolic and subsymbolic learning: Three studies. </title> <editor> In Michalski, R. and Tecuci, G., editors, </editor> <booktitle> Machine Learning: A Multistrategy Approach, </booktitle> <volume> volume 4, </volume> <pages> pages 318-362. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference: [162] <author> Wnek, J., Sarma, J., Wahab, A., and Michalski, R. </author> <title> Comparison learning paradigms via diagrammatic visulization: A case study in single concept learning using symbolic, neural net and genetic algorithm methods. </title> <type> Technical report, </type> <institution> Computer Science Department, George Mason University, </institution> <year> 1990. </year>
Reference-contexts: The first one, MONK's benchmark, is an artificial set and was designed to compare the capabilities of a number of learning algorithms, consists of three separate classification tasks, one of which is noisy, and each of which has binary-coded inputs and a single binary output <ref> [143, 146, 162] </ref>.
Reference: [163] <author> Wolpert, D. H. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(2) </volume> <pages> 241-259, </pages> <year> 1992. </year>
Reference-contexts: the samples, and there may be repetitions within an individual sample. 3 The outputs of the k 1 cross-validation networks can however be combined with the help of a second level `master' network, trained with respect to the desired outputs of the training data, to form a large single network <ref> [163] </ref>. This technique is called `stacked generalisation'. 96 6.3 Regularisation Techniques `optimal stopping of training' to be discussed in the next section. 6.3 Regularisation Techniques Regularisation techniques avoid over-fitting and automate the model selection process. They result in smoother models, which in turn, results in better generalisation performance.
Reference: [164] <author> Woodland, P. C. </author> <title> Weight limiting, weight quantisation & generalisation in multilayer perceptrons. </title> <booktitle> In Proceedings IEE First International Conference Artificial Neural Nets, </booktitle> <pages> pages 297-300, </pages> <address> London, 1989. </address> <publisher> IEE, </publisher> <address> London, England. 153 REFERENCES </address>
Reference-contexts: Based on this idea, a multiple-thresholding method has been proposed for generating discrete-weight networks <ref> [28, 164] </ref>. In this simple method, the continuous weights of a fully trained network are discretised using a staircase function.
Reference: [165] <author> Wray, J. and Green, G. G. R. </author> <title> Neural networks, approximation theory, and finite precision computing. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(1) </volume> <pages> 31-37, </pages> <year> 1995. </year>
Reference-contexts: Cascade-correlation [37]), but the author is unaware of any work in which just b w , or both q and b w simultaneously, were varied during training. 3.4 Approximation Capabilities It has been pointed out by Wray and Green <ref> [165] </ref> that the very fact that feedforward networks are implemented in software/hardware on computers of finite word-lengths implies that the hidden-neuron activation functions are finite polynomials 7 , and are therefore not capable of universal approximation, as was shown by Leshno et al. in [83].
Reference: [166] <author> Xie, Y. and Jabri, M. A. </author> <title> Training algorithms for limited precision feedforward neural nets. </title> <type> SEDAL technical report 1991-8-3, </type> <institution> Department of Electrical Engineering, University of Sydney, </institution> <address> NSW 2006, Australia, </address> <year> 1991. </year>
Reference-contexts: In this method, all of the weights are perturbed in turn and the associated change in the output of the network is used to approximate local gradients [59]. This technique requires only feedforward calculations for its operation, which simplifies its implementation in hardware <ref> [166] </ref>. It lacks the mathematical efficiency of BP however, and therefore requires a large number of epochs to reach acceptable solutions. Stochastic methods are the less common alternative to steepest descent. A popular representative of these methods is simulated annealing. <p> Most workers, however, agree that networks with weights of medium resolution are adequate to model many learning tasks to the desired accuracy [33, 169], <ref> [58, 60, 112, 166, 167] </ref> 6 . The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two [77, 138] 6 or sum-of-powers-of-two [88, 89] 6 . <p> The weight update rule for this technique is: w = E o initial E o perturbed w initial w perturbed = ffiw (3.1) This technique, although not as efficient as BP, requires only feedforward calculations for its operation, which simplifies its implementation in hardware <ref> [166] </ref>. <p> They generally require a large number of epochs for convergence and each epoch generally requires the recalculation of E o for every training example and every weight modification. Combining the gradient-descent based and stochastic steps can also be fruitful. The Combined Search Algorithm <ref> [166] </ref> algorithm is an example of such an approach. It uses BP to search for solutions, and stochastic steps to jump out of shallow local minima. <p> The most likely reason was the formation of new local minima due to discretisation. It is well known that the standard BP algorithm sometimes gets stuck in local minima [122]. The superposition of the weight discretisation process on BP can result in changes in the shapes of these minima <ref> [166] </ref> or even an increase in their number. At the start of discrete-weight learning, E o is large, and the output error minimisation process dominates. Conversely, the weight discretisation process has the upper hand when E o is small.
Reference: [167] <author> Xie, Y. and Jabri, M. A. </author> <title> Analysis of the effects of quantization on multilayer neural networks using a statistical model. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 334-338, </pages> <year> 1992. </year>
Reference-contexts: Most workers, however, agree that networks with weights of medium resolution are adequate to model many learning tasks to the desired accuracy [33, 169], <ref> [58, 60, 112, 166, 167] </ref> 6 . The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two [77, 138] 6 or sum-of-powers-of-two [88, 89] 6 .
Reference: [168] <author> Yasunaga, M., Masuda, N., Yagyu, M., Asai, M., Shibata, K., Ooyama, M., Yamada, M., Sakaguchi, T., and Hashimoto, M. </author> <title> A self-learning neural network composed of 1152 digital neurons in wafer scale LSIs. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <pages> pages 1844-1849, </pages> <address> Seattle, WA, July 1991. </address> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference-contexts: The computational accuracy of this chip depends upon the absence of correlation between pulse trains. This is achieved by implementing each weight with its own random number generator. The presence of on-chip BP training makes this chip very useful for embedded applications. The WSI 21 neurocomputer from Hitachi <ref> [168] </ref> uses an input, an address, and an output bus to connect 1152 neurons in a SIMD 22 array. The address bus is used to sequentially select the 8-bit weights from common memory, and the dot product of inputs and weights is sequentially accumulated.
Reference: [169] <author> Yoo, H. and Pimmel, R. L. </author> <title> Weight discretization in back-propagation neural network classifiers. </title> <editor> In Dagli, C. H. et al., editor, </editor> <booktitle> Intelligent Engineering Systems through Artificial Neural Networks, </booktitle> <pages> pages 167-172. </pages> <publisher> ASME Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Most workers, however, agree that networks with weights of medium resolution are adequate to model many learning tasks to the desired accuracy <ref> [33, 169] </ref>, [58, 60, 112, 166, 167] 6 . The networks with low resolution weights have mostly been considered by the workers who were interested in streamlining the multiplication operation in neurons. They considered weight values that were either powers-of-two [77, 138] 6 or sum-of-powers-of-two [88, 89] 6 . <p> The quality of this approximation can be improved in one of two ways [22] either by allowing a larger number of discrete levels <ref> [40, 169] </ref>, or by having more hidden neurons [58]. This chapter, while discussing both, will emphasise the latter approach, and will mainly be concerned with weights having small integer values.

Reference: [1] <author> Chieueh, T. D., and Goodman, R. M.: </author> <title> `Learning algorithms for neural networks with ternary weights,' </title> <booktitle> presented at the First Annual Meeting of INNS, </booktitle> <address> Boston, MA, </address> <month> September </month> <year> 1988, </year> <title> abstract: </title> <booktitle> Neural Networks, </booktitle> <year> 1988, </year> <note> 1, p. 166 </note>
Reference-contexts: Internal Revenue Service tax form 1040EZ <ref> [1] </ref>. 109 6.4 Feature reduction (256 ! 32) of the handwritten numeral data. <p> This set of functions, N 1 (sin; S 1 ), is clearly not dense on compacta in C (R), being unable, for example, to universally approximate sin (20x) on <ref> [1; 1] </ref>. An application of Theorem 2.3 provides the following very useful result: Lemma 2.2 If is the logistic function then for w j 2 S d [ f0g; N d (; B)is uniformly dense in compacta in C (R d ). <p> Feedforward networks with offsets and hyperbolic tangent activation function in both the hidden and output layer neurons were used for these simulations. The training data was scaled to the range <ref> [1; 1] </ref>. These simulations involved clean data therefore the L 1 -norm, E o m , was used as the error function: training was stopped when E o m was less than a prespecified " and all the weights had reached integer values. <p> E o was found to decrease at rate of q 1=2 , where q is the number of hidden neurons [4, 64]. 4.3 Approximating CWNs with IWNs In the work reported here, mappings of the form f : R 2 ! f1; 1g 1 , R being a closed interval <ref> [1; 1] </ref>, were used for comparing the IWN and CWN decision surfaces for a set of 10 classification problems (Figure 4.4) which were used for numerous training runs on 2:q:1 networks, with and without skip-layer synapses 2 . The results for those simulations are summarised in Table 4.1. <p> This set was then split into two equal, balanced subsets for training and testing. All eight attributes were then standardised to zero mean and unit variance [13]. x x where x is the calculated mean, and x the standard deviation. All attribute values outside the <ref> [1; 1] </ref> range were truncated to f1; 1g. <p> Twelve individuals were asked to provide 100 samples each while following a given writing style, resulting in 120 examples of each numeral. The sample style given to the writers was similar to the one required for the U.S. Internal Revenue Service's machine readable tax form 1040EZ <ref> [1] </ref> (see Figure 6.3). The raw images of those samples were normalised 20 This database has been kindly made publicly available by Isabelle Guyon at ftp://hope.caltech.- edu/pub/mackay/data/att.database 109 6.5 Generalisation Experiments and then thresholded to fit a 16 fi 16 binary pixel grid.
Reference: [2] <author> Woodland, P. C.: </author> <title> `Weight limiting, weight quantisation & generalisation in multilayer perceptrons,' </title> <booktitle> Proc. IEE First Int. Conf. Artificial Neural Nets, </booktitle> <address> London, </address> <year> 1989, </year> <pages> pp. 297-300 </pages>
Reference-contexts: The quickest way to perform a multiplication in a digital electronic implementation is with a flash multiplier. An n-bit fixed-point VLSI flash multiplier consists of n fi n full adders, each one of which is made up of 31 transistors <ref> [2] </ref>. If the classic NETtalk network of Sejnowski and Rosenberg [132] was to be constructed in this very fast incarnation, approximately 10 9 transistors will be required for n = 8.
Reference: [3] <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J.: </author> <title> `Learning internal representation by error backpropagation,' in Parallel Distributed Processing: Explorations in the Microstructure of Cognition, 1, </title> <editor> Rumelhart, D. E., and McClelland, J. L. Eds. </editor> <address> (Cambridge MA: </address> <publisher> MIT Press, </publisher> <year> 1986), </year> <pages> pp. 318-362 </pages>
Reference-contexts: two in poor. 15 2.1 Universal approximation in one dimension. 26 3.1 Two choices for the discretising function Q (w) [84]. 47 3.2 Frequency distribution of numbers generated by tan (RND). 51 3.3 Q prac (w). 53 3.4 Comparison of the actual error term and its approximation in the range <ref> [3; 3] </ref>. 53 3.5 The black-hole function. 54 4.1 The set of decision boundaries of an integer [-3, 3] weight 2-input perceptron with offset. <p> function Q (w) [84]. 47 3.2 Frequency distribution of numbers generated by tan (RND). 51 3.3 Q prac (w). 53 3.4 Comparison of the actual error term and its approximation in the range [3; 3]. 53 3.5 The black-hole function. 54 4.1 The set of decision boundaries of an integer <ref> [-3, 3] </ref> weight 2-input perceptron with offset. <p> a weight resolution of 0.125. w ih = 3. w io = 3. h is plotted along the horizontal axis of the contour plots, and o is along the vertical axis. w ho is the figure in the brackets. 76 5.1 A comparison of the decision boundaries of a integer <ref> [-3, 3] </ref> weight perceptron (same as Figure 4.1) and a multiplier-free perceptron, each with two input synapses and an offset. <p> Two new types of networks are proposed which lend themselves to cost-effective implementations in hardware and have a fast forward-pass capability. These two differ from the conventional model in having extra constraints on their weights: the first allows its weights to take integer values in the range <ref> [3; 3] </ref> only, whereas the second restricts its synapses to the set f1; 0; 1g while allowing unrestricted offsets. The benefits of the first configuration are in having weights which are only 3-bits deep and a multiplication operation requiring a maximum of one shift, one add, and one sign-change instruction. <p> CWNs with many hidden layers may have advantages in having more profound mapping capabilities they can implement a mapping with fewer weights as compared with a 2-layer network having a similar performance. They are not, however, as well understood as 2-layer networks and are harder to train <ref> [3] </ref>. 1.3 Application Examples It has been 10 years since the pioneering work of Rumelhart et al. [123], in which learning in multilayer feedforward networks was first introduced. <p> For example, integer weights in the range <ref> [3; 3] </ref> can be represented by just 3 bits. This property reduces the amount of memory required for weight storage in digital electronic implementations. <p> The two choices that were considered are shown in Figure 3.1. The key feature of these functions is that the zeros of (Q (w) w) have the required integer values. In practice, the application of these functions is restricted to the interval <ref> [3; 3] </ref>, since any weight values outside this interval are truncated to f3; 3g. Of the two, Q tanh (w) is computationally more expensive to generate, yet it does have the advantage in having an adjustable slope between discrete values.
Reference: [4] <author> Von Lehmen, A., Paek, E. G., Liao, P. F., Marrakchi, A., and Patel, J. S.: </author> <title> `Factors influencing learning by backpropagation,' </title> <booktitle> in Proc. </booktitle> <address> IJCN, </address> <month> July </month> <year> 1988, </year> <pages> pp. </pages> <month> I-335-I-341 </month>
Reference-contexts: This logarithmic rate of decrease in error is different from the theoretically calculated degree of approximation result: E o was found to decrease at rate of q 1=2 , where q is the number of hidden neurons <ref> [4, 64] </ref>. 4.3 Approximating CWNs with IWNs In the work reported here, mappings of the form f : R 2 ! f1; 1g 1 , R being a closed interval [1; 1], were used for comparing the IWN and CWN decision surfaces for a set of 10 classification problems (Figure 4.4)
Reference: [5] <author> Fiesler, E., Choudry, A., and Caulfield, H. J.: </author> <title> `A weight discretization paradigm for optical neural networks,' </title> <booktitle> in Proc. Int. Cong. Opt. Sci. & Engg., 1990, SPIE-1281, </booktitle> <pages> pp. 164-173 </pages>
Reference-contexts: There is wealth of literature available on efficient E o minimisation techniques (see <ref> [5] </ref> for a survey). Many of these techniques are 2 nd -order methods they use information about the 2 nd derivative of the error function E o (W) to compute weight modifications.
Reference: [6] <author> Marchesi, M., Benvenuto, N., Orlandi, G., Piazza, F., and Uncini, A.: </author> <title> `Design of multi-layer neural networks with power-of-two weights,' </title> <publisher> IEEE ISCS, </publisher> <address> New Orleans: </address> <month> 1-3 May </month> <year> 1990, </year> <pages> 4, pp. 2951-2954 </pages>
Reference-contexts: Shattered. If a set of functions F includes all possible dichotomies on a set S of points, then S is said to be shattered by F . <ref> [6] </ref> Shrinkage. The difference between the training set accuracy of a network and its ac curacy on a test set. Sigmoidal functions. Definitions vary but are generally taken to be bounded, mono tone, and continuous, e.g. logistic and tanh () functions.
Reference: [7] <author> Thurn, S. B., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., De Jong, K., Dzeroski, S., Fahlman, S. E., Fisher, D., Hamann, R., Kaufman, K., Keller, S., Kononenko, I., Kreuziger, J., Michalski, R. S., Mitchell, T., Pachowicz, P., Reich, Y., Vafaie, H., Van de Welde, W., Wenzel, W., Wnek, J., and Zhang, J.: </author> <title> `The MONK's problems: A performance comparison of different learning algorithms', </title> <institution> Carnegie Mellon University, CMU-CS-91-197, </institution> <month> December </month> <year> 1991. </year> <title> The training and test data sets for the MONK's problems are available at the ftp site ics.uci.edu </title>
Reference-contexts: approximation' [29, 62, 68]? What is the relation of the quality of approximation with the number of neurons or the depth of weights [22, 64]? How difficult is it to train the network [18]? What is the expected generalisation performance and its relation with the number of necessary training examples <ref> [7] </ref>? Similarly, questions can be raised about the convergence properties of a given combination of a neural architecture and a specific learning heuristic [74]. <p> about economy in explanations, was systematically applied, but never explicitly stated by the 13th century philosopher, William of Ockham [141]. 98 6.5 Generalisation Experiments however, a problem with using larger than optimal networks although smaller networks are not guaranteed to be better in general, Baum and Haussler have shown in <ref> [7] </ref> that for a given training set and training error, the worst-case error bounds on unseen data vary proportionally with the number of weights in a network. Moreover, larger-than-optimal networks are slower and demand more storage space.
Reference: [8] <author> Hush, D. R., and Horne, B. G.: </author> <title> `Progress in supervised neural networks: What's new since Lippmann,' </title> <journal> IEEE Signal Processing Mag., 1993, </journal> <volume> 10, </volume> <pages> pp. 8-39 </pages>

Reference: [1] <author> R. W. Brause. </author> <title> The error-bound descriptional complexity of approximation networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(6) </volume> <pages> 177-187, </pages> <year> 1993. </year>
Reference-contexts: Internal Revenue Service tax form 1040EZ <ref> [1] </ref>. 109 6.4 Feature reduction (256 ! 32) of the handwritten numeral data. <p> This set of functions, N 1 (sin; S 1 ), is clearly not dense on compacta in C (R), being unable, for example, to universally approximate sin (20x) on <ref> [1; 1] </ref>. An application of Theorem 2.3 provides the following very useful result: Lemma 2.2 If is the logistic function then for w j 2 S d [ f0g; N d (; B)is uniformly dense in compacta in C (R d ). <p> Feedforward networks with offsets and hyperbolic tangent activation function in both the hidden and output layer neurons were used for these simulations. The training data was scaled to the range <ref> [1; 1] </ref>. These simulations involved clean data therefore the L 1 -norm, E o m , was used as the error function: training was stopped when E o m was less than a prespecified " and all the weights had reached integer values. <p> E o was found to decrease at rate of q 1=2 , where q is the number of hidden neurons [4, 64]. 4.3 Approximating CWNs with IWNs In the work reported here, mappings of the form f : R 2 ! f1; 1g 1 , R being a closed interval <ref> [1; 1] </ref>, were used for comparing the IWN and CWN decision surfaces for a set of 10 classification problems (Figure 4.4) which were used for numerous training runs on 2:q:1 networks, with and without skip-layer synapses 2 . The results for those simulations are summarised in Table 4.1. <p> This set was then split into two equal, balanced subsets for training and testing. All eight attributes were then standardised to zero mean and unit variance [13]. x x where x is the calculated mean, and x the standard deviation. All attribute values outside the <ref> [1; 1] </ref> range were truncated to f1; 1g. <p> Twelve individuals were asked to provide 100 samples each while following a given writing style, resulting in 120 examples of each numeral. The sample style given to the writers was similar to the one required for the U.S. Internal Revenue Service's machine readable tax form 1040EZ <ref> [1] </ref> (see Figure 6.3). The raw images of those samples were normalised 20 This database has been kindly made publicly available by Isabelle Guyon at ftp://hope.caltech.- edu/pub/mackay/data/att.database 109 6.5 Generalisation Experiments and then thresholded to fit a 16 fi 16 binary pixel grid.
Reference: [2] <author> E. Fiesler, A. Choudry, and H. J. Caulfield. </author> <title> A weight discretization paradigm for optical neural networks. </title> <booktitle> In Proc. of the Int. Cong. on Opt. Sc. and Engg., </booktitle> <pages> pages 164-173, </pages> <address> Bellingham, Washington, </address> <year> 1990. </year> <pages> SPIE. </pages>
Reference-contexts: The quickest way to perform a multiplication in a digital electronic implementation is with a flash multiplier. An n-bit fixed-point VLSI flash multiplier consists of n fi n full adders, each one of which is made up of 31 transistors <ref> [2] </ref>. If the classic NETtalk network of Sejnowski and Rosenberg [132] was to be constructed in this very fast incarnation, approximately 10 9 transistors will be required for n = 8.
Reference: [3] <author> A. H. Khan and E. L. Hines. </author> <title> Integer-weight neural nets. </title> <journal> Electron. Lett., </journal> <volume> 30(15): </volume> <pages> 1237-1238, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: two in poor. 15 2.1 Universal approximation in one dimension. 26 3.1 Two choices for the discretising function Q (w) [84]. 47 3.2 Frequency distribution of numbers generated by tan (RND). 51 3.3 Q prac (w). 53 3.4 Comparison of the actual error term and its approximation in the range <ref> [3; 3] </ref>. 53 3.5 The black-hole function. 54 4.1 The set of decision boundaries of an integer [-3, 3] weight 2-input perceptron with offset. <p> function Q (w) [84]. 47 3.2 Frequency distribution of numbers generated by tan (RND). 51 3.3 Q prac (w). 53 3.4 Comparison of the actual error term and its approximation in the range [3; 3]. 53 3.5 The black-hole function. 54 4.1 The set of decision boundaries of an integer <ref> [-3, 3] </ref> weight 2-input perceptron with offset. <p> a weight resolution of 0.125. w ih = 3. w io = 3. h is plotted along the horizontal axis of the contour plots, and o is along the vertical axis. w ho is the figure in the brackets. 76 5.1 A comparison of the decision boundaries of a integer <ref> [-3, 3] </ref> weight perceptron (same as Figure 4.1) and a multiplier-free perceptron, each with two input synapses and an offset. <p> Two new types of networks are proposed which lend themselves to cost-effective implementations in hardware and have a fast forward-pass capability. These two differ from the conventional model in having extra constraints on their weights: the first allows its weights to take integer values in the range <ref> [3; 3] </ref> only, whereas the second restricts its synapses to the set f1; 0; 1g while allowing unrestricted offsets. The benefits of the first configuration are in having weights which are only 3-bits deep and a multiplication operation requiring a maximum of one shift, one add, and one sign-change instruction. <p> CWNs with many hidden layers may have advantages in having more profound mapping capabilities they can implement a mapping with fewer weights as compared with a 2-layer network having a similar performance. They are not, however, as well understood as 2-layer networks and are harder to train <ref> [3] </ref>. 1.3 Application Examples It has been 10 years since the pioneering work of Rumelhart et al. [123], in which learning in multilayer feedforward networks was first introduced. <p> For example, integer weights in the range <ref> [3; 3] </ref> can be represented by just 3 bits. This property reduces the amount of memory required for weight storage in digital electronic implementations. <p> The two choices that were considered are shown in Figure 3.1. The key feature of these functions is that the zeros of (Q (w) w) have the required integer values. In practice, the application of these functions is restricted to the interval <ref> [3; 3] </ref>, since any weight values outside this interval are truncated to f3; 3g. Of the two, Q tanh (w) is computationally more expensive to generate, yet it does have the advantage in having an adjustable slope between discrete values.
Reference: [4] <author> H. Yoo and R. L. Pimmel. </author> <title> Weight discretization in back-propagation neural network classifiers. </title> <editor> In C. H. Dagli et al., eds., </editor> <booktitle> Intel. Engg. Sys. through ANNs., </booktitle> <pages> pages 167-172. </pages> <publisher> ASME Press, </publisher> <address> New York, </address> <year> 1991. </year> <title> 196 Publications from This Thesis (g) (h) (i) (j) (k) (l) ! 2:2:1 net with double-precision weights (a-f); ! 2:2:1 net with integer weights (g-l). td d td d td d td d td d XOR1 XOR 3 4 XOR 1 2 XOR 1 4 XOR 1 8 l l l w ih Z Z w io A A </title>
Reference-contexts: This logarithmic rate of decrease in error is different from the theoretically calculated degree of approximation result: E o was found to decrease at rate of q 1=2 , where q is the number of hidden neurons <ref> [4, 64] </ref>. 4.3 Approximating CWNs with IWNs In the work reported here, mappings of the form f : R 2 ! f1; 1g 1 , R being a closed interval [1; 1], were used for comparing the IWN and CWN decision surfaces for a set of 10 classification problems (Figure 4.4)
References-found: 181

