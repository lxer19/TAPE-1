URL: http://polaris.cs.uiuc.edu/reports/1144.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: The Importance of Prepass Code Scheduling for Superscalar and Superpipelined Processors  
Author: Pohua P. Chang Daniel M. Lavery Scott A. Mahlke William Y. Chen Wen-mei W. Hwu 
Keyword: Index terms Code scheduling, control-intensive programs, optimizing compiler, register allocation, superpipelined processors, superscalar processors.  
Affiliation: Center for Reliable and High-Performance Computing, University of Illinois, Urbana-Champaign, Illinois,  University of Illinois, Urbana-Champaign, Illinois, 61801.  
Note: The authors are with the  61801. Daniel Lavery is also with the Center for Supercomputing Research and Development,  
Date: March 8, 1994  
Abstract: Superscalar and superpipelined processors utilize parallelism to achieve peak performance that can be several times higher than that of conventional scalar processors. In order for this potential to be translated into the speedup of real programs, the compiler must be able to schedule instructions so that the parallel hardware is effectively utilized. Previous work has shown that prepass code scheduling helps to produce a better schedule for scientific programs. But the importance of prescheduling has never been demonstrated for control-intensive non-numeric programs. These programs are significantly different from the scientific programs because they contain frequent branches. The compiler must do global scheduling in order to find enough independent instructions. In this paper, the code optimizer and scheduler of the IMPACT-I C compiler is described. Within this framework, we study the importance of prepass code scheduling for a set of production C programs. It is shown that, in contrast to the results previously obtained for scientific programs, prescheduling is not important for compiling control-intensive programs to the current generation of superscalar and superpipelined processors. However, if some of the current restrictions on upward code motion can be removed in future architectures, prescheduling would substantially improve the execution time of this class of programs on both superscalar and su-perpipelined processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Goodman and W.-C. Hsu, </author> <title> "Code Scheduling and Register Allocation in Large Basic Blocks," </title> <booktitle> in Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pp. 442-452, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: On the other hand, if code scheduling is performed before register allocation (prepass or prescheduling), the register lifetimes may be lengthened, which may increase the amount of spill code added by the register allocator. In previous work, Goodman and Hsu <ref> [1] </ref> showed that a prepass scheduler can keep track of the number of available registers to avoid introducing excessive spill code. Hwu and Chang [2] showed that a prescheduling, register allocation, postscheduling sequence extracts more performance from scientific benchmarks than postscheduling alone. <p> Temporary values should be produced as late as possible and used as early as possible. The second disadvantage can be reduced by increasing the register file size or more tightly integrating the code scheduler and register allocator as in <ref> [1] </ref>. We evaluate the performance of prescheduling for various register file sizes, but do not consider more integrated schemes in this paper.
Reference: [2] <author> W. W. Hwu and P. P. Chang, </author> <title> "Exploiting Parallel Microprocessor Microarchitectures with a Compiler Code Generator," </title> <booktitle> in Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 45-53, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: In previous work, Goodman and Hsu [1] showed that a prepass scheduler can keep track of the number of available registers to avoid introducing excessive spill code. Hwu and Chang <ref> [2] </ref> showed that a prescheduling, register allocation, postscheduling sequence extracts more performance from scientific benchmarks than postscheduling alone. Both of these results apply to scientific programs 2 with code scheduling and register allocation performed within large basic blocks. The importance of prescheduling has never been demonstrated for control-intensive non-numeric programs.
Reference: [3] <author> J. L. Hennessy and T. Gross, </author> <title> "Postpass Code Optimization of Pipeline Constraints," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 422-448, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: The study done in this paper shows that for architectures that relax the current restrictions on upward code motion, prescheduling helps to achieve this goal. In other related work, Hennessy and Gross <ref> [3] </ref> provided a good description of the code scheduling problem and a scheduling algorithm. Fisher [4] and Ellis [5] described a very effective global scheduling algorithm called trace scheduling. A paper by Chaitin [6] presented the graph-coloring-based register allocation algorithm on which our global register allocator is based.
Reference: [4] <author> J. A. Fisher, </author> <title> "Trace Scheduling: A Technique for Global Microcode Compaction," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. c-30, </volume> <pages> pp. 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: The study done in this paper shows that for architectures that relax the current restrictions on upward code motion, prescheduling helps to achieve this goal. In other related work, Hennessy and Gross [3] provided a good description of the code scheduling problem and a scheduling algorithm. Fisher <ref> [4] </ref> and Ellis [5] described a very effective global scheduling algorithm called trace scheduling. A paper by Chaitin [6] presented the graph-coloring-based register allocation algorithm on which our global register allocator is based. This paper is organized as follows. <p> Because the algorithm does not take into account the cost of instructions that cannot be overlapped, it may allocate registers in a way that handicaps the code scheduler. 2.4 Superblock Scheduling This section describes the IMPACT-I code scheduler, which is based on a new variation of trace scheduling <ref> [4, 5] </ref> that we call superblock scheduling. The idea is to select frequently executed paths through the code and optimize them, perhaps at the expense of the less frequently executed paths. <p> In the traces formed in step 1, there may be many side exits and entrances. The side entrances especially increase the difficulty of code scheduling because complex bookkeeping must be done when code is moved above and below these entrances <ref> [4] </ref>. These complex repairs could be avoided if side entrances could be removed from the trace. One way to do this would be not to add a block to a trace if it produces a side entrance.
Reference: [5] <author> J. R. Ellis, "Bulldog: </author> <title> A Compiler for VLIW Architectures," </title> <type> Ph.D Thesis, </type> <institution> MIT Press, </institution> <address> Cam-bridge, MA, </address> <year> 1986. </year>
Reference-contexts: In other related work, Hennessy and Gross [3] provided a good description of the code scheduling problem and a scheduling algorithm. Fisher [4] and Ellis <ref> [5] </ref> described a very effective global scheduling algorithm called trace scheduling. A paper by Chaitin [6] presented the graph-coloring-based register allocation algorithm on which our global register allocator is based. This paper is organized as follows. <p> Because the algorithm does not take into account the cost of instructions that cannot be overlapped, it may allocate registers in a way that handicaps the code scheduler. 2.4 Superblock Scheduling This section describes the IMPACT-I code scheduler, which is based on a new variation of trace scheduling <ref> [4, 5] </ref> that we call superblock scheduling. The idea is to select frequently executed paths through the code and optimize them, perhaps at the expense of the less frequently executed paths. <p> The algorithm and heuristics we use for trace selection were first proposed by Ellis <ref> [5] </ref> and improved by Chang and Hwu [19]. An example of the result of trace selection on a weighted flow graph can be seen in [16].
Reference: [6] <author> G. J. Chaitin, </author> <title> "Register Allocation and Spilling Via Graph Coloring," </title> <journal> ACM SIGPLAN Notices, </journal> <volume> vol. 17, </volume> <pages> pp. 98-105, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: In other related work, Hennessy and Gross [3] provided a good description of the code scheduling problem and a scheduling algorithm. Fisher [4] and Ellis [5] described a very effective global scheduling algorithm called trace scheduling. A paper by Chaitin <ref> [6] </ref> presented the graph-coloring-based register allocation algorithm on which our global register allocator is based. This paper is organized as follows. Section 2 gives the necessary background on prescheduling and postscheduling, our C compiler, and its register allocator and scheduler. <p> The machine-independent optimizations include the classic local and global code optimizations [13], inline expansion of frequently executed functions [14], instruction placement optimization [15], profile-based classic code optimizations [16], and profile-based optimizations to increase the available instruction-level parallelism [17]. The machine-dependent optimizations include profile-based branch prediction [18], graph-coloring-based register allocation <ref> [6] </ref>, and code scheduling. The results in this paper are based on the IMPACT compiler implementation. The task of evaluating the importance of the results for other compiler systems is left to the reader. <p> The task of evaluating the importance of the results for other compiler systems is left to the reader. The following sections describe the global register allocator and scheduler. 9 2.3 Register Allocation The IMPACT-I global register allocator is based on the graph-coloring algorithm described in <ref> [6] </ref>. The algorithm constructs an interference graph in which each node represents a value. An arc is added between two nodes if they are ever simultaneously live. Two adjacent nodes cannot be allocated to the same register.
Reference: [7] <author> P. P. Chang, S. A. Mahlke, W. Y. Chen, N. J. Warter, and W. W. Hwu, </author> <title> "IMPACT: An Architectural Framework for Multiple-Instruction-Issue Processors," </title> <booktitle> in Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 266-275, </pages> <month> May </month> <year> 1991. </year> <month> 45 </month>
Reference-contexts: This is why prescheduling is so important for scientific programs. We show that, using global optimization techniques combined with the proper architectural support, enough parallelism can be extracted from control-intensive programs to make prescheduling necessary. 2.2 IMPACT-I C Compiler The IMPACT-I C Compiler <ref> [7] </ref> is a retargetable, optimizing compiler designed to generate very efficient code for pipelined and multiple-instruction-issue processors. Code generators have been built for the MIPS R2000 [8], the Sun SPARC [9], the AMD 29K [10], the Intel i860 [11], and the HP PA [12] processors.
Reference: [8] <author> G. Kane, </author> <title> MIPS R2000 RISC Architecture. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, Inc., </publisher> <year> 1987. </year>
Reference-contexts: Code generators have been built for the MIPS R2000 <ref> [8] </ref>, the Sun SPARC [9], the AMD 29K [10], the Intel i860 [11], and the HP PA [12] processors. IMPACT-I is used to study the effectiveness of new code optimization techniques and to study alternative approaches in the design of processors that exploit instruction-level parallelism.
Reference: [9] <author> Sun Microsystems, </author> <title> "The SPARC Architecture Manual," Part No. 800-1399-07, Revision 50, </title> <address> Mountain View, CA, </address> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: Code generators have been built for the MIPS R2000 [8], the Sun SPARC <ref> [9] </ref>, the AMD 29K [10], the Intel i860 [11], and the HP PA [12] processors. IMPACT-I is used to study the effectiveness of new code optimization techniques and to study alternative approaches in the design of processors that exploit instruction-level parallelism.
Reference: [10] <author> Advanced Micro Devices, </author> <title> "Am29000 32-Bit Streamlined Instruction Processor," Users Manual, </title> <address> Sunnyvale, CA, </address> <year> 1988. </year>
Reference-contexts: Code generators have been built for the MIPS R2000 [8], the Sun SPARC [9], the AMD 29K <ref> [10] </ref>, the Intel i860 [11], and the HP PA [12] processors. IMPACT-I is used to study the effectiveness of new code optimization techniques and to study alternative approaches in the design of processors that exploit instruction-level parallelism. The compiler contains a profiler to identify the most frequently executed program paths.
Reference: [11] <author> Intel, </author> <title> "i860 64-bit Microprocessor," Order Number 240296-002, </title> <address> Santa Clara, CA, </address> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: Code generators have been built for the MIPS R2000 [8], the Sun SPARC [9], the AMD 29K [10], the Intel i860 <ref> [11] </ref>, and the HP PA [12] processors. IMPACT-I is used to study the effectiveness of new code optimization techniques and to study alternative approaches in the design of processors that exploit instruction-level parallelism. The compiler contains a profiler to identify the most frequently executed program paths.
Reference: [12] <author> Hewlett-Packard Company, </author> <title> "Precision Architecture and Instruction Set Reference Manual, 3rd Ed.," Part Number 09740-90039, </title> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: Code generators have been built for the MIPS R2000 [8], the Sun SPARC [9], the AMD 29K [10], the Intel i860 [11], and the HP PA <ref> [12] </ref> processors. IMPACT-I is used to study the effectiveness of new code optimization techniques and to study alternative approaches in the design of processors that exploit instruction-level parallelism. The compiler contains a profiler to identify the most frequently executed program paths.
Reference: [13] <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: The compiler contains a profiler to identify the most frequently executed program paths. This information is used to guide the global code optimization and scheduling. IMPACT-I currently performs a wide variety of machine-independent and machine-dependent code optimizations. The machine-independent optimizations include the classic local and global code optimizations <ref> [13] </ref>, inline expansion of frequently executed functions [14], instruction placement optimization [15], profile-based classic code optimizations [16], and profile-based optimizations to increase the available instruction-level parallelism [17]. The machine-dependent optimizations include profile-based branch prediction [18], graph-coloring-based register allocation [6], and code scheduling. <p> They include: branch target expansion, loop peeling, loop unrolling, register renaming, induction variable expansion, accumulator variable expansion, operation migration, operation combining, and tree height reduction. 4 Traditional local and global optimizations that do not utilize profile information are also performed at this point <ref> [13] </ref>. 14 2.4.5 Step 4: Dependence Graph Construction In this step, a conservative dependence graph is built for each superblock.
Reference: [14] <author> P. P. Chang, S. A. Mahlke, W. Y. Chen, and W. W. Hwu, </author> <title> "Profile-Guided Automatic Inline Expansion for C Programs," </title> <journal> Software Practice and Experience, </journal> <volume> vol. 22, </volume> <pages> pp. 349-369, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This information is used to guide the global code optimization and scheduling. IMPACT-I currently performs a wide variety of machine-independent and machine-dependent code optimizations. The machine-independent optimizations include the classic local and global code optimizations [13], inline expansion of frequently executed functions <ref> [14] </ref>, instruction placement optimization [15], profile-based classic code optimizations [16], and profile-based optimizations to increase the available instruction-level parallelism [17]. The machine-dependent optimizations include profile-based branch prediction [18], graph-coloring-based register allocation [6], and code scheduling. The results in this paper are based on the IMPACT compiler implementation. <p> The results from all the runs are averaged and used to assign weights to the nodes and arcs of the graphs. Frequently executed function calls are then inline expanded <ref> [14] </ref>. 11 2.4.2 Step 1: Trace Selection The goal of trace selection is to divide the function into a set of traces such that for each block X, if there is a block Y immediately following (preceding) X in a trace, Y is the block most likely to be executed after
Reference: [15] <author> W. W. Hwu and P. P. Chang, </author> <title> "Achieving High Instruction Cache Performance with an Optimizing Compiler," </title> <booktitle> in Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 242-251, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This information is used to guide the global code optimization and scheduling. IMPACT-I currently performs a wide variety of machine-independent and machine-dependent code optimizations. The machine-independent optimizations include the classic local and global code optimizations [13], inline expansion of frequently executed functions [14], instruction placement optimization <ref> [15] </ref>, profile-based classic code optimizations [16], and profile-based optimizations to increase the available instruction-level parallelism [17]. The machine-dependent optimizations include profile-based branch prediction [18], graph-coloring-based register allocation [6], and code scheduling. The results in this paper are based on the IMPACT compiler implementation. <p> Once the traces have been selected, the basic blocks of each trace are laid out sequentially in memory <ref> [15] </ref>.
Reference: [16] <author> P. P. Chang, S. A. Mahlke, and W. W. Hwu, </author> <title> "Using Profile Information to Assist Classic Code Optimizations," </title> <journal> Software Practice and Experience, </journal> <volume> vol. 21, </volume> <pages> pp. 1301-1321, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: IMPACT-I currently performs a wide variety of machine-independent and machine-dependent code optimizations. The machine-independent optimizations include the classic local and global code optimizations [13], inline expansion of frequently executed functions [14], instruction placement optimization [15], profile-based classic code optimizations <ref> [16] </ref>, and profile-based optimizations to increase the available instruction-level parallelism [17]. The machine-dependent optimizations include profile-based branch prediction [18], graph-coloring-based register allocation [6], and code scheduling. The results in this paper are based on the IMPACT compiler implementation. <p> The final subsection comments on some concerns that have been expressed about the effects of superblock scheduling on the code size and compile time. 2.4.1 Program Representation, Profiling, and Preparation In our C compiler, a function is represented as a weighted flow graph <ref> [16] </ref>. Several steps are taken to prepare the program for optimization and code scheduling. First, the flow graphs are generated for each function. Probes are then inserted into all the basic blocks to collect the execution counts, and the program is profiled several times with different inputs. <p> The algorithm and heuristics we use for trace selection were first proposed by Ellis [5] and improved by Chang and Hwu [19]. An example of the result of trace selection on a weighted flow graph can be seen in <ref> [16] </ref>. A node is not added to a trace unless its execution count is higher than a minimum count and the probability of entering it from its predecessor or leaving it for its successor in the trace is greater than a minimum probability 3 . <p> At this point, the trace, with only a single entrance remaining, becomes a superblock that can be optimized with special handling only for the side exits. An example of superblock formation can be seen in <ref> [16] </ref>. When a block is copied, its execution count in the original trace is reduced by the weight of the side entrances removed. If the block has multiple successors, the proportion of the weight that should be subtracted from each arc is not known. <p> For the profile-based optimizations, this approximate information is good enough. For accurate analysis of the final schedule however, the transformed program must be reprofiled after superblock optimization. 13 An additional benefit of tail duplication is that code optimizations can be more easily applied to superblocks than to traces <ref> [16] </ref>. The IMPACT-I compiler uses the superblock as a common foundation for both optimizations and code scheduling. 2.4.4 Step 3: Superblock Optimization After superblock formation, many classic code optimizations are performed that take advantage of the profile information encoded in the superblock structure 4 . <p> These optimizations are designed to decrease the number of instructions on the frequently executed paths, perhaps at the expense of the infrequently executed ones <ref> [16] </ref>. They include: constant propagation, copy propagation, constant combining, common subexpression elimination, redundant load and store elimination, dead code removal, loop invariant code removal, loop induction variable elimination, and global variable migration. <p> For the benchmarks described earlier, the performance of IMPACT-I is slightly better than that of the MIPS C compiler <ref> [16] </ref>.
Reference: [17] <author> S. A. Mahlke, W. Y. Chen, J. C. Gyllenhaal, W. W. Hwu, P. P. Chang, and T. Kiyohara, </author> <title> "Compiler Code Transformations for Superscalar-Based High-Performance Systems," </title> <booktitle> in Proceedings of Supercomputing `92, </booktitle> <month> Nov. </month> <year> 1992. </year> <month> 46 </month>
Reference-contexts: IMPACT-I currently performs a wide variety of machine-independent and machine-dependent code optimizations. The machine-independent optimizations include the classic local and global code optimizations [13], inline expansion of frequently executed functions [14], instruction placement optimization [15], profile-based classic code optimizations [16], and profile-based optimizations to increase the available instruction-level parallelism <ref> [17] </ref>. The machine-dependent optimizations include profile-based branch prediction [18], graph-coloring-based register allocation [6], and code scheduling. The results in this paper are based on the IMPACT compiler implementation. The task of evaluating the importance of the results for other compiler systems is left to the reader. <p> They include: constant propagation, copy propagation, constant combining, common subexpression elimination, redundant load and store elimination, dead code removal, loop invariant code removal, loop induction variable elimination, and global variable migration. Next, several profile-based code transformations are performed that increase the available instruction-level parallelism of the intermediate code <ref> [17] </ref> [20]. These optimizations increase the size of superblocks and eliminate data dependences between instructions. They are applied only to the most frequently executed superblocks to control code expansion and compile time.
Reference: [18] <author> P. P. Chang and W. W. Hwu, </author> <title> "Forward Semantic: A Compiler-assisted Instruction Fetch Method for Heavily Pipelined Processors," </title> <booktitle> in Proceedings of the 22nd International Workshop on Microprogramming and Microarchitecture, </booktitle> <pages> pp. 188-198, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: The machine-independent optimizations include the classic local and global code optimizations [13], inline expansion of frequently executed functions [14], instruction placement optimization [15], profile-based classic code optimizations [16], and profile-based optimizations to increase the available instruction-level parallelism [17]. The machine-dependent optimizations include profile-based branch prediction <ref> [18] </ref>, graph-coloring-based register allocation [6], and code scheduling. The results in this paper are based on the IMPACT compiler implementation. The task of evaluating the importance of the results for other compiler systems is left to the reader. <p> For example, the hardware can do register renaming for register dependences and memory access sequence control for memory dependences. 6 For the superscalar processors, multiple branches can be issued in a cycle and the architecture uses a squashing branch scheme <ref> [18] </ref>. 15 2.4.6 Step 5: Dependence Graph Optimization In this step, the dependence graph is optimized by removing some of the dependence arcs. During the list scheduling step (described in the next subsection), the instructions are reordered to improve the execution time within the constraints of the dependences. <p> Its instruction set is similar to the MIPS R2000 instruction set. Table 1 shows the instruction latencies. Instructions are issued in order (there is no dynamic code scheduling). The processor is assumed to have interlocks for structural and read-after-write hazards. The microarchitecture uses a squashing branch scheme <ref> [18] </ref> and profile-based branch prediction. One branch slot is allocated by the compiler for each predicted-taken branch. The processor has 32 integer registers and 32 floating-point registers 9 . <p> The instruction fetch and decode unit is also more heavily pipelined to keep the microarchitecture balanced. Because of this and the more deeply pipelined compare-and-branch units, the number of cycles of delay due to mispredicted branches and the number of instructions squashed increases <ref> [18] </ref>. For the superscalar processor, the additional datapaths, functional units, and instruction unit logic may increase the cycle time. For the superpipelined processor, the cycle time is actually 25 Table 2: The benchmarks.
Reference: [19] <author> P. P. Chang and W. W. Hwu, </author> <title> "Trace Selection for Compiling Large C Application Programs to Microcode," </title> <booktitle> in Proceedings of the 21st International Microprogramming Workshop, </booktitle> <pages> pp. 21-29, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: The algorithm and heuristics we use for trace selection were first proposed by Ellis [5] and improved by Chang and Hwu <ref> [19] </ref>. An example of the result of trace selection on a weighted flow graph can be seen in [16].
Reference: [20] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bringmann, R. G. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lavery, </author> <title> "The Superblock: An Effective Technique for VLIW and Superscalar Compilation," </title> <note> to appear in The Journal of Supercomputng. </note>
Reference-contexts: They include: constant propagation, copy propagation, constant combining, common subexpression elimination, redundant load and store elimination, dead code removal, loop invariant code removal, loop induction variable elimination, and global variable migration. Next, several profile-based code transformations are performed that increase the available instruction-level parallelism of the intermediate code [17] <ref> [20] </ref>. These optimizations increase the size of superblocks and eliminate data dependences between instructions. They are applied only to the most frequently executed superblocks to control code expansion and compile time. <p> The importance of prescheduling may vary with different heuristics. The evaluation of the importance of prescheduling for different heuristics is beyond the scope of this paper. 20 2.4.8 The Effect of Superblock Scheduling on Compile Time and Code Size In <ref> [20] </ref>, we have measured the code expansion and compile time increase due to trace selection, superblock formation and optimization for the benchmarks used in this paper. The code size is increased by an average of 100%. Cache simulation results in [20] show that despite the code size increase, an instruction cache <p> of Superblock Scheduling on Compile Time and Code Size In <ref> [20] </ref>, we have measured the code expansion and compile time increase due to trace selection, superblock formation and optimization for the benchmarks used in this paper. The code size is increased by an average of 100%. Cache simulation results in [20] show that despite the code size increase, an instruction cache of 16K bytes or larger performs nearly as well as an ideal cache. Since most future processors will have an instruction cache at least this large, we do not expect code expansion to be a problem. <p> However, this extra effort is worthwhile if it can significantly reduce the execution time of important frequently-executed programs such as the Unix programs that make up part of our benchmark set. Currently, most microprocessor manufacturers are already producing superscalar processors with issue rates between 2 and 5. In <ref> [20] </ref>, it is shown that the superblock techniques do significantly improve the performance of important programs for these issue rates. The increased compile time can be viewed as part of the overall workload on a machine.
Reference: [21] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman, </author> <title> "A VLIW Architecture for a Trace Scheduling Compiler," </title> <booktitle> in Proceedings of the 2nd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 180-192, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: This model is called general percolation. In this model, the architecture provides non-trapping versions of the instructions that can cause exceptions. Whenever an instruction is moved upward across a branch, 16 the non-trapping version is used. A similar approach has been implemented in the Multiflow Trace computer <ref> [21] </ref> 7 . If an exception occurs during a non-trapping instruction, the exception is simply ignored (except for page faults, which are handled normally). An invalid value is placed in the destination register for loads and arithmetic operations.
Reference: [22] <author> M. D. Smith, M. S. Lam, and M. A. Horowitz, </author> <title> "Boosting Beyond Static Scheduling in a Superscalar Processor," </title> <booktitle> in Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 344-354, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Smith, Lam, and Horowitz described a method called boosting which uses extra hardware to remove both the first and second restriction without ignoring exceptions <ref> [22] </ref>. We have shown that boosting and general percolation have similar performance [23]. Currently, we are investigating sentinel scheduling, a very promising new technique which allows the code scheduling flexibility of general percolation without ignoring exceptions and without requiring much extra hardware [24].
Reference: [23] <author> P. P. Chang, N. J. Warter, S. A. Mahlke, W. Y. Chen, and W. W. Hwu, </author> <title> "Three Su-perblock Scheduling Models for Superscalar and Superpipelined Processors," </title> <institution> Center for Reliable and High-Performance Computing Report CRHC-91-25, University of Illinois at Urbana-Champaign, </institution> <month> Oct. </month> <year> 1991. </year> <month> 47 </month>
Reference-contexts: Smith, Lam, and Horowitz described a method called boosting which uses extra hardware to remove both the first and second restriction without ignoring exceptions [22]. We have shown that boosting and general percolation have similar performance <ref> [23] </ref>. Currently, we are investigating sentinel scheduling, a very promising new technique which allows the code scheduling flexibility of general percolation without ignoring exceptions and without requiring much extra hardware [24].
Reference: [24] <author> S. A. Mahlke, W. Y. Chen, W. W. Hwu, B. R. Rau, and M. S. Schlansker, </author> <title> "Sentinel Schedul--ing for VLIW and Superscalar Processors," </title> <booktitle> in Proceedings of the Fifth Annual International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: We have shown that boosting and general percolation have similar performance [23]. Currently, we are investigating sentinel scheduling, a very promising new technique which allows the code scheduling flexibility of general percolation without ignoring exceptions and without requiring much extra hardware <ref> [24] </ref>. The results achieved with general percolation in this paper confirm the importance of speculative code motion and show the potential of these new techniques.
Reference: [25] <author> P. P. Chang, W. Y. Chen, S. A. Mahlke, and W. W. Hwu, </author> <title> "Comparing Static and Dynamic Code Scheduling for Multiple-Instruction-Issue Processors," </title> <booktitle> in Proceedings of the 24th International Symposium and Workshop on Microarchitecture, </booktitle> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Using these traces, we have simulated the superscalar execution of the benchmarks in a previous study and found that the simulated execution time matches the time calculated as described above <ref> [25] </ref>. We also verified that the program output during this execution and trace generation was correct 8 . The execution time result for each compilation is reported as a speedup relative to the compilation for the base microarchitecture.
Reference: [26] <author> N. P. Jouppi and D. W. Wall, </author> <title> "Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines," </title> <booktitle> in Proceedings of the Third Annual International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 272-282, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: Also note that the speedups for the dual-issue superscalar machine (in the previous experiment) and the single-issue, 2X-superpipelined machine are similar. The same is true for the superscalar machine with issue rate 4 and the dual-issue, 2X-superpipelined machine. This matches results that have been reported in the literature before <ref> [26] </ref>. support general percolation. The base architecture is a single-issue processor with no prescheduling and restricted percolation. 2 and 3 are single-issue, 2X- and 3X-superpipelined processors respectively. 4 and 6 are dual-issue, 2X- and 3X-superpipelined processors respectively.
References-found: 26

