URL: http://www.cs.rutgers.edu/~uli/PC98.ps
Refering-URL: http://www.cs.rutgers.edu/~uli/pubs.html
Root-URL: http://www.cs.rutgers.edu
Title: Tools and Techniques for Automatic Data Layout: A Case Study  
Author: Eduard Ayguade Jordi Garcia Ulrich Kremer 
Affiliation: Computer Architecture Department Universitat Politecnica de Catalunya  Department of Computer Science Rutgers University  
Abstract: Parallel architectures with physically distributed memory providing computing cycles and large amounts of memory are becoming more and more common. To make such architectures truly usable, programming models and support tools are needed to ease the programming effort for these parallel systems. Automatic data distribution tools and techniques play an important role in achieving that goal. This paper discusses state-of-the-art approaches to fully automatic data and computation partitioning. A kernel application is used as a case study to illustrate the main differences of four representative approaches. The paper concludes with a discussion of promising future research directions for automatic data layout.
Abstract-found: 1
Intro-found: 1
Reference: [ACG + 94] <author> V. Adve, A. Carle, E. Granston, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, J. Mellor-Crummey, C-W. Tseng, and S. Warren. </author> <title> Requirements for data-parallel programming environments. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 48-58, </pages> <year> 1994. </year>
Reference-contexts: An intraprocedural prototype of a data layout tool based on the discussed framework has been implemented as part of the D System <ref> [ACG + 94] </ref>. Experiments indicate that good data layouts can be determined efficiently. ADI example in the PCFG shown on the left. To simplify the DLG, node and edge weights are not shown. Phases may have candidate layout search spaces of different sizes.
Reference: [AGG + 94] <author> E. Ayguade, J. Garcia, M. Girones, J. Labarta, J. Torres, and M. Valero. </author> <title> Detecting and using affinity in an automatic data distribution tool. </title> <editor> In K. Pingali et al., editors, </editor> <booktitle> Proceedings of the 7th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 61-75, </pages> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year> <note> Springer-Verlag. </note>
Reference-contexts: The original heuristic solution algorithm proposed by Li and Chen has been improved by several groups in order to generate better alignments with only a low increase in algorithm complexity <ref> [AGG + 94, LC96] </ref>.
Reference: [AGG + 95] <author> E. Ayguade, J. Garcia, M. Girones, M.L. Grande, and J. Labarta. </author> <title> Data redistribution in an automatic data distribution tool. </title> <editor> In C.-H. Huang et al., editors, </editor> <booktitle> Proceedings of the 8th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 407-421, </pages> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year> <note> Springer-Verlag. </note>
Reference-contexts: Other approaches are based on the exploration of a search space of candidate solutions for a given set of phases and using a cut function to prune the exhaustive search; Kelly and Pugh consider the problem at the intraprocedural level [KP96] and Ayguade et al. at the interprocedural level <ref> [AGG + 95] </ref>. Alternatively, Palermo and Banerjee use a different approach to the problem that starts from a static solution for the whole program [PB95]. Then, it is recursively decomposed into a hierarchy of candidate phases using a cost function.
Reference: [AGG + 97] <author> E. Ayguade, J. Garcia, M. Girones, M.L. Grande, and J. Labarta. DDT: </author> <title> A research tool for automatic data distribution. </title> <journal> Scientific Programming, </journal> <volume> Vol 6(1) </volume> <pages> 73-94, </pages> <month> Spring </month> <year> 1997. </year> <month> 19 </month>
Reference-contexts: They prove the NP-completeness of the problem and propose a heuristic algorithm. Knobe, Lucas, and Steele [KLS90] use a similar graph formulation: the preference graph. In addition to communication related preferences, preferences that reflect possible parallelism are also represented in the graph (anti-preferences). Ayguade et al. <ref> [AGG + 97] </ref> extend the CAG representation in the framework of their DDT (Data Distribution Tool) to include alignment preferences that would lead to an efficient parallelization of the loops. <p> In order to decide the distribution strategy, some groups have proposed iterative algorithms that evaluate the behavior of a set of distributions <ref> [LC91, Gup92, AGG + 97] </ref>, most of them restricting to one or two dimensional distributions, either BLOCK or CYCLIC (n). Wholey uses a hill climbing approach to find multidimensional distributions [Who92]. The proposal starts assigning two processors to a single dimension.
Reference: [AGGL96] <author> E. Ayguade, J. Garcia, M.L. Grande, and J. Labarta. </author> <title> Data distribution and loop paral--lelization for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 9th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, CA, </address> <month> August </month> <year> 1996. </year> <month> Sprinver-Verlag. </month>
Reference-contexts: For cache-coherent NUMA systems, the estimation of data movement has to consider the physical allocation of data across the memory modules; data elements are moved either in pages or cache lines. Invalidations and false sharing are additional aspects to consider when considering alternatives for data mapping <ref> [AGGL96, TA96] </ref>. For cahe-coherent distributed memory NUMA architectures, the performance model may have to consider data locality across different levels of the memory hierarchy, i.e., local memory and caches. Recently, researchers have started to develop automatic data and 18 computation mapping techniques that consider memory hierarchies of multiple levels.
Reference: [AL93] <author> J. Anderson and M.S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN'93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: However in real codes, a communication-free alignment can be rarely found. Bau et al. propose different problem formulations to minimize communication costs when communication-free partitioning is not possible [BKK + 94a]. Anderson and Lam <ref> [AL93, And97a] </ref> combine mathematical and graph based problem representations to find a minimal communication cost alignment. However, the majority of the approaches use a weighted graph based formulation. The advantage of a graph based formulation is that it allows an easier representation of tradeoff decisions between communication and exploitable parallelism. <p> In addition, several groups have extended the parallel loop model to consider pipelined computations. Synchronization is used to ensure the correct overlap of chunks of dependent iterations <ref> [AL93, Kre95, KP96] </ref>. 3.3 Remapping Problem For complex problems, static data alignments and distributions may be insufficient to obtain acceptable performance. In this case remapping actions between code blocks (usualy named phases) can improve the efficiency of the solution, but communication is required to reorganize the data. <p> The NP-completeness of the dynamic problem is proven in [Kre93]. Anderson and Lam use a heuristic which combines loop nests (with potentially different distributions) in such a way that the largest potential communication costs are eliminated while still maintaining sufficient parallelism <ref> [AL93] </ref>. <p> Universitat Politecnica de Catalunya, that solves the whole problem in a single step, starting from an enumeration of phases. 4.1 Anderson and Lam Anderson and Lam in the framework of the SUIF compiler at Stanford University, discuss a compiler algorithm for automatic data and computation mapping for dense matrix computations <ref> [AL93, And97a] </ref>. The input programs consist of generally nested loops with or without explicit parallelism. Each loop can be either a forall, doacross, or sequential loop. Automatic parallelization is performed for sequential loops. <p> Since the data and computation mappings only specify which array elements and iterations are local to a single processor, a last step is needed to determine the actual virtual processor on which the data and computation are mapped. This last step is referred to as orientation and displacement computation <ref> [AL93] </ref>. A greedy algorithm is proposed to match as closely as possible the orientations between components. ADI example The ADI example fits the author's base case for dynamic data layout where the program consists of one outer sequential loop containing a number of perfectly nested loops.
Reference: [And97a] <author> J. Anderson. </author> <title> Automatic Computation and Data Decomposition for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: However in real codes, a communication-free alignment can be rarely found. Bau et al. propose different problem formulations to minimize communication costs when communication-free partitioning is not possible [BKK + 94a]. Anderson and Lam <ref> [AL93, And97a] </ref> combine mathematical and graph based problem representations to find a minimal communication cost alignment. However, the majority of the approaches use a weighted graph based formulation. The advantage of a graph based formulation is that it allows an easier representation of tradeoff decisions between communication and exploitable parallelism. <p> Universitat Politecnica de Catalunya, that solves the whole problem in a single step, starting from an enumeration of phases. 4.1 Anderson and Lam Anderson and Lam in the framework of the SUIF compiler at Stanford University, discuss a compiler algorithm for automatic data and computation mapping for dense matrix computations <ref> [AL93, And97a] </ref>. The input programs consist of generally nested loops with or without explicit parallelism. Each loop can be either a forall, doacross, or sequential loop. Automatic parallelization is performed for sequential loops. <p> Such a mapping can be referred to as a data and computation alignment. They also address issues related to distribution by calculating a mapping from the virtual processor array onto the physical processor space, using either a block, cyclic, or block-cyclic folding function <ref> [And97a] </ref>. Their work handles data and computation alignments that may change dynamically, i.e., are not the same for the entire program. <p> More experiments are needed to validate the different approaches to automatic data and computation mapping. A crucial mile stone in allowing comprehensive experimentation is interprocedural analysis since large real application programs consist of many procedures. The PARADIGM and SUIF projects have already interprocedural prototype implementations <ref> [PB96, And97a] </ref>. An interprocedural implementation based on the Kennedy and Kremer framework is currently under development at Rutgers University. Besides the need for more experimental validation, future challenges include data and computation mappings under resource constraints and mappings for objective functions different from merely minimal execution times.
Reference: [And97b] <author> J. Anderson. </author> <title> Private communication, </title> <month> August </month> <year> 1997. </year>
Reference-contexts: Anderson and Lam, and Palermo and Bannerjee report execution times of their implementations in the order of seconds for programs in their validation program test suites <ref> [And97b, Pal97] </ref>. However it is important to consider the expected execution times of the approaches by Kremer and Kennedy, and Garcia et al. since they use 0-1 integer programming techniques to find 17 optimal solutions.
Reference: [BFKK91] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Each routine has an associated cost which is a function of the block size, communication parameters of the target architecture, and number of processors. This strategy has been adopted by most proposals as a way to estimate the data movement cost. Kennedy and Kremer estimate performance using training sets <ref> [BFKK91] </ref>, a collection of kernel routines that measure run times of several communication and computation patterns in a given architecture. The benefits of parallel loop execution are usually computed from the profiled sequential execution time and considering the number of processors that will execute that loop in parallel. <p> Based on the resulting information, the execution model and target architecture machine model computes the overall execution time. The execution model can predict the performance of pipelined program execution with different granularities. The machine model uses a training set approach <ref> [BFKK91] </ref>. Finally, a solution to the global data layout selection problem chooses exactly one candidate data layout for each phase such that the overall cost of the resulting path is minimal. This problem has been shown to be NP-complete [Kre93].
Reference: [Bix92] <author> R. Bixby. </author> <title> Implementing the Simplex method: The initial basis. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4(3), </volume> <year> 1992. </year>
Reference-contexts: Table 1 shows the execution times of prototype implementations of their ap-proaches in seconds for three benchmark programs. The timings were taken on single processor SUN workstations. The 0-1 problem instances were solved by general purpose integer programming tools CPLEX v4.0 <ref> [Bix92] </ref> (Kremer and Kennedy) and LINGO v3.1 [LIN94] (Garcia et al.).
Reference: [BKA96] <author> R. Barua, D. Kranz, and A. Agarwal. </author> <title> Communication-minimal partitioning of parallel loops and data arrays for cache-coherent distributed-memory multiprocessors. </title> <booktitle> In Proceedings of the 9th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, CA, </address> <month> August </month> <year> 1996. </year> <note> Springer-Verlag. </note>
Reference-contexts: Recently, researchers have started to develop automatic data and 18 computation mapping techniques that consider memory hierarchies of multiple levels. The tech-niques are able to deal with the tradeoffs between parallelism, inter-processor communcication, and cache traffic <ref> [BKA96] </ref>. More experiments are needed to validate the different approaches to automatic data and computation mapping. A crucial mile stone in allowing comprehensive experimentation is interprocedural analysis since large real application programs consist of many procedures. The PARADIGM and SUIF projects have already interprocedural prototype implementations [PB96, And97a].
Reference: [BKK + 94a] <author> D. Bau, I. Kodukula, V. Kotlyar, K. Pingali, and P. Stodghill. </author> <title> Solving alignment using elementary linear algebra. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Ramanujam and Sadayappan [RS89] use matrix notation to represent a communication-free static alignment, and solve the problem with linear algebra. However in real codes, a communication-free alignment can be rarely found. Bau et al. propose different problem formulations to minimize communication costs when communication-free partitioning is not possible <ref> [BKK + 94a] </ref>. Anderson and Lam [AL93, And97a] combine mathematical and graph based problem representations to find a minimal communication cost alignment. However, the majority of the approaches use a weighted graph based formulation.
Reference: [BKK94b] <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT94), </booktitle> <pages> pages 111-122, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Bixby, Kennedy, and Kremer formulate the dynamic data partitioning problem as a 0-1 integer programming problem that selects a single candidate data layout from predetermined candidate layout search spaces for each phase <ref> [BKK94b] </ref>. The NP-completeness of the dynamic problem is proven in [Kre93]. Anderson and Lam use a heuristic which combines loop nests (with potentially different distributions) in such a way that the largest potential communication costs are eliminated while still maintaining sufficient parallelism [AL93]. <p> A PCFG is a compacted version of a control flow graph where all CFG nodes associated with a phase are represented by a single node in the PCFG, and edges are annotated with control flow information such as branch probabilities or frequency of execution <ref> [BKK94b, Kre95] </ref>. Candidate layout search spaces are generated for each phase in the PCFG based on heuristics. Heuristics are needed to restrict the search space sizes, but should avoid the elimination of promising candidates. <p> Therefore a hyperedge associated to this loop is added connecting the corresponding nodes in the CPG. This hyperedge is shown with bold line in the same Figure 6.a. In terms of remapping information, the phase control flow analysis generates the PCFG similar to the one generated by <ref> [BKK94b] </ref>, and detects that the six phases are contained within an iterative loop of max iterations. Therefore forward remapping between phases has to be accounted for max times, and backward remapping between the last and the first phases within the loop has to be accounted for max-1 times.
Reference: [CGC96] <author> S. Chakrabarti, M. Gupta, and J-D. Choi. </author> <title> Global communication analysis and optimization. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 68-78, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Shared name space models can be supported in hardware (shared memory [Ost89, FFG + 95]), through a combination of hardware and operating system layers (distributed shared memory [LLG + 90, KCDZ94, IDFL96, KHS + 97]), or mostly through compiler technology 1 (distributed memory <ref> [KLS + 94, Tse93, CGC96] </ref>). All shared name space implementations have in common that in order to achieve good and scalable performance most computations have to be local, i.e., each processor accesses data already available in its local memory or local cache.
Reference: [CGS93] <author> S. Chatterjee, J.R. Gilbert, and R. Schreiber. </author> <title> The alignment-distribution graph. </title> <editor> In U. Baner-jee et al., editors, </editor> <booktitle> Proceedings of the 6th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year> <note> Springer-Verlag. </note>
Reference-contexts: Laure and Chapman weight CAG edges with an estimate of the data movement cost assuming the worst case [LC96]. Similarly, Kremer determines the edge weights as the maximum data volume of communication that may occur if the corresponding alignment preference is not satisfied [Kre95]. Schreiber et al. <ref> [CGS93] </ref> propose an alternative representation to solve the alignment problem: the alignment distribution graph (ADG). Computations are represented by nodes with edges modeling the flow of data. Edges are weighted by the communication costs that will occur if the source and sink nodes do not have compatible alignments.
Reference: [CGSS94] <author> S. Chatterjee, J.R. Gilbert, R. Schreiber, and T.J. She*er. </author> <title> Array distribution in data-parallel programs. </title> <editor> In K. Pingali et al., editors, </editor> <booktitle> Proceedings of the 7th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Then, it is recursively decomposed into a hierarchy of candidate phases using a cost function. Taking into account the remapping costs between these phases, the most efficient sequence of phases is selected. A similar divide-and-conquer approach is used by Chatterjee et al. to solve the dynamic alignment problem <ref> [CGSS94] </ref>. Finally, Garcia, Ayguade and Labarta define a single data structure, named the Communication Parallelism Graph (CPG), to represent the whole data mapping problem. There is a trade-off between minimizing data movement and maximizing load balance on processors.
Reference: [FFG + 95] <author> D. Fenwick, D. Foley, W. Gist, S. VanDoren, and D. Wissell. </author> <title> The Alphaserver 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1), </volume> <year> 1995. </year>
Reference-contexts: The advantage of a single name space is that the user does not have to explicitly manage processor local name spaces, local computations, and necessary communication between processors. Shared name space models can be supported in hardware (shared memory <ref> [Ost89, FFG + 95] </ref>), through a combination of hardware and operating system layers (distributed shared memory [LLG + 90, KCDZ94, IDFL96, KHS + 97]), or mostly through compiler technology 1 (distributed memory [KLS + 94, Tse93, CGC96]).
Reference: [GAL95] <author> J. Garcia, E. Ayguade, and J. Labarta. </author> <title> A novel approach towards automatic data distribution. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: There is a trade-off between minimizing data movement and maximizing load balance on processors. Therefore they propose to solve the alignment, distribution, and remapping problems in a single step <ref> [GAL95, GAL96] </ref>. The CPG includes information related to both data movement and parallelization costs. <p> problem that was tested (approx. instance size: 200 phases, 1400 nodes, and 13,000 edges) was solved optimally in less than 3.4 seconds on a SUN UltraSparc using the general-purpose mixed integer programming tool CPLEX with dual simplex as the initial node relaxation. 4.4 Garcia, Ayguade and Labarta Garcia et al. <ref> [GAL95, GAL96] </ref> at the Universitat Politecnica de Catalunya in Spain, propose a framework to automatically determine the data mapping and parallelization strategy for a Fortran 77 program, in the context of a parallelizing environment for massive parallel processor 14 systems.
Reference: [GAL96] <author> J. Garcia, E. Ayguade, and J. Labarta. </author> <title> Dynamic data distribution with control flow analysis. </title> <booktitle> In Proceedings of Supercomputing'96, </booktitle> <address> Pittsburgh, PA, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: There is a trade-off between minimizing data movement and maximizing load balance on processors. Therefore they propose to solve the alignment, distribution, and remapping problems in a single step <ref> [GAL95, GAL96] </ref>. The CPG includes information related to both data movement and parallelization costs. <p> problem that was tested (approx. instance size: 200 phases, 1400 nodes, and 13,000 edges) was solved optimally in less than 3.4 seconds on a SUN UltraSparc using the general-purpose mixed integer programming tool CPLEX with dual simplex as the initial node relaxation. 4.4 Garcia, Ayguade and Labarta Garcia et al. <ref> [GAL95, GAL96] </ref> at the Universitat Politecnica de Catalunya in Spain, propose a framework to automatically determine the data mapping and parallelization strategy for a Fortran 77 program, in the context of a parallelizing environment for massive parallel processor 14 systems.
Reference: [Gar97] <author> J. Garcia. </author> <title> Automatic Data Distribution for Massively Parallel Processors. </title> <type> PhD thesis, </type> <institution> Universitat Politecnica de Catalunya, </institution> <address> Barcelona, Spain, </address> <month> April </month> <year> 1997. </year> <note> Available as UPC-DAC-97-27 or UPC-CEPBA-97-08. 20 </note>
Reference-contexts: Similarly, Garcia et al. also start by defining phases and the kind of solutions allowed for each of them. Allowing different kind of solutions (such as BLOCK or CYCLIC, or multidimensional distributions) for each phase results in CPG replication and therefore in an increase of the problem complexity <ref> [Gar97] </ref>. Anderson and Lam, and Palermo and Bannerjee report execution times of their implementations in the order of seconds for programs in their validation program test suites [And97b, Pal97].
Reference: [GB92] <author> M. Gupta and P. Banerjee. </author> <title> Compile-time estimation of communication costs on multicom--puters. </title> <booktitle> In Proceedings of the 6th International Parallel Processing Symposium, </booktitle> <address> Beverly Hills, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: to eliminate remaining nearest neighbor communication. 4.2 Palermo and Banerjee PARADIGM (PARAlelizing compiler for DIstributed-memory General-purpose Multicomput-ers) is a research tool developed at the University of Illinois at Urbana-Champaign, that can automatically select dynamic data distributions starting from static distributions generated using a constraint-based algorithm [GB93] and compile-time cost estimations <ref> [GB92] </ref> based on empirically measured parameters. The technique proposed by Palermo and Banerjee for the automatic selection of dynamic data mappings [PB95] can be broken down into two main steps. First the program is recursively 10 decomposed into a hierarchy of candidate phases.
Reference: [GB93] <author> M. Gupta and P. Banerjee. </author> <title> PARADIGM: A compiler for automatic data distribution on mul-ticomputers. </title> <booktitle> In Proceedings of the 7th ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Displacement tries to eliminate remaining nearest neighbor communication. 4.2 Palermo and Banerjee PARADIGM (PARAlelizing compiler for DIstributed-memory General-purpose Multicomput-ers) is a research tool developed at the University of Illinois at Urbana-Champaign, that can automatically select dynamic data distributions starting from static distributions generated using a constraint-based algorithm <ref> [GB93] </ref> and compile-time cost estimations [GB92] based on empirically measured parameters. The technique proposed by Palermo and Banerjee for the automatic selection of dynamic data mappings [PB95] can be broken down into two main steps. First the program is recursively 10 decomposed into a hierarchy of candidate phases.
Reference: [Gup92] <author> M. Gupta. </author> <title> Automatic Data Partitioning on Distributed Memory Multicomputers. </title> <type> PhD thesis, </type> <institution> Center for Reliable and High-Performance Computing, University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Most authors that have adopted the CAG as their basic infrastructure to solve the alignment problem, have targetted their efforts to accurately weighting the edges in the CAG. For instance, Gupta in the PARADIGM system, weights edges representing communication costs assuming a default distribution for the aligned arrays <ref> [Gup92] </ref>. Laure and Chapman weight CAG edges with an estimate of the data movement cost assuming the worst case [LC96]. Similarly, Kremer determines the edge weights as the maximum data volume of communication that may occur if the corresponding alignment preference is not satisfied [Kre95]. <p> In order to decide the distribution strategy, some groups have proposed iterative algorithms that evaluate the behavior of a set of distributions <ref> [LC91, Gup92, AGG + 97] </ref>, most of them restricting to one or two dimensional distributions, either BLOCK or CYCLIC (n). Wholey uses a hill climbing approach to find multidimensional distributions [Who92]. The proposal starts assigning two processors to a single dimension.
Reference: [IDFL96] <author> L. Iftode, C. Dubnicki, E.W. Felten, and K. Li. </author> <title> Improving release-consistent shared virtual memory using automatic update. </title> <booktitle> In Proceedings of the 2nd International Symposium on High Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Shared name space models can be supported in hardware (shared memory [Ost89, FFG + 95]), through a combination of hardware and operating system layers (distributed shared memory <ref> [LLG + 90, KCDZ94, IDFL96, KHS + 97] </ref>), or mostly through compiler technology 1 (distributed memory [KLS + 94, Tse93, CGC96]).
Reference: [KCDZ94] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter USENIX Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: Shared name space models can be supported in hardware (shared memory [Ost89, FFG + 95]), through a combination of hardware and operating system layers (distributed shared memory <ref> [LLG + 90, KCDZ94, IDFL96, KHS + 97] </ref>), or mostly through compiler technology 1 (distributed memory [KLS + 94, Tse93, CGC96]).
Reference: [KHS + 97] <author> L Kontothanassis, G. Hunt, R. Stets, N. Hardavellas, M. Cierniak, S. Parthasarathy, W. Meira, S. Dwarkadas, and M. Scott. </author> <title> VM-based shared memory on low-latency, remote-memory-access networks. </title> <booktitle> In Proceedings of the 24th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Shared name space models can be supported in hardware (shared memory [Ost89, FFG + 95]), through a combination of hardware and operating system layers (distributed shared memory <ref> [LLG + 90, KCDZ94, IDFL96, KHS + 97] </ref>), or mostly through compiler technology 1 (distributed memory [KLS + 94, Tse93, CGC96]).
Reference: [KK95] <author> K. Kennedy and U. Kremer. </author> <title> Automatic data layout for High Performance Fortran. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: The original heuristic solution algorithm proposed by Li and Chen has been improved by several groups in order to generate better alignments with only a low increase in algorithm complexity [AGG + 94, LC96]. Alternatively, Kennedy and Kremer <ref> [KK95] </ref> propose the use of 0 1 integer programming techniques to find an optimal solution to the minimum cost partitioning of the CAG. 3.2 Distribution Problem The arrays distribution strategy specifies a partition of the group of aligned arrays onto an abstract processors grid, according to a given distribution fashion. <p> An appropriate formulation of the problem has been shown to be effective for these kind of problems. The first step in the method is to decompose the code into phases, according to the definition of phase made in <ref> [KK95] </ref>. Each phase in the program is represented by a subgraph in the CPG. Subgraphs are structured in columns of nodes: one column is an array, and each node in a column is a distributable dimension of the array.
Reference: [KLS90] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Typically, the alignment problem is solved as a minimal cost graph partitioning problem. Most of the graph based approaches have been inspired by the original proposals of Li and Chen [LC90], and Knobe, Lucas, and Steele <ref> [KLS90] </ref>. 5 Li and Chen [LC90] present the Component Alignment Graph (CAG), a graph where nodes represent array dimensions and edges reflect preferences for alignment. Edges in the graph are weighted with 1 or " to reflect either a strong or a competing preference respectively. <p> Finding the optimal alignment is reduced to a minimal graph partitioning problem over the CAG. They prove the NP-completeness of the problem and propose a heuristic algorithm. Knobe, Lucas, and Steele <ref> [KLS90] </ref> use a similar graph formulation: the preference graph. In addition to communication related preferences, preferences that reflect possible parallelism are also represented in the graph (anti-preferences).
Reference: [KLS + 94] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Shared name space models can be supported in hardware (shared memory [Ost89, FFG + 95]), through a combination of hardware and operating system layers (distributed shared memory [LLG + 90, KCDZ94, IDFL96, KHS + 97]), or mostly through compiler technology 1 (distributed memory <ref> [KLS + 94, Tse93, CGC96] </ref>). All shared name space implementations have in common that in order to achieve good and scalable performance most computations have to be local, i.e., each processor accesses data already available in its local memory or local cache.
Reference: [KP96] <author> W. Kelly and W. Pugh. </author> <title> Minimizing communication while preserving parallelism. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <address> Philadelphia, PE, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In addition, several groups have extended the parallel loop model to consider pipelined computations. Synchronization is used to ensure the correct overlap of chunks of dependent iterations <ref> [AL93, Kre95, KP96] </ref>. 3.3 Remapping Problem For complex problems, static data alignments and distributions may be insufficient to obtain acceptable performance. In this case remapping actions between code blocks (usualy named phases) can improve the efficiency of the solution, but communication is required to reorganize the data. <p> Other approaches are based on the exploration of a search space of candidate solutions for a given set of phases and using a cut function to prune the exhaustive search; Kelly and Pugh consider the problem at the intraprocedural level <ref> [KP96] </ref> and Ayguade et al. at the interprocedural level [AGG + 95]. Alternatively, Palermo and Banerjee use a different approach to the problem that starts from a static solution for the whole program [PB95]. Then, it is recursively decomposed into a hierarchy of candidate phases using a cost function.
Reference: [Kre93] <author> U. Kremer. </author> <title> NP-completeness of dynamic remapping. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year> <note> Also available as technical report CRPC-TR93-330-S (D Newsletter #8), </note> <institution> Rice University. </institution>
Reference-contexts: Bixby, Kennedy, and Kremer formulate the dynamic data partitioning problem as a 0-1 integer programming problem that selects a single candidate data layout from predetermined candidate layout search spaces for each phase [BKK94b]. The NP-completeness of the dynamic problem is proven in <ref> [Kre93] </ref>. Anderson and Lam use a heuristic which combines loop nests (with potentially different distributions) in such a way that the largest potential communication costs are eliminated while still maintaining sufficient parallelism [AL93]. <p> The machine model uses a training set approach [BFKK91]. Finally, a solution to the global data layout selection problem chooses exactly one candidate data layout for each phase such that the overall cost of the resulting path is minimal. This problem has been shown to be NP-complete <ref> [Kre93] </ref>. Rather than resorting to heuristic prematurely, the framework capitalizes on state-of-the-art 0-1 integer programming technology to compute optimal solutions to two NP-complete problems, namely the data layout selection problem and the interdimensional alignment problem.
Reference: [Kre95] <author> U. Kremer. </author> <title> Automatic Data Layout for Distributed Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> October </month> <year> 1995. </year> <note> Available as CRPC-TR95-559-S. </note>
Reference-contexts: Laure and Chapman weight CAG edges with an estimate of the data movement cost assuming the worst case [LC96]. Similarly, Kremer determines the edge weights as the maximum data volume of communication that may occur if the corresponding alignment preference is not satisfied <ref> [Kre95] </ref>. Schreiber et al. [CGS93] propose an alternative representation to solve the alignment problem: the alignment distribution graph (ADG). Computations are represented by nodes with edges modeling the flow of data. <p> In addition, several groups have extended the parallel loop model to consider pipelined computations. Synchronization is used to ensure the correct overlap of chunks of dependent iterations <ref> [AL93, Kre95, KP96] </ref>. 3.3 Remapping Problem For complex problems, static data alignments and distributions may be insufficient to obtain acceptable performance. In this case remapping actions between code blocks (usualy named phases) can improve the efficiency of the solution, but communication is required to reorganize the data. <p> A PCFG is a compacted version of a control flow graph where all CFG nodes associated with a phase are represented by a single node in the PCFG, and edges are annotated with control flow information such as branch probabilities or frequency of execution <ref> [BKK94b, Kre95] </ref>. Candidate layout search spaces are generated for each phase in the PCFG based on heuristics. Heuristics are needed to restrict the search space sizes, but should avoid the elimination of promising candidates. <p> The final data layout search space for each phase consists of the cross product of the alignment and distribution candidates of the search spaces in each phase. Kremer discusses different heuristics for alignment and distribution analysis in <ref> [Kre95] </ref>. Heuristics for alignment analysis use operations over a lattice of conflict-free CAGs. <p> The resulting CAGs are included in the individual phase search spaces. A node in the DLG represents a candidate data layout in the data layout search space of a phase. The edges represent possible remapping between candidate layouts. The DLG construction as described in <ref> [Kre95] </ref> uses the assumption that only a single copy of an array can exist at any time during program execution. Phases from which data layouts can be inherited are determined by solving the least recently referenced data flow problem (LR-Refs).
Reference: [Kre96] <author> U. Kremer. </author> <title> Automatic data layout with read-only replication and memory constraints. </title> <type> Technical Report LCSR-TR93-283, </type> <institution> Department of Computer Science, Rutgers University, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: Phases from which data layouts can be inherited are determined by solving the least recently referenced data flow problem (LR-Refs). More recent work extends the framework to allow read-only replication where profitable <ref> [Kre96] </ref>. DLG nodes and edges are weighted with their estimated execution times. A powerful performance estimator based on a compiler model, an execution model, and a machine model is used to predict the execution times of each candidate layout and the costs of possible remapping between candidate layouts. <p> Different data mappings may induce different memory requirements of the generated node programs. Read-only replication can be used to reduce communication costs under the constraint that the additional memory requirements can be satisfied <ref> [Kre96] </ref>. New objective functions for data and computation mappings will be needed in programming environments that provide computation and memory resources in a time-shared fashion. In such systems, users will be charged for the actual amount of computing resources used by their program execution.
Reference: [LC90] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: Typically, the alignment problem is solved as a minimal cost graph partitioning problem. Most of the graph based approaches have been inspired by the original proposals of Li and Chen <ref> [LC90] </ref>, and Knobe, Lucas, and Steele [KLS90]. 5 Li and Chen [LC90] present the Component Alignment Graph (CAG), a graph where nodes represent array dimensions and edges reflect preferences for alignment. <p> Typically, the alignment problem is solved as a minimal cost graph partitioning problem. Most of the graph based approaches have been inspired by the original proposals of Li and Chen <ref> [LC90] </ref>, and Knobe, Lucas, and Steele [KLS90]. 5 Li and Chen [LC90] present the Component Alignment Graph (CAG), a graph where nodes represent array dimensions and edges reflect preferences for alignment. Edges in the graph are weighted with 1 or " to reflect either a strong or a competing preference respectively.
Reference: [LC91] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol 2(3), </volume> <month> July </month> <year> 1991. </year> <month> 21 </month>
Reference-contexts: In order to decide the distribution strategy, some groups have proposed iterative algorithms that evaluate the behavior of a set of distributions <ref> [LC91, Gup92, AGG + 97] </ref>, most of them restricting to one or two dimensional distributions, either BLOCK or CYCLIC (n). Wholey uses a hill climbing approach to find multidimensional distributions [Who92]. The proposal starts assigning two processors to a single dimension. <p> The cost model presented by Li and Chen is based on a pattern matching between the refer 6 ence pattern of an assignment statement within a loop and a predefined set of data movement routines <ref> [LC91] </ref>. Each routine has an associated cost which is a function of the block size, communication parameters of the target architecture, and number of processors. This strategy has been adopted by most proposals as a way to estimate the data movement cost.
Reference: [LC96] <author> E. Laure and B. Chapman. </author> <title> A refined method for alignment analysis combining inter- and intradimensional alignment preferences. </title> <editor> In M. Gerndt, editor, </editor> <booktitle> Proceedings of the 6th International Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 169-180, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: For instance, Gupta in the PARADIGM system, weights edges representing communication costs assuming a default distribution for the aligned arrays [Gup92]. Laure and Chapman weight CAG edges with an estimate of the data movement cost assuming the worst case <ref> [LC96] </ref>. Similarly, Kremer determines the edge weights as the maximum data volume of communication that may occur if the corresponding alignment preference is not satisfied [Kre95]. Schreiber et al. [CGS93] propose an alternative representation to solve the alignment problem: the alignment distribution graph (ADG). <p> The original heuristic solution algorithm proposed by Li and Chen has been improved by several groups in order to generate better alignments with only a low increase in algorithm complexity <ref> [AGG + 94, LC96] </ref>.
Reference: [LIN94] <author> LINDO Systems Inc. </author> <title> LINGO Optimization Modeling Language, </title> <month> April </month> <year> 1994. </year>
Reference-contexts: Table 1 shows the execution times of prototype implementations of their ap-proaches in seconds for three benchmark programs. The timings were taken on single processor SUN workstations. The 0-1 problem instances were solved by general purpose integer programming tools CPLEX v4.0 [Bix92] (Kremer and Kennedy) and LINGO v3.1 <ref> [LIN94] </ref> (Garcia et al.).
Reference: [LLG + 90] <author> D. Lenoski, J. Laudon, K Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Shared name space models can be supported in hardware (shared memory [Ost89, FFG + 95]), through a combination of hardware and operating system layers (distributed shared memory <ref> [LLG + 90, KCDZ94, IDFL96, KHS + 97] </ref>), or mostly through compiler technology 1 (distributed memory [KLS + 94, Tse93, CGC96]).
Reference: [Ost89] <author> A. Osterhaug, </author> <title> editor. Guide to Parallel Programming on Sequent Computer Systems. </title> <publisher> Sequent Technical Publications, </publisher> <address> San Diego, CA, </address> <year> 1989. </year>
Reference-contexts: The advantage of a single name space is that the user does not have to explicitly manage processor local name spaces, local computations, and necessary communication between processors. Shared name space models can be supported in hardware (shared memory <ref> [Ost89, FFG + 95] </ref>), through a combination of hardware and operating system layers (distributed shared memory [LLG + 90, KCDZ94, IDFL96, KHS + 97]), or mostly through compiler technology 1 (distributed memory [KLS + 94, Tse93, CGC96]).
Reference: [Pal97] <author> D. Palermo. </author> <title> Private communication, </title> <month> August </month> <year> 1997. </year>
Reference-contexts: Anderson and Lam, and Palermo and Bannerjee report execution times of their implementations in the order of seconds for programs in their validation program test suites <ref> [And97b, Pal97] </ref>. However it is important to consider the expected execution times of the approaches by Kremer and Kennedy, and Garcia et al. since they use 0-1 integer programming techniques to find 17 optimal solutions.
Reference: [PB95] <author> D.J. Palermo and P. Banerjee. </author> <title> Automatic selection of dynamic partitioning schemes for distributed-memory multicomputers. </title> <editor> In C.-H. Huang et al., editors, </editor> <booktitle> Proceedings of the 8th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Alternatively, Palermo and Banerjee use a different approach to the problem that starts from a static solution for the whole program <ref> [PB95] </ref>. Then, it is recursively decomposed into a hierarchy of candidate phases using a cost function. Taking into account the remapping costs between these phases, the most efficient sequence of phases is selected. A similar divide-and-conquer approach is used by Chatterjee et al. to solve the dynamic alignment problem [CGSS94]. <p> The technique proposed by Palermo and Banerjee for the automatic selection of dynamic data mappings <ref> [PB95] </ref> can be broken down into two main steps. First the program is recursively 10 decomposed into a hierarchy of candidate phases. Then, taking into account the cost of redis-tributing the data between different phases, the most efficient sequence of phases and transitions is selected.
Reference: [PB96] <author> D.J. Palermo and P. Banerjee. </author> <title> Interprocedural array redistribution with data-flow analysis. </title> <booktitle> In Proceedings of the 9th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, CA, </address> <month> August </month> <year> 1996. </year> <month> Sprinver-Verlag. </month>
Reference-contexts: First the program is recursively 10 decomposed into a hierarchy of candidate phases. Then, taking into account the cost of redis-tributing the data between different phases, the most efficient sequence of phases and transitions is selected. The authors extend their proposal for the interprocedural case in <ref> [PB96] </ref>. Initially, the whole program is viewed as a single phase for which a static distribution is determined. The phase decomposition step is based on the Communication Graph. <p> More experiments are needed to validate the different approaches to automatic data and computation mapping. A crucial mile stone in allowing comprehensive experimentation is interprocedural analysis since large real application programs consist of many procedures. The PARADIGM and SUIF projects have already interprocedural prototype implementations <ref> [PB96, And97a] </ref>. An interprocedural implementation based on the Kennedy and Kremer framework is currently under development at Rutgers University. Besides the need for more experimental validation, future challenges include data and computation mappings under resource constraints and mappings for objective functions different from merely minimal execution times.
Reference: [RB95] <author> S. Ramaswamy and P. Banerjee. </author> <title> Automatic generation of efficient array redistribution routines for distributed memory multicomputers. </title> <booktitle> In Frontiers95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Once the program has been decomposed into a hierarchy of phases, redistribution costs are estimated using the model presented in <ref> [RB95] </ref>. With these costs, the Phase Transition Graph is built, where nodes are phases and edges are used to connect transitions between them, taking into account control flow information (loops and backward branches).
Reference: [RS89] <author> J. Ramanujam and P. Sadayappan. </author> <title> A methodology for parallelizing programs for multicom-puters and complex memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Array alignment requirements are induced by the characteristics of the computations in which the arrays are referenced. These requirements can be expressed mathematically through a set of equations or algebraic formulation, or can be represented as a weighted graph problem. Ramanujam and Sadayappan <ref> [RS89] </ref> use matrix notation to represent a communication-free static alignment, and solve the problem with linear algebra. However in real codes, a communication-free alignment can be rarely found. Bau et al. propose different problem formulations to minimize communication costs when communication-free partitioning is not possible [BKK + 94a].
Reference: [TA96] <author> S. Tandri and T.S. Abdelrahman. </author> <title> Automatic data and computation partitioning on scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Nineth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Tandri and Abdelrahman use a variant of the CAG, called the NUMA-CAG, that represents the affinity between parallel loop iterations and array dimensions <ref> [TA96] </ref>. The selection of a good alignment requires a communication cost model. Most authors that have adopted the CAG as their basic infrastructure to solve the alignment problem, have targetted their efforts to accurately weighting the edges in the CAG. <p> For cache-coherent NUMA systems, the estimation of data movement has to consider the physical allocation of data across the memory modules; data elements are moved either in pages or cache lines. Invalidations and false sharing are additional aspects to consider when considering alternatives for data mapping <ref> [AGGL96, TA96] </ref>. For cahe-coherent distributed memory NUMA architectures, the performance model may have to consider data locality across different levels of the memory hierarchy, i.e., local memory and caches. Recently, researchers have started to develop automatic data and 18 computation mapping techniques that consider memory hierarchies of multiple levels.
Reference: [Tse93] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> January </month> <year> 1993. </year> <institution> Rice COMP TR93-199. </institution>
Reference-contexts: Shared name space models can be supported in hardware (shared memory [Ost89, FFG + 95]), through a combination of hardware and operating system layers (distributed shared memory [LLG + 90, KCDZ94, IDFL96, KHS + 97]), or mostly through compiler technology 1 (distributed memory <ref> [KLS + 94, Tse93, CGC96] </ref>). All shared name space implementations have in common that in order to achieve good and scalable performance most computations have to be local, i.e., each processor accesses data already available in its local memory or local cache.
Reference: [WH95] <author> D. Wood and M. Hill. </author> <title> Cost-effective parallel computing. </title> <booktitle> IEEE Computer, </booktitle> <month> Feb </month> <year> 1995. </year>
Reference-contexts: Using a parallel platform instead of a uniprocessor is often the only choice for running computation or resource intensive applications. In fact, some researchers argue that machines with a large main memory should always have multiple processors in order to make cost-effective use of the memory's capacity and bandwidth <ref> [WH95] </ref>. If the current trend can be used as an indication for future developments, parallel platforms will soon become ubiquitous. A significant step towards ease of use of parallel platforms and program portability across platforms has been the development of shared name space programming models with or without explicit parallelism.
Reference: [Who92] <author> S. Wholey. </author> <title> Automatic data mapping for distributed-memory parallel computers. </title> <booktitle> In Proceedings of the 6th ACM International Conference on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Wholey uses a hill climbing approach to find multidimensional distributions <ref> [Who92] </ref>. The proposal starts assigning two processors to a single dimension. Then iteratively doubles the number of processors and decides the dimension where these should have to be assigned.
References-found: 48

