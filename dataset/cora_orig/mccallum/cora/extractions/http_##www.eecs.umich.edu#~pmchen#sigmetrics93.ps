URL: http://www.eecs.umich.edu/~pmchen/sigmetrics93.ps
Refering-URL: http://www.eecs.umich.edu/~pmchen/
Root-URL: http://www.cs.umich.edu
Email: pmchen@cs.Berkeley.EDU, pattrsn@cs.Berkeley.EDU  
Title: on Measurement and Modeling of Computer Systems A New Approach to I/O Performance Evaluation Self-Scaling
Author: Peter M. Chen David A. Patterson 
Address: Berkeley  
Affiliation: Computer Science Division, Dept. of EECS University of California,  
Note: -1- to appear in the 1993 ACM SIGMETRICS Conference  
Abstract: Current I/O benchmarks suffer from several chronic problems: they quickly become obsolete, they do not stress the I/O system, and they do not help in understanding I/O system performance. We propose a new approach to I/O performance analysis. First, we propose a self-scaling benchmark that dynamically adjusts aspects of its workload according to the performance characteristic of the system being measured. By doing so, it automatically scales across current and future systems. The evaluation aids in understanding system performance by reporting how performance varies according to each of five workload parameters. Second, we propose predicted performance, a technique for using the results from the self-scaling evaluation to quickly estimate the performance for workloads that have not been measured. We show that this technique yields reasonably accurate performance estimates and argue that this method gives a far more accurate comparative performance evaluation than traditional single point benchmarks. We apply our new evaluation technique by measuring a SPARCstation 1+ with one SCSI disk, an HP 730 with one SCSI-II disk, a Sprite LFS DECstation 5000/200 with a four-disk disk array, a Convex C240 minisupercomputer with a four disk array, and a Solbourne 5E/905 fileserver with a four disk array. 
Abstract-found: 1
Intro-found: 1
Reference: [Anon85] <editor> Anon and et al., </editor> <title> ``A Measure of Transaction Processing Power'', </title> <journal> Datamation, </journal> <volume> 31, </volume> <month> 7 (April 1, </month> <year> 1985), </year> <pages> 112-118. </pages>
Reference-contexts: Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B <ref> [Anon85, TPCA89, TPCB90] </ref>, Sdet [Gaede81, Gaede82, -4- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh LADDIS 9 MB 1 MB100 MB 163 MB 4.5 MB MB touched) (% time in I/O I/O limited poor good Current State of I/O Benchmarks Bonnie IOStoneSdet (6)Andrew TPCB helps understand system scaling strategy fair comparison generally applicable 63% system, lack a well-defined
Reference: [Baker91] <author> M. G. Baker, J. H. Hartman, M. D. Kupfer, K. W. Shirriff and J. K. Ousterhout, </author> <title> ``Measurements of a Distributed File System'', </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year> <editor> [Bechtolsheim90] A. V. Bechtolsheim and E. H. Frank, </editor> <booktitle> ``Sun's SPARCstation 1: A Workstation for the 1990s'', Procedures of the IEEE Computer Society International Conference (COMPCON), Spring 1990, </booktitle> <pages> 184-188. </pages>
Reference: [Berry89] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum and J. Martin, </author> <title> ``The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers'', </title> <journal> International Journal of Supercomputing Applications, </journal> <month> Fall </month> <year> 1989. </year>
Reference-contexts: When benchmarks are representative of users' applications, they channel vendor optimization and research efforts into improvements that benefit users. Good benchmarks also assist users in purchasing machines by allowing fair, relevant comparisons. Recent efforts to standardize benchmarks, such as SPEC [Scott90] and Perfect Club <ref> [Berry89] </ref>, have increased our understanding of computing performance and helped create a fair playing field on which companies can compete. These standardization efforts have focused on CPU-intensive applications [Scott90], however, and intentionally avoided I/O intensive applications [Berry89]. <p> Recent efforts to standardize benchmarks, such as SPEC [Scott90] and Perfect Club <ref> [Berry89] </ref>, have increased our understanding of computing performance and helped create a fair playing field on which companies can compete. These standardization efforts have focused on CPU-intensive applications [Scott90], however, and intentionally avoided I/O intensive applications [Berry89]. In this paper, we develop criteria for ideal I/O benchmarks and show how current I/O benchmarks fall short of these.
Reference: [Bray90] <author> T. Bray, </author> <title> Bonnie source code, </title> <booktitle> netnews posting, </booktitle> <year> 1990. </year>
Reference-contexts: LADDIS was not available for execution at this time, but a pre-release beta version spends 63% of its execution time doing reads and write; the rest of the time is spent in other NFS opera tions, such as lookup (17%) and getattr (6%). hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh SPEC91b, SPEC91a], Bonnie <ref> [Bray90] </ref>, and IOStone [Park90] 1 . Of these, only Bonnie and IOStone specifically focus on measuring I/O performance.
Reference: [Chen90] <author> P. M. Chen and D. A. Patterson, </author> <title> ``Maximizing Performance in a Striped Disk Array'', </title> <booktitle> Proceedings of the 1990 ACM SIGARCH Conference on Computer Architecture, </booktitle> <address> Seattle WA, </address> <month> May </month> <year> 1990, </year> <pages> 322-331. </pages> <month> -26- </month>
Reference-contexts: For instance, IOStone tries to exercise the memory hierarchy but touches only 1 MB of user data. Perhaps at the time IOStone was written 1 MB was a lot of data but no longer. One recent example of how -3- I/O systems are evolving is disk arrays <ref> [Patterson88, Gibson91, Chen90, Salem86] </ref> Disk arrays allow multiple I/Os to be in progress simultaneously. Most current I/O benchmark do not scale the number of processes issuing I/O, and hence are unable to properly stress disk arrays. <p> System ConvexOS 10.1 (BSD derived) SunOS 4.1A.2 (revised) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c c c c c c c c c [DECstation90] uses a three disk RAID disk array [Patterson88] with a 16 KB striping unit <ref> [Chen90] </ref> and is configured without redundancy. The SPECmark rating is a measure of the processor speed; ratings are relative to the speed of a VAX 11/780.
Reference: [Chen92] <author> P. M. Chen and D. A. Patterson, </author> <title> ``A New Approach to I/O Benchmarks -- Adaptive Evaluation, Predicted Performance'', </title> <institution> UCB/Computer Science Dpt. 92/679, University of California at Berkeley, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: In this section, we set this performance point at 75% of the maximum performance. Using a simple iterative approach, it is possible to find a focal vector for which each workload parameter is simultaneously at its 75% performance point <ref> [Chen92] </ref>. Figure 3 shows results from a benchmark that self-scales all parameters. The system being measured is the one disk SPARCstation of Figure 2.
Reference: [DECstation90] <institution> DECstation 5000 Model 200 Technical Overview, Digital Equipment Corporation, </institution> <year> 1990. </year>
Reference-contexts: Bus Peak Speed 200 MB/s 128 MB/s ?? Memory Size 1024 MB 384 MB Operating System ConvexOS 10.1 (BSD derived) SunOS 4.1A.2 (revised) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c c c c c c c c c <ref> [DECstation90] </ref> uses a three disk RAID disk array [Patterson88] with a 16 KB striping unit [Chen90] and is configured without redundancy. The SPECmark rating is a measure of the processor speed; ratings are relative to the speed of a VAX 11/780.
Reference: [Ferrari84] <author> D. Ferrari, </author> <booktitle> ``On the Foundations of Artificial Workload Design'', ACM SIGMETRICS 1984, </booktitle> <year> 1984, </year> <pages> 8-14. </pages>
Reference-contexts: In this paper, a workload refers to a user-level program with parameter values for each of the above five parameters. This program spawns and controls several processes if necessary. The most important question in developing a synthetic workload is the question of representativeness <ref> [Ferrari84] </ref>.
Reference: [Gaede81] <author> S. </author> <title> Gaede, ``Tools for Research in Computer Workload Characterization'', Experimental Computer Performance and Evaluation, 1981. </title> <editor> D. Ferrari, M. Spadoni, </editor> <publisher> eds.. </publisher>
Reference: [Gaede82] <author> S. </author> <title> Gaede, ``A Scaling Technique for Comparing Interactive System Capacities'', </title> <booktitle> 13th International Conference on Management and Performance Evaluation of Computer Systems, </booktitle> <year> 1982, </year> <pages> 62-67. </pages> <note> CMG 1982. </note>
Reference: [Gibson91] <author> G. A. Gibson, </author> <title> ``Redundant Disk Arrays: Reliable, Parallel Secondary Storage'', </title> <institution> UCB/Computer Science Dpt. 91/613, University of California at Berkeley, </institution> <month> December </month> <year> 1991. </year> <note> also available from MIT Press, </note> <year> 1992. </year>
Reference-contexts: For instance, IOStone tries to exercise the memory hierarchy but touches only 1 MB of user data. Perhaps at the time IOStone was written 1 MB was a lot of data but no longer. One recent example of how -3- I/O systems are evolving is disk arrays <ref> [Patterson88, Gibson91, Chen90, Salem86] </ref> Disk arrays allow multiple I/Os to be in progress simultaneously. Most current I/O benchmark do not scale the number of processes issuing I/O, and hence are unable to properly stress disk arrays.
Reference: [HP730] <institution> HP Apollo Series 700 Model 730 PA-RISC Workstation, Hewlett-Packard, </institution> <year> 1992. </year>
Reference-contexts: The SPECmark rating is a measure of the processor speed; ratings are relative to the speed of a VAX 11/780. The full name of the HP 730 is the HP Series 700 Model 730 <ref> [HP730] </ref>. -7- applied to a much wider range of applications than today's benchmarks. Of course, the accuracy of the prediction determines how effectively prediction can be used to compare systems. We explore the method and accuracy of prediction in Section 9. 5.
Reference: [Horning91] <author> R. Horning, L. Johnson, L. Thayer, D. Li, V. Meier, C. Dowdell and D. Roberts, </author> <title> ``System Design for a Low Cost PA-RISC Desktop Workstation'', </title> <booktitle> Procedures of the IEEE Computer Society International Conference (COMPCON), Spring 1991, </booktitle> <pages> 208-213. </pages>
Reference-contexts: This high performance is due to the fast memory system of the HP 730 (peak memory bandwidth is 264 MB/s) and to the use of a VLSI memory controller to accelerate cache-memory write backs <ref> [Horning91] </ref>. shown in. The curves are similar to the SPARCstation 1+, with three main differences: g Absolute performance is very high. File cache performance reaches 25 MB/s (Figure 15b); disk performance reaches almost 10 MB/s (Figure 15f).
Reference: [Howard88] <author> J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham and M. J. West, </author> <title> ``Scale and Performance in a Distributed File System'', </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988), </year> <pages> 51-81. </pages>
Reference-contexts: Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew <ref> [Howard88] </ref>, TPC-B [Anon85, TPCA89, TPCB90], Sdet [Gaede81, Gaede82, -4- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh LADDIS 9 MB 1 MB100 MB 163 MB 4.5 MB MB touched) (% time in I/O I/O limited poor good Current State of I/O Benchmarks Bonnie IOStoneSdet (6)Andrew TPCB helps understand system scaling strategy fair comparison generally applicable 63% system, lack
Reference: [Nielsen91] <author> M. J. K. Nielsen, </author> <title> ``DECstation 5000 Model 200'', </title> <booktitle> Procedures of the IEEE Computer Society International Conference (COMPCON), Spring 1991, </booktitle> <pages> 220-225. </pages>
Reference: [Ousterhout88] <author> J. K. Ousterhout, A. Cherenson, F. Douglis and M. Nelson, </author> <title> ``The Sprite Network Operating System'', </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 23-36. </pages>
Reference-contexts: Each simultaneously running script uses 3.5 MB. -5- improvement. We show a qualitative evaluation of today's I/O benchmarks in Figure 1 and make the fol lowing observations: g Many I/O benchmarks are not I/O limited. On a DECstation 5000/200 running the Sprite Operating System <ref> [Ousterhout88] </ref>, Andrew, Sdet 2 , and IOStone spend 25% or less of their time doing I/O. Further, many of the benchmarks touch very little data. IOStone touches only 1 MB of user data; Andrew touches only 4.5 MB. g Today's I/O benchmarks do not help in understanding system performance.
Reference: [Ousterhout89] <author> J. K. Ousterhout and F. Douglis, </author> <title> ``Beating the I/O Bottleneck: A Case for Log-Structured File Systems'', </title> <type> SIGOPS 23, </type> <month> 1 (January </month> <year> 1989), </year> <pages> 11-28. </pages>
Reference: [Park90] <author> A. Park and J. C. Becker, ``IOStone: </author> <title> A synthetic file system benchmark'', Computer Architecture News 18, </title> <month> 2 (June </month> <year> 1990), </year> <pages> 45-52. </pages> <month> -27- </month>
Reference-contexts: LADDIS was not available for execution at this time, but a pre-release beta version spends 63% of its execution time doing reads and write; the rest of the time is spent in other NFS opera tions, such as lookup (17%) and getattr (6%). hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh SPEC91b, SPEC91a], Bonnie [Bray90], and IOStone <ref> [Park90] </ref> 1 . Of these, only Bonnie and IOStone specifically focus on measuring I/O performance.
Reference: [Patterson88] <author> D. A. Patterson, G. Gibson and R. H. Katz, </author> <title> ``A Case for Redundant Arrays of Inexpensive Disks (RAID)'', </title> <booktitle> International Conference on Management of Data (SIGMOD), </booktitle> <month> June </month> <year> 1988, </year> <pages> 109-116. </pages>
Reference-contexts: 1. Introduction As processors continue to improve their performance faster than I/O devices <ref> [Patterson88] </ref>, I/O will increasingly become the system bottleneck. There is therefore an increased need to understand and compare the performance of I/O systems, hence the need for I/O-intensive benchmarks. The benefits of good benchmarks are well understood. <p> For instance, IOStone tries to exercise the memory hierarchy but touches only 1 MB of user data. Perhaps at the time IOStone was written 1 MB was a lot of data but no longer. One recent example of how -3- I/O systems are evolving is disk arrays <ref> [Patterson88, Gibson91, Chen90, Salem86] </ref> Disk arrays allow multiple I/Os to be in progress simultaneously. Most current I/O benchmark do not scale the number of processes issuing I/O, and hence are unable to properly stress disk arrays. <p> Memory Size 1024 MB 384 MB Operating System ConvexOS 10.1 (BSD derived) SunOS 4.1A.2 (revised) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c c c c c c c c c [DECstation90] uses a three disk RAID disk array <ref> [Patterson88] </ref> with a 16 KB striping unit [Chen90] and is configured without redundancy. The SPECmark rating is a measure of the processor speed; ratings are relative to the speed of a VAX 11/780.
Reference: [Rosenblum91] <author> M. Rosenblum and J. K. Ousterhout, </author> <title> ``The Design and Implementation of a Log-Structured File System'', </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference: [Rosenblum92] <author> M. Rosenblum, </author> <title> Sprite LFS Write Cache Size, </title> <type> personal communication, </type> <month> July 6, </month> <year> 1992. </year>
Reference-contexts: The write cache of LFS is smaller because LFS limits the number of dirty cache blocks to avoid deadlock during cleaning. The effective file cache size for writes is only 5-8 MB, while for reads it is 20 MB <ref> [Rosenblum92] </ref>. 4 In contrast, when uniqueBytes is large enough to exercise the disk for both reads and writes, writes are faster than reads (Figure 8b). This phenomenon is due to Sprite's LFS, which improves write performance by grouping multiple small writes into fewer large writes. 8.1.3.
Reference: [SPEC91a] <editor> SPEC SDM Release 1.0 Manual, </editor> <title> System Performance Evaluation Cooperative, </title> <year> 1991. </year>
Reference-contexts: First TrySelf-Scaling All Workload Parameters A self-scaling benchmark is one that adjusts the workloads that it runs and reports based on the capabilities of the system being measured. Sdet and TPC-B both do this for one aspect of the workload, that is, load (processNum) <ref> [SPEC91a, TPCB90] </ref>. Sdet reports the maximum throughput, which occurs at different loads for different systems. TPC-B reports maximum throughput subject to a response time constraint; this also occurs at different loads for different systems.
Reference: [SPEC91b] <institution> SPEC SDM Release 1.0 Technical Fact Sheet, Franson and Haggerty Associates, </institution> <note> 1991. </note> <author> [Saavedra-Barrera89] R. H. Saavedra-Barrera, A. J. Smith and E. Miya, </author> <title> ``Machine Characterization Based on an Abstract High-Level Language Machine'', </title> <journal> IEEE Transactions on Computers 38, </journal> <month> 12 (December </month> <year> 1989), </year> <pages> 1659-1679. </pages>
Reference: [Salem86] <author> K. Salem and H. Garcia-Molina, </author> <title> ``Disk Striping'', </title> <booktitle> Proceedings of the Second International Conference on Data Engineering, </booktitle> <year> 1986, </year> <pages> 336-342. </pages>
Reference-contexts: For instance, IOStone tries to exercise the memory hierarchy but touches only 1 MB of user data. Perhaps at the time IOStone was written 1 MB was a lot of data but no longer. One recent example of how -3- I/O systems are evolving is disk arrays <ref> [Patterson88, Gibson91, Chen90, Salem86] </ref> Disk arrays allow multiple I/Os to be in progress simultaneously. Most current I/O benchmark do not scale the number of processes issuing I/O, and hence are unable to properly stress disk arrays.
Reference: [Scott90] <author> V. Scott, </author> <title> ``Is Standardization of Benchmarks Feasible?'', </title> <booktitle> Proceedings of the BUSCON Conference, </booktitle> <address> Long Beach, CA, </address> <month> Feb 14, </month> <year> 1990, </year> <pages> 139-147. </pages>
Reference-contexts: The benefits of good benchmarks are well understood. When benchmarks are representative of users' applications, they channel vendor optimization and research efforts into improvements that benefit users. Good benchmarks also assist users in purchasing machines by allowing fair, relevant comparisons. Recent efforts to standardize benchmarks, such as SPEC <ref> [Scott90] </ref> and Perfect Club [Berry89], have increased our understanding of computing performance and helped create a fair playing field on which companies can compete. These standardization efforts have focused on CPU-intensive applications [Scott90], however, and intentionally avoided I/O intensive applications [Berry89]. <p> Recent efforts to standardize benchmarks, such as SPEC <ref> [Scott90] </ref> and Perfect Club [Berry89], have increased our understanding of computing performance and helped create a fair playing field on which companies can compete. These standardization efforts have focused on CPU-intensive applications [Scott90], however, and intentionally avoided I/O intensive applications [Berry89]. In this paper, we develop criteria for ideal I/O benchmarks and show how current I/O benchmarks fall short of these. <p> Results should be reproducible; optimizations that are allowed and disallowed must be explicitly stated; the machine environment on which the benchmarking takes place must be well-defined and reported, and so on. In this paper, we leave this aspect of benchmarking standardization organizations such as SPEC <ref> [Scott90] </ref> and the Transaction Processing Performance Council [TPCA89, TPCB90].
Reference: [TPCA89] <editor> TPC Benchmark A Standard Specification, </editor> <booktitle> Transaction Processing Performance Council, </booktitle> <month> November 10, </month> <year> 1989. </year>
Reference-contexts: In this paper, we leave this aspect of benchmarking standardization organizations such as SPEC [Scott90] and the Transaction Processing Performance Council <ref> [TPCA89, TPCB90] </ref>. <p> Current I/O Benchmarks In this section, we examine current benchmarks used to evaluate I/O systems. The benchmarks we consider are Andrew [Howard88], TPC-B <ref> [Anon85, TPCA89, TPCB90] </ref>, Sdet [Gaede81, Gaede82, -4- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh LADDIS 9 MB 1 MB100 MB 163 MB 4.5 MB MB touched) (% time in I/O I/O limited poor good Current State of I/O Benchmarks Bonnie IOStoneSdet (6)Andrew TPCB helps understand system scaling strategy fair comparison generally applicable 63% system, lack a well-defined

References-found: 26

