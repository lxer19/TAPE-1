URL: http://www.cs.duke.edu/~jsv/Papers/HoV95.pdcfull.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node32.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Parallel Lossless Image Compression Using Huffman and Arithmetic Coding  
Author: Paul G. Howard and Jeffrey Scott Vitter 
Abstract: A shorter version of this paper appears in Proceedings of the IEEE Data Compression Conference, Snowbird, Utah, March 23-26, 1992, 299-308. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Chevion, E. D. Karnin & E. Walach, </author> <title> "High Efficiency, Multiplication Free Approximation of Arithmetic Coding," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & J. H. Reif, eds., </editor> <address> Snowbird, Utah, </address> <month> Apr. </month> <pages> 8-11, </pages> <year> 1991, </year> <pages> 43-52. </pages>
Reference-contexts: Recent research has focused on approximations to the arithmetic that reduce the time required without sacrificing much coding efficiency. Work by Rissanen and Mohiuddin [18], Chevion et al. <ref> [1] </ref>, Feygin et al. [3], and Printz and Stubley [16] has involved approximate multiplication; Neal [15] uses approximate division. In [9,10,11] we present complete details of an 4 alternative practical approach, called quasi-arithmetic coding, in which we precompute all multipli-cations and divisions and store the results in lookup tables.
Reference: [2] <author> S. De Agostino & J. A. Storer, </author> <title> "Parallel Algorithms for Optimal Compression using Dictionaries with the Prefix Property," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <pages> 24-26, </pages> <year> 1992, </year> <pages> 52-61. </pages>
Reference-contexts: Parallel decoding is difficult, since the decoding processors cannot easily determine the lengths or the starting locations of the output codewords, although in fact De Agostino and Storer <ref> [2] </ref> show that this can be done by assigning processors to encoded bits. 2.2 Bit-transpose coding We can achieve decodability by rearranging (transposing) the output bits.
Reference: [3] <author> G. Feygin, P. G. Gulak & P. Chow, </author> <title> "Minimizing Error and VLSI Complexity in the Multiplication Free Approximation of Arithmetic Coding," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <journal> 30-Apr. </journal> <volume> 1, </volume> <year> 1993, </year> <pages> 118-127. 8 </pages>
Reference-contexts: Recent research has focused on approximations to the arithmetic that reduce the time required without sacrificing much coding efficiency. Work by Rissanen and Mohiuddin [18], Chevion et al. [1], Feygin et al. <ref> [3] </ref>, and Printz and Stubley [16] has involved approximate multiplication; Neal [15] uses approximate division. In [9,10,11] we present complete details of an 4 alternative practical approach, called quasi-arithmetic coding, in which we precompute all multipli-cations and divisions and store the results in lookup tables.
Reference: [4] <author> R. G. Gallager & D. C. Van Voorhis, </author> <title> "Optimal Source Codes for Geometrically Distributed Integer Alphabets," </title> <journal> IEEE Trans. Inform. Theory IT-21 (Mar. </journal> <year> 1975), </year> <pages> 228-230. </pages>
Reference-contexts: It can be shown that the correct choice of the parameter m produces an optimal prefix code for a given exponential distribution <ref> [4] </ref>, but Golomb codes are useful for other decreasing distributions as well. Rice [17] independently discovered the special case of Golomb codes where m = 2 k for some integer k.
Reference: [5] <author> M. R. Garey, </author> <title> "Optimal Binary Search Trees with Restricted Maximum Depth," </title> <journal> SIAM J. </journal> <volume> Com-put. </volume> <month> 3 (June </month> <year> 1974), </year> <pages> 101-110. </pages>
Reference: [6] <author> S. W. Golomb, </author> <title> "Run-Length Encodings," </title> <journal> IEEE Trans. Inform. Theory IT-12 (July 1966), </journal> <pages> 399-401. </pages>
Reference-contexts: Golomb codes <ref> [6] </ref> and Rice codes [17] can be used to encode data from distributions in which the probabilities are arranged in approximately decreasing order. This condition usually holds for the prediction errors encountered in lossless image compression. Golomb codes are parameterized by a positive integer parameter m.
Reference: [7] <author> R. W. </author> <title> Hamming, </title> <booktitle> Coding and Information Theory , Prentice-Hall, </booktitle> <address> Englewood Cliffs, N.J., </address> <year> 1980. </year>
Reference-contexts: Huffman codes are instantaneous; that is, the receiver knows immediately when a complete symbol has been received, and does not have to look further to correctly identify the symbol. Instantaneous codes are just those with the prefix property : no code word is a prefix of another code word <ref> [7, pages 53-55] </ref>. Parallel coding is simplified since the code bits for all pixels are disjoint and independent. 2.1 Codeword-length coding One simple approach involves assigning one pixel to each processor and noting that each encoding processor can easily compute the code length of its pixel.
Reference: [8] <author> P. G. Howard & J. S. Vitter, </author> <title> "New Methods for Lossless Image Compression Using Arithmetic Coding," </title> <booktitle> Information Processing and Management 28 (1992), </booktitle> <pages> 765-779. </pages>
Reference-contexts: We present general-purpose algorithms for encoding and decoding using both Huffman and arithmetic coding, and apply them to the problem of lossless compression of high-resolution grayscale images. Our system for lossless image compression has four components <ref> [8] </ref>: pixel sequence, prediction, error modeling, and coding. In [8] we introduced MLP, a multi-level progressive method for lossless image compression. <p> We present general-purpose algorithms for encoding and decoding using both Huffman and arithmetic coding, and apply them to the problem of lossless compression of high-resolution grayscale images. Our system for lossless image compression has four components <ref> [8] </ref>: pixel sequence, prediction, error modeling, and coding. In [8] we introduced MLP, a multi-level progressive method for lossless image compression. The pixel sequencer of MLP divides an m fi m image into 2 log 2 m levels, the pixels in each level forming a checkerboard pattern; each level contains twice as many pixels as the preceding one. <p> fall back to bit-transpose coding, which may require L late phases, so the total number of phases needed is at most L log 2 (2n=p). 2.5 Parallel Huffman coding in practice We have simulated parallel Huffman compression for a set of 14 Landsat Thematic Mapper images; these images, described in <ref> [8] </ref>, are 512 fi 512 8-bit grayscale images. We simulate only the last level of coding (n = 131;072 pixels) for each image, using p = 4;096 processors. For our test images, the number of early phases is at most 7, the average being 5.6.
Reference: [9] <author> P. G. Howard & J. S. Vitter, </author> <title> "Practical Implementations of Arithmetic Coding," in Image and Text Compression, </title> <editor> J. A. Storer, ed., </editor> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Massachusetts, </address> <year> 1992, </year> <pages> 85-112. </pages>
Reference: [10] <author> P. G. Howard & J. S. Vitter, </author> <title> "Design and Analysis of Fast Text Compression Based on Quasi-Arithmetic Coding," </title> <booktitle> Information Processing and Management 30 (1994), </booktitle> <pages> 777-794, </pages> <note> also appears in shorter form in the proceedings of the Data Compression Conference, </note> <editor> J. A. Storer and M. Cohn, eds., </editor> <address> Snowbird, Utah, March 30-April 1, </address> <year> 1993, </year> <month> 98-107.. </month>
Reference: [11] <author> P. G. Howard & J. S. Vitter, </author> <title> "Arithmetic Coding for Data Compression," </title> <booktitle> Proc. IEEE 82 (June 1994), </booktitle> <pages> 857-865. </pages>
Reference: [12] <author> P. G. Howard & J. S. Vitter, </author> <title> "Error Modeling for Hierarchical Lossless Image Compression," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <pages> 24-26, </pages> <year> 1992, </year> <pages> 269-278. </pages>
Reference-contexts: This happened to pixel 10 in the example of Figure 1. 4 Compression loss can be derived from the definition of compression gain in the revised version of <ref> [12] </ref> to be 100 log e (actual average code length=entropy); it is expressed in units of percent log ratio, denoted by the ffi ffi symbol. <p> In <ref> [12] </ref> we give an implicit method for estimating local image variances that leads to better compression than any other published lossless image compression method.
Reference: [13] <author> A. K. Huber, </author> <title> "A Hybrid Algorithm for Compression of Infrared Images of Space," </title> <institution> Utah State University, M.S. </institution> <type> Thesis, </type> <year> 1993. </year>
Reference: [14] <author> D. A. Huffman, </author> <title> "A Method for the Construction of Minimum Redundancy Codes," </title> <booktitle> Proceedings of the Institute of Radio Engineers 40 (1952), </booktitle> <pages> 1098-1101. </pages>
Reference-contexts: For simplicity we do not consider input data routing, and we defer the issues of parallel prediction and error modeling to Section 4, where they are discussed briefly. 2 Parallel Huffman Coding We now develop the basic parallel coding algorithms using Huffman coding <ref> [14] </ref>. In practice Huff-man coding is often the best choice for statistical coding, since the resulting codes are usually close to optimal and they can be very fast if implemented by lookup tables.
Reference: [15] <author> R. M. Neal, </author> <title> "Fast Arithmetic Coding Using Low-Precision Division," </title> <type> Unpublished manuscript, </type> <year> 1987. </year>
Reference-contexts: Recent research has focused on approximations to the arithmetic that reduce the time required without sacrificing much coding efficiency. Work by Rissanen and Mohiuddin [18], Chevion et al. [1], Feygin et al. [3], and Printz and Stubley [16] has involved approximate multiplication; Neal <ref> [15] </ref> uses approximate division. In [9,10,11] we present complete details of an 4 alternative practical approach, called quasi-arithmetic coding, in which we precompute all multipli-cations and divisions and store the results in lookup tables. We review quasi-arithmetic coding in Section 3.1.
Reference: [16] <author> H. Printz & P. Stubley, </author> <title> "Multialphabet Arithmetic Coding at 16 MBytes/sec," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <journal> 30-Apr. </journal> <volume> 1, </volume> <year> 1993, </year> <pages> 128-137. </pages>
Reference-contexts: Recent research has focused on approximations to the arithmetic that reduce the time required without sacrificing much coding efficiency. Work by Rissanen and Mohiuddin [18], Chevion et al. [1], Feygin et al. [3], and Printz and Stubley <ref> [16] </ref> has involved approximate multiplication; Neal [15] uses approximate division. In [9,10,11] we present complete details of an 4 alternative practical approach, called quasi-arithmetic coding, in which we precompute all multipli-cations and divisions and store the results in lookup tables. We review quasi-arithmetic coding in Section 3.1.
Reference: [17] <author> R. F. Rice, </author> <title> "Some Practical Universal Noiseless Coding Techniques," </title> <institution> Jet Propulsion Laboratory, JPL Publication 79-22, Pasadena, California, </institution> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: Golomb codes [6] and Rice codes <ref> [17] </ref> can be used to encode data from distributions in which the probabilities are arranged in approximately decreasing order. This condition usually holds for the prediction errors encountered in lossless image compression. Golomb codes are parameterized by a positive integer parameter m. <p> It can be shown that the correct choice of the parameter m produces an optimal prefix code for a given exponential distribution [4], but Golomb codes are useful for other decreasing distributions as well. Rice <ref> [17] </ref> independently discovered the special case of Golomb codes where m = 2 k for some integer k. Restricting m to be a power of 2 leads to codes that are easy to implement in hardware or software.
Reference: [18] <author> J. J. Rissanen & K. M. Mohiuddin, </author> <title> "A Multiplication-Free Multialphabet Arithmetic Code," </title> <journal> IEEE Trans. Comm. </journal> <month> 37 (Feb. </month> <year> 1989), </year> <pages> 93-98. </pages>
Reference-contexts: Practical implementations of arithmetic coding use fixed precision arithmetic [9,11,20], but they still run slowly because of the multiplications (and sometimes divisions) required. Recent research has focused on approximations to the arithmetic that reduce the time required without sacrificing much coding efficiency. Work by Rissanen and Mohiuddin <ref> [18] </ref>, Chevion et al. [1], Feygin et al. [3], and Printz and Stubley [16] has involved approximate multiplication; Neal [15] uses approximate division.
Reference: [19] <author> D. C. Van Voorhis, </author> <title> "Constructing Codes with Bounded Codeword Lengths," </title> <journal> IEEE Trans. Inform. Theory IT-20 (Mar. </journal> <year> 1974), </year> <pages> 288-290. </pages>
Reference: [20] <author> I. H. Witten, R. M. Neal & J. G. Cleary, </author> <title> "Arithmetic Coding for Data Compression," </title> <journal> Comm. </journal> <note> ACM 30 (June 1987), 520-540. 9 </note>
Reference-contexts: current interval lies entirely in the left or right half of the full interval, we output 0 or 1 respectively, discard the unused half of the full interval, and expand the remaining half so that it fills the full interval. (We also use a trick, the bits-to-follow mechanism explained in <ref> [20] </ref>, that enables us to expand the interval when we know that the current interval is entirely within the middle half of the full interval.) The intermediate intervals computed by the coder can be thought of as states, each determined by the endpoints of the interval. <p> The only arithmetic involved is in precomputing the tables; the arithmetic is done according to the Witten-Neal-Cleary algorithm <ref> [20] </ref>. A one-state coder corresponds to Huffman coding. As we increase the number of states, we increase the precision of the coder and hence its compression efficiency. Using just a few states often provides efficiency considerably greater than that of Huffman coding.
References-found: 20

