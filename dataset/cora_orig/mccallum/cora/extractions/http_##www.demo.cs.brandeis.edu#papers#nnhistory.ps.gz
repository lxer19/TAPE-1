URL: http://www.demo.cs.brandeis.edu/papers/nnhistory.ps.gz
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Title: Connectionism: Past, Present, and Future  
Author: Jordan B. Pollack 
Affiliation: Computer Information Science Department The Ohio State University  
Abstract-found: 0
Intro-found: 1
Reference: <author> Ackley, D. H., Hinton, G. E. & Sejnowski, T. J. </author> <year> (1985). </year> <title> A learning algorithm for Boltzmann Machines. </title> <journal> Cognitive Science, </journal> <volume> 9, </volume> <pages> 147-169. </pages>
Reference-contexts: The number of units needed for his network to parse sentences of length n rises as O (n 3 ). Selman (1985) also reports an automatic construction for networks which can parse bounded-length context-free grammars. His system is stochastic, and based on the Boltzmann Machine notions of <ref> (Ackley et al., 1985) </ref>. Again we have a machine for sentences of bounded length. Another feature of Selman's system is that the connectionist constraint of limited processing cycles is ignored and a parse may take several thousand cycles of annealing.
Reference: <author> Allen, R. </author> <year> (1987). </year> <title> Several Studies on Natural Language and Back Propagation. </title> <booktitle> In Institute of Electrical and Electronics Engineers First International Conference on Neural Networks. </booktitle> <address> San Diego, II-335-342. </address>
Reference-contexts: One of the major foci of current connectionism is the application of back-propagation to diverse areas such as sonar (Gorman & Sejnowski, 1988), speech (Elman & Stork, 1987), machine translation <ref> (Allen, 1987) </ref>, and the invention and investigation of numerous tweaks and twiddles to the algorithm (Cater, 1987; Dahl, 1987; Stornetta & Huberman, 1987).
Reference: <author> Anderson, J. A., Silverstein, J. W., Ritz, S. A. & Jones, R. S. </author> <year> (1977). </year> <title> Distinctive Features, Categorical Perception, and Probability Learning: Some Applications of a Neural Model. </title> <journal> Psychological Review, </journal> <volume> 84, </volume> <pages> 413-451. </pages>
Reference-contexts: Though beyond the scope of this history, significant developments and analyses can be found in the works of Teuvo Kohonen (Kohonen, 1977; Kohonen et al., 1981) and David Willshaw (Willshaw, 1981). <ref> (Anderson et al., 1977) </ref> described experiments with a saturating linear model for pattern association and learning called the ``Brain-State in a Box'' or BSB model.
Reference: <author> Ashby, W. R. </author> <year> (1960). </year> <title> Design for a Brain: The origin of adaptive behaviour (Second Edition). </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Barto, A. G., Anderson, C. W. & Sutton, R. S. </author> <year> (1982). </year> <title> Synthesis of Nonlinear Control Surfaces by a layered Associative Search Network. </title> <journal> Biological Cybernetics, </journal> <volume> 43, </volume> <pages> 175-185. </pages>
Reference-contexts: These are (1) Associative Reward-Penalty, (2) Boltzmann Machine Learning, and (3) Back Propagation. 4.4.1. Associative Reward-Penalty Working with the goal-seeking units of (Klopf, 1982), Andrew Barto and colleagues published results in 1982 on one of the first perceptron-like networks to break the linear learning barrier <ref> (Barto et al., 1982) </ref>. Using a two-layered feed-forward network they demonstrated a system which learned to navigate towards either of 2 locational goals in a small landscape. They showed that in order to have done this successfully, the system had to essentially learn exclusive-or, a nonlinear function.
Reference: <author> Barto, A. G. </author> <year> (1985). </year> <title> Learning by statistical cooperation of self-interested neuron-like computing elements. </title> <journal> Human Neurobiology, </journal> <volume> 4, </volume> <pages> 229-256. </pages>
Reference-contexts: This early work, on a specific network with a few quirks, was subsequently developed into into a more general model of learning, the Associative Reward-Penalty or A R -P algorithm. See <ref> (Barto, 1985) </ref> for an overview of the work. 4.4.2. Boltzmann Machines Anneal To toughen anything, made brittle from the action of fire, by exposure to continuous and slowly diminished heat, or by other equivalent process.
Reference: <author> Cater, J. P. </author> <year> (1987). </year> <title> Successfully using peak learning rates of 10 (and greater) in back-propagation networks with the heuristic learning algorithm. </title> <booktitle> In Institute of Electrical and Electronics Engineers First International Conference on Neural Networks. </booktitle> <address> San Diego, II-645-652. </address>
Reference: <author> Cottrell, G. W. </author> <year> (1985). </year> <title> Connectionist Parsing. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society. </booktitle> <address> Irvine, CA. </address>
Reference: <author> Cottrell, G. W. </author> <year> (1985). </year> <title> A Connectionist Approach to Word-Sense Disambiguation. </title> <institution> TR154, Rochester: University of Rochester, Computer Science Department. </institution>
Reference: <author> Crutchfield, J. P., Farmer, J. D., Packard, N, H. & Shaw, R. S. </author> <year> (1986). </year> <title> Chaos. </title> <journal> Scientific American, </journal> <volume> 255, </volume> <pages> 46-57. </pages>
Reference: <author> Cun, Y. </author> <title> Le (1985). A Learning Scheme for Asymmetric Threshold Networks. </title> <booktitle> In Proceedings of Cognitiva 85. Paris, </booktitle> <pages> 599-604. </pages>
Reference: <author> Dahl, E. D. </author> <year> (1987). </year> <title> Accelerated learning using the generalized delta rule. </title> <booktitle> In Institute of Electrical and Electronics Engineers First International Conference on Neural Networks. </booktitle> <address> San Diego, II-523-530. </address>
Reference: <author> Dreyfus, H. L. & Dreyfus, S. E. </author> <year> (1988). </year> <title> Making a Mind versus Modeling the Brain: </title> <booktitle> Artificial Intelligence Again at the Crossroads. Daedalus, </booktitle> <pages> 117. </pages>
Reference-contexts: Rather than being brand-new, it is actually the rebirth of a research program which thrived from the 40's through the 60's and then was severely retrenched in the 70's. Connectionism is often posed as a paradigmatic competitor to the Symbolic Processing tradition of Artificial Intelligence <ref> (Dreyfus & Dreyfus, 1988) </ref>, and, indeed, the counterpoint in the timing of their intellectual and commercial fortunes may lead one to believe that research in cognition is merely a zero-sum game. <p> Discussions of this controversy can be found in (Rumelhart & Zipser, 1986) or <ref> (Dreyfus & Dreyfus, 1988) </ref>, and some interesting perspectives on some of the personalities involved can be found in chapter 4 of McCorduck (1979). 3.1. Minsky & Papert I was trying to concentrate on a certain problem but was getting bored and sleepy.
Reference: <author> Dyer, M. G. </author> <year> (1983). </year> <title> In Depth Understanding. </title> <publisher> Cambridge: MIT Press. </publisher>
Reference-contexts: One problem is how to get infinite generative capacity into a system with finite resources (i.e., the competence/performance distinction). Another is the question of reconstructive memory, which has only been crudely approximated by AI systems <ref> (Dyer, 1983) </ref>. Yet another is the symbol-grounding problem, which is how to get a symbolic system to touch ground in real-world perception and action, when all systems seem to bottom out at an a priori set of semantic primitives.
Reference: <author> Dyer, M. G., Flowers, M. & Wang, Y. A. </author> <year> (1988). </year> <title> Weight Matrix = Pattern of Activation: Encoding Semantic Networks as Distributed Representations in DUAL, a PDP architecture. </title> <institution> UCLA-Artificial Intelligence-88-5, Los Angeles: Artificial Intelligence Laboratory, UCLA. </institution>
Reference-contexts: There is also research in progress along the lines of Hinton's (unpublished) proposal for reduced descriptions as a way out of the superposition/concatenation difficulty for distributed representations. For example (Pollack, 1988) demonstrates a reconstructive distributed memory for variable sized trees, and <ref> (Dyer et al., 1988) </ref> show a network construction for representing simple semantic networks as labelled directed graphs. - 11 - As problems in capacity, representation, and control get solved, we may expect a new blooming of connectionist applications in areas currently dominated by traditional symbolic processing.
Reference: <author> Elman, J. & Stork, D. </author> <year> (1987). </year> <title> Session on Speech Recognition and Synthesis. </title> <booktitle> In Institute of Electrical and Electronics Engineers First International Conference on Neural Networks. </booktitle> <address> San Diego, IV-381-504. </address>
Reference-contexts: In fact, back-propagation is so widely being used today, that it is threatening to become a subfield of its own. One of the major foci of current connectionism is the application of back-propagation to diverse areas such as sonar (Gorman & Sejnowski, 1988), speech <ref> (Elman & Stork, 1987) </ref>, machine translation (Allen, 1987), and the invention and investigation of numerous tweaks and twiddles to the algorithm (Cater, 1987; Dahl, 1987; Stornetta & Huberman, 1987).
Reference: <author> Elman, J. L. </author> <year> (1988). </year> <title> Finding Structure in Time. </title> <type> Report 8801, </type> <address> San Diego: </address> <note> Center for Research in Language, UCSD. </note>
Reference: <author> Fanty, M. </author> <year> (1985). </year> <title> Context-free parsing in Connectionist Networks. </title> <institution> TR174, Rochester, N.Y.: University of Rochester, Computer Science Department. </institution>
Reference: <author> Fodor, J. & Pylyshyn, A. </author> <year> (1988). </year> <title> Connectionism and Cognitive Architecture: A Critical Analysis. </title> <journal> Cognition, </journal> <volume> 28, </volume> <pages> 3-71. </pages>
Reference-contexts: One must be careful that a model being proposed can actually represent the elements of the domain being modeled. One of the major attacks on connectionism has been on the inadequacy of its representations, especially on their lack of compositionality <ref> (Fodor & Pylyshyn, 1988) </ref>. In feature-based distributed representations, such as the one used by (Kawamoto, 1985), if the entire feature system is needed to represent a single element, then attempting to represent a structure involving those elements cannot be managed in the same system.
Reference: <author> Gasser, M. & Dyer, M.G. </author> <year> (1988). </year> <title> Sequencing in a Connectionist Model of Language Processing. </title> <booktitle> In Proceedings of the 12th International Conference on Computational Linguistics. </booktitle> <address> Budapest. </address> - <note> 13 - Gleick, J. </note> <year> (1987). </year> <title> Chaos: Making a new science. </title> <address> New York: </address> <publisher> Viking. </publisher>
Reference: <author> Gorman, R. P. & Sejnowski, T. J. </author> <year> (1988). </year> <title> Analysis of hidden units in a layered nework trained to classify sonar targets. </title> <booktitle> Neural Networks, </booktitle> <volume> 1, </volume> <pages> 75-90. </pages>
Reference-contexts: In fact, back-propagation is so widely being used today, that it is threatening to become a subfield of its own. One of the major foci of current connectionism is the application of back-propagation to diverse areas such as sonar <ref> (Gorman & Sejnowski, 1988) </ref>, speech (Elman & Stork, 1987), machine translation (Allen, 1987), and the invention and investigation of numerous tweaks and twiddles to the algorithm (Cater, 1987; Dahl, 1987; Stornetta & Huberman, 1987).
Reference: <author> Grebogi, C., Ott, E. & Yorke, J. A. </author> <year> (1987). </year> <title> Chaos, Strange Attractors, and Fractal Basin Boundaries in Nonlinear Dynamics. </title> <journal> Science, </journal> <volume> 238, </volume> <pages> 632-638. </pages>
Reference: <author> Grossberg, S. </author> <year> (1987). </year> <title> Competitive Learning: From Interactive Activation to Adaptive Resonance. </title> <journal> Cognitive Science, </journal> <volume> 11, </volume> <pages> 23-63. </pages>
Reference: <author> Hanson, S. J. & Kegl, J. </author> <year> (1987). </year> <title> PARSNIP: A connectionist network that learns natural language grammar from exposure to natural language sentences. </title> <booktitle> In Proceedings of the Ninth Conference of the Cognitive Science Society. Seattle, </booktitle> <pages> 106-119. </pages>
Reference: <author> Hebb, D. O. </author> <year> (1949). </year> <title> The Organization of Behavior: A Neuropsychological Theory. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Hinton, G. E. </author> <year> (1986). </year> <title> Learning Distributed Representations of Concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society. </booktitle> <address> Amherst, MA, </address> <pages> 1-12. </pages>
Reference-contexts: It can be seen that for the analog case, small changes in the input (by slowly changing weights) cause correspondingly small changes in the output. Back-propagation has been used quite successfully. (Sejnowski & Rosenberg, 1986) reported a text-to-speech program which was trained from phonetic data, and <ref> (Hinton, 1986) </ref> showed that, under proper constraints, back-propagation can develop semantically interpretable ``hidden'' features. In fact, back-propagation is so widely being used today, that it is threatening to become a subfield of its own.
Reference: <author> Hopfield, J. J. </author> <year> (1982). </year> <title> Neural Networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Sciences USA, </booktitle> <volume> 79, </volume> <pages> 2554-2558. </pages>
Reference-contexts: There are several ideas from physics which have entered into the discussion, and perhaps the most notable contributions have come from J. J. Hopfield. In <ref> (Hopfield, 1982) </ref>, he laid out a system for building associative memories based on an analogy to a well-studied physical system, spin glasses.
Reference: <author> Hopfield, J. J. & Tank, D. W. </author> <year> (1985). </year> <title> `Neural' computation of decisions in optimization problems. </title> <journal> Biological Cybernetics, </journal> <volume> 52, </volume> <pages> 141-152. </pages>
Reference-contexts: But new names keep appearing, such as Neurocomputing, Neuro-engineering, and Artificial Neural Systems... 12 Feldman & Ballard (1982, p. 206.) 13 Hopfield (1982, p. 2554). - 7 - In <ref> (Hopfield & Tank, 1985) </ref> he extended his technique of bulk programming of weights to analog devices and applied it to the solution of optimization problems, such as the NP-complete Travelling Salesman problem. <p> For example, in the past-tense model (Rumelhart & McClelland, 1986), there is no obvious means to conjugate from, say, past to present tense, without another 200,000 weights. In the Travelling Salesman network <ref> (Hopfield & Tank, 1985) </ref>, there is no way to add a city to the problem without configuring an entire new network. 4.5.2. Predicting the future The existence and recognition of these problems is slowly causing a change in the direction of near-term connectionist research.
Reference: <author> Huberman, B. A. & Hogg, T. </author> <year> (1987). </year> <title> Phase Transitions in Artificial Intelligence Systems. </title> <journal> Artificial Intelligence, </journal> <volume> 33, </volume> <pages> 155-172. </pages>
Reference-contexts: Figure 5 shows a set of disciplines which are almost communicating today, and implies that the shortest path between AI and Chaos is quite long. There has already been some intrusion of interest in chaos in the physics-based study of neural networks as dynamical systems. For example both <ref> (Huberman & Hogg, 1987) </ref> and (Kurten, 1987) show how phase-transitions occur in particular neural-like systems, and (Lapedes, 1988) demonstrate how a network trained to predict a simple iterated function would follow that function's bifurcations into chaos.
Reference: <author> Jordan, M. I. </author> <year> (1986). </year> <title> Serial Order: A Parallel Distributed Processing Approach. </title> <type> ICS report 8608, </type> <institution> La Jolla: Institute for Cognitive Science, UCSD. </institution>
Reference: <author> Kawamoto, A. H. </author> <year> (1985). </year> <title> Dynamic Processes in the (Re)Solution of Lexical Ambiguity. </title> <type> Doctoral Dissertation, </type> <institution> Providence: Department of Psychology, Brown University. </institution>
Reference-contexts: Anderson was able to apply a type of Hebbian associative learning rule to find weights for this system. BSB models are still being used productively, for example, in the lexical access model of <ref> (Kawamoto, 1985) </ref>. It is almost impossible to quantify the huge contribution of Stephen Grossberg to neural modelling. The scholarly output of Grossberg and his colleagues at Boston University's Center for Adaptive Systems throughout the seventies is daunting in its mathematical sophistication. <p> One of the major attacks on connectionism has been on the inadequacy of its representations, especially on their lack of compositionality (Fodor & Pylyshyn, 1988). In feature-based distributed representations, such as the one used by <ref> (Kawamoto, 1985) </ref>, if the entire feature system is needed to represent a single element, then attempting to represent a structure involving those elements cannot be managed in the same system.
Reference: <author> Kirkpatrick, S., Gelatt, C. D. & Vecchi, M. P. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220, </volume> <pages> 671-680. </pages>
Reference-contexts: You have been wasted one moment by the vertical rays of the sun and the next annealed hiss ing hot by the salt sea spray. 16 Another notion from physics which has been ported into connectionism is simulated annealing. Based on the work of <ref> (Kirkpatrick et al., 1983) </ref>, Ackley, Hinton, & Sejnowski (1985) devised an iterative connectionist network which relaxes into a global minimum.
Reference: <author> Klopf, A. H. </author> <year> (1982). </year> <title> The Hedonistic Neuron. </title> <address> Washington, D.C.: </address> <publisher> Hemisphere Publishing Corporation. </publisher>
Reference-contexts: These are (1) Associative Reward-Penalty, (2) Boltzmann Machine Learning, and (3) Back Propagation. 4.4.1. Associative Reward-Penalty Working with the goal-seeking units of <ref> (Klopf, 1982) </ref>, Andrew Barto and colleagues published results in 1982 on one of the first perceptron-like networks to break the linear learning barrier (Barto et al., 1982). Using a two-layered feed-forward network they demonstrated a system which learned to navigate towards either of 2 locational goals in a small landscape.
Reference: <author> Kohonen, T. </author> <year> (1977). </year> <title> Associative Memory: A Systems-Theoretical Approach. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> Kohonen, T., Oja, E. & Lehtio, P. </author> <year> (1981). </year> <title> Storage and Processing of Information in Distributed Associative Memory Systems. </title> <booktitle> In G. </booktitle>
Reference: <author> E. Hinton & J. A. Anderson, (Eds.), </author> <title> Parallel models of associative memory. </title> <address> Hillsdale: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Kurten, K. E. </author> <year> (1987). </year> <title> Phase transitions in quasirandom neural networks. </title> <booktitle> In Institute of Electrical and Electronics Engineers First International Conference on Neural Networks. </booktitle> <address> San Diego, II-197-20. </address>
Reference-contexts: There has already been some intrusion of interest in chaos in the physics-based study of neural networks as dynamical systems. For example both (Huberman & Hogg, 1987) and <ref> (Kurten, 1987) </ref> show how phase-transitions occur in particular neural-like systems, and (Lapedes, 1988) demonstrate how a network trained to predict a simple iterated function would follow that function's bifurcations into chaos.
Reference: <author> Lenat, D. B. </author> <year> (1977). </year> <title> The Ubiquity of Discovery. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence. </booktitle> <address> Cambridge, MA, </address> <pages> 1093-1105. </pages>
Reference-contexts: Neural network researchers just could not easily publish their work in the AI journals or conferences. hhhhhhhhhhhhhhhhhh 5 Rosenblatt (1962, p. 111). 6 Rosenblatt (1959, p. 449). This type of claim has not been repeated in AI history until the advent of ``Discovery systems'' <ref> (Lenat, 1977) </ref>. 7 Minsky (1986, p. 42). 8 Rumelhart & Zipser (1986, p. 158). - 5 - A lot of the work dealt with associative or content addressable memories.
Reference: <author> Mandelbrot, B. </author> <year> (1982). </year> <title> The Fractal Geometry of Nature. </title> <address> San Francisco: </address> <publisher> Freeman. </publisher>
Reference-contexts: Just as Mandelbrot claims to have replaced the ideal integer-dimensional euclidean geometry - 12 - with a more natural fractional dimensional (fractal) geometry <ref> (Mandelbrot, 1982) </ref>, so may we have ultimately to create a non-aristotelian representational base. I have no concrete idea on what such a substrate would look like, but consider something like the Mandelbrot set as the basis for a reconstructive memory.
Reference: <author> McClelland, J. L. & Rumelhart, D. E. </author> <year> (1981). </year> <title> An interactive activation model of the effect of context in perception: Part 1. An account of basic findings. </title> <journal> Psychology Review, </journal> <volume> 88, </volume> <pages> 375-407. </pages>
Reference: <author> McClelland, J. & Kawamoto, A. </author> <year> (1986). </year> <title> Mechanisms of Sentence Processing: Assigning Roles to Constituents. </title> <editor> In J. L. McClelland, D. E. Rumelhart & the PDP research Group, (Eds.), </editor> <booktitle> Parallel Distributed Processing: Experiments in the Microstructure of Cognition, </booktitle> <volume> Vol. </volume> <pages> 2. </pages> <address> Cambridge: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Task Control A final problem is that many neural models use every ``allowable'' device they have to do a single task. This leaves no facility for changing tasks, or even changing the size of tasks, except massive duplication and modification of resources. For example, in the past-tense model <ref> (Rumelhart & McClelland, 1986) </ref>, there is no obvious means to conjugate from, say, past to present tense, without another 200,000 weights. In the Travelling Salesman network (Hopfield & Tank, 1985), there is no way to add a city to the problem without configuring an entire new network. 4.5.2.
Reference: <author> McCullogh, W. S. & Pitts, W. </author> <year> (1943). </year> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5, </volume> <pages> 115-133. </pages>
Reference-contexts: The connections serve rather to establish autonomous central activities, which then are the basis of further learning. 2 hhhhhhhhhhhhhhhhhh Copyright 1988 by Jordan Pollack. To appear in Artificial Intelligence Review. 1 <ref> (McCullogh & Pitts, 1943) </ref>, p. 124. 2 Hebb (1949, p. xix), emphasis mine. - 2 - A|B 1 B B Logical primitives AND, OR, and NOT implemented with McCullogh & Pitts neurons.
Reference: <author> Minsky, M. & Papert, S. </author> <year> (1969). </year> <title> Perceptrons. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Minsky, M. </author> <year> (1986). </year> <title> The Society of Mind. </title> <address> New York: </address> <publisher> Simon & Schuster. </publisher>
Reference: <author> Norman, D. A & Rumelhart, D. E. </author> <year> (1975). </year> <title> Explorations in Cognition. </title> <address> San Francisco: </address> <publisher> W. H. Freeman & Co.. </publisher>
Reference: <author> Parker, D. B. </author> <year> (1985). </year> <institution> Learning-Logic. Technical Report-47, Cambridge: MIT Center for Computational Research in Economics and Management Science. </institution>
Reference-contexts: Back-Propagation A more robust procedure for learning in multiple levels of perceptron-like units was independently invented and reinvented by several people. In 1981, David Parker apparently disclosed self-organizing logic gates to Stanford University with an eye towards patenting, and he reported his invention in <ref> (Parker, 1985) </ref>; Parker also recently discovered that Paul Werbos developed it in a 1974 mathematics thesis from Harvard University. Yann Le Cun (1985) described a similar procedure in French, and Rumelhart, Hinton, & Williams (1986) reported their method, finally, in English.
Reference: <author> Pollack, J. B. </author> <year> (1987). </year> <title> Cascaded Back Propagation on Dynamic Connectionist Networks. </title> <booktitle> In Proceedings of the Ninth Conference of the Cognitive Science Society. Seattle, </booktitle> <pages> 391-404. </pages>
Reference: <author> Pollack, J. B. </author> <year> (1988). </year> <title> Recursive Auto-Associative Memory: Devising Compositional Distributed Representations. </title> <booktitle> In Proceedings of the Tenth Annual Conference of the Cognitive Science Society. </booktitle> <address> Montreal. </address>
Reference-contexts: There is also research in progress along the lines of Hinton's (unpublished) proposal for reduced descriptions as a way out of the superposition/concatenation difficulty for distributed representations. For example <ref> (Pollack, 1988) </ref> demonstrates a reconstructive distributed memory for variable sized trees, and (Dyer et al., 1988) show a network construction for representing simple semantic networks as labelled directed graphs. - 11 - As problems in capacity, representation, and control get solved, we may expect a new blooming of connectionist applications in
Reference: <author> Rosenblatt, F. </author> <year> (1959). </year> <title> Two theorems of statistical separability in the perceptron. </title> <booktitle> In Mechanization of Thought Processes, </booktitle> <volume> Vol. </volume> <pages> 1. </pages> <address> London: Her Majesty's Stationary Office. </address>
Reference: <author> Rosenblatt, F. </author> <year> (1962). </year> <title> Principles of Neurodynamics. </title> <address> New York: </address> <publisher> Spartan. </publisher>
Reference: <author> Rumelhart, D. E. & McClelland, J. L. </author> <year> (1982). </year> <title> An interactive activation model of the effect of context in perception: Part 2 The contextual enhancement effect and some tests and extensions of the model. </title> <journal> Psychology Review, </journal> <volume> 89, </volume> <pages> 60-94. </pages>
Reference: <author> Rumelhart, D. E. & Zipser, D. </author> <year> (1986). </year> <title> Feature Discovery by Competitive Learning. </title> <editor> In D. E. Rumelhart, J. L. McClelland & the PDP research Group, (Eds.), </editor> <booktitle> Parallel Distributed Processing: Experiments in the Microstructure of Cognition, </booktitle> <volume> Vol. </volume> <pages> 1. </pages> <address> Cambridge: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Discussions of this controversy can be found in <ref> (Rumelhart & Zipser, 1986) </ref> or (Dreyfus & Dreyfus, 1988), and some interesting perspectives on some of the personalities involved can be found in chapter 4 of McCorduck (1979). 3.1. Minsky & Papert I was trying to concentrate on a certain problem but was getting bored and sleepy. <p> Task Control A final problem is that many neural models use every ``allowable'' device they have to do a single task. This leaves no facility for changing tasks, or even changing the size of tasks, except massive duplication and modification of resources. For example, in the past-tense model <ref> (Rumelhart & McClelland, 1986) </ref>, there is no obvious means to conjugate from, say, past to present tense, without another 200,000 weights. In the Travelling Salesman network (Hopfield & Tank, 1985), there is no way to add a city to the problem without configuring an entire new network. 4.5.2.
Reference: <author> Rumelhart, D. E. & McClelland, J. L. </author> <year> (1986). </year> <title> On Learning the Past Tenses of English Verbs. </title> <editor> In J. L. McClelland, D. E. Rumelhart & the PDP research Group, (Eds.), </editor> <booktitle> Parallel Distributed Processing: Experiments in the Microstructure of Cognition, </booktitle> <volume> Vol. </volume> <pages> 2. </pages> <address> Cambridge: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Discussions of this controversy can be found in <ref> (Rumelhart & Zipser, 1986) </ref> or (Dreyfus & Dreyfus, 1988), and some interesting perspectives on some of the personalities involved can be found in chapter 4 of McCorduck (1979). 3.1. Minsky & Papert I was trying to concentrate on a certain problem but was getting bored and sleepy. <p> Task Control A final problem is that many neural models use every ``allowable'' device they have to do a single task. This leaves no facility for changing tasks, or even changing the size of tasks, except massive duplication and modification of resources. For example, in the past-tense model <ref> (Rumelhart & McClelland, 1986) </ref>, there is no obvious means to conjugate from, say, past to present tense, without another 200,000 weights. In the Travelling Salesman network (Hopfield & Tank, 1985), there is no way to add a city to the problem without configuring an entire new network. 4.5.2.
Reference: <author> Sabbah, D. </author> <year> (1982). </year> <title> A Connectionist Approach to Visual Recognition. </title> <institution> TR107: University of Rochester, Computer Science Department. </institution>
Reference: <author> Sampson, G. </author> <year> (1986). </year> <title> A stochastic approach to parsing. </title> <booktitle> In COLING. </booktitle> <address> Bonn. </address>
Reference-contexts: This type of relaxation has been used in two parsing models so far, (Selman, 1985) and <ref> (Sampson, 1986) </ref>, and is a computational primitive in the connectionist production system of (Touretzky & Hinton, 1985). The real beauty of the Boltzmann Machine comes through in its very simple learning rule.
Reference: <author> Sejnowski, T. J. & Rosenberg, C. R. </author> <year> (1986). </year> <title> NETtalk: A parallel network that learns to read aloud. </title> <institution> JHU/EECS-86/01: The Johns Hopkins University, Electrical Engineering and Computer Science Department. </institution>
Reference-contexts: Graphs of these two functions are depicted in figure 4. It can be seen that for the analog case, small changes in the input (by slowly changing weights) cause correspondingly small changes in the output. Back-propagation has been used quite successfully. <ref> (Sejnowski & Rosenberg, 1986) </ref> reported a text-to-speech program which was trained from phonetic data, and (Hinton, 1986) showed that, under proper constraints, back-propagation can develop semantically interpretable ``hidden'' features. In fact, back-propagation is so widely being used today, that it is threatening to become a subfield of its own.
Reference: <author> Selman, B. </author> <year> (1985). </year> <title> Rule-Based Processing in a Connectionist System for Natural Language Understanding. </title> <institution> CSRI-168, Toronto, Canada: University of Toronto, Computer Systems Research Institute. </institution> - <note> 14 - Shastri, </note> <author> L. </author> <year> (1988). </year> <title> Semantic Nets: An evidential formalization and its connectionist realization. </title> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This type of relaxation has been used in two parsing models so far, <ref> (Selman, 1985) </ref> and (Sampson, 1986), and is a computational primitive in the connectionist production system of (Touretzky & Hinton, 1985). The real beauty of the Boltzmann Machine comes through in its very simple learning rule.
Reference: <author> Smolensky, P. </author> <year> (1986). </year> <title> Information Processing in Dynamical Systems: Foundations of Harmony Theory. </title> <editor> In D. E. Rumelhart, J. L. McClelland & the PDP research Group, (Eds.), </editor> <booktitle> Parallel Distributed Processing: Experiments in the Microstructure of Cognition, </booktitle> <volume> Vol. </volume> <pages> 1. </pages> <address> Cambridge: </address> <publisher> MIT Press. </publisher>
Reference: <author> Smolensky, P. </author> <title> (To appear). On the proper treatment of Connectionism. </title> <booktitle> In Behavioral and Brain Sciences. </booktitle> . 
Reference: <author> Stornetta, W. S. & Huberman, B. A. </author> <year> (1987). </year> <title> An Improved three-layer back propagation algorithm. </title> <booktitle> In Institute of Electrical and Electronics Engineers First International Conference on Neural Networks. </booktitle> <address> San Diego, II-637-644. </address>
Reference: <author> Touretzky, D. S. & Hinton, G. E. </author> <year> (1985). </year> <title> Symbols among the neurons: details of a connectionist inference architecture. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence. </booktitle> <address> Los Angeles, CA. </address>
Reference-contexts: This type of relaxation has been used in two parsing models so far, (Selman, 1985) and (Sampson, 1986), and is a computational primitive in the connectionist production system of <ref> (Touretzky & Hinton, 1985) </ref>. The real beauty of the Boltzmann Machine comes through in its very simple learning rule. Given a desired set of partial states to learn and an initial set of weights, the learning procedure, using only local information, can interactively adjust the weights.
Reference: <author> Willshaw, D. J. </author> <year> (1981). </year> <title> Holography, Associative Memory, and Inductive Generalization. </title> <editor> In G. E. Hinton & J. A. Anderson, (Eds.), </editor> <booktitle> Parallel models of associative memory. </booktitle> <address> Hillsdale: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Though beyond the scope of this history, significant developments and analyses can be found in the works of Teuvo Kohonen (Kohonen, 1977; Kohonen et al., 1981) and David Willshaw <ref> (Willshaw, 1981) </ref>. (Anderson et al., 1977) described experiments with a saturating linear model for pattern association and learning called the ``Brain-State in a Box'' or BSB model.
References-found: 62

