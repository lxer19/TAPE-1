URL: http://www.eecs.umich.edu/~davidvc/publications/hldvt97_verify.ps
Refering-URL: http://www.eecs.umich.edu/~davidvc/publications.html
Root-URL: http://www.cs.umich.edu
Email: E-mail: -halasaad, davidvc, jhayes, tnm, brown-@eecs.umich.edu  
Title: High-level design verification of microprocessors via error modeling. Proc. IEEE Int. Workshop on High Level
Author: H. Al-Asaad, D. Van Campenhout, J. P. Hayes, T. Mudge, R. B. Brown. Hussain Al-Asaad, David Van Campenhout, John P. Hayes, Trevor Mudge, and Richard B. Brown 
Address: Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory Department of Electrical Engineering and Computer Science The University of Michigan,  
Note: 1 of 8  Nov. 1997, pp. 194-201.  
Abstract: 1. This research is supported by DARPA under Contract No. DABT63-96-C-0074. The results presented herein do not neces sarily reect the position or the policy of the U.S. Government. Abstract A project is under way at the University of Michigan to develop a design verification methodology for microprocessor hardware based on modeling design errors and generating simulation vectors for the modeled errors via physical fault testing techniques. We have developed a method to systematically collect design error data, and gathered concrete error data from a number of microprocessor design projects. The error data are being used to derive error models suitable for design verification testing. Design verification is done by simulating tests targeted at instances of the modeled errors. We are conducting experiments in which targeted tests are generated for modeled errors in circuits ranging from RTL combinational circuits to pipelined microprocessors. The experiments gauge the quality of the error models and explore test generation for these models. This paper describes our approach and presents some initial experimental results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aharon et al., </author> <title> Verification of the IBM RISC System/ 6000 by dynamic biased pseudo-random test program generator, </title> <journal> IBM Systems Journal, </journal> <volume> Vol. 30, No. 4, </volume> <pages> pp. 527538, </pages> <year> 1991. </year>
Reference-contexts: It provides a cheap way to take advantage of the billion-cycles-a-day simulation-capacity of networked workstations available in many big design organizations. Sophisticated systems have been developed that are biased towards corner cases, thus improving the quality of the tests significantly <ref> [1] </ref>. Another source of test stimuli, thanks to advances in simulator and emulator technology, is existing application and system software. Successfully booting the operating system has become a common quality requirement [13,20].
Reference: [2] <author> H. Al-Asaad and J. P. Hayes, </author> <title> Design verification via simulation and automatic test pattern generation, </title> <booktitle> Proc. IEEE/ ACM International Conference on Computer-Aided Design, </booktitle> <year> 1995, </year> <pages> pp. 174-180. </pages>
Reference-contexts: A different approach is to use design error models to guide test generation. This exploits the similarity between hardware design verification and physical fault testing, as illustrated by Figure 1. For example, Al-Asaad and Hayes <ref> [2] </ref> define a class of design error models for gate-level combinational circuits. They describe how each of these errors can be mapped onto single-stuck line (SSL) errors that can be targeted with standard ATPG tools. <p> Section 3 discusses design error modeling. Coverage experiments are described in section 4, followed by concluding remarks. 2 Design Error Collection Motivation Hardware design verification and physical fault testing are closely related at the conceptual level <ref> [2] </ref>. The basic task of physical fault testing (hardware design verification) is to generate tests that distinguish the correct circuit from faulty (erroneous) ones. The class of faulty circuits to be considered is defined by a logical fault model. <p> Also, synthesis tools ag all wrong bus width errors (category 10) and sensitivity list errors in the always statement (category 13). A set of error models that satisfy the requirements for the restricted case of gate-level logic circuits was developed in <ref> [2] </ref>. Several of these models appear useful for the higher-level (RTL) designs found in Verilog descriptions as well.
Reference: [3] <author> G. Al Hayek and C. Robach, </author> <title> From specification validation to hardware testing: A unified method, </title> <booktitle> Proc. IEEE International Test Conference, </booktitle> <year> 1996, </year> <pages> pp. 885893. </pages>
Reference-contexts: Although still considered too costly for industrial purposes, mutation testing is one of the only approaches that has yielded an automatic test generation system for software testing [19]. Recently, Al Hayek and Robach <ref> [3] </ref> have applied mutation testing to hardware design verification. They demonstrate their approach with verification examples of small VHDL modules. This paper addresses design verification for high-level designs such as microprocessors via error modeling and test generation. A block diagram summarizing our methodology is shown in Figure 2. <p> Some BOEs are redundant such as the BOE on B (PARITY), but most BOEs are easily detectable. Consider, for example, the BOE on D. One possible way to activate the error is to set D <ref> [3] </ref> = 1 and D [0] = 0. To propagate the error to a primary output, the path across IN-MUX and then OUT-MUX is selected. <p> are: Sel-A = 0 Usel_D = 1 Usel_A8B = 0 Usel_G = 0 PassB = 0 PassA = 1 PassH = 0 F-shift = 0 F-add = 0 F-and = 0 F-xor = 0 Solving the gate-level logic equations for G and C we get: G [1:2] = 01 C <ref> [3] </ref> = 1 C [5:7] = 011 All signals not mentioned in the above test have dont care values. We generated tests for all BOEs in the c880. We found that just 10 tests detect all 22 detectable BOEs and serve to prove that another 2 BOEs are redundant.
Reference: [4] <author> B. Beizer, </author> <title> Software Testing Techniques, </title> <publisher> Van Nostrand Rein-hold, </publisher> <address> New York, 2nd edition, </address> <year> 1990. </year>
Reference: [5] <author> F. Brglez and H. Fujiwara, </author> <title> A neutral netlist of 10 combinational benchmark circuits and a target translator in fortran, </title> <booktitle> Proc. IEEE International Symposium on Circuits and Systems, </booktitle> <year> 1985, </year> <pages> pp. 695-698. </pages>
Reference-contexts: Therefore, complete coverage of BOEs, BSEs, and MSEs is achieved with only 5 tests. Experiment 2: The c880 ALU In this example, we try to generate tests for some modeled design errors in the c880 ALU, a member of the ISCAS-85 benchmark suite <ref> [5] </ref>. A high-level model based on a Verilog description of the ALU [18] is shown in Figure 5, and is composed of six modules: an adder, two multiplexing units, a parity unit, and two control units. It has 60 inputs and 26 outputs.
Reference: [6] <author> R. Brown et al., </author> <title> Complementary GaAs technology for a GHz microprocessor, </title> <booktitle> Technical Digest of the GaAs IC Symposium, </booktitle> <year> 1996, </year> <pages> pp. 313-316. </pages>
Reference-contexts: Collection method and statistics The considerations above have led us to implement a systematic method for collecting design errors. Our method uses the CVS revision management tool [8] and targets ongoing design projects at the University of Michi-gan, including the PUMA high-performance microprocessor project <ref> [6] </ref> and various class projects, all of which employ Verilog as the hardware description medium. Designers are asked to archive a new revision whenever a design error is corrected or whenever the design process is interrupted, making it possible to isolate single design errors. <p> Design error data has been collected so far from four VLSI design class projects that involve implementing the DLX microprocessor [15], and from the design of PUMAs fixed point and floating point units <ref> [6] </ref>. The distributions found for the various representative design errors are summarized in Table 1.
Reference: [7] <editor> F. Casaubieilh et al., </editor> <title> Functional verification methodology of Chameleon processor, </title> <booktitle> Proc. ACM/IEEE Design Automation Conference, </booktitle> <year> 1996, </year> <pages> pp. 421426. </pages>
Reference: [8] <author> P. Cederqvist et al., </author> <title> Version Management with CVS, Signum Support AB, </title> <type> Linkoping, </type> <institution> Sweden, </institution> <year> 1992. </year>
Reference-contexts: Some insight into what can go wrong in a large microprocessor design project is provided in [10]. Collection method and statistics The considerations above have led us to implement a systematic method for collecting design errors. Our method uses the CVS revision management tool <ref> [8] </ref> and targets ongoing design projects at the University of Michi-gan, including the PUMA high-performance microprocessor project [6] and various class projects, all of which employ Verilog as the hardware description medium.
Reference: [9] <author> A. K. Chandra et al., </author> <title> AVPGEN a test generator for architecture verification, </title> <journal> IEEE Transactions on Very Large Scale Integration (VLSI) Systems, </journal> <volume> Vol. 3, </volume> <pages> pp. 188200, </pages> <month> June </month> <year> 1995. </year>
Reference: [10] <author> R. P. Colwell and R. A. Lethin, </author> <title> Latent design faults in the development of the Multiflow TRACE/200, </title> <journal> IEEE Transactions on Reliability, </journal> <volume> Vol. 43, No. 4, </volume> <pages> pp. 557565, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Specification description Test generator Implementation simulator Specification simulator Error model Test cases Equal? Implementation description 3 of 8 ple of a user-oriented bug list can be found in [21]. Some insight into what can go wrong in a large microprocessor design project is provided in <ref> [10] </ref>. Collection method and statistics The considerations above have led us to implement a systematic method for collecting design errors.
Reference: [11] <author> R. A. DeMillo, R. J. Lipton, and F. G. Sayward, </author> <title> Hints on test data selection: Help for the practicing programmer, </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 3441, </pages> <month> April </month> <year> 1978. </year>
Reference-contexts: This provides a method to generate tests with a provably high coverage for certain classes of modeled errors. A second method originated from observing the similarities between software testing and hardware design verification. Mutation testing <ref> [11] </ref> considers programs, termed mutants, that differ from the program under test by a single simple error.
Reference: [12] <author> S. Devadas, A. Ghosh, and K. Keutzer, </author> <title> Observability-based code coverage metric for functional simulation, </title> <booktitle> Proc. IEEE/ACM International Conference on Computer-Aided Design, </booktitle> <year> 1996, </year> <pages> pp. 418425. </pages>
Reference-contexts: This poses the problem of quantifying the effectiveness of a test set. Various coverage metrics have been proposed to address this problem. These include code coverage metrics from software testing [1,4,7], finite state machine coverage [16,17,23], architectural event coverage [17], and observability-based metrics <ref> [12] </ref>. A shortcoming of all these metrics is that the relationship between the metric and detection of classes of design errors is not well understood. A different approach is to use design error models to guide test generation.
Reference: [13] <author> G. Ganapathy et al., </author> <title> Hardware emulation for functional verification of K5, </title> <booktitle> Proc. ACM/IEEE Design Automation Conference, </booktitle> <year> 1996, </year> <pages> pp. 315318. </pages>
Reference: [14] <author> M. C. Hansen and J. P. Hayes, </author> <title> High-level test generation using physically-induced faults, </title> <booktitle> Proc. VLSI Test Symposium, </booktitle> <year> 1995, </year> <pages> pp. 20-28. </pages>
Reference-contexts: Bit width error 0.0 2.2 11. If statement 1.1 1.6 12. Declaration statement 0.0 1.6 13. Always statement 0.4 1.4 14. FSM error 3.1 0.3 15. Wrong operator 1.7 0.3 16. Others 1.1 5.8 5 of 8 Experiment 1: The 74283 adder An RTL model <ref> [14] </ref> of the 74283 4-bit fast adder [24] is shown in Figure 4. It consists of a carry-lookahead generator (CLG) and a few word gates. The 74283 adder is an SSL-irredundant circuit, i.e. all SSL faults are detectable. <p> We targeted BSEs with bus widths 8 and 4 only. We found that by adding 3 tests to the ten tests generated for BOEs, we are able to detect all 27 BSEs affecting the c880s multibit buses. Since the multi lookahead adder <ref> [14] </ref>.
Reference: [15] <author> J. Hennessy and D. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufman, </publisher> <address> San Francisco, </address> <year> 1990. </year>
Reference-contexts: The error classification shown in the report is the result of the analysis of error data from several earlier design projects. Design error data has been collected so far from four VLSI design class projects that involve implementing the DLX microprocessor <ref> [15] </ref>, and from the design of PUMAs fixed point and floating point units [6]. The distributions found for the various representative design errors are summarized in Table 1. <p> This experiment demonstrates that good coverage of the modeled errors can be achieved with very small test sets. Experiment 3: A pipelined microprocessor This experiment considers the well-known DLX microprocessor <ref> [15] </ref>. The particular DLX version considered is a student-written design that implements 44 instructions, has a five-stage pipeline and branch prediction logic, and consists of 1552 lines of structural Verilog code, excluding the models for library modules such as adders, register-files, etc.
Reference: [16] <author> A. Hosseini, D. Mavroidis, and P. Konas, </author> <title> Code generation and analysis for the functional verification of microprocessors, </title> <booktitle> Proc. ACM/IEEE Design Automation Conference, </booktitle> <year> 1996, </year> <pages> pp. 305310. </pages>
Reference: [17] <author> M. Kantrowitz and L. M. Noack, </author> <title> Im done simulating; Now what? Verification coverage analysis and correctness checking of the DECchip 21164 Alpha microprocessor, </title> <booktitle> Proc. ACM/IEEE Design Automation Conference, </booktitle> <year> 1996, </year> <pages> pp. 325 330. </pages>
Reference-contexts: This poses the problem of quantifying the effectiveness of a test set. Various coverage metrics have been proposed to address this problem. These include code coverage metrics from software testing [1,4,7], finite state machine coverage [16,17,23], architectural event coverage <ref> [17] </ref>, and observability-based metrics [12]. A shortcoming of all these metrics is that the relationship between the metric and detection of classes of design errors is not well understood. A different approach is to use design error models to guide test generation.
Reference: [18] <author> H. Kim, </author> <title> C880 high-level Verilog description, </title> <type> Internal report, </type> <institution> University of Michigan, </institution> <year> 1996. </year>
Reference-contexts: Experiment 2: The c880 ALU In this example, we try to generate tests for some modeled design errors in the c880 ALU, a member of the ISCAS-85 benchmark suite [5]. A high-level model based on a Verilog description of the ALU <ref> [18] </ref> is shown in Figure 5, and is composed of six modules: an adder, two multiplexing units, a parity unit, and two control units. It has 60 inputs and 26 outputs. The gate-level implementation of the ALU has 383 gates.
Reference: [19] <author> K. N. King and A. J. Offutt, </author> <title> A Fortran language system for mutation-based software testing, </title> <journal> Software Practice and Experience, </journal> <volume> Vol. 21, </volume> <pages> pp. 685718, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Although still considered too costly for industrial purposes, mutation testing is one of the only approaches that has yielded an automatic test generation system for software testing <ref> [19] </ref>. Recently, Al Hayek and Robach [3] have applied mutation testing to hardware design verification. They demonstrate their approach with verification examples of small VHDL modules. This paper addresses design verification for high-level designs such as microprocessors via error modeling and test generation.
Reference: [20] <author> J. Kumar, </author> <title> Prototyping the M68060 for concurrent verification, </title> <journal> IEEE Design & Test of Computers, </journal> <volume> Vol. 14, No. 1, </volume> <pages> pp. 3441, </pages> <year> 1997. </year>
Reference: [21] <institution> MIPS Technologies Inc., MIPS R4000PC/SC Errata, Processor Revision 2.2 and 3.0, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Specification description Test generator Implementation simulator Specification simulator Error model Test cases Equal? Implementation description 3 of 8 ple of a user-oriented bug list can be found in <ref> [21] </ref>. Some insight into what can go wrong in a large microprocessor design project is provided in [10]. Collection method and statistics The considerations above have led us to implement a systematic method for collecting design errors.
Reference: [22] <author> A. J. Offutt et al., </author> <title> An experimental determination of sufficient mutant operators, </title> <journal> ACM Transactions on Software Engineering & Methodology, </journal> <volume> Vol. 5, </volume> <pages> pp. 99-118, </pages> <month> April </month> <year> 1996. </year>
Reference: [23] <author> S. Palnitkar, P. Saggurti, and S.-H. Kuang, </author> <title> Finite state machine trace analysis program, </title> <booktitle> Proc. International Ver-ilog HDL Conference, </booktitle> <year> 1994, </year> <pages> pp. 5257. </pages>
Reference: [24] <editor> Texas Instruments, </editor> <booktitle> The TTL Logic Data Book, </booktitle> <address> Dallas, </address> <year> 1988. </year>
Reference-contexts: If statement 1.1 1.6 12. Declaration statement 0.0 1.6 13. Always statement 0.4 1.4 14. FSM error 3.1 0.3 15. Wrong operator 1.7 0.3 16. Others 1.1 5.8 5 of 8 Experiment 1: The 74283 adder An RTL model [14] of the 74283 4-bit fast adder <ref> [24] </ref> is shown in Figure 4. It consists of a carry-lookahead generator (CLG) and a few word gates. The 74283 adder is an SSL-irredundant circuit, i.e. all SSL faults are detectable.
Reference: [25] <author> M. R. Woodward, </author> <title> Mutation testing its origin and evolution, </title> <journal> Information & Software Technology, </journal> <volume> Vol. 35, </volume> <pages> pp. 163 169, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: The bug data reported by the student for this error was shown earlier in Figure 3. In this test generation experiment, we only consider two basic error models: the SSL fault model, and the inverter insertion (INV) model adopted from mutation testing <ref> [25] </ref>. Under the INV model, the faulty design differs from the fault-free one in that it has an extra inverter inserted in any one of the signal lines.
Reference: [26] <author> M. Yoeli (ed.), </author> <title> Formal Verification of Hardware Design, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> Calif., </address> <year> 1990. </year> <title> Table 2 Coverage of modeled and actual design errors in the instruction decode stage of the DLX pipeline by various test sets. Target errors of test set Coverage (%) of each error type INV SSL Actual Inverter insertion (INV) 100 82 92 Single stuck-line (SSL) 100 100 92 Actual design errors 72 53 100 </title>
Reference-contexts: There are two broad approaches to hardware design verification: formal methods and simulation-based methods. Formal methods try to verify the correctness of a system by means of mathematical proof <ref> [26] </ref>. Such methods consider all possible behaviors of the models representing the system and its specification, whereas simulation-based methods can only consider a subset of all behaviors. The accuracy and completeness of the system and specification models is a fundamental limitation for any formal method.
References-found: 26

