URL: http://www.cs.washington.edu/homes/thu/papers/eqesched.ps
Refering-URL: http://www.cs.washington.edu/homes/thu/jobsearch/index.html
Root-URL: 
Title: Using Runtime Measured Workload Characteristics in Parallel Processor Scheduling Job Scheduling Strategies for Parallel inherent
Author: Thu D. Nguyen, Raj Vaswani, and John Zahorjan 
Date: 1996.  
Note: Appears in  Processing, Volume 1162 of Lecture Notes in Computer Science. Springer-Verlag,  Despite the  This work was supported in part by the National Science Foundation (Grants CCR-9123308 and CCR-9200832) and the Washington Technology Center.  
Address: Box 352350  Seattle, WA 98195-2350 USA  
Affiliation: Department of Computer Science and Engineering,  University of Washington,  
Abstract: We consider the use of runtime measured workload characteristics in parallel processor scheduling. Although many researchers have considered the use of application characteristics in this domain, most of this work has assumed that such information is available a priori. In contrast, we propose and evaluate experimentally dynamic processor allocation policies that rely on determining job characteristics at runtime; in particular, we focus on measuring and using job efficiency and speedup. Our work is intended to be a first step towards the eventual development of production schedulers that use runtime measured workload characteristics in making their decisions. We consider two distinct scheduling scenarios: interactive systems, where minimizing response time is the goal, and batch systems, where maximizing useful instruction throughput is the goal. In both these environments, our experimental results validate the following observations: Our experiments are performed using prototype implementations running on a 50-node KSR-2 shared memory multipro - cessor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1):5379, </volume> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: The second problem we face is one of efficiency. Given an initial allocation of P processors, MGS normally starts searching within the the interval <ref> [1; P ] </ref>. While MGS converges relatively quickly (O (log (P )) steps are required), the cost of individual probes can be quite large if the job has poor speedup at the probed number of processors. <p> Since speedups at numbers of processors less than S (P ) must be less than S (P ), we know that the globally best number of processors must fall in [S (P ); P ]. Our search therefore starts in this interval instead of <ref> [1; P ] </ref>. For applications with good speedup, the interval [S (P ); P ] will typically be small, allowing self-tuning to be performed with little overhead. For applications with poor speedup but only modest slowdown, speedup will be similar at all points between [1; P ] and so, again, self-tuning <p> starts in this interval instead of <ref> [1; P ] </ref>. For applications with good speedup, the interval [S (P ); P ] will typically be small, allowing self-tuning to be performed with little overhead. For applications with poor speedup but only modest slowdown, speedup will be similar at all points between [1; P ] and so, again, self-tuning can be carried out with little overhead. Only in the case where an application initially achieves good speedup but then slows down significantly as its allocation grows does self-tuning incur significant overhead. <p> It is clearly possible to do much more dynamic scheduling, e.g., <ref> [20, 1, 6, 14] </ref>; we did not do so because of the very large incremental implementation cost relative to our more restrictive change, and because we expect that ST-EQUI would perform even better when jobs are more responsive to changes in their allocations. (Of the three policies, ST- EQUI reallocates processors
Reference: [2] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Scharzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum, and J. Martin. </author> <title> The PERFECT Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 3(3):540, </volume> <year> 1989. </year>
Reference-contexts: We have implemented prototypes of both schedulers on a 50-node KSR-2. We evaluate the effectiveness of these prototypes using workload mixes comprised of hand-coded parallel applications from the SPLASH [24] benchmark suite and compiler-parallelized applications from the PERFECT Club <ref> [2] </ref> benchmark suite. Our central result is that the use of runtime measurements can improve system performance substantially, despite the inevitable noise in the gathered data and the associated overheads.
Reference: [3] <author> S.-H. Chiang, R. K. Mansharamani, and M. K. Vernon. </author> <title> Use of Application Characteristics and Limited Preemption for Run-To-Completion Parallel Processor Scheduling Policies. </title> <booktitle> 12 In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 3344, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Section 6 concludes our work. 2 2 Related Work As previously mentioned, many researchers have studied the use of application characteristics by processor sched- ulers of multiprogrammed multiprocessor systems. Majum- dar et al. [13], Chiang et al. <ref> [3] </ref>, Leutenegger and Vernon [12], Sevcik [22, 23], Ghosal et al. [9], Rosti et al. [21] and others have proposed using application characteristics such as speedup, average parallelism, and processor working set to improve the performance of static processor schedulers.
Reference: [4] <author> E. C. Cooper and R. P. Draves. </author> <title> C Threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Measuring communication and system overhead involves little more than periodically reading these counters. Measuring idleness is slightly more involved; we instrument all synchronization code in our runtime systems (KSR PRESTO [11] and CThreads <ref> [4] </ref>) to keep track of elapsed idle time using the wall-clock hardware counter. This idleness measurement scheme is relatively overhead free because idleness accounting is performed mostly when the processor would otherwise be idle.
Reference: [5] <author> L. Dowdy. </author> <title> On the Partitioning of Multiprocessor Systems. </title> <type> Technical report, </type> <institution> Vanderbilt University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: In particular, having just measured the efficiency of an application on P processors to be *, we use the function Ef f iciency (p) = (1 + fi)=(p + fi) <ref> [5] </ref>, choosing fi so that the function evaluates to * at p = P . Next, we determine allocations by following an equal efficiency rule; that is, we allocate processors in a way that causes all applications to have about equal efficiencies according to our extrapolated curves 7 .
Reference: [6] <author> D. L. Eager and J. Zahorjan. Chores: </author> <title> Enhanced RunTime Support for Shared-Memory Parallel Computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1):132, </volume> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: It is clearly possible to do much more dynamic scheduling, e.g., <ref> [20, 1, 6, 14] </ref>; we did not do so because of the very large incremental implementation cost relative to our more restrictive change, and because we expect that ST-EQUI would perform even better when jobs are more responsive to changes in their allocations. (Of the three policies, ST- EQUI reallocates processors
Reference: [7] <author> D. G. Feitelson and B. Nitzberg. </author> <title> Job Characteristics of a Production Parallel Scientific Workload on the NASA Ames iPSC/860. </title> <booktitle> In Proceedings of the IPPS'95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pages 337 360, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: We then examine two distinct scheduling scenarios: interactive systems, where minimizing response time is the goal, and batch systems, where maximizing the rate at which useful work is completed is the goal. Both kinds of computing already have significant roles on existing large scale parallel platforms <ref> [7] </ref>. For the interactive environment, we propose a scheduler that uses measured speedups to adjust the processor allocation of each running job, attempting to maximize job speedup. <p> This decision is supported by the measurements in <ref> [7] </ref>, which indicate that multiprogramming levels of 2, 3, and 4 are the three most common during daytime hours in a production environment. <p> 0.23 0.36 MP3D 0.55 0.44 0.63 0.36 Barnes-FLO52-MP3D Barnes 0.23 0.31 0.23 0.29 FLO52 0.14 0.20 0.27 0.23 Table 1: Job Throughput Rates (jobs/ minute). 11 for the interactive environment) to reflect the likely larger size of jobs submitted for batch execution. (This change is supported by the measurements in <ref> [7] </ref>.) Additionally, we present here results only for those workloads that include a Barnes job, the representative from the class of jobs having good speedup.
Reference: [8] <author> D. G. Feitelson and L. Rudolph. </author> <title> Coscheduling Based on Runtime Identification of Activity Working Sets. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 23(2):135160, </volume> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Thus, the effectiveness of their scheduler is dependent on the existence of application idle periods that are long relative to processor reallocation overheads. Feitelson and Rudolph <ref> [8] </ref> take a similar approach to ours, proposing to dynamically gather information about communicating sets of processes in an attempt to relax the constraints of co-scheduling.
Reference: [9] <author> D. Ghosal, G. Serazzi, and S. Tripathi. </author> <title> The Processor Working Set and Its Use in Scheduling Multiprocessor Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(5):443 453, </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: Section 6 concludes our work. 2 2 Related Work As previously mentioned, many researchers have studied the use of application characteristics by processor sched- ulers of multiprogrammed multiprocessor systems. Majum- dar et al. [13], Chiang et al. [3], Leutenegger and Vernon [12], Sevcik [22, 23], Ghosal et al. <ref> [9] </ref>, Rosti et al. [21] and others have proposed using application characteristics such as speedup, average parallelism, and processor working set to improve the performance of static processor schedulers.
Reference: [10] <author> K. Guha. </author> <title> Using Parallel Program Characteristics in Dynamic Processor Allocation Policies. </title> <type> Technical Report CS95-03, </type> <institution> Department of Computer Science, York University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: More recently, Guha <ref> [10] </ref> has proposed that application characteristics such as efficiency and execution time can also be used profitably by dynamic processor schedulers. All of these studies, however, assume that accurate historical performance data is provided to the scheduler at job submission time.
Reference: [11] <institution> Kendall Square Research Inc., </institution> <address> 170 Tracer Lane, Waltham, MA 02154. </address> <booktitle> KSR/Series Principles of Operation, </booktitle> <year> 1994. </year>
Reference-contexts: The KSR-2 has an interconnection network that is a hierarchy of rings. The basic communication time between two rings is roughly four times that for communication within any one <ref> [11] </ref>. Because of this, MP3D, which has poor locality, achieves optimal speedup at a number of processors that depends strongly on the location of allocated processors. In particular, if all allocated processors are located on the same ring, MP3D's speedup peaks at 12 processors. <p> Measuring communication and system overhead involves little more than periodically reading these counters. Measuring idleness is slightly more involved; we instrument all synchronization code in our runtime systems (KSR PRESTO <ref> [11] </ref> and CThreads [4]) to keep track of elapsed idle time using the wall-clock hardware counter. This idleness measurement scheme is relatively overhead free because idleness accounting is performed mostly when the processor would otherwise be idle.
Reference: [12] <author> S. T. Leutenegger and M. K. Vernon. </author> <title> The Performance of Multiprogrammed Multiprocessor Scheduling Policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 226236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Section 6 concludes our work. 2 2 Related Work As previously mentioned, many researchers have studied the use of application characteristics by processor sched- ulers of multiprogrammed multiprocessor systems. Majum- dar et al. [13], Chiang et al. [3], Leutenegger and Vernon <ref> [12] </ref>, Sevcik [22, 23], Ghosal et al. [9], Rosti et al. [21] and others have proposed using application characteristics such as speedup, average parallelism, and processor working set to improve the performance of static processor schedulers.
Reference: [13] <author> S. Majumdar, D. L. Eager, and R. B. Bunt. </author> <title> Scheduling in Multiprogrammed Parallel Systems. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 104113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Section 6 concludes our work. 2 2 Related Work As previously mentioned, many researchers have studied the use of application characteristics by processor sched- ulers of multiprogrammed multiprocessor systems. Majum- dar et al. <ref> [13] </ref>, Chiang et al. [3], Leutenegger and Vernon [12], Sevcik [22, 23], Ghosal et al. [9], Rosti et al. [21] and others have proposed using application characteristics such as speedup, average parallelism, and processor working set to improve the performance of static processor schedulers.
Reference: [14] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4):379 400, </volume> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: It is clearly possible to do much more dynamic scheduling, e.g., <ref> [20, 1, 6, 14] </ref>; we did not do so because of the very large incremental implementation cost relative to our more restrictive change, and because we expect that ST-EQUI would perform even better when jobs are more responsive to changes in their allocations. (Of the three policies, ST- EQUI reallocates processors
Reference: [15] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A Dynamic Processor Allocation Policy for Multiprogrammed SharedMemory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2):146178, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: All of these studies, however, assume that accurate historical performance data is provided to the scheduler at job submission time. In contrast, we concentrate on using recently measured characteristics of running jobs to optimize performance for the specific workload in execution. McCann et al. <ref> [15] </ref> have proposed a dynamic scheduler that uses application-provided runtime idleness information to dynamically adjust processor allocations to improve processor utilization. <p> Under EQUI, each currently executing job is allocated an equal number of processors. Processor reallocations take place at job arrival and departure times. EQUI is representative of the space sharing approach to processor allocation that has been found to perform well for multiprogrammed shared-memory multiprocessors <ref> [26, 15] </ref>. ST-EQUI: At the highest level, the specific policy we propose to take advantage of runtime estimated speedup allocates an equal number of processors to each executing job, just as with EQUI.
Reference: [16] <author> G. P. McCormick. </author> <title> Nonlinear Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1983. </year>
Reference-contexts: Our current implementation of self-tuning employs a heuristic-based optimization technique that is an adaptation of the method of golden sections (MGS) <ref> [16] </ref> to find the best allocation. MGS is a simple optimization procedure that finds the maximum of a unimodal function over a finite interval by iteratively computing and comparing function values and narrowing the interval in which the maximum may occur 4 .
Reference: [17] <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan. </author> <title> Maximizing Speedup Through Self-Tuning of Processor Allocation. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <pages> pages 463468, </pages> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: ST-EQUI: At the highest level, the specific policy we propose to take advantage of runtime estimated speedup allocates an equal number of processors to each executing job, just as with EQUI. However, each time a reallocation takes place, each affected job engages in a self-tuning procedure <ref> [17] </ref> to estimate how many of its allocated processors should actually be used to maximize its current speedup. (We briefly describe self-tuning in Section 4.2.) It is well- known that many applications do not speed up monotonically with the number of allocated processors; instead, they slow down when executed on more <p> To better understand this new source of overhead, we next present the self-tuning procedure in somewhat more detail. 4.2 Self-Tuning In this section, we present a brief overview of self-tuning. Comprehensive details can be found in <ref> [17] </ref>, which examines the use of this technique in a static (essentially unipro- gramming) environment. <p> While this heuristic does not guarantee that self-tuning will always find the global maximum, the experiments in <ref> [17] </ref> show that this procedure works remarkably well, almost always converging to a near optimal value. The second problem we face is one of efficiency. Given an initial allocation of P processors, MGS normally starts searching within the the interval [1; P ]. <p> One limitation of our quick conversion of the benchmark programs, though, is that we have implemented application level dynamic scheduling only at itera <p>- 6 We show in <ref> [17] </ref> that self-tuning is effective for a much larger number of applications than the 3 representative applications used in this study. tion boundaries; the applications examine and adjust to the number of available processors each time they begin an iteration, but do not do so while executing any one iteration.
Reference: [18] <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan. </author> <title> Parallel Application Characterization for Multiprocessor Scheduling Policy Design. </title> <editor> In D. G. Feitelson and L. Rudolph, editors, </editor> <title> Job Scheduling Strategies for Parallel Processing, </title> <booktitle> volume 1162 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: For- tunately, our experience with a wide variety of benchmark programs shows that parallelization overhead is typically small <ref> [18] </ref>. Thus, we require only estimates of the first three components (idleness, communication, and system overhead) to accurately assess efficiency. On the KSR-2, we rely on a combination of hardware and software support to measure inefficiencies. <p> Empirical evidence shows that successive iterations tend to behave similarly, so that measurements taken for a particular iteration are good predictors of near future behavior <ref> [18] </ref>. Thus, for such applications, we equate a measurement interval to an application iteration, providing a basis by which to reasonably compare a job's performance as its processor allocation is varied. Note, however, that in general, our approach does not require applications to be iterative. <p> between the kernel thread's dequeuing and subsequent enqueuing (or termination) of a user-level thread. 4 Interactive Environments: Improv <p>- ing Response Time In this section, we describe and evaluate a scheduling policy designed to improve response time in interactive environments through the use of runtime gathered job characteristics. 3 In <ref> [18] </ref>, we found that five of the ten SPLASH applications and all seven of the Perfect Club applications we could compile were iterative. 4.1 Policies To evaluate whether runtime measurements can be used beneficially by a scheduler, we compare the multiprogramming performance of the following three policies: EQUI: The basic scheduling <p> Our previous detailed study of these applications <ref> [18] </ref> suggests that they can be divided into three broad classes: * Good speedup. Most of the hand-coded applications fall into this class, which is characterized by fairly good speedup that mostly rises monotonically as the job receives more processors. <p> This class consists of applications whose speedup is irregular, e.g., it varies over time or exhibits multiple local maxima. Such behavior can be observed in both hand-coded and compiler <br>- parallelized applications <ref> [18] </ref>.
Reference: [19] <author> E. W. Parsons and K. C. Sevcik. </author> <title> Multiprocessor Scheduling for High-Variability Service Time Distribution. </title> <booktitle> In Proceedings of the IPPS'95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pages 127145, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Rather, we assume that some other mechanism, such as the feedback scheduling employed in sequential systems, is used for this purpose. (Parsons and Sevcik <ref> [19] </ref> present the design and evaluation of two such schemes, for example.) We consider the workload mix we schedule to be the subset of a larger job mix chosen for current execution by such a mechanism. 4.4 Implementation At the user-level, we implement process control to avoid loss of efficiency due
Reference: [20] <author> C. Polychronopoulos and D. Kuck. </author> <title> Guided Self-Scheduling: </title>
Reference-contexts: It is clearly possible to do much more dynamic scheduling, e.g., <ref> [20, 1, 6, 14] </ref>; we did not do so because of the very large incremental implementation cost relative to our more restrictive change, and because we expect that ST-EQUI would perform even better when jobs are more responsive to changes in their allocations. (Of the three policies, ST- EQUI reallocates processors
References-found: 20

