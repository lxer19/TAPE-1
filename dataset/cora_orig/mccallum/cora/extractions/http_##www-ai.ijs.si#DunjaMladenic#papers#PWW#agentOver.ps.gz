URL: http://www-ai.ijs.si/DunjaMladenic/papers/PWW/agentOver.ps.gz
Refering-URL: http://www-ai.ijs.si/DunjaMladenic/pww.html
Root-URL: 
Email: E-mail: Dunja.Mladenic@ijs.si  
Phone: Phone: (+38)(61) 1773 272, Fax: (+38)(61) 1258-158  
Title: Text-learning and intelligent agents  
Author: Dunja Mladenic 
Web: http://www-ai.ijs.si/DunjaMladenic  
Address: Jamova 39, 1100 Ljubljana, Slovenia  
Affiliation: Department for Intelligent Systems, J.Stefan Institute,  
Abstract: The paper gives overview of some of the recent work in intelligent agents, describing the two frequently used approaches: content-based and collaborative approach. The usage of machine learning techniques on text databases (usually referred to as text-learning) is an important part of content-based intelligent agents that work on text documents. The most popular among them are agents for locating information on World Wide Web and Usenet news filtering agents. Despite the popularity, there is not much work on finding the most suitable machine learning techniques to be used in text-learning on that domains. This paper gives an overview of some work in text-learning through the prism of the three research questions important for development of text-learning intelligent agents: what representation is used for documents, how is the high number of features dealt with and which learning algorithm is used. Brief description and inside structure of content-based intelligent agent named Personal Web-Watcher that uses text-learning for user customized Web browsing is given as an example.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ackerman, M., Billsus, D., Gaffney, S., Hettich, S., Khoo, G., Kim, D.J., Klefstad, R., Lowe, C., Ludeman, A., Muramatsu, J., Omori, K., Pazzani, M.J., Semler, D., Starr, B., Yap, P., </author> <title> Learning Probabilistic User Profiles, </title> <journal> AI magazine, pp. </journal> <volume> 47-56, Vol. 18, no. 2, </volume> <month> Summer </month> <year> 1997. </year>
Reference-contexts: Letizia trays to help with using the time while user is reading a Web document to perform a breath-first search from the current document. Potentially interesting hyperlinks found are suggested to the user in a separate Netscape window. Pazzani et al. [37], Ackerman et al. <ref> [1] </ref>, [38] developed Syskill & Webert, a system that collects ratings of the explored Web pages from the user and learns a user profile from them. Pages are separated according to their topic and a separate profile is learned for each topic.
Reference: [2] <author> Apte, C., Damerau, F., Weiss, </author> <title> S.M., Toward Language Independent Au tomated Learning of Text Categorization Models, </title> <booktitle> Proc. of the 7th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <address> Dubline, </address> <year> 1994. </year>
Reference-contexts: The question is how much can we gain considering additional information in learning (and what information to consider) and what is the price we have to pay for it? There is currently no well studied comparison or 9 Paper reference Document Feature Learning Representation Selection Apte et al. <ref> [2] </ref> bag-of-words stop list+ Decision Rules (freq) frequency weight Armstrong et al. [3] bag-of-words informativity TFIDF Winnow, WordStat Balabanovic and bag-of-words stop list+stemming+ TFIDF Shoham [4] (freq) keep 10 best words Bartell et al. [6] bag-of-words latent semantic | (freq) indexing using SVD Berry et al. [7] bag-of-words latent semantic TFIDF <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. [3], [10], [31], [33], [37],[38] [44]) or frequency of a word in a given document (eg. <ref> [2] </ref>, [4] [6], [7], [20],[28], [44]). <p> There is also some work that uses additional information such as word position [10] or word tuples called n-grams [12], [42]. (2) One of the frequently used approaches to reduce number of different words is to use "stop-list" containing common English words like "a", "the", "with" (eg. <ref> [2] </ref>, [4], [37],[38], [44]) or pruning the infrequent and/or very frequent words (eg. [10], [20]). Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. <p> Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. <ref> [2] </ref>, [3], [4], [20], [28], [31], [37], [38],[45]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [6], [7], [14]). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space <p> A variant of k-Nearest Neighbor was also used by Yang [44]. Joachims [20] introduced Probabilistic T F IDF that takes into account document representation. Apte et al. <ref> [2] </ref> used Decision Rules. Co-hen [10] used Decision Rules and the Inductive Logic Programming systems FOIL and FLIPPER. Lewis and Gale [31] used a combination of a Naive Bayesian classifier and logistic regression. Maes [33] used Memory-Based reasoning.
Reference: [3] <author> Armstrong, R., Freitag, D., Joachims, T., Mitchell, T., WebWatcher: </author> <title> A Learning Apprentice for the World Wide Web, </title> <booktitle> AAAI 1995 Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments, </booktitle> <address> Stanford, </address> <month> March </month> <year> 1995. </year> <note> http://www.cs.cmu.edu/afs/cs/project/theo-6/web-agent/www/webagents-plus.ps.Z </note>
Reference-contexts: Armstrong et al. <ref> [3] </ref> developed WebWatcher, a system that assists user in locating information on the World Wide Web taking keywords from the user, suggesting hyperlinks and receiving evaluation. The system also offers a possibility to get some more similar documents. <p> Text-learning can be described as the usage of machine learning techniques on text databases. There is quite some work in the area of learning over text documents that is not necessary 8 Paper reference Agent name Development org. Help user Content-based agents Armstrong et al. <ref> [3] </ref> WebWatcher CMU browsing WWW Balabanovic and Shoham [4] Lira Stanford browsing WWW Goldman et al. [15] Musag Hebrew Univ. browsing WWW Lieberman [32] Letizia MIT browsing WWW Pazzani et al. [37], [38] Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder <p> information in learning (and what information to consider) and what is the price we have to pay for it? There is currently no well studied comparison or 9 Paper reference Document Feature Learning Representation Selection Apte et al. [2] bag-of-words stop list+ Decision Rules (freq) frequency weight Armstrong et al. <ref> [3] </ref> bag-of-words informativity TFIDF Winnow, WordStat Balabanovic and bag-of-words stop list+stemming+ TFIDF Shoham [4] (freq) keep 10 best words Bartell et al. [6] bag-of-words latent semantic | (freq) indexing using SVD Berry et al. [7] bag-of-words latent semantic TFIDF Foltz and Dumais [14] (freq) indexing using SVD Cohen [10] bag-of-words infrequent <p> There is some evidence in Information Retrieval research, that for long documents, considering information additional to bag-of-words is not worth efforts. Many current systems that learn on text use the bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. <ref> [3] </ref>, [10], [31], [33], [37],[38] [44]) or frequency of a word in a given document (eg. [2], [4] [6], [7], [20],[28], [44]). <p> Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. [2], <ref> [3] </ref>, [4], [20], [28], [31], [37], [38],[45]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [6], [7], [14]). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. <ref> [3] </ref>, [4], [7], [20], [37],[38]). Armstrong et al. [3] used a statistical approach they called WordStat that assumes mutual independence of words. Pazzani et al. [37], [38] used in their ex 11 periments a Naive Bayesian classifier on Boolean vectors, Nearest Neighbor and symbolic learning using Decision Trees. <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. <ref> [3] </ref>, [4], [7], [20], [37],[38]). Armstrong et al. [3] used a statistical approach they called WordStat that assumes mutual independence of words. Pazzani et al. [37], [38] used in their ex 11 periments a Naive Bayesian classifier on Boolean vectors, Nearest Neighbor and symbolic learning using Decision Trees. <p> Text-learning can be applied on collected information to help user browsing the Web. Our work on Personal WebWatcher [36] is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" <ref> [3] </ref>, [19] and some other work related to learning apprentice and learning from text [20], [25], [30], [35]. The idea of a learning apprentice is to automatically customize to individual users, using each user interaction as a training example.
Reference: [4] <author> Balabanovic, M., Shoham, Y., </author> <title> Learning Information Retrieval Agents: Ex periments with Automated Web Browsing, </title> <booktitle> AAAI 1995 Spring Symposium on 15 Information Gathering from Heterogeneous, Distributed Environments, </booktitle> <address> Stan-ford, </address> <month> March </month> <year> 1995. </year> <note> http://robotics.stanford.edu/people/marko/papers/lira.ps </note>
Reference-contexts: The system also offers a possibility to get some more similar documents. The data saved by the system contains information about the keywords user typed in, hyperlinks that were followed and the evaluation given by the user at the end of the search. Balabanovic and Shoham <ref> [4] </ref> developed "a system which learns to browse the Internet on behalf of a user". It searches the World Wide Web taking bounded amount of time, selects the best pages and receives an evaluation from the user. The evaluation is used to update the search and selection heuristics. <p> There is quite some work in the area of learning over text documents that is not necessary 8 Paper reference Agent name Development org. Help user Content-based agents Armstrong et al. [3] WebWatcher CMU browsing WWW Balabanovic and Shoham <ref> [4] </ref> Lira Stanford browsing WWW Goldman et al. [15] Musag Hebrew Univ. browsing WWW Lieberman [32] Letizia MIT browsing WWW Pazzani et al. [37], [38] Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], <p> we have to pay for it? There is currently no well studied comparison or 9 Paper reference Document Feature Learning Representation Selection Apte et al. [2] bag-of-words stop list+ Decision Rules (freq) frequency weight Armstrong et al. [3] bag-of-words informativity TFIDF Winnow, WordStat Balabanovic and bag-of-words stop list+stemming+ TFIDF Shoham <ref> [4] </ref> (freq) keep 10 best words Bartell et al. [6] bag-of-words latent semantic | (freq) indexing using SVD Berry et al. [7] bag-of-words latent semantic TFIDF Foltz and Dumais [14] (freq) indexing using SVD Cohen [10] bag-of-words infrequent words Decision Rules pruned ILP Joachims [20] bag-of-words in/frequent words+ TFIDF, PrTFIDF, (freq) <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. [3], [10], [31], [33], [37],[38] [44]) or frequency of a word in a given document (eg. [2], <ref> [4] </ref> [6], [7], [20],[28], [44]). <p> There is also some work that uses additional information such as word position [10] or word tuples called n-grams [12], [42]. (2) One of the frequently used approaches to reduce number of different words is to use "stop-list" containing common English words like "a", "the", "with" (eg. [2], <ref> [4] </ref>, [37],[38], [44]) or pruning the infrequent and/or very frequent words (eg. [10], [20]). Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. <p> Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. [2], [3], <ref> [4] </ref>, [20], [28], [31], [37], [38],[45]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [6], [7], [14]). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of words <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. [3], <ref> [4] </ref>, [7], [20], [37],[38]). Armstrong et al. [3] used a statistical approach they called WordStat that assumes mutual independence of words. Pazzani et al. [37], [38] used in their ex 11 periments a Naive Bayesian classifier on Boolean vectors, Nearest Neighbor and symbolic learning using Decision Trees.
Reference: [5] <author> Balabanovic, M., Shoham, Y., Fab: </author> <title> Content-Based, Collaborative Recom mendation, </title> <journal> Communications of the ACM, pp. </journal> <volume> 66-70, Vol. 40, no. 3, </volume> <month> March </month> <year> 1997. </year>
Reference-contexts: We will rather present examples of the research work on the systems called intelligent agents and some of their main characteristics. Two approaches to intelligent agents are frequently used <ref> [5] </ref>: content-based and collaborative. The next two Sections 2.1, 2.2 describe these two approaches and give examples of the systems based on them. <p> The network model is constructed incrementally with new users, searching for the co-occurrence of names in close proximity in any documents publicly available on the Web. Balabanovic and Shoham <ref> [5] </ref> developed a system for Web documents recommendation that combines content-based and collaborative approach. Content-based approach is used to generate profile that represent a single user's interests and collaborative approach is used to find similar users. The user's ratings are used to update their personal profile. <p> Minnesota Usenet news filtering Kautz et al. [22], [23] Referral Web AT&T labs. finding expert Combination of content-based and collaborative agent Balabanovic and Shoham <ref> [5] </ref> Fab Stanford browsing WWW Krulwich [27] Lifestyle Finder AgentSoft Ltd. browsing WWW Table 1: Summary of some content-based and collaborative intelligent agents described with their names, development organization and functionality. related to the Web.
Reference: [6] <editor> Bartell, B.T., Cottrell, G.W., Belew, </editor> <title> R.K., Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling, </title> <booktitle> Proceedings of the ACM SIG Information Retrieval, </booktitle> <address> Copenhagen, </address> <year> 1992. </year>
Reference-contexts: no well studied comparison or 9 Paper reference Document Feature Learning Representation Selection Apte et al. [2] bag-of-words stop list+ Decision Rules (freq) frequency weight Armstrong et al. [3] bag-of-words informativity TFIDF Winnow, WordStat Balabanovic and bag-of-words stop list+stemming+ TFIDF Shoham [4] (freq) keep 10 best words Bartell et al. <ref> [6] </ref> bag-of-words latent semantic | (freq) indexing using SVD Berry et al. [7] bag-of-words latent semantic TFIDF Foltz and Dumais [14] (freq) indexing using SVD Cohen [10] bag-of-words infrequent words Decision Rules pruned ILP Joachims [20] bag-of-words in/frequent words+ TFIDF, PrTFIDF, (freq) informativity Naive Bayes Lam et al. [28] bag-of-words (freq) <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. [3], [10], [31], [33], [37],[38] [44]) or frequency of a word in a given document (eg. [2], [4] <ref> [6] </ref>, [7], [20],[28], [44]). <p> Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. [2], [3], [4], [20], [28], [31], [37], [38],[45]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. <ref> [6] </ref>, [7], [14]). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of words that appeared in training documents [41], sum all interesting document vectors and use the resulting vector as a
Reference: [7] <author> Berry, M.W., Dumais, S.T., OBrein, G.W., </author> <title> Using linear algebra for intel ligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> Vol. 37, No. 4., </volume> <pages> pp. 573-595, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Selection Apte et al. [2] bag-of-words stop list+ Decision Rules (freq) frequency weight Armstrong et al. [3] bag-of-words informativity TFIDF Winnow, WordStat Balabanovic and bag-of-words stop list+stemming+ TFIDF Shoham [4] (freq) keep 10 best words Bartell et al. [6] bag-of-words latent semantic | (freq) indexing using SVD Berry et al. <ref> [7] </ref> bag-of-words latent semantic TFIDF Foltz and Dumais [14] (freq) indexing using SVD Cohen [10] bag-of-words infrequent words Decision Rules pruned ILP Joachims [20] bag-of-words in/frequent words+ TFIDF, PrTFIDF, (freq) informativity Naive Bayes Lam et al. [28] bag-of-words (freq) mutual info. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. [3], [10], [31], [33], [37],[38] [44]) or frequency of a word in a given document (eg. [2], [4] [6], <ref> [7] </ref>, [20],[28], [44]). <p> Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. [2], [3], [4], [20], [28], [31], [37], [38],[45]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [6], <ref> [7] </ref>, [14]). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of words that appeared in training documents [41], sum all interesting document vectors and use the resulting vector as a model <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. [3], [4], <ref> [7] </ref>, [20], [37],[38]). Armstrong et al. [3] used a statistical approach they called WordStat that assumes mutual independence of words. Pazzani et al. [37], [38] used in their ex 11 periments a Naive Bayesian classifier on Boolean vectors, Nearest Neighbor and symbolic learning using Decision Trees.
Reference: [8] <author> Burke, R., Hammond, K., Kozlovsky, J., </author> <title> Knowledge-based Information Re trieval for Semi-Structured Text, </title> <booktitle> Working Notes from AAAI Fall Symposium on AI Applications in Knowledge Navigation and Retrieval, </booktitle> <pages> pp. 19-24, </pages> <booktitle> American Association for Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: To extract the topics area, ContacFinder uses some heuristics, for example, extracts semantically significant phrases (eg., fully capitalized words, short phrases: 1 to 5 words, words in a different format from surrounding text). Hammond et al. [16], Burke et al. <ref> [8] </ref>, [9] developed a system that uses a "natural language question-based interface to access distributed text information sources" and helps the user to find answers to her/his question in a databases such as FAQ files. <p> [4] Lira Stanford browsing WWW Goldman et al. [15] Musag Hebrew Univ. browsing WWW Lieberman [32] Letizia MIT browsing WWW Pazzani et al. [37], [38] Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. <ref> [8] </ref>, [9] FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. from Internet Mitchell et al. [35] Calendar Apprentice CMU meeting scheduling Collaborative agents Maes [33] Ringo MIT finding music Firefly MIT music, movie, book Terveen et al.
Reference: [9] <author> Burke, R., Hammond, K., Kulyukin, V., Lytinen, S., Tomuro, N., Schoen berg, S., </author> <title> Question Answering from Frequently Asked Question Files, </title> <journal> AI magazine, pp. </journal> <volume> 57-66, Vol. 18, no. 2, </volume> <month> Summer </month> <year> 1997. </year>
Reference-contexts: To extract the topics area, ContacFinder uses some heuristics, for example, extracts semantically significant phrases (eg., fully capitalized words, short phrases: 1 to 5 words, words in a different format from surrounding text). Hammond et al. [16], Burke et al. [8], <ref> [9] </ref> developed a system that uses a "natural language question-based interface to access distributed text information sources" and helps the user to find answers to her/his question in a databases such as FAQ files. <p> Lira Stanford browsing WWW Goldman et al. [15] Musag Hebrew Univ. browsing WWW Lieberman [32] Letizia MIT browsing WWW Pazzani et al. [37], [38] Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], <ref> [9] </ref> FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. from Internet Mitchell et al. [35] Calendar Apprentice CMU meeting scheduling Collaborative agents Maes [33] Ringo MIT finding music Firefly MIT music, movie, book Terveen et al. [43]
Reference: [10] <author> Cohen, </author> <title> W.W., Learning to Classify English Text with ILP Methods, Work shop on Inductive Logic Programming, </title> <address> Leuven, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: et al. [3] bag-of-words informativity TFIDF Winnow, WordStat Balabanovic and bag-of-words stop list+stemming+ TFIDF Shoham [4] (freq) keep 10 best words Bartell et al. [6] bag-of-words latent semantic | (freq) indexing using SVD Berry et al. [7] bag-of-words latent semantic TFIDF Foltz and Dumais [14] (freq) indexing using SVD Cohen <ref> [10] </ref> bag-of-words infrequent words Decision Rules pruned ILP Joachims [20] bag-of-words in/frequent words+ TFIDF, PrTFIDF, (freq) informativity Naive Bayes Lam et al. [28] bag-of-words (freq) mutual info. <p> There is some evidence in Information Retrieval research, that for long documents, considering information additional to bag-of-words is not worth efforts. Many current systems that learn on text use the bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. [3], <ref> [10] </ref>, [31], [33], [37],[38] [44]) or frequency of a word in a given document (eg. [2], [4] [6], [7], [20],[28], [44]). There is also some work that uses additional information such as word position [10] or word tuples called n-grams [12], [42]. (2) One of the frequently used approaches to reduce <p> bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. [3], <ref> [10] </ref>, [31], [33], [37],[38] [44]) or frequency of a word in a given document (eg. [2], [4] [6], [7], [20],[28], [44]). There is also some work that uses additional information such as word position [10] or word tuples called n-grams [12], [42]. (2) One of the frequently used approaches to reduce number of different words is to use "stop-list" containing common English words like "a", "the", "with" (eg. [2], [4], [37],[38], [44]) or pruning the infrequent and/or very frequent words (eg. [10], [20]). <p> as word position <ref> [10] </ref> or word tuples called n-grams [12], [42]. (2) One of the frequently used approaches to reduce number of different words is to use "stop-list" containing common English words like "a", "the", "with" (eg. [2], [4], [37],[38], [44]) or pruning the infrequent and/or very frequent words (eg. [10], [20]). Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. <p> A variant of k-Nearest Neighbor was also used by Yang [44]. Joachims [20] introduced Probabilistic T F IDF that takes into account document representation. Apte et al. [2] used Decision Rules. Co-hen <ref> [10] </ref> used Decision Rules and the Inductive Logic Programming systems FOIL and FLIPPER. Lewis and Gale [31] used a combination of a Naive Bayesian classifier and logistic regression. Maes [33] used Memory-Based reasoning.
Reference: [11] <author> Drummond,C., Ionescu, D., Holte, R., </author> <title> A Learning Agent that Assists the Browsing of Software Libraries, </title> <type> Technical Report TR-95-12, </type> <institution> Computer Science Dept., University of Ottawa, </institution> <year> 1995. </year>
Reference-contexts: A set of the 15 highly scored Web documents is suggested to each user. Users evaluation of suggested documents is used to evaluate system performance. 2.3 Some other related systems Holte and Drummond [18], Drummond et al. <ref> [11] </ref> designed a system that assists browsing of software libraries, taking keywords from the user and using a rule-based system with forward chaining inference, assuming that the library consists of one type of items and the user goal is a single item.
Reference: [12] <author> Mc Elligott, M., Sorensen, H., </author> <title> An emergent approach to information filtering, </title> <journal> Abakus. U.C.C. Computer Science Journal, </journal> <volume> Vol 1, No. 4, </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: Naive Bayes Maes [33] bag-of-words+ mail/news header info.+ Memory-Based header info. selecting keywords reasoning Mladenic [36] bag-of-words informativity Naive Bayes Nearest Neighbor Pazzani et al. [37], [38] bag-of-words stop list+ TFIDF, Naive Bayes, informativity Nearest Neighbor, Neural Networks, Decision Trees Sorensen and n-gram graph weighting graph connectionist combined Mc Elligott <ref> [12] </ref>, [42] (only bigrams) edges with Genetic Algorithms Yang [44] bag-of-words stop list k-Nearest Neighbor Table 2: Document representation, feature selection and learning algorithms used in some text-learning approaches. <p> There is also some work that uses additional information such as word position [10] or word tuples called n-grams <ref> [12] </ref>, [42]. (2) One of the frequently used approaches to reduce number of different words is to use "stop-list" containing common English words like "a", "the", "with" (eg. [2], [4], [37],[38], [44]) or pruning the infrequent and/or very frequent words (eg. [10], [20]). <p> Apte et al. [2] used Decision Rules. Co-hen [10] used Decision Rules and the Inductive Logic Programming systems FOIL and FLIPPER. Lewis and Gale [31] used a combination of a Naive Bayesian classifier and logistic regression. Maes [33] used Memory-Based reasoning. McElligot and Sorensen <ref> [12] </ref>, [42] used a connectionist approach combined with Genetic Algorithms, Lam et al. [28] used Bayesian Network Induction. There is currently no strong evidence about the superiority of any of the given algorithms for text-learning over different domains.
Reference: [13] <author> Etizioni, O., Weld, D., </author> <title> A Softbot-Based Interface to the Internet, </title> <journal> Commu nications of the ACM Vol. </journal> <volume> 37, No. 7, pp.72-79, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: Etizioni and Weld <ref> [13] </ref> offer an integrated interface to the Internet combining UNIX shell and the World Wide Web to interact with Internet resources. Their agent accepts high-level user goals and dynamically synthesizes the appropriate sequence of Internet commands to satisfy those goals.
Reference: [14] <author> Foltz, P. W. and Dumais, S. T., </author> <title> Personalized information delivery: An anal ysis of information filtering methods, </title> <journal> Communications of the ACM, </journal> <volume> 35(12), pp.51-60, </volume> <year> 1992. </year>
Reference-contexts: Decision Rules (freq) frequency weight Armstrong et al. [3] bag-of-words informativity TFIDF Winnow, WordStat Balabanovic and bag-of-words stop list+stemming+ TFIDF Shoham [4] (freq) keep 10 best words Bartell et al. [6] bag-of-words latent semantic | (freq) indexing using SVD Berry et al. [7] bag-of-words latent semantic TFIDF Foltz and Dumais <ref> [14] </ref> (freq) indexing using SVD Cohen [10] bag-of-words infrequent words Decision Rules pruned ILP Joachims [20] bag-of-words in/frequent words+ TFIDF, PrTFIDF, (freq) informativity Naive Bayes Lam et al. [28] bag-of-words (freq) mutual info. <p> Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. [2], [3], [4], [20], [28], [31], [37], [38],[45]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [6], [7], <ref> [14] </ref>). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of words that appeared in training documents [41], sum all interesting document vectors and use the resulting vector as a model for
Reference: [15] <author> Goldman, C.V., Langer, A., Rosenschein, J.S., Musag: </author> <title> an agent that learns what you mean, </title> <journal> Applied Artificial Intelligence, </journal> <volume> 11, </volume> <pages> pp. 413-435, </pages> <year> 1997. </year>
Reference-contexts: It searches the World Wide Web taking bounded amount of time, selects the best pages and receives an evaluation from the user. The evaluation is used to update the search and selection heuristics. Goldman et al. <ref> [15] </ref> developed Musag, a system that takes keywords from the user and searches the Web for relevant documents. <p> There is quite some work in the area of learning over text documents that is not necessary 8 Paper reference Agent name Development org. Help user Content-based agents Armstrong et al. [3] WebWatcher CMU browsing WWW Balabanovic and Shoham [4] Lira Stanford browsing WWW Goldman et al. <ref> [15] </ref> Musag Hebrew Univ. browsing WWW Lieberman [32] Letizia MIT browsing WWW Pazzani et al. [37], [38] Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], [9] FAQFinder Univ. of Chicago answer question Kamba
Reference: [16] <author> Hammond, K., Burke, R., Schmitt, K., </author> <title> A Case-Based Approach to Knowl edge Navigation, </title> <booktitle> AAAI Workshop on Indexing and Reuse in Multimedia Systems, </booktitle> <pages> pp. 46-57, </pages> <booktitle> American Association for Artificial Intelligence, </booktitle> <year> 1994. </year> <month> 16 </month>
Reference-contexts: To extract the topics area, ContacFinder uses some heuristics, for example, extracts semantically significant phrases (eg., fully capitalized words, short phrases: 1 to 5 words, words in a different format from surrounding text). Hammond et al. <ref> [16] </ref>, Burke et al. [8], [9] developed a system that uses a "natural language question-based interface to access distributed text information sources" and helps the user to find answers to her/his question in a databases such as FAQ files.
Reference: [17] <author> Hedberg, S., </author> <title> Agents for sale: first wave of intelligent agents go commercial, </title> <journal> IEEE Expert Intelligent systems & their applications, pp. </journal> <volume> 16-19, Vol 11, no. 6, </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: The system trays to overcome the problem of sparse coverage of ratings with building models of virtual users interested in a very narrow range of music. The same group developed the Firefly system for music, movie and books recommendation <ref> [17] </ref>. Firefly requires from the user to begin with rating several predefined items, to ensure the possibility of comparing any two user (recall that two users can be compared only if they rated exact same items).
Reference: [18] <author> Holte, R.C., Drummond, C., </author> <title> A Learning Apprentice For Browsing, </title> <booktitle> AAAI Spring Symposium on Software Agents, </booktitle> <year> 1994. </year>
Reference-contexts: Questionary is used to get users characteristics and group similar users in terms of demographic data. A set of the 15 highly scored Web documents is suggested to each user. Users evaluation of suggested documents is used to evaluate system performance. 2.3 Some other related systems Holte and Drummond <ref> [18] </ref>, Drummond et al. [11] designed a system that assists browsing of software libraries, taking keywords from the user and using a rule-based system with forward chaining inference, assuming that the library consists of one type of items and the user goal is a single item.
Reference: [19] <author> Joachims, T., Mitchell, T., Freitag, D., Armstrong, R., WebWatcher: </author> <title> Ma chine Learning and Hypertext, </title> <address> Fachgruppentreffen Maschinelles Lernen, Dortmund, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Text-learning can be applied on collected information to help user browsing the Web. Our work on Personal WebWatcher [36] is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" [3], <ref> [19] </ref> and some other work related to learning apprentice and learning from text [20], [25], [30], [35]. The idea of a learning apprentice is to automatically customize to individual users, using each user interaction as a training example.
Reference: [20] <author> Joachims, T., </author> <title> A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 143|151, </pages> <year> 1997. </year>
Reference-contexts: and bag-of-words stop list+stemming+ TFIDF Shoham [4] (freq) keep 10 best words Bartell et al. [6] bag-of-words latent semantic | (freq) indexing using SVD Berry et al. [7] bag-of-words latent semantic TFIDF Foltz and Dumais [14] (freq) indexing using SVD Cohen [10] bag-of-words infrequent words Decision Rules pruned ILP Joachims <ref> [20] </ref> bag-of-words in/frequent words+ TFIDF, PrTFIDF, (freq) informativity Naive Bayes Lam et al. [28] bag-of-words (freq) mutual info. <p> word position [10] or word tuples called n-grams [12], [42]. (2) One of the frequently used approaches to reduce number of different words is to use "stop-list" containing common English words like "a", "the", "with" (eg. [2], [4], [37],[38], [44]) or pruning the infrequent and/or very frequent words (eg. [10], <ref> [20] </ref>). Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. [2], [3], [4], [20], [28], [31], [37], <p> words (eg. [10], <ref> [20] </ref>). Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. [2], [3], [4], [20], [28], [31], [37], [38],[45]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [6], [7], [14]). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of words that <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. [3], [4], [7], <ref> [20] </ref>, [37],[38]). Armstrong et al. [3] used a statistical approach they called WordStat that assumes mutual independence of words. Pazzani et al. [37], [38] used in their ex 11 periments a Naive Bayesian classifier on Boolean vectors, Nearest Neighbor and symbolic learning using Decision Trees. <p> Pazzani et al. [37], [38] used in their ex 11 periments a Naive Bayesian classifier on Boolean vectors, Nearest Neighbor and symbolic learning using Decision Trees. A variant of k-Nearest Neighbor was also used by Yang [44]. Joachims <ref> [20] </ref> introduced Probabilistic T F IDF that takes into account document representation. Apte et al. [2] used Decision Rules. Co-hen [10] used Decision Rules and the Inductive Logic Programming systems FOIL and FLIPPER. Lewis and Gale [31] used a combination of a Naive Bayesian classifier and logistic regression. <p> Text-learning can be applied on collected information to help user browsing the Web. Our work on Personal WebWatcher [36] is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" [3], [19] and some other work related to learning apprentice and learning from text <ref> [20] </ref>, [25], [30], [35]. The idea of a learning apprentice is to automatically customize to individual users, using each user interaction as a training example. Personal WebWatcher can be seen as content-based personal intelligent agent that helps user browsing the Web.
Reference: [21] <author> Kamba, T., Sakagami, H., Koseki, Y., ANATAGONOMY: </author> <title> a personalized newspaper on the World Wide Web, </title> <journal> International Journal Human-Computer Studies, </journal> <volume> 46, </volume> <pages> pp. 789-803, </pages> <year> 1997. </year>
Reference-contexts: Questions from relevant FAQ files are matched against user's question and five best matching questions are returned together with their answers. Kamba et al. <ref> [21] </ref> developed Antagonomy, a system that composes personalized newspaper on the Web. The system monitors user operations on the articles and reflects them in the user profile. <p> Univ. browsing WWW Lieberman [32] Letizia MIT browsing WWW Pazzani et al. [37], [38] Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], [9] FAQFinder Univ. of Chicago answer question Kamba et al. <ref> [21] </ref> Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. from Internet Mitchell et al. [35] Calendar Apprentice CMU meeting scheduling Collaborative agents Maes [33] Ringo MIT finding music Firefly MIT music, movie, book Terveen et al. [43] PHOAKS AT&T labs. browsing WWW Rucker and Marcos [40] Siteseer
Reference: [22] <author> Kautz, H., Selman, B., Shah, M., </author> <title> Referral Web: Combining Social Networks and Collaborative Filtering, </title> <journal> Communications of the ACM, pp. </journal> <volume> 63-65, Vol. 40, no. 3, </volume> <month> March </month> <year> 1997. </year>
Reference-contexts: This problem of ratings sparsity is common for the collaborative approaches and different systems address it in different ways. GroupLense partitions the set of news messages into clusters that are commonly read together improving the local density of ratings. Kautz et al. <ref> [22] </ref>, [23] developed Referral Web, "an interactive system for reconstructing, visualizing, and searching the social networks on the World Wide Web". The motivation is, similar as for already mentioned content-based system Contact Finder, to help in searching for an expert on a given topic. <p> Minnesota Usenet news filtering Kautz et al. <ref> [22] </ref>, [23] Referral Web AT&T labs. finding expert Combination of content-based and collaborative agent Balabanovic and Shoham [5] Fab Stanford browsing WWW Krulwich [27] Lifestyle Finder AgentSoft Ltd. browsing WWW Table 1: Summary of some content-based and collaborative intelligent agents described with their names, development organization and functionality. related to the
Reference: [23] <author> Kautz, H., Selman, B., Shah, M., </author> <title> The Hidden Web, </title> <journal> AI magazine, pp. </journal> <volume> 27-36, Vol. 18, no. 2, </volume> <month> Summer </month> <year> 1997. </year>
Reference-contexts: This problem of ratings sparsity is common for the collaborative approaches and different systems address it in different ways. GroupLense partitions the set of news messages into clusters that are commonly read together improving the local density of ratings. Kautz et al. [22], <ref> [23] </ref> developed Referral Web, "an interactive system for reconstructing, visualizing, and searching the social networks on the World Wide Web". The motivation is, similar as for already mentioned content-based system Contact Finder, to help in searching for an expert on a given topic. <p> Minnesota Usenet news filtering Kautz et al. [22], <ref> [23] </ref> Referral Web AT&T labs. finding expert Combination of content-based and collaborative agent Balabanovic and Shoham [5] Fab Stanford browsing WWW Krulwich [27] Lifestyle Finder AgentSoft Ltd. browsing WWW Table 1: Summary of some content-based and collaborative intelligent agents described with their names, development organization and functionality. related to the Web.
Reference: [24] <author> Konstan, J.A., Miller, B.N., Maltz, D., Herlocker, J.L., Gordon, L.R., Riedl, J., GroupLense: </author> <title> Applying Filtering to Usenet News, </title> <journal> Communications of the ACM, pp. </journal> <volume> 77-87, Vol. 40, no. 3, </volume> <month> March </month> <year> 1997. </year> <title> [25] de Kroon, </title> <editor> H.C.M, Mitchell, T., Kerckhoffs, E.J.H., </editor> <title> Improving Learning Accuracy in Information Filtering, </title> <booktitle> ICML-96 Workshop Machine learning meets human computer interaction, </booktitle> <year> 1996. </year> <note> http://www.ics.forth.gr/ mous-taki/ICML96 HCI ML/kroon.ps </note>
Reference-contexts: When making recommendation, priority is given to URLs from more similar folders and URLs that appear in several bookmark files of similar users. Konstan et al. <ref> [24] </ref> developed GroupLense, "a collaborative filtering system for Usenet news". The system has a two-part database to store ratings that users have given to messages and correlations between pairs of users based on their ratings. <p> Internet Fish MIT extract info. from Internet Mitchell et al. [35] Calendar Apprentice CMU meeting scheduling Collaborative agents Maes [33] Ringo MIT finding music Firefly MIT music, movie, book Terveen et al. [43] PHOAKS AT&T labs. browsing WWW Rucker and Marcos [40] Siteseer Imana, Inc. browsing WWW Konstan et al. <ref> [24] </ref> GroupLense Univ.
Reference: [26] <author> Krulwich, B., Burkey, C., </author> <title> The ContactFinder agent: Answering buletin board questions with referrals, </title> <booktitle> Proceedings of the Thirteenth National Conference on Artificial Intelligence AAAI 96, </booktitle> <address> pp.10-15, </address> <year> 1996. </year>
Reference-contexts: NewsWeeder additionally assigns predicted rating to each article and generate a personalized list of the top articles (eg., 50 articles predicted as the most interesting) found among all articles. Krulwich and Burkey <ref> [26] </ref> proposed "The ContactFinder agent" that reads and responds to bulletin board messages; assists users by referring them to other people who can help them and; categorizes messages and extracts their topic areas. The system operates in the two phases. <p> al. [3] WebWatcher CMU browsing WWW Balabanovic and Shoham [4] Lira Stanford browsing WWW Goldman et al. [15] Musag Hebrew Univ. browsing WWW Lieberman [32] Letizia MIT browsing WWW Pazzani et al. [37], [38] Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey <ref> [26] </ref> ContactFinder Andersen Consult. finding expert Burke et al. [8], [9] FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. from Internet Mitchell et al. [35] Calendar Apprentice CMU meeting scheduling Collaborative agents Maes [33] Ringo MIT finding
Reference: [27] <author> Krulwich, B., </author> <title> Lifestyle Finder, </title> <journal> AI magazine, pp. </journal> <volume> 37-46, Vol. 18, no. </volume> <pages> 2, </pages> <note> Sum mer 1997. </note>
Reference-contexts: The user's ratings are used to update their personal profile. The two approaches are combined in recommendation when the pages matching user's profile as well as the pages highly rated by similar users are recommended. Similarity between users is measured by the similarity of their profiles. Krulwich <ref> [27] </ref> developed Lifestyle finder, a system for user profiles generation based on the usage of demographic data. The system combines content-based and collaborative approach. Questionary is used to get users characteristics and group similar users in terms of demographic data. <p> Minnesota Usenet news filtering Kautz et al. [22], [23] Referral Web AT&T labs. finding expert Combination of content-based and collaborative agent Balabanovic and Shoham [5] Fab Stanford browsing WWW Krulwich <ref> [27] </ref> Lifestyle Finder AgentSoft Ltd. browsing WWW Table 1: Summary of some content-based and collaborative intelligent agents described with their names, development organization and functionality. related to the Web.
Reference: [28] <author> Lam, W., Low, K.F., Ho, C.Y., </author> <title> Using Bayesian Network Induction Approach for Text Categorization, </title> <booktitle> 15th International Joint Conference on Artificial Intelligence IJCAI97, </booktitle> <pages> pp. 745|750, </pages> <year> 1995. </year> <month> 17 </month>
Reference-contexts: et al. [6] bag-of-words latent semantic | (freq) indexing using SVD Berry et al. [7] bag-of-words latent semantic TFIDF Foltz and Dumais [14] (freq) indexing using SVD Cohen [10] bag-of-words infrequent words Decision Rules pruned ILP Joachims [20] bag-of-words in/frequent words+ TFIDF, PrTFIDF, (freq) informativity Naive Bayes Lam et al. <ref> [28] </ref> bag-of-words (freq) mutual info. <p> Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. [2], [3], [4], [20], <ref> [28] </ref>, [31], [37], [38],[45]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [6], [7], [14]). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of words that appeared <p> Lewis and Gale [31] used a combination of a Naive Bayesian classifier and logistic regression. Maes [33] used Memory-Based reasoning. McElligot and Sorensen [12], [42] used a connectionist approach combined with Genetic Algorithms, Lam et al. <ref> [28] </ref> used Bayesian Network Induction. There is currently no strong evidence about the superiority of any of the given algorithms for text-learning over different domains.
Reference: [29] <author> LaMacchia, </author> <title> B.A., Internet Fish, A revised version of a thesis proposal at MIT, </title> <year> 1996. </year>
Reference-contexts: The layout of the composed newspaper is based on the scores given to the articles that reflects the degree of article matching the user profile. For example, more articles with the higher score are placed on the top of the newspaper. LaMacchia <ref> [29] </ref> proposed Internet Fish, a class of resource-discovery tools designed to help users extract useful information from the Internet. The system includes a natural language interface that currently permits only limited, structured interaction. <p> browsing WWW Pazzani et al. [37], [38] Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], [9] FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia <ref> [29] </ref> Internet Fish MIT extract info. from Internet Mitchell et al. [35] Calendar Apprentice CMU meeting scheduling Collaborative agents Maes [33] Ringo MIT finding music Firefly MIT music, movie, book Terveen et al. [43] PHOAKS AT&T labs. browsing WWW Rucker and Marcos [40] Siteseer Imana, Inc. browsing WWW Konstan et al.
Reference: [30] <author> Lang, K., </author> <title> News Weeder: Learning to Filter Netnews, </title> <booktitle> Proc. of the 12th In ternational Conference on Machine Learning ICML95, </booktitle> <year> 1995. </year>
Reference-contexts: Pages are separated according to their topic and a separate profile is learned for each topic. Generated user profile is used to form queries for the existing search engines in order to get more potentially 4 interesting documents. Lang <ref> [30] </ref> developed NewsWeeder, a system for electronic news filtering that uses text-learning to generate models of user interests. The system uses World Wide Web interface to enable user access the news in a usual way and to enable the system to collect user's ratings as feedback. <p> Help user Content-based agents Armstrong et al. [3] WebWatcher CMU browsing WWW Balabanovic and Shoham [4] Lira Stanford browsing WWW Goldman et al. [15] Musag Hebrew Univ. browsing WWW Lieberman [32] Letizia MIT browsing WWW Pazzani et al. [37], [38] Syskill & Webert UCI browsing WWW Lang <ref> [30] </ref> NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], [9] FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. from Internet Mitchell et al. [35] Calendar Apprentice CMU <p> Text-learning can be applied on collected information to help user browsing the Web. Our work on Personal WebWatcher [36] is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" [3], [19] and some other work related to learning apprentice and learning from text [20], [25], <ref> [30] </ref>, [35]. The idea of a learning apprentice is to automatically customize to individual users, using each user interaction as a training example. Personal WebWatcher can be seen as content-based personal intelligent agent that helps user browsing the Web.
Reference: [31] <author> Lewis, D.,D., Gale, W., A., </author> <title> A Sequential Algorithm for Training Text Clas sifiers, </title> <booktitle> Proc. of the 7th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <address> Dubline, </address> <year> 1994. </year>
Reference-contexts: Bayesian Network Lewis et al. <ref> [31] </ref> bag-of-words log likelihood logistic regression ratio with Naive Bayes Maes [33] bag-of-words+ mail/news header info.+ Memory-Based header info. selecting keywords reasoning Mladenic [36] bag-of-words informativity Naive Bayes Nearest Neighbor Pazzani et al. [37], [38] bag-of-words stop list+ TFIDF, Naive Bayes, informativity Nearest Neighbor, Neural Networks, Decision Trees Sorensen and n-gram <p> There is some evidence in Information Retrieval research, that for long documents, considering information additional to bag-of-words is not worth efforts. Many current systems that learn on text use the bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. [3], [10], <ref> [31] </ref>, [33], [37],[38] [44]) or frequency of a word in a given document (eg. [2], [4] [6], [7], [20],[28], [44]). <p> Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. [2], [3], [4], [20], [28], <ref> [31] </ref>, [37], [38],[45]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [6], [7], [14]). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of words that appeared in <p> A variant of k-Nearest Neighbor was also used by Yang [44]. Joachims [20] introduced Probabilistic T F IDF that takes into account document representation. Apte et al. [2] used Decision Rules. Co-hen [10] used Decision Rules and the Inductive Logic Programming systems FOIL and FLIPPER. Lewis and Gale <ref> [31] </ref> used a combination of a Naive Bayesian classifier and logistic regression. Maes [33] used Memory-Based reasoning. McElligot and Sorensen [12], [42] used a connectionist approach combined with Genetic Algorithms, Lam et al. [28] used Bayesian Network Induction.
Reference: [32] <author> Lieberman, H., Letizia: </author> <title> An Agent that Assists Web Browsing, </title> <booktitle> 14th Interna tional Joint Conference on Artificial Intelligence IJCAI95, </booktitle> <year> 1995. </year>
Reference-contexts: Generated thesaurus is used in document retrieval to extend a set of given keywords. Lieberman <ref> [32] </ref> developed Letizia, "a user interface agent that assists a user browsing the World Wide Web". The system doesn't require any keywords or rating from the user, it infers the user interests from the browsing behavior. The users of the Web browsers usually perform depth-first search. <p> Help user Content-based agents Armstrong et al. [3] WebWatcher CMU browsing WWW Balabanovic and Shoham [4] Lira Stanford browsing WWW Goldman et al. [15] Musag Hebrew Univ. browsing WWW Lieberman <ref> [32] </ref> Letizia MIT browsing WWW Pazzani et al. [37], [38] Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], [9] FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized
Reference: [33] <author> Maes, P., </author> <title> Agents that Reduce Work and Information Overload, </title> <journal> Communi cations of the ACM Vol. </journal> <volume> 37, No. 7, pp.30-40, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: It uses these rules to provide advice to the user for new, unscheduled meetings. 5 2.2 Collaborative approach In the collaborative approach (sometimes referred to as social-learning <ref> [33] </ref>) the system searches for the users with similar interest and recommends the items those users liked. Instead of computing similarity between items the system computes similarity between the users. The assumption is that users provide ratings for all the seen items. <p> Regarding the music taste, User2 would be considered similar to User1 by collaborative approach while User3 would be considered different from User1. Collaborative approach is usually used for non-text data (eg., movie, music), but there are also systems that use it on text data (eg., for news filtering). Maes <ref> [33] </ref> described ideas for "interface agents" that learn from the user as well 6 as from other agents. As examples of such agents they developed agents for: electronic mail handling, meeting scheduling, electronic news filtering and entertainment recommendation. <p> Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], [9] FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. from Internet Mitchell et al. [35] Calendar Apprentice CMU meeting scheduling Collaborative agents Maes <ref> [33] </ref> Ringo MIT finding music Firefly MIT music, movie, book Terveen et al. [43] PHOAKS AT&T labs. browsing WWW Rucker and Marcos [40] Siteseer Imana, Inc. browsing WWW Konstan et al. [24] GroupLense Univ. <p> Bayesian Network Lewis et al. [31] bag-of-words log likelihood logistic regression ratio with Naive Bayes Maes <ref> [33] </ref> bag-of-words+ mail/news header info.+ Memory-Based header info. selecting keywords reasoning Mladenic [36] bag-of-words informativity Naive Bayes Nearest Neighbor Pazzani et al. [37], [38] bag-of-words stop list+ TFIDF, Naive Bayes, informativity Nearest Neighbor, Neural Networks, Decision Trees Sorensen and n-gram graph weighting graph connectionist combined Mc Elligott [12], [42] (only bigrams) <p> There is some evidence in Information Retrieval research, that for long documents, considering information additional to bag-of-words is not worth efforts. Many current systems that learn on text use the bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. [3], [10], [31], <ref> [33] </ref>, [37],[38] [44]) or frequency of a word in a given document (eg. [2], [4] [6], [7], [20],[28], [44]). <p> Apte et al. [2] used Decision Rules. Co-hen [10] used Decision Rules and the Inductive Logic Programming systems FOIL and FLIPPER. Lewis and Gale [31] used a combination of a Naive Bayesian classifier and logistic regression. Maes <ref> [33] </ref> used Memory-Based reasoning. McElligot and Sorensen [12], [42] used a connectionist approach combined with Genetic Algorithms, Lam et al. [28] used Bayesian Network Induction. There is currently no strong evidence about the superiority of any of the given algorithms for text-learning over different domains.
Reference: [34] <author> Menczer, F., ARACHNID: </author> <title> Adaptive Retrieval Agents Choosing Heuristic Neighborhood for Information Discovery, </title> <booktitle> Proc. of the Fourteenth International Conference on Machine Learning ICML '97, </booktitle> <pages> pp. 227-235, </pages> <month> July, </month> <year> 1997. </year>
Reference-contexts: Etizioni and Weld [13] offer an integrated interface to the Internet combining UNIX shell and the World Wide Web to interact with Internet resources. Their agent accepts high-level user goals and dynamically synthesizes the appropriate sequence of Internet commands to satisfy those goals. Menczer <ref> [34] </ref> developed "adaptive intelligent methods to automate on-line information search and discovery in the Web" (like Web robots or crawlers) based on population of intelligent agents.
Reference: [35] <author> Mitchell, T., Caruana, R., Freitag, D., McDermott, J., Zabowski, D., </author> <title> Ex perience with a Learning Personal Assistant, </title> <journal> Communications of the ACM Vol. </journal> <volume> 37, No. 7, pp.81-91, </volume> <month> July </month> <year> 1994. </year> <note> http://www.cs.cmu.edu/afs/cs/user/mitchell/ftp/cacm.ps.Z </note>
Reference-contexts: The system includes a natural language interface that currently permits only limited, structured interaction. The system also includes help in browsing the Web by usage of the existing search engines and user's rating of the documents. Mitchell et al. <ref> [35] </ref> proposed Calendar Apprentice, a system that helps a user in meeting scheduling. The system is connected to the user's electronic calendar and generates sets of rules capturing the user's scheduling preferences and some other information about individual attendees of meetings. <p> browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], [9] FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. from Internet Mitchell et al. <ref> [35] </ref> Calendar Apprentice CMU meeting scheduling Collaborative agents Maes [33] Ringo MIT finding music Firefly MIT music, movie, book Terveen et al. [43] PHOAKS AT&T labs. browsing WWW Rucker and Marcos [40] Siteseer Imana, Inc. browsing WWW Konstan et al. [24] GroupLense Univ. <p> Text-learning can be applied on collected information to help user browsing the Web. Our work on Personal WebWatcher [36] is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" [3], [19] and some other work related to learning apprentice and learning from text [20], [25], [30], <ref> [35] </ref>. The idea of a learning apprentice is to automatically customize to individual users, using each user interaction as a training example. Personal WebWatcher can be seen as content-based personal intelligent agent that helps user browsing the Web.
Reference: [36] <author> Mladenic, D., </author> <title> Personal WebWatcher: Implementation and Design, </title> <type> Technical Report IJS-DP-7472, </type> <month> October, </month> <year> 1996. </year> <note> http://www-ai.ijs.si/DunjaMladenic/papers/PWW/ </note>
Reference-contexts: Bayesian Network Lewis et al. [31] bag-of-words log likelihood logistic regression ratio with Naive Bayes Maes [33] bag-of-words+ mail/news header info.+ Memory-Based header info. selecting keywords reasoning Mladenic <ref> [36] </ref> bag-of-words informativity Naive Bayes Nearest Neighbor Pazzani et al. [37], [38] bag-of-words stop list+ TFIDF, Naive Bayes, informativity Nearest Neighbor, Neural Networks, Decision Trees Sorensen and n-gram graph weighting graph connectionist combined Mc Elligott [12], [42] (only bigrams) edges with Genetic Algorithms Yang [44] bag-of-words stop list k-Nearest Neighbor Table <p> Text-learning can be applied on collected information to help user browsing the Web. Our work on Personal WebWatcher <ref> [36] </ref> is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" [3], [19] and some other work related to learning apprentice and learning from text [20], [25], [30], [35].
Reference: [37] <author> Pazzani, M., Muramatsu, J., Billsus, D., Syskill & Webert: </author> <title> Identifying in teresting web sites, </title> <booktitle> AAAI Spring Symposium on Machine Learning in Information Access, Stanford, March 1996 and Proceedings of the Thirteenth National Conference on Artificial Intelligence AAAI 96, </booktitle> <address> pp.54-61, </address> <year> 1996. </year>
Reference-contexts: Letizia trays to help with using the time while user is reading a Web document to perform a breath-first search from the current document. Potentially interesting hyperlinks found are suggested to the user in a separate Netscape window. Pazzani et al. <ref> [37] </ref>, Ackerman et al. [1], [38] developed Syskill & Webert, a system that collects ratings of the explored Web pages from the user and learns a user profile from them. Pages are separated according to their topic and a separate profile is learned for each topic. <p> Help user Content-based agents Armstrong et al. [3] WebWatcher CMU browsing WWW Balabanovic and Shoham [4] Lira Stanford browsing WWW Goldman et al. [15] Musag Hebrew Univ. browsing WWW Lieberman [32] Letizia MIT browsing WWW Pazzani et al. <ref> [37] </ref>, [38] Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], [9] FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. <p> Bayesian Network Lewis et al. [31] bag-of-words log likelihood logistic regression ratio with Naive Bayes Maes [33] bag-of-words+ mail/news header info.+ Memory-Based header info. selecting keywords reasoning Mladenic [36] bag-of-words informativity Naive Bayes Nearest Neighbor Pazzani et al. <ref> [37] </ref>, [38] bag-of-words stop list+ TFIDF, Naive Bayes, informativity Nearest Neighbor, Neural Networks, Decision Trees Sorensen and n-gram graph weighting graph connectionist combined Mc Elligott [12], [42] (only bigrams) edges with Genetic Algorithms Yang [44] bag-of-words stop list k-Nearest Neighbor Table 2: Document representation, feature selection and learning algorithms used in <p> Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. Many approaches use language independent approach and introduce some sort of word weighting in order to select only the best words (eg. [2], [3], [4], [20], [28], [31], <ref> [37] </ref>, [38],[45]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [6], [7], [14]). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of words that appeared in training <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. [3], [4], [7], [20], <ref> [37] </ref>,[38]). Armstrong et al. [3] used a statistical approach they called WordStat that assumes mutual independence of words. Pazzani et al. [37], [38] used in their ex 11 periments a Naive Bayesian classifier on Boolean vectors, Nearest Neighbor and symbolic learning using Decision Trees. A variant of k-Nearest Neighbor was also used by Yang [44]. Joachims [20] introduced Probabilistic T F IDF that takes into account document representation.
Reference: [38] <author> Pazzani, M., Billsus, D., </author> <title> Learning and Revising User Profiles: The Identi fication of Interesting Web Sites, Machine Learning 27, </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 313|331, </pages> <year> 1997. </year>
Reference-contexts: Letizia trays to help with using the time while user is reading a Web document to perform a breath-first search from the current document. Potentially interesting hyperlinks found are suggested to the user in a separate Netscape window. Pazzani et al. [37], Ackerman et al. [1], <ref> [38] </ref> developed Syskill & Webert, a system that collects ratings of the explored Web pages from the user and learns a user profile from them. Pages are separated according to their topic and a separate profile is learned for each topic. <p> Help user Content-based agents Armstrong et al. [3] WebWatcher CMU browsing WWW Balabanovic and Shoham [4] Lira Stanford browsing WWW Goldman et al. [15] Musag Hebrew Univ. browsing WWW Lieberman [32] Letizia MIT browsing WWW Pazzani et al. [37], <ref> [38] </ref> Syskill & Webert UCI browsing WWW Lang [30] NewsWeeder CMU Usenet news filtering Krulwich and Burkey [26] ContactFinder Andersen Consult. finding expert Burke et al. [8], [9] FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. from <p> Bayesian Network Lewis et al. [31] bag-of-words log likelihood logistic regression ratio with Naive Bayes Maes [33] bag-of-words+ mail/news header info.+ Memory-Based header info. selecting keywords reasoning Mladenic [36] bag-of-words informativity Naive Bayes Nearest Neighbor Pazzani et al. [37], <ref> [38] </ref> bag-of-words stop list+ TFIDF, Naive Bayes, informativity Nearest Neighbor, Neural Networks, Decision Trees Sorensen and n-gram graph weighting graph connectionist combined Mc Elligott [12], [42] (only bigrams) edges with Genetic Algorithms Yang [44] bag-of-words stop list k-Nearest Neighbor Table 2: Document representation, feature selection and learning algorithms used in some <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. [3], [4], [7], [20], [37],<ref> [38] </ref>). Armstrong et al. [3] used a statistical approach they called WordStat that assumes mutual independence of words. Pazzani et al. [37], [38] used in their ex 11 periments a Naive Bayesian classifier on Boolean vectors, Nearest Neighbor and symbolic learning using Decision Trees. A variant of k-Nearest Neighbor was also used by Yang [44]. Joachims [20] introduced Probabilistic T F IDF that takes into account document representation. <p> There is currently no strong evidence about the superiority of any of the given algorithms for text-learning over different domains. Comparison of some learning algorithms given by Pazzani and Billsus <ref> [38] </ref> shows that document representation including feature selection is more promising source of classification accuracy improvement than finding better learning algorithm. 4 Text-learning for user customized Web brows ing One of the available information sources is the World Wide Web and it is currently growing quickly, attracting many users with different
Reference: [39] <author> Rocchio, J., </author> <title> Relevance Feedback in Information Retrieval, in The SMART Retrieval System: Experiments in Automatic Document Processing, Chapter 14, </title> <publisher> pp.313-323, Prentice-Hall Inc., </publisher> <year> 1971. </year>
Reference-contexts: the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of words that appeared in training documents [41], sum all interesting document vectors and use the resulting vector as a model for classification (based on <ref> [39] </ref> relevance feedback method).
Reference: [40] <author> Rucker, J., Marcos, J.P., Siteseer: </author> <title> Personalized Navigation for the Web, </title> <journal> Communications of the ACM, pp. </journal> <volume> 73-75, Vol. 40, no. 3, </volume> <month> March </month> <year> 1997. </year> <month> 18 </month>
Reference-contexts: The special attention is given to the problem of distinguishing recommended Web pages (URLs) from advertised or announced pages. The system includes categorization rules that implement a strategy to distinguish the different purposes for which the Web resources are mentioned. Rucker and Marcos <ref> [40] </ref> developed Siteseer, a "Web-page recommendation system that uses an individual's bookmarks and the organization of bookmarks within folders for predicting and recommending relevant pages". The system measures the degree of overlap (such as common URLs) between the bookmark files of different users and group users according to that similarity. <p> al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. from Internet Mitchell et al. [35] Calendar Apprentice CMU meeting scheduling Collaborative agents Maes [33] Ringo MIT finding music Firefly MIT music, movie, book Terveen et al. [43] PHOAKS AT&T labs. browsing WWW Rucker and Marcos <ref> [40] </ref> Siteseer Imana, Inc. browsing WWW Konstan et al. [24] GroupLense Univ.
Reference: [41] <author> Salton, G., Buckley, C., </author> <title> Term Weighting Approaches in Automatic Text Retrieval, </title> <type> Technical report, </type> <institution> COR-87-881, Department of Computer Science, Cornell University, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [6], [7], [14]). (3) One of the well-established techniques for text classification in Information Retrieval is to represent each document with bag-of-words as a T F IDF -vector in the space of words that appeared in training documents <ref> [41] </ref>, sum all interesting document vectors and use the resulting vector as a model for classification (based on [39] relevance feedback method). <p> The exact formulas used in different approaches may slightly vary (some factors are added, normalization performed <ref> [41] </ref>) but the idea remains the same. A new document is then represented as a vector in the same vector space as the generated model and the distance between them is measured (usually defined as a cosine of an angle between vectors) in order to classify the document.
Reference: [42] <author> Sorensen, H., McElligott, M., PSUN: </author> <title> A Profiling System for Usenet News, </title> <booktitle> CIKM'95 Intelligent Information Agents Workshop, </booktitle> <address> Baltimore, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Bayes Maes [33] bag-of-words+ mail/news header info.+ Memory-Based header info. selecting keywords reasoning Mladenic [36] bag-of-words informativity Naive Bayes Nearest Neighbor Pazzani et al. [37], [38] bag-of-words stop list+ TFIDF, Naive Bayes, informativity Nearest Neighbor, Neural Networks, Decision Trees Sorensen and n-gram graph weighting graph connectionist combined Mc Elligott [12], <ref> [42] </ref> (only bigrams) edges with Genetic Algorithms Yang [44] bag-of-words stop list k-Nearest Neighbor Table 2: Document representation, feature selection and learning algorithms used in some text-learning approaches. <p> There is also some work that uses additional information such as word position [10] or word tuples called n-grams [12], <ref> [42] </ref>. (2) One of the frequently used approaches to reduce number of different words is to use "stop-list" containing common English words like "a", "the", "with" (eg. [2], [4], [37],[38], [44]) or pruning the infrequent and/or very frequent words (eg. [10], [20]). <p> Apte et al. [2] used Decision Rules. Co-hen [10] used Decision Rules and the Inductive Logic Programming systems FOIL and FLIPPER. Lewis and Gale [31] used a combination of a Naive Bayesian classifier and logistic regression. Maes [33] used Memory-Based reasoning. McElligot and Sorensen [12], <ref> [42] </ref> used a connectionist approach combined with Genetic Algorithms, Lam et al. [28] used Bayesian Network Induction. There is currently no strong evidence about the superiority of any of the given algorithms for text-learning over different domains.
Reference: [43] <author> Terveen, L., Hill, W., Amento, B., McDonald, D., Creter, J., PHOAKS: </author> <title> A System for Sharing Recommendations, </title> <journal> Communications of the ACM, pp. </journal> <volume> 59-62, Vol. 40, no. 3, </volume> <month> March </month> <year> 1997. </year>
Reference-contexts: Firefly requires from the user to begin with rating several predefined items, to ensure the possibility of comparing any two user (recall that two users can be compared only if they rated exact same items). Terveen et al. <ref> [43] </ref> propose PHOAKS, a system that automatically recognizes and redistributes recommendations of the Web resources mined from the Usenet news messages. The systems assumes the roles of provider and recommendation recipient are specialized and different. It reuses recommendations from existing online conversations. <p> [9] FAQFinder Univ. of Chicago answer question Kamba et al. [21] Antagonomy NEC corp. personalized newspaper LaMacchia [29] Internet Fish MIT extract info. from Internet Mitchell et al. [35] Calendar Apprentice CMU meeting scheduling Collaborative agents Maes [33] Ringo MIT finding music Firefly MIT music, movie, book Terveen et al. <ref> [43] </ref> PHOAKS AT&T labs. browsing WWW Rucker and Marcos [40] Siteseer Imana, Inc. browsing WWW Konstan et al. [24] GroupLense Univ.
Reference: [44] <author> Yang, Y., </author> <title> Expert Network: Effective and Efficient Learning form Human Decisions in Text Categorization and Retrieval, </title> <booktitle> Proc. of the 7th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <address> Dubline, </address> <year> 1994. </year>
Reference-contexts: header info. selecting keywords reasoning Mladenic [36] bag-of-words informativity Naive Bayes Nearest Neighbor Pazzani et al. [37], [38] bag-of-words stop list+ TFIDF, Naive Bayes, informativity Nearest Neighbor, Neural Networks, Decision Trees Sorensen and n-gram graph weighting graph connectionist combined Mc Elligott [12], [42] (only bigrams) edges with Genetic Algorithms Yang <ref> [44] </ref> bag-of-words stop list k-Nearest Neighbor Table 2: Document representation, feature selection and learning algorithms used in some text-learning approaches. Bag-of-words is used on Boolean features unless notified that word frequency is used (freq). directions fro text document representation that we are aware of. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. [3], [10], [31], [33], [37],[38] <ref> [44] </ref>) or frequency of a word in a given document (eg. [2], [4] [6], [7], [20],[28], [44]). <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indicating if a specific word occurred in document (eg. [3], [10], [31], [33], [37],[38] <ref> [44] </ref>) or frequency of a word in a given document (eg. [2], [4] [6], [7], [20],[28], [44]). <p> There is also some work that uses additional information such as word position [10] or word tuples called n-grams [12], [42]. (2) One of the frequently used approaches to reduce number of different words is to use "stop-list" containing common English words like "a", "the", "with" (eg. [2], [4], [37],[38], <ref> [44] </ref>) or pruning the infrequent and/or very frequent words (eg. [10], [20]). Connected to the particular language is also word stemming that 10 reduces number of different words using a language-specific stemming algorithm. <p> Pazzani et al. [37], [38] used in their ex 11 periments a Naive Bayesian classifier on Boolean vectors, Nearest Neighbor and symbolic learning using Decision Trees. A variant of k-Nearest Neighbor was also used by Yang <ref> [44] </ref>. Joachims [20] introduced Probabilistic T F IDF that takes into account document representation. Apte et al. [2] used Decision Rules. Co-hen [10] used Decision Rules and the Inductive Logic Programming systems FOIL and FLIPPER. Lewis and Gale [31] used a combination of a Naive Bayesian classifier and logistic regression.
Reference: [45] <author> Yang, Y., Pedersen, J.O., </author> <title> A Comparative Study on Feature Selection in Text Categorization, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. </pages> <month> 412-420 </month> <year> 1997. </year> <month> 19 </month>
References-found: 44

