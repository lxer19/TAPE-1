URL: ftp://ftp.cs.virginia.edu/pub/isotach/JPDC95.ps
Refering-URL: http://www.cs.virginia.edu/~isotach/pubs.html
Root-URL: http://www.cs.virginia.edu
Title: Combining Atomic Actions*  
Author: C. Craig Williams and Paul F. Reynolds, Jr. 
Note: *Research supported in part by DARPA, NASA, and the University of Maryland and in part by JPL Contract #957721.  
Address: Charlottesville, Virginia 22903  
Affiliation: Department of Computer Science, University of Virginia  
Abstract: A combining network can combine concurrently issued operations on the same variable and fan out responses to the processes that issued the operations. Combining networks have been proposed with the goals of reducing message traffic, hotspots within the network, and contention for shared variables. This paper extends the class of operations that can be combined to include operations from flat atomic actions. We describe a class of networks called isotach networks and show that isotach networks can correctly combine operations from different flat atomic actions. Also we show that isotach combining networks can combine pipelined operations without sacrificing sequential consistency. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Gottlieb, A., Grishman, R., Kruskal, C. P., McAuliffe, K. P., Rudolph, L., and Snir, M. </author> <title> The NYU Ultracomputer --- Designing an MIMD Shared Memory Parallel Computer. </title> <journal> IEEE Transactions on Computers 32 , 2 (February 1983), </journal> <pages> 175-189. </pages>
Reference-contexts: 1. INTRODUCTION A combining network is an interconnection network that can combine concurrent operations on the same variable into a single composite operation whose execution has the same effect as some serial execution of the original operations. Several combining networks have been built or proposed, including the NYU Ultracomputer <ref> [1] </ref>, the CHoPP machine [2], IBM's RP3 [3], the Yale Fluent [4], and Hsu and Yew's shuffle-exchange network [5]. In this paper we show that a class of networks called isotach combining networks can combine operations from different atomic actions. <p> COMBINING NETWORKS Before showing how to combine operations in an isotach network, we review how operations are combined in a conventional (non-isotach), combining network such as the NYU Ultra-computer <ref> [1] </ref>. On a network such as the NYU Ultracomputer, the routing paths for operations accessing a given variable V form a full binary tree with a PE at each leaf, the MM containing V at the root, and a switch at each interior node.
Reference: 2. <author> Sullivan, H., Bashkow, T.R., and Klappholz, D. </author> <title> A Large Scale, Homogeneous, Fully Distributed Parallel Machine. </title> <booktitle> 4th Annual Symposium on Computer Architecture, </booktitle> <year> 1977, </year> <pages> pp. 105-134. </pages>
Reference-contexts: Several combining networks have been built or proposed, including the NYU Ultracomputer [1], the CHoPP machine <ref> [2] </ref>, IBM's RP3 [3], the Yale Fluent [4], and Hsu and Yew's shuffle-exchange network [5]. In this paper we show that a class of networks called isotach combining networks can combine operations from different atomic actions. The ability to combine operations from atomic actions increases concurrency in accessing shared memory.
Reference: 3. <author> Pfister, et al., </author> <title> G.F. The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture. </title> <booktitle> Int. Conf. on Parallel Processing, </booktitle> <year> 1985, </year> <pages> pp. 764-771. </pages>
Reference-contexts: Several combining networks have been built or proposed, including the NYU Ultracomputer [1], the CHoPP machine [2], IBM's RP3 <ref> [3] </ref>, the Yale Fluent [4], and Hsu and Yew's shuffle-exchange network [5]. In this paper we show that a class of networks called isotach combining networks can combine operations from different atomic actions. The ability to combine operations from atomic actions increases concurrency in accessing shared memory.
Reference: 4. <author> Ranade, A.G., Bhatt, S.N., and Johnsson, </author> <title> S.L. The Fluent Abstract Machine. </title> <type> Tech. Rep. 573, </type> <institution> Yale University, Dept. of Computer Science, </institution> <month> January, </month> <year> 1988. </year>
Reference-contexts: Several combining networks have been built or proposed, including the NYU Ultracomputer [1], the CHoPP machine [2], IBM's RP3 [3], the Yale Fluent <ref> [4] </ref>, and Hsu and Yew's shuffle-exchange network [5]. In this paper we show that a class of networks called isotach combining networks can combine operations from different atomic actions. The ability to combine operations from atomic actions increases concurrency in accessing shared memory. <p> Gibbons proposed a similar network for the purpose of supporting barrier synchronization [17, 18]. These networks realize a rudimentary isotach logical time system in which each logical time consists of only a single component, the pulse component. The Yale Fluent network proposed by Ranade <ref> [4, 19] </ref> is more pertinent. This network, proposed to support efficient emulation of a concurrent-read, concurrent-write (CRCW) PRAM, realizes the 4-tuple isotach logical time system described above. We return to Ranade's work in section 4. 2.2.1. <p> The idea of requiring that switches combine operations in a given order dictated by the pId's of the issuing PE's has been proposed by other researchers as a way to compute parallel prefixes within the ICN <ref> [4, 9] </ref>. The combining algorithm is otherwise identical to the algorithm in conventional combining networks. Since we have assumed for simplicity that the network maintains the velocity invariant only in the forward direction, the switches must store combining information in associative lookup queues.
Reference: 5. <author> Hsu, W. T. and Yew, P. </author> <title> An Effective Synchronization Network for Hot-Spot Accesses. </title> <booktitle> ACM Transactions on Computer Systems 10 , 3 (August 1992), </booktitle> <pages> 167-189. </pages>
Reference-contexts: Several combining networks have been built or proposed, including the NYU Ultracomputer [1], the CHoPP machine [2], IBM's RP3 [3], the Yale Fluent [4], and Hsu and Yew's shuffle-exchange network <ref> [5] </ref>. In this paper we show that a class of networks called isotach combining networks can combine operations from different atomic actions. The ability to combine operations from atomic actions increases concurrency in accessing shared memory.
Reference: 6. <author> Pfister, G. F. and Norton, V. A. </author> <title> Hot Spot Contention and Combining in Multistage Interconnection Networks. </title> <journal> IEEE Transactions on Computers 34 , 10 (October, </journal> <year> 1985), </year> <pages> 943-948. </pages>
Reference-contexts: Combining networks also reduce network load and may reduce the incidence of hot-spots <ref> [6] </ref>.
Reference: 7. <author> Gottlieb, A., Lubachevsky, B.D., and Rudolph, L. </author> <title> Basic Techniques for the Efficient Coordination of Very Large Numbers of Cooperating Sequential Processors. </title> <booktitle> ACM Transactions on Programming Languages and Systems 5 , 2 (April 1983), </booktitle> <pages> 164-189. </pages>
Reference-contexts: Combining networks also reduce network load and may reduce the incidence of hot-spots [6]. Combining networks have been shown to be useful in constructing highly concurrent versions of some synchronization primitives and data structures <ref> [7] </ref>. 2 A large class of operations, including all associative operations, are combinable [8, 9], but there is an important class of operations that has not been shown to be combinable operations from atomic actions.
Reference: 8. <author> Gottlieb, A. and Kruskal, </author> <title> C.P. Coordinating Parallel Processors: A Partial Unification. </title> <journal> SIGARCH News, </journal> <month> October </month> <year> 1981, </year> <pages> pp. 16-24. 32 </pages>
Reference-contexts: Combining networks also reduce network load and may reduce the incidence of hot-spots [6]. Combining networks have been shown to be useful in constructing highly concurrent versions of some synchronization primitives and data structures [7]. 2 A large class of operations, including all associative operations, are combinable <ref> [8, 9] </ref>, but there is an important class of operations that has not been shown to be combinable operations from atomic actions. <p> Combining is recursive, i.e., an operation combined at one stage may itself be the product of combining at a previous stage. All associative operations on the same variable can be combined <ref> [8] </ref>, as well as atomically processed RMW operations, boolean operations, pseudo associative operations such as exponentiation, and some synchronization operations [9]). Both homogenous and nonhomogenous operations, 16 e.g., READ's with WRITE's, can be combined. Switch Algorithm.
Reference: 9. <author> Kruskal, C.P., Rudolph, L., and Snir, M. </author> <title> Efficient Synchronization on Multiprocessors with Shared Memory. </title> <journal> ACM Trans. Prog. Lang. </journal> <note> and Systems 10 , 4 (October 1988), 579-601. </note>
Reference-contexts: Combining networks also reduce network load and may reduce the incidence of hot-spots [6]. Combining networks have been shown to be useful in constructing highly concurrent versions of some synchronization primitives and data structures [7]. 2 A large class of operations, including all associative operations, are combinable <ref> [8, 9] </ref>, but there is an important class of operations that has not been shown to be combinable operations from atomic actions. <p> For example, the atomic action in which process P1 reads shared variables A and B is a flat atomic action. The atomic action in which P2 increments A while decrementing B is also a flat atomic action assuming the increment and decrement operations are atomically processed read-modify-write (RMW) operations <ref> [9] </ref>. In the case of this pair of atomic actions, our result means that the network can combine P1's READ to A with P2's increment and P1's READ to B with P2's decrement. <p> In this paper, we show that combining operations within an isotach network maintains sequential consistency. Combining networks have previously been proven correct by using, in place of sequential consistency, a weaker correctness criterion requiring only that messages with the same source and destination be received in order <ref> [9] </ref>. This paper is organized as follows. In section 2 we summarize our previous work on isotach networks in order to make this paper self-contained. We define the isotach network, show how it can be implemented, and describe isotach-based techniques for enforcing atomicity and sequential consistency. <p> All associative operations on the same variable can be combined [8], as well as atomically processed RMW operations, boolean operations, pseudo associative operations such as exponentiation, and some synchronization operations <ref> [9] </ref>). Both homogenous and nonhomogenous operations, 16 e.g., READ's with WRITE's, can be combined. Switch Algorithm. <p> In general, any pair of operations that can be combined can be combined in either orientation <ref> [9] </ref>. A READ and a WRITE can be combined, in that order, by forwarding a SWAP that assigns the 17 value supplied by the WRITE and satisfying the READ by the value returned by the SWAP. <p> Kruskal, Rudolph, and Snir prove this combining algorithm is correct by showing that the result of executing each composite operation OP c is equivalent to the result of a serial execution of the operations OP c represents <ref> [9] </ref>. An original operation, i.e., an operation issued by a process, represents itself and a composite operation produced by combining OP i and OP j represents the operations OP i and OP j represent. <p> If f c = f n f f n -1 f , . . . , f f 1 , then execution of OP c is equivalent to the serial execution of op 1 , . . . , op n , in that order (Lemma 4.1 in <ref> [9] </ref>). In other words, execution of OP c is equivalent to serial execution of the original operations OP c represents in the reverse of the order in which their functions appear in the composition of functions that constitute f c 1 . <p> The algorithm is similar to the isotach-network hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 This statement of the result differs from that in original paper <ref> [9] </ref> because we use f i f f j to denote f i (f j (x)) whereas in the original paper it denotes f j (f i (x )). 18 algorithm described in Section 2 except that operations and responses traverse the network three times. <p> The idea of requiring that switches combine operations in a given order dictated by the pId's of the issuing PE's has been proposed by other researchers as a way to compute parallel prefixes within the ICN <ref> [4, 9] </ref>. The combining algorithm is otherwise identical to the algorithm in conventional combining networks. Since we have assumed for simplicity that the network maintains the velocity invariant only in the forward direction, the switches must store combining information in associative lookup queues. <p> The result of executing an operation OP x is equivalent to a serial execution of the original operations OP x represents in the reverse of the order in which their functions appear in f x (Lemma 4.1, <ref> [9] </ref>). Since f j appears before f i in f x , OP i appears before OP j in the recursive expansion of OP x . ` LEMMA 6. The logical execution time of each operation is not changed by combining. PROOF. <p> In a conventional combining network, the switch that combines a WRITE and READ must hold the READ until the response to the forwarded operation returns from memory even though the value to be returned is already known. Returning the READ immediately can cause a violation of sequential consistency <ref> [9, 24] </ref>. The source of the problem is that a process in a conventional network relies on the receipt of a response to an operation as evidence that its previous operation has been executed at memory.
Reference: 10. <author> Lomet, </author> <title> D.B. Process Structuring, Synchronization, and Recovery Using Atomic Actions. </title> <booktitle> Proceedings Conference on Language Design for Reliable Software, SIGPLAN Notices Notices 12 , 3 (March 1977), </booktitle> <pages> 128-137. </pages>
Reference-contexts: A (non-trivial) atomic action is a group of two or more operations issued by the same process that must be executed atomically, i.e., without interleaving by other operations so that the atomic action appears to be executed instantaneously, as a single indivisible step <ref> [10] </ref>. With one exception (discussed below), operations from atomic actions are not combinable. The reason such operations cannot be combined is simple operations can be combined only if they are simultaneously present in the network.
Reference: 11. <author> Reynolds, Jr., Paul F., Williams, Craig, and Wagner, Jr., Raymond R. </author> <title> Parallel Operations. </title> <type> Tech. Rep. 89-16, </type> <institution> University of Virginia, Department of Computer Science, </institution> <month> December, </month> <year> 1989. </year>
Reference: 12. <author> Williams, Craig and Reynolds, Jr., Paul F. </author> <title> On Variables as Access Sequences in Parallel Asynchronous Computations. </title> <type> Tech. Rep. 89-17, </type> <institution> University of Virginia, Department of Computer Science, </institution> <month> December, </month> <year> 1989. </year>
Reference-contexts: Using the concepts of serializability and execution equivalence developed in the context of database concurrency control, see e.g. [22], we have shown an execution that conforms with the atomicity and sequencing rules is atomic at the flat atomic action level and sequentially consistent <ref> [12, 13] </ref>.
Reference: 13. <author> Williams, </author> <title> C.C. Concurrency Control in Asynchronous Computations. </title> <type> Ph.D. Thesis, </type> <institution> University of Virginia, </institution> <year> 1993. </year>
Reference-contexts: Using the concepts of serializability and execution equivalence developed in the context of database concurrency control, see e.g. [22], we have shown an execution that conforms with the atomicity and sequencing rules is atomic at the flat atomic action level and sequentially consistent <ref> [12, 13] </ref>. <p> These assumptions simplify the combining algorithm and proof by ensuring that no switch receives more than two operations on V in any pulse, but a modified version of the combining algorithm can be proven without relying on either assumption <ref> [13, 23] </ref>. The assumptions are unnecessary if an additional constraint is placed on the switches: a switch can combine operations OP i and OP j only if, in the absence of combining, it could route OP i and OP j one after the other, with no intervening operation.
Reference: 14. <author> Lamport, L. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocessor Programs. </title> <journal> IEEE Trans. on Computers 28 (1979), </journal> <pages> 690-691. </pages>
Reference-contexts: A second contribution of this paper is to show that operations can be combined without sacrificing sequential consistency. An execution is sequentially consistent if the order in which operations are executed is consistent with the order specified by each individual process's sequential program <ref> [14] </ref>. This ordering guarantee is so basic that it is easily taken for granted, yet it is expensive to enforce.
Reference: 15. <author> Lamport, L. </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System. </title> <journal> Comm. </journal> <note> ACM 21 , 7 (July 1978), 558-565. </note>
Reference-contexts: Isotach Logical Time Systems Informally, a logical time system is a set of rules for numbering events of interest, that is, for assigning each event a logical time. Lamport's classic paper on logical time systems <ref> [15] </ref> describes a logical time system in which the times assigned are consistent with the happened before relation, a relation over the events of sending and receiving messages that captures the notion of potential causality.
Reference: 16. <author> Awerbuch, B. </author> <title> Complexity of Network Synchronization. </title> <journal> J. </journal> <note> ACM 32 , 4 (October 1985), 804-823. </note>
Reference-contexts: There have been several networks proposed that can be classified as isotach networks. The first such network is the alpha-synchronizer network proposed by Awerbuch as a way to execute SIMD graph algorithms on asynchronous networks <ref> [16] </ref>. Gibbons proposed a similar network for the purpose of supporting barrier synchronization [17, 18]. These networks realize a rudimentary isotach logical time system in which each logical time consists of only a single component, the pulse component. The Yale Fluent network proposed by Ranade [4, 19] is more pertinent.
Reference: 17. <author> Birk, Y., Gibbons, P.B., Sanz, J.L.C., and Soroker, D. </author> <title> A Simple Mechanism for Efficient Barrier Synchronization in MIMD Machines. </title> <type> Tech. Rep. RJ 7078, </type> <institution> IBM, </institution> <month> October </month> <year> 1989. </year> <month> 33 </month>
Reference-contexts: There have been several networks proposed that can be classified as isotach networks. The first such network is the alpha-synchronizer network proposed by Awerbuch as a way to execute SIMD graph algorithms on asynchronous networks [16]. Gibbons proposed a similar network for the purpose of supporting barrier synchronization <ref> [17, 18] </ref>. These networks realize a rudimentary isotach logical time system in which each logical time consists of only a single component, the pulse component. The Yale Fluent network proposed by Ranade [4, 19] is more pertinent.
Reference: 18. <author> Gibbons, P.B. </author> <title> The Asynchronous PRAM: A Semi-Synchronous Model for Shared Memory MIMD Machines. </title> <type> 89-062, </type> <institution> International Computer Science Institute, Berkeley, California, </institution> <month> December, </month> <year> 1989. </year>
Reference-contexts: There have been several networks proposed that can be classified as isotach networks. The first such network is the alpha-synchronizer network proposed by Awerbuch as a way to execute SIMD graph algorithms on asynchronous networks [16]. Gibbons proposed a similar network for the purpose of supporting barrier synchronization <ref> [17, 18] </ref>. These networks realize a rudimentary isotach logical time system in which each logical time consists of only a single component, the pulse component. The Yale Fluent network proposed by Ranade [4, 19] is more pertinent.
Reference: 19. <author> Ranade, </author> <title> A.G. How to Emulate Shared Memory. </title> <booktitle> IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <publisher> Los Angeles,1987, </publisher> <pages> pp. 185-194. </pages>
Reference-contexts: Gibbons proposed a similar network for the purpose of supporting barrier synchronization [17, 18]. These networks realize a rudimentary isotach logical time system in which each logical time consists of only a single component, the pulse component. The Yale Fluent network proposed by Ranade <ref> [4, 19] </ref> is more pertinent. This network, proposed to support efficient emulation of a concurrent-read, concurrent-write (CRCW) PRAM, realizes the 4-tuple isotach logical time system described above. We return to Ranade's work in section 4. 2.2.1.
Reference: 20. <author> Wu, C. and Feng, T. </author> <title> On a Class of Multistage Interconnection Networks. </title> <journal> IEEE Trans. </journal> <note> on Computers 29 , 8 (August 1980), 694-702. </note>
Reference-contexts: Fig. 1 shows an equidistant network with reverse baseline topology. This network, under renumbering of inputs and outputs, is equivalent to many other common equidistant networks including the omega and indirect binary cube <ref> [20] </ref>. We use D to denote the number of stages in the network. FIFO links.
Reference: 21. <author> Reynolds, Jr., Paul F., Williams, Craig, and Wagner, Jr., Raymond R. </author> <title> Empirical Analysis of Isotach Networks. </title> <type> Tech. Rep. 92-19, </type> <institution> University of Virginia, Dept. of Computer Science, </institution> <month> June, </month> <year> 1992. </year>
Reference-contexts: We have completed a simulation study of isotach networks similar to the one described in this section <ref> [21] </ref>. The study compares the performance of isotach and conventional systems 13 under a variety of synthetic workloads. <p> The ability to combine operations that are pipelined or that are from atomic actions improves the concurrency of accessing shared memory by increasing the opportunities for combining. Directions for future work include studying the performance of isotach combining networks. Our previous simulation study of non-combining isotach networks <ref> [21] </ref> suggests that the performance of isotach combining networks relative to conventional combining networks will depend on the incidence of flat atomic actions and the extent to which data dependences allow processes to take advantage of pipelining.
Reference: 22. <author> Papadimitriou, C. </author> <title> Database Concurrency Control. </title> <publisher> Computer Science Press, </publisher> <year> 1986. </year>
Reference-contexts: Using the concepts of serializability and execution equivalence developed in the context of database concurrency control, see e.g. <ref> [22] </ref>, we have shown an execution that conforms with the atomicity and sequencing rules is atomic at the flat atomic action level and sequentially consistent [12, 13].
Reference: 23. <author> Williams, C.C. and Reynolds, Jr., P.F. </author> <title> Combining Atomic Actions in a Recombining Network. </title> <type> Tech. Rep. 91-33, </type> <institution> University of Virginia, Department of Computer Science, </institution> <month> November, </month> <year> 1991. </year>
Reference-contexts: These assumptions simplify the combining algorithm and proof by ensuring that no switch receives more than two operations on V in any pulse, but a modified version of the combining algorithm can be proven without relying on either assumption <ref> [13, 23] </ref>. The assumptions are unnecessary if an additional constraint is placed on the switches: a switch can combine operations OP i and OP j only if, in the absence of combining, it could route OP i and OP j one after the other, with no intervening operation.

References-found: 23

