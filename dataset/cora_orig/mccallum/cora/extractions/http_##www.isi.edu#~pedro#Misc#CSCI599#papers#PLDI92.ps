URL: http://www.isi.edu/~pedro/Misc/CSCI599/papers/PLDI92.ps
Refering-URL: http://www.isi.edu/~pedro/Misc/CSCI599/papers/presentations.html
Root-URL: http://www.isi.edu
Email: (permissions@acm.org).  
Phone: fax +1 (212) 869-0481,  
Author: Laurie J. Hendren Joseph Hummel Alexandru Nicolau 
Affiliation: School of Computer Science McGill University  Dept. of Information and Computer Science  
Address: or  UC-Irvine  
Note: In the Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, San Francisco, California, June 17-19, 1992, pages 249-260. Copyright c fl1992 ACM (see notice below). Copyright c fl1992 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept., ACM Inc.,  
Abstract: ions for Recursive Pointer Data Structures: Abstract Even though impressive progress has been made in the area of optimizing and parallelizing programs with arrays, the application of similar techniques to programs with pointer data structures has remained difficult. In this paper we introduce a new approach that leads to improved analysis and transformation of programs with recursively-defined pointer data structures. Our approach is based on a mechanism for the Abstract Description of Data Structures (ADDS), which makes explicit the important properties, such as dimensionality, of pointer data structures. Numerous examples demonstrate that ADDS definitions are both natural to specify and flexible enough to describe complex, cyclic pointer data structures. We discuss how an abstract data structure description can improve program analysis by presenting an analysis approach that combines an alias analysis technique, path matrix analysis, with information available from an ADDS declaration. Given this improved alias analysis technique, we provide a concrete example of applying a software pipelining transformation to loops involving pointer data structures. 
Abstract-found: 1
Intro-found: 1
Reference: [AK87] <author> Randy Allen and Ken Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4), </volume> <month> October </month> <year> 1987. </year>
Reference-contexts: A good deal of work has been done in the area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization <ref> [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90] </ref>, along with various instruction scheduling strategies such as software pipelining [RG82, AN88a, AN88b, Lam88, EN89]. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area.
Reference: [AN88a] <author> A. Aiken and A. Nicolau. </author> <title> Optimal Loop Paral-lelization. </title> <booktitle> In Proceedings of the SIGPLAN 1988 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 308-317, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90], along with various instruction scheduling strategies such as software pipelining <ref> [RG82, AN88a, AN88b, Lam88, EN89] </ref>. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area. <p> For example, a simple loop to initialize every node of a linked-list showed 47% speedup on the MIPS architecture for a list of size 100 with 3-unrolling (see [HG92] for more details and timings). In this section we present an example of a more powerful loop transformation, software pipelining <ref> [RG82, AN88a, AN88b, Lam88, EN89] </ref>. Given the current trend towards machines supporting higher degrees of parallelism, e.g. VLIW and superscalar, this type of optimization offers larger speedups. Once again, consider the code fragment discussed in the previous subsection 5.1.2, which manipulates a two-way linked-list of points.
Reference: [AN88b] <author> A. Aiken and A. Nicolau. </author> <title> Perfect Pipelining: A new loop parallelization technique. </title> <booktitle> In Proceedings of the 1988 European Symposium on Programming. Springer Verlag Lecture Notes in Computer Science No.300, </booktitle> <month> March </month> <year> 1988. </year>
Reference-contexts: area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90], along with various instruction scheduling strategies such as software pipelining <ref> [RG82, AN88a, AN88b, Lam88, EN89] </ref>. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area. <p> For example, a simple loop to initialize every node of a linked-list showed 47% speedup on the MIPS architecture for a list of size 100 with 3-unrolling (see [HG92] for more details and timings). In this section we present an example of a more powerful loop transformation, software pipelining <ref> [RG82, AN88a, AN88b, Lam88, EN89] </ref>. Given the current trend towards machines supporting higher degrees of parallelism, e.g. VLIW and superscalar, this type of optimization offers larger speedups. Once again, consider the code fragment discussed in the previous subsection 5.1.2, which manipulates a two-way linked-list of points.
Reference: [App85] <author> Andrew W. Appel. </author> <title> An Efficient Program for Many-Body Simulation. </title> <journal> SIAM J. Sci. Stat. Com-put., </journal> <volume> 6(1) </volume> <pages> 85-103, </pages> <year> 1985. </year> <month> 259 </month>
Reference-contexts: This is problematic, since numerous data structures in imperative programs|linked-lists and trees for example|are typically built using recursively-defined pointer data structures. Furthermore, such data structures are used not only in symbolic processing, but also in some classes of scientific codes (e.g. computational geometry [Sam90] and the so-called tree-codes <ref> [App85, BH86] </ref>).
Reference: [ASU87] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ull--man. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: A good deal of work has been done in the area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization <ref> [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90] </ref>, along with various instruction scheduling strategies such as software pipelining [RG82, AN88a, AN88b, Lam88, EN89]. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area.
Reference: [BH86] <author> Josh Barnes and Piet Hut. </author> <title> A Hierarchical O(NlogN) Force-Calculation Algorithm. </title> <journal> Nature, </journal> <volume> 324 </volume> <pages> 446-449, </pages> <month> 4 December </month> <year> 1986. </year> <title> The code can be obtained from Prof. </title> <institution> Barnes at the University of Hawaii. </institution>
Reference-contexts: This is problematic, since numerous data structures in imperative programs|linked-lists and trees for example|are typically built using recursively-defined pointer data structures. Furthermore, such data structures are used not only in symbolic processing, but also in some classes of scientific codes (e.g. computational geometry [Sam90] and the so-called tree-codes <ref> [App85, BH86] </ref>).
Reference: [CON] <author> CONVEX Computer Corporation. </author> <title> CONVEX C and FORTRAN Language Reference Manuals. </title> <year> 1990. </year>
Reference-contexts: However, code annotations are often difficult to use, due to the varied kinds of information that must be conveyed to the compiler. For example, the programmer may need to specify the data dependencies [Lar89], the transformation to apply <ref> [CON] </ref>, or the "distinctness" of data [KKK90]. <p> Also, the compiler typically takes these annotations on blind faith, and so is unable to warn the programmer if a coding change invalidates an annotation; this is true for <ref> [Lar89, CON] </ref>, 250 while in [KKK90] it was suggested that the system could perform dynamic checks provided that the programmer uniquely "tags" each node.
Reference: [CWZ90] <author> D.R. Chase, M. Wegman, and F.K. Zadek. </author> <title> Analysis of pointers and structures. </title> <booktitle> In Proceedings of the SIGPLAN `90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 296-310, </pages> <year> 1990. </year>
Reference-contexts: One class of solutions has been the development of advanced alias analysis techniques (also called structure estimation techniques) that attempt to statically approximate dynamically-allocated data structures with some abstraction. The most commonly used abstraction has been k-limited graphs [JM81], and variations on k-limited graphs <ref> [LH88a, LH88b, HPR89, CWZ90] </ref>. The major disadvantage of these techniques is that the approximation introduces cycles in the abstraction, and thus list-like or tree-like data structures cannot be distinguished from data structures that truly contain cycles. <p> The major disadvantage of these techniques is that the approximation introduces cycles in the abstraction, and thus list-like or tree-like data structures cannot be distinguished from data structures that truly contain cycles. The work by Chase et al. <ref> [CWZ90] </ref> has addressed this problem to some degree; however, their method fails to find accurate structure estimates in the presence of general recursion. This is a serious drawback since programs with recursively-defined data structures often use recursion as well.
Reference: [DH79] <author> J.J. Dongarra and A.R. Hinds. </author> <title> Unrolling loops in FORTRAN. </title> <journal> Software-Practice and Experience, </journal> <volume> 9 </volume> <pages> 219-226, </pages> <year> 1979. </year>
Reference-contexts: A good deal of work has been done in the area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization <ref> [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90] </ref>, along with various instruction scheduling strategies such as software pipelining [RG82, AN88a, AN88b, Lam88, EN89]. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area. <p> When transforming code that operates on a data structure, loop transformations typically require the structure to exhibit list-like properties, while coarse-grain transformations typically require tree-like properties. Such properties are all expressible using ADDS. Earlier work [HG92] has shown the applicability of loop unrolling <ref> [DH79] </ref> on scalar architectures. For example, a simple loop to initialize every node of a linked-list showed 47% speedup on the MIPS architecture for a list of size 100 with 3-unrolling (see [HG92] for more details and timings).
Reference: [EN89] <author> Kemal Ebcioglu and Toshio Nakatani. </author> <title> A New Compilation Technique for Parallelizing Loops Loops with Unpredictable Branches on a VLIW Architecture. </title> <booktitle> In Proceedings of the Second Workshop on Programming Languages and Compilers for Parallel Computing, Research Monographs in Parallel and Distributed Computing. </booktitle> <publisher> MIT-Press, </publisher> <year> 1989. </year>
Reference-contexts: area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90], along with various instruction scheduling strategies such as software pipelining <ref> [RG82, AN88a, AN88b, Lam88, EN89] </ref>. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area. <p> For example, a simple loop to initialize every node of a linked-list showed 47% speedup on the MIPS architecture for a list of size 100 with 3-unrolling (see [HG92] for more details and timings). In this section we present an example of a more powerful loop transformation, software pipelining <ref> [RG82, AN88a, AN88b, Lam88, EN89] </ref>. Given the current trend towards machines supporting higher degrees of parallelism, e.g. VLIW and superscalar, this type of optimization offers larger speedups. Once again, consider the code fragment discussed in the previous subsection 5.1.2, which manipulates a two-way linked-list of points.
Reference: [Gro91] <author> Josef Grosch. </author> <title> Tool support for data structures. </title> <journal> Structured Programming, </journal> <volume> 12(1) </volume> <pages> 31-38, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: It should be noted that a programming language and its compilers could directly support data structures such as TwoWayLL and PBinTree via predefined, abstract data types (e.g. see <ref> [Gro91] </ref> for a tool that generates, from a context-free grammar description, ADTs for graph-like structures, and [Sol90] for an implementation of parallelizable lists). However, a quick survey of the literature (or a data structures text [Sta80]) would reveal a wide variety of important pointer data structures.
Reference: [Gua88] <author> Vincent A. Guarna Jr. </author> <title> A technique for analyzing pointer and structure references in parallel restructuring compilers. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 212-220, </pages> <year> 1988. </year>
Reference-contexts: However, it has the disadvantage that it cannot handle cyclic structures (even if the cyclic nature would not hamper optimizing or parallelizing transformations). Related approaches include those based on more traditional dependence analysis (e.g. <ref> [Gua88] </ref>, which assumes that structures do not have cycles) and abstract interpretation techniques (e.g.
Reference: [Har89] <author> W. Ludwell Harrison III. </author> <title> The interprocedural analysis and automatic parallelization of scheme programs. </title> <journal> Lisp and Symbolic Computation, </journal> 2(3/4):179-396, 1989. 
Reference-contexts: Related approaches include those based on more traditional dependence analysis (e.g. [Gua88], which assumes that structures do not have cycles) and abstract interpretation techniques (e.g. Harri-son in <ref> [Har89] </ref> presents a technique designed for list-like structures commonly used in Scheme programs, while more recently he has been developing a uniform mechanism for handling both symbolic data and arrays using the notion of generalized iteration space [Har91]).
Reference: [Har91] <author> W. Ludwell Harrison III. </author> <title> Generalized iteration space and the parallelization of symbolic programs. </title> <editor> In Ian Foster and Evan Tick, editors, </editor> <booktitle> Proceedings of the Workshop on Computation of Symbolic Languages for Parallel Computers. </booktitle> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1991. </year> <month> ANL-91/34. </month>
Reference-contexts: Harri-son in [Har89] presents a technique designed for list-like structures commonly used in Scheme programs, while more recently he has been developing a uniform mechanism for handling both symbolic data and arrays using the notion of generalized iteration space <ref> [Har91] </ref>). In general, even though each of these methods work for certain classes of pointer data structures, they undoubtably fail in the presence of general, possibly cyclic structures and recursion. Code annotations represent a compromise, since the programmer can specify what the compiler cannot determine.
Reference: [Hen90] <author> Laurie J. Hendren. </author> <title> Parallelizing Programs with Recursive Data Structures. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <month> April </month> <year> 1990. </year> <type> TR 90-1114. </type>
Reference-contexts: This is a serious drawback since programs with recursively-defined data structures often use recursion as well. Another method, path matrix analysis, was designed to specifically deal with distinguishing tree-like data structures from DAG-like (shared) and graph-like (cyclic) structures <ref> [HN90, Hen90] </ref>. This analysis uses the special properties exhibited by treelike structures to provide a more accurate analysis of list-like and tree-like structures even in the presence of recursion. <p> Our approach to the analysis problem is a combined one, in which safe analysis techniques are used in conjunction with ADDS declarations. In particular, we are developing an approach to the static analysis of ADDS data structures that is an extension of path matrix analysis <ref> [HN90, Hen90] </ref>, called general path matrix analysis. Path matrix analysis was originally designed to automatically discover and exploit the properties of acyclic data structures. With the help of ADDS, general path matrix analysis is capable of handling cyclic data structures as well.
Reference: [HG92] <author> Laurie J. Hendren and Guang R. Gao. </author> <title> Designing Programming Languages for Analyzability: A Fresh Look at Pointer Data Structures. </title> <note> In Proceedings of the 4th IEEE International Conference on Computer Languages (to appear, also available as ACAPS Technical Memo 28, </note> <institution> McGill University), </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: This implies information must be collected and maintained at run-time, which is not considered in this paper. 3.2 Speculative Traversability In all cases, a data structure declared using ADDS is required to be speculatively traversable <ref> [HG92] </ref>. This property allows one to traverse past the "end" of a data structure without causing a run-time error. <p> This will clearly aid in fine-grain transformations where dependency analysis is crucial. When transforming code that operates on a data structure, loop transformations typically require the structure to exhibit list-like properties, while coarse-grain transformations typically require tree-like properties. Such properties are all expressible using ADDS. Earlier work <ref> [HG92] </ref> has shown the applicability of loop unrolling [DH79] on scalar architectures. For example, a simple loop to initialize every node of a linked-list showed 47% speedup on the MIPS architecture for a list of size 100 with 3-unrolling (see [HG92] for more details and timings). <p> Such properties are all expressible using ADDS. Earlier work <ref> [HG92] </ref> has shown the applicability of loop unrolling [DH79] on scalar architectures. For example, a simple loop to initialize every node of a linked-list showed 47% speedup on the MIPS architecture for a list of size 100 with 3-unrolling (see [HG92] for more details and timings). In this section we present an example of a more powerful loop transformation, software pipelining [RG82, AN88a, AN88b, Lam88, EN89]. Given the current trend towards machines supporting higher degrees of parallelism, e.g. VLIW and superscalar, this type of optimization offers larger speedups.
Reference: [HN90] <author> Laurie J. Hendren and Alexandru Nicolau. </author> <title> Paral-lelizing Programs with Recursive Data Structures. </title> <journal> IEEE Trans. on Parallel and Distributed Computing, </journal> <volume> 1(1) </volume> <pages> 35-47, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This is a serious drawback since programs with recursively-defined data structures often use recursion as well. Another method, path matrix analysis, was designed to specifically deal with distinguishing tree-like data structures from DAG-like (shared) and graph-like (cyclic) structures <ref> [HN90, Hen90] </ref>. This analysis uses the special properties exhibited by treelike structures to provide a more accurate analysis of list-like and tree-like structures even in the presence of recursion. <p> Our approach to the analysis problem is a combined one, in which safe analysis techniques are used in conjunction with ADDS declarations. In particular, we are developing an approach to the static analysis of ADDS data structures that is an extension of path matrix analysis <ref> [HN90, Hen90] </ref>, called general path matrix analysis. Path matrix analysis was originally designed to automatically discover and exploit the properties of acyclic data structures. With the help of ADDS, general path matrix analysis is capable of handling cyclic data structures as well.
Reference: [HPR89] <author> Susan Horwitz, Phil Pfeiffer, and Thomas Reps. </author> <title> Dependence analysis for pointer variables. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 28-40, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: One class of solutions has been the development of advanced alias analysis techniques (also called structure estimation techniques) that attempt to statically approximate dynamically-allocated data structures with some abstraction. The most commonly used abstraction has been k-limited graphs [JM81], and variations on k-limited graphs <ref> [LH88a, LH88b, HPR89, CWZ90] </ref>. The major disadvantage of these techniques is that the approximation introduces cycles in the abstraction, and thus list-like or tree-like data structures cannot be distinguished from data structures that truly contain cycles.
Reference: [JM81] <author> N. D. Jones and S. S. Muchnick. </author> <title> Program Flow Analysis, Theory and Applications, chapter 4, </title> <journal> Flow Analysis and Optimization of LISP-like Structures, </journal> <pages> pages 102-131. </pages> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: One class of solutions has been the development of advanced alias analysis techniques (also called structure estimation techniques) that attempt to statically approximate dynamically-allocated data structures with some abstraction. The most commonly used abstraction has been k-limited graphs <ref> [JM81] </ref>, and variations on k-limited graphs [LH88a, LH88b, HPR89, CWZ90]. The major disadvantage of these techniques is that the approximation introduces cycles in the abstraction, and thus list-like or tree-like data structures cannot be distinguished from data structures that truly contain cycles.
Reference: [KKK90] <author> David Klappholz, Apostolos D. Kallis, and Xi-angyun Kang. </author> <title> Refined C: An Update. </title> <editor> In David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 331-357. </pages> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: However, code annotations are often difficult to use, due to the varied kinds of information that must be conveyed to the compiler. For example, the programmer may need to specify the data dependencies [Lar89], the transformation to apply [CON], or the "distinctness" of data <ref> [KKK90] </ref>. Also, the compiler typically takes these annotations on blind faith, and so is unable to warn the programmer if a coding change invalidates an annotation; this is true for [Lar89, CON], 250 while in [KKK90] it was suggested that the system could perform dynamic checks provided that the programmer uniquely <p> specify the data dependencies [Lar89], the transformation to apply [CON], or the "distinctness" of data <ref> [KKK90] </ref>. Also, the compiler typically takes these annotations on blind faith, and so is unable to warn the programmer if a coding change invalidates an annotation; this is true for [Lar89, CON], 250 while in [KKK90] it was suggested that the system could perform dynamic checks provided that the programmer uniquely "tags" each node.
Reference: [Kuc78] <author> D.J. Kuck. </author> <title> The Structure of Computers and Computations: Volume I. </title> <publisher> Wiley, </publisher> <year> 1978. </year>
Reference-contexts: A good deal of work has been done in the area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization <ref> [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90] </ref>, along with various instruction scheduling strategies such as software pipelining [RG82, AN88a, AN88b, Lam88, EN89]. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area.
Reference: [Lam88] <author> Monica Lam. </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines. </title> <booktitle> In Proceedings of the SIGPLAN 1988 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90], along with various instruction scheduling strategies such as software pipelining <ref> [RG82, AN88a, AN88b, Lam88, EN89] </ref>. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area. <p> For example, a simple loop to initialize every node of a linked-list showed 47% speedup on the MIPS architecture for a list of size 100 with 3-unrolling (see [HG92] for more details and timings). In this section we present an example of a more powerful loop transformation, software pipelining <ref> [RG82, AN88a, AN88b, Lam88, EN89] </ref>. Given the current trend towards machines supporting higher degrees of parallelism, e.g. VLIW and superscalar, this type of optimization offers larger speedups. Once again, consider the code fragment discussed in the previous subsection 5.1.2, which manipulates a two-way linked-list of points.
Reference: [Lar89] <author> James R. Larus. </author> <title> Restructuring Symbolic Programs for Concurrent Execution on Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of California, Berke-ley, </institution> <year> 1989. </year>
Reference-contexts: Code annotations represent a compromise, since the programmer can specify what the compiler cannot determine. However, code annotations are often difficult to use, due to the varied kinds of information that must be conveyed to the compiler. For example, the programmer may need to specify the data dependencies <ref> [Lar89] </ref>, the transformation to apply [CON], or the "distinctness" of data [KKK90]. <p> Also, the compiler typically takes these annotations on blind faith, and so is unable to warn the programmer if a coding change invalidates an annotation; this is true for <ref> [Lar89, CON] </ref>, 250 while in [KKK90] it was suggested that the system could perform dynamic checks provided that the programmer uniquely "tags" each node. <p> For example, consider the following two recursive type declarations: type BinTree - int data; BinTree *left; BinTree *right; -; type TwoWayLL - int data; TwoWayLL *next; TwoWayLL *prev; -; 2 It should be noted that Larus in <ref> [Lar89] </ref> discussed a similar approach for Lisp; however, his approach required code (not type) annotations, described only acyclic structures, and could not be used in code fragments which might modify the structure.
Reference: [LH88a] <author> James R. Larus and Paul N. Hilfinger. </author> <title> Detecting conflicts between structure accesses. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 21-34, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: One class of solutions has been the development of advanced alias analysis techniques (also called structure estimation techniques) that attempt to statically approximate dynamically-allocated data structures with some abstraction. The most commonly used abstraction has been k-limited graphs [JM81], and variations on k-limited graphs <ref> [LH88a, LH88b, HPR89, CWZ90] </ref>. The major disadvantage of these techniques is that the approximation introduces cycles in the abstraction, and thus list-like or tree-like data structures cannot be distinguished from data structures that truly contain cycles.
Reference: [LH88b] <author> James R. Larus and Paul N. Hilfinger. </author> <title> Restructuring Lisp programs for concurrent execution. </title> <booktitle> In Proceedings of the ACM/SIGPLAN PPEALS 1988 Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <pages> pages 100-110, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: One class of solutions has been the development of advanced alias analysis techniques (also called structure estimation techniques) that attempt to statically approximate dynamically-allocated data structures with some abstraction. The most commonly used abstraction has been k-limited graphs [JM81], and variations on k-limited graphs <ref> [LH88a, LH88b, HPR89, CWZ90] </ref>. The major disadvantage of these techniques is that the approximation introduces cycles in the abstraction, and thus list-like or tree-like data structures cannot be distinguished from data structures that truly contain cycles.
Reference: [Lov77] <author> D.B. Loveman. </author> <title> Program Improvement by Source-to-Source Transformation. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 121-145, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: A good deal of work has been done in the area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization <ref> [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90] </ref>, along with various instruction scheduling strategies such as software pipelining [RG82, AN88a, AN88b, Lam88, EN89]. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area.
Reference: [MR90] <author> Michael Metcalf and John Reid. </author> <title> Fortran 90 Explained. </title> <publisher> Oxford University Press, </publisher> <year> 1990. </year>
Reference-contexts: Note that Fortran 90 has taken a similar stance in its treatment of pointers to variables. Variables accessible through pointers must be explicitly declared as either pointers or targets <ref> [MR90] </ref>. This simple declaration greatly improves the accuracy of alias analysis in the presence of pointers. However, also note that the presence of a description mechanism such as ADDS is not enough by itself.
Reference: [NPW91] <author> A. Nicolau, R. Potasman, and H. Wang. </author> <title> Register Allocation, Renaming and their impact on Fine-grain Parallelism. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: As a result, the code exhibits a theoretical speedup of 5. Note that the copy statement S6 is removed as part of the pipelining process, via (enhanced) copy propagation <ref> [NPW91] </ref>. In general, software pipelining can lead to even larger speedups, depending on the characteristics of the loop body.
Reference: [PW86] <author> David A. Padua and Michael J. Wolfe. </author> <title> Advanced compiler optimization for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: A good deal of work has been done in the area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization <ref> [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90] </ref>, along with various instruction scheduling strategies such as software pipelining [RG82, AN88a, AN88b, Lam88, EN89]. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area.
Reference: [RG82] <author> B. R. Rau and C. D. Glaeser. </author> <title> Efficient Code Generation for Horizontal Architectures: </title> <booktitle> Compiler Techniques and Architectural Support. In Proceedings of the 9th Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1982. </year>
Reference-contexts: area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90], along with various instruction scheduling strategies such as software pipelining <ref> [RG82, AN88a, AN88b, Lam88, EN89] </ref>. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area. <p> For example, a simple loop to initialize every node of a linked-list showed 47% speedup on the MIPS architecture for a list of size 100 with 3-unrolling (see [HG92] for more details and timings). In this section we present an example of a more powerful loop transformation, software pipelining <ref> [RG82, AN88a, AN88b, Lam88, EN89] </ref>. Given the current trend towards machines supporting higher degrees of parallelism, e.g. VLIW and superscalar, this type of optimization offers larger speedups. Once again, consider the code fragment discussed in the previous subsection 5.1.2, which manipulates a two-way linked-list of points.
Reference: [Sam90] <author> Hanan Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: This is problematic, since numerous data structures in imperative programs|linked-lists and trees for example|are typically built using recursively-defined pointer data structures. Furthermore, such data structures are used not only in symbolic processing, but also in some classes of scientific codes (e.g. computational geometry <ref> [Sam90] </ref> and the so-called tree-codes [App85, BH86]). <p> where X || Y - int data; LoLs *across is uniquely forward along X; LoLs *back is backward along X; LoLs *down is uniquely forward along Y; LoLs *up is backward along Y; -; An interesting three-dimensional structure that has both dependent and independent dimensions is the two-dimensional range tree <ref> [Sam90] </ref>, used to answer queries such as "find all points within the interval x1: : : x2" or "find all points within the bounding rectangle (x1,y1) and (x2,y2)." As illustrated below, it is a binary tree of binary trees, where the leaves of each tree are linked together to form a
Reference: [Sol90] <author> Jon A. Solworth. </author> <title> The PARSEQ Project: An Interim Report. </title> <editor> In David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 490-510. </pages> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: It should be noted that a programming language and its compilers could directly support data structures such as TwoWayLL and PBinTree via predefined, abstract data types (e.g. see [Gro91] for a tool that generates, from a context-free grammar description, ADTs for graph-like structures, and <ref> [Sol90] </ref> for an implementation of parallelizable lists). However, a quick survey of the literature (or a data structures text [Sta80]) would reveal a wide variety of important pointer data structures. Implementations for these structures can differ widely as well.
Reference: [Sta80] <author> Thomas A. Standish. </author> <title> Data Structure Techniques. </title> <publisher> Addison-Wesley, </publisher> <year> 1980. </year>
Reference-contexts: The flexibility of ADDS is illustrated by more exotic recursive pointer data structures. Typically such structures exhibit multiple dimensions, where dimensions are either "independent" (disjoint) or "dependent." For example, an orthogonal list <ref> [Sta80] </ref>, used to implement sparse matrices, has two dependent dimensions X and Y (much like the two-dimensional array it represents): We say X and Y are dependent since one traversal along X and another traversal along Y may lead to a common node or substructure. <p> However, a quick survey of the literature (or a data structures text <ref> [Sta80] </ref>) would reveal a wide variety of important pointer data structures. Implementations for these structures can differ widely as well.
Reference: [ZC90] <author> Hans Zima and Barbara Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: A good deal of work has been done in the area of analysis and transformation in the presence of arrays and loops, and as a result numerous techniques have been developed; for example, invariant code motion, induction variable elimination, loop unrolling, and vectorization <ref> [Lov77, Kuc78, DH79, PW86, AK87, ASU87, ZC90] </ref>, along with various instruction scheduling strategies such as software pipelining [RG82, AN88a, AN88b, Lam88, EN89]. Unfortunately, codes that utilize dynamically-allocated pointer data structures are much more difficult to analyze, and therefore there has not been as much progress in this area.
References-found: 34

