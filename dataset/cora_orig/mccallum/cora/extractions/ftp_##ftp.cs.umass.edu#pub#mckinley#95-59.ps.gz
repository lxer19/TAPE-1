URL: ftp://ftp.cs.umass.edu/pub/mckinley/95-59.ps.gz
Refering-URL: http://celestial.cs.umass.edu/~mckinley/papers.html
Root-URL: 
Email: weemsg@cs.umass.edu  
Phone: (413) 545-1249 (fax)  
Title: Compiling for Heterogeneous Systems: A Survey and an Approach  
Author: Kathryn S. McKinley, J. Eliot B. Moss, Sharad K. Singhai, Glen E. Weaver, Charles C. Weems fmckinley, moss, singhai, weaver, 
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Date: September 1995  
Pubnum: CMPSCI Techincal Report 95-59  
Abstract: Large applications tend to contain several models of parallelism, but only a few of these map efficiently to the single model of parallelism embodied in a homogeneous parallel system. Heterogeneous parallel systems incorporate diverse models of parallelism within a single machine or across machines. These systems are already pervasive in industrial and academic settings and offer a wealth of underutilized resources for achieving high performance. Unfortunately, heterogeneity complicates software development. We believe that compilers can and should assist in managing this complexity. We identify four goals for extending compilers to assist with managing heterogeneity: exploiting available resources, targeting changing resources, adjusting optimization to suit a target, and allowing programming models and languages to evolve. These goals do not require changes to the individual pieces of a compiler so much as a restructuring of a compiler's software architecture to increase its flexibility. We examine the features and flexibility of six important parallelizing compilers to find existing solutions for flexibility. Where no solutions exist, we propose architectural changes to compilers. 
Abstract-found: 1
Intro-found: 1
Reference: [AALL93] <author> S. Amarasinghe, J. Anderson, M. Lam, and A. Lim. </author> <title> An overview of a compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: It also lacks many of the sophisticated analyses, optimizations, and transformations because of its compiler framework nature. 2.5 SUIF Stanford University Intermediate Format (SUIF) is a compiler framework designed for research in compilation techniques, especially automatic parallelization <ref> [W + 94, AALL93, Sta94] </ref>. SUIF functions as either a source-to-C translator or a native code compiler. SUIF has been used to study parallelization for both shared-memory and distributed shared-memory machines [W + 94].
Reference: [B + 93] <author> F. Bodin et al. </author> <title> Distributed pC++: Basic ideas for an object parallel language. </title> <journal> Scientific Programming, </journal> <volume> 2(3), </volume> <month> Fall </month> <year> 1993. </year>
Reference-contexts: Using Sage++, a compiler writer can implement tests for legality and a richer set of transformations and optimizations. The compiler writer is responsible for writing the code to update the dependence graph after program changes. Applications of Sage++ Sage++ has been used to support new language extensions <ref> [BPMG94, Y + 94, M + 94, B + 93] </ref>. Sage++ is useful not only for building source-to-source translators but also other tools that depend on programs as input. Some traditional uses of Sage++ include optimizing expressions in scientific library code, instrumenting user code, and preprocessing user annotations in Fortran-S.
Reference: [B + 94] <author> F. Bodin et al. Sage++: </author> <title> An object-oriented toolkit and class library for building Fortran and C++ restructuring tools. </title> <booktitle> In Second Object-Oriented Numerics Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Modifying the IR to support additional languages may require changes to the consistency checks and analysis routines, but adding new nodes for an existing language should be easy because of Polaris's object-oriented design. 2.4 Sage++ Sage++ from Indiana University is a toolkit for building source-to-source translators <ref> [B + 94] </ref>. The authors foresee optimizing translators, simulation of language extensions, language preprocessors, and code instrumentation as possible applications of Sage++. Sage++ is written in C++ and provides parses for C, C++, pC++, Fortran 77, and Fortran 90.
Reference: [B + 95] <author> W. Blume et al. </author> <title> Effective Automatic Parallelization with Polaris. </title> <journal> International Journal of Parallel Programming, </journal> <month> May </month> <year> 1995. </year>
Reference-contexts: Changing the IR to support new languages or models of parallelism requires modifications throughout ParaScope. The Fortran-D compiler has a monolithic structure which makes it difficult to extend. 2.3 Polaris Polaris from the University of Illinois, is an optimizing source-to-source translator <ref> [B + 95, P + 93, F + 94] </ref>. The authors goal for Polaris is automatic parallelization of sequential Fortran programs for a variety of architectures. The authors have thus far focused on parallelization for shared-memory machines. Polaris is written in C++, and it compiles Fortran 77.
Reference: [BE94] <author> W. Blume and R. Eigenmann. </author> <title> The range test: A dependence test for symbolic, non-linear expressions. </title> <type> CSRD 1345, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Instead of using traditional dependence analysis, Po-laris builds symbolic lower and upper bounds for variable references and propagates these ranges as needed throughout the program using a demand driven algorithm [BE95b, BE95a]. Polaris's range test then uses these ranges to disprove dependences <ref> [BE94] </ref>. For optimization, Polaris includes scalar and array privatiza-tion, induction variable substitution, and reduction recognition and replacement.
Reference: [BE95a] <author> Bill Blume and Rudolf Eigenmann. </author> <title> Demand-driven, symbolic range propagation. </title> <booktitle> Proceedings of the Eighth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: In their experience, auto-inlining is sufficient for Polaris to carry out flow-sensitive analysis [Gro95a, Gro95b]. Instead of using traditional dependence analysis, Po-laris builds symbolic lower and upper bounds for variable references and propagates these ranges as needed throughout the program using a demand driven algorithm <ref> [BE95b, BE95a] </ref>. Polaris's range test then uses these ranges to disprove dependences [BE94]. For optimization, Polaris includes scalar and array privatiza-tion, induction variable substitution, and reduction recognition and replacement.
Reference: [BE95b] <author> William Blume and Rudolf Eigenmann. </author> <title> Symbolic range propagation. </title> <booktitle> Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: In their experience, auto-inlining is sufficient for Polaris to carry out flow-sensitive analysis [Gro95a, Gro95b]. Instead of using traditional dependence analysis, Po-laris builds symbolic lower and upper bounds for variable references and propagates these ranges as needed throughout the program using a demand driven algorithm <ref> [BE95b, BE95a] </ref>. Polaris's range test then uses these ranges to disprove dependences [BE94]. For optimization, Polaris includes scalar and array privatiza-tion, induction variable substitution, and reduction recognition and replacement.
Reference: [BHMM94] <author> D. Brown, S. Hackstadt, A. Malony, and B. Mohr. </author> <title> Program analysis environments for parallel language systems: the TAU environment. </title> <booktitle> In Proceedings of the 2nd Workshop on Environments and Tools For Parallel Scientific Computing, </booktitle> <pages> pages 162-171, </pages> <address> Townsend, Tennessee, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: But, Sage++ has also been used to construct a suit 7 of programming environment tools: tuning and analysis utility (TAU), file and class display (Fancy), call graph extended display (Cagey), class hierarchy browser (Classy), routine and data access profile display (Racy) and event and state display (Easy) <ref> [MBM94, BHMM94] </ref>. Summary Sage++ is a convenient tool for building source-to-source translators. It is not tied to any particular hardware architecture which makes it portable, and it provides basic routines needed by compilers.
Reference: [BKK94] <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <booktitle> In International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <pages> pages 111-122, </pages> <address> Montreal, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The Fortran-D compiler adds communication transformations such 5 as message vectorization, communication selection, message coalescing, message aggregation, and message pipelining. In addition, recent work addresses automatic data partitioning <ref> [BKK94] </ref>. Summary ParaScope is a mature system for automatic and interactive parallelization which provides a wide selection of analyses, optimizations, and transformations. As a source-to-source translator, it is not tightly coupled to any particular machine.
Reference: [BL94] <author> R. Butler and E. Lusk. </author> <title> Monitors, messages, and clusters: the p4 parallel programming system. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 547-564, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 <ref> [BL94] </ref>, and MPI [Mes94]. Heterogeneous processing [SC92, Tur93, Tur95, KPSW93, GY93] is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93].
Reference: [BPMG94] <author> F. Bodin, T. Priol, P. Mehrotra, and D. Gannon. </author> <title> Directions in parallel programming: HPF, shared virtual memory and object parallelism in pC++. </title> <type> Technical Report 94-54, </type> <institution> ICASE, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Using Sage++, a compiler writer can implement tests for legality and a richer set of transformations and optimizations. The compiler writer is responsible for writing the code to update the dependence graph after program changes. Applications of Sage++ Sage++ has been used to support new language extensions <ref> [BPMG94, Y + 94, M + 94, B + 93] </ref>. Sage++ is useful not only for building source-to-source translators but also other tools that depend on programs as input. Some traditional uses of Sage++ include optimizing expressions in scientific library code, instrumenting user code, and preprocessing user annotations in Fortran-S.
Reference: [C + 93] <author> K. Cooper et al. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Third, Parafrase-2 tackles the automatic identification of control parallelism which, when combined with techniques for finding loop parallelism, gives the compiler more choices in transforming a program to match a particular target architecture. 2.2 ParaScope Rice University's ParaScope is an interactive parallel programming environment built around a source-to-source translator <ref> [C + 93, KMT93] </ref>. It provides sophisticated global program analysis and a rich set of program transformations. ParaScope is a research tool and has been specialized for several Fortran derivatives. This section concentrates on the Fortran-D version of the ParaScope called the D system.
Reference: [CMRZ94] <author> B. Chapman, P. Mehrotra, J. Van Rosendale, and H. Zima. </author> <title> A software architecture for multidisciplinary appications: Integrating task and data parallelism. </title> <type> Technical Report 94-18, </type> <institution> ICASE, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: In turn, many features of Vienna Fortran have been incorporated into HPF. VFCS is an interesting system because many other supporting tools have been developed besides the compiler. Some recent work addresses task and data parallelism in Vienna Fortran and HPF <ref> [CMRZ94, HHM + 94] </ref>.
Reference: [CMZ92a] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Though this flexibility facilitates exploration of optimization ordering for different architectures, SUIF neither assists in dynamically selecting passes nor provides guards against inappropriate orderings. 2.6 Vienna Fortran Compilation System (VFCS) The Vienna Fortran Compilation System (VFCS) from the University of Vienna is an interactive, source-to-source translator for Vienna Fortran <ref> [CMZ92a, CMZ92b, ZC93, ZCMM93] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. The Vienna Fortran language extends Fortran with constructs for distributing data across the processors of a distributed-memory multiprocessing system (DMMP).
Reference: [CMZ92b] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Vienna Fortran a Fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1992. </year>
Reference-contexts: Though this flexibility facilitates exploration of optimization ordering for different architectures, SUIF neither assists in dynamically selecting passes nor provides guards against inappropriate orderings. 2.6 Vienna Fortran Compilation System (VFCS) The Vienna Fortran Compilation System (VFCS) from the University of Vienna is an interactive, source-to-source translator for Vienna Fortran <ref> [CMZ92a, CMZ92b, ZC93, ZCMM93] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. The Vienna Fortran language extends Fortran with constructs for distributing data across the processors of a distributed-memory multiprocessing system (DMMP).
Reference: [CPZ95] <author> B. Chapman, M. Pantano, and H. Zima. </author> <title> Supercompilers for massively parallel architectures. </title> <booktitle> In Aizu International Symposium on Parallel Algorithms/Architectures Synthesis (pAs '95), </booktitle> <pages> pages 315-322, </pages> <address> Aizu-Wakamatsu, Fukushima, Japan, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: SUIF and Polaris use a fixed ordering of transformations for each target, and therefore perform valid optimizations according to a predefined strategy. VFCS performs static performance measurement and dynamic performance measurements based upon external tools P 3 T [Fah94] and MEDEA <ref> [CPZ95] </ref> respectively, to determine profitability. Closely related to the profitability issue is ordering criteria. Optimizations applied in different orders may produce dramatically different results. In interactive mode, ParaScope, Parafrase-2, Polaris and VFCSallow the user to select any ordering of optimizations.
Reference: [EHLP91] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect benchmark programs. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: For example, headers for Fortran DO loops are represented by a subclass of Statement that has additional fields for items such as the end of the loop and the index variable. Optimizations The authors of Polaris performed a study in which they parallelized the Perfect Club benchmark by hand <ref> [EHLP91] </ref>. They found that a few new analyses and optimizations, in addition to ones already prevalent in commercially available parallelizing translators, would significantly improve a translator's ability to par-allelize code. For analysis, they added constant propagation, inlining (also used for optimization), and their own version of symbolic data dependence analysis.
Reference: [F + 90] <author> G. Fox et al. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: ParaScope is a research tool and has been specialized for several Fortran derivatives. This section concentrates on the Fortran-D version of the ParaScope called the D system. Fortran-D enhances Fortran 77 and 90 with data decomposition specifications <ref> [F + 90] </ref>. The output of the D System is an efficient message-passing distributed memory program [HKT91, HKT92, Tse93]. System architecture The D System accepts programs written in either a subset of Fortran 77D or Fortran 90D and converts them into an abstract syntax tree (AST) representation.
Reference: [F + 94] <author> K. Faigin et al. </author> <title> The polaris internal representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(5) </volume> <pages> 553-586, </pages> <month> Oct. </month> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Changing the IR to support new languages or models of parallelism requires modifications throughout ParaScope. The Fortran-D compiler has a monolithic structure which makes it difficult to extend. 2.3 Polaris Polaris from the University of Illinois, is an optimizing source-to-source translator <ref> [B + 95, P + 93, F + 94] </ref>. The authors goal for Polaris is automatic parallelization of sequential Fortran programs for a variety of architectures. The authors have thus far focused on parallelization for shared-memory machines. Polaris is written in C++, and it compiles Fortran 77.
Reference: [Fah94] <author> T. Fahringer. </author> <title> Using the P 3 T to guide the parallelization and optimization effort under the Vienna Fortran compilation system. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: However, dynamic data distribution is quite expensive and Vienna Fortran does not provide mechanisms to measure the trade-off between the cost of redistribution and the additional communication cost. Some recent work uses static performance measurements to guide optimization process <ref> [Fah94, FZ93] </ref>. Summary The design goal of Vienna Fortran is to generate optimized code for distributed-memory machines. Vienna Fortran borrows upon SUPERB, a predecessor of VFCS [ZBG88]. In turn, many features of Vienna Fortran have been incorporated into HPF. <p> SUIF and Polaris use a fixed ordering of transformations for each target, and therefore perform valid optimizations according to a predefined strategy. VFCS performs static performance measurement and dynamic performance measurements based upon external tools P 3 T <ref> [Fah94] </ref> and MEDEA [CPZ95] respectively, to determine profitability. Closely related to the profitability issue is ordering criteria. Optimizations applied in different orders may produce dramatically different results. In interactive mode, ParaScope, Parafrase-2, Polaris and VFCSallow the user to select any ordering of optimizations.
Reference: [FGMS93] <author> S. Feldman, D. Gay, M. Maimone, and N. Schryer. </author> <title> A Fortran-to-C converter. </title> <institution> Computing Science 149, AT&T Bell Laboratories, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: SUIF functions as either a source-to-C translator or a native code compiler. SUIF has been used to study parallelization for both shared-memory and distributed shared-memory machines [W + 94]. SUIF accepts source code written in either Fortran 77 or C, but a modified version of f2c <ref> [FGMS93] </ref> is used to convert Fortran code to C code. The modifications to f2c retain some Fortran specific information that further aids in analysis. System architecture SUIF has a flexible organization, with each analysis and optimization coded as a separate, independent pass.
Reference: [FZ93] <author> T. Fahringer and H. Zima. </author> <title> A static parameter based performance prediction tool for parallel programs. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: However, dynamic data distribution is quite expensive and Vienna Fortran does not provide mechanisms to measure the trade-off between the cost of redistribution and the additional communication cost. Some recent work uses static performance measurements to guide optimization process <ref> [Fah94, FZ93] </ref>. Summary The design goal of Vienna Fortran is to generate optimized code for distributed-memory machines. Vienna Fortran borrows upon SUPERB, a predecessor of VFCS [ZBG88]. In turn, many features of Vienna Fortran have been incorporated into HPF.
Reference: [GP94] <author> M. B. Girkar and C. Polychronopoulos. </author> <title> The hierarchical task graph as a universal intermediate representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(5), </volume> <year> 1994. </year>
Reference-contexts: This section describes the general approach, facilities for interacting with users, programming model, organization, intermediate representation, and optimizations and transformations of the six systems. Table 1 summarizes this information and Table 2 lists specific optimizations and transformations. 2.1 Parafrase-2 Parafrase-2 is a source-to-source translator from the University of Illinois <ref> [P + 89, GP94] </ref>. The goal of Para-frase-2 is to be a research tool for investigating compiler support for multiple languages and target architectures. Rather than try to build everything into Parafrase-2 from the beginning, the authors strive to make it flexible enough for later additions.
Reference: [Gro95a] <author> J. Grout. </author> <title> Inline expansion for the polaris research compiler. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1995. </year>
Reference-contexts: For analysis, they added constant propagation, inlining (also used for optimization), and their own version of symbolic data dependence analysis. Polaris performs auto-inlining, which is the automatic inlining of subroutines that are fifty lines or less. In their experience, auto-inlining is sufficient for Polaris to carry out flow-sensitive analysis <ref> [Gro95a, Gro95b] </ref>. Instead of using traditional dependence analysis, Po-laris builds symbolic lower and upper bounds for variable references and propagates these ranges as needed throughout the program using a demand driven algorithm [BE95b, BE95a]. Polaris's range test then uses these ranges to disprove dependences [BE94].
Reference: [Gro95b] <author> J. Grout. </author> <type> Personal communication, </type> <month> September </month> <year> 1995. </year>
Reference-contexts: The annotations tell the backend compiler where to apply specific transformations, but the backend compiler is free to perform additional transformations of its own (e.g., at least one of their backend compilers performs loop coallesc-ing <ref> [Gro95b] </ref>). For its internal IR, Polaris exploits the data abstraction of a C++'s class hierarchy to improve the quality of Polaris code. <p> For analysis, they added constant propagation, inlining (also used for optimization), and their own version of symbolic data dependence analysis. Polaris performs auto-inlining, which is the automatic inlining of subroutines that are fifty lines or less. In their experience, auto-inlining is sufficient for Polaris to carry out flow-sensitive analysis <ref> [Gro95a, Gro95b] </ref>. Instead of using traditional dependence analysis, Po-laris builds symbolic lower and upper bounds for variable references and propagates these ranges as needed throughout the program using a demand driven algorithm [BE95b, BE95a]. Polaris's range test then uses these ranges to disprove dependences [BE94].
Reference: [GY93] <author> A. Ghafoor and J. Yang. </author> <title> A distributed heterogeneous supercomputing management system. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 78-86, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [Mes94]. Heterogeneous processing <ref> [SC92, Tur93, Tur95, KPSW93, GY93] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [HHM + 94] <author> M. Haines, B. Hess, P. Mehrotra, J. Van Rosendale, and H. Zima. </author> <title> Runtime support for data parallel tasks. </title> <type> Technical Report 94-26, </type> <institution> ICASE, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: In turn, many features of Vienna Fortran have been incorporated into HPF. VFCS is an interesting system because many other supporting tools have been developed besides the compiler. Some recent work addresses task and data parallelism in Vienna Fortran and HPF <ref> [CMRZ94, HHM + 94] </ref>.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <type> Technical Report TR91-149, </type> <institution> Rice University, </institution> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: This section concentrates on the Fortran-D version of the ParaScope called the D system. Fortran-D enhances Fortran 77 and 90 with data decomposition specifications [F + 90]. The output of the D System is an efficient message-passing distributed memory program <ref> [HKT91, HKT92, Tse93] </ref>. System architecture The D System accepts programs written in either a subset of Fortran 77D or Fortran 90D and converts them into an abstract syntax tree (AST) representation. The Fortran-D back end uses loop transformations and communication optimizations to build efficient message-passing, SPMD node programs.
Reference: [HKT92] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This section concentrates on the Fortran-D version of the ParaScope called the D system. Fortran-D enhances Fortran 77 and 90 with data decomposition specifications [F + 90]. The output of the D System is an efficient message-passing distributed memory program <ref> [HKT91, HKT92, Tse93] </ref>. System architecture The D System accepts programs written in either a subset of Fortran 77D or Fortran 90D and converts them into an abstract syntax tree (AST) representation. The Fortran-D back end uses loop transformations and communication optimizations to build efficient message-passing, SPMD node programs.
Reference: [HMA95] <author> M. Hall, B. Murphy, and S. Amarasinghe. </author> <title> Interprocedural analysis for parallelization. </title> <booktitle> In Proceedings of the Eighth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: For analysis, SUIF can compute data dependence, control dependence, symbolic constants, and interprocedural information. The data dependence analyzer uses a suite of tests of varying complexity to obtain accurate results quickly. The interprocedural information is computed by an unusually extensive collection of analyses <ref> [HMA95] </ref>. SUIF structures its interprocedural analyses as a bottom-up pass that summarizes the behavior of each subroutine, followed by a top-down pass that applies calling contexts to each subroutine's summary description to compute its final analysis result. <p> Parafrase-2, ParaScope, Polaris, SUIF, and VFCS provide in-terprocedural analysis. Polaris recently incorporated interprocedural symbolic constant propagation [Pau95]. Parafrase-2, ParaScope, and VFCS [Zim95] perform flow-insensitive interprocedural analysis by summarizing where variables are referenced or modified. SUIF's FIAT tool provides a powerful framework for both flow-insensitive and flow-sensitive analysis <ref> [HMA95] </ref>. Optimizations A wide range of optimizations is supported by these compilers. Optimizations performed by uniprocessor compilers are termed as traditional. Except Sage++, all compilers provide traditional optimizations as listed in Table 2.
Reference: [KMCP93] <author> A. E. Klietz, A. V. Malevsky, and K. Chin-Purcell. </author> <title> A case study in metacomputing: Distributed simulations of mixing in turbulent convection. </title> <booktitle> In Workshop on Heterogeneous Processing, </booktitle> <pages> pages 101-106, </pages> <month> April </month> <year> 1993. </year>
Reference: [KMT93] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in an interactive parallel programming tool. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 5(7) </volume> <pages> 575-602, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Third, Parafrase-2 tackles the automatic identification of control parallelism which, when combined with techniques for finding loop parallelism, gives the compiler more choices in transforming a program to match a particular target architecture. 2.2 ParaScope Rice University's ParaScope is an interactive parallel programming environment built around a source-to-source translator <ref> [C + 93, KMT93] </ref>. It provides sophisticated global program analysis and a rich set of program transformations. ParaScope is a research tool and has been specialized for several Fortran derivatives. This section concentrates on the Fortran-D version of the ParaScope called the D system.
Reference: [KPSW93] <author> A. Khokhar, V. Prasanna, M. Shaaban, and C. Wang. </author> <title> Heterogeneous computing: Challenges and opportunities. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 18-27, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [Mes94]. Heterogeneous processing <ref> [SC92, Tur93, Tur95, KPSW93, GY93] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask. <p> A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [Mes94]. Heterogeneous processing [SC92, Tur93, Tur95, KPSW93, GY93] is the well-orchestrated use of heterogeneous hardware to execute a single application <ref> [KPSW93] </ref>. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [M + 94] <author> A. Malony et al. </author> <title> Performance analysis of pC++: A portable data-parallel programming system for scalable parallel computers. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: Using Sage++, a compiler writer can implement tests for legality and a richer set of transformations and optimizations. The compiler writer is responsible for writing the code to update the dependence graph after program changes. Applications of Sage++ Sage++ has been used to support new language extensions <ref> [BPMG94, Y + 94, M + 94, B + 93] </ref>. Sage++ is useful not only for building source-to-source translators but also other tools that depend on programs as input. Some traditional uses of Sage++ include optimizing expressions in scientific library code, instrumenting user code, and preprocessing user annotations in Fortran-S.
Reference: [MBM94] <author> B. Mohr, D. Brown, and A. Malony. </author> <title> TAU: A portable parallel program analysis environment for pC++. </title> <booktitle> In Proceedings of CONPAR 94 - VAPP VI, </booktitle> <institution> University of Linz, Austria, </institution> <month> September </month> <year> 1994. </year> <note> LNCS 854. </note>
Reference-contexts: But, Sage++ has also been used to construct a suit 7 of programming environment tools: tuning and analysis utility (TAU), file and class display (Fancy), call graph extended display (Cagey), class hierarchy browser (Classy), routine and data access profile display (Racy) and event and state display (Easy) <ref> [MBM94, BHMM94] </ref>. Summary Sage++ is a convenient tool for building source-to-source translators. It is not tied to any particular hardware architecture which makes it portable, and it provides basic routines needed by compilers.
Reference: [Mes94] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard, </title> <type> v1.0. Technical report, </type> <institution> University of Tennessee, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI <ref> [Mes94] </ref>. Heterogeneous processing [SC92, Tur93, Tur95, KPSW93, GY93] is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [P + 89] <author> C. Polychronopoulos et al. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1(1), </volume> <year> 1989. </year>
Reference-contexts: This section describes the general approach, facilities for interacting with users, programming model, organization, intermediate representation, and optimizations and transformations of the six systems. Table 1 summarizes this information and Table 2 lists specific optimizations and transformations. 2.1 Parafrase-2 Parafrase-2 is a source-to-source translator from the University of Illinois <ref> [P + 89, GP94] </ref>. The goal of Para-frase-2 is to be a research tool for investigating compiler support for multiple languages and target architectures. Rather than try to build everything into Parafrase-2 from the beginning, the authors strive to make it flexible enough for later additions.
Reference: [P + 93] <author> D. A. Padua et al. </author> <title> Polaris: A new-generation parallelizing compiler for MPPs. </title> <type> Technical Report CSRD-1306, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Changing the IR to support new languages or models of parallelism requires modifications throughout ParaScope. The Fortran-D compiler has a monolithic structure which makes it difficult to extend. 2.3 Polaris Polaris from the University of Illinois, is an optimizing source-to-source translator <ref> [B + 95, P + 93, F + 94] </ref>. The authors goal for Polaris is automatic parallelization of sequential Fortran programs for a variety of architectures. The authors have thus far focused on parallelization for shared-memory machines. Polaris is written in C++, and it compiles Fortran 77.
Reference: [Pau95] <author> D. A. Pauda. </author> <type> Personal communication, </type> <month> September </month> <year> 1995. </year>
Reference-contexts: VFCS provides intraprocedural irregular access pattern analysis based on PARTI routines [SCMB90]. Parafrase-2, ParaScope, Polaris, SUIF, and VFCS provide in-terprocedural analysis. Polaris recently incorporated interprocedural symbolic constant propagation <ref> [Pau95] </ref>. Parafrase-2, ParaScope, and VFCS [Zim95] perform flow-insensitive interprocedural analysis by summarizing where variables are referenced or modified. SUIF's FIAT tool provides a powerful framework for both flow-insensitive and flow-sensitive analysis [HMA95]. Optimizations A wide range of optimizations is supported by these compilers.
Reference: [PE95] <author> W. Pottenger and R. Eigenmann. </author> <title> Idiom recognition in the Polaris parallelizing compiler. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference: [SC92] <author> L. Smarr and C. E. Catlett. </author> <title> Metacomputing. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 45-52, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [Mes94]. Heterogeneous processing <ref> [SC92, Tur93, Tur95, KPSW93, GY93] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [SCMB90] <author> J. Saltz, K. Crowely, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: Optimizations work directly on the program database, and transformations are guided by an interactive kernel. Optimizations VFCS performs traditional data flow and data dependence analyses. It also has interprocedural communication and dynamic distribution analysis. It optimizes irregular access pattern using PARTI routines <ref> [SCMB90] </ref>. VFCS was specifically designed for distributed-memory machines and has an extensive set of communication optimizations besides loop optimizations. It performs overlap analysis to determine which non-local elements of a partitioned array are used in a processor and inserts communication primitives based on the analysis. <p> All the systems (except Sage++) perform intraprocedural symbolic analysis to support traditional optimizations, but ParaScope and Parafrase-2 have extensive interprocedural symbolic analysis such as forward propagation of symbolics. VFCS provides intraprocedural irregular access pattern analysis based on PARTI routines <ref> [SCMB90] </ref>. Parafrase-2, ParaScope, Polaris, SUIF, and VFCS provide in-terprocedural analysis. Polaris recently incorporated interprocedural symbolic constant propagation [Pau95]. Parafrase-2, ParaScope, and VFCS [Zim95] perform flow-insensitive interprocedural analysis by summarizing where variables are referenced or modified. SUIF's FIAT tool provides a powerful framework for both flow-insensitive and flow-sensitive analysis [HMA95].
Reference: [SGDM94] <author> V.S. Sunderam, G.A. Geist, J. Dongarra, and P. Manchek. </author> <title> The PVM concurrent computing system: Evolution, experiences, and trends. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 531-545, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM <ref> [SGDM94] </ref>, p4 [BL94], and MPI [Mes94]. Heterogeneous processing [SC92, Tur93, Tur95, KPSW93, GY93] is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93].
Reference: [Sta94] <author> Stanford Compiler Group. </author> <title> The SUIF library. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: It also lacks many of the sophisticated analyses, optimizations, and transformations because of its compiler framework nature. 2.5 SUIF Stanford University Intermediate Format (SUIF) is a compiler framework designed for research in compilation techniques, especially automatic parallelization <ref> [W + 94, AALL93, Sta94] </ref>. SUIF functions as either a source-to-C translator or a native code compiler. SUIF has been used to study parallelization for both shared-memory and distributed shared-memory machines [W + 94].
Reference: [Tse93] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: This section concentrates on the Fortran-D version of the ParaScope called the D system. Fortran-D enhances Fortran 77 and 90 with data decomposition specifications [F + 90]. The output of the D System is an efficient message-passing distributed memory program <ref> [HKT91, HKT92, Tse93] </ref>. System architecture The D System accepts programs written in either a subset of Fortran 77D or Fortran 90D and converts them into an abstract syntax tree (AST) representation. The Fortran-D back end uses loop transformations and communication optimizations to build efficient message-passing, SPMD node programs.
Reference: [Tur93] <author> L. H. Turcotte. </author> <title> A survey of software environments for exploiting networked computing resources. </title> <type> Technical Report MSSU-EIRS-ERC-93-2, </type> <institution> NSF Engineering Research Center, Mississippi State University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [Mes94]. Heterogeneous processing <ref> [SC92, Tur93, Tur95, KPSW93, GY93] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [Tur95] <author> L. H. Turcotte. </author> <title> Cluster computing. </title> <editor> In Albert Y. Zomaya, editor, </editor> <booktitle> Parallel and Distributed Computing Handbook, chapter 26. </booktitle> <publisher> McGraw-Hill, </publisher> <month> October </month> <year> 1995. </year> <month> 25 </month>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [Mes94]. Heterogeneous processing <ref> [SC92, Tur93, Tur95, KPSW93, GY93] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [W + 89] <author> C. Weems et al. </author> <title> The image understanding architecture. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(3) </volume> <pages> 251-282, </pages> <year> 1989. </year>
Reference-contexts: Tightly-coupled heterogeneous computers integrate different parallel This work was supported in part by the Advanced Research Projects Agency under contract N00014-94-1-0742, monitored by the Office of Naval Research. architectures together within a single machine (e.g., Meiko CS-2, IBM SP-2, and IUA <ref> [W + 89] </ref>). Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [Mes94].
Reference: [W + 94] <author> R. Wilson et al. </author> <title> The SUIF compiler system: A parallelizing and optimizing research compiler. </title> <journal> SIGPLAN, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: It also lacks many of the sophisticated analyses, optimizations, and transformations because of its compiler framework nature. 2.5 SUIF Stanford University Intermediate Format (SUIF) is a compiler framework designed for research in compilation techniques, especially automatic parallelization <ref> [W + 94, AALL93, Sta94] </ref>. SUIF functions as either a source-to-C translator or a native code compiler. SUIF has been used to study parallelization for both shared-memory and distributed shared-memory machines [W + 94]. <p> SUIF functions as either a source-to-C translator or a native code compiler. SUIF has been used to study parallelization for both shared-memory and distributed shared-memory machines <ref> [W + 94] </ref>. SUIF accepts source code written in either Fortran 77 or C, but a modified version of f2c [FGMS93] is used to convert Fortran code to C code. The modifications to f2c retain some Fortran specific information that further aids in analysis.
Reference: [WL91] <author> M. E. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: For optimization, SUIF has a wide assortment of traditional and loop optimizations (see Table 2), including unimodular loop transformations (i.e., interchange, reversal, and skewing) <ref> [WL91] </ref>. SUIF also has an extensive collection of interprocedural transformations: parallelization, data privatization, inlining, cloning, and reduction recognition. Lastly, SUIF includes a code generator which allows it to not only transform a sequential program to a parallel program, but also immediately generate assembly code for the parallel program.
Reference: [Y + 94] <author> S. Yang et al. </author> <title> High performance fortran interface to the parallel C++. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Using Sage++, a compiler writer can implement tests for legality and a richer set of transformations and optimizations. The compiler writer is responsible for writing the code to update the dependence graph after program changes. Applications of Sage++ Sage++ has been used to support new language extensions <ref> [BPMG94, Y + 94, M + 94, B + 93] </ref>. Sage++ is useful not only for building source-to-source translators but also other tools that depend on programs as input. Some traditional uses of Sage++ include optimizing expressions in scientific library code, instrumenting user code, and preprocessing user annotations in Fortran-S.
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: Some recent work uses static performance measurements to guide optimization process [Fah94, FZ93]. Summary The design goal of Vienna Fortran is to generate optimized code for distributed-memory machines. Vienna Fortran borrows upon SUPERB, a predecessor of VFCS <ref> [ZBG88] </ref>. In turn, many features of Vienna Fortran have been incorporated into HPF. VFCS is an interesting system because many other supporting tools have been developed besides the compiler. Some recent work addresses task and data parallelism in Vienna Fortran and HPF [CMRZ94, HHM + 94].
Reference: [ZC93] <author> H. Zima and B. Chapman. </author> <title> Compiling for distributed-memory systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 264-287, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Though this flexibility facilitates exploration of optimization ordering for different architectures, SUIF neither assists in dynamically selecting passes nor provides guards against inappropriate orderings. 2.6 Vienna Fortran Compilation System (VFCS) The Vienna Fortran Compilation System (VFCS) from the University of Vienna is an interactive, source-to-source translator for Vienna Fortran <ref> [CMZ92a, CMZ92b, ZC93, ZCMM93] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. The Vienna Fortran language extends Fortran with constructs for distributing data across the processors of a distributed-memory multiprocessing system (DMMP).
Reference: [ZCMM93] <author> H. Zima, B. Chapman, H. Moritsch, and P. Mehrotra. </author> <title> Dynamic data distributions in Vienna Fortran. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Though this flexibility facilitates exploration of optimization ordering for different architectures, SUIF neither assists in dynamically selecting passes nor provides guards against inappropriate orderings. 2.6 Vienna Fortran Compilation System (VFCS) The Vienna Fortran Compilation System (VFCS) from the University of Vienna is an interactive, source-to-source translator for Vienna Fortran <ref> [CMZ92a, CMZ92b, ZC93, ZCMM93] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. The Vienna Fortran language extends Fortran with constructs for distributing data across the processors of a distributed-memory multiprocessing system (DMMP).
Reference: [Zim95] <author> H. Zima. </author> <type> Personal communication, </type> <month> September </month> <year> 1995. </year> <month> 26 </month>
Reference-contexts: VFCS provides intraprocedural irregular access pattern analysis based on PARTI routines [SCMB90]. Parafrase-2, ParaScope, Polaris, SUIF, and VFCS provide in-terprocedural analysis. Polaris recently incorporated interprocedural symbolic constant propagation [Pau95]. Parafrase-2, ParaScope, and VFCS <ref> [Zim95] </ref> perform flow-insensitive interprocedural analysis by summarizing where variables are referenced or modified. SUIF's FIAT tool provides a powerful framework for both flow-insensitive and flow-sensitive analysis [HMA95]. Optimizations A wide range of optimizations is supported by these compilers. Optimizations performed by uniprocessor compilers are termed as traditional.
References-found: 55

