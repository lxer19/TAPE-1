URL: ftp://speech.cse.ogi.edu/pub/docs/fsj/deng.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/fsj/html/bak.html
Root-URL: http://www.cse.ogi.edu
Email: (deng@crg5.uwaterloo.ca)  
Title: Submitted to Free Speech Journal Speech Recognition Using Autosegmental Representation of Phonological Units with Interface
Author: Li Deng 
Address: Waterloo, Waterloo, Ontario, Canada  
Affiliation: Department of Electrical and Computer Engineering University of  
Abstract: A novel speech recognizer is described which capitalizes on multi-dimensional articulatory structures and incorporates key ideas from autosegmental phonology and articulatory phonology. The novelty has been in the design of the atomic units of speech so as to arrive at a unified and parsimonious way to account for the context-dependent behaviors of speech acoustics. At the heart of the recognizer is a procedure developed to automatically convert a probabilistic overlap pattern over five articulatory feature dimensions into a finite-state automaton which serves as the phonological construct of the recognizer. The phonetic-interface component of the recognizer, based on the nonstationary-state hidden Markov model, is also described. Some preliminary phonetic recognition results on the TIMIT database are reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Browman and L. Goldstein. </author> <title> "Articulatory phonology: An overview," </title> <journal> Phonetica, </journal> <volume> Vol.49, </volume> <pages> 155-180, </pages> <year> 1992. </year>
Reference-contexts: Articulatory phonology proposes that the relatively low-level articulatory process can be 5 used to account for many phonological rules <ref> [1] </ref>. The phonological component of the speech recognizer, called the overlapping articulatory feature model, described here is grounded on the articulatory mis-alignment model but the constraints on the mis-alignment are implemented carefully using the statistical means offered by the HMM-based interface component.
Reference: [2] <author> L. Deng. </author> <title> "A generalized hidden Markov model with state-conditioned trend functions of time for the speech signal," </title> <booktitle> Signal Processing, Vol.27, No.1, </booktitle> <pages> pp. 65-78, </pages> <year> 1992. </year>
Reference-contexts: The phonetic interface model will also be described, which is based on a rigorous formulation of the nonstationary-state or trended hidden Markov model (HMM) developed earlier <ref> [2, 4] </ref>. We shall emphasize the interplay between the design of the phonological model and that of the interface model according to the underlying physical relationship between the phonological units and their phonetic correlates (e.g., static or dynamic acoustic patterning). <p> On the other hand, if the assimilated feature (s) are all in the secondary articulatory feature dimension (s), then the corresponding feature bundle defines a static unit. 4.2 Model implementation The interface model has been implemented using the trended HMM theory <ref> [2] </ref>. For each of the transitional units or states, a nonstationary process consisting of a mixture of polynomial functions of time embedded in white noise is used to represent the acoustic trajectory of speech.
Reference: [3] <author> L. Deng and D. Sun. </author> <title> "A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features," </title> <journal> J. Acoust. Soc. Am., </journal> <volume> Vol. 95, </volume> <pages> pp. 2702-2719, </pages> <year> 1994. </year>
Reference-contexts: state construction (especially useful for multilingual speech recognition including Asian languages) is currently underway. 7 without constraints on the size of the vocabulary (American English). 3.3 Articulatory state construction At the heart of the recognizer is an algorithm for automatic conversion of any probabilistic and fractional articulatory feature overlap pattern <ref> [3] </ref> into a finite state automaton (or equivalently a Markov-state transition graph), which is described below. Let the phonemic transcription be f 1 ; f 2 ; ; f T for a speech utterance. <p> The process of mapping phonemic representation to the articulatory states (i.e. finite-state automata) involves the following four steps: (1) Map each phoneme f i to a context-independent composite articulatory state s i consisting of five-tupled feature values (See details in <ref> [3] </ref>).
Reference: [4] <author> L. Deng, M. Aksmanovic, D. Sun, and J. </author> <title> Wu "Speech recognition using hidden Markov models with polynomial regression functions as nonstationary states," </title> <journal> IEEE Trans. Speech Audio Proc., </journal> <volume> 2(4), </volume> <pages> 507-520, </pages> <year> 1994. </year>
Reference-contexts: The phonetic interface model will also be described, which is based on a rigorous formulation of the nonstationary-state or trended hidden Markov model (HMM) developed earlier <ref> [2, 4] </ref>. We shall emphasize the interplay between the design of the phonological model and that of the interface model according to the underlying physical relationship between the phonological units and their phonetic correlates (e.g., static or dynamic acoustic patterning).
Reference: [5] <author> L. Deng and H. Sameti. </author> <title> "Transitional speech units and their representation by the regressive Markov states: Applications to speech recognition," </title> <journal> IEEE Trans. Speech Audio Proc. </journal> <note> (to appear in July, </note> <year> 1996). </year>
Reference-contexts: The same TIMIT phonetic recognition task is used, except a smaller amount of training data (40 speakers only) and a new N-best list were used. (A significantly greater computation is required for the trended HMM with the polynomial order higher than zero; see further details in <ref> [5] </ref>.) Table 3 shows the top-one re-scored phonetic recognition results, agian in 15 No. Corr. Acc. Sub. Del. Ins. 1 77.57% 70.82% 7.22% 5.20% 6.75% 10 81.47% 74.35% 3.75% 4.77% 7.13% Table 2.
Reference: [6] <author> L. Deng and D. Braam. </author> <title> "Context-dependent Markov model structured by locus equations: Application to phonetic classification," </title> <journal> J. Acoust. Soc. Am., </journal> <volume> Vol. 96, </volume> <pages> pp. 2008-2025, </pages> <year> 1994. </year>
Reference-contexts: in the above speech recognition framework, let's decompose the word-to-acoustics probability P (OjW) in Eqn.(1) into P (OjW ) = F max F P (OjF )P (F jW ); (2) 1 This can either be speech waveforms [15], or continuous-valued acoustic vectors (such as cepstral vectors [16] or formant frequencies <ref> [6] </ref>), or discrete-valued vector-quantized codes [12]. 3 where F is a discrete-valued phonological construct and specifies, according to probabil-ity P (F jW ), how words and word sequences W can be expressed in terms of a particular organization of a small set of fundamental phonological units; P (OjF ) is the
Reference: [7] <author> J. A. Goldsmith. </author> <title> Autosegmental and Metrical Phonology, </title> <publisher> Blackwell: Oxford, </publisher> <year> 1990. </year>
Reference-contexts: That is, the different features characterizing a segment often do not take new values synchronously when moving to the next segment. Autosegmental phonology has been developed largely by recognizing such a failure <ref> [7] </ref>. Although some of such asynchronous feature changes occur at the planning stage of speech production, many of the changes arise from mis-timing (imperfect timing, mis-alignment, or overlapping) across different articulators due to purely physical reasons that can be explained without resort to further causes.
Reference: [8] <author> M. Huckvale. </author> <title> "Tiered segmentation of speech: Opportunities, methods, problems and challenges," in Speech, hearing, </title> <booktitle> and language: work in progress, </booktitle> <volume> Vol. </volume> <pages> 7, </pages> <address> University College London, </address> <pages> pp. 133-152. </pages>
Reference-contexts: We also note a related hierarchical approach to segmenting speech in <ref> [8] </ref>. 3 Overlapping-feature induced finite-state automata as the phonological model 3.1 Motivation and overview A major motivation of the phonological model constructed from overlapping articulatory features described in this section is the failure of what phonologists call the absolute slicing hypothesis.
Reference: [9] <author> B.H. Juang and L. R. Rabiner. </author> <title> "The segmental k-means algorithm for estimating parameters of hidden Markov models," </title> <journal> IEEE Trans. Speech Audio Proc., </journal> <volume> Vol. 38, pp.1639-1641, </volume> <year> 1990. </year>
Reference-contexts: An efficient algorithm has been developed for automatic learning of the parameters of the above interface model. The algorithm is motivated by and is an extension of the segmental K-means algorithm developed in the past for training conventional HMMs <ref> [9] </ref>.
Reference: [10] <author> J. Keyser and K. Stevens. </author> <title> "Feature geometry and the vocal tract," </title> <journal> Phonology, </journal> <volume> Vol. 11(2), </volume> <year> 1994, </year> <pages> pp. 207-236. </pages>
Reference-contexts: This design has been partially motivated by a modern version of the feature geometry theory that involves the critical notion of active articulator <ref> [10, 14] </ref>. This notion is connected to the view that for each consonant there is 11 one active articulator forming the main vocal tract constriction. The active articulator is defined from a general principle | the most anterior articulator below the supranasal node that dominates terminal features. <p> The active articulator is defined from a general principle | the most anterior articulator below the supranasal node that dominates terminal features. Under the supranasal node, there are three possible active articulatory features: lips, tongue blade, and tongue dorsum. According to the theory outlined in <ref> [10] </ref>, for a speech segment with the dominant node being supranasal, the acoustic correlates of the corresponding active articulatory features that characterize the segment are spread over a region near the time when there is a rapid spectral change, exhibiting a strongly dynamic pattern.
Reference: [11] <editor> N. Lass (ed.) </editor> <title> Principles of Experimental Phonetics, </title> <editor> Ed. N. Lass, Mosby: </editor> <address> London, </address> <year> 1995. </year>
Reference-contexts: We shall call this latter mapping device from a phonological organization to speech acoustics as the interface model. According to the phonetic theory (rf. <ref> [11] </ref>), the interface model ideally should consist of at least three hierarchical levels of mapping: from phonological symbols to motor commands (M), from motor commands to articulation (A), and from articulation to acoustics (O).
Reference: [12] <author> J. Mahkoul and R. </author> <title> Schwartz "State of the art in continuous speech recognition," </title> <journal> Proc. Natl. Acad. Sci. USA, </journal> <volume> Vol. 92, </volume> <pages> 9956-9963, </pages> <year> 1995. </year>
Reference-contexts: framework, let's decompose the word-to-acoustics probability P (OjW) in Eqn.(1) into P (OjW ) = F max F P (OjF )P (F jW ); (2) 1 This can either be speech waveforms [15], or continuous-valued acoustic vectors (such as cepstral vectors [16] or formant frequencies [6]), or discrete-valued vector-quantized codes <ref> [12] </ref>. 3 where F is a discrete-valued phonological construct and specifies, according to probabil-ity P (F jW ), how words and word sequences W can be expressed in terms of a particular organization of a small set of fundamental phonological units; P (OjF ) is the probability that a particular organization <p> consisting of linear phonetic sequences as the phonological model, and of the interface model established by one-level mapping from phonological symbols to acoustics; this one-level mapping is implemented very simply by the use of state-conditioned i.i.d. (independent and identically distributed) output distributions associated with the (stationary) states of the HMM <ref> [16, 12] </ref>. One important point to note is that in the conventional technology, the HMM as a one-level mapping device has been developed largely independent of the phonological model.
Reference: [13] <author> C. Pereira and M. Riley. </author> <title> "Speech recognition by composition of weighted finite automata," </title> <type> CMP-LG archive paper 9603001, </type> <year> 1996. </year>
Reference-contexts: Hence, at least theoretically, our one-level interface model (the trended HMMs, see Section 4) is a better approximation to the realistic multi-level hierarchy (from phonology to acoustics) than the conventional stationary-state HMM. A multi-level approach to speech recognition has also been proposed in <ref> [13] </ref>, where more gross levels (acoustics to phones to words and to sentences) of the speech representation were presented compared with the multi-level representation (acoustics to articulation to motor commands to features to words and to sentences) discussed in this section.
Reference: [14] <author> E. Sagey. </author> <title> "The representation of features and relations in nonlinear phonology," </title> <type> PH.D. dissertation, </type> <address> 1986, </address> <publisher> M.I.T., </publisher> <address> Cambridge, MA. </address>
Reference-contexts: This design has been partially motivated by a modern version of the feature geometry theory that involves the critical notion of active articulator <ref> [10, 14] </ref>. This notion is connected to the view that for each consonant there is 11 one active articulator forming the main vocal tract constriction. The active articulator is defined from a general principle | the most anterior articulator below the supranasal node that dominates terminal features.
Reference: [15] <author> H. Sheikhzadeh and L. </author> <title> Deng "Waveform-based speech recognition using hidden filter models: Parameter selection and sensitivity to power normalization," </title> <journal> IEEE Trans. Speech Audio Proc., </journal> <volume> 2, </volume> <pages> 80-91, </pages> <year> 1994. </year>
Reference-contexts: order to examine the distinct but related roles of phonological and interface models in the above speech recognition framework, let's decompose the word-to-acoustics probability P (OjW) in Eqn.(1) into P (OjW ) = F max F P (OjF )P (F jW ); (2) 1 This can either be speech waveforms <ref> [15] </ref>, or continuous-valued acoustic vectors (such as cepstral vectors [16] or formant frequencies [6]), or discrete-valued vector-quantized codes [12]. 3 where F is a discrete-valued phonological construct and specifies, according to probabil-ity P (F jW ), how words and word sequences W can be expressed in terms of a particular organization
Reference: [16] <author> S. </author> <title> Young "Large vocabulary continuous speech recognition: a review," </title> <booktitle> Proc. IEEE Workshop on Automatic Speech Recognition, Snowbird, Utah, </booktitle> <pages> 3-28, </pages> <year> 1995. </year> <month> 20 </month>
Reference-contexts: phonological and interface models in the above speech recognition framework, let's decompose the word-to-acoustics probability P (OjW) in Eqn.(1) into P (OjW ) = F max F P (OjF )P (F jW ); (2) 1 This can either be speech waveforms [15], or continuous-valued acoustic vectors (such as cepstral vectors <ref> [16] </ref> or formant frequencies [6]), or discrete-valued vector-quantized codes [12]. 3 where F is a discrete-valued phonological construct and specifies, according to probabil-ity P (F jW ), how words and word sequences W can be expressed in terms of a particular organization of a small set of fundamental phonological units; P <p> consisting of linear phonetic sequences as the phonological model, and of the interface model established by one-level mapping from phonological symbols to acoustics; this one-level mapping is implemented very simply by the use of state-conditioned i.i.d. (independent and identically distributed) output distributions associated with the (stationary) states of the HMM <ref> [16, 12] </ref>. One important point to note is that in the conventional technology, the HMM as a one-level mapping device has been developed largely independent of the phonological model.
References-found: 16

