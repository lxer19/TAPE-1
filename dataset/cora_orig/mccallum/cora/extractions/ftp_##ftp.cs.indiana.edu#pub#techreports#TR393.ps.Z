URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR393.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Title: CIMGS: An Incomplete Orthogonal Factorization Preconditioner  
Author: Xiaoge Wang Kyle Gallivan Randall Bramley 
Date: December 16, 1993  
Address: Bloomington  Illinois- Urbana  Bloomington  
Affiliation: Department of Computer Science Indiana University  Department of Electrical and Computer Engineering University of  Department of Computer Science Indiana University  
Abstract: A new preconditioner (called CIMGS) based on an incomplete orthogonal factorization is derived, analyzed, and tested. Although designed for preconditioning least squares problems, it is also applicable to more general symmetric positive definite matrices. CIMGS is robust both theoretically and empirically, existing (in exact arithmetic) for any full rank matrix. Numerically it is more robust than an incomplete Cholesky factorization preconditioner, and the conjugate gradient iterative method preconditioned with CIMGS compares favorably with using Cholesky factorization on the normal equations. Theoretical results show that the CIMGS factorization has better backward error properties than complete Cholesky factorization does, and for systems whose normal equations are M-matrices, CIMGS induces a regular splitting, better estimates the complete Cholesky factor R c as the set of dropped positions gets smaller, and lies between complete Cholesky factorization and incomplete Cholesky factorization in its approximation properties. Those properties usually hold numerically, even when A T A is not an M-matrix. When the drop set satisfies a mild and easily verified (or enforced) property, the upper triangular factor CIMGS generates is the same as the one incomplete Cholesky factorization does. This allows guaranteeing the existence of IC factorization, based solely on the target sparsity pattern. fl Work supported by NSF grants CDA-9309746 and CCR-9120105
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Ashby, </author> <title> Polynomial Preconditioning for Conjugate Gradient Methods, </title> <type> PhD thesis, </type> <institution> University of Illinois Urbana-Champaign, </institution> <year> 1987. </year> <note> Also available as Tech. Rep. 1355, </note> <institution> Department of Computer Science, University of Illinois - Urbana. </institution> <note> [2] , Minimax polynomial preconditioning for Hermitian linear systems, SIAM J. Mat. Anal. Appl., </note> <month> 12 </month> <year> (1991), </year> <pages> pp. 766-789. 26 </pages>
Reference-contexts: Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization [11], polynomial preconditioning <ref> [1, 2] </ref>, and incomplete orthogonal factorization [14, 10, 18, 16]. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1.
Reference: [3] <author> M. W. Berry and R. J. Plemmons, </author> <title> Algorithms and experiments for structural mechanics on high-performance architectrues, </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, 64 (1987), </booktitle> <pages> pp. 487-507. </pages>
Reference-contexts: Such problems occur frequently in scientific and engineering applications such as linear programming [5], augmented Lagrangian methods for CFD [12], and the natural factor method in partial differential equations [9] <ref> [3] </ref>. Minimizing (1) by solving the normal equations A T Ax = A T b is a common and often efficient approach, because A T A is symmetric and positive definite. There are many well developed and reliable methods, both direct and iterative, for solving such systems.
Reference: [4] <author> A. Bj orck, </author> <title> SSOR preconditioning methods for sparse least squares problems, </title> <booktitle> in Proceedings of the Computer Science and Statistics: 12-th Annual Symposium on the Interface, </booktitle> <editor> J. F. Gentleman, ed., </editor> <address> University of Waterloo, Waterloo, Ontario, Canada, </address> <month> May </month> <year> 1979, </year> <pages> pp. 21-25. </pages>
Reference-contexts: Because the rate of convergence of the CG algorithm is related to the condition number of the matrix that it is applied to, finding an effective preconditioner is crucial. Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR <ref> [4] </ref>, incomplete Cholesky factorization [11], polynomial preconditioning [1, 2], and incomplete orthogonal factorization [14, 10, 18, 16]. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1.
Reference: [5] <author> I. Chio, C. L. Monma, and D. Shanno, </author> <title> Further development of a primal-dual interior point method, </title> <journal> ORSA Journal on Computing, </journal> <volume> 2 (1990), </volume> <pages> pp. 304-311. </pages>
Reference-contexts: Such problems occur frequently in scientific and engineering applications such as linear programming <ref> [5] </ref>, augmented Lagrangian methods for CFD [12], and the natural factor method in partial differential equations [9] [3]. Minimizing (1) by solving the normal equations A T Ax = A T b is a common and often efficient approach, because A T A is symmetric and positive definite.
Reference: [6] <author> E. Chu, A. George, J. Liu, and E. Ng, SPARSPAK: </author> <title> Waterloo sparse matrix package, user's guide for SPARSPAK-A, </title> <type> Tech. Rep. </type> <institution> CS-84-36, Department of Computer Science, University of Waterloo, </institution> <year> 1984. </year>
Reference-contexts: The CIMGS and incomplete Cholesky factorizations used for comparison in the experiments were implemented in standard Fortran. The packages SPARSPAK-A <ref> [6] </ref> used for Cholesky factorization and SPARSPAK-B [7] are also in Fortran but benefit from the more careful consideration typical of a numerical software package. All tests were run on a single processor of an Alliant FX/2800.
Reference: [7] <author> A. George and E. Ng, SPARSPAK: </author> <title> Waterloo sparse matrix package, user's guide for SPARSPAK-B, </title> <type> Tech. Rep. </type> <institution> CS-84-37, Department of Computer Science, University of Waterloo, </institution> <year> 1984. </year>
Reference-contexts: The CIMGS and incomplete Cholesky factorizations used for comparison in the experiments were implemented in standard Fortran. The packages SPARSPAK-A [6] used for Cholesky factorization and SPARSPAK-B <ref> [7] </ref> are also in Fortran but benefit from the more careful consideration typical of a numerical software package. All tests were run on a single processor of an Alliant FX/2800.
Reference: [8] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference-contexts: A well-known drawback of this approach is that the condition number of the normal equations is the square of the condition number of the original linear least squares problem. Orthogonal factorization methods <ref> [8] </ref> avoid this problem, but they require more floating point operations and potentially can require O (mn) storage, which is unacceptable for systems with large m.
Reference: [9] <author> M. T. Heath, R. J. Plemmons, and R. C. Ward, </author> <title> Sparse orthogonal schemes for structural optimization using the force method, </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 5 (1984), </volume> <pages> pp. 514-532. </pages>
Reference-contexts: Such problems occur frequently in scientific and engineering applications such as linear programming [5], augmented Lagrangian methods for CFD [12], and the natural factor method in partial differential equations <ref> [9] </ref> [3]. Minimizing (1) by solving the normal equations A T Ax = A T b is a common and often efficient approach, because A T A is symmetric and positive definite. There are many well developed and reliable methods, both direct and iterative, for solving such systems.
Reference: [10] <author> A. Jennings and M. A. Ajiz, </author> <title> Incomplete methods for solving A T Ax = b, </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 5 (1984), </volume> <pages> pp. 978-987. </pages>
Reference-contexts: Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization [11], polynomial preconditioning [1, 2], and incomplete orthogonal factorization <ref> [14, 10, 18, 16] </ref>. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1. <p> This suggests using an incomplete orthogonal factorization: A QR. Existing incomplete orthogonal factorization preconditioners can be divided into two classes: incomplete Gram-Schmidt, such as the methods presented in <ref> [14, 10, 16] </ref>, and incomplete Givens, such as the method in [18]. Incomplete Gram-Schmidt type methods are in general robust because they can avoid numerical breakdown when A is full rank. Furthermore, they are effective in accelerating the convergence of CG.
Reference: [11] <author> J. A. Meijerink and H. A. van der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix, </title> <journal> Mathematics of Computation, </journal> <volume> 31 (1977), </volume> <pages> pp. 148-162. </pages>
Reference-contexts: Because the rate of convergence of the CG algorithm is related to the condition number of the matrix that it is applied to, finding an effective preconditioner is crucial. Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization <ref> [11] </ref>, polynomial preconditioning [1, 2], and incomplete orthogonal factorization [14, 10, 18, 16]. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1. <p> For MGS one of the eigenvalues is made exactly 1 and the remaining ones are in an interval bounded by the minimum and maximum eigenvalues of the normal equations of the original matrix. A similar result in <ref> [11] </ref> shows that A T A = LL T ^ E is a regular splitting, where L is the IC factor of A T A, ^ E 0, and ( ^ E) &lt; 1. It is straightforward to transform this result into one similar to Theorem 4.
Reference: [12] <author> M.Fortin and R. Glowinski, </author> <title> Augmented Lagrangian methods: applications to the numerical solution of boundary-value problems, </title> <publisher> North-Holland, </publisher> <year> 1983. </year>
Reference-contexts: Such problems occur frequently in scientific and engineering applications such as linear programming [5], augmented Lagrangian methods for CFD <ref> [12] </ref>, and the natural factor method in partial differential equations [9] [3]. Minimizing (1) by solving the normal equations A T Ax = A T b is a common and often efficient approach, because A T A is symmetric and positive definite.
Reference: [13] <author> C. Paige and M. Saunders, </author> <title> Solution of sparse indefinite systems of equations and least squares problems, </title> <type> Tech. Rep. </type> <institution> STAN-CS-73-399, Stanford University, </institution> <year> 1973. </year>
Reference-contexts: In addition to the algebraic quantities mentioned above, floating point operation counts and the cpu time were also collected in the experiments and are used to assess the efficiency of the various approaches. A conjugate gradient iterative method, CGLS <ref> [13] </ref> is used in the experiments as the basic iterative method to solve the least squares problems and, for square matrices, the nonsymmetric linear systems. Of course, even though CGLS does not form the normal equations explicitly, its convergence depends on their spectrum.
Reference: [14] <author> Y. Saad, </author> <title> Preconditioning techniques for nonsymmetric and indefinite linear systems, </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 24 (1988), </volume> <pages> pp. 89-105. </pages>
Reference-contexts: Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization [11], polynomial preconditioning [1, 2], and incomplete orthogonal factorization <ref> [14, 10, 18, 16] </ref>. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1. <p> This suggests using an incomplete orthogonal factorization: A QR. Existing incomplete orthogonal factorization preconditioners can be divided into two classes: incomplete Gram-Schmidt, such as the methods presented in <ref> [14, 10, 16] </ref>, and incomplete Givens, such as the method in [18]. Incomplete Gram-Schmidt type methods are in general robust because they can avoid numerical breakdown when A is full rank. Furthermore, they are effective in accelerating the convergence of CG. <p> One way of reducing computations is to use a numerical dropping technique to keep both the Q and R factors sparse, as is done in ILQ <ref> [14] </ref> and the incomplete Givens method of [18]. The price these methods pay for efficiency is robustness, because dropping small entries can lead to zero elements on the diagonal of R. Restart techniques have to be used for these methods to assure robustness.
Reference: [15] <author> Y. Saad, SPARSKIT: </author> <title> a basic tool kit for sparse matrix computations, </title> <type> tech. rep., </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, Illinois, </institution> <year> 1990. </year>
Reference-contexts: Reducing * allows the iterative method to solve the problem, and improves performance as shown in Figure 3. The optimal value of * is unknown in general. This is a potential difficulty of CIMGS, which it shares with other drop tolerance methods such as ILUT preconditioning <ref> [15] </ref>. In general, the more ill conditioned problems are, the smaller * may need to be.
Reference: [16] <author> X. Wang, </author> <title> Incomplete factorization preconditioning for linear least squares problems, </title> <type> PhD thesis, </type> <institution> University of Illinois Urbana-Champaign, </institution> <year> 1993. </year> <note> Also available as Technical Report # UIUCDCS-R-93-1834, </note> <institution> Department of Computer Science, University of Illinois at Urbana - Champaign. </institution>
Reference-contexts: Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization [11], polynomial preconditioning [1, 2], and incomplete orthogonal factorization <ref> [14, 10, 18, 16] </ref>. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1. <p> This suggests using an incomplete orthogonal factorization: A QR. Existing incomplete orthogonal factorization preconditioners can be divided into two classes: incomplete Gram-Schmidt, such as the methods presented in <ref> [14, 10, 16] </ref>, and incomplete Givens, such as the method in [18]. Incomplete Gram-Schmidt type methods are in general robust because they can avoid numerical breakdown when A is full rank. Furthermore, they are effective in accelerating the convergence of CG. <p> Therefore, n (0) o is independent, a contradiction. So if A has full rank, r kk 6= 0 for k = 1; 2; : : : ; n and the factorization must exist. 2 More detailed studies of IMGS can be found in <ref> [16] </ref>. In general, IMGS is robust and effective at reducing the number of CG iterations. Its main weakness is the much higher cost of computing the preconditioner compared to other preconditioning methods. The new algorithm CIMGS now described will produce the same preconditioner, while greatly reducing the computation cost. <p> arithmetic, the natural next question to ask is how CIMGS is affected by rounding errors, that is, how does the "compression" technique affect the stability of IMGS? Earlier analysis has shown that IMGS is less likely than modified Gram-Schmidt to break down due to possible numerical rank deficiency of A <ref> [16] </ref>. The next theorem says that numerical rank deficiency is less likely to occur for CIMGS than it is for complete Cholesky factorization. That in turn implies that CIMGS may breakdown earlier than IMGS as the condition of A worsens . <p> The proof can be found in <ref> [16] </ref>. If A T A is an M-matrix, Theorem 4 bounds the distance between Q and an orthogonal matrix since it implies that (Q T Q) 2. <p> Furthermore, it is easy to modify the target sparsity pattern in order to satisfy the hypothesis of Theorem 7; see <ref> [16] </ref> for more details. In general, CIMGS gives a different factor R from the one given by IC. <p> Computing them will involve `compute-then-drop', which is a waste of time and space. A detailed discussion of an algorithm that identifies these unnecessary computations by symbolic analysis can be found in <ref> [16] </ref>.
Reference: [17] <author> J. H. Wilkinson, </author> <title> A priori error analysis of algebraic processes, </title> <booktitle> in Proc. International Congress of Mathematicians, </booktitle> <year> 1968, </year> <pages> pp. 119-129. </pages>
Reference-contexts: Therefore, E (1) CIMGS and E (1) CHOL are equal in those positions for which updating is performed. For the other positions, the elements of E (1) CIMGS are equal to 0. Using Wilkinson's estimates <ref> [17] </ref>, we get k E CIMGS k 2 c 1 k B k 2 : Since B (2) CIMGS equals the reduced matrix after applying one step of Cholesky factorization to B + u 12 u T (1) CIMGS , again using Wilkinson's result gives k B CIMGS k 2 k
Reference: [18] <author> Z. Zlatev and H.B.Nielson, </author> <title> Solving large and sparse linear least squares problems by conjugate gradient algorithm, Computers and Mathematics with Applications, </title> <booktitle> 15 (1988), </booktitle> <pages> pp. 185-202. </pages>
Reference-contexts: Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization [11], polynomial preconditioning [1, 2], and incomplete orthogonal factorization <ref> [14, 10, 18, 16] </ref>. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1. <p> This suggests using an incomplete orthogonal factorization: A QR. Existing incomplete orthogonal factorization preconditioners can be divided into two classes: incomplete Gram-Schmidt, such as the methods presented in [14, 10, 16], and incomplete Givens, such as the method in <ref> [18] </ref>. Incomplete Gram-Schmidt type methods are in general robust because they can avoid numerical breakdown when A is full rank. Furthermore, they are effective in accelerating the convergence of CG. <p> One way of reducing computations is to use a numerical dropping technique to keep both the Q and R factors sparse, as is done in ILQ [14] and the incomplete Givens method of <ref> [18] </ref>. The price these methods pay for efficiency is robustness, because dropping small entries can lead to zero elements on the diagonal of R. Restart techniques have to be used for these methods to assure robustness.
References-found: 17

