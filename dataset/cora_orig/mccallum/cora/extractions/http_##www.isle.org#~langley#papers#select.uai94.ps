URL: http://www.isle.org/~langley/papers/select.uai94.ps
Refering-URL: http://www.isle.org/publications.html
Root-URL: 
Email: (Langley@flamingo.stanford.edu)  (Sage@flamingo.stanford.edu)  
Title: Induction of Selective Bayesian Classifiers  
Author: Pat Langley Stephanie Sage 
Address: 2451 High Street, Palo Alto, CA 94301  
Affiliation: Institute for the Study of Learning and Expertise  
Note: From Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (1994). Seattle, WA: Morgan Kaufmann.  
Abstract: In this paper, we examine previous work on the naive Bayesian classifier and review its limitations, which include a sensitivity to correlated features. We respond to this problem by embedding the naive Bayesian induction scheme within an algorithm that carries out a greedy search through the space of features. We hypothesize that this approach will improve asymptotic accuracy in domains that involve correlated features without reducing the rate of learning in ones that do not. We report experimental results on six natural domains, including comparisons with decision-tree induction, that support these hypotheses. In closing, we discuss other approaches to extending naive Bayesian classifiers and outline some directions for future research. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. R., & Matessa, M. </author> <year> (1992). </year> <title> Explorations of an incremental, Bayesian algorithm for categorization. </title> <note> Machine Learning , 9 , 275-308. </note>
Reference: <author> Brodley, C. E., & Utgoff, P. E. </author> <year> (1992). </year> <title> Multivari-ate versus univariate decision trees (Coins Technical Report 92-8). </title> <institution> Amherst: University of Mas-sachusetts, Department of Computer and Information Science. </institution>
Reference: <author> Buntine, W. </author> <year> (1990). </year> <title> A theory of learning classification rules. </title> <type> Dissertation, </type> <institution> Department of Computer Science, University of Technology, Sydney. </institution>
Reference-contexts: We have borrowed this term from Kononenko (1990); other common names for the method include the simple Bayesian classifier (Langley, 1993) and idiot Bayes <ref> (Buntine, 1990) </ref>.
Reference: <author> Caruana, R. A., & Freitag, D. </author> <title> (in press). Greedy attribute selection. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference: <author> Cestnik, G. </author> <year> (1990). </year> <title> Estimating probabilities: A crucial task for machine learning. </title> <booktitle> Proceedings of the Ninth European Conference on Artificial Intelligence (pp. </booktitle> <pages> 147-149). </pages> <address> Stockholm, Sweden. </address>
Reference: <author> Cestnik, G., Kononenko, I, & Bratko, I. </author> <year> (1987). </year> <title> Assistant-86: A knowledge-elicitation tool for sophisticated users. </title> <editor> In I. Bratko & N. Lavrac (Eds.) </editor> <booktitle> Progress in machine learning. </booktitle> <publisher> Sigma Press. </publisher>
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Tay-lor, W., & Freeman, D. </author> <year> (1988). </year> <title> Autoclass: A Bayesian classification system. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 54-64). </pages> <address> Ann Arbor: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <note> Machine Learning , 3 , 261-284. </note>
Reference: <author> Connolly, D. </author> <year> (1993). </year> <title> Constructing hidden variables in Bayesian networks via conceptual clustering. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 65-72). </pages> <address> Amherst, MA: </address> <note> Mor-gan Kaufmann. Selective Bayesian Classifiers 406 Cooper, </note> <author> G. F., & Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <note> Machine Learning , 9 , 309-347. </note>
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: Supervised Bayesian methods have long been used within the field of pattern recognition <ref> (Duda & Hart, 1973) </ref>, but only in the past few years have they received attention within the machine learning community (e.g., Clark & Niblett, 1989; Kononenko, 1990, 1991; Langley, Iba, & Thompson, 1992).
Reference: <author> Fisher, D. H. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <booktitle> Machine Learning, </booktitle> <pages> 2 , 139-172. </pages>
Reference: <author> Gennari, J. H., Langley, P., & Fisher, D. H. </author> <year> (1989). </year> <title> Models of incremental concept formation. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 40 , 11-61. </pages>
Reference: <author> Hadzikadic, M., & Yun, D. </author> <year> (1989). </year> <title> Concept formation by incremental conceptual clustering. </title> <booktitle> Proceedings of the Eleventh International Joint Conference Artificial Intelligence (pp. </booktitle> <pages> 831-836). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most data sets. </title> <booktitle> Machine Learning, </booktitle> <pages> 11 , 63-90. </pages>
Reference: <author> John, G. H., Kohavi, R., & Pfleger, K. </author> <title> (in press). Irrelevant features and the subset selection problem. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference: <author> Kittler, J. </author> <year> (1986). </year> <title> Feature selection and extraction. In Young & Fu, (eds.), Handbook of pattern recognition and image processing . New York: </title> <publisher> Academic Press. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1990). </year> <title> Comparison of inductive and naive Bayesian learning approaches to automatic knowledge acquisition. </title> <editor> In B. Wielinga et al. (Eds.), </editor> <booktitle> Current trends in knowledge acquisition. </booktitle> <address> Amster-dam: </address> <publisher> IOS Press. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1991). </year> <title> Semi-naive Bayesian classifier. </title> <booktitle> Proceedings of the Sixth European Working Session on Learning (pp. </booktitle> <pages> 206-219). </pages> <address> Porto, Portugal: </address> <publisher> Pittman. </publisher>
Reference: <author> Kubat, M., Flotzinger, D., & Pfurtscheller, G. </author> <year> (1993). </year> <title> Discovering patterns in EEG signals: Comparative study of a few methods. </title> <booktitle> Proceedings of the 1993 Eu-ropean Conference on Machine Learning (pp. </booktitle> <pages> 367-371). </pages> <address> Vienna: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Langley, P. </author> <year> (1993). </year> <title> Induction of recursive Bayesian classifiers. </title> <booktitle> Proceedings of the 1993 European Conference on Machine Learning (pp. </booktitle> <pages> 153-164). </pages> <address> Vi-enna: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We have borrowed this term from Kononenko (1990); other common names for the method include the simple Bayesian classifier <ref> (Langley, 1993) </ref> and idiot Bayes (Buntine, 1990). <p> Other encodings lead to a more complex story, but the effect is nearly the same. Nevertheless, like perceptrons, Bayesian classifiers are Selective Bayesian Classifiers 401 typically limited to learning classes that can be separated by a single decision boundary. 3 Although we have addressed this limitation in other work <ref> (Langley, 1993) </ref>, we will not focus on it here. Another important assumption that the naive Bayesian classifier makes is that, within each class, the probability distributions for attributes are independent of each other.
Reference: <author> Langley, P., & Iba, W. </author> <year> (1993). </year> <title> Average-case analysis of a nearest neighbor algorithm. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 889-894). </pages> <address> Chambery, France. </address>
Reference-contexts: We have borrowed this term from Kononenko (1990); other common names for the method include the simple Bayesian classifier <ref> (Langley, 1993) </ref> and idiot Bayes (Buntine, 1990). <p> Other encodings lead to a more complex story, but the effect is nearly the same. Nevertheless, like perceptrons, Bayesian classifiers are Selective Bayesian Classifiers 401 typically limited to learning classes that can be separated by a single decision boundary. 3 Although we have addressed this limitation in other work <ref> (Langley, 1993) </ref>, we will not focus on it here. Another important assumption that the naive Bayesian classifier makes is that, within each class, the probability distributions for attributes are independent of each other.
Reference: <author> Langley, P., Iba, W., & Thompson, K. </author> <year> (1992). </year> <title> An analysis of Bayesian classifiers. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 223-228). </pages> <address> San Jose, CA: </address> <publisher> AAAI. </publisher>
Reference-contexts: Experiments with Bayesian Classifiers Previous comparative studies have shown that the naive Bayesian classifier outperforms more sophisticated methods such as decision-tree induction in some domains, but that it performs significantly worse in others <ref> (Langley et al., 1992) </ref>. We hypothesized that the first result reflects decision trees' reliance on axis-parallel splits, which poorly mimic the actual decision boundaries in some domains. In contrast, we posited that the naive Bayesian classifier did poorly in domains containing redundant attributes. <p> The simplicity of the selective Bayesian classifier should also lend itself to average-case analyses <ref> (Langley et al., 1992) </ref>, which would let us compare our experimental results to theoretical ones, at least in synthetic domains.
Reference: <author> Langley, P., & Sage, S. </author> <title> (in press). Oblivious decision trees and abstract cases. </title> <booktitle> Proceedings of the AAAI94 Workshop on Case-Based Reasoning . Seattle, </booktitle> <address> WA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> McKusick, K. B., & Langley, P. </author> <year> (1991). </year> <title> Constraints on tree structure in concept formation. </title> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 810-816). </pages> <address> Sydney: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Michie, D., & Al Attar, A. </author> <year> (1991). </year> <title> Use of sequential Bayes with class probability trees. </title> <editor> In J. E. Hayes-Michie, D. Michie, & E. Tyugu (Eds.), </editor> <booktitle> Machine intelligence 12 . Oxford: </booktitle> <publisher> Oxford University Press. </publisher>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic reasoning in intelligent systems: Networks of plausible inference. </title> <address> San Ma-teo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Another important assumption that the naive Bayesian classifier makes is that, within each class, the probability distributions for attributes are independent of each other. One can model attribute dependence within the Bayesian framework <ref> (Pearl, 1988) </ref>, but determining such dependencies and estimating them from limited training data is much more difficult. Thus, the independence assumption has clear attractions. Unfortunately, it is unrealistic to expect this assumption to hold in the natural world. Correlations among attributes in a given domain are common. <p> Selective Bayesian Classifiers 404 (a) the small soybean domain and (b) DNA promoters. Selective Bayes incorporates all attributes for the soybean data, giving an identical curve to that for the naive method. Research on the induction of Bayesian networks <ref> (Pearl, 1988) </ref> generalizes this basic approach to handling attribute dependence. Cooper and Herskovits' (1992) K2 algorithm carries out a greedy search through the space of Bayesian networks, but it requires the user to specify an ordering on the attributes, and it does not introduce new features.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> C4.5: Programs for machine learning . San Mateo, </title> <address> CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schlimmer, J. C. </author> <year> (1987). </year> <title> Incremental adjustment of representations for learning. </title> <booktitle> Proceedings of the Fourth International Machine Learning Workshop (pp. </booktitle> <pages> 79-90). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Skalak, D. B. </author> <title> (in press). Prototype and feature selection by sampling and random mutation hill-climbing algorithms. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference: <author> Warner, H. R., Toronto, A. F., Veasy, L. G. & Stephen-son, R. </author> <year> (1961). </year> <title> A mathematical approach to medical diagnosis: Application to congenital heart diseases. </title> <journal> Journal of the American Medical Association, </journal> <pages> 177 , 177-183. </pages>
Reference: <author> Widrow, B., & Winter, R. G. </author> <year> (1988). </year> <title> Neural nets for adaptive filtering and adaptive pattern recognition. </title> <booktitle> IEEE Computer, </booktitle> <month> March, </month> <pages> 25-39. </pages>
Reference-contexts: In addition, we should consider the usefulness of other selection techniques, such as Kubat et al.'s method, and compare our technique to frameworks with similar representational power that do not rely on the independence assumption, such as the LMS algorithm and related techniques <ref> (Widrow & Winter, 1988) </ref>. The simplicity of the selective Bayesian classifier should also lend itself to average-case analyses (Langley et al., 1992), which would let us compare our experimental results to theoretical ones, at least in synthetic domains.
References-found: 31

