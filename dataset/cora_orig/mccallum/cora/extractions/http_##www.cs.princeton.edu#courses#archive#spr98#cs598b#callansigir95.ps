URL: http://www.cs.princeton.edu/courses/archive/spr98/cs598b/callansigir95.ps
Refering-URL: http://www.cs.princeton.edu/courses/archive/spr98/cs598b/papers.html
Root-URL: http://www.cs.princeton.edu
Email: fcallan,zlu,croftg@cs.umass.edu  
Title: Searching Distributed Collections With Inference Networks  
Author: James P. Callan Zhihong Lu W. Bruce Croft 
Address: Amherst, MA 01003-4610, USA  
Affiliation: Computer Science Department, University of Massachusetts  
Abstract: The use of information retrieval systems in networked environments raises a new set of issues that have received little attention. These issues include ranking document collections for relevance to a query, selecting the best set of collections from a ranked list, and merging the document rankings that are returned from a set of collections. This paper describes methods of addressing each issue in the inference network model, discusses their implementation in the INQUERY system, and presents experimental results demonstrating their effectiveness. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. P. Callan, W. B. Croft, and J. Broglio. </author> <title> TREC and TIPSTER experiments with INQUERY. </title> <booktitle> Information Processing and Management, </booktitle> <publisher> (in press). </publisher>
Reference-contexts: The TREC collection is heterogeneous, containing 17 subcollec-tions from different sources and/or periods of time (Table 1). The subcollections vary widely in size, in number of documents, and in average document length. Experiments were conducted with 100 queries developed for TREC topics 51-150 during previous TREC and TIPSTER evaluations <ref> [1] </ref>. 2 The inference network can incorporate proximity information and operators, but it is impractical to do so for collection ranking. 2 Table 1: The TREC document collections used for experi-ments. The TREC volume number is shown in parentheses. <p> The average optimal rank O i for each collection in TREC Volume 1 is shown in Table 2. INQUERY's algorithms for ranking documents have been documented extensively <ref> [11; 12; 2; 1] </ref>, so this discussion is confined to the changes necessary to rank collections.
Reference: [2] <author> J. P. Callan, W. B. Croft, and S. M. Harding. </author> <title> The IN-QUERY retrieval system. </title> <booktitle> In Proceedings of the Third International Conference on Database and Expert Systems Applications, </booktitle> <pages> pages 78-83, </pages> <address> Valencia, Spain, 1992. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: There are also many more inverted lists, but only those that match terms in the query are ac cessed. The effectiveness of this approach to ranking collections was evaluated using the INQUERY retrieval system <ref> [11; 12; 2] </ref> and the 3 gigabyte TREC document collection. The TREC collection is heterogeneous, containing 17 subcollec-tions from different sources and/or periods of time (Table 1). The subcollections vary widely in size, in number of documents, and in average document length. <p> The average optimal rank O i for each collection in TREC Volume 1 is shown in Table 2. INQUERY's algorithms for ranking documents have been documented extensively <ref> [11; 12; 2; 1] </ref>, so this discussion is confined to the changes necessary to rank collections. <p> Figure 3b shows the effect on average 11 point precision. The results suggest that it is necessary to store at least the 20% most frequent terms, and that there is some advantage to storing all of the terms. 6.2 Proximity Information The INQUERY system <ref> [2] </ref>, which is based on the inference network model, extends the inference network formalism to include proximity operators. One could also use term proximity information for collection ranking, but it would require that the location of each term in each document in each collection be stored in the CORI index.
Reference: [3] <author> P. B. Danzig, J. Ahn, J. Noll, and K. Obraczka. </author> <title> Distributed indexing: A scalable mechanism for distributed information retrieval. </title> <editor> In A. Bookstein, Y. Chiaramella, G. Salton, and V. V. Raghavan, editors, </editor> <booktitle> Proceedings of the Fourteenth Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 220-229, </pages> <address> Chicago, IL, </address> <month> October </month> <year> 1991. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: Danzig, et al, showed how to automatically maintain similar groupings in distributed environments <ref> [3] </ref>. They used broker agents that maintained centralized indices for particular subjects by periodically querying remote collections. Both of these approaches simplify collection selection for users whose information needs can be anticipated to some extent. The EXPERT CONIT retrieval system [7] is an early example of automating collection selection.
Reference: [4] <author> S. T. Dumais. </author> <title> Latent semantic indexing (LSI) and TREC-2. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Second Text REtrieval Conference (TREC-2), </booktitle> <pages> pages 105-115, </pages> <address> Gaithersburg, MD, </address> <year> 1994. </year> <institution> National Institute of Standards and Technology, </institution> <note> Special Publication 500-215. </note>
Reference-contexts: Some have successfully used document scores from the different collections to create a merged ranking [6; 8], but others have had problems with this approach <ref> [4] </ref>. Voorhees, et al, call this the collection fusion problem, and describe two solutions [13]. One solution is to interleave the rankings, in a round-robin fashion. A second solution is uneven interleaving, biased by the expected relevance of the collection to the query. <p> This can viewed as desirable, because the idf represents the term's importance in a particular collection. It can also be viewed as undesirable, because an important query term can behave erratically, rewarding the random mention of a term in one collection and penalizing its common use in another <ref> [4] </ref>. The problem of incomparable scores can be overcome in some cases by normalizing statistics such as idf for the set of collections being searched [6]. <p> The only difference between the normalized and unnormalized scores was the idf component. This result confirms previous research suggesting that unnormal-ized idf s can give misleading results <ref> [4] </ref>. Ranking based on document scores and collection specific weights was about as effective as ranking based on normalized scores.
Reference: [5] <author> L. Gravano, H. Garc ia-Molina, and A. Tomasic. </author> <title> The effectiveness of GLOSS for the text database discovery problem. </title> <booktitle> In Proceedings of SIGMOD 94, </booktitle> <pages> pages 126-137. </pages> <publisher> ACM, </publisher> <month> September </month> <year> 1994. </year>
Reference-contexts: Relevance judgements for the most similar training queries determine whether, and how much, to retrieve from each collection. This technique may be practical for relatively static collections, but obtaining relevance judgements could be problematic for widely distributed and dynamic collections. GLOSS <ref> [5] </ref> estimates the number of potentially relevant documents in collection C for a Boolean AND query Q as jCj t*Q jCj , where t is a term in Q, df t is the number of documents in C containing t, and jCj is the number of documents in C.
Reference: [6] <author> K. L. Kwok, L. Grunfeld, and D. D. Lewis. TREC-3 ad-hoc, </author> <title> routing retrieval and thresholding experiments using PIRCS. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Third Text REtrieval Conference (TREC-3), </booktitle> <address> Gaithersburg, MD, </address> <note> (in press). </note> <institution> National Institute of Standards and Technology, </institution> <note> Special Publication 500-225. </note>
Reference-contexts: This task is simple if the results are an unordered set of documents, but it is more difficult if results are ranked lists of documents. Some have successfully used document scores from the different collections to create a merged ranking <ref> [6; 8] </ref>, but others have had problems with this approach [4]. Voorhees, et al, call this the collection fusion problem, and describe two solutions [13]. One solution is to interleave the rankings, in a round-robin fashion. <p> The problem of incomparable scores can be overcome in some cases by normalizing statistics such as idf for the set of collections being searched <ref> [6] </ref>. The intent is to normalize document scores to obtain precisely the same results that would be obtained if the individual document collections were merged into a single unified collection. The difficulty of normalizing document scores for a set of collections depends upon the retrieval algorithms employed.
Reference: [7] <author> R. S. Marcus. </author> <title> An experimental comparison of the effectiveness of computers and humans as search intermediaries. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 34 </volume> <pages> 381-404, </pages> <year> 1983. </year>
Reference-contexts: They used broker agents that maintained centralized indices for particular subjects by periodically querying remote collections. Both of these approaches simplify collection selection for users whose information needs can be anticipated to some extent. The EXPERT CONIT retrieval system <ref> [7] </ref> is an early example of automating collection selection. EXPERT decided on a query-by-query basis which collections were most appropriate, albeit for a relatively static set of homogeneous 1 Personal communication from a commercial retrieval service. 1 collections.
Reference: [8] <author> A. Moffat and J. Zobel. </author> <title> Information retrieval systems for large document collections. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Third Text REtrieval Conference (TREC-3), </booktitle> <address> Gaithersburg, MD, </address> <note> (in press). </note> <institution> National Institute of Standards and Technology, </institution> <note> Special Publication 500-225. </note>
Reference-contexts: It's effectiveness is not known yet, due to limited evaluation and the lack of support for other forms of query. Moffat, et al, used a centralized index on blocks of B documents in individual collections <ref> [8] </ref>. For example, each block might be 10 documents concatenated together. A new query first retrieves block identifiers from the centralized index, then searches the highly ranked blocks to retrieve documents. <p> This task is simple if the results are an unordered set of documents, but it is more difficult if results are ranked lists of documents. Some have successfully used document scores from the different collections to create a merged ranking <ref> [6; 8] </ref>, but others have had problems with this approach [4]. Voorhees, et al, call this the collection fusion problem, and describe two solutions [13]. One solution is to interleave the rankings, in a round-robin fashion.
Reference: [9] <author> Edie Rasmussen. </author> <title> Clustering algorithms. </title> <editor> In Frakes and Baeza-Yates, editors, </editor> <booktitle> Information Retrieval: Data Structures and Algorithms, chapter 16, </booktitle> <pages> pages 419-442. </pages> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: There are any number of ways to decide how far down the collection rankings to go. One could choose the top n, any collection with a score greater than some threshold, or the top group as defined by some clustering method. We investigated the latter approach. A single-pass algorithm <ref> [9] </ref> was used to cluster the collection rankings for each query. Collections were clustered on the basis of their scores, as determined by the collection ranking algorithm (Section 3). The cluster difference threshold was low (0.0012), creating clusters that tended to be smaller than necessary.
Reference: [10] <author> S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, and M. Gatford. </author> <note> Okapi at TREC-3. </note> <editor> In D. K. Harman, editor, </editor> <booktitle> The Third Text REtrieval Conference (TREC-3), </booktitle> <address> Gaithersburg, MD, </address> <note> (in press). </note> <institution> National Institute of Standards and Technology, </institution> <note> Special Publication 500-225. </note>
Reference-contexts: Scaling df by max df tends to obscure small (and not-so-small) sets of interesting documents in large collections. Recent experiments with document retrieval suggest that it may be better to scale tf by tf +K, for some small K <ref> [10] </ref>. The analogue for this task would be to scale df by df + K, replacing Equation 1 above with Equation 4 below. T = d t + (1 d t) df + K When ranking documents, it makes sense to make K a function of document length.
Reference: [11] <author> H. R. Turtle and W. B. Croft. </author> <title> Evaluation of an inference network-based retrieval model. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 9(3) </volume> <pages> 187-222, </pages> <year> 1991. </year>
Reference-contexts: A second solution is uneven interleaving, biased by the expected relevance of the collection to the query. The latter approach was substantially more effective in experiments with the TREC collec tion. 3 Ranking Collections With Inference Networks Inference networks are a probabilistic approach to information retrieval <ref> [12; 11] </ref>. The traditional use of inference networks for document retrieval is a directed acyclic graph in which documents are represented by leaves, and the root node represents an information need (Figure 1). A major part of the collection selection problem is ranking collections for a given information need. <p> There are also many more inverted lists, but only those that match terms in the query are ac cessed. The effectiveness of this approach to ranking collections was evaluated using the INQUERY retrieval system <ref> [11; 12; 2] </ref> and the 3 gigabyte TREC document collection. The TREC collection is heterogeneous, containing 17 subcollec-tions from different sources and/or periods of time (Table 1). The subcollections vary widely in size, in number of documents, and in average document length. <p> The average optimal rank O i for each collection in TREC Volume 1 is shown in Table 2. INQUERY's algorithms for ranking documents have been documented extensively <ref> [11; 12; 2; 1] </ref>, so this discussion is confined to the changes necessary to rank collections. <p> The probabilistic query operators that combine the beliefs accruing from the query terms remained unchanged <ref> [11; 12] </ref>. The proximity operators were replaced by strict Boolean AND operators, due to the lack of proximity information in CORI nets (discussed in Section 6.2). This approach was used to rank TREC volume 1 collections for topics 51-100. The mean-squared error, averaged over 50 queries, was 2.3471.
Reference: [12] <author> Howard R. Turtle and W. Bruce Croft. </author> <title> Efficient probabilistic inference for text retrieval. </title> <booktitle> In RIAO 3 Conference Proceedings, </booktitle> <pages> pages 644-661, </pages> <address> Barcelona, Spain, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: A second solution is uneven interleaving, biased by the expected relevance of the collection to the query. The latter approach was substantially more effective in experiments with the TREC collec tion. 3 Ranking Collections With Inference Networks Inference networks are a probabilistic approach to information retrieval <ref> [12; 11] </ref>. The traditional use of inference networks for document retrieval is a directed acyclic graph in which documents are represented by leaves, and the root node represents an information need (Figure 1). A major part of the collection selection problem is ranking collections for a given information need. <p> There are also many more inverted lists, but only those that match terms in the query are ac cessed. The effectiveness of this approach to ranking collections was evaluated using the INQUERY retrieval system <ref> [11; 12; 2] </ref> and the 3 gigabyte TREC document collection. The TREC collection is heterogeneous, containing 17 subcollec-tions from different sources and/or periods of time (Table 1). The subcollections vary widely in size, in number of documents, and in average document length. <p> The average optimal rank O i for each collection in TREC Volume 1 is shown in Table 2. INQUERY's algorithms for ranking documents have been documented extensively <ref> [11; 12; 2; 1] </ref>, so this discussion is confined to the changes necessary to rank collections. <p> The probabilistic query operators that combine the beliefs accruing from the query terms remained unchanged <ref> [11; 12] </ref>. The proximity operators were replaced by strict Boolean AND operators, due to the lack of proximity information in CORI nets (discussed in Section 6.2). This approach was used to rank TREC volume 1 collections for topics 51-100. The mean-squared error, averaged over 50 queries, was 2.3471.
Reference: [13] <author> E. M. Voorhees, N. K. Gupta, and B. Johnson-Laird. </author> <title> The collection fusion problem. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Third Text REtrieval Conference (TREC-3), </booktitle> <address> Gaithersburg, MD, </address> <note> (in press). </note> <institution> National Institute of Standards and Technology, </institution> <note> Special Publication 500-225. </note>
Reference-contexts: It used rule-based inferencing to match the in-formation need to a knowledge-base describing document collections, producing a ranked list of collections. Voorhees, et al, explored ranking collections using the similarity of a new query to training queries <ref> [13] </ref>. Relevance judgements for the most similar training queries determine whether, and how much, to retrieve from each collection. This technique may be practical for relatively static collections, but obtaining relevance judgements could be problematic for widely distributed and dynamic collections. <p> Some have successfully used document scores from the different collections to create a merged ranking [6; 8], but others have had problems with this approach [4]. Voorhees, et al, call this the collection fusion problem, and describe two solutions <ref> [13] </ref>. One solution is to interleave the rankings, in a round-robin fashion. A second solution is uneven interleaving, biased by the expected relevance of the collection to the query. <p> After a set of collections is searched, the ranked results from each collection must be merged into a single ranking. If only the document rankings are available, the results from each collection can be interleaved <ref> [13] </ref>. This solution is not satisfying, for it is unlikely that all of the collections have equal numbers or proportions of relevant documents. However, it is difficult to do anything more sophisticated without more information than just the document rankings. <p> However, this approach is costly if collections are distributed widely across networks, because (C 1) n documents are retrieved, sent across the network, and then discarded without a user seeing them <ref> [13] </ref>. This cost raises the question of whether it is possible to safely retrieve fewer than n documents from collections with low ranks or scores. We have experimented with a heuristic that uses the collection ranking information to decide how much to retrieve from each collection.
References-found: 13

