URL: http://www.cs.rpi.edu/tr/91-32.ps
Refering-URL: http://www.cs.rpi.edu/tr/
Root-URL: 
Phone: 950  
Title: Memory Optimization for Parallel Functional Programs  
Author: Balaram Sinharoy Boleslaw Szymanski 
Address: P.O. Box  Poughkeepsie, NY 12602, USA Troy, NY 12180-3590, USA  
Affiliation: Large Scale Computing Division Department of Computer Science IBM Corporation,  Rensselaer Polytechnic Institute  
Note: to appear in Computing Systemes in Engineering  
Abstract: Parallel functional languages use single valued variables to avoid semantically irrelevant data dependence constraints. Programs containing iterations that redefine variables in a procedural language have the corresponding variables declared with additional dimensions in a single assignment language. This extra temporal dimension, unless optimized, requires an exorbitant amount of memory and in parallel programs imposes a large delay between the data producer and consumers. For certain loop arrangements, a window containing a few elements of the dimension can be created. Usually, there are many ways for defining a loop arrangement in an implementation of a functional program and a trade-off between the memory saving and the needed level of parallelism has to be taken into account when selecting the implementation. In this paper we prove that the problem of determining the best loop arrangement by partitioning the dependence graph is NP-hard. In addition, we describe a heuristic for solving this problem. Finally, we present examples of parallel functional programs in which the memory optimization results in reducing the local and shared memory requirements and communication delays.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> B. K. Szymanski (ed.), </editor> <booktitle> Parallel Functional Programming Languages and Compilers, </booktitle> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Parallel functional languages use single valued variables to avoid semantically irrelevant data dependence constraints <ref> [1] </ref>. Programs containing iterations that redefine variables in a procedural language have the corresponding variables declared with additional dimensions in a single assignment language. Often a window containing a few elements of the dimension can be created. Only window elements need to be kept in the local memory. <p> In this paper we consider functional programs written as a set of statements defining array variables over their index domains (cf. EPL or Crystal functional languages <ref> [1] </ref>). Therefore each statement can be thought of as nested in the loops iterating over these index domains. Loops over the same statement can be nested in a different order. Loops with the same index domain iterating over different statements often can be merged together. <p> For loop arrangement (i 3 ; i 2 ; i 1 ): [3; ; ] 2. For loop arrangement (i 2 ; i 3 ; i 1 ): <ref> [1; 3; ] </ref> 3. For loop arrangement (i 1 ; i 2 ; i 3 ): [4; ; ] The algorithm presented in Subsection 2.2 will select loop arrangement (i 2 ; i 3 ; i 1 ) as requiring the minimum memory. <p> Scan and reduce are language constructs in some parallel languages that allow such operations. Reduce applied to a vector of values produces a scalar result, whereas scan results in a vector of partial results <ref> [1] </ref>. <p> The heuristic algorithms have been implemented on Sequent Balance Symmetry 81000, a shared memory multiprocessor system, and incorporated with the EPL compiler <ref> [1] </ref>. The results obtained when heuristics were applied to EPL programs for LU decomposition and for solving a system of linear statements by Jacobi iterations showed that automatic memory optimization matched the efficiency of handwritten codes.
Reference: [2] <author> B. K. Szymanski and N. Prywes, </author> <title> "Efficient Handling of Data Structures in Definitional Languages," </title> <booktitle> Science of Computer Programming 10 221-45 (1988). </booktitle>
Reference-contexts: All data dependencies can be represented as various kinds of edges in the dependence graph <ref> [2] </ref>. Each node in the graph contains information about the represented variable such as the number and ranges of its dimensions, its type and class, conditions guarding its definitions, etc. <p> Determination of suitable partitions of a program and mapping of the partitions onto the processors has been studied extensively in the literature (for example, <ref> [2, 3, 4, 5, 6, 7] </ref>). <p> Loop merging often increases the opportunity of forming windows on variables, but it may also limit extraction of parallelism. Thus, a trade-off between memory saving and the number of independent tasks must be considered when selecting the loop arrangement. The following heuristic was proposed in <ref> [2, 8] </ref> to minimize memory for Model programs. The data dependence graph is decomposed into strongly connected components (abbreviated as SCC) 1 . The quotient graph, Q, of the array graph with respect to the SCCs, is formed. <p> The size of the loop nest is k. A loop nest is maximal if it is not contained in any other loop nest. 2 The execution ordering of the statements in a functional program is determined by a recursive procedure called the scheduler <ref> [2] </ref>. At each level of recursion, the scheduler linearly orders all SCCs in the dependence graph of the program to satisfy the existing data dependencies.
Reference: [3] <author> J. Baxter and J. H. Patel, </author> <title> "The LAST Algorithm: A Heuristic-Based Static Task Allocation Algorithm," </title> <booktitle> Proceedings of the 1989 International Conference on Parallel Processing, II, </booktitle> <pages> pp. 217 - 222, </pages> <year> 1989. </year>
Reference-contexts: For loop arrangement (i 3 ; i 2 ; i 1 ): <ref> [3; ; ] </ref> 2. For loop arrangement (i 2 ; i 3 ; i 1 ): [1; 3; ] 3. <p> Determination of suitable partitions of a program and mapping of the partitions onto the processors has been studied extensively in the literature (for example, <ref> [2, 3, 4, 5, 6, 7] </ref>).
Reference: [4] <author> L. Bic, M. D. Nagel and J. M. A. Roy, </author> <title> "Automatic Data/Program Partitioning Using the Single Assignment Principle," </title> <booktitle> Proceedings of Supercomputing 1989, </booktitle> <address> Reno, Nevada, </address> <pages> pp. 551 - 556, </pages> <year> 1989. </year>
Reference-contexts: For loop arrangement (i 3 ; i 2 ; i 1 ): [3; ; ] 2. For loop arrangement (i 2 ; i 3 ; i 1 ): [1; 3; ] 3. For loop arrangement (i 1 ; i 2 ; i 3 ): <ref> [4; ; ] </ref> The algorithm presented in Subsection 2.2 will select loop arrangement (i 2 ; i 3 ; i 1 ) as requiring the minimum memory. <p> Determination of suitable partitions of a program and mapping of the partitions onto the processors has been studied extensively in the literature (for example, <ref> [2, 3, 4, 5, 6, 7] </ref>).
Reference: [5] <author> P.-Z. Lee and Z. M. Kedem, </author> <title> "Mapping Nested Loop Algorithms into Multidimensional Systolic Arrays," </title> <booktitle> IEEE Transactions on Parallel and Distributed Processing 1 (1990). </booktitle>
Reference-contexts: Determination of suitable partitions of a program and mapping of the partitions onto the processors has been studied extensively in the literature (for example, <ref> [2, 3, 4, 5, 6, 7] </ref>).
Reference: [6] <author> J.-K. Peir and R. Cytron, </author> <title> "Minimum Distance: A Method for Partitioning Recurrences for Multiprocessors," </title> <booktitle> Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pp. 217-225, </pages> <year> 1987. </year>
Reference-contexts: Determination of suitable partitions of a program and mapping of the partitions onto the processors has been studied extensively in the literature (for example, <ref> [2, 3, 4, 5, 6, 7] </ref>).
Reference: [7] <author> B. Sinharoy and B. K. Szymanski, </author> <title> "Finding Optimum Wavefront of Parallel Computation," </title> <journal> Journal of Parallel Algorithms and Applications, </journal> <month> 1 </month> <year> (1993). </year>
Reference-contexts: Determination of suitable partitions of a program and mapping of the partitions onto the processors has been studied extensively in the literature (for example, <ref> [2, 3, 4, 5, 6, 7] </ref>).
Reference: [8] <author> K. S. Lu, </author> <title> Program Optimization based on a Non-Procedural Specification, </title> <publisher> Ph. </publisher> <address> D. </address> <institution> dissertation, University of Pennsyl-vania, </institution> <address> Philadelphia, PA, </address> <year> 1981. </year>
Reference-contexts: Loop merging often increases the opportunity of forming windows on variables, but it may also limit extraction of parallelism. Thus, a trade-off between memory saving and the number of independent tasks must be considered when selecting the loop arrangement. The following heuristic was proposed in <ref> [2, 8] </ref> to minimize memory for Model programs. The data dependence graph is decomposed into strongly connected components (abbreviated as SCC) 1 . The quotient graph, Q, of the array graph with respect to the SCCs, is formed.
Reference: [9] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability A Guide to the Theory of NP-Completeness, </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year> <month> 10 </month>
Reference-contexts: It is a well known NP-complete problem <ref> [9] </ref>. 5 where I i 's are subscripts, c i 's are given constants and f i 's are given functions.
References-found: 9

