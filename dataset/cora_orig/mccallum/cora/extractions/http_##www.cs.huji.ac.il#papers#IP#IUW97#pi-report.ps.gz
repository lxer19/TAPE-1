URL: http://www.cs.huji.ac.il/papers/IP/IUW97/pi-report.ps.gz
Refering-URL: http://www.cs.huji.ac.il/papers/IP/IUW97/index.html
Root-URL: http://www.cs.huji.ac.il
Email: fpeleg,shashua,werman,daphnag@cs.huji.ac.il irani@wisdom.weizmann.ac.il  
Title: Multi-Sensor Representation of Extended Scenes using Multi-View Geometry  
Author: Shmuel Peleg Amnon Shashua Michal Irani Daphna Weinshall Michael Werman 
Address: 91904 Jerusalem, Israel Rehovot, Israel  
Affiliation: Institute of Computer Science Dept. of Mathematics The Hebrew University of Jerusalem Weizmann Institute of Science  
Abstract: In order to provide visual surveillance and monitoring (VSAM) systems with tracking and visualization capabilities, we will develop algorithms that will address the problems and limitations of the existing state-of-the-art technology. In particular, we will: (i) Develop image alignment techniques based on the Trilinear Tensor, both for single and for multiple sensors, to allow for accurate correspondence across sensors and time. (ii) Develop alignment techniques of sequences obtained by sensors of different modalities. (iii) Develop Scene Manifolds as extended scene representations that will efficiently combine information from single/multiple sensors. These representations will apply for a wide range of scenes and camera motions. (iv) Develop algorithms for detection and continuous tracking of moving objects across time and sensors. The detection of moving objects will be performed using Parallax Geometry constraints. (v) Develop means for the operator to visualize the scene. These include: Scene manifold projection, novel view synthesis, and video stabilization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Avidan and A. Shashua. </author> <title> Novel view synthesis in tensor space. </title> <booktitle> In CVPR-97, </booktitle> <address> San Juan, Puerto Rico, </address> <month> June </month> <year> 1997. </year> <title> Also http://www.cs.huji.ac.il/~shashua. a) b) are two original views. (c) is a synthesized new view. </title>
Reference-contexts: Applications to fl This research was funded by DARPA through the U.S. Office of Naval Research under grant N00014-93-1-1202. URL: http://www.cs.huji.ac.il/labs/vision new view generation and to image stabilization are discussed in <ref> [13, 1] </ref>. Theoretical analysis shows that these methods are superior to existing two-frame methods, as they are free of degenerate scene or camera configurations [17]. A novel and efficient motion recovery method has also been developed [12]. <p> In general IU contributions, correspondence techniques become more robust as geometry and photometry are combined together; view-synthesis methods use the 2D imagery directly without the need for a detailed 3D model <ref> [1] </ref>; video stabilization can be handled robustly under the most general situations (unlike existing methods) [13]; moving object detection from moving cameras can be handled using our duality of scene and motion invariants [6, 21, 7]; and extended scene representations will become more general than today's 2D mosaicing techniques [11] and <p> One implication of this result is that one can synthesize new tensors from a given tensor and user specification of the position of a novel camera. Further details and demonstrations can be found in <ref> [1] </ref>. 7 Local Curve Matching and Object Classification Initial experiments on object recognition using local curve matching [3] gave promising results, and we plan to continue and develop more robust versions. Currently we have developed an algorithm which computes the distance between two contours of possibly rather different shapes.
Reference: [2] <author> S. Avidan and A. Shashua. </author> <title> Unifying two-view and three-view geometry. </title> <booktitle> In IUW-97, </booktitle> <address> New Or-leans, Louisiana, May 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The result is the introduction of a "Tensor Brightness Constraint" which is a parametric equation for solving for ego-motion directly from the spatio-temporal derivatives of the image sequence. In <ref> [2] </ref> we show that the fundamental matrix of two views is embedded in the tensorial equation as a rank-2 trivalent tensor, and we introduce a standard set of tensorial operators that apply to both two views and three views alike.
Reference: [3] <author> Y. Gdalyahu and D. Weinshall. </author> <title> Local curve matching for object recognition without prior knowledge. </title> <booktitle> In IUW-97, </booktitle> <address> New Orleans, Louisiana, May 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Further details and demonstrations can be found in [1]. 7 Local Curve Matching and Object Classification Initial experiments on object recognition using local curve matching <ref> [3] </ref> gave promising results, and we plan to continue and develop more robust versions. Currently we have developed an algorithm which computes the distance between two contours of possibly rather different shapes. The method is local and fast, relying on both local similarities and global alignment.
Reference: [4] <author> G. Halevi and D. Weinshall. </author> <title> Motion of disturbances: Detection and tracking of multi-body non rigid motion. </title> <booktitle> In CVPR-97, </booktitle> <address> San Juan, Puerto Rico, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: This method proved effective in tracking many independently moving objects, like ants on the forest floor or people in a crowd <ref> [4] </ref>. 2 Tensor-based Image Alignment Three views give rise to a set of trilinear matching constraints that first became prominent in [15], whose coefficients form a tensor ("trilinear tensor") which encodes the relative camera locations, and whose underlying theory has been studied intensively.
Reference: [5] <author> M. Irani, P. Anandan, , and S. Hsu. </author> <title> Mosaic based representations of video sequences and their applications. </title> <booktitle> In ICCV, </booktitle> <pages> pages 605-611, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year> <pages> IEEE-CS. </pages>
Reference-contexts: Combining these two constraints together is expected to provide a more powerful rigidity constraint for moving object detection. 5 Scene Manifolds "Scene Manifolds" is an alternative scene representation to image mosaics, which are currently used to represent panoramic view by stitching together a sequence of images <ref> [5] </ref>. Traditional image mosaics use a projection of all images into a common mosaic-image-plane, thus creating unacceptable distortions for images whose original image-plane is substantially different in angle from the mosaic-image-plane.
Reference: [6] <author> M. Irani and P. Anandan. </author> <title> Parallax geometry of pairs of points for 3D scene analysis. </title> <booktitle> In ECCV-96, pages I:17-30, </booktitle> <address> Cambridge, UK, April 1996. </address> <publisher> Springer. </publisher>
Reference-contexts: These techniques rely in part on the use of multilinear constraints across three or more views and the trilinear tensor [15, 19], parallax geometry <ref> [6] </ref>, and the duality of camera and scene invariants [6, 21]. Applications to fl This research was funded by DARPA through the U.S. Office of Naval Research under grant N00014-93-1-1202. URL: http://www.cs.huji.ac.il/labs/vision new view generation and to image stabilization are discussed in [13, 1]. <p> These techniques rely in part on the use of multilinear constraints across three or more views and the trilinear tensor [15, 19], parallax geometry [6], and the duality of camera and scene invariants <ref> [6, 21] </ref>. Applications to fl This research was funded by DARPA through the U.S. Office of Naval Research under grant N00014-93-1-1202. URL: http://www.cs.huji.ac.il/labs/vision new view generation and to image stabilization are discussed in [13, 1]. <p> combined together; view-synthesis methods use the 2D imagery directly without the need for a detailed 3D model [1]; video stabilization can be handled robustly under the most general situations (unlike existing methods) [13]; moving object detection from moving cameras can be handled using our duality of scene and motion invariants <ref> [6, 21, 7] </ref>; and extended scene representations will become more general than today's 2D mosaicing techniques [11] and more efficient than 3D CAD-based techniques. The use of true multi-view approaches will enable a simple and efficient way of relating infor mation across frames and across sensor modalities. <p> It provides a means for recovering 3D structure without the need to estimate camera motion (or camera geometry), and the means for detecting inconsistent 3D motions (i.e., independently moving objects) without the need to estimate neither camera motion (or camera geometry), nor scene geometry. For more details see <ref> [6] </ref>. The parallax geometry provides a multi-frame rigidity constraint which is complimentary to the epipolar rigidity constraint.
Reference: [7] <author> M. Irani and P. Anandan. </author> <title> A unified approach to moving object detection in 2D and 3D scenes. </title> <booktitle> In ARPA Image Understanding Workshop, pages by the unstabilized motion of the hand-held camera. </booktitle> <pages> 707-718, </pages> <address> Palm Springs, California, February 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: combined together; view-synthesis methods use the 2D imagery directly without the need for a detailed 3D model [1]; video stabilization can be handled robustly under the most general situations (unlike existing methods) [13]; moving object detection from moving cameras can be handled using our duality of scene and motion invariants <ref> [6, 21, 7] </ref>; and extended scene representations will become more general than today's 2D mosaicing techniques [11] and more efficient than 3D CAD-based techniques. The use of true multi-view approaches will enable a simple and efficient way of relating infor mation across frames and across sensor modalities.
Reference: [8] <author> M. Irani and P. Anandan. </author> <title> Robust multi-sensor alignment. </title> <booktitle> In IUW-97, </booktitle> <address> New Orleans, Louisiana, May 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Yet, these have been rarely used for multi-sensor alignment. These two constraints will be added to our ongoing multi-sensor alignment work, which currently exploits only appearance information, and is reported in <ref> [8] </ref>. 4 Parallax Geometry for Moving Ob ject Detection Moving object detection is a difficult problem in general. It has been shown to have robust solutions for specific cases.
Reference: [9] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Computing occluding and transparent motions. </title> <journal> International Journal of Computer Vision, </journal> <volume> 12(1) </volume> <pages> 5-16, </pages> <year> 1994. </year>
Reference-contexts: It has been shown to have robust solutions for specific cases. In particular, in cases where induces camera motion can be described parametrically (e.g., <ref> [9] </ref>), or in cases where the induced 3D scene information in the image plane is dominant relative to effects of moving objects.
Reference: [10] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Recovery of ego-motion using image stabilization. </title> <booktitle> In CVPR-94, </booktitle> <pages> pages 454-460, </pages> <address> Seattle, WA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The technical problem behind video stabilization is the extraction and subsequent cancelation of the rotational component of camera motion across the video sequence. Most existing methods take one of the following two approaches. One approach is to compute the camera rotation only after computing the camera translation (the epipole) <ref> [19, 10] </ref>. The second approach assumes a specific 3D scene structure, e.g. assuming the existence and the detection of 3D planes in the scene [10, 18]. <p> One approach is to compute the camera rotation only after computing the camera translation (the epipole) [19, 10]. The second approach assumes a specific 3D scene structure, e.g. assuming the existence and the detection of 3D planes in the scene <ref> [10, 18] </ref>. We have shown [13] that the camera rotation can be extracted directly from the tensor coefficients without the need to first recover the translational component of camera motion.
Reference: [11] <author> S. Peleg and J. Herman. </author> <title> Panoramic mosaics with videobrush. </title> <booktitle> In IUW-97, </booktitle> <address> New Orleans, Louisiana, May 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A novel and efficient motion recovery method has also been developed [12]. Implementations of these methods demonstrated superior numerical stability for purposes of new view generation and of video stabilization. In addition, the "manifold projection" method <ref> [11, 14] </ref> can provide "scene manifolds", extended panoramic views of a scene without the usual distortions and loss of resolution associated with the traditional perspective image mosaics. These capabilities are detailed below. 1.1 Expected Impact This research handles fundamental issues in multiple view correspondence, moving object detection, and visualization. <p> model [1]; video stabilization can be handled robustly under the most general situations (unlike existing methods) [13]; moving object detection from moving cameras can be handled using our duality of scene and motion invariants [6, 21, 7]; and extended scene representations will become more general than today's 2D mosaicing techniques <ref> [11] </ref> and more efficient than 3D CAD-based techniques. The use of true multi-view approaches will enable a simple and efficient way of relating infor mation across frames and across sensor modalities.
Reference: [12] <author> Y. Rosenberg and M. Werman. </author> <title> Representing local motion as a probability distribution matrix and object tracking. </title> <booktitle> In IUW-97, </booktitle> <address> New Orleans, Louisiana, May 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: URL: http://www.cs.huji.ac.il/labs/vision new view generation and to image stabilization are discussed in [13, 1]. Theoretical analysis shows that these methods are superior to existing two-frame methods, as they are free of degenerate scene or camera configurations [17]. A novel and efficient motion recovery method has also been developed <ref> [12] </ref>. Implementations of these methods demonstrated superior numerical stability for purposes of new view generation and of video stabilization.
Reference: [13] <author> B. Rousso, S. Avidan, A. Shashua, and S. Peleg. </author> <title> Robust recovery of camera rotation from three frames. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> San Francisco, Califor-nia, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Applications to fl This research was funded by DARPA through the U.S. Office of Naval Research under grant N00014-93-1-1202. URL: http://www.cs.huji.ac.il/labs/vision new view generation and to image stabilization are discussed in <ref> [13, 1] </ref>. Theoretical analysis shows that these methods are superior to existing two-frame methods, as they are free of degenerate scene or camera configurations [17]. A novel and efficient motion recovery method has also been developed [12]. <p> In general IU contributions, correspondence techniques become more robust as geometry and photometry are combined together; view-synthesis methods use the 2D imagery directly without the need for a detailed 3D model [1]; video stabilization can be handled robustly under the most general situations (unlike existing methods) <ref> [13] </ref>; moving object detection from moving cameras can be handled using our duality of scene and motion invariants [6, 21, 7]; and extended scene representations will become more general than today's 2D mosaicing techniques [11] and more efficient than 3D CAD-based techniques. <p> One approach is to compute the camera rotation only after computing the camera translation (the epipole) [19, 10]. The second approach assumes a specific 3D scene structure, e.g. assuming the existence and the detection of 3D planes in the scene [10, 18]. We have shown <ref> [13] </ref> that the camera rotation can be extracted directly from the tensor coefficients without the need to first recover the translational component of camera motion. <p> This method was implemented and shown to provide robust video stabilization under general conditions <ref> [13] </ref>. 6.2 Novel View Synthesis Another application of interest is the possibility of generating views from novel viewing positions from a collection of other views.
Reference: [14] <author> B. Rousso, S. Peleg, and I. Finci. </author> <title> Mosaicing with generalized strips. </title> <booktitle> In IUW-97, </booktitle> <address> New Orleans, Louisiana, May 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A novel and efficient motion recovery method has also been developed [12]. Implementations of these methods demonstrated superior numerical stability for purposes of new view generation and of video stabilization. In addition, the "manifold projection" method <ref> [11, 14] </ref> can provide "scene manifolds", extended panoramic views of a scene without the usual distortions and loss of resolution associated with the traditional perspective image mosaics. These capabilities are detailed below. 1.1 Expected Impact This research handles fundamental issues in multiple view correspondence, moving object detection, and visualization. <p> projection is a ball centered at the rotation point. (ii) When a camera is translating in a plane, always looking perpendicular to the plane, the "manifold projection" will be an orthographic projection onto that plane. (iii) When the camera is zooming or is moving forward, the manifold is a cylinder <ref> [14] </ref>. 6 Visualization 6.1 Video Stabilization Another application of the "3 views as unit of analysis" is for video stabilization, which is a necessary component for building a visualization engine of the project.
Reference: [15] <author> A. Shashua. </author> <title> Algebraic functions for recognition. </title> <journal> IEEE-PAMI, </journal> <volume> 17 </volume> <pages> 779-789, </pages> <year> 1995. </year>
Reference-contexts: These techniques rely in part on the use of multilinear constraints across three or more views and the trilinear tensor <ref> [15, 19] </ref>, parallax geometry [6], and the duality of camera and scene invariants [6, 21]. Applications to fl This research was funded by DARPA through the U.S. Office of Naval Research under grant N00014-93-1-1202. URL: http://www.cs.huji.ac.il/labs/vision new view generation and to image stabilization are discussed in [13, 1]. <p> This method proved effective in tracking many independently moving objects, like ants on the forest floor or people in a crowd [4]. 2 Tensor-based Image Alignment Three views give rise to a set of trilinear matching constraints that first became prominent in <ref> [15] </ref>, whose coefficients form a tensor ("trilinear tensor") which encodes the relative camera locations, and whose underlying theory has been studied intensively.
Reference: [16] <author> A. Shashua and S. Avidan. </author> <title> The rank4 constraint in multiple view geometry. </title> <booktitle> In ECCV-96, pages II:196-206, </booktitle> <address> Cambridge, UK, April 1996. </address> <publisher> Springer. </publisher>
Reference-contexts: The technical idea is based on the fact that under certain arrangements of views, the trilinear tensors of triplets of views live in a small dimensional subspace <ref> [16] </ref>. One implication of this result is that one can synthesize new tensors from a given tensor and user specification of the position of a novel camera.
Reference: [17] <author> A. Shashua and S. Maybank. </author> <title> Degenerate n points configurations of three views: do critical surfaces exist? Nov. </title> <year> 1996. </year>
Reference-contexts: Office of Naval Research under grant N00014-93-1-1202. URL: http://www.cs.huji.ac.il/labs/vision new view generation and to image stabilization are discussed in [13, 1]. Theoretical analysis shows that these methods are superior to existing two-frame methods, as they are free of degenerate scene or camera configurations <ref> [17] </ref>. A novel and efficient motion recovery method has also been developed [12]. Implementations of these methods demonstrated superior numerical stability for purposes of new view generation and of video stabilization. <p> In [2] we show that the fundamental matrix of two views is embedded in the tensorial equation as a rank-2 trivalent tensor, and we introduce a standard set of tensorial operators that apply to both two views and three views alike. In <ref> [17] </ref> we show that there are no critical surfaces for the computation of the trilinear tensor.
Reference: [18] <author> A. Shashua and N. Navab. </author> <title> Relative affine structure: Theory and application to 3D reconstruction from perspective views. </title> <booktitle> In CVPR-94, </booktitle> <pages> pages 483-489, </pages> <address> Seattle, WA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: One approach is to compute the camera rotation only after computing the camera translation (the epipole) [19, 10]. The second approach assumes a specific 3D scene structure, e.g. assuming the existence and the detection of 3D planes in the scene <ref> [10, 18] </ref>. We have shown [13] that the camera rotation can be extracted directly from the tensor coefficients without the need to first recover the translational component of camera motion.
Reference: [19] <author> A. Shashua and M. Werman. </author> <title> Trilinearity of three perspective views and its associated tensor. </title> <booktitle> In ICCV, </booktitle> <pages> pages 920-925, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year> <pages> IEEE-CS. </pages>
Reference-contexts: These techniques rely in part on the use of multilinear constraints across three or more views and the trilinear tensor <ref> [15, 19] </ref>, parallax geometry [6], and the duality of camera and scene invariants [6, 21]. Applications to fl This research was funded by DARPA through the U.S. Office of Naval Research under grant N00014-93-1-1202. URL: http://www.cs.huji.ac.il/labs/vision new view generation and to image stabilization are discussed in [13, 1]. <p> The technical problem behind video stabilization is the extraction and subsequent cancelation of the rotational component of camera motion across the video sequence. Most existing methods take one of the following two approaches. One approach is to compute the camera rotation only after computing the camera translation (the epipole) <ref> [19, 10] </ref>. The second approach assumes a specific 3D scene structure, e.g. assuming the existence and the detection of 3D planes in the scene [10, 18].
Reference: [20] <author> G. Stein and A. Shashua. </author> <title> Direct methods for estimation of structure and motion from three views. </title> <booktitle> In IUW-97, </booktitle> <address> New Orleans, Louisiana, </address> <month> May </month> <year> 1997. </year> <note> Morgan Kaufmann. To Appear in CVPR'97. </note>
Reference-contexts: The tensor elements can be linearly recovered from at least 7 matching points across three views, and the tenso-rial equations (trilinearities) provide a matching constraint for use in image alignment tasks (such as "image transfer" applications). In <ref> [20] </ref> we show that the tensor can combine the information of spatial-temporal derivatives instead of matching points with the geometric constraint of camera motion.
Reference: [21] <author> D. Weinshall, M. Werman, and A. Shashua. </author> <title> Duality of multi-point and multi-frame geometry: Fundamental shape matrices and tensors. </title> <booktitle> In ECCV-96, pages II:217-227, </booktitle> <address> Cambridge, UK, April 1996. </address> <publisher> Springer. </publisher>
Reference-contexts: These techniques rely in part on the use of multilinear constraints across three or more views and the trilinear tensor [15, 19], parallax geometry [6], and the duality of camera and scene invariants <ref> [6, 21] </ref>. Applications to fl This research was funded by DARPA through the U.S. Office of Naval Research under grant N00014-93-1-1202. URL: http://www.cs.huji.ac.il/labs/vision new view generation and to image stabilization are discussed in [13, 1]. <p> combined together; view-synthesis methods use the 2D imagery directly without the need for a detailed 3D model [1]; video stabilization can be handled robustly under the most general situations (unlike existing methods) [13]; moving object detection from moving cameras can be handled using our duality of scene and motion invariants <ref> [6, 21, 7] </ref>; and extended scene representations will become more general than today's 2D mosaicing techniques [11] and more efficient than 3D CAD-based techniques. The use of true multi-view approaches will enable a simple and efficient way of relating infor mation across frames and across sensor modalities.
References-found: 21

