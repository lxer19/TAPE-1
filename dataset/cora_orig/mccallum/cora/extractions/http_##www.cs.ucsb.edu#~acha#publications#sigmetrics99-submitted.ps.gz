URL: http://www.cs.ucsb.edu/~acha/publications/sigmetrics99-submitted.ps.gz
Refering-URL: http://www.cs.ucsb.edu/~acha/publications/sigmetrics99-submitted.html
Root-URL: http://www.cs.ucsb.edu
Title: Availability and Utility of Idle Memory in Workstation Clusters  
Author: Anurag Acharya Sanjeev Setia 
Date: 1998  
Note: October  (0.5 GB-5 GB) on these workstation pools.  
Address: Santa Barbara, CA 93106  Fairfax, VA 22030  
Affiliation: Dept. of Computer Science University of California  Dept. of Computer Science George Mason University  
Abstract: Technical Report TRCS98-26 Dept of Computer Science University of California, Santa Barbara Abstract In this paper, we examine the availability and utility of idle memory in workstation clusters. We attempt to answer the following questions. First, how much of the total memory in a workstation cluster can be expected to be idle? This provides an estimate of the opportunity for hosting guest data. Second, how much memory can be expected to be idle on individual workstations? This helps determine the recruitment policy how much memory should be recruited on individual hosts? Third, what is the distribution of memory idle-times? This indicates how long guest data can be expected to survive; applications that access their data-sets frequently within the expected life-time of guest data are more likely to benefit from exploiting idle memory. Fourth, how much performance improvement can be achieved for off-the-shelf clusters without customizing the operating system and/or the processor firmware? Finally, how long and how frequently might a user have to wait to reclaim her machine if she volunteers to host guest pages on her machine? This helps answer the question of social acceptability. To answer the questions relating to the availability of idle memory, we have analyzed two-week long traces from two workstation pools with different sizes, locations, and patterns of use. To evaluate the expected benefits and costs, we have simulated five data-intensive applications 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, G. Edjlali, and J. Saltz. </author> <title> The utility of exploiting idle workstations for parallel computation. </title> <booktitle> In Proceedings of 1997 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1997. </year>
Reference-contexts: Exploiting idle workstations for hosting guest computation has been a popular research area. Systems that utilize idle workstations for running sequential jobs have been in production use for many years [15]; there has been significant amount of research on exploiting idle workstations for hosting parallel computations (for e.g. <ref> [1, 4, 5] </ref>).
Reference: [2] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In Proc. of 20th Int'l Conf. on Very Large Databases (VLDB), </booktitle> <address> Santiago, Chile, </address> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The data was stored in the DB2 database from IBM. The total database size was 5.2 GB (including the indices) and it was stored in 831 files. dmine: this application tries to extract association rules from retail data <ref> [2, 19] </ref>. The dataset consists of 50 million transactions, with an average transaction size of 10 items and a maximal potentially frequent set size of 3 (see [2, 19] for details). <p> 5.2 GB (including the indices) and it was stored in 831 files. dmine: this application tries to extract association rules from retail data <ref> [2, 19] </ref>. The dataset consists of 50 million transactions, with an average transaction size of 10 items and a maximal potentially frequent set size of 3 (see [2, 19] for details). The dataset size for this program was 4 GB and was partitioned into 8 files. lu: this application computes the dense LU decomposition of an out-of-core matrix [13].
Reference: [3] <author> Anonymous. </author> . <type> Technical Report TRCS98-02, </type> <institution> Dept of Computer Science, University of , 1998. </institution>
Reference-contexts: The design of this system, known as Dodo <ref> [3] </ref>, is based on that of Condor [15] which harvests idle processors. Each application is linked to a library that implements the functionality needed by the application in order to create, read, write, and delete remote memory regions.
Reference: [4] <author> R. Arpaci, A. Dusseau, A. Vahdat, L. Liu, T. Anderson, and D. Patterson. </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations. </title> <booktitle> In Proceedings of the 1995 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 267-78, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Exploiting idle workstations for hosting guest computation has been a popular research area. Systems that utilize idle workstations for running sequential jobs have been in production use for many years [15]; there has been significant amount of research on exploiting idle workstations for hosting parallel computations (for e.g. <ref> [1, 4, 5] </ref>). <p> Note that the memory recruitment policy used in our simulations selects hosts randomly from among the idle hosts in the cluster; if necessary, a policy such as that proposed by Arpaci et al <ref> [4] </ref> can be used to limit the number of reclamations per day for a particular host. Impact of variation in network interconnect: Our next set of results examines the impact of changing the network interconnect on the speedup obtained for individual applications.
Reference: [5] <author> A. Chowdhury, L. Nicklas, S. Setia, and E. White. </author> <title> Supporting dynamic space-sharing on clusters of non-dedicated workstations. </title> <booktitle> In Proceedings of the 17th International Conference on Distributed Computing, </booktitle> <year> 1997. </year>
Reference-contexts: Exploiting idle workstations for hosting guest computation has been a popular research area. Systems that utilize idle workstations for running sequential jobs have been in production use for many years [15]; there has been significant amount of research on exploiting idle workstations for hosting parallel computations (for e.g. <ref> [1, 4, 5] </ref>).
Reference: [6] <author> A. Cockcroft. </author> <title> How Much RAM is Enough? Sun World Online 11 , May 1996. </title>
Reference-contexts: The second command is used for two purposes. First, it reports the amount of memory available for allocation (reported as totalpages). The difference between the total physical memory and memory available for allocation is used for the initial load of the kernel <ref> [6] </ref>. Second, it reads the system pages kernel statistics structure to determine the number of pages used by the kernel from this available pool [6] (see Figure 1 for sample output from this command). * List of active processes: we used ps to determine the list of active processes. <p> The difference between the total physical memory and memory available for allocation is used for the initial load of the kernel <ref> [6] </ref>. Second, it reads the system pages kernel statistics structure to determine the number of pages used by the kernel from this available pool [6] (see Figure 1 for sample output from this command). * List of active processes: we used ps to determine the list of active processes. Every process that is reported by ps to consume non-zero CPU time was marked as an active process.
Reference: [7] <author> D. Comer and J. Griffioen. </author> <title> A new design for distributed systems: The remote memory model. </title> <booktitle> In Proceedings of the 1990 USENIX Summer Conference, </booktitle> <year> 1990. </year> <note> 11 http://www.sunworld.com/sunworldonline/swol-05-1996/swol-05-perf.html 19 </note>
Reference-contexts: Second, commodity-priced machines have a limit on the amount of memory; server-class machines that can have larger memories come at a substantial premium. Several research projects have proposed using memory of idle workstations for hosting guest data <ref> [7, 8, 11, 12, 14, 20, 22, 27] </ref>.
Reference: [8] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson. </author> <title> Cooperative Caching: Using Remote Memory to Improve File System Performance. </title> <booktitle> In Proceedings of the First Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 267-80, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: Second, commodity-priced machines have a limit on the amount of memory; server-class machines that can have larger memories come at a substantial premium. Several research projects have proposed using memory of idle workstations for hosting guest data <ref> [7, 8, 11, 12, 14, 20, 22, 27] </ref>. <p> extending the memory hierarchy of mul-ticomputers by introducing a remote memory server layer; Felten and Zahorjan [12] examined the idea of using remote client memory instead of disk for virtual memory paging; Schilit and Duchamp [23] investigated the use of remote memory paging for diskless portable machines; Dahlin et al <ref> [8] </ref> and Sarkar et al [22] propose schemes to use idle memory to increase the effective file cache size; Feeley et al [11] describe a low-level global memory management system that uses idle memory to back up both file pages and virtual memory page.
Reference: [9] <author> S. Donaldson, J. Hill, and D. Skillicorn. </author> <title> BSP Clusters: High Performance, Reliable and Very Low Cost. </title> <type> Technical Report PRG-TR-5-98, </type> <institution> Oxford University Computing Laboratory, </institution> <year> 1998. </year>
Reference-contexts: We based these parameters on a commercial evaluation report of a gigabit ethernet switch [25] and on a description of an efficient communication library for PC clusters <ref> [9] </ref>. To evaluate the impact of network bandwidth on the performance of remote caching, we performed additional experiments assuming a 100Mbps switched ethernet interconnect with 10 MB/s end-to-end bandwidth and 30 s end-to-end latency (based on [18]).
Reference: [10] <author> Y. Endo, Z. Wang, J. B. Chen, and M. Seltzer. </author> <title> Using latency to evaluate interactive system performance. </title> <booktitle> In Proceedings of the 2nd USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 185-200, </pages> <year> 1996. </year>
Reference-contexts: This delay is a result of the workstation's memory contents being paged out to accommodate guest data while the workstation was idle. Measuring this delay directly is difficult it poses some of the same challenges that arise in benchmarking interactive applications <ref> [10] </ref>. However, this delay will depend on the size of the memory context that will need to be re-established when the owner reclaims her workstations. We refer to this metric as the memory context size (M CS) and use it as a measure of the cost of exploiting idle memory.
Reference: [11] <author> M. Feeley, W. Morgan, F. Pighin, A. Karlin, H. Levy, and C. Thekkath. </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 201-12, </pages> <month> Dec </month> <year> 1995. </year>
Reference-contexts: Second, commodity-priced machines have a limit on the amount of memory; server-class machines that can have larger memories come at a substantial premium. Several research projects have proposed using memory of idle workstations for hosting guest data <ref> [7, 8, 11, 12, 14, 20, 22, 27] </ref>. <p> remote client memory instead of disk for virtual memory paging; Schilit and Duchamp [23] investigated the use of remote memory paging for diskless portable machines; Dahlin et al [8] and Sarkar et al [22] propose schemes to use idle memory to increase the effective file cache size; Feeley et al <ref> [11] </ref> describe a low-level global memory management system that uses idle memory to back up both file pages and virtual memory page. These efforts have focused on developing efficient mechanisms for hosting guest data. In this paper, we examine the opportunity for hosting guest data in real workstation clusters. <p> Each application is linked to a library that implements the functionality needed by the application in order to create, read, write, and delete remote memory regions. In contrast to global memory systems such as GMS <ref> [11] </ref> where 10 (a) all clusterA hosts (b) idle clusterA hosts (c) all clusterB hosts (d) idle clusterB hosts Host type 4MB 8MB 16MB 32MB 64MB 128MB 256MB 512MB 32MB hosts (all) 32hrs 6hrs 1hr - 32MB hosts (idle) 24min 16min 5min - 64MB hosts (all) 38hrs 25hrs 9hrs 39min -
Reference: [12] <author> E. Felten and J. Zahorjan. </author> <title> Issues in implementation of a remote memory paging system. </title> <type> Technical Report 91-03-09, </type> <institution> Department of Computer Science, University of Washington, </institution> <year> 1991. </year>
Reference-contexts: Second, commodity-priced machines have a limit on the amount of memory; server-class machines that can have larger memories come at a substantial premium. Several research projects have proposed using memory of idle workstations for hosting guest data <ref> [7, 8, 11, 12, 14, 20, 22, 27] </ref>. <p> Several research projects have proposed using memory of idle workstations for hosting guest data [7, 8, 11, 12, 14, 20, 22, 27]. Iftode et al [14] propose extending the memory hierarchy of mul-ticomputers by introducing a remote memory server layer; Felten and Zahorjan <ref> [12] </ref> examined the idea of using remote client memory instead of disk for virtual memory paging; Schilit and Duchamp [23] investigated the use of remote memory paging for diskless portable machines; Dahlin et al [8] and Sarkar et al [22] propose schemes to use idle memory to increase the effective file
Reference: [13] <author> B. Hendrickson and D. Womble. </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 15(5), </volume> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The dataset size for this program was 4 GB and was partitioned into 8 files. lu: this application computes the dense LU decomposition of an out-of-core matrix <ref> [13] </ref>. The dataset consisted of an 8192 fi 8192 double precision matrix (total size 536 MB) with a slab size of 64 columns. The data is stored in 8 files. We chose these three applications as representatives of three classes of applications.
Reference: [14] <author> L. Iftode, K. Li, and K. Petersen. </author> <title> Memory Servers for Multicomputers. </title> <booktitle> In COMPCON Spring'93 Digest of Papers, </booktitle> <pages> pages 538-47, </pages> <month> Feb </month> <year> 1993. </year>
Reference-contexts: Second, commodity-priced machines have a limit on the amount of memory; server-class machines that can have larger memories come at a substantial premium. Several research projects have proposed using memory of idle workstations for hosting guest data <ref> [7, 8, 11, 12, 14, 20, 22, 27] </ref>. <p> Second, commodity-priced machines have a limit on the amount of memory; server-class machines that can have larger memories come at a substantial premium. Several research projects have proposed using memory of idle workstations for hosting guest data [7, 8, 11, 12, 14, 20, 22, 27]. Iftode et al <ref> [14] </ref> propose extending the memory hierarchy of mul-ticomputers by introducing a remote memory server layer; Felten and Zahorjan [12] examined the idea of using remote client memory instead of disk for virtual memory paging; Schilit and Duchamp [23] investigated the use of remote memory paging for diskless portable machines; Dahlin et
Reference: [15] <author> M. Litzkow and M. Livny. </author> <title> Experiences with the Condor Distributed Batch System. </title> <booktitle> In Proceedings of the IEEE Workshop on Experimental Distributed Systems, </booktitle> <pages> pages 97-101, </pages> <month> Oct </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Idle workstations have traditionally been harvested for their cycles. Exploiting idle workstations for hosting guest computation has been a popular research area. Systems that utilize idle workstations for running sequential jobs have been in production use for many years <ref> [15] </ref>; there has been significant amount of research on exploiting idle workstations for hosting parallel computations (for e.g. [1, 4, 5]). <p> The design of this system, known as Dodo [3], is based on that of Condor <ref> [15] </ref> which harvests idle processors. Each application is linked to a library that implements the functionality needed by the application in order to create, read, write, and delete remote memory regions.
Reference: [16] <author> R. McDougal. </author> <title> The RMCmem kernel module. </title> <address> ftp://playground.sun.com/pub/memtool, </address> <month> June </month> <year> 1998. </year> <title> [17] 512 MB PC100 SDRAM with ECC 12 . Micro X-press Inc. </title> <address> 5406 west 78th street Indianapolis, </address> <note> IN 46268, </note> <month> Oct </month> <year> 1998. </year>
Reference-contexts: Processes created for capturing information for these experiments were not considered active. * Physical memory used by active processes: we determine the physical memory used by active processes using the memps and pmem 3 utilities provided by RMCmem, the experimental kernel module made available by Richard McDougal <ref> [16] </ref>. The memps utility provides information about the total physical memory used by a list of processes. For each process in the list, it provides a breakdown of the resident set size into shared and private regions.
Reference: [18] <author> Mier Communications. </author> <title> Evaluation of 10/100 BaseT Switches. </title> <address> http://www.mier.com/reports/- cisco/cisco2916mxl.pdf, </address> <month> April </month> <year> 1998. </year>
Reference-contexts: To evaluate the impact of network bandwidth on the performance of remote caching, we performed additional experiments assuming a 100Mbps switched ethernet interconnect with 10 MB/s end-to-end bandwidth and 30 s end-to-end latency (based on <ref> [18] </ref>). We assume that the workstation has a 133 MB/s PCI bus with one Ultra-SCSI (40 MB/s) string and two Ultra-SCSI disks. The disk parameters used in our simulation were based on published parameters of the Seagate Cheetah 9 disk [24].
Reference: [19] <author> A. Mueller. </author> <title> Fast sequential and parallel algorithms for association rule mining: A comparison. </title> <type> Technical Report CS-TR-3515, </type> <institution> University of Maryland, College Park, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: The data was stored in the DB2 database from IBM. The total database size was 5.2 GB (including the indices) and it was stored in 831 files. dmine: this application tries to extract association rules from retail data <ref> [2, 19] </ref>. The dataset consists of 50 million transactions, with an average transaction size of 10 items and a maximal potentially frequent set size of 3 (see [2, 19] for details). <p> 5.2 GB (including the indices) and it was stored in 831 files. dmine: this application tries to extract association rules from retail data <ref> [2, 19] </ref>. The dataset consists of 50 million transactions, with an average transaction size of 10 items and a maximal potentially frequent set size of 3 (see [2, 19] for details). The dataset size for this program was 4 GB and was partitioned into 8 files. lu: this application computes the dense LU decomposition of an out-of-core matrix [13].
Reference: [20] <author> T. Narten and R. Yavatkar. </author> <title> Remote Memory as a Resource in Distributed Systems. </title> <booktitle> In Proceedings of the 3rd Workshop on Workstation Operating Systems, </booktitle> <pages> pages 132-6, </pages> <address> April 1992. [21] 128 MB PC100 SDRAM 13 . Advanced PCBoost, PO Box 80811, Rancho Santa Margarita, CA 92688, </address> <month> Oct </month> <year> 1998. </year>
Reference-contexts: Second, commodity-priced machines have a limit on the amount of memory; server-class machines that can have larger memories come at a substantial premium. Several research projects have proposed using memory of idle workstations for hosting guest data <ref> [7, 8, 11, 12, 14, 20, 22, 27] </ref>.
Reference: [22] <author> P. Sarkar and J. Hartman. </author> <title> Efficient cooperative caching using hints. </title> <booktitle> In Proceedings of the 2nd Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1996. </year>
Reference-contexts: Second, commodity-priced machines have a limit on the amount of memory; server-class machines that can have larger memories come at a substantial premium. Several research projects have proposed using memory of idle workstations for hosting guest data <ref> [7, 8, 11, 12, 14, 20, 22, 27] </ref>. <p> mul-ticomputers by introducing a remote memory server layer; Felten and Zahorjan [12] examined the idea of using remote client memory instead of disk for virtual memory paging; Schilit and Duchamp [23] investigated the use of remote memory paging for diskless portable machines; Dahlin et al [8] and Sarkar et al <ref> [22] </ref> propose schemes to use idle memory to increase the effective file cache size; Feeley et al [11] describe a low-level global memory management system that uses idle memory to back up both file pages and virtual memory page.
Reference: [23] <author> B. Schilit and D. Duchamp. </author> <title> Adaptive remote paging for mobile computers. </title> <type> Technical Report CUCS-004-91, </type> <institution> Department of Computer Science, Columbia University, </institution> <year> 1991. </year> <note> 12 http://www.microx-press.com/online/ following link from http://www.pricewatch.com 13 http://www.pcboost.com following link from http://www.pricewatch.com 20 </note>
Reference-contexts: Iftode et al [14] propose extending the memory hierarchy of mul-ticomputers by introducing a remote memory server layer; Felten and Zahorjan [12] examined the idea of using remote client memory instead of disk for virtual memory paging; Schilit and Duchamp <ref> [23] </ref> investigated the use of remote memory paging for diskless portable machines; Dahlin et al [8] and Sarkar et al [22] propose schemes to use idle memory to increase the effective file cache size; Feeley et al [11] describe a low-level global memory management system that uses idle memory to back
Reference: [24] <author> Seagate Technology Inc. </author> <title> The Cheetah 9LP Family: ST39102 Product Manual, </title> <month> July </month> <year> 1998. </year> <note> Publication number 83329240 Rev B. </note>
Reference-contexts: We assume that the workstation has a 133 MB/s PCI bus with one Ultra-SCSI (40 MB/s) string and two Ultra-SCSI disks. The disk parameters used in our simulation were based on published parameters of the Seagate Cheetah 9 disk <ref> [24] </ref>. We assume an average seek time of 5.8ms, a maximum seek time of 15.7ms, a rotational speed of 10000 RPM, and an average media transfer rate of 18 MB/s.
Reference: [25] <institution> The Tolly Group. 3Com Corporation Superstack II 9300 Gigabit Ethernet Performance. </institution> <note> Available off http://www.tolly.com, Jan 1998. </note>
Reference-contexts: For the core set of experi-ments, we assumed the workstations in the cluster were connected by a gigabit ethernet with an end-to-end bandwidth of 70 MB/s, and an end-to-end latency of 7.4s. We based these parameters on a commercial evaluation report of a gigabit ethernet switch <ref> [25] </ref> and on a description of an efficient communication library for PC clusters [9]. To evaluate the impact of network bandwidth on the performance of remote caching, we performed additional experiments assuming a 100Mbps switched ethernet interconnect with 10 MB/s end-to-end bandwidth and 30 s end-to-end latency (based on [18]).
Reference: [26] <author> M. Uysal, A. Acharya, and J. Saltz. </author> <title> Requirements of I/O systems for parallel machines: An application-driven study. </title> <type> Technical Report CS-TR-3802, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1997. </year>
Reference-contexts: The idle memory daemon recruits the remaining memory. 3.2 Applications We used three real applications and two synthetic benchmarks in our study. For the real applications, we used traces captured by running these applications on realistic workloads <ref> [26] </ref>. These traces were obtained using AIX trace utility on eight processors of an IBM SP-2. <p> The motivation for using a "first in" policy comes from the fact that the applications used in our simulations exhibit scan and triangle-scan I/O access patterns <ref> [26] </ref>. For such applications, a "first in" policy is a good caching policy. In addition, we used two data-intensive synthetic benchmarks to evaluate the impact of access-patterns not exhibited by the real applications. For both these benchmarks, we used a 2 GB dataset.
Reference: [27] <author> G. Voelker, E. Anderson, T. Kimbrel, M. Feeley, J. Chase, A. Karlin, and H. Levy. </author> <title> Implementing cooperative prefetching and caching in a globally-managed memory system. </title> <booktitle> In Proceedings of the 1998 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 33-43, </pages> <year> 1998. </year>
Reference-contexts: Second, commodity-priced machines have a limit on the amount of memory; server-class machines that can have larger memories come at a substantial premium. Several research projects have proposed using memory of idle workstations for hosting guest data <ref> [7, 8, 11, 12, 14, 20, 22, 27] </ref>.
Reference: [28] <author> G. Voelker, H. Jamrozik, M. Vernon, H. Levy, and E. Lazowska. </author> <title> Managing Server Load in Global Memory Systems. </title> <booktitle> In Proceedings of the 1997 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 127-138, </pages> <month> June </month> <year> 1997. </year> <month> 21 </month>
Reference-contexts: We make this assumption for social acceptance reasons. Workstation owners are usually reluctant to have their resources utilized by guests while it is in use. As shown by a recent study <ref> [28] </ref>, serving remote memory requests can potentially degrade the performance of local jobs. In order to further limit the impact of hosting guest data on workstation owners, we restrict the maximum amount of memory that is recruited from a workstation.
References-found: 26

