URL: http://http.cs.berkeley.edu/~rywang/papers/sigmetrics94.ps
Refering-URL: http://http.cs.berkeley.edu/~rywang/papers/sigmetrics94.html
Root-URL: 
Email: pattrsn-@cs.berkeley.edu  
Title: A Quantitative Analysis of Cache Policies for Scalable Network File Systems  
Author: Michael D. Dahlin, Clifford J. Mather, Randolph Y. Wang, Thomas E. Anderson, and David A. Patterson 
Note: This work is supported in part by the Advanced Research Projects Agency (N00600-93-C-2481), the National Science Foundation (CDA 8722788), California MICRO, Digital Equipment Corporation, the AT&T Foundation, Xerox Corporation, and Siemens Corporation. Dahlin was also supported under a National Science Foundation Graduate Research Fellowship. Anderson was also supported by a National Science Foundation Young Investigator Award.  
Address: -dahlin, cjmather, rywang, tea,  
Affiliation: Computer Science Division, University of California at Berkeley  
Abstract: more than commodity workstations, even for the server. In reality, the widely used SUN Network File System, NFS [Sand85], has spawned a new industry dedicated to building the high-performance multiprocessor systems needed to scale NFS to more than a few dozen clients. The Andrew File System, AFS [Howa88], was designed to reduce server load relative to NFS in the interest of scalability, but its ultimate scalability is limited because AFS still relies on a central server to receive a copy of all modified data and to supply data for all client cache miss requests. Also, NFS and AFS must generally use specialized servers rather than commodity desktop workstations as server machines because the server must support enough disks to hold a copy of the entire file system, and desktop workstations are generally limited to a single SCSI string. Commodity workstations are a more cost effective way to buy computing power and I/O bandwidth because server machines must be designed with greater I/O expandability and because development costs of the more complicated servers must be amortized over a smaller sales volume. For instance in the SUN product line, a server costs three times as much as a similarly configured workstation. Trends in file system use promise to place even heavier demands on the central servers of file systems. Baker et al. [Bake91] report that the size of large files grew by an order of magnitude between 1985 and 1992. If this trend continues, the cost of transferring all data through the central server may become prohibitive. File systems are also being asked to manage data over wide area networks (WANs) where bandwidth restrictions limit the amount of data that can be supplied from a central source. At the same time, technology trends are giving clients tremendous amounts of disk space, main memory, and processing power and are also providing high-speed low-latency networks to tie these resources together. Inexpensive disks make it feasible for clients to store large amounts of data locally. A 1.3 GB SCSI disk currently costs less than $1000, and most workstations sold today are configured with significant amounts of local disk. Similarly, the aggregate memories and processing resources of client machines dwarf the capacity of a single server machine. Additionally, high speed local area networks (LANs) allow clients to access data from peers across a local area network almost as quickly as they can access local data. This paper investigates the quantitative benefits of utilizing cache techniques oriented towards extreme scalability Abstract Current network file system protocols rely heavily on a central server to coordinate file activity among client workstations. This central server can become a bottleneck that limits scalability for environments with large numbers of clients. In central server systems such as NFS and AFS, all client writes, cache misses, and coherence messages are handled by the server. To keep up with this workload, expensive server machines are needed, configured with high-performance CPUs, memory systems, and I/O channels. Since the server stores all data, it must be physically capable of connecting to many disks. This reliance on a central server also makes current systems inappropriate for wide area network use where the network bandwidth to the server may be limited. In this paper, we investigate the quantitative performance effect of moving as many of the server responsibilities as possible to client workstations to reduce the need for high-performance server machines. We have devised a cache protocol in which all data reside on clients and all data transfers proceed directly from client to client. The server is used only to coordinate these data transfers. This protocol is being incorporated as part of our experimental file system, xFS. We present results from a trace-driven simulation study of the protocol using traces from a 237 client NFS installation. We find that the xFS protocol reduces server load by more than a factor of six compared to AFS without significantly affecting response time or file availability. 
Abstract-found: 1
Intro-found: 1
Reference: [Arch86] <author> James Archibald and Jean-Loup Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4:273298, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: The protocol has four pieces inspired by efforts to achieve scalable cache coherence in massively parallel processors <ref> [Arch86, Leno90] </ref>. The protocol uses a no write through policy, utilizes client-to-client data transfers, implements write ownership, and takes advantage of cluster servers. In this paper we compare the effects of this protocol to a baseline AFS system. <p> Write ownership based cache consistency. The first time a client closes a modified file the server is notified, triggering an invalidation of all copies cached on other clients. At that point the client has exclusive write ownership <ref> [Arch86] </ref> and may modify the file freely without notifying the server; there are no other copies of the data to be invalidated. A client will lose exclusive ownership of a file when another client opens the file for reading. Its copy will be invalidated if another client acquires exclusive ownership.
Reference: [Bake91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proc. of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 198212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Modified files remain on each clients local disk. The elimination of write through is motivated by a number of studies showing that when a client writes a file, it is often deleted or quickly rewritten by the same client <ref> [Thom87, Bake91, Blaz91, Kist92] </ref>. We confirmed this pattern for the Berkeley NFS traces. <p> This solution is scalable since it adds no additional server messages if the server knows ahead of time which clients mirror writes to each other. The copy delay chosen is a trade-off between performance and availability guarantees with longer delays significantly reducing client-to-client bandwidth <ref> [Bake91] </ref> while increasing the length of time the file is vulnerable to a single point failure. In the future we plan to investigate these trade-offs for cli-ent-to-client transfers and also plan to look at using striping to reduce the cost of high availability.
Reference: [Birr93] <author> Andrew D. Birrell, Andy Hisgen, Chuck Jerian, Timothy Mann, and Garett Swart. </author> <title> The Echo Distributed File System. </title> <type> Technical Report 111, </type> <institution> Digital Equipment Corp. Systems Research Center, </institution> <year> 1993. </year>
Reference-contexts: If stronger availability guarantees are needed, client-to-client data replication of recently modified data provides a scalable solution. Shortly after a client closes a file for writing, it would send the data to one or more other clients <ref> [Lisk91, Birr93] </ref>. This solution is scalable since it adds no additional server messages if the server knows ahead of time which clients mirror writes to each other.
Reference: [Blaz91] <author> Matt Blaze and Rafael Alonso. </author> <title> Long-Term Caching Strategies for Very Large Distributed File 13 Systems. </title> <booktitle> In Proc. of the Summer 1991 USENIX, </booktitle> <pages> pages 3 15, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Modified files remain on each clients local disk. The elimination of write through is motivated by a number of studies showing that when a client writes a file, it is often deleted or quickly rewritten by the same client <ref> [Thom87, Bake91, Blaz91, Kist92] </ref>. We confirmed this pattern for the Berkeley NFS traces. <p> Clustering is inspired by the DASH multiprocessor architecture [Leno90] which clusters processing nodes on busses as we cluster clients on LANs. Clustering improves scalability by off-loading some central server state to cluster servers and isolating the central server from changes in state only affecting clients in the cluster <ref> [Blaz91, Munt92, Sand92] </ref>. <p> The diskless Sprite clients, however, must write data through to the server within about 30 seconds to reduce vulnerability to crashes. xFSs extends delayed writes to a no write through policy by using the cli-ents local disks. Blaze and Alonso <ref> [Blaz91, Blaz92] </ref> suggest dynamically building hierarchies for widely shared data. Once a server has supplied a threshold number of copies of a file, the server will refuse to supply the data to any more clients. Instead, the request will be forwarded to a client already caching the file.
Reference: [Blaz92] <author> Matt Blaze and Rafael Alonso. </author> <title> Dynamic Hierarchical Caching in Large-Scale Distributed File Systems. </title> <booktitle> In Proc. of the 12th International Conf. on Distributed Computing Systems, </booktitle> <pages> pages 521528, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The diskless Sprite clients, however, must write data through to the server within about 30 seconds to reduce vulnerability to crashes. xFSs extends delayed writes to a no write through policy by using the cli-ents local disks. Blaze and Alonso <ref> [Blaz91, Blaz92] </ref> suggest dynamically building hierarchies for widely shared data. Once a server has supplied a threshold number of copies of a file, the server will refuse to supply the data to any more clients. Instead, the request will be forwarded to a client already caching the file.
Reference: [Blaz93] <author> Matt Blaze. </author> <title> Caching in Large-Scale Distributed File Systems. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: During the full seven day trace 141,574 files were referenced. We gathered this trace by monitoring network activity on each of the four Ethernets. On each subnet we placed a workstation that monitored all network traffic using rpcspy <ref> [Blaz93] </ref> which is built on the Ultrix Packetfilter interface [Mogu87]. Over the trace period, rpcspy reported that it dropped 4% of all network traffic calls due to buffer over-ow. We postprocessed the NFS trace to reect the semantics of the AFS and xFS protocols.
Reference: [Chen93] <author> Peter M. Chen and David A. Patterson. </author> <title> A New Approach to I/O Performance EvaluationSelf-Scaling I/O Benchmarks, Predicted I/O Performance. </title> <booktitle> In Proc. of 1993 ACM SIGMETRICS, </booktitle> <pages> pages 112, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We estimated this time by measuring the time for a DECstation 5000/200 to handle an NFS getattr request. For the CPU bandwidth for large requests, we use the time to supply file system data from the in-memory file cache reported in <ref> [Chen93] </ref>. We assume that the machines use disks that rotate at 5400 RPM, that the typical seek time is 4ms 2 , and that the disk bandwidth is 2MB/s.
Reference: [Coyn93] <author> Robert A. Coyne and Harry Hulen. </author> <title> An Introduction to the Mass Storage System Reference Model, Version 5. </title> <booktitle> In Twelfth IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 4753, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Client-to-client data transfers are an example of separating the control and data paths as sug-gested by the Mass Storage Reference Model <ref> [Coyn93] </ref>. The first two parts of the protocol allow all data to be stored on client disks, implementing what is referred to for multiprocessors as a cache only memory architecture (COMA) [Hage92, Rost93]. <p> The mass storage system reference model <ref> [Coyn93] </ref> decouples location and name service from the actual storage of data. The model defines a name server and location server that locate the storage server that actually manages the bitfile.
Reference: [Gold93] <author> Jonathan S. Goldick, Kathy Benninger, Woody Brown, Christopher Kirby, Christopher Maher, Daniel S. Nydick, and Bill Zumach. </author> <title> An AFS-Based Supercomputing Environment. </title> <booktitle> In Twelfth IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 127132, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The mass storage system reference model [Coyn93] decouples location and name service from the actual storage of data. The model defines a name server and location server that locate the storage server that actually manages the bitfile. Goldick et al. <ref> [Gold93] </ref> have implemented an AFS-based storage system which allows data to reside in up to 32 separate locations. In xFS each client acts as a storage server, and server and cluster servers together act as a two-level location server.
Reference: [Hage92] <author> Erik Hagersten, Anders Landin, and Seif Haridi. </author> <booktitle> DDMA Cache-Only Memory Architecture. IEEE Computer, </booktitle> <address> 25(9):4554, </address> <year> 1992. </year>
Reference-contexts: The first two parts of the protocol allow all data to be stored on client disks, implementing what is referred to for multiprocessors as a cache only memory architecture (COMA) <ref> [Hage92, Rost93] </ref>. An important detail of this approach is that we must guarantee that the clients dont dis-card the last copy of any file. The clients do this by marking one copy of each file as permanent.
Reference: [Henn90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: This estimate of the typical disk overhead differs from the aver-age seek time reported by manufacturers because it accounts for locality seen in real workloads <ref> [Henn90] </ref> while the manufacturer-reported average seek is the mean time over all possible source and destination tracks seeks that average one third of the distance across the disk surface.
Reference: [Hitz90] <author> David Hitz, Guy Harris, James K. Lau, and Allan M. Schwartz. </author> <title> Using UNIX as One Component of a Lightweight Distributed Kernel for Multiprocessor File Servers. </title> <booktitle> In Proc. of the Winter 1990 USENIX, </booktitle> <pages> pages 285 296, </pages> <year> 1990. </year>
Reference-contexts: Section 6 examines the potentially thorny issues of backup, availability, and security that arise when clients are given responsibilities that were formerly the servers. We survey related network 1. The Auspex is built with special hardware to allow it to support this large number of clients <ref> [Hitz90] </ref>. file system studies in Section 7. Finally in Section 8 we summarize our conclusions. 2 File System Cache Protocols An important factor in a file systems scalability is its caching policy. File systems use caches to improve response time and to reduce server load.
Reference: [Howa88] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):5181, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: This section surveys some other combinations of these schemes that have been suggested as methods to achieve scalable file systems. The Andrew file system, AFS, was designed with scalability as a main criteria <ref> [Howa88, Saty90] </ref>.
Reference: [Katz91] <author> Randy H. Katz, Thomas E. Anderson, John K. Ousterhout, and David A. Patterson. </author> <title> Robo-Line Storage: Low Latency High Capacity Storage Systems Over Geographically Distributed Networks. </title> <type> Sequoia 2000 Technical Report 91/3, </type> <institution> University of California, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: We plan to use tertiary storage robots to manage the backup media. Tertiary storage robots provide from hundreds of gigabytes to tens of terabytes of storage with file access times measured in tens of seconds <ref> [Katz91] </ref>. The robots provide deep storage as traditional tape systems do, but they have the added advantage that all files are on-line in the sense that a user may access the data without human intervention.
Reference: [Kist92] <author> James J. Kistler and M. Satyanarayanan. </author> <title> Disconnected Operation in the Coda File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1):325, </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: Modified files remain on each clients local disk. The elimination of write through is motivated by a number of studies showing that when a client writes a file, it is often deleted or quickly rewritten by the same client <ref> [Thom87, Bake91, Blaz91, Kist92] </ref>. We confirmed this pattern for the Berkeley NFS traces. <p> File systems can also be replicated across multiple servers to improve scalability for reading files, at a cost of making file writes more expensive <ref> [Lisk91, Kist92] </ref>. We will show in Section 5 that xFS-style clustering reduces the cost of both reads and writes. The combination of these four changes allows xFS to be dramatically more scalable than AFS. <p> We are particularly concerned about minimizing network usage for wide area network file systems, where network bandwidth can become a bottleneck. Bandwidth can also be an issue for mobile computing using wireless interconnects <ref> [Kist92] </ref>. xFS using the assumption that each packet sent has a header of 128 bytes. The table indicates that xFS reduces total network traffic by 52%. <p> Even higher availability could be achieved using explicit data replication. Large client caches and file replication from caching and backup reduce xFSs vulnerability to unavailable clients. Large client caches provide some insulationthe crash of one machine will often not be noticed by others <ref> [Kist92] </ref>. xFS also automatically stores redundant copies of shared read files in different caches increasing the availability of those files, and on-line backup provides added copies of older files.
Reference: [Lazo86] <author> Edward D. Lazowska, John Zahorjan, David R. Cheriton, and Willy Zwaenepoel. </author> <title> File Access Performance of Diskless Workstations. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(3):238268, </volume> <month> August </month> <year> 1986. </year>
Reference-contexts: This approach is clearly an oversimplification: not all requests to a given piece of hardware will have the same overheads and bandwidths and the actual overheads are unlikely to exactly match those for current systems. Nevertheless, these simple assumptions provide a starting point for system evaluation. <ref> [Lazo86] </ref> used a similar approach in parameterizing perfor-mance for network file system simulations. The processor overhead time represents the CPU and memory subsystem time to send or receive one network request and do a small amount of work in the file system.
Reference: [Leno90] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proc. of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 148159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The protocol has four pieces inspired by efforts to achieve scalable cache coherence in massively parallel processors <ref> [Arch86, Leno90] </ref>. The protocol uses a no write through policy, utilizes client-to-client data transfers, implements write ownership, and takes advantage of cluster servers. In this paper we compare the effects of this protocol to a baseline AFS system. <p> On the other hand, if ownership is transferred between clients from different clusters, the central server must be involved so it can know that a new cluster server is responsible for tracking the ownership of the file. Clustering is inspired by the DASH multiprocessor architecture <ref> [Leno90] </ref> which clusters processing nodes on busses as we cluster clients on LANs. Clustering improves scalability by off-loading some central server state to cluster servers and isolating the central server from changes in state only affecting clients in the cluster [Blaz91, Munt92, Sand92].
Reference: [Lisk91] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. </author> <title> Replication in the Harp File System. </title> <booktitle> In Proc. of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 226 238, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: File systems can also be replicated across multiple servers to improve scalability for reading files, at a cost of making file writes more expensive <ref> [Lisk91, Kist92] </ref>. We will show in Section 5 that xFS-style clustering reduces the cost of both reads and writes. The combination of these four changes allows xFS to be dramatically more scalable than AFS. <p> If stronger availability guarantees are needed, client-to-client data replication of recently modified data provides a scalable solution. Shortly after a client closes a file for writing, it would send the data to one or more other clients <ref> [Lisk91, Birr93] </ref>. This solution is scalable since it adds no additional server messages if the server knows ahead of time which clients mirror writes to each other.
Reference: [Mogu87] <author> J. Mogul, R. Rashid, and M. Accetta. </author> <title> The Packet Filter: An Efficient Mechanism for User-Level Network Code. </title> <booktitle> In Proc. of the 11th ACM Symposium on Operating Systems Principles, </booktitle> <year> 1987. </year>
Reference-contexts: During the full seven day trace 141,574 files were referenced. We gathered this trace by monitoring network activity on each of the four Ethernets. On each subnet we placed a workstation that monitored all network traffic using rpcspy [Blaz93] which is built on the Ultrix Packetfilter interface <ref> [Mogu87] </ref>. Over the trace period, rpcspy reported that it dropped 4% of all network traffic calls due to buffer over-ow. We postprocessed the NFS trace to reect the semantics of the AFS and xFS protocols.
Reference: [Munt92] <author> D. Muntz and P. Honeyman. </author> <title> Multi-level Caching in Distributed File Systems or Your cache aint nuthin but trash. </title> <booktitle> In Proc. of the Winter 1992 USENIX, </booktitle> <pages> pages 305 313, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Clustering is inspired by the DASH multiprocessor architecture [Leno90] which clusters processing nodes on busses as we cluster clients on LANs. Clustering improves scalability by off-loading some central server state to cluster servers and isolating the central server from changes in state only affecting clients in the cluster <ref> [Blaz91, Munt92, Sand92] </ref>. <p> The authors also suggest a number of strategies which clients may use to guess which other client has desired data without going to the server. These hinting techniques could be applied to an xFS implementation. Muntz and Honeyman <ref> [Munt92] </ref> studied the effect of putting an intermediate data server between the central server and the clients in an Andrew system. They found that the hit rates at the intermediate server were surprisingly low.
Reference: [Nels88] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1), </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: Delayed writes of about 30 seconds have been used in other network file systems to reduce writes to the server without putting too much data at risk of loss in a client crash <ref> [Nels88] </ref>, but xFSs use of client disks allows a complete no write through policy where files are never writ-ten to the server. 2. Client-to-client data transfers. <p> Note that the servers disk need not be backed up; the server can reconstruct its list of cached copies and metadata by polling the cluster servers <ref> [Nels88] </ref>. Backup over the network exerts a small additional load on the system. Although we did not include this load in the simulations, its impact on performance should be small. <p> In xFS each client acts as a storage server, and server and cluster servers together act as a two-level location server. This study indicated that this division greatly reduced the load on the central resource, the central server. Sprite <ref> [Nels88] </ref> uses delayed writes to the server to reduce server load. The diskless Sprite clients, however, must write data through to the server within about 30 seconds to reduce vulnerability to crashes. xFSs extends delayed writes to a no write through policy by using the cli-ents local disks.
Reference: [NIS92] <institution> The Digital Signature Standard Proposed by NIST. Communications of the ACM, 35(7):3640, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: When it forwards a clients request for data, it includes the digest for the pristine data in the forwarding packet. The digest in the forwarding packet is protected using an encrypted digital signature <ref> [NIS92] </ref>. The protected part of the forwarding packet would also include a request identifier to protect against playback attacks. The client supplying data forwards the protected digest along with the data. The original client then verifies the data supplied against the original digest.
Reference: [Pang92] <author> James Y.C. Pang, Deepinder S. Gill, and Songnian Zhou. </author> <title> Implementation and Performance of Cluster-Based File Replication in Large-Scale Distributed Systems. </title> <type> Technical report, </type> <institution> Computer Science Research Institute, University of Toronto, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Because of this result, xFS is designed with intermediate servers that field only consistency requests; the intermediate servers do not store data. We believe it is feasible to provide enough storage on the intermediate servers to hold all of a clusters consistency information The Frolic system <ref> [Pang92, Sand92] </ref> implements replication of files among cluster servers. When a client accesses data from a remote cluster, Frolic creates a copy of the data in the local cluster. Clients use a different protocol, such as NFS, to access data from the local cluster server.
Reference: [Rive92] <author> R. Rivest. </author> <title> The MD4 Message-Digest Algorithm. Request for Comments 1320, Network Working Group, </title> <type> ISI, </type> <month> April </month> <year> 1992. </year>
Reference-contexts: A client verifies untrusted data using a message digest, a special checksum that can be calculated efficiently, but for which it is computationally infeasible to create different data to match <ref> [Rive92] </ref>. The server stores a 128-bit digest for each 64 KB data chunk with its list of chunks cached at the clients. When it forwards a clients request for data, it includes the digest for the pristine data in the forwarding packet.
Reference: [Rost93] <author> E. Rosti, E. Smirni, T. D. Wagner, A. W. Apon, and L.W. Dowdy. </author> <title> The KSR1: Experimentation and Modeling of Poststore. </title> <booktitle> In Proc. of 1993 ACM SIGMETRICS, </booktitle> <pages> pages 7485, </pages> <year> 1993. </year>
Reference-contexts: The first two parts of the protocol allow all data to be stored on client disks, implementing what is referred to for multiprocessors as a cache only memory architecture (COMA) <ref> [Hage92, Rost93] </ref>. An important detail of this approach is that we must guarantee that the clients dont dis-card the last copy of any file. The clients do this by marking one copy of each file as permanent.
Reference: [Sand85] <author> Russel Sandberg, David Goldberg, Steve Kleiman, Dan Walsh, and Bob Lyon. </author> <title> Design and Implementation of the Sun Network Filesystem. </title> <booktitle> In Proc. of the Summer 1985 USENIX, </booktitle> <pages> pages 119130, </pages> <month> June </month> <year> 1985. </year>
Reference: [Sand92] <author> Harjinder S. Sandhu and Songnian Zhou. </author> <title> Cluster Based File Replication in Large-Scale Distributed Systems. </title> <booktitle> In Proc. of 1992 ACM SIGMETRICS, </booktitle> <pages> pages 91 102, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Clustering is inspired by the DASH multiprocessor architecture [Leno90] which clusters processing nodes on busses as we cluster clients on LANs. Clustering improves scalability by off-loading some central server state to cluster servers and isolating the central server from changes in state only affecting clients in the cluster <ref> [Blaz91, Munt92, Sand92] </ref>. <p> Because of this result, xFS is designed with intermediate servers that field only consistency requests; the intermediate servers do not store data. We believe it is feasible to provide enough storage on the intermediate servers to hold all of a clusters consistency information The Frolic system <ref> [Pang92, Sand92] </ref> implements replication of files among cluster servers. When a client accesses data from a remote cluster, Frolic creates a copy of the data in the local cluster. Clients use a different protocol, such as NFS, to access data from the local cluster server.
Reference: [Saty89] <author> Mahadev Satyanarayanan. </author> <title> Integrating Security in a Large Distributed System. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 247280, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Communication between trusting clients does not require the steps described here. The confidentiality of data cached on client disks is also addressed by AFS <ref> [Saty89] </ref> and the techniques used there apply to xFS as well, for data read by the client. xFS, however, adds one new way that data can be brought into a clients cache: clients ush data to one another as their caches fill.
Reference: [Saty90] <author> Mahadev Satyanarayanan. </author> <title> Scalable, Secure, and Highly Available Distributed File Access. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 921, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This section surveys some other combinations of these schemes that have been suggested as methods to achieve scalable file systems. The Andrew file system, AFS, was designed with scalability as a main criteria <ref> [Howa88, Saty90] </ref>.
Reference: [Spas94] <author> Marjana Spasojevic and M. Satyanarayanan. </author> <title> A Usage Profile and Evaluation of a Wide-Area Distributed File System. </title> <booktitle> In Proc. of the Winter 1994 USENIX, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: The resulting trace is similar to other measured AFS workloads in macro characteristics. Our simulated AFS server supplied on average 5.0 MB to each of its clients per day for read opens; <ref> [Spas94] </ref> measured 5.3 MB per client per day for a large AFS installation. We measured a 5.7 MB per client per day write back load; 4.7 MB per client per day loads were measured by [Spas94]. This trace reects the file system activity of a real system. <p> supplied on average 5.0 MB to each of its clients per day for read opens; <ref> [Spas94] </ref> measured 5.3 MB per client per day for a large AFS installation. We measured a 5.7 MB per client per day write back load; 4.7 MB per client per day loads were measured by [Spas94]. This trace reects the file system activity of a real system. Although this enhances our confidence that the trace is realistic, the capabilities of the traced system can limit the activity seen in the trace. The prime example of this limitation is on peak load.
Reference: [Thom87] <author> James Gordon Thompson. </author> <title> Efficient Analysis of Caching Systems. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1987. </year>
Reference-contexts: Modified files remain on each clients local disk. The elimination of write through is motivated by a number of studies showing that when a client writes a file, it is often deleted or quickly rewritten by the same client <ref> [Thom87, Bake91, Blaz91, Kist92] </ref>. We confirmed this pattern for the Berkeley NFS traces.
Reference: [Wolf89] <author> Joel Wolf. </author> <title> The Placement Optimization Problem: </title>
Reference-contexts: Name space splitting improves file system scalability by manually splitting the file system into logical pieces, each managed by a different server. However, it can be difficult to divide files among servers so as to balance load and avoid hot spots <ref> [Wolf89] </ref>. Name space splitting is, however, useful when the different parts of the file system are managed by different administrative domains. xFS can support this splitting by using multiple central servers, with each cluster server providing a combined file system view to its clients.
References-found: 32

