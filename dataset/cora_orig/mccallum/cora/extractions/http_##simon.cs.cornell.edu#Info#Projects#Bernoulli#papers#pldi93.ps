URL: http://simon.cs.cornell.edu/Info/Projects/Bernoulli/papers/pldi93.ps
Refering-URL: 
Root-URL: 
Title: Dependence-Based Program Analysis  
Author: Richard Johnson Keshav Pingali 
Address: Ithaca, NY 14853  
Affiliation: Department of Computer Science Cornell University,  
Abstract: Program analysis and optimization can be speeded up through the use of the dependence flow graph (DFG), a representation of program dependences which generalizes def-use chains and static single assignment (SSA) form. In this paper, we give a simple graph-theoretic description of the DFG and show how the DFG for a program can be constructed in O(EV ) time. We then show how forward and backward dataflow analyses can be performed efficiently on the DFG, using constant propagation and elimination of partial redundancies as examples. These analyses can be framed as solutions of dataflow equations in the DFG. Our construction algorithm is of independent interest because it can be used to construct a program's control dependence graph in O(E) time and its SSA representation in O(EV ) time, which are improvements over existing algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Consider the problem of constant propagation using def-use chains: the standard algorithm replaces a use of a variable with a constant if the right hand side of every definition reaching that use is that constant <ref> [ASU86] </ref>. In Figure 1 (a), this algorithm determines that the use of x in the conditional branch can be replaced by the constant 1, and this fact is determined without propagating the value of x through the assignment to y that is between the definition and use of x. <p> Most constant if (p) then f z := 1; else f z := 2; y := x p := true if (p) then f x := 1 g else f x := 2 g (a) all-paths (b) possible-paths propagation algorithms in the literature, such as the def-use chain algorithm <ref> [ASU86] </ref>, discover such all-paths constants. However, additional constants may be found if we ignore definitions inside dead regions of code. In Figure 3 (b), the predicate of the conditional can be determined to be constant.
Reference: [Bec92] <author> Micah Beck. </author> <title> Translating FORTRAN to Dataflow Graphs. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The DFG was suggested to us by the work of Cartwright and Felleison who showed the advantages of an executable representation of program dependences [CF89]. We previously introduced the DFG using a dataflow machine style operational semantics <ref> [PBJ + 91, Bec92] </ref>. Dataflow machine graphs are also the basis of the program dependence web (PDW) of Ballance, McCabe and Ottenstein [BMO90], as well as the original SSA graphs due to Shapiro and Saint [SS70].
Reference: [BJP91] <author> Micah Beck, Richard Johnson, and Keshav Pingali. </author> <title> From control flow to dataflow. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 118-129, </pages> <year> 1991. </year>
Reference-contexts: For parallelization, the simple picture of the DFG in this paper can be extended to include aliasing, data structures, anti- and output dependences, loop recognition, and distance/direction information for loop-carried dependences. Our treatment of aliasing, and anti- and output dependences is discussed in an earlier paper <ref> [BJP91] </ref>. We are implementing a DFG tool-kit for parallelization and optimization, and we will report on experimental results in another paper. We conclude with a discussion of what we have learned so far from our implementation. <p> This view has two advantages. First, aliasing can be handled very simply <ref> [BJP91] </ref>. Second, control dependences can also be combined with anti- and output dependences without the need for a new conceptual framework. Finally, in the context of optimization, control dependence equivalence is more important than control dependences per se.
Reference: [BMO90] <author> Robert A. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein. </author> <title> The Program Dependence Web: A representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <month> June 20-22, </month> <year> 1990. </year>
Reference-contexts: 1 Introduction A number of recent papers have focused attention on the problem of speeding up program optimization <ref> [FOW87, BMO90, CCF91, PBJ + 91, CFR + 91, DRZ92] </ref>. Most optimization algorithms are based on dataflow analysis. Classic examples are Kildall's constant propagation algorithm [Kil73], and Morel and Renvoise's algorithm for elimination of partial redundancies [MR79]. <p> We previously introduced the DFG using a dataflow machine style operational semantics [PBJ + 91, Bec92]. Dataflow machine graphs are also the basis of the program dependence web (PDW) of Ballance, McCabe and Ottenstein <ref> [BMO90] </ref>, as well as the original SSA graphs due to Shapiro and Saint [SS70]. However, our experience in implementing and using a representation based on these ideas is that a full-blown dataflow graph representation is neither necessary nor desirable.
Reference: [CCF91] <author> Jong-Deok Choi, Ron Cytron, and Jeanne Ferrante. </author> <title> Automatic construction of sparse data flow evaluation graphs. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 55-66, </pages> <month> January 21-23, </month> <year> 1991. </year>
Reference-contexts: 1 Introduction A number of recent papers have focused attention on the problem of speeding up program optimization <ref> [FOW87, BMO90, CCF91, PBJ + 91, CFR + 91, DRZ92] </ref>. Most optimization algorithms are based on dataflow analysis. Classic examples are Kildall's constant propagation algorithm [Kil73], and Morel and Renvoise's algorithm for elimination of partial redundancies [MR79]. <p> A generalization of the SSA approach, called the sparse data flow evaluation graph, has been proposed to address this problem, but sparse graphs take O (N 3 ) time to construct, where N is the number of nodes in the control flow graph <ref> [CCF91, DRZ92] </ref>. In this paper, we show how these problems can be solved using the dependence flow graph (DFG), which can be viewed as a generalization of def-use chains and SSA form.
Reference: [CF89] <author> Robert Cartwright and Matthias Felleisen. </author> <title> The semantics of program dependence. </title> <booktitle> In Proceedings of the SIG-PLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 13-27, </pages> <month> June 21-23, </month> <year> 1989. </year>
Reference-contexts: The DFG was suggested to us by the work of Cartwright and Felleison who showed the advantages of an executable representation of program dependences <ref> [CF89] </ref>. We previously introduced the DFG using a dataflow machine style operational semantics [PBJ + 91, Bec92]. Dataflow machine graphs are also the basis of the program dependence web (PDW) of Ballance, McCabe and Ottenstein [BMO90], as well as the original SSA graphs due to Shapiro and Saint [SS70]. <p> The PDG of a program is the union of its control and data dependences. There have been many efforts to give a formal semantics to the PDG, with the objective of using the semantics in correctness proofs of program transformations <ref> [HPR88, Sel89, CF89] </ref>. However, this has proved to be difficult. For example, it has been shown that two programs with the same PDG have the same input-output behavior, but the proof is long and intricate even for structured programs.
Reference: [CFR + 89] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 25-35, </pages> <month> January 11-13, </month> <year> 1989. </year>
Reference-contexts: The size problem can be overcome by using a factored representation of def-use chains called static single assignment (SSA) form, which has worst-case asymptotic size O (EV ) <ref> [CFR + 89, CFR + 91] </ref>. However, SSA form cannot be used for backward dataflow problems. <p> We describe how to do this in O (E) time. This algorithm is of independent interest since it can be used to build a program's control dependence graph in O (E) time and its SSA representation in O (EV ) time, which are improvements over existing algorithms <ref> [CFS90, CFR + 89] </ref>. In Section 4, we show how to use the DFG in a forward dataflow problem: constant propagation with dead code elimination. This algorithm is faster than the standard control flow graph algorithm, yet it does as good a job of optimizing programs. <p> backward dataflow analysis, and the worst-case size of def-use chains is O (E 2 V ) [RT81], which is rather large. 2.3 Static single assignment form Static single assignment form solves the size problem of defuse chains by introducing a so-called -function to combine def-use edges having the same destination <ref> [CFR + 89, CFR + 91] </ref>. In an SSA representation, each use of a variable is reached by exactly one definition or -function. Figure 1 (b) shows the SSA form for the previous example. <p> Constructing the SSA Representation: If the SSA representation of a program is desired, we can construct it in O (EV ) time by first building the DFG representation and then eliding switches and converting merges to -functions. Unlike the standard algorithm <ref> [CFR + 89] </ref>, our algorithm does not require computation of the dominance relation or dominance frontiers and is therefore much simpler to implement. 4 Forward dataflow analysis In this section, we present constant propagation as an example of forward dataflow analysis using the DFG. Consider Figure 3 (a).
Reference: [CFR + 91] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction A number of recent papers have focused attention on the problem of speeding up program optimization <ref> [FOW87, BMO90, CCF91, PBJ + 91, CFR + 91, DRZ92] </ref>. Most optimization algorithms are based on dataflow analysis. Classic examples are Kildall's constant propagation algorithm [Kil73], and Morel and Renvoise's algorithm for elimination of partial redundancies [MR79]. <p> The size problem can be overcome by using a factored representation of def-use chains called static single assignment (SSA) form, which has worst-case asymptotic size O (EV ) <ref> [CFR + 89, CFR + 91] </ref>. However, SSA form cannot be used for backward dataflow problems. <p> backward dataflow analysis, and the worst-case size of def-use chains is O (E 2 V ) [RT81], which is rather large. 2.3 Static single assignment form Static single assignment form solves the size problem of defuse chains by introducing a so-called -function to combine def-use edges having the same destination <ref> [CFR + 89, CFR + 91] </ref>. In an SSA representation, each use of a variable is reached by exactly one definition or -function. Figure 1 (b) shows the SSA form for the previous example.
Reference: [CFS90] <author> Ron Cytron, Jeanne Ferrante, and Vivek Sarkar. </author> <title> Compact representations for control dependence. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 337-351, </pages> <month> June 20-22, </month> <year> 1990. </year>
Reference-contexts: We describe how to do this in O (E) time. This algorithm is of independent interest since it can be used to build a program's control dependence graph in O (E) time and its SSA representation in O (EV ) time, which are improvements over existing algorithms <ref> [CFS90, CFR + 89] </ref>. In Section 4, we show how to use the DFG in a forward dataflow problem: constant propagation with dead code elimination. This algorithm is faster than the standard control flow graph algorithm, yet it does as good a job of optimizing programs.
Reference: [Dha91] <author> Dhananjay M. Dhamdhere. </author> <title> Practical adaptation of the global optimization algorithm of Morel and Renvoise. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 291-294, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: For example, in assignment to x and delete the other computations of x+1, even though there is no redundancy in the original program. There has been much discussion in the literature about code motion strategies <ref> [DS88, Dha91, KRS92] </ref>, but to our knowledge there is no experimental data showing the superiority of any single strategy. Our approach to epr has the virtue of simplicity. Figure 5 shows the dataflow equations for the dependence-based algorithm.
Reference: [DRZ92] <author> Dhananjay M. Dhamdhere, Barry K. Rosen, and F. Ken-neth Zadeck. </author> <title> How to analyze large programs efficiently and informatively. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 212-223, </pages> <month> June 17-19, </month> <year> 1992. </year>
Reference-contexts: 1 Introduction A number of recent papers have focused attention on the problem of speeding up program optimization <ref> [FOW87, BMO90, CCF91, PBJ + 91, CFR + 91, DRZ92] </ref>. Most optimization algorithms are based on dataflow analysis. Classic examples are Kildall's constant propagation algorithm [Kil73], and Morel and Renvoise's algorithm for elimination of partial redundancies [MR79]. <p> A generalization of the SSA approach, called the sparse data flow evaluation graph, has been proposed to address this problem, but sparse graphs take O (N 3 ) time to construct, where N is the number of nodes in the control flow graph <ref> [CCF91, DRZ92] </ref>. In this paper, we show how these problems can be solved using the dependence flow graph (DFG), which can be viewed as a generalization of def-use chains and SSA form. <p> More elaborate approaches are possible. For example, we can avoid propagating false from expressions other than x+y that use x or y by computing ANT in two phases as is done in some CFG-based dataflow analyses <ref> [DRZ92] </ref>. It is also possible to compute ANT directly by simultaneously following dependences for all variables used in the expression, relying on a depth-first numbering scheme to order these dependences. <p> Finally, the DFG is built only once prior to optimization (it is, of course, updated as optimization is performed). This approach is simpler to implement than other approaches that build a special-purpose graph for each expression for which partial redundancies must be eliminated <ref> [DRZ92] </ref>. 6 Conclusions In this paper, we have presented an approach to speeding up program analysis and optimization that is based on the use of the dependence flow graph.
Reference: [DS88] <author> K.-H. Drechsler and M. P. Stadel. </author> <title> A solution to a problem with Morel and Renvoise's Global Optimization by Suppression of Partial Redundancies. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 635-640, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: For example, in assignment to x and delete the other computations of x+1, even though there is no redundancy in the original program. There has been much discussion in the literature about code motion strategies <ref> [DS88, Dha91, KRS92] </ref>, but to our knowledge there is no experimental data showing the superiority of any single strategy. Our approach to epr has the virtue of simplicity. Figure 5 shows the dataflow equations for the dependence-based algorithm.
Reference: [FOW87] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependencygraph and its uses in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: 1 Introduction A number of recent papers have focused attention on the problem of speeding up program optimization <ref> [FOW87, BMO90, CCF91, PBJ + 91, CFR + 91, DRZ92] </ref>. Most optimization algorithms are based on dataflow analysis. Classic examples are Kildall's constant propagation algorithm [Kil73], and Morel and Renvoise's algorithm for elimination of partial redundancies [MR79]. <p> To place our work in perspective, it is useful to understand the differences between the DFG and the program dependence graph (PDG) <ref> [FOW87] </ref>. The PDG of a program is the union of its control and data dependences. There have been many efforts to give a formal semantics to the PDG, with the objective of using the semantics in correctness proofs of program transformations [HPR88, Sel89, CF89].
Reference: [GM64] <author> Murray Gell-Mann. </author> <title> A schematic model of baryons and mesons. </title> <journal> Physics Letters, </journal> <volume> 8(3) </volume> <pages> 214-215, </pages> <month> February </month> <year> 1964. </year>
Reference-contexts: These difficulties in giving semantics to a program, once it has been broken down into its control and data dependences, are not unlike the difficulties in giving semantics to a program once it has been broken down into assignments and GOTOs. Like quarks in nuclei <ref> [GM64] </ref> or conditional jumps in the stored program computer, control dependence is a deep and fundamental notion. However, quarks do not exist in isolation, and conditional jumps are implicit and hidden in modern programming language control structures.
Reference: [HPR88] <author> Susan Horwitz, Jan Prins, and Thomas Reps. </author> <title> On the adequacy of program dependence graphs for representing programs. </title> <booktitle> In Conference Record of the 15th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 146-157, </pages> <month> January 13-15, </month> <year> 1988. </year>
Reference-contexts: The PDG of a program is the union of its control and data dependences. There have been many efforts to give a formal semantics to the PDG, with the objective of using the semantics in correctness proofs of program transformations <ref> [HPR88, Sel89, CF89] </ref>. However, this has proved to be difficult. For example, it has been shown that two programs with the same PDG have the same input-output behavior, but the proof is long and intricate even for structured programs.
Reference: [Joh93] <author> Richard Johnson. </author> <title> Dependence-Based Compilation (working title). </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1993. </year> <note> Expected in Septem-ber. </note>
Reference-contexts: This kind of refinement is easy to do in the DFG since the dependence edges tell us exactly where we must look to find the desired information. We are evaluating these heuristics and we refer the interested reader to the forthcoming thesis of one of the authors <ref> [Joh93] </ref>. Our epr algorithm is simple in part because it is edge-based rather than node-based like conventional presentations of epr.
Reference: [Kas75] <author> V. N. Kas'janov. </author> <title> Distinguishing hammocks in a directed graph. </title> <journal> Soviet Math. Doklady, </journal> <volume> 16(5) </volume> <pages> 448-450, </pages> <year> 1975. </year>
Reference-contexts: For lack of space, we omit the proof of this theorem. A related structure called a hammock has been discussed in the literature <ref> [Kas75] </ref>. Hammocks are not the same as single-entry single-exit regions since the exit node in a hammock can be the target of edges outside the hammock; besides, the algorithm for finding hammocks is O (EN ).
Reference: [Kil73] <author> Gary A. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Conference Record of the ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-206, </pages> <month> October 1-3, </month> <year> 1973. </year>
Reference-contexts: 1 Introduction A number of recent papers have focused attention on the problem of speeding up program optimization [FOW87, BMO90, CCF91, PBJ + 91, CFR + 91, DRZ92]. Most optimization algorithms are based on dataflow analysis. Classic examples are Kildall's constant propagation algorithm <ref> [Kil73] </ref>, and Morel and Renvoise's algorithm for elimination of partial redundancies [MR79]. These algorithms are usually implemented using vectors of boolean, integer or real values to represent sets of assertions, such as x is 5 here, or y+z is available here. <p> Both algorithms find all-paths and possible-paths constants, but the DFG algorithm is asymptotically faster by a factor of O (V ). We use Kildall's framework for constant propagation <ref> [Kil73] </ref>. Define a lattice consisting of all constant values and two distinguished values &gt; and ?. Uses of variables are assigned values from the lattice during constant propagation. Initially, each use is mapped to ?, meaning that we have no information yet about the values of the variable at runtime.
Reference: [KRS92] <author> Jens Knoop, Oliver Ruthing, and Bernhard Steffen. </author> <title> Lazy code motion. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 224-234, </pages> <month> June 17-19, </month> <year> 1992. </year>
Reference-contexts: For example, in assignment to x and delete the other computations of x+1, even though there is no redundancy in the original program. There has been much discussion in the literature about code motion strategies <ref> [DS88, Dha91, KRS92] </ref>, but to our knowledge there is no experimental data showing the superiority of any single strategy. Our approach to epr has the virtue of simplicity. Figure 5 shows the dataflow equations for the dependence-based algorithm.
Reference: [LFK + 93] <author> P. Geoffrey Lowney, Stefan M. Freudenberger, Thomas J. Karzes, W. D. Lichtenstein, Robert P. Nix, John S. O'Donnell, and John C. Ruttenberg. </author> <title> The Mul-tiflow trace scheduling compiler. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1/2), </volume> <month> January </month> <year> 1993. </year>
Reference-contexts: For example, the Multiflow compiler performed predicate analysis to determine additional constants: if the predicate at a switch is x=1, we can propagate the constant 1 for x on the true side of the conditional even if we cannot determine the value of x for the false side <ref> [LFK + 93] </ref>. It is easy to extend both the DFG and CFG algorithms to accomplish this, but this extension seems difficult in SSA-based algorithms [WZ91] since SSA edges bypass switches in the CFG. We omit the proof of correctness of the DFG algorithm.
Reference: [MR79] <author> Etienne Morel and Claude Renvoise. </author> <title> Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: Most optimization algorithms are based on dataflow analysis. Classic examples are Kildall's constant propagation algorithm [Kil73], and Morel and Renvoise's algorithm for elimination of partial redundancies <ref> [MR79] </ref>. These algorithms are usually implemented using vectors of boolean, integer or real values to represent sets of assertions, such as x is 5 here, or y+z is available here. One vector is associated with each point in the control flow graph and initialized appropriately. <p> This algorithm is faster than the standard control flow graph algorithm, yet it does as good a job of optimizing programs. In Section 5, we show how to solve a backward dataflow problem: anticipatability of expressions, which is an important step in the elimination of partial redundancies <ref> [MR79] </ref>. Finally, in Section 6, we describe what we have learned so far in our implementation. 2 A graph-theoretic characterization of dependence flow graphs We give a graph-theoretic characterization of def-use chains, static single assignment form, and the dependence flow graph. <p> We then show how anticipatable expressions can be used in a powerful optimization called elimination of partial redundancies, which subsumes common subexpression elimination and loop-invariant removal <ref> [MR79] </ref>. 5.1 Anticipatability Definition 8 An expression e is totally (partially) anticipatable at a point p if, on every (some) path in the CFG from p to end, there is a computation of e before an assignment to any of the variables in e. <p> If a redundant computation is preceded by computations of the same value on all execution paths, we say the computation is totally redundant; otherwise we say the computation is partially redundant. A classic dataflow algorithm for the removal of partial redundancies is due to Morel and Renvoise <ref> [MR79] </ref>. We complete our discussion of DFG-based analysis by showing how we can implement epr. The basic idea is to insert new computations into the CFG where it is safe and profitable to do so, thereby making partially redundant computations totally redundant. <p> Placing computations at nodes is complicated by the presence of control flow edges whose source is a switch and whose destination is a merge, such as the back edge of repeat-until loops. This complication is eliminated by adding empty basic blocks to split such edges <ref> [MR79] </ref>, but these blocks must later be removed if no code is moved into them. DFG algorithms are naturally edge-based and avoid these complications. Our epr algorithm propagates information only through the portion of the control flow graph where the variables in the expression are live.
Reference: [PBJ + 91] <author> Keshav Pingali, Micah Beck, Richard Johnson, Mayan Moudgill, and Paul Stodghill. </author> <title> Dependence Flow Graphs: An algebraic approach to program dependencies. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 67-78, </pages> <month> January 21-23, </month> <year> 1991. </year>
Reference-contexts: 1 Introduction A number of recent papers have focused attention on the problem of speeding up program optimization <ref> [FOW87, BMO90, CCF91, PBJ + 91, CFR + 91, DRZ92] </ref>. Most optimization algorithms are based on dataflow analysis. Classic examples are Kildall's constant propagation algorithm [Kil73], and Morel and Renvoise's algorithm for elimination of partial redundancies [MR79]. <p> Second, this lack of control flow information in def-use chains affects the precision of analysis even in forward dataflow problems such as constant propagation <ref> [WZ85, PBJ + 91] </ref>. Finally, the worst--case size of def-use chains is O (E 2 V ) where E is the number of edges in the control flow graph and V is the number of variables [RT81]. <p> The DFG was suggested to us by the work of Cartwright and Felleison who showed the advantages of an executable representation of program dependences [CF89]. We previously introduced the DFG using a dataflow machine style operational semantics <ref> [PBJ + 91, Bec92] </ref>. Dataflow machine graphs are also the basis of the program dependence web (PDW) of Ballance, McCabe and Ottenstein [BMO90], as well as the original SSA graphs due to Shapiro and Saint [SS70]. <p> The limitations of def-use chains have been discussed extensively in the literature <ref> [WZ85, PBJ + 91] </ref>, and we summarize them here. Consider the problem of constant propagation using def-use chains: the standard algorithm replaces a use of a variable with a constant if the right hand side of every definition reaching that use is that constant [ASU86].
Reference: [RT81] <author> John H. Reif and Robert E. Tarjan. </author> <title> Symbolic program analysis in almost-linear time. </title> <journal> SIAM Journal on Computing, </journal> <volume> 11(1) </volume> <pages> 81-93, </pages> <month> February </month> <year> 1981. </year>
Reference-contexts: Finally, the worst--case size of def-use chains is O (E 2 V ) where E is the number of edges in the control flow graph and V is the number of variables <ref> [RT81] </ref>. The size problem can be overcome by using a factored representation of def-use chains called static single assignment (SSA) form, which has worst-case asymptotic size O (EV ) [CFR + 89, CFR + 91]. However, SSA form cannot be used for backward dataflow problems. <p> To summarize, algorithms using def-use chains can produce less optimized code than algorithms performing dataflow analysis directly on the control flow graph. In addition, def-use chains cannot be used for backward dataflow analysis, and the worst-case size of def-use chains is O (E 2 V ) <ref> [RT81] </ref>, which is rather large. 2.3 Static single assignment form Static single assignment form solves the size problem of defuse chains by introducing a so-called -function to combine def-use edges having the same destination [CFR + 89, CFR + 91].
Reference: [Sel89] <author> Rebecca P. Selke. </author> <title> A rewriting semantics for program dependence graphs. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 12-24, </pages> <month> January 11-13, </month> <year> 1989. </year>
Reference-contexts: The PDG of a program is the union of its control and data dependences. There have been many efforts to give a formal semantics to the PDG, with the objective of using the semantics in correctness proofs of program transformations <ref> [HPR88, Sel89, CF89] </ref>. However, this has proved to be difficult. For example, it has been shown that two programs with the same PDG have the same input-output behavior, but the proof is long and intricate even for structured programs.
Reference: [SS70] <author> R. M. Shapiro and H. </author> <title> Saint. The representation of algorithms. </title> <type> Technical Report CA-7002-1432, </type> <institution> Mas-sachusetts Computer Associates, </institution> <month> February </month> <year> 1970. </year>
Reference-contexts: We previously introduced the DFG using a dataflow machine style operational semantics [PBJ + 91, Bec92]. Dataflow machine graphs are also the basis of the program dependence web (PDW) of Ballance, McCabe and Ottenstein [BMO90], as well as the original SSA graphs due to Shapiro and Saint <ref> [SS70] </ref>. However, our experience in implementing and using a representation based on these ideas is that a full-blown dataflow graph representation is neither necessary nor desirable.
Reference: [WZ85] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <booktitle> In Conference Record of the 12th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 291-299, </pages> <address> Jan-uary 14-16, </address> <year> 1985. </year>
Reference-contexts: Second, this lack of control flow information in def-use chains affects the precision of analysis even in forward dataflow problems such as constant propagation <ref> [WZ85, PBJ + 91] </ref>. Finally, the worst--case size of def-use chains is O (E 2 V ) where E is the number of edges in the control flow graph and V is the number of variables [RT81]. <p> The limitations of def-use chains have been discussed extensively in the literature <ref> [WZ85, PBJ + 91] </ref>, and we summarize them here. Consider the problem of constant propagation using def-use chains: the standard algorithm replaces a use of a variable with a constant if the right hand side of every definition reaching that use is that constant [ASU86]. <p> By ignoring the definition on the unexecuted branch, the use of x in the last statement can be determined to have value 1. Such possible-paths constants are common in code generated from inline expansion of procedures or macros <ref> [WZ85] </ref>, but algorithms that use def-use chains alone do not find these constants. We will discuss the standard CFG algorithm, which solves a set of dataflow equations in the control flow graph, and the DFG algorithm, which solves a set of equations in the dependence flow graph.
Reference: [WZ91] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: It is easy to extend both the DFG and CFG algorithms to accomplish this, but this extension seems difficult in SSA-based algorithms <ref> [WZ91] </ref> since SSA edges bypass switches in the CFG. We omit the proof of correctness of the DFG algorithm.
References-found: 27

