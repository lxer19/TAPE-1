URL: ftp://ftp.cs.virginia.edu/pub/dissertations/9505.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/dissertations/README.html
Root-URL: http://www.cs.virginia.edu
Title: A Dissertation  Maximizing Memory Bandwidth for Streamed Computations  
Author: Sally A. McKee 
Degree: Presented to the Faculty of the School of Engineering and Applied Science at the University of Virginia In Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy (Computer Science)  
Date: May 1995  
Note: by  
Abstract-found: 0
Intro-found: 1
Reference: [Adv90] <author> S.V. Adve, and M.D. Hill, </author> <title> Weak Ordering A New Definition, </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 18(2) </volume> <pages> 2-14, </pages> <month> June </month> <year> 1990. </year>
Reference: [Adv91] <author> S.V. Adve, </author> <title> V.S. Adve, M.D. Hill, M.K. Vernon, Comparison of Hardware and Software Cache Coherence Schemes, </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(3) </volume> <pages> 234-243, </pages> <month> May </month> <year> 1991. </year>
Reference: [Aga88] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz, </author> <title> An Evaluation of Directory Schemes for Cache Coherence, </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 16(2) </volume> <pages> 280-289, </pages> <month> May </month> <year> 1988. </year>
Reference: [Ale93] <author> M.J. Alexander, M.W. Bailey, B.R. Childers, J.W. Davidson, and S. Jinturkar, </author> <title> Memory Bandwidth Optimizations for Wide-Bus Machines, </title> <booktitle> Proceedings of the IEEE 26th Hawaii International Conference on Systems Sciences (HICSS-26), </booktitle> <pages> pages 466-475, </pages> <month> January </month> <year> 1993. </year> <note> (incorrectly published under M.A. Alexander et al.) </note>
Reference-contexts: The compiler detects streams (if stream detection is performed at all); the compiler determines the order of the memory accesses (stream elements are generally accessed a cache line at a time); and the compiler decides where in the instruction stream the accesses are issued. Compiler optimizations for wide-bus machines <ref> [Ale93] </ref> and memory-access coalescing [Dav94] also fall into the category, as do schemes that prefetch into registers [ChM92,Kog81] or into a special preload buffer [ChB92]. The ordering selected in the latter prefetching schemes is simply the processors natural access order for the computation.
Reference: [Alu95] <author> Aluwihare, </author> <title> A.S., </title> <type> Masters thesis, </type> <institution> School of Engineering and Applied Science, University of Virginia, </institution> <note> to be published August 1995. </note>
Reference-contexts: Since there are two interleaved banks of memory, for streams with relatively prime strides the SMC can deliver one data item every 25ns processor cycle. Further details of the design, implementation, and testing of the SMC ASIC and daughterboard can be found in other publications <ref> [McG94, Lan95a, Lan95b, Alu95] </ref>. Chapter 6: The SMC Hardware 136 6.3 Programmers Interface The processor interacts with the SMC via a set of memory-mapped registers.
Reference: [And92] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <title> LAPACK Working Note 20: LAPACK: A Portable Linear Algebra Library for High-Performance Computers, </title> <type> Technical Report UT-CS-90-105, </type> <institution> Department of Computer Science, University of Tennessee, </institution> <month> May </month> <year> 1990. </year> <note> Bibliography 184 </note>
Reference: [Arc86] <author> J. Archibald, and J.-L. Baer, </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference: [Arc88] <author> J.K. Archibald, </author> <title> A Cache Coherence Approach for Large Multiprocessor Systems, </title> <booktitle> Proceedings of the ACM/IEEE International Conference on Supercomputing, </booktitle> <pages> pages 337-345, </pages> <year> 1988. </year>
Reference: [Atk87] <author> R.R. Atkinson, </author> <title> and E.M. McCreight, The Dragon Processor, </title> <booktitle> Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-II), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 15(5) </volume> <pages> 65-69, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: a processor detects a memory reference to an object that has been copied into its local memory, it either invalidates [Goo83,Kat85] its local copy so that the next reference will force a current copy to be obtained from global shared memory, or it updates the copy with the new value <ref> [Atk87, Tha87] </ref>. The term snooping usually refers to this type of coherence mechanism for bus-based, shared-memory multiprocessors, but the same principles can be applied to maintain coherence between I/O and cache, between cache and the SMC, between different FIFOs in the SMC, or even between I/O and the SMC.
Reference: [Bae91] <author> J.-L. Baer, and T.-F. Chen, </author> <title> An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty, </title> <booktitle> Proceedings of ACM Supercomputing91, </booktitle> <address> Albuquerque, NM, </address> <pages> pages 176-186, </pages> <month> November </month> <year> 1991. </year>
Reference: [Bai87] <author> D.H. Bailey, </author> <title> Vector Computer Memory Bank Contention, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(3):293-298, </volume> <month> March </month> <year> 1987. </year>
Reference: [Bal88] <author> M. Balakrishnan, R. Jain, and C.S. Raghavendra, </author> <title> On Array Storage for Conict-Free Memory Access for Parallel Processors, </title> <booktitle> Proceedings of the International Conference on Parallel Processing (vol. I: Architecture), </booktitle> <pages> pages 103-107, </pages> <year> 1988. </year>
Reference-contexts: Their scheme uses time multiplexing to force the processors to take turns accessing the interleaved memory banks: each CPU can access a subset of the banks on each turn. The scheme does not reorder accesses to maximize a CPUs utilization of its time slots. Balakrishnan, Jain, and Raghavendra <ref> [Bal88] </ref> and Seznec and Lenfant [Sez92] propose array storage schemes to avoid bank conicts for parallel processors. Such schemes could be used to increase the number of strides for which SMC systems using FC ordering would perform well.
Reference: [Bal93] <author> K. Bala, M.F. Kaashoek, and W.E. Weihl, </author> <title> Software Prefetching and Caching for Translation Lookaside Buffers, </title> <booktitle> Proceedings of the Usenix First Symposium on Operating Systems Design and Implementation (OSDI), published as ACM Operating Systems Review, </booktitle> <volume> 28(5) </volume> <pages> 243-253, </pages> <month> Winter </month> <year> 1994. </year>
Reference: [Ben90] <author> J.K. Bennett, J.B. Carter, W. Zwaenepoel, </author> <title> Adaptive Software Cache Management for Distributed Shared Memory Architectures, </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 18(2) </volume> <pages> 125-134, </pages> <month> June </month> <year> 1990. </year>
Reference: [Ben91] <author> M.E. Benitez, and J.W. Davidson, </author> <title> Code Generation for Streaming: An Access/Execute Mechanism, </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(2) </volume> <pages> 132-141, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Since it contains a first-order linear recurrence, it cannot be vectorized. Nonetheless, the compiler can generate streaming code using Benitez and Davidsons <ref> [Ben91] </ref> recurrence detection and optimization algorithm. In the optimized code, each computed value is retained in a register so that it will be available for use as on the following iteration (see Chapter 7 for a full description of the algorithm). <p> Chapter 2: Access Ordering 15 Each of these functions may be addressed at compile time, CT, or by hardware at run time, RT. This taxonomy classifies access ordering systems by a tuple indicating the time at which each function is performed. 2.2.1 Compile-Time Schemes Benitez and Davidson <ref> [Ben91] </ref> detect streams at compile time, and Moyer [Moy93] has derived access-ordering algorithms relative to a precise analytic model of memory systems. Moyers scheme unrolls loops and groups accesses to each stream, so that the cost of each DRAM page-miss can be amortized over several references to the same page. <p> Another advantage is that this combined hardware/software scheme requires no heroic compiler technology the compiler need only detect the presence of streams, as in Benitez and Davidsons algorithm <ref> [Ben91] </ref>. Information about the streams is transmitted to the SMC at run-time. What follows is a bound on SMC performance for loading a single vector of a multiple-vector computation.
Reference: [Ben94] <author> M.E. Benitez, </author> <title> Retargetable Register Allocation, </title> <type> Ph.D. thesis, </type> <institution> School of Engineering and Applied Science, University of Virginia, </institution> <month> May </month> <year> 1994. </year> <note> Bibliography 185 </note>
Reference: [Bir91] <author> P.L. Bird, and R.A. Uhlig, </author> <title> Using Lookahead to Reduce Memory Bank Contention for Decoupled Operand References, </title> <booktitle> Proceedings of ACM Supercomputing91, </booktitle> <address> Albuquerque, NM, </address> <pages> pages 187-196, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: In addition, cache-based schemes suffer from cache conicts: the prefetched data may replace other needed data, or may be evicted before it is used. None of these schemes explicitly orders accesses to fully exploit the underlying memory architecture. The lookahead technique proposed by Bird and Uhlig <ref> [Bir91] </ref> uses a Bank Active Scoreboard to order accesses dynamically to avoid bank contention, but like most others, this scheme does nothing to exploit device characteristics such as fast-page mode.
Reference: [Bol89] <author> W.J. Bolosky, R.P. Fitzgerald, </author> <title> and M.L. Scott, Simple But Effective Techniques for NUMA Memory Management, </title> <booktitle> Proceedings of the 12th International Symposium on Operating Systems Principles (SOSP), published as ACM Operating Systems Review, </booktitle> <volume> 23(5) </volume> <pages> 19-31, </pages> <month> December </month> <year> 1989. </year>
Reference: [Bud71] <author> P. Budnik, and D. Kuck, </author> <title> The Organization and Use of Parallel Memories, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-20(12):1566-1569, </volume> <month> December </month> <year> 1971. </year>
Reference: [Bur94] <author> D.C. Burger, R.S. Hyder, B.P. Miller, and D.A. Wood, </author> <title> Paging Tradeoffs in Distributed-Shared-Memory Multiprocessors, </title> <booktitle> Proceedings of ACM Supercomputing94, </booktitle> <address> Washington, D.C., </address> <pages> pages 590-599, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Leutenegger [Leu90] and Ousterhout et al. [Ous80] argue for gang scheduling of CPU resources. Burger et al. <ref> [Bur94] </ref> confirm the importance of gang CPU scheduling and argue that for good Chapter 7: Compiling for Dynamic Access Ordering 154 performance, virtual memory pages must be gang scheduled as well.
Reference: [Bur95] <author> D.C. Burger, J.R. Goodman, and A. Kagi, </author> <title> The Declining Effectiveness of Dynamic Caching for General-Purpose Microprocessors, </title> <type> Technical Report 1261, </type> <institution> Department of Computer Science, University of Wisconsin, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: The work of Peris et al. [Per94] strongly suggests that memory considerations must be incorporated in the resource allocation policies for distributed parallel systems. Other studies focus specifically on memory hierarchy utilization. For instance, Loshin and Budge [Los92] argue for memory hierarchy management by the compiler. Burger et al. <ref> [Bur95] </ref> demonstrate the declining effectiveness of dynamic caching for general-purpose microprocessors, also arguing for explicit compiler management of the memory hierarchy. 7.8 Summary This chapter has addressed the compiler aspects of our proposed hardware/software approach to the memory bandwidth problem: stream detection, code transformations, optimal FIFO depth selection, parallelization schemes, and
Reference: [Cal91] <author> D. Callahan, K. Kennedy, and A. Porterfield, </author> <title> Software Prefetching, </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(2) </volume> <pages> 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference: [Car89] <author> S. Carr, </author> <title> Memory Hierarchy Management, </title> <type> Ph.D thesis, </type> <institution> Rice University, </institution> <year> 1989. </year>
Reference: [Car91] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel, </author> <title> Implementation and Performance of Munin, </title> <booktitle> Proceedings of the 13th International Symposium on Operating Systems Principles (SOSP), published as ACM Operating Systems Review, </booktitle> <volume> 25(5) </volume> <pages> 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference: [Cas93] <author> Epoch Users Manual 3.1, </author> <title> Cascade Design Automation, </title> <year> 1993. </year>
Reference-contexts: The SMC can deliver a 64-bit doubleword of data every cycle. The SMC was designed using a top-down approach with state-of-the art synthesis tools <ref> [Cas93, Log92, Men93] </ref>. The hardware design has been validated using four different methods: functional simulation, gate-level simulation, static timing analysis, and back-annotated timing simulation.
Reference: [Cas94] <editor> Cascade Delay Calculation Manual, </editor> <title> Document No. </title> <booktitle> 93-0071-Rev. 2, Cascade Design Automation, </booktitle> <month> May </month> <year> 1994. </year>
Reference: [Cen78] <author> L.M. Censier and P. Feautrier, </author> <title> A New Solution to Coherence Problems in Multicache Systems, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December </month> <year> 1978. </year> <note> Cited in [Lil93]. </note>
Reference-contexts: Directory-based approaches require a processor to communicate with a common directory whenever the CPUs actions may cause an inconsistency between its local memory and those of other processors or the global shared memory <ref> [Cen78] </ref>. The directory maintains information about which processors have a copy of which objects. Before a processor can write to an object, it must request exclusive access from the directory. The directory sends messages to all processors with a local copy of the object, forcing them to invalidate their copies.
Reference: [Cha91] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal, </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme, </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Bibliography 186 Languages and Operating Systems (ASPLOS-IV), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(2) </volume> <pages> 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference: [Cha94] <author> D. Chaiken and A. Agarwal, </author> <title> Software-Extended Coherent Shared Memory: Performance and Cost, </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 22(2) </volume> <pages> 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference: [Che86] <author> D.R. Cheriton, G.A. Slavenburg, and P.D. Boyle, </author> <title> Software-Controlled Caches in the VMP Multiprocessor, </title> <booktitle> Proceedings of the 13th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 14(2): </volume> <pages> 366-374, </pages> <month> June </month> <year> 1986. </year>
Reference: [Che92] <author> T.-F. Chen and J.-L. Baer, </author> <title> Reducing Memory Latency via Non-blocking and Prefetching Caches, </title> <type> Technical Report UW-CSE-92-06-03, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> July </month> <year> 1992. </year>
Reference: [ChB92] <author> W.Y. Chen, R.A. Bringmann, S.A. Mahlke, R.E. Hank, and J.E. Sicolo, </author> <title> An Efficient Architecture for Loop Based Data Preloading, </title> <booktitle> Proceedings of the IEEE 25th Annual International Symposium on Microarchitecture (Micro-25), </booktitle> <address> Portland, OR, </address> <pages> pages 92-101, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Compiler optimizations for wide-bus machines [Ale93] and memory-access coalescing [Dav94] also fall into the category, as do schemes that prefetch into registers [ChM92,Kog81] or into a special preload buffer <ref> [ChB92] </ref>. The ordering selected in the latter prefetching schemes is simply the processors natural access order for the computation. All prefetching techniques attempt to overlap memory latency with computation, which can lead to significant performance increases.
Reference: [ChM92] <author> W.Y. Chen, S.A. Mahlke, and W.-M. Hwu, </author> <title> Tolerating Data Access Latency with Register Preloading, </title> <booktitle> Proceedings of the ACM/IEEE International Conference on Supercomputing, </booktitle> <year> 1992. </year>
Reference: [Che86] <author> T. Cheung, and J.E. Smith, </author> <title> A Simulation Study of the CRAY X-MP Memory System, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(7):613-622, </volume> <month> July </month> <year> 1986. </year>
Reference: [Chi94] <author> T. Chiueh, </author> <title> Sunder: A Programmable Hardware Prefetch Architecture for Numerical Loops, </title> <booktitle> Proceedings of ACM Supercomputing94, </booktitle> <address> Washington, D.C., </address> <pages> pages 488-497, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Again, schemes that decouple the issuing of the memory accesses from the processors instruction execution without performing sophisticated access scheduling can be considered schemes. For instance, Chieuh <ref> [Chi94] </ref> proposes a programmable prefetch engine that fetches vector data for the next loop iteration. This data is stored in a special buffer, the Array Register File, until the corresponding iteration is executed, at which point the prefetched data is transferred to cache.
Reference: [Cox89] <author> A.L. Cox and R.J. Fowler, </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM, </title> <booktitle> Proceedings of the 12th International Symposium on Operating Systems Principles (SOSP), published as ACM Operating Systems Review, </booktitle> <volume> 23(5) </volume> <pages> 32-44, </pages> <month> December </month> <year> 1989. </year>
Reference: [Cyt88] <author> R. Cytron, S. Karlovsky, </author> <title> and K.P. McAuliffe, Automatic Management of Programmable Caches, </title> <booktitle> Proceedings of the International Conference on Parallel Processing (vol. I: Architecture), </booktitle> <pages> pages 229-238, </pages> <year> 1988. </year>
Reference-contexts: Obviously, any coherence scheme with a deleterious impact on the performance of other parts of the system becomes unattractive. Although snooping mechanisms may be relatively simple to implement, they are often prohibitive either in cost or in serialization <ref> [Cyt88] </ref>. The most effective solutions to the coherence problem will likely involve a combination of hardware and software. This section briey surveys the potential compile-time solutions for uniprocessor SMC/cache coherence. <p> Whether or not this is the case depends on the parameters of the particular system in question. Programmable caches allow the compiler to manage coherence through software. This requires at least two operations: invalidate and post (which copies a value back to main memory). Cytron et al. <ref> [Cyt88] </ref> develop algorithms to determine when a cached value must update its shared variable, or when a cached value is potentially stale. Their work shows how automatic techniques can effectively manage software-controlled caches.
Reference: [Dah94] <author> F. Dahlgren and P. Stenstrom, </author> <title> Effectiveness of Hardware-based Sequential and Stride Prefetching in Shared Memory Multiprocessors, </title> <booktitle> Proceedings of Bibliography 187 the Fourth Workshop on Scalable Shared-Memory Multiprocessors, </booktitle> <address> Chicago, </address> <month> April </month> <year> 1994. </year>
Reference: [Dah95] <author> F. Dahlgren, M. Dubois, and P. Stenstrom, </author> <title> Sequential Hardware Prefetching in Shared-Memory Multiprocessors, </title> <note> to appear in IEEE Transactions on Parallel and Distributed Systems, </note> <year> 1995. </year>
Reference: [Dav94] <author> J.W. Davidson and S. Jinturkar, </author> <title> Memory Access Coalescing: a Technique for Eliminating Redundant Memory Accesses, </title> <booktitle> Proceedings of the SIGPLAN 94 Conference on Programming Language Design and Implementation, published as ACM SIGPLAN Notices, </booktitle> <volume> 29(6) </volume> <pages> 186-195, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Compiler optimizations for wide-bus machines [Ale93] and memory-access coalescing <ref> [Dav94] </ref> also fall into the category, as do schemes that prefetch into registers [ChM92,Kog81] or into a special preload buffer [ChB92]. The ordering selected in the latter prefetching schemes is simply the processors natural access order for the computation.
Reference: [Den68] <author> P. Denning, </author> <title> The Working Set Model for Program Behavior, </title> <journal> Communications of the ACM, </journal> <volume> 11(5) </volume> <pages> 323-333, </pages> <month> May </month> <year> 1968. </year>
Reference: [Dig92] <institution> Alpha Architecture Handbook, Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference-contexts: Although rare, these instructions are available in some commercial processors, such as the Convex C-1 [Wal85] and Intel i860 [Int91]. Most current microprocessors (including the DEC Alpha <ref> [Dig92] </ref>, MIPS [Kan92], Intel 80486, Pentium, and i860 [Tab91], and the PowerPC [Mot93]) provide a means of specifying some memory pages as non-cacheable, even though these mechanisms is not generally accessible to the user. <p> Architectures that can prefetch larger blocks require even fewer instructions (for instance, the DEC Alpha can prefetch up to 512 bytes <ref> [Dig92] </ref>). Figure 2.7 depicts the DRAM costs incurred by block prefetching in the absence of a block-prefetch instruction. <p> The compiler could place all stream data in non-cacheable memory, thereby achieving the same effect as a system in which the SMC and cache reference physically distinct memory partitions. Most current microprocessors (including the DEC Alpha <ref> [Dig92] </ref>, MIPS [Kan92], Intel 80x86 series and i860 [Tab91], and the PowerPC [Mot93]) provide a means of specifying some memory pages as non-cacheable. Another option is to ush the cache before entering streaming loops. <p> This mapping information must be supplied (from the page table) before the process can Chapter 8: Other Systems Issues 163 continue. The TLB can be considered another component of the memory hierarchy. Several modern architectures (including the MIPS R2000/3000 [Kan92], the DEC Alpha <ref> [Dig92] </ref>, and the HP PA-RISC) handle TLB misses in software [Bal94]. This makes the hardware simpler and the operating system more exible, but it also increases the penalty for a TLB miss. <p> For instance, the number of translations that must be performed may be minimized through the use of superpages, contiguous sets of virtual memory pages such that each set is treated as a unit. 1 Several recent microprocessor architectures support superpages, including the MIPS R4x00 [Kan92], DEC Alpha <ref> [Dig92] </ref>, SPARC, PowerPC, ARM, and HP PA-RISC [Tal94]. 1. Superpages are restricted to being a power of 2 times the base page size, and must be aligned (with respect to its size) in both the virtual and physical address spaces [Tal94].
Reference: [Don90] <author> J.J. Dongarra, J. DuCroz, I. Duff, and S. Hammerling, </author> <title> A set of Level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference: [Don91] <author> J.J. Dongarra, I.S. Duff, D.C. Sorensen, and H.A. van der Vorst, </author> <title> Solving Linear Systems on Vector and Shared Memory Computers, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference: [Dub86] <author> M. Dubois, C. Scheurich, and F.A. Briggs, </author> <title> Memory Access Buffering in Multiprocessors, </title> <booktitle> Proceedings of the 13th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 14(2) </volume> <pages> 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: More than one processor is using the same set of DRAM pages, so many page-hits are possible. i (a) scale (b) swap Chapter 4: Multiprocessor SMC Performance 112 4.5 Related Work Dubois, Scheurich, and Briggs <ref> [Dub86] </ref> study the effects of buffering memory requests on multiprocessor performance, proposing a framework to analyze coherence properties. Their approach allows them to identify restrictions to buffering that different coherence policies impose on shared-memory systems.
Reference: [Dub88] <author> M. Dubois, C. Scheurich, and F.A. Briggs, </author> <title> Synchronization, Coherence, and Event Ordering in Multiprocessors, </title> <journal> IEEE Computer, </journal> <volume> 21(2), </volume> <month> February </month> <year> 1988. </year>
Reference: [Duf85] <author> I.S. Duff, </author> <title> Data Structures, Algorithms and Software for Sparse Matrices, </title> <booktitle> in Sparsity and Its Applications, </booktitle> <pages> pages 1-30, </pages> <editor> ed. D.J. Evans, </editor> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: Although there are some instances where working with sparse matrices using some form of hash coding can be useful, the regular way in which sparse matrix computations access their data makes such a scheme generally inappropriate for scientific computation <ref> [Duf85] </ref>. Hashing tends to spread out accesses to the data structure: sets of sparse matrix index values that are close together are unlikely to map to memory locations that are near one another. This lack of locality of reference renders streaming inappropriate for such data structures. <p> Unfortunately, manipulating this data structure by row or column requires scanning the entire structure. It is not uncommon to permit input to sparse matrix routines using this form, but a more structured form is commonly used when performing operations on the data <ref> [Duf85] </ref>. Use of Coordinates Another simple method is to use a one-dimensional array for the storage of the non-zero elements in each row, along with their coordinates. Elements may or may not be sorted by column within each row. <p> If the structure is to be modified dynamically, then we must either leave gaps to accommodate additional data, or we must allocate more space for the row (for example, at the end of the structure), and copy its contents. Various garbage-collection schemes can be used to manage growth. Duff <ref> [Duf85] </ref> gives a thorough explanation of this general storage technique. Since some variation of this scheme is commonly used in practice, this is the data organization that we assume in this chapter.
Reference: [Eva85] <author> D.J. Evans, </author> <title> Iterative Methods for Sparse Matrices, </title> <booktitle> in Sparsity and Its Applications, </booktitle> <pages> pages 45-112, </pages> <editor> ed. D.J. Evans, </editor> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: In general, a matrix having no more than 20% non-zero entries would benefit from special treatment, and a typical large sparse matrix usually has five to ten non-zeros per row <ref> [Eva85] </ref>. Sparse matrices often arise in discretized problems from such domains as electrical networks, structural analyses, partial differential equations, power distribution systems, nuclear physics, and operational research.
Reference: [FuP91] <author> J.W.C. Fu and J.H. Patel, </author> <title> Prefetching in Multiprocessor Vector Cache Memories, </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(3) </volume> <pages> 54-63, </pages> <month> May </month> <year> 1991. </year> <note> Bibliography 188 </note>
Reference-contexts: Nonblocking caches and prefetching to cache [Bae91,Cal91,Dah94, Gup91,Kla91, Mow92,Soh91], prefetching to registers (as in the IBM 3033 [Kog81], or as proposed by Fu, Patel, and Janssens [FuP92]), or prefetching to special preload buffers <ref> [FuP91] </ref> can be Chapter 2: Access Ordering 39 used to overlap memory accesses with computation, or to overlap the latencies of more than one access. These methods can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth.
Reference: [FuP92] <author> J.W.C. Fu, J.H. Patel, and B.L. Janssens, </author> <title> Stride Directed Prefetching in Scalar Processors, </title> <booktitle> Proceedings of the IEEE 25th Annual International Symposium on Microarchitecture (Micro-25), </booktitle> <address> Portland, OR, </address> <pages> pages 102-110, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Nonblocking caches and prefetching to cache [Bae91,Cal91,Dah94, Gup91,Kla91, Mow92,Soh91], prefetching to registers (as in the IBM 3033 [Kog81], or as proposed by Fu, Patel, and Janssens <ref> [FuP92] </ref>), or prefetching to special preload buffers [FuP91] can be Chapter 2: Access Ordering 39 used to overlap memory accesses with computation, or to overlap the latencies of more than one access. These methods can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth.
Reference: [Gal87] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh, </author> <title> The Impact of Hierarchical Memory Systems on Linear Algebra Algorithm Design, </title> <type> Technical Report UIUCSRD 625, </type> <institution> University of Illinois, </institution> <year> 1987. </year> <note> Also published in International Journal of Supercomputer Applications, 2(1) 12-48, Spring 1988. The latter reference is cited in [Car93,Har91a]. </note>
Reference: [Gal89] <author> K. Gallivan, D. Gannon, W. Jalby, A. Malony, and H. Wijshoff, </author> <title> Behavioral Characterization of Multiprocessor Memory Systems: a Case Study, </title> <journal> Performance Evaluation Review, </journal> <volume> 17(1) </volume> <pages> 79-88, </pages> <month> May </month> <year> 1989. </year>
Reference: [Gan87] <author> D. Gannon, and W. Jalby, </author> <title> The Inuence of Memory Hierarchy on Algorithm Organization: Programming FFTs on a Vector Multiprocessor, in The Characteristics of Parallel Algorithms, </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: For instance, in parallelizing a Fast Fourier Transform, Gannon and Jalby use copying to generate the transpose of a matrix, giving both row-wise and column-wise array accesses the same locality of reference <ref> [Gan87] </ref>. Lam, Rothberg, and Wolf [Lam91] investigate blocking in conjunction with copying in order to eliminate self-interference, or cache misses caused by more than one element of a given vector mapping to the same location.
Reference: [Gao93] <author> Q.S. Gao, </author> <title> The Chinese Remainder Theorem and the Prime Memory System, </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 21(2) </volume> <pages> 337-340, </pages> <month> May </month> <year> 1993. </year>
Reference: [Gha90] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy, </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors, </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 18(2) </volume> <pages> 15-26, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Accesses to other shared variables between these synchronization points can occur in any arbitrary order. Each processor must guarantee that all of its outstanding shared-memory accesses complete before it issues a synchronization operation [Lil93]. The release consistency model <ref> [Gha90] </ref> weakens the ordering constraints on synchronization variables by splitting the synchronization operation into separate acquire and release operations. In order to obtain exclusive access to some shared-memory object, a processor executes an acquire operation.
Reference: [Gha91] <author> K. Gharachorloo, A. Gupta, and J. Hennessy, </author> <title> Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors, </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(2) </volume> <pages> 245-257, </pages> <month> April </month> <year> 1991. </year>
Reference: [Gol93] <author> G. Golub and J.M. Ortega, </author> <title> Scientific Computation: An Introduction with Parallel Computing, </title> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: Streaming Static Access Dynamic Access OrderingOrdering Example Related Work Compile-Time Run-Time Taxonomy i" x i z i y i x i 1 ( ) Chapter 2: Access Ordering 13 This computation occurs frequently in practice, especially in the solution of partial differential equations by finite difference or finite element methods <ref> [Gol93] </ref>. Since it contains a first-order linear recurrence, it cannot be vectorized. Nonetheless, the compiler can generate streaming code using Benitez and Davidsons [Ben91] recurrence detection and optimization algorithm. <p> This computation occurs in matrix-vector multiplication by diagonals, which is useful for the diagonally sparse matrices that arise frequently when solving parabolic or elliptic partial differential equations by finite element or finite difference methods <ref> [Gol93] </ref>. These benchmarks were selected because they are representative of the access patterns found in real codes, including the inner loops of blocked algorithms. 1 Nonetheless, our results indicate that variations in the processors reference sequence have little effect on the SMCs ability to improve bandwidth. 1. <p> Let another array hold the number of entries in each row. Given an initial approximation x 0 to the solution, the next iterate is given by <ref> [Gol93] </ref>. For the sake of simplicity in our example, let us assume that the diagonal element a ii is the first item stored in each row. Pseudocode for a possible memory access pattern is depicted in Figure 5.3: 1.
Reference: [Goo83] <author> J.R. Goodman, </author> <title> Using Cache Memory to Reduce Processor-Memory Traffic, </title> <booktitle> Proceedings of the 10th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 11(3) </volume> <pages> 124-131, </pages> <month> June </month> <year> 1983. </year>
Reference: [Goo85] <author> J.R. Goodman, J. Hsieh, K. Liou, A.R. Pleszkun, P.B. Schechter, and H.C. Young, </author> <title> PIPE: A VLSI Decoupled Architecture, </title> <booktitle> Proceedings of the 12th Bibliography 189 Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 13(3) </volume> <pages> 20-27, </pages> <month> June </month> <year> 1985. </year>
Reference: [Goo88] <author> J.R. Goodman and P.J. Woest, </author> <title> The Wisconsin Multicube: A New Large-Scale Cache-Coherent Multiprocessor, </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 16(2) </volume> <pages> 422-431, </pages> <month> May </month> <year> 1988. </year>
Reference: [Goo91] <author> J.R. Goodman, </author> <title> Cache Consistency and Sequential Consistency, </title> <type> Technical Report 1006, </type> <institution> Department of Computer Science, University of Wisconsin, </institution> <month> February </month> <year> 1991. </year> <type> Also Technical Report Number 61, </type> <institution> IEEE Scalable Coherent Interface (SCI) Working Group, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: In other words, the execution of the parallel program must appear as some interleaving of the execution of the parallel processes on a sequential machine [Lam79]. This strong ordering of memory accesses severely limits the allowable overlap of memory operations. Other memory consistency models, such as processor consistency <ref> [Gha90,Gha91, Goo91] </ref>, weak ordering [Adv90,Dub86,Dub88], and release consistency [Car91,Gha90], allow a greater overlap of memory reads and writes. <p> In other words, a multiprocessor is processor consistent if the result of any execution is the same as if the operations of each individual processor appeared in the sequential order specified by its program <ref> [Goo91] </ref>. The weak-ordering consistency model [Dub86,Dub88] relaxes the guaranteed ordering of events of the sequential and processor consistency models such that only memory accesses to programmer-defined synchronization variables are guaranteed to occur in a sequentially consistent order.
Reference: [Gor90] <author> E.H. Gornish, E.D. Granston, and A.V. Veidenbaum, </author> <title> Compiler-directed Data Prefetching in Multiprocessor with Memory Hierarchies, </title> <booktitle> Proceedings of the ACM/IEEE International Conference on Supercomputing, </booktitle> <pages> pages 354-368, </pages> <month> June </month> <year> 1990. </year>
Reference: [Gup91] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.-D. Weber, </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques, </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, SIGARCH Computer Architecture News:254-263, </booktitle> <month> May </month> <year> 1991. </year>
Reference: [Har87] <author> D.T. Harper and J. </author> <title> Jump, Vector Access Performance in Parallel Memories Using a Skewed Storage Scheme, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1440-1449, </volume> <month> December </month> <year> 1987. </year>
Reference: [Har89] <author> D.T. Harper, </author> <title> Address Transformation to Increase Memory Performance, </title> <booktitle> Proceedings of the ACM/IEEE International Conference on Supercomputing, </booktitle> <year> 1989. </year>
Reference: [Har91a] <author> D.T. Harper, </author> <title> Block, Multistride Vector, and FFT Accesses in Parallel Memory Systems, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(1), </volume> <pages> 43-51, </pages> <month> January </month> <year> 1991. </year>
Reference: [Har91b] <author> D.T. Harper, </author> <title> Reducing Memory Contention in Shared Memory Multiprocessors, </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, SIGARCH Computer Architecture News:66-73, </booktitle> <month> May </month> <year> 1991. </year>
Reference: [Hen90] <author> J.L. Hennessy and D.A. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference: [Hil93] <author> M.D. Hill, J.R. Larus, S.K. Reinhardt, and D.A. Wood, </author> <title> Cooperative Shared Bibliography 190 Memory: Software and Hardware for Scalable Multiprocessors, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <month> November </month> <year> 1993. </year> <booktitle> A preliminary version appeared in Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), published as ACM SIGPLAN Notices, </booktitle> 27(9):262-273, September 1992. 
Reference: [IEE92] <author> Memory Catches Up, </author> <title> Special Report, </title> <journal> IEEE Spectrum, </journal> <volume> 29(10) </volume> <month> 34-53 October </month> <year> 1992. </year> <title> [Int91] i860 XP Microprocessor Data Book, </title> <publisher> Intel Corporation, </publisher> <year> 1991. </year>
Reference-contexts: These include distributed shared memories, systems composed of devices like the new Rambus [Ram92] or JEDEC synchronous DRAMs <ref> [IEE92] </ref>, and disks.
Reference: [Jin94] <author> S. Jinturkar, </author> <title> Data-Specific Optimizations, </title> <type> Ph.D. thesis proposal, </type> <institution> Department of Computer Science, University of Virginia, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Possible solutions to these problems are surveyed in Section 7.6. These range from hardware-based snooping schemes, to combination schemes that provide hardware support (e.g., in the form of programmable caches) for compiler-managed coherence or data-specific optimizations <ref> [Jin94] </ref> that select appropriate code to execute based on run-time analysis. Enforcing coherence largely in software is usually cheaper to implement, and fits in well with the general RISC philosophy of moving complexity to software, keeping hardware simple in order to make it fast. <p> Their results suggest that this reference marking can reduce invalidation requests significantly, especially when combined with locality-preserving task partitioning and scheduling. A third type of compiler assistance involves generating multiple versions of a piece of code at compile-time, as in Jinturkars <ref> [Jin94] </ref> data-specific optimizations, and dynamically selecting the appropriate one to execute. This approach could be used to determine at run-time whether a vector is shared (whether or not coherence actions are necessary at all) and to select an appropriate course of action. <p> It may be feasible to enforce coherence on blocks of stream data up to the size of DRAM pages. Using a larger granularity decreases the number of coherence messages required during a computation. The results of Li, Mounes-Toussi, and Lilja [LiZ93,LiM94], Nguyen et al. [Ngu94] and Jinturkar <ref> [Jin94] </ref> suggest that much of the responsibility for maintaining consistency can be moved to the compiler, so that the accompanying hardware mechanisms can be made as simple and fast as possible. The compilers knowledge of stream access patterns should make it easier to generate efficient code to maintain coherence.
Reference: [Kan92] <author> G. Kane and J. Heinrich, </author> <title> MIPS RISC Architecture, </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: Although rare, these instructions are available in some commercial processors, such as the Convex C-1 [Wal85] and Intel i860 [Int91]. Most current microprocessors (including the DEC Alpha [Dig92], MIPS <ref> [Kan92] </ref>, Intel 80486, Pentium, and i860 [Tab91], and the PowerPC [Mot93]) provide a means of specifying some memory pages as non-cacheable, even though these mechanisms is not generally accessible to the user. <p> The compiler could place all stream data in non-cacheable memory, thereby achieving the same effect as a system in which the SMC and cache reference physically distinct memory partitions. Most current microprocessors (including the DEC Alpha [Dig92], MIPS <ref> [Kan92] </ref>, Intel 80x86 series and i860 [Tab91], and the PowerPC [Mot93]) provide a means of specifying some memory pages as non-cacheable. Another option is to ush the cache before entering streaming loops. <p> This mapping information must be supplied (from the page table) before the process can Chapter 8: Other Systems Issues 163 continue. The TLB can be considered another component of the memory hierarchy. Several modern architectures (including the MIPS R2000/3000 <ref> [Kan92] </ref>, the DEC Alpha [Dig92], and the HP PA-RISC) handle TLB misses in software [Bal94]. This makes the hardware simpler and the operating system more exible, but it also increases the penalty for a TLB miss. <p> For instance, the number of translations that must be performed may be minimized through the use of superpages, contiguous sets of virtual memory pages such that each set is treated as a unit. 1 Several recent microprocessor architectures support superpages, including the MIPS R4x00 <ref> [Kan92] </ref>, DEC Alpha [Dig92], SPARC, PowerPC, ARM, and HP PA-RISC [Tal94]. 1. Superpages are restricted to being a power of 2 times the base page size, and must be aligned (with respect to its size) in both the virtual and physical address spaces [Tal94].
Reference: [Kat85] <author> R.H. Katz, S.J. Eggers, D.A. Wood, C.L. Perkins, and R.G. Sheldon, </author> <title> Implementing a Cache Consistency Protocol, </title> <booktitle> Proceedings of the 12th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 13(3) </volume> <pages> 276-283, </pages> <month> June </month> <year> 1985. </year>
Reference: [Kla91] <author> A.C. Klaiber and H.M. Levy, </author> <title> An Architecture for Software-Controlled Data Prefetching, </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(3) </volume> <pages> 43-53, </pages> <month> May </month> <year> 1991. </year>
Reference: [Kog81] <author> P.M. Kogge, </author> <title> The Architecture of Pipelined Computers, </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year> <note> Cited in [Dub86]. </note>
Reference-contexts: Most of this research focuses on: a) hiding or tolerating memory latency, b) decreasing the number of cache misses incurred, or c) avoiding bank conicts in an interleaved memory system. Nonblocking caches and prefetching to cache [Bae91,Cal91,Dah94, Gup91,Kla91, Mow92,Soh91], prefetching to registers (as in the IBM 3033 <ref> [Kog81] </ref>, or as proposed by Fu, Patel, and Janssens [FuP92]), or prefetching to special preload buffers [FuP91] can be Chapter 2: Access Ordering 39 used to overlap memory accesses with computation, or to overlap the latencies of more than one access.
Reference: [Knu73] <author> D.E. Knuth, </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol. 1, </volume> <pages> pages 299-304, </pages> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Which structure will yield the best performance depends on the access patterns of the computation as well as the characteristics of the particular memory system. Linked Lists Linked-list schemes provide equivalent access by rows and columns <ref> [Knu73] </ref>. Each list entry contains two pointers, one to the next non-zero element in the row and one to the next non-zero element in the column. The symmetry of access to rows and columns simplifies coding, and adding or deleting entries dynamically becomes easy.
Reference: [Lai92] <author> M. Laird, </author> <title> A Comparison of Three Current Superscalar Designs, </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(3) </volume> <pages> 14-21, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: This allows us to amortize the overhead of the page miss over as many accesses as there are registers available to hold data. The Intel i960MM has a local register cache with 240 entries that could be used to store vector elements for this scheme <ref> [Lai92] </ref>, and the AMD AM29000 has 192 registers [Tab91], but most processors have far fewer registers at their disposal. Assuming for vectors of 64-bit words would probably be optimistic for most computations and current architectures.
Reference: [Lam79] <author> L. Lamport, </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):241-248, </volume> <month> September </month> <year> 1979. </year> <note> Cited in [Lil93]. </note>
Reference-contexts: In other words, the execution of the parallel program must appear as some interleaving of the execution of the parallel processes on a sequential machine <ref> [Lam79] </ref>. This strong ordering of memory accesses severely limits the allowable overlap of memory operations. Other memory consistency models, such as processor consistency [Gha90,Gha91, Goo91], weak ordering [Adv90,Dub86,Dub88], and release consistency [Car91,Gha90], allow a greater overlap of memory reads and writes.
Reference: [Lam91] <author> M. Lam, E. Rothberg, and M.E. Wolf, </author> <title> The Cache Performance and Optimizations of Blocked Algorithms, </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(2) </volume> <pages> 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: For instance, in parallelizing a Fast Fourier Transform, Gannon and Jalby use copying to generate the transpose of a matrix, giving both row-wise and column-wise array accesses the same locality of reference [Gan87]. Lam, Rothberg, and Wolf <ref> [Lam91] </ref> investigate blocking in conjunction with copying in order to eliminate self-interference, or cache misses caused by more than one element of a given vector mapping to the same location. <p> These studies assume a uniform memory access cost, and so they dont address minimizing the time to load vector data into cache. These techniques will also deliver better performance when integrated with methods to make more efficient use of memory resources. Lam, Rothberg, and Wolf <ref> [Lam91] </ref> develop a model of data conicts and demonstrate that the amount of cache interference is highly dependent on block size and vector stride, with large variations in performance for matrices of different sizes. <p> These reports do not attempt to develop a general performance model, nor do they present measured timing results specific to this particular optimization. Chapter 2: Access Ordering 40 Copying incurs an overhead cost proportional to the amount of data being copied, but the benefits often outweigh the cost <ref> [Lam91] </ref>, and Temam, Granston, and Jalby [Tem93] present a compile-time technique for determining when copying is advantageous. Using caching loads to create the copy can cause subtle problems with self-interference. As new data from the original vector is loaded, it may evict cache lines holding previously copied data.
Reference: [Lan95a] <author> T.C. Landon, R.H. Klenke, J.H. Aylor, M.H. Salinas, and S.A. McKee, </author> <title> An Bibliography 191 Approach for Optimizing Synthesized High-Speed ASICs, </title> <booktitle> to appear in Proceedings of the IEEE International ASIC Conference (ASIC95), </booktitle> <address> Austin, TX, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: Since there are two interleaved banks of memory, for streams with relatively prime strides the SMC can deliver one data item every 25ns processor cycle. Further details of the design, implementation, and testing of the SMC ASIC and daughterboard can be found in other publications <ref> [McG94, Lan95a, Lan95b, Alu95] </ref>. Chapter 6: The SMC Hardware 136 6.3 Programmers Interface The processor interacts with the SMC via a set of memory-mapped registers.
Reference: [Lan95b] <author> T.C. Landon, </author> <title> Optimizing Synthesized High-Speed ASICs, </title> <type> Masters thesis, </type> <institution> School of Engineering and Applied Science, University of Virginia, </institution> <note> to be published August 1995. </note>
Reference-contexts: Since there are two interleaved banks of memory, for streams with relatively prime strides the SMC can deliver one data item every 25ns processor cycle. Further details of the design, implementation, and testing of the SMC ASIC and daughterboard can be found in other publications <ref> [McG94, Lan95a, Lan95b, Alu95] </ref>. Chapter 6: The SMC Hardware 136 6.3 Programmers Interface The processor interacts with the SMC via a set of memory-mapped registers.
Reference: [Law79] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh, </author> <title> Basic Linear Algebra Subprograms for Fortran Usage, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(3) </volume> <pages> 308-329, </pages> <month> September </month> <year> 1979. </year> <note> Cited in [Don91]. </note>
Reference: [Lee87] <author> R.L. Lee, P.-C. Yew, and D.H. Lawrie, </author> <title> Data Prefetching in Shared Memory Multiprocessors, </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 28-31, </pages> <month> August </month> <year> 1987. </year>
Reference: [Lee90] <author> K. Lee, </author> <title> On the Floating Point Performance of the i860 Microprocessor, </title> <type> NAS Technical Report RNR-90-019, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: Several schemes for avoiding bank contention, either by address transformations, skewing, prime memory systems, or dynamically scheduling accesses have been published [Bir91,Bud71,Gao93,Har87,Har89,Rau91]; these, too, are complementary to the techniques for improving bandwidth that we analyze here. Both Moyer [Moy91] and Lee <ref> [Lee90] </ref> investigate the oating point and memory performance of the i860XR. Results from our experiments with this architecture agree largely with their findings. 2.5 Summary As processors become faster, memory bandwidth is rapidly becoming the performance bottleneck in the application of high performance microprocessors to vector-like algorithms.
Reference: [Lee91] <author> K. Lee, </author> <title> Achieving High Performance on the i860 Microprocessor, </title> <type> NAS Technical Report RNR-91-029, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> October </month> <year> 1991. </year>
Reference: [Lee93] <author> K. Lee, </author> <title> The NAS860 Library Users Manual, </title> <type> NAS Technical Report RND-93-003, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Moyers scheme unrolls loops and groups accesses to each stream, so that the cost of each DRAM page-miss can be amortized over several references to the same page. Lee develops subroutines to mimic Cray instructions on the Intel i860XR <ref> [Lee93] </ref>. His routine for streaming vector elements reads data in blocks (using non-caching load instructions) and then writes the data to a pre-allocated portion of cache. Meadows describes a similar scheme for the PGI i860 compiler [Mea92], and Loshin and Budge give a general description of the technique [Los92].
Reference: [Len90] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy, </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor, </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 18(2) </volume> <pages> 148-159, </pages> <month> June </month> <year> 1990. </year>
Reference: [Leu90] <author> S.T. </author> <title> Leutenegger, Issues in Multiprogrammed Multiprocessor Scheduling, </title> <type> Ph.D. thesis, </type> <institution> University of Wisconsin-Madison, </institution> <type> Technical Report CS-TR-90-954, </type> <month> August </month> <year> 1990. </year>
Reference-contexts: For instance, Li and Petersen [LiP91] show that for memory system extensions, direct management of remote memories performs better than using the extended memory modules as a transparent cache between main memory and disk. Leutenegger <ref> [Leu90] </ref> and Ousterhout et al. [Ous80] argue for gang scheduling of CPU resources. Burger et al. [Bur94] confirm the importance of gang CPU scheduling and argue that for good Chapter 7: Compiling for Dynamic Access Ordering 154 performance, virtual memory pages must be gang scheduled as well.
Reference: [Lil93] <author> D.A. Lilja, </author> <title> Cache Coherence in Large-Scale Shared-Memory Multiprocessors: Issues and Comparisons, </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 303-338, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The first of these, the memory consistency model, defines the order of writes to different objects from the point of view of each of the processors, whereas the second, the coherence mechanism, ensures that all processors see all of the writes to a specific object in the same logical order <ref> [Lil93] </ref>. 8.1.1 Memory Consistency Models The systems memory consistency model defines the programmers view of the time ordering of events (read, write, and synchronization operations) that occur on different processors. <p> The fewer assurances the system makes with respect to the order of events, the greater the potential overlap of operations within the same processor and among different processors <ref> [Lil93] </ref>. Exploiting this potential concurrency can increase system performance [Gha91,Gup91,Tor90,Zuc92]. The sequential consistency model requires that all memory operations are executed in the order defined by the program, and that each access to the shared memory must complete before the next shared-memory access can begin [Lil93]. <p> processor and among different processors <ref> [Lil93] </ref>. Exploiting this potential concurrency can increase system performance [Gha91,Gup91,Tor90,Zuc92]. The sequential consistency model requires that all memory operations are executed in the order defined by the program, and that each access to the shared memory must complete before the next shared-memory access can begin [Lil93]. In other words, the execution of the parallel program must appear as some interleaving of the execution of the parallel processes on a sequential machine [Lam79]. This strong ordering of memory accesses severely limits the allowable overlap of memory operations. <p> Accesses to other shared variables between these synchronization points can occur in any arbitrary order. Each processor must guarantee that all of its outstanding shared-memory accesses complete before it issues a synchronization operation <ref> [Lil93] </ref>. The release consistency model [Gha90] weakens the ordering constraints on synchronization variables by splitting the synchronization operation into separate acquire and release operations. In order to obtain exclusive access to some shared-memory object, a processor executes an acquire operation. <p> After receiving the new value, the directory sends a copy to the requesting (reading) processor <ref> [Lil93] </ref>. Directory schemes differ in the granularity of the objects for which coherence is maintained, the amount of information they maintain about shared objects, where that information is stored, and whether copies are invalidated or updated when the objects value changes. <p> Weak ordering can be implemented by delaying a writing processor only when it accesses a synchronization variable. The processor must ensure that it has received acknowledgments from the directory for all its writes to shared-data objects before it proceeds past a synchronization point <ref> [Lil93] </ref>. These schemes also differ in the extent of the role software plays in maintaining coherence: some schemes rely entirely on hardware [Aga88,Arc85], whereas others use Chapter 8: Other Systems Issues 161 minimal hardware and move many of the responsibilities to software.
Reference: [LiH89] <author> K. Li and P. Hudak, </author> <title> Memory Coherence in Shared Virtual Memory Systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference: [LiM94] <author> Z. Li, F. Mounes-Toussi, and D.J. Lilja, </author> <title> Improving the Performance of a Directory-Based Cache Coherence Mechanism with Compiler Assistance, </title> <note> Bibliography 192 submitted to IEEE Transactions on Parallel and Distributed Systems, </note> <year> 1994. </year>
Reference: [LiN94] <author> Z. Li, and T.N. Nguyen, </author> <title> An Empirical Study of the Work Load Distribution Under Static Scheduling, </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: If the CPUs proceed at different rates, some may cross page boundaries slightly sooner than others, but recent empirical studies suggest that the slowest processor is normally not more than the mean execution time of one loop iteration behind the average processor <ref> [LiN94] </ref>. Alternatively, a static scheduling scheme could partition the data as we have done for prescheduling. SMC results for this kind of block scheduling would be identical to those for our model of prescheduling. <p> Balakrishnan, Jain, and Raghavendra [Bal88] and Seznec and Lenfant [Sez92] propose array storage schemes to avoid bank conicts for parallel processors. Such schemes could be used to increase the number of strides for which SMC systems using FC ordering would perform well. Li and Nguyen <ref> [LiN94] </ref> study the empirical performances of static and dynamic scheduling. Here cyclic scheduling refers to Fortran DOALL loops (as in our model of this scheduling technique), and dynamic scheduling refers to self scheduling, in which processors compete for parallel loop iterations by fetching and updating a loop index variable. <p> Li and Nguyens studies of workload distribution support this conclusion <ref> [LiN94] </ref>. Cyclic scheduling can thus be viewed as an instance of gang scheduling of memory resources, in this case DRAM pages. Such explicit, cooperative management of shared resources has been shown to be an important factor in obtaining good performance on multiprocessor platforms.
Reference: [LiP91] <author> K. Li and K. Petersen, </author> <title> Evaluation of Memory System Extensions, </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(3) </volume> <pages> 84-93, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Cyclic scheduling can thus be viewed as an instance of gang scheduling of memory resources, in this case DRAM pages. Such explicit, cooperative management of shared resources has been shown to be an important factor in obtaining good performance on multiprocessor platforms. For instance, Li and Petersen <ref> [LiP91] </ref> show that for memory system extensions, direct management of remote memories performs better than using the extended memory modules as a transparent cache between main memory and disk. Leutenegger [Leu90] and Ousterhout et al. [Ous80] argue for gang scheduling of CPU resources.
Reference: [LiZ93] <author> Z. Li, </author> <title> Software Assistance for Directory-Based Caches, </title> <booktitle> Proceedings of the IEEE International Parallel Processing Symposium, </booktitle> <year> 1993. </year>
Reference: [Log92] <editor> Smartmodel Library Reference Manual, </editor> <booktitle> Logic Modeling Corporation, </booktitle> <year> 1992. </year>
Reference-contexts: The SMC can deliver a 64-bit doubleword of data every cycle. The SMC was designed using a top-down approach with state-of-the art synthesis tools <ref> [Cas93, Log92, Men93] </ref>. The hardware design has been validated using four different methods: functional simulation, gate-level simulation, static timing analysis, and back-annotated timing simulation.
Reference: [Los92] <author> D. Loshin, and D. Budge, </author> <title> Breaking the Memory Bottleneck, Parts 1 & 2, </title> <booktitle> Supercomputing Review, </booktitle> <month> January/February </month> <year> 1992. </year>
Reference-contexts: His routine for streaming vector elements reads data in blocks (using non-caching load instructions) and then writes the data to a pre-allocated portion of cache. Meadows describes a similar scheme for the PGI i860 compiler [Mea92], and Loshin and Budge give a general description of the technique <ref> [Los92] </ref>. Traditional caching and cache-based software prefetching techniques [Cal91,Che92,Gor90,Kla91] may also be considered schemes. <p> This may limit the applicability of cache-based access ordering techniques discussed here. Block-size limitations can be circumvented by providing a separate buffer space for vector operands. Loshin and Budge <ref> [Los92] </ref> describe streaming in an article on compiler management of the memory hierarchy. Lees investigations of the NASPACK library and the work of Meadows, Nakamoto, and Schuster [Mea92] on the PGI i860 compiler both address streaming in conjunction with other operations. <p> The work of Peris et al. [Per94] strongly suggests that memory considerations must be incorporated in the resource allocation policies for distributed parallel systems. Other studies focus specifically on memory hierarchy utilization. For instance, Loshin and Budge <ref> [Los92] </ref> argue for memory hierarchy management by the compiler.
Reference: [McG94] <editor> S.W. McGee, R.H. Klenke, J.H. Aylor, and A.J. Schwab, </editor> <title> Design of a Processor Bus Interface ASIC for the Stream Memory Controller, </title> <booktitle> Proceedings of the IEEE International ASIC Conference (ASIC94), </booktitle> <address> Rochester, NY, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: Since there are two interleaved banks of memory, for streams with relatively prime strides the SMC can deliver one data item every 25ns processor cycle. Further details of the design, implementation, and testing of the SMC ASIC and daughterboard can be found in other publications <ref> [McG94, Lan95a, Lan95b, Alu95] </ref>. Chapter 6: The SMC Hardware 136 6.3 Programmers Interface The processor interacts with the SMC via a set of memory-mapped registers.
Reference: [McK93a] <author> S.A. McKee, </author> <title> Hardware Support for Dynamic Access Ordering: Performance of Some Design Options, </title> <type> Technical Report CS-93-08, </type> <institution> Department of Computer Science, University of Virginia, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: The first of these encourages several banks to be working on the same FIFO, while the second encourages different banks to be working on different FIFOs. It is not intuitively obvious which of these is preferable, and in fact, our experiments demonstrate no consistent performance advantage to either <ref> [McK93a] </ref>. 3.2 Analytic Models For the systems we consider, bandwidth is limited by how many page-misses a computation incurs. <p> There is no notion of dynamic load balancing with respect to data size or number of processors. This type of scheduling is particularly appropriate for applications exhibiting functional parallelism, where each CPU performs a different task. Since performance on a single CPU is relatively independent of access pattern <ref> [McK93a] </ref>, we model prescheduled computations by running the same benchmark 1. As in Osterhaug [Ost89], we use scheduling to refer to when and how a computation is divided into tasks. For the purposes of this discussion, scheduling is synonymous with partitioning. Chapter 4: Multiprocessor SMC Performance 78 on all processors. <p> policy simulated for a given memory system and benchmark. 4.4.1 Ordering Policy The overwhelming similarity of the performance curves presented in Chapter 3 and our uniprocessor SMC studies indicates that neither the ordering strategy nor the processors access pattern has a large effect on the MSUs ability to optimize bandwidth <ref> [McK93a, McK93c] </ref>. For moderately long vectors whose stride is relatively prime to the number of memory banks, the SMC consistently delivers nearly the full system bandwidth. <p> In the uniprocessor SMC study, FC is called A1, Token BC is called T1, Token TBC is called T2, and Exhaustive BC is called R1 <ref> [McK93a] </ref>. <p> Given that the effects of changes in relative vector alignment, vector length, or the implementation of an ordering policy (e.g. different FIFO orderings) are fairly independent of the processors access pattern, most of the graphs presented here focus on a single benchmark, daxpy. Like the uniprocessor SMC systems studied <ref> [McK93a] </ref>, multiprocessor SMC performance approaches (and often exceeds) 90% of the peak system bandwidth for sufficiently long vectors and appropriately-sized FIFOs. four, and eight processors.
Reference: [McK93b] <author> S.A. McKee, </author> <title> An Analytic Model of SMC Performance, </title> <type> Technical Report CS-93-54, </type> <institution> Department of Computer Science, University of Virginia, </institution> <month> November </month> <year> 1993. </year>
Reference: [McK93c] <author> S.A. McKee, </author> <title> Uniprocessor SMC Performance on Vectors with Non-Unit Strides, </title> <type> Technical Report CS-93-67, </type> <institution> Department of Computer Science, University of Virginia, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: policy simulated for a given memory system and benchmark. 4.4.1 Ordering Policy The overwhelming similarity of the performance curves presented in Chapter 3 and our uniprocessor SMC studies indicates that neither the ordering strategy nor the processors access pattern has a large effect on the MSUs ability to optimize bandwidth <ref> [McK93a, McK93c] </ref>. For moderately long vectors whose stride is relatively prime to the number of memory banks, the SMC consistently delivers nearly the full system bandwidth.
Reference: [McK94a] <author> S.A. McKee, R.H. Klenke, A.J. Schwab, Wm.A. Wulf, S.A. Moyer, C. Hitchcock, and J.H. Aylor, </author> <title> Experimental Implementation of Dynamic Access Ordering, </title> <booktitle> Proceedings of the IEEE 27th Hawaii International Conference on Systems Sciences (HICSS-27), </booktitle> <pages> pages 431-440, </pages> <address> Maui, HI, </address> <month> January </month> <year> 1994. </year>
Reference: [McK94b] <author> S.A. McKee, S.A. Moyer, Wm.A. Wulf, and C. Hitchcock, </author> <title> Increasing Memory Bandwidth for Vector Computations, </title> <booktitle> Lecture Notes in Computer Science 782: Proceedings of the Conference on Programming Languages and Systems Architectures (PLSA, </booktitle> <address> Zurich, Switzerland), </address> <pages> pages 87-104, </pages> <publisher> Springer Verlag, </publisher> <year> 1994. </year> <note> Bibliography 193 </note>
Reference: [McK94c] <author> S.A. McKee, </author> <title> Dynamic Access Ordering for Symmetric Shared-Memory Multiprocessors, </title> <type> Technical Report CS-94-14, </type> <institution> Department of Computer Science, University of Virginia, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Unless otherwise stated, all vectors are aligned to DRAM page boundaries, tasks are apportioned such that all vectors (and each CPUs vector blocks, for block-scheduled workloads) are aligned to begin in bank 0 , and the MSU uses interleaved FIFO ordering. The multiprocessor SMC technical report <ref> [McK94c] </ref> gives complete simulation results for all benchmarks on a wider range of SMC configurations. We present only highlights of these results here. <p> When their performances differ, TBCs is slightly lower. Exhaustive bank-selection affords little advantage over either variation of the simpler Token bank selection. Similarly, changing the ordering in which banks or FIFOs are considered generally results in performance differences of less than 1% of peak <ref> [McK94c] </ref>. (a) Token BC (seq. sets) (b) Token BC (mod. sets) (c) Token TBC (seq. sets) (d) Exhaustive BC (e) FC Chapter 4: Multiprocessor SMC Performance 96 FIFO-Centric ordering performs slightly worse than Bank-Centric ordering for relatively shallow FIFO depths.
Reference: [McK94d] <author> S.A. McKee, </author> <title> Analytic Models of SMC Performance, </title> <type> Technical Report CS-94-38, </type> <institution> Department of Computer Science, University of Virginia, </institution> <month> October </month> <year> 1994. </year>
Reference: [McK95a] <author> S.A. McKee and Wm.A. Wulf, </author> <title> Access Ordering and Memory-Conscious Cache Utilization, </title> <booktitle> Proceedings of the First IEEE Symposium on High Performance Computer Architecture (HPCA), </booktitle> <pages> pages 253-262, </pages> <address> Raleigh, NC, </address> <month> January </month> <year> 1995. </year>
Reference: [McK95b] <author> S.A. McKee, </author> <title> Wm.A. Wulf, and T.C. Landon, Bounds on Memory Bandwidth in Streamed Computations, </title> <note> to appear in Proceedings of Europar95, </note> <institution> Stockholm, Sweden, </institution> <month> August </month> <year> 1995. </year>
Reference: [McM86] <author> F.H. McMahon, </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> UCRL-53745, Lawrence Livermore National Laboratory, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: with timings on an Intel i860XR Throughout this and subsequent chapters, the term page refers to a DRAM page, unless otherwise noted. 2.1 Bandwidth Example To illustrate one aspect of the bandwidth problem and how it might be addressed at compile time consider executing the fifth Livermore Loop (tridiagonal elimination) <ref> [McM86] </ref> using non-caching accesses to reference a single bank of page-mode DRAMs. For simplicity, we omit arithmetic instructions from our code fragments. <p> The dynamic access ordering results given here are for a computation involving three vector operands (such as the first and fifth Livermore Loops <ref> [McM86] </ref>, hydro z l 4=( ) Chapter 2: Access Ordering 31 fragment and our tridiag example from Section 2.1). Average cycles per element will be slightly lower for computations on fewer vectors and slightly higher for computations involving more. <p> Daxpy, copy, scale, and swap are from the BLAS (Basic Linear Algebra Subroutines) [Law79,Don79]. These vector computations occur frequently in practice, and thus have been collected into libraries of highly optimized routines for various host architectures. Hydro and tridiag are the first and fifth Livermore Loops <ref> [McM86] </ref>, a set of kernels culled from important scientific computations. The former is a fragment of a hydrodynamics computation, and the latter performs tridiagonal gaussian elimination. Since these two benchmarks share the same access pattern, their simulation results are identical, and will be presented together. <p> The retained values are pipelined through a set of registers, advancing one register on each iteration until consumed by a read instruction, as shown in Figure 7.2. As an example, consider the fifth Livermore loop, tridiagonal elimination <ref> [McM86] </ref>. Naive C code for this loop is depicted in Figure 7.3 (a). On each iteration, only the x value from the previous iteration is needed, and so a single register suffices to hold the retained values for this particular recurrence, as in Figure 7.3 (b).
Reference: [Mea92] <author> L. Meadows, S. Nakamoto, and V. Schuster, </author> <title> A Vectorizing Software Pipelining Compiler for LIW and Superscalar Architectures, </title> <booktitle> Proceedings of RISC92, </booktitle> <pages> pages 331-343. </pages>
Reference-contexts: Lee develops subroutines to mimic Cray instructions on the Intel i860XR [Lee93]. His routine for streaming vector elements reads data in blocks (using non-caching load instructions) and then writes the data to a pre-allocated portion of cache. Meadows describes a similar scheme for the PGI i860 compiler <ref> [Mea92] </ref>, and Loshin and Budge give a general description of the technique [Los92]. Traditional caching and cache-based software prefetching techniques [Cal91,Che92,Gor90,Kla91] may also be considered schemes. <p> Block-size limitations can be circumvented by providing a separate buffer space for vector operands. Loshin and Budge [Los92] describe streaming in an article on compiler management of the memory hierarchy. Lees investigations of the NASPACK library and the work of Meadows, Nakamoto, and Schuster <ref> [Mea92] </ref> on the PGI i860 compiler both address streaming in conjunction with other operations. These reports do not attempt to develop a general performance model, nor do they present measured timing results specific to this particular optimization. <p> Research on blocking and copying has focused primarily on improving performance for data that is reused, the traditional assumption being that there is no advantage to applying these transformations to data that is only used once. In contrast, reports on the NASPACK routines [Lee91,Lee93] and the PGI compiler <ref> [Mea92] </ref> suggest that by exploiting memory properties, these techniques may also benefit single-use vectors and those that do not remain in cache between uses. Our results support these conclusions. Palacharla and Kessler [Pal95] investigate software restructuring to improve memory performance on a Cray T3D.
Reference: [Men93] <author> System-1076, </author> <title> Quicksim II Users Manual, Mentor Graphics Corporation, 1993. [Mic94] 1994 DRAM Data Book, Extended Data Out, Technical Note TN-04-21, Micron Semiconductor, </title> <publisher> Inc., </publisher> <year> 1994. </year>
Reference-contexts: The SMC can deliver a 64-bit doubleword of data every cycle. The SMC was designed using a top-down approach with state-of-the art synthesis tools <ref> [Cas93, Log92, Men93] </ref>. The hardware design has been validated using four different methods: functional simulation, gate-level simulation, static timing analysis, and back-annotated timing simulation.
Reference: [Mot93] <author> Motorola, Inc., </author> <title> PowerPC 601 RISC Microprocessor Users Manual, </title> <year> 1993. </year>
Reference-contexts: Although rare, these instructions are available in some commercial processors, such as the Convex C-1 [Wal85] and Intel i860 [Int91]. Most current microprocessors (including the DEC Alpha [Dig92], MIPS [Kan92], Intel 80486, Pentium, and i860 [Tab91], and the PowerPC <ref> [Mot93] </ref>) provide a means of specifying some memory pages as non-cacheable, even though these mechanisms is not generally accessible to the user. Our investigation targets one aspect of cache performance that has been overlooked: the time to load a vector, regardless of whether or not data is reused. <p> The compiler could place all stream data in non-cacheable memory, thereby achieving the same effect as a system in which the SMC and cache reference physically distinct memory partitions. Most current microprocessors (including the DEC Alpha [Dig92], MIPS [Kan92], Intel 80x86 series and i860 [Tab91], and the PowerPC <ref> [Mot93] </ref>) provide a means of specifying some memory pages as non-cacheable. Another option is to ush the cache before entering streaming loops.
Reference: [Mow92] <author> T.C. Mowry, M. Lam, and A. Gupta, </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching, </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), published as ACM SIGPLAN Notices, </booktitle> <volume> 27(9) </volume> <pages> 62-73, </pages> <month> September </month> <year> 1992. </year>
Reference: [Moy91] <author> S.A. Moyer, </author> <title> Performance of the iPSC/860 Node Architecture, </title> <type> Technical Report IPC-TR-91-007, </type> <institution> Institute for Parallel Computation, University of Virginia, </institution> <year> 1991. </year>
Reference-contexts: The i860XR cache controller prevents block-prefetching as described in Section 2.3.1.2. On (a) (b) (e) stride = 2 (f) stride = 3 stride = 4 Chapter 2: Access Ordering 36 this machine, each successive cache-line fill incurs a 7-cycle delay <ref> [Moy91] </ref>, causing the memory controller to transition to its idle state. The next memory access takes as long as a DRAM page-miss, regardless of whether or not it lies in the same page as the previous access. <p> Several schemes for avoiding bank contention, either by address transformations, skewing, prime memory systems, or dynamically scheduling accesses have been published [Bir91,Bud71,Gao93,Har87,Har89,Rau91]; these, too, are complementary to the techniques for improving bandwidth that we analyze here. Both Moyer <ref> [Moy91] </ref> and Lee [Lee90] investigate the oating point and memory performance of the i860XR. Results from our experiments with this architecture agree largely with their findings. 2.5 Summary As processors become faster, memory bandwidth is rapidly becoming the performance bottleneck in the application of high performance microprocessors to vector-like algorithms.
Reference: [Moy93] <author> S.A. Moyer, </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. Thesis, </type> <institution> School of Engineering and Applied Science, University of Virginia, </institution> <month> May </month> <year> 1993. </year> <note> Also Technical Report CS-93-18, </note> <institution> Department of Computer Science, </institution> <month> April </month> <year> 1993. </year> <note> Bibliography 194 </note>
Reference-contexts: This taxonomy classifies access ordering systems by a tuple indicating the time at which each function is performed. 2.2.1 Compile-Time Schemes Benitez and Davidson [Ben91] detect streams at compile time, and Moyer <ref> [Moy93] </ref> has derived access-ordering algorithms relative to a precise analytic model of memory systems. Moyers scheme unrolls loops and groups accesses to each stream, so that the cost of each DRAM page-miss can be amortized over several references to the same page. <p> On some architectures, it may be possible to overlap the writes to cache with non-caching loads, in which case t cw drops out of the equation. 2.3.1.4 Static Access Ordering Moyer derives compile-time ordering algorithms <ref> [Moy93] </ref> to maximize bandwidth for non-caching register accesses. This approach unrolls loops and orders non-caching memory operations to exploit architectural and device features of the target memory system. The tridiag example of Section 2.1 illustrates the resulting bandwidth benefits: unrolling eight times yields a performance improvement of almost a 200%. <p> Exploiting the read-ahead mechanism also exploits the fast-page mode of the T3Ds memory components. In order to make effective use of both architectural features, the authors recommend unrolling loops and grouping accesses to each vector, as in Moyers static access ordering <ref> [Moy93] </ref>. They also implement block prefetching (as described in Section 2.3.1.2) by reading one element of each cache line for a block of data before entering the inner loop. <p> Because the SMC reorders accesses, differences in operand alignment have little effect on its ability to maximize bandwidth: the SMC performances in Figure 3.19 (b) and (c) are almost identical. 1. The non-SMC data was generated with Moyers static access ordering software <ref> [Moy93] </ref>. Chapter 3: Uniprocessor SMC Performance 73 3.4 Summary In Chapter 2 we saw that reordering can optimize stream accesses to exploit the underlying memory architecture. In this chapter, we investigated combining compile-time detection of streams with execution-time selection of the access order and issue. <p> Unfortunately, using the SMC results in lower performance than using non-caching loads to access the data in the natural order of the computation. The processor stalls for longer periods of time when using the SMC, since 1. The non-SMC results were generated using Moyers static access ordering software <ref> [Moy93] </ref>. sn h n d ( )+( ) t ph t cycles 1.5n t cycles snt ph h n d ( ) t pm +( ) t cycles d 2= (a) ratio = 2 (b) ratio = 4 (c) ratio = 8 Chapter 5: Sparse Matrix Computations 125 it must often <p> The cost of switching between reading and writing should be amortized over as many accesses as possible. Good performance requires unrolling loops and grouping reads and writes in order to minimize the number of bus read/ write transitions. As in Moyers static access ordering methods <ref> [Moy93] </ref>, the degree to which this can be done depends on processor parameters such as the size of the register file.
Reference: [Ngu94] <author> T.N. Nguyen, F. Mounes-Toussi, D.J. Lilja, and Z. Li, </author> <title> A Compiler-assisted Scheme for Adaptive Cache Coherence Enforcement, </title> <booktitle> Proceedings of Parallel Architectures and Compilation Techniques (PACT94), </booktitle> <year> 1994. </year>
Reference-contexts: Compiler-assisted mechanisms rely on the compiler to reduce the coherence overhead, either by telling the directory hardware which type of coherence action to1 perform for a given reference, or by decreasing the number of coherence actions generated by the program. For instance, Nguyen et al. <ref> [Ngu94] </ref> present a compile-time optimization that selects updating, invalidating, or neither for each write reference in a program. This adaptive coherence enforcement mechanism frequently results in less total network traffic than hardware-only mechanisms. <p> It may be feasible to enforce coherence on blocks of stream data up to the size of DRAM pages. Using a larger granularity decreases the number of coherence messages required during a computation. The results of Li, Mounes-Toussi, and Lilja [LiZ93,LiM94], Nguyen et al. <ref> [Ngu94] </ref> and Jinturkar [Jin94] suggest that much of the responsibility for maintaining consistency can be moved to the compiler, so that the accompanying hardware mechanisms can be made as simple and fast as possible.
Reference: [0Kr90] <author> B.W. OKrafka and A.R. </author> <title> Newton, An Empirical Evaluation of Two Memory-Efficient Directory Methods, </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 18(2) </volume> <pages> 138-147, </pages> <month> June </month> <year> 1990. </year>
Reference: [Ost89] <author> A. Osterhaug, ed., </author> <title> Guide to Parallel Programming on Sequent Computer Systems, </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: With more buffer space devoted to operands from that page, more accesses can be issued to it in succession, resulting in greater bandwidth. Three general scheduling techniques are commonly used to parallelize workloads: prescheduling, static scheduling, and dynamic scheduling <ref> [Ost89] </ref>. 1 Prescheduling requires that the programmer divide the workload among the processors before compiling the program. There is no notion of dynamic load balancing with respect to data size or number of processors. <p> This type of scheduling is particularly appropriate for applications exhibiting functional parallelism, where each CPU performs a different task. Since performance on a single CPU is relatively independent of access pattern [McK93a], we model prescheduled computations by running the same benchmark 1. As in Osterhaug <ref> [Ost89] </ref>, we use scheduling to refer to when and how a computation is divided into tasks. For the purposes of this discussion, scheduling is synonymous with partitioning. Chapter 4: Multiprocessor SMC Performance 78 on all processors.
Reference: [Ous80] <author> J.K. Ousterhout, D.A. Scelza, and P.S. Sindhu, </author> <title> Medusa: An Experiment in Distributed Operating System Structure, </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 92-105, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: For instance, Li and Petersen [LiP91] show that for memory system extensions, direct management of remote memories performs better than using the extended memory modules as a transparent cache between main memory and disk. Leutenegger [Leu90] and Ousterhout et al. <ref> [Ous80] </ref> argue for gang scheduling of CPU resources. Burger et al. [Bur94] confirm the importance of gang CPU scheduling and argue that for good Chapter 7: Compiling for Dynamic Access Ordering 154 performance, virtual memory pages must be gang scheduled as well.
Reference: [Pal95] <author> S. Palacharla and R.E. Kessler, </author> <title> Code Restructuring to Exploit Page Mode and Read-Ahead Features of the Cray T3D, work in progress, personal communication with R.E. </title> <type> Kessler, </type> <month> February </month> <year> 1995. </year>
Reference-contexts: The lookahead technique proposed by Bird and Uhlig [Bir91] uses a Bank Active Scoreboard to order accesses dynamically to avoid bank contention, but like most others, this scheme does nothing to exploit device characteristics such as fast-page mode. Palacharla and Kessler <ref> [Pal95] </ref> investigate code restructuring techniques to exploit an unit-stride read-ahead stream buffer and fast-page mode memory devices on the Cray T3D. <p> In contrast, reports on the NASPACK routines [Lee91,Lee93] and the PGI compiler [Mea92] suggest that by exploiting memory properties, these techniques may also benefit single-use vectors and those that do not remain in cache between uses. Our results support these conclusions. Palacharla and Kessler <ref> [Pal95] </ref> investigate software restructuring to improve memory performance on a Cray T3D. This machine includes a single, stride-one read-ahead stream buffer to prefetch data to cache. When enabled, the read-ahead buffer fetches the next consecutive cache line whenever there is a cache miss. <p> grouping accesses, as in Section 2.3.1.2 and Section 7.2, have been used to compile for at least one other dynamic access ordering system: Palacharla and Kessler employ these techniques in conjunction with preloading data to cache in order to exploit page-mode devices and the read-ahead hardware of the Cray T3D <ref> [Pal95] </ref>. As discussed in Section 4.6 and Section 7.4, the superior performance of cyclic scheduling over block scheduling results from the fact that the former allows all processors to share the same working set of DRAM pages throughout most of the computation.
Reference: [Per94] <author> V.G.J. Peris, M.S. Squillante, and V.K. Naik, </author> <title> Analysis of the Impact of Memory in Distributed Parallel Processing Systems, </title> <booktitle> Proceedings of Sigmetrics94, </booktitle> <address> Santa Clara, CA, </address> <year> 1994. </year>
Reference-contexts: They show that the traditional benefits that paging provides on uniprocessors are diminished by the interactions between the CPU scheduling discipline, the applications synchronization patterns, context switching and paging overheads, and the applications page reference patterns. The work of Peris et al. <ref> [Per94] </ref> strongly suggests that memory considerations must be incorporated in the resource allocation policies for distributed parallel systems. Other studies focus specifically on memory hierarchy utilization. For instance, Loshin and Budge [Los92] argue for memory hierarchy management by the compiler.
Reference: [Por89] <author> A.K. Porterfield, </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications, </title> <type> Ph.D. Thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference: [Prz90] <author> S. Przybylski, </author> <title> The Performance Impact of Block Sizes and Fetch Strategies, </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 18(2) </volume> <pages> 160-169, </pages> <month> June </month> <year> 1990. </year>
Reference: [Qui91] <author> R. Quinnell, </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991. </year>
Reference: [Ram92] <institution> Architectural Overview, Rambus Inc., Mountain View, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: This dissertation focuses on maximizing bandwidth for interleaved memories composed of page-mode DRAMs, but the concepts presented here apply to any memory system in which access costs are sensitive to the history of requests. These include distributed shared memories, systems composed of devices like the new Rambus <ref> [Ram92] </ref> or JEDEC synchronous DRAMs [IEE92], and disks.
Reference: [Rau91] <author> B.R. Rau, </author> <title> Pseudo-Randomly Interleaved Memory, </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture (ISCA), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(3) </volume> <pages> 74-83, </pages> <month> May </month> <year> 1991. </year>
Reference: [Sez92] <author> A. Seznec and J. Lenfant, </author> <title> Interleaved Parallel Schemes: Improving Memory Throughput on Supercomputers, </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <address> Gold Coast, Australia, </address> <booktitle> published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 20(2) </volume> <pages> 246-255, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The scheme does not reorder accesses to maximize a CPUs utilization of its time slots. Balakrishnan, Jain, and Raghavendra [Bal88] and Seznec and Lenfant <ref> [Sez92] </ref> propose array storage schemes to avoid bank conicts for parallel processors. Such schemes could be used to increase the number of strides for which SMC systems using FC ordering would perform well. Li and Nguyen [LiN94] study the empirical performances of static and dynamic scheduling.
Reference: [Shi91] <author> H. Shing and L.M. Ni, </author> <title> A Conict-Free Memory Design for Bibliography 195 Multiprocessors, </title> <booktitle> Proceedings of ACM Supercomputing91, </booktitle> <pages> pages 46-55, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Their approach allows them to identify restrictions to buffering that different coherence policies impose on shared-memory systems. Shing and Ni <ref> [Shi91] </ref> propose a shared memory organization and interconnection network structure that supports conict-free accesses to the shared memory in multiprocessors. Their scheme uses time multiplexing to force the processors to take turns accessing the interleaved memory banks: each CPU can access a subset of the banks on each turn.
Reference: [Skl92] <author> I. Sklenar, </author> <title> Prefetch Unit for Vector Operation on Scalar Computers, </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(4) </volume> <pages> 31-37, </pages> <month> September </month> <year> 1992. </year>
Reference: [Smi84] <author> J.E. Smith, </author> <title> Decoupled Access/Execute Architectures, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4) </volume> <pages> 289-308, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: One way to do this is via Benitez and Davidsons code generation and optimization algorithms [Ben92]. Their algorithms were developed for the WM, a novel superscalar architecture with hardware support for streaming [Wul93]. Although designed for architectures with hardware support for the access/execute model of computation in general <ref> [Smi84] </ref>, many of the techniques are applicable to stock microprocessors. Although these algorithms were not developed as part this dissertation, the compiler technology they represent is a necessary part of our approach to access ordering. We therefore include a description of the algorithms here.
Reference: [Smi87] <author> J.E. Smith, G.E. Dermer, </author> <title> B.D. Vanderwarn, S.D. Klinger, C.M. Roszewski, D.L. Fowler, and D.R. Scidmore, The ZS-l Central Processor, </title> <booktitle> Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-II), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 15(5) </volume> <pages> 199-204, </pages> <month> October </month> <year> 1987. </year>
Reference: [Soh91] <author> G. Sohi and M. Franklin, </author> <title> High Bandwidth Memory Systems for Superscalar Processors, </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 19(2) </volume> <pages> 53-62, </pages> <month> April </month> <year> 1991. </year>
Reference: [Sta90] <author> W. </author> <title> Stallings, </title> <booktitle> Computer Organization and Architecture: Principles of Principles of Structure and Function, 2nd ed., </booktitle> <pages> page 104, </pages> <publisher> MacMillan, </publisher> <year> 1990. </year>
Reference: [Sto93] <author> H.S. Stone, </author> <title> High-Performance Computer Architecture, </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference: [Tab91] <author> D. Tabak, </author> <title> Advanced Microprocessors, </title> <publisher> McGraw-Hill, </publisher> <year> 1991. </year>
Reference-contexts: Although rare, these instructions are available in some commercial processors, such as the Convex C-1 [Wal85] and Intel i860 [Int91]. Most current microprocessors (including the DEC Alpha [Dig92], MIPS [Kan92], Intel 80486, Pentium, and i860 <ref> [Tab91] </ref>, and the PowerPC [Mot93]) provide a means of specifying some memory pages as non-cacheable, even though these mechanisms is not generally accessible to the user. <p> The Intel i960MM has a local register cache with 240 entries that could be used to store vector elements for this scheme [Lai92], and the AMD AM29000 has 192 registers <ref> [Tab91] </ref>, but most processors have far fewer registers at their disposal. Assuming for vectors of 64-bit words would probably be optimistic for most computations and current architectures. <p> The compiler could place all stream data in non-cacheable memory, thereby achieving the same effect as a system in which the SMC and cache reference physically distinct memory partitions. Most current microprocessors (including the DEC Alpha [Dig92], MIPS [Kan92], Intel 80x86 series and i860 <ref> [Tab91] </ref>, and the PowerPC [Mot93]) provide a means of specifying some memory pages as non-cacheable. Another option is to ush the cache before entering streaming loops.
Reference: [Tal94] <author> M. Talluri and M.D. Hill, </author> <title> Surpassing the TLB Performance of Superpages with Less Operating System Support, </title> <booktitle> Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: of translations that must be performed may be minimized through the use of superpages, contiguous sets of virtual memory pages such that each set is treated as a unit. 1 Several recent microprocessor architectures support superpages, including the MIPS R4x00 [Kan92], DEC Alpha [Dig92], SPARC, PowerPC, ARM, and HP PA-RISC <ref> [Tal94] </ref>. 1. Superpages are restricted to being a power of 2 times the base page size, and must be aligned (with respect to its size) in both the virtual and physical address spaces [Tal94]. <p> architectures support superpages, including the MIPS R4x00 [Kan92], DEC Alpha [Dig92], SPARC, PowerPC, ARM, and HP PA-RISC <ref> [Tal94] </ref>. 1. Superpages are restricted to being a power of 2 times the base page size, and must be aligned (with respect to its size) in both the virtual and physical address spaces [Tal94].
Reference: [Tem93] <author> O. Temam, E.D. Granston, and W. Jalby, </author> <title> To Copy or Not to Copy: A Compile-Time Technique for Assessing When Data Copying Should Be Used to Eliminate Cache Conicts, </title> <booktitle> Proceedings of ACM Supercomputing93, </booktitle> <pages> pages 410-419, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Chapter 2: Access Ordering 40 Copying incurs an overhead cost proportional to the amount of data being copied, but the benefits often outweigh the cost [Lam91], and Temam, Granston, and Jalby <ref> [Tem93] </ref> present a compile-time technique for determining when copying is advantageous. Using caching loads to create the copy can cause subtle problems with self-interference. As new data from the original vector is loaded, it may evict cache lines holding previously copied data.
Reference: [Tha87] <author> C.P. Thacker and L.C. Stewart, Firey: </author> <title> A Multiprocessor Workstation, </title> <booktitle> Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-II), published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 15(5) </volume> <pages> 164-172, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: a processor detects a memory reference to an object that has been copied into its local memory, it either invalidates [Goo83,Kat85] its local copy so that the next reference will force a current copy to be obtained from global shared memory, or it updates the copy with the new value <ref> [Atk87, Tha87] </ref>. The term snooping usually refers to this type of coherence mechanism for bus-based, shared-memory multiprocessors, but the same principles can be applied to maintain coherence between I/O and cache, between cache and the SMC, between different FIFOs in the SMC, or even between I/O and the SMC.
Reference: [Tor90] <author> J. Torrellas and J. Hennessy, </author> <title> Estimating the Performance Advantages of Relaxing Consistency in a Shared-Memory Multiprocessor, </title> <booktitle> Proceedings Bibliography 196 of the International Conference on Parallel Processing (vol. I, Architecture), </booktitle> <pages> pages 26-33, </pages> <year> 1990. </year>
Reference: [Val92] <author> M. Valero, T. Lang, J.M. Llabera, M. Peiron, E. Ayguad, and J.J. Navarro, J.J., </author> <title> Increasing the Number of Strides for Conict-Free Vector Access, </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <address> Gold Coast, Australia, </address> <booktitle> published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 20(2) </volume> <pages> 372-381, </pages> <month> May </month> <year> 1992. </year>
Reference: [Wal85] <author> S. Wallach, </author> <title> The CONVEX C-1 64-bit Supercomputer, </title> <booktitle> Proceedings of Compcon Spring85, </booktitle> <month> February </month> <year> 1985. </year>
Reference-contexts: Dynamic access ordering requires a small amount of special-purpose hardware, and our static and dynamic access ordering techniques both require non-caching load instructions. Although rare, these instructions are available in some commercial processors, such as the Convex C-1 <ref> [Wal85] </ref> and Intel i860 [Int91]. Most current microprocessors (including the DEC Alpha [Dig92], MIPS [Kan92], Intel 80486, Pentium, and i860 [Tab91], and the PowerPC [Mot93]) provide a means of specifying some memory pages as non-cacheable, even though these mechanisms is not generally accessible to the user.
Reference: [Web89] <author> W.-D. Weber and A. Gupta, </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors, </title> <booktitle> Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), published as ACM SIGARCH Computer Architecture News, </booktitle> 16(?):243-256, April 1989. 
Reference: [Wil87] <author> A.W. Wilson, </author> <title> Hierarchical Cache/Bus Architecture for Shared Memory Multiprocessors, </title> <booktitle> Proceedings of the 14th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <year> 1987. </year> <note> Cited in [Lil93]. </note>
Reference: [Wol89] <author> M. Wolfe, </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference: [Wol94] <author> R. Wolski, </author> <type> personal communication, </type> <month> November </month> <year> 1994. </year>
Reference-contexts: Another option is to ush the cache before entering streaming loops. Completely ushing the cache may be prohibitively expensive, making startup costs too large for streaming to be profitable in most circumstances (for instance, this was true for the Meiko system used at Livermore Labs <ref> [Wol94] </ref>). Whether or not this is the case depends on the parameters of the particular system in question. Programmable caches allow the compiler to manage coherence through software. This requires at least two operations: invalidate and post (which copies a value back to main memory).
Reference: [Wul92] <author> Wm. A. Wulf, </author> <title> Evaluation of the WM Architecture, </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <address> Gold Coast, Australia, </address> <booktitle> published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 20(2) </volume> <pages> 382-390, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Chapter 2: Access Ordering 28 This organization is both simple and practical from an implementation standpoint: similar designs have been built. In fact, the organization is almost identical to the stream units of the WM architecture <ref> [Wul92] </ref>, or may be thought of as a special case of a decoupled access-execute architecture [Goo85,Smi87]. Another advantage is that this combined hardware/software scheme requires no heroic compiler technology the compiler need only detect the presence of streams, as in Benitez and Davidsons algorithm [Ben91].
Reference: [Wul95] <author> Wm. A. Wulf and S.A. McKee, </author> <title> Hitting the Wall: Implications of the Obvious, </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 23(1) </volume> <pages> 20-24, </pages> <month> March </month> <year> 1995. </year> <institution> also University of Virginia, Department of Computer Science, </institution> <type> Technical Report CS-94-38, </type> <month> December </month> <year> 1994. </year>
Reference: [Yan92] <author> Q. Yang, and L.W. Yang, </author> <title> A Novel Cache Design for Vector Processing, where, </title> <year> 1992. </year>
Reference: [Zuc92] <author> R.N. Zucker and J.-L. Baer, </author> <title> A Performance Study of Memory Consistency Models, </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <address> Gold Coast, Australia, </address> <booktitle> published as ACM SIGARCH Computer Architecture News, </booktitle> <volume> 20(2) </volume> <pages> 2-12, </pages> <month> May </month> <year> 1992. </year>
References-found: 147

