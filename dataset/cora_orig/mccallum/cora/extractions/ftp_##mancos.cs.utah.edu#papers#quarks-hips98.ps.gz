URL: ftp://mancos.cs.utah.edu/papers/quarks-hips98.ps.gz
Refering-URL: ftp://mancos.cs.utah.edu/papers/quarks-hips98-abs.html
Root-URL: 
Title: Making Distributed Shared Memory Simple, Yet Efficient  
Author: Mark Swanson Leigh Stoller John Carter 
Affiliation: Computer Systems Laboratory University of Utah  
Abstract: Recent research on distributed shared memory (DSM) has focussed on improving performance by reducing the communication overhead of DSM. Features added include lazy release consistency-based coherence protocols and new interfaces that give programmers the ability to hand tune communication. These features have increased DSM performance at the expense of requiring increasingly complex DSM systems or increasingly cumbersome programming. They have also increased the computation overhead of DSM, which has partially offset the communication-related performance gains. We chose to implement a simple DSM system, Quarks, with an eye towards hiding most computation overhead while using a very low latency transport layer to reduce the effect of communication overhead. The resulting performance is comparable to that of far more complex DSM systems, such as Treadmarks and Cashmere. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Bal, M. Kaashoek, and A. Tanenbaum. Orca: </author> <title> A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 190-205, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. Since this time, the driving goal of DSM research has been to improve performance by any means necessary <ref> [1, 2, 4, 7, 9, 11, 13, 14, 21] </ref>. The performance of DSM systems is dominated by two issues: (i) the communication overhead required to maintain consistency and (ii) the computation overhead required to service remote requests and compute the information needed to maintain consistency. <p> Over the years, there has been extensive research aimed at dramatically reducing the communication overhead required to keep DSM-managed data consistent. These efforts have included: * exploiting object encapsulation to give programmers the ability to precisely specify the coher ence unit <ref> [1, 2, 7, 9] </ref>, * allowing programmers to explicitly lock data to specific nodes and thereby control at a fine grain when data is communicated [9, 11], * supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing [4, 14], and * using various <p> However, these efforts to reduce communication overhead often increase the system complexity and computation overhead. Modern DSM systems have added other overheads as well. Many modern systems require programmers to write their programs in very constrained ways <ref> [1, 2, 7, 13] </ref>, in effect making the DSM programmer perform much of the work required of message passing programmers in terms of specifying what data should be shipped when. Other DSM systems require complex runtime systems or large amounts of DRAM for storing DSM state [14, 20].
Reference: [2] <author> B. Bershad, M. Zekauskas, and W. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In COMPCON '93, </booktitle> <pages> pages 528-537, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. Since this time, the driving goal of DSM research has been to improve performance by any means necessary <ref> [1, 2, 4, 7, 9, 11, 13, 14, 21] </ref>. The performance of DSM systems is dominated by two issues: (i) the communication overhead required to maintain consistency and (ii) the computation overhead required to service remote requests and compute the information needed to maintain consistency. <p> Over the years, there has been extensive research aimed at dramatically reducing the communication overhead required to keep DSM-managed data consistent. These efforts have included: * exploiting object encapsulation to give programmers the ability to precisely specify the coher ence unit <ref> [1, 2, 7, 9] </ref>, * allowing programmers to explicitly lock data to specific nodes and thereby control at a fine grain when data is communicated [9, 11], * supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing [4, 14], and * using various <p> supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing [4, 14], and * using various forms of relaxed consistency models, thereby increasing the restrictions imposed on the programmer but increasing the flexibility of the underlying DSM system in when it must communicate <ref> [2, 4, 14] </ref>. These innovations improved DSM performance to varying degrees, sometimes to the point where soft ware DSM systems can rival the performance of hard-ware DSM systems for moderately-grained applications [8]. However, these efforts to reduce communication overhead often increase the system complexity and computation overhead. <p> However, these efforts to reduce communication overhead often increase the system complexity and computation overhead. Modern DSM systems have added other overheads as well. Many modern systems require programmers to write their programs in very constrained ways <ref> [1, 2, 7, 13] </ref>, in effect making the DSM programmer perform much of the work required of message passing programmers in terms of specifying what data should be shipped when. Other DSM systems require complex runtime systems or large amounts of DRAM for storing DSM state [14, 20]. <p> Many DSM systems have attacked individual components of this overhead. For example, CRL [13], Midway <ref> [2] </ref>, and Shasta [20] eliminate memory protection and page fault handling overheads by detecting memory references via annotated load and store operations. Unfortunately, these efforts turned out to increase system overhead [20] or add cumbersome programming requirements [2, 13]. <p> For example, CRL [13], Midway [2], and Shasta [20] eliminate memory protection and page fault handling overheads by detecting memory references via annotated load and store operations. Unfortunately, these efforts turned out to increase system overhead [20] or add cumbersome programming requirements <ref> [2, 13] </ref>. <p> No-Copy Twin Creation We found that twin creation, which involves a 4-kilobyte bcopy (), adds significant overhead. This same observation is what drove Midway to detect writes explicitly rather than via diffs <ref> [2] </ref>. We reduced the impact of twin creation in the common case as follows. Originally, whenever a clean page is dirtied, a twin is saved to facilitate diff creation at the next release point. When a page's diffs are propagated, its twin become stale. <p> Lazy release consistency reduces the number of messages required to maintain consistency, but the implementation is more expensive in terms of computation and memory overhead [14]. Midway <ref> [2] </ref> proposes a DSM system with entry consistency, a memory consistency model weaker than release consistency. The goal of Midway is to minimize communication costs by aggressively exploiting the relationship between shared variables and the synchronization objects that protect them.
Reference: [3] <author> N. Boden et al. </author> <title> Myrinet A gigabit-per-second local-area network. </title> <journal> IEEE MICRO, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Although the results presented in Section 3 assume a DD communication layer with limited hardware support, we expect that the use of other modern lightweight protocols, such as Fast Messages [19], and commodity high-speed interconnects, such as Myrinet <ref> [3] </ref>, would produce similar results. The availability of a low latency, high bandwidth communication layer led us to focus on building simple, easy to implement, computationally cheap, and portable mechanisms wherever possible, even if they required additional messages.
Reference: [4] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related communication in distributed shared memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-243, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. Since this time, the driving goal of DSM research has been to improve performance by any means necessary <ref> [1, 2, 4, 7, 9, 11, 13, 14, 21] </ref>. The performance of DSM systems is dominated by two issues: (i) the communication overhead required to maintain consistency and (ii) the computation overhead required to service remote requests and compute the information needed to maintain consistency. <p> * exploiting object encapsulation to give programmers the ability to precisely specify the coher ence unit [1, 2, 7, 9], * allowing programmers to explicitly lock data to specific nodes and thereby control at a fine grain when data is communicated [9, 11], * supporting multiple programmer-selected coher ence protocols <ref> [4] </ref>, * supporting multiple concurrent writers to elimi nate false sharing [4, 14], and * using various forms of relaxed consistency models, thereby increasing the restrictions imposed on the programmer but increasing the flexibility of the underlying DSM system in when it must communicate [2, 4, 14]. <p> specify the coher ence unit [1, 2, 7, 9], * allowing programmers to explicitly lock data to specific nodes and thereby control at a fine grain when data is communicated [9, 11], * supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing <ref> [4, 14] </ref>, and * using various forms of relaxed consistency models, thereby increasing the restrictions imposed on the programmer but increasing the flexibility of the underlying DSM system in when it must communicate [2, 4, 14]. <p> supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing [4, 14], and * using various forms of relaxed consistency models, thereby increasing the restrictions imposed on the programmer but increasing the flexibility of the underlying DSM system in when it must communicate <ref> [2, 4, 14] </ref>. These innovations improved DSM performance to varying degrees, sometimes to the point where soft ware DSM systems can rival the performance of hard-ware DSM systems for moderately-grained applications [8]. However, these efforts to reduce communication overhead often increase the system complexity and computation overhead. <p> Unfortunately, these efforts turned out to increase system overhead [20] or add cumbersome programming requirements [2, 13]. Among the optimizations we applied to Quarks to reduce the impact of DSM computation overhead are the following: * We employed a release-consistent write-update protocol, similar to that employed by Munin <ref> [4] </ref>, which allowed diff creation to be performed in parallel with communication. * Local diffs are applied to the clean twins as they are computed rather than copying a new twin on the next write to a page. * Incoming messages may be handled asynchronously, when the local node is idle <p> Quarks supports two consistency protocols: a release-consistent write update protocol and a sequentially-consistent write invalidate protocol. For synchronization, Quarks supports both locks and barriers, where locks are distributed while barriers are centralized. Many of Quarks' core mechanisms are derived from its predecessor, Munin <ref> [4] </ref>. Conventional wisdom holds that minimizing communication overhead is the key factor required to make DSM systems efficient. This has led to a wide variety of optimizations that reduce the number of messages required to maintain consistency, as described in Section 1. <p> Update propagation overhead that might have been at least partially hidden with otherwise idle time near a synchronization point was no longer hidden. In general, any DSM operation that interferes with useful user computation directly impacts performance and should be avoided. Self-invalidation of Unused Pages In self-invalidation <ref> [4] </ref>, a process decides that certain shared pages are no longer of interest, and it unilaterally drops them from its working set. The advantage in doing this is that updates for those pages will no longer need to be applied by the process, saving it time. <p> There is also a significant cost to any false-positives identified and then self-invalidated. The subsequent access, which defines the case as a false positive, causes a page reload since the process no longer has a valid copy of the data. We used self-invalidation to support a competitive update scheme <ref> [4, 10] </ref>. However, we were unable to achieve any performance improvement using this method over experiments using a variety of thresh-hold values. 3 Experimental Evaluation In this section, we evaluate the performance of software DSM system we have developed. First, the experimental setup is described.
Reference: [5] <author> J. Carter, A. Ranganathan, and S. Susarla. </author> <title> Building clustered services and applications using a global memory system. </title> <booktitle> In Submitted to The Eighteenth International Conference on Distributed Computing Systems, </booktitle> <year> 1998. </year>
Reference-contexts: How can DSM technologies be employed to improve the performance, portability, and availability of distributed applications and services, such as distributed (clustered) file systems, dis tributed databases, and directory services <ref> [5] </ref>? 3. To what extent can highly tailored software DSM systems be made to perform comparably to dedicated hardware DSM machines, given modest amounts of hardware support? As part of the Avalanche multiprocessor project, we support a mixed message passing and DSM model as part of the base architecture.
Reference: [6] <author> S. Chandra, J. Larus, and A. Rogers. </author> <booktitle> Where is time spent in message-passing and shared-memory programs? In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 61-73, </pages> <month> Oct. </month> <year> 1994. </year>
Reference: [7] <author> J. Chase, F. Amador, E. Lazowska, H. Levy, and R. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. Since this time, the driving goal of DSM research has been to improve performance by any means necessary <ref> [1, 2, 4, 7, 9, 11, 13, 14, 21] </ref>. The performance of DSM systems is dominated by two issues: (i) the communication overhead required to maintain consistency and (ii) the computation overhead required to service remote requests and compute the information needed to maintain consistency. <p> Over the years, there has been extensive research aimed at dramatically reducing the communication overhead required to keep DSM-managed data consistent. These efforts have included: * exploiting object encapsulation to give programmers the ability to precisely specify the coher ence unit <ref> [1, 2, 7, 9] </ref>, * allowing programmers to explicitly lock data to specific nodes and thereby control at a fine grain when data is communicated [9, 11], * supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing [4, 14], and * using various <p> However, these efforts to reduce communication overhead often increase the system complexity and computation overhead. Modern DSM systems have added other overheads as well. Many modern systems require programmers to write their programs in very constrained ways <ref> [1, 2, 7, 13] </ref>, in effect making the DSM programmer perform much of the work required of message passing programmers in terms of specifying what data should be shipped when. Other DSM systems require complex runtime systems or large amounts of DRAM for storing DSM state [14, 20].
Reference: [8] <author> A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Raja-mony, and W. Zwaenepoel. </author> <title> Software versus hardware shared-memory implementation: A case study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: These innovations improved DSM performance to varying degrees, sometimes to the point where soft ware DSM systems can rival the performance of hard-ware DSM systems for moderately-grained applications <ref> [8] </ref>. However, these efforts to reduce communication overhead often increase the system complexity and computation overhead. Modern DSM systems have added other overheads as well. <p> It uses a single-writer, write-invalidate protocol for all data, with virtual memory pages as the units of consistency. The large size of the consistency unit and the single-writer protocol makes Ivy prone to large amounts of communication due to false sharing. Lazy release consistency [14], as used in Tread-Marks <ref> [8] </ref>, is an algorithm for implementing release consistency different from the one presented in this paper. Instead of updating every cached copy of a data item whenever the modifying thread performs a release operation, only the cached copies on the processor that next acquires the released lock are updated.
Reference: [9] <author> P. Dasgupta, et al., </author> <title> The design and implementation of the Clouds distributed operating system. </title> <journal> Computing Systems Journal, </journal> <volume> 3, </volume> <month> Winter </month> <year> 1990. </year>
Reference-contexts: Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. Since this time, the driving goal of DSM research has been to improve performance by any means necessary <ref> [1, 2, 4, 7, 9, 11, 13, 14, 21] </ref>. The performance of DSM systems is dominated by two issues: (i) the communication overhead required to maintain consistency and (ii) the computation overhead required to service remote requests and compute the information needed to maintain consistency. <p> Over the years, there has been extensive research aimed at dramatically reducing the communication overhead required to keep DSM-managed data consistent. These efforts have included: * exploiting object encapsulation to give programmers the ability to precisely specify the coher ence unit <ref> [1, 2, 7, 9] </ref>, * allowing programmers to explicitly lock data to specific nodes and thereby control at a fine grain when data is communicated [9, 11], * supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing [4, 14], and * using various <p> These efforts have included: * exploiting object encapsulation to give programmers the ability to precisely specify the coher ence unit [1, 2, 7, 9], * allowing programmers to explicitly lock data to specific nodes and thereby control at a fine grain when data is communicated <ref> [9, 11] </ref>, * supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing [4, 14], and * using various forms of relaxed consistency models, thereby increasing the restrictions imposed on the programmer but increasing the flexibility of the underlying DSM system in when it
Reference: [10] <author> M. Dubois, L. Barroso, J. Wang, and Y. Chen. </author> <title> Delayed consistency and its effects on the miss rate of parallel programs. </title> <booktitle> In Proceedings of Supercomputing'91, </booktitle> <pages> pages 197-206, </pages> <year> 1991. </year>
Reference-contexts: There is also a significant cost to any false-positives identified and then self-invalidated. The subsequent access, which defines the case as a false positive, causes a page reload since the process no longer has a valid copy of the data. We used self-invalidation to support a competitive update scheme <ref> [4, 10] </ref>. However, we were unable to achieve any performance improvement using this method over experiments using a variety of thresh-hold values. 3 Experimental Evaluation In this section, we evaluate the performance of software DSM system we have developed. First, the experimental setup is described.
Reference: [11] <author> B. Fleisch and G. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 211-223, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. Since this time, the driving goal of DSM research has been to improve performance by any means necessary <ref> [1, 2, 4, 7, 9, 11, 13, 14, 21] </ref>. The performance of DSM systems is dominated by two issues: (i) the communication overhead required to maintain consistency and (ii) the computation overhead required to service remote requests and compute the information needed to maintain consistency. <p> These efforts have included: * exploiting object encapsulation to give programmers the ability to precisely specify the coher ence unit [1, 2, 7, 9], * allowing programmers to explicitly lock data to specific nodes and thereby control at a fine grain when data is communicated <ref> [9, 11] </ref>, * supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing [4, 14], and * using various forms of relaxed consistency models, thereby increasing the restrictions imposed on the programmer but increasing the flexibility of the underlying DSM system in when it
Reference: [12] <author> K. Gharachorloo, D. Lenoski, et al. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Release Consistent Write Update Quarks' write update protocol was derived from Munin, and supports multiple concurrent writers to different portions of a shared region. It exploits the release consistency memory model <ref> [12] </ref>, which (informally) allows processors to independently modify regions of shared data between synchronization points.
Reference: [13] <author> K. Johnson, M. Kaashoek, and D. Wallach. </author> <title> CRL: High performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <year> 1995. </year>
Reference-contexts: Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. Since this time, the driving goal of DSM research has been to improve performance by any means necessary <ref> [1, 2, 4, 7, 9, 11, 13, 14, 21] </ref>. The performance of DSM systems is dominated by two issues: (i) the communication overhead required to maintain consistency and (ii) the computation overhead required to service remote requests and compute the information needed to maintain consistency. <p> However, these efforts to reduce communication overhead often increase the system complexity and computation overhead. Modern DSM systems have added other overheads as well. Many modern systems require programmers to write their programs in very constrained ways <ref> [1, 2, 7, 13] </ref>, in effect making the DSM programmer perform much of the work required of message passing programmers in terms of specifying what data should be shipped when. Other DSM systems require complex runtime systems or large amounts of DRAM for storing DSM state [14, 20]. <p> Many DSM systems have attacked individual components of this overhead. For example, CRL <ref> [13] </ref>, Midway [2], and Shasta [20] eliminate memory protection and page fault handling overheads by detecting memory references via annotated load and store operations. Unfortunately, these efforts turned out to increase system overhead [20] or add cumbersome programming requirements [2, 13]. <p> For example, CRL [13], Midway [2], and Shasta [20] eliminate memory protection and page fault handling overheads by detecting memory references via annotated load and store operations. Unfortunately, these efforts turned out to increase system overhead [20] or add cumbersome programming requirements <ref> [2, 13] </ref>.
Reference: [14] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <year> 1992. </year>
Reference-contexts: Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. Since this time, the driving goal of DSM research has been to improve performance by any means necessary <ref> [1, 2, 4, 7, 9, 11, 13, 14, 21] </ref>. The performance of DSM systems is dominated by two issues: (i) the communication overhead required to maintain consistency and (ii) the computation overhead required to service remote requests and compute the information needed to maintain consistency. <p> specify the coher ence unit [1, 2, 7, 9], * allowing programmers to explicitly lock data to specific nodes and thereby control at a fine grain when data is communicated [9, 11], * supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing <ref> [4, 14] </ref>, and * using various forms of relaxed consistency models, thereby increasing the restrictions imposed on the programmer but increasing the flexibility of the underlying DSM system in when it must communicate [2, 4, 14]. <p> supporting multiple programmer-selected coher ence protocols [4], * supporting multiple concurrent writers to elimi nate false sharing [4, 14], and * using various forms of relaxed consistency models, thereby increasing the restrictions imposed on the programmer but increasing the flexibility of the underlying DSM system in when it must communicate <ref> [2, 4, 14] </ref>. These innovations improved DSM performance to varying degrees, sometimes to the point where soft ware DSM systems can rival the performance of hard-ware DSM systems for moderately-grained applications [8]. However, these efforts to reduce communication overhead often increase the system complexity and computation overhead. <p> Other DSM systems require complex runtime systems or large amounts of DRAM for storing DSM state <ref> [14, 20] </ref>. To attack communication overhead, we implemented a very low overhead communication subsystem using a Direct Deposit Protocol [23] developed as part of the Avalanche multiprocessor project. <p> To avoid such complexities as vector timestamps, indefinite buffering of changes, and distributed garbage collection <ref> [14] </ref>, Quarks uses an eager implementation of release consistency, where changes to shared data are flushed to remote nodes immediately when a release operation is performed. <p> It uses a single-writer, write-invalidate protocol for all data, with virtual memory pages as the units of consistency. The large size of the consistency unit and the single-writer protocol makes Ivy prone to large amounts of communication due to false sharing. Lazy release consistency <ref> [14] </ref>, as used in Tread-Marks [8], is an algorithm for implementing release consistency different from the one presented in this paper. <p> Lazy release consistency reduces the number of messages required to maintain consistency, but the implementation is more expensive in terms of computation and memory overhead <ref> [14] </ref>. Midway [2] proposes a DSM system with entry consistency, a memory consistency model weaker than release consistency. The goal of Midway is to minimize communication costs by aggressively exploiting the relationship between shared variables and the synchronization objects that protect them.
Reference: [15] <author> D. Khandekar. </author> <title> Quarks: Distributed shared memory as a basic building block for complex parallel and distributed systems. </title> <type> Technical Report Master Thesis, </type> <institution> University of Utah, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: We did so by eliminating many high overhead system calls and overlapping computation and communication whenever possible. The result is the Quarks DSM system <ref> [15] </ref>. <p> We described the Quarks DSM system <ref> [15] </ref>, which lacks many of the complex communication-reducing features of other modern DSM systems (e.g., lazy release consistency-based coherence protocols, cumbersome user-directed coherence, strict object semantics, etc.). Despite, or perhaps because of, its simplicity, Quarks performs comparably to these more sophisticated systems.
Reference: [16] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> Sept. </month> <year> 1979. </year>
Reference-contexts: The first DSM system, Ivy [18], was fairly simple: data was transparently managed in page-grained chunks, normal data and synchronization were treated identically, and all data was kept sequentially consistent <ref> [16] </ref>. Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. Since this time, the driving goal of DSM research has been to improve performance by any means necessary [1, 2, 4, 7, 9, 11, 13, 14, 21].
Reference: [17] <author> J. Laudon and D. Lenoski. </author> <title> The SGI Origin: A cc-NUMA highly scalable server. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 241-251, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: This has led to a wide variety of optimizations that reduce the number of messages required to maintain consistency, as described in Section 1. Recently, DSM systems have emerged that exploit highly specialized communication hardware to attack the problem of communication overhead head-on <ref> [17, 21] </ref>. In these systems, low message latency and high bandwidth communication translates directly into improved performance. Along these lines, when designing Quarks we exploited a very low latency messaging layer called Direct Deposit (DD) that was developed as part of the Avalanche multiprocessor project.
Reference: [18] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Distributed shared memory (DSM) is a software abstraction of shared memory on a distributed memory multiprocessor or workstation cluster. The first DSM system, Ivy <ref> [18] </ref>, was fairly simple: data was transparently managed in page-grained chunks, normal data and synchronization were treated identically, and all data was kept sequentially consistent [16]. Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. <p> The implementation of Quarks' two consistency protocols are detailed in this section. Sequentially Consistent Write Invalidate Quarks' write invalidate protocol is the same basic single-writer protocol that originated in Ivy <ref> [18] </ref>. Any number of nodes can have read-only copies of a given page. When any node attempts to modify such a shared region, the attempted store instruction will trigger a page fault, which is caught by the Quarks runtime system. <p> We limit our discussion to those systems that are most related to the work presented in this paper. Ivy was the first software DSM system <ref> [18] </ref>. It uses a single-writer, write-invalidate protocol for all data, with virtual memory pages as the units of consistency. The large size of the consistency unit and the single-writer protocol makes Ivy prone to large amounts of communication due to false sharing.
Reference: [19] <author> S. Paikin, Lauria, and A. Chien. </author> <title> High performance messaging on workstations: Illinois fast messages (FM) for Myrinet. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <year> 1995. </year>
Reference-contexts: The details of our implementation of DD can be found in Section 2.1. Although the results presented in Section 3 assume a DD communication layer with limited hardware support, we expect that the use of other modern lightweight protocols, such as Fast Messages <ref> [19] </ref>, and commodity high-speed interconnects, such as Myrinet [3], would produce similar results. The availability of a low latency, high bandwidth communication layer led us to focus on building simple, easy to implement, computationally cheap, and portable mechanisms wherever possible, even if they required additional messages.
Reference: [20] <author> D. Scales and K. Gharachorloo. </author> <title> Towards transparent and efficient distributed shared memory. </title> <booktitle> In Proceedings of the 16th ACM Symposium on Operating Systems Principles, </booktitle> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: Other DSM systems require complex runtime systems or large amounts of DRAM for storing DSM state <ref> [14, 20] </ref>. To attack communication overhead, we implemented a very low overhead communication subsystem using a Direct Deposit Protocol [23] developed as part of the Avalanche multiprocessor project. <p> Many DSM systems have attacked individual components of this overhead. For example, CRL [13], Midway [2], and Shasta <ref> [20] </ref> eliminate memory protection and page fault handling overheads by detecting memory references via annotated load and store operations. Unfortunately, these efforts turned out to increase system overhead [20] or add cumbersome programming requirements [2, 13]. <p> Many DSM systems have attacked individual components of this overhead. For example, CRL [13], Midway [2], and Shasta <ref> [20] </ref> eliminate memory protection and page fault handling overheads by detecting memory references via annotated load and store operations. Unfortunately, these efforts turned out to increase system overhead [20] or add cumbersome programming requirements [2, 13]. <p> For some programs, mak ing this association is easy. However, for programs that use nested data structures or arrays, it is not clear if making a one-to-one association is feasible without forcing programmers to completely rewrite their programs. Shasta <ref> [20] </ref> uses binary rewriting techniques to support the transparent interposition of software DSM on unmodified binaries. Load and store instructions are instrumented to test whether the shared data being accessed is locally present, and, for store instructions, writable.
Reference: [21] <author> R. Stets, S. Dwarkadas, N. Hardavellas, G. Hunt, L. Kontothanassis, S. Parthasarathy, and M. Scott. Cashmere-2L: </author> <title> Software coherent shared memory on a clustered remote write network. </title> <booktitle> In Proceedings of the 16th ACM Symposium on Operating Systems Principles, </booktitle> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: Although Ivy performed well on some applications, it performed poorly for programs with significant degrees of sharing, which limited its applicability. Since this time, the driving goal of DSM research has been to improve performance by any means necessary <ref> [1, 2, 4, 7, 9, 11, 13, 14, 21] </ref>. The performance of DSM systems is dominated by two issues: (i) the communication overhead required to maintain consistency and (ii) the computation overhead required to service remote requests and compute the information needed to maintain consistency. <p> This has led to a wide variety of optimizations that reduce the number of messages required to maintain consistency, as described in Section 1. Recently, DSM systems have emerged that exploit highly specialized communication hardware to attack the problem of communication overhead head-on <ref> [17, 21] </ref>. In these systems, low message latency and high bandwidth communication translates directly into improved performance. Along these lines, when designing Quarks we exploited a very low latency messaging layer called Direct Deposit (DD) that was developed as part of the Avalanche multiprocessor project. <p> Shasta was able to support a version of Oracle Parallel Server on a workstation cluster, but its performance is currently poor due to the high overhead associated with checking every load and store to potentially shared data. Cashmere <ref> [21] </ref> exploits DEC's Memory Channel hardware, which supports very high performance writes to remote memory, to reduce communication overhead, avoid the need for global directory locking, and avoid expensive TLB shootdowns.
Reference: [22] <author> L. Stoller, R. Kuramkote, and M. Swanson. </author> <title> PAINT-PA instruction set interpreter. </title> <type> Technical Report UUCS-96-009, </type> <institution> University of Utah Computer Science Department, </institution> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: The simulation results reported here were obtained from execution-driven simulations using the Paint simulator <ref> [22] </ref>. Paint simulates the HP PA RISC 1.1 architecture and includes an instruction set interpreter and detailed cycle-level models of a first level cache, system bus, and memory system similar to those found in HP J-class systems.
Reference: [23] <author> L. Stoller and M. Swanson. </author> <title> Direct deposit: A basic user-level protocol for carpet clusters. </title> <type> Technical Report UUCS-95-003, </type> <institution> University of Utah Computer Science Department, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Other DSM systems require complex runtime systems or large amounts of DRAM for storing DSM state [14, 20]. To attack communication overhead, we implemented a very low overhead communication subsystem using a Direct Deposit Protocol <ref> [23] </ref> developed as part of the Avalanche multiprocessor project. In this paper we explore the extent to which DSM performance can be improved by attacking the other source of DSM overhead: the computation overhead. <p> In Section 2.2 we discuss Quarks' design, followed by a description of computation-reducing optimizations that explored, both successful (Section 2.3) and unsucessful (Section 2.4). 2.1 Direct Deposit Based Communi cation The Quarks implementation described herein uses the Direct Deposit Protocol <ref> [23] </ref> as its transport layer. DD is an instance of a sender-based protocol [24] (SBP). The key concept of SBPs is a connection based mechanism that enables the sender to manage a reserved receive buffer within the receiving process' address space that is obtained when the connection is established.
Reference: [24] <author> J. Wilkes. </author> <title> Hamlyn an interface for sender-based communication. </title> <type> Technical Report HPL-OSR-92-13, </type> <institution> Hewlett-Packard Research Laboratory, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: DD is an instance of a sender-based protocol <ref> [24] </ref> (SBP). The key concept of SBPs is a connection based mechanism that enables the sender to manage a reserved receive buffer within the receiving process' address space that is obtained when the connection is established.
Reference: [25] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Second, read only pages suffer a final setting of the page protection to read-only, which is not required for a writable page and which cannot be hidden. 3.2.2 Whole Program Results In this section we report results using six programs. Three (FFT, LU,and water) are from the Splash2 suite <ref> [25] </ref>. SOR and gauss are from a study by Chan-dra, et al.[6]. Radix is from the Splash1 suite. Figure 6 gives the problem size for each application, and the uniprocessor version runtime. scale of the x axis (node count) is not linear.
References-found: 25

