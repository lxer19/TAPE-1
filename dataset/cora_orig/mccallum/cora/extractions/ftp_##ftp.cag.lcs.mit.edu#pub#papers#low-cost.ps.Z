URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/low-cost.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/low-cost.html
Root-URL: 
Title: Low-Cost Support for Fine-Grain Synchronization in Multiprocessors  
Author: David Kranz, Beng-Hong Lim, Donald Yeung and Anant Agarwal 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: As multiprocessors scale beyond the limits of a few tens of processors, they must look beyond traditional methods of synchronization to minimize serialization and achieve the high degrees of parallelism required to utilize large machines. By allowing synchronization at the level of the smallest unit of memory, fine-grain synchronization achieves these goals. Unfortunately, supporting efficient fine-grain synchronization without inordinate amounts of hardware has remained a challenge. This paper describes the support for fine-grain synchronization provided by the Alewife system. The premise underlying Alewife's implementation is that successful synchronization attempts are the common case when serialization is minimized through word-level synchronization. For our applications, the failure rates were less than 7%. Efficiency at low hardware cost is achieved by providing hardware support to streamline successful synchronization attempts and relegating other non-critical operations to software. Alewife provides a large synchronization name space by associating full/empty bits with each memory word. Suc- cessful synchronization attempts execute at normal load-store speeds, while attempts that fail invoke appropriate software trap handlers through a fast trap mechanism. The software handlers deal with the issues of retrying versus blocking, queueing, and rescheduling. The efficiency of Alewife's mechanisms is analyzed by comparing the costs of various synchronization operations and parallel application execution time. In several applications we studied, our hardware support improved performance by 35%-50%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Beng-Hong Lim, David A. Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: We provide some evidence that this observation is indeed true for real applications executing on Alewife-for the applications we studied, the failure rates were less than 7%. * The required hardware support is simple enough that our processor, Sparcle <ref> [1] </ref>, was implemented by making minor modifications to LSI Logic's existing SPARC processor [19], without affecting the speed of the processor.
Reference: [2] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: The programming language must allow parallelism and synchronization at the level of a data word to be expressed easily, and fine-grain synchronization must be implemented efficiently at the system level, in both hardware and software. This paper focuses on the support for fine-grain synchroninzation in the Alwife machine <ref> [2] </ref>, a shared-address space, distributed-memory multiprocessor being developed at MIT. At the 2 system level, Alewife provides hardware-support for fine-grain synchronization in the form of full/empty bits (as in the HEP [18]) and efficient traps.
Reference: [3] <author> Gail Alverson, Robert Alverson, and David Callahan. </author> <title> Exploiting Heterogeneous Parallelism on a Multithreaded Multiprocessor. </title> <booktitle> In Workshop on Multithreaded Computers, Proceedings of Supercomputing '91. ACM Sigraph & IEEE, </booktitle> <month> November </month> <year> 1991. </year> <month> 19 </month>
Reference-contexts: Other architectures have solved this problem by having multiple state bits per memory location <ref> [3, 16] </ref>. Instead of providing additional hardware support, we take a different approach. Like SPARC, Sparcle supports an atomic register-memory swap operation. <p> will become increasingly pronounced for large machine sizes where false dependencies are particularly detrimental. 6 Related Work The advantages of fine-grain synchronization have been noted before, and several machines that address this issue with varying degrees of synchronization granularity have been built or proposed, including HEP [18], Monsoon [16], Tera <ref> [3] </ref>, MDP [6], Cedar [7], Multicube [8], and the KSR1 [17]. In fact, our language notation for fine-grain synchronization and hardware 17 "C" = Coarse-grain barriers, "S" = Software J-structures, and "H" = Hardware J-structures. full/empty bits were inspired by the HEP.
Reference: [4] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <booktitle> In Proceedings of the Workshop on Graph Reduction, (Springer-Verlag Lecture Notes in Computer Science 279), </booktitle> <month> September/October </month> <year> 1986. </year>
Reference-contexts: Fine-grain data-level synchronization is expressed using data structures with accessors that implicitly synchronize. We call these structures J-structure and L-structure arrays. A J-structure is a data structure for producer-consumer style synchronization inspired by I- structures <ref> [4] </ref>. A J-structure is like an array, but each element has additional state: full or empty. The initial state of a J-structure element is empty. A reader of an element waits until the element's state is full before returning the value.
Reference: [5] <author> Paul S. Barth, Rishiyur S. Nikhil, and Arvind. M-Structures: </author> <title> Extending a Parallel, Non-strict, Functional Language with State. </title> <booktitle> In Proceedings of the 5th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: As for J-structures, an error is signalled if the location is already full. An L-structure therefore allows mutually exclusive access to each of its elements. The synchronizing L-structure reads and writes can be used to implement M-structures <ref> [5] </ref>. However, L-structures are different from M-structures in that they allow multiple non-locking readers, and a store to a full element signals an error 1 . 3.2 Control Parallelism Using control parallelism, a programmer specifies that a given expression X may be executed in parallel with the current thread.
Reference: [6] <author> W. J. Dally et al. </author> <title> Architecture of a Message-Driven Processor. </title> <booktitle> In Proceedings of the 14th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 189-196, </pages> <address> Washington, D.C., </address> <month> June </month> <year> 1987. </year> <note> IEEE. </note>
Reference-contexts: increasingly pronounced for large machine sizes where false dependencies are particularly detrimental. 6 Related Work The advantages of fine-grain synchronization have been noted before, and several machines that address this issue with varying degrees of synchronization granularity have been built or proposed, including HEP [18], Monsoon [16], Tera [3], MDP <ref> [6] </ref>, Cedar [7], Multicube [8], and the KSR1 [17]. In fact, our language notation for fine-grain synchronization and hardware 17 "C" = Coarse-grain barriers, "S" = Software J-structures, and "H" = Hardware J-structures. full/empty bits were inspired by the HEP.
Reference: [7] <author> Daniel Gajski, David Kuck, Duncan Lawrie, and Ahmed Saleh. </author> <title> Cedar A Large Scale Multiprocessor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 524-529, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: for large machine sizes where false dependencies are particularly detrimental. 6 Related Work The advantages of fine-grain synchronization have been noted before, and several machines that address this issue with varying degrees of synchronization granularity have been built or proposed, including HEP [18], Monsoon [16], Tera [3], MDP [6], Cedar <ref> [7] </ref>, Multicube [8], and the KSR1 [17]. In fact, our language notation for fine-grain synchronization and hardware 17 "C" = Coarse-grain barriers, "S" = Software J-structures, and "H" = Hardware J-structures. full/empty bits were inspired by the HEP.
Reference: [8] <author> James R. Goodman and Philip J. Woest. </author> <title> The Wisconsin Multicube: A New Large Scale CacheCoherent Multiprocessor. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 422-431, </pages> <address> Hawaii, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: machine sizes where false dependencies are particularly detrimental. 6 Related Work The advantages of fine-grain synchronization have been noted before, and several machines that address this issue with varying degrees of synchronization granularity have been built or proposed, including HEP [18], Monsoon [16], Tera [3], MDP [6], Cedar [7], Multicube <ref> [8] </ref>, and the KSR1 [17]. In fact, our language notation for fine-grain synchronization and hardware 17 "C" = Coarse-grain barriers, "S" = Software J-structures, and "H" = Hardware J-structures. full/empty bits were inspired by the HEP.
Reference: [9] <author> Douglas Johnson. </author> <title> Trap Architectures for Lisp Systems. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: When these new instructions trap, they vector to a handler specific to the register containing the placeholder. 2 The trap vector dispatch for misaligned address traps was modified in the same way. 2 motivated by <ref> [9] </ref> 7 4.2 Implementation of J- and L-Structures We now describe in more detail the implementation of J- and L-structures, and present machine code for synchronously reading and writing these structures. These synchronization structures provide for data-dependency and mutual-exclusion, and are primitives upon which other synchronization operations can be built.
Reference: [10] <author> Kirk Johnson. </author> <title> Semi-C Reference Manual. ALEWIFE Memo No. </title> <type> 20, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: The Alewife system currently supports two programming languages: Mul-T [13], a parallel Lisp language, and Semi-C. Semi-C <ref> [10] </ref> is a parallel C-like language with extensions for expressing parallel execution.
Reference: [11] <author> A. Karlin, K. Li, M. Manasse, and S. Owicki. </author> <title> Empirical Studies of Competitive Spinning for A Shared-Memory Multiprocessor. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: We do not always need to block a waiting thread. On a failed synchronization, the trap handler is responsible for implementing the waiting algorithm that decides whether to poll or to block the thread. Karlin et al. <ref> [11] </ref> and Lim and Agarwal [14] investigate the performance of various waiting algorithms. They show polling for some length of time before blocking can lead to better performance, and investigate various methods for determining how long to poll before blocking.
Reference: [12] <author> David Kranz, Beng-Hong Lim, Kirk Johnson, John Kubiatowicz, and Anant Agarwal. </author> <title> Integrating Message-Passing and Shared-Memory; Early Experience (Extended Abstract). </title> <booktitle> In Proceedings of the Second Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: This leads to load-balanced threads. In the coarse-grain implementation, a barrier is placed between each Jacobi iteration. The barrier implementation is based on combining trees and is highly optimized for the Alewife machine. A barrier incurs a small latency 3 of 20 sec on 64 processors <ref> [12] </ref>. By comparison, typical software implementations (e.g. Intel DELTA and iPSC/860, Kendall Square KSR1) take well over 400 sec. In the fine-grain implementation, borders of each subgrid are implemented as J-structures. Thus fine-grain synchronization occurs between nearest neighbors through the J-structures.
Reference: [13] <author> David A. Kranz, R. Halstead, and E. Mohr. Mul-T: </author> <title> A High-Performance Parallel Lisp. </title> <booktitle> In Proceedings of SIGPLAN '89, Symposium on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Using future provides a form of fine-grain synchronization because synchronization can occur between the producer and consumers of an arbitrary expression, e.g., a procedure call can start executing while some of its arguments are still being computed. The Alewife system currently supports two programming languages: Mul-T <ref> [13] </ref>, a parallel Lisp language, and Semi-C. Semi-C [10] is a parallel C-like language with extensions for expressing parallel execution.
Reference: [14] <author> Beng-Hong Lim and Anant Agarwal. </author> <title> Waiting Algorithms for Synchronization in Large-Scale Multiprocessors. </title> <note> To appear in ACM Transactions on Computer Systems. Also available as MIT VLSI Memo 91-632, </note> <month> February </month> <year> 1991, 1991. </year>
Reference-contexts: This information can significantly reduce the blocking overhead by reducing the number of registers that need to be saved. <ref> [14] </ref> describes how the cost of blocking can be reduced to less than 100 cycles on a processor with single-cycle loads and stores and with information on live registers. <p> We do not always need to block a waiting thread. On a failed synchronization, the trap handler is responsible for implementing the waiting algorithm that decides whether to poll or to block the thread. Karlin et al. [11] and Lim and Agarwal <ref> [14] </ref> investigate the performance of various waiting algorithms. They show polling for some length of time before blocking can lead to better performance, and investigate various methods for determining how long to poll before blocking.
Reference: [15] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The system must, however, ensure that the current thread and X can be executed concurrently if necessary (e.g., to avoid deadlock). We call this behavior, where a new thread is created at runtime only for deadlock avoidance or load-balancing purposes, lazy task creation <ref> [15] </ref>. Using future provides a form of fine-grain synchronization because synchronization can occur between the producer and consumers of an arbitrary expression, e.g., a procedure call can start executing while some of its arguments are still being computed.
Reference: [16] <author> G. M. Papadopoulos and D.E. Culler. Monsoon: </author> <title> An Explicit Token-Store Architecture. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1990. </year> <note> IEEE. </note>
Reference-contexts: Handling failure completely in software is our biggest saving in complexity because blocking a thread is a complex operation. In contrast, machines that support blocking in hardware pay an enormous hardware cost. For example, Monsoon <ref> [16] </ref> implements special I-structure boards to queue failed synchronizations in hardware. Even though we expect successful synchronizations to be the common case, we would like to handle failed synchronizations as efficiently as possible. <p> Other architectures have solved this problem by having multiple state bits per memory location <ref> [3, 16] </ref>. Instead of providing additional hardware support, we take a different approach. Like SPARC, Sparcle supports an atomic register-memory swap operation. <p> false dependencies will become increasingly pronounced for large machine sizes where false dependencies are particularly detrimental. 6 Related Work The advantages of fine-grain synchronization have been noted before, and several machines that address this issue with varying degrees of synchronization granularity have been built or proposed, including HEP [18], Monsoon <ref> [16] </ref>, Tera [3], MDP [6], Cedar [7], Multicube [8], and the KSR1 [17]. In fact, our language notation for fine-grain synchronization and hardware 17 "C" = Coarse-grain barriers, "S" = Software J-structures, and "H" = Hardware J-structures. full/empty bits were inspired by the HEP.
Reference: [17] <author> James B. Rothnie. </author> <title> Architecture of the KSR1 Computer System, </title> <address> March 1992. </address> <publisher> MIT LCS Seminar, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: dependencies are particularly detrimental. 6 Related Work The advantages of fine-grain synchronization have been noted before, and several machines that address this issue with varying degrees of synchronization granularity have been built or proposed, including HEP [18], Monsoon [16], Tera [3], MDP [6], Cedar [7], Multicube [8], and the KSR1 <ref> [17] </ref>. In fact, our language notation for fine-grain synchronization and hardware 17 "C" = Coarse-grain barriers, "S" = Software J-structures, and "H" = Hardware J-structures. full/empty bits were inspired by the HEP.
Reference: [18] <author> B.J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. </title> <booktitle> SPIE, </booktitle> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: This paper focuses on the support for fine-grain synchroninzation in the Alwife machine [2], a shared-address space, distributed-memory multiprocessor being developed at MIT. At the 2 system level, Alewife provides hardware-support for fine-grain synchronization in the form of full/empty bits (as in the HEP <ref> [18] </ref>) and efficient traps. The processor implements load and store instructions that trap on various states of the full/empty bit. Little or no overhead is incurred if the referenced data is available. If the data required by a read operation is not yet available, the processor traps. <p> of avoiding false dependencies will become increasingly pronounced for large machine sizes where false dependencies are particularly detrimental. 6 Related Work The advantages of fine-grain synchronization have been noted before, and several machines that address this issue with varying degrees of synchronization granularity have been built or proposed, including HEP <ref> [18] </ref>, Monsoon [16], Tera [3], MDP [6], Cedar [7], Multicube [8], and the KSR1 [17]. In fact, our language notation for fine-grain synchronization and hardware 17 "C" = Coarse-grain barriers, "S" = Software J-structures, and "H" = Hardware J-structures. full/empty bits were inspired by the HEP.
Reference: [19] <author> SPARC Architecture Manual, </author> <year> 1988. </year> <institution> SUN Microsystems, Mountain View, California. </institution>
Reference-contexts: some evidence that this observation is indeed true for real applications executing on Alewife-for the applications we studied, the failure rates were less than 7%. * The required hardware support is simple enough that our processor, Sparcle [1], was implemented by making minor modifications to LSI Logic's existing SPARC processor <ref> [19] </ref>, without affecting the speed of the processor. This processor has been running in our laboratory since March 1992. * We evaluate the efficacy of our implementation by showing the performance of several applications running on a detailed Alewife simulator. We compare coarse-grained (barrier) and fine-grained implementations of these applications.
Reference: [20] <author> Donald Yeung and Anant Agarwal. </author> <title> Experience with Fine-Grain Synchronization in MIMD Machines for Preconditioned Conjugate Gradient. </title> <institution> MIT Lab for Computer Science Tech. Memo MIT-LCS-TM479, MIT, </institution> <address> Cambridge, MA 02139, </address> <month> October </month> <year> 1992. </year> <month> 20 </month>
Reference-contexts: All elements in the global solution array are allocated as a three-dimensional J-structure. An implicit barrier, however, is still needed to implement two dot product operations that occur in each MICCG iteration. (For a detailed discussion of our study and implementation of MICCG, see <ref> [20] </ref>). 5.2 Measurements A performance comparison of the coarse-grain versus fine-grain implementations appears in speedup for 64 processors for MICCG was not attainable due to simulation limits on the problem size). Performance is better in the fine-grain implementation for both applications. <p> MICCG has the added problem that the data-dependencies in the solver operation are complex. Enforcing these dependencies with barriers results in a large number of barriers (and in fact, the number of barriers necessarily increases with machine size; see <ref> [20] </ref>). Because of the flexibility that fine-grain J-structures provides in expressing these data-dependencies, a significant reduction in synchronization overhead is attained.
References-found: 20

