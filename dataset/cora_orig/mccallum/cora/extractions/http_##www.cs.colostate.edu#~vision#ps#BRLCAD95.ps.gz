URL: http://www.cs.colostate.edu/~vision/ps/BRLCAD95.ps.gz
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Title: Reduction of BRL/CAD Models and Their Use in Automatic Target Recognition Algorithms  
Author: Mark R. Stevens and J. Ross Beveridge and Michael E. Goss 
Note: This work was sponsored by the Advanced Research Projects Agency (ARPA) under grant DAAH04-93-G-422, monitored by the U. S. Army Research Office.  
Address: Fort Collins, CO 80523-1873  
Affiliation: Computer Science Department Colorado State University  
Abstract: We are currently developing an Automatic Target Recognition (ATR) algorithm to locate an object using multi-sensor data. The ATR algorithm will determine corresponding points between a range (LADAR) image, a color (CCD) image, a thermal (FLIR) image and a BRL/CAD model of the object being located. The success of this process depends in part on which features can be automatically extracted from the model database. The BRL/CAD models we have for this process contain more detail than can be productively used by our ATR algorithm and must be reduced to a more appropriate form. This paper presents algorithms we are developing in order to reduce the BRL/CAD models a level of detail appropriate for the ATR algorithm. A secondary feature of these algorithms is to also maintain a parallel version with details sufficient to appear realistic when rendered. This rendering enables the ATR system to animate its search procedure for monitoring and debugging. Model reduction begins by converting the Constructive Solid Geometry BRL/CAD model to a set of polyhedra. All polyhedra that are completely contained within other polyhedra are automatically discarded. We will also allow the user to discard objects based on a specified minimum bounding box. From the reduced model, we can extract 3D point sampled surface information as well as 3D features generated by the model silhouette. These features are then used by the ATR algorithm. Algorithms for determining the visible silhouette surfaces and the features for an arbitrary viewpoint are developed in this paper. The ATR algorithm will use the three dimensional features to refine a pose estimate of the model relative to sensors. This pose in turn provides a basis for measuring the quality of the match between 3D model features and sensor features. 
Abstract-found: 1
Intro-found: 1
Reference: [ aJRB94 ] <author> Anthony N. A. Schwickerath and J. Ross Bev-eridge. </author> <title> Object to multisensor coregistration with eight degrees of freedom. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 481-490. ARPA, </pages> <month> nov </month> <year> 1994. </year>
Reference-contexts: The second stage extends LADAR probing techniques [ BJLP92 ] to generate target type and target pose hypotheses. Finally, given object type and pose hypotheses, an error reduction approach will generate a best-fit match between the sensor and model features <ref> [ aJRB94 ] </ref> . Using both multi-sensor data and detailed geometric constraints derived from the models, it is expected this final stage will be highly reliable and discriminating. A sample of the multi-sensor data is shown in Figures 2, 3, and 4. <p> It is key that these edges still be represented in 3D, as opposed to projected into 2D templates, because the 3D structure can constrain the object pose refinement portion of the recognition process <ref> [ KH94; aJRB94 ] </ref> . Due to the implicit nature of the BRL/CAD CSG representation, vertex, edge and face information needed in order to derive 3D silhouette information is not directly available. <p> Our ATR algorithms dynamically update the 3D position and orientation of the object model relative to the sensors so as to align the visible contours in FLIR and CCD with silhouette inducing edges in the 3D model <ref> [ KH94; aJRB94 ] </ref> . The ATR algorithm needs to be aware of which silhouette edges are visible for any given viewpoint. To generate a list of visible 3D edges, each face is projected onto a 2D image plane.
Reference: [ BDHR94 ] <author> Shashi Buluswar, Bruce A. Draper, Allen Hanson, and Edward Riseman. </author> <title> Non-parametric Classification of Pixels Under Varying Outdoor Illumination. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1619-1626, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The ATR algorithm being developed will use a three stage strategy [ BHP94 ] . First, a detection process suggests regions of interest within the image worth further consideration as possible targets. An innovation at this stage is the use of color as an additional detection queue <ref> [ BDHR94 ] </ref> . The second stage extends LADAR probing techniques [ BJLP92 ] to generate target type and target pose hypotheses. Finally, given object type and pose hypotheses, an error reduction approach will generate a best-fit match between the sensor and model features [ aJRB94 ] .
Reference: [ Bes88 ] <author> Paul J. Besl. </author> <title> Geometric modeling and computer vision. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76(8) </volume> <pages> 936-958, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Model-based object recognition techniques often identify modeled objects in a scene by relating stored geometric models to features extracted from sensor data <ref> [ Bes88 ] </ref> . A common goal is to place a 3D model in a scene at roughly the correct position, with similar scale, and orientation to the object in the scene.
Reference: [ BHP94 ] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Integrated color ccd, flir & ladar based object modeling and recognition. </title> <type> Technical report, </type> <institution> Colorado State University and Alliant Tech-systems and University of Massachusetts, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Data from three sensor modalities was collected: color (CCD) images, thermal (FLIR) imagery, and depth (LADAR) information. The ATR algorithm being developed will use a three stage strategy <ref> [ BHP94 ] </ref> . First, a detection process suggests regions of interest within the image worth further consideration as possible targets. An innovation at this stage is the use of color as an additional detection queue [ BDHR94 ] .
Reference: [ BJLP92 ] <author> James Bevington, Randy Johnston, Joel Lee, and Richard Peters. </author> <title> A Modular Target Recognition Algorithm for LADAR. </title> <booktitle> In Proc of the 2nd Automatic Target Recognizer Systems and Technology Conference, </booktitle> <pages> pages 91 - 104, </pages> <address> Fort Belvoir, VA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: First, a detection process suggests regions of interest within the image worth further consideration as possible targets. An innovation at this stage is the use of color as an additional detection queue [ BDHR94 ] . The second stage extends LADAR probing techniques <ref> [ BJLP92 ] </ref> to generate target type and target pose hypotheses. Finally, given object type and pose hypotheses, an error reduction approach will generate a best-fit match between the sensor and model features [ aJRB94 ] .
Reference: [ Bon86 ] <author> Luisa Bonfigliolo. </author> <title> An algorithm for silhouette of curved surfaces based on graphical relations. </title> <booktitle> Computer-Aided Design, </booktitle> <volume> 18(2) </volume> <pages> 95-101, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: The arcs in the graph represent movement from one set of visible features to another, and this characteristic has been exploited in the ATR process [ PD87 ] , [ Pla88 ] . Other techniques have used projection techniques similar to ours <ref> [ Bon86; GM88; SJ89 ] </ref> . A more relevant discussion of feature extraction appears in [ SD92 ] , and describes a simple test we implement for determining whether or not an edge represents a silhouette edge.
Reference: [ BPY94 ] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA data collection final report. </title> <type> Technical Report CS-94-118, </type> <institution> Computer Science Dept., Colorado State University, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: The data set used to test the algorithms was obtained in November of 1993 at Fort Carson, Colorado <ref> [ BPY94 ] </ref> . Data from three sensor modalities was collected: color (CCD) images, thermal (FLIR) imagery, and depth (LADAR) information. The ATR algorithm being developed will use a three stage strategy [ BHP94 ] .
Reference: [ GBSF94 ] <author> Michael E. Goss, J. Ross Beveridge, Mark R. Stevens, and Aaron Fuegi. </author> <title> Visualization and verification of automatic target recognition results using combined range and optical imagery. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 491-494. ARPA, </pages> <month> nov </month> <year> 1994. </year>
Reference-contexts: Effort expended extracting detail from object models not manifest in the sensor data is effort wasted. To help us better understand the character and quality of the multi-sensor data, we have developed a visualization and verification tool known as RangeView <ref> [ GBSF94 ] </ref> , [ GBSF95 ] . RangeView allows all three sensor images to be viewed simultaneously as well as the current position of the model in relation to the scene.
Reference: [ GBSF95 ] <author> Michael E. Goss, J. Ross Beveridge, Mark R. Stevens, and Aaron Fuegi. </author> <title> Three-dimensional visualization environment for multisensor data analysis, interpretation, and model-based object recognition. In Proceedings: Visual Data Exploration and Analysis. </title> <address> IS&T/SPIE, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Effort expended extracting detail from object models not manifest in the sensor data is effort wasted. To help us better understand the character and quality of the multi-sensor data, we have developed a visualization and verification tool known as RangeView [ GBSF94 ] , <ref> [ GBSF95 ] </ref> . RangeView allows all three sensor images to be viewed simultaneously as well as the current position of the model in relation to the scene. The user also has the ability to interactively modify any of the viewing parameters to allow a better interpretation of the scene.
Reference: [ GM88 ] <author> Ziv Gigus and Jitendra Malik. </author> <title> Computing the aspect graph for line drawings of polyhedral objects. </title> <booktitle> IEEE Proceedings on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 654-661, </pages> <year> 1988. </year>
Reference-contexts: The arcs in the graph represent movement from one set of visible features to another, and this characteristic has been exploited in the ATR process [ PD87 ] , [ Pla88 ] . Other techniques have used projection techniques similar to ours <ref> [ Bon86; GM88; SJ89 ] </ref> . A more relevant discussion of feature extraction appears in [ SD92 ] , and describes a simple test we implement for determining whether or not an edge represents a silhouette edge.
Reference: [ KD87 ] <author> Matthew R. Korn and Charles R. Dyer. </author> <title> 3d multi-view object representations for model-based object recognition. </title> <journal> Pattern Recognition, </journal> <volume> 20(1) </volume> <pages> 91-103, </pages> <year> 1987. </year>
Reference-contexts: It is our hope that the on-demand approach will prove reasonable using suitable hardware graphics accelerators. However, the same underlying algorithm can be used off-line to build up a stored aspect graph <ref> [ KD87 ] </ref> . The more important issue is the nature and form of the algorithm used to generate the visible 3D features for different viewpoints.
Reference: [ KH94 ] <author> Rakesh Kumar and Allen R. Hanson. </author> <title> Robust methods for estimating pose and a sensitivity analysis. </title> <booktitle> CVGIP:Image Understanding, </booktitle> <volume> 11, </volume> <year> 1994. </year>
Reference-contexts: It is key that these edges still be represented in 3D, as opposed to projected into 2D templates, because the 3D structure can constrain the object pose refinement portion of the recognition process <ref> [ KH94; aJRB94 ] </ref> . Due to the implicit nature of the BRL/CAD CSG representation, vertex, edge and face information needed in order to derive 3D silhouette information is not directly available. <p> Our ATR algorithms dynamically update the 3D position and orientation of the object model relative to the sensors so as to align the visible contours in FLIR and CCD with silhouette inducing edges in the 3D model <ref> [ KH94; aJRB94 ] </ref> . The ATR algorithm needs to be aware of which silhouette edges are visible for any given viewpoint. To generate a list of visible 3D edges, each face is projected onto a 2D image plane.
Reference: [ KvD76 ] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The singu-larities of visual mapping. </title> <journal> Biological Cybernetics, </journal> <volume> 24 </volume> <pages> 51-59, </pages> <year> 1976. </year>
Reference-contexts: Earlier feature extraction processes have focused on development of a model representation known as the aspect graph. Aspect graphs are founded on the notion that there are regions on a view sphere in which all viewpoints in that region share a constant model topology <ref> [ KvD76; KvD79 ] </ref> . The arcs in the graph represent movement from one set of visible features to another, and this characteristic has been exploited in the ATR process [ PD87 ] , [ Pla88 ] .
Reference: [ KvD79 ] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The internal representation of shape with respect to vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year>
Reference-contexts: Earlier feature extraction processes have focused on development of a model representation known as the aspect graph. Aspect graphs are founded on the notion that there are regions on a view sphere in which all viewpoints in that region share a constant model topology <ref> [ KvD76; KvD79 ] </ref> . The arcs in the graph represent movement from one set of visible features to another, and this characteristic has been exploited in the ATR process [ PD87 ] , [ Pla88 ] .
Reference: [ LTH86 ] <author> D.H Laidlaw, W.B. Trumbore, and J.F. Hughes. </author> <title> Constructive solid geometry for polyhedral objects. </title> <journal> Computer Graphics, </journal> <volume> 20(4) </volume> <pages> 161-170, </pages> <month> August </month> <year> 1986. </year> <note> Proceedings of SIGGRAPH 86. </note>
Reference-contexts: However, model point/line information is readily available in a polyhedral model, and thus the first step is to transform models into a polyhedral representation. Several different algorithms have been developed to convert a model from CSG into a set of polyhedra <ref> [ RV85; LTH86; PS86; NAT90 ] </ref> . These methods are all based on the principal that each object used in a boolean operation can be split into a set of planar faces, each of which lies completely inside or outside the other object. <p> A classi fication algorithm can then be used to determine which faces are retained. 3.1 The Conversion Algorithm We have chosen to utilize the algorithm presented in <ref> [ LTH86 ] </ref> for convert the CSG representation into polyhedra. The conversion algorithm is summarized below along with some minor refinements which we found improved the quality of the resulting model. For a complete description of the algorithm the reader is referred to [ LTH86 ] , or [ RV85 ] <p> chosen to utilize the algorithm presented in <ref> [ LTH86 ] </ref> for convert the CSG representation into polyhedra. The conversion algorithm is summarized below along with some minor refinements which we found improved the quality of the resulting model. For a complete description of the algorithm the reader is referred to [ LTH86 ] , or [ RV85 ] which provides a discussion of the mathematical basis for the algorithm. The conversion algorithm traverses the CSG tree transforming the leaf nodes from BRL/CAD primitives into polyhedra. <p> The process continues until the entire CSG tree has been traversed and the model has been converted to its new polyhedral representation. We have made several additions to the algorithm presented by Laidlaw <ref> [ LTH86 ] </ref> . Their splitting algorithm tends to fragment the faces of an object into many smaller faces when one of the objects is small compared to the other.
Reference: [ NAT90 ] <author> Bruce Naylor, John Amanatides, and William Thibault. </author> <title> Merging BSP trees yields polyhedral set operations. </title> <journal> Computer Graphics, </journal> <volume> 24(4) </volume> <pages> 115-124, </pages> <month> August </month> <year> 1990. </year> <note> Proceedings of SIGGRAPH 90. </note>
Reference-contexts: However, model point/line information is readily available in a polyhedral model, and thus the first step is to transform models into a polyhedral representation. Several different algorithms have been developed to convert a model from CSG into a set of polyhedra <ref> [ RV85; LTH86; PS86; NAT90 ] </ref> . These methods are all based on the principal that each object used in a boolean operation can be split into a set of planar faces, each of which lies completely inside or outside the other object.
Reference: [ PD87 ] <author> Harry Platinga and Charles Dyer. </author> <title> Visibility, occlusion, and the aspect graph. </title> <type> Technical Report 736, </type> <institution> University of Wisconsin - Madison, </institution> <month> Decem-ber </month> <year> 1987. </year>
Reference-contexts: The arcs in the graph represent movement from one set of visible features to another, and this characteristic has been exploited in the ATR process <ref> [ PD87 ] </ref> , [ Pla88 ] . Other techniques have used projection techniques similar to ours [ Bon86; GM88; SJ89 ] .
Reference: [ Pla88 ] <author> William Harry Plantinga. </author> <title> The ASP: A Continuous, Viewer-Centered Object Representation for Computer Vision. </title> <type> PhD thesis, </type> <institution> University of Wisconsin at Madison, </institution> <year> 1988. </year>
Reference-contexts: The arcs in the graph represent movement from one set of visible features to another, and this characteristic has been exploited in the ATR process [ PD87 ] , <ref> [ Pla88 ] </ref> . Other techniques have used projection techniques similar to ours [ Bon86; GM88; SJ89 ] . A more relevant discussion of feature extraction appears in [ SD92 ] , and describes a simple test we implement for determining whether or not an edge represents a silhouette edge.
Reference: [ PS86 ] <author> L. K. Putnam and P. A. Subrahmanyam. </author> <title> Boolean operations on n-dimensional objects. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 6(6) </volume> <pages> 43-51, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: However, model point/line information is readily available in a polyhedral model, and thus the first step is to transform models into a polyhedral representation. Several different algorithms have been developed to convert a model from CSG into a set of polyhedra <ref> [ RV85; LTH86; PS86; NAT90 ] </ref> . These methods are all based on the principal that each object used in a boolean operation can be split into a set of planar faces, each of which lies completely inside or outside the other object.
Reference: [ RV85 ] <author> A. A. G. Requicha and H. B. Voelcker. </author> <title> Boolean operations in solid modeling: Boundary evaluation and merging algorithms. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 73(1), </volume> <month> January </month> <year> 1985. </year>
Reference-contexts: However, model point/line information is readily available in a polyhedral model, and thus the first step is to transform models into a polyhedral representation. Several different algorithms have been developed to convert a model from CSG into a set of polyhedra <ref> [ RV85; LTH86; PS86; NAT90 ] </ref> . These methods are all based on the principal that each object used in a boolean operation can be split into a set of planar faces, each of which lies completely inside or outside the other object. <p> The conversion algorithm is summarized below along with some minor refinements which we found improved the quality of the resulting model. For a complete description of the algorithm the reader is referred to [ LTH86 ] , or <ref> [ RV85 ] </ref> which provides a discussion of the mathematical basis for the algorithm. The conversion algorithm traverses the CSG tree transforming the leaf nodes from BRL/CAD primitives into polyhedra.
Reference: [ SD92 ] <author> W. Brent Seales and Charles R. Dyer. </author> <title> Modeling the rim appearance. </title> <booktitle> In Proceedings of the 3rd International Conference on Computer Vision, </booktitle> <pages> pages 698-701, </pages> <year> 1992. </year>
Reference-contexts: Other techniques have used projection techniques similar to ours [ Bon86; GM88; SJ89 ] . A more relevant discussion of feature extraction appears in <ref> [ SD92 ] </ref> , and describes a simple test we implement for determining whether or not an edge represents a silhouette edge. The aspect graph is a stored representation in which visible features are precomputed for all relevant viewpoints. <p> Each edge associated with a visible face is also marked as visible. We use a simple heuristic to reduce the number of edges needing to be clipped due to occlusion. An edge cannot be on the silhouette if both faces which share the edge are visible <ref> [ SD92 ] </ref> . As an edge is marked as visible, a simple check is made to determine if the other face sharing the edge is visible. If the other face is visible, the edge can not lie on the silhouette and it is marked not visible.
Reference: [ SJ89 ] <author> Thawach Sripradisvarakul and Ramesh Jain. </author> <title> Generating aspect graphs for curved objects. </title> <booktitle> In Proceedings of the IEEE Workshop on Interpretation of 3D Scenes, </booktitle> <pages> pages 109-115. </pages> <publisher> IEEE, </publisher> <year> 1989. </year>
Reference-contexts: The arcs in the graph represent movement from one set of visible features to another, and this characteristic has been exploited in the ATR process [ PD87 ] , [ Pla88 ] . Other techniques have used projection techniques similar to ours <ref> [ Bon86; GM88; SJ89 ] </ref> . A more relevant discussion of feature extraction appears in [ SD92 ] , and describes a simple test we implement for determining whether or not an edge represents a silhouette edge.
Reference: [ Sny92 ] <author> John M. Snyder. </author> <title> Generative Modeling for Computer Graphics and CAD: Symbolic Shape Design Using Interval Analysis. </title> <publisher> Academic Press, </publisher> <address> first edition, </address> <year> 1992. </year>
Reference-contexts: The ATR algorithm and RangeView communicate, thus enabling RangeView to update the display of the hypothesized target model relative to the data. 3 Converting CSG to Polyhedra Polyhedral representations can be used to describe any type of object that can be bounded by a set of planar faces <ref> [ Sny92 ] </ref> . The two main advantages of polyhedral models over CSG representations are: the ease with which they can be rendered on modern graphics hardware, and the ability to readily analyze their shape.
References-found: 23

