URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/avrim/www/Papers/mts_experts.ps.gz
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/avrim/www/Papers/pubs.html
Root-URL: http://www.cs.cmu.edu
Email: avrim+@cs.cmu.edu  cburch+@cmu.edu  
Title: On-line Learning and the Metrical Task System Problem analysis showing how two recent algorithms for
Author: Avrim Blum Carl Burch 
Note: An  
Date: July 6, 1998  
Address: Pittsburgh, PA 15213-3891  Pittsburgh, PA 15213-3891  
Affiliation: School of Computer Science Carnegie Mellon University  School of Computer Science Carnegie Mellon University  
Abstract: In this paper, we relate the problem of combining expert advice, studied extensively in the Computational Learning Theory literature, to the Metrical Task System (MTS) problem, studied extensively in the area of On-line Algorithms. We show that these problems contain several interesting similarities and see how algorithms designed for each can be used to achieve good bounds and new approaches for solving the other. Specific contributions of this paper include: Finally, we present an experimental comparison of how these algorithms perform on a process migration problem, a problem that combines aspects of both the experts-tracking and MTS formalisms.
Abstract-found: 1
Intro-found: 1
Reference: [Bar96] <author> Y. Bartal. </author> <title> Probabilistic approximations of metric spaces and its algorithmic applications. </title> <booktitle> In Proc IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 183-193, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: In fact, an algorithm for the r-unfair ratio on the uniform metric space is at the heart of the best algorithm known for the MTS problem on arbitrary metric spaces [BBBT97] (together with an approximation of arbitrary spaces by hierarchical ones <ref> [Bar96] </ref>). We will see that the "r" parameter has a natural interpretation in the experts setting as well. MTS algorithms are naturally divided between the deterministic and randomized algorithms.
Reference: [BBBT97] <author> Y. Bartal, A. Blum, C. Burch, and A. Tomkins. </author> <title> A polylog(n)-competitive algorithm for metrical task systems. </title> <booktitle> In Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 711-719, </pages> <year> 1997. </year>
Reference-contexts: In fact, an algorithm for the r-unfair ratio on the uniform metric space is at the heart of the best algorithm known for the MTS problem on arbitrary metric spaces <ref> [BBBT97] </ref> (together with an approximation of arbitrary spaces by hierarchical ones [Bar96]). We will see that the "r" parameter has a natural interpretation in the experts setting as well. MTS algorithms are naturally divided between the deterministic and randomized algorithms. <p> In contrast, we describe below an algorithm whose competitive ratio is 1+O (log (n)=r), resulting in a much better Experts-DTF partitioning bound. 3.4 Odd-Exponent The good performance of Linear suggests generalizing for more than two experts. Bartal et al <ref> [BBBT97] </ref> analyze a generalization that can also be applied in an experts setting. 8 Algorithm Odd-Exponent [BBBT97]: Let t be an odd integer, and let ^ L i represent the reduced loss of expert i. <p> Bartal et al <ref> [BBBT97] </ref> analyze a generalization that can also be applied in an experts setting. 8 Algorithm Odd-Exponent [BBBT97]: Let t be an odd integer, and let ^ L i represent the reduced loss of expert i. Then place p i = n 1 n X r probability on expert i. 3.4.1 Theoretical performance The following theorem of Bartal et al [BBBT97] shows that this strategy performs well in <p> in an experts setting. 8 Algorithm Odd-Exponent <ref> [BBBT97] </ref>: Let t be an odd integer, and let ^ L i represent the reduced loss of expert i. Then place p i = n 1 n X r probability on expert i. 3.4.1 Theoretical performance The following theorem of Bartal et al [BBBT97] shows that this strategy performs well in a uniform task system. Theorem 5 ([BBBT97]) For an n-node uniform task system, Odd-Exponent has an r-unfair competitive ratio of 1 + 2n 1=t t=r. <p> Proof. This is a corollary of Theorem 5 by applying Theorem 1. The additive term comes from the fact that in the analysis of <ref> [BBBT97] </ref>, the algorithm competes against not the minimum reduced loss, but the average, which is at most r more. <p> better than these theorems indicate. 3.4.2 Implementation In an implementation of Odd-Exponent, using reduced loss strictly as defined above introduces a problem: the algorithm could allocate negative probability to an expert. (Consider the case where one expert has reduced loss of r while the rest are zero.) The analysis of <ref> [BBBT97] </ref>, concerned with theoretical guarantees, skirts the issue by observing that we may assume without loss of generality that experts with zero probability incur zero loss, and furthermore, because of the one step lookahead in the MTS setting, that an expert never receives a greater loss than that needed to set <p> migration data algorithm competitive ratio partitioning bound Linear (n = 2) 1 + 1=r [BKRS92] (1 + 1=2r)L + (r + 1=2)k (Th 3) Marking H n (1 + 1=r) [BLS92] H n (1 + 1=r)L + H n (r + 1)k (Cor 4) Odd-Exponent 1 + 2e ln (n)=r <ref> [BBBT97] </ref> (1 + 1=c)L + (2e (c + 1) ln n)k (Cor 6) Thresh unbounded (1fi)(1ff) L + ln (n=fiff) (1fi)(1ff) k (Th 7) Share 1 + 8 ln ((n + 2)(2r + 1))=r (Th 9) (1fi)(1ff) L + ln ((n+2)=ff) Table 3: Summary of theoretical results 16 have the MTS
Reference: [BKRS92] <author> A. Blum, H. Karloff, Y. Rabani, and M. Saks. </author> <title> A decomposition theorem and lower bounds for randomized server problems. </title> <booktitle> In Proc IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 197-207, </pages> <year> 1992. </year> <month> 17 </month>
Reference-contexts: Then its total cost is i maxf0; p t i p t+1 ! For a refined analysis of the MTS problem, we use the r-unfair competitive ratio considered in <ref> [BKRS92] </ref> and formalized explicitly by Seiden [Sei96]. Here the on-line algorithm pays the same amount as before, but OPT pays r times more for movement. That is, the off-line player pays rd i;j + ` j for a task. <p> Several MTS algorithms allocate probability to experts as a function of the reduced losses. An algorithm with this property must place zero probability on all pinned experts. 3.1 Two experts The following algorithm of Blum, Karloff, Rabani, and Saks <ref> [BKRS92] </ref> achieves an optimal competitive ratio for the r-unfair MTS problem on two states. In the Experts-DTF setting, the algorithm can be viewed as follows. Algorithm Linear [BKRS92]: The algorithm has one parameter r. Let ^ L i represent the reduced loss of expert i with respect to r. <p> with this property must place zero probability on all pinned experts. 3.1 Two experts The following algorithm of Blum, Karloff, Rabani, and Saks <ref> [BKRS92] </ref> achieves an optimal competitive ratio for the r-unfair MTS problem on two states. In the Experts-DTF setting, the algorithm can be viewed as follows. Algorithm Linear [BKRS92]: The algorithm has one parameter r. Let ^ L i represent the reduced loss of expert i with respect to r. The algorithm allocates p 0 = 2 ^ L 1 ^ L 0 probability to expert 0 and the rest to expert 1 (whose probability equation is symmetric). <p> fi = 2:6 fi 10 6 ; ff = 1 fi 10 10 2.94 0.15 44.21 fi = 0:5; ff = 0:01 4.03 Table 2: Performance relative to optimal off-line sequence (d = 1:0) on process migration data algorithm competitive ratio partitioning bound Linear (n = 2) 1 + 1=r <ref> [BKRS92] </ref> (1 + 1=2r)L + (r + 1=2)k (Th 3) Marking H n (1 + 1=r) [BLS92] H n (1 + 1=r)L + H n (r + 1)k (Cor 4) Odd-Exponent 1 + 2e ln (n)=r [BBBT97] (1 + 1=c)L + (2e (c + 1) ln n)k (Cor 6) Thresh unbounded
Reference: [BLS92] <author> A. Borodin, N. Linial, and M. Saks. </author> <title> An optimal online algorithm for metrical task systems. </title> <journal> J of the ACM, </journal> <volume> 39(4) </volume> <pages> 745-763, </pages> <year> 1992. </year>
Reference-contexts: Including such a cost, then, is a natural extension of the experts framework. This notion of an on-line algorithm having state, with a cost for moving between states, is captured by a problem studied in the On-line Algorithms literature called the Metrical Task System (MTS) problem <ref> [BLS92, IS95] </ref>. In this problem, we imagine the on-line algorithm as controlling a system that can be in one of n states or configurations. <p> Finally, in Section 5 we see an empirical comparison of these algorithms and others for the process migration problem. 2 2 Definitions and general relations 2.1 The MTS problem and the competitive ratio In the Metrical Task System (MTS) problem <ref> [BLS92, IS95] </ref>, an on-line algorithm controls a system with n states located at points in a space with distance metric d. The algorithm receives, one at a time, a sequence of tasks, each a cost vector specifying the cost of performing the task in each state. <p> MTS algorithms are naturally divided between the deterministic and randomized algorithms. Theoretically, the guarantees of deterministic algorithms are necessarily bad; the competitive ratio of a deterministic on-line MTS algorithms is at least 2n 1 <ref> [BLS92] </ref>. This is because an adversary who knows the deterministic algorithm can determine the algorithm's location, and so the adversary 3 can force the algorithm to pay heavily. A randomized algorithm can use coin flips to hide its location from an oblivious adversary. <p> Thus the total cost for the partition is at most k X 1 1 1 1 as desired. 7 3.2 Work-Function For the MTS problem on more than two states, the Work-Function algorithm of Borodin, Linial, and Saks <ref> [BLS92] </ref> is provably optimal for deterministic algorithms (even on general metrics). Algorithm Work-Function [BLS92]: We maintain the reduced loss of each expert. <p> the total cost for the partition is at most k X 1 1 1 1 as desired. 7 3.2 Work-Function For the MTS problem on more than two states, the Work-Function algorithm of Borodin, Linial, and Saks <ref> [BLS92] </ref> is provably optimal for deterministic algorithms (even on general metrics). Algorithm Work-Function [BLS92]: We maintain the reduced loss of each expert. <p> Since expert 3 is now pinned by expert 4, it would move to expert 4. Because of the adversarial model of competitive analysis, deterministic algorithms have very poor ratios. The best achievable guarantee for deterministic algorithms is 2n 1 <ref> [BLS92] </ref>. Borodin, Linial, and Saks show that Work-Function achieves this guarantee. <p> The most widely known randomized algorithm for the uniform metric space is the Marking algorithm of Borodin, Linial, and Saks <ref> [BLS92] </ref> and Fiat et al [FKL + 91]. Algorithm Marking: We maintain a counter for each state. At the beginning of each phase, the counters are reset to 0, and the algorithm occupies a random state. Given a cost vector `, we increment the ith counter by ` i . <p> This algorithm was designed for the "fair" setting in which r = 1. For that case, its competitive ratio is 2H n (where H n 2 [ln n; ln n + 1] is the nth harmonic number), which is optimal to constant factors <ref> [BLS92] </ref>. For the "unfair" setting, however, the competitive ratio does not decrease as substantially with r as we would like: the ratio becomes (1 + 1=r)H n . Therefore, the partitioning bound resulting from Theorem 1 is not so good. <p> fi = 0:5; ff = 0:01 4.03 Table 2: Performance relative to optimal off-line sequence (d = 1:0) on process migration data algorithm competitive ratio partitioning bound Linear (n = 2) 1 + 1=r [BKRS92] (1 + 1=2r)L + (r + 1=2)k (Th 3) Marking H n (1 + 1=r) <ref> [BLS92] </ref> H n (1 + 1=r)L + H n (r + 1)k (Cor 4) Odd-Exponent 1 + 2e ln (n)=r [BBBT97] (1 + 1=c)L + (2e (c + 1) ln n)k (Cor 6) Thresh unbounded (1fi)(1ff) L + ln (n=fiff) (1fi)(1ff) k (Th 7) Share 1 + 8 ln ((n +
Reference: [Chu94] <author> T. Chung. </author> <title> Approximate methods for sequential decision making using expert advice. </title> <booktitle> In Proc Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 183-189. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: We could imagine modeling the question of when and where such a process should move in the "decision-theoretic" experts framework of Freund and Schapire [FS95] and Chung <ref> [Chu94] </ref> as follows. <p> This is somewhat like diversifying one's holdings in the stock market, for instance. 2.2 Tracking experts in the decision-theoretic setting The second setting we consider is the "decision-theoretic" framework for learning from expert advice (also called the on-line allocation problem) <ref> [FS95, Chu94] </ref>. In this problem the learning algorithm faces a sequence of trials. For trial t, the algorithm chooses a probability distribution p t over a set of n experts.
Reference: [Esk90] <author> M. Eskicioglu. </author> <title> Process migration in distributed systems: A comparitive survey. </title> <type> Technical Report TR 90-3, </type> <institution> University of Alberta, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: In research process migration systems, the time for a process to move is roughly proportional to its size. For a 100-KB process, the time is about a second <ref> [Esk90] </ref>. Our distance corresponds to large but reasonable memory usage. Our simulations compared the performance of nine algorithms, including four simple control algorithms: Uniform The algorithm picks a random machine and stays there.
Reference: [FKL + 91] <author> A. Fiat, R. Karp, M. Luby, L. McGeoch, D. Sleator, and N. Young. </author> <title> Competitive paging algorithms. </title> <journal> J of Algorithms, </journal> <volume> 12 </volume> <pages> 685-699, </pages> <year> 1991. </year>
Reference-contexts: The most widely known randomized algorithm for the uniform metric space is the Marking algorithm of Borodin, Linial, and Saks [BLS92] and Fiat et al <ref> [FKL + 91] </ref>. Algorithm Marking: We maintain a counter for each state. At the beginning of each phase, the counters are reset to 0, and the algorithm occupies a random state. Given a cost vector `, we increment the ith counter by ` i .
Reference: [FS95] <author> Y. Freund and R. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory, </booktitle> <pages> pages 23-37, </pages> <year> 1995. </year>
Reference-contexts: We could imagine modeling the question of when and where such a process should move in the "decision-theoretic" experts framework of Freund and Schapire <ref> [FS95] </ref> and Chung [Chu94] as follows. <p> This is somewhat like diversifying one's holdings in the stock market, for instance. 2.2 Tracking experts in the decision-theoretic setting The second setting we consider is the "decision-theoretic" framework for learning from expert advice (also called the on-line allocation problem) <ref> [FS95, Chu94] </ref>. In this problem the learning algorithm faces a sequence of trials. For trial t, the algorithm chooses a probability distribution p t over a set of n experts.
Reference: [HW95] <author> M. Herbster and M. Warmuth. </author> <title> Tracking the best expert. </title> <booktitle> In Proc International Conference on Machine Learning, </booktitle> <pages> pages 286-294. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Section 4 explores the other direction: we analyze two algorithms for tracking the best expert in the decision-theoretic setting based on established weighted-experts algorithms <ref> [HW95, LW94] </ref> and show that one of them achieves good performance for the MTS problem, whereas the other has an unbounded competitive ratio. <p> We call this the Experts-DTF setting. This problem is typically analyzed with the goal of performing nearly as well as the best expert on the given sequence of trials. We consider the partitioning bound, a stronger goal (considered by Herbster and Warmuth <ref> [HW95] </ref> and Littlestone and Warmuth [LW94] for specific classes of loss functions) of performing nearly as well as the best sequence of experts. <p> Unfortunately, for the same L coefficient, the k coefficient guaranteed by Corollary 6 is approximately 2e times worse than for an algorithm (presented below) based on Herbster and Warmuth's weight-sharing algorithm <ref> [HW95] </ref>. <p> Indeed, in the experiments of Section 5, Thresh performs comparably to the Share algorithm we now consider.) 4.2 A weight-sharing experts algorithm We now describe a weight-sharing algorithm based on the Variable-share algorithm of Herbster and Warmuth <ref> [HW95] </ref> and prove that it achieves good bounds in both the Experts-DTF and MTS settings. Algorithm Share: The algorithm has two parameters: fi 2 [0; 1] is the usual penalty parameter and ff 2 [0; 1=2] is a "sharing" parameter.
Reference: [IS95] <author> S. Irani and S. Seiden. </author> <title> Randomized algorithms for metrical task systems. </title> <booktitle> In Intl. Workshop on Algorithms and Data Structures, </booktitle> <pages> pages 159-170, </pages> <year> 1995. </year>
Reference-contexts: Including such a cost, then, is a natural extension of the experts framework. This notion of an on-line algorithm having state, with a cost for moving between states, is captured by a problem studied in the On-line Algorithms literature called the Metrical Task System (MTS) problem <ref> [BLS92, IS95] </ref>. In this problem, we imagine the on-line algorithm as controlling a system that can be in one of n states or configurations. <p> Finally, in Section 5 we see an empirical comparison of these algorithms and others for the process migration problem. 2 2 Definitions and general relations 2.1 The MTS problem and the competitive ratio In the Metrical Task System (MTS) problem <ref> [BLS92, IS95] </ref>, an on-line algorithm controls a system with n states located at points in a space with distance metric d. The algorithm receives, one at a time, a sequence of tasks, each a cost vector specifying the cost of performing the task in each state.
Reference: [LW94] <author> N. Littlestone and M. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: Section 4 explores the other direction: we analyze two algorithms for tracking the best expert in the decision-theoretic setting based on established weighted-experts algorithms <ref> [HW95, LW94] </ref> and show that one of them achieves good performance for the MTS problem, whereas the other has an unbounded competitive ratio. <p> We call this the Experts-DTF setting. This problem is typically analyzed with the goal of performing nearly as well as the best expert on the given sequence of trials. We consider the partitioning bound, a stronger goal (considered by Herbster and Warmuth [HW95] and Littlestone and Warmuth <ref> [LW94] </ref> for specific classes of loss functions) of performing nearly as well as the best sequence of experts. <p> Then we see that another algorithm, a weight-sharing algorithm based on the ideas of Herbster and Warmuth, succeeds in both settings. 4.1 A weight-threshold algorithm The WML algorithm, due to Littlestone and Warmuth, is the following strategy for the on-line discrete prediction problem. Algorithm WML <ref> [LW94] </ref>: The algorithm uses two parameters, fi 2 [0; 1] and ff 2 [0; 1=2], and maintains a weight w i for each expert, initialized to 1.
Reference: [Sei96] <author> S. Seiden. </author> <title> Unfair problems and randomized algorithms for metrical task systems. </title> <type> Manuscript, </type> <month> April </month> <year> 1996. </year>
Reference-contexts: Then its total cost is i maxf0; p t i p t+1 ! For a refined analysis of the MTS problem, we use the r-unfair competitive ratio considered in [BKRS92] and formalized explicitly by Seiden <ref> [Sei96] </ref>. Here the on-line algorithm pays the same amount as before, but OPT pays r times more for movement. That is, the off-line player pays rd i;j + ` j for a task. This parameter r is known to the on-line and off-line MTS algorithms.
Reference: [Tom97] <author> A. Tomkins. </author> <title> Practical and theoretical issues in prefetching and caching. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> October </month> <year> 1997. </year> <note> CMU-CS-97-181. 18 </note>
Reference-contexts: A proof of this folklore result is in the appendix of <ref> [Tom97] </ref>. Consider now some ffi-elementary task vector, with non-zero cost in state i. Say that before processing this task, the algorithm had weight w i at state i and the total weight was W , so the probability p i at state i was w i =W .
References-found: 13

