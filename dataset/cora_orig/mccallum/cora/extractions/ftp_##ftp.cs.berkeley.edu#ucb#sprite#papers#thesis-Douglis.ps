URL: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/thesis-Douglis.ps
Refering-URL: http://www.cs.berkeley.edu/projects/sprite/sprite.papers.html
Root-URL: http://www.cs.berkeley.edu
Title: Transparent Process Migration in the Sprite Operating System  
Author: Frederick Douglis 
Date: September 1990  
Address: Berkeley, California 94720  
Affiliation: Computer Science Division Department of Electrical Engineering and Computer Sciences University of California  
Abstract-found: 0
Intro-found: 1
Reference: [ABB + 86] <author> M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> Mach: A new kernel foundation for UNIX development. </title> <booktitle> In Proceedings of the USENIX 1986 Summer Conference, </booktitle> <month> July </month> <year> 1986. </year>
Reference-contexts: The cached file blocks may represent a substantial amount of storage, in some cases greater than the process's virtual memory. * Message channels. In a message-based operating system such as Mach <ref> [ABB + 86] </ref> or V [Che88], state of this form would exist in place of open files. (In such a system message channels would be used to access files, whereas in Sprite, file-like channels are used for interprocess communication.) The state associated with a message channel includes buffered messages plus information
Reference: [AF89] <author> Y. Artsy and R. Finkel. </author> <title> Designing a process migration facility: The Charlotte experience. </title> <journal> IEEE Computer, </journal> <volume> 22(9) </volume> <pages> 47-56, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Kernels cache the most recent known location of a process, but if they find that a process is no longer at the same location, they multicast to find the 18 CHAPTER 2. BACKGROUND AND RELATED WORK process's new site. Charlotte In Charlotte <ref> [AF89] </ref>, another message-passing system, Artsy and Finkel designed migration to be as fault-tolerant as possible. Migration can be aborted any time before migration reaches a commit point (the time at which the process's address space and message channels are transferred). <p> One simple method for transferring virtual memory is to send the process's entire memory image to the target machine at migration time, as in Charlotte <ref> [AF89] </ref> and LOCUS [PW85]. This approach is simple but it has two disadvantages. First, the transfer can take many seconds, even using the highest transfer rate allowed by the network. During 30 CHAPTER 4. <p> As will be seen below, Sprite achieves transparency by involving the home machine in some operations for remote processes. 4.3.1 Messages Versus Kernel Calls On the surface, it might appear that transparency is particularly easy to achieve in a message-based system like Accent [Zay87b], Charlotte <ref> [AF89] </ref>, or V [Che88]. In these systems all of a process's interactions with the rest of the world occur in a uniform fashion 36 CHAPTER 4. PROCESS MIGRATION MECHANISM through message channels.
Reference: [AK88] <author> R. Alonso and K. Kyrimis. </author> <title> A process migration implementation for a unix system. </title> <booktitle> In Proceedings of the USENIX 1988 Winter Conference, </booktitle> <pages> pages 365-372, </pages> <address> Dallas, TX, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: This model of checkpoint/restart does have the advantage that a process depends only on its current host for resources, so processes can move from a host that is about to be shut down. It is also relatively simple to implement. Alonso/Kyrimis Alonso and Kyrimis <ref> [AK88] </ref> also implemented a checkpoint/restart facility on top of UNIX, with changes to the kernel to support fast dumping and restoring of kernel data structures and to maintain additional information about names of open files.
Reference: [Amd67] <author> G. </author> <title> Amdahl. Validity of the single processor approach to achieving large scale computing capabilities. </title> <booktitle> In Proc. AFIPS Spring Joint Computer Conference, </booktitle> <address> Atlantic City, N.J., </address> <month> April </month> <year> 1967. </year>
Reference-contexts: As "Amdahl's Law" suggests, by decreasing the time required for the parallelizable portion of pmake's execution, the sequential portion eventually dominates the total execution time <ref> [Amd67] </ref>. Therefore, the greatest speedup is obtained when the sequential portion is a small fraction of the total execution time. The performance improvement due to load sharing also depends on the ability of the application to execute efficiently on many machines simultaneously.
Reference: [Baa86] <author> E. H. Baalbergen. </author> <title> Parallel and distributed compilations in loosely-coupled systems: A case study. </title> <booktitle> In Proceedings of Workshop on Large Grain Parallelism, </booktitle> <address> Providence, RI, </address> <month> October </month> <year> 1986. </year>
Reference-contexts: Baalbergen showed a speedup of 3.5 to compile the same file 4 times in parallel on separate Amoeba hosts <ref> [Baa86] </ref>.
Reference: [BBNG + 89] <author> A. Barak, R. Ben-Nattan, S. Guday, L. Picherski, O. Sasson, and E. Siegel-mann. </author> <title> Running distributed applications under the MOSIX multi-computer system. </title> <type> Technical Report 89-15, </type> <institution> Hebrew University of Jerusalem, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: It allows users to specify that programs should be run on lightly-loaded hosts, or for daemons to migrate processes for the purposes of smoothing the load across multiple hosts. MOSIX MOSIX <ref> [BS85, BBNG + 89, BSW89] </ref>, also a UNIX derivative, uses a single paradigm to move processes both between hosts and between processors within a multiprocessor. Intrahost 20 CHAPTER 2. <p> They implemented distributed versions of the traveling-salesman problem, several simulations, and image processing tasks. The traveling-salesman problem obtained a speedup that was nearly linear in the number of processors, while other applications showed less speedup due to communications overhead <ref> [BBNG + 89] </ref>. (It is important to note, however, that these speedups were obtained on shared-memory multiprocessors, and that comparable measurements on a loosely-coupled distributed system might be affected more strongly by communication costs.) 2.5 Summary By distributing processes across multiple hosts, a system can substantially reduce the time needed to
Reference: [Ber85] <author> Brian Bershad. </author> <title> Load balancing with maitre d'. </title> <type> Technical Report UCB/CSD 86/276, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> December </month> <year> 1985. </year>
Reference-contexts: Maitre d'and Butler provide load sharing facilities using remote invocation, with varying degrees of transparency and autonomy. Maitre d' Maitre d' <ref> [Ber85] </ref> is designed for a collection of time-sharing computers, each with its own file system. Each host typically runs two daemons, one (maitrd ) that manages exporting tasks during times of high load and another (garcon) that accepts remote tasks when the local load is low.
Reference: [Bir85] <author> K. Birman. </author> <title> Replication and fault-tolerance in the ISIS system. </title> <booktitle> In Proceedings of the 10th Symposium on Operating System Principles, </booktitle> <pages> pages 79-86, </pages> <address> Orcas Island, WA, </address> <month> December </month> <year> 1985. </year> <note> ACM. </note>
Reference-contexts: This method, used in MOSIX [BSW89, BS85], reduces overhead but still suffers from distributed state. Another possible approach is a replicated fault-tolerant facility that would run on only a few unloaded hosts, such as the facility currently being implemented in the ISIS system <ref> [Bir85, Coo90] </ref>. The remainder of this section describes these two alternatives. Probabilistic Host Selection Barak and Shiloh described a distributed host selection facility for MOS (later MOSIX) that used a probabilistic algorithm to disseminate host information [BS85]. <p> In addition, if processes do not simply request idle hosts from a server on their own host, they need a mechanism for locating a server. A system such as ISIS <ref> [Bir85] </ref>, which provides support for replicated fault-tolerant applications, can address these issues. ISIS permits a distributed facility to be logically centralized, yet composed of fault-tolerant components executing on multiple hosts.
Reference: [BN84] <author> A. D. Birrell and B. J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: Each host runs a distinct copy of the Sprite kernel, but the kernels work closely together using a remote-procedure-call (RPC) mechanism [Wel86] similar to that described by Birrell and Nelson <ref> [BN84] </ref>. All the hosts on the network share a common high-performance file system [Nel88, Wel90]. Sprite permits the data of a file to be cached in the memory of one or more machines, with file servers responsible for guaranteeing "consistent access" to the cached 22 3.2. SPRITE 23 data.
Reference: [BS85] <author> A. Barak and A. Shiloh. </author> <title> A distributed load-balancing policy for a multicom-puter. </title> <journal> Software|Practice and Experience, </journal> <volume> 15(9) </volume> <pages> 901-913, </pages> <month> September </month> <year> 1985. </year> <note> 114 BIBLIOGRAPHY 115 </note>
Reference-contexts: It allows users to specify that programs should be run on lightly-loaded hosts, or for daemons to migrate processes for the purposes of smoothing the load across multiple hosts. MOSIX MOSIX <ref> [BS85, BBNG + 89, BSW89] </ref>, also a UNIX derivative, uses a single paradigm to move processes both between hosts and between processors within a multiprocessor. Intrahost 20 CHAPTER 2. <p> For example, Plan 9 [PPTT90] forwards gettimeofday calls from processor servers to a user's own machine in order to keep times consistent across hosts, and MOSIX <ref> [BS85] </ref> and Remote UNIX [Lit87] forward calls as described in Section 2.3. Forwarding also occurs from the home machine to a remote process's current machine. <p> Two variations on the distributed approach address these problems. With probabilistic host selection, each host periodically updates state on a random subset of other hosts. This method, used in MOSIX <ref> [BSW89, BS85] </ref>, reduces overhead but still suffers from distributed state. Another possible approach is a replicated fault-tolerant facility that would run on only a few unloaded hosts, such as the facility currently being implemented in the ISIS system [Bir85, Coo90]. The remainder of this section describes these two alternatives. <p> The remainder of this section describes these two alternatives. Probabilistic Host Selection Barak and Shiloh described a distributed host selection facility for MOS (later MOSIX) that used a probabilistic algorithm to disseminate host information <ref> [BS85] </ref>. Each host kept track of the most recent information it had about other hosts. In order to avoid using obsolete information, a host would "age" old data as newer data arrived, thus giving more weight to recent data than to old data.
Reference: [BSW89] <author> A. Barak, A. Shiloh, and R. Wheeler. </author> <title> Flood prevention in the MOSIX load-balancing scheme. </title> <journal> IEEE Computer Society Technical Committee on Operating Systems Newsletter, </journal> <volume> 3(1) </volume> <pages> 23-27, </pages> <month> Winter </month> <year> 1989. </year>
Reference-contexts: The relative advantages and disadvantages of centralized and distributed techniques 1.4. THESIS OVERVIEW 5 for finding idle hosts, with respect to simplicity, scalability, performance, and fault tolerance, have been discussed in the literature with no clear-cut resolution (e.g., <ref> [BSW89, TL88, The86] </ref>). I examine a range of approaches to this problem and conclude that a centralized approach has advantages over distributed approaches in nearly all aspects. A fourth contribution, and perhaps the most important one, is a demonstration of the effectiveness of process migration in a production environment. <p> It allows users to specify that programs should be run on lightly-loaded hosts, or for daemons to migrate processes for the purposes of smoothing the load across multiple hosts. MOSIX MOSIX <ref> [BS85, BBNG + 89, BSW89] </ref>, also a UNIX derivative, uses a single paradigm to move processes both between hosts and between processors within a multiprocessor. Intrahost 20 CHAPTER 2. <p> Since the load sharing facility is itself responsible for many of the sudden bursts of processing activity within the system, it can anticipate changes in load by increasing the count of runnable processes by the number of processes expected to migrate onto a host in the near future. MOSIX <ref> [BSW89] </ref> uses this approach to prevent many hosts from "flooding" an idle host by migrating many processes onto it before its measured load is high enough to prevent further migrations. The second issue for host selection, host autonomy, arises when the system contains personal workstations. <p> Two variations on the distributed approach address these problems. With probabilistic host selection, each host periodically updates state on a random subset of other hosts. This method, used in MOSIX <ref> [BSW89, BS85] </ref>, reduces overhead but still suffers from distributed state. Another possible approach is a replicated fault-tolerant facility that would run on only a few unloaded hosts, such as the facility currently being implemented in the ISIS system [Bir85, Coo90]. The remainder of this section describes these two alternatives. <p> Loads are exchanged once per second in order to ensure that hosts have fairly recent information about all other hosts. The value of a host's load is artificially modified to include anticipated fluctuations due to migration <ref> [BSW89] </ref>. However, substantial changes in the load of a host still take several seconds to propagate to other hosts, and in the meantime hosts with obsolete load information may attempt to migrate onto a loaded host.
Reference: [Cab86] <author> L.-F. Cabrera. </author> <title> The influence of workload on load balancing strategies. </title> <type> Technical Report RJ5271, </type> <institution> IBM Almaden Research Center, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: more than 78% of processes living less than one second) and that processes that live a long time are expected to live longer; he concluded that process placement at creation time would be wasteful, and instead long-running processes should be migrated after they have executed for a period of time <ref> [Cab86] </ref>. I would argue that migration for the purpose of ensuring workstation autonomy is at least as important as migration for redistributing load. In our environment, in fact, these two goals are interrelated.
Reference: [Che87] <author> A. R. Cherenson. </author> <title> The sprite internet protocol server. </title> <type> Master's thesis, </type> <institution> Computer Science Division, Dept. of Electrical Engineering and Computer Sciences, University of California, Berkeley, </institution> <address> CA, </address> <year> 1987. </year>
Reference-contexts: The migration of a process is transparent to the processes with which it communicates, because only the operating system stores the location of the processes that use the pseudo-device. Even communication using Internet (IP/UDP/TCP) protocols over sockets is passed via a pseudo-device to and from a server process <ref> [Che87] </ref>, so Internet socket IPC does not pose any particular problem for migration. 3.2.1 Considerations for Migration Design Several other aspects of the Sprite environment were particularly important in the design of Sprite's process migration facility: Idle hosts are plentiful.
Reference: [Che88] <author> D. R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: The cached file blocks may represent a substantial amount of storage, in some cases greater than the process's virtual memory. * Message channels. In a message-based operating system such as Mach [ABB + 86] or V <ref> [Che88] </ref>, state of this form would exist in place of open files. (In such a system message channels would be used to access files, whereas in Sprite, file-like channels are used for interprocess communication.) The state associated with a message channel includes buffered messages plus information about senders and receivers. * <p> As will be seen below, Sprite achieves transparency by involving the home machine in some operations for remote processes. 4.3.1 Messages Versus Kernel Calls On the surface, it might appear that transparency is particularly easy to achieve in a message-based system like Accent [Zay87b], Charlotte [AF89], or V <ref> [Che88] </ref>. In these systems all of a process's interactions with the rest of the world occur in a uniform fashion 36 CHAPTER 4. PROCESS MIGRATION MECHANISM through message channels.
Reference: [Com86] <institution> Computer Science Division, University of California, Berkeley. </institution> <note> UNIX User's Reference Manual, 4.3 Berkeley Software Distribution, Virtual VAX-11 Version, </note> <month> April </month> <year> 1986. </year>
Reference-contexts: One possible mechanism for taking advantage of idle hosts is remote invocation: programs may be started on other hosts but not moved once they have begun. An example of a remote invocation facility is the Berkeley UNIX rsh <ref> [Com86] </ref> command, which sends commands and their arguments to an intermediary on another host and receives the output of the commands from the intermediary. <p> Section 3.4 summarizes load sharing in Sprite. 3.2 Sprite Sprite is an operating system for a collection of personal workstations and file servers on a local-area network [OCD + 88]. Sprite's kernel-call interface is much like that of 4.3 BSD UNIX <ref> [Com86] </ref>, but Sprite's implementation is a new one that provides a high degree of network integration. Each host runs a distinct copy of the Sprite kernel, but the kernels work closely together using a remote-procedure-call (RPC) mechanism [Wel86] similar to that described by Birrell and Nelson [BN84]. <p> This appendix lists how each system call is handled in Sprite to ensure transparent process migration. Because Sprite attempts to be compatible with 4.3BSD UNIX, and UNIX is more widely known than Sprite, I list the system calls available in 4.3BSD UNIX <ref> [Com86] </ref>. All system calls that are available in Sprite but have no UNIX equivalent are handled transparently by the remote host, with one exception: the call to initiate migration is forwarded home, since it affects processes relative to their home machine. 109 110 APPENDIX A.
Reference: [Coo90] <author> R. Cooper. </author> <type> Personal Communication, </type> <year> 1990. </year>
Reference-contexts: This method, used in MOSIX [BSW89, BS85], reduces overhead but still suffers from distributed state. Another possible approach is a replicated fault-tolerant facility that would run on only a few unloaded hosts, such as the facility currently being implemented in the ISIS system <ref> [Bir85, Coo90] </ref>. The remainder of this section describes these two alternatives. Probabilistic Host Selection Barak and Shiloh described a distributed host selection facility for MOS (later MOSIX) that used a probabilistic algorithm to disseminate host information [BS85]. <p> Since each process would react to each message, this facility would have essentially the same performance characteristics as the physically centralized server described in Section 6.3.2. A prototype host selection facility using ISIS is currently being developed at Cornell University <ref> [Coo90] </ref>. 6.3.4 Multicast Requests Another way to address the problems of a centralized repository for host information, namely processor contention and reliability, is to have no repository at all. Instead of having hosts announce their state in advance, they respond to requests for idle hosts if they are available.
Reference: [Dan82] <author> R. Dannenberg. </author> <title> Resource Sharing in a Network of Personal Computers. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> December </month> <year> 1982. </year> <note> Report No. CMU-CS-82-152. </note>
Reference-contexts: Butler Butler serves as an agent to manage resources in a collection of workstations. The Butler system has had two incarnations, one as a research prototype and one that has received considerable use. Dannenberg's prototype <ref> [Dan82] </ref> was built on the Accent system and used process migration to reclaim hosts; process migration in Accent is described below in Section 2.3.3. <p> As in V, processes are accessed via communication links that are independent of location, so the act of migration is transparent. Accent Dannenberg's Butler was a prototype remote execution facility that used process migration to transfer processes between hosts <ref> [Dan82] </ref>. In this version, virtual memory was transferred monolithically. Zayas modified Accent to migrate processes using copy-on-reference virtual memory [Zay87a, Zay87b]. When a process migrates, its memory image is initially left on the source machine; only the process's page tables, registers, and message channels need be transferred immediately.
Reference: [DO91] <author> F. Douglis and J. Ousterhout. </author> <title> Transparent process migration: Design alternatives and the Sprite implementation. </title> <journal> Software|Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-785, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The time to select and release an idle host using DECstation 3100 workstations was measured to be 56 milliseconds. <ref> [DO91] </ref>. 6.3. HOST SELECTION MECHANISM 69 call to obtain the data from the server. Normally, no changes are pending and the stream is not readable, so no network operations should be needed.
Reference: [ELZ88] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> The limited performance benefits of migrating active processes for load sharing. </title> <booktitle> In ACM SIGMETRICS 1988, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: For several years, the research community has debated the desirability of process migration with respect to load sharing. Eager, et al., reported that process placement is much more important than migrating active processes and that migration provides limited improvement beyond placement <ref> [ELZ88] </ref>. Krueger and Livny, on the other hand, found that migration can provide significant improvement beyond initial placement [KL88].
Reference: [Fel79] <author> S. I. Feldman. </author> <title> Make | a program for maintaining computer programs. </title> <journal> Software|Practice and Experience, </journal> <volume> 9(4) </volume> <pages> 255-265, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: Process migration should be automatic in two respects. First, load should be spread across idle machines without user intervention. The selection of what processes to migrate and the hosts to which to migrate them can often be performed by the system. For example, a user who types make <ref> [Fel79] </ref> to compile a program need not be aware that compilations are performed on other hosts; the user is only aware that the compilations execute quickly. <p> Their ability to execute over a slower network depends primarily on their requirements for interprocess communication. The next sections consider existing applications that could take advantage of load sharing. 2.4.1 Parallel compilation Make <ref> [Fel79] </ref> is a UNIX program that allows users to specify commands to bring files up to date and dependencies between files. It is commonly used to recompile programs when some of their source files have changed, though it may be used for other purposes as well. <p> The effective processor utilization of a sample set of 100 independent (parallelizable) simulations was over 800%, compared to 300% for the 12-way parallel compilation mentioned above. 7.4.1 Representative Pmake Performance The pmake program, like make <ref> [Fel79] </ref>, generates a dependency graph from its input specification, determines which files are out-of-date, and recreates each out-of-date file (or "target"). Unlike make, it can find disjoint dependency subgraphs and recreate independent targets in parallel.
Reference: [FH89] <author> C. J. Fleckenstein and D. Hemmendinger. </author> <title> Using a global name space for parallel execution of UNIX tools. </title> <journal> Communications of the ACM, </journal> <volume> 32(9) </volume> <pages> 1085-1090, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Fleckenstein and Hemmendinger themselves implemented a parallel make using Linda and NFS, obtaining a speedup of two to three using four hosts, with minimal improvement beyond four hosts for their benchmark <ref> [FH89] </ref>. Baalbergen showed a speedup of 3.5 to compile the same file 4 times in parallel on separate Amoeba hosts [Baa86].
Reference: [GC89] <author> Cary G. Gray and David R. Cheriton. Leases: </author> <title> an efficient fault-tolerant mechanism for distributed file cache consistency. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 202-210, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Obviously, the file system is already stressed by some applications of migration, but performing file name lookups on client workstations should reduce contention considerably. (Gray and Cheriton's file "leases" <ref> [GC89] </ref> are promising in this respect.) With many hosts the host selection facility may also potentially become a bottleneck, unless host assignments may be cached effectively to reduce the rate of requests to a central server.
Reference: [KL88] <author> P. Krueger and M. Livny. </author> <title> A comparison of preemptive and non-preemptive load distributing. </title> <booktitle> In Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 123-130, </pages> <address> San Jose, CA, </address> <month> June </month> <year> 1988. </year> <note> IEEE. 116 BIBLIOGRAPHY </note>
Reference-contexts: Eager, et al., reported that process placement is much more important than migrating active processes and that migration provides limited improvement beyond placement [ELZ88]. Krueger and Livny, on the other hand, found that migration can provide significant improvement beyond initial placement <ref> [KL88] </ref>.
Reference: [LH89] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Virtual memory transfer becomes much more complicated if the process to be migrated is sharing writable virtual memory with some other process on the source machine. In principle, it is possible to maintain the shared virtual memory even after one of the sharing processes migrates <ref> [LH89] </ref>, but this changes the cost of shared accesses so dramatically that it seemed unreasonable. Currently, shared writable virtual memory almost never occurs in Sprite, so Sprite simply disallows migration for processes using it.
Reference: [Lit87] <author> M. Litzkow. </author> <title> Remote UNIX. </title> <booktitle> In Proceedings of the USENIX 1987 Summer Conference, </booktitle> <month> June </month> <year> 1987. </year>
Reference-contexts: For example, the V System restricts process migration to programs whose output does not depend upon their location [The86], and Remote UNIX migrates only single-process noninteractive jobs that are not permitted to communicate with other processes <ref> [Lit87] </ref>. Ideally, any process should be able to migrate at any time without affecting its results or the results of other processes. Process migration should be automatic in two respects. First, load should be spread across idle machines without user intervention. <p> Remote Unix and Condor Remote UNIX is a remote execution facility for UNIX, suitable for background CPU-intensive applications <ref> [Lit87] </ref>. It requires no modifications to the UNIX kernel, instead using a special run-time library to forward nearly all system calls to a surrogate process running on the host that initiated the remote process. As in Smith and Ioannidis's system, processes can be checkpointed and resumed elsewhere. <p> It turns out that neither of these initial impressions is correct. For example, it would be possible to implement forwarding in a kernel-call-based system by leaving all of the kernel state on the home machine and using remote procedure calls to forward home every kernel call, as Remote UNIX <ref> [Lit87] </ref> does. This would result in an approach very similar to forwarding messages, and our initial plan was to use an approach like this for Sprite. Unfortunately, an approach based entirely on forwarding kernel calls or forwarding messages will not work in practice, for two reasons. <p> For example, Plan 9 [PPTT90] forwards gettimeofday calls from processor servers to a user's own machine in order to keep times consistent across hosts, and MOSIX [BS85] and Remote UNIX <ref> [Lit87] </ref> forward calls as described in Section 2.3. Forwarding also occurs from the home machine to a remote process's current machine.
Reference: [LLM88] <author> M. Litzkow, M. Livny, and M. </author> <title> Mutka. Condor | a hunter of idle workstations. </title> <booktitle> In Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 104-111, </pages> <address> San Jose, CA, </address> <month> June </month> <year> 1988. </year> <note> IEEE. </note>
Reference-contexts: BACKGROUND AND RELATED WORK connections or parent-child relationships. The initial implementation of Remote UNIX also did not support several system calls, including fork (), exec (), and network-related calls, though those were to be added. Condor <ref> [LLM88] </ref> uses Remote UNIX to make use of idle workstations. Tasks are submitted to Condor using a "batch" mode; i.e., no interaction with the user is permitted, and the tasks are queued for execution if no host is immediately available. <p> The underlying operating system may differ radically from one facility to another, affecting the feasibility and complexity of transparent remote execution. A facility may be used for only long-running noninteractive processes, such as Condor is <ref> [LLM88] </ref>, or it may be intended for a mixture of interactive and noninteractive processes with varying needs for resources. The particular environment influences the design of the facility, within the spectrum of alternatives described in Chapter 2. <p> Sprite is of course not alone in these respects. For example, Butler used a shared file to store the state about each host; later, for improved performance, Nichols changed Butler to use a centralized process [Nic90]. Condor also uses a centralized process to control access to idle hosts <ref> [LLM88] </ref>. This thesis supports Nichols's and Litzkow's conclusions with respect to centralization, as well as examining additional issues such as fair allocation of hosts. In this section I compare four host selection architectures using the criteria listed above. <p> As a final note, during the measured one-month period the total processor utilization of the system was 2.3% (averaged over all times of day). Its utilization is an order of mag 8.3. PROCESS EVICTION 95 nitude smaller than the utilization reported for Condor <ref> [LLM88] </ref>, but as new long-running parallelizable applications execute on Sprite, its utilization will likely increase significantly. 8.3 Process Eviction When a user returns to a workstation, the execution of foreign processes on the workstation may degrade the user's interactive response.
Reference: [ML87] <author> M. Mutka and M. Livny. </author> <title> Profiling workstations' available capacity for remote execution. </title> <booktitle> In Performance '87, Proceedings of the 12th IFIP WG 7.3 Symposium on Computer Performance, </booktitle> <pages> pages 529-544, </pages> <address> Brussels, Belgium, </address> <month> December </month> <year> 1987. </year>
Reference-contexts: Idle hosts are especially common in academic environments: * Mutka and Livny reported that on average, 70% of workstations in their environment were idle <ref> [ML87] </ref>. * Theimer and Lantz reported that one-third of the 70 workstations in a particular cluster were completely idle, even during the day [TL88]. * In the Butler system, 50-70 out of 350 workstations were typically in the free pool during the day, with over 100 available machines at night [Nic87]. <p> Mutka and Livny found that hosts that have been idle for a substantial period of time will most likely remain idle for a long time, while hosts that have been idle for a short time are likely to become active again quickly <ref> [ML87] </ref>. My own measurements, reported below in Section 8.5, support their results. Other criteria, such as hardware configurations and file caching, can permit a host selection facility to make a more intelligent selection from among multiple available hosts.
Reference: [Mor] <author> J. Morris. </author> <title> Confirmed via personal communication. </title>
Reference-contexts: example, simulators are commonly run multiple times with varying parameters, and multiple instances of a simulator could run 1 On the subject of workstations versus time-sharing systems, the famous one-liner by Jim Morris comes to mind: "The nice thing about an Alto is that it doesn't get faster at night." <ref> [Mor] </ref> 1 2 CHAPTER 1. INTRODUCTION simultaneously and report their results to a coordinating process. Simulators often take minutes or hours to execute; if a simulator is consuming memory and processor cycles on a host whose owner returns, the owner may be adversely affected for a prolonged period of time.
Reference: [MvRT + 90] <author> S. Mullender, G. van Rossum, A. Tanenbaum, R. van Renesse, and H. van Staveren. </author> <title> Amoeba: A distributed operating system for the 1990s. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 44-53, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Other computation is performed on a collection of processors shared by all users. On those processors, all scheduling decisions are made automatically by system software. Users submit jobs to the system without any idea of where they will execute, and users do not have priority for particular machines. Amoeba <ref> [MvRT + 90] </ref> and Plan 9 [PPTT90] are examples of this approach. At the other end of the policy spectrum lies rsh, which provides no policy support whatsoever. In this model, each host is completely independent. <p> In this sense, Sprite's approach to load sharing is similar to Amoeba's "processor pool," which permits applications to reserve hosts for an extended period of time <ref> [MvRT + 90] </ref>. This technique for avoiding contention is also similar to MOSIX's method of increasing the reported processor load with the expectation that the actual load will immediately increase.
Reference: [Nel88] <author> M. N. Nelson. </author> <title> Physical Memory Management in a Network Operating System. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA 94720, </address> <month> November </month> <year> 1988. </year> <note> Available as Technical Report UCB/CSD 88/471. </note>
Reference-contexts: Each host runs a distinct copy of the Sprite kernel, but the kernels work closely together using a remote-procedure-call (RPC) mechanism [Wel86] similar to that described by Birrell and Nelson [BN84]. All the hosts on the network share a common high-performance file system <ref> [Nel88, Wel90] </ref>. Sprite permits the data of a file to be cached in the memory of one or more machines, with file servers responsible for guaranteeing "consistent access" to the cached 22 3.2. SPRITE 23 data. <p> While server data caching is important for performance, since it avoids disk traffic, client caching is even more important: caching data on clients not only reduces network traffic, but it reduces server processor utilization as well. As a result, servers can support more clients effectively <ref> [Nel88] </ref>. In exchange for higher performance and lower utilization, however, the system must face the complexity of keeping the caches consistent. As I described in Section 5.2.2, the I/O server is the central point for cache consistency operations in Sprite. <p> Because file data caching in Sprite is effective, name lookups are the greatest cause of contention for file server processing. In his thesis, Nelson estimated that adding client name caching would reduce file server utilization by as much as a factor of two <ref> [Nel88] </ref>. It would also decrease network utilization by up to a factor of two. Measurements of server load during parallel compilations demonstrate that name caching is imperative if the full benefits of migration are to be exploited.
Reference: [Nic87] <author> D. Nichols. </author> <title> Using idle workstations in a shared computing environment. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 5-12, </pages> <address> Austin, TX, </address> <month> November </month> <year> 1987. </year> <note> ACM. </note>
Reference-contexts: [ML87]. * Theimer and Lantz reported that one-third of the 70 workstations in a particular cluster were completely idle, even during the day [TL88]. * In the Butler system, 50-70 out of 350 workstations were typically in the free pool during the day, with over 100 available machines at night <ref> [Nic87] </ref>. * My own results, presented in Chapter 8, indicate that 65-70% of hosts in Sprite are idle on average during the day, with up to 80% idle at night and on weekends. The rest of this chapter is organized as follows. <p> The Butler system has had two incarnations, one as a research prototype and one that has received considerable use. Dannenberg's prototype [Dan82] was built on the Accent system and used process migration to reclaim hosts; process migration in Accent is described below in Section 2.3.3. Nichols <ref> [Nic87] </ref> reimplemented Butler for the Andrew system, making no kernel modifications and requiring no changes to application programs. 1 The decision to avoid kernel modifications limits Butler's functionality in exchange for simplicity and portability. <p> Some workstations are available for public use and are not used on a regular basis. However, after discounting for extra workstations, I still find a sizable fraction of hosts available, concurring with Theimer [TL88], Nichols <ref> [Nic87] </ref>, and others.
Reference: [Nic90] <author> D. Nichols. </author> <title> Multiprocessing in a Network of Workstations. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: Sprite is of course not alone in these respects. For example, Butler used a shared file to store the state about each host; later, for improved performance, Nichols changed Butler to use a centralized process <ref> [Nic90] </ref>. Condor also uses a centralized process to control access to idle hosts [LLM88]. This thesis supports Nichols's and Litzkow's conclusions with respect to centralization, as well as examining additional issues such as fair allocation of hosts.
Reference: [NWO88] <author> M. Nelson, B. Welch, and J. Ousterhout. </author> <title> Caching in the Sprite network file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: File servers are responsible for guaranteeing "consistent access" to the cached data; this is performed by disabling caching when files are accessed by multiple hosts while any host is writing the file, and by flushing modified data to file servers if a process on a different host opens a file <ref> [NWO88] </ref>. Sprite's caching policy impacts process migration and load sharing in two ways: 42 5.1.
Reference: [OCD + 88] <author> J. Ousterhout, A. Cherenson, F. Douglis, M. Nelson, and B. Welch. </author> <title> The Sprite network operating system. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: These terms will be discussed in detail in the next chapter. 1.3 Research Contributions This thesis describes the design, implementation, and performance of a process migration facility for the Sprite operating system <ref> [OCD + 88] </ref>. The goals of process migration in Sprite are, first, to make the computing power of a collection of workstations available to users as though it were a single, fast time-sharing system; and second, to respect the response-time demands of individual users. <p> Section 3.3 presents the design of a transparent load sharing facility for Sprite using process migration. Section 3.4 summarizes load sharing in Sprite. 3.2 Sprite Sprite is an operating system for a collection of personal workstations and file servers on a local-area network <ref> [OCD + 88] </ref>. Sprite's kernel-call interface is much like that of 4.3 BSD UNIX [Com86], but Sprite's implementation is a new one that provides a high degree of network integration.
Reference: [PPTT90] <author> R. Pike, D. Presotto, K. Thompson, and H. Trickey. </author> <title> Plan 9 from Bell Labs. </title> <booktitle> In UKUUG Summer 1990 Conference Proceedings, </booktitle> <pages> pages 1-9, </pages> <address> London, England, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: On those processors, all scheduling decisions are made automatically by system software. Users submit jobs to the system without any idea of where they will execute, and users do not have priority for particular machines. Amoeba [MvRT + 90] and Plan 9 <ref> [PPTT90] </ref> are examples of this approach. At the other end of the policy spectrum lies rsh, which provides no policy support whatsoever. In this model, each host is completely independent. <p> Despite the performance penalty, however, forwarding calls is a simple solution to a wide range of problems, and several other systems forward calls automatically in some or all cases. For example, Plan 9 <ref> [PPTT90] </ref> forwards gettimeofday calls from processor servers to a user's own machine in order to keep times consistent across hosts, and MOSIX [BS85] and Remote UNIX [Lit87] forward calls as described in Section 2.3. Forwarding also occurs from the home machine to a remote process's current machine.
Reference: [PW85] <author> G. J. Popek and B. J. Walker, </author> <title> editors. The LOCUS Distributed System Architecture. </title> <booktitle> Computer Systems Series. </booktitle> <publisher> The MIT Press, </publisher> <year> 1985. </year> <note> BIBLIOGRAPHY 117 </note>
Reference-contexts: Accent provides name and location transparency to processes by means of message channels. It does not provide automatic load sharing, though Butler could easily be reim-plemented on top of the current Accent process migration facility. 2.3. IMPLEMENTATIONS OF LOAD SHARING 19 LOCUS LOCUS <ref> [PW85] </ref> is a UNIX-like system that provides two forms of migration, one for transferring a process at an arbitrary time and one for performing an exec on another host. <p> One simple method for transferring virtual memory is to send the process's entire memory image to the target machine at migration time, as in Charlotte [AF89] and LOCUS <ref> [PW85] </ref>. This approach is simple but it has two disadvantages. First, the transfer can take many seconds, even using the highest transfer rate allowed by the network. During 30 CHAPTER 4. PROCESS MIGRATION MECHANISM this time the process is frozen: it cannot execute on either the source or destination machine. <p> Process migration occurs most often during an exec system call, which completely replaces the process's address space. If migration occurs during an exec, the new address space is created on the destination machine so there is no virtual memory to transfer. As others have observed (e.g., LOCUS <ref> [PW85] </ref>), the performance of virtual memory transfer for exec-time migration is not an issue. Virtual memory transfer is an issue, however, when migration is used to evict a process from a machine whose user has returned. <p> The server stores the access position for the stream as well as information about the hosts that have references to it. Another possible approach to shared access positions is the one used in LOCUS <ref> [PW85] </ref>. If process migration causes a file access position to be shared between machines, LOCUS lets the sharing machines take turns managing the access position.
Reference: [RE87] <author> E. Roberts and J. Ellis. </author> <title> parmake and dp: Experience with a distributed, parallel implementation of make. </title> <booktitle> In Proceedings from the Second Workshop on Large-Grained Parallelism. </booktitle> <institution> Software Engineering Institute, Carnegie-Mellon University, </institution> <month> November </month> <year> 1987. </year> <note> Report CMU/SEI-87-SR-5. </note>
Reference-contexts: Each command is executed almost as though the user were establishing a new login session on the remote host. Other commands exist that are similar to rsh in functionality but have lower overhead; these facilities include Topaz's "distant processes" <ref> [RE87] </ref> and SunOS's rex [Sun87]. Performance considerations aside, remote invocation does not offer the flexibility or ease of use that is desirable for automatic use of idle hosts in a workstation environment. <p> Baalbergen showed a speedup of 3.5 to compile the same file 4 times in parallel on separate Amoeba hosts [Baa86]. Roberts and Ellis have obtained speedups ranging from 6 to 12 using 15 machines, with the limiting factor being disk traffic to and from the controlling machine <ref> [RE87] </ref>. 2.4.2 Distributed Applications Barak, et al., converted several single-machine programs to take advantage of MOSIX's dynamic load balancing by executing in parallel on a multiprocessor. They implemented distributed versions of the traveling-salesman problem, several simulations, and image processing tasks.
Reference: [SGK + 85] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and implementation of the Sun network filesystem. </title> <booktitle> In Proceedings of the USENIX 1985 Summer Conference, </booktitle> <pages> pages 119-130, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: The lack of a general remote execution mechanism prevented load sharing policies from being implemented in actual systems, though they were of theoretical interest. The advent of NFS <ref> [SGK + 85] </ref> and other network file systems dramatically simplified remote execution. Processes on different hosts could now share a similar, if not identical, file system.
Reference: [SI89] <author> J. M. Smith and J. Ioannidis. </author> <title> Implementing remote fork() with checkpoint/restart. </title> <journal> IEEE Computer Society Technical Committee on Operating Systems Newsletter, </journal> <volume> 3(1) </volume> <pages> 15-19, </pages> <month> Winter </month> <year> 1989. </year>
Reference-contexts: Migration would be "restricted" because the new process would not have the same process identifier or parent process, and it might not have the same access to network connections or other open files. Smith/Ioannidis Smith and Ioannidis <ref> [SI89] </ref> implemented a variant of the fork () system call that would create an executable image of a process on a new machine using checkpoint/restart. The executable image would be copied to another host using a standard file-copying program such as rcp.
Reference: [Sun87] <author> Sun Microsystems. </author> <title> Sun Release 4.0 Commands Reference Manual, </title> <year> 1987. </year>
Reference-contexts: Each command is executed almost as though the user were establishing a new login session on the remote host. Other commands exist that are similar to rsh in functionality but have lower overhead; these facilities include Topaz's "distant processes" [RE87] and SunOS's rex <ref> [Sun87] </ref>. Performance considerations aside, remote invocation does not offer the flexibility or ease of use that is desirable for automatic use of idle hosts in a workstation environment.
Reference: [SvE89] <author> A. Stolcke and T. von Eicken. </author> <title> Distributed probabilistic load information management in sprite. Term project, </title> <institution> Computer Science 262, University of Califor-nia, Berkeley, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: However, substantial changes in the load of a host still take several seconds to propagate to other hosts, and in the meantime hosts with obsolete load information may attempt to migrate onto a loaded host. Stolcke and von Eicken <ref> [SvE89] </ref> built a distributed host selection facility for Sprite based 6.3. HOST SELECTION MECHANISM 73 on the original MOSIX model. They then compared their distributed probabilistic facility with the shared file approach that was used in Sprite at the time.
Reference: [The86] <author> M. Theimer. </author> <title> Preemptable Remote Execution Facilities for Loosely-Coupled Distributed Systems. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1986. </year>
Reference-contexts: For example, the V System restricts process migration to programs whose output does not depend upon their location <ref> [The86] </ref>, and Remote UNIX migrates only single-process noninteractive jobs that are not permitted to communicate with other processes [Lit87]. Ideally, any process should be able to migrate at any time without affecting its results or the results of other processes. Process migration should be automatic in two respects. <p> The relative advantages and disadvantages of centralized and distributed techniques 1.4. THESIS OVERVIEW 5 for finding idle hosts, with respect to simplicity, scalability, performance, and fault tolerance, have been discussed in the literature with no clear-cut resolution (e.g., <ref> [BSW89, TL88, The86] </ref>). I examine a range of approaches to this problem and conclude that a centralized approach has advantages over distributed approaches in nearly all aspects. A fourth contribution, and perhaps the most important one, is a demonstration of the effectiveness of process migration in a production environment. <p> In the following sections I describe how existing systems provide transparency, and what special measures they take (if any) to improve the performance or general usability of migration. V System Theimer implemented process migration in V <ref> [The86, TLC85] </ref>, a message-passing system, with an emphasis on reducing the time during which a process is moving between hosts and unable to execute. <p> To summarize, the V System allows a process to continue executing on the source host while its address space is transferred to the target <ref> [The86, TLC85] </ref>, while Accent transfers a process and retrieves its memory image as the memory is referenced [Zay87a, Zay87b]. In V, pre-copying reduces freeze times substantially, but the need to copy pages multiple times can increase the total amount of work to migrate a process. <p> As a result, migration has been one of the most fragile parts of the Sprite kernel. It often breaks when seemingly unrelated parts of the kernel are modified. I believe that the problem is inherent in the nature of migration (Theimer had similar problems in his implementation <ref> [The86] </ref>), but I have used two techniques to lessen the difficulties. 34 CHAPTER 4. PROCESS MIGRATION MECHANISM My first approach to the problem of migration fragility was to introduce migration version numbers . <p> Third, Section 6.3.3 considers techniques that distribute host information across multiple hosts to avoid processor contention and increase reliability. Finally, Section 6.3.4 describes a stateless mechanism that uses multicast to send requests for hosts, and then selects from those hosts that respond first <ref> [The86, TL88] </ref>. Table 6.2 summarizes the characteristics of each architecture. 6.3.1 Shared File If host selection is performed using a shared file, then the information about host availability is centralized but selection decisions are distributed.
Reference: [TL88] <author> M. Theimer and K. Lantz. </author> <title> Finding idle machines in a workstation-based distributed system. </title> <booktitle> In Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 112-122, </pages> <address> San Jose, CA, </address> <month> June </month> <year> 1988. </year> <note> IEEE. </note>
Reference-contexts: The relative advantages and disadvantages of centralized and distributed techniques 1.4. THESIS OVERVIEW 5 for finding idle hosts, with respect to simplicity, scalability, performance, and fault tolerance, have been discussed in the literature with no clear-cut resolution (e.g., <ref> [BSW89, TL88, The86] </ref>). I examine a range of approaches to this problem and conclude that a centralized approach has advantages over distributed approaches in nearly all aspects. A fourth contribution, and perhaps the most important one, is a demonstration of the effectiveness of process migration in a production environment. <p> Idle hosts are especially common in academic environments: * Mutka and Livny reported that on average, 70% of workstations in their environment were idle [ML87]. * Theimer and Lantz reported that one-third of the 70 workstations in a particular cluster were completely idle, even during the day <ref> [TL88] </ref>. * In the Butler system, 50-70 out of 350 workstations were typically in the free pool during the day, with over 100 available machines at night [Nic87]. * My own results, presented in Chapter 8, indicate that 65-70% of hosts in Sprite are idle on average during the day, with <p> There are a number of possible criteria for the design of a host selection facility. For example, Theimer and Lantz listed three principal goals: performance, scalability, and fault tolerance <ref> [TL88] </ref>. The performance of host selection should be sufficient to impose a minimal impact on processes not using the facility and low latency on scheduling processes that do use the facility. By their definition of scalability, a host selection facility should be able to support hundreds of hosts. <p> Theimer and Lantz compared implementations of a centralized server and a distributed request-response protocol using multicast (which permits one message to be sent to many recipients without an undue burden on hosts not receiving the message) <ref> [TL88] </ref>. They found that a centralized server can service thousands of clients efficiently if status update messages are limited to idle hosts, whereas a distributed implementation using broadcast or multicast results in a high demand for network bandwidth that would limit a system to a few hundred hosts. <p> Third, Section 6.3.3 considers techniques that distribute host information across multiple hosts to avoid processor contention and increase reliability. Finally, Section 6.3.4 describes a stateless mechanism that uses multicast to send requests for hosts, and then selects from those hosts that respond first <ref> [The86, TL88] </ref>. Table 6.2 summarizes the characteristics of each architecture. 6.3.1 Shared File If host selection is performed using a shared file, then the information about host availability is centralized but selection decisions are distributed. <p> A central server is similarly vulnerable to the failure of the host on which the process runs, and it is also vulnerable to software faults in the server process itself. Theimer and Lantz noted that there are two reasonable alternatives for making host selection tolerate failures <ref> [TL88] </ref>. The facility can be replicated, so that one failure does not disable the facility, or the facility can be restarted as soon as its failure is detected. Replication requires additional complexity and run-time overhead, and is discussed below in the context of distributed servers. <p> Theimer and Lantz note that this cost greatly detracts from the desirability of having all hosts perform their own scheduling decisions, since even busy hosts must pay the cost of processing incoming update messages <ref> [TL88] </ref>. Two variations on the distributed approach address these problems. With probabilistic host selection, each host periodically updates state on a random subset of other hosts. This method, used in MOSIX [BSW89, BS85], reduces overhead but still suffers from distributed state. <p> Theimer and Lantz estimated that a decentralized facility based on this design scales to at most a few hundred hosts, while a centralized architecture using stateless communication could handle thousands of hosts <ref> [TL88] </ref>. A second problem with the querying approach is that there is no global information about the state of the system or previous host assignments. <p> Some workstations are available for public use and are not used on a regular basis. However, after discounting for extra workstations, I still find a sizable fraction of hosts available, concurring with Theimer <ref> [TL88] </ref>, Nichols [Nic87], and others.
Reference: [TLC85] <author> M. Theimer, K. Lantz, and D. Cheriton. </author> <title> Preemptable remote execution facilities for the V-System. </title> <booktitle> In Proceedings of the 10th Symposium on Operating System Principles, </booktitle> <pages> pages 2-12, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: In the following sections I describe how existing systems provide transparency, and what special measures they take (if any) to improve the performance or general usability of migration. V System Theimer implemented process migration in V <ref> [The86, TLC85] </ref>, a message-passing system, with an emphasis on reducing the time during which a process is moving between hosts and unable to execute. <p> To summarize, the V System allows a process to continue executing on the source host while its address space is transferred to the target <ref> [The86, TLC85] </ref>, while Accent transfers a process and retrieves its memory image as the memory is referenced [Zay87a, Zay87b]. In V, pre-copying reduces freeze times substantially, but the need to copy pages multiple times can increase the total amount of work to migrate a process.
Reference: [Wel86] <author> B. B. Welch. </author> <title> The Sprite remote procedure call system. </title> <type> Technical Report UCB/CSD 86/302, </type> <institution> Computer Science Division, EECS Department, University of California, Berkeley, </institution> <month> June </month> <year> 1986. </year>
Reference-contexts: Sprite's kernel-call interface is much like that of 4.3 BSD UNIX [Com86], but Sprite's implementation is a new one that provides a high degree of network integration. Each host runs a distinct copy of the Sprite kernel, but the kernels work closely together using a remote-procedure-call (RPC) mechanism <ref> [Wel86] </ref> similar to that described by Birrell and Nelson [BN84]. All the hosts on the network share a common high-performance file system [Nel88, Wel90].
Reference: [Wel90] <author> B. B. Welch. </author> <title> Naming, State Management, and User-Level Extensions in the Sprite Distributed File System. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA 94720, </address> <month> February </month> <year> 1990. </year> <note> Available as Technical Report UCB/CSD 90/567. </note>
Reference-contexts: Each host runs a distinct copy of the Sprite kernel, but the kernels work closely together using a remote-procedure-call (RPC) mechanism [Wel86] similar to that described by Birrell and Nelson [BN84]. All the hosts on the network share a common high-performance file system <ref> [Nel88, Wel90] </ref>. Sprite permits the data of a file to be cached in the memory of one or more machines, with file servers responsible for guaranteeing "consistent access" to the cached 22 3.2. SPRITE 23 data. <p> In turn, an I/O handle includes a count of the streams referring to it, the mode in which the file is accessed (reading, writing, and so on), and an identifier for a particular file on a particular I/O server <ref> [Wel90] </ref>. This hierarchy is depicted in Figure 5.3. 46 CHAPTER 5. INTERACTION WITH THE FILE SYSTEM point to the I/O server, which stores its own copy of the I/O handle. <p> He addressed the problem of shared access positions, and he implemented special code to support migrating streams atomically, so that the state maintained by the I/O server for crash recovery and cacheability could be kept consistent. In his thesis <ref> [Wel90] </ref>, Welch describes the current implementation of the Sprite file system and its 5.3. TRANSFERRING OPEN FILES 49 support for migration. In the remainder of this section, I highlight the important aspects of this implementation as it relates to migration. <p> FUTURE WORK 107 Increasing the scale of the system has ramifications beyond those relating to load sharing. For example, in his thesis, Welch discussed the issue of Sprite file servers handling ten times as many clients as they currently do <ref> [Wel90] </ref>. It would be edifying to expand Sprite to handle many more machines, and to evaluate how the file system and host selection facility are stressed.
Reference: [WM89] <author> B. J. Walker and R. M. Mathews. </author> <title> Process migration in AIX's transparent computing facility (TCF). </title> <journal> IEEE Computer Society Technical Committee on Operating Systems Newsletter, </journal> <volume> 3(1) </volume> <pages> 5-7, </pages> <month> Winter </month> <year> 1989. </year>
Reference-contexts: For example, they appear in the listing of active processes on that machine, and they will obtain host information for that machine rather than their origin site. The AIX Transparent Computing Facility <ref> [WM89] </ref>, which derived from LOCUS, supplies some programs to help users select machines for remote execution. It allows users to specify that programs should be run on lightly-loaded hosts, or for daemons to migrate processes for the purposes of smoothing the load across multiple hosts.
Reference: [WO88] <author> B. B. Welch and J. K. Ousterhout. </author> <title> Pseudo devices: User-level extensions to the Sprite file system. </title> <booktitle> In USENIX 1988 Summer Conference, </booktitle> <pages> pages 37-49, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1988. </year> <note> 118 BIBLIOGRAPHY </note>
Reference-contexts: As I discuss in the next chapter, paging via the file system simplifies migration because the functionality to demand-page a process over the network already exists. Sprite's interprocess communication paradigm also simplifies migration considerably. Processes communicate using a special type of file known as a pseudo-device <ref> [WO88] </ref>. The migration of a process is transparent to the processes with which it communicates, because only the operating system stores the location of the processes that use the pseudo-device. <p> The process initiates communication with the central server. In Sprite, this operation is performed using the file system: the process opens a special file, known as a pseudo-device <ref> [WO88] </ref>, and the operating system passes the open operation on to the server. Communication using a reliable stream protocol, such as TCP/IP, is another possibility. 2. The process requests one or more hosts.
Reference: [Zay87a] <author> E. Zayas. </author> <title> Attacking the process migration bottleneck. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 13-22, </pages> <address> Austin, TX, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: On the other hand, Zayas has shown that a process with a large address space can migrate away from a host much faster if it leaves its virtual memory pages on the source host and retrieves them later on a demand basis <ref> [Zay87a] </ref>. In this case, improving performance also increases the migrating process's residual dependencies on the source. If the host with the process's memory image later fails at any time during the process's lifetime, the process might be unable to execute. <p> Accent Dannenberg's Butler was a prototype remote execution facility that used process migration to transfer processes between hosts [Dan82]. In this version, virtual memory was transferred monolithically. Zayas modified Accent to migrate processes using copy-on-reference virtual memory <ref> [Zay87a, Zay87b] </ref>. When a process migrates, its memory image is initially left on the source machine; only the process's page tables, registers, and message channels need be transferred immediately. As the process executes, it demand-pages its memory from the source. <p> To summarize, the V System allows a process to continue executing on the source host while its address space is transferred to the target [The86, TLC85], while Accent transfers a process and retrieves its memory image as the memory is referenced <ref> [Zay87a, Zay87b] </ref>. In V, pre-copying reduces freeze times substantially, but the need to copy pages multiple times can increase the total amount of work to migrate a process.
Reference: [Zay87b] <author> E. Zayas. </author> <title> The Use of Copy-On-Reference in a Process Migration System. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1987. </year> <note> Report No. CMU-CS-87-121. </note>
Reference-contexts: Accent Dannenberg's Butler was a prototype remote execution facility that used process migration to transfer processes between hosts [Dan82]. In this version, virtual memory was transferred monolithically. Zayas modified Accent to migrate processes using copy-on-reference virtual memory <ref> [Zay87a, Zay87b] </ref>. When a process migrates, its memory image is initially left on the source machine; only the process's page tables, registers, and message channels need be transferred immediately. As the process executes, it demand-pages its memory from the source. <p> for each component consists of some combination of transferring state and arranging for forwarding. 4.2.1 Virtual Memory Transfer Virtual memory transfer is the aspect of migration that has been discussed the most in the literature, perhaps because it is believed to be the limiting factor in the speed of migration <ref> [Zay87b] </ref>. One simple method for transferring virtual memory is to send the process's entire memory image to the target machine at migration time, as in Charlotte [AF89] and LOCUS [PW85]. This approach is simple but it has two disadvantages. <p> To summarize, the V System allows a process to continue executing on the source host while its address space is transferred to the target [The86, TLC85], while Accent transfers a process and retrieves its memory image as the memory is referenced <ref> [Zay87a, Zay87b] </ref>. In V, pre-copying reduces freeze times substantially, but the need to copy pages multiple times can increase the total amount of work to migrate a process. <p> As will be seen below, Sprite achieves transparency by involving the home machine in some operations for remote processes. 4.3.1 Messages Versus Kernel Calls On the surface, it might appear that transparency is particularly easy to achieve in a message-based system like Accent <ref> [Zay87b] </ref>, Charlotte [AF89], or V [Che88]. In these systems all of a process's interactions with the rest of the world occur in a uniform fashion 36 CHAPTER 4. PROCESS MIGRATION MECHANISM through message channels.
Reference: [Zho87] <author> S. Zhou. </author> <title> Performance Studies of Dynamic Load Balancing in Distributed Systems. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> October </month> <year> 1987. </year> <note> Technical Report No. UCB/CSD 87/376. </note>
Reference-contexts: For example, Zhou traced a VAX-11/780 4.3 BSD UNIX system and found that the mean process execution time was 1.5 seconds with a standard deviation of 19.1 seconds <ref> [Zho87] </ref>. We presumed that the same pattern would hold in Sprite. Under this assumption, migrating active processes will provide significant performance improvements only if the overhead of migration is extremely low (a few hundred milliseconds at most) or migration is limited to processes that are known to be long-running. <p> For example, Zhou compared several algorithms for load balancing, including a centralized server, a broadcast-based distributed system, and a hybrid system in which each host sends its load to a central coordinator, which periodically broadcasts the load of every host in a single message <ref> [Zho87] </ref>. Theimer and Lantz compared implementations of a centralized server and a distributed request-response protocol using multicast (which permits one message to be sent to many recipients without an undue burden on hosts not receiving the message) [TL88].
References-found: 51

