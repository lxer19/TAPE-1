URL: http://www.cs.umn.edu/Research/Agassiz/Paper/poulsen.icpp94.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Poulsen, D. K., and Yew, P.-C., </author> <title> Data Prefetching and Data Forwarding in Shared Memory Multiprocessors, </title> <institution> University of Illinois, </institution> <type> CSRD Report 1330, </type> <month> April </month> <year> 1994. </year>
Reference: [2] <author> Callahan, D., Kennedy, K., and Porterfield, A., </author> <title> "Software Prefetching", </title> <booktitle> Proceedings of ASPLOS IV, </booktitle> <pages> pp. 40-52. </pages>
Reference-contexts: INTRODUCTION Memory latency has an important effect on shared memory multiprocessor performance. Communication between processors distinguishes the multiprocessor memory latency problem from the uniprocessor case. Uniprocessor caches are used to hide memory latency by exploiting spatial and temporal locality, lowering cache miss ratios. Data prefetching <ref> [2, 3] </ref> has been used successfully to enhance uniprocessor cache performance. In cache coherent multiprocessors, sharing misses occur in addition to uniprocessor nonsharing misses. Sharing accesses cause communication between processors when cache misses cause shared blocks to be brought to requesting caches from other caches. <p> Many different prefetching architectures and algorithms have been described in the literature. This paper focuses on software-initiated non-binding prefetching into cache <ref> [2, 3] </ref>. In these schemes, explicit prefetching instructions are inserted into application codes; prefetched cache blocks are still exposed to the cache replacement policy and coherence protocol.
Reference: [3] <author> Mowry, T. C., Lam, M. S., and Gupta, A., </author> <title> "Design and Evaluation of a Compiler Algorithm for Prefetching", </title> <booktitle> Proceedings of ASPLOS V, </booktitle> <pages> pp. 62-73. </pages>
Reference-contexts: INTRODUCTION Memory latency has an important effect on shared memory multiprocessor performance. Communication between processors distinguishes the multiprocessor memory latency problem from the uniprocessor case. Uniprocessor caches are used to hide memory latency by exploiting spatial and temporal locality, lowering cache miss ratios. Data prefetching <ref> [2, 3] </ref> has been used successfully to enhance uniprocessor cache performance. In cache coherent multiprocessors, sharing misses occur in addition to uniprocessor nonsharing misses. Sharing accesses cause communication between processors when cache misses cause shared blocks to be brought to requesting caches from other caches. <p> Many different prefetching architectures and algorithms have been described in the literature. This paper focuses on software-initiated non-binding prefetching into cache <ref> [2, 3] </ref>. In these schemes, explicit prefetching instructions are inserted into application codes; prefetched cache blocks are still exposed to the cache replacement policy and coherence protocol. <p> Data Prefetching Prefetching is performed using either a blocked vector algorithm that specifically targets vector statements and vectorizable loops, or a software pipelined algorithm that extends techniques described in <ref> [3] </ref> to support serial, parallel, and vectorizable loops. The blocked vector algorithm offers the advantages of simplicity of implementation and low instruction overhead, whereas the software pipelined algorithm allows longer memory latencies to be hidden more effectively, especially for non-vectorizable loops. <p> Round-robin scheduling and processor self-scheduling of parallel loop iterations are both supported. The algorithm uses loop splitting and software pipelining transformations, estimates loop execution times, computes prefetch distances, and schedules prefetching in a manner similar to that of <ref> [3] </ref>.
Reference: [4] <author> Tullsen, D. M., and Eggers, S. J., </author> <title> "Limitations of Cache Prefetching on a Bus-Based Multiprocessor", </title> <booktitle> Proceedings of ISCA 1993, </booktitle> <pages> pp. 278-288. </pages>
Reference-contexts: Each processor has a 64K word, lockup-free, 4-way set associative, write-back cache with single word cache blocks. A large cache size is used to highlight communication effects while de-emphasizing cache conflict behavior. Single word cache blocks are used to minimize false sharing effects caused by prefetching <ref> [4] </ref>. Cache coherence is implemented using a three state, directory-based invalidation protocol. Simulations model network contention and queuing delay as well as traffic due to coherence transactions. The particular application codes studied are listed in Figure 2 along with the number of events (equivalent to trace size) simulated. <p> 1.2 1.0 1.0 1.0 0.19 0.10 0.29 0.56 1.0 0.39 base pf-v pf-s fwd opt TRFD base pf-v pf-s fwd opt QCD base pf-v pf-s fwd opt FLO52 base pf-v pf-s fwd opt ARC2D base pf-v pf-s fwd opt DYFESM were or were not prefetched, and prefetch in progress misses <ref> [4] </ref>. The latter represents misses where prefetched data had not yet arrived at a processor from main memory. Prefetch in progress misses incur less latency than other misses depending on the distance between the prefetch and the miss [4]. code 0 1 &lt;=4 clus. mc/bc TRFD 66.4% 1.1% 32.5% 0.0% 0.0% <p> opt DYFESM were or were not prefetched, and prefetch in progress misses <ref> [4] </ref>. The latter represents misses where prefetched data had not yet arrived at a processor from main memory. Prefetch in progress misses incur less latency than other misses depending on the distance between the prefetch and the miss [4]. code 0 1 &lt;=4 clus. mc/bc TRFD 66.4% 1.1% 32.5% 0.0% 0.0% QCD 96.4% 0.0% 1.8% 1.1% 0.7% ARC2D 67.9% 10.2% 21.2% 0.5% 0.2% DYFESM 16.0% 37.3% 13.5% 32.6% 0.6% The following sections discuss the results for each code in more detail. <p> FWD was more effective in attacking invalidation misses and resulted in a lower overall miss ratio. Miss ratios under prefetching were aggravated by prefetch in progress misses. Bandwidth Requirements The availability of sufficient bandwidth is important to the performance of prefetching <ref> [4] </ref> and forwarding. Figure 6 gives the memory and network statistics for the experiments described above. For each code and scheme, the increase in memory requests (MSGS), the average main memory access latency (L AVG), and the average network utilization (U AVG) are given. <p> Reductions in miss ratios were offset by increases in prefetch in progress misses in TRFD, QCD, and DYFESM. Prefetching was effective in attacking both conflict (FLO52, ARC2D) and invalidation misses. Prefetching was found to be less effective in attacking invalidation misses in <ref> [4] </ref>; however, false sharing was shown to be a contributor to this effect. A small cache block size was used in the experiments described above to eliminate false sharing and increase prefetching effectiveness for invalidation misses.
Reference: [5] <author> Andrews, J. B., Beckmann, C. J., and Poulsen, D. K., </author> <title> "Notification and Multicast Networks for Synchronization and Coherence", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4), </volume> <month> August </month> <year> 1992, </year> <pages> pp. 332-350. </pages>
Reference-contexts: Sharing accesses cause communication between processors when cache misses cause shared blocks to be brought to requesting caches from other caches. Data prefetching has the potential to hide memory latency for both sharing and nonsharing accesses; however, data forwarding <ref> [5] </ref> may be a more effective technique than prefetching for reducing the latency of sharing accesses. Many different prefetching architectures and algorithms have been described in the literature. This paper focuses on software-initiated non-binding prefetching into cache [2, 3].
Reference: [6] <author> Mowry, T. C., and Gupta, A., </author> <title> "Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2), </volume> <month> June </month> <year> 1991, </year> <pages> pp. 87-106. </pages>
Reference-contexts: In these schemes, explicit prefetching instructions are inserted into application codes; prefetched cache blocks are still exposed to the cache replacement policy and coherence protocol. Although data prefetching has been shown to be effective in reducing memory latency in shared memory multiprocessors <ref> [6, 7] </ref>, few multiprocessor studies have considered the implementation of compiler algorithms for multiprocessor prefetching [8] and the performance impact of these algorithms on large, numerical applications with loop-level and vector parallelism.
Reference: [7] <author> Kuck, D., et. al., </author> <title> "The Cedar System and an Initial Performance Study", </title> <booktitle> Proceedings of ISCA 1993, </booktitle> <pages> pp. 213-223. </pages>
Reference-contexts: In these schemes, explicit prefetching instructions are inserted into application codes; prefetched cache blocks are still exposed to the cache replacement policy and coherence protocol. Although data prefetching has been shown to be effective in reducing memory latency in shared memory multiprocessors <ref> [6, 7] </ref>, few multiprocessor studies have considered the implementation of compiler algorithms for multiprocessor prefetching [8] and the performance impact of these algorithms on large, numerical applications with loop-level and vector parallelism.
Reference: [8] <author> Mowry, T. C., </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching, </title> <type> Ph.D Thesis, </type> <institution> Stanford University, Dept. of Elec. Engr., </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Although data prefetching has been shown to be effective in reducing memory latency in shared memory multiprocessors [6, 7], few multiprocessor studies have considered the implementation of compiler algorithms for multiprocessor prefetching <ref> [8] </ref> and the performance impact of these algorithms on large, numerical applications with loop-level and vector parallelism. Data forwarding is a technique for integrating fine-grained (*) This work was supported in part by the National Science Foundation under grant nos.
Reference: [9] <author> Lenoski, D., et. al., </author> <title> "The Stanford Dash Multiprocessor", </title> <journal> IEEE Computer, </journal> <volume> 25(3), </volume> <month> March </month> <year> 1992, </year> <pages> pp. 63-79. </pages>
Reference-contexts: This paper considers the use of data forwarding for optimizing sharing accesses. The forwarding mechanisms considered are software-initiated non-binding mechanisms that perform sender-initiated forwarding of data to other processors' caches. One example of this type of mechanism is the Dash Deliver instruction <ref> [9] </ref>, an operation that transmits a copy of a cache block to explicitly specified cluster caches. PREFETCHING AND FORWARDING SCHEMES Data prefetching is used to reduce the memory latency of both sharing and nonsharing accesses, while data forwarding is used to reduce latency exclusively for sharing accesses. <p> A Forwarding Write is a single instruction that combines a write and a sender-initiated forwarding operation, where the semantics of the forwarding operation are similar to those of the Dash Deliver instruction <ref> [9] </ref>. Destination processors are specified via a bit vector; data are forwarded to the caches of each specified processor. Using Forwarding Write operations to combine writes and forwarding operations allows instruction overhead to be reduced, providing forwarding with a potential performance advantage over prefetching.
Reference: [10] <author> Poulsen, D. K., and Yew, P.-C., </author> <title> "Execution-Driven Tools for Parallel Simulation of Parallel Architectures and Applications", </title> <booktitle> Proceedings of Supercomputing 1993, </booktitle> <pages> pp. 860-869. </pages>
Reference-contexts: Unique forwarding information is produced for each distinct source statement that writes each array; each execution of a particular statement uses the same forwarding information. EXPERIMENTAL FRAMEWORK Experimental results are acquired through execution-driven simulations of parallel application codes using EPG-sim <ref> [10] </ref>. Simulations are driven by parallel versions of Perfect Benchmarks (R) codes [11]. These parallel Cedar Fortran [12] codes express parallelism using parallel loops and vector statements. Prefetching algorithms and instrumentation for profile-based forwarding are implemented in the Parafrase-2 parallelizing compiler [13].
Reference: [11] <author> Berry, M., et. al., </author> <title> "The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers", </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3), </volume> <month> Fall </month> <year> 1989, </year> <pages> pp. 5-40. </pages>
Reference-contexts: EXPERIMENTAL FRAMEWORK Experimental results are acquired through execution-driven simulations of parallel application codes using EPG-sim [10]. Simulations are driven by parallel versions of Perfect Benchmarks (R) codes <ref> [11] </ref>. These parallel Cedar Fortran [12] codes express parallelism using parallel loops and vector statements. Prefetching algorithms and instrumentation for profile-based forwarding are implemented in the Parafrase-2 parallelizing compiler [13].
Reference: [12] <author> Padua, D., Hoeflinger, J., Jaxon, G., and Eigenmann, R., </author> <title> The Cedar Fortran Project, </title> <institution> University of Illinois, </institution> <type> CSRD Report 1262, </type> <month> October </month> <year> 1992. </year>
Reference-contexts: EXPERIMENTAL FRAMEWORK Experimental results are acquired through execution-driven simulations of parallel application codes using EPG-sim [10]. Simulations are driven by parallel versions of Perfect Benchmarks (R) codes [11]. These parallel Cedar Fortran <ref> [12] </ref> codes express parallelism using parallel loops and vector statements. Prefetching algorithms and instrumentation for profile-based forwarding are implemented in the Parafrase-2 parallelizing compiler [13]. The modeled system is a 32 processor, cache coherent, distributed shared memory architecture with processor / cache / memory nodes connected via multistage networks. <p> The codes were parallelized using an optimizing compiler and then hand-optimized to exploit available parallelism, increase locality, and reduce memory latency <ref> [12] </ref>.
Reference: [13] <author> Polychronopoulos, C. D., et. al., </author> <title> "Parafrase-2: An Environment for Parallelizing, Partitioning, Synchronizing and Scheduling Programs on Multiprocessors", </title> <booktitle> Proceedings of ICPP 1989, </booktitle> <pages> pp. </pages> <month> II-39-48. </month>
Reference-contexts: Simulations are driven by parallel versions of Perfect Benchmarks (R) codes [11]. These parallel Cedar Fortran [12] codes express parallelism using parallel loops and vector statements. Prefetching algorithms and instrumentation for profile-based forwarding are implemented in the Parafrase-2 parallelizing compiler <ref> [13] </ref>. The modeled system is a 32 processor, cache coherent, distributed shared memory architecture with processor / cache / memory nodes connected via multistage networks. Uniform memory access (UMA) to main memory is assumed to simplify simulations.
References-found: 13

