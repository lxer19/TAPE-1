URL: ftp://farside.nswc.navy.mil/pub/bwallet/papers/regressionga.ps.Z
Refering-URL: http://www.cs.gmu.edu:80/research/gag/pubs.html
Root-URL: 
Date: 1996.  
Note: To appear in the Proceedings of the 28th Symposium on the Interface,  
Abstract: Given a data set consisting of a large number of predictors plus a response, the problem addressed in this work is to select a minimal model which correctly predicts the response. Methods for achieving this subsetting of the predictors have been the topic of a considerable amount of study within the statistics community. Unfortunately, current methods often fail when the predictors are highly correlated. Furthermore, because of the exponential growth of the number of possible subsets as the number of candidate predictors increase, current methods have great difficulty handling high dimensional data sets. This paper details a method for variable selection using genetic algorithms. A genetic algorithm is described which uses a unique two criteria population management scheme. This method is explorative in nature, and allows for an approximation of the all possible subsets method over a set of interesting model sizes. Results of an application of this method to data are discussed.
Abstract-found: 1
Intro-found: 1
Reference: <author> S. Chatterjee and M Laudato, </author> <booktitle> Genetic algorithms and their statistical applications presented at the 155th meeting of the American Statistical Association, </booktitle> <year> 1995. </year>
Reference: <author> K.A. De Jong, </author> <type> personal conversation, </type> <year> 1995. </year>
Reference-contexts: Wegman 2 1 Naval Surface Warfare Center Systems Research and Technology Department Dahlgren, Virginia 22448 2 George Mason University Center for Computational Statistics Fairfax, Virginia 22030 To appear in the Proceedings of the 28th Symposium on the Interface, 1996. while not enough focus had been placed upon their explorative nature <ref> (De Jong, 1995) </ref>. Since many of the techniques for doing regression are interactive in nature, it seems that perhaps an explorative genetic algorithm might be more appropriate. As was previously stated, the fit of a model cannot be decreased by adding an additional term.
Reference: <author> N.R. Draper, H. Smith, </author> <title> Applied Regression Analysis. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1981. </year>
Reference: <author> C.L. Mallows, </author> <title> Some comments on C P Technometrics, </title> <journal> Vol. </journal> <volume> 15, No. 4, </volume> <pages> pp 661-675, </pages> <month> November </month> <year> 1973. </year>
Reference-contexts: Therefore, it would seem obvious that it is necessary to use some scoring system that penalizes for model size. The classical scoring system for comparing different models of different sizes is Mallows C P <ref> (Mallows, 1973) </ref>. However, our experience has shown that using a canonical genetic algorithm to select a model based simply upon optimizing the C P will lead to less than optimal results.
Reference: <author> C.L. Mallows, </author> <title> More comments on C P Technometrics, </title> <journal> Vol. </journal> <volume> 37, No. 4, </volume> <pages> pp. 362-372, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: This is not a surprising result since previous research has shown that when dealing with a large number of predictors, there are often unfortunate false correlations (Rencher, 1980). Indeed, Mallows recently wrote that simply minimizing the C P is not always a good thing <ref> (Mallows, 1995) </ref>. In a recent discussion, De Jong stated that he felt that too much emphasis within the evolutionary computation community had been placed on hard optimization problems A Genetic Algorithm for Best Subset Selection in Linear Regression Bradley C. Wallet 1 , David J. Marchette 1 , Jeffery L.
Reference: <author> Z. Michalewicz, </author> <title> Genetic Algorithms + Data Structures = Evolution Programs, </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: A through coverage of the field of genetic algorithms and other forms of evolutionary computation can be found in Michalewicz <ref> (Michalewicz, 1994) </ref>. 2.2.1 Selection Schemes The selection from P g-1 to form P g is typically done via a stochastic method.
Reference: <author> A.J. Miller, </author> <title> Subset Selection in Regression, </title> <address> New York: </address> <publisher> Champman and Hall, </publisher> <year> 1990. </year>
Reference-contexts: 1. INTRODUCTION The problem of dimensionality reduction in linear regression has been the topic of a considerable amount of study <ref> (Miller, 1990) </ref>. The all subsets regression method of dimensionality reduction becomes intractable due to the exponential growth of the number of possible subsets. Similarly, stepwise methods fail when there is a significant amount of correlation between the predictors (Berk, 1978). <p> Other techniques, such as ridge regression, do well under some circumstances, but generally break down when the response is strongly correlated with a linear combination of regressor but only weakly correlated with the individual regressors <ref> (Miller, 1990) </ref>. Also, because the number of possible subsets grows expo nentially with the increasing of the number of candidate predictors, no current methods cope well with high dimensional data. <p> A comparison of the various models found for a number of interesting model sizes by the different methods is located in Table 3. In all cases, the models found by the GA can be shown to be optimal via an exhaustive search <ref> (Miller 1990) </ref>.
Reference: <author> A.C. Rencher and F.C. Pun, </author> <title> Ination of R 2 in best subset regression Technometrics, </title> <journal> Vol. </journal> <volume> 22, No. 1, </volume> <pages> pp 49-53, </pages> <month> Febru-ary </month> <year> 1980. </year>
Reference-contexts: This is not a surprising result since previous research has shown that when dealing with a large number of predictors, there are often unfortunate false correlations <ref> (Rencher, 1980) </ref>. Indeed, Mallows recently wrote that simply minimizing the C P is not always a good thing (Mallows, 1995).
Reference: <editor> B. C. Wallet, D. J. Marchette, and J. L. Solka, </editor> <booktitle> A matrix representation for genetic algorithms Proceedings of the 10th Annual International AeroSense Symposium: Object Recog-nition IV, </booktitle> <year> 1996. </year>
Reference-contexts: candidate solutions are usually b' b 0 b 1 b r , , ,( )= R 2 SS regression SS model -= To appear in the Proceedings of the 28th Symposium on the Interface, 1996. encoded as binary bit strings though richer alphabets and richer data structures are sometimes used <ref> (Wallet, Marchette, and Solka, 1996) </ref>. More formally, using an alphabet =-0,1-, a point of consideration, xW, is represented as an l length sting c=(a 1 ...a l ) where a i . A bit string, cC, is called a chromosome.
Reference: <author> K. R. William, </author> <title> Designed Experiments Rubber Age, </title> <journal> Vol. </journal> <volume> 100, </volume> <pages> pp. 65-71, </pages> <year> 1968. </year>
References-found: 10

