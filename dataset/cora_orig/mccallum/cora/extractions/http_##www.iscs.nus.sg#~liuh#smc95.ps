URL: http://www.iscs.nus.sg/~liuh/smc95.ps
Refering-URL: 
Root-URL: 
Email: fliuh,tanstg@iscs.nus.sg  
Title: X2R: A Fast Rule Generator  
Author: Huan Liu and Sun Teck Tan 
Address: Ridge, Singapore 0511  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Note: To appear in Proceedings of IEEE International Conference on Systems, Man and Cybernetics, Oct., 1995, Vancouver, Canada.  
Abstract: Although they can learn from raw data, many concept learning algorithms require that the training data contain only discrete data. However, real world problems contain, more often than not, both numeric and discrete data. So before these algorithms can be applied, data dis-cretization (quantization) is needed. This paper introduces X2R, a simple and fast algorithm that can be applied to both numeric and discrete data, and generate rules from datasets like Season-Classification, Golf-Playing that contain continuous and/or discrete data. The empirical results demonstrate that X2R can effectively generate rules from the raw data and perform better than some of its peers in terms of the quality of rules and time complexities. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Kerber. Chimerge: </author> <title> Discretization of numeric attributes. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 123-128. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: A common practice is to use equal-width-intervals or equal-frequency-intervals. It has been pointed out that although these obvious simple methods sometimes work well, they have problems <ref> [1] </ref>. Another problem in concept learning is of relevant features [8]. Some attributes are not useful at all in retrospection. Because lack of knowledge about an application domain, people tend to collect data along all the dimensions they consider probably useful.
Reference: [2] <author> H. Liu and R. Setiono. </author> <title> Discretization of ordinal attributes and feature selection. </title> <type> Technical report, </type> <institution> Department of Info Sys and Comp Sci, National University of Singapore, </institution> <year> 1995. </year>
Reference-contexts: The rules are a summary of the raw data, and the explicitness of rules allows human experts to judge, to modify, and to expand the rules. This paper is organized as follows: the following section describes the rule generation algorithm, X2R, with a brief introduction of Chi2 <ref> [2] </ref> that performs discretization and feature selection on numeric data. Section 3 is the related work which will be used in Section 4 for the purpose of comparison. Section 4 presents the empirical results and comparisons with some similar systems. <p> Section 5 concludes our work and proposes some further research issues. 2 X2R Algorithm Since X2R performs best in presence of numeric data when it is running with Chi2, we give a brief introduction on Chi2 first. The Chi2 algorithm is a discretization algo 1 rithm <ref> [2] </ref> which is based on the 2 statistic. Chi2 begins with a high significance level, e.g., 0.5, for all numeric attributes of discretization. Each attribute i is associated with a sigLvl [i], and takes turns for merging. Each attribute is sorted according to its values.
Reference: [3] <author> H. Lu, R. Setiono, and H. Liu. Neurorule: </author> <title> A connectionist approach to data mining. </title> <note> In submission to VLDB'95, </note> <year> 1995. </year>
Reference-contexts: Due to their advantages and disadvantages, both types of rules should find their applications in various circumstances. X2R has been successfully used as an auxiliary tool in several applications such as neural networks understanding [7], data mining <ref> [3] </ref>. It has been proven to be effective. What should be further investigated is the possibility to extend it and then directly apply it to large datasets for concept learning.
Reference: [4] <author> D.T. Pham and M.S. Aksoy. </author> <title> An algorithm for automatic rule induction. </title> <journal> Artificial Intelligence in Engineering, </journal> <volume> 8, </volume> <year> 1994. </year>
Reference-contexts: Two rule extraction systems, RULES and RULES-2, are selected since they were recently reported in <ref> [5, 4] </ref>. Brief descriptions of the two systems are given below. RULES may require at most n a iterations, where n a is the number of attributes. The first iteration produces rules with one condition, and the second iteration results in rules with two conditions, etc. <p> RULES-2 is an improved version of RULES. The difference is that instead of considering the values of all unclassified examples, in each iteration, only the values of one unclassified example are used to produce rules for classifying that example. Many sample datasets are presented in <ref> [5, 4] </ref>. For the comparing purpose, two datasets are chosen since the details of the rules generated from the two sets have been given in [5, 4], such as the conditions of rules, number of rules, etc. They have also done some analysis on comparing their methods with ID3 [6]. <p> Many sample datasets are presented in <ref> [5, 4] </ref>. For the comparing purpose, two datasets are chosen since the details of the rules generated from the two sets have been given in [5, 4], such as the conditions of rules, number of rules, etc. They have also done some analysis on comparing their methods with ID3 [6]. The two datasets are Season-Classification and Golf-Playing. The Season-Classification dataset contains discrete data only. The Golf-Playing dataset dataset contains both numeric and discrete data. 1. <p> Rewriting the three rules in their original terms, they are: Rule 1: If Outlook = sunny & Humid. 85 then don't play Rule 2: If Outlook = rainy & Wind = strong then don't play Default Rule: play. As reported in <ref> [4] </ref>, RULES-2 produced 8 and 14 rules for options 1 and 2. About half of these rules involve two attributes. Discussion The rules generated by X2R are shorter and the number of rules is smaller. But they achieve 100% accuracy.
Reference: [5] <author> D.T. Pham and M.S. Aksoy. </author> <title> Rules: A simple rule extraction system. </title> <journal> Expert Systems With Applications, </journal> <volume> 8(1), </volume> <year> 1995. </year>
Reference-contexts: Two rule extraction systems, RULES and RULES-2, are selected since they were recently reported in <ref> [5, 4] </ref>. Brief descriptions of the two systems are given below. RULES may require at most n a iterations, where n a is the number of attributes. The first iteration produces rules with one condition, and the second iteration results in rules with two conditions, etc. <p> RULES-2 is an improved version of RULES. The difference is that instead of considering the values of all unclassified examples, in each iteration, only the values of one unclassified example are used to produce rules for classifying that example. Many sample datasets are presented in <ref> [5, 4] </ref>. For the comparing purpose, two datasets are chosen since the details of the rules generated from the two sets have been given in [5, 4], such as the conditions of rules, number of rules, etc. They have also done some analysis on comparing their methods with ID3 [6]. <p> Many sample datasets are presented in <ref> [5, 4] </ref>. For the comparing purpose, two datasets are chosen since the details of the rules generated from the two sets have been given in [5, 4], such as the conditions of rules, number of rules, etc. They have also done some analysis on comparing their methods with ID3 [6]. The two datasets are Season-Classification and Golf-Playing. The Season-Classification dataset contains discrete data only. The Golf-Playing dataset dataset contains both numeric and discrete data. 1. <p> As reported in <ref> [5] </ref>, 7 rules are generated by RULES, three of them involve two attributes, the other 4 rules are exactly the same as Rule 0, Rule 1, Rule 2, and Rule 4.
Reference: [6] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: For the comparing purpose, two datasets are chosen since the details of the rules generated from the two sets have been given in [5, 4], such as the conditions of rules, number of rules, etc. They have also done some analysis on comparing their methods with ID3 <ref> [6] </ref>. The two datasets are Season-Classification and Golf-Playing. The Season-Classification dataset contains discrete data only. The Golf-Playing dataset dataset contains both numeric and discrete data. 1.
Reference: [7] <author> R. Setiono and H. Liu. </author> <title> Understanding neural networks via rule extraction. </title> <booktitle> In IJCAI-95, Proceedings International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: It is expected that order-insensitive rules are not as short as order-sensitive ones. Due to their advantages and disadvantages, both types of rules should find their applications in various circumstances. X2R has been successfully used as an auxiliary tool in several applications such as neural networks understanding <ref> [7] </ref>, data mining [3]. It has been proven to be effective. What should be further investigated is the possibility to extend it and then directly apply it to large datasets for concept learning.
Reference: [8] <author> N. Wyse, R. Dubes, and A.K. Jain. </author> <title> A critical evaluation of intrinsic dimensionality algorithms. In E.S. </title> <editor> Gelsema and Kanal L.N., editors, </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <pages> pages 415-425. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1980. </year>
Reference-contexts: A common practice is to use equal-width-intervals or equal-frequency-intervals. It has been pointed out that although these obvious simple methods sometimes work well, they have problems [1]. Another problem in concept learning is of relevant features <ref> [8] </ref>. Some attributes are not useful at all in retrospection. Because lack of knowledge about an application domain, people tend to collect data along all the dimensions they consider probably useful. Feature selection is a task to select the minimum number of attributes needed to represent the data accurately.
References-found: 8

