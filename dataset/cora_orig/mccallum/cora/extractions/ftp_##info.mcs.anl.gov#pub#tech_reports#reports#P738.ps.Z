URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P738.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts99.htm
Root-URL: http://www.mcs.anl.gov
Title: Data Sonification and Sound Visualization  
Author: Hans G. Kaper Elizabeth Wiebel 
Affiliation: Mathematics and Computer Science Division, Argonne National Laboratory Sever Tipei School of Music, University of Illinois  Mathematics and Computer Science Division, Argonne National Laboratory  
Abstract: This article describes a collaborative project between researchers in the Mathematics and Computer Science Division at Argonne National Laboratory and the Computer Music Project of the University of Illinois at Urbana-Champaign. The project focuses on the use of sound for the exploration and analysis of complex data sets in scientific computing. The article addresses digital sound synthesis in the context of DIASS, a Digital Instrument for Additive Sound Synthesis, and sound visualization in a virtual-reality environment by means of M4CAVE. It describes the procedures and preliminary results of some experiments in scientific sonification and sound visualization. While most computational scientists routinely use visual imaging techniques to explore and analyze large data sets, they tend to be much less familiar with the use of sound. Yet, sound signals carry significant amounts of information and can be used advantageously to increase the bandwidth of the human/computer interface. The project described in this article focuses on scientific sonification|the faithful rendering of scientific data in sounds|and the visualization of sounds in a virtual-reality environment. The project, which grew out of an effort to apply the latest supercomputing technology to the process of music composition (see Box 1), is a joint collaboration between Argonne National Laboratory (ANL, Mathematics and Computer Science Division) and the University of Illinois at Urbana-Champaign (UIUC, Computer Music Project). Digital sound synthesis is addressed in Section 1. The discussion is centered around DIASS, a Digital Instrument for Additive Sound Synthesis. Section 2 describes some experiments in scientific sonification. Sound visualization in a virtual-reality (VR) environment is discussed in Section 3. Here, the main tool is M4CAVE, a program to visualize sounds from a score file. Section 4 contains some more general observations about the project. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baecker, R. M., J. Grudin, W. Buxton, and S. Greenberg, </author> <title> Readings in Human-Computer Interaction: </title> <booktitle> Toward the Year 2000, second edition, </booktitle> <publisher> Morgan Kaufmann Publ., Inc., </publisher> <address> San Francisco, </address> <year> 1995 </year>
Reference-contexts: The importance of sound localization is recognized by ongoing work at NASA-Ames [27]. The evaluation of auditory display techniques is reported extensively at the annual conferences of ICAD, the International Conference on Auditory Display; see [12]. Sound as a component of the human/computer interface is discussed in <ref> [1] </ref>. Most of the attempts described above used MIDI-controlled synthesizer sounds, which have drastic limitations in the number and range of their control parameters.
Reference: [2] <author> Bargar, R., I. Choi, S. Das, and C. Goudeseune, </author> <title> "Model-based interactive sound for an immersive virtual environment," </title> <booktitle> Proc. 1994 Int'l. Computer Music Conference (Tokyo, Japan), </booktitle> <pages> pp. 471-477. 15 </pages>
Reference-contexts: Sound as a component of the human/computer interface is discussed in [1]. Most of the attempts described above used MIDI-controlled synthesizer sounds, which have drastic limitations in the number and range of their control parameters. Bargar et al. <ref> [2] </ref> at the National Computational Science Alliance (NCSA) have developed a complex instrument with interactive capabilities, which includes the VSS sound server for the CAVE virtual-reality environment. 9 2.2 What We Have Done So Far Much of our work so far has been focused on the development of DIASS [13, 10].
Reference: [3] <author> Beauchamp, J., </author> <title> Music 4C Introduction, Computer Music Project, </title> <institution> School of Music, University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year> <note> URL: http://cmp-rs.music.uiuc.edu/cmp/software/m4c.html </note>
Reference-contexts: SCORE TM Music notation and printing CAVE Printed score composition program VR visualization sound file MP1 S Binary M4CAVE DIASS_M4C Sound synthesis DIASSIN Score editor 1.1.1 The Instrument The DIASS instrument functions as part of the M4C synthesis language developed by Beauchamp and his associates at the University of Illinois <ref> [3] </ref>. Synthesis languages like M4C are designed around the notion that the user creates an instrument together with a score that references the instrument.
Reference: [4] <author> Bly, S., </author> <title> Sound and Computer Information Presentation, </title> <type> PhD thesis, </type> <institution> University of California - Davis, </institution> <note> 1982 (unpublished) </note>
Reference-contexts: The authors were able to detect quantum oscillations between two weakly coupled reservoirs of superfluid 3 He using sound, where oscilloscope traces failed to reveal structure. Several other experiments reported in the literature refer to situations where sounds are used in combination with visual images for data analysis. Bly <ref> [4] </ref> ran discriminant analysis experiments using sound and graphics to represent multivari-ate, time-varying, and logarithmic data. Mezrich et al. [14] used sound and dynamic graphics to represent multivariable time series data.
Reference: [5] <author> Fletcher, H. and W. A. Munson, </author> <title> "Loudness, its definition, measurement, and calculation," </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 5 (1933), </volume> <pages> 82 </pages>
Reference: [6] <author> Gropp, W., E. Lusk, and A. Skjellum, </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface, </title> <publisher> MIT Press, </publisher> <year> 1994. </year> <note> See also URL: http://www.mcs.anl.gov/mpi/index.html </note>
Reference-contexts: The M4C synthesis language is imbedded in the C language. As part of the current project, the instrument and relevant parts of M4C were redesigned for a distributed-memory environment. The parallel implementation uses the standard MPI message-passing library <ref> [6] </ref>.
Reference: [7] <author> Hiller, L. and L. Isaacson, </author> <title> Experimental Music, </title> <type> McGraw-Hill, </type> <note> 1959; reprinted by Greenwood Press, </note> <year> 1983 </year>
Reference: [8] <author> Hiller, L., </author> <title> Computer Music Retrospective, Compact disc WER 60128-50, </title> <publisher> WERGO Schallplatten GmbH, Mainz, </publisher> <address> Germany, </address> <year> 1989 </year>
Reference: [9] <author> International Organization for Standardization (ISO), </author> <title> "Acoustics Normal equal-loudness level contours," </title> <journal> Publ. </journal> <volume> No. </volume> <year> 226:1987 </year>
Reference: [10] <author> Kaper, H. G., D. Ralley, J. M. Restrepo, and S. Tipei, </author> <title> "Additive synthesis with DIASS M4C on Argonne National Laboratory's IBM POWERparallel System (SP)," </title> <booktitle> Proc. 1995 Int'l. Computer Music Conference (Banff, Canada), </booktitle> <pages> pp. 351-352 </pages>
Reference-contexts: al. [2] at the National Computational Science Alliance (NCSA) have developed a complex instrument with interactive capabilities, which includes the VSS sound server for the CAVE virtual-reality environment. 9 2.2 What We Have Done So Far Much of our work so far has been focused on the development of DIASS <ref> [13, 10] </ref>. In addition, we have used DIASS for two preliminary experiments in scientific sonifica-tion, one in chemistry, the other in materials science. The first experiment used data from Dr. Jeff Tilson, a computational chemist at ANL, who studied the binding of a carbon atom to a protonated thiophene molecule.
Reference: [11] <author> Kaper, H. G., D. Ralley, and S. Tipei, </author> <title> "Perceived equal loudness of complex tones: A software implementation for computer music composition," </title> <booktitle> Proc. 1996 Int'l. Conference in Music Perception and Cognition (Montreal, Canada), </booktitle> <pages> pp. 127-132 </pages>
Reference-contexts: Also, when the variable x 1 increases or decreases, we should be able to perceive a proportional increase or decrease in the loudness level. The loudness routines in DIASS incorporate the relevant results of psychoacoustic research <ref> [11] </ref> and give the user full control over the perceived loudness of a sound. They also scale each partial so each sample value fits in a 16-bit register (see Box 2). Anticlip.
Reference: [12] <author> Kramer, G. (ed.), </author> <title> Auditory Display: Sonification, Audification, and Auditory Interfaces, </title> <booktitle> Proc. ICAD '92, </booktitle> <publisher> Addison-Wesley Publ. Co., </publisher> <year> 1994. </year> <note> For proceedings of later conferences, consult URL: http://www.santafe.edu/ icad </note>
Reference-contexts: The importance of sound localization is recognized by ongoing work at NASA-Ames [27]. The evaluation of auditory display techniques is reported extensively at the annual conferences of ICAD, the International Conference on Auditory Display; see <ref> [12] </ref>. Sound as a component of the human/computer interface is discussed in [1]. Most of the attempts described above used MIDI-controlled synthesizer sounds, which have drastic limitations in the number and range of their control parameters.
Reference: [13] <author> Kriese, C. and S. Tipei, </author> <title> "A compositional approach to additive synthesis on supercomputers," </title> <booktitle> Proc. 1992 Int'l. Computer Music Conference (San Jose, Cal-ifornia), </booktitle> <pages> pp. 394-395 </pages>
Reference-contexts: al. [2] at the National Computational Science Alliance (NCSA) have developed a complex instrument with interactive capabilities, which includes the VSS sound server for the CAVE virtual-reality environment. 9 2.2 What We Have Done So Far Much of our work so far has been focused on the development of DIASS <ref> [13, 10] </ref>. In addition, we have used DIASS for two preliminary experiments in scientific sonifica-tion, one in chemistry, the other in materials science. The first experiment used data from Dr. Jeff Tilson, a computational chemist at ANL, who studied the binding of a carbon atom to a protonated thiophene molecule.
Reference: [14] <author> Mezrich, J., S. Frysinger, and R. Slivjanovski, </author> <title> "Dynamic Representation of mul-tivariate time series data," </title> <journal> J. Amer. Stat. Ass. </journal> <volume> 79 (1984), </volume> <pages> 34-40 16 </pages>
Reference-contexts: Several other experiments reported in the literature refer to situations where sounds are used in combination with visual images for data analysis. Bly [4] ran discriminant analysis experiments using sound and graphics to represent multivari-ate, time-varying, and logarithmic data. Mezrich et al. <ref> [14] </ref> used sound and dynamic graphics to represent multivariable time series data. The "Exvis" experiment at the University of Massachusetts at Lowell [20] expanded this work by assigning sonic attributes to visual icons. The importance of sound localization is recognized by ongoing work at NASA-Ames [27].
Reference: [15] <author> Pereverzev, S. V., A. Loshak, S. Backhaus, J. C. Davis, and R. E. Packard, </author> <title> "Quantum oscillations between two weakly coupled reservoirs of superfluid 3 He," </title> <booktitle> Nature 388 (1997), </booktitle> <pages> 449-451 </pages>
Reference-contexts: His experiment showed that motivated expert users can easily adapt to complex auditory displays. Recently, a successfull application of scientific sonification was reported in physics by Pereverzev et al. <ref> [15] </ref>. The authors were able to detect quantum oscillations between two weakly coupled reservoirs of superfluid 3 He using sound, where oscilloscope traces failed to reveal structure. Several other experiments reported in the literature refer to situations where sounds are used in combination with visual images for data analysis.
Reference: [16] <author> Roads, C., </author> <title> The Computer Music Tutorial, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1996 </year>
Reference-contexts: The synthesis program reads the instrument, feeds it the data from the score file, and computes the final audio signal, which is then written to a sound file for later playback <ref> [16] </ref>. The M4C synthesis language is imbedded in the C language. As part of the current project, the instrument and relevant parts of M4C were redesigned for a distributed-memory environment. The parallel implementation uses the standard MPI message-passing library [6].
Reference: [17] <author> Roederer, J. G., </author> <title> The Physics and Psychophysics of Music, 3rd edition. </title> <publisher> Springer-Verlag, </publisher> <year> 1995 </year>
Reference: [18] <author> Rossing, T. D., </author> <title> The Science of Sound, </title> <publisher> Addison-Wesley Publ. Co., </publisher> <year> 1990 </year>
Reference: [19] <editor> Simoni, M. (ed.), </editor> <booktitle> Proc. ICMC98, Int'l Computer Music Conference (Ann Arbor, </booktitle> <address> Michigan), </address> <note> October 1998; see also proceedings of earlier conferences </note>
Reference: [20] <author> Smith, S. and M. Williams, </author> <title> "The use of sound in an exploratory visualization experiment," </title> <institution> CS Dept., U. Mass. at Lowell, Tech report R-89-002, </institution> <year> 1989 </year>
Reference-contexts: Bly [4] ran discriminant analysis experiments using sound and graphics to represent multivari-ate, time-varying, and logarithmic data. Mezrich et al. [14] used sound and dynamic graphics to represent multivariable time series data. The "Exvis" experiment at the University of Massachusetts at Lowell <ref> [20] </ref> expanded this work by assigning sonic attributes to visual icons. The importance of sound localization is recognized by ongoing work at NASA-Ames [27]. The evaluation of auditory display techniques is reported extensively at the annual conferences of ICAD, the International Conference on Auditory Display; see [12].
Reference: [21] <author> Stevens, S. S., </author> <title> "Neural events and the psychophysical law," </title> <booktitle> Science 170 (1970), </booktitle> <pages> 1043 </pages>
Reference: [22] <author> Tipei, S., </author> <title> "The computer: A composer's collaborator," </title> <booktitle> Leonardo 22(2) 1989, </booktitle> <pages> 189-195 </pages>
Reference: [23] <author> Tipei, S., </author> <title> "Manifold compositions | A (super)computer-assisted composition experiment in progress," </title> <booktitle> Proc. 1989 Int'l. Computer Music Conference (Colum-bus, </booktitle> <publisher> Ohio), </publisher> <pages> pp. 324-327 </pages>
Reference: [24] <author> Tipei, S., "A.N.L.-folds. </author> <note> mani 1943-0000; mani 1985r-2101; mani 1943r-0101; mani 1996m-1001; mani 1996t-2001" (1996). Report ANL/MCS-P679-0897, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory </institution>
Reference-contexts: Besides the obvious short score files to test the implementation of these mappings, we have used score files generated with DIASS of various musical compositions, notably the "A.N.L.-folds" of Tipei <ref> [24] </ref>. A.N.L.-folds is an example of a manifold composition described in Box 1. Each member of A.N.L.-folds lasts exactly 2'26", comprises between 200 and 500 sounds of medium to great complexity.
Reference: [25] <institution> URL: </institution> <note> http://mcs.anl.gov/appliedmath/Sonification/index.html </note>
Reference-contexts: They have merely served to demonstrate the capabilities of DIASS and explore various mappings from the degrees of freedom in the data to the parameters controlling the sound synthesis process. Samples can be heard on the Web <ref> [25] </ref>. 2.3 What We Have Found So Far General conclusions are that (i) the sounds produced in each experiment conveyed information about the qualitative nature of the data, and (ii) DIASS is a flexible and sophisticated tool capable of rendering subtle variations in the data. <p> The visual objects rotate or pulse when tremolo or vibrato is applied, and their color varies when reverberation is present. An optional grid in the background shows the octaves divided into twelve equal increments. Figure 3 | taken from our Web site <ref> [25] </ref>, where more samples can be found | shows a visualization of nine sounds with different numbers of partials.
Reference: [26] <institution> URL: </institution> <note> http://www.evl.uic.edu/pape/CAVE/prog/CAVEGuide.html </note>
Reference-contexts: Visualizing sounds is one of these means. In this project we are focusing on the visualization of sounds in the CAVE, a room-size virtual-reality (VR) environment <ref> [26] </ref>, and on the ImmersaDesk, a two-dimensional version. 3.1 M4CAVE A Visualization Tool The software collectively known as M4CAVE takes a score file from the sound synthesis program DIASS and renders the sounds represented by the score as visual images in a CAVE or Immersadesk.
Reference: [27] <author> Wenzel, E., S. Fisher, P. Stone, and S. Foster, </author> <title> "A system for three-dimensional acoustic `visualization' in a virtual environment workstation," </title> <booktitle> Proc. Visualization '90: First IEEE Conf. on Visualization, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <address> Washington, </address> <pages> pp. 329-337 </pages>
Reference-contexts: Mezrich et al. [14] used sound and dynamic graphics to represent multivariable time series data. The "Exvis" experiment at the University of Massachusetts at Lowell [20] expanded this work by assigning sonic attributes to visual icons. The importance of sound localization is recognized by ongoing work at NASA-Ames <ref> [27] </ref>. The evaluation of auditory display techniques is reported extensively at the annual conferences of ICAD, the International Conference on Auditory Display; see [12]. Sound as a component of the human/computer interface is discussed in [1].
Reference: [28] <author> Xenakis, I., </author> <title> Formalized Music: Thought and Mathematics in Musical Composition, revised edition, </title> <publisher> Pendragon Press, </publisher> <year> 1992 </year>
Reference: [29] <author> Yeung, E., </author> <title> "Pattern recognition by audio representation of multivariate analytical data," </title> <booktitle> Analytical Chemistry 52 (1980), </booktitle> <pages> 1120-1123 17 </pages>
Reference-contexts: One of the goals 8 of our project is to demonstrate that, with an instrument like DIASS, one can probe multidimensional datasets with surgical precision and uncover structures that may be hidden to the eye. 2.1 Past Experiments An early experiment with scientific sonification was done by Yeung <ref> [29] </ref>. Seven chemical variables were matched with seven variables of sound: two with frequency, one each with loudness, decay, direction, duration, and rest (silence between sounds).
Reference: [30] <author> Zwicker, E. and B. Scharf, </author> <title> "A model of loudness summation," </title> <journal> Psych. Rev. </journal> <volume> 72 (1965), </volume> <pages> 3 </pages>

References-found: 30

