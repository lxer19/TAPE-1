URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/grad-nnsp94/grad-nnsp94.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers.html
Root-URL: http://www.neci.nj.nec.com
Email: back@s1.elec.uq.oz.au, ericwan@eeap.ogi.edu lawrence@s1.elec.uq.oz.au, act@s1.elec.uq.oz.au  
Title: A Unifying View of Some Training Algorithms for Multilayer Perceptrons with FIR Filter Synapses  
Author: Andrew Back*, Eric A. Wan**, Steve Lawrence*, Ah Chung Tsoi* 
Address: P.O. Box 91000, Portland, Oregon 97291, USA  
Affiliation: *Department of Electrical and Computer Engineering, University of Queensland, St. Lucia, Queensland 4072. Australia. **Department of Electrical Engineering and Applied Physics Oregon Graduate Institute of Science Technology  
Abstract: Recent interest has come about in deriving various neural network architectures for modelling time-dependent signals. A number of algorithms have been published for multilayer perceptrons with synapses described by finite impulse response (FIR) and infinite impulse response (IIR) filters (the latter case is also known as Locally Recurrent Globally Feedforward Networks). The derivations of these algorithms have used different approaches in calculating the gradients, and in this note, we present a short, but unifying account of how these different algorithms compare for the FIR case, both in derivation, and performance. New algorithms are subsequently presented. Simulation results have been performed to benchmark these algorithms. In this note, results are compared for the Mackey-Glass chaotic time series against a number of other methods including a standard multilayer perceptron, and a local approximation method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Back, A.D. and Tsoi, </author> <title> A.C., A Time Series Modelling Methodology Using FIR and IIR Synapses", </title> <booktitle> Proc. Workshop on Neural Networks for Statistical and Economic Data, Dublin, DOSES, Statistical Office of European Communities, </booktitle> <publisher> F. Murtagh (Ed.), </publisher> <pages> pp. 187-194, </pages> <year> 1990. </year>
Reference-contexts: INTRODUCTION As a means of capturing time-dependent signals in a nonlinear framework, multilayer perceptrons (MLPs) with synapses described by filters have recently been proposed <ref> [1, 2, 17] </ref>. These approaches replace the traditional scalar synaptic weights with finite impulse response (FIR) filters commonly used in digital filter theory. The architecture can be considered an extension of earlier work in which time delays were incorporated as a means of capturing time-dependent input information. <p> FIR networks provide a more general model for distributed time representations. An algorithm for training networks having FIR synapses was first published by Wan [17]. A similar algorithm for the same network as well as the case for IIR synapses was published by Back and Tsoi <ref> [1, 2] </ref>. We focus on these algorithms in this paper, comparing their derivations and presenting a brief, but unifying view of them. Related work which has been presented in [4, 6, 7, 11] and [14] among others, will not be considered here. <p> GRADIENT COMPUTATION IN FIR SYNAPSES USING AN INSTANTANEOUS COST FUNCTION An algorithm for updating the weights in an FIR network may be obtained by consid ering the instantaneous error E (t) <ref> [1, 17] </ref>. <p> We have not taken into account the performance surface over time. This is the method adopted in <ref> [1, 2] </ref>. Note the ffi terms are essentially calculated using standard backpropagation through the w km0 taps; the rest of the coefficients in the FIR synapse are ignored, since we only assume a relationship between z l k (t) and ^y l+1 km (t) instantaneously at time t.
Reference: [2] <author> Back, A.D. and Tsoi,A.C., </author> <title> FIR and IIR Synapses, a New Neural Network Architecture for Time Series Modelling". </title> <journal> Neural Computation, </journal> <volume> vol 3. no. 3, </volume> <pages> pp. 375-385, </pages> <year> 1991. </year>
Reference-contexts: INTRODUCTION As a means of capturing time-dependent signals in a nonlinear framework, multilayer perceptrons (MLPs) with synapses described by filters have recently been proposed <ref> [1, 2, 17] </ref>. These approaches replace the traditional scalar synaptic weights with finite impulse response (FIR) filters commonly used in digital filter theory. The architecture can be considered an extension of earlier work in which time delays were incorporated as a means of capturing time-dependent input information. <p> FIR networks provide a more general model for distributed time representations. An algorithm for training networks having FIR synapses was first published by Wan [17]. A similar algorithm for the same network as well as the case for IIR synapses was published by Back and Tsoi <ref> [1, 2] </ref>. We focus on these algorithms in this paper, comparing their derivations and presenting a brief, but unifying view of them. Related work which has been presented in [4, 6, 7, 11] and [14] among others, will not be considered here. <p> A derivation of the partial terms is given in <ref> [2] </ref>. <p> We have not taken into account the performance surface over time. This is the method adopted in <ref> [1, 2] </ref>. Note the ffi terms are essentially calculated using standard backpropagation through the w km0 taps; the rest of the coefficients in the FIR synapse are ignored, since we only assume a relationship between z l k (t) and ^y l+1 km (t) instantaneously at time t.
Reference: [3] <author> Back, A.D. and Tsoi, </author> <title> A.C., An Adaptive Lattice Architecture for Dynamic Multilayer Perceptrons", </title> <journal> Neural Computation, </journal> <volume> Vol 4, No. 6, </volume> <pages> pp. 922-931, </pages> <year> 1992. </year>
Reference: [4] <author> Bengio, Y. De Mori, R., Gori, M. </author> <title> Learning the dynamic nature of s peech with backpropagation for sequences. </title> <journal> Pattern recognition Letters. </journal> <volume> Vol. 13, </volume> <pages> pp 375 - 385, </pages> <year> 1992. </year>
Reference-contexts: We focus on these algorithms in this paper, comparing their derivations and presenting a brief, but unifying view of them. Related work which has been presented in <ref> [4, 6, 7, 11] </ref> and [14] among others, will not be considered here. Our aim is to compare the forms of the training algorithms, and to provide an indication of how they perform on some practical prediction problems.
Reference: [5] <author> M. Casdagli, </author> <title> Chaos and Deterministic versus Stochastic Non-linear Modelling", </title> <journal> J. R. Statist. Soc. B, 1991, </journal> <volume> 54, No. 2, </volume> <pages> pp. 303-328. </pages>
Reference-contexts: This is sometimes referred to as a phase space. As a means of comparing each algorithm, we benchmark their relative performances againsta windowed input MLP, and a local approximation method developed by Cas-dagli <ref> [5] </ref> (a version of the nearest neighbor method). Obviously, there are many variations in which this could have been done. Our intent is to provide a reasonable means of quickly assessing the performance of these algorithms which may provide a starting point for anyone interested in considering them further.
Reference: [6] <author> P. Frasconi, M. Gori and G. </author> <title> Soda, "Local Feedback Multilayered Networks", </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 1, </volume> <year> 1992. </year>
Reference-contexts: We focus on these algorithms in this paper, comparing their derivations and presenting a brief, but unifying view of them. Related work which has been presented in <ref> [4, 6, 7, 11] </ref> and [14] among others, will not be considered here. Our aim is to compare the forms of the training algorithms, and to provide an indication of how they perform on some practical prediction problems.
Reference: [7] <author> Gori, M., Bengio, Y., Mori, R.D. </author> <title> BPS: a learning algorithm for capturing the dynamic nature of speech. </title> <booktitle> Intern. Joint Conf on Neural Networks, </booktitle> <volume> Vol II, </volume> <pages> pp 417 - 423, </pages> <year> 1989. </year>
Reference-contexts: We focus on these algorithms in this paper, comparing their derivations and presenting a brief, but unifying view of them. Related work which has been presented in <ref> [4, 6, 7, 11] </ref> and [14] among others, will not be considered here. Our aim is to compare the forms of the training algorithms, and to provide an indication of how they perform on some practical prediction problems.
Reference: [8] <author> N.A. Gershenfeld, </author> <title> and A.S. Weigend, The Future of Time Series: Learning and Understanding", in Time Series Prediction: Forecasting the Future and Understanding the Past, Eds. A.S Weigend, and N.A. </title> <publisher> Gershenfeld, Addison-Wesley: </publisher> <address> Reading MA, </address> <year> 1993. </year>
Reference: [9] <author> U. Hubner, C.O. Weiss, N.B. Abraham, and D. Tang, </author> <title> Lorenz-like Chaos in NH 3 -FIR Lasers", in Time Series Prediction: Forecasting the Future and Understanding the Past, Eds. A.S Weigend, and N.A. </title> <publisher> Gershenfeld, Addison-Wesley: </publisher> <address> Reading MA, </address> <year> 1993. </year>
Reference: [10] <author> Lapedes, A. and Farber, R., </author> <title> Nonlinear Signal Processing using Neural Networks: Prediction and System modelling, </title> <type> Tech Report LA-UR87-2662, </type> <institution> Los Alamos National Laboratory, </institution> <year> 1987. </year>
Reference-contexts: For example, in the Time Delay Neural Network used by Waibel et al [20], the outputs of a layer in a feedforward network are buffered several time steps and then fed fully connected to the next layer. Lapedes and Farber's <ref> [10] </ref> use of a time-window as the input to a multilayer network for applications in time series prediction is equivalent to one layer of time delay synapses at the input. FIR networks provide a more general model for distributed time representations. <p> Therefore, if we allow the model to predict 6 time-steps into the future, and we wish to see how it performs out to 400 time-steps, we allow the model to use its shorter predictions to "bootstrap" itself out. This follows the conventions adopted by Lapedes and Farber <ref> [10] </ref> and Stokbro and Umberger [16]. 5 0.2 0.6 1 1.4 Output Time Mackey-Glass Using Backprop 6:5:1 L=0.05 W=6 D=6 T=6: Test Set 1 Performance d (t) (a) 0.4 0.8 1.2 0 50 100 150 200 250 Output Time Mackey-Glass Using Nn 6:1:1 A=2 K=20 W=6 D=6 T=6: Test Set 1
Reference: [11] <author> Leighton, R.R. and Conrath, </author> <title> B.C., "The Autoregressive Backpropagation Algorithm", </title> <booktitle> Proc. Int. Joint Conf. Neural Networks, </booktitle> <year> 1991. </year>
Reference-contexts: We focus on these algorithms in this paper, comparing their derivations and presenting a brief, but unifying view of them. Related work which has been presented in <ref> [4, 6, 7, 11] </ref> and [14] among others, will not be considered here. Our aim is to compare the forms of the training algorithms, and to provide an indication of how they perform on some practical prediction problems.
Reference: [12] <author> Mackey, M.C., and Glass, L., </author> <title> Oscillation and Chaos in Physiological Control Systems", </title> <journal> Science, </journal> <volume> vol. 197, </volume> <pages> pp. 287, </pages> <year> 1977. </year>
Reference-contexts: Our aim was to test two basic approaches to time series prediction, namely, the traditional approach of using past values of the time series directly, and secondly, the 1 The Mackey-Glass <ref> [12] </ref> equation is described by _x (t) = bx (t) + ax (to) 1+x (to) 10 ; where o =30, a=0.2, and b=0.1. approach of embedding the time series in a phase space, and using the delay coordinates as the vector of inputs 2 .
Reference: [13] <author> Packard, N., Crutchfield, J., Farmer, D., and Shaw, R., </author> <title> Geometry froma time series", </title> <journal> Phys. Rev. Lett., </journal> <volume> vol 45., </volume> <pages> pp. 712-716. </pages>
Reference-contexts: This approach, proposed by Takens <ref> [13, 15] </ref> involves sampling the time series at some delayed time values to create a delay coordinate vector. This is sometimes referred to as a phase space.
Reference: [14] <author> Poddar, P. Unnikrishnan, </author> <title> K.P. Memory neuron networks: A Prolegomenon. </title> <institution> General Motors Research Laboratories Report GMR-7493, </institution> <month> October 21, </month> <year> 1991. </year>
Reference-contexts: We focus on these algorithms in this paper, comparing their derivations and presenting a brief, but unifying view of them. Related work which has been presented in [4, 6, 7, 11] and <ref> [14] </ref> among others, will not be considered here. Our aim is to compare the forms of the training algorithms, and to provide an indication of how they perform on some practical prediction problems.
Reference: [15] <author> Takens F., </author> <title> Detecting Strange Attractors in Turbulence", </title> <booktitle> Lecture Notes in Math., </booktitle> <volume> vol 898, </volume> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: This approach, proposed by Takens <ref> [13, 15] </ref> involves sampling the time series at some delayed time values to create a delay coordinate vector. This is sometimes referred to as a phase space.
Reference: [16] <author> Stokbro K. and Umberger D.K., </author> <title> Forecasting with Weighted Maps", in Nonlinear Modeling and Forecasting, </title> <booktitle> SFI Studies in the Sciences of Complexity, Proc. </booktitle> <volume> Vol. XII, </volume> <editor> Eds. M. Cadagli, and S. Eubanks. </editor> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: This follows the conventions adopted by Lapedes and Farber [10] and Stokbro and Umberger <ref> [16] </ref>. 5 0.2 0.6 1 1.4 Output Time Mackey-Glass Using Backprop 6:5:1 L=0.05 W=6 D=6 T=6: Test Set 1 Performance d (t) (a) 0.4 0.8 1.2 0 50 100 150 200 250 Output Time Mackey-Glass Using Nn 6:1:1 A=2 K=20 W=6 D=6 T=6: Test Set 1 Performance d (t) (b) 0.4
Reference: [17] <author> Wan, </author> <title> E.A., Temporal backpropagation for FIR neural networks", </title> <booktitle> Proc. Int. Joint Conf. Neural Networks, </booktitle> <address> San Diego, </address> <month> June </month> <year> 1990, </year> <pages> pp I 575-580. </pages>
Reference-contexts: INTRODUCTION As a means of capturing time-dependent signals in a nonlinear framework, multilayer perceptrons (MLPs) with synapses described by filters have recently been proposed <ref> [1, 2, 17] </ref>. These approaches replace the traditional scalar synaptic weights with finite impulse response (FIR) filters commonly used in digital filter theory. The architecture can be considered an extension of earlier work in which time delays were incorporated as a means of capturing time-dependent input information. <p> FIR networks provide a more general model for distributed time representations. An algorithm for training networks having FIR synapses was first published by Wan <ref> [17] </ref>. A similar algorithm for the same network as well as the case for IIR synapses was published by Back and Tsoi [1, 2]. We focus on these algorithms in this paper, comparing their derivations and presenting a brief, but unifying view of them. <p> GRADIENT COMPUTATION IN FIR SYNAPSES USING AN INSTANTANEOUS COST FUNCTION An algorithm for updating the weights in an FIR network may be obtained by consid ering the instantaneous error E (t) <ref> [1, 17] </ref>. <p> Algorithm IC-2 Instantaneous Cost Accumulated Gradient ffi l ^x l m=1 d=0 km w l+1 m (t) ^x l m=1 d=0 km w l+1 m (t d) ^x l m=1 km W l+1 m (t): (14) This is similar to the second algorithm proposed by Wan in <ref> [17] </ref> (discussed in a subsequent section of this paper). In this case, we have the backpropagated error being obtained from a backward filter and all coefficients in the FIR synapse have an influence on the ffi value. <p> We will discuss the relative performance of the different methods in the results section. GRADIENT COMPUTATION IN FIR SYNAPSES USING A TOTAL COST FUNCTION This section reviews the algorithms derived by Wan in <ref> [17, 18] </ref>. Gradient adaptation is based on the total squared error over the entire sequence of inputs, as opposed to the instantaneous error measure used previously.
Reference: [18] <author> Wan, </author> <title> E.A., Time Series Prediction by Using a Connectionist Network with Internal Delay Lines'", </title> <editor> in A. Weigend and N. Gershenfeld, eds., </editor> <title> Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison-Wesley, </publisher> <pages> pages 195-218, </pages> <year> 1994. </year>
Reference-contexts: We will discuss the relative performance of the different methods in the results section. GRADIENT COMPUTATION IN FIR SYNAPSES USING A TOTAL COST FUNCTION This section reviews the algorithms derived by Wan in <ref> [17, 18] </ref>. Gradient adaptation is based on the total squared error over the entire sequence of inputs, as opposed to the instantaneous error measure used previously.
Reference: [19] <author> Wan, </author> <title> E.A., Finite Impulse Response Neural Networks with Applications in Time Series Prediction", </title> <type> PhD Dissertation, </type> <institution> Stanford University, </institution> <month> November, </month> <year> 1993. </year>
Reference-contexts: In this case an expression for ffi is obtained by maintaining the dependence over all values 3 of the input sequence. Derivations given in <ref> [19] </ref> leads to the following algorithms. Algorithm TC-1 is very inefficient for networks with more than two layers. Algorithm TC-2 on the other hand, uses the same update equations (15) and (16) as before. In this case we derive a slightly different equation for the ffi term.
Reference: [20] <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K., </author> <title> Phoneme recognition using time-delay neural networks", </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> vol ASSP-37, </volume> <month> March, </month> <year> 1989. </year>
Reference-contexts: The architecture can be considered an extension of earlier work in which time delays were incorporated as a means of capturing time-dependent input information. For example, in the Time Delay Neural Network used by Waibel et al <ref> [20] </ref>, the outputs of a layer in a feedforward network are buffered several time steps and then fed fully connected to the next layer.
References-found: 20

