URL: ftp://ftp.imag.fr/pub/CALCUL_FORMEL/RAPPORT/1996/RT157-b.ps.gz
Refering-URL: http://www-lmc.imag.fr/CF/publi.html
Root-URL: http://www.imag.fr
Title: RAPPORT TECHNIQUE Some algorithms for matrix polynomials  
Author: G. Villard 
Note: RT157 Part 2 Mars 96  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> D. Augot and P. Camion. </author> <title> The minimal polynomials, characteristic subspaces, normal bases and the Frobenius form. </title> <type> Technical Report 2006, </type> <institution> INRIA France, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Following the converse process of the one used to construct A from T (x), from L H (x) we can contruct a matrix A 0 . Since L H (x) is triangular, A 0 is Hessenberg, it is said to be either in shift-Hessenberg form <ref> [1] </ref> or in polycyclic form [34]. In addition, A 0 is similar to A. We can also define Y 0 = K n [x]= t L H (x)K n [x] and we get a commuting diagram similar to (14) with a coordinate function p 0 .
Reference: 2. <author> S. Barnett. </author> <title> Greatest common divisor of two polynomials. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 3 </volume> <pages> 7-9, </pages> <year> 1970. </year>
Reference: 3. <author> S. Barnett. </author> <title> Greatest common divisor of several polynomials. </title> <journal> Proc. Cambridge Philos. Society, </journal> <volume> 70 </volume> <pages> 263-268, </pages> <year> 1971. </year> <month> 18 </month>
Reference: 4. <author> S. Barnett and P. Lancaster. </author> <title> Some properties of the bezoutian for polynomial matrices. Linear Algebra and Multilinear Algebra, </title> <booktitle> 9 </booktitle> <pages> 99-110, </pages> <year> 1980. </year>
Reference: 5. <author> V.A. Belyi, V.B. Khazanov, and V.N. Kublanovskaya. </author> <title> Spectral problems for matrix pencils. Methods and algorithms III. </title> <journal> Soviet J. Numer. Anal. Math. Model., </journal> <volume> 4(1) </volume> <pages> 19-51, </pages> <year> 1989. </year>
Reference-contexts: The univariate determinant computation in [30] may be viewed as the computation of a column reduced form. More generally, strong links are well knowns between column reduced forms and the determination of the finite eigenstructure of a polynomial matrix [41,40], and also with the factorization algorithm of Kublanovskaya <ref> [5] </ref>. As said previously the Popov form will be the first step toward the Hermite form. It will be computed in x3 by a polynomial matrix gcrd operation. Apart from using matrix equivalence and indentity (1), a gcrd can be computed from the generalized Sylvester resultant matrix in [8].
Reference: 6. <author> D. Bini and L. Gemignani. </author> <title> Fast parallel computation of the polynomial remainder sequence via Bezout and Hankel matrices. </title> <journal> SIAM J. Comput., </journal> <volume> 24(1) </volume> <pages> 63-77, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: If H (x) has a right irreducible description N (x) D 1 (x) and a left irreducible description C 1 (x) L (x) such that the denominator matrices D (x) and C (x) have degrees bounded by ffi, then rank M (ffi) = . Proof (Compare proposition 2.1 in <ref> [6] </ref> for the scalar polynomial gcd). By minimality of we know that rank M ( + i) = rank M (1) = , i 0 (lemma 6.5-7, [20]). We apply the "if" part of theorem 2.1. <p> Proof (Compare proposition 3.6 in <ref> [6] </ref> for the scalar polynomial gcd). This is the "only if" part of theorem 2.1 and corresponds to the method proposed in [26] to compute a minimal realization. Indeed, once D (x) in Popov form is computed, a satisfying triplet (C; A; B) can be readily obtained [20]. <p> One additional matrix inversion is needed to build M (d + 1). 2 3.2 Dual approach Clearly, it is also possible to exactly match the scalar case for the polynomial gcd <ref> [6] </ref>. For u (x) and v (x), the authors of this paper proceed in two steps: first computing a reduced fraction v (x)=u (x) then obtaining the gcd as u (x)=u (x). In the same way, we may consider H (x) = A (x) 1 (x).
Reference: 7. <author> D. Bini and V. Pan. </author> <title> Polynomial and matrix computations. </title> <publisher> Birkhauser, </publisher> <year> 1994. </year>
Reference-contexts: These values essentially come from the inefficiency of known solutions <ref> [7] </ref> to the problem of computing the nullspace of a matrix, or more precisely, 6 as we will see, to the problem of determining the lexicographically first maximal linearly independent subset of the set of the columns of a matrix. <p> These complexities does not express the boolean computational cost of the algorithms. We will not detail the corresponding studies in this paper. Further developments should be done [38] using the Bezoutian for matrix polynomials [4,37]. From results in <ref> [7] </ref> concerning the scalar polynomial gcd, very good bounds may be expected. The paper is organized as follows: we first remind some results on the polynomial matrix gcd in x1 and on the minimal realization problem in x2. <p> We let n = O () and d = O (). For a survey of basic sequential and parallel complexities, e.g. for matrix or polynomial multiplication, we refer to <ref> [7] </ref>. <p> Over an arbitrary field, the costs are dominated by the solution of L (nd). As for the processor bound of the solution of the nullspace problem in [9] this leads to n times <ref> [7] </ref> the processor bound for the rank. This latter problem is solved using the algorithm in [32]. The number of steps if O (log 3 (nd)) using O ~ ((nd) 4 M (nd)) processors.
Reference: 8. <author> R.R. Bitmead, S.Y. Kung, B.D.O. Anderson, and T. Kailath. </author> <title> Greatest common divisors via generalized Sylvester and Bezout matrices. </title> <journal> IEEE Trans. Automat. Control., </journal> <volume> 23(6) </volume> <pages> 1043-1047, </pages> <year> 1978. </year>
Reference-contexts: As said previously the Popov form will be the first step toward the Hermite form. It will be computed in x3 by a polynomial matrix gcrd operation. Apart from using matrix equivalence and indentity (1), a gcrd can be computed from the generalized Sylvester resultant matrix in <ref> [8] </ref>. However this method seems to lead to the same conclusions than when using the Sylvester matrix in [27] and does not seem fully appropriate to parallelism. <p> If a right matrix fraction description is not irreducible, suppose we have a way to compute an irreducible description N (x) D 1 (x) of H (x), then a gcrd of N (x) and D (x) is computed as G (x) = D 1 (x)D (x) <ref> [8] </ref>. This is basically the approach used for the scalar polynomial gcd in [6,7].
Reference: 9. <author> A. Borodin, J. von zur Gathen, and J. Hopcroft. </author> <title> Fast parallel matrix and gcd computations. </title> <journal> Information and Control, </journal> <volume> 52 </volume> <pages> 241-256, </pages> <year> 1982. </year>
Reference-contexts: This matrix is obtained by solving the system (9), for instance by isolating a square invertible submatrix of M ffi using a second solution of L (nffi) as done in <ref> [9] </ref> for the nullspace computation. Then D (x) is computed under Popov form simply by reordering the columns of T (x) with respect to their degrees, and N (x) is deduced from identity (10). <p> The Hermite form is computed from the Popov form in time O (log 4 (nd)) using O ((nd) 1=2 M (nd)) processors. Over an arbitrary field, the costs are dominated by the solution of L (nd). As for the processor bound of the solution of the nullspace problem in <ref> [9] </ref> this leads to n times [7] the processor bound for the rank. This latter problem is solved using the algorithm in [32]. The number of steps if O (log 3 (nd)) using O ~ ((nd) 4 M (nd)) processors.
Reference: 10. <author> D. Coppersmith. </author> <title> Solving homogeneous linear equations over GF(2) via block Wiedemann algorithm. </title> <journal> Mathematics of Computation, </journal> <volume> 62(205) </volume> <pages> 333-350, </pages> <year> 1994. </year>
Reference-contexts: By construction, D (x) is of determinantal degree , proposition 2.2 tells us that N (x) D 1 (x) is irreducible. 2 Costs will be detailed in x5. Before concluding, let us remind the links with the extended Euclidean algorithm (or Berlekamp-Massey algorithm) used in <ref> [10] </ref> to solve sparse linear equations. The corresponding problem and algorithm to find a generator of a matrix sequence correspond to the solution of linear systems (11) and (9).
Reference: 11. <author> D. Coppersmith and S. Winograd. </author> <title> Matrix multiplication via arithmetic progressions. </title> <booktitle> In 19 th. Annual ACM Symp. Theory Comp., </booktitle> <pages> pages 1-6, </pages> <year> 1987. </year>
Reference-contexts: This gives O ~ (ndM (n)) over K. Then by proposition 4.1 the remaining cost is O (log (nd)) times the cost of a solution of L (nd). We know it is O ~ (M (nd)) using the 17 algorihm in [25]. The complexity given in <ref> [11] </ref> for matrix multiplication leads to the announced cost O ~ ((nd) 2:4 ): Using standard arithmetic, unlike the eliminations in [18,16] but as the probabilistic triangularization in [39], our algorithm is susceptable to a evaluation/interpolation scheme.
Reference: 12. <author> W. Eberly. </author> <title> Parallel independent subsets and matrix factorizations. </title> <booktitle> In Proc. 3rd IEEE Conference on Parallel and Distributed Processing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Then, we follow results of [17] for the rank, of [23] for the nullspace (O (log 2 n) steps) and of <ref> [12] </ref> for the subset of independent columns (fiO (log n) steps), and we use, as proposed in [13], the parallel handling of [25] for the block Krylov system involved in proposition 4.1 (fiO (log n) steps).
Reference: 13. <author> M. Giesbrecht. </author> <title> Nearly optimal algorithms for canonical matrix forms. </title> <journal> SIAM Journal on Computing, </journal> <note> 1994. To appear. </note>
Reference-contexts: Then, we follow results of [17] for the rank, of [23] for the nullspace (O (log 2 n) steps) and of [12] for the subset of independent columns (fiO (log n) steps), and we use, as proposed in <ref> [13] </ref>, the parallel handling of [25] for the block Krylov system involved in proposition 4.1 (fiO (log n) steps). The Hermite form is computed from the Popov form in time O (log 4 (nd)) using O ((nd) 1=2 M (nd)) processors.
Reference: 14. <author> I. Gohberg, P. Lancaster, and L. Rodman. </author> <title> Matrix polynomials. </title> <publisher> Academic Press, </publisher> <address> New-York, </address> <year> 1982. </year>
Reference-contexts: But, as emphasized by <ref> [14] </ref>, to study a nonsingular matrix polynomial P (x) it is natural to study systems for which the transfer function is H fl (x) = P 1 (x). Precisely we are going to see that H fl (x) leads to a suitable choice H (x) to compute T (x).
Reference: 15. <author> W.B. Gragg and A. Lindquist. </author> <title> On the partial realization problem. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 50 </volume> <pages> 277-319, </pages> <year> 1983. </year>
Reference: 16. <author> J.L. Hafner and K.S. Mc Curley. </author> <title> Asymptotically fast triangularization of matrices over rings. </title> <journal> SIAM J. Comput., </journal> <volume> 20(6) </volume> <pages> 1068-1083, </pages> <year> 1991. </year>
Reference-contexts: Such a triangularization is computed in time O ~ (ndM (n)) <ref> [16] </ref> if M (n) is the complexity of matrix product. The same complexity holds for the best randomized solution [39]. Thus, a first problem was to apply fast matrix multiplication to the Hermite form computation.
Reference: 17. <author> O. Ibarra, S. Moran, and L.E. Rosier. </author> <title> A note on the parallel complexity of computing the rank of order n matrices. </title> <journal> Information Processing Letters, </journal> <volume> 11:162, </volume> <year> 1980. </year>
Reference-contexts: Together with the cost of the operations on polynomials of degree nd, in time O (log (nd)) using O ~ (nd) processors, this gives O ~ (n 3=2 dM (n)). Then, we follow results of <ref> [17] </ref> for the rank, of [23] for the nullspace (O (log 2 n) steps) and of [12] for the subset of independent columns (fiO (log n) steps), and we use, as proposed in [13], the parallel handling of [25] for the block Krylov system involved in proposition 4.1 (fiO (log n)
Reference: 18. <author> C.S. Iliopoulos. </author> <title> Worst-case complexity bounds on algorithms for computing the canonical structure of finite abelian groups and the Hermite and Smith normal forms of an integer matrix. </title> <journal> SIAM J. Comput., </journal> <volume> 18(4) </volume> <pages> 658-669, </pages> <year> 1989. </year>
Reference-contexts: F. Viallet, F-38031 Grenoble Cedex, Gilles.Villard@imag.fr. 5 * Sequential point of view. Over an abstract field K the Hermite form can be computed in sequential time O ~ (n 4 d) <ref> [18] </ref>. To consider a unique parameter, let be such that n = O () and d = O (). The previous complexity is O ~ ( 5 ). <p> This latter problem will be solved using block Hankel matrices (being inspired by the scalar case). Using standard polynomial and matrix multiplications the best previous algorithms was requiring O (n 5 d 2 ) = O ( 7 ) arithmetic operations for the elimination over polynomials in <ref> [18] </ref> or O (n 4 d 3 ) = O ( 7 ) for the elimination over constants in [27].
Reference: 19. <author> N. Jacobson. </author> <title> Lectures in Abstract Algebra II, Linear Algebra. </title> <publisher> Springer-Verlag, </publisher> <year> 1953. </year>
Reference-contexts: The cost is that of O (log ()) solutions of L (). The fact that nd terminates the proof. 2 4.2 Modules over a P.I.D These results can be expressed using the language of modules. Well known from a theoretical point of view <ref> [19] </ref>, this approach has not been often use to derive practical algorithms for computing matrix normal forms [28,1,31]. In [31] the author computes transformations for normal forms of matrices over a field (under similarity) from transformations for corresponding normal forms of matrix polynomials (under unimodular equivalence).
Reference: 20. <author> T. Kailath. </author> <title> Linear systems. </title> <publisher> Prentice Hall, </publisher> <year> 1980. </year>
Reference-contexts: Introduction The problem of computing the greatest common divisor (gcd) of scalar polynomials in K [x] (K is a commutative field) or of polynomial matrices in M m;n (K [x]) has attracted a lot of attention and has many applications in linear systems, control and realization theory, see <ref> [20] </ref> and references therein. The reverse approach has also being widely considered, that is the use of linear system theory and especially of the theory of realizations [21,15] for the computation of scalar polynomial gcd's [2,3]. <p> Our scheme of proof is inspired from the scalar case in [6,7]. We give two propositions. Their two assertions correspond to the "if" and the "only if" part respectively of the following theorem which proof is omitted. Theorem 2.1 (theorem 6.5-1, <ref> [20] </ref>). A matrix fraction description is irreducible if and only if the determinantal degree of the denominator matrix is the dimension of any minimal realization of the fraction (right and left irreducible descriptions of the same fraction have the same denominator determinantal degree). The following result is derived from [26,20]. <p> Proof (Compare proposition 2.1 in [6] for the scalar polynomial gcd). By minimality of we know that rank M ( + i) = rank M (1) = , i 0 (lemma 6.5-7, <ref> [20] </ref>). We apply the "if" part of theorem 2.1. Since N (x) D 1 (x) is irreducible, the determinantal degree of D (x) is . Using proposition 1.1, D (x) can be brought in Popov form T (x) up to a right unimodular factor: D (x)V (x) = T (x). <p> Thus T 0 (x) has determinantal degree strictly lower than . We also construct the associated numerator S 0 (x) using <ref> [20] </ref>: 2 4 ffi1 S 0 3 5 = 6 6 H 1 0 : : : 0 . . . . . . . . . 0 3 7 5 6 T 0 . . . 1 7 11 Now, the expansions of H (x) using C 1 (x) L <p> is L (x 1 )T 0 (x 1 ) C (x 1 )S 0 (x 1 ) j 0 mod x L (x)T 0 (x) C (x)S 0 (x) = 0: Now, since C 1 (x) L (x) is irreducible, the determinantal degree of C (x) must be (lemma 6.3-8, <ref> [20] </ref>) at most equal to the one of T 0 (x) and thus strictly lower than . This contradicts the irreducibility of the fractions. 2 Proposition 2.2 Let H (x) = N (x)D 1 (x) be as in proposition 2.1. <p> This is the "only if" part of theorem 2.1 and corresponds to the method proposed in [26] to compute a minimal realization. Indeed, once D (x) in Popov form is computed, a satisfying triplet (C; A; B) can be readily obtained <ref> [20] </ref>. If D (x) is nonsingular and satisfies (11) then one can built a description N (x) D 1 (x) of H (x) (using arguments similar to those used in the proof of proposition 2.1). <p> And since the determinantal degree of the denominator is we know that the fraction is irreducible. Using for instance lemma 6.5-5 in <ref> [20] </ref>, we get that D (x) = D (x)G 1 (x) where G (x) is a gcrd of N (x) and D (x) (the fraction has been "simplified by" G (x)). 2 In the rest of the paper, we will view the above propositions as giving a procedure to reduce a <p> Proposition 4.1 The column Hermite normal form L H (x) of T (x) in Popov form as given by proposition 3.1, can be computed in O (log (nd)) solutions of L (nd). Proof. We do not detail the proof, it is based on classical results in <ref> [20] </ref>. By construction (x6.4.4, [20]), H (x) = C (xI A) 1 B = N (x)T 1 (x): Since (C; A; B) is a minimal realization of H (x) then [20] there exists a matrix (x) in M ;n (K [x]) right coprime with D (x) such that (xI A) 1 <p> Proof. We do not detail the proof, it is based on classical results in <ref> [20] </ref>. By construction (x6.4.4, [20]), H (x) = C (xI A) 1 B = N (x)T 1 (x): Since (C; A; B) is a minimal realization of H (x) then [20] there exists a matrix (x) in M ;n (K [x]) right coprime with D (x) such that (xI A) 1 B = (x)T 1 <p> Proof. We do not detail the proof, it is based on classical results in <ref> [20] </ref>. By construction (x6.4.4, [20]), H (x) = C (xI A) 1 B = N (x)T 1 (x): Since (C; A; B) is a minimal realization of H (x) then [20] there exists a matrix (x) in M ;n (K [x]) right coprime with D (x) such that (xI A) 1 B = (x)T 1 (x): Now, L H (x) is right equivalent to T (x) thus the same holds for it and for a matrix (x): (xI A) 1 B
Reference: 21. <author> R.E. Kalman, P. Falb, and M. Arbib. </author> <title> Topics in mathematical system theory. </title> <publisher> McGraw-Hill, </publisher> <address> New-York, </address> <year> 1969. </year>
Reference: 22. <author> E. Kaltofen, M.S. Krishnamoorthy, and B.D. Saunders. </author> <title> Fast parallel computation of Hermite and Smith forms of polynomials matrices. </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 8 4, </volume> <pages> pp 683-690, </pages> <year> 1987. </year>
Reference-contexts: Very few algorithms exist that compute the Hermite form fast using polynomially many processors. The solutions in [18,16] are elimination processes and are thus highly sequential. The problem has been shown to belong to the class N C in <ref> [22] </ref>. Nevertheless, this latter approach involves O (n 2 d) structured linear systems of dimension n 3 d. Since quite prohibitive, the cost has not been precisely computed by the authors.
Reference: 23. <author> E. Kaltofen and V. Pan. </author> <title> Processor efficient parallel solution of linear systems over an abstract field. </title> <booktitle> In Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architecture. </booktitle> <address> ACM-Press, </address> <year> 1991. </year>
Reference-contexts: Together with the cost of the operations on polynomials of degree nd, in time O (log (nd)) using O ~ (nd) processors, this gives O ~ (n 3=2 dM (n)). Then, we follow results of [17] for the rank, of <ref> [23] </ref> for the nullspace (O (log 2 n) steps) and of [12] for the subset of independent columns (fiO (log n) steps), and we use, as proposed in [13], the parallel handling of [25] for the block Krylov system involved in proposition 4.1 (fiO (log n) steps).
Reference: 24. <author> N. Karcanias and M. Mitrouli. </author> <title> A matrix pencil based numerical method for the computation of the gcd of polynomials. </title> <journal> IEEE Trans. Autom. Contr., </journal> <volume> 39(5) </volume> <pages> 977-981, </pages> <year> 1994. </year>
Reference-contexts: The reverse approach has also being widely considered, that is the use of linear system theory and especially of the theory of realizations [21,15] for the computation of scalar polynomial gcd's [2,3]. This has led to various computational results: giving either efficient numerical procedures as for instance developped in <ref> [24] </ref> or both sequential and parallel complexity results [6,7]. From these latter points of view, the multivariable case, involving computations with polynomial matrices [26], is much less studied. <p> Thus one has to take ffi = nd and to deal with M (ffi) = M (nd) which is of dimensions mnd fi n 2 d. This dual point of view, when applied to the computation of the gcd of n polynomials, resembles the method in <ref> [24] </ref> for this latter problem which is a particular case of the Popov or of the Hermite normal form for a 1 fi n row matrix.
Reference: 25. <author> W. Keller-Gehrig. </author> <title> Fast algorithms for the characteristic polynomial. </title> <journal> Theoretical Computer Science, </journal> <volume> 36 </volume> <pages> 309-317, </pages> <year> 1985. </year>
Reference-contexts: This linear system is computed by first isolating the lexicographically first independent columns of M (A; B; ). Since the matrix is block Krylov, we can restrict ourselves to fi 2 matrices as done in <ref> [25] </ref>. The cost is that of O (log ()) solutions of L (). The fact that nd terminates the proof. 2 4.2 Modules over a P.I.D These results can be expressed using the language of modules. <p> Conversely, given T (x) it is possible to compute its Hermite form if A and A 0 are known. Results for the computation of a shift-Hessenberg form of a matrix, such that the algorithm in <ref> [25] </ref>, could thus be used for another proof of proposition 4.1. <p> This gives O ~ (ndM (n)) over K. Then by proposition 4.1 the remaining cost is O (log (nd)) times the cost of a solution of L (nd). We know it is O ~ (M (nd)) using the 17 algorihm in <ref> [25] </ref>. The complexity given in [11] for matrix multiplication leads to the announced cost O ~ ((nd) 2:4 ): Using standard arithmetic, unlike the eliminations in [18,16] but as the probabilistic triangularization in [39], our algorithm is susceptable to a evaluation/interpolation scheme. <p> Then, we follow results of [17] for the rank, of [23] for the nullspace (O (log 2 n) steps) and of [12] for the subset of independent columns (fiO (log n) steps), and we use, as proposed in [13], the parallel handling of <ref> [25] </ref> for the block Krylov system involved in proposition 4.1 (fiO (log n) steps). The Hermite form is computed from the Popov form in time O (log 4 (nd)) using O ((nd) 1=2 M (nd)) processors. Over an arbitrary field, the costs are dominated by the solution of L (nd).
Reference: 26. <author> S.Y. Kung. </author> <title> Multivariable and multidimensional systems: analysis and design. </title> <type> PhD thesis, </type> <institution> Dpt. of Electrical Engineering, Stanford University, </institution> <month> June </month> <year> 1977. </year> <month> 19 </month>
Reference-contexts: This has led to various computational results: giving either efficient numerical procedures as for instance developped in [24] or both sequential and parallel complexity results [6,7]. From these latter points of view, the multivariable case, involving computations with polynomial matrices <ref> [26] </ref>, is much less studied. Precisely, this paper aims at using standard results from linear systems theory to obtain new and better complexity bounds for the computation of the Hermite normal form via the polynomial matrix gcd. <p> Proof (Compare proposition 3.6 in [6] for the scalar polynomial gcd). This is the "only if" part of theorem 2.1 and corresponds to the method proposed in <ref> [26] </ref> to compute a minimal realization. Indeed, once D (x) in Popov form is computed, a satisfying triplet (C; A; B) can be readily obtained [20].
Reference: 27. <author> S.E. Labhalla, H. Lombardi, and R. Marlin. </author> <title> Algorithmes de calcul de la reduction d'Hermite d'une matrice a coefficients polynomiaux. </title> <booktitle> Theoretical Computer Science, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Using standard polynomial and matrix multiplications the best previous algorithms was requiring O (n 5 d 2 ) = O ( 7 ) arithmetic operations for the elimination over polynomials in [18] or O (n 4 d 3 ) = O ( 7 ) for the elimination over constants in <ref> [27] </ref>. The new deterministic one, of cost O ~ (n 3 d 3 ) = O ~ ( 6 ) is thus also better and will match the cost of the best randomized solution [39] for a unimodular triangularization. * Parallel point of view. <p> Apart from using matrix equivalence and indentity (1), a gcrd can be computed from the generalized Sylvester resultant matrix in [8]. However this method seems to lead to the same conclusions than when using the Sylvester matrix in <ref> [27] </ref> and does not seem fully appropriate to parallelism.
Reference: 28. <author> H. Luneburg. </author> <title> On rational form of endomorphims : a primer to constructive algebra. </title> <booktitle> Wissenschaftsverlag, Mannheim, </booktitle> <year> 1987. </year>
Reference: 29. <author> C.C. MacDuffee. </author> <title> The theory of matrices. </title> <publisher> Chelsea, </publisher> <address> New-York, </address> <year> 1956. </year>
Reference-contexts: In the same way, the different normal forms will be either obtained by right equivalence and be column forms or by left equivalence and be row forms. For each form we give a unique definition by column operations, the other one follows obviously. There exists <ref> [29] </ref> a unique matrix L H (x) right equivalent to P (x) and under (column) Hermite normal form: L H (x) = P (x)V (x) with V (x) unimodular in M n;n (K [x]) . <p> In the following we will restrict ourselves to the case where N (x) is in M m;n (K [x]) and D (x) is nonsingular in M n;n (K [x]) . 1.1 Definition A gcrd of N (x) and D (x) is <ref> [29] </ref> any polynomial matrix G (x) such that i) G (x) is a right divisor of N (x) and D (x): for some matrices N (x) and D (x) we have N (x) = N (x)G (x); D (x) = D (x)G (x); ii) any other right divisor G 1 (x) <p> In order to define the gcrd uniquely, it is thus sufficient to consider any row normal form to represent the equivalence class of all the gcrd's. The row Hermite form is a possible choice; besides, one way to compute a gcrd <ref> [29] </ref> is to compute the Hermite form L H (x) of t [ t N (x); t D (x)] by left equivalence: U (x) N (x) 0 : (1) And G (x) is a gcrd of N (x) and D (x).
Reference: 30. <author> D. Manocha and J.F. Canny. </author> <title> Multipolynomial resultants and linear algebra. </title> <booktitle> In International Symposium on Symbolic and Algebraic Computation, </booktitle> <address> Berkeley California USA. </address> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: The univariate determinant computation in <ref> [30] </ref> may be viewed as the computation of a column reduced form. More generally, strong links are well knowns between column reduced forms and the determination of the finite eigenstructure of a polynomial matrix [41,40], and also with the factorization algorithm of Kublanovskaya [5].
Reference: 31. <author> T. Mulders. </author> <title> Computation of normal forms for matrices. </title> <booktitle> In Algoritmen In De Algebra, A Seminar on Algebraic Algorithms, </booktitle> <year> 1993. </year> <institution> University of Nijmegen, Netherlands. </institution>
Reference-contexts: Well known from a theoretical point of view [19], this approach has not been often use to derive practical algorithms for computing matrix normal forms [28,1,31]. In <ref> [31] </ref> the author computes transformations for normal forms of matrices over a field (under similarity) from transformations for corresponding normal forms of matrix polynomials (under unimodular equivalence).
Reference: 32. <author> K. Mulmuley. </author> <title> A fast parallel algorithm to compute the rank of a matrix over an arbitrary field. </title> <journal> Combinatorica, </journal> <volume> 7(1) </volume> <pages> 101-104, </pages> <year> 1987. </year>
Reference-contexts: Over an arbitrary field, the costs are dominated by the solution of L (nd). As for the processor bound of the solution of the nullspace problem in [9] this leads to n times [7] the processor bound for the rank. This latter problem is solved using the algorithm in <ref> [32] </ref>. The number of steps if O (log 3 (nd)) using O ~ ((nd) 4 M (nd)) processors. The unique transformation matrix is obtained with the same bounds. 2 Conclusion We have given new complexity bounds for some operations on matrix polynomials.
Reference: 33. <author> A. Nerode. </author> <title> Linear automaton transformations. </title> <journal> Proc. American Math. Soc., </journal> <volume> 9 </volume> <pages> 541-544, </pages> <year> 1958. </year>
Reference-contexts: We refer to this paper and to [42] for the reverse procedure (from a system-theoretic point of view this is Nerode's equivalence to compute special forms of realizations <ref> [33] </ref>). For T (x) in Popov form, the quotient K [x]-module Y = K n [x]= t T (x)K n [x] is a K [x]-module and also a vector space of dimension over K.
Reference: 34. <author> P. Ozello. </author> <title> Calcul exact des formes de Jordan et de Frobenius d'une matrice. </title> <type> PhD thesis, </type> <institution> Universite Scientifique et Medicale de Grenoble, France, </institution> <year> 1987. </year>
Reference-contexts: Since L H (x) is triangular, A 0 is Hessenberg, it is said to be either in shift-Hessenberg form [1] or in polycyclic form <ref> [34] </ref>. In addition, A 0 is similar to A. We can also define Y 0 = K n [x]= t L H (x)K n [x] and we get a commuting diagram similar to (14) with a coordinate function p 0 .
Reference: 35. <author> V.M. Popov. </author> <title> Invariant description of linear, time-invariant controllable systems. </title> <journal> SIAM J. Contr., </journal> <volume> 10 </volume> <pages> 252-264, </pages> <month> May </month> <year> 1972. </year>
Reference: 36. <author> F.P. Preparata and D.V. Sarwate. </author> <title> An improved parallel processor bound in fast matrix inversion. </title> <journal> Inform. Proc. Lett., </journal> <volume> 7 </volume> <pages> 148-150, </pages> <year> 1978. </year>
Reference-contexts: The computation can be performed in time O (log 3 (nd)) using O ~ ((nd) 4 M (nd)) = O ( 12:8 ) processors over an arbitrary field. Proof. Over a field of characteristic zero, we refer to <ref> [36] </ref> for matrix inversion. The number of processors required is O (n 1=2 M (n)) for O (log 2 n) arithmetic steps.
Reference: 37. <author> V. Ptak and H.K. Wimmer. </author> <title> On the Bezoutian for polynomial matrices. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 71 </volume> <pages> 267-272, </pages> <year> 1985. </year>
Reference: 38. <author> M.P. Quere and G. </author> <type> Villard. </type> <institution> Matrices quasi-Hankel et Bezoutiennes pour le calcul de la forme normale d'Hermite de matrices polynomiales. Preprint IMAG, Grenoble France, </institution> <year> 1996. </year>
Reference-contexts: These complexities does not express the boolean computational cost of the algorithms. We will not detail the corresponding studies in this paper. Further developments should be done <ref> [38] </ref> using the Bezoutian for matrix polynomials [4,37]. From results in [7] concerning the scalar polynomial gcd, very good bounds may be expected. The paper is organized as follows: we first remind some results on the polynomial matrix gcd in x1 and on the minimal realization problem in x2. <p> This methods compares favourably with the above one in the sens that it can be easily adapted for singular matrices ((x) can be build from a well chosen nonzero minor of P (x)) <ref> [38] </ref>. But its main drawback is that the degree of P fl (x) can be as large as nd. Thus one has to take ffi = nd and to deal with M (ffi) = M (nd) which is of dimensions mnd fi n 2 d.
Reference: 39. <author> A. Storjohann. </author> <title> Computation of Hermite and Smith normal forms of matrices. </title> <type> Master's thesis, </type> <institution> 1994. University of Waterloo, Canada. </institution>
Reference-contexts: Such a triangularization is computed in time O ~ (ndM (n)) [16] if M (n) is the complexity of matrix product. The same complexity holds for the best randomized solution <ref> [39] </ref>. Thus, a first problem was to apply fast matrix multiplication to the Hermite form computation. We will partially solve it by proposing a solution with deterministic sequential time O ~ (ndM (n) + M (nd)). <p> The new deterministic one, of cost O ~ (n 3 d 3 ) = O ~ ( 6 ) is thus also better and will match the cost of the best randomized solution <ref> [39] </ref> for a unimodular triangularization. * Parallel point of view. Very few algorithms exist that compute the Hermite form fast using polynomially many processors. The solutions in [18,16] are elimination processes and are thus highly sequential. The problem has been shown to belong to the class N C in [22]. <p> We know it is O ~ (M (nd)) using the 17 algorihm in [25]. The complexity given in [11] for matrix multiplication leads to the announced cost O ~ ((nd) 2:4 ): Using standard arithmetic, unlike the eliminations in [18,16] but as the probabilistic triangularization in <ref> [39] </ref>, our algorithm is susceptable to a evaluation/interpolation scheme. Since two polynomials of degree d are multiplied in O (n 2 d 2 ), the cost of matrix inversion is O (n 4 d 2 ).
Reference: 40. <author> P. Van Dooren and P. Dewilde. </author> <title> The eigenstructure of an arbitrary polynomial matrix: </title> <journal> computational aspects. Linear Algebra and its Applications, </journal> <volume> 50 </volume> <pages> 545-579, </pages> <year> 1983. </year>
Reference: 41. <author> P.M. Van Dooren, P. Dewilde, and J. Vandewalle. </author> <title> On the determination of the Smith-Macmillan form of a rational matrix from its Laurent expansion. </title> <journal> IEEE Trans. Cir. Sys., </journal> <volume> 26(3) </volume> <pages> 180-189, </pages> <month> March </month> <year> 1979. </year>
Reference: 42. <author> G. Villard. </author> <title> Fast parallel algorithms for matrix reduction to normal forms. </title> <type> Technical Report APACHE 16, </type> <institution> IMAG Grenoble France, </institution> <year> 1995. </year>
Reference-contexts: In [31] the author computes transformations for normal forms of matrices over a field (under similarity) from transformations for corresponding normal forms of matrix polynomials (under unimodular equivalence). We refer to this paper and to <ref> [42] </ref> for the reverse procedure (from a system-theoretic point of view this is Nerode's equivalence to compute special forms of realizations [33]).

References-found: 42

