URL: http://www.cs.cmu.edu/~seth/papers/jpdc.ps.Z
Refering-URL: http://www.cs.cmu.edu/~seth/papers.html
Root-URL: 
Title: Lazy Threads: Implementing a Fast Parallel Call  
Author: Seth Copen Goldstein Klaus Erik Schauser David E. Culler 
Affiliation: Computer Science Division, University of California-Berkeley  Department of Computer Science, University of California-Santa Barbara  Computer Science Division, University of California-Berkeley  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler activations: effective kernel support for the user-level management of parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1), </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: Cthreads is a run-time library which provides multiple threads of control and synchronization primitives for parallel programming at the level of the C language [7]. Scheduler activations reduce the overhead by moving fine-grained threads completely to the user level and relying on the kernel only for infrequent cases <ref> [1] </ref>. Synthesis is an operating systems kernel for a parallel and distributed computational environment which is interesting in our context because it integrates dynamic load balancing capabilities and applies dynamic compilation techniques [24].
Reference: [2] <author> A. W. Appel. </author> <title> Compiling with continuations. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Instead of passing the child two return addresses, the parent calls the child with a single address from which it can derive both addresses. At the implementation level, this use of multiple return addresses can be thought of as an extended version of continuation passing <ref> [2] </ref>, where the child is passed two different continuations, one for normal return and one for suspension. The compiler ensures that the suspension entry point precedes the normal return entry point by a fixed number of instructions.
Reference: [3] <author> Arvind and D. E. Culler. </author> <title> Dataflow architectures. </title> <booktitle> In Annual Reviews in Computer Science, </booktitle> <volume> volume 1, </volume> <pages> pages 225-253. </pages> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1986. </year>
Reference-contexts: Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations [9, 21, 26, 29, 30, 33, 35, 37, 39], and by supporting fine-grained parallel execution directly in hardware <ref> [3, 19, 31] </ref>. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations [9, 30, 26, 39, 37, 33, 17], and direct hardware support for fine-grained parallel execution <ref> [19, 3] </ref>. These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. The common goal is to reduce the overhead associated with managing the logical parallelism.
Reference: [4] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, P. Lisiecki, K. H. Randall, A. Shaw, and Y. Zhou. </author> <title> Cilk 1.1 reference manual. </title> <institution> MIT Lab for Comp. Sci., 545 Technology Square, </institution> <address> Cambridge, MA 02139, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. These approaches, among others, have been used in implementing parallel programming languages such as ABCL [40], CC++ [6], Charm [20], Cid [29], Cilk <ref> [4] </ref>, Concert [21], Id90 [9, 30], Mul-T [23], and Olden [5]. Still, a fork remains substantially more expensive than a simple sequential call. <p> These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk <ref> [4] </ref>, Olden [5], and Cid [29]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [5] <author> M.C. Carlisle, A. Rogers, J.H. Reppy, and L.J. Hendren. </author> <title> Early experiences with Olden (parallel programming). </title> <booktitle> In Languages and Compilers for Parallel Computing. 6th International Workshop Proceedings, </booktitle> <pages> pages 1-20. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: These approaches, among others, have been used in implementing parallel programming languages such as ABCL [40], CC++ [6], Charm [20], Cid [29], Cilk [4], Concert [21], Id90 [9, 30], Mul-T [23], and Olden <ref> [5] </ref>. Still, a fork remains substantially more expensive than a simple sequential call. Our goal is to support an unrestricted parallel thread model and yet bring the cost of thread creation, termination, and switching down to essentially the cost of a sequential call. <p> These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden <ref> [5] </ref>, and Cid [29]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole. <p> This system can be used to implement user-level thread packages directly within the ML language We use stacklets for efficient stack-based frame allocation in parallel programs. Previous work in [17] describes similar ideas for handling continuations efficiently. Olden <ref> [5] </ref> uses a spaghetti stack. In both systems, the allocation of a new stack frame always requires memory references and a garbage collector. Olden's thread model is more powerful than ours, since in Olden threads can migrate.
Reference: [6] <author> K.M. Chandy and C. Kesselman. </author> <title> Compositional C++: compositional parallel programming. </title> <booktitle> In Languages and Compilers for Parallel Computing. 5th International Workshop Proceedings, </booktitle> <pages> pages 124-44. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. These approaches, among others, have been used in implementing parallel programming languages such as ABCL [40], CC++ <ref> [6] </ref>, Charm [20], Cid [29], Cilk [4], Concert [21], Id90 [9, 30], Mul-T [23], and Olden [5]. Still, a fork remains substantially more expensive than a simple sequential call. <p> The C implementation shows that these primitives introduce little or no overhead over sequential programs. The Id90 implementation shows that for complete programs we achieve a substantial improvement over previous work. Our techniques can be applied to other programming languages <ref> [6, 40] </ref>, thread packages [11], and multithreaded execution models. In Section 7 we discuss related work. Our work, Lazy Threads, relies extensively on compiler optimizations and cannot simply be implemented with function calls to a user-level threads library without substantial loss of efficiency. <p> These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ <ref> [6] </ref>, Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [7] <author> E. C. Cooper and R. P. Draves. C-Threads. </author> <type> Technical Report CMU-CS-88-154, </type> <institution> Carnegie-Mellon University, </institution> <month> February </month> <year> 1988. </year>
Reference-contexts: We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages <ref> [11, 34, 7, 16] </ref>, compiler techniques and clever run-time representations [9, 30, 26, 39, 37, 33, 17], and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g. <p> We extend these two ideas to form synchronizers. Finally, many light-weight thread packages have been developed. Cthreads is a run-time library which provides multiple threads of control and synchronization primitives for parallel programming at the level of the C language <ref> [7] </ref>. Scheduler activations reduce the overhead by moving fine-grained threads completely to the user level and relying on the kernel only for infrequent cases [1].
Reference: [8] <author> D. Culler, A. Dusseau, S. Goldstein, S. Lumetta, T. von eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: These languages stand in contrast to languages with a single logical thread of control, such as High Performance Fortran [22], or a fixed set of threads, such as Split-C <ref> [8] </ref> or MPI [12]. There are many reasons to have the logical parallelism of the program exceed the physical parallelism of the machine, including ease of expression and better resource utilization in the presence of synchronization delays, load imbalance, and long communication latency [26, 39].
Reference: [9] <author> D. E. Culler, S. C. Goldstein, K. E. Schauser, and T. von Eicken. </author> <title> TAM a compiler controlled threaded abstract machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 347-370, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Unfortunately, a parallel call or thread fork is fundamentally more expensive than a sequential call because of the thread and storage management, data transfer, scheduling, and synchronization involved. Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations <ref> [9, 21, 26, 29, 30, 33, 35, 37, 39] </ref>, and by supporting fine-grained parallel execution directly in hardware [3, 19, 31]. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. These approaches, among others, have been used in implementing parallel programming languages such as ABCL [40], CC++ [6], Charm [20], Cid [29], Cilk [4], Concert [21], Id90 <ref> [9, 30] </ref>, Mul-T [23], and Olden [5]. Still, a fork remains substantially more expensive than a simple sequential call. Our goal is to support an unrestricted parallel thread model and yet bring the cost of thread creation, termination, and switching down to essentially the cost of a sequential call. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations <ref> [9, 30, 26, 39, 37, 33, 17] </ref>, and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. <p> These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 <ref> [9, 30] </ref>, CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole. <p> Our work grew out of previous efforts to implement the non-strict functional language Id90 for commodity parallel machines. Our earlier work developed a Threaded Abstract Machine (TAM) which serves as an intermediate compilation target <ref> [9] </ref>. The two key differences between this work and TAM are that under TAM calls are always parallel, and due to TAM's scheduling hierarchy, calling another function does not immediately transfer control. <p> The programs are described in <ref> [9] </ref>. <p> Our threads, however, are stronger than those in TAM <ref> [9] </ref> and in some user-level threads packages, e.g. Chorus [34], which require that the maximum stack size be specified upon thread creation so that memory can be preallocated. 2 Another approach would have the compiler package up the following code as a separate function and use the 2-way fork pattern.
Reference: [10] <author> D. R. Engler, D. K. Lowenthal, and Andrews G. R. </author> <title> Shared Filaments: efficient fine-grain parallelism on shared-memory multiprocessors. </title> <type> Technical Report TR 93-13a, </type> <institution> University of Arizona, </institution> <month> April </month> <year> 1993. </year> <month> 33 </month>
Reference-contexts: They take a diametrically opposing point of view in that all calls, sequential or parallel, use the same representation. This triples the direct function call/return 28 overhead and prevents the use of registers. A much simpler thread model is advocated in Shared Filaments <ref> [10] </ref> and Distributed Filaments [13]. A filament is a very lightweight thread which does not have a stack associated with it. This works well when a thread does not fork other threads.
Reference: [11] <author> J.E. Faust and H.M. Levy. </author> <title> The performance of an object-oriented threads package. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 278-88, </pages> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: The C implementation shows that these primitives introduce little or no overhead over sequential programs. The Id90 implementation shows that for complete programs we achieve a substantial improvement over previous work. Our techniques can be applied to other programming languages [6, 40], thread packages <ref> [11] </ref>, and multithreaded execution models. In Section 7 we discuss related work. Our work, Lazy Threads, relies extensively on compiler optimizations and cannot simply be implemented with function calls to a user-level threads library without substantial loss of efficiency. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages <ref> [11, 34, 7, 16] </ref>, compiler techniques and clever run-time representations [9, 30, 26, 39, 37, 33, 17], and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g.
Reference: [12] <author> Message Passing Interface Forum. </author> <title> Mpi: a message-passing interface standard. </title> <journal> International Journal of Supercomputer Applications and High Performance Computing, </journal> <volume> vol.8,(no.3-4):169-416, </volume> <month> Fall-Winter </month> <year> 1994. </year>
Reference-contexts: These languages stand in contrast to languages with a single logical thread of control, such as High Performance Fortran [22], or a fixed set of threads, such as Split-C [8] or MPI <ref> [12] </ref>. There are many reasons to have the logical parallelism of the program exceed the physical parallelism of the machine, including ease of expression and better resource utilization in the presence of synchronization delays, load imbalance, and long communication latency [26, 39].
Reference: [13] <author> V.W. Freeh, D.K. Lowenthal, and G.R. Andrews. </author> <title> Distributed Filaments: efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 201-13. </pages> <publisher> USENIX Assoc, </publisher> <year> 1994. </year>
Reference-contexts: They take a diametrically opposing point of view in that all calls, sequential or parallel, use the same representation. This triples the direct function call/return 28 overhead and prevents the use of registers. A much simpler thread model is advocated in Shared Filaments [10] and Distributed Filaments <ref> [13] </ref>. A filament is a very lightweight thread which does not have a stack associated with it. This works well when a thread does not fork other threads.
Reference: [14] <author> S. C. Goldstein, D. E. Culler, and K. E. Schauser. </author> <title> Lazy Threads, Stacklets, and Synchronizers: Enabling primitives for compiling parallel languages. </title> <type> Technical report, </type> <institution> University of California at Berkeley, </institution> <year> 1995. </year>
Reference-contexts: At this time our Id90 compiler uses a primitive version of explicit seed creation. In addition to the primitives described so far, the compiler uses strands, a mechanism to support fine-grained parallelism within a thread <ref> [14] </ref>. We see a performance improvement ranging from 1.1 times faster for coarse-grained programs, like blocked matrix multiply (MMT), to 2.7 times faster for more finely-grained programs. We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity.
Reference: [15] <author> D. Grunwald, B. Calder, S. Vajracharya, and H. Srinivasan. </author> <title> Heaps o' Stacks: combined heap-based activation allocation for parallel programs. </title> <address> URL: http://www.cs.colorado.edu/ grunwald/, </address> <year> 1994. </year>
Reference-contexts: integrating the data into the parent frame and indicating to the parent that its child has returned. 3.3 Compilation To reduce the cost of frame allocation even further, we construct a call graph which enables us to determine for all but the recursive calls whether an overflow check is needed <ref> [15] </ref>. Each function has two entry points, one that checks stacklet overflow and another that does not. If the compiler can determine that no check is needed, it uses the latter entry point.
Reference: [16] <author> M. Haines, D. Cronk, and P. Mehrotra. </author> <title> On the design of Chant: a talking threads package. </title> <booktitle> In Proceedings Supercomputing '94 (Cat. No.94CH34819), </booktitle> <pages> pages 350-9. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1994. </year>
Reference-contexts: We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages <ref> [11, 34, 7, 16] </ref>, compiler techniques and clever run-time representations [9, 30, 26, 39, 37, 33, 17], and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g. <p> Synthesis is an operating systems kernel for a parallel and distributed computational environment which is interesting in our context because it integrates dynamic load balancing capabilities and applies dynamic compilation techniques [24]. Chant <ref> [16] </ref> is a lightweight threads package which is used in the implementation of an HPF extension called Opus [25]. Chant provides an interface for lightweight, user-level threads which have the capability of communication and synchronization across separate address spaces.
Reference: [17] <author> R. Hieb, R. Kent Dybvig, and C. Bruggeman. </author> <title> Representing control in the presence of first-class continuations. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 66-77, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations <ref> [9, 30, 26, 39, 37, 33, 17] </ref>, and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. <p> This system can be used to implement user-level thread packages directly within the ML language We use stacklets for efficient stack-based frame allocation in parallel programs. Previous work in <ref> [17] </ref> describes similar ideas for handling continuations efficiently. Olden [5] uses a spaghetti stack. In both systems, the allocation of a new stack frame always requires memory references and a garbage collector. Olden's thread model is more powerful than ours, since in Olden threads can migrate.
Reference: [18] <author> U. H olzle, C. Chambers, and D. Ungar. </author> <title> Debugging optimized code with dynamic deoptimization. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 32-43, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The way thread seeds encode future work builds on the use of multiple offsets from a single return address to handle special cases. This technique was used in SOAR [36]. It was also applied to Self, which uses parent controlled return continuations to handle debugging <ref> [18] </ref>. We extend these two ideas to form synchronizers. Finally, many light-weight thread packages have been developed. Cthreads is a run-time library which provides multiple threads of control and synchronization primitives for parallel programming at the level of the C language [7].
Reference: [19] <author> H. F. Jordan. </author> <title> Performance measurement on HEP a pipelined MIMD computer. </title> <booktitle> In Proc. of the 10th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Stockholm, Sweden, </address> <month> June </month> <year> 1983. </year>
Reference-contexts: Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations [9, 21, 26, 29, 30, 33, 35, 37, 39], and by supporting fine-grained parallel execution directly in hardware <ref> [3, 19, 31] </ref>. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations [9, 30, 26, 39, 37, 33, 17], and direct hardware support for fine-grained parallel execution <ref> [19, 3] </ref>. These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. The common goal is to reduce the overhead associated with managing the logical parallelism.
Reference: [20] <author> L.V. Kale and S. Krishnan. CHARM++: </author> <title> a portable concurrent object oriented system based on C++. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 91-108, </pages> <month> Oct. </month> <year> 1993. </year> <month> 34 </month>
Reference-contexts: In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. These approaches, among others, have been used in implementing parallel programming languages such as ABCL [40], CC++ [6], Charm <ref> [20] </ref>, Cid [29], Cilk [4], Concert [21], Id90 [9, 30], Mul-T [23], and Olden [5]. Still, a fork remains substantially more expensive than a simple sequential call. <p> These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm <ref> [20] </ref>, Opus [25], Cilk [4], Olden [5], and Cid [29]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [21] <author> V. Karamcheti and A. Chien. </author> <title> Concert: efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Proceedings SUPERCOMPUTING '93, </booktitle> <pages> pages 598-607. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Unfortunately, a parallel call or thread fork is fundamentally more expensive than a sequential call because of the thread and storage management, data transfer, scheduling, and synchronization involved. Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations <ref> [9, 21, 26, 29, 30, 33, 35, 37, 39] </ref>, and by supporting fine-grained parallel execution directly in hardware [3, 19, 31]. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. These approaches, among others, have been used in implementing parallel programming languages such as ABCL [40], CC++ [6], Charm [20], Cid [29], Cilk [4], Concert <ref> [21] </ref>, Id90 [9, 30], Mul-T [23], and Olden [5]. Still, a fork remains substantially more expensive than a simple sequential call.
Reference: [22] <author> C. koelbel, D. Loveman, R. Schreiber, G. Steel Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: These languages stand in contrast to languages with a single logical thread of control, such as High Performance Fortran <ref> [22] </ref>, or a fixed set of threads, such as Split-C [8] or MPI [12].
Reference: [23] <author> D.A. Kranz, Jr. Halstead, R.H., and E. Mohr. Mul-T: </author> <title> a high-performance parallel Lisp. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 81-90, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: These approaches, among others, have been used in implementing parallel programming languages such as ABCL [40], CC++ [6], Charm [20], Cid [29], Cilk [4], Concert [21], Id90 [9, 30], Mul-T <ref> [23] </ref>, and Olden [5]. Still, a fork remains substantially more expensive than a simple sequential call. Our goal is to support an unrestricted parallel thread model and yet bring the cost of thread creation, termination, and switching down to essentially the cost of a sequential call. <p> A previous approach, load-based inlining, provides both an eager parallel call and a sequential call for each fork. When a potentially parallel call is encountered the load characteristics of the parallel machine are used to decide whether to execute it sequentially (inline it) or execute it in parallel <ref> [23] </ref>. Unfortunately, these decisions introduce overhead and are irrevocable, which can lead to serious load imbalances or deadlock. Another approach, taken by Lazy Task Creation, is to perform the fork like a sequential call, but record the point of execution after the fork (i.e. the continuation) in the parent. <p> These approaches have been used to implement many parallel languages, e.g. Mul-T <ref> [23] </ref>, Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. The common goal is to reduce the overhead associated with managing the logical parallelism. <p> One of the simplest schemes is load based inlining, which uses load characteristics of the parallel machine to decide at the time a potentially parallel call is encountered whether to execute it sequentially (inline it) or execute it in parallel <ref> [23] </ref>. This has the advantage of dynamically increasing the granularity of the program. However, these decisions are irrevocable, which can lead to serious load imbalances or deadlock. Our approach builds on lazy task creation (LTC) which maintains a data structure to record previously 27 encountered parallel calls [26].
Reference: [24] <author> H. Massalin and C. Pu. </author> <title> Threads and input/output in the Synthesis kernel. </title> <booktitle> In Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: Synthesis is an operating systems kernel for a parallel and distributed computational environment which is interesting in our context because it integrates dynamic load balancing capabilities and applies dynamic compilation techniques <ref> [24] </ref>. Chant [16] is a lightweight threads package which is used in the implementation of an HPF extension called Opus [25]. Chant provides an interface for lightweight, user-level threads which have the capability of communication and synchronization across separate address spaces.
Reference: [25] <author> P. Mehrotra and M. Haines. </author> <title> An overview of the Opus language and runtime system. </title> <booktitle> In Languages and Compilers for Parallel Computing. 7th International Workshop Proceedings, </booktitle> <pages> pages 346-60. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <month> AN4917658. </month>
Reference-contexts: These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus <ref> [25] </ref>, Cilk [4], Olden [5], and Cid [29]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole. <p> Chant [16] is a lightweight threads package which is used in the implementation of an HPF extension called Opus <ref> [25] </ref>. Chant provides an interface for lightweight, user-level threads which have the capability of communication and synchronization across separate address spaces.
Reference: [26] <author> E. Mohr, D.A. Kranz, and Jr. Halstead, </author> <title> R.H. Lazy task creation: a technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol.2,(no.3):264-80, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: There are many reasons to have the logical parallelism of the program exceed the physical parallelism of the machine, including ease of expression and better resource utilization in the presence of synchronization delays, load imbalance, and long communication latency <ref> [26, 39] </ref>. Moreover, the semantics of the language or the synchronization primitives may allow dependencies to be expressed in such a way that progress can be made only by interleaving multiple threads, effectively running them in parallel even on a single processor [28]. <p> Unfortunately, a parallel call or thread fork is fundamentally more expensive than a sequential call because of the thread and storage management, data transfer, scheduling, and synchronization involved. Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations <ref> [9, 21, 26, 29, 30, 33, 35, 37, 39] </ref>, and by supporting fine-grained parallel execution directly in hardware [3, 19, 31]. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> It is the point in the computation where an independent thread may be, but is not necessarily required to be created. This property has previously been exploited through load-based inlining and Lazy Task Creation (LTC), which attempt to execute parallel calls sequentially when parallelism is not required <ref> [26] </ref>. In this paper, we go much further, performing a potentially parallel call almost exactly like a stack-based sequential call. Our code generation strategy avoids creating task descriptors, initializing synchronization variables, or even explicitly enqueuing tasks. <p> This highly optimized assembly version runs only 18% faster than the synchronizer version, which incorporates all the mechanisms for multithreading support. Next, we look at the efficiency of work-stealing combined with seeds on a parallel machine by examining the performance of the synthetic benchmark proposed in <ref> [26] </ref> and also used in [39]. Grain is a doubly recursive program that computes a sum, where each leaf executes a loop of g instructions, thus allowing us to control the granularity of the leaf nodes. We compare its efficiency to that of sequential C code compiled by gcc. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain <ref> [26, 39] </ref>. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations [9, 30, 26, 39, 37, 33, 17], and direct hardware support for fine-grained parallel execution [19, 3]. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations <ref> [9, 30, 26, 39, 37, 33, 17] </ref>, and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. <p> This has the advantage of dynamically increasing the granularity of the program. However, these decisions are irrevocable, which can lead to serious load imbalances or deadlock. Our approach builds on lazy task creation (LTC) which maintains a data structure to record previously 27 encountered parallel calls <ref> [26] </ref>. When a processor runs out of work, dynamic load balancing can be effected by stealing previously created lazy tasks from other processors. These ideas were studied for Mul-T running on shared-memory machines.
Reference: [27] <author> J. G. Morrisett and A. Tolmach. </author> <title> Procs and Locks: a portable multiprocessing platform for standard ML of New Jersey. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Unlike the techniques we use, it restricts the behavior of the program in an attempt to reduce the cost of futures. Leapfrogging has been implemented using Cthreads, a light-weight thread package. Another interesting approach, based on SML/NJ, represents threads by simple continuations <ref> [27] </ref>, which in turn can be represented efficiently. Since continuations are supported by the language model, important aspects of the thread system, such as scheduling and synchronization, can be described in the language itself.
Reference: [28] <author> R. S. Nikhil. </author> <title> Id (version 88.0) reference manual. </title> <type> Technical Report CSG Memo 284, </type> <institution> MIT Lab for Comp. Sci., </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: Moreover, the semantics of the language or the synchronization primitives may allow dependencies to be expressed in such a way that progress can be made only by interleaving multiple threads, effectively running them in parallel even on a single processor <ref> [28] </ref>. Regardless of how the dynamic logical parallelism is expressed at the language level, the underlying execution model has two key features. First, the control model supports the dynamic creation of multiple threads with independent lifetimes. Second, each thread may require an unbounded stack.
Reference: [29] <author> R. S. Nikhil. Cid: </author> <title> A parallel, shared memory C for distributed-memory machines. </title> <booktitle> In Languages and Compilers for Parallel Computing. 7th International Workshop Proceedings. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Unfortunately, a parallel call or thread fork is fundamentally more expensive than a sequential call because of the thread and storage management, data transfer, scheduling, and synchronization involved. Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations <ref> [9, 21, 26, 29, 30, 33, 35, 37, 39] </ref>, and by supporting fine-grained parallel execution directly in hardware [3, 19, 31]. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. These approaches, among others, have been used in implementing parallel programming languages such as ABCL [40], CC++ [6], Charm [20], Cid <ref> [29] </ref>, Cilk [4], Concert [21], Id90 [9, 30], Mul-T [23], and Olden [5]. Still, a fork remains substantially more expensive than a simple sequential call. <p> These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid <ref> [29] </ref>. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [30] <author> R.S. Nikhil. </author> <title> A multithreaded implementation of Id using P-RISC graphs. </title> <booktitle> In Languages and Compilers for Parallel Computing. 6th International Workshop Proceedings, </booktitle> <pages> pages 390-405. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Unfortunately, a parallel call or thread fork is fundamentally more expensive than a sequential call because of the thread and storage management, data transfer, scheduling, and synchronization involved. Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations <ref> [9, 21, 26, 29, 30, 33, 35, 37, 39] </ref>, and by supporting fine-grained parallel execution directly in hardware [3, 19, 31]. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. These approaches, among others, have been used in implementing parallel programming languages such as ABCL [40], CC++ [6], Charm [20], Cid [29], Cilk [4], Concert [21], Id90 <ref> [9, 30] </ref>, Mul-T [23], and Olden [5]. Still, a fork remains substantially more expensive than a simple sequential call. Our goal is to support an unrestricted parallel thread model and yet bring the cost of thread creation, termination, and switching down to essentially the cost of a sequential call. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations <ref> [9, 30, 26, 39, 37, 33, 17] </ref>, and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. <p> These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 <ref> [9, 30] </ref>, CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [31] <author> M.D. Noakes, D.A. Wallach, and W.J. Dally. </author> <title> The J-Machine multicomputer: an architectural evalua-tion. </title> <booktitle> In Computer Architecture News, </booktitle> <pages> pages 224-35, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations [9, 21, 26, 29, 30, 33, 35, 37, 39], and by supporting fine-grained parallel execution directly in hardware <ref> [3, 19, 31] </ref>. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread.
Reference: [32] <author> A. Rogers, M.C. Carlisle, J.H. Reppy, and L.J. Hendren. </author> <title> Supporting dynamic data structures on distributed-memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol.17,(no.2):233-63, </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: Olden's thread model is more powerful than ours, since in Olden threads can migrate. The idea is that a thread computation which is following references to unstructured heap-allocated data might increase locality if migration occurs <ref> [32] </ref>. On the other hand, this model requires migrating the current call frame as well as disallowing local pointers to other frames on the stack.
Reference: [33] <author> A. Rogers, J. Reppy, and L. Hendren. </author> <title> Supporting SPMD execution for dynamic data structures. </title> <booktitle> In Languages and Compilers for Parallel Computing. 5th International Workshop Proceedings, </booktitle> <pages> pages 192-207. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Unfortunately, a parallel call or thread fork is fundamentally more expensive than a sequential call because of the thread and storage management, data transfer, scheduling, and synchronization involved. Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations <ref> [9, 21, 26, 29, 30, 33, 35, 37, 39] </ref>, and by supporting fine-grained parallel execution directly in hardware [3, 19, 31]. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations <ref> [9, 30, 26, 39, 37, 33, 17] </ref>, and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29].
Reference: [34] <author> M. Rozier, V. Abrossimov, F. Armand, I. Boule, M. Gien, M. Guillemont, F. Herrman, C. Kaiser, S. Langlois, P. Leonard, and W. Neuhauser. </author> <title> Overview of the CHORUS distributed operating system. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 39-69. </pages> <publisher> USENIX Assoc, </publisher> <year> 1992. </year>
Reference-contexts: We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages <ref> [11, 34, 7, 16] </ref>, compiler techniques and clever run-time representations [9, 30, 26, 39, 37, 33, 17], and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g. <p> Our threads, however, are stronger than those in TAM [9] and in some user-level threads packages, e.g. Chorus <ref> [34] </ref>, which require that the maximum stack size be specified upon thread creation so that memory can be preallocated. 2 Another approach would have the compiler package up the following code as a separate function and use the 2-way fork pattern.
Reference: [35] <author> K. Taura, S. Matsuoka, and A. Yonezawa. StackThreads: </author> <title> an abstract machine for scheduling fine-grain threads on stock CPUs. </title> <booktitle> In Theory and Practice of Parallel Programming. International Workshop TPPP '94. Proceedings, </booktitle> <pages> pages 121-36. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <month> AN4986592. </month>
Reference-contexts: Unfortunately, a parallel call or thread fork is fundamentally more expensive than a sequential call because of the thread and storage management, data transfer, scheduling, and synchronization involved. Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations <ref> [9, 21, 26, 29, 30, 33, 35, 37, 39] </ref>, and by supporting fine-grained parallel execution directly in hardware [3, 19, 31]. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> The idea is that a thread computation which is following references to unstructured heap-allocated data might increase locality if migration occurs [32]. On the other hand, this model requires migrating the current call frame as well as disallowing local pointers to other frames on the stack. StackThreads <ref> [35] </ref> uses both a stack and the heap for storing activation frames in an attempt to reduce overhead for fine-grained programs running on a single processor. Activation frames are initially placed on the stack and if they block, they are moved onto the heap.
Reference: [36] <author> D. M. Ungar. </author> <title> The design and evaluation of a high performance Smalltalk system. </title> <publisher> ACM distinguished dissertations. MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: The way thread seeds encode future work builds on the use of multiple offsets from a single return address to handle special cases. This technique was used in SOAR <ref> [36] </ref>. It was also applied to Self, which uses parent controlled return continuations to handle debugging [18]. We extend these two ideas to form synchronizers. Finally, many light-weight thread packages have been developed.
Reference: [37] <author> M.T. Vandevoorde and E.S. Roberts. WorkCrews: </author> <title> an abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> vol.17,(no.4):347-66, </volume> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Unfortunately, a parallel call or thread fork is fundamentally more expensive than a sequential call because of the thread and storage management, data transfer, scheduling, and synchronization involved. Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations <ref> [9, 21, 26, 29, 30, 33, 35, 37, 39] </ref>, and by supporting fine-grained parallel execution directly in hardware [3, 19, 31]. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations <ref> [9, 30, 26, 39, 37, 33, 17] </ref>, and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29].
Reference: [38] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In the case of a remote fork, the stub handler uses indirect active messages <ref> [38] </ref> to return data and control to the parent's message handler, which in turn is responsible for integrating the data into the parent frame and indicating to the parent that its child has returned. 3.3 Compilation To reduce the cost of frame allocation even further, we construct a call graph which
Reference: [39] <author> D.B. Wagner and B.G. Calder. </author> <title> Leapfrogging: a portable technique for implementing efficient futures. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 208-17, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: There are many reasons to have the logical parallelism of the program exceed the physical parallelism of the machine, including ease of expression and better resource utilization in the presence of synchronization delays, load imbalance, and long communication latency <ref> [26, 39] </ref>. Moreover, the semantics of the language or the synchronization primitives may allow dependencies to be expressed in such a way that progress can be made only by interleaving multiple threads, effectively running them in parallel even on a single processor [28]. <p> Unfortunately, a parallel call or thread fork is fundamentally more expensive than a sequential call because of the thread and storage management, data transfer, scheduling, and synchronization involved. Previous work has sought to reduce this cost by using a combination of compiler techniques and clever run-time representations <ref> [9, 21, 26, 29, 30, 33, 35, 37, 39] </ref>, and by supporting fine-grained parallel execution directly in hardware [3, 19, 31]. In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. <p> Next, we look at the efficiency of work-stealing combined with seeds on a parallel machine by examining the performance of the synthetic benchmark proposed in [26] and also used in <ref> [39] </ref>. Grain is a doubly recursive program that computes a sum, where each leaf executes a loop of g instructions, thus allowing us to control the granularity of the leaf nodes. We compare its efficiency to that of sequential C code compiled by gcc. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain <ref> [26, 39] </ref>. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations [9, 30, 26, 39, 37, 33, 17], and direct hardware support for fine-grained parallel execution [19, 3]. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. 26 of granularity. We use the synthetic benchmark Grain [26, 39]. 7 Related Work Attempts to accommodate logical parallelism include thread packages [11, 34, 7, 16], compiler techniques and clever run-time representations <ref> [9, 30, 26, 39, 37, 33, 17] </ref>, and direct hardware support for fine-grained parallel execution [19, 3]. These approaches have been used to implement many parallel languages, e.g. Mul-T [23], Id90 [9, 30], CC++ [6], Charm [20], Opus [25], Cilk [4], Olden [5], and Cid [29]. <p> Our implementation works on both distributed- and shared-memory systems. Finally, LTC also depends on a garbage collector, which hides many of the costs of stack management. Another proposed technique for improving LTC is leapfrogging <ref> [39] </ref>. Unlike the techniques we use, it restricts the behavior of the program in an attempt to reduce the cost of futures. Leapfrogging has been implemented using Cthreads, a light-weight thread package.
Reference: [40] <author> A. Yonezawa. </author> <title> ABCL- an object-oriented concurrent system . MIT Press series in computer systems. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> 36 </month>
Reference-contexts: In many cases, the cost of the fork is reduced by severely restricting what can be done in a thread. These approaches, among others, have been used in implementing parallel programming languages such as ABCL <ref> [40] </ref>, CC++ [6], Charm [20], Cid [29], Cilk [4], Concert [21], Id90 [9, 30], Mul-T [23], and Olden [5]. Still, a fork remains substantially more expensive than a simple sequential call. <p> The C implementation shows that these primitives introduce little or no overhead over sequential programs. The Id90 implementation shows that for complete programs we achieve a substantial improvement over previous work. Our techniques can be applied to other programming languages <ref> [6, 40] </ref>, thread packages [11], and multithreaded execution models. In Section 7 we discuss related work. Our work, Lazy Threads, relies extensively on compiler optimizations and cannot simply be implemented with function calls to a user-level threads library without substantial loss of efficiency.
References-found: 40

