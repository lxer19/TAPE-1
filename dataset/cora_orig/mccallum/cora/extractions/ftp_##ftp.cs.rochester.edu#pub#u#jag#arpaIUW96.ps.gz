URL: ftp://ftp.cs.rochester.edu/pub/u/jag/arpaIUW96.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/jag/publications.html
Root-URL: 
Email: fjag,nelsong@cs.rochester.edu  
Title: On-line Estimation of Visual-Motor Models using Active Vision  
Author: Martin Jagersand, Randal Nelson 
Web: http://www.cs.rochester.edu/u/fjag,nelsong  
Address: Rochester, Rochester, NY 14627  
Affiliation: Department of Computer Science, University of  
Date: 1996  
Note: In Proc. of ARPA Image Understanding Workshop,  
Abstract: We present a novel approach for combined visual model acqusition and agent control. The approach differs from previous work in that a full coupled Ja-cobian is estimated on-line without any prior models, or the introduction of special calibration movements. We show how the estimated models can be used for visual space robot task specification, planning and control. In the other direction the same type of models can be used for view synthesis.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Weiss L. E. Sanderson A. C. Neumann C. P. </author> <title> Dynamic Sensor-Based Control of Robots with Visual Feedback J. of Robotics and Aut. </title> <publisher> v. </publisher> <month> RA-3 </month> <year> 1987 </year>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. Results with visual servoing and varying degrees of model adaption have been presented for robot arms <ref> [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] </ref> 1 . Visual models suitable for specifying visual alignments have also been studied [19, 8, 7]. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models. <p> The changes in visual appearance are recorded in a perception or feature vector y = (y 1 : : : y m ) T . Visual features can be drawn from a large class of visual measurements <ref> [1, 9] </ref>, but we have found that the ones which can be represented as points positions or point vectors in camera space are suitable [10]. We track features such as boundary discontinuities (lines,corners) and surface markings. <p> In our active framework the agent also knows along which direction Dx the system changes. This leaves only a one dimensional search space along y predicted = J Dx + y k ; 2 <ref> [0; 1] </ref> in feature space. Note however that we cannot simply constrain the tracker output to this space. That would take away the innovation term in our model updating, and the system would no longer adapt its model to a changing environment.
Reference: [2] <author> Feddema J. T. Lee G. C. S. </author> <title> Adaptive Image Feature Prediction and Control for Visual Tracking with a Hand-Eye Coordinated Camera IEEE Tr. </title> <journal> on Systems, Man and Cyber., </journal> <volume> v 20, no 5 1990 </volume>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. Results with visual servoing and varying degrees of model adaption have been presented for robot arms <ref> [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] </ref> 1 . Visual models suitable for specifying visual alignments have also been studied [19, 8, 7]. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models. <p> A combined model acquisition and control approach has many advantages. In addition to permitting uncalibrated visual servo control, the on-line estimated models are useful for (1) prediction and constraining search in visual tracking <ref> [13, 2] </ref>, (2) perfomring local coordinate transformations between manipulator (joint), world, and visual frames [9, 13], and (3) synthesizing views from a basis of agent poses [11]. <p> This is not appropriate in most manipulation settings, where the calibration movements would interfere with the task. Partial modeling of the viewing geometry using an In Proc. of ARPA Image Understanding Workshop, 1996 ARMAX model and estimating only one or a few parameters (e.g. depth) has also been tried <ref> [2, 6] </ref>. This however restricts the camera-robot configurations and environments to structured, easy to model settings. We seek instead, an online method, which estimates the Ja-cobian by simply observing the process, without introducing any extra calibration movements.
Reference: [3] <author> Conkie A. Chongstitvatana P. </author> <title> An Uncalibrated Stereo Visual Servo System DAITR#475, </title> <address> Edinburgh 1990 </address>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. Results with visual servoing and varying degrees of model adaption have been presented for robot arms <ref> [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] </ref> 1 . Visual models suitable for specifying visual alignments have also been studied [19, 8, 7]. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models. <p> We need a control system capable of turning these goal perceptions into motor actions x. A simple control law, occuring in some form in most visual servoing research (e.g. <ref> [3, 19, 16] </ref>) is where K is a gain matrix.
Reference: [4] <author> Curwen R. Blake A. </author> <title> Dynamic Contours: Real time active splines In Active Vision Ed Blake, </title> <publisher> Yuille MIT Press 1992. </publisher>
Reference-contexts: The Oxford snakes <ref> [4] </ref> are used to track surface discontinuities.
Reference: [5] <author> Wijesoma S. W. Wolfe D. F. H. Richards R. J. </author> <title> Eye-to-Hand Coordination for vision guided Robot Control Applications Int. </title> <journal> J. of Robotics Research, </journal> <volume> v 12 No 1 1993 </volume>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. Results with visual servoing and varying degrees of model adaption have been presented for robot arms <ref> [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] </ref> 1 . Visual models suitable for specifying visual alignments have also been studied [19, 8, 7]. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models.
Reference: [6] <author> Papanikolopoulos N. P. Khosla P. K. </author> <title> Adaptive Robotic Visual Tracking: </title> <journal> Theory and Experiments IEEE Tr. on Aut. </journal> <note> Control Vol 38 no 3 1993 </note>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. Results with visual servoing and varying degrees of model adaption have been presented for robot arms <ref> [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] </ref> 1 . Visual models suitable for specifying visual alignments have also been studied [19, 8, 7]. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models. <p> This is not appropriate in most manipulation settings, where the calibration movements would interfere with the task. Partial modeling of the viewing geometry using an In Proc. of ARPA Image Understanding Workshop, 1996 ARMAX model and estimating only one or a few parameters (e.g. depth) has also been tried <ref> [2, 6] </ref>. This however restricts the camera-robot configurations and environments to structured, easy to model settings. We seek instead, an online method, which estimates the Ja-cobian by simply observing the process, without introducing any extra calibration movements.
Reference: [7] <author> Harris M. </author> <title> Vision Guided Part Alignment with Degraded Data DAI TR #615, </title> <address> Edinburgh 1993 </address>
Reference-contexts: Results with visual servoing and varying degrees of model adaption have been presented for robot arms [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] 1 . Visual models suitable for specifying visual alignments have also been studied <ref> [19, 8, 7] </ref>. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models.
Reference: [8] <author> Hollinghurst N. Cipolla R. </author> <title> Uncalibrated Stereo Hand-Eye Coordination Brit. </title> <booktitle> Machine Vision Conf 1993 </booktitle>
Reference-contexts: Results with visual servoing and varying degrees of model adaption have been presented for robot arms [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] 1 . Visual models suitable for specifying visual alignments have also been studied <ref> [19, 8, 7] </ref>. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models.
Reference: [9] <author> Jagersand M. Nelson R. </author> <title> Adaptive Differential Visual Feedback for uncalibrated hand-eye coordination and motor control TR# 579, </title> <type> U. </type> <institution> of Rochester 1994. </institution>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. Results with visual servoing and varying degrees of model adaption have been presented for robot arms <ref> [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] </ref> 1 . Visual models suitable for specifying visual alignments have also been studied [19, 8, 7]. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models. <p> A combined model acquisition and control approach has many advantages. In addition to permitting uncalibrated visual servo control, the on-line estimated models are useful for (1) prediction and constraining search in visual tracking [13, 2], (2) perfomring local coordinate transformations between manipulator (joint), world, and visual frames <ref> [9, 13] </ref>, and (3) synthesizing views from a basis of agent poses [11]. We have found such an adaptive approach to be particularly helpful in robot arm manipulation when carrying out difficult tasks, such as manipulation of flexible material [9, 13], or performing large rotations for exploring object shape [12]. <p> perfomring local coordinate transformations between manipulator (joint), world, and visual frames <ref> [9, 13] </ref>, and (3) synthesizing views from a basis of agent poses [11]. We have found such an adaptive approach to be particularly helpful in robot arm manipulation when carrying out difficult tasks, such as manipulation of flexible material [9, 13], or performing large rotations for exploring object shape [12]. <p> The changes in visual appearance are recorded in a perception or feature vector y = (y 1 : : : y m ) T . Visual features can be drawn from a large class of visual measurements <ref> [1, 9] </ref>, but we have found that the ones which can be represented as points positions or point vectors in camera space are suitable [10]. We track features such as boundary discontinuities (lines,corners) and surface markings. <p> These experiments are described in more detail in our technical report <ref> [9] </ref>. On a PUMA 761 we found that repeatability is 35 % better under visual servo control than under standard joint control. On a worn PUMA 762, with significant backlash, we got a repeatability improvement of 5 times with the visual control. The Utah/MIT dextrous hand has 16 controllable DOF's.
Reference: [10] <author> Jagersand M. </author> <title> Perception level control for uncalibrated hand-eye coordination and motor actions Thesis proposal, </title> <institution> University of Rochester, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: In normal operation, no extra calibration movements should be needed for the model estimation. fl Supported by ARPA subcontract Z8440902 through University of Maryland 1 For a review of this work we direct the reader to <ref> [10] </ref> or [15]. A combined model acquisition and control approach has many advantages. <p> Visual features can be drawn from a large class of visual measurements [1, 9], but we have found that the ones which can be represented as points positions or point vectors in camera space are suitable <ref> [10] </ref>. We track features such as boundary discontinuities (lines,corners) and surface markings. Redundant visual perceptions (m n) are desirable as they are used to constrain the raw visual sensory information. 2 Vectors are written bold, scalars plain and matrices capitalized. <p> Positioning accuracy increased 4 times with m = 16 compared to m = 4. We have tried using the visual servoing in solving several complex, real world tasks, such as playing checkers, setting a table, solving a kids puzzle and changing a light bulb <ref> [10] </ref>. Visual space programming is different from conventional robot programming in that commands are given in image space rather than world space. This makes user friendly programmer interfaces easy to implement.
Reference: [11] <author> M. Jagersand. </author> <title> Model Free View Synthesis of an Articulated Agent, </title> <type> Technical Report 595, </type> <institution> Computer Science Department, University of Rochester, Rochester, </institution> <address> New York, </address> <year> 1995. </year>
Reference-contexts: In addition to permitting uncalibrated visual servo control, the on-line estimated models are useful for (1) prediction and constraining search in visual tracking [13, 2], (2) perfomring local coordinate transformations between manipulator (joint), world, and visual frames [9, 13], and (3) synthesizing views from a basis of agent poses <ref> [11] </ref>. We have found such an adaptive approach to be particularly helpful in robot arm manipulation when carrying out difficult tasks, such as manipulation of flexible material [9, 13], or performing large rotations for exploring object shape [12].
Reference: [12] <author> Kutulakos K. Jagersand M. </author> <title> Exploring objects by purposive viewpoint control and invariant-based hand-eye coordination Workshop on vision for robots In conjunction with IROS 1995. </title>
Reference-contexts: We have found such an adaptive approach to be particularly helpful in robot arm manipulation when carrying out difficult tasks, such as manipulation of flexible material [9, 13], or performing large rotations for exploring object shape <ref> [12] </ref>. For a dextrous multifinger robot hand, such as the Utah/MIT hand, the fully adaptive approach is appealing because dextrous manipulation of a grasped object is much harder to model accurately than a typical robot arm system, where the object is rigidly attached to the end effector. <p> For instance identifying the three lines forming a corner on a rectangular box in two cameras, or two poses, gives an Euclidean base P . Using more views improves the accuracy of the base <ref> [12] </ref>. Often an incomplete base is enough (i.e. to move up we only need to identify a vertical line near the robot in each of the cameras).
Reference: [13] <author> Jagersand M. Nelson R. </author> <title> Visual Space Task Specification, </title> <booktitle> Planning and Control In Proc on IEEE Symp. on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. Results with visual servoing and varying degrees of model adaption have been presented for robot arms <ref> [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] </ref> 1 . Visual models suitable for specifying visual alignments have also been studied [19, 8, 7]. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models. <p> A combined model acquisition and control approach has many advantages. In addition to permitting uncalibrated visual servo control, the on-line estimated models are useful for (1) prediction and constraining search in visual tracking <ref> [13, 2] </ref>, (2) perfomring local coordinate transformations between manipulator (joint), world, and visual frames [9, 13], and (3) synthesizing views from a basis of agent poses [11]. <p> A combined model acquisition and control approach has many advantages. In addition to permitting uncalibrated visual servo control, the on-line estimated models are useful for (1) prediction and constraining search in visual tracking [13, 2], (2) perfomring local coordinate transformations between manipulator (joint), world, and visual frames <ref> [9, 13] </ref>, and (3) synthesizing views from a basis of agent poses [11]. We have found such an adaptive approach to be particularly helpful in robot arm manipulation when carrying out difficult tasks, such as manipulation of flexible material [9, 13], or performing large rotations for exploring object shape [12]. <p> perfomring local coordinate transformations between manipulator (joint), world, and visual frames <ref> [9, 13] </ref>, and (3) synthesizing views from a basis of agent poses [11]. We have found such an adaptive approach to be particularly helpful in robot arm manipulation when carrying out difficult tasks, such as manipulation of flexible material [9, 13], or performing large rotations for exploring object shape [12]. <p> The downside is we know no strong convergence results of the DFP method in combination with inexact line searches (which are used in our trust region controller). unstructured, hard to model environments. In previous work <ref> [13] </ref> we have shown how a visually guided robot arm can be instructed to solve a variety of hand-eye tasks by: (1) A sequence of images, describing the task at hand. (2) By having a human draw a sketch describing the visual alignments in the task. (3) Using a video image <p> This makes user friendly programmer interfaces easy to implement. We have tried having the robot operator: (1) Draw visual sketches of the desired movements. (2) Point out objects and alignments in video images. (3) Show an image sequence depicting the task (see <ref> [13] </ref>). In fig.5 the PUMA robot solves a kid's puzzle under visual control. The operator points in an image, using the computer mouse, directing which piece goes where. The program decomposes this into transportation, alignment and insertion movements, and plans trajectories in visual space (white lines in fig.).
Reference: [14] <institution> Jagersand Visual Servoing using Trust Region Methods and Estimation of the Full Coupled Visual-Motor JacobianIASTED Applications of Robotics and Control, </institution> <year> 1996 </year>
Reference-contexts: Intuitively both these techniques aid to synchronize actions with model acquisition, so that the actions never run too far ahead before the local model has been adapted to the new environment. For details and theoretical properties of these two methods see our control theory paper <ref> [14] </ref>. 5.2 Visual space task representation and planning To date work in image/feature space visual control has demonstrated low level servoing behaviors, achieving a single visual alignment, eg. [16, 19].
Reference: [15] <author> Corke P. I. </author> <title> High-Performance Visual Closed-Loop Robot Control PhD thesis U of Melbourne 1994. </title>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. Results with visual servoing and varying degrees of model adaption have been presented for robot arms <ref> [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] </ref> 1 . Visual models suitable for specifying visual alignments have also been studied [19, 8, 7]. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models. <p> In normal operation, no extra calibration movements should be needed for the model estimation. fl Supported by ARPA subcontract Z8440902 through University of Maryland 1 For a review of this work we direct the reader to [10] or <ref> [15] </ref>. A combined model acquisition and control approach has many advantages. <p> The model is valid around the current system configuration x k , and described by the image <ref> [15] </ref> or visual-motor Jacobian defined as (J j;i )(x k ) = @x i The image Jacobian not only relates visual changes to motor changes, as is exploited in visual feedback control but also highly constrains the possible visual changes to the subspace y k+1 &lt; m of y k+1 =
Reference: [16] <author> Hosoda K. Asada M. </author> <title> Versatile Visual Servoing without Knowledge of True Jacobian Proc. </title> <booktitle> IROS 1994. </booktitle>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. Results with visual servoing and varying degrees of model adaption have been presented for robot arms <ref> [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] </ref> 1 . Visual models suitable for specifying visual alignments have also been studied [19, 8, 7]. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models. <p> We need a control system capable of turning these goal perceptions into motor actions x. A simple control law, occuring in some form in most visual servoing research (e.g. <ref> [3, 19, 16] </ref>) is where K is a gain matrix. <p> For details and theoretical properties of these two methods see our control theory paper [14]. 5.2 Visual space task representation and planning To date work in image/feature space visual control has demonstrated low level servoing behaviors, achieving a single visual alignment, eg. <ref> [16, 19] </ref>. A remaining principal challenge is how to specify complex tasks in visual space, divide them up into subtasks, plan trajectories in visual space, and select different primitive visual servoing behaviors and visual goals.
Reference: [17] <author> Fuentes O. Nelson R. </author> <title> Morphing hands and virtual tools TR# 551, </title> <institution> Dept of CS, U. of Rochester 1994. </institution>
Reference-contexts: The Utah/MIT dextrous hand has 16 controllable DOF's. The four fingers form a parallel kinematic chain when grasping an object. Fine manipulation of an object in the hand is much more difficult than with a robot arm <ref> [17] </ref>. Manipulating a rigid object in 6 DOF using the visual servo control we note a 73 % improvement in repeatability compared to Cartesian space joint feedback control. We have evaluated the model estimation in 3, 6 and 12 controlled DOF.
Reference: [18] <author> B. H. Yoshimi P. K. </author> <title> Allen Active, </title> <booktitle> Uncalibrated Visual Servo-ing ARPA IUW, </booktitle> <year> 1993. </year>
Reference-contexts: Visual servoing, when supplemented with on-line visual model estimation, fits into the active vision paradigm. Results with visual servoing and varying degrees of model adaption have been presented for robot arms <ref> [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] </ref> 1 . Visual models suitable for specifying visual alignments have also been studied [19, 8, 7]. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models.
Reference: [19] <author> Hager G. </author> <title> Calibration-Free Visual Control Using Projective Invariance In Proc. </title> <booktitle> of 5:th ICCV 1995. </booktitle>
Reference-contexts: Results with visual servoing and varying degrees of model adaption have been presented for robot arms [1, 3, 2, 5, 6, 9, 13, 15, 16, 18] 1 . Visual models suitable for specifying visual alignments have also been studied <ref> [19, 8, 7] </ref>. However, the focus of this work has been the movement (ser-voing) of the robot, not on on-line estimation of high DOF visual-motor models. <p> We need a control system capable of turning these goal perceptions into motor actions x. A simple control law, occuring in some form in most visual servoing research (e.g. <ref> [3, 19, 16] </ref>) is where K is a gain matrix. <p> For details and theoretical properties of these two methods see our control theory paper [14]. 5.2 Visual space task representation and planning To date work in image/feature space visual control has demonstrated low level servoing behaviors, achieving a single visual alignment, eg. <ref> [16, 19] </ref>. A remaining principal challenge is how to specify complex tasks in visual space, divide them up into subtasks, plan trajectories in visual space, and select different primitive visual servoing behaviors and visual goals.
Reference: [20] <author> Nayar S. Nene S. Murase H. </author> <title> Subspace Methods for Robot Vision TR CUCS-06-95 CS, </title> <address> Columbia, </address> <year> 1995. </year>
Reference-contexts: Representations based on this idea have been used for recognition problem what, and for indexing locations where <ref> [20, 22, 21] </ref>. There are several ways to choose the eigen images. In our case we will be looking at the same agent, in different poses, and all the images we want to represent are fairly similar. <p> In our case we will be looking at the same agent, in different poses, and all the images we want to represent are fairly similar. In this case it is advantageous to use a basis specifically designed for the agent. In summary (see also <ref> [20, 22] </ref>) this can be done by acquiring a (large) number p of size q fi q images I k ; k = 1 : : : p; I 2 &lt; (q 2 ) of the agent in different poses.
Reference: [21] <author> Rao R. Ballard D. </author> <title> An Active Vision Architecture based on Iconic Representations TR 548, </title> <type> CS, </type> <institution> University of Rochester, </institution> <year> 1995 </year>
Reference-contexts: Representations based on this idea have been used for recognition problem what, and for indexing locations where <ref> [20, 22, 21] </ref>. There are several ways to choose the eigen images. In our case we will be looking at the same agent, in different poses, and all the images we want to represent are fairly similar.
Reference: [22] <author> Turk M. Pentland A. </author> <title> Eigenfaces for recognition In J of Cognitive Neuroscience v3 nr1, </title> <address> p71-86, </address> <year> 1991. </year>
Reference-contexts: Representations based on this idea have been used for recognition problem what, and for indexing locations where <ref> [20, 22, 21] </ref>. There are several ways to choose the eigen images. In our case we will be looking at the same agent, in different poses, and all the images we want to represent are fairly similar. <p> In our case we will be looking at the same agent, in different poses, and all the images we want to represent are fairly similar. In this case it is advantageous to use a basis specifically designed for the agent. In summary (see also <ref> [20, 22] </ref>) this can be done by acquiring a (large) number p of size q fi q images I k ; k = 1 : : : p; I 2 &lt; (q 2 ) of the agent in different poses.
Reference: [23] <author> Garcia, </author> <title> Zangwill Pathways to solutions, fixed points, and equilibria, </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: In the trust region method, the current ff indicates the distance for which the estimated model is valid. For the second we use a technique known in numerical analysis as inbaddning [25] or homotopy methods <ref> [23] </ref>, which involves the generation of intermediate goals or way points along the way to the main goal y fl , transforming a globally non convex problem into a set of locally convex sub-problems.
Reference: [24] <author> Fletcher R. </author> <title> Practical Methods of Optimization Chichester, second ed. </title> <year> 1987 </year>
Reference-contexts: On a higher level, vision based control can lead to user friendly visual robot programming or teaching interfaces, suitable for use in 3 Updating formulas of rank 1 and 2 are most common. The mo tivation of popular rank 2 formulas, such as the Davidson, Fletcher, Powell (DFP) <ref> [24] </ref> is that they preserve symmetric (X i = ~ i ~ T i ) positive definiteness of J , and thus guarantees that the search direction Dx of a quasi-Newton type controller is a descent direction for .
Reference: [25] <author> Gustafsson I. </author> <note> Till ampad Optimeringsl ara Komp., </note> <institution> Inst. for Inf. Beh., </institution> <note> Chalmers 1991. </note>
Reference-contexts: In the trust region method, the current ff indicates the distance for which the estimated model is valid. For the second we use a technique known in numerical analysis as inbaddning <ref> [25] </ref> or homotopy methods [23], which involves the generation of intermediate goals or way points along the way to the main goal y fl , transforming a globally non convex problem into a set of locally convex sub-problems. <p> The robot controller uses the learned models to predict how to move to achieve new goals. We have showed how to improve a standard, Newton-type visual servoing algorithm. We use a trust region method to achieve convergence for difficult transfer functions, and inbaddning <ref> [25] </ref> or homotopy methods to transform a positioning task on a non-convex domain of the transfer function to a series of smaller tasks, each on a smaller convex domain.
Reference: [26] <author> Dahlquist G. Bjorck A. </author> <title> Numerical Methods Second Ed, Pren-tice Hall, </title> <type> 199x, preprint. </type>
Reference-contexts: Dynamic stability of the robot at this low sampling frequency is achieved by a secondary set of high bandwidth joint feedback controllers. This popular controller however has two major deficiencies. First, even for a convex problem (f T f convex) it is not guaranteed to be convergent <ref> [26] </ref>, and second in the case of a non convex problem it often does not converge at all [26]. Previous work has overcome this problem by making only a single, small distance move within a relatively smooth and well scaled region of f . <p> This popular controller however has two major deficiencies. First, even for a convex problem (f T f convex) it is not guaranteed to be convergent <ref> [26] </ref>, and second in the case of a non convex problem it often does not converge at all [26]. Previous work has overcome this problem by making only a single, small distance move within a relatively smooth and well scaled region of f . To solve whole, real tasks this is not a viable solution. We adopt a trust region method [26] similar to the well known Marquart step <p> it often does not converge at all <ref> [26] </ref>. Previous work has overcome this problem by making only a single, small distance move within a relatively smooth and well scaled region of f . To solve whole, real tasks this is not a viable solution. We adopt a trust region method [26] similar to the well known Marquart step length (ff) adaption schema to solve the first problem. In the trust region method, the current ff indicates the distance for which the estimated model is valid.
References-found: 26

