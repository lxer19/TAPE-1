URL: http://www.cse.psu.edu/~barlow/GEbound2.ps
Refering-URL: http://www.cse.psu.edu/~barlow/papers.html
Root-URL: http://www.cse.psu.edu
Title: GROWTH IN GAUSSIAN ELIMINATION, ORTHOGONAL MATRICES, AND THE TWO-NORM  
Author: JESSE L. BARLOW AND HONGYUAN ZHA 
Keyword: Key words. LU factorization, orthogonal invariance, triangular matrix.  
Note: AMS subject classifications. 65F05,65F35  
Abstract: It is shown that maximal growth for Gaussian elimination with partial pivoting as measured in the 2-norm is achieved by orthogonal matrices. A precise bound on that growth is given. 1. Introduction. We consider Gaussian elimination with partial pivoting 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Barlow, </author> <title> More accurate bidiagonal reduction for computing the singular value decompostion. </title> <note> manuscript in preparation, </note> <year> 1997. </year>
Reference-contexts: The second proposition establishes a sharp bound for that growth. Proposition 1.2. Let ae (k) n and ae n be defined by (1.9). Then ae (k) p 1 3 1=2 and therefore ae n + O ( n) = 3 Proposition 1.1 arose from work by the first author <ref> [1] </ref> on the error analysis of bidiagonal reduction. In that work, it was necessary to understand GEPP applied to orthogonal matrices. The two authors then proved Proposition 1.2 to give a precise value for worst case growth. In the next section, we prove Propositions 1.1 and 1.2.
Reference: [2] <author> D. Faddeev, V. Kublanovskaya, and V. Faddeeva, </author> <title> Solution of linear algebraic systems with rectangular matrices, </title> <booktitle> Proc. </booktitle> <institution> Steklov Inst. Math, </institution> <month> 96 </month> <year> (1968), </year> <pages> pp. 93-111. </pages>
Reference-contexts: Let ~ L k be defined by (2.5). Then kL 1 k k 2 :(2.6) The second lemma bounds k ~ L 1 k k 2 . It is similar to a bound given by Faddeev, Kublanovskaya, and Fadeeva <ref> [2] </ref> whose proof is given by Lawson and Hanson [4, pp.28-35]. Lemma 2.2. Let ~ L k 2 &lt; nfin be defined by (2.5) and let ae (k) n be defined by (1.9).
Reference: [3] <author> N. Higham, </author> <title> Accuracy and Stability of Numerical Algorithms, </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: the matrix ~ L k = ( ~ ` ij ) defined by ~ ` ij = &lt; 1 if i = j 0 otherwise :(2.5) The first lemma is a slightly different version of a bound for the norm of the inverse of a triangular matrix given in Higham <ref> [3, pp.159-161,Theorems 8.11 and 8.13] </ref>. The proof is nearly identical to what is given there, so we omit it. Lemma 2.1.
Reference: [4] <author> C. Lawson and R. Hanson, </author> <title> Solving Least Squares Problems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliff, NJ, </address> <year> 1974. </year>
Reference-contexts: Let ~ L k be defined by (2.5). Then kL 1 k k 2 :(2.6) The second lemma bounds k ~ L 1 k k 2 . It is similar to a bound given by Faddeev, Kublanovskaya, and Fadeeva [2] whose proof is given by Lawson and Hanson <ref> [4, pp.28-35] </ref>. Lemma 2.2. Let ~ L k 2 &lt; nfin be defined by (2.5) and let ae (k) n be defined by (1.9). Then ae (k) k k 2 k ~ L 1 4 k 1 + n 3 1=2 Proof.
Reference: [5] <author> B. Parlett, </author> <title> The Symmetric Eigenvalue Problem, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1980. </year>
Reference-contexts: If k &lt; n 1, oe n1 ( ~ L k ) = 1, otherwise oe n1 ( ~ L k ) p Proof. First consider the case k = n 1. Let ~ L n1 = ( F n1 e n ):(2.9) The Cauchy interlace theorem <ref> [5, p.186,Theorem 10-1-2] </ref> applied to ~ L T ~ L n1 states that oe n1 ( ~ L n1 ) oe n1 (F n1 ): We now show that oe n1 (F n1 ) = p Note that F T n2 B n2 0 where B n2 is empty if n
Reference: [6] <author> G. Stewart, </author> <title> Introduction to Matrix Computations, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: By a similar argument Q = P L k Q k (2.2) where Q k = A k R 1 and L k and A k are given in (1.5)-(1.6). Since the first k columns of Q k are zero below the diagonal, the uniqueness of the L-U decomposition <ref> [6, pp.121,Theorem 2.6] </ref> assures us that (2.2) is the kth stage of GEPP applied to Q. No row interchanges are needed since all entries of L k below the diagonal will have absolute value less than 1.
Reference: [7] <author> J. Wilkinson, </author> <title> Error analysis of direct methods of matrix inversion, </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 8 (1961), </volume> <pages> pp. </pages> <month> 281-330. </month> <title> [8] , The Algebraic Eigenvalue Problem, </title> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1965. </year> <month> 9 </month>
Reference-contexts: Equation (1.7) assures that L = (` ij ), where j` ij j 1 for all i and j and that the same holds for each L k . Wilkinson <ref> [7] </ref> showed that the stability of GEPP depends upon the growth factor i A = (k) max (i;j) ja ij j He also showed that max A2&lt; nfin i A = 2 k ; and that i n = max A2&lt; nfin i A = 2 n1 : The decomposition (1.1)
References-found: 7

