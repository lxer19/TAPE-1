URL: ftp://www.cs.rutgers.edu/pub/technical-reports/lcsr-tr-267.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: MODEL-BASED REFINEMENT OF SEARCH HEURISTICS  Written under the direction of  
Author: BY MICHAEL W. BARLEY L. Steinberg 
Degree: A dissertation submitted to the Graduate School|New Brunswick  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  and approved by  
Date: May, 1996  
Note: Graduate Program in Computer Science  
Address: New Jersey  Brunswick, New Jersey  
Affiliation: Rutgers, The State University of  New  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Barrett and D. Weld. </author> <title> Partial-order planning: Evaluating possible efficiency gains. </title> <journal> Artificial Intelligence, </journal> <volume> 67(1), </volume> <year> 1994. </year>
Reference-contexts: The second assumption not only helps guarantee that one can find a meta-level path to the solution without backtracking, it also guarantees that the search is systematic [16]. Thus, if the other two assumptions hold then for any problem-solver (e.g., SNLP <ref> [1] </ref>) which is systematic, one can identify which of its search heuristics to modify in time linear with respect to the size of the supplied solution. 5.4.2 Computational Complexity of Compute Modifications There are two computationally expensive activities in the Compute Modifications component.
Reference: [2] <author> J. Bennett and T. Dietterich. </author> <title> The test incorporation hypothesis and the weak methods. </title> <type> Technical Report TR 86-30-4, </type> <institution> Dept. of Computer Science, Oregon State University, Corvallis, Oregon, </institution> <year> 1986. </year>
Reference-contexts: However, after the test movement, the information that the unsatisfied preconditions of any step are legal alternatives has been hidden within the generator. With test movement, one can still see in the generator the logic that generated all of the legal alternatives. However, another speed-up technique, test incorporation <ref> [2] </ref>, loses even this. In test incorporation, not only is the rejection test moved into the generator, it actually replaces part of the generator.
Reference: [3] <author> N. Bhatnagar. </author> <title> On-Line Learning From Search Failures. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: This thesis also identifies sufficient conditions for eliminating these three types of interactions. 1.2 Current Approaches and Their Limitations 1.2.1 FS2 and SOAR There are two systems that modify their rejection heuristics: Bhatnagar's FS2 <ref> [3] </ref> and Lee's version of SOAR [11]. I will briefly discuss how they modify their rejection heuristics and discuss the systems in more detail in Chapter 2. FS2 treats its explicit rejection rules (which Bhatnagar calls "heuristic censors") as a type of preference rule. <p> Research is currently being carried on in all three areas. We will first discuss the research that is the most germane to this thesis, namely, investigations into how to improve a problem-solvers coverage. 2.1 Increasing Coverage 2.1.1 FS2 FS2 <ref> [3] </ref> dynamically learns, relaxes, and modifies its rejection heuristics (which Bhat-nagar calls "heuristic censors") during the course of its attempt to solve a problem. During its attempt to solve a problem, FS2 may learn overgeneral rejection heuristics that impede its progress towards finding a solution. <p> I say one modification is more effective than another when it produces more performance improvement. There are two existing algorithms for modifying rejection heuristics: SOAR [11] has a problem-level failure-based (PLF ) algorithm and FS2 <ref> [3] </ref> has an edge-level success-based (ELS ) algorithm. The SOAR PLF algorithm learns modifications that check whether to suspend a rejection heuristic for all nodes within the problem's search space based on a characterization of when SOAR could not solve the training problem using that rejection heuristic.
Reference: [4] <author> D. Borrajo and M. Veloso. </author> <title> Incremental learning of control knowledge for nonlinear problem solving. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: The regressed condition became the precondition for when not to select that operator to work towards that goal 2 . Unfortunately, the impossibility theory is domain specific and each new domain requires a new one. 2.2.3 HAMLET HAMLET <ref> [4] </ref> learns PRODIGY select rules for the different types of choice points found in NOLIMIT [20], PRODIGY's non-linear problem-solver. HAMLET uses two mechanisms to produce overgeneral preconditions for these search control rules.
Reference: [5] <author> W. Braudaway. </author> <title> Knowledge compilation for incorporating constraints. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: To enhance the flexibility of the problem-solving system, the steps of generating and of rejecting search alternatives should be kept separate and explicit. However, often the problem-solver can be speeded up by moving the rejection tests into the generator, this is known as test movement <ref> [5] </ref>. For example, Table 1.1 shows a generator and a rejection heuristic for a planning domain.
Reference: [6] <author> D. Chapman. </author> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelligence, </journal> <volume> 32(3), </volume> <year> 1987. </year>
Reference-contexts: A restricted version of this algorithm has been implemented as an extension to the PRODIGY [13] problem-solver. This extension is called Bacall. The main restriction is that Bacall can only modify the linearity [19] and the strong linearity <ref> [6] </ref> rejection heuristics. Linearity directs the problem-solver to work on goals in a strict depth-first fashion, i.e., pick a top-level goal and an operator to achieve it, then pick one of that operator's preconditions and an operator to achieve that, etc. <p> I implemented Bacall as an extension to the PRODIGY planner [13]. PRODIGY extends traditional totally-ordered-plan means-ends planning by possessing a facility for allowing the user to easily encode search control rules for a particular domain. Like many other such planners, PRODIGY also uses the Strong Linearity <ref> [6] </ref> rejection heuristic to reduce the number of totally-ordered plans it explores. Informally, Strong Linearity forbids subplans for different goals to be interleaved. However, Strong Linearity also causes PRODIGY to fail to solve some solvable problems.
Reference: [7] <author> O. Etzioni. </author> <title> A Structural Theory of Explanation-Based Learning. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1990. </year>
Reference-contexts: Currently the success-based justifications are computed from a solution. Unfortunately, these solution-based success-based justifications tend to be too specific. One approach is to compute more general success-based explanations is Etzioni's use of problem space graphs (PSG). While Etzioni's STATIC <ref> [7] </ref> computes the requisite conditions for the guaranteed success of nodes in a PSG, the conditions one wants computed should guarantee success if the given edge is used. I should explore variations of STATIC that can compute these conditions.
Reference: [8] <author> O. Etzioni and R. Etzioni. </author> <title> Statistical methods for analyzing speedup learning experiments. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: However, as Segre, Elkan, and Russell [18] point out, if the "faster" system has problems censored by resource limits then experiments that compare these two systems' problem-solving speeds can exhibit an horizon effect, i.e., changing the resource bounds can radically change which system appears faster. Etzioni and Etzioni <ref> [8] </ref> describes two conservative statistical tests for analyzing speedup experiments. Both of these tests ignore the magnitude of the differences between the systems' problem-solving times. Instead they either just look at the sign of the differences or they look at both the sign and the ranking of the differences. <p> With the expert search control theory the system often goes directly to the solution. Because they ignore the actual magnitudes of the differences in problem-solving times, the tests proposed by Etzioni and Etzioni <ref> [8] </ref> are not appropriate here. <p> As Etzioni and Etzioni <ref> [8] </ref> point out, this assumption may not be true in speedup learning experiments. However, the intent is not to "prove" that using the best modification set causes PRODIGY to be faster when attempting to solve the scalability problem set than when it uses the base modification set.
Reference: [9] <author> O. Etzioni and S. Minton. </author> <title> Why EBL produces overly-specific knowledge: A critique of the PRODIGY approaches. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> Aberdeen, Scotland, </address> <year> 1992. </year>
Reference: [10] <author> J. Laird, A. Newell, and P. Rosenbloom. </author> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33(1), </volume> <year> 1987. </year>
Reference-contexts: The holes can appear in different locations in different problems. Figure 3.1 shows a problem in this domain. 3.2 FCPS: An Example Problem-Solver There are a number of systems <ref> [10, 13] </ref> that make their problem-solving components flexible by making some of the search control knowledge explicitly modifiable, which 19 RIGHT (?Col, ?Row) Preconditions: TILE-AT (?Col, ?Row) ?Col &lt; 4 not (HOLE-AT (?Col + 1, ?Row)) Deletes: TILE-AT (?Col, ?Row) Adds: TILE-AT (?Col + 1, ?Row) LEFT (?Col, ?Row) Preconditions: TILE-AT
Reference: [11] <author> S. Lee. </author> <title> Multi-Method Planning. </title> <type> PhD thesis, </type> <institution> University of Southern California, </institution> <address> Los Angeles. CA, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: This thesis also identifies sufficient conditions for eliminating these three types of interactions. 1.2 Current Approaches and Their Limitations 1.2.1 FS2 and SOAR There are two systems that modify their rejection heuristics: Bhatnagar's FS2 [3] and Lee's version of SOAR <ref> [11] </ref>. I will briefly discuss how they modify their rejection heuristics and discuss the systems in more detail in Chapter 2. FS2 treats its explicit rejection rules (which Bhatnagar calls "heuristic censors") as a type of preference rule. <p> The negation of the expression is added to the preconditions of the heuristic censor (s) which pruned away that edge. I call this an success-based edge-level modification. Lee's approach <ref> [11] </ref> to handling rejection heuristics in SOAR is completely different. Lee's approach is to use a sequence of planning methods to attempt to solve problems. <p> If there were any such edges then the weakest preconditions for making that progress (i.e., achieving that goal) is computed for that edge and and its negation is and-ed to the offending rejection heuristic's preconditions. 14 2.1.2 SOAR Lee's approach <ref> [11] </ref> to handling rejection heuristics in SOAR is completely different. Lee's approach is to use a totally-ordered sequence of planning methods to attempt to solve problems. <p> I say one modification is more effective than another when it produces more performance improvement. There are two existing algorithms for modifying rejection heuristics: SOAR <ref> [11] </ref> has a problem-level failure-based (PLF ) algorithm and FS2 [3] has an edge-level success-based (ELS ) algorithm.
Reference: [12] <author> D. McAllester and D. Rosenblitt. </author> <title> Systematic nonlinear planning. </title> <booktitle> In Proceedings of the Ninth National Conference of Artificial Intelligence, </booktitle> <address> Anaheim, CA, </address> <year> 1991. </year>
Reference-contexts: For example, if one removes the G3 Goal Reject Rule then the CHOOSE-GOALS decision procedure will produce a candidate set containing G3 and G5. The CHOOSE-OPERATORS decision procedure will then select OP3 to achieve G3, and the problem is solved. Note, however, if the search procedure is systematic <ref> [16, 12] </ref> (i.e., there is never more than one meta-level path to any partial solution) then if the rejection rules separately sanction a solution then they will sanction that solution jointly. 5.1.4 Summary I have identified three types of undesirable search control rule interactions that can occur in BCPS, namely: (1)
Reference: [13] <author> S. Minton. </author> <title> Learning Effective Search Control Knowledge: An Explanation-Based Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: The first identifies which rejection heuristics should be modified. The second computes the appropriate modification for each identified heuristic. The final part installs the modification into the problem-solver's set of explicit search control rules. A restricted version of this algorithm has been implemented as an extension to the PRODIGY <ref> [13] </ref> problem-solver. This extension is called Bacall. The main restriction is that Bacall can only modify the linearity [19] and the strong linearity [6] rejection heuristics. <p> HAMLET is an example of a system that attempts such a compromise. While these systems learned more than just rejection search control rules, I will only discuss that one aspect. 2.2.1 PRODIGY-EBL PRODIGY-EBL <ref> [13] </ref> learned rejection search control rules from failed search subtrees. The learner used a theory of the problem-solver to identify the leaf failure reasons which were then regressed up the subtree. While the theory was relatively complete, it did not capture all of the relevant aspects of every failure. <p> The holes can appear in different locations in different problems. Figure 3.1 shows a problem in this domain. 3.2 FCPS: An Example Problem-Solver There are a number of systems <ref> [10, 13] </ref> that make their problem-solving components flexible by making some of the search control knowledge explicitly modifiable, which 19 RIGHT (?Col, ?Row) Preconditions: TILE-AT (?Col, ?Row) ?Col &lt; 4 not (HOLE-AT (?Col + 1, ?Row)) Deletes: TILE-AT (?Col, ?Row) Adds: TILE-AT (?Col + 1, ?Row) LEFT (?Col, ?Row) Preconditions: TILE-AT <p> The immutable portion typically decides which search choice selection needs to be made next and whether the choice selection is backtrackable. The explicitly modifiable portions usually make the choice selections, e.g., deciding which operator to use to achieve a given goal, etc. I will follow Minton's <ref> [13] </ref> terminology and call the former the problem-solving procedure and call the latter the decision procedures. While the decision procedures are modifiable, not all parts of the decision procedure need be directly modifiable. <p> I implemented Bacall as an extension to the PRODIGY planner <ref> [13] </ref>. PRODIGY extends traditional totally-ordered-plan means-ends planning by possessing a facility for allowing the user to easily encode search control rules for a particular domain. Like many other such planners, PRODIGY also uses the Strong Linearity [6] rejection heuristic to reduce the number of totally-ordered plans it explores.
Reference: [14] <author> T. Mitchell, R. Keller, and S. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year> <month> 149 </month>
Reference-contexts: However, this is an area that requires more research. 3 While I have been assuming that the specialization test has been generated via an explanation-based process (i.e., Explanation Based Learning <ref> [14] </ref>), induction techniques might also be used. 4 Success explanations are usually cheap to test because the variables are existentially quantified while the tests in failure explanations usually contain some variables that are universally quantified. 50 4.3.2 Granularity Another design choice is: Once the system has decided that a rejection heuristic
Reference: [15] <author> T. Mitchell, S. Mahadevan, and L. Steinberg. </author> <title> Leap: A Learning Apprentice Approach for VLSI Design. </title> <booktitle> In Proceedings of the Ninth International Conference on Artificial Intelligence, </booktitle> <address> Los Angeles, CA, </address> <year> 1985. </year>
Reference-contexts: One of these sources of knowledge is a solution that is acceptable to the user. If the user is a domain expert using the problem-solver as a labor-saving tool, then the system can act as a Learning-Apprentice <ref> [15] </ref>. <p> Thus SHAPeS has adopted this learning apprentice approach <ref> [15] </ref>. 53 Chapter 5 Analysis In this chapter I discuss my correctness, generality, and computational complexity claims about the SHAPeS algorithm. In these discussions of correctness, I will analyze the effect of modifying the problem-solver's search control logic upon its coverage.
Reference: [16] <author> J. Pearl. </author> <title> Heuristics: intelligent search strategies for computer problem solving. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: For example, if one removes the G3 Goal Reject Rule then the CHOOSE-GOALS decision procedure will produce a candidate set containing G3 and G5. The CHOOSE-OPERATORS decision procedure will then select OP3 to achieve G3, and the problem is solved. Note, however, if the search procedure is systematic <ref> [16, 12] </ref> (i.e., there is never more than one meta-level path to any partial solution) then if the rejection rules separately sanction a solution then they will sanction that solution jointly. 5.1.4 Summary I have identified three types of undesirable search control rule interactions that can occur in BCPS, namely: (1) <p> One was always able to select the "right" alternative, therefore backtracking is unnecessary. The second assumption not only helps guarantee that one can find a meta-level path to the solution without backtracking, it also guarantees that the search is systematic <ref> [16] </ref>.
Reference: [17] <author> M. Perez and J. Carbonell. </author> <title> Control knowledge to improve plan quality. </title> <booktitle> In Proceedings of the Second International Conference on AI Planning Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: While many systems may implicitly learn search control rules to improve solution quality, QUALITY <ref> [17] </ref> does so explicitly. 2.3.1 QUALITY Perez [17] attacked the problem of learning search control rules that lead to better solutions. <p> While many systems may implicitly learn search control rules to improve solution quality, QUALITY <ref> [17] </ref> does so explicitly. 2.3.1 QUALITY Perez [17] attacked the problem of learning search control rules that lead to better solutions. Learning was triggered when the user could find a better solution (according to a given objective function 3 ) than the one found by the planner's current set of search control rules.
Reference: [18] <author> A. Segre, C. Elkan, and A. Russell. </author> <title> A critical look at experimental evaluation of ebl. </title> <journal> Machine Learning, </journal> <volume> 6(2), </volume> <year> 1991. </year>
Reference-contexts: Therefore, for these runs, a deviation of more than .5% will be statistically significant. The in situ problems always ran to completion (regardless of whether a solution was found or not) and thus were not affected by resource horizon <ref> [18] </ref> effects. However, this was not true for the scalability problems, and consequently those results could be so affected. 6.2.4 Modification Sets From both in situ problem sets, Bacall learned a set of Bacall refinements which would enable PRODIGY to solve more problems in that problem set 4 . <p> When the "faster" problem-solver always finished its attempts at solving a problem, this approach was valid. However, as Segre, Elkan, and Russell <ref> [18] </ref> point out, if the "faster" system has problems censored by resource limits then experiments that compare these two systems' problem-solving speeds can exhibit an horizon effect, i.e., changing the resource bounds can radically change which system appears faster.
Reference: [19] <author> G. Sussman. </author> <title> A Computer Model of Skill Acquisition. </title> <publisher> American Elsevier Publishing Company, </publisher> <year> 1975. </year>
Reference-contexts: The final part installs the modification into the problem-solver's set of explicit search control rules. A restricted version of this algorithm has been implemented as an extension to the PRODIGY [13] problem-solver. This extension is called Bacall. The main restriction is that Bacall can only modify the linearity <ref> [19] </ref> and the strong linearity [6] rejection heuristics. Linearity directs the problem-solver to work on goals in a strict depth-first fashion, i.e., pick a top-level goal and an operator to achieve it, then pick one of that operator's preconditions and an operator to achieve that, etc. <p> In particular, I will describe the implementation of the ELF algorithm, the domains, the training and testing problem sets, and the ELF generated modification sets. 6.2.1 Implementation: Bacall Unfortunately, the implementation, called Bacall, preceded the theory and is restricted to refining one specific rejection heuristic, Linearity <ref> [19] </ref>.
Reference: [20] <author> M. Veloso. </author> <title> Learning by Analogical Resoning in General Problem Solving. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> August </month> <year> 1992. </year> <month> 150 </month>
Reference-contexts: Unfortunately, the impossibility theory is domain specific and each new domain requires a new one. 2.2.3 HAMLET HAMLET [4] learns PRODIGY select rules for the different types of choice points found in NOLIMIT <ref> [20] </ref>, PRODIGY's non-linear problem-solver. HAMLET uses two mechanisms to produce overgeneral preconditions for these search control rules.
References-found: 20

