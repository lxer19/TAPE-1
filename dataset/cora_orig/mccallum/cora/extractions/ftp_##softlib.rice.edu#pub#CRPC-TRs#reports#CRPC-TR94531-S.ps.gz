URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94531-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: e-mail: rshankar, ranka@top.cis.syr.edu  
Title: Random Data Accesses on a Coarse-grained Parallel Machine II. One-to-many and Many-to-one Mappings  
Author: Ravi V. Shankar Sanjay Ranka 
Date: October 1994  
Address: Syracuse, NY 13244-4100  
Affiliation: School of Computer and Information Science Syracuse University,  
Abstract: This paper describes deterministic communication-efficient algorithms for performing random data accesses with hot spots on a coarse-grained parallel machine. The general random access read/write operations with hot spots can be completed in Cn=p (+ lower order terms) time and is optimal and scalable provided n O(p 3 +p 2 t =) (n is the number of elements distributed across p processors, t is the start-up overhead and 1= is the data transfer rate). C is a small constant between 3 and 4 for the random access write operation, slightly higher for the random access read operation. Monotonic random access reads/writes can be completed with smaller constants and are optimal for smaller n as well. The random access read/write operations provide the framework for the communication-efficient simulation of CREW and CRCW PRAMs on a coarse-grained distributed memory parallel machine. A companion paper [24] deals with the problem of performing dynamic permutations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Seungjo Bae, Sanjay Ranka, Ravi V. Shankar. </author> <title> The HARD Primitive Scalable Parallelization of the FORALL statement and the gather/scatter Primitives in HPF, </title> <note> (in preparation). 15 </note>
Reference-contexts: We believe that the above primitives will be of crucial importance in the implementation of data parallel constructs such as those found in High Performance Fortran <ref> [1] </ref>. These primitives are also important for the implementation of the BSP model [27] on coarse-grained machines. The parallelization of data parallel applications on coarse-grained machines have been limited to regular applications or irregular applications with relatively static structure and limited hot spots.
Reference: [2] <author> A. Bar-No. and S. Kipnis. </author> <title> Designing Broadcasting Algorithms for the Postal Model for Message--Passing Systems, </title> <booktitle> Proceedings of 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> pp. 13-22. </pages>
Reference-contexts: This permits us to use the two-level model and view the underlying interconnection network as a virtual crossbar network connecting the processors. The logP [7] model and the postal model <ref> [2] </ref> are 2 theoretical models based on the above philosophy, for coarse-grained machines. * Sending a Message Assuming no node contention, the time taken to send a message from one processor to another is modeled as t + m, where m is the size of the message. * Global Combine and
Reference: [3] <author> S. Bhatt, M. Chen, C. Lin, and P. Liu. </author> <title> Abstractions for parallel n-body simulation, </title> <booktitle> Scalable High-Performance Computing Conference SHPCC, </booktitle> <year> 1992. </year>
Reference-contexts: A simple example of this is the generation of a locally essential tree on every processor, as is done for the parallelization of the Barnes-Hut-based n-body implementations <ref> [28, 3, 26] </ref>. The inverse of the above scenario is one in which data items are written into an accumulation array. Different entries within the accumulation array receive different numbers of writes.
Reference: [4] <author> G. E. Blleloch. NESL: </author> <title> A nested data parallel language, </title> <type> Technical Report CMU-CS-93-129, </type> <institution> Carnegie Mellon University, </institution> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Preliminary results for exploiting spatial locality for adaptive and irregular applications have been presented in [18, 19, 20]. The H-PRAM model [10, 11] presented a framework for the exploitation of structural locality. Structural locality has also been exploited in nested data parallel languages such as NESL <ref> [4] </ref> and in Proteus [16]. Our algorithms for parallel random data accesses were designed to incorporate such additions in the immediate future.
Reference: [5] <author> Shahid H. Bokhari. </author> <title> Complete Exchange on the iPSC/860, </title> <type> ICASE Technical Report No. 91-4, </type> <institution> NASA Langley Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: Although our algorithms are analyzed under the assumptions of a virtual crossbar, most of them are architecture independent and can be efficiently implemented on meshes and hypercubes. For example, complete exchange is a well-studied problem for which several algorithms are available in the literature using hypercubes <ref> [5] </ref> (time requirements proportional to traffic) and meshes [9] (time 3 requirements based on the cross-section bandwidth). The global combine and prefix scans can be completed on a hypercube in the time specified above.
Reference: [6] <author> Zeki Bozkus, Sanjay Ranka, Geoffrey C. Fox. </author> <title> Benchmarking the CM-5 Multicomputer, </title> <booktitle> Proceedings of the Frontiers of Massively Parallel Computation, </booktitle> <pages> pp. 100-107, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The global combine and prefix scans can be completed on a hypercube in the time specified above. The presence of specialized hardware networks like the control network on the CM-5 can speed up the operation considerably for small vectors <ref> [6] </ref>. 3 RAW with Limited Hot Spots Special cases of the hot spots problem arise when the hot spots are distributed uniformly across the processors. This is illustrated through three examples in figure 3. In example (a) the hot spots could be resolved through simple local processing and no communication.
Reference: [7] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, T. von Eicken, </author> <title> LogP: Towards a Realistic Model of Parallel Computation, </title> <booktitle> Proceedings of 4th ACM Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pp. 1-12, </pages> <year> 1993. </year>
Reference-contexts: This permits us to use the two-level model and view the underlying interconnection network as a virtual crossbar network connecting the processors. The logP <ref> [7] </ref> model and the postal model [2] are 2 theoretical models based on the above philosophy, for coarse-grained machines. * Sending a Message Assuming no node contention, the time taken to send a message from one processor to another is modeled as t + m, where m is the size of
Reference: [8] <author> Willian J. Dally and Chuck L. Seitz. </author> <title> Deadlock-Free Message Routing in Multiprocessor Interconnection Networks, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 36(5):pp. </volume> <pages> 547-553, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Communication between processors has a start-up overhead of t , while the data transfer rate is 1=. For our complexity analysis we assume that t and are constant, independent of the link congestion and distance between two nodes. With new techniques such as wormhole routing and randomized routing <ref> [15, 14, 8, 17] </ref>, the distance between communicating processors seems to be less of a determining factor on the amount of time needed to complete the communication.
Reference: [9] <author> S. E. Hambrusch, F. Hameed, and A. A. Khokhar, </author> <title> Communication Operations on Coarse-Grained Mesh Architectures, </title> <type> Technical Report, </type> <institution> Department of Computer Science, Purdue University. </institution>
Reference-contexts: For example, complete exchange is a well-studied problem for which several algorithms are available in the literature using hypercubes [5] (time requirements proportional to traffic) and meshes <ref> [9] </ref> (time 3 requirements based on the cross-section bandwidth). The global combine and prefix scans can be completed on a hypercube in the time specified above.
Reference: [10] <author> T. Heywood and S. Ranka. </author> <title> A Practical Hierarchical Model of Parallel Computation: The Model, </title> <journal> Journal of Parallel and Distributed Computing, November 1992, </journal> <volume> vol. 3, </volume> <pages> pp. 212-232. </pages>
Reference-contexts: Another important requirement for efficient data parallel implementations is the exploitation of structural as well as spatial locality in applications. Preliminary results for exploiting spatial locality for adaptive and irregular applications have been presented in [18, 19, 20]. The H-PRAM model <ref> [10, 11] </ref> presented a framework for the exploitation of structural locality. Structural locality has also been exploited in nested data parallel languages such as NESL [4] and in Proteus [16]. Our algorithms for parallel random data accesses were designed to incorporate such additions in the immediate future.
Reference: [11] <author> T. Heywood and S. Ranka. </author> <title> A Practical Hierarchical Model of Parallel Computation: Binary Tree and FFT Graph Algorithms, </title> <journal> Journal of Parallel and Distributed Computing, November 1992, </journal> <volume> vol. 3, </volume> <pages> pp. 233-249. </pages>
Reference-contexts: Another important requirement for efficient data parallel implementations is the exploitation of structural as well as spatial locality in applications. Preliminary results for exploiting spatial locality for adaptive and irregular applications have been presented in [18, 19, 20]. The H-PRAM model <ref> [10, 11] </ref> presented a framework for the exploitation of structural locality. Structural locality has also been exploited in nested data parallel languages such as NESL [4] and in Proteus [16]. Our algorithms for parallel random data accesses were designed to incorporate such additions in the immediate future.
Reference: [12] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification, </title> <month> March </month> <year> 1994. </year>
Reference-contexts: Figure 2 shows an example of a RAW where collisions are resolved using a binary associative operator (shown as a + in the figure). A forall statement of the following type in High Performance Fortran (HPF) <ref> [12] </ref> results in a RAR: forall (i=0:n-1) A (i) = D (P (i)) while the loop shown below when executed in parallel results in a RAW: do (i=0:n-1) In a RAW, the n elements writing data can be viewed as n threads of control in the p processors.
Reference: [13] <author> Joseph Jaja. </author> <title> An Introduction to Parallel Algorithms Addison-Wesley, </title> <year> 1992. </year>
Reference-contexts: This eliminates the sending of requested element addresses from the intermediate to source processors, reducing time taken by pt + n=p. 8 CREW/CRCW PRAM Simulation The PRAM is a shared-memory parallel programming model that has been widely used for the design of parallel algorithms <ref> [13] </ref>. It is an abstract model which does not differentiate between the cost of unit computation and unit communication. By simulating a PRAM on a more realistic (but still sufficiently general) model, an efficient and reasonably architecture-independent implementation of shared memory on distributed memory machines can be provided.
Reference: [14] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms, </title> <address> Benjamin-Cummings, </address> <year> 1994. </year>
Reference-contexts: Communication between processors has a start-up overhead of t , while the data transfer rate is 1=. For our complexity analysis we assume that t and are constant, independent of the link congestion and distance between two nodes. With new techniques such as wormhole routing and randomized routing <ref> [15, 14, 8, 17] </ref>, the distance between communicating processors seems to be less of a determining factor on the amount of time needed to complete the communication.
Reference: [15] <author> C. Leiserson et al. </author> <title> The Network Architecture of the Connection Machine CM-5, </title> <booktitle> Proc. 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> San Diego, CA, </address> <year> 1992. </year> <month> 16 </month>
Reference-contexts: Communication between processors has a start-up overhead of t , while the data transfer rate is 1=. For our complexity analysis we assume that t and are constant, independent of the link congestion and distance between two nodes. With new techniques such as wormhole routing and randomized routing <ref> [15, 14, 8, 17] </ref>, the distance between communicating processors seems to be less of a determining factor on the amount of time needed to complete the communication.
Reference: [16] <author> P. Mills, L. Nyland, J. Prins, J. Reif, and R. Wagner. </author> <title> Protoyping Parallel and Distributed Programs in Proteus, </title> <booktitle> Proceedings of Symposium on Parallel and Distributed Processing, </booktitle> <year> 1991. </year>
Reference-contexts: The H-PRAM model [10, 11] presented a framework for the exploitation of structural locality. Structural locality has also been exploited in nested data parallel languages such as NESL [4] and in Proteus <ref> [16] </ref>. Our algorithms for parallel random data accesses were designed to incorporate such additions in the immediate future.
Reference: [17] <author> Lionel M. Ni and Philip K. McKinley. </author> <title> A Survey of Wormhole Routing Techniques in Direct Networks, </title> <journal> IEEE Computer, </journal> <volume> 26(2) </volume> <pages> 62-76, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Communication between processors has a start-up overhead of t , while the data transfer rate is 1=. For our complexity analysis we assume that t and are constant, independent of the link congestion and distance between two nodes. With new techniques such as wormhole routing and randomized routing <ref> [15, 14, 8, 17] </ref>, the distance between communicating processors seems to be less of a determining factor on the amount of time needed to complete the communication.
Reference: [18] <author> Chao-Wei Ou and Sanjay Ranka. </author> <title> Parallel Remapping Algorithms for Adaptive Problems, </title> <type> Frontiers '95. </type> <note> To appear. </note>
Reference-contexts: Another important requirement for efficient data parallel implementations is the exploitation of structural as well as spatial locality in applications. Preliminary results for exploiting spatial locality for adaptive and irregular applications have been presented in <ref> [18, 19, 20] </ref>. The H-PRAM model [10, 11] presented a framework for the exploitation of structural locality. Structural locality has also been exploited in nested data parallel languages such as NESL [4] and in Proteus [16].
Reference: [19] <author> Chao-Wei Ou and Sanjay Ranka. </author> <title> Parallel Incremental Graph Partitioning Using Linear Programming, </title> <booktitle> Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Another important requirement for efficient data parallel implementations is the exploitation of structural as well as spatial locality in applications. Preliminary results for exploiting spatial locality for adaptive and irregular applications have been presented in <ref> [18, 19, 20] </ref>. The H-PRAM model [10, 11] presented a framework for the exploitation of structural locality. Structural locality has also been exploited in nested data parallel languages such as NESL [4] and in Proteus [16].
Reference: [20] <author> Chao-Wei Ou, Sanjay Ranka, and Geoffrey Fox. </author> <title> Fast Mapping And Remapping Algorithm For Irregular and Adaptive Problems, </title> <booktitle> Proceedings of the 1993 International Conference on Parallel and Distributed Systems, </booktitle> <address> Taipei, Taiwan, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Another important requirement for efficient data parallel implementations is the exploitation of structural as well as spatial locality in applications. Preliminary results for exploiting spatial locality for adaptive and irregular applications have been presented in <ref> [18, 19, 20] </ref>. The H-PRAM model [10, 11] presented a framework for the exploitation of structural locality. Structural locality has also been exploited in nested data parallel languages such as NESL [4] and in Proteus [16].
Reference: [21] <author> Victor K. Prasanna, Cho-Li Wang, </author> <title> Scalable Data Parallel Object Recognition using Geometric Hashing on the CM-5. </title> <booktitle> Scalable High Performance Computing Conference, </booktitle> <address> SHPCC, </address> <year> 1994. </year>
Reference-contexts: Different entries within the accumulation array receive different numbers of writes. Often hot spots tend to dictate the time taken to perform the communication, and their distribution is available only at run-time. Such hot spots are inherent features of many algorithms such as those for histogramming, geometric hashing <ref> [21, 25] </ref>, and database searching. Our algorithms alleviate the hot spots problem by dynamically stretching and shrinking them as necessary. The algorithms are communication-efficient and the number of data movements between processors is kept very low.
Reference: [22] <author> D. Nassimi and S. Sahni. </author> <title> Data Broadcasting in SIMD Computers, </title> <journal> IEEE Transactions on Computers C-30(2):101-107 (1981). </journal>
Reference-contexts: 1 Introduction Let n be the number of elements distributed across p processors. In a Random Access Read (RAR), each of the n elements may need to read data from another element <ref> [22] </ref>. The data is available in array D. Each element has the index of the element from which data is needed in array P . That is, element i needs D (P (i)). Figure 1 shows an example of a RAR. <p> 4 5 6 7 Pointer P 7 . 0 7 1 6 3 0 After RAR D (7) - D (0) D (7) D (1) D (6) D (3) D (0) In a Random Access Write (RAW) each of the n elements may need to write data to another element <ref> [22] </ref>. The data is available in array D. Each element has, in array P , the index of the element to which it has to send its data. Unlike the RAR case, it is possible to have collisions during a RAW. <p> The remainder of the steps can be performed in t q + n=p, since each processor sends out no more than q messages and these q messages contain a total of n=p elements. Therefore, a monotonic RAW can be performed in t q + n=p time. The generalize primitive <ref> [22] </ref> shown in figure 12, can also be done using an algorithm similar to the monotonic RAW algorithm. Here, D (i), the data from element i (0 i &lt; n) has to be sent to elements P (i 1) + 1 to P (i) (assume P (1) = 1). <p> Two companion primitives, the concentrate and the distribute also work with sorted destinations but, unlike generalize, perform only one-to-one mappings. These primitives were originally defined in <ref> [22] </ref> and algorithms for these for a coarse-grained parallel machine were presented in [24]. On a coarse-grained architecture, the generalize primitive involves each of the sending processors sending out either dr=pe or br=pc elements. The total number of elements sent out r n.
Reference: [23] <author> Ravi V. Shankar, Khaled A. Alsabti, Sanjay Ranka. </author> <title> The Transportation Primitive, </title> <type> CIS Technical Report, </type> <institution> Syracuse University, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: These asymptotic results are based on a worst case analysis. A probabilistic analysis brings the O (p 2 ) term in the traffic requirement down to O (p p ln p) <ref> [23] </ref>. Although our algorithms are analyzed under the assumptions of a virtual crossbar, most of them are architecture independent and can be efficiently implemented on meshes and hypercubes. <p> An important feature of the parallel RAW algorithm presented here is the grouping of elements into 2 The two stages in the RAW algorithm are entirely different from the two stages in the algorithms for dynamic permutations [24] or transportation <ref> [23] </ref>. 5 buckets (as explained below). The parallel RAW algorithm takes O (kp + n=kp) + 3n=p time, k being the number of buckets per processor. The algorithm is optimal with a complexity of 3n=p when n O (p 3 + p 2 t =).
Reference: [24] <author> Ravi V. Shankar, Sanjay Ranka. </author> <title> Random Data Accesses on a Coarse-Grained Parallel Machine I. One-to-one Mappings, </title> <type> CIS Technical Report, </type> <institution> Syracuse University, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: This is a bounded transportation problem, similar to the communication that underlies a dynamic permutation. It can be performed in 2n=p time for n O (p 3 + p 2 t =) (see <ref> [24] </ref> for details). 1 In all the communication matrices that appear in this paper, the row and column numbers give the indices of the sending and receiving processors respectively. <p> The RAW algorithm that deals with arbitrary hot spots includes two major communication stages and involves: * pre-processing at the source processors, * communication between the source and intermediate processors (which resembles the commu nication underlying a dynamic permutation <ref> [24] </ref>), * pre-processing at the intermediate processors (which includes prefix scans that resolve some of the collisions at hot spots), * communication between the intermediate and destination processors (which resembles the com munication underlying a monotonic dynamic permutation [24]), and * post-processing at the destination processors. <p> and intermediate processors (which resembles the commu nication underlying a dynamic permutation <ref> [24] </ref>), * pre-processing at the intermediate processors (which includes prefix scans that resolve some of the collisions at hot spots), * communication between the intermediate and destination processors (which resembles the com munication underlying a monotonic dynamic permutation [24]), and * post-processing at the destination processors. Dividing the threads writing data and the elements in shared memory being written to, equally among the p processors, results in n=p threads and n=p amount of shared memory per processor. <p> An important feature of the parallel RAW algorithm presented here is the grouping of elements into 2 The two stages in the RAW algorithm are entirely different from the two stages in the algorithms for dynamic permutations <ref> [24] </ref> or transportation [23]. 5 buckets (as explained below). The parallel RAW algorithm takes O (kp + n=kp) + 3n=p time, k being the number of buckets per processor. The algorithm is optimal with a complexity of 3n=p when n O (p 3 + p 2 t =). <p> Message coalescing is preceded by a sequential reshu*ing of the data local to each processor, if the element size is small. If the element size is large, local reshu*ing can be avoided through the use of a set of pointers to describe each message <ref> [24] </ref>. 4.2 Communication Between Source and Intermediate Processors The communication between the source and intermediate processors, as determined by the preprocessing stage, is a bounded transportation problem, similar to the communication that underlies a dynamic permutation. <p> multiple one-to-one communications take O (log p)t + O (n=kp) time combined and the constants involved are very small. 4.4 Communication Between Intermediate and Destination Processors The communication between the intermediate and destination processors is again a bounded transportation problem, similar to the communication that underlies a monotonic dynamic permutation <ref> [24] </ref>. Figure 8 shows the second stage's communication for the example introduced in figure 4. Consider a bucket B b that is spread across many processors. A processor that contains B b as a preceding partial bucket does not send out any elements of B b . <p> The upper bound on the number of elements received at any destination processor is dn=pe since each of the k buckets at a destination receives no more than n=kp elements. This communication can be completed in (n=p + n=kp) time <ref> [24] </ref>. <p> The predetermined contention at the sparse buckets can be used to modify the initial stretching of buckets such that the traffic the bound at the receiving processors is brought down to n=p. Communication for the first stage of the RAW now takes only 2n=p time (Optimizations described in <ref> [24] </ref> can also be applied to the first stage). Thus the RAW algorithm retains the small constants associated with the communication time even when the sparse buckets are not processed using the two-stage algorithm. <p> Two companion primitives, the concentrate and the distribute also work with sorted destinations but, unlike generalize, perform only one-to-one mappings. These primitives were originally defined in [22] and algorithms for these for a coarse-grained parallel machine were presented in <ref> [24] </ref>. On a coarse-grained architecture, the generalize primitive involves each of the sending processors sending out either dr=pe or br=pc elements. The total number of elements sent out r n. Each of the receiving processors receives no more than dn=pe elements from no more than q processors (q &lt; p). <p> By simulating a PRAM on a more realistic (but still sufficiently general) model, an efficient and reasonably architecture-independent implementation of shared memory on distributed memory machines can be provided. The various algorithms presented 14 in the companion paper <ref> [24] </ref> described the simulation of the exclusive read and exclusive write capabilities. The algorithms presented in this paper describe the simulation of the concurrent read and concurrent write capabilities.
Reference: [25] <author> Ravi V. Shankar, Sanjay Ranka. </author> <title> Histogramming based Algorithms on a Coarse-grained Parallel Machine, </title> <note> (in preparation). </note>
Reference-contexts: Different entries within the accumulation array receive different numbers of writes. Often hot spots tend to dictate the time taken to perform the communication, and their distribution is available only at run-time. Such hot spots are inherent features of many algorithms such as those for histogramming, geometric hashing <ref> [21, 25] </ref>, and database searching. Our algorithms alleviate the hot spots problem by dynamically stretching and shrinking them as necessary. The algorithms are communication-efficient and the number of data movements between processors is kept very low.
Reference: [26] <author> J. P. Singh. </author> <title> Parallel Hierarchical N-body Methods and Their Implications for Multiprocessors, </title> <type> Ph.D. thesis, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: A simple example of this is the generation of a locally essential tree on every processor, as is done for the parallelization of the Barnes-Hut-based n-body implementations <ref> [28, 3, 26] </ref>. The inverse of the above scenario is one in which data items are written into an accumulation array. Different entries within the accumulation array receive different numbers of writes.
Reference: [27] <author> L. G. Valiant, </author> <title> A Bridging Model for Parallel Computation, </title> <journal> Communication of the ACM, </journal> <volume> vol. 2, No. 8, </volume> <year> 1990, </year> <pages> pp. 103-111. </pages> <note> SIAM Journal of Sci. and Stat. Computation, </note> <year> 1991. </year>
Reference-contexts: We believe that the above primitives will be of crucial importance in the implementation of data parallel constructs such as those found in High Performance Fortran [1]. These primitives are also important for the implementation of the BSP model <ref> [27] </ref> on coarse-grained machines. The parallelization of data parallel applications on coarse-grained machines have been limited to regular applications or irregular applications with relatively static structure and limited hot spots.
Reference: [28] <author> M. Warren and J. Salmon. </author> <title> Astrophysical N-body simulations using hierarchical tree data structures, </title> <booktitle> Supercomputing, </booktitle> <year> 1992. </year> <month> 17 </month>
Reference-contexts: A simple example of this is the generation of a locally essential tree on every processor, as is done for the parallelization of the Barnes-Hut-based n-body implementations <ref> [28, 3, 26] </ref>. The inverse of the above scenario is one in which data items are written into an accumulation array. Different entries within the accumulation array receive different numbers of writes.
References-found: 28

