URL: http://www.ai.univie.ac.at/~juffi/lig/Papers/lynch-thesis.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Email: Email: rlynch@iol.ie  Email: niall.griffith@ul.ie  
Title: Application of Temporal Difference Learning to Draughts  
Author: by Mark Lynch Supervisor: Niall Griffith 
Address: Limerick, Ireland.  
Affiliation: Dept. of CSIS, University of  
Note: An  
Abstract: Final Year Report NeuroDraughts Abstract NeuroDraughts is a draughts playing program which follows the approach of both NeuroGammon (G.Tesauro) and NeuroChess (S.Thrun). It uses an artificial neural network trained by the method of temporal difference learning. It achieves a high level of play simply by self play, with minimal search, and without any expert game analysis. Keywords: Temporal Difference Learning, Input modelling, Game Theory, Draughts, Checkers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Derek Oldbury. </author> <title> Move Over or How to win at Draughts. </title>
Reference-contexts: Anyone doubting the complexity of the game should refer to Oldsburys great book on the game, Move-Over <ref> [1] </ref> or to [14]. 1.3 Why Temporal Difference Learning (TD)? The TD (lambda) family of learning procedures have been applied with astounding success in the last decade. Most notable among these successes is G. Tesauros NeuroGammon [2] which plays backgammon at world champion level. S.
Reference: [2] <author> G.J. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <year> 1992. </year>
Reference-contexts: Most notable among these successes is G. Tesauros NeuroGammon <ref> [2] </ref> which plays backgammon at world champion level. S. Thrun also created Neuro-Chess which played a relatively strong game [3]. The TD procedure is nothing new however having been first applied by A.L. Samuels in his famous checkers program all the way back in 1959 [4].
Reference: [3] <author> S. Thrun. </author> <title> Learning to play the game of chess. </title> <editor> In G. Tesauro, D.Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> San Mateo, CA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Most notable among these successes is G. Tesauros NeuroGammon [2] which plays backgammon at world champion level. S. Thrun also created Neuro-Chess which played a relatively strong game <ref> [3] </ref>. The TD procedure is nothing new however having been first applied by A.L. Samuels in his famous checkers program all the way back in 1959 [4]. The procedure has since been formalised and convergence proven by Sutton [5].
Reference: [4] <author> A.L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of research and development, </journal> <volume> 3, </volume> <pages> pages 210-229, </pages> <year> 1959. </year>
Reference-contexts: Tesauros NeuroGammon [2] which plays backgammon at world champion level. S. Thrun also created Neuro-Chess which played a relatively strong game [3]. The TD procedure is nothing new however having been first applied by A.L. Samuels in his famous checkers program all the way back in 1959 <ref> [4] </ref>. The procedure has since been formalised and convergence proven by Sutton [5]. <p> If just the raw input is used, small changes in the board can produce drastically different evaluations which goes against the smooth nature of the game. By mapping the board to a set of features (adapted from those provided by Samuels <ref> [4] </ref>) we increase the ability of the Evaluation network to generalise and thereby glean more information from each game. As part of the purpose of NeuroDraughts was to show the advantages (or plain necessity) of feature mapping several different strategies were tried.
Reference: [5] <author> R.S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: S. Thrun also created Neuro-Chess which played a relatively strong game [3]. The TD procedure is nothing new however having been first applied by A.L. Samuels in his famous checkers program all the way back in 1959 [4]. The procedure has since been formalised and convergence proven by Sutton <ref> [5] </ref>. TD is particularly well suited to game playing because instead of forming pairs between the actual outcome and each state encountered in the game, instead it updates its prediction at each time step to the prediction at the next time step. <p> One thread could be working out the next move whereas the other could be training for the current move. The famous example of why TD is superior to ordinary supervised learning approachs is the following game playing example as given in <ref> [5] </ref>. See Figure 3. Supposing you land in the novel state (which you have previously never encountered) and subsequently end up in the bad position but still win the game. What value is then associated with the novel state? 2 For a more detailed description of TD please see [2],[5].
Reference: [6] <author> D.E. Rummelhart, G.E. Hinton and R.J. Williams. </author> <title> Learning internal representation by error propagation. </title> <editor> In D.E. Rummelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. I + II. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year> <note> Final Year Report NeuroDraughts 31 </note>
Reference-contexts: Final Year Report NeuroDraughts 14 3.2 The Evaluation Network At the base of NeuroDraughts is a neural network. The network consists of a layer of inputs, hidden and a single output unit. The network is trained using back propagation with momentum <ref> [6] </ref> [7]. This net is trained to evaluate boards after the player in question has moved. 3.3 Representation of Board How the board is fed to the evaluation network was a major issue for NeuroDraughts.
Reference: [7] <author> Don Tveter. </author> <note> How to succeed at backprop. Available at http://www.mcs.com/~drt/software/backprop.zip </note>
Reference-contexts: On the third graph of figure 2 there is no single straight line that will place the open and filled circles in separate regions. 1 For a more comprehensive treatment of this problem please see <ref> [7] </ref>. Final Year Report NeuroDraughts 9 Training a MLP becomes a lot more complex due to the addition of the hidden layer. The backpropagation algorithm is basically a generalisation of the delta rule as used in the single layer perceptron. <p> Final Year Report NeuroDraughts 14 3.2 The Evaluation Network At the base of NeuroDraughts is a neural network. The network consists of a layer of inputs, hidden and a single output unit. The network is trained using back propagation with momentum [6] <ref> [7] </ref>. This net is trained to evaluate boards after the player in question has moved. 3.3 Representation of Board How the board is fed to the evaluation network was a major issue for NeuroDraughts. Although as little domain specific knowledge as possible was the aim, in practice this was impossible.
Reference: [8] <author> Martin Schmidt. </author> <title> A unification of Genetic Algorithms, Neural Networks and Fuzzy Logic. The GANNFL approach. </title>
Reference: [9] <author> Paul E. Utgoff. </author> <title> Feature function learning for value approximation. </title> <type> Technical Report 96 09. </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address> <month> Jan 20, </month> <year> 1996. </year>
Reference: [10] <author> Tom E. Fawcett. </author> <title> Feature discovery for problem solving systems. </title> <type> CMPSCI Technical Report 93-49. </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address> <month> May </month> <year> 1993. </year>
Reference: [11] <author> Tom E. Fawcett, Paul E. Utgoff. </author> <title> Automatic feature generation for problem solving systems. </title> <type> COINS technical report 92-9, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address> <month> Jan </month> <year> 1992. </year>
Reference: [12] <author> Ben Krse, Peter van der Smagt. </author> <title> An Introduction to Neural Networks. </title>
Reference: [13] <author> Michael Gherrity. </author> <title> A Game Learning Machine. </title> <type> Doctorate Thesis, </type> <institution> University of California, </institution> <address> San Diego. </address> <year> 1993. </year>
Reference-contexts: Given this information it is entirely feasible that the system can learn simply by exploring the state space using the transition rules given, as well as applying the appropriate rewards. A system created by Michael Gherrity, SAL <ref> [13] </ref>, is just such a system. It can theoretically learn to play any game based on a board. The user simply provides the system with the game structure and a list of rules governing play. Through self play it then develops above average levels of play. <p> This involved reading many journal papers and consulting with both Gerald Tesauro and Sebastian Thrun, who are probably the worlds leading authorities on the use of TD methods in game playing. Several papers exist which share some similar elements to NeuroDraughts, namely <ref> [13] </ref>, [15], [17], [18], [19], [20], [21] and [22]. 4.1 Design Strategy The first priority in the design of NeuroDraughts was to create and test an MLP. The TD elements would then be incorporated and the board mapping functions added.
Reference: [14] <author> Jonathan Schaeffer and Robert Lake, </author> <title> Solving the Game of Checkers. </title> <institution> University of Alberta. </institution>
Reference-contexts: Anyone doubting the complexity of the game should refer to Oldsburys great book on the game, Move-Over [1] or to <ref> [14] </ref>. 1.3 Why Temporal Difference Learning (TD)? The TD (lambda) family of learning procedures have been applied with astounding success in the last decade. Most notable among these successes is G. Tesauros NeuroGammon [2] which plays backgammon at world champion level. S.
Reference: [15] <author> Robert Levinson, </author> <title> General Game-Playing and Reinforcement Learning. </title> <type> UCSC-CRL-95 06. </type> <institution> University of California, Santa Cruz. </institution>
Reference-contexts: This involved reading many journal papers and consulting with both Gerald Tesauro and Sebastian Thrun, who are probably the worlds leading authorities on the use of TD methods in game playing. Several papers exist which share some similar elements to NeuroDraughts, namely [13], <ref> [15] </ref>, [17], [18], [19], [20], [21] and [22]. 4.1 Design Strategy The first priority in the design of NeuroDraughts was to create and test an MLP. The TD elements would then be incorporated and the board mapping functions added.
Reference: [16] <author> Justin A. Boyan. </author> <title> Modular Neural Networks for Learning Context-Dependant Game Strategies. </title> <type> Masters Thesis, </type> <institution> Cambridge University. </institution>
Reference: [17] <author> David Kenneth Olson. </author> <title> Learning to Play Games from Experience: An Application of Artificial Neural Networks and Temporal Difference Learning. </title> <month> Dec, 93. </month>
Reference-contexts: This involved reading many journal papers and consulting with both Gerald Tesauro and Sebastian Thrun, who are probably the worlds leading authorities on the use of TD methods in game playing. Several papers exist which share some similar elements to NeuroDraughts, namely [13], [15], <ref> [17] </ref>, [18], [19], [20], [21] and [22]. 4.1 Design Strategy The first priority in the design of NeuroDraughts was to create and test an MLP. The TD elements would then be incorporated and the board mapping functions added.
Reference: [18] <author> David Stoutamire. </author> <title> Machine Learning, Game Playing and Go. </title>
Reference-contexts: This involved reading many journal papers and consulting with both Gerald Tesauro and Sebastian Thrun, who are probably the worlds leading authorities on the use of TD methods in game playing. Several papers exist which share some similar elements to NeuroDraughts, namely [13], [15], [17], <ref> [18] </ref>, [19], [20], [21] and [22]. 4.1 Design Strategy The first priority in the design of NeuroDraughts was to create and test an MLP. The TD elements would then be incorporated and the board mapping functions added.
Reference: [19] <author> Marco A. Weiring. </author> <title> TD Learning of Game Evaluation Functions with Hierarchical Neural Architectures. </title> <institution> University of Amsterdam, </institution> <month> April 95. </month>
Reference-contexts: This involved reading many journal papers and consulting with both Gerald Tesauro and Sebastian Thrun, who are probably the worlds leading authorities on the use of TD methods in game playing. Several papers exist which share some similar elements to NeuroDraughts, namely [13], [15], [17], [18], <ref> [19] </ref>, [20], [21] and [22]. 4.1 Design Strategy The first priority in the design of NeuroDraughts was to create and test an MLP. The TD elements would then be incorporated and the board mapping functions added.
Reference: [20] <author> Peter Sommerlund, </author> <title> Artificial Neural Networks applied to strategic games. </title> <note> May 96. Final Year Report NeuroDraughts 32 </note>
Reference-contexts: This involved reading many journal papers and consulting with both Gerald Tesauro and Sebastian Thrun, who are probably the worlds leading authorities on the use of TD methods in game playing. Several papers exist which share some similar elements to NeuroDraughts, namely [13], [15], [17], [18], [19], <ref> [20] </ref>, [21] and [22]. 4.1 Design Strategy The first priority in the design of NeuroDraughts was to create and test an MLP. The TD elements would then be incorporated and the board mapping functions added.
Reference: [21] <author> David Carmel and Shaul Markovitch. </author> <title> Learning Models of Opponents Strategy in Game Playing. </title> <type> CIS Report #9318. June 93. </type> <institution> Technion, Israel Institute of Technology. </institution>
Reference-contexts: This involved reading many journal papers and consulting with both Gerald Tesauro and Sebastian Thrun, who are probably the worlds leading authorities on the use of TD methods in game playing. Several papers exist which share some similar elements to NeuroDraughts, namely [13], [15], [17], [18], [19], [20], <ref> [21] </ref> and [22]. 4.1 Design Strategy The first priority in the design of NeuroDraughts was to create and test an MLP. The TD elements would then be incorporated and the board mapping functions added.

References-found: 21

