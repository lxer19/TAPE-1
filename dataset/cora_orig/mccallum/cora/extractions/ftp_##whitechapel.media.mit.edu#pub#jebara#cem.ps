URL: ftp://whitechapel.media.mit.edu/pub/jebara/cem.ps
Refering-URL: http://jebara.www.media.mit.edu/people/jebara/cem.html
Root-URL: http://www.media.mit.edu
Email: f jebara,sandy g@media.mit.edu  
Title: Maximum Conditional Likelihood via Bound Maximization and the CEM Algorithm  
Author: Tony Jebara and Alex Pentland 
Note: EM are demonstrated.  
Web: http://www.media.mit.edu/ jebara  
Address: Cambridge MA  
Affiliation: Vision and Modeling, MIT Media Laboratory,  
Abstract: We present the CEM (Conditional Expectation Maximization) algorithm as an extension of the EM (Expectation Maximization) algorithm to conditional density estimation under missing data. A bounding and maximization process is given to specifically optimize conditional likelihood instead of the usual joint likelihood. We apply the method to conditioned mixture models and use bounding techniques to derive the model's update rules. Monotonic convergence, computational efficiency and regression results superior to 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amari. </author> <title> Information geometry of em and em algorithms for neural networks. Neural Networks, </title> <type> 8(9), </type> <year> 1995. </year>
Reference-contexts: Thus, many conditional problems are solved by first estimating joint models then conditioning them. This results in concise solutions such as the Nadarya-Watson estimator [2], Xu's mixture of experts [7], and Amari's em-neural networks <ref> [1] </ref>.
Reference: [2] <author> C. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford Press, </publisher> <year> 1996. </year>
Reference-contexts: However, popularity of maximum joint likelihood and EM techniques remains strong in part due to their elegance and convergence properties. Thus, many conditional problems are solved by first estimating joint models then conditioning them. This results in concise solutions such as the Nadarya-Watson estimator <ref> [2] </ref>, Xu's mixture of experts [7], and Amari's em-neural networks [1]. <p> Thus, many conditional problems are solved by first estimating joint models then conditioning them. This results in concise solutions such as the Nadarya-Watson estimator [2], Xu's mixture of experts [7], and Amari's em-neural networks [1]. However, direct conditional density approaches <ref> [2, 4] </ref> can offer solutions with higher conditional likelihood on test data than their joint counter-parts. (a) L a = 4:2 L c b = 1:8 Popat [6] describes a simple visualization example where 4 clusters must be fit with 2 Gaussian models as in Figure 1. <p> However, EM is not as useful when applied to conditional density estimation and maximum conditional likelihood problems. Here, one typically resorts to other local optimization techniques such as gradient descent or second order Hessian methods <ref> [2] </ref>. We therefore introduce CEM, a variant of EM, which targets condi tional likelihood while maintaining desirable convergence properties. The CEM algorithm operates by directly bounding and decoupling conditional likelihood and simplifies M-step calculations.
Reference: [3] <author> A. Dempster, N. Laird, and D. Rubin. </author> <title> Maximum likelihood from incomplete data via the em algorithm. </title> <journal> Journal of the Royal Statistical Society, B39, </journal> <year> 1977. </year>
Reference-contexts: It is therefore of interest to directly optimize conditional models using con ditional likelihood. We introduce the CEM (Conditional Expectation Maximization) algorithm for this purpose and apply it to the case of Gaussian mixture models. 2 EM and Conditional Likelihood For joint densities, the tried and true EM algorithm <ref> [3] </ref> maximizes joint likelihood over data. However, EM is not as useful when applied to conditional density estimation and maximum conditional likelihood problems. Here, one typically resorts to other local optimization techniques such as gradient descent or second order Hessian methods [2].
Reference: [4] <author> M. Jordan and R. Jacobs. </author> <title> Hierarchical mixtures of experts and the em algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: Thus, many conditional problems are solved by first estimating joint models then conditioning them. This results in concise solutions such as the Nadarya-Watson estimator [2], Xu's mixture of experts [7], and Amari's em-neural networks [1]. However, direct conditional density approaches <ref> [2, 4] </ref> can offer solutions with higher conditional likelihood on test data than their joint counter-parts. (a) L a = 4:2 L c b = 1:8 Popat [6] describes a simple visualization example where 4 clusters must be fit with 2 Gaussian models as in Figure 1. <p> At this point, one would resort to a Generalized EM (GEM) approach which requires gradient or second-order ascent techniques for the M-step. For example, Jordan et al. overcome the difficult M-step caused by EM with an Iteratively Re-Weighted Least Squares algorithm in the mixtures of experts architecture <ref> [4] </ref>. 3 Conditional Expectation Maximization The EM algorithm can be extended by substituting Jensen's inequality for a different bound. Consider the upper variational bound of a logarithm x 1 log (x) (which becomes a lower bound on the negative log). <p> true lower bounding function Q (Equation 6). l c Q (fi t ; fi t1 ) = i=1 m=1 p (m; x i ; y i jfi t ) n=1 p (n; x i jfi t ) n=1 p (n; x i jfi t1 ) The Mixture of Experts formalism <ref> [4] </ref> offers a graceful representation of a conditional density using experts (conditional sub-models) and gates (marginal sub-models).
Reference: [5] <author> X. Meng and D. Rubin. </author> <title> Maximum likelihood estimation via the ecm algorithm: A general framework. </title> <journal> Biometrika, </journal> <volume> 80(2), </volume> <year> 1993. </year>
Reference-contexts: Both gates and experts are optimized independently and have no variables in common. An update is performed over the experts and then over the gates. If each of those causes an increase, we converge to a local maximum of conditional log-likelihood (as in Expectation Conditional Maximization <ref> [5] </ref>). p (yjx; fi) = m=1 ff n N (x; n xx )fiN (y; m yx ( m x ); m yx ( m xy ) n=1 ff n N (x; n x ; n xx ) P M x ; n P M (8) To update the experts, we hold
Reference: [6] <author> A. Popat. </author> <title> Conjoint probabilistic subband modeling (phd. </title> <type> thesis). Technical Report 461, </type> <institution> M.I.T. Media Laboratory, </institution> <year> 1997. </year>
Reference-contexts: However, direct conditional density approaches [2, 4] can offer solutions with higher conditional likelihood on test data than their joint counter-parts. (a) L a = 4:2 L c b = 1:8 Popat <ref> [6] </ref> describes a simple visualization example where 4 clusters must be fit with 2 Gaussian models as in Figure 1. Here, the model in (a) has a superior joint likelihood (L a &gt; L b ) and hence a better p (x; y) solution.
Reference: [7] <author> L. Xu, M. Jordan, and G. Hinton. </author> <title> An alternative model for mixtures of experts. </title> <booktitle> In Neural Information Processing Systems 7, </booktitle> <year> 1995. </year> <note> 2 http://www.ics.uci.edu/~mlearn/MLRepository.html </note>
Reference-contexts: However, popularity of maximum joint likelihood and EM techniques remains strong in part due to their elegance and convergence properties. Thus, many conditional problems are solved by first estimating joint models then conditioning them. This results in concise solutions such as the Nadarya-Watson estimator [2], Xu's mixture of experts <ref> [7] </ref>, and Amari's em-neural networks [1].
References-found: 7

