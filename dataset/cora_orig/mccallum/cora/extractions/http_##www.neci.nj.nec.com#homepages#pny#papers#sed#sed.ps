URL: http://www.neci.nj.nec.com/homepages/pny/papers/sed/sed.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/pny/papers/sed/main.html
Root-URL: 
Title: Learning String Edit Distance  
Author: Eric Sven Ristad Peter N. Yianilos 
Keyword: string edit distance, Levenshtein distance, stochastic transduction, pattern recognition, prototype dictionary, spelling correction, string correction, string similarity, string classification, speech recognition, pronunciation modeling, Switchboard corpus.  
Abstract: Research Report CSTR-532-96 October 1996; Revised April 1997 Abstract In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other. In this report, we provide a stochastic model for string edit distance. Our stochastic model allows us to learn the string edit distance function from a corpus of examples. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech. In this application, we learn a string edit distance function with one fourth the error rate of the untrained Levenshtein distance. Our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> COMLEX pronouncing lexicon, version 0.2. </editor> <booktitle> Linguistic Data Consortium LDC95L3, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Each step of the transduction generates either a substitution pair ha; bi, a deletion pair ha; *i, or an insertion pair h*; bi according to a probability function ffi : E ! <ref> [0; 1] </ref>. Being a probability function, ffi () satisfies the following constraints: a: 8z 2 E [ 0 ffi (z) 1 ] P Note that the null operation h*; *i is not included in the alphabet E of edit operations. <p> Approximately 280,000 words of Switchboard have been manually assigned phonetic transcripts at ICSI using a proprietary phonetic alphabet [6]. The Switchboard corpus also includes a pronouncing lexicon with 71,100 entries using a modified Pronlex phonetic alphabet (long form) <ref> [1] </ref>. In order to make the pronouncing lexicon compatible with the ICSI corpus of phonetic transcripts, we removed 148 entries from the lexicon and 73,068 samples from the ICSI corpus. 4 After filtering, our pronouncing lexicon had 70,952 entries for 66,284 syntactic words over an alphabet of 42 phonemes.
Reference: [2] <author> Baum, L., and Eagon, J. </author> <title> An inequality with applications to statistical estimation for probabilistic functions of a Markov process and to models for ecology. Bull. </title> <booktitle> AMS 73 (1967), </booktitle> <pages> 360-363. </pages>
Reference-contexts: For this task, we employ the powerful expectation maxi mization (EM) algorithm <ref> [2, 3, 4] </ref>. The EM algorithm is an iterative algorithm 4 that maximizes the probability of the training data according to the model. See [15] for a review. As its name suggests, the EM algorithm consists of two steps.
Reference: [3] <author> Baum, L., Petrie, T., Soules, G., and Weiss, N. </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Ann. Math. Stat. </journal> <volume> 41 (1970), </volume> <pages> 164-171. </pages>
Reference-contexts: For this task, we employ the powerful expectation maxi mization (EM) algorithm <ref> [2, 3, 4] </ref>. The EM algorithm is an iterative algorithm 4 that maximizes the probability of the training data according to the model. See [15] for a review. As its name suggests, the EM algorithm consists of two steps.
Reference: [4] <author> Dempster, A., Laird, N., and Rubin, D. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statist. Soc. Ser. B (methodological) 39 (1977), </journal> <pages> 1-38. </pages>
Reference-contexts: For this task, we employ the powerful expectation maxi mization (EM) algorithm <ref> [2, 3, 4] </ref>. The EM algorithm is an iterative algorithm 4 that maximizes the probability of the training data according to the model. See [15] for a review. As its name suggests, the EM algorithm consists of two steps. <p> probability of a surface form y v is the marginal of our model (5), which may be computed by summing over all the lexical entries L. p (y v j; L) = hw;x t i2L We estimate the parameters of our model (5) using expectation maximization for finite mixture models <ref> [4] </ref>.
Reference: [5] <author> Godfrey, J., Holliman, E., and McDaniel, J. </author> <title> Switchboard: telephone speech corpus for research and development. </title> <booktitle> In Proc. IEEE ICASSP (Detroit, </booktitle> <year> 1995), </year> <pages> pp. 517-520. </pages>
Reference-contexts: It also leads to a variant of string edit distance, that aggregates the many different ways to transform one string into another. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in the Switchboard corpus of conversational speech <ref> [5] </ref>. In this application, we learn a string edit distance function that reduces the error rate of the untrained Levenshtein distance by a factor of four. Let us first define our notation. <p> Experimental results obtained using this ad-hoc approach are presented in appendix A. Let us now apply our stochastic approach to the Switchboard corpus of conversational speech. 3.1 Switchboard Corpus The Switchboard corpus contains over 3 million words of spontaneous telephone speech conversations <ref> [5] </ref>. It is considered one of the most difficult corpora for speech recognition (and pronunciation recognition) because of the tremendous variability of spontaneous speech. As of Summer 1996, speech recognition technology has a word error rate above 45% on the Switchboard corpus.
Reference: [6] <author> Greenberg, S., Hollenbach, J., and Ellis, D. </author> <title> Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus. </title> <booktitle> In Proc. ICSLP (Philadelphia, </booktitle> <month> October </month> <year> 1996). </year>
Reference-contexts: The same speech recognition technology achieves a word error rate of less than 5% on read speech. Approximately 280,000 words of Switchboard have been manually assigned phonetic transcripts at ICSI using a proprietary phonetic alphabet <ref> [6] </ref>. The Switchboard corpus also includes a pronouncing lexicon with 71,100 entries using a modified Pronlex phonetic alphabet (long form) [1].
Reference: [7] <author> Hall, P., and Dowling, G. </author> <title> Approximate string matching. </title> <journal> Computing Surveys 12, </journal> <volume> 4 (1980), </volume> <pages> 381-402. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 21]. Many excellent reviews of the string edit distance literature are available <ref> [7, 9, 14, 20] </ref>. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11]. <p> Many excellent reviews of the string edit distance literature are available [7, 9, 14, 20]. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11]. A stochastic interpretation of string edit distance was first noted by Hall and Dowling <ref> [7, p.390-1] </ref> in their review of approximate string matching algorithms, but without a proposal for learning the edit costs. The principal contribution of this report is an effective algorithm for learning the primitive edit costs.
Reference: [8] <author> Jelinek, F., and Mercer, R. L. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Pattern Recognition in Practice (Amsterdam, </booktitle> <month> May 21-23 </month> <year> 1980), </year> <editor> E. S. Gelsema and L. N. Kanal, Eds., </editor> <publisher> North Holland, </publisher> <pages> pp. 381-397. </pages>
Reference-contexts: Such a stochastic transducer can be further strengthened with state-conditional interpolation <ref> [8] </ref>. We could also condition our probabilities ffi (z t jz t1 tn ; s) on a hidden state s drawn from a finite state space.
Reference: [9] <author> Kukich, K. </author> <title> Techniques for automatically correcting words in text. </title> <booktitle> ACM Compute. Surveys 24 (1992), </booktitle> <pages> 377-439. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 21]. Many excellent reviews of the string edit distance literature are available <ref> [7, 9, 14, 20] </ref>. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11].
Reference: [10] <author> Levenshtein, V. </author> <title> Binary codes capable of correcting deletions, </title> <journal> insertions, and reversals. Soviet Physics Doklady 10, </journal> <volume> 10 (1966), </volume> <pages> 707-710. </pages>
Reference-contexts: 1 Introduction In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other <ref> [10] </ref>. In this report, we provide a stochastic model for string edit distance. Our stochastic interpretation allows us to learn the optimal string edit distance function from a corpus of examples. <p> Note that the symbols ? and ! are not part of either the ICSI phonetic alphabet or the Pronlex phonetic alphabet (long forms), and are only used in the ICSI corpus. 11 Our seven models consist of Levenshtein distance <ref> [10] </ref> as well as six variants resulting from our two interpretations of three models. 5 Our two interpretations are the stochastic edit distance (4) and the classic edit distance (3), also called the Viterbi edit distance.
Reference: [11] <author> Marzal, A., and Vidal, E. </author> <title> Computation of normalized edit distance and applications. </title> <journal> IEEE Trans. </journal> <volume> PAMI 15, 9 (1993), </volume> <pages> 926-932. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 21]. Many excellent reviews of the string edit distance literature are available [7, 9, 14, 20]. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance <ref> [11] </ref>. A stochastic interpretation of string edit distance was first noted by Hall and Dowling [7, p.390-1] in their review of approximate string matching algorithms, but without a proposal for learning the edit costs. The principal contribution of this report is an effective algorithm for learning the primitive edit costs.
Reference: [12] <author> Masek, W., and Paterson, M. </author> <title> A faster algorithm computing string edit distances. </title> <journal> J. Comput. System Sci. </journal> <volume> 20 (1980), </volume> <pages> 18-31. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming <ref> [12, 21] </ref>. Many excellent reviews of the string edit distance literature are available [7, 9, 14, 20]. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11].
Reference: [13] <author> Oomman, B. </author> <title> Constrained string editing. </title> <booktitle> Information Sciences 40 (1986), </booktitle> <pages> 267-284. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 21]. Many excellent reviews of the string edit distance literature are available [7, 9, 14, 20]. Several variants have been proposed, including the constrained edit distance <ref> [13] </ref> and the normalized edit distance [11]. A stochastic interpretation of string edit distance was first noted by Hall and Dowling [7, p.390-1] in their review of approximate string matching algorithms, but without a proposal for learning the edit costs.
Reference: [14] <author> Peterson, J. </author> <title> Computer programs for detecting and correcting spelling errors. </title> <booktitle> Comm. ACM 23 (1980), </booktitle> <pages> 676-687. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 21]. Many excellent reviews of the string edit distance literature are available <ref> [7, 9, 14, 20] </ref>. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11].
Reference: [15] <author> Redner, R. A., and Walker, H. F. </author> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> SIAM Review 26, </journal> <volume> 2 (1984), </volume> <pages> 195-239. </pages>
Reference-contexts: For this task, we employ the powerful expectation maxi mization (EM) algorithm [2, 3, 4]. The EM algorithm is an iterative algorithm 4 that maximizes the probability of the training data according to the model. See <ref> [15] </ref> for a review. As its name suggests, the EM algorithm consists of two steps. In the expectation step, we accumulate the expectation of each hidden event on the training corpus. In our case the hidden events are the edit operations used to generate the string pairs.
Reference: [16] <author> Riley, M., Ljolje, A., Hindle, D., and Pereira, F. </author> <title> The AT&T 60,000 word speech-to-text system. </title> <booktitle> In Eurospeech'95: ECSA 4th Eu-ropean Conference on Speech Communication and Technology (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <editor> J. M. Pardo, E. Enrquez, J. Ortega, J. Ferreiros, J. Macas, and F.J.Valverde, Eds., </editor> <volume> vol. 1, </volume> <booktitle> European Speech Communication Association, </booktitle> <pages> pp. 207-210. </pages>
Reference-contexts: Current speech recognition technology typically employs a sparse hand-crafted pronouncing lexicon and assumes a uniform distribution on the pronunciations given the words. When the vocabulary is large or contains many proper nouns, then the pronouncing lexicon may be constructed semi-automatically using a text-to-speech system <ref> [16] </ref>. Our results suggest that a significant performance improvement is possible by employing a richer pronouncing lexicon, constructed directly from observed pronunciations, along with an adapted lexical entry model.
Reference: [17] <author> Riley, M. D., and Ljolje, A. </author> <title> Automatic generation of detailed pronunciation lexicons. In Automatic Speech and Speaker Recognition: Advanced Topics, </title> <editor> C.-H. Lee, F. K. Soong, and K. K. Paliwal, Eds. </editor> <publisher> Kluwer Academic, </publisher> <address> Boston, </address> <month> March </month> <year> 1996, </year> <note> ch. 12. </note>
Reference-contexts: Adapting p (x t jw) improves performance in experiments E3 and E4, but not in E1 and E2. Adapting p (w) and p (x t jw) together yields an unexpectedly large improvement in experiments E3 and E4, when compared to the improvement obtained by adapting each separately. <ref> [17] </ref>, who show an improvement in speech recognizer performance by employing a richer pronunciation model than is customary. 6 6 Our approach differs from the approach described by Riley and Ljolje [17] in three important ways. <p> an unexpectedly large improvement in experiments E3 and E4, when compared to the improvement obtained by adapting each separately. <ref> [17] </ref>, who show an improvement in speech recognizer performance by employing a richer pronunciation model than is customary. 6 6 Our approach differs from the approach described by Riley and Ljolje [17] in three important ways. Firstly, our underlying pronouncing lexicon is constructed directly from the observed pronunciations, without any human intervention, while their underlying lexicon is obtained from a hand-built text-to-speech system.
Reference: [18] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Finite growth models. </title> <type> Tech. Rep. </type> <institution> CS-TR-533-96, Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: Each iteration of the EM algorithm is guaranteed to either increase the probability of the training corpus or not change the model parameters. The correctness of our algorithm is shown in related work <ref> [18] </ref>. expectation-maximization (, C) 1. until convergence 2. forall z in E [ fl (z) := 0; ] 3. for i = 1 to n 4. expectation-step (x T i ; y V i ; ; fl); 5. maximization-step (,fl); The fl (z) variable is used to accumulate the expected number <p> Convergence. The expectation-maximization () algorithm given above is guaranteed to converge to a local maximum on a given corpus C, by a reduction to finite growth models <ref> [18] </ref>. Here we demonstrate that there may be multiple local maxima, and that only one of these need be a global maxima. Consider a transducer with alphabets A = fa; bg and B = fcg being trained on a corpus C consisting of exactly one string pair habb; cci.
Reference: [19] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Learning string edit distance. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference (San Francisco, </booktitle> <month> July 8-11 </month> <year> 1997), </year> <editor> D. Fisher, Ed., </editor> <publisher> Morgan Kaufmann. </publisher>
Reference: [20] <author> Sankoff, D., and Kruskal, J. B., Eds. </author> <title> Time warps, string edits, and macromolecules: the theory and practice of sequence comparison. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 21]. Many excellent reviews of the string edit distance literature are available <ref> [7, 9, 14, 20] </ref>. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11].
Reference: [21] <author> Wagner, R., and Fisher, M. </author> <title> The string to string correction problem. </title> <booktitle> JACM 21 (1974), </booktitle> <pages> 168-173. 24 </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming <ref> [12, 21] </ref>. Many excellent reviews of the string edit distance literature are available [7, 9, 14, 20]. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11].
References-found: 21

