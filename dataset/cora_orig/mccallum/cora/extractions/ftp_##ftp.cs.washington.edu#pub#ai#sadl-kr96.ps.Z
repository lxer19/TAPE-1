URL: ftp://ftp.cs.washington.edu/pub/ai/sadl-kr96.ps.Z
Refering-URL: http://www.cs.washington.edu/homes/weld/pubs.html
Root-URL: 
Email: fkgolden, weldg@cs.washington.edu  
Title: Representing Sensing Actions: The Middle Ground Revisited expressive, and planning becomes intractable. Within the classical
Author: Keith Golden Daniel Weld Mark Boddy, Bob Doorenbos, Oren Etzioni, Marc Friedman, Robert Goldman, Neal Lesh, Greg Linden, Mike Perkowitz, Rich Segal, Jonathan Shakes and Ellen Spertus 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Note: To appear in KR '96  too  Many thanks to  for helpful comments. This research was funded in part by Office of Naval Research Grant N00014-94-1-0060, by National Science Foundation Grant IRI-9303461, by ARPA Rome Labs grant F30602-95-1-0024, by a gift from Rockwell International Palo Alto Research, and by a Microsoft Graduate Fellowship  
Abstract: Among languages handling sensing actions and information goals, there is a similar spectrum of expressiveness. uwl, an extension of strips, can't express goals like "Rename the file paper.tex to kr.tex." Nor can it represent universally quantified goals or effects. At the other extreme are elegant languages [ 22, 21, 17 ] for which effective planners do not exist. In this paper, we combine elements of uwl and adl, to define sadl: a middle-ground representation for sensing actions. Underlying our language are two insights, missing from uwl: 1) Knowledge goals are inherently temporal. 2) Knowledge preconditions are unnecessary for an important class of domains (those obeying a Markov property). sadl is expressive enough to encode the rich domain theory of the Internet Soft-bot, including hundreds of UNIX and Internet operators; yet it supports tractable inference by planners such as xii [ 11, 10 ] . 
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> Fahiem Bacchus, Craig Boutilier, and Adam Grove. </author> <title> Rewarding behaviors. </title> <booktitle> In Proc. 14th Nat. Conf. on AI, </booktitle> <year> 1995. </year>
Reference-contexts: Partially-observable Markov Decision Processes [ 20, 2 ] provide an elegant representation of sensing actions and actions with uncertain outcomes in Markov domains. However, they don't lend themselves to efficient algorithms. With few exceptions, such as <ref> [ 1 ] </ref> , work in MDPs assumes that reward functions (goals) are Markov as well, so temporal goals like initially are inexpressible. A number of contingent planning systems have introduced novel representations of uncertainty and sensing actions.
Reference: [ 2 ] <author> A. R. Cassandra, L. P. Kaebling, and M. L. Littman. </author> <title> Algorithms for partially observable markov decision processes. </title> <type> Technical report 94-14, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1994. </year>
Reference-contexts: Partially-observable Markov Decision Processes <ref> [ 20, 2 ] </ref> provide an elegant representation of sensing actions and actions with uncertain outcomes in Markov domains. However, they don't lend themselves to efficient algorithms.
Reference: [ 3 ] <author> E. Davis. </author> <title> Knowledge preconditions for plans. </title> <type> Technical Report 637, </type> <institution> NYU Computer Science Department, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 INTRODUCTION One of the stumbling blocks to past research in plan ning with incomplete information has been inadequate or imprecisely defined languages for representing information goals and sensing actions. Many researchers have devised formalisms for reasoning about knowledge and action <ref> [ 21, 22, 23, 5, 3, 32, 17 ] </ref> , but those languages are too expressive to be used in practical planning algorithms. uwl [ 9 ] offered a more tractable representation (based on strips) that was tailored to current planning technology, but as Levesque [ 17 ] observes, the semantics <p> Morgen-stern [ 22 ] generalized Moore's results to express partial knowledge that agents have about the knowledge of other agents (e.g. "John knows what Bill said"), using a substantially more expressive logic, which is syntactic rather than modal. Davis <ref> [ 3 ] </ref> extended Moore's theory to handle contingent plans, though, like Moore, he doesn't discuss actions with indeterminate effects. Levesque [ 17 ] offers an elegant theory of when a plan, with conditionals and loops, achieves a satisfaction goal in the presence of incomplete information.
Reference: [ 4 ] <author> D. Draper, S. Hanks, and D. Weld. </author> <title> A probabilistic model of action for least-commitment planning with information gathering. </title> <booktitle> In Proc. 10th Conf. on Uncertainty in Artifical Intelligence, </booktitle> <year> 1994. </year>
Reference-contexts: A number of contingent planning systems have introduced novel representations of uncertainty and sensing actions. Warplan-C [ 34 ] tags actions as conditional, meaning they have two possible outcomes: P or :P . C-buridan <ref> [ 16, 4 ] </ref> uses a probabilistic action language that can represent conditional, observational effects, including noisy sensors, and effects that cause information loss. Unlike sadl, the C-buridan language is propositional, and makes no distinction between knowledge goals and goals of satisfaction.
Reference: [ 5 ] <author> M. Drummond. </author> <title> Situated control rules. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Representation and Reasoning, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: 1 INTRODUCTION One of the stumbling blocks to past research in plan ning with incomplete information has been inadequate or imprecisely defined languages for representing information goals and sensing actions. Many researchers have devised formalisms for reasoning about knowledge and action <ref> [ 21, 22, 23, 5, 3, 32, 17 ] </ref> , but those languages are too expressive to be used in practical planning algorithms. uwl [ 9 ] offered a more tractable representation (based on strips) that was tailored to current planning technology, but as Levesque [ 17 ] observes, the semantics
Reference: [ 6 ] <author> O. Etzioni, K. Golden, and D. Weld. </author> <title> Sound and efficient closed-world reasoning for planning. </title> <journal> Artificial Intelligence, </journal> <note> 1997. (To appear). </note>
Reference-contexts: Such inference would be costly if it were done using first-order theorem-proving in the situation calculus. We have devised efficient algorithms for doing this reasoning, which we describe in <ref> [ 6, 8 ] </ref> . The translation of 8 effects into the situation calculus is straightforward (Other logical operators follow the same form): EFF (8~x:E; a; s) = 8~x:EFF (E; a; s) (12) This definition of 8 effects may seem anticlimactic.
Reference: [ 7 ] <author> O. Etzioni and D. Weld. </author> <title> A softbot-based interface to the Internet. </title> <journal> CACM, </journal> <volume> 37(7) </volume> <pages> 72-76, </pages> <year> 1994. </year>
Reference-contexts: Since sadl supports universally quantified information goals and universally quantified, conditional, observational effects, it is expressive enough to represent hundreds of unix and Internet commands. 1 sadl (pronounced "Saddle") stands for "Sensory Ac tion Description Language." Indeed, four years of painful experience writing and debugging the Internet Softbot <ref> [ 7 ] </ref> knowledge base forced us to uncover and remedy some subtle confusions about information goals: * In a dynamic world, knowledge goals are inherently temporal | If proposition P is true at one time point and false in another, which time point do we mean when we ask about <p> Indeed, the Internet Softbot <ref> [ 7 ] </ref> follows an analogous strategy when directed to find a particular user, file or a web page, whose location is unknown.
Reference: [ 8 ] <author> Oren Etzioni, Keith Golden, and Dan Weld. </author> <title> Tractable closed-world reasoning with updates. </title> <booktitle> In Proc. 4th Int. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 178-189, </pages> <year> 1994. </year>
Reference-contexts: Such inference would be costly if it were done using first-order theorem-proving in the situation calculus. We have devised efficient algorithms for doing this reasoning, which we describe in <ref> [ 6, 8 ] </ref> . The translation of 8 effects into the situation calculus is straightforward (Other logical operators follow the same form): EFF (8~x:E; a; s) = 8~x:EFF (E; a; s) (12) This definition of 8 effects may seem anticlimactic.
Reference: [ 9 ] <author> Oren Etzioni, Steve Hanks, Daniel Weld, Denise Draper, Neal Lesh, and Mike Williamson. </author> <title> An approach to planning with incomplete information. </title> <booktitle> In Proc. 3rd Int. Conf. on Principles of Knowl--edge Representation and Reasoning, </booktitle> <pages> pages 115-125, </pages> <year> 1992. </year>
Reference-contexts: Many researchers have devised formalisms for reasoning about knowledge and action [ 21, 22, 23, 5, 3, 32, 17 ] , but those languages are too expressive to be used in practical planning algorithms. uwl <ref> [ 9 ] </ref> offered a more tractable representation (based on strips) that was tailored to current planning technology, but as Levesque [ 17 ] observes, the semantics of uwl are unclear | the definitions were made relative to a specific planning algo rithm. <p> For example, satisfy (in.dir (myfile, tex), tv) means "Find out whether or not myfile is in tex." Although the semantics of uwl was defined procedu-rally <ref> [ 9 ] </ref> , we provide sadl's semantics in terms of the situation calculus. The situation calculus [ 19 ] is a first order logic used to capture changes to the world that come about by the execution of actions. <p> Thus, the definition of hands-off requires that P not change value during execution of the plan. Etzioni et al <ref> [ 9 ] </ref> noted that together, satisfy+hands-off can be used to indicate a "look but don't touch" goal: the agent may sense the fluent's value, but is forbidden to change it. While hands-off goals are clearly useful, we argue that they are an overly restrictive way of specifying knowledge goals. <p> In particular, they outlaw changing the value of a fluent after it has been sensed. 2.2 KNOWLEDGE GOALS ARE INHERENTLY TEMPORAL Before explaining the sadl approach to knowledge goals, we discuss the uwl find-out annotation. find-out is problematic because the original definition was in terms of a particular planning algorithm <ref> [ 9 ] </ref> . The motivation for find-out was the existence of goals for which hands-off is too restrictive, but satisfy alone is too permissive. For example, given the goal "Tell me what files are in directory tex," executing rm tex/* and reporting "None" would clearly be inappropriate. <p> Yet the knowledge that the directory is now empty is relevant to the information goal. Proponents of find-out argued that rm was unacceptable for the first goal, but acceptable in service of the con junction <ref> [ 9 ] </ref> . We contend that this definition is unclear and unacceptable; a plan that satisfies the conjunction A ^ B should also be a solution to A. While the examples used to justify the original find-out definition are evocative, their persuasive powers stem from ambiguity.
Reference: [ 10 ] <author> K. Golden, O. Etzioni, and D. Weld. </author> <title> Planning with execution and incomplete information. </title> <type> Technical Report 96-01-09, </type> <institution> University of Washington, Department of Computer Science and Engineering, </institution> <month> February </month> <year> 1996. </year> <note> Available via FTP from pub/ai/ at ftp.cs.washington.edu. </note>
Reference-contexts: By the defi nition of fg 1 for satisfy goals, that follows iff S 0 j= KNOW (current.shell (csh) ^ KNOW (protection tex, readable), which is true by assumption. 6 TRACTABILITY sadl is implemented by xii <ref> [ 11, 10 ] </ref> , a partial-order planner whose performance is comparable to the ucpop/snlp family of classical planners. We analyze its performance in terms of the refinement paradigm described in [ 15 ] | xii has three refinement operations: goal establishment, conflict resolution and action execution. <p> knowledge preconditions from actions, but using secondary preconditions to clearly indicate when subgoaling to acquire knowledge could be useful. sadl is expressive enough to represent real-world domains, such as UNIX and the World Wide Web, yet restricted enough to be used efficiently by modern planning al gorithms, such as xii <ref> [ 10 ] </ref> .
Reference: [ 11 ] <author> Keith Golden, Oren Etzioni, and Dan Weld. </author> <title> Omnipotence without omniscience: Sensor management in planning. </title> <booktitle> In Proc. 12th Nat. Conf. on AI, </booktitle> <pages> pages 1048-1054, </pages> <year> 1994. </year>
Reference-contexts: By the defi nition of fg 1 for satisfy goals, that follows iff S 0 j= KNOW (current.shell (csh) ^ KNOW (protection tex, readable), which is true by assumption. 6 TRACTABILITY sadl is implemented by xii <ref> [ 11, 10 ] </ref> , a partial-order planner whose performance is comparable to the ucpop/snlp family of classical planners. We analyze its performance in terms of the refinement paradigm described in [ 15 ] | xii has three refinement operations: goal establishment, conflict resolution and action execution.
Reference: [ 12 ] <author> Robert P. Goldman and Mark S. Boddy. </author> <title> Expressive Planning And Explicit Knowledge. </title> <booktitle> In Proc. 3rd Intl. Conf. on AI Planning Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: However, Levesque doesn't discuss knowledge goals, and his sensory actions can return only T or F, and can't change the state of the world. Goldman and Boddy <ref> [ 12 ] </ref> present a clean language for contingent plans with context-dependent effects and nonde-terminism. However, like Levesque, they don't allow variables in sensing actions: possible outcomes are represented as a disjuction. Shoham [ 33 ] presents a language, with explicit time, for representing beliefs and communication among multiple agents. <p> Unlike sadl, the C-buridan language is propositional, and makes no distinction between knowledge goals and goals of satisfaction. C-buridan and Cassandra [ 29 ] (and wcpl <ref> [ 12 ] </ref> ) can represent and reason with uncertain outcomes of actions as disjunctions, allowing them to deal with correlations between multiple unknown variables (e.g. either it is raining and Fido is wet, or it is sunny and Fido is dry).
Reference: [ 13 ] <author> Peter Haddawy and Steve Hanks. </author> <title> Utility Models for Goal-Directed Decision-Theoretic Planners. </title> <type> Technical Report 93-06-04, </type> <institution> Univ. of Wash-ington, Dept. of Computer Science and Engineering, </institution> <month> September </month> <year> 1993. </year> <note> Submitted to Artificial Intelligence. Available via FTP from pub/ai/ at ftp.cs.washington.edu. </note>
Reference-contexts: This is a general problem with sat-isficing plans: anything goes as long as the goal is achieved. Specifying all the undesired outcomes with every goal would be tedious and error-prone. A better solution is to separate the criteria of goal satisfaction from background preferences, as is done in <ref> [ 37, 13, 35 ] </ref> .
Reference: [ 14 ] <author> F. Ingrand, R. Chatila, R. Alami, and F Robert. </author> <title> PRS: A high level supervision and control language for autonomous mobile robots. </title> <booktitle> In Proceedings of the 1996 IEEE International Conference On Robotics and Automation, </booktitle> <year> 1996. </year>
Reference-contexts: Shoham [ 33 ] presents a language, with explicit time, for representing beliefs and communication among multiple agents. Agents can request other agents to perform actions, which can include (nested) communicative actions, but not arbitrary goals. A discrete temporal logic, without 8, is used to represent beliefs. prs <ref> [ 14 ] </ref> is a procedural language that can represent a similar class of goals as sadl, but lacks temporal goals such as initially. prs has annotation achieve corresponding to sadl satisfy, preserve corresponding to hands-off, and test corresponding to satisfy+hands-off, as well as several procedural constructs that have no corresponding
Reference: [ 15 ] <author> S. Kambhampati, C. Knoblock, and Q. Yang. </author> <title> Planning as refinement search: A unified framework for evaluating design tradeoffs in partial order planning. </title> <journal> Artificial Intelligence, </journal> <volume> 76 </volume> <pages> 167-238, </pages> <year> 1995. </year>
Reference-contexts: We analyze its performance in terms of the refinement paradigm described in <ref> [ 15 ] </ref> | xii has three refinement operations: goal establishment, conflict resolution and action execution. Goal establishment involves possibly adding an action to the plan, and adding an interval protection constraint (IPC) to prevent the goal from being clobbered. In sadl, there are three possible intervals to consider.
Reference: [ 16 ] <author> N. Kushmerick, S. Hanks, and D. Weld. </author> <title> An Algorithm for Probabilistic Planning. </title> <journal> Artificial Intelligence, </journal> <volume> 76 </volume> <pages> 239-286, </pages> <year> 1995. </year>
Reference-contexts: A number of contingent planning systems have introduced novel representations of uncertainty and sensing actions. Warplan-C [ 34 ] tags actions as conditional, meaning they have two possible outcomes: P or :P . C-buridan <ref> [ 16, 4 ] </ref> uses a probabilistic action language that can represent conditional, observational effects, including noisy sensors, and effects that cause information loss. Unlike sadl, the C-buridan language is propositional, and makes no distinction between knowledge goals and goals of satisfaction.
Reference: [ 17 ] <author> Hector Levesque. </author> <title> What is planning in the presence of sensing? In Proc. </title> <booktitle> 14th Nat. Conf. on AI, </booktitle> <year> 1996. </year>
Reference-contexts: 1 INTRODUCTION One of the stumbling blocks to past research in plan ning with incomplete information has been inadequate or imprecisely defined languages for representing information goals and sensing actions. Many researchers have devised formalisms for reasoning about knowledge and action <ref> [ 21, 22, 23, 5, 3, 32, 17 ] </ref> , but those languages are too expressive to be used in practical planning algorithms. uwl [ 9 ] offered a more tractable representation (based on strips) that was tailored to current planning technology, but as Levesque [ 17 ] observes, the semantics <p> reasoning about knowledge and action [ 21, 22, 23, 5, 3, 32, 17 ] , but those languages are too expressive to be used in practical planning algorithms. uwl [ 9 ] offered a more tractable representation (based on strips) that was tailored to current planning technology, but as Levesque <ref> [ 17 ] </ref> observes, the semantics of uwl are unclear | the definitions were made relative to a specific planning algo rithm. In our efforts to define a semantics for uwl, we determined that uwl confused information goals with maintenance goals, and conflated knowledge goals with knowledge preconditions. <p> Davis [ 3 ] extended Moore's theory to handle contingent plans, though, like Moore, he doesn't discuss actions with indeterminate effects. Levesque <ref> [ 17 ] </ref> offers an elegant theory of when a plan, with conditionals and loops, achieves a satisfaction goal in the presence of incomplete information. However, Levesque doesn't discuss knowledge goals, and his sensory actions can return only T or F, and can't change the state of the world.
Reference: [ 18 ] <author> J. McCarthy. </author> <title> Circumscription a form of non-monotonic reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 13(1,2):27-39, </volume> <month> April </month> <year> 1980. </year>
Reference-contexts: But this example makes it clear that the proposed preconditions of Netscape are simply under-specified. They should be "The directory contains a file named netscape.bookmarks, which is a valid bookmarks file, and : : : " This is just the qualification problem <ref> [ 18 ] </ref> in disguise.
Reference: [ 19 ] <author> J. McCarthy and P. J. Hayes. </author> <title> Some philosophical problems from the standpoint of artificial intelligence. </title> <booktitle> In Machine Intelligence 4, </booktitle> <pages> pages 463-502. </pages> <publisher> Edinburgh University Press, </publisher> <year> 1969. </year>
Reference-contexts: For example, satisfy (in.dir (myfile, tex), tv) means "Find out whether or not myfile is in tex." Although the semantics of uwl was defined procedu-rally [ 9 ] , we provide sadl's semantics in terms of the situation calculus. The situation calculus <ref> [ 19 ] </ref> is a first order logic used to capture changes to the world that come about by the execution of actions. A fluent is a proposition whose truth value changes over time. <p> Conflict resolution and action execution also take O (n 2 ) time. In contrast, note that goal establishment and conflict resolution are undecidable in the situation calculus. 7 RELATED WORK McCarthy and Hayes <ref> [ 19 ] </ref> first argued that an agent needs to reason about its ability to perform an action. Moore [ 21 ] devised a theory of knowledge and action, based on a variant of the situation calculus with possible-worlds semantics.
Reference: [ 20 ] <author> G. E. Monahan. </author> <title> A survey of partially observable markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: Partially-observable Markov Decision Processes <ref> [ 20, 2 ] </ref> provide an elegant representation of sensing actions and actions with uncertain outcomes in Markov domains. However, they don't lend themselves to efficient algorithms.
Reference: [ 21 ] <author> R. Moore. </author> <title> A Formal Theory of Knowledge and Action. </title> <editor> In J. Hobbs and R. Moore, editors, </editor> <title> Formal Theories of the Commonsense World. </title> <publisher> Ablex, </publisher> <address> Norwood, NJ, </address> <year> 1985. </year>
Reference-contexts: 1 INTRODUCTION One of the stumbling blocks to past research in plan ning with incomplete information has been inadequate or imprecisely defined languages for representing information goals and sensing actions. Many researchers have devised formalisms for reasoning about knowledge and action <ref> [ 21, 22, 23, 5, 3, 32, 17 ] </ref> , but those languages are too expressive to be used in practical planning algorithms. uwl [ 9 ] offered a more tractable representation (based on strips) that was tailored to current planning technology, but as Levesque [ 17 ] observes, the semantics <p> (P ) Q; s 0 ; fag n 1 ) = GOAL (P; s 0 ; fag n 1 ) ) GOAL (Q; s 0 ; fag n 1 ) Logical operators such as ^, _, and 9 follow the same form as above. 2.4 KNOWLEDGE PRECONDITIONS CONSIDERED HARMFUL Moore <ref> [ 21 ] </ref> identified two kinds of knowledge preconditions an agent must satisfy in order to execute an action in support of some proposition P : First, the agent must know a rigid designator (i.e., an unambiguous, executable description) of the action. <p> An action's effects will only be realized if the action is executed when its preconditions are satisfied. Furthermore, the agent always knows when it executes an action, and it knows the effects of that action. Following Moore <ref> [ 21 ] </ref> : 8s:GOAL ( a ; s; fg) ) 8s 00 :[K (s 00 ; DO (a; s)) , 9s 0 :K (s 0 ; s) ^ s 00 = DO (a; s 0 ) ^ EFF (" a ; a; s)] (9) The fact that the agent knows <p> In contrast, note that goal establishment and conflict resolution are undecidable in the situation calculus. 7 RELATED WORK McCarthy and Hayes [ 19 ] first argued that an agent needs to reason about its ability to perform an action. Moore <ref> [ 21 ] </ref> devised a theory of knowledge and action, based on a variant of the situation calculus with possible-worlds semantics. He provided an analysis of knowledge preconditions, which we discussed earlier, and information-providing effects.
Reference: [ 22 ] <author> Leora Morgenstern. </author> <title> Knowledge preconditions for actions and plans. </title> <booktitle> In Proceedings of IJCAI-87, </booktitle> <pages> pages 867-874, </pages> <year> 1987. </year>
Reference-contexts: 1 INTRODUCTION One of the stumbling blocks to past research in plan ning with incomplete information has been inadequate or imprecisely defined languages for representing information goals and sensing actions. Many researchers have devised formalisms for reasoning about knowledge and action <ref> [ 21, 22, 23, 5, 3, 32, 17 ] </ref> , but those languages are too expressive to be used in practical planning algorithms. uwl [ 9 ] offered a more tractable representation (based on strips) that was tailored to current planning technology, but as Levesque [ 17 ] observes, the semantics <p> Second, the agent must know that executing the action will in fact achieve P . Subsequent work, e.g. <ref> [ 22 ] </ref> , generalized this framework to handle scenarios where multiple agents reasoned about each other's knowledge. In the interest of tractability, we take a much narrower view, assuming away Moore's first type of knowledge precondition and refuting the need for his second type. <p> Moore [ 21 ] devised a theory of knowledge and action, based on a variant of the situation calculus with possible-worlds semantics. He provided an analysis of knowledge preconditions, which we discussed earlier, and information-providing effects. Morgen-stern <ref> [ 22 ] </ref> generalized Moore's results to express partial knowledge that agents have about the knowledge of other agents (e.g. "John knows what Bill said"), using a substantially more expressive logic, which is syntactic rather than modal.
Reference: [ 23 ] <author> Leora Morgenstern. </author> <title> Foundations of a Logic of Knowledge, Action, and Communication. </title> <type> PhD thesis, </type> <address> New York University, </address> <year> 1988. </year>
Reference-contexts: 1 INTRODUCTION One of the stumbling blocks to past research in plan ning with incomplete information has been inadequate or imprecisely defined languages for representing information goals and sensing actions. Many researchers have devised formalisms for reasoning about knowledge and action <ref> [ 21, 22, 23, 5, 3, 32, 17 ] </ref> , but those languages are too expressive to be used in practical planning algorithms. uwl [ 9 ] offered a more tractable representation (based on strips) that was tailored to current planning technology, but as Levesque [ 17 ] observes, the semantics
Reference: [ 24 ] <author> E. Pednault. </author> <title> Toward a Mathematical Theory of Plan Synthesis. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: Information Expressiveness ! Complete strips adl Situation Calculus Incomplete uwl SADL Moore et al In this paper, we define a new action representation language, sadl, 1 that combines ideas from uwl with those from Pednault's adl <ref> [ 26, 24 ] </ref> . Just as adl marked the "middle ground" on the tractability spectrum between strips and the situation calculus, sadl offers an advantageous combination of expressiveness and efficiency. <p> We don't list observe (P; T) above, since it is subsumed by the conjunction observe (P; v) ^ v = T (similarly for F). Given these definitions, we can state the conditions under which an action changes or preserves a fluent's truth value. Following Pednault <ref> [ 24 ] </ref> , we define a ' to be the conditions under which an executable action a will establish ', and a ' to be the conditions under which a will preserve '.
Reference: [ 25 ] <author> E. Pednault. </author> <title> Synthesizing plans that contain actions with context-dependent effects. </title> <journal> Computational Intelligence, </journal> <volume> 4(4) </volume> <pages> 356-372, </pages> <year> 1988. </year>
Reference-contexts: Unannotated preconditions merely need to be satisfied in the final state, and it isn't necessary that they be known true. a 1 (') = a ' ) (35) Logical operators are simply regressed back to the initial state, since their interpretation is the same across all situations, as detailed in <ref> [ 25 ] </ref> . With these definitions, we can show that regression is correct | that is, if the conditions returned by a 1 () are true, and fag n 1 is successfully executed, then will indeed be true.
Reference: [ 26 ] <author> E. Pednault. </author> <title> ADL: Exploring the middle ground between STRIPS and the situation calculus. </title> <booktitle> In Proc. 1st Int. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 324-332, </pages> <year> 1989. </year>
Reference-contexts: Information Expressiveness ! Complete strips adl Situation Calculus Incomplete uwl SADL Moore et al In this paper, we define a new action representation language, sadl, 1 that combines ideas from uwl with those from Pednault's adl <ref> [ 26, 24 ] </ref> . Just as adl marked the "middle ground" on the tractability spectrum between strips and the situation calculus, sadl offers an advantageous combination of expressiveness and efficiency. <p> however, stems from the way in which when introduces secondary preconditions; these are required for 8 effects, where the when clause restricts the universe of discourse to a finite set, and indicates precisely the range of the quantifier. 3.2 CONDITIONAL EFFECTS A secondary precondition, i.e. one associated with an effect <ref> [ 26 ] </ref> , defines the conditions under which action execution will achieve that effect. Unlike primary preconditions, secondary preconditions need not be true for the action to be executed.
Reference: [ 27 ] <author> M. Peot and D. Smith. </author> <title> Conditional Nonlinear Planning. </title> <booktitle> In Proc. 1st Intl. Conf. on AI Planning Systems, </booktitle> <pages> pages 189-197, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: By using the U truth value, sadl gives up the ability to represent these correlations (i.e. as far as the agent knows, it is raining and fido is dry). However, reasoning with U truth values is more efficient than the possible-worlds representation used to handle disjunction. cnlp <ref> [ 27 ] </ref> , like sadl, uses a three-valued logic to represent uncertainty.
Reference: [ 28 ] <author> Martha Pollack. </author> <title> The uses of plans. </title> <journal> Artificial Intelligence, </journal> <volume> 57(1), </volume> <year> 1992. </year>
Reference-contexts: Such background preferences could be expressed in terms of a utility function over world states [ 30 ] , a measure of plan quality <ref> [ 28, 36 ] </ref> , or an explicit notion of harm [ 35 ] .
Reference: [ 29 ] <author> L. Pryor and G. Collins. </author> <title> Planning for contingencies: A decision-based approach. </title> <journal> Journal of Artificial Intelligence Research, </journal> <year> 1996. </year>
Reference-contexts: C-buridan [ 16, 4 ] uses a probabilistic action language that can represent conditional, observational effects, including noisy sensors, and effects that cause information loss. Unlike sadl, the C-buridan language is propositional, and makes no distinction between knowledge goals and goals of satisfaction. C-buridan and Cassandra <ref> [ 29 ] </ref> (and wcpl [ 12 ] ) can represent and reason with uncertain outcomes of actions as disjunctions, allowing them to deal with correlations between multiple unknown variables (e.g. either it is raining and Fido is wet, or it is sunny and Fido is dry).
Reference: [ 30 ] <author> Howard Raiffa. </author> <title> Decision Analysis: Introductory Lectures on Choices Under Uncertainty. </title> <publisher> Addison-Wesley, </publisher> <year> 1968. </year>
Reference-contexts: If the desire is merely that the agent should avoid moving the files unnecessarily, then we want the original solution, with some background preference to minimize unnecessary changes. Such background preferences could be expressed in terms of a utility function over world states <ref> [ 30 ] </ref> , a measure of plan quality [ 28, 36 ] , or an explicit notion of harm [ 35 ] .
Reference: [ 31 ] <author> R. Reiter. </author> <title> The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression. </title> <editor> In Vladimir Lifschitz, editor, </editor> <booktitle> Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy, </booktitle> <pages> pages 359-380. </pages> <publisher> Academic Press, </publisher> <year> 1991. </year>
Reference-contexts: To fully specify the sadl semantics, it is necessary to express the strips assumption in terms of the situation calculus. We use the formulation introduced in <ref> [ 31 ] </ref> , and augmented in [ 32 ] to account for sensing actions.
Reference: [ 32 ] <author> R. Scherl and H. Levesque. </author> <title> The frame problem and knowledge producing actions. </title> <booktitle> In Proc. 11th Nat. Conf. on AI, </booktitle> <pages> pages 689-695, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 INTRODUCTION One of the stumbling blocks to past research in plan ning with incomplete information has been inadequate or imprecisely defined languages for representing information goals and sensing actions. Many researchers have devised formalisms for reasoning about knowledge and action <ref> [ 21, 22, 23, 5, 3, 32, 17 ] </ref> , but those languages are too expressive to be used in practical planning algorithms. uwl [ 9 ] offered a more tractable representation (based on strips) that was tailored to current planning technology, but as Levesque [ 17 ] observes, the semantics <p> We use s n as a shorthand for DO (fag n 1 ; s 0 ). Our formulation of sadl is based on Scherl and Levesque's <ref> [ 32 ] </ref> solution to the frame problem for knowledge-producing actions. We adopt their com pleteness assumptions, and their formulation of incomplete knowledge, and thus their results (i.e. the persistence of knowledge and of ignorance) hold for us as well. <p> To fully specify the sadl semantics, it is necessary to express the strips assumption in terms of the situation calculus. We use the formulation introduced in [ 31 ] , and augmented in <ref> [ 32 ] </ref> to account for sensing actions. This strategy consists of providing a formula for each fluent, called a successor state axiom, that specifies the value of the fluent in terms of 1) the action executed, and 2) the conditions that held before the action was executed. <p> In other worlds, myfile is not writable and won't be compressed. The result is that it becomes unknown whether myfile is compressed. Similarly, if P was known previously and not changed, then by the successor state axioms for P and K, P will continue to be known. <ref> [ 32 ] </ref> . The above formula correctly describes how K changes, but it is a little unwieldy if what we want to know about is KNOW ('). Intuitively, KNOW (') becomes true if ' is known to become true, or ' is observed.
Reference: [ 33 ] <author> Y. Shoham. </author> <title> Agent-oriented programming. </title> <journal> Artificial Intelligence, </journal> <volume> 60(1) </volume> <pages> 51-92, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Goldman and Boddy [ 12 ] present a clean language for contingent plans with context-dependent effects and nonde-terminism. However, like Levesque, they don't allow variables in sensing actions: possible outcomes are represented as a disjuction. Shoham <ref> [ 33 ] </ref> presents a language, with explicit time, for representing beliefs and communication among multiple agents. Agents can request other agents to perform actions, which can include (nested) communicative actions, but not arbitrary goals.
Reference: [ 34 ] <author> D. Warren. </author> <title> Generating Conditional Plans and Programs. </title> <booktitle> In Proceedings of AISB Summer Conference, </booktitle> <pages> pages 344-354, </pages> <institution> University of Edinburgh, </institution> <year> 1976. </year>
Reference-contexts: With few exceptions, such as [ 1 ] , work in MDPs assumes that reward functions (goals) are Markov as well, so temporal goals like initially are inexpressible. A number of contingent planning systems have introduced novel representations of uncertainty and sensing actions. Warplan-C <ref> [ 34 ] </ref> tags actions as conditional, meaning they have two possible outcomes: P or :P . C-buridan [ 16, 4 ] uses a probabilistic action language that can represent conditional, observational effects, including noisy sensors, and effects that cause information loss.
Reference: [ 35 ] <author> Dan Weld and Oren Etzioni. </author> <title> The first law of robotics (a call to arms). </title> <booktitle> In Proc. 12th Nat. Conf. on AI, </booktitle> <pages> pages 1042-1047, </pages> <year> 1994. </year>
Reference-contexts: By combining initially with satisfy we can express "tidiness" goals (modify P at will, but restore its initial value by plan's end) <ref> [ 35 ] </ref> . Furthermore, we can express goals such as "Find the the file currently named paper.tex, and rename it to kr.tex," which are impossible to express in uwl. <p> Such background preferences could be expressed in terms of a utility function over world states [ 30 ] , a measure of plan quality [ 28, 36 ] , or an explicit notion of harm <ref> [ 35 ] </ref> . Note that even if we decide to forbid moving the files from tex, there are still other actions, such as deleting all the files in important/papers, or sending threatening email to president@whitehouse.gov that haven't been excluded. <p> This is a general problem with sat-isficing plans: anything goes as long as the goal is achieved. Specifying all the undesired outcomes with every goal would be tedious and error-prone. A better solution is to separate the criteria of goal satisfaction from background preferences, as is done in <ref> [ 37, 13, 35 ] </ref> .
Reference: [ 36 ] <author> D. E. Wilkins. </author> <title> Practical Planning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Such background preferences could be expressed in terms of a utility function over world states [ 30 ] , a measure of plan quality <ref> [ 28, 36 ] </ref> , or an explicit notion of harm [ 35 ] .
Reference: [ 37 ] <author> M. Williamson and S. Hanks. </author> <title> Optimal planning with a goal-directed utility model. </title> <booktitle> In Proc. 2nd Intl. Conf. on AI Planning Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: This is a general problem with sat-isficing plans: anything goes as long as the goal is achieved. Specifying all the undesired outcomes with every goal would be tedious and error-prone. A better solution is to separate the criteria of goal satisfaction from background preferences, as is done in <ref> [ 37, 13, 35 ] </ref> .
References-found: 37

