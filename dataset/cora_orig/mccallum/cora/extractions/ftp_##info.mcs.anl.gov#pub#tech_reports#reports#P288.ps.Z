URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P288.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts92.htm
Root-URL: http://www.mcs.anl.gov
Title: ADIFOR: Automatic Differentiation in a Source Translator Environment  
Author: Christian Bischof, Alan Carle, Andreas Griewank, 
Keyword: Derivative, gradient, Jacobian, automatic differentiation, chain rule, ParaScope Parallel Programming Environment, source transformation and optimization.  
Affiliation: Argonne National Laboratory,  Rice University, George Corliss, Argonne National Laboratory, and  Argonne National Laboratory  
Abstract: The numerical methods employed in the solution of many scientific computing problems require the computation of derivatives of a function f : R n ! R m . ADIFOR (Automatic Differentiation In FORtran) is a source transformation tool that accepts Fortran 77 code for the computation of a function and writes portable Fortran 77 code for the computation of the derivatives. In contrast to previous approaches, ADI-FOR views automatic differentiation as a source transformation problem and employs the data analysis capabilities of the ParaScope Fortran programming environment. Experimental results show that ADIFOR can handle real-life codes and that ADIFOR-generated codes are competitive with divided-difference approximations of derivatives. In addition, studies suggest that the source-transformation approach to automatic dif-ferentation may improve the time required to compute derivatives by orders of magnitude. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi I. Sethi, and Jeffrey D. Ull-man. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: By applying constant folding and forward substitution, we eliminate multiplications by 1.0 and additions of 0.0, and we reduce the number of variables that must be allocated to hold derivative values <ref> [1] </ref>. In summary, ADIFOR proceeds as follows: 1. The user specifies the subroutine that corresponds to the "function" for which he wishes derivatives, as well as the variable names that correspond to dependent and independent variables. These names can be subroutine parameters or variables in common blocks.
Reference: [2] <author> W. Baur and V. Strassen. </author> <title> The complexity of partial derivatives. </title> <journal> Theoretical Computer Science, </journal> <volume> 22:317 - 330, </volume> <year> 1983. </year>
Reference-contexts: This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in [14, 16, 17]. Wolfe observed [23], and Baur and Strassen confirmed <ref> [2] </ref>, that if care is taken in handling quantities which are common to the (rational) function and its derivatives, then the cost of evaluating a gradient with n components is a small multiple of the cost of evaluating the underlying scalar function. <p> For this restricted case, the reverse mode code can be implemented entirely as inline code, thereby avoiding potentially recursive programming implied by the Baur and Strassen proof <ref> [2] </ref>. A simple example will illustrate the advantages of the hybrid mode. Consider the statement w = y=(z fl z fl z); where y and z depend on the independent variables. We have already computed ry and rz and now wish to compute rw.
Reference: [3] <author> Christian Bischof, Alan Carle, George Corliss, An-dreas Griewank, and Paul Hovland. </author> <title> Generating derivative codes from Fortran programs. </title> <type> Preprint MCS-P263-0991, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Ar-gonne, Ill. </institution> <note> 60439, 1991. Also appeared as Technical Report 91185, Center for Research in Parallel Computation, </note> <institution> Rice University, Houston, Tex. </institution> <month> 77251. </month>
Reference-contexts: ADIFOR (Automatic Differentiation In FORtran) <ref> [3] </ref> augments the original source code with additional statements that propagate values of derivative objects in addition to the values of the variables computed in the original code.
Reference: [4] <author> Christian Bischof and Paul Hovland. </author> <title> Using ADI-FOR to compute dense and sparse Jacobians. </title> <type> Technical Memorandum ANL/MCS-TM-158, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill. </institution> <month> 60439, October </month> <year> 1991. </year> <note> ADIFOR Working Note # 2. </note>
Reference-contexts: The resulting code generated by ADIFOR can be called by user programs in a flexible manner to be used in conjunction with standard software tools for optimization, solving nonlinear equations, or for stiff ordinary differential equations. A discussion of calling the ADIFOR-generated code from users' programs in included in <ref> [4] </ref>. The ease of use of ADIFOR follows from its basis in a sophisticated compilation environment. <p> The user then selects the variables (in either parameter lists or common blocks) that correspond to the independent and dependent variables. ADIFOR then determines which other variables throughout the program require derivative information. A detailed description of the use of ADIFOR-generated code appears in <ref> [4] </ref>. Intuitive Interface: An X-windows interface for AD-IFOR (called xadifor) makes it easy for the user to create the ASCII script file that ADIFOR reads. <p> This code implements the steady shock tracking method for the axisymmetric blunt body problem [21]. The Jacobian has a banded structure. The "normal" Jacobian has 190 columns, although the Jacobian compression techniques outlined in <ref> [4] </ref> requires only 28 columns. Table 1 summarizes the time required by the ADIFOR-generated derivative codes with respect to divided differences. These tests were run on a SPARCsta-tion 1, a SPARC 4/490, or an IBM RS6000/550. Different machines are cited because of the different sources of the codes being run.
Reference: [5] <author> Paul T. Boggs and Janet E. Rogers. </author> <title> Orthogonal distance regression. </title> <journal> Contemporary Mathematics, </journal> <volume> 112:183 - 193, </volume> <year> 1990. </year>
Reference-contexts: The "heart" problem was given to us by Janet Rogers, National Institute of Standards and Technology in Boulder, Colorado. The code submitted to ADIFOR computes elementary Jacobian matrices which are then assembled to a large sparse Jacobian matrix used in an orthogonal-distance regression fit <ref> [5] </ref>. The code named "adiabatic" is from Larry Biegler, Chemical Engineering Department, Carnegie-Mellon University, and implements adiabatic flow, a common module in chemical engineering [22]. The code named "reactor" was given to us by Hussein Khalil, Reactor Analysis and Safety Division, Argonne National Laboratory.
Reference: [6] <author> J. C. Butcher. </author> <title> Implicit Runge-Kutta processes. </title> <journal> Math. Comp., </journal> <volume> 18:50 - 64, </volume> <year> 1964. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function. Probably the best known are gradient methods for optimization [11], Newton's method for the solution of nonlinear systems [9, 11], and the numerical solution of stiff ordinary differential equations <ref> [6, 10] </ref>. The function f to be differentiated is usually represented in the form of a computer program, not in a closed form as a single expression.
Reference: [7] <author> D. Callahan, K. Cooper, R. T. Hood, Ken Kennedy, and Linda M. Torczon. </author> <title> ParaScope: a parallel programming environment. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1988. </year>
Reference-contexts: In contrast, the reverse mode code requires space for five scalar auxiliary adjoint objects and has only one vector assignment. 4 ADIFOR Design: Principles and Advantages ADIFOR has been developed within the context of the ParaScope Parallel Programming Environment <ref> [7] </ref>, which combines dependence analysis with interprocedu-ral analysis to support ambitious interprocedural code optimization and semi-automatic parallelization of Fortran programs.
Reference: [8] <author> Bruce D. Christianson. </author> <title> Automatic Hessians by reverse accumulation. </title> <type> Technical Report NOC TR228, </type> <institution> The Numerical Optimisation Center, Hat-field Polytechnic, Hatfield, U.K., </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and mul-tivariate higher-order derivatives <ref> [8, 15, 20] </ref>. This discussion is intended to demonstrate that the principles underlying automatic differentiation are not complicated: We just associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code.
Reference: [9] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Soft--ware for estimating sparse Jacobian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10:329 - 345, </volume> <year> 1984. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function. Probably the best known are gradient methods for optimization [11], Newton's method for the solution of nonlinear systems <ref> [9, 11] </ref>, and the numerical solution of stiff ordinary differential equations [6, 10]. The function f to be differentiated is usually represented in the form of a computer program, not in a closed form as a single expression.
Reference: [10] <author> G. Dahlquist. </author> <title> A special stability problem for linear multistep methods. </title> <journal> BIT, </journal> <volume> 3:27 - 43, </volume> <year> 1963. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function. Probably the best known are gradient methods for optimization [11], Newton's method for the solution of nonlinear systems [9, 11], and the numerical solution of stiff ordinary differential equations <ref> [6, 10] </ref>. The function f to be differentiated is usually represented in the form of a computer program, not in a closed form as a single expression.
Reference: [11] <author> John Dennis and R. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function. Probably the best known are gradient methods for optimization <ref> [11] </ref>, Newton's method for the solution of nonlinear systems [9, 11], and the numerical solution of stiff ordinary differential equations [6, 10]. The function f to be differentiated is usually represented in the form of a computer program, not in a closed form as a single expression. <p> 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function. Probably the best known are gradient methods for optimization [11], Newton's method for the solution of nonlinear systems <ref> [9, 11] </ref>, and the numerical solution of stiff ordinary differential equations [6, 10]. The function f to be differentiated is usually represented in the form of a computer program, not in a closed form as a single expression.
Reference: [12] <author> Lawrence C. W. Dixon. </author> <title> Automatic differentiation and parallel processing in optimisation. </title> <type> Technical Report No. 180, </type> <institution> The Numerical Optimisation Center, Hatfield Polytechnic, Hatfield, U.K., </institution> <year> 1987. </year>
Reference-contexts: Current tools (see [18]) achieve this by storing a record of every computation performed. An interpreter performs a backward pass on this "tape." The resulting overhead often dominates the complexity advantage of the reverse mode in an actual implementation (see <ref> [12, 13] </ref>). We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and mul-tivariate higher-order derivatives [8, 15, 20].
Reference: [13] <author> Lawrence C. W. Dixon. </author> <title> Use of automatic differentiation for calculating Hessians and Newton steps. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 114 - 125. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Current tools (see [18]) achieve this by storing a record of every computation performed. An interpreter performs a backward pass on this "tape." The resulting overhead often dominates the complexity advantage of the reverse mode in an actual implementation (see <ref> [12, 13] </ref>). We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and mul-tivariate higher-order derivatives [8, 15, 20].
Reference: [14] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <editor> In M. Iri and K. Tanabe, editors, </editor> <booktitle> Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83 - 108. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: We call x the independent variable and y the dependent variable. There are four approaches to computing derivatives (these issues are discussed in more detail in <ref> [14] </ref>): Hand-Coded: Computing derivatives by hand is difficult and error-prone, especially as the problem complexity increases. <p> Assume that rt contains the derivatives of t with respect to the independent variables x, rt = @ t @ t ! We can propagate these derivatives by using elementary differentiation arithmetic based on the chain rule <ref> [14, 20] </ref> for computing the derivatives of y (1) and y (2), as shown in Figure 3. <p> This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [14, 16, 17] </ref>.
Reference: [15] <author> Andreas Griewank. </author> <title> Automatic evaluation of first-and higher-derivative vectors. </title> <editor> In R. Seydel, F. W. Schneider, T. Kupper, and H. Troger, editors, </editor> <booktitle> Proceedings of the Conference at Wurzburg, </booktitle> <month> Aug. </month> <year> 1990, </year> <title> Bifurcation and Chaos: Analysis, Algorithms, </title> <journal> Applications, </journal> <volume> volume 97, </volume> <pages> pages 135 - 148. </pages> <publisher> Birkhauser Verlag, </publisher> <address> Basel, Switzerland, </address> <year> 1991. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and mul-tivariate higher-order derivatives <ref> [8, 15, 20] </ref>. This discussion is intended to demonstrate that the principles underlying automatic differentiation are not complicated: We just associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code.
Reference: [16] <author> Andreas Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. Optimization Methods and Software, </title> <note> to appear. Also appeared as Preprint MCS-P228-0491, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, 9700 S. Cass Ave., Argonne, Ill. </institution> <month> 60439, </month> <year> 1991. </year>
Reference-contexts: This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [14, 16, 17] </ref>.
Reference: [17] <author> Andreas Griewank, David Juedes, Jay Srinivasan, and Charles Tyner. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <journal> ACM Trans. Math. Software, </journal> <note> to appear. </note>
Reference-contexts: This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [14, 16, 17] </ref>.
Reference: [18] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 315 - 329. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Despite the advantages of the reverse mode from the viewpoint of complexity, the implementation for the general case is quite complicated. It requires the ability to access in reverse order the instructions performed for the computation of f and the values of their operands and results. Current tools (see <ref> [18] </ref>) achieve this by storing a record of every computation performed. An interpreter performs a backward pass on this "tape." The resulting overhead often dominates the complexity advantage of the reverse mode in an actual implementation (see [12, 13]). <p> As a result, a variety of implementations of automatic differentiation have been developed over the years (see <ref> [18] </ref> for a survey). 3 A Hybrid Approach For efficiency in ADIFOR, we have adopted a hybrid approach to computing derivatives that is generally based on the forward mode, but uses the reverse mode to compute the gradients of assignment statements containing complex expressions. <p> In addition, the code generated by automatic differentiation is easy to transport between different machines. ADIFOR takes those requirements into account. Its user interface is simple, and the ADIFOR-generated code is efficient and portable. In comparison with other implementations of automatic differentiation (see <ref> [18] </ref> for a survey), ADIFOR provides the following features: Portability: ADIFOR produces vanilla Fortran 77 code. ADIFOR-generated derivative code requires no run-time support and can easily be ported be tween different computing environments. Generality: ADIFOR supports almost all of Fortran 77, including nested subroutines, common blocks, and equivalences.
Reference: [19] <author> Jorge J. </author> <title> More. On the performance of algorithms for large-scale bound constrained problems. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Large-Scale Numerical Optimization, </booktitle> <pages> pages 32 - 45. </pages> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: This can easily be done if the loop body is a basic block. The potential of this technique is impressive. Hand-compiling reverse mode code for the loop bodies of the torsion problem, a problem in the MINPACK-2 test set collection <ref> [19] </ref>, we obtained the performance shown in Figure 5. This figure shows the ratio of gradient/function evaluation on a Solbourne SE/900 for the current ADIFOR version and 0 20 40 60 80 o o o o o ------ current ADIFOR -.-.-.
Reference: [20] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Assume that rt contains the derivatives of t with respect to the independent variables x, rt = @ t @ t ! We can propagate these derivatives by using elementary differentiation arithmetic based on the chain rule <ref> [14, 20] </ref> for computing the derivatives of y (1) and y (2), as shown in Figure 3. <p> We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and mul-tivariate higher-order derivatives <ref> [8, 15, 20] </ref>. This discussion is intended to demonstrate that the principles underlying automatic differentiation are not complicated: We just associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code.
Reference: [21] <author> G. R. Shubin, A. B. Stephens, H. M. Glaz, A. B. Wardlaw, and L. B. Hackerman. </author> <title> Steady shock tracking, Newton's method, and the supersonic blunt body problem. </title> <journal> SIAM J. on Sci. and Stat. Computing, </journal> <volume> 3(2):127 - 144, </volume> <month> June </month> <year> 1982. </year>
Reference-contexts: This code implements the steady shock tracking method for the axisymmetric blunt body problem <ref> [21] </ref>. The Jacobian has a banded structure. The "normal" Jacobian has 190 columns, although the Jacobian compression techniques outlined in [4] requires only 28 columns. Table 1 summarizes the time required by the ADIFOR-generated derivative codes with respect to divided differences.
Reference: [22] <author> J. M. Smith and H. C. Van Ness. </author> <title> Introduction to Chemical Engineering. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: The code submitted to ADIFOR computes elementary Jacobian matrices which are then assembled to a large sparse Jacobian matrix used in an orthogonal-distance regression fit [5]. The code named "adiabatic" is from Larry Biegler, Chemical Engineering Department, Carnegie-Mellon University, and implements adiabatic flow, a common module in chemical engineering <ref> [22] </ref>. The code named "reactor" was given to us by Hussein Khalil, Reactor Analysis and Safety Division, Argonne National Laboratory.
Reference: [23] <author> Philip Wolfe. </author> <title> Checking the calculation of gradients. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 6(4):337 - 343, </volume> <year> 1982. </year>
Reference-contexts: This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in [14, 16, 17]. Wolfe observed <ref> [23] </ref>, and Baur and Strassen confirmed [2], that if care is taken in handling quantities which are common to the (rational) function and its derivatives, then the cost of evaluating a gradient with n components is a small multiple of the cost of evaluating the underlying scalar function.
References-found: 23

