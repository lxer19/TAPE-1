URL: http://www.stat.washington.edu:80/tech.reports/tr317.ps
Refering-URL: http://www.stat.washington.edu:80/tech.reports/
Root-URL: 
Title: Principal Curve Clustering With Noise  
Author: Derek Stanford and Adrian E. Raftery 
Note: February 28, 1997  
Abstract: Technical Report 317 Department of Statistics University of Washington. 1 Derek Stanford is Graduate Research Assistant and Adrian E. Raftery is Professor of Statistics and Sociology, both at the Department of Statistics, University of Washington, Box 354322, Seattle, WA 98195-4322, USA. E-mail: stanford@stat.washington.edu and raftery@stat.washington.edu. Web: http://www.stat.washington.edu/raftery. This research was supported by ONR grants N00014-96-1-0192 and N00014-96-1-0330. The authors are grateful to Simon Byers, Gilles Celeux and Christian Posse for helpful discussions. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allard, D. </author> <year> (1995), </year> <title> "Nonparametric Maximum Likelihood Estimation of Features in Spatial Point Processes Using Voronoi Tesselation," </title> <type> Technical Report 293, </type> <institution> Department of Statistics, University of Washington. </institution> <note> (http://www.stat.washington.edu/tech.reports/tr293.ps). </note>
Reference-contexts: The first of these can be done by a human, or by various automatic methods such as nonparametric maximum likelihood using the Voronoi tesselation <ref> (Allard 1995) </ref>, or Kth nearest neighbor clutter removal (Byers and Raftery 1996).
Reference: <author> Banfield, J.D., and Raftery, A.E. </author> <year> (1992), </year> <title> "Ice Floe Identification in Satellite Images Using Mathematical Morphology and Clustering about Principal Curves," </title> <journal> Journal of the American Statistical Association, </journal> <volume> 87, </volume> <pages> 7-16. </pages>
Reference-contexts: Keep merging until the desired number of clusters is reached. Deciding when to stop clustering is more difficult for open curves than for closed curves. In the closed curve case, clustering stops when any merge would lead to an increase in P <ref> (Banfield and Raftery 1992) </ref>. For open curves, this method leads to an overfitting problem in which we end up with too many clusters. V fl can be made arbitrarily close to zero by increasing the number of clusters.
Reference: <author> Banfield, J.D., and Raftery, A.E. </author> <year> (1993), </year> <title> "Model-Based Gaussian and Non-Gaussian Clustering," </title> <journal> Biometrics, </journal> <volume> 49, </volume> <pages> 803-821. </pages>
Reference-contexts: The first step is to separate the features from the noise, which we did using 9th nearest neighbor denoising (Byers and Raftery, 1996); the resulting feature points are shown in Figure 3. We then used MCLUST <ref> (Banfield and Raftery 1993) </ref> to provide an initial clustering into 9 clusters; this is shown in Figure 4. We used 9 clusters for the initial clustering because this is the largest number of clusters for which MCLUST returns a clustering in which each cluster has at least 7 points. <p> In combination with the denoising method of Byers and Raftery (1996) and an initial clustering method such as MCLUST <ref> (Banfield and Raftery, 1993) </ref>, we have an approach which takes noisy spatial point process data and automatically extracts curvilinear features. The method appears to work well in simulated and real examples. One way that this kind of data may arise is from image processing.
Reference: <author> Byers, S.D., and Raftery, A.E. </author> <year> (1996), </year> <title> "Nearest Neighbor Clutter Removal for Estimating Features in Spatial Point Processes," </title> <type> Technical Report 305, </type> <institution> Department of Statistics, 19 University of Washington. </institution> <note> (http://www.stat.washington.edu/tech.reports/tr305.ps). </note>
Reference-contexts: The first of these can be done by a human, or by various automatic methods such as nonparametric maximum likelihood using the Voronoi tesselation (Allard 1995), or Kth nearest neighbor clutter removal <ref> (Byers and Raftery 1996) </ref>. <p> Note that some of the background noise points will fall inside the regions of feature points; these noise points will be indistinguishable from feature points. The first step is to separate the features from the noise, which we did using 9th nearest neighbor denoising <ref> (Byers and Raftery, 1996) </ref>; the resulting feature points are shown in Figure 3. We then used MCLUST (Banfield and Raftery 1993) to provide an initial clustering into 9 clusters; this is shown in Figure 4.
Reference: <author> Celeux, G., and Govaert, G. </author> <year> (1992), </year> <title> "A Classification EM Algorithm and Two Stochastic Versions," </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 14, </volume> <pages> 315-332. </pages>
Reference: <author> Dasgupta, A., and Raftery, A.E. </author> <year> (1997), </year> <title> "Detecting Features in Spatial Point Processes with Clutter via Model-Based Clustering," </title> <journal> Journal of the American Statistical Association, </journal> <note> to appear. (http://www.stat.washington.edu/tech.reports/tr295.ps, until publication). </note>
Reference: <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977), </year> <title> "Maximum Likelihood from Incomplete Data via the EM Algorithm (with Discussion)," </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39, </volume> <pages> 1-38. </pages>
Reference: <author> Green, P. </author> <year> (1990), </year> <title> "On the Use of the EM Algorithm for Penalized Likelihood Estimation," </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 52, </volume> <pages> 443-452. </pages>
Reference: <author> Hastie, T., and Stuetzle, W. </author> <year> (1989), </year> <title> "Principal Curves," </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84, </volume> <pages> 502-516. </pages>
Reference-contexts: Principal curves can be used in higher dimensions, or our approach could be modified to use a different model as the basis for features, such as principal surfaces <ref> (Hastie and Steutzle 1989) </ref> or adaptive principal surfaces (LeBlanc and Tibshirani 1994). Many variations of the EM algorithm are available. Green (1990) introduces the One Step Late (OLS) algorithm, a version of EM for use with penalized likelihoods.
Reference: <author> Hastie, T., and Tibshirani, R. </author> <year> (1990), </year> <title> Generalized Additive Models, </title> <address> New York: </address> <publisher> Chapman & Hall. </publisher>
Reference: <author> Kass, R.E. and Raftery, A.E. </author> <year> (1995), </year> <title> "Bayes Factors," </title> <journal> Journal of the American Statistical Association, </journal> <volume> 90, </volume> <pages> 773-795. </pages>
Reference-contexts: Each combination of number of features and degrees of freedom (i.e. smoothness of a feature) considered is viewed as specifying a possible model for the data, and the competing models are compared using Bayes factors <ref> (Kass and Raftery 1995) </ref>. We approximate the Bayes factor using the BIC (Schwarz 1978); the difference between the BIC values for two models is approximately equal to twice the log Bayes factor when unit information priors for the model parameters are used (Kass and Wasserman 1995). <p> We approximate the Bayes factor using the BIC (Schwarz 1978); the difference between the BIC values for two models is approximately equal to twice the log Bayes factor when unit information priors for the model parameters are used <ref> (Kass and Wasserman 1995) </ref>. These are priors that contain about the same amount of information as a single typical observation. This approach has been found to work well for mixture models by Roeder and Wasserman (1995). <p> The larger the BIC, the more the model is favored by the data. Conventionally, differences of 2-6 between BIC values for models represent positive evidence, differences of 6-10 correspond to strong evidence, while differences greater than 10 indicate very strong evidence <ref> (Kass and Raftery 1995) </ref>. 3 Initialization 3.1 Denoising and Initial Clustering The performance of the CEM-PCC algorithm can be sensitive to the starting value, so it is important to have a good starting value.
Reference: <author> Kass, R.E. and Wasserman, L. </author> <year> (1995), </year> <title> "A Reference Bayesian Test for Nested Hypotheses and its Relationship to the Schwarz Criterion," </title> <journal> Journal of the American Statistical Association, </journal> <volume> 90, </volume> <pages> 928-934. </pages>
Reference-contexts: Each combination of number of features and degrees of freedom (i.e. smoothness of a feature) considered is viewed as specifying a possible model for the data, and the competing models are compared using Bayes factors <ref> (Kass and Raftery 1995) </ref>. We approximate the Bayes factor using the BIC (Schwarz 1978); the difference between the BIC values for two models is approximately equal to twice the log Bayes factor when unit information priors for the model parameters are used (Kass and Wasserman 1995). <p> We approximate the Bayes factor using the BIC (Schwarz 1978); the difference between the BIC values for two models is approximately equal to twice the log Bayes factor when unit information priors for the model parameters are used <ref> (Kass and Wasserman 1995) </ref>. These are priors that contain about the same amount of information as a single typical observation. This approach has been found to work well for mixture models by Roeder and Wasserman (1995). <p> The larger the BIC, the more the model is favored by the data. Conventionally, differences of 2-6 between BIC values for models represent positive evidence, differences of 6-10 correspond to strong evidence, while differences greater than 10 indicate very strong evidence <ref> (Kass and Raftery 1995) </ref>. 3 Initialization 3.1 Denoising and Initial Clustering The performance of the CEM-PCC algorithm can be sensitive to the starting value, so it is important to have a good starting value.
Reference: <author> Latham, G., and Anderssen, R. </author> <year> (1994), </year> <title> "Assessing Quantification for the EM algorithm," Linear Algebra and its Applications, </title> <type> 210, </type> <month> Oct, </month> <pages> 89-122. </pages>
Reference: <author> Latham, G. </author> <year> (1995), </year> <title> "Existence of EMS Solutions and A-Priori Estimates," </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 16, 3, </volume> <pages> 943-953. </pages>
Reference: <author> LeBlanc, M., and Tibshirani, R. </author> <year> (1994), </year> <title> "Adaptive Principal Surfaces," </title> <journal> Journal of the American Statistical Association, </journal> <volume> 89, 425, </volume> <pages> 53-64. </pages>
Reference-contexts: Principal curves can be used in higher dimensions, or our approach could be modified to use a different model as the basis for features, such as principal surfaces (Hastie and Steutzle 1989) or adaptive principal surfaces <ref> (LeBlanc and Tibshirani 1994) </ref>. Many variations of the EM algorithm are available. Green (1990) introduces the One Step Late (OLS) algorithm, a version of EM for use with penalized likelihoods.
Reference: <author> Lu, W. </author> <year> (1995), </year> <title> "The Expectation-Smoothing Approach for Indirect Curve Estimation," </title> <type> Unpublished manuscript. </type>
Reference: <author> Nychka, D. </author> <year> (1990), </year> <title> "Some Properties of Adding a Smoothing Step to the EM Algorithm," </title> <journal> Statistics and Probability Letters, </journal> <volume> 9, </volume> <pages> 187-193. </pages>
Reference: <author> Prim, R. </author> <year> (1957), </year> <title> "Shortest Connection Networks and some Generalizations," </title> <journal> Bell System Technical Journal, </journal> <pages> 1389-1401. </pages> <note> 20 Roeder, </note> <author> K. and Wasserman, L. </author> <year> (1995), </year> <title> "Practical Bayesian Density Estimation Using Mix--tures of Normals," </title> <type> Technical Report no. 633, </type> <institution> Department of Statistics, Carnegie-Mellon University. </institution> <note> (http://lib.stat.cmu.edu/www/cmu-stats/tr/tr633/tr633.html) Schwarz, </note> <author> G. </author> <year> (1978), </year> <title> "Estimating the Dimension of a Model," </title> <journal> The Annals of Statistics, </journal> <volume> 6, </volume> <pages> 461-464. </pages>
Reference: <author> Silverman, B., Jones, M., Wilson, J., and Nychka, D. </author> <year> (1990), </year> <title> "A Smoothed EM Approach to Indirect Estimation Problems, with Particular Reference to Stereology and Emission Tomography (with Discussion)," </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 52, </volume> <pages> 271-324. </pages>
Reference: <author> Tibshirani, R. </author> <year> (1992), </year> <title> "Principal Curves Revisited," </title> <journal> Statistics and Computing, </journal> <volume> 2, </volume> <pages> 183-190. </pages>
Reference: <author> Tibshirani, R. and Hastie, T. </author> <year> (1987), </year> <title> "Local Likelihood Estimation," </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82, </volume> <pages> 559-568. </pages>
Reference: <author> Wold, S. </author> <year> (1974), </year> <title> "Spline Functions in Data Analysis," </title> <journal> Technometrics, </journal> <volume> 16, </volume> <pages> 1-11. </pages>
Reference-contexts: The amount of smoothing in each feature cluster is measured by the degrees of freedom (DF ) used in fitting the principal curve to that cluster. We use a cubic B-spline <ref> (Wold, 1974) </ref> in fitting the principal curves; specifically, we use the function principal.curve (obtained from Statlib) which calls the Splus function smooth.spline.
Reference: <author> Zahn, C. </author> <year> (1971), </year> <title> "Graph-Theoretical Methods for Detecting and Describing Gestalt Structures," </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-20,1, </volume> <pages> 68-86. 21 </pages>
References-found: 23

