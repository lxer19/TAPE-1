URL: ftp://ftp.cs.colorado.edu/users/alw/papers/CU-CS-825-97.ps
Refering-URL: http://www.cs.colorado.edu/~alw/RecentPubs.html
Root-URL: http://www.cs.colorado.edu
Email: fjcook,alwg@cs.colorado.edu votta@bell-labs.com  
Title: A Methodology for Cost-Effective Analysis of In-Place Software Processes  
Author: Jonathan E. Cook Lawrence G. Votta and Alexander L. Wolf 
Note: A version of this report to appear in IEEE Transactions on Software Engineering  
Date: January 1997  
Address: Boulder, CO 80309 USA Naperville, IL 60566 USA  
Affiliation: Department of Computer Science Systems and Software Research Laboratory University of Colorado Bell Laboratories  University of Colorado Department of Computer Science  
Pubnum: Technical Report CU-CS-825-97  
Abstract: Process studies and improvement efforts typically call for new instrumentation on the process in order to collect the data they have deemed necessary. This can be intrusive and expensive, and resistance to the extra workload often foils the study before it begins. The result is neither interesting new knowledge nor an improved process. In many organizations, however, extensive historical process and product data already exist. Can these existing data be used to empirically explore what process factors might be affecting the outcome of the process? If they can, organizations would have a cost-effective method for quantitatively, if not causally, understanding their process and its relationship to the product. We present a case study that analyzes an in-place industrial process and takes advantage of existing data sources. In doing this, we also illustrate and propose a methodology for such exploratory empirical studies. The case study makes use of several readily available repositories of process data in the industrial organization. Our results show that readily available data can be used to correlate both simple aggregate metrics and complex process metrics with defects in the product. Through the case study, we give evidence supporting the claim that exploratory empirical studies can provide significant results and benefits while being cost effective in their The work of J.E. Cook and A.L. Wolf was supported in part by the National Science Foundation under grant CCR-93-02739 and by the Air Force Material Command, Rome Laboratory, and the Defense Advanced Research Projects Agency under Contract Number F30602-94-C-0253. The content of the information does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. demands on the organization.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V.R. Basili and D.M. Weiss. </author> <title> A methodology for collecting valid software engineering data. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 10(6) </volume> <pages> 728-737, </pages> <year> 1984. </year>
Reference-contexts: Current methods for this analysis first define what is to be learned, and then instrument the process for data collection and met-rics calculation to support that learning. This approach is embodied in the Goal-Question-Metric paradigm and its descendants <ref> [1, 2, 4, 10] </ref>, and reflects an orientation toward experiments that are both controlled and confirmatory.
Reference: [2] <author> I. Bhandari, M. Halliday, E. Tarver, D. Brown, J. Chaar, and R. Chillarege. </author> <title> A case study of software process improvement during development. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1157-1170, December 1993. 
Reference-contexts: Current methods for this analysis first define what is to be learned, and then instrument the process for data collection and met-rics calculation to support that learning. This approach is embodied in the Goal-Question-Metric paradigm and its descendants <ref> [1, 2, 4, 10] </ref>, and reflects an orientation toward experiments that are both controlled and confirmatory.
Reference: [3] <author> M.G. Bradac, D.E. Perry, and L.G. Votta. </author> <title> Prototyping a process monitoring experiment. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 774-784, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: While certainly not true of all processes, in our subject process we were able to find enough data of high quality for valid statistical analysis. There are many kinds of data we could examine, but we chose to look at event data <ref> [3, 15] </ref> because they neatly characterize the dynamic behavior of the process in terms of the sequencing of its major activities. The event data come from several sources.
Reference: [4] <author> L.J. Chmura, A.F. Norcio, and T.J. Wicinski. </author> <title> Evaluating software design process by analyzing change data over time. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(7) </volume> <pages> 729-739, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Current methods for this analysis first define what is to be learned, and then instrument the process for data collection and met-rics calculation to support that learning. This approach is embodied in the Goal-Question-Metric paradigm and its descendants <ref> [1, 2, 4, 10] </ref>, and reflects an orientation toward experiments that are both controlled and confirmatory.
Reference: [5] <author> J.E. Cook and A.L. Wolf. </author> <title> Toward Metrics for Process Validation. </title> <booktitle> In Proceedings of the Third International Conference on the Software Process, </booktitle> <pages> pages 33-44. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: As a first part of our case study, simple aggregate metrics, such as the number of source lines changed and the elapsed time of the process, were examined for correlation with the process success metric. In the second part of the study, process validation tools <ref> [5] </ref> were used to measure the correspondence between the process as executed and the process as prescribed by a formal process model. <p> and Prescribed Processes Process validation measures the behavioral correspondence between an executing process and a formal model of that process, by cataloging both the missed activities and the extra activities A full description of the metrics that we review here, and of the theory behind them, can be found elsewhere <ref> [5, 6] </ref>. Behavioral correspondence is a critical measurement for our case study because it can help to relate process successes and failures to deviations from the prescribed process.
Reference: [6] <author> J.E. Cook and A.L. Wolf. </author> <title> Process discovery and validation through event-data analysis. </title> <type> Technical Report CU-CS-817-96, </type> <institution> Department of Computer Science, University of Colorado, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: and Prescribed Processes Process validation measures the behavioral correspondence between an executing process and a formal model of that process, by cataloging both the missed activities and the extra activities A full description of the metrics that we review here, and of the theory behind them, can be found elsewhere <ref> [5, 6] </ref>. Behavioral correspondence is a critical measurement for our case study because it can help to relate process successes and failures to deviations from the prescribed process.
Reference: [7] <author> J.L. Devore. </author> <title> Probability and Statistics for Engineering and the Sciences. </title> <address> Brooks/Cole, Pacific Grove, California, </address> <note> 3rd edition, </note> <year> 1991. </year>
Reference-contexts: Our analyses centered on performing statistical significance tests for each metric that was calculated. Most of the analyses were performed using metrics whose values are numeric. For those metrics, we used the Mann-Whitney significance test <ref> [7, Chapter 15] </ref>, which does not assume an underlying distribution of the data but is still nearly as powerful as standard significance tests that do assume a distribution. 2 The premise behind this test is that if there is no difference between the two populations (the null hypothesis), then when the
Reference: [8] <author> C.M. Judd, E.R. Smith, </author> <title> and L.H. Kidder. Research Methods in Social Relations. </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <address> Fort Worth, sixth edition, </address> <year> 1991. </year>
Reference-contexts: Here we discuss threats to the construct, internal, and external validity of our results. We use the definitions of validity given by Judd, Smith, and Kidder <ref> [8] </ref>. 7 Construct validity is concerned with how well the metrics used in the study faithfully and successfully reflect real-world attributes and values.
Reference: [9] <author> A. Kouchakdjian, S. Green, and V.R. Basili. </author> <title> Evaluation of the Cleanroom methodology in the Software Engineering Laboratory. </title> <booktitle> In Proc. Fourteenth Software Engineering Workshop, </booktitle> <institution> NASA Goddard Space Flight Center, Greenbelt, MD, </institution> <year> 1989. </year>
Reference-contexts: This approach is embodied in the Goal-Question-Metric paradigm and its descendants [1, 2, 4, 10], and reflects an orientation toward experiments that are both controlled and confirmatory. While it has succeeded in substantially advancing our understanding of general software engineering methods, typified by the studies of Cleanroom Software Engineering <ref> [9, 11] </ref>, the approach can be both expensive and intrusive when applied to in-place, specialized industrial software processes that are seeking rapid improvements. Furthermore, it often ignores the past history of a process, only viewing the process from a point in time after the instrumentation has been established.
Reference: [10] <author> R.W. Selby, A.A. Porter, D.C. Schmidt, and J. Berney. </author> <title> Metric-driven analysis and feedback systems for enabling empirically guided software development. </title> <booktitle> In Proceedings of the 13th International Conference on Software Engineering, </booktitle> <pages> pages 288-298. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: Current methods for this analysis first define what is to be learned, and then instrument the process for data collection and met-rics calculation to support that learning. This approach is embodied in the Goal-Question-Metric paradigm and its descendants <ref> [1, 2, 4, 10] </ref>, and reflects an orientation toward experiments that are both controlled and confirmatory.
Reference: [11] <author> S.W. Sherer, A. Kouchakdjian, and P.G. Arnold. </author> <title> Experience using Cleanroom software engineering. </title> <journal> IEEE Software, </journal> <volume> 13(3), </volume> <month> May </month> <year> 1996. </year>
Reference-contexts: This approach is embodied in the Goal-Question-Metric paradigm and its descendants [1, 2, 4, 10], and reflects an orientation toward experiments that are both controlled and confirmatory. While it has succeeded in substantially advancing our understanding of general software engineering methods, typified by the studies of Cleanroom Software Engineering <ref> [9, 11] </ref>, the approach can be both expensive and intrusive when applied to in-place, specialized industrial software processes that are seeking rapid improvements. Furthermore, it often ignores the past history of a process, only viewing the process from a point in time after the instrumentation has been established.
Reference: [12] <author> Allan S.Lee. </author> <title> A Scientific Methodology for MIS Case Studies. </title> <journal> MIS Quarterly, </journal> <pages> pages 33-50, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: Although this exploratory study is not an experiment (it is a case study), the credibility of the results and the ability to interpret them is greatly improved using a rigorous empirical framework <ref> [12] </ref>. Therefore, we describe our study below similar to the way we would describe an experiment. 5 3.1 Overview of the Subject Process The process we studied was a customer-initiated software update process for a large telecommunications software product.
Reference: [13] <author> G.W. Snedecor and W.G. Cochran, </author> <title> editors. Statistical Methods. </title> <institution> Iowa State University Press, </institution> <note> 8th edition, </note> <year> 1989. </year>
Reference-contexts: Each metric we used has a distribution plot shown in Appendix B. 9 or about 23%. 3 A statistical result called the Bonferroni inequality allows us to compute a conservative upper bound on the overall significance level of a set of tests <ref> [13] </ref>. This inequality deduces that the joint significance level of N tests, each performed at an ff significance level, is at least N ff. Thus, to ensure a significance level p for some set of N tests, each test should be evaluated at the p=N significance level.
Reference: [14] <author> L.G. Votta and M.L. Zajac. </author> <title> Design process improvement case study using process waiver data. </title> <booktitle> In Proceedings of the Fifth European Software Engineering Conference (ESEC'95), </booktitle> <pages> pages 44-58. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: No extra data collection is necessary and there is little intrusion into the developers' activities. Compared to new instrumentation on a process, this effort is minimal. A similar kind of historical data analysis was successfully employed in a recent study conducted by Votta and Zajac <ref> [14] </ref>, in which they looked at process waiver data (data indicating when a process was exempted from its prescribed activities) and correlated this with the outcome of the product.
Reference: [15] <author> A.L. Wolf and D.S. Rosenblum. </author> <title> A Study in Software Process Data Capture and Analysis. </title> <booktitle> In Proceedings of the Second International Conference on the Software Process, </booktitle> <pages> pages 115-124. </pages> <publisher> IEEE Computer Society, </publisher> <month> February </month> <year> 1993. </year>
Reference-contexts: While certainly not true of all processes, in our subject process we were able to find enough data of high quality for valid statistical analysis. There are many kinds of data we could examine, but we chose to look at event data <ref> [3, 15] </ref> because they neatly characterize the dynamic behavior of the process in terms of the sequencing of its major activities. The event data come from several sources.
Reference: [16] <author> R. Yin. </author> <title> Case Study Research: Design and Methods. </title> <publisher> SAGE Publications, </publisher> <address> Thousand Oaks, California, 2nd edition, </address> <year> 1994. </year> <month> 22 </month>
References-found: 16

