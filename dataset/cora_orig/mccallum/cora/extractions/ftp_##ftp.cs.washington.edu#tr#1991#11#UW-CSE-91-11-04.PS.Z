URL: ftp://ftp.cs.washington.edu/tr/1991/11/UW-CSE-91-11-04.PS.Z
Refering-URL: http://www.cs.washington.edu/research/compiler/papers.d/rtcg-case.html
Root-URL: 
Title: A Case for Runtime Code Generation  
Address: Seattle, WA 98195 USA  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington,  
Abstract: David Keppel, Susan J. Eggers and Robert R. Henry Technical Report 91-11-04 
Abstract-found: 1
Intro-found: 1
Reference: [Bed89] <author> Robert Bedichek. </author> <title> Some Efficient Architecture Simulation Techniques. </title> <booktitle> Winter '90 USENIX Conference, </booktitle> <pages> pages 53-63, </pages> <month> 26 October, </month> <year> 1989. </year>
Reference-contexts: Better RTCG optimization would cause longer compile times but yield code asymptotically 5-10 times faster than the static code [CU91]. Another virtual machine, g88, is a statically-compiled 88000 simulator that translates instructions to threaded code <ref> [Bed89] </ref>. It runs on the 68000 and takes, on average, about twenty 68000 machine instructions to simu late one 88000 instruction. <p> For example, floating-point operations could be executed using hardware, where they might be simulated in cross-machine simulation. However, g88 makes the same optimizations to get its performance <ref> [Bed89] </ref>. Page 6 test is a large overhead. It is impractical to fully--case the code, since a record with F fields can have F ! cases. Common-casing can exploit record and sort vector statistics, but is only as good as the statistics.
Reference: [Bed91] <author> Robert Bedichek. </author> <type> Personal communication, </type> <month> October </month> <year> 1991. </year>
Reference-contexts: Another virtual machine, g88, is a statically-compiled 88000 simulator that translates instructions to threaded code [Bed89]. It runs on the 68000 and takes, on average, about twenty 68000 machine instructions to simu late one 88000 instruction. Simulating only user-mode code would drop the ratio to ten to one <ref> [Bed91] </ref>. shade is a SPARC simulator and instruction-level profiler that runs on a SPARC and uses RTCG [CK91]. On the gcc and doduc benchmarks from the SPEC suite, shade spends about 10% and 0.1% of its total runtime doing compilation, respectively.
Reference: [CAK + 81] <author> D. D. Chamberlin, M. M. Astrahan, W. F. King, R. Alorie, J. W. Mehl, T. G. Price, M. Schkolnick, P. Griffiths Selinger, D. R. Slutz, B. W. Wade, and R. A. Yost. </author> <title> Support for Repetitive Transactions and Ad Hoc Queries in System R. </title> <journal> ACM Transactions on Databse Systems, </journal> <volume> 6(1) </volume> <pages> 70-94, </pages> <month> March </month> <year> 1981. </year>
Reference-contexts: It is functionally required for incremental compilers [PS84], dynamic linkers [HO91, sys90], and debuggers [Kes90]. Other systems use RTCG to improve performance: high-performance virtual machines [DS84, May87], interactive systems that demand good response time on repeated actions or inner loops [PLR85, MP89], and database query optimization <ref> [CAK + 81] </ref>. Finally, at least one processor uses runtime generation of microcode [DM87], and runtime programming of gate arrays has been proposed [JFnt]. A second reason for reexamining RTCG is that hardware implementations are changing in ways that may Page 1 again favor it. <p> Finally, few comparative studies demonstrated RTCG's performance benefits, so it was difficult to determine which programs were appropriate for an RTCG implementation. For example, it is reported that database queries are much faster if the query is dynam ically optimized and compiled <ref> [CAK + 81] </ref>; yet there are no published quantitative analyses of when, how much, and why it is faster. 3 Changes In Technology Recent changes in hardware and software make RTCG more likely to be useful. <p> a training suite [Hen87], but in most existing RTCG systems 1 When the source code is not known statically, RTCG can still be profitable, since the user input will be translated to some intermediate representation anyway [Tho68], and code generation may be but a small part of the total cost <ref> [CAK + 81] </ref>. Page 5 the templates are derived manually. With a template compiler, nearly all the compilation is static, including some parts of code generation, assembly, and linking.
Reference: [CK91] <author> Robert F. Cmelik and David Keppel. Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling. </title> <type> Technical Report (internal), </type> <institution> Sun Microsystems, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: It runs on the 68000 and takes, on average, about twenty 68000 machine instructions to simu late one 88000 instruction. Simulating only user-mode code would drop the ratio to ten to one [Bed91]. shade is a SPARC simulator and instruction-level profiler that runs on a SPARC and uses RTCG <ref> [CK91] </ref>. On the gcc and doduc benchmarks from the SPEC suite, shade spends about 10% and 0.1% of its total runtime doing compilation, respectively.
Reference: [CU91] <author> Craig Chambers and David Ungar. </author> <title> Making Pure Object-Oriented Languages Practical. </title> <booktitle> OOPSLA '91 Proceedings; SIGPLAN Notices, </booktitle> <volume> 26(11) </volume> <pages> 1-15, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Fast static-code implementations [Mir87] interpret the byte codes using a threaded code interpreter. An RTCG implementation that does simple optimizations [DS84] is 40% faster than fast static-code implementations [Mir91]. Better RTCG optimization would cause longer compile times but yield code asymptotically 5-10 times faster than the static code <ref> [CU91] </ref>. Another virtual machine, g88, is a statically-compiled 88000 simulator that translates instructions to threaded code [Bed89]. It runs on the 68000 and takes, on average, about twenty 68000 machine instructions to simu late one 88000 instruction.
Reference: [DM87] <author> David R. Ditzel and Hubert R. McLellan. </author> <title> Branch Folding in the CRISP Microprocessor: Reducing Branch Delay to Zero. </title> <booktitle> Proceedings of the 14th Annual International Symposium on Computer Architecture; Computer Architecture News, </booktitle> <volume> 15(2) </volume> <pages> 2-9, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Other systems use RTCG to improve performance: high-performance virtual machines [DS84, May87], interactive systems that demand good response time on repeated actions or inner loops [PLR85, MP89], and database query optimization [CAK + 81]. Finally, at least one processor uses runtime generation of microcode <ref> [DM87] </ref>, and runtime programming of gate arrays has been proposed [JFnt]. A second reason for reexamining RTCG is that hardware implementations are changing in ways that may Page 1 again favor it. When memories got larger, code size became less important. However, small fast memories have been reintroduced as caches.
Reference: [DS84] <author> Peter Deutsch and Alan M. Schiffman. </author> <title> Efficient Implementation of the Smalltalk-80 System. </title> <booktitle> 11th Annual Symposium on Principles of Programming Languages (POPL 11), </booktitle> <pages> pages 297-302, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: It is functionally required for incremental compilers [PS84], dynamic linkers [HO91, sys90], and debuggers [Kes90]. Other systems use RTCG to improve performance: high-performance virtual machines <ref> [DS84, May87] </ref>, interactive systems that demand good response time on repeated actions or inner loops [PLR85, MP89], and database query optimization [CAK + 81]. Finally, at least one processor uses runtime generation of microcode [DM87], and runtime programming of gate arrays has been proposed [JFnt]. <p> Another hardware trend is that CPUs are getting faster relative to the memory hierarchy. Thus, it may be profitable to expend CPU time to alleviate other bottlenecks. For example, it might sometimes be better to generate code at runtime than to page in statically-compiled code <ref> [DS84, SC91] </ref> 3.2 Software Technology Several trends in software technology may also contribute to RTCG's utility. First, as optimizers have improved, interest has turned from data-independent optimizations to data-dependent optimizations. For example, profiles are used to improve branch prediction and drive code placement. <p> Also, the dynamic compiler can optimize across several virtual machine instructions. The Smalltalk-80 byte-coded virtual machine has both static-code and RTCG implementations. Fast static-code implementations [Mir87] interpret the byte codes using a threaded code interpreter. An RTCG implementation that does simple optimizations <ref> [DS84] </ref> is 40% faster than fast static-code implementations [Mir91]. Better RTCG optimization would cause longer compile times but yield code asymptotically 5-10 times faster than the static code [CU91]. Another virtual machine, g88, is a statically-compiled 88000 simulator that translates instructions to threaded code [Bed89].
Reference: [FH91] <author> Chris W. Fraser and Robert R. Henry. </author> <title> Hard-Coding Bottom-Up Code Generation Tables to Save Time and Space. </title> <journal> Software Practice and Experience, </journal> <volume> 21(1) </volume> <pages> 1-12, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Current automatic code generator generators can build code generators that rival hand-written code generators in both speed and code quality, and should be able to produce object code directly from a portable intermediate representation, executing as few as a few hundred instructions per instruction generated <ref> [FH91] </ref>, a rate that is comparable to hand-written code generators. In some circumstances, dynamic compiler performance can be improved even further using a template compiler (to be discussed in x4.3). The extra specialization makes code generation even faster, though perhaps at the expense of code quality.
Reference: [Har77] <author> Anders Haraldsson. </author> <title> A Program Manipulation System Based on Partial Evaluation. </title> <institution> Technical Report Linkoping Studies in Science and Technology Dissertations No. 14, Department of Mathematics, Linkoping University, S-581 83 Linkoping, Sweden, </institution> <year> 1977. </year>
Reference-contexts: Finally, partial evaluation may be able to derive fast special-purpose compilers automatically from their general-purpose counterparts plus the source code for the routine to be compiled. Although partial evaluation itself is a well-established technique used in compiler transformations <ref> [Har77] </ref>, our initial experiments with Similix [JSS89] lead us to believe that partially evaluating a program as large and complex as a conventional compiler or code generator is not yet possible.
Reference: [Hen87] <author> Robert R. Henry. </author> <title> Code Generation by Table Lookup. </title> <type> Technical Report 87-07-07, </type> <institution> University of Washington Computer Science, </institution> <year> 1987. </year> <note> REFERENCES Page 8 </note>
Reference-contexts: The compiler is retargeted by rewrit ing the machine-code templates. The runtime compiler concatenates templates and fills in the holes with values computed at runtime. The latter approach is similar to generating code using machine-dependent virtual machine instructions automatically derived from a training suite <ref> [Hen87] </ref>, but in most existing RTCG systems 1 When the source code is not known statically, RTCG can still be profitable, since the user input will be translated to some intermediate representation anyway [Tho68], and code generation may be but a small part of the total cost [CAK + 81].
Reference: [HO91] <author> W. Wilson Ho and Rondald A Olsson. </author> <title> An Approach to Genuine Dynamic Linking. </title> <journal> Software-Practice and Experience, </journal> <volume> 21(4) </volume> <pages> 375-390, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Why do we bring it up again? One reason is that despite technology advances and bad press, RTCG is still being used in a large number of research and produc tion systems. It is functionally required for incremental compilers [PS84], dynamic linkers <ref> [HO91, sys90] </ref>, and debuggers [Kes90]. Other systems use RTCG to improve performance: high-performance virtual machines [DS84, May87], interactive systems that demand good response time on repeated actions or inner loops [PLR85, MP89], and database query optimization [CAK + 81].
Reference: [JFnt] <author> Charles Johnson and David L. Fox. </author> <title> The Silicon Palimpset A Programming Model for Electrically Reconfigurable Processors. </title> <note> SIGForth (to appear), 7 March 1991 preprint. </note>
Reference-contexts: Finally, at least one processor uses runtime generation of microcode [DM87], and runtime programming of gate arrays has been proposed <ref> [JFnt] </ref>. A second reason for reexamining RTCG is that hardware implementations are changing in ways that may Page 1 again favor it. When memories got larger, code size became less important. However, small fast memories have been reintroduced as caches.
Reference: [Jou90] <author> Norman P. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: When memories got larger, code size became less important. However, small fast memories have been reintroduced as caches. Current trends are for smaller first-level caches and larger penalties for misses <ref> [Jou90, MBB + 91] </ref>. In addition, implementation-dependent compiler optimizations, such as loop unrolling, can affect cache performance substantially; RTCG lets a program dynamically adapt to particular implementations within a family of architectures. Third, recent advances in software methods may solve some portability and programability problems. <p> Page 2 3.1 Hardware Technology Large memories made program performance independent of code size. However, caching, especially the current trend of smaller first-level caches and growing miss penalties <ref> [Jou90, MBB + 91] </ref>, has reintroduced the size cost. Implementations of the same architecture may have caches whose size and miss penalties vary by more than an order of magnitude. Statically-compiled binaries must run well on all members of an architectural family.
Reference: [JSS89] <author> Neil D. Jones, Peter Sestoft, and Harald Sonder-gaard. </author> <title> Mix: A Self-Applicable Partial Evaluator for Experiments in Compiler Generation. Lisp and Symbolic Computation, </title> <address> 2(9-50):10, </address> <year> 1989. </year>
Reference-contexts: Third, recent advances in software methods may solve some portability and programability problems. Machine-dependencies can be encapsulated in a re-targetable code generator, while instruction space coherency and protection can be hidden using a portable interface [Kep91]. In addition, partial evaluation <ref> [JSS89] </ref> may be useful in automatically deriving runtime compilers that are optimized for the particular application. Two original and major problems still remain. First, many systems use RTCG in ad-hoc ways, so their generality and portability suffer. <p> Finally, partial evaluation may be able to derive fast special-purpose compilers automatically from their general-purpose counterparts plus the source code for the routine to be compiled. Although partial evaluation itself is a well-established technique used in compiler transformations [Har77], our initial experiments with Similix <ref> [JSS89] </ref> lead us to believe that partially evaluating a program as large and complex as a conventional compiler or code generator is not yet possible. However, RTCG will clearly benefit from advances in the practice of partial evaluation. 3.3 Workload Changes A third change is in workload composition.
Reference: [Kep91] <author> David Keppel. </author> <title> A Portable Interface for On-The-Fly Instruction Space Modification. </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pages 86-95, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Third, recent advances in software methods may solve some portability and programability problems. Machine-dependencies can be encapsulated in a re-targetable code generator, while instruction space coherency and protection can be hidden using a portable interface <ref> [Kep91] </ref>. In addition, partial evaluation [JSS89] may be useful in automatically deriving runtime compilers that are optimized for the particular application. Two original and major problems still remain. First, many systems use RTCG in ad-hoc ways, so their generality and portability suffer. <p> Third, using a code generator hidden behind a good interface will hide the details of instruction space changes. That will then allow instruction caching issues to be encapsulated in a portable interface, solving instruction cache portability problems <ref> [Kep91] </ref>. Finally, partial evaluation may be able to derive fast special-purpose compilers automatically from their general-purpose counterparts plus the source code for the routine to be compiled.
Reference: [Kes90] <author> Peter Kessler. </author> <title> Fast Breakpoints: </title> <booktitle> Design and Implementation. Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation; SIGPLAN Notices, </booktitle> <volume> 25(6) </volume> <pages> 78-84, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Why do we bring it up again? One reason is that despite technology advances and bad press, RTCG is still being used in a large number of research and produc tion systems. It is functionally required for incremental compilers [PS84], dynamic linkers [HO91, sys90], and debuggers <ref> [Kes90] </ref>. Other systems use RTCG to improve performance: high-performance virtual machines [DS84, May87], interactive systems that demand good response time on repeated actions or inner loops [PLR85, MP89], and database query optimization [CAK + 81].
Reference: [Loc87] <author> Bart N. Locanthi. </author> <title> Fast BitBlt With asm() and CPP. </title> <booktitle> European Unix Users Group Conference Proceedings (EUUG), </booktitle> <month> September </month> <year> 1987. </year>
Reference-contexts: A straightforward RTCG version of bitblt is about 10 times faster than a straightforward static-code implementation [Mir87]. An optimized RTCG bitblt runs twice as fast as machine-dependent static code and four times as fast as portable C code that has been partially but not fully cased <ref> [Loc87] </ref>. The dynamic compiler is substantially smaller than the statically optimized bitblt, 2.8KB instead of 8.0KB [Loc87]. bitblt illustrates how RTCG can improve code quality and also demonstrates that the benefit is data-dependent: for small rectangles, the compile time would always be larger than any savings from dynamic compilation. <p> An optimized RTCG bitblt runs twice as fast as machine-dependent static code and four times as fast as portable C code that has been partially but not fully cased <ref> [Loc87] </ref>. The dynamic compiler is substantially smaller than the statically optimized bitblt, 2.8KB instead of 8.0KB [Loc87]. bitblt illustrates how RTCG can improve code quality and also demonstrates that the benefit is data-dependent: for small rectangles, the compile time would always be larger than any savings from dynamic compilation. <p> Thus, small rectangles are handled by statically-generated code <ref> [PLR85, Loc87] </ref>. 4.2 Cost Model We now generalize several performance factors exposed in the bitblt example.
Reference: [May87] <author> Cathy May. </author> <title> A Fast S/370 Simulator. </title> <booktitle> Proceedings of the ACM SIGPLAN 1987 Symposium on Interpreters and Interpretive Techniques; SIGPLAN Notices, </booktitle> <volume> 22(6) </volume> <pages> 1-13, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: It is functionally required for incremental compilers [PS84], dynamic linkers [HO91, sys90], and debuggers [Kes90]. Other systems use RTCG to improve performance: high-performance virtual machines <ref> [DS84, May87] </ref>, interactive systems that demand good response time on repeated actions or inner loops [PLR85, MP89], and database query optimization [CAK + 81]. Finally, at least one processor uses runtime generation of microcode [DM87], and runtime programming of gate arrays has been proposed [JFnt].
Reference: [MBB + 91] <author> T. N. Mudge, R. B. Brown, W. P. Birmingham, J. A. Dykstra, A. I. Kayssi, R. J. Lomax, O. A. Olukotun, K. A. Sakallah, and R. Milano. </author> <title> The Design of a Micro-Supercomputer. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 57-64, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: When memories got larger, code size became less important. However, small fast memories have been reintroduced as caches. Current trends are for smaller first-level caches and larger penalties for misses <ref> [Jou90, MBB + 91] </ref>. In addition, implementation-dependent compiler optimizations, such as loop unrolling, can affect cache performance substantially; RTCG lets a program dynamically adapt to particular implementations within a family of architectures. Third, recent advances in software methods may solve some portability and programability problems. <p> Page 2 3.1 Hardware Technology Large memories made program performance independent of code size. However, caching, especially the current trend of smaller first-level caches and growing miss penalties <ref> [Jou90, MBB + 91] </ref>, has reintroduced the size cost. Implementations of the same architecture may have caches whose size and miss penalties vary by more than an order of magnitude. Statically-compiled binaries must run well on all members of an architectural family.
Reference: [McF91] <author> Scott McFarling. </author> <title> Procedure Merging with Instruction Caches. </title> <booktitle> ACM SIGPLAN '91 Conference on Programming Language Design and Implementation (PLDI); SIGPLAN Notices, </booktitle> <volume> 26(6) </volume> <pages> 71-79, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: First, as optimizers have improved, interest has turned from data-independent optimizations to data-dependent optimizations. For example, profiles are used to improve branch prediction and drive code placement. However, a profile represents an execution that might not be typical of a given execution <ref> [Wal91, McF91] </ref>. By comparison, data-dependent optimizations done at runtime use the program's real data values and real behavior. Dynamic compilation would also allow the choice of data placement algorithms for a particular layout to be made at runtime [SC91].
Reference: [Mir87] <author> Eliot Miranda. </author> <title> BrouHaHa A Portable Smalltalk Interpreter. </title> <booktitle> OOPSLA 87 Proceedings; SIGPLAN Notices, </booktitle> <volume> 22(12) </volume> <pages> 354-364, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: For example, the loop may be unrolled for the specific line length, which is profitable for short inner loops. For large rectangles, the optimizations more than pay for the compile time. A straightforward RTCG version of bitblt is about 10 times faster than a straightforward static-code implementation <ref> [Mir87] </ref>. An optimized RTCG bitblt runs twice as fast as machine-dependent static code and four times as fast as portable C code that has been partially but not fully cased [Loc87]. <p> Dispatching is done during compilation and the machine code is cached, so both compiling and dispatching costs can be avoided on reexecution. Also, the dynamic compiler can optimize across several virtual machine instructions. The Smalltalk-80 byte-coded virtual machine has both static-code and RTCG implementations. Fast static-code implementations <ref> [Mir87] </ref> interpret the byte codes using a threaded code interpreter. An RTCG implementation that does simple optimizations [DS84] is 40% faster than fast static-code implementations [Mir91]. Better RTCG optimization would cause longer compile times but yield code asymptotically 5-10 times faster than the static code [CU91].
Reference: [Mir91] <author> Eliot Miranda. </author> <title> Portable Fast Direct Threaded Code. </title> <type> Technical Report USENET comp.compilers article, </type> <institution> ID #3035@redstar.cs.qmw.ac.uk, Computer Science Dept, QMW, University of London, UK, </institution> <month> 28 March </month> <year> 1991. </year>
Reference-contexts: The Smalltalk-80 byte-coded virtual machine has both static-code and RTCG implementations. Fast static-code implementations [Mir87] interpret the byte codes using a threaded code interpreter. An RTCG implementation that does simple optimizations [DS84] is 40% faster than fast static-code implementations <ref> [Mir91] </ref>. Better RTCG optimization would cause longer compile times but yield code asymptotically 5-10 times faster than the static code [CU91]. Another virtual machine, g88, is a statically-compiled 88000 simulator that translates instructions to threaded code [Bed89].
Reference: [MP89] <author> Henry Massalin and Calton Pu. </author> <title> Threads and Input/Output in the Synthesis Kernel. </title> <booktitle> Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 191-201, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: It is functionally required for incremental compilers [PS84], dynamic linkers [HO91, sys90], and debuggers [Kes90]. Other systems use RTCG to improve performance: high-performance virtual machines [DS84, May87], interactive systems that demand good response time on repeated actions or inner loops <ref> [PLR85, MP89] </ref>, and database query optimization [CAK + 81]. Finally, at least one processor uses runtime generation of microcode [DM87], and runtime programming of gate arrays has been proposed [JFnt].
Reference: [PHH88] <author> Steven Przybylski, Mark Horowitz, and John Hen-nessy. </author> <title> Performance Tradeoffs in Cache Design. </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 290-298, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The cache parameters vary from run to run, but are fixed over any given run. Thus, a runtime compiler can optimize the simulator for each run, compiling the cache parameters into the code as constants <ref> [PHH88] </ref>. 4 Performance Opportunities Runtime code generation can lead to better performance simply because a compiler can generate better code when it has more information, and all information is available at runtime.
Reference: [PLR85] <author> Rob Pike, Bart N. Locanthi, and John F. Reiser. </author> <title> Hardware/Software Trade-offs for Bitmap Graphics on the Blit. </title> <journal> Software Practice and Experience, </journal> <volume> 15(2) </volume> <pages> 131-151, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: It is functionally required for incremental compilers [PS84], dynamic linkers [HO91, sys90], and debuggers [Kes90]. Other systems use RTCG to improve performance: high-performance virtual machines [DS84, May87], interactive systems that demand good response time on repeated actions or inner loops <ref> [PLR85, MP89] </ref>, and database query optimization [CAK + 81]. Finally, at least one processor uses runtime generation of microcode [DM87], and runtime programming of gate arrays has been proposed [JFnt]. <p> A specialized routine produced by RTCG may be smaller than its statically-compiled general-purpose counterpart and may fit into a cache where the larger one will not. For example, a dynamically compiled bitblt routine may be one-quarter the size of the static-code version <ref> [PLR85] </ref>. In-cache execution can increase the instruction issue rate and reduce out-of-cache traffic. Dynamically-generated code may also have fewer branches than the static code, thus improving prefetching performance. Finally, dynamic specialization may eliminate dynamically dead code and place dynamically-adjacent code adjacent in the cache, reducing cache conflicts. <p> balance point where the runtime compiler consumes some but not all input data, does little work, produces substantially better code, and then runs the improved code on the remaining data. 4.1 Bitblt: An Example We now examine bitblt, a bit-transfer operation used frequently in the core of raster graphics systems <ref> [PLR85] </ref>. <p> The code can be further optimized for overlapping rectangles. Although each of the optimizations can be done statically, each requires replicating the code. Fully-casing may lead to an implementation as large as 1MB <ref> [PLR85] </ref>. Common-casing is hard, since a one-pixel change in the rectangles' positions or size can affect half a dozen op Page 4 Running Time # Of Items of Input Data Unoptimized Static Code Optimized Static Code Low-Optimization Dynamic Code High-Optimization Dynamic Code cost N startup m timizations. <p> Thus, small rectangles are handled by statically-generated code <ref> [PLR85, Loc87] </ref>. 4.2 Cost Model We now generalize several performance factors exposed in the bitblt example.
Reference: [PS84] <author> Lori L. Pollock and Mary Lou Soffa. </author> <title> Incremental Compilation of Locally Optimized Code. </title> <booktitle> Conference Record of the 12th Annual Symposium on Principles of Programming Languages (POPL 12), </booktitle> <pages> pages 152-164, </pages> <year> 1984. </year>
Reference-contexts: Why do we bring it up again? One reason is that despite technology advances and bad press, RTCG is still being used in a large number of research and produc tion systems. It is functionally required for incremental compilers <ref> [PS84] </ref>, dynamic linkers [HO91, sys90], and debuggers [Kes90]. Other systems use RTCG to improve performance: high-performance virtual machines [DS84, May87], interactive systems that demand good response time on repeated actions or inner loops [PLR85, MP89], and database query optimization [CAK + 81].
Reference: [SC91] <author> Harold S. Stone and John Cocke. </author> <booktitle> Computer Architecture in the 1990s. IEEE Computer, </booktitle> <pages> pages 30-38, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Another hardware trend is that CPUs are getting faster relative to the memory hierarchy. Thus, it may be profitable to expend CPU time to alleviate other bottlenecks. For example, it might sometimes be better to generate code at runtime than to page in statically-compiled code <ref> [DS84, SC91] </ref> 3.2 Software Technology Several trends in software technology may also contribute to RTCG's utility. First, as optimizers have improved, interest has turned from data-independent optimizations to data-dependent optimizations. For example, profiles are used to improve branch prediction and drive code placement. <p> By comparison, data-dependent optimizations done at runtime use the program's real data values and real behavior. Dynamic compilation would also allow the choice of data placement algorithms for a particular layout to be made at runtime <ref> [SC91] </ref>. Second, retargetable compilers should allow RTCG problems to be stated in a machine-independent way without sacrificing either the quality of the machine dependent code or the speed of generating it.
Reference: [sys90] <institution> System V Application Binary Interface. Prentice-Hall, </institution> <year> 1990. </year>
Reference-contexts: Why do we bring it up again? One reason is that despite technology advances and bad press, RTCG is still being used in a large number of research and produc tion systems. It is functionally required for incremental compilers [PS84], dynamic linkers <ref> [HO91, sys90] </ref>, and debuggers [Kes90]. Other systems use RTCG to improve performance: high-performance virtual machines [DS84, May87], interactive systems that demand good response time on repeated actions or inner loops [PLR85, MP89], and database query optimization [CAK + 81].
Reference: [Tho68] <author> Ken Thompson. </author> <title> Regular Expression Search Algorithm. </title> <journal> Communications of the Association for Computing Machinery (CACM), </journal> <volume> 11(6) </volume> <pages> 419-422, </pages> <month> June </month> <year> 1968. </year>
Reference-contexts: approach is similar to generating code using machine-dependent virtual machine instructions automatically derived from a training suite [Hen87], but in most existing RTCG systems 1 When the source code is not known statically, RTCG can still be profitable, since the user input will be translated to some intermediate representation anyway <ref> [Tho68] </ref>, and code generation may be but a small part of the total cost [CAK + 81]. Page 5 the templates are derived manually. With a template compiler, nearly all the compilation is static, including some parts of code generation, assembly, and linking.
Reference: [Wal91] <author> David W. Wall. </author> <title> Predicting Program Behavior Using Real or Estimated Profiles. </title> <booktitle> ACM SIGPLAN '91 Conference on Programming Language Design and Implementation; SIGPLAN Notices, </booktitle> <volume> 26(6), </volume> <month> June </month> <year> 1991. </year>
Reference-contexts: First, as optimizers have improved, interest has turned from data-independent optimizations to data-dependent optimizations. For example, profiles are used to improve branch prediction and drive code placement. However, a profile represents an execution that might not be typical of a given execution <ref> [Wal91, McF91] </ref>. By comparison, data-dependent optimizations done at runtime use the program's real data values and real behavior. Dynamic compilation would also allow the choice of data placement algorithms for a particular layout to be made at runtime [SC91].
Reference: [Wel84] <author> Terry A. Welch. </author> <title> A Technique for High Performance Data Compression. </title> <journal> IEEE Computer, </journal> <volume> 17(6) </volume> <pages> 8-19, </pages> <month> June </month> <year> 1984. </year> <note> REFERENCES Page 9 </note>
Reference-contexts: All other com pressed codes are used as indicies into a table of variable length strings that are simply appended to the output. As a baseline, the C implementation of pardoz decompresses at roughly four times the rate of uncompress, a standard implementation of Lempel-Ziv decompression <ref> [Wel84] </ref>. The stream of compressed data codes can be viewed as a program for an interpreter with one primitive for each string length and one for the escape. We implemented a threaded-code interpreter for pardoz in assembly language, with unrolled loops and hand-scheduled loads and stores.
References-found: 31

