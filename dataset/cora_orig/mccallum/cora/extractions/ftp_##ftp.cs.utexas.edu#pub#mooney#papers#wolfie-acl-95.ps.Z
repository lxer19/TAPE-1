URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/wolfie-acl-95.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: cthomp@cs.utexas.edu  
Title: Acquisition of a Lexicon from Semantic Representations of Sentences  
Author: Cynthia A. Thompson 
Address: 2.124 Taylor Hall Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Note: Appears in 33rd Annual Meeting of the Association for Computational Linguistics  
Abstract: A system, Wolfie, that acquires a mapping of words to their semantic representation is presented and a preliminary evaluation is performed. Tree least general generalizations (TLGGs) of the representations of input sentences are performed to assist in determining the representations of individual words in the sentences. The best guess for a meaning of a word is the TLGG which overlaps with the highest percentage of sentence representations in which that word appears. Some promising experimental results on a non-artificial data set are presented.
Abstract-found: 1
Intro-found: 1
Reference: <author> Berwick, Robert C., and Pilato, S. </author> <year> (1987). </year> <title> Learning syntax by automata induction. </title> <journal> Machine Learning, </journal> <volume> 2(1) </volume> <pages> 9-38. </pages>
Reference-contexts: In the long term, we would like to communicate with computers as easily as we do with people. Learning word meanings is an important step in this direction. Some other approaches to the lexical acquisition problem depend on knowledge of syntax to assist in lexical learning <ref> (Berwick and Pilato, 1987) </ref>. Also, most of these have not demonstrated the ability to tie in to the rest of a language learning system (Hastings and Lytinen, 1994; Kazman, 1990; Siskind, 1994). Finally, unnatural data is sometimes needed (Siskind, 1994).
Reference: <author> Hastings, Peter, and Lytinen, </author> <title> Steven (1994). The ups and downs of lexical acquisition. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 754-759. </pages>
Reference: <author> Kazman, </author> <title> Rick (1990). Babel: A psychologically plausible cross-linguistic model of lexical and syntactic acquisition. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 75-79. </pages> <address> Evanston, IL. </address>
Reference: <author> McClelland, James L., and Kawamoto, A. H. </author> <year> (1986). </year> <title> Mechanisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In Rumelhart, D. E., and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> 318-362. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: One way to test this is by examining the results by hand. Another way to test this is to use the results to assist a larger learning system. The corpus used is based on that of <ref> (McClelland and Kawamoto, 1986) </ref>. That corpus is a set of 1475 sentence/case-structure pairs, produced from a set of 19 sentence templates. We modified only the case-structure portion of these pairs.
Reference: <author> Plotkin, Gordon D. </author> <year> (1970). </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., and Michie, D., editors, </editor> <booktitle> Machine Intelligence (Vol. 5). </booktitle> <address> New York: </address> <publisher> Elsevier North-Holland. </publisher>
Reference-contexts: Our trees have labels on their arcs; thus a tree with root p, one child c, and an arc label to that child l is denoted [p,l:c]. TLGGs are related to the LGGs of <ref> (Plotkin, 1970) </ref>. Summarizing that work, the LGG of two clauses is the least general clause that subsumes both clauses. For example, given the trees [ate,agt:[person,sex:male,age:adult], pat:[food,type:cheese]] and [hit,inst:[inst,type:ball], pat:[person,sex:male,age:child]] the TLGGs are [person,sex:male] and [male].
Reference: <author> Schank, Roger C. </author> <year> (1975). </year> <title> Conceptual Information Processing. </title> <publisher> Oxford: North-Holland. </publisher>
Reference-contexts: Our system, Wolfie (WOrd Learning From Interpreted Examples), learns this mapping from training examples consisting of sentences paired with their semantic representation. The representation used here is based on Conceptual Dependency (CD) <ref> (Schank, 1975) </ref>. The results of our system can be used to fl This research was supported by the National Science Foundation under grant IRI-9310819 assist a larger language acquisition system; in particular, we use the results as part of the input to Chill (Zelle and Mooney, 1993).
Reference: <author> Siskind, Jeffrey M. </author> <year> (1994). </year> <title> Lexical acquisition in the presence of noise and homonymy. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 760-766. </pages>
Reference-contexts: Also, most of these have not demonstrated the ability to tie in to the rest of a language learning system (Hastings and Lytinen, 1994; Kazman, 1990; Siskind, 1994). Finally, unnatural data is sometimes needed <ref> (Siskind, 1994) </ref>. We present a lexical acquisition system that learns a mapping of words to their semantic representation, and which overcomes the above problems. Our system, Wolfie (WOrd Learning From Interpreted Examples), learns this mapping from training examples consisting of sentences paired with their semantic representation.
Reference: <author> Zelle, John M., and Mooney, Raymond J. </author> <year> (1993). </year> <title> Learning semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 817-822. </pages>
Reference-contexts: The results of our system can be used to fl This research was supported by the National Science Foundation under grant IRI-9310819 assist a larger language acquisition system; in particular, we use the results as part of the input to Chill <ref> (Zelle and Mooney, 1993) </ref>. Chill learns to parse sentences into case-role representations by analyzing a sample of sentence/case-role pairings. By extending the representation of each word to a CD representation, the problem faced by Chill is made more difficult.
Reference: <institution> Washington, </institution> <address> D.C. </address>
References-found: 9

