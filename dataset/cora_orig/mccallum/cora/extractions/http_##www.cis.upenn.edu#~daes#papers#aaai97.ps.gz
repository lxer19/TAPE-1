URL: http://www.cis.upenn.edu/~daes/papers/aaai97.ps.gz
Refering-URL: http://www.cis.upenn.edu/~daes/papers.html
Root-URL: 
Email: daes@linc.cis.upenn.edu  dale@research.nj.nec.com  
Title: A New Metric-Based Approach to Model Selection  
Author: Dale Schuurmans 
Date: July 1997.  
Note: Appears in Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97), Providence, RI,  
Address: Philadelphia, PA 19104-6228  4 Independence Way Princeton, NJ 08540  
Affiliation: Institute for Research in Cognitive Science University of Pennsylvania  NEC Research Institute  
Abstract: We introduce a new approach to model selection that performs better than the standard complexity-penalization and hold-out error estimation techniques in many cases. The basic idea is to exploit the intrinsic metric structure of a hypothesis space, as determined by the natural distribution of unlabeled training patterns, and use this metric as a reference to detect whether the empirical error estimates derived from a small (labeled) training sample can be trusted in the region around an empirically optimal hypothesis. Using simple metric intuitions we develop new geometric strategies for detecting overfitting and performing robust yet responsive model selection in spaces of candidate functions. These new metric-based strategies dramatically outperform previous approaches in experimental studies of classical polynomial curve fitting. Moreover, the technique is simple, efficient, and can be applied to most function learning tasks. The only requirement is access to an auxiliary collection of unlabeled training data. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. </author> <year> 1994. </year> <title> Bagging predictors. </title> <type> Technical Report 421, </type> <institution> Statistics Department, UC Berkeley. </institution>
Reference: <author> Cherkassky, V.; Mulier, F.; and Vapnik, V. </author> <year> 1996. </year> <title> Comparison of VC-method with classical methods for model selection. </title> <type> Preprint. </type>
Reference-contexts: To determine the efficacy of TRI we compared its performance to a number of standard model selection strategies, including two well-known penalization strategies|generalized cross validation GCV (Craven & Wahba 1979) and structural risk minimization SRM (Vapnik 1996) (under the formulations reported in <ref> (Cherkassky, Mulier, & Vapnik 1996) </ref>)|and 10-fold cross validation 10CV, a standard hold-out method (Efron 1979; Kohavi 1995). We conducted a simple series of experiments by fixing a uniform domain distribution P X on the unit interval [0; 1], and then fixing various target functions f : [0; 1] ! IR.
Reference: <author> Craven, P., and Wahba, G. </author> <year> 1979. </year> <title> Smoothing noisy data with spline functions. </title> <journal> Numer. Math. </journal> <volume> 31 </volume> <pages> 377-403. </pages>
Reference-contexts: There are many variants of this basic approach, including the minimum description length principle (Rissanen 1986), "Bayesian" maximum a posteriori selection, structural risk minimization (Vapnik 1982; 1996), "generalized" cross valida tion <ref> (Craven & Wahba 1979) </ref> (different from real cross validation; below), and regularization (Moody 1992). These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is the same. The other most common strategy is hold-out testing. <p> To determine the efficacy of TRI we compared its performance to a number of standard model selection strategies, including two well-known penalization strategies|generalized cross validation GCV <ref> (Craven & Wahba 1979) </ref> and structural risk minimization SRM (Vapnik 1996) (under the formulations reported in (Cherkassky, Mulier, & Vapnik 1996))|and 10-fold cross validation 10CV, a standard hold-out method (Efron 1979; Kohavi 1995).
Reference: <author> Efron, B. </author> <year> 1979. </year> <journal> Computers and the theory of statistics. SIAM Review 21 </journal> <pages> 460-480. </pages>
Reference: <author> Galarza, C.; Rietman, E.; and Vapnik, V. </author> <year> 1996. </year> <title> Applications of model selection techniques to polynomial approximation. </title> <type> Preprint. </type>
Reference: <author> Geman, S.; Bienenstock, E.; and Doursat, R. </author> <year> 1992. </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Comp. </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference-contexts: This, then, is the fundamental dilemma of machine learning: we need to make our hypothesis classes as expressive as possible to maximize our chances representing a good hypothesis, but we also need to restrict these classes to ensure that we can reliably distinguish good from bad hypotheses <ref> (Geman, Bienenstock, & Doursat 1992) </ref>. Thus, there is a tradeoff between our ability to represent a good function and our ability to identify a good function, if one exists. The question of what to do in the face of this dilemma dominates much of machine learning research. <p> Some progress in this direction is reported in a companion paper (Schuurmans, Ungar, & Foster 1997) which develops a general characterization of the difficulty of model selection problems based on the standard bias/variance decomposition of expected hypothesis er ror <ref> (Geman, Bienenstock, & Doursat 1992) </ref>. Here we characterize model selection problems by the shapes of their approximation-error and variance profiles, and use this to delineate the conditions where traditional techniques are most prone to catastrophic mistakes and where our techniques obtain their greatest advantage.
Reference: <author> Haussler, D. </author> <year> 1992. </year> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Infor. Comput. </journal> <volume> 100 </volume> <pages> 78-150. </pages>
Reference: <author> Heckerman, D., and Chickering, D. </author> <year> 1996. </year> <title> A comparison of scientific and engineering criteria for Bayesian model selection. </title> <type> Technical Report MSR-TR-96-12, </type> <institution> Microsoft Research. </institution>
Reference-contexts: For example, one could be interested in finding a simple model of the underlying phenomenon that gives some insight into its fundamental nature, rather than simply producing a function that predicts well on future test examples <ref> (Heckerman & Chickering 1996) </ref>. However, we will focus on the traditional machine learning goal of minimizing prediction error. 4 One could consider more elaborate strategies that choose hypotheses from outside the sequence; e.g., by averaging several hypotheses together (Opitz & Shavlik 1996; Breiman 1994).
Reference: <author> Kearns, M.; Mansour, Y.; Ng, A.; and Ron, D. </author> <year> 1995. </year> <title> An experimental and theoretical comparison of model selection methods. </title> <booktitle> In Proceedings COLT-95. </booktitle>
Reference-contexts: Nevertheless, applying our techniques to classification tasks is another important direction for future research. Here we hope to compare our results with the earlier study <ref> (Kearns et al. 1995) </ref>. Acknowledgements Much of this work was performed at the National Research Council Canada.
Reference: <author> Kohavi, R. </author> <year> 1995. </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In Proceedings IJCAI-95. </booktitle>
Reference: <author> Moody, J. </author> <year> 1992. </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <booktitle> In Proceedings NIPS-4. </booktitle>
Reference-contexts: There are many variants of this basic approach, including the minimum description length principle (Rissanen 1986), "Bayesian" maximum a posteriori selection, structural risk minimization (Vapnik 1982; 1996), "generalized" cross valida tion (Craven & Wahba 1979) (different from real cross validation; below), and regularization <ref> (Moody 1992) </ref>. These strategies differ in the specific complexity values they assign and the particular tradeoff function they optimize, but the basic idea is the same. The other most common strategy is hold-out testing.
Reference: <author> Opitz, D., and Shavlik, J. </author> <year> 1996. </year> <title> Generating accurate and diverse members of a neural-network ensemble. </title> <booktitle> In Proceedings NIPS-8. </booktitle>
Reference: <author> Pollard, D. </author> <year> 1984. </year> <title> Convergence of Stochastic Processes. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Rissanen, J. </author> <year> 1986. </year> <title> Stochastic complexity and modeling. </title> <journal> Ann. Statist. </journal> <volume> 14 </volume> <pages> 1080-1100. </pages>
Reference-contexts: There are many variants of this basic approach, including the minimum description length principle <ref> (Rissanen 1986) </ref>, "Bayesian" maximum a posteriori selection, structural risk minimization (Vapnik 1982; 1996), "generalized" cross valida tion (Craven & Wahba 1979) (different from real cross validation; below), and regularization (Moody 1992).
Reference: <author> Schaffer, C. </author> <year> 1993. </year> <title> Overfitting avoidance as bias. </title> <journal> Mach. Learn. </journal> <volume> 10(2) </volume> <pages> 153-178. </pages>
Reference-contexts: Hold-out methods implicitly take some of this information into account, but do so indirectly and less effectively than the metric-based strategies introduced here. Although there is no "free lunch" in general (Schaffer 1994) and we cannot claim to obtain a universal improvement for every model selection problem <ref> (Schaffer 1993) </ref>, we claim that one should be able to exploit additional information about the task (here knowledge of P X ) to obtain significant improvements across a wide range of problem types and conditions. Our empirical results for polynomial regression support this view.
Reference: <author> Schaffer, C. </author> <year> 1994. </year> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings ML-94. </booktitle>
Reference-contexts: Note that complexity-penalization strategies completely ignore this information, and as a result are heavily punished in our experiments. Hold-out methods implicitly take some of this information into account, but do so indirectly and less effectively than the metric-based strategies introduced here. Although there is no "free lunch" in general <ref> (Schaffer 1994) </ref> and we cannot claim to obtain a universal improvement for every model selection problem (Schaffer 1993), we claim that one should be able to exploit additional information about the task (here knowledge of P X ) to obtain significant improvements across a wide range of problem types and conditions.
Reference: <author> Schuurmans, D.; Ungar, L.; and Foster, D. </author> <year> 1997. </year> <title> Characterizing the generalization performance of model selection strategies. </title> <note> In Proceedings ML-97. To appear. </note>
Reference-contexts: Our empirical results for polynomial regression support this view. An important direction for future research is to develop theoretical support for our strategies. Some progress in this direction is reported in a companion paper <ref> (Schuurmans, Ungar, & Foster 1997) </ref> which develops a general characterization of the difficulty of model selection problems based on the standard bias/variance decomposition of expected hypothesis er ror (Geman, Bienenstock, & Doursat 1992). <p> However, as discussed in <ref> (Schuurmans, Ungar, & Foster 1997) </ref>, we do not expect to achieve as dramatic successes here, since classification involves a bounded loss which does not permit catastrophic errors (i.e., distances greater than 1). Nevertheless, applying our techniques to classification tasks is another important direction for future research.
Reference: <author> Vapnik, V. </author> <year> 1982. </year> <title> Estimation of Dependences Based on Empirical Data. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Vapnik, V. </author> <year> 1996. </year> <title> The Nature of Statistical Learning Theory. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To determine the efficacy of TRI we compared its performance to a number of standard model selection strategies, including two well-known penalization strategies|generalized cross validation GCV (Craven & Wahba 1979) and structural risk minimization SRM <ref> (Vapnik 1996) </ref> (under the formulations reported in (Cherkassky, Mulier, & Vapnik 1996))|and 10-fold cross validation 10CV, a standard hold-out method (Efron 1979; Kohavi 1995). <p> To determine the efficacy of TRI we compared its performance to a number of standard model selection strategies, including two well-known penalization strategies|generalized cross validation GCV (Craven & Wahba 1979) and structural risk minimization SRM (Vapnik 1996) (under the formulations reported in <ref> (Cherkassky, Mulier, & Vapnik 1996) </ref>)|and 10-fold cross validation 10CV, a standard hold-out method (Efron 1979; Kohavi 1995). We conducted a simple series of experiments by fixing a uniform domain distribution P X on the unit interval [0; 1], and then fixing various target functions f : [0; 1] ! IR.
Reference: <author> Weiss, S. M., and Kulikowski, C. A. </author> <year> 1991. </year> <title> Computer Systems that Learn. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Repeated testing in this manner does introduce some bias in the error esti mates, but the results are still generally better than considering a single hold-out partition <ref> (Weiss & Ku likowski 1991) </ref>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 h fl 4 h fl 0 set.
References-found: 20

