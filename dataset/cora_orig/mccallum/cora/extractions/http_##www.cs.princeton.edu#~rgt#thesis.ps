URL: http://www.cs.princeton.edu/~rgt/thesis.ps
Refering-URL: http://www.cs.princeton.edu/~rgt/abstract.html
Root-URL: http://www.cs.princeton.edu
Title: MIXTURE MODELS FOR NATURAL LANGUAGE PROCESSING  
Author: Robert G. Thomas III 
Degree: A DISSERTATION PRESENTED TO THE FACULTY  IN CANDIDACY FOR THE DEGREE OF DOCTOR OF PHILOSOPHY RECOMMENDED FOR ACCEPTANCE BY THE DEPARTMENT OF COMPUTER SCIENCE  
Date: June 1998  
Affiliation: OF PRINCETON UNIVERSITY  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> N. Abramson. </author> <title> Information Theory and Coding. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: Text compression can be viewed as a two-stage process, (1) building a model of the text and (2) encoding the text relative to the model. The second stage of the process has been "solved." Arithmetic coding <ref> [1, 24, 35, 39] </ref> allows the encoding of the message to approach arbitrarily close to the entropy of the text conditioned upon the model. Thus, the real challenge is to construct a good model of the text. <p> A basic Markov model = hn; A; ffii consists of a model order n, a finite alphabet A, and a state output probability function ffi : A n fi A ! <ref> [0; 1] </ref>. The model order dictates how many symbols in the past sequence condition the prediction. The state output probability function specifies the probability of a particular symbol given the past n symbols. Please examine figure 1 for a graphical representation of the model. <p> The lack of an expensive estimation procedure allows for online adaptation to be performed quickly. A backoff model = hn; A; ffii consists of a model order n, a finite alphabet A and a state output probability function ffi : A n fiA ! <ref> [0; 1] </ref> where ffi (jx m 8 : 1 ; ) if c (jx m ff (; x m 2 ) otherwise (7) where f is some function that determines the probability with which follows x m 1 contingent upon the fact that has followed x m 1 in the training <p> A mixture Markov model = hn; A; ffi; i consists of a model order n, a finite alphabet A, a set of state output probability functions ffi = fffi i : 0 i ng for each state order such that ffi i : A i fi A ! <ref> [0; 1] </ref>, and a set of mixture parameter functions that define the mixture parameters of , = f i : 0 i ng such that i : A i ! [0; 1]. <p> fffi i : 0 i ng for each state order such that ffi i : A i fi A ! <ref> [0; 1] </ref>, and a set of mixture parameter functions that define the mixture parameters of , = f i : 0 i ng such that i : A i ! [0; 1]. Under this paradigm, the ffi functions are usually the empirical distributions of the training corpus, and the parameters are adjusted so as to maximize the performance of the model on a withheld data set. <p> Inter-step transitions are allowed between a state of order less than or equal to n at step t to a state of any order less than or equal to n at step t + 1. : A n fi A n ! <ref> [0; 1] </ref>. Figure 4 demonstrates the general nonemitting model's parameter utilization. The probability that a state of order i, x t1 ti , is used at step t depends on the set of contexts that could have been utilized at step t 1. <p> = c' 9. return x Algorithm 5 General Nonuniform Model Generation Algorithm general-nonuniform-generate (T; ) 1. x = * 2. while jxj &lt; T 3. pick context length i in [0,min (t 1; n 1)] according to (x t1 Q j=min (t1;n) tj ) 4. pick prediction length j in <ref> [1, max (T-(t-1), n-i)] </ref> according to (j j x t1 Q j1 ti )) note: (max (T (t 1); n i) j x t1 ti ) = 1 5. pick symbols, j 1 , according to ffi ( j ti ; j; ) j 7. return x CHAPTER 2. <p> NATURAL LANGUAGE TEXT MODELING 30 Algorithm 6 Collapsed Nonuniform Model Generation Algorithm collapsed-nonuniform-generate (T; ) 1. x = * 2. while jxj &lt; T 3. pick context length i in [0,min ((t 1); n 1)] according to (x t1 Q min (t1;n) tj ) 4. pick prediction length j in <ref> [1, max (T-(t-1), n-i)] </ref> according to (1 (x t1 j Q j1 ti y k note: (x t1 ti y 1 ) = 0 5. pick symbols, j 1 , according to ffi ( j ti ; j; ) j 7. return x The collapsed nonuniform model's generation algorithm, algorithm 6, <p> A hidden alignment model = hA; B; ffi; i consists of a finite grapheme alphabet A, a finite phoneme alphabet B, a function ffi : fA fi B ? g ? ! <ref> [0; 1] </ref> that represents the probability that a orthographic string will have a particular aligned baseform, and a mixture parameter function : A ? ! [0; 1] that determines the probability that a given orthographic string is utilized to condition the baseform prediction. <p> ffi; i consists of a finite grapheme alphabet A, a finite phoneme alphabet B, a function ffi : fA fi B ? g ? ! <ref> [0; 1] </ref> that represents the probability that a orthographic string will have a particular aligned baseform, and a mixture parameter function : A ? ! [0; 1] that determines the probability that a given orthographic string is utilized to condition the baseform prediction. <p> Such a graph represents all possible overlapping paths given the model. In essence, it implicitly represents all possible baseforms that the model could generate for a given orthographic string. Assuming CHAPTER 3. PHONETIC BASEFORM GENERATION 131 that each partial alignment y has a (y) <ref> [0; 1] </ref> associated with it that defines the degree to which the model should use y in the solution, a scoring function can be defined over the paths.
Reference: [2] <author> Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. </author> <title> A Maximum Likelihood Approach to Continuous Speech Recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(2):179-190, </volume> <month> March </month> <year> 1983. </year>
Reference: [3] <author> L. Baum and J. Eagon. </author> <title> An Inequality with Applications to Statistical Estimation for Probabilistic Functions of a Markov Process and to Models for Ecology. Bull. </title> <booktitle> AMS 73 1967, </booktitle> <pages> 360-363. </pages>
Reference-contexts: Expectation Maximization The expectation maximization algorithm is used to estimate the mixture parameters of the mixture models considered in this chapter (see <ref> [3, 4] </ref> for a detailed description of the algorithm). The expectation maximization algorithm has two steps: the expectation step that calculates the expected number of times each hidden event occurs in the model, and the maximization step that estimates the mixture parameters based on the accumulated expectations.
Reference: [4] <author> L. Baum, T. Petrie, G. Soules and N. Weiss. </author> <title> A Maximization Technique Occurring in the Statistical Analysis of Proababilistic Functions of Markov Chains. </title> <journal> Ann. Math. Stat. </journal> <volume> 41 1970, </volume> <pages> 164-171. </pages>
Reference-contexts: Expectation Maximization The expectation maximization algorithm is used to estimate the mixture parameters of the mixture models considered in this chapter (see <ref> [3, 4] </ref> for a detailed description of the algorithm). The expectation maximization algorithm has two steps: the expectation step that calculates the expected number of times each hidden event occurs in the model, and the maximization step that estimates the mixture parameters based on the accumulated expectations.
Reference: [5] <author> Timothy C. Bell, John G. Cleary and Ian H. Witten. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <address> NJ, </address> <year> 1990. </year>
Reference-contexts: We discuss three here: speech recognition, text compression, and spell checking. While these three applications are extremely useful and important, other applications may also require an accurate natural language text model, e.g. text routing/classification, predicting user input <ref> [5] </ref>, and automatic user customization. We also introduce the problem of generating phonetic representations, known as baseforms, for proper names. Successful models of baseforms would allow such applications as voice-dialing to be realized. Speech Recognition The goal of speech recognition is to transcribe an acoustic signal to the intended text. <p> Nonetheless, our efficiency claims rely on the fact that an actual implementation would contain a re-written version of the product assignment. 2.4.1 String Generation Algorithms Each of the natural language models considered in this chapter can be repeatedly sampled to generate a string. Cleary, Bell, and Whitten <ref> [5] </ref> generated text randomly with character-based basic Markov models of various orders and we include their results to give a feel for how model order impacts model quality. * Order 0 model: fsn'iaad ir Intns hynci,.aais oayimh t n .at oeotc fheotyi t afrtgt oidtsO, wrr thraeoe rdaFr ce.g psNo is.emahntawe,ei <p> The family of Ziv and Lempel algorithms are dictionary-based compression approaches that are very popular primarily due the excellent real-world compression performance achieved with favorable time and space complexities. See Cleary, Bell, and Witten for a good in-depth description of many of the LZ algorithms <ref> [5] </ref>. The gzip algorithm encodes both the model and the uncompressed message in the compressed file, as any real-world compression system must.
Reference: [6] <author> Edward A. Bender and S. Gill Williamson. </author> <booktitle> Foundations of Applied Combinatorics. </booktitle> <publisher> Addison-Wesley, </publisher> <address> CA, </address> <year> 1991. </year>
Reference: [7] <author> Adam L. Berger, Stephen A. Della Peitra and Vincent J. Della Pietra. </author> <title> A Maximum Entropy Approach to Natural Language Processing. </title> <journal> Computational Linguistics, </journal> <volume> vol. 22, num 1, </volume> <year> 1996. </year>
Reference: [8] <author> Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Jennifer C. Lai and Robert L. Mercer. </author> <title> An Estimate of an Upper Bound for the Entropy of English. </title> <journal> Computational Linguistics, </journal> <volume> vol. </volume> <pages> 18, </pages> <note> num 1, 1992. 148 BIBLIOGRAPHY 149 </note>
Reference: [9] <author> Roy J. Byrd and Martin S. Chodorow. </author> <title> Using an On-line Dictionary to Find Rhyming Words and Pronunciations for Unknown Words. </title> <booktitle> Proc. 23rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1985. </year>
Reference: [10] <author> Stanley Chen and Joshua Goodman. </author> <title> An Empirical Study of Smoothing Techniques for Language Modeling. </title> <booktitle> Proc. of 34th ACL, </booktitle> <year> 1996. </year>
Reference-contexts: A different approach is to condition the current prediction on x t1 tn and the n proper suffixes of x t1 tn . A mixture Markov model combines information from n + 1 separate conditional distributions to determine P (x t j x t1 tn ; ). <ref> [27, 10, 26, 36, 46, 50, 38, 33] </ref> Formally, a CHAPTER 2.
Reference: [11] <author> Kenneth W. Church and William A. Gale. </author> <title> A Comparision of the Enhanced Good-Turing and Deleted Estimation Methods for Estimating Probabilities of English Bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol 5, </volume> <year> 1991. </year>
Reference: [12] <author> J. Cleary and I. Witten. </author> <title> Data Compression using Adaptive Coding and Partial String Matching. </title> <journal> IEEE Trans. Comm., COM-32, </journal> <volume> 4, </volume> <year> 1984. </year>
Reference-contexts: This method is referred to as Laplace's Law of Succession.[32] 2.3.2 Backoff Models An alternative solution to the zero-frequency problem is to allow the model to "back off" to lower order states when a higher order state has not witnessed a particular event. <ref> [12, 30, 41, 34] </ref> Such a model is called, not surprisingly, a backoff model. Backoff models are attractive because they are computationally efficient. The parameters of a backoff model are usually estimated directly from the frequency statistics of the training text.
Reference: [13] <author> Cecil H. Coker, Kenneth W. Church, and Mark Y. Liberman. </author> <title> Morphology and Rhyming: Two Powerful Alternatives to Letter-to-Sound Rules. </title> <booktitle> Conference on Speech Synthesis. </booktitle> <address> Autrans, France, </address> <year> 1990. </year>
Reference: [14] <author> Thomas M. Cover and Roger C. King. </author> <title> A Convergent Gambling Estimate of the Entropy of English. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> Vol. IT-24, No. 4, </volume> <month> July </month> <year> 1978. </year>
Reference: [15] <author> J.N. Darroch and D. Ratcliff. </author> <title> Generalized Iterative Scaling for Log-Linear Models. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 43(5): </volume> <year> 1470-1480,1972. </year>
Reference: [16] <author> Carl de Marcken. </author> <title> Unsupervise Language Acquisition. </title> <type> Ph.D. Dissertation, </type> <institution> MIT, </institution> <year> 1996. </year>
Reference: [17] <author> Michael J. Dedina and Howard C. Nusbaum. </author> <title> PRONOUNCE: a Program for Pronunciation by Analogy. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 55-64, </pages> <year> 1991. </year>
Reference-contexts: In the next subsection we introduce the terminology and formalisms used throughout the chapter. This subsection is followed by a discussion of the models proposed by Dedina and Nusbaum <ref> [17] </ref> and Yvon [61] for automatic baseform generation. From a high-level view, both of these models construct baseforms by exploiting known pronunciations of similar words in a pre-constructed dictionary of orthographic strings aligned with their respective baseforms. <p> As such, it is necessary for the engineer to postulate a possible generative process which manifests itself in a generative model. We now examine two analogy-based algorithms that motivate our stochastic model. Dedina and Nusbaum Dedina and Nusbaum <ref> [17] </ref> generate a baseform for an unknown word, w, by exploiting orthographic entries in D that are similar to w.
Reference: [18] <author> Sabine Deligne, Frederic Bimbot and Fran~cois Yvon. </author> <title> Phonetic Transcription by Variable Length Sequences: </title> <booktitle> Joint Multigrams. Proc. European Conference on Speech Communication and Technology (Eurospeech). </booktitle> <address> Madrid, </address> <month> Sep </month> <year> 1995. </year> <note> BIBLIOGRAPHY 150 </note>
Reference: [19] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum-likelihood from Incom-pleted Data via the EM Algorithm. </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference: [20] <author> W.N. Francis and H. Kucera. </author> <title> Frequency Analysis of English Usage: Lexicon and Grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> MA. </address>
Reference: [21] <author> I.J. </author> <title> Good. The Population Frequencies of Species and the Estimation of Population Parameters. </title> <journal> Biometrika, </journal> <volume> vol. </volume> <pages> 40, </pages> <note> num. 3 and 4, </note> <year> 1953. </year>
Reference: [22] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference: [23] <author> X.D. Huang, Y. Ariki, and M.A. Jack. </author> <title> Hidden Markov Models for Speech Recognition. </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference: [24] <author> F. Jelinek. </author> <title> Probabilistic Information Theory. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: Text compression can be viewed as a two-stage process, (1) building a model of the text and (2) encoding the text relative to the model. The second stage of the process has been "solved." Arithmetic coding <ref> [1, 24, 35, 39] </ref> allows the encoding of the message to approach arbitrarily close to the entropy of the text conditioned upon the model. Thus, the real challenge is to construct a good model of the text.
Reference: [25] <author> F. Jelinek, J.D. Lafferty and R.L. Mercer. </author> <title> Basic Methods of Probabilistic Context Free Grammars. Speech Recognition and Understanding: </title> <booktitle> Recent Advances, NATO ASI Series, </booktitle> <volume> Vol. F 75, </volume> <editor> Edited by P. Laface and R. De Mori. </editor> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference: [26] <author> Frederick Jelinek, Robert L. Mercer and Salim Roukos. </author> <title> Principles of Lexical Language Modeling for Speech Recognition. </title>
Reference-contexts: A different approach is to condition the current prediction on x t1 tn and the n proper suffixes of x t1 tn . A mixture Markov model combines information from n + 1 separate conditional distributions to determine P (x t j x t1 tn ; ). <ref> [27, 10, 26, 36, 46, 50, 38, 33] </ref> Formally, a CHAPTER 2.
Reference: [27] <author> F. Jelinek and R.L. Mercer. </author> <title> Interpolated Estimation of Markov Source Parameters from Sparse Data. Pattern Recognition in Practice (Amsterdam, 1970), </title> <editor> E. S. Gelsema and L. N. Kanal, Eds., </editor> <publisher> North Holland, </publisher> <pages> 381-397. </pages>
Reference-contexts: A different approach is to condition the current prediction on x t1 tn and the n proper suffixes of x t1 tn . A mixture Markov model combines information from n + 1 separate conditional distributions to determine P (x t j x t1 tn ; ). <ref> [27, 10, 26, 36, 46, 50, 38, 33] </ref> Formally, a CHAPTER 2. <p> The mixture Markov model differs from the basic Markov model in that multiple histories are used to assign probability to the next symbol whereas the basic Markov model uses only one history. We consider five mixture Markov models in this chapter: the classic model <ref> [27] </ref>, the hierarchical nonemitting model [46], the general nonemitting model, the general nonuniform model, and the collapsed nonuniform model.[50] The classic mixture Markov model was proposed by Jelinek and Mercer in 1980. The remaining four models were devised by Ristad and Thomas. <p> Although many linguists would correctly assert that these mixture model classes are incapable of modeling certain linguistic structures, the models, nonetheless, are a useful engineering tool that adequately model many real-world situations. Our results demonstrate that the classic natural language text model, proposed by Jelinek and Mercer <ref> [27] </ref>, can be improved upon by utilizing alternative interpretations of the mixture parameters. <p> In addition, these experiments helped to investigate the questions of how to tie parameters, which withholding algorithm to use, and which model order is best? Model Class The empirical evaluation of the model classes clearly show that the popular classic model <ref> [27] </ref> can be improved upon by utilizing alternative model classes. Although the hierarchical nonemitting model and the collapsed nonuniform model need more time and space to function, they consistently outperformed the classic model. Overall, the results strongly motivate the use of the hierarchical nonemitting model in all experimental situations.
Reference: [28] <author> Philip N. Johnson-Laird. </author> <title> The Computer and the Mind: An Introduction to Cognitive Science. </title> <publisher> Harvard University Press, </publisher> <address> MA, </address> <year> 1988. </year>
Reference: [29] <author> Philip N. Johnson-Laird. </author> <title> Mental Models: Towards a Cognitive Science of Language, Inference, </title> <publisher> and Consciousness Harvard University Press, </publisher> <address> MA, </address> <year> 1983. </year> <note> BIBLIOGRAPHY 151 </note>
Reference: [30] <author> S. Katz. </author> <title> Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. </title> <journal> IEEE Trans. ASSP 35 1987, </journal> <pages> 400-401. </pages>
Reference-contexts: This method is referred to as Laplace's Law of Succession.[32] 2.3.2 Backoff Models An alternative solution to the zero-frequency problem is to allow the model to "back off" to lower order states when a higher order state has not witnessed a particular event. <ref> [12, 30, 41, 34] </ref> Such a model is called, not surprisingly, a backoff model. Backoff models are attractive because they are computationally efficient. The parameters of a backoff model are usually estimated directly from the frequency statistics of the training text.
Reference: [31] <author> T. Kohonen. </author> <title> Dynamically Expanding Context, with Application to the Correction of Symbol Strings in the Recognition of Continuous Speech. </title> <booktitle> Proc. 8th International Conference on Pattern Recognition, </booktitle> <year> 1986. </year>
Reference: [32] <author> P.S. </author> <title> Laplace. Philosophical Essay on Probabilities. </title> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <title> Translated by A.I. Dale from 5th French edition of 1825. </title>
Reference: [33] <author> D.J. MacKay and L.C. Peto. </author> <title> A Hierarchical Dirichlet Language Model. </title> <booktitle> Natural Language Engineering 1,1994. </booktitle>
Reference-contexts: A different approach is to condition the current prediction on x t1 tn and the n proper suffixes of x t1 tn . A mixture Markov model combines information from n + 1 separate conditional distributions to determine P (x t j x t1 tn ; ). <ref> [27, 10, 26, 36, 46, 50, 38, 33] </ref> Formally, a CHAPTER 2.
Reference: [34] <author> A. Moffat. </author> <title> A Note on the PPM Data Compression Algorithm. </title> <type> Research Report 88/7, </type> <institution> Departmnet of Computer Science, University of Melbourne, </institution> <address> Parkville, Vic-toria, Australia, </address> <year> 1988. </year>
Reference-contexts: This method is referred to as Laplace's Law of Succession.[32] 2.3.2 Backoff Models An alternative solution to the zero-frequency problem is to allow the model to "back off" to lower order states when a higher order state has not witnessed a particular event. <ref> [12, 30, 41, 34] </ref> Such a model is called, not surprisingly, a backoff model. Backoff models are attractive because they are computationally efficient. The parameters of a backoff model are usually estimated directly from the frequency statistics of the training text.
Reference: [35] <author> R. </author> <title> Pasco. Source Coding Algorithms for Fast Data Compression. </title> <type> Ph.D. Thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <year> 1976. </year>
Reference-contexts: Text compression can be viewed as a two-stage process, (1) building a model of the text and (2) encoding the text relative to the model. The second stage of the process has been "solved." Arithmetic coding <ref> [1, 24, 35, 39] </ref> allows the encoding of the message to approach arbitrarily close to the entropy of the text conditioned upon the model. Thus, the real challenge is to construct a good model of the text.
Reference: [36] <author> G. Potamianos and F. Jelinek. </author> <title> The Study of N-gram and Decision Tree Letter Language Modeling Methods. </title> <type> Technical Report, </type> <institution> Center for Language and Speech Processing, The Johns Hopkins University, </institution> <year> 1997. </year>
Reference-contexts: A different approach is to condition the current prediction on x t1 tn and the n proper suffixes of x t1 tn . A mixture Markov model combines information from n + 1 separate conditional distributions to determine P (x t j x t1 tn ; ). <ref> [27, 10, 26, 36, 46, 50, 38, 33] </ref> Formally, a CHAPTER 2.
Reference: [37] <author> T. Reinhardt. </author> <title> Anaphora and Semantic Interpretation. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1983. </year>
Reference-contexts: These experiments strongly suggest that the modeler must be attentive to detail as many modeling decisions can have a significant impact on the model's predictive acumen. Although finite order mixture models are provably incapable of capturing certain linguistic phenomenon like anaphora <ref> [37] </ref>, a successful model is, nonetheless, able to assign accurate probabilities to many strings drawn from the natural language.
Reference: [38] <author> G. Riccardi, R. Pieraccini, and E. Bocchieri. </author> <title> Stochastic Automata for Language Modeling. </title> <booktitle> Computer Speech and Language 10, </booktitle> <year> 1996. </year>
Reference-contexts: A different approach is to condition the current prediction on x t1 tn and the n proper suffixes of x t1 tn . A mixture Markov model combines information from n + 1 separate conditional distributions to determine P (x t j x t1 tn ; ). <ref> [27, 10, 26, 36, 46, 50, 38, 33] </ref> Formally, a CHAPTER 2.
Reference: [39] <author> J.J. Rissanen. </author> <title> Generalized Kraft Inequality and Arithmetic Coding. </title> <journal> IBM J. Research and Development, </journal> <volume> 20, </volume> <pages> 198-203, </pages> <month> May </month> <year> 1976. </year>
Reference-contexts: Text compression can be viewed as a two-stage process, (1) building a model of the text and (2) encoding the text relative to the model. The second stage of the process has been "solved." Arithmetic coding <ref> [1, 24, 35, 39] </ref> allows the encoding of the message to approach arbitrarily close to the entropy of the text conditioned upon the model. Thus, the real challenge is to construct a good model of the text.
Reference: [40] <author> J. Rissanen. </author> <title> Probability Estimation for Symbols Observed or Not. </title> <booktitle> International Symposium on Information Theory, </booktitle> <address> (Quebec, CN), </address> <year> 1983. </year> <note> BIBLIOGRAPHY 152 </note>
Reference: [41] <author> J. Rissanen. </author> <title> A Universal Data Compression System. </title> <journal> IEEE Trans. Inform. Theory IT-29, </journal> <volume> 5, </volume> <year> 1983. </year>
Reference-contexts: This method is referred to as Laplace's Law of Succession.[32] 2.3.2 Backoff Models An alternative solution to the zero-frequency problem is to allow the model to "back off" to lower order states when a higher order state has not witnessed a particular event. <ref> [12, 30, 41, 34] </ref> Such a model is called, not surprisingly, a backoff model. Backoff models are attractive because they are computationally efficient. The parameters of a backoff model are usually estimated directly from the frequency statistics of the training text.
Reference: [42] <author> J. Rissanen. </author> <title> Complexity of Strings in the Class of Markov Sources. </title> <journal> IEEE Trans. Inform. Theory IT-32, </journal> <volume> 4, </volume> <year> 1986. </year>
Reference: [43] <author> Eric Sven Ristad. </author> <title> The Language Complexity Game. </title> <publisher> MIT Press, </publisher> <address> MA, </address> <year> 1993. </year>
Reference: [44] <author> Eric Sven Ristad and Peter N. Yianilos. </author> <title> Library of Practical Abstractions. </title>
Reference: [45] <author> Eric Sven Ristad. </author> <title> A Natural Law of Succession. </title> <type> Technical Report CS-TR-495-95, </type> <institution> Princeton University, </institution> <year> 1995. </year>
Reference: [46] <author> Eric Sven Ristad and Robert G. Thomas. </author> <title> Hierarchical Non-emitting Models. </title> <booktitle> Proc. of 35th ACL, </booktitle> <year> 1997. </year>
Reference-contexts: A different approach is to condition the current prediction on x t1 tn and the n proper suffixes of x t1 tn . A mixture Markov model combines information from n + 1 separate conditional distributions to determine P (x t j x t1 tn ; ). <ref> [27, 10, 26, 36, 46, 50, 38, 33] </ref> Formally, a CHAPTER 2. <p> The mixture Markov model differs from the basic Markov model in that multiple histories are used to assign probability to the next symbol whereas the basic Markov model uses only one history. We consider five mixture Markov models in this chapter: the classic model [27], the hierarchical nonemitting model <ref> [46] </ref>, the general nonemitting model, the general nonuniform model, and the collapsed nonuniform model.[50] The classic mixture Markov model was proposed by Jelinek and Mercer in 1980. The remaining four models were devised by Ristad and Thomas. <p> Thus 0 n 0 &gt; 1; 9x 2 A n 0 +1 [P h (xj) 6= P b (xj 0 )]. 2 Theorem 2 The class of hierarchical nonemitting models is strictly more powerful than the class of basic Markov models Proof: By lemmas 3 and 4. <ref> [46] </ref> We may then conclude that the hierarchical nonemitting model class is also more powerful than the class of classic mixture Markov models. Corollary 1 The class of hierarchical nonemitting mixture Markov models is strictly more powerful than the class of classic mixture Markov models. CHAPTER 2. <p> For example, both the hierarchical nonemitting and collapsed nonuniform model classes consistently outperformed the classic model in empirical evaluations and Ristad and Thomas have proven that the hierarchical nonemitting model is strictly more powerful than the classic model from a theoretical point of view <ref> [46] </ref>. Our empirical results also elucidate the impact of important model decisions such as parameter tying scheme, withholding algorithm, model order, and training corpus. The hidden alignment model was proposed in chapter three as a model class for generating phonetic baseforms.
Reference: [47] <author> Eric Sven Ristad and Robert G. Thomas. </author> <title> Hierarchical Non-emitting Models. </title> <type> Technical Report CS-TR-544-97, </type> <year> 1997. </year>
Reference: [48] <author> Eric Sven Ristad and Robert G. Thomas. </author> <title> Nonmonotonic Extension Models. </title> <type> Technical Report CS-TR-486-95, </type> <institution> Princeton University, </institution> <year> 1995. </year>
Reference: [49] <author> Eric Sven Ristad and Robert G. Thomas. </author> <title> Nonuniform Markov Models. </title> <type> Technical Report CS-TR-536-96, </type> <institution> Princeton University, </institution> <year> 1995. </year>
Reference: [50] <author> Eric Sven Ristad and Robert G. Thomas. </author> <title> Nonuniform Markov Models. </title> <booktitle> IEEE ICAASP Conference, </booktitle> <year> 1997. </year>
Reference-contexts: A different approach is to condition the current prediction on x t1 tn and the n proper suffixes of x t1 tn . A mixture Markov model combines information from n + 1 separate conditional distributions to determine P (x t j x t1 tn ; ). <ref> [27, 10, 26, 36, 46, 50, 38, 33] </ref> Formally, a CHAPTER 2.
Reference: [51] <author> Eric Sven Ristad and Robert G. Thomas. </author> <title> New Techniques for Context Modeling. </title> <booktitle> Proc. of 33rd ACL, </booktitle> <year> 1995. </year>
Reference: [52] <author> Eric Sven Ristad and Robert G. Thomas. </author> <title> Context Models in the MDL Framework. </title> <booktitle> IEEE Data Compression Conference, </booktitle> <year> 1995. </year>
Reference: [53] <author> Eric Sven Ristad and Peter N. Yianilos. </author> <title> Finite Growth Models. </title> <type> Technical Report CS-TR-533-96, </type> <institution> Princeton University, </institution> <year> 1996. </year> <note> BIBLIOGRAPHY 153 </note>
Reference-contexts: The value of a source-sink path is the product of the weights along it and the FGM's value is the sum over all such paths of their values <ref> [53] </ref>." Since the estimation problem for the mixture models considered in this thesis may be reduced to the estimation problem for finite growth models, by inspection, we may conclude that EM improves or maintains the current parameter set relative to L.
Reference: [54] <author> Roni Rosenfeld. </author> <title> Adaptive Statistical Language Modeling: A Maximum Entropy Approach. </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Department, Carnegie Mellon University, CMU-CS-94-138, </institution> <year> 1994. </year>
Reference: [55] <author> T. Sejnowski and C.R. Rosenberg. </author> <title> Parallel Networks that Learn to Pronounce English Text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference: [56] <author> C.E. Shannon. </author> <title> A Mathematical Theory of Communication. </title> <journal> Bell System Technical Journal, </journal> <volume> 27, </volume> <pages> 398-403, </pages> <month> July, </month> <year> 1948. </year>
Reference: [57] <author> C.E. Shannon. </author> <title> Prediction and Entropy of Printed English. </title> <journal> Bell System Technical Journal, </journal> <pages> 50-64, </pages> <year> 1951. </year>
Reference-contexts: H (Y j ) &lt; H (Y j 0 ) implies that is a better model of Y because betters models the "statistical structure" of Y . <ref> [57] </ref> If Y is a very long representative string of the natural language, H (Y j ) &lt; H (Y j 0 ) also strengthens the assertion that is better than 0 at modeling the particular natural language.
Reference: [58] <author> Daniel D.K. Sleator and Davy Temperley. </author> <title> Parsing English with a Link Grammar. </title> <type> Technical Report CMU-CS-91-196, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year>
Reference: [59] <author> Kari Torkkola. </author> <title> An Efficient Way to Learn English Grapheme-to-Phoneme Rules Automatically. </title> <booktitle> Proc. International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> vol. 2, </volume> <year> 1993. </year>
Reference: [60] <author> Peter N. Yianilos. </author> <title> Topics in Computational Hidden State Modeling. </title> <type> Ph.D. Dissertation. </type> <institution> Princeton University Computer Science Department, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: Yianilos <ref> [60] </ref> proves that the expectation maximization estimate ^ satisfies P (Lj ^ ) P (Lj) for finite growth models.
Reference: [61] <author> Fran~cois Yvon. </author> <title> Grapheme-to-Phoneme Conversion Using Multiple Unbounded Overlapping Chunks. </title> <booktitle> Proc. Conference on New Methods in Natural Language Processing, </booktitle> <year> 1996. </year>
Reference-contexts: The third chapter of this thesis develops a probabilistic model of baseform generation. Partially motivated by the substantial successes of probabilistic models in other natural language processing domains, the model places a successful non-probabilistic model developed by Yvon <ref> [61] </ref> in a probabilistic framework. The model, referred to as CHAPTER 1. INTRODUCTION 8 the hidden alignment model, generates baseforms by analogy: utilizing large, overlapping substring matches between the given word and words in a training dictionary of ASCII strings aligned with their baseform. <p> The third chapter examines the problem of generating a phonetic baseform for an arbitrary orthographic string. The hidden alignment model is outlined and compared to a nonstochastic model proposed by Yvon. <ref> [61] </ref> The empirical evaluation of the models shows that Yvon's model is better than the hidden alignment model. We discuss why this is true and offer suggestions for future research. The concluding chapter reiterates the results presented in chapters two and three, offering a more general overview of this work. <p> In the next subsection we introduce the terminology and formalisms used throughout the chapter. This subsection is followed by a discussion of the models proposed by Dedina and Nusbaum [17] and Yvon <ref> [61] </ref> for automatic baseform generation. From a high-level view, both of these models construct baseforms by exploiting known pronunciations of similar words in a pre-constructed dictionary of orthographic strings aligned with their respective baseforms. <p> The language user generates a baseform for an arbitrary orthographic string by utilizing known alignments in their lexicon. The desire to favor minimum edge paths is sensible since the likelihood of mistakes is probably greatest at the point of overlap <ref> [61] </ref>. Yvon Francois Yvon [61] extended Dedina and Nusbaum's work by considering multiple symbol overlaps. In line 7 of the best-path function, Dedina and Nusbaum impose the overlap-by-one constraint. Yvon relaxes this constraint by allowing for alignments that overlap by more than one symbol. <p> The language user generates a baseform for an arbitrary orthographic string by utilizing known alignments in their lexicon. The desire to favor minimum edge paths is sensible since the likelihood of mistakes is probably greatest at the point of overlap <ref> [61] </ref>. Yvon Francois Yvon [61] extended Dedina and Nusbaum's work by considering multiple symbol overlaps. In line 7 of the best-path function, Dedina and Nusbaum impose the overlap-by-one constraint. Yvon relaxes this constraint by allowing for alignments that overlap by more than one symbol. <p> The hidden alignment model was proposed in chapter three as a model class for generating phonetic baseforms. Although the model class is probabilistic in nature, 133 CHAPTER 4. CONCLUSION 134 a simpler model proposed by Yvon <ref> [61] </ref> outperforms the hidden alignment model. These results are disappointing and we identify several veins of future research in this area. 4.1 Natural Language Text Modeling Several important contributions were introduced in the natural language text modeling chapter of the thesis. <p> The contributions made in that chapter are now discussed. 4.2.1 Novel Model Class We provide a new model class, the hidden alignment model, that couches the generative process of baseforms in probabilistic terms, Partially motivated by a non-stochastic model proposed by Yvon <ref> [61] </ref>, the hidden alignment model can be used to generate a baseform for orthographic strings. In chapter three, we provide string generation, parameter estimation, and probability evaluation algorithms for the hidden alignment model.
Reference: [62] <author> Fran~cois Yvon. </author> <title> Prononcer Par Analogie: </title> <type> Motivations, </type> <institution> Formalisations, et Evaluations These de Doctorat de l'Ecole Nationale Superieure des Telecommunications, </institution> <address> ENST E 015, Paris (France), </address> <year> 1996. </year>
Reference: [63] <author> Fran~cois Yvon. </author> <title> Paradigmatic Cascades: a Linguistically Sound Model of Pronunciation by Analogy. </title> <booktitle> Proc. 35th annual meeting of the Association for Computational Linguistic, </booktitle> <address> Madrid, July 97. BIBLIOGRAPHY 154 </address>
Reference: [64] <author> J. Ziv and A. Lempel. </author> <title> A Universal Algorithm for Sequential Data Compression. </title> <journal> IEEE Trans. Information Theory, </journal> <volume> IT-23 (3), </volume> <pages> 337-343, </pages> <year> 1977. </year>
Reference-contexts: NATURAL LANGUAGE TEXT MODELING 73 We now compare the compression ability of the readily available gzip to the best hierarchical nonemitting models we were able to construct for all three corpora. Gzip is based on the LZ77 coding scheme introduced by Jacob Ziv and Abraham Lempel in 1977 <ref> [64] </ref>. The family of Ziv and Lempel algorithms are dictionary-based compression approaches that are very popular primarily due the excellent real-world compression performance achieved with favorable time and space complexities. See Cleary, Bell, and Witten for a good in-depth description of many of the LZ algorithms [5].
References-found: 64

