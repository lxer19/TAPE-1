URL: http://www.cs.umn.edu/Users/dept/users/kumar/ijpp2.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: Arpanet: kumar@sally.utexas.edu  
Title: Parallel Depth First Search, Part II: Analysis  
Author: Vipin Kumar and V. Nageshwara Rao 
Note: This work was supported by Army Research Office grant DAAG29-84-K-0060 to the Artificial Intelligence Laboratory, and Office of Naval Research Grant N00014-86-K-0763 to the  
Address: Austin, Texas 78712  Austin.  
Affiliation: Department of Computer Sciences, University of Texas at Austin,  computer science department at the University of Texas at  
Abstract: This paper presents the analysis of a parallel formulation of depth-first search. At the heart of this parallel formulation is a dynamic work-distribution scheme that divides the work between different processors. The effectiveness of the parallel formulation is strongly influenced by the work-distribution scheme and the target architecture. We introduce the concept of isoefficiency function to characterize the effectiveness of different architectures and work-distribution schemes. Many researchers considered the ring architecture to be quite suitable for parallel depth-first search. Our analytical and experimental results show that hypercube and shared-memory architectures are significantly better. The analysis of previously known work-distribution schemes motivated the design of substantially improved schemes for ring and shared-memory architectures. In particular, we present a work-distribution algorithm which guarantees close to optimal performance on a shared-memory/!-network-with-message-combining architecture (e.g. RP3). Much of the analysis presented in this paper is applicable to other parallel algorithms in which work is dynamically shared between different processors (e.g., parallel divide-and-conquer algorithms). The concept of isoefficiency is 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Raphael A. Finkel and Udi Manber. </author> <title> Dib a distributed implementation of backtracking. </title> <journal> ACM Trans. of Progr. Lang. and Systems, </journal> <volume> 9 No. 2 </volume> <pages> 235-256, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Note that distributed-memory systems (including hypercubes) cannot obtain better isoefficiency curve than O (N 2 log N ) using this work-distribution scheme because of this communication bottleneck. Finkel and Manber discuss a number of different work-distribution scheme in their implementation of parallel depth-first search on the ring architecture <ref> [1] </ref>. In one of their schemes, each processor maintains a local variable, target, to point to a donor processor. target is incremented (modulo N) every time the processor seeks work. This can be viewed as an adaptation of our simple work-distribution scheme for the shared-memory architecture to the ring architecture. <p> Part of the analysis presented in Section 7 uses the same technique that Manber used for the analysis of interference. Manber's analysis served as a basis for the design of parallel depth-first search scheme presented in <ref> [1] </ref>. This scheme has a much better isoefficiency function (O (N 3 log W ) ) for the ring architecture than the one analyzed in Section 5. <p> In contrast, our second work-distribution method presented in Section 7 guarantees iso-efficiency function of O (N log N ) for shared-memory architectures with message combining. Furthermore, the constant factor in O (N log N ) is very small. Many researchers <ref> [14, 1, 8] </ref> have considered the ring architecture to be highly suitable for parallel depth-first search. Our analysis shows that the ring architecture (even with the best known work-distribution scheme) has much worse performance than the hypercube or shared-memory architectures. 10 Conclusions. <p> Since the isoefficiency function has to be at least linear, we can 21 Interconnection Diameter Isoefficiency Order of dependence Work-distribution scheme 1-ring N fi N Exponential Section5,Wah [14],Monien [8] 1-ring N N 3 log N Cubic Polynomial Finkel and Manber <ref> [1] </ref> 1-ring N N 2 log N Quad. Polynomial Section8 Hypercube log N N 1:57 Small Polynomial Section6 Shared-memory 1 N 2 fl log N Quad.
Reference: [2] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU ultracomputer designing a MIMD, shared memory parallel computer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32, No. 2 </volume> <pages> 175-189, </pages> <month> February </month> <year> 1983. </year> <month> 22 </month>
Reference-contexts: But for 15-puzzle, this limitation does not take effect for the range of processors we experimented with ( 120). On shared-memory/!-network architectures that use message combining (e.g. RP3 [10], the Ultracomputer <ref> [2] </ref>), this problem does not arise at all.
Reference: [3] <author> R. E. Korf. </author> <title> Depth-first iterative-deepening: An optimal admissible tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 97-109, </pages> <year> 1985. </year>
Reference-contexts: The development of these new schemes was motivated by the analysis of the earlier schemes. From our analysis, it is clear that on suitable architectures, it is feasible to speedup depth-first search by several orders of magnitude. Experimental validation of the analysis was done by parallelizing the IDA* algorithm <ref> [3, 4] </ref> to solve the 15-puzzle problem [9] on BBN Butterfly 1 , Intel Hypercube iPSC/1 2 and a ring embedded in the Intel Hypercube. Our analysis of parallel DFS is also applicable to other parallel algorithms in which work is shared dynamically among processors. <p> This is true of most practical problems solved by DFS. If the search space is not bounded (or is very deep), then simple DFS may never terminate (or take a very long time). Note that our analysis is applicable for iterative-deepening depth-first search algorithms (e.g., IDA* <ref> [3, 4] </ref>) even if the search space is not bounded. The reason is that each iteration of these algorithms performs depth-first search in a bounded part of the search space.
Reference: [4] <author> Richard Korf. </author> <title> Optimal path finding algorithms. </title> <editor> In L. N. Kanal and Vipin Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: The development of these new schemes was motivated by the analysis of the earlier schemes. From our analysis, it is clear that on suitable architectures, it is feasible to speedup depth-first search by several orders of magnitude. Experimental validation of the analysis was done by parallelizing the IDA* algorithm <ref> [3, 4] </ref> to solve the 15-puzzle problem [9] on BBN Butterfly 1 , Intel Hypercube iPSC/1 2 and a ring embedded in the Intel Hypercube. Our analysis of parallel DFS is also applicable to other parallel algorithms in which work is shared dynamically among processors. <p> This is true of most practical problems solved by DFS. If the search space is not bounded (or is very deep), then simple DFS may never terminate (or take a very long time). Note that our analysis is applicable for iterative-deepening depth-first search algorithms (e.g., IDA* <ref> [3, 4] </ref>) even if the search space is not bounded. The reason is that each iteration of these algorithms performs depth-first search in a bounded part of the search space.
Reference: [5] <author> T. H. Lai and Sartaj Sahni. </author> <title> Anomalies in parallel branch and bound algorithms. </title> <journal> Communications of the ACM, </journal> <pages> pages 594-602, </pages> <year> 1984. </year>
Reference-contexts: The reason is that each iteration of these algorithms performs depth-first search in a bounded part of the search space. To simplify the analysis (i.e., to avoid dealing with speedup anomalies <ref> [13, 5] </ref>) we assume that both sequential and parallel DFS search the whole bounded space for all solution paths. In the case of IDA* (sequential or parallel), it means that all optimal (i.e., least cost) solution paths need to be found.
Reference: [6] <author> J. Lee, E. Shragowitz, and S. Sahni. </author> <title> A hypercube algorithm for the 0/1 knapsack problem. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 699-706, </pages> <year> 1987. </year>
Reference-contexts: The rate of increase of W with respect to (w.r.t.) N is dependent upon the architecture and the work-distribution algorithm. In many parallel algorithms (e.g., parallel DFS, parallel 0/1 knapsack <ref> [6] </ref>, parallel algorithms for the shortest path problem [11], parallel quicksort [11]), it is possible to obtain linear speedup on arbitrarily many processors by simply increasing the problem size (i.e., the sequential execution time W ). The required rate of growth of W w.r.t. <p> The concept of isoefficiency is extremely useful in characterizing the scalability of parallel algorithms for which linear speedup for arbitrarily many processors can be obtained by simply increasing the problem size. For example, the isoefficiency function of the parallel algorithm for solving the 0/1 knapsack problem given <ref> [6] </ref> is O (N log N ); hence it is highly scalable. On the other hand, a frequently used parallel formulation of quicksort [11] has an exponential isoefficiency function, which means that the formulation is not capable of using many processors effectively.
Reference: [7] <author> Udi Manber. </author> <title> On maintaining dynamic information in a concurrent environment. </title> <journal> SIAM J. of Computing, </journal> <volume> 15 No. 4 </volume> <pages> 1130-1142, </pages> <year> 1986. </year>
Reference-contexts: The variable target is incremented (modulo N ) every time the processor seeks work. For this work-distribution algorithm, V (N ) = N 2 in the worst case. (This result was proved by Manber in a somewhat different context <ref> [7] </ref>). Thus from Equation 2, the isoefficiency function is O (N 2 log N ). In deriving this expression we assumed that U comm = O (1). <p> Isoefficiency functions of other two schemes of Finkel and Manber can be computed similarly, and are also O (N 3 log N ). 9 Related Research. Manber <ref> [7] </ref> has designed a data structure, called "concurrent pool" that can facilitate work sharing among concurrent processes, and can be incorporated in a parallel depth-first search formulation.
Reference: [8] <author> B. Monien and O. Vornberger. </author> <title> The ring machine. </title> <type> Technical report, </type> <institution> University of Paderborn, </institution> <address> FRG, </address> <year> 1985. </year> <note> Also in Computers and Artificial Intelligence, 3(1987). </note>
Reference-contexts: In contrast, our second work-distribution method presented in Section 7 guarantees iso-efficiency function of O (N log N ) for shared-memory architectures with message combining. Furthermore, the constant factor in O (N log N ) is very small. Many researchers <ref> [14, 1, 8] </ref> have considered the ring architecture to be highly suitable for parallel depth-first search. Our analysis shows that the ring architecture (even with the best known work-distribution scheme) has much worse performance than the hypercube or shared-memory architectures. 10 Conclusions. <p> We have introduced the concept of isoefficiency function to characterize the effectiveness of different architectures and work-distribution schemes. The work-distribution schemes used by earlier researchers for the ring architecture were found to be substantially inferior to the one presented in this paper. Furthermore, other researchers <ref> [14, 8] </ref> considered ring to be quite suitable for parallel depth-first search. Our analytical and experimental results show that hypercube and shared-memory architectures are significantly better. We presented a work-distribution algorithm for the shared-memory/!-network-with-message-combining architecture (e.g., RP3) which has better performance than previously known algorithms. <p> Since the isoefficiency function has to be at least linear, we can 21 Interconnection Diameter Isoefficiency Order of dependence Work-distribution scheme 1-ring N fi N Exponential Section5,Wah [14],Monien <ref> [8] </ref> 1-ring N N 3 log N Cubic Polynomial Finkel and Manber [1] 1-ring N N 2 log N Quad. Polynomial Section8 Hypercube log N N 1:57 Small Polynomial Section6 Shared-memory 1 N 2 fl log N Quad.
Reference: [9] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga, </publisher> <address> Palo Alto, CA, </address> <year> 1980. </year>
Reference-contexts: From our analysis, it is clear that on suitable architectures, it is feasible to speedup depth-first search by several orders of magnitude. Experimental validation of the analysis was done by parallelizing the IDA* algorithm [3, 4] to solve the 15-puzzle problem <ref> [9] </ref> on BBN Butterfly 1 , Intel Hypercube iPSC/1 2 and a ring embedded in the Intel Hypercube. Our analysis of parallel DFS is also applicable to other parallel algorithms in which work is shared dynamically among processors. Section 2 gives a brief review of a parallel formulation of DFS.
Reference: [10] <author> G. F. Pfister et al. </author> <title> The IBM research parallel processor prototype (RP3). </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 764-797, </pages> <year> 1985. </year>
Reference-contexts: This means that asymptotically, W should grow as O (N 2 log N ) to avoid contention for TARGET. But for 15-puzzle, this limitation does not take effect for the range of processors we experimented with ( 120). On shared-memory/!-network architectures that use message combining (e.g. RP3 <ref> [10] </ref>, the Ultracomputer [2]), this problem does not arise at all.
Reference: [11] <author> Michael J. Quinn. </author> <title> Designing Efficient Algorithms for Parallel Computers. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: The rate of increase of W with respect to (w.r.t.) N is dependent upon the architecture and the work-distribution algorithm. In many parallel algorithms (e.g., parallel DFS, parallel 0/1 knapsack [6], parallel algorithms for the shortest path problem <ref> [11] </ref>, parallel quicksort [11]), it is possible to obtain linear speedup on arbitrarily many processors by simply increasing the problem size (i.e., the sequential execution time W ). The required rate of growth of W w.r.t. <p> The rate of increase of W with respect to (w.r.t.) N is dependent upon the architecture and the work-distribution algorithm. In many parallel algorithms (e.g., parallel DFS, parallel 0/1 knapsack [6], parallel algorithms for the shortest path problem <ref> [11] </ref>, parallel quicksort [11]), it is possible to obtain linear speedup on arbitrarily many processors by simply increasing the problem size (i.e., the sequential execution time W ). The required rate of growth of W w.r.t. <p> For example, the isoefficiency function of the parallel algorithm for solving the 0/1 knapsack problem given [6] is O (N log N ); hence it is highly scalable. On the other hand, a frequently used parallel formulation of quicksort <ref> [11] </ref> has an exponential isoefficiency function, which means that the formulation is not capable of using many processors effectively.
Reference: [12] <author> V. Nageshwara Rao and V. Kumar. </author> <title> Parallel depth-first search, part I: Implementation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):479-499, December 1987. 
Reference-contexts: 1 Introduction This paper presents the analysis of our parallel formulation of depth-first search presented in <ref> [12] </ref>. At the heart of this parallel formulation is a dynamic work-distribution scheme that divides the work between different processors. <p> Each processor maintains its own local stack on which it executes DFS. When its local stack is empty, the processor tries to get some of the untried alternatives from the stack of another processor. In the formulation we implemented in <ref> [12] </ref>, an idle processor tries to get work (in a round-robin fashion) only from its immediate neighbors; i.e., in a 2-ring, it can get work from any of its two neighbors; in a 1-ring, it can get work from only one neighbor; in a hypercube, it can get work from log <p> This assumption simply says that the splitting function is not unreasonable. All these assumptions are satisfied by the cost-bounded DFS (i.e., the last iteration of IDA*) presented in Section 4.4 of <ref> [12] </ref>. This algorithm was used to solve the 15-puzzle problem in all the experiments discussed in this paper. 3.2 Definitions 1. Problem size W : is the size of the space searched (in number of nodes) 2. <p> Unit Communication time U comm : is the mean time taken for getting some work (a stack) from a neighboring processor. U comm depends upon the size of the message transferred (which depends upon the actual splitting strategy used <ref> [12] </ref>), the distance between the donor and the requesting processors, and the communication speed of the 5 underlying hardware. For simplicity, in our analysis, we assume that the message size is fixed. <p> For simplicity, in our analysis, we assume that the message size is fixed. Even if we assume that the size of the message grows as O (log W ) (which is a better approximation for the splitting strategy used in our implementation in <ref> [12] </ref>), the results change only slightly. 4 The Isoefficiency Function The efficiency (and speedup) achieved in parallel DFS is determined by the architecture, the work-distribution algorithm, the number of processors and the problem size. <p> For a given problem size W , increasing the number of processors N causes the efficiency to decrease because T comm increases while T calc remains the same. For a fixed N , increasing W improves efficiency because T calc increases and (for the work-distribution schemes used in <ref> [12] </ref>) T comm does not increase proportionately. (For example, see the speedup curve for the Intel Hypercube in [12]). If N is increased, then we can keep the efficiency fixed (i.e., maintain the speedup to be linear) by increasing W . <p> For a fixed N , increasing W improves efficiency because T calc increases and (for the work-distribution schemes used in <ref> [12] </ref>) T comm does not increase proportionately. (For example, see the speedup curve for the Intel Hypercube in [12]). If N is increased, then we can keep the efficiency fixed (i.e., maintain the speedup to be linear) by increasing W . The rate of increase of W with respect to (w.r.t.) N is dependent upon the architecture and the work-distribution algorithm. <p> Since the value of T comm used in the analysis is only a lower bound, the actual isoefficiency function can be worse than exponential. This explains the poor performance of parallel DFS on large (&gt; 16 processors) 1-ring and 2-ring in <ref> [12] </ref>. Fig. 2 shows experimentally obtained isoefficiency curves of parallel DFS for 15-puzzle on a 1-ring embedded in the Intel Hypercube. Clearly these curves show exponential growth. Since N and W are plotted on logarithmic scales, a polynomial growth of W w.r.t. N would have resulted in a linear curve. <p> This is confirmed by our experiments with parallel DFS on 15-puzzle. Fig. 3 shows experimentally obtained isoefficiency curves for parallel DFS for the 15-puzzle problem on the Intel Hypercube. N and W are plotted on logarithmic scales. In these experiments, the third splitting strategy given in Section 3.2.1 of <ref> [12] </ref>, was used, which tries to keep fl close to 0.5. 5 Clearly the isoefficiency function even for this case has a polynomial growth. <p> work available in any processor is less than (1 ff)W After 2V (N ) requests, maximum work available in any processor is less than (1 ff) 2 W 5 Due to the nonuniform structure of the search tree, there is no guarantee that fl ' 0:5. 6 As discussed in <ref> [12] </ref>, untried alternatives are transferred from the stack of the donor processor to the requester processor only if they are above a user specified level called cutoff depth. This ensure that the size of the work given out by a donor is at least (roughly) b cutoff . <p> In the work-distribution scheme for the shared-memory architecture implemented in <ref> [12] </ref>, each processor maintains a local variable `target' to point to a donor processor. The variable target is incremented (modulo N ) every time the processor seeks work. <p> RP3 [10], the Ultracomputer [2]), this problem does not arise at all. In such systems, simultaneous atomic-add requests to TARGET are combined at intermediate nodes of !-network (where 7 This new work-distribution algorithm is obtained by replacing the second line of GETWORK () in <ref> [12] </ref> by the line "TARGET = atomic-add (I,1) mod N", and by replacing TARGET for target in the rest of the procedure. 15 120k 80k 40k 100 9080 70 60 50 40 302010 E = .82 E = .96 Number of processors N Problem Size W work-distribution scheme.
Reference: [13] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> Superlinear speedup in state-space search. </title> <booktitle> In Proceedings of the 1988 Foundation of Software Technology and Theoretical Computer Science, number 338 in Lecture Notes in Computer Science, </booktitle> <pages> pages 161-174. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: The reason is that each iteration of these algorithms performs depth-first search in a bounded part of the search space. To simplify the analysis (i.e., to avoid dealing with speedup anomalies <ref> [13, 5] </ref>) we assume that both sequential and parallel DFS search the whole bounded space for all solution paths. In the case of IDA* (sequential or parallel), it means that all optimal (i.e., least cost) solution paths need to be found. <p> In the case of IDA* (sequential or parallel), it means that all optimal (i.e., least cost) solution paths need to be found. The possibility of superlinear speedup in our parallel formulation of depth-first search is discussed in <ref> [13] </ref>. We assume that the effective branching factor (defined below) of the search space is greater then 1 + e (where e is an arbitrarily small positive constant).
Reference: [14] <author> Benjamin W. Wah and Y. W. Eva Ma. </author> <title> Manip amulticomputer architecture for solving combinatorial extremum-search problems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-33, </volume> <month> May </month> <year> 1984. </year>
Reference-contexts: In contrast, our second work-distribution method presented in Section 7 guarantees iso-efficiency function of O (N log N ) for shared-memory architectures with message combining. Furthermore, the constant factor in O (N log N ) is very small. Many researchers <ref> [14, 1, 8] </ref> have considered the ring architecture to be highly suitable for parallel depth-first search. Our analysis shows that the ring architecture (even with the best known work-distribution scheme) has much worse performance than the hypercube or shared-memory architectures. 10 Conclusions. <p> We have introduced the concept of isoefficiency function to characterize the effectiveness of different architectures and work-distribution schemes. The work-distribution schemes used by earlier researchers for the ring architecture were found to be substantially inferior to the one presented in this paper. Furthermore, other researchers <ref> [14, 8] </ref> considered ring to be quite suitable for parallel depth-first search. Our analytical and experimental results show that hypercube and shared-memory architectures are significantly better. We presented a work-distribution algorithm for the shared-memory/!-network-with-message-combining architecture (e.g., RP3) which has better performance than previously known algorithms.
References-found: 14

