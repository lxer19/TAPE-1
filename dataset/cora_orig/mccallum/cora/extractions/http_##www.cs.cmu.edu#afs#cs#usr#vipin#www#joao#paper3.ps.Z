URL: http://www.cs.cmu.edu/afs/cs/usr/vipin/www/joao/paper3.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/vipin/www/parest.html
Root-URL: 
Email: e-mail:biegler@cmu.edu  
Title: Data Reconciliation and Gross Error Detection for Dynamic Systems  
Author: Jo~ao S. Albuquerque Lorenz T. Biegler 
Note: Supported by a fellowship grant from Programa CI ENCIA, JNICT, Lisbon, Portugal. To whom all correspondence should be addressed.  
Date: March 29, 1996  
Address: Pittsburgh, PA 15232 USA  
Affiliation: Department of Chemical Engineering Carnegie Mellon University  
Abstract: Gross error detection plays a vital role in parameter estimation and data reconciliation for both dynamic and steady state systems. In particular, recent advances in process optimization now allow data reconciliation of dynamic systems and appropriate problem formulations need to be considered for them. Data errors due to either miscalibrated or faulty sensors or just random events nonrepresentative of the underlying statistical distribution, can induce heavy biases in the parameter estimates and in the reconciled data. In this paper we concentrate on robust estimators and exploratory statistical methods which allow us to detect the gross errors as the data reconciliation is performed. These robust methods have the property of being insensitive to departures from ideal statistical distributions and therefore are insensitive to the presence of outliers. Once the regression is done, the outliers can be detected readily by using exploratory statistical techniques. An important feature for performance of the optimization algorithm and uniqueness of the reconciled data is the ability to classify the variables according to their observability and redundancy properties. Here an observable variable is an unmeasured quantity which can be estimated from the measured variables through the physical model while a nonredundant variable is a measured variable which cannot be estimated other than through its measurements. Variable classification can be used as an aid to design instrumentation schemes. In this 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Albuquerque, J. S. , Biegler, L. T. </author> , <title> `Decomposition Algorithms for OnLine Estimation with Nonlinear DAE Models',accepted for publication in Comput. </title> <institution> Chem. Engng., </institution> <year> 1996. </year>
Reference-contexts: 1 Introduction While data reconciliation is a common tool for steady state systems, application to dynamic systems is still in its infancy [15, 22]. Moreover, the dynamic problem has some interesting characteristics related to the optimization formulation and interpretation of results. In a previous paper <ref> [1] </ref> we developed an efficient Successive Quadratic Programming (SQP) based method for data reconciliation and parameter estimation for systems described by Differential and Algebraic Equation Systems (DAE). <p> Here an estimation problem with constraints that have small curvature will always have a unique global solution and the GN approximation will be guaranteed to converge. Moreover, since the nonlinear data reconciliation problem is frequently solved with SQP, the GN approximation leads to significant improvements in the convergence rate <ref> [1, 27] </ref>. Unlike the contaminated normal objective, this is further improved by the Fair function since it always has positive curvature. <p> Note that the residuals for the regression with the Fair function seem to be more compressed around the origin, which would indicate that the Fair function picked a better choice of outliers than the other methods. 5.2 Two connected tanks In this example, which we treated previously <ref> [1] </ref> we have two tanks connected by a valve. The measured variables are the flows F 0 ; F 1 ; F 2 and the levels of liquid h 1 ; h 2 . <p> In the first experiment, we simulated the data with Normal noise along with large random shifts in the measured variables, except for F 0 . These shifts simulate the gross errors. We performed the regression with our SQP decomposition strategies <ref> [1] </ref> using the least squares function, the contaminated Normal [27] and the Fair function tuned for an asymptotic relative efficiency of 95% for the Normal distribution (c=1.3998, see equation (16)). However, on all three runs the least squares estimate was used for F 0 . <p> Although this problem is in steady-state, equations (62),(63) and (64) form a parabolic Partial Differential Equation which can be converted into an ODE system with initial conditions in t, using the Method of Weighted Residuals [2]. It can thus be solved efficiently by our decomposition techniques <ref> [1] </ref>. More details on how to treat distributed systems will be left to a future paper. The measured variable is the bulk concentration C a at the length L of the reactor.
Reference: [2] <author> Ames, W. F. </author> <title> ,`Numerical Methods for Partial Differential Equations', </title> <publisher> Academic Press, </publisher> <address> third edition,San Diego, </address> <year> 1992. </year>
Reference-contexts: Although this problem is in steady-state, equations (62),(63) and (64) form a parabolic Partial Differential Equation which can be converted into an ODE system with initial conditions in t, using the Method of Weighted Residuals <ref> [2] </ref>. It can thus be solved efficiently by our decomposition techniques [1]. More details on how to treat distributed systems will be left to a future paper. The measured variable is the bulk concentration C a at the length L of the reactor.
Reference: [3] <author> Basu, A. , Paliwal, K. K. </author> <title> ,`Robust M-Estimates and Generalized M-Estimates for Autoregressive Parameter Estimation',TENCON 89, </title> <booktitle> Fourth IEEE Region 10 International Conference, </booktitle> <address> Bombay, India, </address> <year> 1989. </year>
Reference-contexts: The most important for us are the M-Estimators, which are generalizations of the Maximum-Likelihood-Estimator. Their form is: min n X (~ i ; ) (13) Several estimates have been proposed in the literature <ref> [18, 11, 3] </ref> but to our knowledge only the Fair function is convex and has continuous first and second derivatives. Its form is: (~) = c 2 j~j log (1 + c where c is a tuning parameter.
Reference: [4] <author> Brenan, K. E. , Campbell, S. L. , Petzold, L. R. </author> <title> ,`Numerical Solution of Initial-Value Problems in Differential-Algebraic Equations', </title> <publisher> Elsevier, </publisher> <address> NY, </address> <year> 1989. </year>
Reference-contexts: As in our previous approach we discretize the DAE model using standard Implicit Runge-Kutta methods (IRK) <ref> [4] </ref>.
Reference: [5] <author> Cleland, F. A. , Wilhelm, R. H. </author> <title> ,`Diffusion and Reaction in a Viscous-flow Tubular Reactor',AIChE Journal, </title> <booktitle> 2 (4),pp. </booktitle> <pages> 489-497, </pages> <year> 1956. </year>
Reference-contexts: The measurements are the bulk concentration at the end of the reactor and the unknown parameter is the kinetic constant <ref> [5] </ref>.
Reference: [6] <author> Crowe, C. M. </author> <title> ,`Observability and Redundancy of Process Data for Steady State Reconciliation',Chem. </title> <institution> Eng. Sci. ,44 (12),pp. 2909-2917,1989. </institution>
Reference-contexts: Finally, uniqueness of the reconciled measurements is strongly linked to the problem formulation, the performance of the optimization and statistical interpretation of the results. This must be analyzed by a careful variable classification. For steady state processes Crowe <ref> [6] </ref>, Stanley and Mah [23], Kretsovalis and Mah [14] and Swartz [24] developed methods based on necessary and sufficient conditions for redundancy and observability. We extend this work using efficient sparse linear algebra methods and introduce the concept of collective redundancy. <p> If a variable, or a parameter is unobservable, then its value cannot be inferred from the measurements and the solution of the regression problem will be nonunique in these variables, and meaningless. 12 The application of observability results for steady state systems <ref> [6] </ref> to the dynamic case is straightforward. Consider equation (41), and assume a nonzero change x u in the unmeasured variables x u . Then T u x u = 0 for the perturbation to be feasible. <p> Observability requires a zero row of U 1 1 U 2 . This approach is similar to the one developed by Crowe <ref> [6] </ref> and Swartz [24] except that we are now using it on dynamic systems with a sparse LU decomposition. 3.3 Redundancy A variable is defined as nonredundant if deletion of its measurements will make this variable unobservable. <p> We then apply the unobservability test to: (Z T P T m ) k x k = 0 (48) where (Z T P T m ) k are the columns of (Z T P T m ) corresponding to variable x k . Previous studies <ref> [6, 24] </ref> required the coefficient matrix in equation (47) to have a zero column for the associated variable to be nonredundant, and thus nonunique and unobservable if deleted. This condition is sufficient but not necessary for nonuniqueness in dynamic systems.
Reference: [7] <author> DeGroot, M. H. </author> <title> ,`Probability and Statistics', </title> <publisher> Addison-Wesley, </publisher> <address> second edition, </address> <year> 1986. </year>
Reference-contexts: independent across the data sets, and if we use a flat prior (; x u ), problem (5) will become a nonlinear least squares problem. 4 To take into account the presence of outliers, we redevelop the contaminated normal distribution [12, 9, 27] as the outcome of a Bernoulli trial <ref> [7] </ref>. Here the two possible outcomes are: G = fGross error occurredg with probability and R = fRandom error occurredg with probability 1 . When G occurs, the error * will follow a distribution p (*jG). This will be the gross error distribution. <p> Another important notion is the relative efficiency for some nominal distribution, say Gaussian. For the single parameter case it is defined as: E = V act where is V opt is the optimal error variance of an estimator with this nominal distribution given by the Cramer-Rao inequality <ref> [7] </ref> and V act is the error variance attained by the estimator considered. As c becomes smaller, the Fair function estimator becomes less sensitive to outliers and its influence function becomes smaller. However its asymptotic efficiency with respect to the Normal distribution also decreases. <p> A Maximum-a-Posteriori estimator based on this distribution will not be robust, except in the special case when the Cauchy distribution <ref> [7] </ref> is used to model the gross errors. However this distribution does not lead to a convex function. The Fair estimator is not scale invariant, so if we are trying to estimate x from its measurement x we should use an estimate of the scale .
Reference: [8] <author> Dennis, J. E. ,Schnabel, R. B. </author> <title> ,`Numerical Methods for Unconstrained Optimization and Nonlinear Equations', </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983. </year>
Reference-contexts: This leads to ill-behaved optimization problems where the solution will be difficult to obtain. Although these problems can be stabilized by using trust region based approaches <ref> [8, 10] </ref> the solution in these variables will be statistically meaningless as these nonredundant variables are not related to other measured variables through the model; their only sources of information are their own measurements.
Reference: [9] <author> Fariss, R. H. , Law, V. H. </author> <title> ,`An Efficient Computational Technique for Generalized Application of Maximum Likelihood to Improve Correlation of Experimental Data', </title> <journal> Comput. Chem. Engng. </journal> , <volume> 3, </volume> <pages> pp. 95-104, </pages> <year> 1979, </year>
Reference-contexts: Also due to the sequential nature of the method, many regressions may have to be performed until we are satisfied. Another approach is to take into account the presence of outliers from the very beginning. A common method is the use of contaminated error distributions <ref> [12, 9, 27] </ref>, in which the objective function allows for two error structures, each with a certain probability of occurrence. In this manner, the regression 2 accommodates the presence of outliers and gross error detection can be per-formed simultaneously. <p> (5) If the measurement noise * is Normally distributed and independent across the data sets, and if we use a flat prior (; x u ), problem (5) will become a nonlinear least squares problem. 4 To take into account the presence of outliers, we redevelop the contaminated normal distribution <ref> [12, 9, 27] </ref> as the outcome of a Bernoulli trial [7]. Here the two possible outcomes are: G = fGross error occurredg with probability and R = fRandom error occurredg with probability 1 . When G occurs, the error * will follow a distribution p (*jG). <p> With a flat uniform prior this leads to the -contaminated Normal distribution with Normal contamination <ref> [12, 9, 27, 13] </ref>: p (; x u jx m ) / (1 ) exp 2 x m x m 2 (10) exp 2 x m x m 2 where b &gt; 1 is the ratio of the standard deviation of the gross error distribution to the standard deviation of the
Reference: [10] <author> Gopal, V. ,Biegler, L. T. </author> <title> ,`Nonsmooth Dynamic Simulation with Linear Programming Based Methods', </title> <note> submitted to Comp. </note> <institution> Chem. Engng. </institution> , <year> 1995. </year>
Reference-contexts: This leads to ill-behaved optimization problems where the solution will be difficult to obtain. Although these problems can be stabilized by using trust region based approaches <ref> [8, 10] </ref> the solution in these variables will be statistically meaningless as these nonredundant variables are not related to other measured variables through the model; their only sources of information are their own measurements.
Reference: [11] <author> Huber, Peter J. </author> <title> ,`Robust Statistics',New York,Wiley,1980. </title>
Reference-contexts: Also it often leads to nonconvex and complex objective functions prone to underflow problems. Another simultaneous approach is to use objective functions which, due to their mathematical structure, are insensitive to deviations from the ideal assumptions <ref> [18, 11] </ref>. These estimators tend to look at the bulk of the data and ignore atypical values. <p> The most important for us are the M-Estimators, which are generalizations of the Maximum-Likelihood-Estimator. Their form is: min n X (~ i ; ) (13) Several estimates have been proposed in the literature <ref> [18, 11, 3] </ref> but to our knowledge only the Fair function is convex and has continuous first and second derivatives. Its form is: (~) = c 2 j~j log (1 + c where c is a tuning parameter. <p> Another popular class of statistics are the R-Estimators, which are based on rank tests. Although they are more difficult to compute they seem to have very good robustness and asymptotic properties. For more on these methods, refer to <ref> [11, 18, 28, 19] </ref>. To deal with nonredundant variables, one can use an M-Estimator on replicated measurements, or one can preprocess the data for each time instant on this variable with L-Estimators, and feed the result to a least squares estimator.
Reference: [12] <author> Jeffreys, H. </author> , <title> `An Alternative to the Rejection of Observations', </title> <journal> Proc. R. Soc. , A137, </journal> <volume> 78, </volume> <year> 1932. </year>
Reference-contexts: Also due to the sequential nature of the method, many regressions may have to be performed until we are satisfied. Another approach is to take into account the presence of outliers from the very beginning. A common method is the use of contaminated error distributions <ref> [12, 9, 27] </ref>, in which the objective function allows for two error structures, each with a certain probability of occurrence. In this manner, the regression 2 accommodates the presence of outliers and gross error detection can be per-formed simultaneously. <p> (5) If the measurement noise * is Normally distributed and independent across the data sets, and if we use a flat prior (; x u ), problem (5) will become a nonlinear least squares problem. 4 To take into account the presence of outliers, we redevelop the contaminated normal distribution <ref> [12, 9, 27] </ref> as the outcome of a Bernoulli trial [7]. Here the two possible outcomes are: G = fGross error occurredg with probability and R = fRandom error occurredg with probability 1 . When G occurs, the error * will follow a distribution p (*jG). <p> With a flat uniform prior this leads to the -contaminated Normal distribution with Normal contamination <ref> [12, 9, 27, 13] </ref>: p (; x u jx m ) / (1 ) exp 2 x m x m 2 (10) exp 2 x m x m 2 where b &gt; 1 is the ratio of the standard deviation of the gross error distribution to the standard deviation of the
Reference: [13] <author> Johnston, L. P. M. ,Kramer, A. M. </author> <title> ,`Maximum Likelihood Data Rectification: Steady State Systems', </title> <journal> AIChE Journal, </journal> <volume> 41 (11),pp. </volume> <pages> 2145-2426, </pages> <year> 1995. </year>
Reference-contexts: With a flat uniform prior this leads to the -contaminated Normal distribution with Normal contamination <ref> [12, 9, 27, 13] </ref>: p (; x u jx m ) / (1 ) exp 2 x m x m 2 (10) exp 2 x m x m 2 where b &gt; 1 is the ratio of the standard deviation of the gross error distribution to the standard deviation of the
Reference: [14] <author> Kretsovalis,A. ,Mah,R. S. H. </author> <title> ,`Observability and Redundancy Classification in Generalized Process Networks-I. </title> <journal> Theorems',Comput. Chem. Engng. </journal> <volume> ,12, </volume> <pages> pp. 671-687, </pages> <year> 1988. </year>
Reference-contexts: Finally, uniqueness of the reconciled measurements is strongly linked to the problem formulation, the performance of the optimization and statistical interpretation of the results. This must be analyzed by a careful variable classification. For steady state processes Crowe [6], Stanley and Mah [23], Kretsovalis and Mah <ref> [14] </ref> and Swartz [24] developed methods based on necessary and sufficient conditions for redundancy and observability. We extend this work using efficient sparse linear algebra methods and introduce the concept of collective redundancy.
Reference: [15] <author> Liebman, M. J. ,Edgar, T. F. ,Lasdon, L. ,S. </author> <title> ,`Efficient Data Reconcilia--tion and Estimation for Dynamic Processes using Nonlinear Programming Techniques', </title> <journal> Comp. Chem. Engng. </journal> , <volume> 16 (10/11),p. 963, </volume> <year> 1992 </year>
Reference-contexts: 1 Introduction While data reconciliation is a common tool for steady state systems, application to dynamic systems is still in its infancy <ref> [15, 22] </ref>. Moreover, the dynamic problem has some interesting characteristics related to the optimization formulation and interpretation of results. In a previous paper [1] we developed an efficient Successive Quadratic Programming (SQP) based method for data reconciliation and parameter estimation for systems described by Differential and Algebraic Equation Systems (DAE).
Reference: [16] <author> Narasimhan, S. , Mah, R. S. H. </author> , <title> `Generalized Likelihood Ratios for Gross Error Identification in Dynamic Processes', </title> <journal> AIChE Journal, </journal> <volume> 34 (8), </volume> <pages> pp. 1321-1331, </pages> <year> 1988. </year>
Reference-contexts: Initially, sequential tests were used where a regression using the least squares assumption was performed, and the residuals were studied using statistical tests in order to see which of those residuals followed the initial distributional assumptions <ref> [16, 25] </ref>. Once a gross error was detected, a new regression was performed and tests were done until the data were free of errors.
Reference: [17] <author> Rawlings, J. O. </author> , <title> `Applied Regression Analysis. A Research Tool', </title> <address> Pacific Groove, CA, </address> <publisher> Wadsworth & Brooks, </publisher> <year> 1988. </year>
Reference-contexts: An outlier in one measurement may show up in a totally different measurement after the regression is performed. These are defined as leverage points <ref> [17] </ref>. Also due to the sequential nature of the method, many regressions may have to be performed until we are satisfied. Another approach is to take into account the presence of outliers from the very beginning. <p> These are probability plots where the residuals are plotted against the order statistics for the Normal distribution (see section 4.1). In these plots <ref> [17] </ref> linearity indicates Normality. 20 The outliers show up as distinct points in the extremes of the graphics. The bulk of the data are nearly Normal (i.e.linear) in all variables confirming that the measurement error does follow a Normal distribution.
Reference: [18] <author> Rey, William J. J. </author> , <title> `Introduction to Robust and Quasi-Robust Statistical Methods', </title> <address> Berlin, New York, </address> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference-contexts: Also it often leads to nonconvex and complex objective functions prone to underflow problems. Another simultaneous approach is to use objective functions which, due to their mathematical structure, are insensitive to deviations from the ideal assumptions <ref> [18, 11] </ref>. These estimators tend to look at the bulk of the data and ignore atypical values. <p> The most important for us are the M-Estimators, which are generalizations of the Maximum-Likelihood-Estimator. Their form is: min n X (~ i ; ) (13) Several estimates have been proposed in the literature <ref> [18, 11, 3] </ref> but to our knowledge only the Fair function is convex and has continuous first and second derivatives. Its form is: (~) = c 2 j~j log (1 + c where c is a tuning parameter. <p> However its asymptotic efficiency with respect to the Normal distribution also decreases. If E is the relative asymptotic efficiency with the Normal distribution, for the single parameter case, then c is crudely related to it by <ref> [18] </ref>: c = 0:21529 E 0:63662 1:02 In general, there is a trade-off between robustness and efficiency. The more robust an estimator is, the less efficient it is. For M-Estimators, the influence function is proportional to the estimator's derivative. <p> Another popular class of statistics are the R-Estimators, which are based on rank tests. Although they are more difficult to compute they seem to have very good robustness and asymptotic properties. For more on these methods, refer to <ref> [11, 18, 28, 19] </ref>. To deal with nonredundant variables, one can use an M-Estimator on replicated measurements, or one can preprocess the data for each time instant on this variable with L-Estimators, and feed the result to a least squares estimator.
Reference: [19] <author> Rousseeuw, P. J. ,Leroy, A. M. </author> <title> ,`Robust Regression and Outlier Detection',New York, </title> <publisher> Wiley, </publisher> <year> 1987. </year>
Reference-contexts: Another popular class of statistics are the R-Estimators, which are based on rank tests. Although they are more difficult to compute they seem to have very good robustness and asymptotic properties. For more on these methods, refer to <ref> [11, 18, 28, 19] </ref>. To deal with nonredundant variables, one can use an M-Estimator on replicated measurements, or one can preprocess the data for each time instant on this variable with L-Estimators, and feed the result to a least squares estimator.
Reference: [20] <author> Ryan,B. F. ,Joiner,L. B. Ryan, T. A. </author> <title> ,`MINITAB Handbook', </title> <address> Boston, </address> <publisher> PWS-Kent Publishing Company, </publisher> <address> Second Edition, </address> <year> 1985. </year>
Reference-contexts: A good description of these methods is given in [28]. Once the outliers have been identified, this identification can always be verified by dropping the outliers and resolving a least squares problem. All these exploratory statistical tools are available in easy to use packages such as MINITAB <ref> [20] </ref> or xlispstat [26].
Reference: [21] <author> Seber,G. A. F. ,Wild,C. J. </author> <title> ,`Nonlinear Regression', </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: N E u C 0 (R T E u ) 0 7 7 7 T m = 6 6 6 (R I E m ) 1 E m F m (R T E m ) 7 7 7 3.2 Observability Observability is closely related to identifiability <ref> [21] </ref>. Here, given a nonlinear model E [y] = f (t; ), where y are the response (measured) variables, t are the explanatory variables, the parameters will be unidentifiable when there exists 1 and 2 such that f (t; 1 ) = f (t; 2 ).
Reference: [22] <author> Sistu, P. B. ,Gopinath, R. S. , Bequette, B. W. </author> <title> ,`Computation Issues in Nonlinear Predictive Control', </title> <journal> Comp. Chem. Engng. </journal> , <volume> 17 (4),p. 361, </volume> <year> 1993 </year>
Reference-contexts: 1 Introduction While data reconciliation is a common tool for steady state systems, application to dynamic systems is still in its infancy <ref> [15, 22] </ref>. Moreover, the dynamic problem has some interesting characteristics related to the optimization formulation and interpretation of results. In a previous paper [1] we developed an efficient Successive Quadratic Programming (SQP) based method for data reconciliation and parameter estimation for systems described by Differential and Algebraic Equation Systems (DAE).
Reference: [23] <author> Stanley, G. M. , Mah, R. S. H. </author> , <title> `Obervability and Redundancy in Process Data Estimation', </title> <journal> Chem. Engr. Sci. </journal> , <volume> 36, </volume> <pages> pp. 259, </pages> <year> 1981. </year>
Reference-contexts: Finally, uniqueness of the reconciled measurements is strongly linked to the problem formulation, the performance of the optimization and statistical interpretation of the results. This must be analyzed by a careful variable classification. For steady state processes Crowe [6], Stanley and Mah <ref> [23] </ref>, Kretsovalis and Mah [14] and Swartz [24] developed methods based on necessary and sufficient conditions for redundancy and observability. We extend this work using efficient sparse linear algebra methods and introduce the concept of collective redundancy.
Reference: [24] <author> Swartz, C. L. E. </author> , <title> `Data Reconciliation for Generalized Flowsheet Applications', </title> <booktitle> paper presented at the American Chemical Society National Meeting, </booktitle> <address> Dallas, Tx, </address> <month> April, </month> <year> 1989. </year>
Reference-contexts: Finally, uniqueness of the reconciled measurements is strongly linked to the problem formulation, the performance of the optimization and statistical interpretation of the results. This must be analyzed by a careful variable classification. For steady state processes Crowe [6], Stanley and Mah [23], Kretsovalis and Mah [14] and Swartz <ref> [24] </ref> developed methods based on necessary and sufficient conditions for redundancy and observability. We extend this work using efficient sparse linear algebra methods and introduce the concept of collective redundancy. <p> Observability requires a zero row of U 1 1 U 2 . This approach is similar to the one developed by Crowe [6] and Swartz <ref> [24] </ref> except that we are now using it on dynamic systems with a sparse LU decomposition. 3.3 Redundancy A variable is defined as nonredundant if deletion of its measurements will make this variable unobservable. <p> We then apply the unobservability test to: (Z T P T m ) k x k = 0 (48) where (Z T P T m ) k are the columns of (Z T P T m ) corresponding to variable x k . Previous studies <ref> [6, 24] </ref> required the coefficient matrix in equation (47) to have a zero column for the associated variable to be nonredundant, and thus nonunique and unobservable if deleted. This condition is sufficient but not necessary for nonuniqueness in dynamic systems. <p> In the final hydrolysis example we show how gross error detection can be performed with nonredundant data and also how to use both M and L-Estimators. 5.1 Heat exchanger network Here we consider the steady state heat exchanger problem presented by Swartz <ref> [24] </ref> where the gross error detection was performed sequentially. A comparison between the serial approach and the simultaneous approach was done by Tjoa [27] where the contaminated normal distribution function (10) was used sucessfully to reconcile the data and detect the outliers. A simplified flow-sheet is presented in figure 1. <p> A simplified flow-sheet is presented in figure 1. In this problem the nonredundant variables are TA1,TA4,FB1,TB1,FC1 and TC1. Table 8 shows the measurements, the standard deviations associated with them and the estimates of the regressions performed in <ref> [24] </ref>, where a least squares objective function was used on all measurements (run 1) and a measurement test was used to detect outliers. In this test, the measurement from variable TA7 was identified as a gross error. <p> In this regression, the measurement of variable TA7 is identified as a gross error and the estimates are very close to those obtained in <ref> [24] </ref> (run 2). The results from these two regressions are essentially the same. Finally, the same table shows the estimates of the regression when the Fair function (22) is used, tuned for an asymptotic relative efficiency of 70% for the Normal distribution (c=0.04409). <p> In all regressions the estimates for the nonredundant variables were the same as the measurements, which is not surprising in light of the discussion in section 4.2. For the nonredundant variables, the results from the Fair function were different from the ones obtained in <ref> [24] </ref> and [27]. In order to explain the difference we plotted the residuals for the redundant variables in dotplots and boxplots (figures 2,3,4, 5,6,7).
Reference: [25] <author> Tamhane, A. C. , Kao,C. , Mah, R. S. H. </author> <title> ,`Gross Error Detection in Serially Correlated Process Data. 2. Dynamic Systems', Ind. </title> <journal> Eng. Chem. Res., </journal> <volume> 31, </volume> <pages> pp. 254-262, </pages> <year> 1992. </year>
Reference-contexts: Initially, sequential tests were used where a regression using the least squares assumption was performed, and the residuals were studied using statistical tests in order to see which of those residuals followed the initial distributional assumptions <ref> [16, 25] </ref>. Once a gross error was detected, a new regression was performed and tests were done until the data were free of errors.
Reference: [26] <editor> Tierney,L. </editor> <address> ,`Lisp-Stat',New York, Wiley,1990. </address>
Reference-contexts: Once the outliers have been identified, this identification can always be verified by dropping the outliers and resolving a least squares problem. All these exploratory statistical tools are available in easy to use packages such as MINITAB [20] or xlispstat <ref> [26] </ref>.
Reference: [27] <author> Tjoa, I. B. , Biegler ,L. T. </author> , <title> `Simultaneous Strategies for Data Reconciliation and Gross Error Detection of Nonlinear Systems', </title> <journal> Comput. Chem. Engng. ,15 (10),pp. </journal> <pages> 679-90, </pages> <year> 1991. </year>
Reference-contexts: Also due to the sequential nature of the method, many regressions may have to be performed until we are satisfied. Another approach is to take into account the presence of outliers from the very beginning. A common method is the use of contaminated error distributions <ref> [12, 9, 27] </ref>, in which the objective function allows for two error structures, each with a certain probability of occurrence. In this manner, the regression 2 accommodates the presence of outliers and gross error detection can be per-formed simultaneously. <p> (5) If the measurement noise * is Normally distributed and independent across the data sets, and if we use a flat prior (; x u ), problem (5) will become a nonlinear least squares problem. 4 To take into account the presence of outliers, we redevelop the contaminated normal distribution <ref> [12, 9, 27] </ref> as the outcome of a Bernoulli trial [7]. Here the two possible outcomes are: G = fGross error occurredg with probability and R = fRandom error occurredg with probability 1 . When G occurs, the error * will follow a distribution p (*jG). <p> With a flat uniform prior this leads to the -contaminated Normal distribution with Normal contamination <ref> [12, 9, 27, 13] </ref>: p (; x u jx m ) / (1 ) exp 2 x m x m 2 (10) exp 2 x m x m 2 where b &gt; 1 is the ratio of the standard deviation of the gross error distribution to the standard deviation of the <p> Here an estimation problem with constraints that have small curvature will always have a unique global solution and the GN approximation will be guaranteed to converge. Moreover, since the nonlinear data reconciliation problem is frequently solved with SQP, the GN approximation leads to significant improvements in the convergence rate <ref> [1, 27] </ref>. Unlike the contaminated normal objective, this is further improved by the Fair function since it always has positive curvature. <p> A comparison between the serial approach and the simultaneous approach was done by Tjoa <ref> [27] </ref> where the contaminated normal distribution function (10) was used sucessfully to reconcile the data and detect the outliers. A simplified flow-sheet is presented in figure 1. In this problem the nonredundant variables are TA1,TA4,FB1,TB1,FC1 and TC1. <p> In this test, the measurement from variable TA7 was identified as a gross error. This measurement was then deleted and a new regression with the least squares was performed (run 2). No gross errors were detected this time. Table 8 also shows the results obtained in <ref> [27] </ref> when the contaminated normal objective function is used with a probability of gross error occurrence () equal to 0.05 and with a ratio of standard deviations (b) equal to 10. <p> In all regressions the estimates for the nonredundant variables were the same as the measurements, which is not surprising in light of the discussion in section 4.2. For the nonredundant variables, the results from the Fair function were different from the ones obtained in [24] and <ref> [27] </ref>. In order to explain the difference we plotted the residuals for the redundant variables in dotplots and boxplots (figures 2,3,4, 5,6,7). <p> In the first experiment, we simulated the data with Normal noise along with large random shifts in the measured variables, except for F 0 . These shifts simulate the gross errors. We performed the regression with our SQP decomposition strategies [1] using the least squares function, the contaminated Normal <ref> [27] </ref> and the Fair function tuned for an asymptotic relative efficiency of 95% for the Normal distribution (c=1.3998, see equation (16)). However, on all three runs the least squares estimate was used for F 0 .
Reference: [28] <author> Tukey, John W. , Hoaglin, David C. , Mosteller, Frederick, </author> <title> `Understanding Robust and Exploratory Data Analysis', </title> <address> New York, </address> <publisher> Wiley, </publisher> <year> 1983. </year>
Reference-contexts: this fashion, an accurate regression can be performed even if nothing is known about the outliers or even the error structure of the data, and exploratory methods can be used to detect gross errors and derive more information from the statistical properties of the data once the residuals are estimated <ref> [28] </ref>. Finally, uniqueness of the reconciled measurements is strongly linked to the problem formulation, the performance of the optimization and statistical interpretation of the results. This must be analyzed by a careful variable classification. <p> The outlier cutoffs are defined as F l ffd F and F u +ffd F where ff is usually set to 1 3 . Measurements outside the cutoffs are considered outliers. A good description of these methods is given in <ref> [28] </ref>. Once the outliers have been identified, this identification can always be verified by dropping the outliers and resolving a least squares problem. All these exploratory statistical tools are available in easy to use packages such as MINITAB [20] or xlispstat [26]. <p> Another popular class of statistics are the R-Estimators, which are based on rank tests. Although they are more difficult to compute they seem to have very good robustness and asymptotic properties. For more on these methods, refer to <ref> [11, 18, 28, 19] </ref>. To deal with nonredundant variables, one can use an M-Estimator on replicated measurements, or one can preprocess the data for each time instant on this variable with L-Estimators, and feed the result to a least squares estimator.

References-found: 28

