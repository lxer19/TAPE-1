URL: ftp://info.mcs.anl.gov/pub/fortran-m/reports/ijsa.ps.Z
Refering-URL: http://www.mcs.anl.gov/fortran-m/papers/
Root-URL: http://www.mcs.anl.gov
Title: Integrated Support for Task and Data Parallelism  
Author: Mani Chandy Ian Foster Ken Kennedy Charles Koelbel Chau-Wen Tseng 
Note: To appear in: Intl. J. Supercomputer Applications  
Date: August 27, 1993  
Address: Pasadena, CA 91125  Argonne, IL 60439  Houston, TX 77251-1892  
Affiliation: Center for Research on Parallel Computation California Institute of Technology  Argonne National Laboratory  Center for Research on Parallel Computation Rice University  
Abstract: We present an overview of research at the CRPC designed to provide an efficient, portable programming model for scientific applications possessing both task and data parallelism. Fortran M programs exploit task parallelism by providing language extensions for user-defined process management and typed communication channels. A combination of compiler and run-time system support ensures modularity, safety, portability, and efficiency. Fortran D and High Performance Fortran programs exploit data parallelism by providing language extensions for user-defined data decomposition specifications, parallel loops, and parallel array operations. Compile-time analysis and optimization yield efficient, portable programs. We design an interface for using a task-parallel language to coordinate concurrent data-parallel computations. The interface permits concurrently executing data-parallel computations to interact through messages. A key notion underlying the proposed interface is the integration of Fortran M resource management constructs and Fortran D data decomposition constructs. fl This research was supported by the National Science Foundation's Center for Research in Parallel Computation, under Contract CCR-8809615.
Abstract-found: 1
Intro-found: 1
Reference: <institution> American National Standards Institutte 1990. ANSI X3J3/S8.115. </institution> <note> Fortran 90. Applied Parallel Research 1992. Forge 90 distributed memory parallelizer: User's guide, version 8.0 edition, </note> <institution> Placerville, California: Applied Parallel Research. </institution>
Reference-contexts: The compiler is available by anonymous ftp from info.mcs.anl.gov, in directory pub/pcn. FM is currently defined as extensions to Fortran 77. However, equivalent extensions can easily be defined for Fortran 90 <ref> (American National Standards Institute, 1990) </ref>, and we plan to extend the FM compiler to accept Fortran 90 syntax. <p> The compiler is then responsible for using the data partitioning information to assign the computations to the processors. We present only an overview of the HPF language; see <ref> (American National Standards Institute, 1990) </ref> for the full language. 3.1 Parallel Operations HPF incorporates all of Fortran 90, including data-parallel operations such as array expressions and assignment, array intrinsics, and where statements. HPF adds a number of additional data-parallel features, including new functions for reductions, combining scatters, and prefix operations.
Reference: <author> Balasundaram, V., Fox, J., Kennedy, K., and Kremer, U. </author> <year> 1990. </year> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th distributed memory computing conference, </booktitle> <address> Charleston, S.C. </address>
Reference: <author> Beguelin, A., Dongarra, J., Geist, G., Manchek, R., and Sunderam, V. </author> <year> 1991. </year> <title> Graphical development tools for network-based concurrent supercomputing. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, N.M. </address>
Reference-contexts: Languages for supporting or coordinating task-level parallelism include PCN (Chandy and Taylor, 1991; Foster, Olson, and Tuecke, 1992), Strand (Foster and Taylor, 1990), Linda (Carriero and Gelernter, 1989), and Delirium (Graham, Lucco, and Sharp, 1993). Tools for exploiting task-level parallelism include Schedule (Dongarra and Sorensen, 1987), Hence <ref> (Beguelin et al., 1991) </ref>, and CODE (Newton and Browne, 1992). PCF Fortran exemplifies a parallel language for exploiting fork-join and loop-level parallelism (Parallel Computing Forum, 1991). In comparison with these systems, Fortran M is designed to provide greater support for modularity and safety.
Reference: <author> Brandes, T. </author> <year> 1993. </year> <title> Automatic translation of data parallel programs to message passing programs. In Proceedings of AP'93 international workshop on automatic distributed memory parallelization, automatic data distribution and automatic parallel performance prediction, </title> <booktitle> Saarbrucken, </booktitle> <address> Germany. </address>
Reference-contexts: Compilers for data-parallel programs include Adapt (Merlin, 1991), Adaptor <ref> (Brandes, 1993) </ref> Aspar (Ikudome et al., 1990), Callahan and Kennedy (Callahan and Kennedy, 1988), Forge90 (Applied Parallel Research, 1992), Id Nouveau (Rogers and Pingali, 1989), and Superb (Zima, Bast, and Gerndt, 1988).
Reference: <author> Callahan, D., and Kennedy, K. </author> <year> 1988. </year> <title> Compiling programs for distributed-memory multiprocessors. </title> <editor> J. </editor> <booktitle> Supercomputing 2 </booktitle> <pages> 151-169. </pages>
Reference-contexts: Compilers for data-parallel programs include Adapt (Merlin, 1991), Adaptor (Brandes, 1993) Aspar (Ikudome et al., 1990), Callahan and Kennedy <ref> (Callahan and Kennedy, 1988) </ref>, Forge90 (Applied Parallel Research, 1992), Id Nouveau (Rogers and Pingali, 1989), and Superb (Zima, Bast, and Gerndt, 1988). The Fortran D compiler adapts techniques from several of these systems, but performs greater compile-time analysis and optimization and relies less on language extensions and run-time support.
Reference: <author> Carriero, N., and Gelernter, D. </author> <year> 1989. </year> <title> Linda in context. </title> <journal> Comm. ACM 32(4) </journal> <pages> 444-458. </pages>
Reference-contexts: Relatively few researchers, however, have focused on the issue of integrating support for both types of parallelism. Languages for supporting or coordinating task-level parallelism include PCN (Chandy and Taylor, 1991; Foster, Olson, and Tuecke, 1992), Strand (Foster and Taylor, 1990), Linda <ref> (Carriero and Gelernter, 1989) </ref>, and Delirium (Graham, Lucco, and Sharp, 1993). Tools for exploiting task-level parallelism include Schedule (Dongarra and Sorensen, 1987), Hence (Beguelin et al., 1991), and CODE (Newton and Browne, 1992). PCF Fortran exemplifies a parallel language for exploiting fork-join and loop-level parallelism (Parallel Computing Forum, 1991).
Reference: <author> Chandy, K. M., and Foster, I. </author> <year> 1993. </year> <title> Deterministic parallel Fortran. </title> <booktitle> In Proceedings of the sixth SIAM conference on parallel processing for scientific computing, </booktitle> <address> Norfolk, Virginia. </address> <note> 19 Chandy, </note> <author> K. M., and Taylor, S. </author> <year> 1991. </year> <title> An introduction to parallel programming. </title> <publisher> Jones and Bartlett. </publisher>
Reference-contexts: Inspired by Fortran D and HPF, it permits programs to use data distribution statements (as discussed in the next section) to create distributed arrays <ref> (Chandy and Foster, 1993) </ref>. (However, this capability is not supported in the prototype compiler.) Semantically, distributed arrays are indistinguishable from nondis-tributed arrays. That is, they are accessible only to the process in which they are declared and are copied when passed as arguments to subprocesses. <p> In the example above, arrays X and Y are not distributed. Hence, redistribution is required to convert X and Y to whatever distribution is required in CONTROLS and DYNAMICS. This may involve considerable communication. FM programs can also define distributed arrays using HPF-style data-distribution statements <ref> (Chandy and Foster, 1993) </ref>. This is illustrated in the following code. Arrays X and Y are distributed over the 10 nodes of the virtual computer in which program PROG3 is executing. In this case, redistribution is required only if the CONTROLS and DYNAMICS computations use a different distribution.
Reference: <author> Chapman, B., Mehrotra, P., and Zima, H. </author> <year> 1992. </year> <title> Programming in Vienna Fortran. </title> <booktitle> Scientific Programming 1(1) </booktitle> <pages> 31-50. </pages>
Reference: <author> Chase, C., Cheung, A., Reeves, A., and Smith, M. </author> <year> 1991. </year> <title> Paragon: A parallel programming environment for scientific applications using communication structures. </title> <booktitle> In Proceedings of the 1991 international conference on parallel processing, </booktitle> <address> St. Charles, Illinois. </address>
Reference-contexts: Languages for expressing data-parallelism include Fortran 90 (American National Standards Institute, 1990; CMF (Thinking Machines Corporation, 1991); Vienna Fortran (Chap-man, Mehrotra, and Zima, 1992), C* (Rose and Steele, 1987), Dataparallel C (Hatcher and Quinn, 1991), Dino (Rosing, Schnabel, and Weaver, 1991), Kali (Koelbel and Mehro-tra, 1991), and Paragon <ref> (Chase et al., 1991) </ref>. Compilers for data-parallel programs include Adapt (Merlin, 1991), Adaptor (Brandes, 1993) Aspar (Ikudome et al., 1990), Callahan and Kennedy (Callahan and Kennedy, 1988), Forge90 (Applied Parallel Research, 1992), Id Nouveau (Rogers and Pingali, 1989), and Superb (Zima, Bast, and Gerndt, 1988).
Reference: <author> Delves, L., Craven, P., and Lloyd, D. </author> <year> 1992. </year> <title> A Fortran 90 compilation system for distributed memory architectures. </title> <booktitle> In Proceedings PACA92 conference, </booktitle> <publisher> IOP. </publisher>
Reference-contexts: Delves et al. describe an extended Fortran 90 compiler which includes both message passing along channels (as extensions to file I/O operations) and remote procedure calls <ref> (Delves et al., 1992) </ref>. However, data distribution issues are not addressed in their prototype implementation. One of the few systems to examine both types of parallelism, the iWarp compiler pursues a sophisticated compile-time approach for exploiting both task and data parallelism on a mesh-connected distributed-memory machine (Subhlok et al., 1993).
Reference: <author> Dongarra, J., and Sorensen, D. </author> <year> 1987. </year> <title> SCHEDULE: Tools for developing and analyzing parallel Fortran programs. In The characteristics of parallel algorithms, </title> <editor> edited by D. Gannon, L. Jamieson, and R. Douglass. </editor> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Languages for supporting or coordinating task-level parallelism include PCN (Chandy and Taylor, 1991; Foster, Olson, and Tuecke, 1992), Strand (Foster and Taylor, 1990), Linda (Carriero and Gelernter, 1989), and Delirium (Graham, Lucco, and Sharp, 1993). Tools for exploiting task-level parallelism include Schedule <ref> (Dongarra and Sorensen, 1987) </ref>, Hence (Beguelin et al., 1991), and CODE (Newton and Browne, 1992). PCF Fortran exemplifies a parallel language for exploiting fork-join and loop-level parallelism (Parallel Computing Forum, 1991). In comparison with these systems, Fortran M is designed to provide greater support for modularity and safety.
Reference: <author> Foster, I., Avalani, B., Choudhary, A., and Xu., M. </author> <title> A compilation system that integrates High Performance Fortran and Fortran M. </title> <booktitle> In Proc. 1994 Scalable High Performance Computing Conf., </booktitle> <address> Knoxville, Tenn., </address> <year> 1994. </year>
Reference-contexts: Some of these ideas are being investigated in a compilation system under development at Argonne National Laboratory and Syracuse University <ref> (Foster et al., 1994) </ref>. First, however, we define both task parallelism and data parallelism, then motivate their integration. 1.1 Task Parallelism In a task-parallel programming paradigm the program consists of a set of (potentially dissimilar) parallel tasks that interact through explicit communication and synchronization. <p> This would allow many processes to cooperate in sending data to other process groups, and would be particularly appropriate for communicating distributed arrays or other data structures. This approach is being explored in a compilation system under development at Argonne National Laboratory and Syracuse University <ref> (Foster et al., 1994) </ref>. A disadvantage is that HPF does not currently provide such operations. An alternative approach is to use an existing mechanism, namely, HPF's extrinsic procedure facility (High Performance Fortran Forum, 1993). In the HPF code, an extrinsic procedure call is made to a FM subroutine.
Reference: <author> Foster, I., and Chandy, K. M. </author> <year> 1992. </year> <title> Fortran M: A language for modular parallel programming. </title> <type> Technical Report MCS-P327-0992, </type> <institution> Argonne National Laboratory. </institution>
Reference-contexts: Section 4 describes the design of the interface and some implementation issues. Section 5 describes related work, and Section 6 gives our conclusions and plans for future work. 2 Fortran M Fortran M (FM) is a language designed by researchers at Argonne and Caltech for expressing task-parallel computation <ref> (Foster and Chandy, 1992) </ref>. It comprises a small set of extensions to Fortran and provides a message-passing parallel programming model, in which programs create processes that interact by sending and receiving messages on typed channels. Two key features of the extensions are their support for determinism and modularity. <p> The PROCESSORS declaration specifies the shape and dimension of a processor array, the LOCATION annotation maps processes to specified elements of this array and, as in PCN, the SUBMACHINE annotation specifies that a process should execute in a subset of the array <ref> (Foster, Olson, and Tuecke, 1992) </ref>. For example, the following code places the CONTROLS and DYNAMICS processes on different virtual processors. PROCESSORS (2) ... PROCESSES PROCESSCALL CONTROLS (...) LOCATION (1) PROCESSCALL DYNAMICS (...) LOCATION (2) ENDPROCESSES In contrast, the following code places each process in a submachine comprising 10 virtual processors. <p> It supports all language constructs except those concerned with data distribution. Preliminary experiments suggest that the performance of basic communication and synchronization operations is competitive with, and in some cases superior to, conventional message-passing libraries <ref> (Foster and Chandy, 1992) </ref>. 3 Fortran D and High Performance Fortran Fortran D was an early language designed by researchers at Rice and Syracuse for expressing data-parallel computation (Fox et al., 1990). It provided data decomposition specifications, parallel loops, and methods for specifying reductions.
Reference: <author> Foster, I., Olson, R., and Tuecke, S. </author> <year> 1992. </year> <title> Productive parallel programming: The PCN approach. </title> <booktitle> Scientific Programming 1(1) </booktitle> <pages> 51-66. </pages>
Reference-contexts: Section 4 describes the design of the interface and some implementation issues. Section 5 describes related work, and Section 6 gives our conclusions and plans for future work. 2 Fortran M Fortran M (FM) is a language designed by researchers at Argonne and Caltech for expressing task-parallel computation <ref> (Foster and Chandy, 1992) </ref>. It comprises a small set of extensions to Fortran and provides a message-passing parallel programming model, in which programs create processes that interact by sending and receiving messages on typed channels. Two key features of the extensions are their support for determinism and modularity. <p> The PROCESSORS declaration specifies the shape and dimension of a processor array, the LOCATION annotation maps processes to specified elements of this array and, as in PCN, the SUBMACHINE annotation specifies that a process should execute in a subset of the array <ref> (Foster, Olson, and Tuecke, 1992) </ref>. For example, the following code places the CONTROLS and DYNAMICS processes on different virtual processors. PROCESSORS (2) ... PROCESSES PROCESSCALL CONTROLS (...) LOCATION (1) PROCESSCALL DYNAMICS (...) LOCATION (2) ENDPROCESSES In contrast, the following code places each process in a submachine comprising 10 virtual processors. <p> It supports all language constructs except those concerned with data distribution. Preliminary experiments suggest that the performance of basic communication and synchronization operations is competitive with, and in some cases superior to, conventional message-passing libraries <ref> (Foster and Chandy, 1992) </ref>. 3 Fortran D and High Performance Fortran Fortran D was an early language designed by researchers at Rice and Syracuse for expressing data-parallel computation (Fox et al., 1990). It provided data decomposition specifications, parallel loops, and methods for specifying reductions.
Reference: <author> Foster, I., Olson, R., and Tuecke, S. </author> <year> 1993. </year> <title> Programming in Fortran M. </title> <type> Technical Report ANL-93/26, </type> <institution> Argonne National Laboratory. </institution>
Reference-contexts: Inspired by Fortran D and HPF, it permits programs to use data distribution statements (as discussed in the next section) to create distributed arrays <ref> (Chandy and Foster, 1993) </ref>. (However, this capability is not supported in the prototype compiler.) Semantically, distributed arrays are indistinguishable from nondis-tributed arrays. That is, they are accessible only to the process in which they are declared and are copied when passed as arguments to subprocesses. <p> on different parallel computers: for example, portable message-passing libraries on heterogeneous networks, low-level message-passing or active messages on multicomputers, shared-memory operations on multiprocessors, or Windows NT services on networks of PCs. 7 The prototype compiler is currently operational on networks of workstations, the IBM SP-1 multicomputer, and the CRAY C90 <ref> (Foster, Olson, and Tuecke, 1993) </ref>. It supports all language constructs except those concerned with data distribution. <p> In the example above, arrays X and Y are not distributed. Hence, redistribution is required to convert X and Y to whatever distribution is required in CONTROLS and DYNAMICS. This may involve considerable communication. FM programs can also define distributed arrays using HPF-style data-distribution statements <ref> (Chandy and Foster, 1993) </ref>. This is illustrated in the following code. Arrays X and Y are distributed over the 10 nodes of the virtual computer in which program PROG3 is executing. In this case, redistribution is required only if the CONTROLS and DYNAMICS computations use a different distribution.
Reference: <author> Foster, I., and Taylor, S. </author> <year> 1990. </year> <title> Strand: New concepts in parallel programming. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Relatively few researchers, however, have focused on the issue of integrating support for both types of parallelism. Languages for supporting or coordinating task-level parallelism include PCN (Chandy and Taylor, 1991; Foster, Olson, and Tuecke, 1992), Strand <ref> (Foster and Taylor, 1990) </ref>, Linda (Carriero and Gelernter, 1989), and Delirium (Graham, Lucco, and Sharp, 1993). Tools for exploiting task-level parallelism include Schedule (Dongarra and Sorensen, 1987), Hence (Beguelin et al., 1991), and CODE (Newton and Browne, 1992).
Reference: <author> Fox, G., Hiranandani, S., Kennedy, K., Koelbel, C., Kremer, U., Tseng, C., and Wu, M. </author> <year> 1990. </year> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University. </institution>
Reference-contexts: Parallelism may be implicit or explicit, with additional annotations describing how the data structures are decomposed and distributed among the physical processors. Data parallelism may be both regular (e.g., dense matrices) or irregular (e.g., sparse matrices). In this paper, we use Fortran D <ref> (Fox et al., 1990) </ref> and High Performance Fortran (HPF) (High Performance Fortran Forum, 1993) as examples of languages that support a data-parallel programming paradigm. Section 3 describes these languages in more detail. A major advantage of data parallelism derives from its scalability. <p> suggest that the performance of basic communication and synchronization operations is competitive with, and in some cases superior to, conventional message-passing libraries (Foster and Chandy, 1992). 3 Fortran D and High Performance Fortran Fortran D was an early language designed by researchers at Rice and Syracuse for expressing data-parallel computation <ref> (Fox et al., 1990) </ref>. It provided data decomposition specifications, parallel loops, and methods for specifying reductions. Fortran D was designed to support machine-independent parallel programming and was motivated by the observation that few languages support efficient data placement (Pancake and Bergmark, 1990).
Reference: <author> Graham, S., Lucco, S., and Sharp, O. </author> <year> 1993. </year> <title> Orchestrating interactions among parallel computations. </title> <booktitle> In Proceedings of the SIGPLAN '93 conference on program language design and implementation, </booktitle> <address> Albuquerque, N.M. </address> <booktitle> Federal HPCC Program 1993. Workshop and Conference on Grand Challenges Applications and Software Technology. </booktitle> <address> GCW-0593. Pittsburgh, Pa. </address>
Reference-contexts: Relatively few researchers, however, have focused on the issue of integrating support for both types of parallelism. Languages for supporting or coordinating task-level parallelism include PCN (Chandy and Taylor, 1991; Foster, Olson, and Tuecke, 1992), Strand (Foster and Taylor, 1990), Linda (Carriero and Gelernter, 1989), and Delirium <ref> (Graham, Lucco, and Sharp, 1993) </ref>. Tools for exploiting task-level parallelism include Schedule (Dongarra and Sorensen, 1987), Hence (Beguelin et al., 1991), and CODE (Newton and Browne, 1992). PCF Fortran exemplifies a parallel language for exploiting fork-join and loop-level parallelism (Parallel Computing Forum, 1991).
Reference: <author> Hall, M. W., Hiranandani, S., Kennedy, K., and Tseng, C. </author> <year> 1992. </year> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, </address> <note> Minn. 20 Hatcher, </note> <author> P., and Quinn, M. </author> <year> 1991. </year> <title> Data-parallel programming on MIMD computers. Cam--bridge, Mass.: The MIT Press. High Performance Fortran Forum 1993. High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225. </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, Texas. </institution>
Reference-contexts: Once the analysis is completed, optimization by the data-parallel compiler may also introduce complexity. We have already mentioned overlap areas as a possible interaction. Another example is interprocedural optimization of communication. Some research has shown that moving communication across procedure boundaries can have great benefits <ref> (Hall et al., 1992) </ref>. However, if the compiler assumes that such optimizations are performed, the FM programmer who wants to call an HPF routine may face serious difficulties. In effect, the task-parallel programmer would have to replicate the data-parallel compiler's analysis.
Reference: <author> Hiranandani, S., Kennedy, K., and Tseng, C. </author> <year> 1992. </year> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Comm. ACM 35(8) </journal> <pages> 66-80. </pages>
Reference: <author> Hiranandani, S., Kennedy, K., and Tseng, C. </author> <year> 1992. </year> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM international conference on supercomputing, </booktitle> <address> Washington, D.C. </address>
Reference: <author> Hiranandani, S., Kennedy, K., and Tseng, C. </author> <year> 1993. </year> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, Oregon. </address>
Reference-contexts: Communication is extracted and combined into vector messages outside both the i and j loops. Preliminary experimental results show that the prototype Fortran D compiler can closely approach the performance of hand-optimized code for stencil computations, but requires additional improvements for linear algebra and pipelined codes <ref> (Hiranandani, Kennedy, and Tseng, 1993) </ref>. 12 4 Integrating FM and HPF In this section, we describe techniques that can be used to interface a task-parallel language (Fortran M) and a data-parallel language (High Performance Fortran).
Reference: <author> Ikudome, K., Fox, G., Kolawa, A., and Flower, J. </author> <year> 1990. </year> <title> An automatic and symbolic par-allelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th distributed memory computing conference, </booktitle> <address> Charleston, S.C. </address>
Reference-contexts: Compilers for data-parallel programs include Adapt (Merlin, 1991), Adaptor (Brandes, 1993) Aspar <ref> (Ikudome et al., 1990) </ref>, Callahan and Kennedy (Callahan and Kennedy, 1988), Forge90 (Applied Parallel Research, 1992), Id Nouveau (Rogers and Pingali, 1989), and Superb (Zima, Bast, and Gerndt, 1988).
Reference: <author> Knobe, K., Lukas, J., and Steele, Jr., G. </author> <year> 1990. </year> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <editor> J. </editor> <booktitle> Parallel and Distributed Computing 8(2) </booktitle> <pages> 102-118. </pages>
Reference-contexts: Since data-parallel programs are relatively close to sequential programs, many compiler analysis and optimization techniques can be adapted to produce parallel programs automatically. The mapping of data and computation can affect performance significantly <ref> (Knobe, Lukas, and Steele, 1990) </ref>. With data-parallel programs, relatively simple data decomposition annotations are sufficient to achieve high performance on advanced parallel architectures. If communication and parallelism are implicit (as in HPF), the user can easily tune the program by small modifications to its data decomposition annotations.
Reference: <author> Koelbel, C., and Mehrotra, P. </author> <year> 1991. </year> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Trans. Parallel and Distributed Systems 2(4) </journal> <pages> 440-451. </pages>
Reference-contexts: Languages for expressing data-parallelism include Fortran 90 (American National Standards Institute, 1990; CMF (Thinking Machines Corporation, 1991); Vienna Fortran (Chap-man, Mehrotra, and Zima, 1992), C* (Rose and Steele, 1987), Dataparallel C (Hatcher and Quinn, 1991), Dino (Rosing, Schnabel, and Weaver, 1991), Kali <ref> (Koelbel and Mehro-tra, 1991) </ref>, and Paragon (Chase et al., 1991).
Reference: <author> Massingill, B. </author> <year> 1993. </year> <title> Integrating task and data parallelism. </title> <type> TR CS-TR-93-01. </type> <institution> Pasadena: California Institute of Technology. </institution>
Reference-contexts: The Fortran D compiler adapts techniques from several of these systems, but performs greater compile-time analysis and optimization and relies less on language extensions and run-time support. Massingill describes mechanisms that allow the use of PCN to coordinate SPMD programs written in message-passing C <ref> (Massingill, 1993) </ref>. However, this framework is more restrictive than that considered here, in that SPMD computations must execute on disjoint sets of processors and cannot communicate.
Reference: <author> Merlin, J. </author> <year> 1991. </year> <title> ADAPTing Fortran-90 array programs for distributed memory architectures. </title> <booktitle> In First international conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria. </address>
Reference-contexts: Compilers for data-parallel programs include Adapt <ref> (Merlin, 1991) </ref>, Adaptor (Brandes, 1993) Aspar (Ikudome et al., 1990), Callahan and Kennedy (Callahan and Kennedy, 1988), Forge90 (Applied Parallel Research, 1992), Id Nouveau (Rogers and Pingali, 1989), and Superb (Zima, Bast, and Gerndt, 1988).
Reference: <author> Newton, P., and Browne, J. C. </author> <year> 1992. </year> <title> The CODE 2.0 graphical parallel programming language. </title> <booktitle> In Proceedings of the 1992 ACM international conference on supercomputing, </booktitle> <address> Wash-ington, D.C. </address>
Reference-contexts: Tools for exploiting task-level parallelism include Schedule (Dongarra and Sorensen, 1987), Hence (Beguelin et al., 1991), and CODE <ref> (Newton and Browne, 1992) </ref>. PCF Fortran exemplifies a parallel language for exploiting fork-join and loop-level parallelism (Parallel Computing Forum, 1991). In comparison with these systems, Fortran M is designed to provide greater support for modularity and safety.
Reference: <author> Pancake, C., and Bergmark, D. </author> <year> 1990. </year> <title> Do parallel languages respond to the needs of scientific programmers? IEEE Computer 23(12) 13-23. Parallel Computing Forum 1991. PCF: Parallel Fortran extensions. Fortran Forum 10(3). </title>
Reference-contexts: The FM programming model is dynamic: processes and channels can be created and deleted dynamically, and references to channels can be included in messages. Nevertheless, computation can be guaranteed to be deterministic; this feature avoids the race conditions that plague many parallel programming systems <ref> (Pancake and Bergmark, 1990) </ref>. Determinism is guaranteed by defining operations on port variables to prevent multiple processes from sending concurrently, by requiring receivers to block until data is available, and by enforcing a copy-in/copy-out semantics on variables passed as arguments to processes. <p> It provided data decomposition specifications, parallel loops, and methods for specifying reductions. Fortran D was designed to support machine-independent parallel programming and was motivated by the observation that few languages support efficient data placement <ref> (Pancake and Bergmark, 1990) </ref>. It brought together ideas found in earlier parallel languages such as CM Fortran (Thinking Machines Corporation 1991) and Kali (Koebel and Mehrotra, 1991), but in a unified framework designed to permit advanced compiler analysis and optimization.
Reference: <author> Rogers, A., and Pingali, K. </author> <year> 1989. </year> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 conference on program language design and implementation, </booktitle> <address> Portland, Oregon. </address> <note> 21 Rose, </note> <author> J., and Steele, Jr., G. </author> <year> 1987. </year> <title> C fl : An extended C language for data parallel program-ming. </title> <booktitle> In Proceedings of the second international conference on supercomputing, </booktitle> <editor> edited by L. Kartashev and S. Kartashev. </editor> <address> Santa Clara. </address>
Reference-contexts: Compilers for data-parallel programs include Adapt (Merlin, 1991), Adaptor (Brandes, 1993) Aspar (Ikudome et al., 1990), Callahan and Kennedy (Callahan and Kennedy, 1988), Forge90 (Applied Parallel Research, 1992), Id Nouveau <ref> (Rogers and Pingali, 1989) </ref>, and Superb (Zima, Bast, and Gerndt, 1988). The Fortran D compiler adapts techniques from several of these systems, but performs greater compile-time analysis and optimization and relies less on language extensions and run-time support.
Reference: <author> Rosing, M., Schnabel, R., and Weaver, R. </author> <year> 1991. </year> <title> The DINO parallel programming language. </title>
Reference-contexts: Languages for expressing data-parallelism include Fortran 90 (American National Standards Institute, 1990; CMF (Thinking Machines Corporation, 1991); Vienna Fortran (Chap-man, Mehrotra, and Zima, 1992), C* (Rose and Steele, 1987), Dataparallel C (Hatcher and Quinn, 1991), Dino <ref> (Rosing, Schnabel, and Weaver, 1991) </ref>, Kali (Koelbel and Mehro-tra, 1991), and Paragon (Chase et al., 1991).
Reference: <author> J. </author> <booktitle> Parallel and Distributed Computing 13(1) </booktitle> <pages> 30-42. </pages>
Reference: <author> Subhlok, J., Stichnoth, J., O'Hallaron, D., and Gross, T. </author> <year> 1993. </year> <title> Exploiting task and data parallelism on a multicomputer. </title> <booktitle> In Proceedings of the fourth ACM SIGPLAN symposium on principles and practice of parallel programming, </booktitle> <address> San Diego. </address> <institution> Thinking Machines Corporation 1991. </institution> <note> CM Fortran reference manual, version 1.0 edition. </note> <institution> Cambridge, Mass.: Thinking Machines Corp. </institution>
Reference-contexts: However, data distribution issues are not addressed in their prototype implementation. One of the few systems to examine both types of parallelism, the iWarp compiler pursues a sophisticated compile-time approach for exploiting both task and data parallelism on a mesh-connected distributed-memory machine <ref> (Subhlok et al., 1993) </ref>. The input Fortran program contains Fortran 90 array constructs, Fortran D data decomposition specifications, and parallel sections. The iWarp compiler analyzes statements and directives to produce a uniform task graph labeled with communication edges, maps each task to a processor, and 18 inserts communications.
Reference: <author> Tseng, C. </author> <year> 1993. </year> <title> An optimizing Fortran D compiler for MIMD distributed-memory machines. </title> <type> Ph.D. thesis. </type> <institution> Dept. of Computer Science, Rice University. </institution>
Reference-contexts: Communication is extracted and combined into vector messages outside both the i and j loops. Preliminary experimental results show that the prototype Fortran D compiler can closely approach the performance of hand-optimized code for stencil computations, but requires additional improvements for linear algebra and pipelined codes <ref> (Hiranandani, Kennedy, and Tseng, 1993) </ref>. 12 4 Integrating FM and HPF In this section, we describe techniques that can be used to interface a task-parallel language (Fortran M) and a data-parallel language (High Performance Fortran).
Reference: <author> Zima, H., Bast, H.-J., and Gerndt, M. </author> <year> 1988. </year> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <booktitle> Parallel Computing 6 </booktitle> <pages> 1-18. 22 </pages>
Reference-contexts: Manage storage. Buffers or "overlaps" <ref> (Zima, Bast, and Gerndt, 1988) </ref> created by extending the local array bounds are allocated to store nonlocal data. 7. Generate code. <p> Compilers for data-parallel programs include Adapt (Merlin, 1991), Adaptor (Brandes, 1993) Aspar (Ikudome et al., 1990), Callahan and Kennedy (Callahan and Kennedy, 1988), Forge90 (Applied Parallel Research, 1992), Id Nouveau (Rogers and Pingali, 1989), and Superb <ref> (Zima, Bast, and Gerndt, 1988) </ref>. The Fortran D compiler adapts techniques from several of these systems, but performs greater compile-time analysis and optimization and relies less on language extensions and run-time support.
References-found: 35

