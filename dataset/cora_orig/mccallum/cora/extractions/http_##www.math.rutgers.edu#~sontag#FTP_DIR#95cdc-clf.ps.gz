URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/95cdc-clf.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: sontag@control.rutgers.edu  sussmann@hamilton.rutgers.edu  
Title: Nonsmooth Control-Lyapunov Functions  
Author: Eduardo Sontag Hector J. Sussmann 
Address: New Brunswick, NJ 08903  New Brunswick, NJ 08903  
Affiliation: Dept. of Mathematics Rutgers University  Dept. of Mathematics Rutgers University  
Abstract: It is shown that the existence of a continuous control-Lyapunov function (CLF) is necessary and sufficient for null asymptotic controllability of nonlinear finite-dimensional control systems. The CLF condition is expressed in terms of a concept of generalized derivative that has been studied in set-valued analysis and the theory of differential inclusions with various names such as "upper contingent derivative." This result generalizes to the non-smooth case the theorem of Artstein relating closed-loop feedback stabilization to smooth CLF's. It relies on viability theory as well as optimal control techniques. A "non-strict" version of the results, analogous to the LaSalle Invariance Principle, is also provided. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Artstein, Z., </author> <title> "Stabilization with relaxed controls," Nonlinear Analysis, Theory, </title> <booktitle> Methods & Applications 7(1983): </booktitle> <pages> 1163-1173. </pages>
Reference-contexts: the existence of a CLF would imply that there is some feedback law u=k (x) so that the origin is a globally asymptotically stable state for the closed-loop system _x = f (x; k (x)) and k is continuous on R n n f0g. (This was proved by Artstein in <ref> [1] </ref>; cf. also [4, 7, 11]). But continuous feedback may fail to exist, even for very simple controllable systems (see e.g. [9], Section 4.8).
Reference: [2] <author> Aubin, J.-P., and A. Cellina, </author> <title> Differential Inclusions: Set-Valued Maps and Viability Theory, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: The proof follows immediately by combining the main result in [10], which gave a necessary condition expressed in terms of Dini derivatives of trajectories, with results from <ref> [2] </ref>. Thus, asymptotic controllability implies the existence of a "Lyapunov function" in the strict sense that derivatives are negative for nonzero states. <p> We now introduce an object widely studied in Set-Valued Analysis (cf., for instance, <ref> [2] </ref>, Def. 1 and Prop. 1 of Section 6.1, where it is called the "upper contingent derivative.") Definition 2.3 For a function V : R n ! R [ f+1g, a ~ 2 R n such that F (~) &lt; +1, and a v 2 R n , the directional subderivative <p> such that F (~) &lt; +1, and a v 2 R n , the directional subderivative of V in the direction of v at ~ is v V (~) := lim inf w ! v t (The notations D + V (~)(v) and D " V (~)(v) are used in <ref> [2, 5] </ref> and [3] respectively, with the same meaning as our D For each fixed ~, the map v7!D v V (~) is lower semi-continuous as an extended-real valued function (cf. [2], page 286); thus fvjD v V (~) ffg is a closed set for any ff. <p> inf w ! v t (The notations D + V (~)(v) and D " V (~)(v) are used in [2, 5] and [3] respectively, with the same meaning as our D For each fixed ~, the map v7!D v V (~) is lower semi-continuous as an extended-real valued function (cf. <ref> [2] </ref>, page 286); thus fvjD v V (~) ffg is a closed set for any ff. Observe that if V is Lipschitz continuous then this definition coincides with that of the classical Dini derivative, that is, lim inf t!0+ [V (~ + tv) V (~)]=t. <p> If U is a compact topological space and f : X fi U ! Y is continuous, then the set valued map F (x) := F (x; U ) = ff (x; u); u 2 U g is USC (see for instance <ref> [2] </ref>, Prop. 1 in Section 1.2). We will henceforth use the abbreviations DI and USCMCC for "differential inclusion" and "upper semi-continuous multifunction with compact convex values," respectively. Let X be a subset of Y = R n . <p> The second ingredient needed to prove Theorem 1 is from the literature on differential inclusions and viability theory. The relevant results are as follows. (We give them in a slightly stronger form than needed, but still not in full generality: in <ref> [2] </ref>, the function "W " is allowed to depend convexly on derivatives _x (t), and in some implications less than continuity of V or W is required.) Theorem 1 in Section 6.3 of [2] shows that 2 implies 1 (with T =1 if X is closed and F (X) is bounded), <p> (We give them in a slightly stronger form than needed, but still not in full generality: in <ref> [2] </ref>, the function "W " is allowed to depend convexly on derivatives _x (t), and in some implications less than continuity of V or W is required.) Theorem 1 in Section 6.3 of [2] shows that 2 implies 1 (with T =1 if X is closed and F (X) is bounded), and Proposition 2 in Section 6.3 of [2] says that 1)2. (Another good reference is [5]; see in particular Theorem 14.1 there.) Fact 2.8 Let F be an USCMCC from X into subsets <p> depend convexly on derivatives _x (t), and in some implications less than continuity of V or W is required.) Theorem 1 in Section 6.3 of <ref> [2] </ref> shows that 2 implies 1 (with T =1 if X is closed and F (X) is bounded), and Proposition 2 in Section 6.3 of [2] says that 1)2. (Another good reference is [5]; see in particular Theorem 14.1 there.) Fact 2.8 Let F be an USCMCC from X into subsets of R n , where X is a locally compact subset of R n . <p> Then D f (x;y) V (x; y) &lt; 0 if (x; y) =2 S. If r &gt; 0, then Fact 2.8 |with W 0| easily implies that the function <ref> [0; 2] </ref> 3 t 7! h r (t) = V (r cos t; r sin t) is nonincreasing on (0; 2). Since V is continuous, and h r (0) = h r (2), we conclude that h r is constant. So V is in fact a radial function, i.e.
Reference: [3] <author> Aubin, J.-P., </author> <title> Viability Theory, </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: (~) &lt; +1, and a v 2 R n , the directional subderivative of V in the direction of v at ~ is v V (~) := lim inf w ! v t (The notations D + V (~)(v) and D " V (~)(v) are used in [2, 5] and <ref> [3] </ref> respectively, with the same meaning as our D For each fixed ~, the map v7!D v V (~) is lower semi-continuous as an extended-real valued function (cf. [2], page 286); thus fvjD v V (~) ffg is a closed set for any ff.
Reference: [4] <author> Bacciotti, A., </author> <title> Local Stabilizability of Nonlinear Control Systems, </title> <publisher> World Scientific, </publisher> <address> London, </address> <year> 1991. </year>
Reference-contexts: a CLF would imply that there is some feedback law u=k (x) so that the origin is a globally asymptotically stable state for the closed-loop system _x = f (x; k (x)) and k is continuous on R n n f0g. (This was proved by Artstein in [1]; cf. also <ref> [4, 7, 11] </ref>). But continuous feedback may fail to exist, even for very simple controllable systems (see e.g. [9], Section 4.8).
Reference: [5] <author> Deimling, K., </author> <title> Multivalued Differential Equations, </title> <publisher> de Gruyter, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: such that F (~) &lt; +1, and a v 2 R n , the directional subderivative of V in the direction of v at ~ is v V (~) := lim inf w ! v t (The notations D + V (~)(v) and D " V (~)(v) are used in <ref> [2, 5] </ref> and [3] respectively, with the same meaning as our D For each fixed ~, the map v7!D v V (~) is lower semi-continuous as an extended-real valued function (cf. [2], page 286); thus fvjD v V (~) ffg is a closed set for any ff. <p> some implications less than continuity of V or W is required.) Theorem 1 in Section 6.3 of [2] shows that 2 implies 1 (with T =1 if X is closed and F (X) is bounded), and Proposition 2 in Section 6.3 of [2] says that 1)2. (Another good reference is <ref> [5] </ref>; see in particular Theorem 14.1 there.) Fact 2.8 Let F be an USCMCC from X into subsets of R n , where X is a locally compact subset of R n . Assume that V and W are two continuous functions X!R 0 . <p> In the example of Remark 4.3, V is a WCLF but the system is not AC, so (iii) ) (i) fails. So all we need is an example of an AC system for which there is no CLF. (It is proved in <ref> [5] </ref> that an AC DI arising from an USCMCC always has a lower semicontinuous "CLF." Our definition requires the CLF to be continuous.) We let f : R 2 ! R 2 be given by f (x; y) = (y; x). Let S = R 0 fi f0g.
Reference: [6] <author> Long, Y., and M. M. Bayoumi, </author> <title> "Feedback stabilization: Control Lyapunov functions modeled by neural networks," </title> <booktitle> in Proc. IEEE Conf. Decision and Control , San Antonio, </booktitle> <address> Dec. 1993, </address> <publisher> IEEE Publications, </publisher> <year> 1993, </year> <pages> pp. 2812-2814. </pages>
Reference-contexts: feedback control design (see the references in [10], and e.g. the many examples in Section 3.6 of the textbook [8]), the optimal control approach of Bellman, "artificial intelligence" techniques based on position evaluations in games and "critics" in learning programs, and can be found in "neural-network" control design (see e.g. <ref> [6] </ref>). An obvious fundamental question arises: is the existence of a continuously differentiable CLF equivalent to the possibility of driving every state asymptotically to zero? If the question is stated in this form, then it is well-known that the answer is negative.
Reference: [7] <author> Isidori, A., </author> <title> Nonlinear Control Systems: An Introduction, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, third ed., </address> <year> 1995. </year>
Reference-contexts: a CLF would imply that there is some feedback law u=k (x) so that the origin is a globally asymptotically stable state for the closed-loop system _x = f (x; k (x)) and k is continuous on R n n f0g. (This was proved by Artstein in [1]; cf. also <ref> [4, 7, 11] </ref>). But continuous feedback may fail to exist, even for very simple controllable systems (see e.g. [9], Section 4.8).
Reference: [8] <author> Slotine, J-J.E., and W. Li, </author> <title> Applied Nonlinear Control, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1991. </year>
Reference-contexts: The idea underlies feedback control design (see the references in [10], and e.g. the many examples in Section 3.6 of the textbook <ref> [8] </ref>), the optimal control approach of Bellman, "artificial intelligence" techniques based on position evaluations in games and "critics" in learning programs, and can be found in "neural-network" control design (see e.g. [6]).
Reference: [9] <author> Sontag, E.D., </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: This property guarantees that for each state ~ there is some control u () such that, solving the initial-value problem (1) with x (0) = ~, the resulting trajectory satisfies x (t)!0 as t!+1. The argument is standard; see for instance the textbook <ref> [9] </ref>. A function V which is part of a Lyapunov pair is generically called a control-Lyapunov function, henceforth abbreviated "CLF." The CLF paradigm is extremely powerful. <p> But continuous feedback may fail to exist, even for very simple controllable systems (see e.g. <ref> [9] </ref>, Section 4.8).
Reference: [10] <author> Sontag, E.D., </author> <title> "A Lyapunov-like characterization of asymptotic controllability," </title> <journal> SIAM J. Control & Opt. </journal> <volume> 21(1983): </volume> <month> 462-471. </month> <title> (See also "A characterization of asymptotic controllability," in Dynamical Systems II (A. </title> <editor> Bednarek and L. Cesari, eds.), </editor> <publisher> Academic Press, </publisher> <address> NY, </address> <year> 1982, </year> <pages> pp. 645-648.) </pages>
Reference-contexts: The idea underlies feedback control design (see the references in <ref> [10] </ref>, and e.g. the many examples in Section 3.6 of the textbook [8]), the optimal control approach of Bellman, "artificial intelligence" techniques based on position evaluations in games and "critics" in learning programs, and can be found in "neural-network" control design (see e.g. [6]). <p> The proof follows immediately by combining the main result in <ref> [10] </ref>, which gave a necessary condition expressed in terms of Dini derivatives of trajectories, with results from [2]. Thus, asymptotic controllability implies the existence of a "Lyapunov function" in the strict sense that derivatives are negative for nonzero states. <p> 0 is the origin.) The map f : Xfi U ! R n is assumed to be locally Lipschitz with respect to (x; u) and to satisfy f (0; 0) = 0. (The Lipschitz property with respect to u can be weakened, but we will need to quote results from <ref> [10] </ref>, where this was made as a blanket assumption.) A control is a bounded measurable map u : I u ! U , where I u 2 I. <p> The first ingredient in the proof is the following restatement of the main result in <ref> [10] </ref>. <p> If there are such V , W , and -, then for each ~ we may pick a ! so that (4) holds; this implies the inequality lim inf t!0+ t 1 [V (x (t)) V (~)] W (~), which is the sufficient condition for AC given in <ref> [10] </ref>. <p> + maxfk!kk; 0g where the minimum is taken over the set of all relaxed controls !: [0; 1)!P (U -(j~j) ), and k is a constant which arises from the function in the definition of AC. (Here we take W (x)=N (jxj), where N : R 0 !R 0 from <ref> [10] </ref> is a strictly increasing, continuous function satisfying also N (0) = 0 and lim r!+1 N (r) = +1, i.e. a function of class K 1 . <p> Remark 2.9 The proof actually shows that in the AC case one has trajectories, corresponding to relaxed controls, which are monotone with respect to V and W , and are defined on the entire [0; +1). (Observe that the cost function used in <ref> [10] </ref> is not additive, because of the term "maxfk!kk; 0g", so the dynamic programming principle does not apply, and hence we cannot conclude that optimal trajectories are monotone.
Reference: [11] <author> Sontag, E.D., </author> <title> "A `universal' construction of Art-stein's theorem on nonlinear stabilization," </title> <journal> Systems and Control Letters, </journal> <volume> 13(1989): </volume> <pages> 117-123. </pages>
Reference-contexts: a CLF would imply that there is some feedback law u=k (x) so that the origin is a globally asymptotically stable state for the closed-loop system _x = f (x; k (x)) and k is continuous on R n n f0g. (This was proved by Artstein in [1]; cf. also <ref> [4, 7, 11] </ref>). But continuous feedback may fail to exist, even for very simple controllable systems (see e.g. [9], Section 4.8).
References-found: 11

