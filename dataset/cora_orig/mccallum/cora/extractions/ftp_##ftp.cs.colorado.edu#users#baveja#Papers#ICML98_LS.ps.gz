URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/ICML98_LS.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: baveja@cs.colorado.edu  
Title: Using Eligibility Traces to Find the Best Memoryless Policy in Partially Observable Markov Decision Processes  
Author: John Loch Satinder Singh 
Address: Boulder, CO 80309-0430  Boulder, CO 80309-0430  
Affiliation: University of Colorado  University of Colorado  
Abstract: Recent research on hidden-state RL problems has concentrated on overcoming partial observability by using memory to estimate state. However, such methods are computationally extremely expensive and thus have very limited applicability. This emphasis on state estimation has come about because it has been widely observed that the presence of hidden state or partial observability renders popular RL methods such as Q-learning and Sarsa useless. However, this observation is misleading in two ways: first, the theoretical results supporting it only apply to RL algorithms that do not use eligibility traces, and second that these results are worst-case results which leaves open the possibility that there may be large classes of hidden-state problems in which RL algorithms work well without any state estimation. In this paper we show empirically that Sarsa(), a well known family of RL algorithms that use eligibility traces, can work very well on hidden state problems that have good memoryless policies, i.e., on RL problems in which there may well be very poor observability but there also exists a mapping from immediate observations to actions that yields near-optimal return. We apply conventional Sarsa() to four test problems taken from the recent work of Littman, Littman Cassandra and Kaelbling, Parr and Russell, and Chrisman, and in each case we show that it is able to find the best, or a very good, memoryless policy without any of the computational expense of state estimation. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 835-846. </pages>
Reference: <author> Chrisman, L. </author> <year> (1992). </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In AAAI-92. </booktitle>
Reference: <author> Jaakkola, T., Singh, S. P., & Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <editor> In Tesauro, G., touretzky, D. S., & Leen, T. K. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 345-352. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lin, L. J. & Mitchell, T. M. </author> <year> (1992). </year> <title> Reinforcement learning with hidden states. </title> <booktitle> In In Proceedings of the Second International Conference on Simulation of Adaptive Behavior: From Animals to Animats. </booktitle>
Reference-contexts: However, in many domains, e.g., in mobile robotics, in multi-agent or distributed control environments, etc., the agent's sensors at best give it partial information about the state of the environment. Such agent-environment interactions suffer from hidden-state <ref> (Lin & Mitchell, 1992) </ref> or perceptual aliasing (Whitehead & Ballard, 1990; Chrisman, 1992) and can be formulated as partially observable Markov decision processes, or POMDPs (Sondik, 1978). Therefore, finding efficient reinforcement learning methods for solving interesting sub-classes of POMDPs is of great practical interest to AI and engineering.
Reference: <author> Littman, M., Cassandra, A., & Kaelbling., L. </author> <year> (1995). </year> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In Prieditis, A. & Russell, S. (Eds.), </editor> <booktitle> Proceedings of the 15 Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 362-370, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Recent research on POMDPs has concentrated on overcoming partial ob-servability by using memory to estimate state (Chrisman, 1992; McCallum, 1993; Lin & Mitchell, 1992) and on developing special purpose planning and learning methods that work with the agent's state of knowledge, or belief state <ref> (Littman et al., 1995) </ref>. <p> In this problem, the best memoryless policy is rather poor compared to policies which use memory. The best memoryless policy yields an average reward per step of 0:024 compared to the memory-based policy found by the Witness algorithm <ref> (Littman et al., 1995) </ref> which yields an average reward per step of 0:1108. Parr & Russell's SPOVA-RL (Smooth Partially Observable Value Approximation Reinforcement Learning) algorithm learns a value function over belief states and did even better yielding an average reward per step of 0:12 with a memory-based policy.
Reference: <author> Littman, M. L. </author> <year> (1994). </year> <title> Memoryless policies: theoretical limitations and practical results. </title> <editor> In Cliff, D., Husbands, P., Meyer, J., & Wilson, S. W. (Eds.), </editor> <booktitle> From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior. </booktitle>
Reference-contexts: However, these results have to be interpreted with caution for the problem of finding optimal memoryless policies in POMDPs is known to be computation-ally challenging <ref> (Littman, 1994) </ref>; they are evidence that Sarsa () is at least competitive to and at best better than other existing algorithms for solving POMDPs when good low-order-memory-based policies exist. 2 POMDP Framework In this section we briefly describe the POMDP framework.
Reference: <author> McCallum, R. A. </author> <year> (1993). </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <editor> In Utgoff, P. (Ed.), </editor> <booktitle> Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <pages> pages 190-196. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Parr, R. & Russell, S. </author> <year> (1995). </year> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence. </booktitle>
Reference-contexts: Sarsa () required less the 60 CPU seconds (on a PC) to find its policy compared to the 12 CPU hours <ref> (Parr & Russell, 1995) </ref> required by the Witness algorithm. The 3-observation performance is shown in Figure 3D and is the same as the 2-observation performance. We were able to verify that the policy found by Sarsa () using 1 previous observation was indeed the optimal policy in that space.
Reference: <author> Rummery, G. A. & Niranjan, M. </author> <year> (1994). </year> <title> On-line Q-learning using connectionist systems. </title> <type> Technical Report Techical Report CUED/F-INFENG/TR 166, </type> <institution> Cambridge University Engineering Dept. </institution>
Reference-contexts: In part, this emphasis on state estimation has come about because it has been widely observed and noted that the presence of hidden state renders popular and succesful RL methods for MDPs, such as Q-learning (Watkins, 1989) and Sarsa <ref> (Rummery & Niranjan, 1994) </ref>, useless on POMDPs (e.g., Whitehead, 1992; Littman, 1994; Singh et al., 1994).
Reference: <author> Singh, S. P., Jaakkola, T., & Jordan, M. I. </author> <year> (1994). </year> <title> Learning without state-estimation in partially observable Markovian decision processes. </title> <editor> In Cohen, W. W. & Hirsh, H. (Eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 284-292. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, this observation is misleading in two ways: first, the theoretical results supporting it only apply to RL algorithms that do not use eligibility traces <ref> (Singh et al., 1994) </ref>, and second, these results are worst-case results which leaves open the possibility that there may be large classes of POMDPs in which existing RL algorithms work well without any state estimation. <p> RL algorithms such as Q-learning and Sarsa are able to provably find such memoryless optimal policies in MDPs. It is known that in POMDPs the best memoryless policy can be arbitrarily suboptimal in the worst case <ref> (Singh et al., 1994) </ref>. <p> In all cases, a value of between 0:8 and 0:975 worked the best. This is qualitatively similar to the results obtained for MDPs, and a bit surprising given that Sarsa (1) (or Monte-Carlo) has been recommended as the way to deal with hidden state <ref> (Singh et al., 1994) </ref>. The data for the learning curves is generated as follows: after every 1000 steps (actions) the greedy policy is evaluated o*ine to generate a problem specific performance metric.
Reference: <author> Sondik, E. J. </author> <year> (1978). </year> <title> The optimal control of partially observable Markov processes over the infinite horizon: discounted case. </title> <journal> Operations Research, </journal> <volume> 26, </volume> <pages> 282-304. </pages>
Reference-contexts: Such agent-environment interactions suffer from hidden-state (Lin & Mitchell, 1992) or perceptual aliasing (Whitehead & Ballard, 1990; Chrisman, 1992) and can be formulated as partially observable Markov decision processes, or POMDPs <ref> (Sondik, 1978) </ref>. Therefore, finding efficient reinforcement learning methods for solving interesting sub-classes of POMDPs is of great practical interest to AI and engineering.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. & Barto, A. G. </author> <year> (1998). </year> <title> Reinforcement Learning: An Introduction. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: 1 Introduction Sequential decision problems in which an agent's sensory observations provide it with the complete state of its environment can be formulated as Markov decision processes, or MDPs, for which a number of very succesful planning <ref> (Sutton & Barto, 1998) </ref> and reinforcement learning (Barto et al., 1983; Sutton, 1988; Watkins, 1989) methods have been developed. However, in many domains, e.g., in mobile robotics, in multi-agent or distributed control environments, etc., the agent's sensors at best give it partial information about the state of the environment.
Reference: <author> Tesauro, G. J. </author> <year> (1995). </year> <title> Temporal difference learning and td-gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38 (3), </volume> <pages> 58-68. </pages>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cambridge, England. </address>
Reference-contexts: In part, this emphasis on state estimation has come about because it has been widely observed and noted that the presence of hidden state renders popular and succesful RL methods for MDPs, such as Q-learning <ref> (Watkins, 1989) </ref> and Sarsa (Rummery & Niranjan, 1994), useless on POMDPs (e.g., Whitehead, 1992; Littman, 1994; Singh et al., 1994).
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990). </year> <title> Active perception and reinforcement learning. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, TX. </address>
Reference: <institution> M. </institution>
References-found: 18

