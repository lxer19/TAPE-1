URL: http://www.research.att.com/library/trs/TRs/97/97.21/97.21.2.body.ps
Refering-URL: http://www.research.att.com/library/trs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: On Reduction via Determinization of Speech-Recognition Lattices  
Author: Adam L. Buchsbaum Raffaele Giancarlo Jeffery R. Westbrook 
Keyword: L  
Address: 180 Park Ave., Florham Park, NJ 07932, USA,  Palermo, Via Archirafi 34, 90123 Palermo, Italy, raf  180 Park Ave., Florham Park, NJ 07932, USA,  
Affiliation: AT&T Labs,  Dipartimento di Matematica ed Applicazioni, Universita di  AT&T Labs,  
Note: AT&T Labs|Research TR 97.21.2  Work supported by AT&T Labs.  
Email: alb@research.att.com.  faele@altair.math.unipa.it.  jeffw@research.att.com.  
Date: July 8, 1997  July 8, 1997  
Abstract: We establish a framework for studying the behavior of automatic speech recognition (ASR) lattices (viewed as automata) undergoing determinization. Using this framework, we provide initial insights into what causes determinization to produce smaller (or bigger) lattices when used in the ASR application. Our results counter the prevailing wisdom that the graph topology underlying an automaton, not the weights on the arcs, governs deterministic expansion. We show that there are graphs that expand solely due to their weights when determinized; i.e., we demonstrate graphs that expand under some weightings yet contract under others. Furthermore, we give evidence that the automata that arise in ASR are either the kind that never expand or else the weight-dependent kind; i.e., we do not find in ASR any instances of automata that always expand under determinization. Therefore, understanding what causes weight dependence becomes essential to providing tools to avoid deterministic expansion in ASR, and we provide some theoretical results that start to explain this behavior. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. R. Bahl, F. Jelinek, and R. L. Mercer. </author> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5:179-190, </volume> <year> 1983. </year>
Reference-contexts: In particular, the A fl algorithm [20] generalizes the classical Dijkstra shortest-path algorithm [14] by exploiting estimates of the shortest path remaining from any particular node to the final node. Beam searching <ref> [1, 26] </ref> is the application of a threshold to the Viterbi algorithm [41] (essentially the Bellman-Ford shortest-path algorithm [2, 17]), in order to stop exploration of paths that are not locally favorable. <p> As the following claim shows, retaining enough bits of the log-random weights drives the collision probability (as discussed in the proof of Theorem 5.6) low enough to ensure this behavior. Claim 5.7 Let X; Y; V; Z 2 R <ref> [1; 2 k 1] </ref>. Let R = b2 b log Xc, S = b2 b log Y c, T = b2 b log V c, and U = b2 b log Zc, for some b k O (1). <p> Thus we derive the analogue to Theorem 5.6 for log-random weights. Theorem 5.8 Let the arcs of RG (k) be weighted with logarithms of numbers chosen independently and uniformly at random from <ref> [1; 2 k 1] </ref> Z . Then the expected number of states in the determinized graph is fi (2 k ).
Reference: [2] <author> R. Bellman. </author> <title> On a routing problem. </title> <journal> Quarterly of Applied Mathematics, </journal> <volume> 16 </volume> <pages> 87-90, </pages> <year> 1958. </year>
Reference-contexts: Beam searching [1, 26] is the application of a threshold to the Viterbi algorithm [41] (essentially the Bellman-Ford shortest-path algorithm <ref> [2, 17] </ref>), in order to stop exploration of paths that are not locally favorable. In the general case, these heuristics eliminate the guarantee that the selected path will indeed be shortest; the peculiar nature of the ASR application, however, tends to make them succeed in practice.
Reference: [3] <author> J. Berstel. </author> <title> Transduction and Context-Free Languages, volume 38 of Leitfaden der angewandten Mathe-matik und Mechanik LAMM. </title> <publisher> Springer-Verlag, </publisher> <year> 1979. </year>
Reference-contexts: In particular, all the lattices in the above ASR cascade can be considered to be finite-state transducers, and each process in the cascade performs a generalized composition operation between its inputs. Berstel et al. <ref> [3, 4] </ref>, Eilenberg [15], and Elgot and Mezei [16] give extensive treatments of the theory of rational trans-ductions and languages and its correspondence to automata. For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR.
Reference: [4] <author> J. Berstel and C. Reutenauer. </author> <title> Rational Series and Their Languages, </title> <booktitle> volume 12 of EATCS Monographs on Theoretical Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: In particular, all the lattices in the above ASR cascade can be considered to be finite-state transducers, and each process in the cascade performs a generalized composition operation between its inputs. Berstel et al. <ref> [3, 4] </ref>, Eilenberg [15], and Elgot and Mezei [16] give extensive treatments of the theory of rational trans-ductions and languages and its correspondence to automata. For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR.
Reference: [5] <author> E. Bocchieri, G. Riccardi, and J. Anantharaman. </author> <title> The 1994 AT&T ATIS CHRONUS recognizer. </title> <booktitle> In Proc. ARPA Spoken Language Technology Workshop, </booktitle> <pages> pages 265-8, </pages> <year> 1995. </year>
Reference-contexts: canonical weight-dependent topologies we find will also manifest these behaviors. 4 Experimental Results from ASR Lattices 4.1 Data These experiments were conducted on a collection of 100 word lattices generated by the AT&T North American Business speech recognizer [36], using a grammar for the Air Travel Information System (ATIS) task <ref> [5] </ref>. Each graph came with a weighting generated by the ASR software; we refer to these weights as speech weights. 4.2 Method For each input, we first determinized the graph with its original set of speech weights. Then we set all edge weights to zero and determinized the result.
Reference: [6] <author> D. Breslauer. </author> <title> The suffix tree of a tree and minimizing sequential transducers. </title> <booktitle> In Proc. 7th Symposium on Combinatorial Pattern Matching, </booktitle> <year> 1996. </year> <month> 24 </month>
Reference-contexts: For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR. Analogues to classical automata determiniza-tion and minimization have been developed and applied to finite-state transducers <ref> [6, 28, 27, 29, 35, 38] </ref> (see Buchsbaum and Giancarlo [7] for historical context and more detailed comparisons of the two ASR perspectives) and, again, whereas in general we cannot expect guaranteed size reductions, the nature of the ASR task seems to constrain the lattices so that these methods tend to
Reference: [7] <author> A. L. Buchsbaum and R. Giancarlo. </author> <title> Algorithmic aspects in speech recognition: An introduction. </title> <journal> ACM Journal of Experimental Algorithmics, </journal> <volume> 2, </volume> <year> 1997. </year> <note> http://www.jea.acm.org. </note>
Reference-contexts: The acoustic models, lexicon, and grammar can be viewed as lattices in addition to the intermediate lattices passed between processes. Buchsbaum and Giancarlo <ref> [7] </ref> provide an overview of speech recognition in an algorithmic framework and give extensive references for details on all the processes in the cascade. It should be intuitive that we can speed the ASR process by producing smaller lattices that encapsulate relevant information. <p> For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR. Analogues to classical automata determiniza-tion and minimization have been developed and applied to finite-state transducers [6, 28, 27, 29, 35, 38] (see Buchsbaum and Giancarlo <ref> [7] </ref> for historical context and more detailed comparisons of the two ASR perspectives) and, again, whereas in general we cannot expect guaranteed size reductions, the nature of the ASR task seems to constrain the lattices so that these methods tend to achieve impressive size reductions. 1 labeled by a word followed
Reference: [8] <author> J. L. Carter and M. N. Wegman. </author> <title> Universal classes of hash functions. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18 </volume> <pages> 143-54, </pages> <year> 1979. </year>
Reference-contexts: Then we set all edge weights to zero and determinized the result. We then applied random weights to the arcs and determinized the result. For each value of i 2 <ref> [0; 8] </ref> we applied weights chosen independently and uniformly at random in the range [0; 2 i 1] to the arcs of the graph and determinized the result. (We performed one trial for each such i.) The experiments were performed on one 150 MHz R4400 processor of a four-processor SGI Challenge <p> Suppose w 1 6= w 2 . What is the probability that R (w 1 ) = R (w 2 )? We can adapt a standard analysis from the theory of universal hash functions <ref> [8] </ref>. Let w 1 = ff 1 ff k and w 2 = fi 1 fi k . Without loss of generality, assume ff k 6= fi k . (The strings must differ somewhere.) Suppose R (w 1 ) = R (w 2 ).
Reference: [9] <author> C. Choffrut. </author> <title> Contributions a l'etude de quelques familles remarquables de function rationnelles. </title> <type> PhD thesis, </type> <address> LITP-Universite Paris 7, Paris, France, </address> <year> 1978. </year>
Reference-contexts: Determinization of weighted automata turns out to be challenging theoretically as well as relevant for its applications to ASR. Theoretically, not all weighted automata can be determinized. It is therefore natural to seek conditions under which a given weighted automaton can be determinized. Elaborating on results obtained by Choffrut <ref> [9] </ref> and Weber and Klemm [42] in the realm of string-to-string transduction, Mohri [28, 29] identifies those conditions and provides an algorithm that checks whether they hold; the algorithm is constructive in the case that the input automaton can be determinized.
Reference: [10] <author> L. Comtet. </author> <title> Advanced Combinatorics: The Art of Finite and Infinite Sequences. </title> <address> D. </address> <publisher> Reidel, Dordrecht, Holland, </publisher> <year> 1974. </year>
Reference-contexts: Can we combine determinization and minimization into a unified algorithm that would avoid this "unnecessary" expansion? Acknowledgements We thank Mehryar Mohri and Fernando Perieira for some fruitful discussions and Bob Sedgewick for pointing us to Comtet <ref> [10] </ref>. Additionally, Fernando provided us with the lattices we used in our experiments.
Reference: [11] <author> K. Culik II and J. Karhumaki. </author> <title> Finite automata computing real functions. </title> <journal> SIAM Journal on Computing, </journal> <volume> 23(4) </volume> <pages> 789-814, </pages> <year> 1994. </year>
Reference-contexts: While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions <ref> [11, 13] </ref>, to compress images [12, 22], and in natural language processing [27, 28, 29, 37, 39]. Other applications to ASR have also been described [30, 31, 36]. 1.2 Determinization of WFA Determinization of WFA is the following problem.
Reference: [12] <author> K. Culik II and P. Rajcani. </author> <title> Iterative weighted finite transductions. </title> <journal> Acta Informatica, </journal> <volume> 32 </volume> <pages> 681-703, </pages> <year> 1995. </year>
Reference-contexts: While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions [11, 13], to compress images <ref> [12, 22] </ref>, and in natural language processing [27, 28, 29, 37, 39]. Other applications to ASR have also been described [30, 31, 36]. 1.2 Determinization of WFA Determinization of WFA is the following problem.
Reference: [13] <author> D. Derencourt, J. Karhumaki, M. Latteux, and A. Terlutte. </author> <title> On computational power of weighted finite automata. </title> <booktitle> In Proc. 17th Symp. on Mathematical Foundations of Computer Science, volume 629 of Lecture Notes in Computer Science, </booktitle> <pages> pages 236-45. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions <ref> [11, 13] </ref>, to compress images [12, 22], and in natural language processing [27, 28, 29, 37, 39]. Other applications to ASR have also been described [30, 31, 36]. 1.2 Determinization of WFA Determinization of WFA is the following problem.
Reference: [14] <author> E. W. Dijkstra. </author> <title> A note on two problems in connexion with graphs. </title> <journal> Numerische Mathematik, </journal> <volume> 1 </volume> <pages> 269-271, </pages> <year> 1959. </year>
Reference-contexts: From this perspective, attempts to reduce the sizes of the intermediate lattices fall into the realm of heuristic search methods, many of which have been investigated for the ASR application. In particular, the A fl algorithm [20] generalizes the classical Dijkstra shortest-path algorithm <ref> [14] </ref> by exploiting estimates of the shortest path remaining from any particular node to the final node.
Reference: [15] <author> S. Eilenberg. </author> <title> Automata, Languages, and Machines, volume A. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1974. </year>
Reference-contexts: In particular, all the lattices in the above ASR cascade can be considered to be finite-state transducers, and each process in the cascade performs a generalized composition operation between its inputs. Berstel et al. [3, 4], Eilenberg <ref> [15] </ref>, and Elgot and Mezei [16] give extensive treatments of the theory of rational trans-ductions and languages and its correspondence to automata. For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR. <p> Obviously, the automaton can be represented as a directed graph, and a path from initial state to final state naturally corresponds to a sequence of transitions; Definition 3.1 formalizes this correspondence. Again we refer the reader to Eilenberg <ref> [15] </ref> for basic information on weighted automata. Weighted automata have been extensively studied and find uses in other applications in addition to ASR. Rabin [32] defines probabilistic automata, in which transition functions are sets of stochastic matrices.
Reference: [16] <author> C. C. Elgot and J. E. Mezei. </author> <title> On relations defined by generalized finite automata. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 9 </volume> <pages> 47-68, </pages> <year> 1965. </year>
Reference-contexts: In particular, all the lattices in the above ASR cascade can be considered to be finite-state transducers, and each process in the cascade performs a generalized composition operation between its inputs. Berstel et al. [3, 4], Eilenberg [15], and Elgot and Mezei <ref> [16] </ref> give extensive treatments of the theory of rational trans-ductions and languages and its correspondence to automata. For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR.
Reference: [17] <author> L. R. Ford and D. R. Fulkerson. </author> <title> Flows in Networks. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1962. </year>
Reference-contexts: Beam searching [1, 26] is the application of a threshold to the Viterbi algorithm [41] (essentially the Bellman-Ford shortest-path algorithm <ref> [2, 17] </ref>), in order to stop exploration of paths that are not locally favorable. In the general case, these heuristics eliminate the guarantee that the selected path will indeed be shortest; the peculiar nature of the ASR application, however, tends to make them succeed in practice.
Reference: [18] <author> J. Goldstine, C. M. R. Kintala, and D. Wotschke. </author> <title> On measuring nondeterminism in regular languages. </title> <journal> Information and Computation, </journal> <volume> 86 </volume> <pages> 179-94, </pages> <year> 1990. </year>
Reference-contexts: We will see in Section 5 that the weight-dependent graphs below exhibit this same phenomenon in a different form. Finally, Goldstine, Kintala, and Wotschke <ref> [18] </ref> refine the hierarchy of Kintala and Wotschke and define the spectrum of a regular language L. The spectrum of L provides the minimum number of states required for an NFA A that accepts L as a function of a suitably defined amount of nondeterminism realized by A. <p> Theorems 5.3 and 5.4 imply that greater deterministic expansions of RG (k) are possible as weights are encoded by additional bits, up to (G). This provides a type of weighted spectrum analogous to the spectra of Goldstine, Kintala, and Wotschke <ref> [18] </ref>. The relationships between the two ideas deserve further study. Finally, recall from Section 2 that the hot graphs contained some nondeterministic choices that could not be resolved until the end of the input. This caused the respective deterministic expansions.
Reference: [19] <author> D. Harel. </author> <title> A linear time algorithm for finding dominators in flow graphs and related problems. </title> <booktitle> In Proc. 17th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 185-94, </pages> <year> 1985. </year>
Reference-contexts: Therefore, we want to study parts of them systematically. We can isolate the state (or states) in the lattice with the most repetition (due to remainders) when determinized; then we can backtrack to find an initial source of nondeterminism. An initial attempt would be to find the dominator <ref> [19, 24, 40] </ref> of that set of states and see if the induced component matches our above conjecture. We also suggest the following extensions to the experiments we report here.
Reference: [20] <author> P. E. Hart, N. J. Nilsson, and B. Raphael. </author> <title> A formal basis for the heuristic determination of minimum cost paths. </title> <journal> IEEE Transactions on Systems, Science, and Cybernetics, </journal> <volume> 4 </volume> <pages> 100-7, </pages> <year> 1968. </year>
Reference-contexts: From this perspective, attempts to reduce the sizes of the intermediate lattices fall into the realm of heuristic search methods, many of which have been investigated for the ASR application. In particular, the A fl algorithm <ref> [20] </ref> generalizes the classical Dijkstra shortest-path algorithm [14] by exploiting estimates of the shortest path remaining from any particular node to the final node.
Reference: [21] <author> R. G. Jeroslaw. </author> <title> There cannot be any algorithm for integer programming with quadratic constraints. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 221-4, </pages> <year> 1973. </year>
Reference-contexts: form x 2 c 2 or xcx 0 c 0 . (The xs and x 0 s are from the indicator variables, and the cs and c 0 s are from the arc-cost variables, used to define the M (w; v)s and M (w)s.) We use a trick from Jeroslaw <ref> [21] </ref> to transform these into quadratic constraints.
Reference: [22] <author> J. Kari and P. Franti. </author> <title> Arithmetic coding of weighted finite automata. </title> <journal> RAIRO Informatique Theorique et Applications, </journal> <volume> 28(3-4):343-60, </volume> <year> 1994. </year>
Reference-contexts: While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions [11, 13], to compress images <ref> [12, 22] </ref>, and in natural language processing [27, 28, 29, 37, 39]. Other applications to ASR have also been described [30, 31, 36]. 1.2 Determinization of WFA Determinization of WFA is the following problem.
Reference: [23] <author> C. M. R. Kintala and D. Wotschke. </author> <title> Amounts of nondeterminism in finite automata. </title> <journal> Acta Informatica, </journal> <volume> 13 </volume> <pages> 199-204, </pages> <year> 1980. </year>
Reference-contexts: Therefore, along the acceptance path for w, allowances must be made for (almost) any unseen symbol to be seen. This inability to resolve the initial nondeterminism (at node x 0 ) until the end of the input (at layer n) produces the deterministic expansion. Continuing, Kintala and Wotschke <ref> [23] </ref> provide a set of NFAs that produces a hierarchy of expansion factors when determinized. Consider the set of languages L h;k = fx1yjx; y 2 f0; 1g fl ; jxj k 1; jyj = k; x has at most h 1's in itg for k 1, h &lt; k.
Reference: [24] <author> T. Lengauer and R. E. Tarjan. </author> <title> A fast algorithm for finding dominators in a flowgraph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 121-41, </pages> <year> 1979. </year>
Reference-contexts: Therefore, we want to study parts of them systematically. We can isolate the state (or states) in the lattice with the most repetition (due to remainders) when determinized; then we can backtrack to find an initial source of nondeterminism. An initial attempt would be to find the dominator <ref> [19, 24, 40] </ref> of that set of states and see if the induced component matches our above conjecture. We also suggest the following extensions to the experiments we report here.
Reference: [25] <author> H. R. Lewis and C. H. Papadimitriou. </author> <title> Elements of the Theory of Computation. </title> <booktitle> Prentice-Hall Software Series. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: We derive this example from Lewis and Papadimitriou <ref> [25, p. 83] </ref>. Figure 5 shows the graph corresponding to a nondeterministic finite-state automaton (NFA) accepting L. We refer to the x i nodes as layer i of the graph.
Reference: [26] <author> B. Lowerre and R. Reddy. </author> <title> The Harpy speech understanding system. In Trends in Speech Recognition, </title> <booktitle> chapter 15, </booktitle> <pages> pages 340-60. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: In particular, the A fl algorithm [20] generalizes the classical Dijkstra shortest-path algorithm [14] by exploiting estimates of the shortest path remaining from any particular node to the final node. Beam searching <ref> [1, 26] </ref> is the application of a threshold to the Viterbi algorithm [41] (essentially the Bellman-Ford shortest-path algorithm [2, 17]), in order to stop exploration of paths that are not locally favorable.
Reference: [27] <author> M. Mohri. </author> <title> Minimization of sequential transducers. </title> <booktitle> In Proc. 5th Symposium on Combinatorial Pattern Matching, volume 807 of Lecture Notes in Computer Science, </booktitle> <pages> pages 151-63, </pages> <year> 1994. </year> <month> 25 </month>
Reference-contexts: For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR. Analogues to classical automata determiniza-tion and minimization have been developed and applied to finite-state transducers <ref> [6, 28, 27, 29, 35, 38] </ref> (see Buchsbaum and Giancarlo [7] for historical context and more detailed comparisons of the two ASR perspectives) and, again, whereas in general we cannot expect guaranteed size reductions, the nature of the ASR task seems to constrain the lattices so that these methods tend to <p> While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions [11, 13], to compress images [12, 22], and in natural language processing <ref> [27, 28, 29, 37, 39] </ref>. Other applications to ASR have also been described [30, 31, 36]. 1.2 Determinization of WFA Determinization of WFA is the following problem.
Reference: [28] <author> M. Mohri. </author> <title> Finite-state transducers in language and speech processing. </title> <journal> Computational Linguistics, </journal> <volume> 23, </volume> <year> 1997. </year>
Reference-contexts: For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR. Analogues to classical automata determiniza-tion and minimization have been developed and applied to finite-state transducers <ref> [6, 28, 27, 29, 35, 38] </ref> (see Buchsbaum and Giancarlo [7] for historical context and more detailed comparisons of the two ASR perspectives) and, again, whereas in general we cannot expect guaranteed size reductions, the nature of the ASR task seems to constrain the lattices so that these methods tend to <p> An appendix provides details of some technical claims made in Section 5. First we review weighted finite automata and how to determinize them. Readers familiar with this material (in particular with Mohri's determinization algorithm <ref> [28] </ref>) may skip to Section 2. 1.1 Weighted Finite Automata A weighted finite automaton (WFA) is a quadruple A = (Q; q 0 ; fl; ffi) such that Q is the set of states, q 0 is the initial state, fl is the set of labels (strings over some finite alphabet <p> While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions [11, 13], to compress images [12, 22], and in natural language processing <ref> [27, 28, 29, 37, 39] </ref>. Other applications to ASR have also been described [30, 31, 36]. 1.2 Determinization of WFA Determinization of WFA is the following problem. <p> Theoretically, not all weighted automata can be determinized. It is therefore natural to seek conditions under which a given weighted automaton can be determinized. Elaborating on results obtained by Choffrut [9] and Weber and Klemm [42] in the realm of string-to-string transduction, Mohri <ref> [28, 29] </ref> identifies those conditions and provides an algorithm that checks whether they hold; the algorithm is constructive in the case that the input automaton can be determinized. <p> As we will see, however, it performs extremely well on lattices arising in ASR. We now briefly discuss the algorithm and its performance. Intuitively, the determinization algorithm devised by Mohri <ref> [28] </ref> generalizes the determinization procedure for nondeterministic finite automata. Given WFA A = (Q; q 0 ; fl; ffi), we construct deterministic WFA A 0 as follows. (We give an example below.) The start state of A 0 is f (q 0 ; 0)g. <p> It 4 with weight w. (b) The result of applying Mohri's determinization algorithm to the automaton of (a). This is derived from Figures 11-12 of Mohri <ref> [28] </ref>. can be shown that c ( ~ t A 0 (w)) = ~ t2T A (w) and thus L (A 0 ) = L (A). <p> Obviously, if we use the smaller lattice, the composition with the language model will be faster. Extensive experiments have shown that, when applied to lattices resulting from the various phases of speech recognition, the determinization algorithm by Mohri <ref> [28] </ref> tends to run in linear time and produce smaller lattices than the ones it takes as input. <p> We denote the graph resulting from the application of Mohri's determinization algorithm <ref> [28] </ref> to G as det (G); assuming that G is deterministic, we denote by min (G) the graph resulting from the application of Morhi's minimization algorithm to G. Let G be the set of all weighted, labeled, directed graphs.
Reference: [29] <author> M. Mohri. </author> <title> On the use of sequential transducers in natural language processing. In Finite-State Language Processing. </title> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR. Analogues to classical automata determiniza-tion and minimization have been developed and applied to finite-state transducers <ref> [6, 28, 27, 29, 35, 38] </ref> (see Buchsbaum and Giancarlo [7] for historical context and more detailed comparisons of the two ASR perspectives) and, again, whereas in general we cannot expect guaranteed size reductions, the nature of the ASR task seems to constrain the lattices so that these methods tend to <p> While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions [11, 13], to compress images [12, 22], and in natural language processing <ref> [27, 28, 29, 37, 39] </ref>. Other applications to ASR have also been described [30, 31, 36]. 1.2 Determinization of WFA Determinization of WFA is the following problem. <p> Theoretically, not all weighted automata can be determinized. It is therefore natural to seek conditions under which a given weighted automaton can be determinized. Elaborating on results obtained by Choffrut [9] and Weber and Klemm [42] in the realm of string-to-string transduction, Mohri <ref> [28, 29] </ref> identifies those conditions and provides an algorithm that checks whether they hold; the algorithm is constructive in the case that the input automaton can be determinized.
Reference: [30] <author> F. Pereira and M. Riley. </author> <title> Speech recognition by composition of weighted finite automata. In Finite-State Language Processing. </title> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: 1 Introduction One of the central problems in automatic speech recognition (ASR) is to reduce the sizes of graphs that represent hypotheses of spoken utterances. Viewing the ASR pipeline as a cascade of processes, shown in (This view of the ASR pipeline comes from Pereira and Riley <ref> [30] </ref>.) The term lattice is used in ASR to denote a directed, labeled graph, which is possibly weighted. There is typically a designated start node s and final node t, and s-t paths induce hypotheses from the arc labels in the normal way. <p> In the general case, these heuristics eliminate the guarantee that the selected path will indeed be shortest; the peculiar nature of the ASR application, however, tends to make them succeed in practice. Recently, Pereira et al. <ref> [30, 31] </ref> adopt the view of ASR as the composition of several finite-state transducers. In particular, all the lattices in the above ASR cascade can be considered to be finite-state transducers, and each process in the cascade performs a generalized composition operation between its inputs. <p> While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions [11, 13], to compress images [12, 22], and in natural language processing [27, 28, 29, 37, 39]. Other applications to ASR have also been described <ref> [30, 31, 36] </ref>. 1.2 Determinization of WFA Determinization of WFA is the following problem.
Reference: [31] <author> F. Pereira, M. Riley, and R. Sproat. </author> <title> Weighted rational transductions and their application to human language processing. </title> <booktitle> In Proc. ARPA Human Language Technology Conf., </booktitle> <pages> pages 249-54, </pages> <year> 1994. </year>
Reference-contexts: In the general case, these heuristics eliminate the guarantee that the selected path will indeed be shortest; the peculiar nature of the ASR application, however, tends to make them succeed in practice. Recently, Pereira et al. <ref> [30, 31] </ref> adopt the view of ASR as the composition of several finite-state transducers. In particular, all the lattices in the above ASR cascade can be considered to be finite-state transducers, and each process in the cascade performs a generalized composition operation between its inputs. <p> While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions [11, 13], to compress images [12, 22], and in natural language processing [27, 28, 29, 37, 39]. Other applications to ASR have also been described <ref> [30, 31, 36] </ref>. 1.2 Determinization of WFA Determinization of WFA is the following problem.
Reference: [32] <author> M. O. Rabin. </author> <title> Probabilistic automata. </title> <journal> Information and Control, </journal> <volume> 6 </volume> <pages> 230-45, </pages> <year> 1963. </year>
Reference-contexts: Again we refer the reader to Eilenberg [15] for basic information on weighted automata. Weighted automata have been extensively studied and find uses in other applications in addition to ASR. Rabin <ref> [32] </ref> defines probabilistic automata, in which transition functions are sets of stochastic matrices.
Reference: [33] <author> L. Rabiner and B.-H. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall Signal Processing Series. Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1993. </year>
Reference-contexts: It should be intuitive that we can speed the ASR process by producing smaller lattices that encapsulate relevant information. For example, one can view the various models (acoustic, lexicon, and grammar) as hidden Markov models (HMMs) <ref> [33, 34] </ref>, in which case the intermediate lattices are implicit representations of the application of various graph search algorithms to the inputs.
Reference: [34] <author> L. R. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <editor> In A. Waibel and K.-F. Lee, editors, </editor> <booktitle> Readings in Speech Recognition, </booktitle> <pages> pages 367-96. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1990. </year>
Reference-contexts: It should be intuitive that we can speed the ASR process by producing smaller lattices that encapsulate relevant information. For example, one can view the various models (acoustic, lexicon, and grammar) as hidden Markov models (HMMs) <ref> [33, 34] </ref>, in which case the intermediate lattices are implicit representations of the application of various graph search algorithms to the inputs.
Reference: [35] <author> C. Reutenauer and M.-P. Schutzenberger. </author> <title> Minimization of rational word functions. </title> <journal> SIAM Journal on Computing, </journal> <volume> 20(4) </volume> <pages> 669-85, </pages> <year> 1991. </year>
Reference-contexts: For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR. Analogues to classical automata determiniza-tion and minimization have been developed and applied to finite-state transducers <ref> [6, 28, 27, 29, 35, 38] </ref> (see Buchsbaum and Giancarlo [7] for historical context and more detailed comparisons of the two ASR perspectives) and, again, whereas in general we cannot expect guaranteed size reductions, the nature of the ASR task seems to constrain the lattices so that these methods tend to
Reference: [36] <author> M. D. Riley, A. Ljolje, D. Hindle, and F. C. N. Pereira. </author> <title> The AT&T 60,000 word speech-to-text system. </title> <editor> In J. M. Pardo, E. Enr iquez, J. Ortega, J. Ferreiros, J. Mac ias, and F. J. Valverde, editors, </editor> <booktitle> Proc. 4th Euro. Conf. on Speech Communication and Technology, </booktitle> <volume> volume 1, </volume> <pages> pages 207-210, </pages> <year> 1995. </year>
Reference-contexts: While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions [11, 13], to compress images [12, 22], and in natural language processing [27, 28, 29, 37, 39]. Other applications to ASR have also been described <ref> [30, 31, 36] </ref>. 1.2 Determinization of WFA Determinization of WFA is the following problem. <p> We believe that whatever canonical weight-dependent topologies we find will also manifest these behaviors. 4 Experimental Results from ASR Lattices 4.1 Data These experiments were conducted on a collection of 100 word lattices generated by the AT&T North American Business speech recognizer <ref> [36] </ref>, using a grammar for the Air Travel Information System (ATIS) task [5]. Each graph came with a weighting generated by the ASR software; we refer to these weights as speech weights. 4.2 Method For each input, we first determinized the graph with its original set of speech weights.
Reference: [37] <author> E. </author> <type> Roche. </type> <institution> Analyse Syntaxique Transformationelle du Francais par Transducteurs et Lexique-Grammaire. </institution> <type> PhD thesis, </type> <address> LITP-Universite Paris 7, Paris, France, </address> <year> 1993. </year>
Reference-contexts: While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions [11, 13], to compress images [12, 22], and in natural language processing <ref> [27, 28, 29, 37, 39] </ref>. Other applications to ASR have also been described [30, 31, 36]. 1.2 Determinization of WFA Determinization of WFA is the following problem.
Reference: [38] <author> E. Roche. </author> <title> Smaller representations for finite-state transducers and finite-state automata. </title> <booktitle> In Proc. 6th Symposium on Combinatorial Pattern Matching, volume 937 of Lecture Notes in Computer Science, </booktitle> <pages> pages 352-65, </pages> <year> 1995. </year>
Reference-contexts: For now, it should be clear, again, that smaller lattices will lead to faster compositions and thus faster ASR. Analogues to classical automata determiniza-tion and minimization have been developed and applied to finite-state transducers <ref> [6, 28, 27, 29, 35, 38] </ref> (see Buchsbaum and Giancarlo [7] for historical context and more detailed comparisons of the two ASR perspectives) and, again, whereas in general we cannot expect guaranteed size reductions, the nature of the ASR task seems to constrain the lattices so that these methods tend to
Reference: [39] <author> M. Silberztein. Dictionnaires electroniques et analise automatique de textes: </author> <title> le systeme INTEX. </title> <type> PhD thesis, Masson, </type> <institution> Paris, France., </institution> <year> 1993. </year>
Reference-contexts: While probabilistic automata are related to WFA, in general WFA need not obey stochastic constraints. WFA have also been used to compute real functions [11, 13], to compress images [12, 22], and in natural language processing <ref> [27, 28, 29, 37, 39] </ref>. Other applications to ASR have also been described [30, 31, 36]. 1.2 Determinization of WFA Determinization of WFA is the following problem.
Reference: [40] <author> R. E. Tarjan. </author> <title> Finding dominators in directed graphs. </title> <journal> SIAM Journal on Computing, </journal> <volume> 3(1) </volume> <pages> 62-89, </pages> <year> 1974. </year>
Reference-contexts: Therefore, we want to study parts of them systematically. We can isolate the state (or states) in the lattice with the most repetition (due to remainders) when determinized; then we can backtrack to find an initial source of nondeterminism. An initial attempt would be to find the dominator <ref> [19, 24, 40] </ref> of that set of states and see if the induced component matches our above conjecture. We also suggest the following extensions to the experiments we report here.
Reference: [41] <author> A. J. </author> <title> Viterbi. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-13:260-9, </volume> <year> 1967. </year>
Reference-contexts: In particular, the A fl algorithm [20] generalizes the classical Dijkstra shortest-path algorithm [14] by exploiting estimates of the shortest path remaining from any particular node to the final node. Beam searching [1, 26] is the application of a threshold to the Viterbi algorithm <ref> [41] </ref> (essentially the Bellman-Ford shortest-path algorithm [2, 17]), in order to stop exploration of paths that are not locally favorable.
Reference: [42] <author> A. Weber and R. Klemm. </author> <title> Economy of description for single-valued transducers. </title> <journal> Information and Computation, </journal> <volume> 118 </volume> <pages> 327-40, </pages> <year> 1995. </year> <month> 26 </month>
Reference-contexts: Theoretically, not all weighted automata can be determinized. It is therefore natural to seek conditions under which a given weighted automaton can be determinized. Elaborating on results obtained by Choffrut [9] and Weber and Klemm <ref> [42] </ref> in the realm of string-to-string transduction, Mohri [28, 29] identifies those conditions and provides an algorithm that checks whether they hold; the algorithm is constructive in the case that the input automaton can be determinized.
References-found: 42

