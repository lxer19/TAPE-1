URL: ftp://ftp.cs.wisc.edu/wwt/sc94_protocols.ps.gz
Refering-URL: http://www.cs.wisc.edu/~stever/pubs.html
Root-URL: 
Email: wwt@cs.wisc.edu  
Title: Application-Specific Protocols for User-Level Shared Memory  
Author: Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, Ioannis Schoinas, Mark D. Hill, James R. Larus, Anne Rogers David A. Wood 
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Note: Appears in: "Supercomputing '94," Nov. 1994. Reprinted by permission of IEEE.  
Abstract: Recent distributed shared memory (DSM) systems and proposed shared-memory machines have implemented some or all of their cache coherence protocols in software. One way to exploit the flexibility of this software is to tailor a coherence protocol to match an application's communication patterns and memory semantics. This paper presents evidence that this approach can lead to large performance improvements. It shows that application-specific protocols substantially improved the performance of three application programs|appbt, em3d, and barnes|over carefully tuned transparent shared memory implementations. The speed-ups were obtained on Blizzard, a fine-grained DSM system running on a 32-node Thinking Machines CM-5. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Si-mon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: All performance improvements given in this section ignore the first iteration, because production runs of 4 Appears in: "Supercomputing '94," Nov. 1994. Reprinted by permission of IEEE. all three applications would require a large number of iterations. 4.1 APPBT Appbt is one of the NAS Parallel Benchmarks <ref> [1] </ref> produced by NASA Ames as representative of the computation and communication patterns in three-dimensional computational fluid dynamics applications. At each time step, appbt performs three computation phases in each of three dimensions. In phase one, it calculates a block tridiagonal matrix A.
Reference: [2] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: One solution is to select a coherence protocol to match an application's semantics and sharing pattern. This approach has been used to improve the performance of DSM systems, such as Munin <ref> [2] </ref>, which allows programmers to annotate data structures with expected access patterns (e.g., read-only or migratory) that Munin uses to select an appropri 1 Appears in: "Supercomputing '94," Nov. 1994. Reprinted by permission of IEEE. ate coherence protocol. <p> A "one size fits all" protocol is a serious bottleneck when it causes excess communication because of a program's sharing patterns. The Munin <ref> [2] </ref> and Stanford FLASH [11] systems enabled a programmer to select from a collection of system-provided protocols.
Reference: [3] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In many ways, these systems are an evolutionary unification of DSM-like custom protocols with hardware, cache-coherent systems, which move data in cache blocks (not pages). FLASH and Typhoon provide software with full control over coherence policy, unlike earlier hardware-software protocol hybrids (e.g., MIT Alewife <ref> [3] </ref> and Wisconsin Dir 1 SW [7, 26]). A second advantage of software protocols is that they lead to a natural unification of message passing with shared memory. <p> Initially, all blocks in a stache page have their access control tags set to Invalid . For both home and stache pages, the library installs a unique set of default handlers that dictate the protocol actions. The default coherence protocol| Stache128 |closely resembles an all-software implementation of LimitLESS <ref> [3] </ref>, except that the block size is 128 bytes. For example, when a processor tries to read an Invalid stache block, the Stache128 handler sends an active message to the home node requesting a read-only copy.
Reference: [4] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: In summary, appbt-update is a factor of 5.7 times faster than the appbt-spin. 4.2 EM3D EM3D models the propagation of electromagnetic waves through objects in three dimensions <ref> [4] </ref>. The problem is formulated as a computation on a bipartite graph with directed edges from E nodes, which represent electric fields, to H nodes, which represent magnetic fields, and vice versa. The computation models the changes in the fields over time.
Reference: [5] <author> Matthew I. Frank and Mary K. Vernon. </author> <title> A Hybrid Shared Memory/Message Passing Parallel Machine. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing (Vol. I Architecture), </booktitle> <pages> pages 232-236, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: A second advantage of software protocols is that they lead to a natural unification of message passing with shared memory. A shared-memory program can send messages when a user or compiler wants to bypass shared-memory overheads and can determine in advance how data will be used <ref> [5, 10] </ref>. A third advantage is the opportunity to develop language-specific memory models, such as Loosely Coherent Memory [14]. Thus far we have argued the benefits of software coherence protocols without specifying whether the software runs in the system (in kernel mode) or at user level.
Reference: [6] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly-Coupled Processor-Network Interface. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 111-122, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Thus an application could use Stache128 to maintain most data structures, but specify Stache32|or some other customized protocol|for a data structure that exhibits false-sharing. 2.3 Implementing Tempest Tempest's messaging and virtual memory support are largely conventional. Active message abstractions can be implemented very efficiently with custom hardware <ref> [6, 18] </ref>, but also have reasonable performance on existing machines [25]. Tempest's virtual memory mechanisms can be implemented as a user-level library on a system that provides mmap () and munmap () or with custom kernel modifications [19].
Reference: [7] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <month> November </month> <year> 1993. </year> <note> Earlier version appeared in ASPLOS V, </note> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: FLASH and Typhoon provide software with full control over coherence policy, unlike earlier hardware-software protocol hybrids (e.g., MIT Alewife [3] and Wisconsin Dir 1 SW <ref> [7, 26] </ref>). A second advantage of software protocols is that they lead to a natural unification of message passing with shared memory. A shared-memory program can send messages when a user or compiler wants to bypass shared-memory overheads and can determine in advance how data will be used [5, 10].
Reference: [8] <author> W. Daniel Hillis and Lewis W. Tucker. </author> <title> The CM-5 Connection Machine: A Scalable Supercomputer. </title> <journal> Communications of the ACM, </journal> <volume> 36(11) </volume> <pages> 31-40, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The CM-5 is a distributed-memory, message-passing parallel computer, in which each processing node contains a 33MHz SPARC microprocessor with a 64KB direct-mapped unified cache, memory management unit, up to 128MB of memory, a custom network interface chip, and optional custom vector units <ref> [8] </ref>. Blizzard does not use the vector units. Blizzard consists of an augmented version of the CM-5's CMOST operating system, a user-level library containing the interface to the Tempest mechanisms, and libraries of default user-level protocols.
Reference: [9] <institution> Kendall Square Research. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: The performance of this variant could also be improved by optimizing the tests. To provide a reference point to gauge the absolute performance of Blizzard, Schoinas, et al. [22] compared a 32-node Blizzard to a Kendall Square Research KSR-1 <ref> [9] </ref>. On six benchmarks Blizzard ranged from 18% to 120% of the speed of a KSR-1. The variation occurs because the ratio of computation to communication differed in each program.
Reference: [10] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatow-icz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A second advantage of software protocols is that they lead to a natural unification of message passing with shared memory. A shared-memory program can send messages when a user or compiler wants to bypass shared-memory overheads and can determine in advance how data will be used <ref> [5, 10] </ref>. A third advantage is the opportunity to develop language-specific memory models, such as Loosely Coherent Memory [14]. Thus far we have argued the benefits of software coherence protocols without specifying whether the software runs in the system (in kernel mode) or at user level.
Reference: [11] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Reprinted by permission of IEEE. ate coherence protocol. DSM systems, unfortunately, perform access control at the page level, which can cause considerable false sharing. Stanford FLASH <ref> [11] </ref> and Wisconsin Typhoon [21] are two recent "hardware" shared-memory systems that implement their protocols in software running on custom coprocessors, which allows protocol specialization. In many ways, these systems are an evolutionary unification of DSM-like custom protocols with hardware, cache-coherent systems, which move data in cache blocks (not pages). <p> A "one size fits all" protocol is a serious bottleneck when it causes excess communication because of a program's sharing patterns. The Munin [2] and Stanford FLASH <ref> [11] </ref> systems enabled a programmer to select from a collection of system-provided protocols. The Tempest interface, implemented in hardware like Typhoon or as a fine-grain DSM system like Blizzard, combines the flexibility of DSM systems, which implement coherence protocols in software, with the fine-grain access control of hardware shared memory.
Reference: [12] <author> James R. Larus. </author> <title> Compiling for Shared-Memory and Message-Passing Computers. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 2(1-4):165-180, </volume> <month> March-December </month> <year> 1994. </year>
Reference-contexts: Finally, a compiler implements an address space by detecting remote accesses with static program analysis or run-time tests and transfers data through explicit messages. These systems have gained a reputation for performing poorly in many circumstances and being overly sensitive to programs' spatial locality and false sharing <ref> [12] </ref>. One reason for shared memory's poor performance is that it typically implements only a single coherence protocol. The policy embodied in a protocol controls a system's response to a remote reference and therefore the message traffic between processors.
Reference: [13] <author> James R. Larus and Thomas Ball. </author> <title> Rewriting Executable Files to Measure Program Behavior. </title> <journal> Software Practice & Experience, </journal> <volume> 24(2) </volume> <pages> 197-218, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: At the other extreme, fine-grain access control can be supported with no hardware changes by rewriting the executable files <ref> [13] </ref> to insert software tests. 3 Blizzard This section describes Blizzard, the Tempest implementation that we use to study the performance of application-specific protocols. Blizzard implements the Tempest mechanisms on a Thinking Machines CM-5 [22].
Reference: [14] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> LCM: Memory System Support for Parallel Language Implementation. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <month> October </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: A shared-memory program can send messages when a user or compiler wants to bypass shared-memory overheads and can determine in advance how data will be used [5, 10]. A third advantage is the opportunity to develop language-specific memory models, such as Loosely Coherent Memory <ref> [14] </ref>. Thus far we have argued the benefits of software coherence protocols without specifying whether the software runs in the system (in kernel mode) or at user level. In fact, the systems discussed above, except Munin and Typhoon, run protocol software in kernel mode.
Reference: [15] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: At one extreme, the proposed Wisconsin Typhoon system adds a custom network interface processor to the cache-coherent bus of a workstation-like processing node to get (simulated) shared-memory performance comparable to a conventional directory-based cache-coherent machine (e.g., Stanford DASH <ref> [15] </ref>). At the other extreme, fine-grain access control can be supported with no hardware changes by rewriting the executable files [13] to insert software tests. 3 Blizzard This section describes Blizzard, the Tempest implementation that we use to study the performance of application-specific protocols.
Reference: [16] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Applications can manage their address space explicitly or, more likely, through a standard library. For example, the default Tempest library provides transparent shared-memory semantics using the Stache allocation policy [21], which is similar to Li and Hudak's fixed distributed manager <ref> [16] </ref>. The li brary allocates a region of each process's private address space for a shared segment. Pages in this segment are assigned a unique "home" processor node, which provides the physical memory. Home nodes can be allocated flexibly.
Reference: [17] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: We tried two approaches to improve performance: reduce the cost of lock operations and reduce the frequency of locking. Our second implementation, Barnes-ml , replaced MCS locks with message passing locks. MCS locks use memory reads and writes to pass messages <ref> [17] </ref>. As a result, they are expensive in high contention situations (requiring as many as eight messages for a lock/unlock pair; however, successive lock/unlock pairs by the same processor on an MCS lock do not require any communication).
Reference: [18] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A Mul-tithreaded Massively Parallel Architecture. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Thus an application could use Stache128 to maintain most data structures, but specify Stache32|or some other customized protocol|for a data structure that exhibits false-sharing. 2.3 Implementing Tempest Tempest's messaging and virtual memory support are largely conventional. Active message abstractions can be implemented very efficiently with custom hardware <ref> [6, 18] </ref>, but also have reasonable performance on existing machines [25]. Tempest's virtual memory mechanisms can be implemented as a user-level library on a system that provides mmap () and munmap () or with custom kernel modifications [19].
Reference: [19] <author> Steven K. Reinhardt, Babak Falsafi, and David A. Wood. </author> <title> Kernel Support for the Wisconsin Wind Tunnel. </title> <booktitle> In Proceedings of the Usenix Symposium on Microkernels and Other Kernel Architectures, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Active message abstractions can be implemented very efficiently with custom hardware [6, 18], but also have reasonable performance on existing machines [25]. Tempest's virtual memory mechanisms can be implemented as a user-level library on a system that provides mmap () and munmap () or with custom kernel modifications <ref> [19] </ref>. The key challenge in implementing Tempest is sup 3 Appears in: "Supercomputing '94," Nov. 1994. Reprinted by permission of IEEE. porting fine-grain access control. Schoinas, et al. [22], showed that many good access control techniques exist, even for existing machines. <p> Blizzard consists of an augmented version of the CM-5's CMOST operating system, a user-level library containing the interface to the Tempest mechanisms, and libraries of default user-level protocols. Blizzard uses a variant of the "executive interface" extensions developed for the Wisconsin Wind Tunnel <ref> [19] </ref>. This interface provides user-level virtual memory management routines and fine-grain access control functionality needed by Tempest.
Reference: [20] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wiscon-sin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Blizzard mitigates the effect of this feature by buffering packets only when they arrive out of order, which is typically 10-20% of packets in our applications. Blizzard uses ECC bits to implement fine-grain memory access control (as does the Wisconsin Wind Tunnel <ref> [20] </ref>). It synthesizes the Invalid state by intentionally setting incorrect ECC values on affected locations. The ReadOnly state requires write protecting the page containing the block. <p> The ReadOnly state requires write protecting the page containing the block. Writes to ReadWrite blocks on the same page as a ReadOnly block cause a protection exception that the system detects and suppresses to complete the write operation. Surprisingly, this use of ECC bits does not reduce ECC coverage <ref> [20] </ref>. ECC bits, however, are not required to implement Tempest. Schoinas, et al. [22], showed that a Blizzard variant that added an explicit, software test before shared-memory loads and stores runs from 108% slower to 2% faster than Blizzard using ECC.
Reference: [21] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Reprinted by permission of IEEE. ate coherence protocol. DSM systems, unfortunately, perform access control at the page level, which can cause considerable false sharing. Stanford FLASH [11] and Wisconsin Typhoon <ref> [21] </ref> are two recent "hardware" shared-memory systems that implement their protocols in software running on custom coprocessors, which allows protocol specialization. In many ways, these systems are an evolutionary unification of DSM-like custom protocols with hardware, cache-coherent systems, which move data in cache blocks (not pages). <p> Finally, the system must ensure that incorrect or malicious user-level protocols do not violate protection or deadlock the system. This paper addresses the first two concerns, but not the third. The Tempest interface <ref> [21] </ref> (Section 2) addresses the first concern. Tempest provides a standard, system-independent interface to mechanisms that enable programmers, compilers, and program libraries to implement and use message passing, transparent shared memory, and hybrid combinations of the two. <p> Its most novel mechanism| fine-grain access control|allows user software to tag blocks (e.g., 32 bytes) as read-write, read-only, or invalid, so local memory can cache remote data transparently [22]. We have implemented Tempest both in a simulation of custom hardware <ref> [21] </ref> and directly on a CM-5 [22]. We are currently porting Tempest to a network of workstations. Section 3 describes Blizzard , our Tempest implementation on a Thinking Machines CM-5 [22]. Blizzard consists of a modified version of the CM-5 operating system and a user-level library. <p> Blizzard uses the CM-5's ECC bits and page-level address translation hardware to synthesize fine-grain access control (at cache block granularity). A Tempest application program is compiled with a standard compiler (e.g., gcc), linked with the Blizzard library and a Tempest-compliant user-level protocol (e.g., Stache <ref> [21] </ref>), and runs directly on a CM-5. Section 4 addresses our second concern, namely, whether tailored protocols will yield performance gains. We optimize protocols for three applications running on Blizzard. For each application, we begin with a transparent shared memory program that uses a standard, library-provided protocol. <p> Reprinted by permission of IEEE. supplied mechanisms for memory access detection and communication. The Tempest mechanisms <ref> [21] </ref> are general enough that programmers, protocol libraries, or compilers can use them to support shared memory, message passing, or hybrid (i.e., combination) applications. The four types of Tempest mechanisms are: Low-Overhead "Active" Messages. <p> While Tempest does not implement the shared address space directly, it provides mechanisms that enable user-level software to do so. Applications can manage their address space explicitly or, more likely, through a standard library. For example, the default Tempest library provides transparent shared-memory semantics using the Stache allocation policy <ref> [21] </ref>, which is similar to Li and Hudak's fixed distributed manager [16]. The li brary allocates a region of each process's private address space for a shared segment. Pages in this segment are assigned a unique "home" processor node, which provides the physical memory. Home nodes can be allocated flexibly. <p> Channels reduce handshaking overhead and eliminate buffering of out-of-order packets. EM3D-channel is 36% faster than EM3D-vector and its performance is nearly identical to the message-passing version of this program (EM3D-MP ). In summary, EM3D-channel is a factor of 16 faster than our best transparent shared memory implementation, EM3D-value. <ref> [21] </ref>. This paper significantly extends that work by looking at a wide spectrum of user-level protocols and evaluating them on the Blizzard system. 2 Tempest's virtual channels, which are part of the bulk data transfer library, are similar to the Virtual Channels provided by the CM-5's CMMD Library.
Reference: [22] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <month> October </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Tempest's mechanisms are low-overhead messages, bulk data transfer, virtual memory management, and fine-grain access control. Its most novel mechanism| fine-grain access control|allows user software to tag blocks (e.g., 32 bytes) as read-write, read-only, or invalid, so local memory can cache remote data transparently <ref> [22] </ref>. We have implemented Tempest both in a simulation of custom hardware [21] and directly on a CM-5 [22]. We are currently porting Tempest to a network of workstations. Section 3 describes Blizzard , our Tempest implementation on a Thinking Machines CM-5 [22]. <p> Its most novel mechanism| fine-grain access control|allows user software to tag blocks (e.g., 32 bytes) as read-write, read-only, or invalid, so local memory can cache remote data transparently <ref> [22] </ref>. We have implemented Tempest both in a simulation of custom hardware [21] and directly on a CM-5 [22]. We are currently porting Tempest to a network of workstations. Section 3 describes Blizzard , our Tempest implementation on a Thinking Machines CM-5 [22]. Blizzard consists of a modified version of the CM-5 operating system and a user-level library. <p> so local memory can cache remote data transparently <ref> [22] </ref>. We have implemented Tempest both in a simulation of custom hardware [21] and directly on a CM-5 [22]. We are currently porting Tempest to a network of workstations. Section 3 describes Blizzard , our Tempest implementation on a Thinking Machines CM-5 [22]. Blizzard consists of a modified version of the CM-5 operating system and a user-level library. Blizzard uses the CM-5's ECC bits and page-level address translation hardware to synthesize fine-grain access control (at cache block granularity). <p> The key challenge in implementing Tempest is sup 3 Appears in: "Supercomputing '94," Nov. 1994. Reprinted by permission of IEEE. porting fine-grain access control. Schoinas, et al. <ref> [22] </ref>, showed that many good access control techniques exist, even for existing machines. <p> Blizzard implements the Tempest mechanisms on a Thinking Machines CM-5 <ref> [22] </ref>. The CM-5 is a distributed-memory, message-passing parallel computer, in which each processing node contains a 33MHz SPARC microprocessor with a 64KB direct-mapped unified cache, memory management unit, up to 128MB of memory, a custom network interface chip, and optional custom vector units [8]. <p> Surprisingly, this use of ECC bits does not reduce ECC coverage [20]. ECC bits, however, are not required to implement Tempest. Schoinas, et al. <ref> [22] </ref>, showed that a Blizzard variant that added an explicit, software test before shared-memory loads and stores runs from 108% slower to 2% faster than Blizzard using ECC. The software version is not always slower, because it does not have to pay the ECC trap overhead on each miss. <p> The software version is not always slower, because it does not have to pay the ECC trap overhead on each miss. The performance of this variant could also be improved by optimizing the tests. To provide a reference point to gauge the absolute performance of Blizzard, Schoinas, et al. <ref> [22] </ref> compared a 32-node Blizzard to a Kendall Square Research KSR-1 [9]. On six benchmarks Blizzard ranged from 18% to 120% of the speed of a KSR-1. The variation occurs because the ratio of computation to communication differed in each program.
Reference: [23] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: They reduce overhead by allowing a pair of processors that repeatedly communicate to handshake once and then to transfer data without further handshaking in the message library. 4.3 BARNES Barnes, one of the SPLASH benchmarks <ref> [23] </ref>, simulates the evolution, over time, of bodies in a gravitational system. Each body is modeled as a point mass and exerts gravitational force on other bodies in the system. At each time step, the program computes new positions and velocities for all bodies.
Reference: [24] <author> Daniel Stodolsky, J. Brad Chen, and Brian Bershad. </author> <title> Fast Interrupt Priority Management in Operating Systems. </title> <booktitle> In Second USENIX Symposium on Microkernels and Other Kernel Archtitectures, </booktitle> <pages> pages 105-110, </pages> <address> September 1993. San Diego, CA. </address>
Reference-contexts: To make Tempest's message and fault handlers atomic (with respect to each other) without invoking system calls to re-enable interrupts, we use an interrupt masking scheme similar to Stodolsky et al.'s <ref> [24] </ref>. Critical sections set a "software-disable" flag on entry and check a "deferred-interrupt" flag on exit. If an interrupt encounters a "software-disable" flag, the handler queues the interrupt and sets the "deferred-interrupt" flag. Tempest's active messages are more general than those in CMAML [25] or TMC's CMMD message-passing library.
Reference: [25] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrating Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The four types of Tempest mechanisms are: Low-Overhead "Active" Messages. Tempest supports an active message abstraction, in which each message specifies a destination node, handler address, and a string of arguments <ref> [25] </ref>. When a message arrives at its destination, it creates a thread that runs the handler atomically with respect to other message handlers. Nothing guarantees atomicity between a handler and the destination node's computation thread, except explicit (user-level) synchronization. Bulk Data Transfer. <p> Active message abstractions can be implemented very efficiently with custom hardware [6, 18], but also have reasonable performance on existing machines <ref> [25] </ref>. Tempest's virtual memory mechanisms can be implemented as a user-level library on a system that provides mmap () and munmap () or with custom kernel modifications [19]. The key challenge in implementing Tempest is sup 3 Appears in: "Supercomputing '94," Nov. 1994. <p> Critical sections set a "software-disable" flag on entry and check a "deferred-interrupt" flag on exit. If an interrupt encounters a "software-disable" flag, the handler queues the interrupt and sets the "deferred-interrupt" flag. Tempest's active messages are more general than those in CMAML <ref> [25] </ref> or TMC's CMMD message-passing library. In particular, Tempest allows messages larger than the CM-5 packet size. This generality makes Tempest protocols independent of a particular implementation, but degrades performance on the CM-5 because messages must be divided into packets and reassembled.
Reference: [26] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shub-hendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-168, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: FLASH and Typhoon provide software with full control over coherence policy, unlike earlier hardware-software protocol hybrids (e.g., MIT Alewife [3] and Wisconsin Dir 1 SW <ref> [7, 26] </ref>). A second advantage of software protocols is that they lead to a natural unification of message passing with shared memory. A shared-memory program can send messages when a user or compiler wants to bypass shared-memory overheads and can determine in advance how data will be used [5, 10].
Reference: [27] <author> William A Wulf. </author> <title> Compilers and Computer Architecture. </title> <journal> IEEE Computer, </journal> <volume> 14(7) </volume> <pages> 41-47, </pages> <month> July </month> <year> 1981. </year> <month> 10 </month>
Reference-contexts: First, system-provided protocols are necessarily limited and may not match an application's needs. Second, system-provided protocols must be parameterized and formulated in a general manner (for example, to allow a variety of cache block sizes), which introduces unnecessary run-time costs <ref> [27] </ref>. Finally, system-level protocols make experimentation difficult and time-consuming and restrict who can try new protocols. Protocols that run at user-level (as part of an application) overcome these drawbacks. <p> It seems impossible to anticipate all useful combinations in efficient, even if parameterized, protocols. This dilemma is a variant of the well-known policy versus mechanisms distinction. System-provided policies often omit an important feature needed by some users or incur the cost of generality by running slower <ref> [27] </ref>. Mechanisms allow users to build the policies appropriate for their programs. While three application programs cannot provide a definitive answer, our opinion is that system-provided protocols are unlikely to be both general and fast enough to compete with application-specific protocols.
References-found: 27

