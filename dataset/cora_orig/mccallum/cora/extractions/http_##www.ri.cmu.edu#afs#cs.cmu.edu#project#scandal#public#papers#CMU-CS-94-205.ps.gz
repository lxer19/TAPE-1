URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-94-205.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/precondition-ipps.html
Root-URL: 
Title: Performance Evaluation of a New Parallel  
Author: Keith D. Gremban Marco ZaghaGary L. Miller 
Date: October 1994  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Note: The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies or endorsements, either expressed or implied, of ARPA, NSF, or the U.S. government.  CMU-CS-94-205  
Pubnum: Preconditioner  
Abstract: This research is sponsored in part by the Wright Laboratory, Aeronautical Systems Center, Air Force Material Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330, and in part by NSF Grant CCR-9016641. Cray C-90 computing time was provided by the Pittsburgh Supercomputing Center under Grant ASC890018P. The U.S. government is authorized to reproduce and distribute reprints for government purposes, notwithstanding any copyright notation thereon. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. L. Alvarado and R. Schreiber, </author> <title> Optimal parallel solution of sparse triangular systems. </title> <journal> SIAM J. Sci. Comput. </journal> <volume> 14(2) </volume> <pages> 446-460, </pages> <year> 1993. </year>
Reference-contexts: An alternative is to use a different algorithm for solving triangular systems. Alvarado and Schreiber <ref> [1] </ref> presented a method of solving a sparse triangular system by representing the inverse as the product of a few sparse factors, which enables solving the system as a sequence of sparse matrix-vector multiplications.
Reference: [2] <author> E. Anderson and Y. Saad, </author> <title> Solving sparse triangular linear systems on parallel computers. </title> <booktitle> Int. J. of High Speed Computing 1(1) </booktitle> <pages> 73-95, </pages> <year> 1989. </year>
Reference-contexts: In [8], Dongarra et al. presented the results of a similar experiment in which ICCG achieved 84% of the Mop rate of unpreconditioned CG. The technique of determining independent nodes to evaluate in parallel is known as level scheduling, and was first discussed in general by Anderson and Saad <ref> [2] </ref>. The effectiveness of ordering nodes for optimal parallel performance is limited by the topology of the original system, however. The excellent results reported above were for regular rectangular graphs.
Reference: [3] <author> M. Arioli, I. Duff, D. Ruiz, </author> <title> Stopping criteria for iterative solvers. </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 13(1) </volume> <pages> 138-144, </pages> <year> 1992. </year>
Reference-contexts: We used as our stopping criterion the condition reported to be superior by Arioli et al. <ref> [3] </ref>: (3) We halted when w 2 1.0 x 10 -10 . while ICCG outperforms STCG in terms of number of iterations required for convergence on small meshes, the curves cross, and STCG is superior as the meshes get fairly large.
Reference: [4] <author> O. Axelsson and V. A. Barker, </author> <title> Finite Element Solution of Boundary Value Problems. </title> <publisher> Academic Press, </publisher> <year> 1984. </year>
Reference: [5] <author> G. E. Blelloch, NESL: </author> <title> A nested data-parallel language. </title> <institution> CMU-CS-93-129, School of Computer Science, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: Additionally, we are currently investigating the performance of various partitioning methods as one step towards constructing a version of STCG that is optimized from end to end. Currently, the code used to generate support tree preconditioners is written in NESL, an experimental data-parallel language <ref> [5] </ref>. The various implementations of PCG were written in Fortran. We made no attempt to go beyond the obvious optimizations to improve the performance of ICCG.
Reference: [6] <author> G. E. Blelloch, M. A. Heroux, and M. Zagha, </author> <title> Segmented operations for sparse matrix computation on vector multiprocessors. </title> <institution> CMU-CS-93-173, School of Computer Science, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: Thus, the bulk of the computation in the PCG algorithm can be implemented with a single general-purpose sparse matrix multiplication subroutine. On the Cray C-90, we use an algorithm called SEGMV, which accommodates arbitrary row sizes using segmented scan operations <ref> [6] </ref>. Compared to other methods (such as Ellpack/Itpack and Jagged Diagonal), SEGMV performance is comparable for structured matrices, and superior for most irregular matrices. Thus our PCG implementations perform well on both regular and irregular meshes.
Reference: [7] <author> L. Dagum, </author> <title> Automatic partitioning of unstructured grids into connected components. </title> <booktitle> Proc. Supercomputing 93. </booktitle>
Reference-contexts: Support trees are not unique, because they depend upon the separators used to construct them. In practice, any method for graph partitioning may be used to construct support trees. For example, we have constructed support trees using variants of dual tree bisection <ref> [7] </ref>, and recursive coordinate bisection [23]. In the near future, we will construct separator trees using spectral separators [16][21][23], and geometric separators [20] as part of our research on the relationship between the method of partitioning and the performance of the corresponding support tree preconditioner.
Reference: [8] <author> J. J. Dongarra, I. S. Duff, D. C. Sorensen, and H. A. van der Vorst, </author> <title> Solving Linear Systems on Vector and Shared Memory Computers. </title> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: very effective at reducing the number of iterations required to con a) Iterations to convergence. b) Total execution time for iterative process on a Cray C-90 (msecs). a) b) B diag A ( )= 3 verge, but is easily parallelized, yielding very high computational rates on vector and parallel architectures <ref> [8] </ref>, [13], [17], [24]. In fact, the computational rates achievable by DSCG can often make up for the high number of iterations, making DSCG the iterative method of choice in many cases [8], [17]. Incomplete Cholesky (IC) preconditioners were first proposed by Meijerink and van der Vorst [19]. <p> ( )= 3 verge, but is easily parallelized, yielding very high computational rates on vector and parallel architectures <ref> [8] </ref>, [13], [17], [24]. In fact, the computational rates achievable by DSCG can often make up for the high number of iterations, making DSCG the iterative method of choice in many cases [8], [17]. Incomplete Cholesky (IC) preconditioners were first proposed by Meijerink and van der Vorst [19]. Let , where E is obtained by performing the standard Cholesky factorization of A while setting wherever . Then is the IC preconditioner of A [12]. <p> There are two major problems with this method that keep it from being applicable in general. First, the form of the factorization is only equivalent to ICCG in the case of rectangular grids <ref> [8] </ref>. <p> In the case of nxn rectangular meshes, nodes that lie along diagonals are independent, and may be processed in parallel. In [25], van der Vorst reports an experiment in which he achieved 76% of the Mop rate of unpreconditioned CG by using Eisenstats method and diagonal ordering. In <ref> [8] </ref>, Dongarra et al. presented the results of a similar experiment in which ICCG achieved 84% of the Mop rate of unpreconditioned CG. The technique of determining independent nodes to evaluate in parallel is known as level scheduling, and was first discussed in general by Anderson and Saad [2]. <p> The various implementations of PCG were written in Fortran. We made no attempt to go beyond the obvious optimizations to improve the performance of ICCG. Numerous other authors have reported on the effects of ordering on ICCG (see, for example, [10]), and on parallel implementations of ICCG (see <ref> [8] </ref>, [24], and [25]). Rather than reproduce their work, we decided to extrapolate values for an optimistic implementation of ICCG. We applied the results of other researchers discussed in section 2 in order to determine an optimistic execution time for ICCG. <p> For example, in the case of square meshes, moderate parallel efficiency can be obtained by ordering the nodes so that the incomplete Cholesky preconditioner is evaluated along diagonals of the mesh <ref> [8] </ref>; for an nxn mesh, this ordering requires 2n parallel steps with an average of n/2 nodes evaluated in parallel at each step. In contrast, the STCG precondi-tioner for an nxn mesh yields parallel steps with an average of n 2 /logn nodes evaluated at each step.
Reference: [9] <author> P. G. Doyle and J. L. Snell, </author> <title> Random Walks and Electric Networks. </title> <journal> Carus Mathematical Monographs #22, Mathematical Association of America, </journal> <year> 1984. </year>
Reference-contexts: A Laplacian matrix also corresponds to a resistive network <ref> [9] </ref>. In this case, an edge weight corresponds to the conductance of the connection between two nodes, and extra diagonal weight corresponds to the conductance of a resistive connection between the node and ground.
Reference: [10] <author> I. S. Duff and G. A. Meurant, </author> <title> The effect of ordering on preconditioned conjugate gradients. </title> <journal> BIT 29 </journal> <pages> 635-657, </pages> <year> 1989. </year>
Reference-contexts: The various implementations of PCG were written in Fortran. We made no attempt to go beyond the obvious optimizations to improve the performance of ICCG. Numerous other authors have reported on the effects of ordering on ICCG (see, for example, <ref> [10] </ref>), and on parallel implementations of ICCG (see [8], [24], and [25]). Rather than reproduce their work, we decided to extrapolate values for an optimistic implementation of ICCG. We applied the results of other researchers discussed in section 2 in order to determine an optimistic execution time for ICCG.
Reference: [11] <author> S. C. Eisenstat, </author> <title> Efficient implementation of a class of preconditioned conjugate gradient methods. </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 2 </volume> <pages> 1-4, </pages> <year> 1981. </year>
Reference-contexts: Three major directions have been followed in attempts to make ICCG more efficient and parallel: reformulations to reduce the amount of work per iteration; determination of orderings to increase the amount of parallelism; use of factored inverses. We discuss each of these approaches in the paragraphs below. Eisenstat <ref> [11] </ref> reported an efficient implementation of ICCG for cases in which the preconditioner can be rep resented in the form , where .
Reference: [12] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: Incomplete Cholesky (IC) preconditioners were first proposed by Meijerink and van der Vorst [19]. Let , where E is obtained by performing the standard Cholesky factorization of A while setting wherever . Then is the IC preconditioner of A <ref> [12] </ref>. The intuition behind IC precon ditioning is that since EE t is an approximate factorization of A, B should be a good approximation of A. Incomplete Cholesky preconditioners are effective at accelerating the rate of convergence.
Reference: [13] <author> A. Greenbaum, C. Li, and H. Z. Chao, </author> <title> Comparison of linear system solvers applied to diffusion-type finite element equations. </title> <journal> Numer. Math. </journal> <volume> 56 </volume> <pages> 529-546, </pages> <year> 1989. </year>
Reference-contexts: Diagonal scaling and incomplete Cholesky [19] are two examples of algebraic precondition-ers. Multilevel preconditioners are less general in that they depend on some knowledge of the differential equation or of the discretization process <ref> [13] </ref>; we classify these preconditioners as a priori, since they depend on knowledge about the construction of the coefficient matrix, rather than on just the matrix itself. The best performance is achieved by multilevel preconditioners; some multilevel preconditioners can achieve nearly optimal convergence rates [15]. <p> DSCG, ICCG, and STCG were each applied to solving the linear systems. The figure shows the number of iterations required for each of the methods to converge and the total execution time. We use the work of Greenbaum, Li, and Chao <ref> [13] </ref> as a guide in our evaluation procedure. We take a typical problem, discretize it at various levels of resolution to obtain problems of various sizes, and compare the performance of the three PCG methods as a function of problem size. <p> effective at reducing the number of iterations required to con a) Iterations to convergence. b) Total execution time for iterative process on a Cray C-90 (msecs). a) b) B diag A ( )= 3 verge, but is easily parallelized, yielding very high computational rates on vector and parallel architectures [8], <ref> [13] </ref>, [17], [24]. In fact, the computational rates achievable by DSCG can often make up for the high number of iterations, making DSCG the iterative method of choice in many cases [8], [17]. Incomplete Cholesky (IC) preconditioners were first proposed by Meijerink and van der Vorst [19].
Reference: [14] <author> K. D. Gremban and G. L. Miller, </author> <title> Towards the Application of Graph Theory to Finding Parallel Preconditioners for Sparse Symmetric Linear Systems. </title> <type> Technical Report, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <note> in preparation. </note>
Reference-contexts: We claim, for H constructed in a certain way depending on the topology and edge weights of G, that H is a good preconditioner for G. We can analytically prove this statement (see <ref> [14] </ref>), but here prefer to provide intuition. <p> Let B be the Lapla-cian matrix corresponding to H. We would like to use B as a preconditioner for A, but B is of order 2n-1, and A is of order n. How can this be accomplished? In another paper <ref> [14] </ref>, we describe the theory proving that B can be used as a preconditioner for A. In this section, we present an overview of the theory in order to gain some intuition. <p> C 0 E 9 In <ref> [14] </ref>, we show the following: K is an effective preconditioner for A; If , then we also have .
Reference: [15] <author> X. -Z. Guo, </author> <title> Multilevel Preconditioners: Analysis, performance enhancements, and parallel algorithms. </title> <institution> CS-TR-2903, Department of Mathematics, University of Maryland, </institution> <year> 1992. </year>
Reference-contexts: Second, the time per iteration due to applying the preconditioner should be small; thus, on parallel machines it is important that the application of the preconditioner be parallelizable. Preconditioners can be categorized as being either algebraic, or multilevel <ref> [15] </ref>. Algebraic preconditioners depend only on the algebraic structure of the coefficient matrix A; we classify these preconditioners as a posteriori, since they depend only on the coefficient matrix and not on the details of the process used to construct the linear system. <p> The best performance is achieved by multilevel preconditioners; some multilevel preconditioners can achieve nearly optimal convergence rates <ref> [15] </ref>. In addition, many multilevel preconditioners can be effectively parallel-ized [13][15]. However, as stated above, they require a priori knowledge involving the formation of the linear system, and such information is often unavailable. A posteriori preconditioners are the most general.
Reference: [16] <author> B. Hendrickson and R. Leland, </author> <title> An improved spectral graph partitioning algorithm for mapping parallel computations. </title> <institution> SAND92-1460, Sandia National Laboratories, </institution> <year> 1992. </year>
Reference: [17] <author> M. A. Heroux, P. Vu, and C. Yang, </author> <title> A parallel preconditioned conjugate gradient package for solving sparse linear systems on a Cray Y-MP. </title> <journal> Appl. Num. Math. </journal> <volume> 8 </volume> <pages> 93-115, </pages> <year> 1991. </year> <month> 25 </month>
Reference-contexts: at reducing the number of iterations required to con a) Iterations to convergence. b) Total execution time for iterative process on a Cray C-90 (msecs). a) b) B diag A ( )= 3 verge, but is easily parallelized, yielding very high computational rates on vector and parallel architectures [8], [13], <ref> [17] </ref>, [24]. In fact, the computational rates achievable by DSCG can often make up for the high number of iterations, making DSCG the iterative method of choice in many cases [8], [17]. Incomplete Cholesky (IC) preconditioners were first proposed by Meijerink and van der Vorst [19]. <p> )= 3 verge, but is easily parallelized, yielding very high computational rates on vector and parallel architectures [8], [13], <ref> [17] </ref>, [24]. In fact, the computational rates achievable by DSCG can often make up for the high number of iterations, making DSCG the iterative method of choice in many cases [8], [17]. Incomplete Cholesky (IC) preconditioners were first proposed by Meijerink and van der Vorst [19]. Let , where E is obtained by performing the standard Cholesky factorization of A while setting wherever . Then is the IC preconditioner of A [12].
Reference: [18] <author> M. S. Khaira, G. L. Miller, and T. J. Shefer, </author> <title> Nested Dissection: A survey and comparison of various nested dissection algorithms. </title> <institution> CMU-CS-92-106R, Computer Science Department, Carnegie Mellon University, </institution> <year> 1992. </year>
Reference: [19] <author> J. A. Meijerink and H. A. van der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Math. Comp. </journal> <volume> 31 </volume> <pages> 148-162, </pages> <year> 1977. </year>
Reference-contexts: Algebraic preconditioners depend only on the algebraic structure of the coefficient matrix A; we classify these preconditioners as a posteriori, since they depend only on the coefficient matrix and not on the details of the process used to construct the linear system. Diagonal scaling and incomplete Cholesky <ref> [19] </ref> are two examples of algebraic precondition-ers. <p> In fact, the computational rates achievable by DSCG can often make up for the high number of iterations, making DSCG the iterative method of choice in many cases [8], [17]. Incomplete Cholesky (IC) preconditioners were first proposed by Meijerink and van der Vorst <ref> [19] </ref>. Let , where E is obtained by performing the standard Cholesky factorization of A while setting wherever . Then is the IC preconditioner of A [12]. <p> Support Trees 3.1 Background Discretization of many boundary value problems using either the finite element or finite difference methods leads to linear systems of the form , where the coefficient matrix A is a symmetric, diagonally domi nant M-matrix. (A is an M-matrix if for , A is nonsingular, and <ref> [19] </ref>.) We call these matri ces Laplacian matrices, or simply Laplacians. Laplacian matrices are isomorphic to edge-weighted undirected graphs.
Reference: [20] <author> G. L. Miller, S. -H. Teng, W. Thurston, and S. A. Vavasis, </author> <title> Automatic mesh partitioning. </title> <booktitle> Proc of the 1992 Workshop on Sparse Matrix Computations: Graph Theory Issues and Algorithms. </booktitle>
Reference-contexts: In practice, any method for graph partitioning may be used to construct support trees. For example, we have constructed support trees using variants of dual tree bisection [7], and recursive coordinate bisection [23]. In the near future, we will construct separator trees using spectral separators [16][21][23], and geometric separators <ref> [20] </ref> as part of our research on the relationship between the method of partitioning and the performance of the corresponding support tree preconditioner. As stated previously, the exact form and weighting of the support tree depends on the partitioning method employed.
Reference: [21] <author> A. Pothen, H. D. Simon, and K. Liou, </author> <title> Partitioning sparse matrices with eigenvectors of graphs. </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference: [22] <author> M. Reid-Miller, G. L. Miller, and F. Modugno, </author> <title> List ranking and parallel tree contraction. in Synthesis of Parallel Algorithms, </title> <editor> ed. John Reif, </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: We call this ordering rake-order and the evaluation process leaf raking, since all existing leaves are raked off the tree at each step. Parallel node evaluation by leaf raking is a special case of a more general parallel algorithm known as parallel tree contraction <ref> [22] </ref>. directed tree: expansions from parents to children can be performed independently in parallel. Leaf raking can result in impressive parallel performance. For example, consider the case of a quadtree support tree for an nxn mesh.
Reference: [23] <author> H. D. Simon, </author> <title> Partitioning of unstructured problems for parallel processing. </title> <journal> Comp. Sys. in Eng. </journal> 2(2/3):135-148, 1991. 
Reference-contexts: Support trees are not unique, because they depend upon the separators used to construct them. In practice, any method for graph partitioning may be used to construct support trees. For example, we have constructed support trees using variants of dual tree bisection [7], and recursive coordinate bisection <ref> [23] </ref>. In the near future, we will construct separator trees using spectral separators [16][21][23], and geometric separators [20] as part of our research on the relationship between the method of partitioning and the performance of the corresponding support tree preconditioner.
Reference: [24] <author> H. A. van der Vorst, </author> <title> ICCG and related methods for 3D problems on vector computers. </title> <journal> Comp. Physics Comm. </journal> <volume> 53 </volume> <pages> 223-235, </pages> <year> 1989. </year>
Reference-contexts: reducing the number of iterations required to con a) Iterations to convergence. b) Total execution time for iterative process on a Cray C-90 (msecs). a) b) B diag A ( )= 3 verge, but is easily parallelized, yielding very high computational rates on vector and parallel architectures [8], [13], [17], <ref> [24] </ref>. In fact, the computational rates achievable by DSCG can often make up for the high number of iterations, making DSCG the iterative method of choice in many cases [8], [17]. Incomplete Cholesky (IC) preconditioners were first proposed by Meijerink and van der Vorst [19]. <p> The various implementations of PCG were written in Fortran. We made no attempt to go beyond the obvious optimizations to improve the performance of ICCG. Numerous other authors have reported on the effects of ordering on ICCG (see, for example, [10]), and on parallel implementations of ICCG (see [8], <ref> [24] </ref>, and [25]). Rather than reproduce their work, we decided to extrapolate values for an optimistic implementation of ICCG. We applied the results of other researchers discussed in section 2 in order to determine an optimistic execution time for ICCG.
Reference: [25] <author> H. A. van der Vorst, </author> <title> High performance preconditioning. </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 10(6) </volume> <pages> 1174-1185, </pages> <year> 1989. </year> <month> 26 </month>
Reference-contexts: By carefully ordering the nodes of the linear system, quite significant parallelism can be obtained in some cases. In the case of nxn rectangular meshes, nodes that lie along diagonals are independent, and may be processed in parallel. In <ref> [25] </ref>, van der Vorst reports an experiment in which he achieved 76% of the Mop rate of unpreconditioned CG by using Eisenstats method and diagonal ordering. In [8], Dongarra et al. presented the results of a similar experiment in which ICCG achieved 84% of the Mop rate of unpreconditioned CG. <p> We made no attempt to go beyond the obvious optimizations to improve the performance of ICCG. Numerous other authors have reported on the effects of ordering on ICCG (see, for example, [10]), and on parallel implementations of ICCG (see [8], [24], and <ref> [25] </ref>). Rather than reproduce their work, we decided to extrapolate values for an optimistic implementation of ICCG. We applied the results of other researchers discussed in section 2 in order to determine an optimistic execution time for ICCG.
References-found: 25

