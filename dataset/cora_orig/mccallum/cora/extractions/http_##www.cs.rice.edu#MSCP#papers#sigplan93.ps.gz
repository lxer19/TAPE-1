URL: http://www.cs.rice.edu/MSCP/papers/sigplan93.ps.gz
Refering-URL: http://www.cs.rice.edu/MSCP/publications.html
Root-URL: 
Title: Interprocedural Constant Propagation: A Study of Jump Function Implementations  
Author: Dan Grove Linda Torczon 
Address: 2550 Garcia Avenue  Mountain View, California 94043 Houston, Texas 77251-1892  
Affiliation: Sun Microsystems Department of Computer Science  Rice University  
Abstract: An implementation of interprocedural constant propagation must model the transmission of values through each procedure. In the framework proposed by Callahan, Cooper, Kennedy, and Torczon in 1986, this intraprocedural propagation is modeled with a jump function. While Callahan et al. propose several kinds of jump functions, they give no data to help choose between them. This paper reports on a comparative study of jump function implementations. It shows that different jump functions produce different numbers of useful constants; it suggests a particular function, called the pass-through parameter jump function, as the most cost-effective in practice. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bowen Alpern, Mark N. Wegman, and F. Ken-neth Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Conference Record of the Fifteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-11, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Thus, we chose an implementation strategy that differs from the one described by Callahan et al. We built a set of jump functions on top of an existing framework for global (intraprocedural) value number ing <ref> [1] </ref>. Callahan et al. envisioned constructing jump functions before any interprocedural data-flow analysis is performed. We found it convenient to simulate the various jump functions on top of global value numbering after the computation of interprocedural summary sets (MOD and REF).
Reference: [2] <author> Robert A. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein. </author> <title> The program dependence web: A representation supporting control, data, and demand-driven interpretation of imperative languages. </title> <journal> SIGPLAN Notices, </journal> <volume> 25(6) </volume> <pages> 257-271, </pages> <month> June </month> <year> 1990. </year> <booktitle> Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: The additional effort of a "complete" propagation does not appear justified by the additional constants found. In fact, the results that we obtained in this study with "complete" propagation can be achieved by basing the jump-function generator on a gated single-assignment form <ref> [2, 10] </ref>. An analyzer based on gated single-assignment form would never consider the dead assignments that we found in the "complete" propagation. 6 This would let the standard polynomial jump function produce the results seen with "complete" propagation.
Reference: [3] <author> Michael Burke and Ron Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <journal> SIG-PLAN Notices, </journal> <volume> 21(7) </volume> <pages> 162-175, </pages> <month> July </month> <year> 1986. </year> <booktitle> Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction. </booktitle>
Reference-contexts: Several techniques for discovering interprocedural constants have been proposed <ref> [3, 4, 13, 16] </ref> (See Section 5, "Other Work"). The study presented in this paper is based on the work of Callahan et al. [4]. They suggest several jump functions for their framework. <p> This technique has been implemented in several compilation systems and has proved to be efficient in practice. Burke and Cytron propose a similar approach for computing interprocedural constants within a unified framework that integrates dependence analysis with interprocedural analysis <ref> [3] </ref>. Intraprocedural constant propagation is employed to determine which constants are available for propagation at call sites. Interprocedural constants are propagated from call sites to the called procedure and used as initial information in the intraprocedural constant propagation.
Reference: [4] <author> David Callahan, Keith D. Cooper, Ken Kennedy, and Linda Torczon. </author> <title> Interprocedural constant propagation. </title> <journal> SIGPLAN Notices, </journal> <volume> 21(7) </volume> <pages> 152-161, </pages> <month> July </month> <year> 1986. </year> <booktitle> Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction. </booktitle>
Reference-contexts: Several techniques for discovering interprocedural constants have been proposed <ref> [3, 4, 13, 16] </ref> (See Section 5, "Other Work"). The study presented in this paper is based on the work of Callahan et al. [4]. They suggest several jump functions for their framework. <p> Several techniques for discovering interprocedural constants have been proposed [3, 4, 13, 16] (See Section 5, "Other Work"). The study presented in this paper is based on the work of Callahan et al. <ref> [4] </ref>. They suggest several jump functions for their framework. Those jump functions vary in complexity and in the class of interprocedural constants that they are expected to propagate. <p> The results presented in this paper were computed using a simple worklist iterative scheme. Alternative formulations based on the binding multi-graph are possible [7]. The method presented by Callahan et al. essentially models the binding graph computation on the call graph <ref> [4] </ref>. Callahan et al. showed a single simple example to demonstrate that different jump function techniques produced different results. They fail, however, to provide any experimental evidence that suggests where the tradeoff lies between jump function complexity and the precision of the CONSTANTS sets. <p> However, each actual parameter depends on exactly one formal parameter. Taken together, these facts suggest that the interprocedural propagation can be completed in O ( P P y cost (J y s )) time. Calla-han et al. present an algorithm that achieves that time bound <ref> [4] </ref>. The implementation used in our experiment was less efficient. Note that cost (J y s ) is greater for the pass-through jump function than for the literal constant and intraprocedu-ral constant jump functions. 3. The polynomial parameter jump function also passes constants along arbitrary paths in the call graph. <p> The polynomial parameter jump function also passes constants along arbitrary paths in the call graph. Each element in support (J y s ) can be lowered at most twice, resulting in the re-evaluation of J y s . Thus the time required for interprocedural propagation is <ref> [4] </ref>: O ( s y cost (J y s ) jsupport (J y s )j) In practice, we found that the number of complex polynomial jump functions actually constructed is small. <p> Callahan, Cooper, Kennedy, and Torczon present an interprocedural constant propagation framework that consists of an algorithm for propagating constants across call boundaries and a family of methods, called jump functions, used to propagate the constants through procedure bodies <ref> [4] </ref>. Both inter-procedural constants passed to called procedures and those returned by a called procedure can be detected using this technique. Because this technique does not make paths through the call graph explicit, it potentially detects fewer constants than the method proposed by Wegman and Zadeck.
Reference: [5] <author> Keith D. Cooper, Mary W. Hall, Robert T. Hood, Ken Kennedy, Kathryn McKinley, John Mellor-Crummey, Linda Torczon, and Scott K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2), </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: the various jump functions described in Section 3. 4.1 Implementation The interprocedural constant propagation system used in this study was built on top of an SSA-based value number graph [8] and the call graph abstraction from ParaScope, a set of tools designed to aid in interactive parallelization of FORTRAN programs <ref> [5] </ref>. Its execution proceeds in four stages: generation of return jump functions, generation of forward jump functions, interprocedural propagation of constants, and recording the results. Generating return jump functions.
Reference: [6] <author> Keith D. Cooper, Mary W. Hall, and Ken Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proceedings of the IEEE Computer Society 1992 International Conference on Computer Languages, </booktitle> <pages> pages 96-105, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: These constants are called interprocedural constants. Interprocedural constants have been used to improve the results of analysis or optimization in a variety of different situations, including computation of dependence information [14], automatic paralleliza-tion of sequential routines [9, 15], and procedure cloning <ref> [6, 13] </ref>. They also improve many scalar optimizations, including intraprocedural constant propagation, dead code elimination, and certain loop transformations. Shen, Li, and Yew examined the improvement in dependence information that is obtained when the values of interprocedural constants are known [14].
Reference: [7] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Interpro-cedural side-effect analysis in linear time. </title> <journal> SIG-PLAN Notices, </journal> <volume> 23(7) </volume> <pages> 57-66, </pages> <month> July </month> <year> 1988. </year> <booktitle> Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: The results presented in this paper were computed using a simple worklist iterative scheme. Alternative formulations based on the binding multi-graph are possible <ref> [7] </ref>. The method presented by Callahan et al. essentially models the binding graph computation on the call graph [4]. Callahan et al. showed a single simple example to demonstrate that different jump function techniques produced different results.
Reference: [8] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Building the SSA graph for a procedure typically requires O (N ) steps, where N is the size of the procedure (the maximum of the number of nodes, edges, variable assignments, or variable references in the procedure) <ref> [8, page 487] </ref>. While the time required for construction of these three jump functions is similar, the complexity of the data structures used by the polynomial parameter jump function is significantly greater than for either the intraprocedural constant or pass-through parameter jump function. <p> These limitations are in line with the goals of our study to compare the sets of integer constants discovered with the various jump functions described in Section 3. 4.1 Implementation The interprocedural constant propagation system used in this study was built on top of an SSA-based value number graph <ref> [8] </ref> and the call graph abstraction from ParaScope, a set of tools designed to aid in interactive parallelization of FORTRAN programs [5]. Its execution proceeds in four stages: generation of return jump functions, generation of forward jump functions, interprocedural propagation of constants, and recording the results. Generating return jump functions. <p> Generating return jump functions. In the first phase, a return jump function for each formal parameter modified by a procedure is calculated in a bottom-up walk over the call graph. At each node in the call graph, the analyzer builds an SSA graph for the corresponding procedure <ref> [8] </ref>. To determine which parameters can have constant values on return from the routine, we value number the SSA graph. The value numbering pass incorporates information provided by already-computed return jump functions for subroutines invoked by the current routine.
Reference: [9] <author> Rudolf Eigenmann and William Blume. </author> <title> An effectiveness study of parallelizing compiler techniques. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: These constants are called interprocedural constants. Interprocedural constants have been used to improve the results of analysis or optimization in a variety of different situations, including computation of dependence information [14], automatic paralleliza-tion of sequential routines <ref> [9, 15] </ref>, and procedure cloning [6, 13]. They also improve many scalar optimizations, including intraprocedural constant propagation, dead code elimination, and certain loop transformations. Shen, Li, and Yew examined the improvement in dependence information that is obtained when the values of interprocedural constants are known [14]. <p> Because many dependence analyzers are incapable of analyzing nonlinear subscripts, this is an important application of interprocedural constants. Another arena in which interprocedural constants are valuable is automatic detection of parallelism. Eigenmann and Blume suggested that interprocedu-ral constants are often used as loop bounds <ref> [9] </ref>. Loop bounds are important for two reasons. First, they improve dependence information, as described above.
Reference: [10] <author> Paul Havlak. </author> <title> Interprocedural Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <address> expected May, </address> <year> 1993. </year>
Reference-contexts: The additional effort of a "complete" propagation does not appear justified by the additional constants found. In fact, the results that we obtained in this study with "complete" propagation can be achieved by basing the jump-function generator on a gated single-assignment form <ref> [2, 10] </ref>. An analyzer based on gated single-assignment form would never consider the dead assignments that we found in the "complete" propagation. 6 This would let the standard polynomial jump function produce the results seen with "complete" propagation.
Reference: [11] <author> Matthew S. Hecht. </author> <title> Flow Analysis of Computer Programs. </title> <booktitle> Programming Languages Series. </booktitle> <publisher> Else-vier North-Holland, Inc., </publisher> <address> 52 Vanderbilt Avenue, New York, NY 10017, </address> <year> 1977. </year>
Reference-contexts: As before, we denote the support set of R x p as support (R x p ). Given the call graph G and the forward and return jump functions for each procedure, we can propagate the VAL sets around the graph using a standard iterative technique <ref> [11, 12] </ref>. The results presented in this paper were computed using a simple worklist iterative scheme. Alternative formulations based on the binding multi-graph are possible [7]. The method presented by Callahan et al. essentially models the binding graph computation on the call graph [4].
Reference: [12] <author> Gary A. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Conference Record of the ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-206, </pages> <month> October </month> <year> 1973. </year>
Reference-contexts: As before, we denote the support set of R x p as support (R x p ). Given the call graph G and the forward and return jump functions for each procedure, we can propagate the VAL sets around the graph using a standard iterative technique <ref> [11, 12] </ref>. The results presented in this paper were computed using a simple worklist iterative scheme. Alternative formulations based on the binding multi-graph are possible [7]. The method presented by Callahan et al. essentially models the binding graph computation on the call graph [4].
Reference: [13] <author> Robert Metzger and Sean Stroud. </author> <title> Interprocedu-ral constant propagation: An empirical study. </title> <note> (To appear in ACM Letters on Programming Languages and Systems). </note>
Reference-contexts: These constants are called interprocedural constants. Interprocedural constants have been used to improve the results of analysis or optimization in a variety of different situations, including computation of dependence information [14], automatic paralleliza-tion of sequential routines [9, 15], and procedure cloning <ref> [6, 13] </ref>. They also improve many scalar optimizations, including intraprocedural constant propagation, dead code elimination, and certain loop transformations. Shen, Li, and Yew examined the improvement in dependence information that is obtained when the values of interprocedural constants are known [14]. <p> Several techniques for discovering interprocedural constants have been proposed <ref> [3, 4, 13, 16] </ref> (See Section 5, "Other Work"). The study presented in this paper is based on the work of Callahan et al. [4]. They suggest several jump functions for their framework. <p> The numbers reported in the next section count the number of constants that this option substituted into each program. Metzger and Stroud suggest that this is the right measurement of effectiveness <ref> [13] </ref>. <p> The method described does not handle returned constants or make paths through the call graph explicit. Metzger and Stroud implemented interprocedural constant propagation in the CONVEX Application Compiler based on ideas presented in Callahan et al. <ref> [13] </ref>. They used interprocedural constants to guide procedure cloning. Their empirical results indicate that goal-directed cloning of procedures based on in-terprocedural constants can substantially increase the number of interprocedural constants available for use by later analysis and optimization passes.
Reference: [14] <author> Zhiyu Shen, Zhiyuan Li, and Pen-Chung Yew. </author> <title> An empirical study of FORTRAN programs for parallelizing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: These constants are called interprocedural constants. Interprocedural constants have been used to improve the results of analysis or optimization in a variety of different situations, including computation of dependence information <ref> [14] </ref>, automatic paralleliza-tion of sequential routines [9, 15], and procedure cloning [6, 13]. They also improve many scalar optimizations, including intraprocedural constant propagation, dead code elimination, and certain loop transformations. <p> They also improve many scalar optimizations, including intraprocedural constant propagation, dead code elimination, and certain loop transformations. Shen, Li, and Yew examined the improvement in dependence information that is obtained when the values of interprocedural constants are known <ref> [14] </ref>. In their experiment, they performed the interproce-dural constant propagation manually and added assertions to the program text about constant variable values. In their examination of FORTRAN library routines, they found that knowledge of interprocedu-ral constants significantly decreased the number of nonlinear subscript references.
Reference: [15] <author> Jaswinder P. Singh and John Hennessy. </author> <title> An empirical investigation of the effectiveness of and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: These constants are called interprocedural constants. Interprocedural constants have been used to improve the results of analysis or optimization in a variety of different situations, including computation of dependence information [14], automatic paralleliza-tion of sequential routines <ref> [9, 15] </ref>, and procedure cloning [6, 13]. They also improve many scalar optimizations, including intraprocedural constant propagation, dead code elimination, and certain loop transformations. Shen, Li, and Yew examined the improvement in dependence information that is obtained when the values of interprocedural constants are known [14].
Reference: [16] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Several techniques for discovering interprocedural constants have been proposed <ref> [3, 4, 13, 16] </ref> (See Section 5, "Other Work"). The study presented in this paper is based on the work of Callahan et al. [4]. They suggest several jump functions for their framework. <p> In scientific FORTRAN codes, inter-procedural constant propagation exposed many constants not found by the intraprocedural propagation. 5 Other Work Several researchers have proposed techniques for propagating interprocedural constants. Wegman and Zadeck propose combining procedure integration with intraprocedural constant propagation to detect inter-procedural constants <ref> [16] </ref>. Because procedure integration makes paths through the program's call graph explicit, the interprocedural information computed along a particular path may be improved. By combining the results of the improved analysis with dead code elimination, it may be possible to eliminate some paths through the program.
References-found: 16

