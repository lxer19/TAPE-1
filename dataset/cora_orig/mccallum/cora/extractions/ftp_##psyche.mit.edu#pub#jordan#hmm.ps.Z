URL: ftp://psyche.mit.edu/pub/jordan/hmm.ps.Z
Refering-URL: http://www.cs.ualberta.ca/~greiner/PSLS/reading.html
Root-URL: 
Title: Probabilistic Independence Networks for Hidden Markov Probability Models  
Author: Padhraic Smyth, David Heckerman, and Michael Jordan 
Note: Copyright c Massachusetts Institute of Technology, 1996  
Date: 1565 February 2, 1996  132  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L. Memo No.  
Abstract: Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper contains a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach. This report describes research done at the Department of Information and Computer Science, University of California, Irvine, the Jet Propulsion Laboratory, California Institute of Technology, Microsoft Research, the Center for Biological and Computational Learning, and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. The authors can be contacted as pjs@aig.jpl.nasa.gov, heckerma@microsoft.com, and jordan@psyche.mit.edu. Support for CBCL is provided in part by a grant from the NSF (ASC-9217041). Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Dept. of Defense. MIJ gratefully acknowledges discussions with Steffen Lauritzen on the application of the IPF algorithm to UPINs. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bishop, Y.M.M., Fienberg, S.E. and Holland, P.W. </author> <year> 1973. </year> <title> Discrete Multivariate Analysis: Theory and Practice. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: We wish to convert such a local specification into a globally consistent joint probability distribution, i.e., a marginal representation. An algorithm known as Iterative Proportional Fitting (IPF) is available to perform this conversion. Classically, IPF proceeds as follows <ref> (Bishop, Fienberg, & Holland, 1973) </ref>. Suppose for simplicity that all of the random variables are discrete (a Gaussian version of IPF is also available (Whittaker 1990)) such that the joint distribution can be represented as a table. The table is initialized with equal values in all of the cells.
Reference: <author> Buntine, W. </author> <year> 1994. </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research. </journal> <pages> 2 159-225. </pages>
Reference: <author> Buntine, W. </author> <title> in press. A guide to the literature on learning probabilistic networks from data. IEEE Transactions on Knowledge and Data Engineering. 1 One caveat: The BIC score is derived under the assumption that the parameter prior is positive throughout its domain. 26 Dawid, </title> <editor> A. P. </editor> <year> 1992. </year> <title> Applications of a general propagation algorithm for probabilistic expert systems. </title> <journal> Statistics and Computing. </journal> <pages> 2 25-36. </pages>
Reference: <author> DeGroot, M. </author> <year> 1970. </year> <title> Optimal Statistical Decisions. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Dempster, A., Laird, N., Rubin, D.1977. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B. </journal> <volume> 39, </volume> <pages> 1-38. </pages>
Reference-contexts: algorithm is called as a subroutine on the tractable graph during the minimization process. 9 Learning and PINs 9.1 Parameter Estimation for PINs The parameters of a graphical model can be estimated with maximum-likelihood (ML), maximum-a-posteriori (MAP), or full Bayesian methods, using traditional techniques such as gradient descent, expectation-maximization (EM) <ref> (e.g., Dempster et al., 1977) </ref>, and Monte-Carlo sampling (e.g., Neal, 1993). For the standard HMM (1,1) model discussed in this paper, where either discrete, Gaussian, or Gaussian-mixture codebooks are used, a ML or MAP estimate using EM is a well-known efficient approach (Poritz 1988; Rabiner 1989).
Reference: <author> Frasconi, P. and Bengio, Y. </author> <year> 1994. </year> <title> An EM approach to grammatical inference: input/output HMMs. </title> <booktitle> Proceedings of the 12th IAPR Intl. Conf. on Pattern Recognition, </booktitle> <publisher> IEEE Computer Society Press. </publisher> <pages> 289-294. </pages>
Reference: <author> Fung, R. M. and Crawford, S. L. </author> <year> 1990. </year> <title> A system for induction of probabilistic models. </title> <booktitle> Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA: </address> <publisher> AAAI, </publisher> <pages> 762-779. </pages>
Reference-contexts: The BIC score is the additive inverse of Rissanen's (1987) minimum description length (MDL). Other scores, which can be viewed as approximations to the marginal likelihood, are hypothesis testing (Raftery 1995) and cross validation <ref> (Fung and Crawford 1990) </ref>. Buntine (in press) provides a comprehensive review of the literature on learning PINs. In the context of HMM (K; J) type structures, an obvious question is how one could learn such structure from data, where K and J are unknown a priori.
Reference: <author> Gauvain, J., Lee, C. </author> <year> 1994. </year> <title> Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains. </title> <journal> IEEE Trans. Sig. Audio Proc.. </journal> <volume> 2, </volume> <pages> 291-298. </pages>
Reference-contexts: These priors have also been used in MAP estimates of standard HMMs <ref> (e.g., Gauvain and Lee, 1994) </ref>. Heckerman and Geiger (1995) describe a simple method for assessing these priors. The use of the EM algorithm for UPINs is similar.
Reference: <author> Geman, S. and Geman, D. </author> <year> 1984. </year> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell. </journal> <volume> 6, </volume> <pages> 721-741. </pages>
Reference: <author> Ghahramani, Z., and Jordan, M. I. </author> <year> 1996. </year> <title> Factorial Hidden Markov models. </title> <editor> In D. S. Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Heckerman, D., and Geiger, D. </author> <year> 1995. </year> <title> Likelihoods and priors for Bayesian networks. </title> <publisher> MSR-TR-95-54, Microsoft, </publisher> <address> Redmond, WA. </address>
Reference: <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <year> 1995. </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning. </journal> <volume> 20, </volume> <pages> 197-243. </pages>
Reference: <author> Isham, V. </author> <year> 1981. </year> <title> An introduction to spatial point processes and Markov random fields. </title> <journal> International Statistical Review. </journal> <volume> 49, </volume> <pages> 21-43. </pages>
Reference-contexts: The clique functions 4 represent the particular parameters associated with the UPIN structure. This corresponds directly to the standard definition of a Markov random field <ref> (Isham 1981) </ref>. The clique functions reflect the relative "compatibility" of the value assignments in the clique. A model p is said to be decomposable if it has a minimal UPIN structure G which is triangulated (Figure 2). A UPIN structure G is decomposable if G is triangulated.
Reference: <author> Jensen, F. V. </author> <year> 1995. </year> <title> Introduction to Bayesian networks. HUGIN Expert A/S, </title> <publisher> Aalborg, Den-mark. </publisher>
Reference-contexts: If no node can be eliminated without adding links, then we choose the node that can be eliminated by adding the links that yield the clique with the smallest state-space <ref> (Jensen 1995) </ref>. After triangulation the JLO algorithm constructs a junction tree from G 0 , i.e., a clique tree satisfying the running intersection property.
Reference: <author> Jensen, F. V., Lauritzen, S. L. and Olesen, K. G., </author> <year> 1990. </year> <title> Bayesian updating in recursive graphical models by local computations. </title> <journal> Computational Statistical Quarterly. </journal> <volume> 4, </volume> <pages> 269-282. </pages>
Reference-contexts: When the schedule of flows is complete one gets a new representation K fl f such that the local potential on each clique is f fl (x C ) = p (x h C ; e), i.e., the joint probability of the local unobserved clique variables and the observed evidence <ref> (Jensen et al. 1990) </ref> (similarly for the separator potential functions). If one marginalizes at the clique over the unobserved local clique variables, X C C ; e) = p (e); (16) one gets the probability of the observed evidence directly.
Reference: <author> Jirousek, R. and Preucil, S. </author> <year> 1995. </year> <title> On the effective implementation of the iterative proportional fitting procedure. </title> <journal> Computational Statistics and Data Analysis. </journal> <volume> 19, </volume> <pages> 177-189. </pages>
Reference: <author> Kent, R. D. & Minifie, F. D. </author> <year> 1977. </year> <title> Coarticulation in recent speech production models. </title> <journal> Journal of Phonetics. </journal> <volume> 5, </volume> <pages> 115-117. </pages>
Reference-contexts: This coupled physical process is not well modeled by the unstructured state transition matrix of HMM (1,1). Moreover, the first-order Markov properties of HMM (1,1) are not well suited to modeling the ubiquitous coarticulation effects that occur in speech, particularly coarticulatory effects that extend across several phonemes <ref> (cf. Kent & Minifie, 1977) </ref>. A variety of techniques have been developed to surmount these basic weaknesses of the HMM (1,1) model, including mixture modeling of emission probabilities, triphone modeling, and discriminative training.
Reference: <author> Lauritzen, S. L. and Spiegelhalter D. J. </author> <year> 1988. </year> <title> Local computations with probabilities on graphical structures and their application to expert systems (with discussion). </title> <journal> J. Roy. Statist. Soc. Ser. </journal> <volume> B. </volume> <pages> 50 157-224. </pages>
Reference: <author> Lauritzen, S., and Wermuth, N. </author> <year> 1989. </year> <title> Graphical models for associations between variables, some of which are qualitative and some quantitative. </title> <journal> Annals of Statistics. </journal> <volume> 17, </volume> <pages> 31-57. </pages> <note> 27 Lauritzen, </note> <author> S. L., Dawid, A. P., Larsen, B. N., and Leimer, H. </author> <title> G 1990. Independence properties of directed Markov fields. </title> <journal> Networks. </journal> <volume> 20, </volume> <pages> 491-505. </pages>
Reference: <author> Lindblom, B. </author> <year> 1990. </year> <title> Explaining phonetic variation: A sketch of the H&H theory. In Speech Production and Speech Modeling, </title> <editor> W.J. Hardcastle and A. Marchal, (Eds.). </editor> <publisher> Kluwer: Dor-drecht. </publisher>
Reference: <author> Lucke, H. </author> <year> 1995. </year> <title> Bayesian Belief Networks as a tool for stochastic parsing. </title> <journal> Speech Communication. </journal> <volume> 16, </volume> <pages> 89-118. </pages>
Reference: <author> Modestino, J. and Zhang, J. </author> <year> 1992. </year> <title> A Markov random field model-based approach to image segmentation. </title> <journal> IEEE Trans. Patt. Anal. Mach. Int. </journal> <volume> 14(6), </volume> <pages> 606-615. </pages>
Reference: <author> Neal, R. </author> <year> 1993. </year> <title> Probabilistic inference using Markov chain Monte Carlo methods. </title> <institution> CRG-TR-93-1, Department of Computer Science, University of Toronto. </institution>
Reference-contexts: tractable graph during the minimization process. 9 Learning and PINs 9.1 Parameter Estimation for PINs The parameters of a graphical model can be estimated with maximum-likelihood (ML), maximum-a-posteriori (MAP), or full Bayesian methods, using traditional techniques such as gradient descent, expectation-maximization (EM) (e.g., Dempster et al., 1977), and Monte-Carlo sampling <ref> (e.g., Neal, 1993) </ref>. For the standard HMM (1,1) model discussed in this paper, where either discrete, Gaussian, or Gaussian-mixture codebooks are used, a ML or MAP estimate using EM is a well-known efficient approach (Poritz 1988; Rabiner 1989).
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Associated with each edge in the junction tree is a separator S, such that S contains the variables in the intersection of the two cliques which it links. Given a junction tree representation, one can factorize p (U) as the product of clique marginals over separator marginals <ref> (Pearl 1988) </ref>: p (u) = C2V C p (x C ) S2V S p (x S ) where p (x C ) and p (x S ) are the marginal (joint) distributions for the variables in clique C and separator S respectively and V C and V S are the set
Reference: <author> Pearl, J., Geiger, D., and Verma, T. </author> <year> 1990. </year> <title> The logic of influence diagrams. Influence Diagrams, Belief Nets, and Decision Analysis. Oliver, </title> <editor> R. M. and Smith, J. Q. (eds.). </editor> <address> Chichester, U.K.: </address> <publisher> John Wiley and Sons. </publisher> <pages> 67-83. </pages>
Reference: <author> Perkell, J. S., Matthies, M. L., Svirsky, M. A., and Jordan, M. I. </author> <year> 1993. </year> <title> Trading relations between tongue-body raising and lip rounding in production of the vowel /u/: A pilot motor equivalence study. </title> <journal> Journal of the Acoustical Society of America. </journal> <volume> 93, </volume> <pages> 2948-2961. </pages>
Reference: <author> Poritz, A. M. </author> <year> 1988. </year> <title> Hidden Markov models: a guided tour. </title> <booktitle> Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. </booktitle> <address> New York: </address> <publisher> IEEE Press. vol.1, </publisher> <pages> 7-13. </pages>
Reference: <author> Rabiner, L., </author> <year> 1989. </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77, </volume> <pages> 257-285. </pages>
Reference: <author> Raftery, A. </author> <year> 1995. </year> <title> Bayesian model selection in social research. </title> <editor> In Marsden, P., </editor> <booktitle> Sociological Methodology. </booktitle> <address> Blackwells, Cambridge, MA. </address>
Reference-contexts: The BIC score is the additive inverse of Rissanen's (1987) minimum description length (MDL). Other scores, which can be viewed as approximations to the marginal likelihood, are hypothesis testing <ref> (Raftery 1995) </ref> and cross validation (Fung and Crawford 1990). Buntine (in press) provides a comprehensive review of the literature on learning PINs.
Reference: <author> Rissanen, J. </author> <year> 1987. </year> <title> Stochastic complexity (with discussion). </title> <journal> Journal of the Royal Statistical Society, Series B. </journal> <volume> 49, </volume> <pages> 223-239 and 253-265. </pages>
Reference: <author> Saul, L. K., and Jordan, M. I. </author> <year> 1995. </year> <title> Boltzmann chains and hidden Markov models. </title> <booktitle> In G. </booktitle>
Reference-contexts: This second property is particularly important| the existence of the JLO algorithm frees us from having to derive particular recursive algorithms on a case-by-case basis. The first model that we consider can be viewed as a coupling of two HMM (1,1) chains <ref> (Saul & Jordan, 1995) </ref>. Such a model can be useful in general sensor fusion problems, for example in the fusion of an audio signal with a video signal in lipreading.
Reference: <editor> Tesauro, D. S. Touretzky & T. K. Leen, (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Saul, L. K., and Jordan, M. I. </author> <year> 1996. </year> <title> Exploiting tractable substructures in intractable networks. </title> <editor> In D. S. Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Shachter, R. D., Anderson, S. K. and Szolovits, P. </author> <year> 1994. </year> <title> Global conditioning for prababilistic inference in belief networks. </title> <booktitle> Proceedings of the Uncertainty in AI Conference 1994, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 514-522. </pages> <note> 28 Schwarz, </note> <author> G. </author> <year> 1978. </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics. </journal> <volume> 6, </volume> <pages> 461-464. </pages>
Reference-contexts: Furthermore, it has been shown that many of the inference algorithms for DPINs are in fact special cases of inference algorithms for UPINs and can be considerably less efficient <ref> (Shachter et al. 1994) </ref>. 4 Modeling HMMs as PINs 4.1 PINs for HMMs In hidden Markov modeling problems (Poritz 1988; Rabiner 1989) we are interested in the set of random variables U = fH 1 ; O 1 ; H 2 ; O 2 ; : : : ; H N1 <p> There are many variations on the basic JLO and Dawid algorithms. For example, Pearl (1988) describes related versions of these algorithms in his early work . However, it can be shown <ref> (Shachter et al. 1994) </ref> that all known exact algorithms for inference on DPINs are equivalent at some level to the JLO and Dawid algorithms. Thus, it is sufficient to consider the JLO and Dawid algorithms in our discussion as they subsume other graphical inference algorithms.
Reference: <author> Smyth, P. </author> <year> 1994. </year> <title> Hidden Markov models for fault detection in dynamic systems. </title> <journal> Pattern Recognition, </journal> <volume> 27, </volume> <pages> 149-164. </pages>
Reference-contexts: Inferring the posterior state probabilities is useful when the states have direct physical interpretations (as in fault monitoring applictions <ref> (Smyth 1994) </ref>) and is also implicitly required during the standard Baum-Welch learning algorithm for HMM (1,1). In general, both of these computations scale as m N where m is the number of states for each hidden variable.
Reference: <author> Spiegelhalter, D. J., Dawid, A. P., Hutchinson, T. A., and Cowell, R. G. </author> <year> 1991. </year> <title> Probabilistic expert systems and graphical modelling: a case study in drug safety. </title> <journal> Phil. Trans. R. Soc. Lond. A. </journal> <volume> 337, </volume> <pages> 387-405. </pages>
Reference-contexts: DPINs have found application in causal modelling in applied statistics and artificial intelligence. Their popularity in these fields stems from the fact that the joint probability model can be specified directly via Equation 3, i.e., via the specification of conditional probability tables or functions <ref> (Spiegelhalter et al. 1991) </ref>. In contrast, UPINs must be specified in terms of clique functions (as in Equation 1) which may not be as easy to work with (cf.
Reference: <author> Spirtes, P. and Meek, C. </author> <year> 1995. </year> <title> Learning Bayesian networks with discrete variables from data. </title> <booktitle> In Proceedings of First International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Montreal, </address> <publisher> QU. Morgan Kaufmann. </publisher>
Reference: <author> Stolorz, P. </author> <year> 1994. </year> <title> Recursive approaches to the statistical physics of lattice proteins. In L. </title>
Reference: <editor> Hunter, ed. </editor> <booktitle> Proc. 27th Hawaii Intl. Conf. on System Sciences, V, </booktitle> <pages> 316-325. </pages>
Reference: <author> Tao, C., </author> <year> 1992. </year> <title> A generalization of the discrete hidden Markov model and of the Viterbi algorithm. </title> <journal> Pattern Recognition, </journal> <volume> 25(11), </volume> <pages> 1381-1387. </pages>
Reference-contexts: What is interesting about this equivalence result is that the graphical algorithms are more general than the F-B and Viterbi algorithms: 1. While special purpose extensions to the standard Viterbi and F-B algorithms can be derived to handle various extensions to HMM (1,1) <ref> (Tao 1992) </ref>, the JLO algorithms provide by definition a completely general exact inference method for any PIN. 2. The graphical algorithms can easily handle other inference tasks besides just calculating the likelihood of the evidence or the MAP solution.
Reference: <author> Vandermeulen, D., Verbeeck, R., Berben, L. Delaere, D., Suetens, P. and Marchal, G. </author> <year> 1994. </year> <title> Continuous voxel classification by stochastic relaxation: theory and application to MR imaging and MR angiography. </title> <booktitle> Image and Vision Computing. </booktitle> <pages> 12(9) 559-572. </pages>
Reference: <author> Whittaker, J. </author> <year> 1990. </year> <title> Graphical Models in Applied Multivariate Statistics, </title> <address> Chichester, UK: </address> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: Conditional independence is symmetric. Note also that marginal independence (no conditioning) does not in general imply conditional independence, nor does conditional independence in general imply marginal independence <ref> (Whittaker 1990) </ref>. With any set of random variables U we can associate a graph G defined as G = (V; E). <p> If E contains a mixture of directed and undirected edges, then it is referred to as a mixed or chain graph. We note in passing that there exists a theory for graphical independence models involving mixed graphs <ref> (Whittaker 1990) </ref> but mixed graphs will not be discussed further in this paper. <p> An algorithm known as Iterative Proportional Fitting (IPF) is available to perform this conversion. Classically, IPF proceeds as follows (Bishop, Fienberg, & Holland, 1973). Suppose for simplicity that all of the random variables are discrete (a Gaussian version of IPF is also available <ref> (Whittaker 1990) </ref>) such that the joint distribution can be represented as a table. The table is initialized with equal values in all of the cells.
Reference: <author> Williams, C., and Hinton, G. E. </author> <year> 1990. </year> <title> Mean field networks that learn to discriminate temporally distorted strings. </title> <booktitle> Proc. Connectionist Models Summer School, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 18-22. </pages>
References-found: 43

