URL: http://now.cs.berkeley.edu/Papers2/Postscript/hpca98.ps
Refering-URL: http://now.cs.berkeley.edu/Papers2/recent.html
Root-URL: 
Email: pattersong@cs.berkeley.edu  
Title: The Architectural Costs of Streaming I/O: A Comparison of Workstations, Clusters, and SMPs  
Author: Remzi H. Arpaci-Dusseau, Andrea C. Arpaci-Dusseau, David E. Culler, Joseph M. Hellerstein, and David A. Patterson fremzi, dusseau, culler, jmh, 
Keyword: Clusters, SMPs, Balance, I/O  
Affiliation: Computer Science Division University of California, Berkeley  
Abstract: We investigate resource usage while performing streaming I/O by contrasting three architectures, a single workstation, a cluster, and an SMP, under various I/O benchmarks. We derive analytical and empirically-based models of resource usage during data transfer, examining the I/O bus, memory bus, network, and processor of each system. By investigating each resource in detail, we assess what comprises a well-balanced system for these workloads. We find that the architectures we study are not well balanced for streaming I/O applications. Across the platforms, the main limitation to attaining peak performance is the CPU, due to lack of data locality. Increasing processor performance (especially with improved block operation performance) will be of great aid for these workloads in the future. For a cluster workstation, the I/O bus is a major system bottleneck, because of the increased load placed on it from network communication. A well-balanced cluster workstation should have copious I/O bus bandwidth, perhaps via multiple I/O busses. The SMP suffers from poor memory-system performance; even when there is true parallelism in the benchmark, contention in the shared-memory system leads to reduced performance. As a result, the clustered workstations provide higher absolute performance for streaming I/O workloads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Agarwal. </author> <title> A Super Scalar Sort Algorithm for RISC Processors. </title> <booktitle> In 1996 ACM SIGMOD Conference, </booktitle> <pages> pages 240-246, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The first is a simple scan, which reads data from disk sequentially, selects matching records, and writes those records to disk. The second benchmark is an external sort. Sorting is a longtime database-industry standard benchmark, and has recently garnered much interest <ref> [1, 4, 15] </ref>. In addition to being useful in a classical database environment, sorting is also typical of the workload placed on systems performing decision support [9]. In this paper, we use the single-node and cluster versions of NOW-Sort, currently the world's fastest disk-to-disk sorting program [4]. <p> Second, the number of keys in each bucket matches the size of the second-level cache. The next step sorts the keys in each bucket, using the algorithm described in <ref> [1] </ref>. Because this step accounts for only a very small fraction of the total execution time and performs no I/O, we do not discuss it further. Finally, the write phase scans the bucket array, gathering sorted records and writing them to disk.
Reference: [2] <author> G. </author> <title> Amdahl. Storage and I/O Parameters and System Potential. </title> <booktitle> In IEEE Computer Group Conference, </booktitle> <pages> pages 371-72, </pages> <month> June </month> <year> 1970. </year>
Reference-contexts: This design principle reminds architects not to focus all engineering effort on any small subset of the system, for performance gains in one sub-system may be obviated by the lack of similar gains in another. The Amdahl/Case rule of thumb provides a guideline for building such balanced systems <ref> [2] </ref>. However, since this rule originated, performance has increased by many orders of magnitude.
Reference: [3] <author> T. E. Anderson, D. E. Culler, and D. A. Patterson. </author> <title> A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1994. </year>
Reference-contexts: We focus on the Sun Ultra 1 workstation, which forms a basis for comparison with the other two systems. The second platform is a cluster of Ultra 1 workstations, an instance of larger-scale systems comprised of commodity workstations and high-speed networks such as ATM and Myrinet <ref> [3, 5, 10, 20] </ref>. By providing a low-latency, high-bandwidth interconnection, these switches have the potential of fusing workstations into a cohesive whole. Implicitly, we evaluate an underlying assumption of clustered systems: that the workstation in its current form is a good building block [3]. <p> By providing a low-latency, high-bandwidth interconnection, these switches have the potential of fusing workstations into a cohesive whole. Implicitly, we evaluate an underlying assumption of clustered systems: that the workstation in its current form is a good building block <ref> [3] </ref>. This assumption may be optimistic, as a machine that is well-balanced for the stand-alone case may not be properly archi-tected for a tightly-integrated cluster environment. The third platform is a small-scale symmetric multiprocessor (SMP), specifically, the Ultra Enterprise 5000.
Reference: [4] <author> A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, D. E. Culler, J. M. Hellerstein, and D. A. Patterson. </author> <title> High-Performance Sorting on Networks of Workstations. </title> <booktitle> In SIGMOD '97, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: The first is a simple scan, which reads data from disk sequentially, selects matching records, and writes those records to disk. The second benchmark is an external sort. Sorting is a longtime database-industry standard benchmark, and has recently garnered much interest <ref> [1, 4, 15] </ref>. In addition to being useful in a classical database environment, sorting is also typical of the workload placed on systems performing decision support [9]. In this paper, we use the single-node and cluster versions of NOW-Sort, currently the world's fastest disk-to-disk sorting program [4]. <p> In addition to being useful in a classical database environment, sorting is also typical of the workload placed on systems performing decision support [9]. In this paper, we use the single-node and cluster versions of NOW-Sort, currently the world's fastest disk-to-disk sorting program <ref> [4] </ref>. The third benchmark is an external transpose. Whereas the first two benchmarks fit into the database domain, transpose is often found in external scientific codes. All benchmarks have been hand optimized for each platform. For the set of benchmarks, we find that none of the systems are well balanced. <p> Again, there is logically no sharing in this benchmark; thus, the resource usage models are identical to the single workstation. 4.2 Sort The most complex benchmark used in this study is the external sort, described in detail in <ref> [4] </ref>. Sorting was chosen by database experts as an excellent test of the memory, I/O, and communication sub-systems of a machine [8]. As described in the scan, we use 10-byte keys within 100-byte records. The basic sorting algorithm is similar on all three platforms.
Reference: [5] <author> N. Boden, D. Cohen, R. E. Felderman, A. Kulawik, and C. Seitz. Myrinet: </author> <title> A Gigabit-per-second Local Area Network. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: We focus on the Sun Ultra 1 workstation, which forms a basis for comparison with the other two systems. The second platform is a cluster of Ultra 1 workstations, an instance of larger-scale systems comprised of commodity workstations and high-speed networks such as ATM and Myrinet <ref> [3, 5, 10, 20] </ref>. By providing a low-latency, high-bandwidth interconnection, these switches have the potential of fusing workstations into a cohesive whole. Implicitly, we evaluate an underlying assumption of clustered systems: that the workstation in its current form is a good building block [3]. <p> Myrinet links can sustain 160 MB/s in each direction <ref> [5] </ref>, and thus has sufficient bandwidth until D r reaches ( 8 7 )160 MB/s. Regarding the ability of Myrinet switches to handle the total communication bandwidth, each switch is a perfect crossbar, and can support 1280 MB/s of aggregate bandwidth when there is no port contention.
Reference: [6] <author> J. B. Chen and B. Bershad. </author> <title> The Impact of Operating System Structure on Memory System Performance. </title> <booktitle> In Proceedings of the 14th Annual Symposium on Operating Systems, </booktitle> <pages> pages 120-133, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Each of our three platforms runs Solaris 2.5.1, a modern, multi-threaded operating system [11]. Though we are presenting a study of architectural characteristics, operating system behavior often dictates usage patterns of the underlying hardware, as shown in <ref> [6, 16, 19] </ref>. Therefore, care must be taken to use the appropriate operating system interfaces. Much of the data movement on the UltraSPARC is performed with special block copy hardware that is available as a part of the VIS instruction set.
Reference: [7] <author> D. E. Culler, L. T. Liu, R. P. Martin, and C. O. Yoshikawa. </author> <title> LogP Performance Assessment of Fast Network Interfaces. </title> <journal> IEEE Micro, </journal> <volume> 2/1996. </volume>
Reference-contexts: In this paper, we use Active Messages 1.0 (GAM) over Myrinet, which has a round-trip latency of roughly 20 s and a bi-directional sustained bandwidth of 40 MB/s (20 MB/s sending, 20 MB/s receiving) <ref> [7] </ref>. The primary unit of data transfer in our benchmark applications is 4 KB. 3.3 Symmetric Multiprocessor The Ultra Enterprise 5000 is a small-scale symmetric multiprocessor with uniform memory access.
Reference: [8] <author> A. et. al. </author> <title> A Measure of Transaction Processing Power. </title> <journal> Data-mation, </journal> <volume> 31(7) </volume> <pages> 112-118, </pages> <year> 1985. </year> <note> Also in Readings in Database Systems, </note> <editor> M.H. Stonebraker ed., </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1989. </year>
Reference-contexts: We now describe each benchmark. 4.1 Scan A sequential scan, modeled after a scan and selection in a database, is the simplest of the three benchmarks. The input set we use in both the scan and the sort is derived from the Datamation <ref> [8] </ref> and MinuteSort [15] sorting benchmarks: 100 byte records with 10-byte keys. In our terminology, key is the size of the key and rec is the size of a record. <p> Sorting was chosen by database experts as an excellent test of the memory, I/O, and communication sub-systems of a machine <ref> [8] </ref>. As described in the scan, we use 10-byte keys within 100-byte records. The basic sorting algorithm is similar on all three platforms. In the first step, the records must be converted from the layout on disk to a format more suitable for efficient sorting.
Reference: [9] <author> J. Gray. </author> <type> Personal Communication, </type> <month> June </month> <year> 1997. </year>
Reference-contexts: The second benchmark is an external sort. Sorting is a longtime database-industry standard benchmark, and has recently garnered much interest [1, 4, 15]. In addition to being useful in a classical database environment, sorting is also typical of the workload placed on systems performing decision support <ref> [9] </ref>. In this paper, we use the single-node and cluster versions of NOW-Sort, currently the world's fastest disk-to-disk sorting program [4]. The third benchmark is an external transpose. Whereas the first two benchmarks fit into the database domain, transpose is often found in external scientific codes.
Reference: [10] <author> M. D. Hill, J. R. Larus, S. Reinhardt, and D. A. Wood. </author> <title> Cooperative-Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <year> 1993. </year>
Reference-contexts: We focus on the Sun Ultra 1 workstation, which forms a basis for comparison with the other two systems. The second platform is a cluster of Ultra 1 workstations, an instance of larger-scale systems comprised of commodity workstations and high-speed networks such as ATM and Myrinet <ref> [3, 5, 10, 20] </ref>. By providing a low-latency, high-bandwidth interconnection, these switches have the potential of fusing workstations into a cohesive whole. Implicitly, we evaluate an underlying assumption of clustered systems: that the workstation in its current form is a good building block [3].
Reference: [11] <author> S. Kleiman, J. Voll, J. Eykholt, A. Shivalingiah, D. Williams, M. Smith, S. Barton, and G. Skinner. </author> <title> Symmetric Multiprocessing in Solaris 2.0. </title> <booktitle> In Proceedings of COMPCON Spring '92, </booktitle> <year> 1992. </year>
Reference-contexts: We extend the disk capacity of the system with one or more fast-wide SCSI controllers, each connected to two external disks. Each of our three platforms runs Solaris 2.5.1, a modern, multi-threaded operating system <ref> [11] </ref>. Though we are presenting a study of architectural characteristics, operating system behavior often dictates usage patterns of the underlying hardware, as shown in [6, 16, 19]. Therefore, care must be taken to use the appropriate operating system interfaces.
Reference: [12] <author> C. L. Kuszmaul. </author> <title> Out-of-core FFTs in a Parallel Application Environment. </title> <type> Technical report, NAS Technical Report, </type> <institution> RND-93-013, </institution> <year> 1993. </year>
Reference-contexts: Finally, during the write phase, each processor gathers its records and writes them to disk, as in the single workstation sort. 4.3 Transpose Our final benchmark is transpose, similar to an operation found in external scientific codes, such as out-of-core FFT <ref> [12] </ref>. The basic operation reads in blocks in row-major order, and writes them to disk in column-major order. Blocks are 4 KB for this benchmark, a departure from the 100-byte records of the two previous benchmarks.
Reference: [13] <author> J. D. McCalpin. </author> <title> Sustainable Memory Bandwidth in Current High-Performance Computers. </title> <type> White Paper, </type> <year> 1995. </year>
Reference-contexts: For example, in the realm of scientific computing, balance is sometimes defined as the number of peak floating-point operations per cycle divided by sustained memory operations per cycle <ref> [13] </ref>. However, for I/O-based workloads, there is not a clear definition. What constitutes a well-balanced architecture for applications with large demands for streaming I/O? For I/O-intensive workloads, we define a well-balanced system as one where all resources simultaneously reach near-peak utilization during input and output phases. <p> Therefore, small capacity systems also have small performance capabilities. Indeed, in our early experiments, our server was configured with only two memory banks, limiting our sustainable performance to 1 GB/s on 8 processors (running a copy micro-benchmark similar to that in <ref> [13] </ref>). 5.3.3 Memory Bus Analysis We have seen that subtleties in program interaction with the cache architecture, the operating system, and network communication, all lead to excess memory traffic.
Reference: [14] <author> S. S. Mukherjee and M. D. Hill. </author> <title> A Case for Making Network Interfaces Less Peripheral. In Hot Interconnects V, </title> <month> Aug. </month> <year> 1997. </year>
Reference-contexts: A recent machine from Sun, the Ultra30 workstation, is a good example of this: two PCI busses make this machine ideal for cluster computing. More radical solutions suggest placing the network interface on the memory bus <ref> [14] </ref>. <p> For example, in the cluster sort, the first copy is explicit in the program, where 100-byte records are copied into larger 4 KB blocks to amortize the overhead of sending a message. This copy could be avoided with tight integration between the network interface and the processor <ref> [14] </ref>, lowering overheads and allowing applications to send small messages at peak rates. The second copy occurs when message layer must copy buffers into portions of the address space setup for DMA transfers. This copy could be avoided by exposing communication buffers to the application.
Reference: [15] <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: </author> <title> A RISC Machine Sort. </title> <booktitle> In 1994 ACM SIGMOD Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: The first is a simple scan, which reads data from disk sequentially, selects matching records, and writes those records to disk. The second benchmark is an external sort. Sorting is a longtime database-industry standard benchmark, and has recently garnered much interest <ref> [1, 4, 15] </ref>. In addition to being useful in a classical database environment, sorting is also typical of the workload placed on systems performing decision support [9]. In this paper, we use the single-node and cluster versions of NOW-Sort, currently the world's fastest disk-to-disk sorting program [4]. <p> We do not use mmap here because it is not a natural match for writing (it can not extend the length of files). Finally, all benchmarks have the capability to access multiple disks concurrently. We use a simple user-level striping library, similar to that described in <ref> [15] </ref>. This library spreads disk blocks across the disk sub-system with a user-specified block size (64 KB) and with minimal CPU overhead. Each benchmark has been hand optimized for the platform in question. Therefore, for each of the three benchmarks, there are three versions of code. <p> We now describe each benchmark. 4.1 Scan A sequential scan, modeled after a scan and selection in a database, is the simplest of the three benchmarks. The input set we use in both the scan and the sort is derived from the Datamation [8] and MinuteSort <ref> [15] </ref> sorting benchmarks: 100 byte records with 10-byte keys. In our terminology, key is the size of the key and rec is the size of a record. <p> It is crucial to group data accesses into second-level cache-sized objects, thereby avoiding the high cost of repeated access to main memory. This style of programming was used extensively in <ref> [15] </ref>, and we plan to investigate the support applications need to achieve better locality during I/O. 6 Conclusions We have presented measurements of the resource costs of data movement on three machine architectures. Across all platforms, we developed models of benchmark resource usage, and validated the models empirically.
Reference: [16] <author> S. E. Perl and R. L. </author> <title> Sites. Studies of Windows NT Performance Using Dynamic Execution Traces. </title> <booktitle> In OSDI 2, </booktitle> <pages> pages 169-184, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Each of our three platforms runs Solaris 2.5.1, a modern, multi-threaded operating system [11]. Though we are presenting a study of architectural characteristics, operating system behavior often dictates usage patterns of the underlying hardware, as shown in <ref> [6, 16, 19] </ref>. Therefore, care must be taken to use the appropriate operating system interfaces. Much of the data movement on the UltraSPARC is performed with special block copy hardware that is available as a part of the VIS instruction set.
Reference: [17] <author> M. Stonebraker. </author> <title> The Case for Shared Nothing. </title> <journal> Database Engineering, </journal> <volume> 9(1), </volume> <year> 1986. </year>
Reference-contexts: Benchmarks read data from disk via memory-mapped files. We use mmap in all benchmarks because the alternative read results in an extra copy to the buffer cache by the operating system, which is problematic for applications that stream through data <ref> [17] </ref>. By using madvise with a sequential access pattern, new pages are prefetched and old pages discarded appropriately. For writing, all benchmarks repeatedly call write with a large (64 KB) buffer, to avoid the high cost of repeated traps into the kernel.
Reference: [18] <author> SunMicrosystems. </author> <title> SBus Specification, Rev. </title> <publisher> B. </publisher> <address> White Paper, </address> <year> 1990. </year>
Reference-contexts: However, as indicated in The S-Bus was targeted to meet the bandwidth requirements of much slower devices, not today's high-speed disks and networks used in tandem. In Sun Microsystems' own words, The S-Bus is optimized for the technologies expected to dominate in the late 1980s and early 1990s <ref> [18] </ref>. It is evident that a new bus is needed to support I/O-intensive applications in a cluster. The 64-bit S-Bus partially solves this, but not without widespread availability of 64-bit cards. SMP S-Bus: As stated above, the SMP S-Bus utilization is identical to the single-node utilization.
Reference: [19] <author> J. Torrellas, A. Gupta, and J. Hennessy. </author> <title> Characterizing the Caching and Synchronization Performance of a Multiprocessor Operating System. </title> <booktitle> In ASPLOS-V, </booktitle> <pages> pages 162-74, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Each of our three platforms runs Solaris 2.5.1, a modern, multi-threaded operating system [11]. Though we are presenting a study of architectural characteristics, operating system behavior often dictates usage patterns of the underlying hardware, as shown in <ref> [6, 16, 19] </ref>. Therefore, care must be taken to use the appropriate operating system interfaces. Much of the data movement on the UltraSPARC is performed with special block copy hardware that is available as a part of the VIS instruction set.
Reference: [20] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 40-53, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: We focus on the Sun Ultra 1 workstation, which forms a basis for comparison with the other two systems. The second platform is a cluster of Ultra 1 workstations, an instance of larger-scale systems comprised of commodity workstations and high-speed networks such as ATM and Myrinet <ref> [3, 5, 10, 20] </ref>. By providing a low-latency, high-bandwidth interconnection, these switches have the potential of fusing workstations into a cohesive whole. Implicitly, we evaluate an underlying assumption of clustered systems: that the workstation in its current form is a good building block [3].
Reference: [21] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Each machine has a single Myrinet card on the S-Bus, which is attached via cable to an eight-port switch; multiple switches can be linked together to form large, arbitrary topologies. Parallel applications in our cluster communicate with Active Messages <ref> [21] </ref>, a high-performance communication layer designed for low latency and high bandwidth switch-based networks. An Active Message is a restricted, lightweight remote procedure call, and is thus an appropriate communication layer for studying the base architectural costs of data movement, because unnecessary copying and buffering of data is avoided.
References-found: 21

