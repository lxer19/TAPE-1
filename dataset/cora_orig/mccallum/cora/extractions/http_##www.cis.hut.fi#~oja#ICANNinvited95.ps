URL: http://www.cis.hut.fi/~oja/ICANNinvited95.ps
Refering-URL: http://www.cis.hut.fi/~oja/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: Erkki.Oja@hut.fi  
Title: PCA, ICA, and Nonlinear Hebbian Learning  
Author: Erkki Oja 
Address: Rakentajanaukio 2C, 02150 Espoo, Finland  
Affiliation: Helsinki University of Technology,  
Abstract: Nonlinear extensions of one-unit and multi-unit Principal Component Analysis (PCA) neural networks, introduced earlier by the author, are reviewed. The networks and their nonlinear Hebbian learning rules are related to other signal expansions like the Projection Pursuit (PP) and the Independent Component Analysis (ICA).
Abstract-found: 1
Intro-found: 1
Reference: <author> Comon, P. </author> <year> (1989), </year> <title> in Proc. of Workshop on Higher-Order Spectral Analysis, </title> <address> Vail, Co., </address> <month> June </month> <year> 1989, </year> <pages> pp. 174 - 179. </pages>
Reference: <author> Comon, P. </author> <year> (1994), </year> <booktitle> Signal Processing 36, </booktitle> <pages> pp. 287 - 314. </pages>
Reference: <author> Friedman, J. and Tukey, J. </author> <year> (1974), </year> <journal> IEEE Trans. Comput. </journal> <volume> C-23, </volume> <pages> pp. 881 - 889. </pages>
Reference: <author> Fyfe, C. </author> <note> et al (1994), </note> <institution> Res. </institution> <type> Report 94/160, </type> <institution> U. of Strathclyde, Dept. of Computer Science. </institution>
Reference-contexts: In the field of neural networks, there has been growing interest in nonlinear extensions of the PCA. The present author generalized the one-unit and multi-unit PCA learning rules to nonlinear versions in (Oja et al, 1991). Recently, some authors have discussed the relation of these rules to Projection Pursuit <ref> (Fyfe et al, 1994, sterberg, 1994) </ref>. Another interesting development is the relation of neural networks to Independent Component Analysis (ICA) (Jutten and Herault, 1991); Karhunen et al (1995) proposed a multilayer network based on linear and nonlinear PCA layers as a neural realization of ICA. <p> However, let us explicitly prevent this by sphering the data first <ref> (see Fyfe et al, 1994) </ref>: by linear preprocessing, the covariance C = Efxx T g can be made equal to the unit matrix. Let us assume that the input vector has been sphered: C = I.
Reference: <author> Haykin, S. </author> <year> (1994): </year> <title> Neural networks: a comprehensive foundation. </title> <publisher> Macmillan College Publ. Co., </publisher> <address> New York, </address> <year> 1994. </year>
Reference: <author> Jutten, C. and Herault, J. </author> <year> (1991), </year> <title> Signal Processing 24, </title> <journal> pp. </journal> <volume> 1 - 10. </volume> <booktitle> Karhunen and Joutsensalo (1994), Neural Networks 7, </booktitle> <pages> pp. 113 - 127. </pages>
Reference-contexts: Recently, some authors have discussed the relation of these rules to Projection Pursuit (Fyfe et al, 1994, sterberg, 1994). Another interesting development is the relation of neural networks to Independent Component Analysis (ICA) <ref> (Jutten and Herault, 1991) </ref>; Karhunen et al (1995) proposed a multilayer network based on linear and nonlinear PCA layers as a neural realization of ICA. These developments will be reviewed here. The starting point are the nonlinear constrained Hebbian rules introduced by the author in Oja et al (1991).
Reference: <author> Karhunen, </author> <note> Wang and Joutsensalo (1995), submitted to ICANN'95, Paris, </note> <month> Oct. </month> <year> 1995. </year>
Reference: <author> Oja, E. </author> <year> (1982), </year> <journal> J. Math. Biol. </journal> <volume> 15, </volume> <pages> pp. 267 - 273. </pages>
Reference-contexts: 1 Introduction A Principal Component Analysis (PCA) network is a one-layer feedforward neural network which is able to extract the principal components of the stream of input vectors. Typically Hebbian type learning rules are used based on the one-unit learning algorithm originally proposed by the author in <ref> (Oja, 1982) </ref>. Many different versions and extensions of this basic algorithm have been proposed during the recent years; for reviews and introductions, see e.g. (Oja, 1992; Haykin, 1994). PCA networks are useful in signal characterization, optimal feature extraction, and data compression.
Reference: <editor> Oja, E. et al (1991), </editor> <booktitle> in Proc. </booktitle> <address> ICANN'91, Espoo, </address> <month> June </month> <year> 1991, </year> <pages> pp. 385 - 390. </pages>
Reference-contexts: In the field of neural networks, there has been growing interest in nonlinear extensions of the PCA. The present author generalized the one-unit and multi-unit PCA learning rules to nonlinear versions in <ref> (Oja et al, 1991) </ref>. Recently, some authors have discussed the relation of these rules to Projection Pursuit (Fyfe et al, 1994, sterberg, 1994). <p> This structure is closely re-lated to both multidimensional PP and ICA. Section 4 is a review of the full ICA network. 2 Nonlinear Hebbian learning 2.1 Learning rule for one neuron Consider first a single artificial neuron receiving an n-dimensional input vector x <ref> (Oja et al, 1991) </ref>. The neuron is trying to adapt its weight vector w so that a function Efe (w T x)g is maximized, where E is the expectation with respect to the (unknown) density of x and e (:) is a continuous objective function. <p> T x) = g (w T x)x: (2) For small values of ff k , eq. (1) can be approximated by w k+1 = w k + ff k (I w k w T k x k ) (3) in which terms proportional to ff 2 k have been dropped <ref> (Oja et al, 1991) </ref>. Depending on the function e (w T x), several cases are covered by this for malism.
Reference: <author> Oja, E. </author> <year> (1992), </year> <booktitle> Neural Networks 5, </booktitle> <pages> pp. 927 - 935. </pages>
Reference: <author> Plumbley, M. </author> <year> (1993), </year> <booktitle> Proc. IEE Conf. </booktitle> <address> ANN'93, Brighton, UK, </address> <month> May </month> <year> 1993, </year> <pages> pp. 86 - 90. </pages>
Reference-contexts: The elements of v have variances equal to 1 and are uncorrelated but in general not independent. The sphering transformation V is implemented by the weights of the linear layer and can be learned in a neural learning algorithm <ref> (Plumbley, 1993) </ref> V k+1 = V k + k (V k x k x T k I)V k : (13) In the second layer, one of the nonlinear Hebbian learning rules (6) or (7) is used, with the sphered v k as input instead of the original x k .
Reference: <author> Sudjianto, A. and Hassoun, M. </author> <year> (1994), </year> <booktitle> in Proc. IEEE ICNN, </booktitle> <address> Orlando, Fla., </address> <month> July </month> <year> 1994, </year> <pages> pp. </pages> <note> 1247 - 1252. </note> <author> sterberg, M. </author> <year> (1994), </year> <title> Dr. </title> <type> Tech. Thesis, </type> <institution> U. of Linkping, Dept. of Electrical Engineering, </institution> <month> Sept. </month> <year> 1994. </year>
References-found: 12

