URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/miikkulainen.subsymbolic-caseroles.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Title: Subsymbolic Case-Role Analysis of Sentences with Embedded Clauses  
Author: Risto Miikkulainen 
Address: Austin  
Affiliation: The University of Texas at  
Abstract: A distributed neural network model called SPEC for processing sentences with recursive relative clauses is described. The model is based on separating the tasks of segmenting the input word sequence into clauses, forming the case-role representations, and keeping track of the recursive embeddings into different modules. The system needs to be trained only with the basic sentence constructs, and it generalizes not only to new instances of familiar relative clause structures, but to novel structures as well. SPEC exhibits plausible memory degradation as the depth of the center embeddings increases, its memory is primed by earlier constituents, and its performance is aided by semantic constraints between the constituents. The ability to process structure is largely due to a central executive network that monitors and controls the execution of the entire system. This way, in contrast to earlier subsymbolic systems, parsing is modeled as a controlled high-level process rather than one based on automatic reflex responses. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, R. B. </author> <year> (1987). </year> <title> Several studies on natural language and back-propagation. </title> <booktitle> In Proceedings of the IEEE First International Conference on Neural Networks (San Diego, CA), </booktitle> <volume> vol. II, </volume> <pages> 335-341. </pages> <address> Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference: <author> Allen, R. B., and Riecken, M. E. </author> <year> (1989). </year> <title> Reference in connectionist language users. </title> <editor> In Pfeifer, R., Schreter, Z., Fogelman Soulie, F., and Steels, L., editors, </editor> <booktitle> Connectionism in Perspective, </booktitle> <pages> 301-308. </pages> <address> New York: </address> <publisher> Elsevier. </publisher>
Reference: <author> Baddeley, A. D. </author> <year> (1986). </year> <title> Working Memory. </title> <publisher> Oxford, </publisher> <address> UK; New York: </address> <publisher> Oxford University Press. 19 Berg, G. </publisher> <year> (1992). </year> <title> A connectionist parser with recursive sentence structure and lexical disambigua-tion. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 32-37. </pages> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Blank, D. S., Meeden, L. A., and Marshall, J. B. </author> <year> (1992). </year> <title> Exploring the symbolic/subsymbolic continuum: A case study of RAAM. </title> <editor> In Dinsmore, J., editor, </editor> <title> The Symbolic and Connectionist Paradigms: Closing the Gap, </title> <address> 113-148. Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Blaubergs, M. S., and Braine, M. D. S. </author> <year> (1974). </year> <title> Short-term memory limitations on decoding self-embedded sentences. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 102 </volume> <pages> 745-748. </pages>
Reference: <author> Blumenthal, A. L., and Boakes, R. </author> <year> (1967). </year> <title> Prompted recall of sentences. Journal of Verbal Learning and Verbal Behavior, </title> <publisher> 6:674676. </publisher>
Reference: <author> Caramazza, A., and Zurif, E. B. </author> <year> (1976). </year> <title> Dissociation of algorithmic and heuristic processes in language comprehension: Evidence from aphasia. </title> <journal> Brain and Language, </journal> <volume> 3 </volume> <pages> 572-582. </pages>
Reference: <author> Chalmers, D. J. </author> <year> (1990). </year> <title> Syntactic transformations on distributed representations. </title> <journal> Connection Science, </journal> <volume> 2 </volume> <pages> 53-62. </pages>
Reference: <author> Chrisman, L. </author> <year> (1992). </year> <title> Learning recursive distributed representations for holistic computation. </title> <journal> Connection Science, </journal> <volume> 3 </volume> <pages> 345-366. </pages>
Reference: <author> Cook, W. A. </author> <year> (1989). </year> <title> Case Grammar Theory. </title> <address> Washington, DC: </address> <publisher> Georgetown University Press. </publisher>
Reference: <author> Cosic, C., and Munro, P. </author> <year> (1988). </year> <title> Learning to represent and understand locative prepositional phrases. </title> <booktitle> In Proceedings of the 10th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 257-262. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Cowan, N. </author> <year> (1988). </year> <title> Evolving conceptions of memory storage, selective attention, and their mutual constraints within the human information-processing system. </title> <journal> Psychological Bulletin, </journal> <volume> 104 </volume> <pages> 163-191. </pages>
Reference: <author> Dolan, C. P. </author> <year> (1989). </year> <title> Tensor Manipulation Networks: Connectionist and Symbolic Approaches to Comprehension, Learning and Planning. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of California, Los Angeles. </institution> <note> Technical Report UCLA-AI-89-06. </note>
Reference: <author> Elman, J. L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211. </pages>
Reference: <author> Elman, J. L. </author> <year> (1991a). </year> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 195-225. </pages>
Reference: <author> Elman, J. L. </author> <year> (1991b). </year> <title> Incremental learning, or The importance of starting small. </title> <booktitle> In Proceedings of the 13th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 443-448. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Fillmore, C. J. </author> <year> (1968). </year> <title> The case for case. In Bach, </title> <editor> E., and Harms, R. T., editors, </editor> <booktitle> Universals in Linguistic Theory, </booktitle> <pages> 0-88. </pages> <address> New York: </address> <publisher> Holt, Rinehart and Winston. </publisher>
Reference: <author> Fodor, J. A., and Pylyshyn, Z. W. </author> <year> (1988). </year> <title> Connectionism and cognitive architecture: A critical analysis. </title> <journal> Cognition, </journal> <volume> 28 </volume> <pages> 3-71. </pages>
Reference: <author> Foss, D. J., and Cairns, H. S. </author> <year> (1970). </year> <title> Some effects of memory limitation upon sentence comprehension and recall. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 9 </volume> <pages> 541-547. </pages> <note> 20 Hadley, </note> <author> R. F. </author> <year> (1992). </year> <title> Compositionality and systematicity in connectionist langauge learning. </title> <booktitle> In Proceedings of the 14th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 659-664. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Huang, M. S. </author> <year> (1983). </year> <title> A developmental study of children's comprehension of embedded sentences with and without semantic constraints. </title> <journal> Journal of Psychology, </journal> <volume> 114 </volume> <pages> 51-56. </pages>
Reference: <author> Jacobs, R. A., Jordan, M. I., and Barto, A. G. </author> <year> (1991). </year> <title> Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks. </title> <journal> Cognitive Science, </journal> <volume> 15 </volume> <pages> 219-250. </pages>
Reference: <author> Jain, A. N. </author> <year> (1991). </year> <title> Parsing complex sentences with structured connectionist networks. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 110-120. </pages>
Reference: <author> Larkin, W., and Burns, D. </author> <year> (1977). </year> <title> Sentence comprehension and memory for embedded structures. </title> <journal> Memory and Cognition, </journal> <volume> 5 </volume> <pages> 17-22. </pages>
Reference: <author> Lee, G., Flowers, M., and Dyer, M. G. </author> <year> (1990). </year> <title> Learning distributed representations of conceptual knowledge and their application to script-based story processing. </title> <journal> Connection Science, </journal> <volume> 2 </volume> <pages> 313-346. </pages>
Reference: <author> Logan, G. D., and Cowan, W. B. </author> <year> (1984). </year> <title> On the ability to inhibit thought and action: A theory of an act of control. </title> <journal> Psychological Review, </journal> <volume> 91 </volume> <pages> 295-327. </pages>
Reference: <author> McClelland, J. L., and Kawamoto, A. H. </author> <year> (1986). </year> <title> Mechanisms of sentence processing: Assigning roles to constituents. </title> <editor> In McClelland, J. L., and Rumelhart, D. E., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 2: Psychological and Biological Models, </booktitle> <pages> 272-325. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: FGREP is a convenient way to form distributed representations for input/output items, but SPEC is not dependent on FGREP. The word representations could have been obtained through semantic feature encoding as well <ref> (as was done by e.g. McClelland and Kawamoto 1986) </ref>. SPEC will even work with random word representations, although some of the advantages of distributed representations (such as generalization, robustness, and context representation) would not be as strong.
Reference: <author> Miikkulainen, R. </author> <year> (1990). </year> <title> A PDP architecture for processing sentences with relative clauses. </title> <editor> In Karlgren, H., editor, </editor> <booktitle> Proceedings of the 13th International Conference on Computational Linguistics, </booktitle> <pages> 201-206. </pages> <address> Helsinki, Finland: Yliopistopaino. </address>
Reference-contexts: Cognitive Science, in press. y Correspondence and requests for reprints should be sent to Risto Miikkulainen, Department of Computer Sciences, The University of Texas at Austin, Austin, TX 787512-1188; risto@cs.utexas.edu. 1 In all examples in this paper, commas are used to indicate clause boundaries for clarity. girl, chased the cat <ref> (Miikkulainen 1990) </ref>. However, such networks cannot parse sentences with novel combinations of relative clauses, such as The girl, who liked the dog, saw the boy, who chased the cat. <p> Several extensions and some completely new architectures that could do that have been proposed. For example, the CLAUSES system <ref> (Miikkulainen 1990) </ref> was an extension of the SRN+FGREP case-role assignment architecture into sentences with multiple clauses. CLAUSES read clause fragments one at a time, brought together the separated constituents, and concatenated the case-role representations into a comprehensive sentence representation in its output layer. <p> This was the approach taken in CLAUSES 2 The representation was limited to three roles for simplicity. More roles could be included if a richer case-role representation was needed. 6 <ref> (Miikkulainen 1990) </ref>. Such representation has two serious limitations: 1. The size of the output layer always poses a hard limit on the number of clauses in the sentence.
Reference: <author> Miikkulainen, R. </author> <year> (1993). </year> <title> Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: In the end, after the networks have learned to produce output close to their targets, they can be connected and they will work well together, even filter out each other's noise <ref> (Miikkulainen 1993) </ref>. Training SPEC is not computationally very intensive with this particular corpus, and therefore, the most convenient training strategy was selected for the experiments reported below. All modules were trained separately and simultaneously on a single machine, sharing the gradually evolving word and hidden-layer representations. <p> The convergence was very strong. After 400 epochs, the average error per output unit was 0.019 for the Parser, 0.008 for the Segmenter (0.002 for the control outputs), and 0.003 for the Stack, while an error level of 0.020 usually results in acceptable performance in similar assembly-based systems <ref> (Miikkulainen 1993) </ref>. The training took approximately three hours on an IBM RS6000 workstation. The final representations, developed by FGREP, reflected the word categories very well. SPEC's performance was then tested on the entire corpus of 98,100 sentences.
Reference: <author> Miikkulainen, R., and Dyer, M. G. </author> <year> (1989). </year> <title> Encoding input/output representations in connectionist cognitive systems. </title> <editor> In Touretzky, D. S., Hinton, G. E., and Sejnowski, T. J., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> 347-356. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Miikkulainen, R., and Dyer, M. G. </author> <year> (1991). </year> <title> Natural language processing with modular neural networks and distributed lexicon. </title> <journal> Cognitive Science, </journal> <volume> 15 </volume> <pages> 343-399. </pages>
Reference: <author> Miller, G. A., and Isard, S. </author> <year> (1964). </year> <title> Free recall of self-embedded English sentences. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 292-303. </pages>
Reference: <author> Munro, P., Cosic, C., and Tabasko, M. </author> <year> (1991). </year> <title> A network for encoding, decoding and translating locative prepositions. </title> <journal> Connection Science, </journal> <volume> 3 </volume> <pages> 225-240. </pages>
Reference: <author> Norman, D. A., and Shallice, T. </author> <year> (1980). </year> <title> Attention to action: Willed and automatic control of behavior. </title> <type> Technical Report 99, </type> <institution> Center for Human Information Processing, University of California, </institution> <address> San Diego. </address> <note> 21 Pollack, </note> <author> J. B. </author> <year> (1987). </year> <title> Cascaded back-propagation on dynamic connectionist networks. </title> <booktitle> In Proceed--ings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 391-404. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Pollack, J. B. </author> <year> (1990). </year> <title> Recursive distributed representations. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 77-105. </pages>
Reference-contexts: The Stack is implemented as a RAAM network <ref> (Pollack 1990) </ref> trained to encode and decode linear lists (figure 3). The input/output of the Stack consists of the Stack's top element and the compressed representation for the rest of the stack.
Reference: <author> Posner, M. I., and Snyder, C. R. </author> <year> (1975). </year> <title> Attention and cognitive control. </title> <editor> In Solso, R. L., editor, </editor> <booktitle> Information Processing and Cognition, </booktitle> <pages> 55-85. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and McClelland, J. L. </author> <year> (1986a). </year> <title> A general framework for parallel distributed processing. </title> <editor> In Rumelhart, D. E., and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> 45-76. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986b). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> 318-362. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The network is trained with examples of input/output sequences, adjusting all forward weights according to the backpropagation algorithm <ref> (Rumelhart et al. 1986b) </ref>. Servan-Schreiber et al. trained an SRN with sample strings from a particular grammar, and it learned to indicate the possible next elements in the sequence.
Reference: <author> Schlesinger, I. M. </author> <year> (1968). </year> <title> Sentence Structure and the Reading Process. The Hague: </title> <publisher> Mouton. </publisher>
Reference: <author> Schneider, W., and Detweiler, M. </author> <year> (1987). </year> <title> A connectionist/control architecture for working memory. </title> <editor> In Bower, G. H., editor, </editor> <booktitle> The Psychology of Learning and Motivation, </booktitle> <volume> vol. 21, </volume> <pages> 53-119. </pages> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Schneider, W., and Shiffrin, R. M. </author> <year> (1977). </year> <title> Controlled and automatic human information processing I: Detection, search, and attention. </title> <journal> Psychological Review, </journal> <volume> 84 </volume> <pages> 1-66. </pages>
Reference: <author> Servan-Schreiber, D., Cleeremans, A., and McClelland, J. L. </author> <year> (1989). </year> <title> Learning sequential structure in simple recurrent networks. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> 643-652. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Servan-Schreiber, D., Cleeremans, A., and McClelland, J. L. </author> <year> (1991). </year> <title> Graded state machines: The representation of temporal contingencies in simple recurrent networks. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 161-194. </pages>
Reference: <author> Shallice, T. </author> <year> (1982). </year> <title> Specific impairments of planning. </title> <journal> Philosophical Transactions of the Royal Society of London B, </journal> <volume> 298 </volume> <pages> 199-209. </pages>
Reference: <author> Shallice, T. </author> <year> (1988). </year> <title> From Neuropsychology to Mental Structure. </title> <address> Cambridge, UK: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Sharkey, N. E., and Sharkey, A. J. C. </author> <year> (1992). </year> <title> A modular design for connectionist parsing. </title> <editor> In Drossaers, M. F. J., and Nijholt, A., editors, </editor> <booktitle> Twente Workshop on Language Technology 3: Connectionism and Natural Language Processing, </booktitle> <pages> 87-96. </pages> <institution> Enschede, the Netherlands: Department of Computer Science, University of Twente. </institution>
Reference: <author> Shiffrin, R. M., and Schneider, W. </author> <year> (1977). </year> <title> Controlled and automatic human information processing II: Perceptual learning, automatic attending, and a general theory. </title> <journal> Psychological Review, </journal> <volume> 84 </volume> <pages> 127-190. </pages>
Reference: <author> Shiffrin, R. M., and Schneider, W. </author> <year> (1984). </year> <title> Automatic and controlled processing revisited. </title> <journal> Psychological Review, </journal> <volume> 91 </volume> <pages> 269-276. </pages> <note> 22 Simmons, </note> <author> R. F., and Yu, Y.-H. </author> <year> (1990). </year> <title> Training a neural network to be a context-sensitive grammar. </title> <booktitle> In Proceedings of the Fifth Rocky Mountain Conference on Artificial Intelligence, </booktitle> <address> Las Cruces, NM, </address> <pages> 251-256. </pages>
Reference: <author> Smolensky, P. </author> <year> (1990). </year> <title> Tensor product variable binding and the representation of symbolic structures in connectionist systems. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 159-216. </pages>
Reference: <author> St. John, M. F. </author> <year> (1992). </year> <title> The story gestalt: A model of knowledge-intensive processes in text comprehension. </title> <journal> Cognitive Science, </journal> <volume> 16 </volume> <pages> 271-306. </pages>
Reference-contexts: Several other observations also indicate that the approach should scale up well. First, as long as SPEC can be trained with the basic constructs, it will generalize to a very large set of new combinations of these constructs. Combinatorial training <ref> (St. John 1992) </ref> of structure is not necessary. In other words, SPEC is capable of dynamic inferencing, previously postulated as very difficult for subsymbolic systems to achieve (Touretzky 1991).
Reference: <author> St. John, M. F., and McClelland, J. L. </author> <year> (1989). </year> <title> Applying contextual constraints in sentence comprehension. </title> <editor> In Touretzky, D. S., Hinton, G. E., and Sejnowski, T. J., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> 338-346. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> St. John, M. F., and McClelland, J. L. </author> <year> (1990). </year> <title> Learning and applying contextual constraints in sentence comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 217-258. </pages>
Reference-contexts: SPEC's generalization capability is based on simplifying the SRN's task through three architectural innovations: (1) training the SRN to generate a sequence of clause case-role representations as its output <ref> (like Stolcke 1990) </ref> instead of a single comprehensive representation, (2) introducing a segmenter network that breaks the input sequence into smaller chunks, and (3) introducing a stack network that memorizes constituents over intervening embedded clauses.
Reference: <author> Stolcke, A. </author> <year> (1990). </year> <title> Learning feature-based semantics with simple recurrent networks. </title> <type> Technical Report TR-90-015, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA. </address>
Reference-contexts: SPEC's generalization capability is based on simplifying the SRN's task through three architectural innovations: (1) training the SRN to generate a sequence of clause case-role representations as its output <ref> (like Stolcke 1990) </ref> instead of a single comprehensive representation, (2) introducing a segmenter network that breaks the input sequence into smaller chunks, and (3) introducing a stack network that memorizes constituents over intervening embedded clauses.
Reference: <author> Stolz, W. S. </author> <year> (1967). </year> <title> A study of the ability to decode grammatically novel sentences. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 6 </volume> <pages> 867-873. </pages>
Reference: <author> Sumida, R. A. </author> <year> (1991). </year> <title> Dynamic inferencing in parallel distributed semantic networks. </title> <booktitle> In Proceedings of the 13th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 913-917. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Touretzky, D. S. </author> <year> (1991). </year> <title> Connectionism and compositional semantics. </title> <editor> In Barnden, J. A., and Pollack, J. B., editors, </editor> <title> High-Level Connectionist Models, </title> <booktitle> vol. 1 of Advances in Connectionist and Neural Computation Theory, </booktitle> <editor> Barnden, J. A., </editor> <booktitle> series editor, </booktitle> <pages> 17-31. </pages> <address> Norwood, NJ: </address> <publisher> Ablex. </publisher>
Reference-contexts: Combinatorial training (St. John 1992) of structure is not necessary. In other words, SPEC is capable of dynamic inferencing, previously postulated as very difficult for subsymbolic systems to achieve <ref> (Touretzky 1991) </ref>. Second, like most subsymbolic systems, SPEC does not need to be trained with a complete set of all combinations of constituents for the basic constructs; a representative sample, like the 200 out of 1088 possible training sentences above, is enough.
Reference: <author> Weckerly, J., and Elman, J. L. </author> <year> (1992). </year> <title> A PDP approach to processing center-embedded sentences. </title> <booktitle> In Proceedings of the 14th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 414-419. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher> <pages> 23 </pages>
References-found: 56

