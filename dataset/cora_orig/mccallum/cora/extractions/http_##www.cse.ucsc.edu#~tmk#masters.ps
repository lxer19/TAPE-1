URL: http://www.cse.ucsc.edu/~tmk/masters.ps
Refering-URL: http://www.cse.ucsc.edu/~tmk/
Root-URL: http://www.cse.ucsc.edu
Title: PREDICTING FILE SYSTEM ACTIONS FROM REFERENCE PATTERNS  approved:  
Author: Thomas M. Kroeger Prof. Darrell D. E. Long, Chair Prof. Glen G. Langdon, Jr. Dr. Richard A. Golding Dean 
Degree: A thesis submitted in partial satisfaction of the requirements for the degree of MASTER OF SCIENCE in COMPUTER ENGINEERING by  
Note: The thesis of Thomas M. Kroeger is  
Date: December 1996  
Affiliation: UNIVERSITY of CALIFORNIA SANTA CRUZ  of Graduate Studies  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Abdulla, G., Abrams, M., and Fox, E. A. </author> <title> Scaling the WWW. Tech. </title> <type> Rep. </type> <institution> TR-96-06, Virginia Polytechnic Institute and State University, Department of Computer Science, </institution> <month> March </month> <year> 1995. </year>
Reference: [2] <author> Almeida, V., Bestavros, A., Crovella, M., and de Oliveira, A. </author> <title> Characterizing Reference Locality in the WWW. In IEEE PDIS'96: </title> <booktitle> The International Conference in Parallel and Distributed Information Systems (Miami Beach, </booktitle> <address> Florida, </address> <month> December </month> <year> 1996), </year> <journal> IEEE Computer Society. </journal>
Reference: [3] <author> Baker, M. G., Hartman, J. H., Kupfer, M. D., Shirriff, K. W., and Ousterhout, J. K. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of Thirteenth Symposium on Operating Systems Principles (August 1991), Association for Computing Machinery, </booktitle> <pages> pp. 198-212. </pages>
Reference-contexts: Using file open events from the Sprite file system traces <ref> [3] </ref> to generate a workload, we measured the hit ratios of both caches and the number of files prefetched by a predictive cache, across variations in cache size, prefetch threshold and model order. <p> To simulate the workload of a file system we used file open events from the Sprite file system traces <ref> [3] </ref>. These traces represent the workload of a distributed file system in an academic research environment. We split these traces into eight 24-hour periods called A through H, to provide eight separate reference streams.
Reference: [4] <author> Bell, T. C., Cleary, J. G., and Witten, I. H. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1990. </year>
Reference-contexts: Our model tracks previous file references patterns through a finite multi-order context modeling technique adapted from the data compression technique Prediction by Partial Match (PPM) <ref> [4] </ref>. We modified this model to efficiently handle a large number of distinct files and adapt to changing reference patterns. Our selector examines the most recently seen patterns and the counts of the events that have followed them to determine which events are likely. <p> The selector uses these distributions to select which files to prefetch. Figure 2.1 illustrates how these components integrate into a I/O cache. Our model tracks observed file reference patterns through a finite multi-order context modeling technique adapted from Prediction by Partial Match (PPM) <ref> [4] </ref>. This model uses a trie [15] to store file reference patterns and the number of times they have occurred. <p> In fact we could describe the next character as occurring under any of the following contexts, "t," "ct" "ect," "ject," "bject" and "object." Techniques that track multiple contexts of varying orders are termed Multi-Order Context Models <ref> [4] </ref>. To prevent the model from quickly growing beyond available resources, most implementations of a multi-order context model limit the highest order tracked to 8 some finite number m; hence the term Finite Multi-Order Context Model. <p> Once we have updated each context in our set of current contexts, we have a 10 new state that describes our file system. Figure 2.2 extends an example from Bell <ref> [4] </ref> to illustrate how this trie would develop when given the sequence of events CACBCAABCA. The first three tries show how our model builds from the initial (empty) state. The last trie shows how our model would look after the entire sequence.
Reference: [5] <author> Bestavros, A., and Cunha, C. </author> <title> A prefetching protocol using client speculation for the WWW. Tech. </title> <type> Rep. </type> <institution> TR-95-011, Boston University, CS Dept, </institution> <address> Boston, MA 02215, </address> <month> April </month> <year> 1995. </year> <month> 43 </month>
Reference-contexts: Padmanabhan and Mogul [23] have pursued using the model of Griffioen and Appleton to track requests at the server and then provide prefetching hints for the client. They show that reference patterns from a web server also offer useful information to support prefetching. Bestavros et al. <ref> [5] </ref> have presented a model for the speculative dissemination of World Wide Web data. This work again shows that reference patterns from a web server 35 can be used as an effective source of information to drive prefetching.
Reference: [6] <author> Cao, P., Felten, E. W., Karlin, A. R., and Li, K. </author> <title> A study of integrated prefetching and caching strategies. </title> <booktitle> In Proceedings of the 1995 SIGMETRICS (May 1995), Association for Computing Machinery. </booktitle>
Reference-contexts: Introduction While the past few years have brought significant gains in processor technology, permanent storage devices have at best seen moderate increases in speed. The resulting bottleneck from I/O system latency has inspired several researchers to re-evaluate how caching is done <ref> [9, 20, 24, 6, 28, 13, 16] </ref>. The purpose of a cache is to reduce latency by using limited high speed storage to buffer data between a workload and a data resource (Fig. 1.1). The workload provides two streams: I/O system requests and write data. <p> Writing High Speed Cache Buffers Policies Prefetching Replacement Data Resource Requests Write Data Read Data Write Data Read Data Request-Results Requests Request-Results Cache Manager (Disk etc...) WORKLOAD In an effort to improve I/O performance several researchers have taken the approach of adapting the workload to provide information about future requests <ref> [13, 24, 6] </ref>. With this informed model they present several methods for managing cache replacement in conjunction with prefetching [13]. Since write behind policies can mask update latency, their efforts focus on evaluating prefetching and replacement together. <p> Patterson et al. [24] present an informed prefetching model that applies cost-benefit analysis to allocate resources. Cao et al. <ref> [6] </ref> examine caching and prefetching in combination and present four rules for successfully combining the two techniques and evaluates several prefetching algorithms to including an aggressive prefetch algorithm. Kimbrel et al. [13] present an algorithm that has the advantages of both informed prefetching and aggressive prefetch while avoiding their limitations.
Reference: [7] <author> Cleary, J. G., Teahan, W. J., and Witten, I. H. </author> <title> Unbounded length contexts for PPM. In Data Compression Conference (March 1995), </title> <publisher> IEEE Computer Society, IEEE Computer Society Press. </publisher>
Reference-contexts: Moffat et al. [21] addressed the model size by periodically constricting a new model with the last 2048 events and then using it to replace the current trie. More recently, Cleary et al. <ref> [7] </ref> have presented methods for extending PPM to support unbounded context lengths. In test cases these contexts rarely extended beyond a length of 10 characters. While this model improved accuracy and saw a 6% increase in compression, it also significantly increased computational complexity and model size.
Reference: [8] <author> Curewitz, K. M., Krishnan, P., and Vitter., J. S. </author> <title> Practical prefetching via data compression. </title> <booktitle> SIGMOD Record 22, </booktitle> <month> 2 (June </month> <year> 1993), </year> <pages> 257-266. </pages>
Reference-contexts: They prove that for a Markov source such techniques converge to an optimal on-line algorithm, and go on to test this work for memory access patterns <ref> [8] </ref> in an object-oriented database and a CAD System. Model size is dealt with by paging portions of the model to secondary mem 33 ory. They also suggest that such methods could have great success within a variety of other applications such as hyper-text.
Reference: [9] <author> Griffioen, J., and Appleton, R. </author> <title> Performance measurements of automatic prefetch-ing. </title> <booktitle> In Parallel and Distributed Computing Systems (September 1995), IEEE, </booktitle> <pages> pp. 165-170. </pages>
Reference-contexts: Introduction While the past few years have brought significant gains in processor technology, permanent storage devices have at best seen moderate increases in speed. The resulting bottleneck from I/O system latency has inspired several researchers to re-evaluate how caching is done <ref> [9, 20, 24, 6, 28, 13, 16] </ref>. The purpose of a cache is to reduce latency by using limited high speed storage to buffer data between a workload and a data resource (Fig. 1.1). The workload provides two streams: I/O system requests and write data. <p> Since write behind policies can mask update latency, their efforts focus on evaluating prefetching and replacement together. With the same goal of reducing I/O latency several other researchers have pursued methods to determine future requests based on previous workload request patterns <ref> [9, 20, 28, 16] </ref>. While these predictive methods vary in their approach, they are all based on the same idea: that previous action patterns provide indications for future references. Informed and predictive caching techniques differ in the following ways. <p> In all cases the parameters appeared to be stable within reasonable limits. 3.3.1 Prefetch Threshold We found a prefetch threshold in the region of 0.05 to 0.1 offers the best hit ratios. of 0.001 to 0.25. From Griffioen and Appleton's work <ref> [9] </ref>, we expected this setting to be quite low. Even so, it is surprising that such an aggressive prefetch threshold produced the best results. <p> Finally, we have adapted PPM in a different manner, without the use of vine pointers. Within the domain of file systems, Griffioen and Appleton <ref> [9] </ref> have developed a predictive model that for each file accumulates frequency counts of the next n files (where n is a parameter of the algorithm). These frequency counts are then used to drive a prefetching cache.
Reference: [10] <author> Gwertzman, J., and Seltzer, M. </author> <title> The Case for Geographical Push Caching. </title> <booktitle> In Fifth Annual Workshop on Hot Operating Systems (Orcas Island, </booktitle> <address> WA, </address> <month> May </month> <year> 1995), </year> <journal> IEEE Computer Society, IEEE, </journal> <pages> pp. 51-55. </pages>
Reference: [11] <author> Gwertzman, J., and Seltzer, M. </author> <title> World wide web cache consistency. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference (San Diego, </booktitle> <address> CA, </address> <month> Jan </month> <year> 1996), </year> <booktitle> USENIX, USENIX, </booktitle> <pages> pp. 141-152. </pages>
Reference: [12] <author> Howard, J. H., Kazar, M. L., Menees, S. G., Nichols, D. A., Satyanarayanan, M., Sidebotham, R. N., and West, M. J. </author> <title> Scale and performance in a distributed file system. </title> <journal> Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988), </year> <pages> 51-81. 44 </pages>
Reference-contexts: Secondly, whole file caching has been used effectively in several distributed file systems <ref> [12, 14, 26] </ref>. Finally, in a mobile environment the possibility of temporary disconnection and the availability of local storage make whole file caching desirable. The cache itself was divided into one kilobyte blocks. We augmented this LRU simulation to model reference patterns and prefetch data as described in Chapter 2.
Reference: [13] <author> Kimbrel, T., Tomkins, A., Patterson, R. H., Bershad, B., Cao, P., Felton, E. W., Gibson, G. A., Karlin, A., and Li, K. </author> <title> A trace-driven comparison of algorithm for parallel prefetching and caching. </title> <booktitle> In Proceedings of Second USENIX Symposium on Operating Systems Design and Implementation (October 1996), USENIX. </booktitle>
Reference-contexts: Introduction While the past few years have brought significant gains in processor technology, permanent storage devices have at best seen moderate increases in speed. The resulting bottleneck from I/O system latency has inspired several researchers to re-evaluate how caching is done <ref> [9, 20, 24, 6, 28, 13, 16] </ref>. The purpose of a cache is to reduce latency by using limited high speed storage to buffer data between a workload and a data resource (Fig. 1.1). The workload provides two streams: I/O system requests and write data. <p> Writing High Speed Cache Buffers Policies Prefetching Replacement Data Resource Requests Write Data Read Data Write Data Read Data Request-Results Requests Request-Results Cache Manager (Disk etc...) WORKLOAD In an effort to improve I/O performance several researchers have taken the approach of adapting the workload to provide information about future requests <ref> [13, 24, 6] </ref>. With this informed model they present several methods for managing cache replacement in conjunction with prefetching [13]. Since write behind policies can mask update latency, their efforts focus on evaluating prefetching and replacement together. <p> With this informed model they present several methods for managing cache replacement in conjunction with prefetching <ref> [13] </ref>. Since write behind policies can mask update latency, their efforts focus on evaluating prefetching and replacement together. With the same goal of reducing I/O latency several other researchers have pursued methods to determine future requests based on previous workload request patterns [9, 20, 28, 16]. <p> Cao et al. [6] examine caching and prefetching in combination and present four rules for successfully combining the two techniques and evaluates several prefetching algorithms to including an aggressive prefetch algorithm. Kimbrel et al. <ref> [13] </ref> present an algorithm that has the advantages of both informed prefetching and aggressive prefetch while avoiding their limitations.
Reference: [14] <author> Kistler, J. J., and Satyanarayanan, M. </author> <title> Disconnected operation in the coda file system. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles (October 1991), Association for Computing Machinery, </booktitle> <pages> pp. 213-25. </pages>
Reference-contexts: Secondly, whole file caching has been used effectively in several distributed file systems <ref> [12, 14, 26] </ref>. Finally, in a mobile environment the possibility of temporary disconnection and the availability of local storage make whole file caching desirable. The cache itself was divided into one kilobyte blocks. We augmented this LRU simulation to model reference patterns and prefetch data as described in Chapter 2.
Reference: [15] <author> Knuth, D. E. </author> <title> Sorting and Searching, </title> <booktitle> vol. 3 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: The selector uses these distributions to select which files to prefetch. Figure 2.1 illustrates how these components integrate into a I/O cache. Our model tracks observed file reference patterns through a finite multi-order context modeling technique adapted from Prediction by Partial Match (PPM) [4]. This model uses a trie <ref> [15] </ref> to store file reference patterns and the number of times they have occurred. <p> The nature of a context model, where one set of contexts is built from the previous set, makes it well suited for a trie <ref> [15] </ref>. A trie is a data structure based on a tree that is used to efficiently store sequences of symbols (e.g. storing sequences of letters from the alphabet). Each node in this trie contains a symbol (e.g. a letter from the alphabet).
Reference: [16] <author> Kroeger, T. M., and Long, D. D. E. </author> <title> Predicting file-system actions from prior events. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference (January 1996), USENIX. </booktitle>
Reference-contexts: Introduction While the past few years have brought significant gains in processor technology, permanent storage devices have at best seen moderate increases in speed. The resulting bottleneck from I/O system latency has inspired several researchers to re-evaluate how caching is done <ref> [9, 20, 24, 6, 28, 13, 16] </ref>. The purpose of a cache is to reduce latency by using limited high speed storage to buffer data between a workload and a data resource (Fig. 1.1). The workload provides two streams: I/O system requests and write data. <p> Since write behind policies can mask update latency, their efforts focus on evaluating prefetching and replacement together. With the same goal of reducing I/O latency several other researchers have pursued methods to determine future requests based on previous workload request patterns <ref> [9, 20, 28, 16] </ref>. While these predictive methods vary in their approach, they are all based on the same idea: that previous action patterns provide indications for future references. Informed and predictive caching techniques differ in the following ways. <p> Thus, a file system buffer cache that tracks file reference patterns and notes predictive sequences should be able to exploit the information in such sequences, by determining likely references and prefetching that data before it is requested. We have developed such a predictive prefetching cache <ref> [16] </ref>. This cache is composed of two parts: a model that tracks the file reference patterns and a selector that uses this information to prefetch data that is likely to be needed.
Reference: [17] <author> Kuenning, G. </author> <title> The design of the seer predictive caching system. </title> <booktitle> In Workshop on Mobile Computing Systems and Applications (December 1994), IEEE Computer Society, </booktitle> <pages> pp. 37-43. </pages>
Reference-contexts: Their work has concluded that such a predictive caching system has promise to be effective for a wide variety of environments. Kuenning et al. <ref> [17] </ref> have extended this work, developing the concept of a semantic distance, and using this to determine groupings of files that should be kept on local disks for mobile computers.
Reference: [18] <author> Kuenning, G., Popek, G. J., and Reiher, P. </author> <title> An analysis of trace data for predictive file caching in mobile computing. </title> <booktitle> In Proceedings of USENIX Summer Technical Conference (1994), USENIX, </booktitle> <pages> pp. 291-303. </pages>
Reference-contexts: They develop simulations based on a mass-storage system modified to use TDAG. Using a synthetic user workload they show that such techniques show great promise. Kuenning, Popek, and Reiher <ref> [18] </ref> have done extensive work analyzing the behavior of file system requests for various mobile environments with the intent of developing a prefetching system that would predict needed files and cache them locally.
Reference: [19] <author> Laird, P., and Saul, R. </author> <title> Discrete sequence prediction and its applications. </title> <booktitle> Machine Learning (1994). </booktitle> <pages> 45 </pages>
Reference-contexts: If a match is found then the previous access tree is used to prefetch files. Since a current access tree must be compared with all those previously seen the computational complexity of this model limits its ability to scale to a large file system. Laird and Saul <ref> [19] </ref> propose the use of Transition Directed Acyclic Graphs (TDAG) to learn the patterns in which items are requested from mass-storage devices and enable 34 cache memories to minimize latency. They develop simulations based on a mass-storage system modified to use TDAG.
Reference: [20] <author> Lei, H., and Duchamp, D. </author> <title> An analytical approach to file prefetching. </title> <booktitle> In Proceedings of USENIX 1997 annual Technical Conference (January 1997), USENIX. </booktitle>
Reference-contexts: Introduction While the past few years have brought significant gains in processor technology, permanent storage devices have at best seen moderate increases in speed. The resulting bottleneck from I/O system latency has inspired several researchers to re-evaluate how caching is done <ref> [9, 20, 24, 6, 28, 13, 16] </ref>. The purpose of a cache is to reduce latency by using limited high speed storage to buffer data between a workload and a data resource (Fig. 1.1). The workload provides two streams: I/O system requests and write data. <p> Since write behind policies can mask update latency, their efforts focus on evaluating prefetching and replacement together. With the same goal of reducing I/O latency several other researchers have pursued methods to determine future requests based on previous workload request patterns <ref> [9, 20, 28, 16] </ref>. While these predictive methods vary in their approach, they are all based on the same idea: that previous action patterns provide indications for future references. Informed and predictive caching techniques differ in the following ways. <p> Over longer periods of time this model would increase in size. Other predictive caching methods used virtual memory to page parts of this trie to secondary storage, and have shown that this can be done without losing all performance gains <ref> [28, 20] </ref>. However, efficiently restricting the buildup of model size has not been addressed within the domain of predictive caching. <p> Nevertheless, they first presented the method of prefetch selection based on a probability threshold. Lei and Duchamp <ref> [20] </ref> have pursued modifying a Unix file system to monitor a process's use of the system calls fork, execve, open, chdir and exit. Using this information they build a tree that represents the access patterns of the process.
Reference: [21] <author> Moffat, A. </author> <title> Implementing the PPM data compression scheme. </title> <journal> IEEE Transactions on Communications 38, </journal> <month> 11 (November </month> <year> 1990), </year> <pages> 1917-1921. </pages>
Reference-contexts: However, efficiently restricting the buildup of model size has not been addressed within the domain of predictive caching. Within the data compression community efficiently restricting the growth of the PPM model has been addressed in a limited manner <ref> [21] </ref>. 4.1 Improving Context Modeling with Partitions In order to retain the predictive nature gained from each file we have pursued a model that retains all first order nodes and reduces space requirements by limiting the number of descendants of each first order node. <p> Further, an application-informed method would not be able to make use of 36 relationships that exist across applications (e.g. make ) cc, ld). 5.4 Data Compression Efforts in data compression strive to improve the computational complexity, adaptability and accuracy of content modeling techniques. Moffat et al. <ref> [21] </ref> addressed the model size by periodically constricting a new model with the last 2048 events and then using it to replace the current trie. More recently, Cleary et al. [7] have presented methods for extending PPM to support unbounded context lengths.
Reference: [22] <author> Ousterhout, J. </author> <title> Why aren't operating systems getting faster as fast as hardware? In Proceedings of USENIX Summer Technical Conference (June 1990), </title> <booktitle> USENIX, </booktitle> <pages> pp. 247-56. </pages>
Reference: [23] <author> Padmanabhan, V. N., and Mogul, J. C. </author> <title> Using Predictive Prefetching to Improve World Wide Web Latency. </title> <booktitle> In Proceedings of the 1996 SIGCOMM (July 1996), </booktitle> <institution> Association for Computing Machinery, Association for Computing Machinery. </institution>
Reference-contexts: With this understanding many researchers have looked for ways to improve current caching techniques. Padmanabhan and Mogul <ref> [23] </ref> have pursued using the model of Griffioen and Appleton to track requests at the server and then provide prefetching hints for the client. They show that reference patterns from a web server also offer useful information to support prefetching.
Reference: [24] <author> Patterson, H., Gibson, G., Ginting, E., Stodolsky, D., and Zelenka, J. </author> <title> Transparent informed prefetching. </title> <booktitle> In Proceedings of thirteenth Symposium on Operating Systems Principles (December 1995), Association for Computing Machinery. </booktitle>
Reference-contexts: Introduction While the past few years have brought significant gains in processor technology, permanent storage devices have at best seen moderate increases in speed. The resulting bottleneck from I/O system latency has inspired several researchers to re-evaluate how caching is done <ref> [9, 20, 24, 6, 28, 13, 16] </ref>. The purpose of a cache is to reduce latency by using limited high speed storage to buffer data between a workload and a data resource (Fig. 1.1). The workload provides two streams: I/O system requests and write data. <p> Writing High Speed Cache Buffers Policies Prefetching Replacement Data Resource Requests Write Data Read Data Write Data Read Data Request-Results Requests Request-Results Cache Manager (Disk etc...) WORKLOAD In an effort to improve I/O performance several researchers have taken the approach of adapting the workload to provide information about future requests <ref> [13, 24, 6] </ref>. With this informed model they present several methods for managing cache replacement in conjunction with prefetching [13]. Since write behind policies can mask update latency, their efforts focus on evaluating prefetching and replacement together. <p> Patterson et al. <ref> [24] </ref> present an informed prefetching model that applies cost-benefit analysis to allocate resources. Cao et al. [6] examine caching and prefetching in combination and present four rules for successfully combining the two techniques and evaluates several prefetching algorithms to including an aggressive prefetch algorithm.
Reference: [25] <author> Trivedi, K. </author> <title> Probability & Statistics with Reliability, Queuing, </title> <booktitle> and Computer Science Applications. </booktitle> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1982. </year> <note> [26] van Renesse, </note> <author> R., Tanenbaum, A. S., and Wilschuts, A. </author> <title> The design of a high-performance file server. </title> <booktitle> In Proceedings of the Ninth International Conference on Distributed Computing System (1989), </booktitle> <publisher> IEEE Computer Society Press., </publisher> <pages> pp. 22-27. </pages>
Reference: [27] <author> Vitter, J. S., and Krishnan, P. </author> <title> Optimal prefetching via data compression. </title> <booktitle> In Proceedings 32nd Annual Symposium on Foundations of Computer Science (October 1991), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 121-130. 46 </pages>
Reference-contexts: However, using reference pattern predictions to combine these two techniques and improve cache performance is a relatively new and promising area of research. Vitter, Krishnan and Curewitz <ref> [27, 28] </ref> were the first to examine the use of compression modeling techniques to track reference patterns and prefetch data.
Reference: [28] <author> Vitter, J. S., and Krishnan, P. </author> <title> Optimal prefetching via data compression. </title> <journal> Journal of the ACM 43, </journal> <month> 5 (September </month> <year> 1996), </year> <pages> 771-793. </pages>
Reference-contexts: Introduction While the past few years have brought significant gains in processor technology, permanent storage devices have at best seen moderate increases in speed. The resulting bottleneck from I/O system latency has inspired several researchers to re-evaluate how caching is done <ref> [9, 20, 24, 6, 28, 13, 16] </ref>. The purpose of a cache is to reduce latency by using limited high speed storage to buffer data between a workload and a data resource (Fig. 1.1). The workload provides two streams: I/O system requests and write data. <p> Since write behind policies can mask update latency, their efforts focus on evaluating prefetching and replacement together. With the same goal of reducing I/O latency several other researchers have pursued methods to determine future requests based on previous workload request patterns <ref> [9, 20, 28, 16] </ref>. While these predictive methods vary in their approach, they are all based on the same idea: that previous action patterns provide indications for future references. Informed and predictive caching techniques differ in the following ways. <p> Over longer periods of time this model would increase in size. Other predictive caching methods used virtual memory to page parts of this trie to secondary storage, and have shown that this can be done without losing all performance gains <ref> [28, 20] </ref>. However, efficiently restricting the buildup of model size has not been addressed within the domain of predictive caching. <p> However, using reference pattern predictions to combine these two techniques and improve cache performance is a relatively new and promising area of research. Vitter, Krishnan and Curewitz <ref> [27, 28] </ref> were the first to examine the use of compression modeling techniques to track reference patterns and prefetch data.
Reference: [29] <author> Williams, S., Abrams, M., Standridge, C. R., Abdulla, C., and Fox, E. A. </author> <title> Removal policies in network caches for world-wide web documents. </title> <booktitle> In Proceedings of the 1996 SIGCOMM (1996), </booktitle> <institution> SIGCOMM, Association for Computing Machinery. </institution>
References-found: 28

