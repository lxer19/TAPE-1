URL: http://www.cs.twsu.edu/~haynes/icjai95.ps
Refering-URL: http://adept.cs.twsu.edu/~thomas/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: [haynes,sandip]@euler.mcs.utulsa.edu  
Title: Evolving Behavioral Strategies in Predators and Prey  
Author: Thomas Haynes and Sandip Sen 
Address: Tulsa  
Affiliation: Department of Mathematical Computer Sciences The University of  
Abstract: The predator/prey domain is utilized to conduct research in Distributed Artificial Intelligence. Genetic Programming is used to evolve behavioral strategies for the predator agents. To further the utility of the predator strategies, the prey population is allowed to evolve at the same time. The expected competitive learning cycle did not surface. This failing is investigated, and a simple prey algorithm surfaces, which is consistently able to evade capture from the predator algorithms.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Benda, V. Jagannathan, and R. Dodhiawalla. </author> <title> On optimal cooperation of knowledge sources. </title> <type> Technical Report BCS-G2010-28, </type> <institution> Boeing AI Center, Boeing Computer Services, Bellevue, WA, </institution> <month> August </month> <year> 1985. </year>
Reference-contexts: Evaluations of the strategies represented by the structures can be accomplished by allowing the agents to execute the particular strategies in the application domain. We can then measure their efficiency and effectiveness by some criteria relevant to the domain. We have used the predator-prey pursuit game <ref> [1] </ref> to test our hypothesis that useful coordination strategies can be evolved using the STGP paradigm for nontrivial problems. This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. <p> The lesson is that evolution can provide opportunities that are not obvious to human designers. 2 2 The Pursuit Problem The original version of the predator-prey pursuit problem was introduced by Benda, et al. <ref> [1] </ref> and consisted of four blue (predator) agents trying to capture a red (prey) agent by surrounding it from four directions on a grid-world. Agent movements were limited to either a horizontal or a vertical step per time unit. The movement of the prey agent was random.
Reference: 2. <editor> Alan H. Bond and Les Gasser. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Mor-gan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: These two types of strategies are referred to as behavioral strategies in that they guide the actions of agents in a domain. The identification, design, and implementation of strategies for coordination is a central research issue in the field of Distributed Artificial Intelligence (DAI) <ref> [2] </ref>. Current research techniques in developing coordination strategies are mostly off-line mechanisms that use extensive domain knowledge to design from scratch the most appropriate cooperation strategy. It is nearly impossible to identify or even prove the existence of the best coordination strategy.
Reference: 3. <editor> Lawrence Davis, editor. </editor> <booktitle> Handbook of genetic algorithms. </booktitle> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: GAs are not guaranteed to find optimal solutions (unlike Simulated Annealing algorithms), they still possess some nice provable properties (optimal allocation of trials to substrings, evaluating exponential number of schemas with linear number of string evaluations, etc.), and have been found to be useful in a number of practical applications <ref> [3] </ref>. Koza's work on Genetic Programming [10] was motivated by the representational constraint, i.e. fixed length encodings, in traditional GAs.
Reference: 4. <author> Les Gasser, Nicolas Rouquette, Randall W. Hill, and John Lieb. </author> <title> Representing and using organizational knowledge in DAI systems. </title> <editor> In Les Gasser and Michael N. Huhns, editors, </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence, </booktitle> <pages> pages 55-78. </pages> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [4, 9, 12, 15, 16] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies.
Reference: 5. <author> Thomas Haynes, Roger Wainwright, and Sandip Sen. </author> <title> Evolving cooperation strategies. </title> <type> Technical Report UTULSA-MCS-94-10, </type> <institution> The University of Tulsa, </institution> <month> December 16, </month> <year> 1994. </year>
Reference-contexts: It is nearly impossible to identify or even prove the existence of the best coordination strategy. In most cases a coordination strategy is chosen if it is reasonably good. In <ref> [5] </ref>, we presented a new approach to developing coordination strategies for multi-agent problem solving situations. Our approach is different from most of the existing techniques for constructing coordination strategies in two ways: Strategies for coordination are incrementally constructed by repeatedly solv ing problems in the domain, i.e., on-line. <p> We rely on an automated method of strategy formulation and modification, that depends very little on domain details and human expertise, and more on problem solving performance on randomly generated problems in the domain. The approach proposed in <ref> [5] </ref> for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed ? This is a preprint of the paper in Gerhard Wei and Sandip Sen, editors, Adaptation and Learning in Multiagent Systems, Lecture Notes in Artificial Intelligence, Springer Verlag, Berlin, Spring 1996. genetic programming (STGP) <p> In our initial experiments on evolving coordination strategies for predator agents in the predator-prey domain, the STGP paradigm was able to evolve a program which had a better strategy than all but one of four manually derived greedy algorithms <ref> [5, 6] </ref>. In the belief that the static program of the prey was limiting the search for a better program, we decided to explore coevolving cooperation strategies in a predator population and avoidance strategies in a prey population. <p> The predators in the original algorithms take turns moving, and thus have no conflict for cells. The predators in the modified algorithms all follow the rules outlined in Section 2. These algorithms are discussed in detail in <ref> [5] </ref>. Fig. 1. <p> We found the evolved strategies still ignored other predator locations. In order to test the second hypothesis, i.e. that the prey was learning to escape the predators, we decided to conduct some experiments where the prey was pitted against our version of Manhattan distance (MD) algorithm <ref> [5] </ref>. The prey was very successful in evading the predators. We were surprised that the algorithm developed by the prey was simple: pick a random direction and move in a straight line in that direction. <p> Why is the linear prey so effective? 2. Is the encoding of domain responsible for our failure to capture the prey? The linear prey is so effective because it avoids locality of movement. The greedy strategies <ref> [9, 5] </ref> rely on the prey staying in a small neighborhood of cells. Locality allows the predators both to surround the prey and to escape any deadlock situations, such as that in Figure 4 (a).
Reference: 6. <author> Thomas Haynes, Roger Wainwright, Sandip Sen, and Dale Schoenefeld. </author> <title> Strongly typed genetic programming in evolving cooperation strategies. </title> <booktitle> In Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 271-278, </pages> <year> 1995. </year>
Reference-contexts: In our initial experiments on evolving coordination strategies for predator agents in the predator-prey domain, the STGP paradigm was able to evolve a program which had a better strategy than all but one of four manually derived greedy algorithms <ref> [5, 6] </ref>. In the belief that the static program of the prey was limiting the search for a better program, we decided to explore coevolving cooperation strategies in a predator population and avoidance strategies in a prey population. <p> Each test case was run with one of twentysix different random seeds. The averaged results of these matchups are shown in Figures 1 and 2, along with the performance of the STGP program generated in <ref> [6] </ref>. Furthermore, four human derived algorithms are also shown: max norm (MN), Manhattan distance (MD), Korf's original max norm (MNO), and Korf's original Manhattan distance (MDO). The max norm algorithms determine the best move to make based on the diagonal distance between a predator and the prey.
Reference: 7. <author> John H. Holland. </author> <booktitle> Adpatation in Natural and Artificial Systems. </booktitle> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1975. </year> <month> 13 </month>
Reference-contexts: It is our conjecture that the real difficulty is because in his experiments the predators and prey take turns moving. In all of our experiments the prey and predator agents move simultaneously. 3 Genetic Programming Holland's work on adaptive systems <ref> [7] </ref> produced a class of biologically inspired algorithms known as genetic algorithms (GAs) that can manipulate and develop solutions to optimization, learning, and other types of problems.
Reference: 8. <editor> Kenneth E. Kinnear, Jr., editor. </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: The representation language used in GPs are computer programs represented as Lisp S-expressions. GPs have attracted a large number of researchers because of the wide range of applicability of this paradigm, and the easily interpretable form of the solutions that are produced by these algorithms <ref> [8, 11] </ref>. In GP, the user needs to specify all of the functions, variables and constants that can be used as nodes in the S-expression or parse tree. Functions, variables and constants which require no arguments become the leaves of the parse trees and thus are called terminals.
Reference: 9. <author> Richard E. Korf. </author> <title> A simple solution to pursuit games. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 183-194, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [4, 9, 12, 15, 16] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies. <p> No two agents were allowed to occupy the same location. The goal of this problem was to show the effectiveness of nine organizational structures, with varying degrees of agent cooperation and control, on the efficiency with which the predator agents could capture the prey. Korf <ref> [9] </ref> claims in his research that a discretization of the continuous world that allows only horizontal and vertical movements is a poor approximation. He calls this the orthogonal game. Korf developed several greedy solutions to problems where eight predators are allowed to move orthogonally as well as diagonally. <p> Why is the linear prey so effective? 2. Is the encoding of domain responsible for our failure to capture the prey? The linear prey is so effective because it avoids locality of movement. The greedy strategies <ref> [9, 5] </ref> rely on the prey staying in a small neighborhood of cells. Locality allows the predators both to surround the prey and to escape any deadlock situations, such as that in Figure 4 (a). <p> Of particular interest is the fact that a simple linearly moving prey is able to successfully evade capture from all evolved or greedy heuristic predator strategies. In particular, this invalidates Korf's claim <ref> [9] </ref> that a simple greedy solution exists for the predator-prey domain.
Reference: 10. <author> John R. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: independent, and uses the strongly typed ? This is a preprint of the paper in Gerhard Wei and Sandip Sen, editors, Adaptation and Learning in Multiagent Systems, Lecture Notes in Artificial Intelligence, Springer Verlag, Berlin, Spring 1996. genetic programming (STGP) paradigm [13], which is an extension of genetic programming (GP) <ref> [10] </ref>. To use the STGP approach for evolving coordination strategies, it is necessary to find an encoding of strategies depicted as symbolic expressions (S-expressions) and choose an evaluation criterion for a strategy corresponding to an arbitrary S-expression. <p> Koza's work on Genetic Programming <ref> [10] </ref> was motivated by the representational constraint, i.e. fixed length encodings, in traditional GAs.
Reference: 11. <author> John R. Koza. </author> <title> Genetic Programming II, Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The representation language used in GPs are computer programs represented as Lisp S-expressions. GPs have attracted a large number of researchers because of the wide range of applicability of this paradigm, and the easily interpretable form of the solutions that are produced by these algorithms <ref> [8, 11] </ref>. In GP, the user needs to specify all of the functions, variables and constants that can be used as nodes in the S-expression or parse tree. Functions, variables and constants which require no arguments become the leaves of the parse trees and thus are called terminals.
Reference: 12. <author> Ran Levy and Jeffrey S. Rosenschein. </author> <title> A game theoretic approach to the pursuit problem. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 195-213, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [4, 9, 12, 15, 16] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies.
Reference: 13. <author> David J. Montana. </author> <title> Strongly typed genetic programming. </title> <type> Technical Report 7866, </type> <institution> Bolt Beranek and Newman, Inc., </institution> <month> March 25, </month> <year> 1994. </year>
Reference-contexts: developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed ? This is a preprint of the paper in Gerhard Wei and Sandip Sen, editors, Adaptation and Learning in Multiagent Systems, Lecture Notes in Artificial Intelligence, Springer Verlag, Berlin, Spring 1996. genetic programming (STGP) paradigm <ref> [13] </ref>, which is an extension of genetic programming (GP) [10]. To use the STGP approach for evolving coordination strategies, it is necessary to find an encoding of strategies depicted as symbolic expressions (S-expressions) and choose an evaluation criterion for a strategy corresponding to an arbitrary S-expression. <p> The set of all terminals is called the terminal set, and the set of all non-terminals is called the non-terminal set. In traditional GP, all of the terminal and non-terminal set members must be of the same type. Montana <ref> [13] </ref> introduced strongly typed genetic programming, in which the variables, constants, arguments, and returned values can be of any type. The only restriction is that the data type for each element be specified beforehand. A STGP algorithm can be described as follows: 1.
Reference: 14. <author> Craig W. Reynolds. </author> <title> Competition, coevolution and the game of tag. </title> <booktitle> In Artificial Life IV. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: We expect that the populations will see-saw between being better on the average. This has been shown in Reynold's work on coevolution in the game of tag <ref> [14] </ref>. In his work, the two opposing agents, from the same population, take turns being the predator and the prey. Whereas in our work, there are separate populations and the predator population has to manage cooperation between multiple agents.
Reference: 15. <author> Larry M. Stephens and Matthias B. Merx. </author> <title> Agent organization as an effector of DAI system performance. </title> <booktitle> In Working Papers of the 9th International Workshop on Distributed Artificial Intelligence, </booktitle> <month> September </month> <year> 1989. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [4, 9, 12, 15, 16] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies. <p> Therefore the prey's fitness is the maximum allowed fitness minus that attained by the predators. 5 Experimental Results Figures 1 and 2 compare some predator algorithms versus prey algorithms. The initial predator locations were taken from 30 test cases from Stephens <ref> [15] </ref> which are used as a base for comparison to previous work in this domain. Each test case was run with one of twentysix different random seeds.
Reference: 16. <author> Larry M. Stephens and Matthias B. Merx. </author> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Proceedings of the 1990 Distributed AI Workshop, </booktitle> <month> October </month> <year> 1990. </year> <title> This article was processed using the L A T E X macro package with LLNCS style 14 </title>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [4, 9, 12, 15, 16] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies.
References-found: 16

