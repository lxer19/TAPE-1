URL: http://chagall.gmu.edu/publication/ECJ97baldwin.ps
Refering-URL: http://chagall.gmu.edu/FORENSIC/jeff.html
Root-URL: 
Email: jbala@dsri.com  -kdejong, rhuang, hvafaie, wechsler-@cs.gmu.edu  
Title: Evolution, Learning, and Instinct: 100 Years of the Baldwin Effect Using Learning to Facilitate the
Author: J. Bala K. De Jong, J. Huang, H. Vafaie, and H. Wechsler 
Keyword: Key Words: Evolutionary computation, Baldwin effect, decision trees, feature selection, hybrid systems, induction, pattern recognition.  
Address: 8260 Greensboro Drive Suite 255 McLean, VA 22102  Fairfax, VA 22030  
Affiliation: Datamat Systems Research, Inc  Department of Computer Science School of Information Technology and Engineering George Mason University  
Note: To Appear in the Special Issue of Evoluationary Computation  
Abstract: This paper describes a hybrid methodology that integrates genetic algorithms and decision tree learning in order to evolve useful subsets of discriminatory features for recognizing complex visual concepts. A genetic algorithm (GA) is used to search the space of all possible subsets of a large set of candidate discrimination features. Candidate feature subsets are evaluated by using C4.5, a decision-tree learning algorithm, to produce a decision tree based on the given features using a limited amount of training data. The classification performance of the resulting decision tree on unseen testing data is used as the fitness of the underlying feature subset. Experimental results are presented to show how increasing the amount of learning significantly improves feature set evolution for difficult visual recognition problems involving satellite and facial image data. In addition, we also report on the extent to which other more subtle aspects of the Baldwin effect are exhibited by the system. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bala, J., P. Pachowicz, and K. De Jong. </author> <year> (1994). </year> <title> Multistrategy Learning from Engineering Data by Integrating Inductive Generalization and Genetic Algorithms, in R.S. </title> <editor> Michalski and G. Tecuci (Eds.), </editor> <title> Machine Learning: A Multistrategy Approach, </title> <booktitle> Vol. IV, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA., </address> <pages> pp. 121-138, </pages> <year> 1994. </year>
Reference: <author> Bala, J., and H. Wechsler. </author> <year> (1996). </year> <title> Shape Analysis Using Multistrategy Learning, </title> <booktitle> Pattern Recognition , 29 (8), </booktitle> <pages> pp. 1323-1334. </pages>
Reference: <author> Bala, J., K. DeJong, J. Huang, H. Vafaie, and H. Wechsler. </author> <year> (1995). </year> <title> Hybrid Learning Using Genetic Algorithms and Decision Trees for Pattern Classification, </title> <booktitle> 14th Int. Joint Conf. on Artificial Intelligence (IJCAI), </booktitle> <address> Montreal, Canada. </address>
Reference: <author> Bala, J., K. DeJong, J. Huang, H. Vafaie, and H. Wechsler. </author> <year> (1996). </year> <title> Visual Routine for Eye Detection Using a Hybrid Genetic Architecture, </title> <booktitle> 13th Int. Conf. on Pattern Recognition (ICPR), </booktitle> <address> Vienna, Austria. </address>
Reference: <author> Balakrishnan, K. and V. </author> <title> Honavar (1995). Evolutionary design of neural architectures: A preliminary taxonomy and guide to the literature. </title> <booktitle> Artificial Intelligence Research Group, </booktitle> <institution> Department of Computer Science, Iowa State University, </institution> <type> Technical Report CS TR #95-01. </type>
Reference: <editor> Belew, R.K. and M. Mitchell (1996). Editors, </editor> <title> "Adaptive Individuals in Evolving Populations: Models and Algorithms", </title> <address> Massachusetts: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Gruau F., and D. Whitley. </author> <year> (1993). </year> <title> Adding Learning to the Cellular Development of Neural Networks: Evolution and the Baldwin Effect, Evolutionary Computation, </title> <journal> Vol.1, </journal> <volume> No.3, </volume> <pages> pp. 213-234. </pages>
Reference-contexts: Examples of such systems include ensembles of neural networks and decision trees (Gutta and Wechsler, 1995), genetic algorithms and neural networks <ref> (Gruau and Whitley, 1993) </ref>, and genetic algorithms and rule-based systems (Bala et al, 1994; Bala and Wechsler, 1995; Vafaie and De Jong, 1994).
Reference: <author> Grefenstette, J., L. David & D. Cerys. </author> <year> (1991). </year> <title> Genesis and OOGA: Two Genetic Algorithms System, TSP: </title> <address> Melorse, MA. </address>
Reference-contexts: Experimental Methodology In order to test our ideas the hybrid architecture described in the previous section has been implemented using standard, readily available software packages with default parameter settings. For the GA module we used GENESIS 4.5 <ref> (Grefenstette, 1991) </ref> with a constant population size of 50, a crossover rate 0.6 and a mutation rate 0.001.
Reference: <author> Gutta, S., and H. Wechsler. </author> <year> (1996). </year> <title> Face Recognition Using Hybrid Classifiers, Pattern Recognition (to appear). </title>
Reference: <author> Gutta, S., J. Huang, I. Shah, D. Singh, B. Takacs, and H. Wechsler. </author> <year> (1995). </year> <title> Benchmark Studies on Face Recognition Int. Workshop on Automatic Face and Gesture Recognition, </title> <address> Zurich, Switzerland. </address>
Reference-contexts: Examples of such systems include ensembles of neural networks and decision trees <ref> (Gutta and Wechsler, 1995) </ref>, genetic algorithms and neural networks (Gruau and Whitley, 1993), and genetic algorithms and rule-based systems (Bala et al, 1994; Bala and Wechsler, 1995; Vafaie and De Jong, 1994).
Reference: <author> Hinton, G. E., and S. J. Nolan. </author> <title> (1987) How Learning Can Guide Evolution, </title> <journal> Complex Systems, </journal> <volume> Vol.1, </volume> <pages> pp. 495-502. </pages>
Reference: <author> Johnson, M. P., P. Maes, and T. Darell. </author> <year> (1994). </year> <title> Evolving Visual Routines, </title> <editor> in R.A. Brooks and P. Maes (Eds.), </editor> <booktitle> Artificial Life IV, </booktitle> <publisher> MIT Press. 19 Kohavi, </publisher> <editor> R., and G. John. </editor> <year> (1995), </year> <title> Wrappers for Feature Subset selection. </title> <type> Technical Report, </type> <institution> Computer Science Department, Stanford University. </institution>
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-Organization in a Perceptual Network, </title> <journal> Computer, </journal> <volume> Vol. 21, No. 3, </volume> <pages> pp. 105-117. </pages>
Reference: <author> Mhlenbein, H., and J. Kinderman. </author> <year> (1989). </year> <title> The dynamics of Evolution and Learning. Toward Genetic Neural Networks, </title> <editor> in R. Pfeifer, Z. Schreter, F. Fogelman-Soulie, and L. Steels (Eds.), </editor> <booktitle> Connectionism in Perspective, </booktitle> <publisher> Elsevier Science, </publisher> <pages> pp. 173-197. </pages>
Reference-contexts: At one level evolution (genotype learning) can be viewed as a coarsely grained global search strategy that only has to get close to the goal. Learning at the phenotype level can then be viewed as a more finely grained, local search strategy for tuning behavior <ref> (Mhlenbein and Kinderman, 1989) </ref>. Although Darwinian theory does not allow for the inheritance of acquired characteristics (Lamarckian evolution), learning (acquired behaviors) can still influence the course of evolution in subtle and interesting ways which biologists refer to as the Baldwin effect.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> The Effect of Noise on Concept Learning, in R.S. </title> <editor> Michalski, J.G. Carbonell and T.M. Mitchell (Eds.), </editor> <title> Machine Learning: an Artificial Intelligence Approach, M o rg a n Kaufmann, </title> <address> San Mateo, CA, </address> <pages> pp. 149-166. </pages>
Reference-contexts: We use a genetic algorithm (GA) to search the space of all possible subsets of a user-provided set of candidate features. An important component of the fitness of a feature set is determined by using C4.5, a tree induction algorithm <ref> (Quinlan, 1986) </ref>, to construct a classifier in the form of decision tree from the given features and a set of sample data, and then testing the performance of the classifier on previously unseen data.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: For the tree induction module we used C4.5 <ref> (Quinlan, 1993) </ref> with the pruning confidence level set to default 25%. These choices remained constant across all experiments; no attempt was made to tune the software to particular problems. Our testing methodology operated as follows.
Reference: <author> Terano, T., and T. Ishino, Y. </author> <year> (1995). </year> <title> Data Analysis Using Simulated Breeding and Inductive Learning Methods, </title> <booktitle> IJCAI'95 Workshop on Data Engineering and Inductive Learning, </booktitle> <pages> pp. 60-69, </pages> <address> Aug., 20, Montreal, Canada. </address>
Reference: <author> Turney, P. </author> <year> (1996). </year> <title> How to Shift Bias: Lessons from the Baldwin Effect, </title> <note> Evolutionary Computation (to appear). </note>
Reference-contexts: Rather, genetic changes are affected indirectly via selection due to the enhanced fitness feedback provided by learning. Computationally, the genotype can be viewed as establishing a context or bias in which learning takes place <ref> (Turney, 1996) </ref>. Evolution of the genotype corresponds to a bias shift due to previous learning, resulting in new contexts which facilitate future learning. Hinton and Nowlan (1987) were among the first to consider computational ways in which evolution of strings and the learning of individuals might interact.
Reference: <author> Turney, P. </author> <year> (1995). </year> <title> Costsensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm, </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> pp. 369-409. </pages>
Reference: <author> Vafaie, H., and K. De Jong. </author> <year> (1994). </year> <title> Improving a Rule Induction System Using Genetic Algorithms, in R.S. </title> <editor> Michalski and G. Tecuci (Eds.), </editor> <title> Machine Learning: A Multistrategy Approach, </title> <booktitle> Vol. IV, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA., </address> <pages> pp. 453-469. </pages>
Reference-contexts: The problem of identifying and exploiting nonlinear interactions among features can be addressed via the use of more sophisticated search techniques such as genetic algorithms (GAs) which provide efficient heuristic methods for searching large spaces <ref> (e.g., Vafaie and De Jong, 1994) </ref>. It is still difficult, however, to develop abstract feature space measures of optimality which guarantee optimality in classification performance. In practice this is best achieved by including some form of performance evaluation of the feature subsets while searching for good subsets.
Reference: <author> Whitley, D., V. S. Gordon, and K. Mathias. </author> <year> (1994). </year> <title> Lamarckian Evolution, the Baldwin Effect, and Function Optimization, </title> <editor> in Y. Davidor, H.P. Schwefel, and R. Manner (Eds.), </editor> <booktitle> PPSN III, </booktitle> <publisher> Springer Verlag, </publisher> <pages> pp. 6-15. </pages>
Reference-contexts: Understanding the computational implications of such effects is an active area of research <ref> (Whitley et al, 1994) </ref>. The (computational) Baldwin effect assumes that local search is employed to estimate the fitness of (chromosome) strings, but that the acquired improvements do not change the genetic encoding of the individual.
References-found: 21

