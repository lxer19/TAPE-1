URL: http://www.dc.ufscar.br/~carmo/relatorios/Alemanha.ps
Refering-URL: http://www.dc.ufscar.br/~carmo/publica.htm
Root-URL: 
Email: email -carmo, flavia- @dc.ufscar.br  
Title: A Fuzzified Nested Generalized Exemplar Theory  
Author: Maria do Carmo Nicoletti Flvia Oliveira Santos Universidade Federal de So Carlos Departamento de Computao C.P. - So Carlos SP Brazil 
Abstract: Inductive learning systems are designed to induce hypotheses, or general descriptions of concepts, from instances of these concepts. Among the wide variety of techniques used in inductive learning systems, algorithms derived from nearest neighbour (NN) pattern classification have been receiving attention lately, mainly due to their incremental nature. Nested Generalized Exemplar (NGE) theory is an inductive learning theory which can be viewed as descent from nearest neighbour classification. In NGE theory, the induced concepts take the form of hyperrectangles in a n-dimensional Euclidean space. The axes of the space are defined by the attributes used for describing the examples. This paper proposes a fuzzified version of the original NGE algorithm, which accepts input examples given as feature/fuzzy value pairs, and generalizes them as fuzzy hyperrectangles. It presents and discusses a metric for evaluating the fuzzy distance between examples, and between example and fuzzy hyperrectangles; criteria for establishing the reliability of fuzzy examples, by strengthening the exemplar which makes the right prediction and weakening the exemplar which makes a wrong one and criteria for producing fuzzy generalizations, based on the union of fuzzy sets. Keywords : exemplar-based learning, nested generalized exemplar, nearest neighbour, fuzzy NGE. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Klir, G.J.; Folger, T.; </author> <title> Fuzzy Sets, </title> <booktitle> Uncertainty and Information . Prentice-Hall International, </booktitle> <year> 1988. </year>
Reference-contexts: The feature-to-feature distance metric between them will be defined as the measure of possibility <ref> [1] </ref> between the corresponding fuzzy sets associated with each feature which describes E new so duas instncias poss v v max v v for j n jp jp x jp jp j j j j In our example, we would have : 13 a) for Exemplar_1 0 0,4 0,8 0 1
Reference: [2] <author> Klir, G.J.; Yuan,B.; </author> <title> Fuzzy Sets and Fuzzy Logic: </title> <booktitle> Theory and Applications . Prentice-Hall International, </booktitle> <year> 1995. </year>
Reference-contexts: exemplars can be measured and the class of the closest one is assumed; b) "translate" each fuzzy exemplar which defines the concept, into a fuzzy production rule, with n inputs (dimension of the Euclidean space) and one output, and next, use one method of fuzzy inference available (such as Mamdani) <ref> [2] </ref>. It is important to notice that the output of a production rule, by Mandani, should be a fuzzy set.
Reference: [3] <author> Martins, </author> <title> C.A.; The use of Decision Trees for generating Inductive Hypothesis in Machine Learning (in Portuguese), Master Dissertation, </title> <address> ICMSC/USP, </address> <year> 1994, </year> <note> 115 pg. </note>
Reference-contexts: Apparently, the decision-tree based concept description language is not appropriate for an incremental approach. These incremental versions need to store verbatim examples in memory, in order to reuse them when a rearrangement of the current expression of the concept is necessary. An implementation of ID5 can be found in <ref> [3] </ref>.
Reference: [4] <author> Medin, D.; Schaffer, M.; </author> <title> Context Theory of Classification Learning. </title> <journal> Psychological Review 85, </journal> <year> 1978, </year> <pages> pp 207-238. </pages>
Reference-contexts: An implementation of ID5 can be found in [3]. The Nested Generalized Exemplar (NGE) theory [7] is an incremental form of inductive learning from examples, which is based on a model of human learning called exemplar-based learning <ref> [4] </ref> and is a form of descent from nearest neighbour pattern classification. 2 The Nested Generalized Exemplar Theory NGE is a learning paradigm based on class exemplars, where an induced hypothesis has the graphical shape of a set of hyperrectangles in a n-dimensional Euclidean space.
Reference: [5] <author> Michalski, R.; Mozetic, I.; Hong, J.; Lavrac, N.; </author> <title> The multipurpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> Proceedings of AAAI-86, </booktitle> <address> Philadelphia, PA, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1986, </year> <pages> pp 1041-1045. </pages>
Reference-contexts: Among the many systems which employ attribute-based languages, two families of systems, namely TDIDT (Top Down Induction of Decision Trees) and AQ, based on the ID3 [6] and AQ <ref> [5] </ref> algorithms respectively, have been particularly sucessful; they have been used recently by many different real-life problems.
Reference: [6] <author> Quinlan, J.R.; </author> <title> Induction of Decision Trees. </title> <booktitle> Machine Learning 1, </booktitle> <year> 1986, </year> <pages> pp 81-106. </pages>
Reference-contexts: Among the many systems which employ attribute-based languages, two families of systems, namely TDIDT (Top Down Induction of Decision Trees) and AQ, based on the ID3 <ref> [6] </ref> and AQ [5] algorithms respectively, have been particularly sucessful; they have been used recently by many different real-life problems.
Reference: [7] <author> Salzberg, </author> <title> S.L.; A Nearest Hyperrectangle Learning Method. </title> <booktitle> Machine Learning 6 , 1991, </booktitle> <pages> pp 251-276. </pages>
Reference-contexts: These incremental versions need to store verbatim examples in memory, in order to reuse them when a rearrangement of the current expression of the concept is necessary. An implementation of ID5 can be found in [3]. The Nested Generalized Exemplar (NGE) theory <ref> [7] </ref> is an incremental form of inductive learning from examples, which is based on a model of human learning called exemplar-based learning [4] and is a form of descent from nearest neighbour pattern classification. 2 The Nested Generalized Exemplar Theory NGE is a learning paradigm based on class exemplars, where an <p> i i i i low er f f low er i i - - &gt; . . 1 (2) 6 The distance between H and E is equivalent to the length of a line dropped perpendicularly from the point E fi to the nearest surface, edge or corner of H <ref> [7] </ref>. Next, the pseudo-code of the original NGE algorithm found in [10] is described. 1. Build an NGE classifier (input: number s of seeds) : 2.
Reference: [8] <author> Schlimmer, J.C.; Fisher, D.; </author> <title> A case study of Incremental Concept Induction. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence , Morgan Kaufmann Publishers, </booktitle> <year> 1986, </year> <pages> pp 496-501. </pages>
Reference-contexts: A new training instance can potentially bring about a 3 rearrangement of the current expression of the concept, although constraints on the extent of the arrangement may be desirable. The ID4 <ref> [8] </ref> and ID5 [9] incremental versions of ID3 have drawbacks with relation to performance. Apparently, the decision-tree based concept description language is not appropriate for an incremental approach.
Reference: [9] <author> Utgoff, P.E.; ID5: </author> <title> An Incremental ID3. </title> <booktitle> Proceedings of the Fifth National Conference on Machine Learning, </booktitle> <institution> University of Michigan, </institution> <month> June </month> <year> 1988, </year> <pages> pp 107-120. </pages>
Reference-contexts: A new training instance can potentially bring about a 3 rearrangement of the current expression of the concept, although constraints on the extent of the arrangement may be desirable. The ID4 [8] and ID5 <ref> [9] </ref> incremental versions of ID3 have drawbacks with relation to performance. Apparently, the decision-tree based concept description language is not appropriate for an incremental approach.
Reference: [10] <author> Wettschereck, D.; Dietterich, T.G.; </author> <title> An Experimental Comparison of the Nearest Neighbor and Nearest-Hyperrectangle Algorithms. </title> <booktitle> Machine Learning 19, </booktitle> <year> 1995, </year> <pages> pp 5-27. </pages>
Reference-contexts: Next, the pseudo-code of the original NGE algorithm found in <ref> [10] </ref> is described. 1. Build an NGE classifier (input: number s of seeds) : 2. Initialization : /* assume training examples are given in random order */ 3. for each of the first s training examples E s call createHyperrectangle (E s ) 4.
References-found: 10

