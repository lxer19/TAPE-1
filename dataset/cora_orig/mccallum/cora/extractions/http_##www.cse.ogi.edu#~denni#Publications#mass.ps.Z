URL: http://www.cse.ogi.edu/~denni/Publications/mass.ps.Z
Refering-URL: http://www.cse.ogi.edu/~denni/publications.html
Root-URL: http://www.cse.ogi.edu
Title: LU TP 91-25 Mass Reconstruction with a Neural Network  
Author: L. Lonnblad C. Peterson and T. Rognvaldsson 
Note: Physics Letters B  1 lonnblad@desyvax (bitnet) lonnblad@apollo3.desy.de (internet) 2 thepcap@seldc52 (bitnet) carsten@thep.lu.se (internet) 3 thepdr@seldc52 (bitnet) denni@thep.lu.se (internet)  
Address: Notkestrae 85, 2 Hamburg 52, Germany  Solvegatan 14A, S-22362 Lund, Sweden  
Affiliation: Deutsches Elektronen-Synchrotron DESY,  Department of Theoretical Physics, University of Lund  
Date: October 1991  278, 181 (1992)  
Abstract: A feed-forward neural network method is developed for reconstructing the invariant mass of hadronic jets appearing in a calorimeter. The approach is illustrated in W ! q q, where W -bosons are produced in pp reactions at SPS collider energies. The neural network method yields results that are superior to conventional methods. This neural network application differs from the classification ones in the sense that an analog number (the mass) is computed by the network, rather than a binary decision being made. As a by-product our application clearly demonstrates the need for using "intelligent" variables in instances when the amount of training instances is limited. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Finding Gluon Jets with a Neural trigger", </title> <journal> Physical Review Letters 65, </journal> <month> 1321 </month> <year> (1990). </year>
Reference-contexts: Introduction Artificial Neural Networks (ANN) have shown great promise for pattern recognition problems in particle physics. In particular, feed-forward classifiers have been used in off-line jet data analysis for quark/gluon separation and b-quark identification with remarkable success <ref> [1, 2, 3, 4, 5] </ref>. The principle is straightforward; a set of feature (classifier) units are parameterized as functions of kinematical data in a non-linear way by means of so called sigmoidal functions (tanh). A training set is used to fit the parameters (weights). <p> 24 input nodes according to coding strategy D above. * Hidden architecture: Three hidden layers are used with 40 nodes in the first, 24 in the second and 10 in the third hidden layer. * Output: One linear output node is trained to give 0:5 (log M W 3:5) 2 <ref> [0; 1] </ref> for M W 2 [40; 160] GeV. * Initialization: All weights are initialized randomly in [-0.1,0.1]. * Temperature: The temperature for all nodes is set to 1:0. 5 W;Z ) using the neural network method (full line) and the conventional "window" method (dashed line) with R = 0.8. *
Reference: [2] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Using Neural Networks to Identify Jets", </title> <journal> Nuclear Physics B 349, </journal> <month> 675 </month> <year> (1991). </year>
Reference-contexts: Introduction Artificial Neural Networks (ANN) have shown great promise for pattern recognition problems in particle physics. In particular, feed-forward classifiers have been used in off-line jet data analysis for quark/gluon separation and b-quark identification with remarkable success <ref> [1, 2, 3, 4, 5] </ref>. The principle is straightforward; a set of feature (classifier) units are parameterized as functions of kinematical data in a non-linear way by means of so called sigmoidal functions (tanh). A training set is used to fit the parameters (weights).
Reference: [3] <author> P. Bhat, L. Lonnblad, K. Meier and K. Sugano, </author> <title> "Using Neural Networks to Identify Jets in Hadron-Hadron Collisions", LU TP 90-13 (to appear in Proc. </title> <booktitle> of the 1990 DPF Summer Study on High Energy Physics Research Directions for the Decade, </booktitle> <address> Colorado), </address> <year> (1990). </year>
Reference-contexts: Introduction Artificial Neural Networks (ANN) have shown great promise for pattern recognition problems in particle physics. In particular, feed-forward classifiers have been used in off-line jet data analysis for quark/gluon separation and b-quark identification with remarkable success <ref> [1, 2, 3, 4, 5] </ref>. The principle is straightforward; a set of feature (classifier) units are parameterized as functions of kinematical data in a non-linear way by means of so called sigmoidal functions (tanh). A training set is used to fit the parameters (weights). <p> Each field is connected to 3 nodes in the first hidden layer, and the second hidden layer contains O (10 10 2 ) nodes. The number of effective weights is thus decreased to O (10 4 ). C: Tower representation <ref> [3] </ref>. Take the E ? of the leading cell in the 20fi24 matrix and assign it to the first node x 1 . Assign the and coordinates to x 2 and x 3 respectively.
Reference: [4] <author> L. Lonnblad, C. Peterson, H. Pi and T. Rognvaldsson, </author> <title> "Self-organizing Networks for Extracting Jet Features", </title> <journal> Computer Physics Communications 67, </journal> <month> 193 </month> <year> (1991). </year>
Reference-contexts: Introduction Artificial Neural Networks (ANN) have shown great promise for pattern recognition problems in particle physics. In particular, feed-forward classifiers have been used in off-line jet data analysis for quark/gluon separation and b-quark identification with remarkable success <ref> [1, 2, 3, 4, 5] </ref>. The principle is straightforward; a set of feature (classifier) units are parameterized as functions of kinematical data in a non-linear way by means of so called sigmoidal functions (tanh). A training set is used to fit the parameters (weights).
Reference: [5] <author> I. Scabai, F. Czako and Z. Fodor, </author> <title> "Quark and Gluon Jet Separation using Neural Networks", </title> <note> ITP Budapest Report 477 (1990). </note>
Reference-contexts: Introduction Artificial Neural Networks (ANN) have shown great promise for pattern recognition problems in particle physics. In particular, feed-forward classifiers have been used in off-line jet data analysis for quark/gluon separation and b-quark identification with remarkable success <ref> [1, 2, 3, 4, 5] </ref>. The principle is straightforward; a set of feature (classifier) units are parameterized as functions of kinematical data in a non-linear way by means of so called sigmoidal functions (tanh). A training set is used to fit the parameters (weights).
Reference: [6] <author> C. Peterson and T. Rognvaldsson, </author> <title> "An Introduction to Artificial Neural Networks", LU TP 91-23 (lectures given at the 1991 CERN School of Computing, </title> <journal> Ystad, </journal> <note> Sweden, to be published in the proceedings) (1991). </note>
Reference-contexts: A training set is used to fit the parameters (weights). The network then "interpolates" to predict answers when confronted with data it has never seen before (the test set). For a general introduction to this field with emphasis on particle physics applications we refer the reader to e.g. ref. <ref> [6] </ref>. When looking for new particles or resonances one almost always encounters the problem of computing invariant masses out of expected decay products. <p> Each receptive field overlaps its neighboring fields and the total number of such fields is 312. Weights from the same relative position within the receptive fields are linked <ref> [6, 7] </ref> such that they are updated identically, which is an effective way of encoding translational invariances. Each field is connected to 3 nodes in the first hidden layer, and the second hidden layer contains O (10 10 2 ) nodes.
Reference: [7] <editor> D.E. Rumelhart, G.E. Hinton and R.J. Williams, </editor> <booktitle> in Parallel Distributed Processing: Explorations in the Microstructure of Cognition Vol. </booktitle> <volume> 1, </volume> <editor> D.E. Rumelhart and J.L. McClelland (Eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge (1986). </address>
Reference-contexts: This kind of neural network application differs from the commonly used classification ones in the sense that the network learns to compute a real number (the mass) and not a set of binary decision values. A variation of the standard back-propagation learning algorithm <ref> [7] </ref> is used together with an appropriate choice of architecture. <p> Each receptive field overlaps its neighboring fields and the total number of such fields is 312. Weights from the same relative position within the receptive fields are linked <ref> [6, 7] </ref> such that they are updated identically, which is an effective way of encoding translational invariances. Each field is connected to 3 nodes in the first hidden layer, and the second hidden layer contains O (10 10 2 ) nodes.
Reference: [8] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Pattern Recognition in High Energy Physics with Artificial Neural Networks - JETNET 2.0", </title> <note> LU TP 91-18 (to appear in Computer Physics Communications) [Program and manual available via Email request] (1991). </note>
Reference-contexts: The JETNET 2.0 package <ref> [8] </ref> is used for all the simulations. The performance of the network is monitored using the test set of 10 4 W -events with a flat mass distribution, measuring the width of the distribution in reconstructed mass divided by true mass.
Reference: [9] <author> H.-U. Bengtsson and T. Sjostrand, </author> <title> "The Lund Monte Carlo for Hadronic Processes: PYTHIA Version 4.8", </title> <journal> Computer Physics Communications 46, </journal> <month> 43 </month> <year> (1987). </year>
Reference-contexts: A variation of the standard back-propagation learning algorithm [7] is used together with an appropriate choice of architecture. The Monte Carlo Data The PYTHIA Monte Carlo <ref> [9] </ref> is used to generate data for the process pp ! W ! q q ! hadrons (1) where the hadrons emerge in a calorimeter detector with variables ; and E ? , strongly resembling the CERN SPS UA2 calorimeter with jj &lt; 2 and a resolution of 20 and 24 <p> The simulation includes the modeling of the underlying event <ref> [9] </ref> with a set of parameters according to the tuning found in reference [11]. The selection of events is 1 made by requiring two jets with E ? &gt; 10 GeV in the jj &lt; 1 region.
Reference: [10] <author> J. Pumplin, </author> <title> "How to Tell Quark Jets form Gluon Jets", </title> <journal> Phys. Rev. </journal> <volume> D44, </volume> <year> 2025 (1991). </year>
Reference-contexts: Furthermore, for each jet three variables are constructed, each of which is known to be sensitive to whether the jet stems from a quark or from a gluon <ref> [10] </ref>. These variables are: n 90 : The minimum number of cells needed to account for 90% of the jets E ? .
Reference: [11] <author> J. Alitti et al., </author> <title> "A Measurement of Two-jet Decays of the W and Z Bosons at the CERN pp Collider", </title> <journal> Zeitschrift fur Physik C 49, </journal> <month> 17 </month> <year> (1991). </year>
Reference-contexts: The simulation includes the modeling of the underlying event [9] with a set of parameters according to the tuning found in reference <ref> [11] </ref>. The selection of events is 1 made by requiring two jets with E ? &gt; 10 GeV in the jj &lt; 1 region. Training Set To prevent the network from learning just one number (M W ), we generate W -bosons with fictive masses for training. <p> In ref. <ref> [11] </ref> it is found that the optimum choice of R is 0:8. When we in the following compare with the "standard"approach, we use the cone algorithm in JETSET [14] called LUCELL. Although this is slightly different from the one used in ref. [11] it can safely be assumed that the choice <p> In ref. <ref> [11] </ref> it is found that the optimum choice of R is 0:8. When we in the following compare with the "standard"approach, we use the cone algorithm in JETSET [14] called LUCELL. Although this is slightly different from the one used in ref. [11] it can safely be assumed that the choice of R = 0:8 as the optimum value still holds.
Reference: [12] <author> E. B. Baum and D. Haussler, </author> <title> "What Size Net Gives Valid Generalization", </title> <booktitle> Neural Computation 1, </booktitle> <month> 151-160 </month> <year> (1989) </year>
Reference-contexts: The reason is that raw information requires many weights (parameters) to process, which with limited training data gives rise to poor generalization performance on the test set <ref> [12] </ref>. In other words one has a situation with too many parameters as compared to training examples (over-fitting). More quantitatively, using raw calorimeter data (A and B) yields generalization results that are comparable to what one obtains with the conventional method described above.
Reference: [13] <author> See e.g. C. Peterson and E. Hartman, </author> <title> "Explorations of the Mean Field Theory Learning Algorithm", </title> <booktitle> Neural Networks 2, 475 (1989). </booktitle> <pages> 8 </pages>
Reference-contexts: In the back-propagation algorithm the weights are changed by gradient descent. An alternative is the so-called Manhattan method <ref> [13] </ref> where the weights are changed by a fixed step size (the learning rate ) that is gradually decreased to 0 during training.
Reference: [14] <author> T. Sjostrand, </author> <title> JETSET 7.3 program and manual see e.g. </title> <editor> B. Bambah et. al., </editor> <title> QCD Generators for LEP, CERN-TH.5466/89 (1989); T. Sjostrand, "The Lund Monte Carlo for Jet Fragmentation and e + e Physics - JETSET version 6.2", </title> <journal> Computer Physics Communications 39, </journal> <note> 347 (1986); T. </note> <author> Sjostrand and M. Bengtsson, </author> <title> "The Lund Monte Carlo for Jet Fragmentation and e + e Physics - JETSET version 6.3 An Update", </title> <journal> Computer Physics Communications 43, </journal> <volume> 367 (1987). </volume> <pages> 9 </pages>
Reference-contexts: D: "Intelligent" variables. The idea here is to use our physics knowledge of what may be useful variables for reconstructing the mass. The procedure we use is to first apply the LUCLUS <ref> [14] </ref> jet finding algorithm with parameters tuned such that there are always at least three jets being found. We then construct the invariant mass of the two largest jets (M 12 ), the three largest jets (M 123 ), and the four largest jets (M 1234 ) 4 . <p> In ref. [11] it is found that the optimum choice of R is 0:8. When we in the following compare with the "standard"approach, we use the cone algorithm in JETSET <ref> [14] </ref> called LUCELL. Although this is slightly different from the one used in ref. [11] it can safely be assumed that the choice of R = 0:8 as the optimum value still holds.
References-found: 14

