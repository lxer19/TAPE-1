URL: http://www.cs.washington.edu/homes/dlee/frontpage/mypapers/paper-cml.ps
Refering-URL: http://www.cs.washington.edu/homes/dlee/frontpage/mypapers/recent_papers.htm
Root-URL: http://www.cs.washington.edu
Email: fbershad,dlee,romerg@cs.washington.edu  bchen@cs.harvard.edu  
Title: Avoiding Conflict Misses Dynamically in Large Direct-Mapped Caches  
Author: Brian N. Bershad Dennis Lee Theodore H. Romer J. Bradley Chen 
Address: Seattle, WA 98195  5000 Forbes Ave. Pittsburgh PA 15213  
Affiliation: Department of Computer Science and Engineering University of Washington  School of Computer Science Carnegie Mellon University  
Abstract: This paper describes a method for improving the performance of a large direct-mapped cache by reducing the number of conflict misses. Our solution consists of two components: an inexpensive hardware device called a Cache Miss Lookaside (CML) buffer that detects conflicts by recording and summarizing a history of cache misses, and a software policy within the operating system's virtual memory system that removes conflicts by dynamically remapping pages whenever large numbers of conflict misses are detected. Using trace-driven simulation of applications and the operating system, we show that a CML buffer enables a large direct-mapped cache to perform nearly as well as a two-way set associative cache of equivalent size and speed, although with lower hardware cost and complexity. 
Abstract-found: 1
Intro-found: 1
Reference: [Agarwal & Pudar 93] <author> Agarwal, A. and Pudar, S. D. </author> <title> Column-Associative Caches: A Technique for Reducing the Miss Rate of Direct Mapped Caches. </title> <booktitle> In Proc. 20th Annual International Symposium On Computer Architecture, </booktitle> <pages> pages 179-190, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The dynamic strategy described in this paper has none of these drawbacks. Conflicts can also be eliminated by other hardware strategies such as cache associativity. More subtle strategies include the victim cache [Jouppi 90], the column-associative cache <ref> [Agarwal & Pudar 93] </ref>, and the assist cache [Kurpanek et al. 94]. These provide a fast path for cache hits and a slightly slower path for some references that would conflict in a direct-mapped cache. <p> 0.31 tomcatv 1.09 0.95 0.72 blocked 0.52 0.52 0.52 unblocked 0.92 0.92 0.92 nasa7.2 0.46 0.46 0.50 nasa7.3 0.79 0.79 0.80 splot 0.99 0.99 0.98 gs 1.09 1.08 1.07 Table 5: Comparing the MCPI of DM, an eight entry victim cache, and CML. 6.2 A column-associative cache A column-associative cache <ref> [Agarwal & Pudar 93] </ref> is a two-way associative cache with fast access to lines that would normally hit in a direct mapped cache.
Reference: [Borg et al. 89] <author> Borg, A., Kessler, R., Lazana, G., and Wall, D. </author> <title> Long Address Traces From RISC Machines: Generation and Analysis. </title> <note> WRL Research Report 89/14, </note> <institution> Digital Equipment Corporation Western Research Laboratory, </institution> <year> 1989. </year>
Reference-contexts: The others are real workloads taken from our computing environment. The traces include references from the applications and the operating system. The X11 workloads also include references from the X11 server. The traces are generated using a tool called epoxie <ref> [Wall 92, Borg et al. 89, Chen 93] </ref> which rewrites MIPS [Kane 88] object files to insert instrumentation instructions before each basic block and memory reference. The instrumented binary emits enough information to reconstruct a complete trace of memory operations made during execution.
Reference: [Bray et al. 90] <author> Bray, B. K., Lynch, W. L., and Flynn, M. J. </author> <title> Page Allocation to Reduce Access Time of Physical Caches. </title> <type> Technical Report CSL-TR-90-454, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: There exist several page coloring strategies that attempt to reduce conflicts by ensuring that contiguous pages in virtual space do not map to conflicting physical pages <ref> [Bray et al. 90, Taylor et al. 90] </ref>.
Reference: [Chambers 93] <author> Chambers, C. </author> <title> The Cecil Language: Specification and Rationale. </title> <type> Technical Report 93-03-05, </type> <institution> University of Washington, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Cecil is a pure object-oriented language being developed at the University of Washington <ref> [Chambers 93] </ref>. 10.1 1679 doduc A Monte-Carlo simulation of the time evolution of a nuclear reactor component described by an 8KB input file (Fortran). 19.4 184 tomcatv A program that generates a vectorized mesh (Fortran). 16.4 56 blocked The "core" of a blocked matrix multiply of two 1024x1024 matrices of eight
Reference: [Chen & Bershad 93] <author> Chen, J. B. and Bershad, B. N. </author> <title> The Impact of Operating System Structure on Memory System Performance. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 120-133, </pages> <month> December </month> <year> 1993. </year>
Reference: [Chen 93] <author> Chen, J. B. </author> <title> Software Methods for System Address Tracing. </title> <booktitle> In Proceedings of the 4th Workshop On Workstation Operating Systems, </booktitle> <pages> pages 178-185. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Octo-ber </month> <year> 1993. </year>
Reference-contexts: The others are real workloads taken from our computing environment. The traces include references from the applications and the operating system. The X11 workloads also include references from the X11 server. The traces are generated using a tool called epoxie <ref> [Wall 92, Borg et al. 89, Chen 93] </ref> which rewrites MIPS [Kane 88] object files to insert instrumentation instructions before each basic block and memory reference. The instrumented binary emits enough information to reconstruct a complete trace of memory operations made during execution.
Reference: [Custer 93] <author> Custer, H. </author> <title> Inside Windows NT. </title> <publisher> Microsoft Press, </publisher> <year> 1993. </year>
Reference-contexts: As previously mentioned, cache conflicts can cause a program's execution time to have high variance, depending on its own activity and the mapping of virtual pages to physical pages. Some operating systems, such as Microsoft's NT <ref> [Custer 93] </ref> and DEC's Ultrix, rely on deterministic page coloring to reduce execution time variability. In essence, a conflict in the last run will remain as a conflict in the next run so this approach can result in low variance at the expense of higher execution times.
Reference: [Dig 91] <institution> Digital Equipment Corporation. Cord, </institution> <year> 1991. </year>
Reference-contexts: Some of these link-time strategies also rely on feedback from previous runs of a program to layout data and instructions for better cache performance <ref> [Pettis & Hansen 90, Dig 91] </ref>. While link-time strategies are effective for some workloads, they are not without problems. Program performance may degrade for inputs other than the input used to structure the program. Link-time strategies do not eliminate inter-domain conflicts in a multiprogrammed environment.
Reference: [Dig 92] <author> Digital Equipment Corporation. </author> <title> DECchip 21064-AA Microprocessor, Hardware Reference Manual, 1992. Order Number: </title> <publisher> EC-N0079-72. </publisher>
Reference-contexts: Each benchmark is run on an otherwise unloaded machine. 4.1 The simulated memory system We use our traces to simulate a memory system modeled on the DEC 3000/500 AXP, which is a workstation with a 150 MHz superscalar Alpha processor <ref> [Dig 92, Dutton et al. 92] </ref>. Our simulated memory system includes a two-level cache, a write-buffer, and a software-managed TLB. The first level cache consists of independent 8 KB instruction and data caches. The second level cache is 512 KB, unified, and inclusive.
Reference: [Dutton et al. 92] <author> Dutton, T., Eiref, D., Kurth, H., Reisert, J., and Stewart, R. </author> <title> The Design of the DEC 3000 AXP Systems, Two High-Performance Workstations. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4) </volume> <pages> 66-81, </pages> <year> 1992. </year> <note> Special Issue. </note>
Reference-contexts: Each benchmark is run on an otherwise unloaded machine. 4.1 The simulated memory system We use our traces to simulate a memory system modeled on the DEC 3000/500 AXP, which is a workstation with a 150 MHz superscalar Alpha processor <ref> [Dig 92, Dutton et al. 92] </ref>. Our simulated memory system includes a two-level cache, a write-buffer, and a software-managed TLB. The first level cache consists of independent 8 KB instruction and data caches. The second level cache is 512 KB, unified, and inclusive.
Reference: [Hill 87] <author> Hill, M. D. </author> <title> Aspects of Cache Memory and In--struction Buffer Performance. </title> <type> PhD dissertation, </type> <institution> University of California at Berkeley, Computer Sciences Division, </institution> <month> November </month> <year> 1987. </year> <note> Number UCB/CSD 87/381. </note>
Reference-contexts: Capacity misses occur when the cache is too small to contain all the active addresses during some period of program execution, and can be avoided only by increasing the size of the cache (or, similarly, by decreasing the size of the working set) <ref> [Hill 87, Smith 82] </ref>. Finally, conflict misses occur when several current active addresses reside in the same cache line.
Reference: [Hill 88] <author> Hill, M. D. </author> <title> A Case for Direct-Mapped Caches. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 25-40, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Consequently, large caches are made from discrete components, where the multiway tag comparison of an associative cache requires additional logic at or near the cache tags, increasing one or all of price, area, and cycle time <ref> [Hill 88, Przybylski et al. 88] </ref>. The cheapest, simplest, and fastest large caches are those that are direct-mapped [Wood 86]. However, direct-mapped caches by themselves exhibit conflict misses because they cannot maintain multiple active lines at the same index. <p> The number above each CML bar shows the number of pages recolored by the CML buffer policy. (in this case, 5 cycles). In practice, this is not likely to be the case <ref> [Hill 88, Przybylski et al. 88, Wood 86] </ref>, and the A2 cache will be somewhat slower than the DM cache. Overall performance then is dictated by the lower A2 cache miss rate coupled with its greater access time.
Reference: [Hosking & Moss 93] <author> Hosking, A. L. and Moss, J. E. B. </author> <title> Protection Traps and Alternatives for Memory Management of an Object Oriented Language. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 106-119, </pages> <month> December </month> <year> 1993. </year>
Reference: [Hwu & Chang 89] <author> Hwu, W.-m. and Chang, P. </author> <title> Achieving High Instruction Cache Performance with an Optimizing Compiler. </title> <booktitle> In Proceedings of the 16th Annual International Symposium On Computer Architecture, </booktitle> <pages> pages 183-191, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Other strategies avoid conflicts by using higher-level information about a program's anticipated reference patterns <ref> [McFarling 89, Hwu & Chang 89] </ref>. These strategies use a program linker that relies on information about the cache memory system, program data structures, and virtual memory system to control the cache mapping.
Reference: [Jouppi 90] <author> Jouppi, N. P. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium On Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Finally, they rely on a fair amount of programmer expertise for their effectiveness. The dynamic strategy described in this paper has none of these drawbacks. Conflicts can also be eliminated by other hardware strategies such as cache associativity. More subtle strategies include the victim cache <ref> [Jouppi 90] </ref>, the column-associative cache [Agarwal & Pudar 93], and the assist cache [Kurpanek et al. 94]. These provide a fast path for cache hits and a slightly slower path for some references that would conflict in a direct-mapped cache. <p> Table 5 compares the L2 MCPI for DM, an eight-entry victim cache that sits between the secondary cache and memory, and CML. We assume that there is a penalty of five processor cycles to access the victim cache on a miss to the secondary cache <ref> [Jouppi 90] </ref>. CML produces a smaller MCPI than the victim cache for gcc-b, cecil, doduc, and tomcatv indicating the presence of conflicts that occur over an extended period of time.
Reference: [Kane 88] <author> Kane, G. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: The traces include references from the applications and the operating system. The X11 workloads also include references from the X11 server. The traces are generated using a tool called epoxie [Wall 92, Borg et al. 89, Chen 93] which rewrites MIPS <ref> [Kane 88] </ref> object files to insert instrumentation instructions before each basic block and memory reference. The instrumented binary emits enough information to reconstruct a complete trace of memory operations made during execution. During tracing, programs and the kernel fill a large trace buffer with accurately interleaved user and kernel references.
Reference: [Kessler & Hill 92] <author> Kessler, R. and Hill, M. D. </author> <title> Page Placement Algorithms for Large Real-Indexed Caches. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(4) </volume> <pages> 338-359, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: page-placement algorithms where the choice of cache page for a given virtual page is determined not only by the original virtual page number, but also by the current state of the virtual memory system, for example the number of pages allocated to a given address space in a given cache-page <ref> [Kessler & Hill 92] </ref>. These schemes are static in that they assign the virtual to physical mapping of the page only at page fault time. <p> Map-color corresponds to deterministic page coloring using 64 colors as described earlier. Map-random assigns a random free physical page to a virtual page at fault time. Finally, map-sequential corresponds to a strategy that sequentially allocates physical page frames in response to a program's page faults <ref> [Kessler & Hill 92] </ref>. This strategy avoids conflicts between pages that are initially accessed close to one another in time. L2 MCPI. Delta-Random is the difference between the L2 MCPI of map-random and map-color, and Delta-Sequential is the difference between the L2 MCPI of map-sequential and map-color.
Reference: [Kurpanek et al. 94] <author> Kurpanek, G., Chan, K., Zheng, J., DeLano, E., and Bryg, W. PA7200: </author> <title> A PA-RISC Processor with Integrated High Performance MP Bus Interface. </title> <booktitle> In Digest of Papers. Spring COMPCON 94, </booktitle> <pages> pages 375-382, </pages> <year> 1994. </year>
Reference-contexts: The dynamic strategy described in this paper has none of these drawbacks. Conflicts can also be eliminated by other hardware strategies such as cache associativity. More subtle strategies include the victim cache [Jouppi 90], the column-associative cache [Agarwal & Pudar 93], and the assist cache <ref> [Kurpanek et al. 94] </ref>. These provide a fast path for cache hits and a slightly slower path for some references that would conflict in a direct-mapped cache. The victim cache and the assist cache tend to work well when there are few conflicts occurring within short intervals of time.
Reference: [McFarling 89] <author> McFarling, S. </author> <title> Program Optimization for Instruction Caches. </title> <booktitle> In Proceedings of the 3rd International Conference On Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Other strategies avoid conflicts by using higher-level information about a program's anticipated reference patterns <ref> [McFarling 89, Hwu & Chang 89] </ref>. These strategies use a program linker that relies on information about the cache memory system, program data structures, and virtual memory system to control the cache mapping.
Reference: [Pettis & Hansen 90] <author> Pettis, K. and Hansen, R. </author> <title> Profile Guided Code Positioning. </title> <booktitle> In Proceedings of the Conference On Programming Language Design and Implementation, </booktitle> <pages> pages 16-26, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Some of these link-time strategies also rely on feedback from previous runs of a program to layout data and instructions for better cache performance <ref> [Pettis & Hansen 90, Dig 91] </ref>. While link-time strategies are effective for some workloads, they are not without problems. Program performance may degrade for inputs other than the input used to structure the program. Link-time strategies do not eliminate inter-domain conflicts in a multiprogrammed environment.
Reference: [Przybylski et al. 88] <author> Przybylski, S. A., Horowitz, M., and Hennessy, J. L. </author> <title> Performance Tradeoffs in Cache Design. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 290-298. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1988. </year>
Reference-contexts: Consequently, large caches are made from discrete components, where the multiway tag comparison of an associative cache requires additional logic at or near the cache tags, increasing one or all of price, area, and cycle time <ref> [Hill 88, Przybylski et al. 88] </ref>. The cheapest, simplest, and fastest large caches are those that are direct-mapped [Wood 86]. However, direct-mapped caches by themselves exhibit conflict misses because they cannot maintain multiple active lines at the same index. <p> The cache size and cycle time are equivalent for all system configurations that we consider in this section. In practice, a two-way set associative cache is unlikely to have the same size and cycle time as a direct-mapped cache made from similarly priced technology <ref> [Przybylski et al. 88] </ref>. We consider this tradeoff in Section 6. 5.1 The effect of the CML buffer on MCPI We evaluate the CML buffer using an interrupt-driven strategy that we call baseline-CML. <p> The number above each CML bar shows the number of pages recolored by the CML buffer policy. (in this case, 5 cycles). In practice, this is not likely to be the case <ref> [Hill 88, Przybylski et al. 88, Wood 86] </ref>, and the A2 cache will be somewhat slower than the DM cache. Overall performance then is dictated by the lower A2 cache miss rate coupled with its greater access time.
Reference: [Smith 82] <author> Smith, A. J. </author> <title> Cache Memories. </title> <journal> ACM Computer Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: Capacity misses occur when the cache is too small to contain all the active addresses during some period of program execution, and can be avoided only by increasing the size of the cache (or, similarly, by decreasing the size of the working set) <ref> [Hill 87, Smith 82] </ref>. Finally, conflict misses occur when several current active addresses reside in the same cache line.
Reference: [Taylor et al. 90] <author> Taylor, G., Davies, P., and Farmwald, M. </author> <title> The TLB Slice A Low-cost High-Speed Address Translation Mechanisms. </title> <booktitle> In Proceedings of the 17th Annual International Symposium On Computer Architecture, </booktitle> <pages> pages 355-363, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: There exist several page coloring strategies that attempt to reduce conflicts by ensuring that contiguous pages in virtual space do not map to conflicting physical pages <ref> [Bray et al. 90, Taylor et al. 90] </ref>.
Reference: [ULT 89] <author> ULTRIX Documentation Group, </author> <title> Digital Equipment Corporation. ULTRIX Documentation Overview for RISC Processors, 1989. Order number AA-NE13A-TE. </title>
Reference-contexts: Finally, the device is flexible, enabling conflicts to be detected under a wide range of memory reference patterns. 4 Trace methodology We use trace-driven simulation of a collection of programs running on DEC's Ultrix 4.2A operating system <ref> [ULT 89] </ref> to evaluate the CML buffer. In this section we describe the programs, our trace collection system, and the simulated memory system. The programs, shown in Table 4, are representative of a range of applications such as compilers, simulators, scientific codes, and graphical (X11) workloads.
Reference: [Wahbe et al. 93] <author> Wahbe, R., Lucco, S., Anderson, T. E., and Graham, S. L. </author> <title> Efficient Software-Based Fault Isolation. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 203-216, </pages> <month> December </month> <year> 1993. </year>
Reference: [Wall 92] <author> Wall, D. W. </author> <title> Systems for Late Code Modification, </title> <address> pages 275-293. </address> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: The others are real workloads taken from our computing environment. The traces include references from the applications and the operating system. The X11 workloads also include references from the X11 server. The traces are generated using a tool called epoxie <ref> [Wall 92, Borg et al. 89, Chen 93] </ref> which rewrites MIPS [Kane 88] object files to insert instrumentation instructions before each basic block and memory reference. The instrumented binary emits enough information to reconstruct a complete trace of memory operations made during execution.
Reference: [Wheeler & Bershad 92] <author> Wheeler, B. and Bershad, B. N. </author> <title> Consistency Management for Virtually Indexed Caches. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 124-136, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: amount of additional hardware and software so that conflict misses, when they occur, can be quickly eliminated through virtual memory operations that change the cache mapping. 1.2 The role of virtual memory and cache conflicts An architecture's virtual memory page size partitions a direct-mapped cache into a set of cache-pages <ref> [Wheeler & Bershad 92] </ref>. Pages in the system's physical address space are partitioned into multiple equivalence classes, or colors, where pages from the same equivalence class map to the same cache-page.
Reference: [Wood 86] <author> Wood, D. A. </author> <title> An In-Cache Address Translation Mechanism. </title> <booktitle> In Proceedings of the 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 358-365. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1986. </year>
Reference-contexts: The cheapest, simplest, and fastest large caches are those that are direct-mapped <ref> [Wood 86] </ref>. However, direct-mapped caches by themselves exhibit conflict misses because they cannot maintain multiple active lines at the same index. <p> The number above each CML bar shows the number of pages recolored by the CML buffer policy. (in this case, 5 cycles). In practice, this is not likely to be the case <ref> [Hill 88, Przybylski et al. 88, Wood 86] </ref>, and the A2 cache will be somewhat slower than the DM cache. Overall performance then is dictated by the lower A2 cache miss rate coupled with its greater access time.
References-found: 28

