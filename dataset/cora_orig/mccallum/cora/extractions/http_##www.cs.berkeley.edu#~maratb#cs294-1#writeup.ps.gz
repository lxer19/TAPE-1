URL: http://www.cs.berkeley.edu/~maratb/cs294-1/writeup.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~maratb/cs294-1/
Root-URL: 
Title: Survey and Comparison of Parallelization Techniques for Genetic Algorithms  
Author: Marat Boshernitsan 
Note: (described in detail in  the WWW page: http://www.cs.berkeley.edu/~maratb/cs294-1/writeup.html.  
Date: May 11, 1996  [6]).  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California, Berkeley  
Pubnum: CS 294-1 Final Project  
Abstract: This paper surveys a number of parallelization techniques for genetic algorithms, focusing on two: the distributed genetic algorithm and the distributed fitness computation. While parallelization of serial algorithms usually requires certain amount of tricks and twitches, the genetic algorithms are parallel by their nature. This feature of genetic algorithms is widely exploited and there exists a number of ways in which a given algorithm may be parallelized. In addition to examining several parallel designs, this paper compares the performance of two implementations (the distributed genetic algorithm and distributed fitness computation) on dual-prcessor SunSPARC 20 for optimization of a simple function f(x) = ( x 0 The author can be reached at: maratb@CS.Berkeley.EDU. An on-line version of this paper can be found on c ) 10
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, T. E., Culler, D. E., and Patterson, D. A. </author> <title> "A Case for NOW (Networks of Workstations)". </title> <booktitle> IEEE Micro, </booktitle> <year> 1994. </year>
Reference-contexts: As the speed of computer networks is rapidly rising, distributed systems of workstations such as NOWs <ref> [1] </ref> are proving to be an even more inexpensive and attractive alternatives to massively-parallel procesor (MPP) architecures described above. All simulations in this paper were performed on SunSPARC 20, a dual-processor SMMP and on a NOW consisting of similar machines.
Reference: [2] <author> De Jong, K. A. </author> <title> "An analysis of the behavior of a class of genetic adaptive systems." </title> <type> (Doctoral dissertation, </type> <institution> University of Michigan). </institution> <note> Dissertation Abstracts International 36(10), 5140B. (University Microfilms No. 76-9381. 6 </note>
Reference-contexts: The graph of this function is shown in Figure 4. We will use the same parameters for our genetic algorithm as Goldberg, who in turns follows De Jong's suggestions in <ref> [2] </ref>: probability of mutation = 0:0333 probability of crossover = 0:6 population size = 30 5 (2 30 1) Since we are interested in collecting profiling information, we will let the algorithm run for 50 generations, even thought that many runs are frequently unnecessary.
Reference: [3] <author> Bakic, V. </author> <title> The PVM GALOPP System 3.0 User's Guide Michigan State University, </title> <year> 1995. </year>
Reference-contexts: The GALOPP System is capable of simulating both serial and parallel implementations of genetic algorithms. The parallel implementation emulates network parallelism (see Section 5) and is implemented using a sohisticated checkpoint/restart system. Of particular interest is PVM extension to GALOPPS implemented by Vera Bakic <ref> [3] </ref>. This extension allows GALOPPS to simulate network model by actually running a number of genetic algorithms in parallel and exchanging individuals according to predefined paths.
Reference: [4] <author> Goodman, E. D. GALOPPS: </author> <title> The "Genetic ALgorithm Optimized for Portability and Parallelism" System Technical Report 95-06-01, </title> <institution> Michigan State University, </institution> <year> 1995. </year>
Reference-contexts: This approach is best fit for "network" parallelization of genetic algorithms and is further studied in Section 5 2.3 GA simulations with GALLOPS All of the simulations in this paper were performed using GALOPPS (Genetic Algorithm Optimized for Portability and Prallelism System) <ref> [4] </ref>. The GALOPPS is a distant descendant of Simple 4 (2 30 1) Genetic Algorithm (SGA) described by David Goldberg in [6]. The GALOPP System is capable of simulating both serial and parallel implementations of genetic algorithms.
Reference: [5] <author> Garey, M. R. and Johnson, D. S. </author> <title> Computers and intractability: a guide to the theory of NP-completeness W. </title> <editor> H. </editor> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: This property of the genetic algorithms forms the basis for what John Holland calls implicit parallelism [9]. Because traditionally the GAs have been used to perform searches and optimizations that would take super-polinomial time (NP) <ref> [5] </ref>, the propery of implicit parallelism is very important. The class of NP problems is vast and their applications are numerous, and so, to achieve resonable performance, one needs to use a non-deterministic Turing Machine (NTM) [10] simulator.
Reference: [6] <author> Goldberg, D. E. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: it simpler than ever to perform evaluations and comparisons of different methods. [XXX more here ?] This paper proceeds to discuss various tools (hardware, software, simulators) for GA paral-lelization, formulate the efficiency problems inherent in serial implementation of genetic algorithms, suggest and evaluate two implementations of simple genetic algorithm (SGA) <ref> [6] </ref>, and finally discuss variations on the suggested implementations. 2 Parallelization Tools 2.1 Hardware The single instruction/single data computers (SISD) are characterized by carrying out single in-structino on one dataum at any given instant. <p> The GALOPPS is a distant descendant of Simple 4 (2 30 1) Genetic Algorithm (SGA) described by David Goldberg in <ref> [6] </ref>. The GALOPP System is capable of simulating both serial and parallel implementations of genetic algorithms. The parallel implementation emulates network parallelism (see Section 5) and is implemented using a sohisticated checkpoint/restart system. Of particular interest is PVM extension to GALOPPS implemented by Vera Bakic [3]. <p> The timing data is collected in memory and is written to disk at approprate points in execution. 3 Serial Genetic Algorithm In this section we will examine the profile the execution of simple genetic algorithm on the same function Goldberg uses in <ref> [6] </ref>, namely f (x) = x c n , where c has been chosen to normalize x, and n is taken to be 10. We will use the chromosome of length 30 bits, and so the normalizing coefficient c will be 2 30 1.
Reference: [7] <author> Grefenstette, J. J. </author> <title> "Parallel adaptive algorithms for function optimization". </title> <type> Technical Report CS-81-19. </type> <institution> Computer Science Department, Vanderbilt University, </institution> <year> 1981. </year>
Reference-contexts: Not surprisingly, GAs come very close to being one of the best simulation techniques precisely because of implicit parallelism. An attractive alternative to exploiting implicit parallelism of GAs is the notion of so-called island or network parallelism. This concept examined by Grefenstette in <ref> [7] </ref> is based on a seemingly simple idea of having several genetic algorithms run with independent memories, independent genetic operations, and independent function evaluations.
Reference: [8] <author> Heitkotter, Jorg and Beasley, David, eds. </author> <title> The Hitch-Hiker's Guide to Evolutionary Computation: A list of Frequently Asked Questions (FAQ) USENET: </title> <publisher> comp.ai.genetic </publisher>
Reference-contexts: 1 Introduction As any other evolutionary algorithm, the genetic algorithms (GA) are conceptually based on simulating the evolution of individual structures via processes of selections, mutation, and reproduction <ref> [8] </ref>. Since GAs are based on the mechanics of natural selecton and natural genetics, it is only "natural" that they are readily parallelizable.
Reference: [9] <author> Holland, J. H. </author> <title> Adaptation in natural and artificial systems. </title> <publisher> The University of Michigan Press, </publisher> <year> 1975. </year>
Reference-contexts: This property of the genetic algorithms forms the basis for what John Holland calls implicit parallelism <ref> [9] </ref>. Because traditionally the GAs have been used to perform searches and optimizations that would take super-polinomial time (NP) [5], the propery of implicit parallelism is very important.
Reference: [10] <author> Hopcroft, J. E. and Ullman, J. D. </author> <title> Introduction to automata theory, languages, </title> <publisher> and computation Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: The class of NP problems is vast and their applications are numerous, and so, to achieve resonable performance, one needs to use a non-deterministic Turing Machine (NTM) <ref> [10] </ref> simulator. Such machine operates by performing guesses along its execution path, which lead (eventually) to a correct solution. An ideal non-deterministic Turing Machine would traverse all possible paths in parallel, hence solving the problem in polynomial time. In practice, various randomized techniques are used to simulate NTM.
Reference: [11] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, Vaidy Sun-deram. </author> <title> PVM (Parallel Virtual Machine): A Users' Guide and Tutorial for Networked Parallel Computing MIT Press, </title> <note> 1994. Also available at http://www.netlib.org/pvm3/book/pvm-book.html. </note>
Reference-contexts: Although this proven to be a severely limitted environment, certain results and conclusions can already be drawn as outlined in Sections 6 and 7. 2.2 Software A host of software systems and libraries exist for development of parallel programs. Of particular interest is Parallel Virtual Machine (PVM) <ref> [11] </ref> which provides a unified framework consisting of programming library and a fully developed parallel process coordinator. PVM is mainly characterized buy its portability and availability on various systems ranging from MPPs to NOWs.
Reference: [12] <author> Trew, Arthur and Wilson, Greg, eds. </author> <title> Past, present, parallel: a survey of available parallel systems Springer-Verlag, </title> <booktitle> 1991. </booktitle> <pages> 7 </pages>
Reference-contexts: the increasing availablility of relatively inexpensive 1 There are very few "true" von Neumann machines around these days; almost all computers use some small-scale parallelism, such as pre-fetching of instructions, but this is usually hidden from the user. 2 parallel computers is starting to change the look of scientific computing <ref> [12] </ref>. Figure 1 shows two popular organizations for parallel computers. In SIMD (Single Instruction/Multiple Data) architecture many simple processing elements (PEs) execute the same instruction, but on different data. The PEs are coordinated by a master CPU which broadcasts program instructions to PEs.
References-found: 12

