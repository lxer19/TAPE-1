URL: http://www-cad.eecs.berkeley.edu/HomePages/aml/publications/ml94_wk.ps.gz
Refering-URL: http://www-cad.eecs.berkeley.edu/HomePages/aml/publications/index.html
Root-URL: http://www.cs.berkeley.edu
Title: Inferring Reduced Ordered Decision Graphs of Minimal Description Length  
Author: Arlindo L. Oliveira Alberto Sangiovanni-Vincentelli 
Date: May 17, 1994  
Address: Berkeley, Berkeley CA 94720  
Affiliation: Dept. of EECS, UC  
Abstract-found: 0
Intro-found: 1
Reference: [BFOS84] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: 1 Introduction The induction of decision trees from labeled training set data <ref> [BFOS84, BFOS84, Qui86, QR89] </ref> has been a successful approach for the induction of classification rules. However, although decision trees can, in principle, represent any concept, they are not concise representations for some concepts of interest.
Reference: [BRB89] <author> K. Brace, R. Rudell, and R. Bryant. </author> <title> Efficient implementation of a BDD package. </title> <booktitle> In Design Automation Conference, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Packages that manipulate reduced ordered decision graphs are widely available and have become the most commonly used tool for function manipulation in the logic synthesis community. Some of these packages are restricted to Boolean functions <ref> [BRB89] </ref> (each non-terminal graph has two and only two outgoing edges) while others [KB90] can accept multi-valued attributes.
Reference: [Bry86] <author> R. E. Bryant. </author> <title> Graph-based algorithms for Boolean function manipulation. </title> <journal> IEEE Transactions on Computers, </journal> <year> 1986. </year>
Reference-contexts: The level of a node is defined as the position of the variable tested at that node in the variable ordering used. Given an ordering, reduced ordered decision graphs are a canonical representation <ref> [Bry86] </ref>, for functions in that domain. This means that given a function F : X 1 fi X 2 ::: fi X n ! C and an ordering of the variables, there is one and only one representation for the function F .
Reference: [Dev93] <author> S. Devadas. </author> <title> Comparing two-level and ordered binary decision diagrams representations of logic functions. </title> <journal> IEEE Transaction on Computer-Aided Design, </journal> <volume> 12(5), </volume> <year> 1993. </year>
Reference-contexts: We conjecture that the performance of the approach described in this paper cannot beat this type of algorithms in these problems. Furthermore, it is know that there are functions that have compact DNF representations but require exponentially large RODGs, no matter what ordering is selected <ref> [Dev93] </ref>. Fringe-like algorithms, however, have a much harder time when the problems at hand don't have a compact DNF representation. In that case, we believe an approach like the one outlined here can be one of the best alternatives available.
Reference: [FS90] <author> Steven J. Friedman and Kenneth J. Supowit. </author> <title> Finding the optimal variable ordering for binary decision diagrams. </title> <journal> IEEE Trans. Comput., </journal> <volume> 39(5) </volume> <pages> 710-713, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Regrettably, selecting the optimal ordering for a given function is hard and cannot be solved exactly in many cases. However, many heuristic algorithms have been proposed for this problem <ref> [FS90, Rud93] </ref>. In our setting, the problem is even more complex because one wants to select the ordering that minimizes the final RODG and this ordering may not be the same as the one that minimizes the original RODG obtained from the decision tree.
Reference: [KB90] <author> T. Kam and R.K. Brayton. </author> <title> Multi-valued decision diagrams. </title> <type> Tech. Report No. </type> <institution> UCB/ERL M90/125, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Packages that manipulate reduced ordered decision graphs are widely available and have become the most commonly used tool for function manipulation in the logic synthesis community. Some of these packages are restricted to Boolean functions [BRB89] (each non-terminal graph has two and only two outgoing edges) while others <ref> [KB90] </ref> can accept multi-valued attributes. All these packages provide at least the same basic functionality: the ability to combine functions using basic Boolean and arithmetic operations and the ability to test for containment or equivalence of two functions.
Reference: [Koh94] <author> Ron Kohavi. </author> <title> Bottom-up induction of oblivious read-once decision graphs. </title> <booktitle> In European Conference in Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: We describe a simple but relatively effective algorithm that derives an ordered decision graph with minimal description length from a decision tree built using standard techniques. The approach proposed here is very different from the one proposed in <ref> [Koh94] </ref> that also used RODGs. His approach as described in the reference, suffers from serious limitations when solving problems with medium to large numbers of attributes although it performs well for small problems.
Reference: [MA91] <author> P. M. Murphy and D. W. Aha. </author> <title> Repository of Machine Learning Databases Machine readable data repository. </title> <institution> University of California, Irvine, </institution> <year> 1991. </year>
Reference-contexts: The monks2 problem was proposed in [TBB + 91]. Dnf2, dnf3, mux11 and par4 are described in [PH90]. Tictactoe is from <ref> [MA91] </ref> and legal, en encoding of legal and illegal positions in chess endings is described in more detail in [Oli94]. For all problems, multi-valued attributes were encoded using a binary encoding. For each problem, we performed 10 runs with randomly generated training sets.
Reference: [MM91] <author> J. J. Mahoney and R. J. Mooney. </author> <title> Initializing ID5R with a domain theory: some negative results. </title> <type> Technical Report 91-154, </type> <institution> CS Dept., University of Texas at Austin, Austin, TX, </institution> <year> 1991. </year>
Reference-contexts: Decision graphs have been proposed as one way to alleviate this problems, but the algorithms proposed so far for the construction of these graphs suffer from serious limitations. <ref> [MM91] </ref> propose to identify related subtrees in a decision tree obtained using standard methods, but reported limited success. [Oli93] proposed a greedy algorithm that performs either a join or a split, according to which one reduces more the description length.
Reference: [Oli93] <author> J. J. Oliver. </author> <title> Decision graphs an extension of decision trees. </title> <type> Technical Report 92/173, </type> <institution> Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <year> 1993. </year>
Reference-contexts: the quality of the generalization performed by a decision tree induced from data suffers because of two well known problems: the replication of subtrees required to represent some concepts and the rapid fragmentation of the training set data when high arity attributes are tested at a node. (See, for instance, <ref> [Oli93] </ref> for a more detailed description of these limitations). <p> Decision graphs have been proposed as one way to alleviate this problems, but the algorithms proposed so far for the construction of these graphs suffer from serious limitations. [MM91] propose to identify related subtrees in a decision tree obtained using standard methods, but reported limited success. <ref> [Oli93] </ref> proposed a greedy algorithm that performs either a join or a split, according to which one reduces more the description length. They reported some good results in relatively simple problems but our experiments using a similar approach failed in more complex problems.
Reference: [Oli94] <author> Arlindo L. Oliveira. </author> <title> Inductive Learning by Selection of Minimal Representations. </title> <type> PhD thesis, </type> <institution> UC Berkeley, </institution> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: The monks2 problem was proposed in [TBB + 91]. Dnf2, dnf3, mux11 and par4 are described in [PH90]. Tictactoe is from [MA91] and legal, en encoding of legal and illegal positions in chess endings is described in more detail in <ref> [Oli94] </ref>. For all problems, multi-valued attributes were encoded using a binary encoding. For each problem, we performed 10 runs with randomly generated training sets.
Reference: [OSV93] <author> Arlindo L. Oliveira and A. Sangiovanni-Vincentelli. </author> <title> Learning complex boolean functions : Algorithms and applications. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> Denver, CO, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Both constructive induction techniques and different forms of post-processing of the decision tree have been proposed. Specially worth mentioning are the fringe-like algorithms 5 6 <ref> [PH90, YRB91, OSV93] </ref> that incrementally build new composite attributes by looking at the tests performed at the fringes of decision trees. Our own experiments with these algorithms have shown that they are remarkably efficient at learning concepts that accept a compact DNF representation.
Reference: [PH90] <author> G. Pagallo and D. Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-100, </pages> <year> 1990. </year>
Reference-contexts: The monks2 problem was proposed in [TBB + 91]. Dnf2, dnf3, mux11 and par4 are described in <ref> [PH90] </ref>. Tictactoe is from [MA91] and legal, en encoding of legal and illegal positions in chess endings is described in more detail in [Oli94]. For all problems, multi-valued attributes were encoded using a binary encoding. For each problem, we performed 10 runs with randomly generated training sets. <p> Both constructive induction techniques and different forms of post-processing of the decision tree have been proposed. Specially worth mentioning are the fringe-like algorithms 5 6 <ref> [PH90, YRB91, OSV93] </ref> that incrementally build new composite attributes by looking at the tests performed at the fringes of decision trees. Our own experiments with these algorithms have shown that they are remarkably efficient at learning concepts that accept a compact DNF representation.
Reference: [QR89] <author> J. R. Quinlan and R. L. Rivest. </author> <title> Inferring decision trees using the Minimum Description Length Principle. </title> <journal> Inform. Comput., </journal> <volume> 80(3) </volume> <pages> 227-248, </pages> <month> March </month> <year> 1989. </year> <note> (An early version appeared as MIT LCS Technical report MIT/LCS/TM-339 (September 1987).). 7 </note>
Reference-contexts: 1 Introduction The induction of decision trees from labeled training set data <ref> [BFOS84, BFOS84, Qui86, QR89] </ref> has been a successful approach for the induction of classification rules. However, although decision trees can, in principle, represent any concept, they are not concise representations for some concepts of interest. <p> According to the MDL principle, one should choose the RODG that minimizes the total message length, t + d. However, as pointed out in <ref> [QR89] </ref>, different linear combinations of t and d are also consistent with a Bayesian interpretation of the MDLP and may be chosen according to different beliefs about the data. <p> The approach can be easily generalized to minimize a different linear combination of t and d, with only minor modifications required in the algorithms described in section 4. Since we will be encoding RODGs using an approach inspired in the one proposed in <ref> [QR89] </ref> for decision trees, we briefly review the later. [QR89] define the encoding of a tree recursively using the following rules: 1. A terminal node is encoded as a 0 followed by an encoding of the default class. 2. <p> Since we will be encoding RODGs using an approach inspired in the one proposed in <ref> [QR89] </ref> for decision trees, we briefly review the later. [QR89] define the encoding of a tree recursively using the following rules: 1. A terminal node is encoded as a 0 followed by an encoding of the default class. 2. <p> A non-terminal node that was visited before is encoded starting with 11 followed by a reference to the (already described) function implemented by that node. Following <ref> [QR89] </ref>, we ignore the issues related with the use of non-integral numbers of bits and we make the description less redundant by noting that when one is deeper in the decision graph not all variables can be usefully tested (only the ones that were node tested previously).
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction The induction of decision trees from labeled training set data <ref> [BFOS84, BFOS84, Qui86, QR89] </ref> has been a successful approach for the induction of classification rules. However, although decision trees can, in principle, represent any concept, they are not concise representations for some concepts of interest. <p> We assume, for now, that a given variable ordering was defined. Section 4.3 details how this ordering is obtained. 4.1 Obtaining a RODG from a decision tree Let T be a decision tree created using one of the approaches proposed in the literature (for example, <ref> [Qui86] </ref>).
Reference: [Qui88] <author> J. R. Quinlan. </author> <title> An empirical comparison of genetic and decision-tree classifiers. </title> <booktitle> In Fifth International Conference on Machine Learning, </booktitle> <pages> pages 135-141, </pages> <year> 1988. </year>
Reference-contexts: Decision trees have troubles with problems like this (a 3-bit multiplexer with 21 irrelevant variables) because, as pointed out in <ref> [Qui88] </ref>, the greedy strategy for node splitting tends to favor testing the data attributes first instead of the selection bits. 6 Conclusions and future work The results presented in section 5 show that the accuracy of the generalization performed by a decision tree algorithm can be improved, in some cases significantly,
Reference: [Rud93] <author> Richard Rudell. </author> <title> Dynamic variable ordering for ordered binary decision diagrams. </title> <booktitle> In ICCAD, </booktitle> <pages> pages 42-47. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Regrettably, selecting the optimal ordering for a given function is hard and cannot be solved exactly in many cases. However, many heuristic algorithms have been proposed for this problem <ref> [FS90, Rud93] </ref>. In our setting, the problem is even more complex because one wants to select the ordering that minimizes the final RODG and this ordering may not be the same as the one that minimizes the original RODG obtained from the decision tree. <p> Our implementation uses the sift algorithm for dynamic RODG ordering <ref> [Rud93] </ref>. A complete description of this algorithm is impossible here due to space limitations. However, the basic idea is simple. It is known that, given a RODG, swapping the order of two adjacent variables can be done very efficiently because only the nodes in those two levels are affected.
Reference: [TBB + 91] <author> S. B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. de Jong, S. Dzeroski, S. E. Fahlman, D. Fisher, R. Hamann, K. Kaufaman, S. Keller, I. Kononenko, J. Kreusiger, R. S. Michalski, T. Mitchell, P. Pachowitz, Y. Reich, H. Vafaic, W. Van de Weldel, W. WEnzel, J. Wnek, and J. Zhang. </author> <title> The monk's problems: a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: The monks2 problem was proposed in <ref> [TBB + 91] </ref>. Dnf2, dnf3, mux11 and par4 are described in [PH90]. Tictactoe is from [MA91] and legal, en encoding of legal and illegal positions in chess endings is described in more detail in [Oli94]. For all problems, multi-valued attributes were encoded using a binary encoding.
Reference: [YRB91] <author> D. S. Yang, L. Rendell, and G. Blix. </author> <title> Fringe-like feature construction: A comparative study and a unifying scheme. </title> <booktitle> In Proceedings of the Eight International Conference in Machine Learning, </booktitle> <pages> pages 223-227, </pages> <address> San Mateo, 1991. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 8 </pages>
Reference-contexts: Both constructive induction techniques and different forms of post-processing of the decision tree have been proposed. Specially worth mentioning are the fringe-like algorithms 5 6 <ref> [PH90, YRB91, OSV93] </ref> that incrementally build new composite attributes by looking at the tests performed at the fringes of decision trees. Our own experiments with these algorithms have shown that they are remarkably efficient at learning concepts that accept a compact DNF representation.
References-found: 19

