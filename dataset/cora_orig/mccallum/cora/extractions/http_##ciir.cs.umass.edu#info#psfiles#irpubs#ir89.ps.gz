URL: http://ciir.cs.umass.edu/info/psfiles/irpubs/ir89.ps.gz
Refering-URL: http://hobart.cs.umass.edu/~zlu/pub.html
Root-URL: 
Email: fzlu,callan,croftg@cs.umass.edu  
Title: Measures in Collection Ranking Evaluation  
Author: Zhihong Lu James P. Callan W. Bruce Croft 
Address: Amherst, MA 01003-4610  
Affiliation: Computer Science Department, University of Massachusetts  
Abstract: As a technique to hunt information on the Internet, collection location has received more attention. Several approaches have been proposed to solve this problem. All these approaches adopt the same procedure: ranking the collections and returning the top-ranks. But these approaches define different measures to evaluate collection ranking and the measures have significant weaknesses. In this paper, we survey the measures used in current research and propose a new pair of measures that are based on the concepts of precision and recall. The new measures overcome the problems found in the current measures.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> James P. Callan, Zhihong Lu and W. Bruce Croft. </author> <title> Searching Distributed Collections with Inference Networks. </title> <booktitle> Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 21-28, </pages> <address> Seattle, </address> <month> July </month> <year> 1995. </year> <institution> Association for Computing Machinery. </institution> <month> 8 </month>
Reference-contexts: In this paper, we survey the measures for collection ranking used in the current research, discuss their strengthes and weaknesses, and propose new measures based on the concepts of precision and recall. In Section 2, we briefly survey the measures used in our previous research <ref> [1] </ref>, NetSerf [2] and GLOSS [4][5]. In Section 3 we define the new measures, which are variations of precision and recall. In the final section, we summerize the discussions. 1 2 A Survey of Measures in Collection Ranking Evaluation 2.1 Ranking with INQUERY In our previous research on collection ranking [1], <p> <ref> [1] </ref>, NetSerf [2] and GLOSS [4][5]. In Section 3 we define the new measures, which are variations of precision and recall. In the final section, we summerize the discussions. 1 2 A Survey of Measures in Collection Ranking Evaluation 2.1 Ranking with INQUERY In our previous research on collection ranking [1], the mean-square error was used to compare the effectiveness of variations to the basic collection ranking algorithms. <p> New measures need to be defined to overcome these problems. Like GLOSS, our new measures are based on the concepts of precision and recall. But we modify the conventional precision and recall by considering the numbers of the relevant documents in each collection. 6 As in our previous research <ref> [1] </ref>, the optimal rank is determined by sorting collections according to the numbers of relevant documents they contain.
Reference: [2] <author> NetSerf: </author> <title> Using Semantic Knowledge to Find Internet Information Archives. </title> <booktitle> Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 4-11, </pages> <address> Seattle, </address> <month> July </month> <year> 1995. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: In this paper, we survey the measures for collection ranking used in the current research, discuss their strengthes and weaknesses, and propose new measures based on the concepts of precision and recall. In Section 2, we briefly survey the measures used in our previous research [1], NetSerf <ref> [2] </ref> and GLOSS [4][5]. In Section 3 we define the new measures, which are variations of precision and recall. <p> two collections, Rank 2 can let the user get 39 relevant documents (the same as the optimal rank), and Rank 3 can only let the user get 2 28 relevant documents (here we assume that the search engine at each individual collection can retrieve all relevant documents). 2.2 NetSerf NetSerf <ref> [2] </ref> defines its measure upon a set of queries: Given the set of relevant archives (collections in our context), count the number of queries for which any of the relevant archives were found in the first n top-ranked hits returned by the systems.
Reference: [3] <institution> Glossary of Servers http://gloss.stanford.edu </institution>
Reference: [4] <author> Luis Gravano and Hector Garcia-Molina. </author> <title> Precision and Recall of GLOSS Estimator for Database Discovery. Stanford Univerdity Technical Note Number STAN-CS-TN-94-10. </title>
Reference-contexts: GLOSS has two versions: the Boolean version <ref> [4] </ref> and the vector-space version [5]. The definitions of the measures in these two versions have slight differences.
Reference: [5] <author> Luis Gravano and Hector Garcia-Molina. </author> <title> Generalizing GLOSS to Vector-Space Databases and Broker Hierarchies. </title> <booktitle> Proceedings of the 21st VLDB Conference, </booktitle> <address> Zurich, </address> <month> Switchland </month> <year> 1995. </year>
Reference-contexts: GLOSS has two versions: the Boolean version [4] and the vector-space version <ref> [5] </ref>. The definitions of the measures in these two versions have slight differences. GLOSS uses databases to refer to collections in our context. 2.3.1 The Boolean Version In the Boolean version of GLOSS, GLOSS defines Right (q; DB), where q is a query and DB is a set of databases.
Reference: [6] <author> WAIS 2.0: </author> <note> Technical Description http://www.wais.com/newhomepages/techtalk.html 9 </note>
References-found: 6

