URL: http://www.umiacs.umd.edu/~gopal/Publications/azriel.ps.gz
Refering-URL: http://www.umiacs.umd.edu/users/gopal/professional.html
Root-URL: 
Title: Multiresolution GMRF Models for Image Segmentation Markov random field (MRF) models, because of the local
Author: Rama Chellappa and Santhana Krishnamachari 
Note: 1: Introduction Among several possible 2-D models for images most of the research has been restricted to  This work was supported in part by Grant #ASC 9318183 from National Science Foundation. A longer version of this chapter is due to appear in the IEEE Transactions on Image Processing.  
Address: College Park, MD 20742  Clarksburg, MD 20871  
Affiliation: Department of Electrical Engineering and Center for Automation Research University of Maryland  Image Processing Department Communication Technology Division COMSAT Laboratories  
Abstract: A multiresolution model for Gauss Markov random fields (GMRF) with application to texture segmentation is presented. Coarser resolution sample fields are obtained by subsampling the sample field at the fine resolution. Although the Markov property is lost under such resolution transformation, coarse resolution non-Markov random fields can be effectively approximated by Markov fields. We present a local conditional distribution invariance approximation to estimate the GMRF parameters at coarser resolutions from the fine resolution parameters. Our experiments with synthetic, Brodatz texture and real satellite images show that this multiresolution technique results in a better segmentation and requires lesser computation than the single resolution algorithm. There has been an increasing emphasis on using statistical techniques for modeling and analyzing images. Typical image processing problems have the following aspects to be dealt with: the identification of an appropriate model that reflects the prior beliefs and knowledge about the family of images that are to be analyzed, the selection of a proper observation model that reflects the nature of the transformations these images undergo during observation, and the selection of a suitable error criterion to be optimized. For many image processing problems, such as image enhancement, image restoration, texture identification and segmentation, prior and observation models and error criteria can be very efficiently selected in a statistical framework. Using statistical models in a Bayesian framework enables posing many image processing problems as statistical inference problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Besag, </author> <title> "On the Statistical Analysis of Dirty Pictures," </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Vol. 48, </volume> <pages> pp. 259-302, </pages> <year> 1986. </year>
Reference-contexts: We also show that the computations for this estimator turn out to be similar to the psuedo likelihood estimator <ref> [1] </ref>, except that the sample covariances are replaced by covariances calculated with respect to the non-Markov measure that is being approximated. We present results on the existence of different sets of GMRF parameters at fine resolution that result in statistically identical coarser resolution random fields. <p> In MRF applications all optimizations are performed based on the local conditional distribution, so, we believe an estimator based on it should be well suited for image analysis applications. We also exemplify the connection between this estimator and the pseudo likelihood estimator <ref> [1] </ref>. The Markov approximation presented in this section is based on linear estimation. Before presenting the details, we will provide a known result regarding the linear estimation of a GMRF. Let Z be a GMRF defined by ( ; 2 ) with a neighborhood . <p> However, the joint densities p (x ) on the whole lattice are not the same, p (x ) is a non-Markov density and q (x ) is a Markov density. 2. It is worth observing that Eq. (8) is similar to the pseudo likelihood estimate [3], <ref> [1] </ref> where the GMRF parameters are obtained by minimizing the products of local conditional densities over the entire lattice. <p> The ICM solution is obtained by performing the following optimization at each lattice site <ref> [1] </ref>: max P (X s jL s ; X s+r ; r 2 )P (L s jL s+r ; r 2 ): This is equivalent to, min 1 log ( 2 (v)) + 2 2 (v) X r (v)x s+r ] 2 fiU (L s = v) the minimization is performed
Reference: [2] <author> C. Bouman and B. Liu, </author> <title> "Multiple Resolution Segmentation of Textured Images," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 13, </volume> <pages> pp. 99-113, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Let X (k) represent a random field, obtained by ordering the random variables on (k) . The parameters of a GMRF defined on a lattice (k) are denoted by f (k) ; <ref> [ 2 ] </ref> (k) g and the associated neighborhood is denoted by (k) . The covariance matrix and the power spectrum associated with X (k) are denoted by (k) and S x (!) respectively. <p> The covariance matrix and the power spectrum associated with X (k) are denoted by (k) and S x (!) respectively. The probability distributions defined on a lattice (k) are indexed by p (k) (:). (0) be a GMRF defined on (0) with parameters f (0) ; <ref> [ 2 ] </ref> (0) g and a neighborhood (0) . The power spectrum of X (0) can be written as in Eq. (4) : S (0) [ 2 ] (0) P (0) M r 1 ! 1 + 2 (5) where ! = f (! 1 ; ! 2 ) : <p> distributions defined on a lattice (k) are indexed by p (k) (:). (0) be a GMRF defined on (0) with parameters f (0) ; <ref> [ 2 ] </ref> (0) g and a neighborhood (0) . The power spectrum of X (0) can be written as in Eq. (4) : S (0) [ 2 ] (0) P (0) M r 1 ! 1 + 2 (5) where ! = f (! 1 ; ! 2 ) : 0 ! 1 M 1; 0 ! 2 N 1g: The subsampling resolution transformation is defined as: X (k) (k1) defined for all s 2 (k) <p> ) belongs to the family of GMRF densities, q (x s jx s+r ; r 2 ) will be of the form given in Eq. (3). q (x s jx s+r ; r 2 ) = p expf [x s r2 r x s+r ] 2 Let ( fl ; <ref> [ 2 ] </ref> fl ) be the parameters corresponding to q fl (x ). To simplify the notation, let Y be the vector containing the neighborhood random variables in a proper order. <p> To simplify the notation, let Y be the vector containing the neighborhood random variables in a proper order. For a first order neighborhood, Y T = X s+(1;0) X s+(0;1) X s+(1;0) X s+(0;1) : Now performing the minimization in terms of the parameters, ( fl ; <ref> [ 2 ] </ref> fl ) = arg min D [p (x s jy ) jj q (x s jy )] = arg min E p log q (X s jY ) = arg max E p [log q (X s jY )] ( ; 2 ) 2 1 log 2 2 <p> p [X s r2 (8) It can be seen that the fl parameters corresponding to q fl (x ) are obtained by minimizing the second term in the Eq. (8) fl = arg min X r Y r ] 2 (9) and using the fl obtained, we can estimate the <ref> [ 2 ] </ref> fl that minimizes Eq. (8), [ 2 ] fl = E p [X s r2 r Y r ] 2 : (10) then, fl = arg min fl = [E p (Y Y T )] 1 E p (X s Y ) (11) [ 2 ] fl = <p> seen that the fl parameters corresponding to q fl (x ) are obtained by minimizing the second term in the Eq. (8) fl = arg min X r Y r ] 2 (9) and using the fl obtained, we can estimate the <ref> [ 2 ] </ref> fl that minimizes Eq. (8), [ 2 ] fl = E p [X s r2 r Y r ] 2 : (10) then, fl = arg min fl = [E p (Y Y T )] 1 E p (X s Y ) (11) [ 2 ] fl = E p (X 2 = E p (X <p> we can estimate the <ref> [ 2 ] </ref> fl that minimizes Eq. (8), [ 2 ] fl = E p [X s r2 r Y r ] 2 : (10) then, fl = arg min fl = [E p (Y Y T )] 1 E p (X s Y ) (11) [ 2 ] fl = E p (X 2 = E p (X 2 In addition, the estimated fl parameters should satisfy the positivity conditions in Eq. (2). Now, returning back to multiresolution discussion, let X (0) be a GMRF defined by ( (0) ; [ 2 ] (0) ) and <p> (X s Y ) (11) <ref> [ 2 ] </ref> fl = E p (X 2 = E p (X 2 In addition, the estimated fl parameters should satisfy the positivity conditions in Eq. (2). Now, returning back to multiresolution discussion, let X (0) be a GMRF defined by ( (0) ; [ 2 ] (0) ) and X (k) be the field obtained by subsampling X (0) , k times. The non-Markov X (k) can be approximated by a GMRF by minimizing Eq. (7). <p> Since p (x ) is Gaussian, p (x s jx s+r ; r 2 ) is also Gaussian with conditional mean P r x s+r (which is the best linear estimate of X s in terms of X s+r ; r 2 ) and conditional variance <ref> [ 2 ] </ref> fl (which is the corresponding minimum mean square error of the estimator) [19]. q fl (x ) being a GMRF with parameters ( fl ; [ 2 ] fl ), from the discussion at the beginning of this section, has the conditional distri bution q fl (x s <p> s+r (which is the best linear estimate of X s in terms of X s+r ; r 2 ) and conditional variance <ref> [ 2 ] </ref> fl (which is the corresponding minimum mean square error of the estimator) [19]. q fl (x ) being a GMRF with parameters ( fl ; [ 2 ] fl ), from the discussion at the beginning of this section, has the conditional distri bution q fl (x s jx s+r ; r 2 ) with the conditional mean P r x s+r and conditional variance [ 2 ] fl . <p> (x ) being a GMRF with parameters ( fl ; <ref> [ 2 ] </ref> fl ), from the discussion at the beginning of this section, has the conditional distri bution q fl (x s jx s+r ; r 2 ) with the conditional mean P r x s+r and conditional variance [ 2 ] fl . However, the joint densities p (x ) on the whole lattice are not the same, p (x ) is a non-Markov density and q (x ) is a Markov density. 2. <p> Therefore, we look at the power spectrum of the subsampled random fields which are simpler functions of the parameters. We show that there exists different sets of GMRF parameters, which on subsampling result in the same pdf at the lower resolution. Since the parameter <ref> [ 2 ] </ref> (0) is a multiplicative factor in the power spectral function, we assume it be equal to one and investigate the existence of different sets of parameters that result in the identical coarser resolution random fields.
Reference: [3] <author> R. Chellappa, </author> <title> "Two-dimensional Discrete Gaussian Markov Random Field Models for Image Processing," in Progress in Pattern Recognition (L. </title> <editor> N. Kanal and A. Rosenfeld, </editor> <booktitle> eds.), </booktitle> <pages> pp. 79-112, </pages> <address> Elsavier, </address> <year> 1985. </year>
Reference-contexts: Before presenting the details, we will provide a known result regarding the linear estimation of a GMRF. Let Z be a GMRF defined by ( ; 2 ) with a neighborhood . Then the best estimate of Z s based on the elements of is given by <ref> [3] </ref>: ^z s = r2 and the mean square error E (Z s ^ Z s ) 2 = 2 : The conditional density p (z s jz r ; r 2 ) is Gaussian with conditional mean P conditional variance 2 : Let X be a random field with a <p> However, the joint densities p (x ) on the whole lattice are not the same, p (x ) is a non-Markov density and q (x ) is a Markov density. 2. It is worth observing that Eq. (8) is similar to the pseudo likelihood estimate <ref> [3] </ref>, [1] where the GMRF parameters are obtained by minimizing the products of local conditional densities over the entire lattice. <p> To compare the computational requirements between the single resolution and multires--olution approaches, we define a unit of computation to be the computation required to perform ICM at a single pixel site. We generated texture images using the technique given in <ref> [3] </ref>.
Reference: [4] <author> F. S. Cohen and D. B. Cooper, </author> <title> "Simple Parallel Hierarchical and Relaxation Algorithms for Segmenting Noncausal Markovian Random Fields," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 9, </volume> <pages> pp. 195-219, </pages> <month> March </month> <year> 1987. </year>
Reference: [5] <author> T. Cover and J. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: In this section we show that it is possible to obtain good Markov approximations for coarser resolution fields. In this section we present a technique to estimate the best GMRF parameters of a non-Markov random field, based on a KL distance measure between local conditional distributions (conditional relative entropy) <ref> [5] </ref>. In MRF applications all optimizations are performed based on the local conditional distribution, so, we believe an estimator based on it should be well suited for image analysis applications. We also exemplify the connection between this estimator and the pseudo likelihood estimator [1].
Reference: [6] <author> G. R. Cross and A. K. Jain, </author> <title> "Markov Random Field Texture Models," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 5, </volume> <pages> pp. 25-39, </pages> <month> Jan </month> <year> 1983. </year>
Reference-contexts: The elements that are included in the neighborhood of the site marked s for different neighborhood orders can be found in <ref> [6] </ref>. For the first order case, = f (1; 0); (0; 1); (1; 0); (0; 1)g, and s = fs + r : r 2 g.
Reference: [7] <author> H. Derin and H. Elliot, </author> <title> "Modeling and Segmentation of Noisy and Textured Images Using Gibbs Random Field," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 9, </volume> <pages> pp. 39-55, </pages> <month> Jan. </month> <year> 1987. </year>
Reference: [8] <author> D. Geiger and F. Girosi, </author> <title> "Parallel and Deterministic Algorithms for MRFs: Surface Reconstruction," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 13, </volume> <pages> pp. 401-413, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Two different approaches have been used to reduce the computational requirement. The first is to use non-optimal, deterministic methods that converge to a local optimal point, but still provide reasonably good results. Geiger and Girosi <ref> [8] </ref> and Zhang [24] use mean field approximations that lead to deterministic relaxation algorithms. Wu and Doerschuk [23] use a tree approximation that replaces the lattice on which an MRF is defined by an acyclic tree which allows replacing the iterative MRF computations by recursive computations.
Reference: [9] <author> S. Geman and D. Geman, </author> <title> "Stochastic Relaxation, Gibbs Distribution and the Bayesian Restoration of Images," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 6, </volume> <pages> pp. 721-741, </pages> <month> Nov </month> <year> 1984. </year>
Reference-contexts: Since MRF models express global statistics in terms of the local neighborhood potentials, all computations are restricted to a local window. This spawned a lot of interest in developing algorithms that utilize local computations to achieve global optimization <ref> [9] </ref>. But the main drawback is that the optimization schemes associated with MRF energy functions are iterative. Typical MRF algorithms visit all lattice sites in a specific order and perform a local computation at each site; this is repeated until some form of convergence is reached.
Reference: [10] <author> B. Gidas, </author> <title> "A Renormalization Group Approach to Image Processing," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 11, No. 2, </volume> <pages> pp. 164-180, </pages> <year> 1989. </year>
Reference: [11] <author> R. Haralick, </author> <title> "Statistical and Structural Approaches to Texture," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 67, </volume> <pages> pp. 610-621, </pages> <month> May </month> <year> 1979. </year>
Reference: [12] <author> F. C. Jeng, </author> <title> "Subsampling of Markov Random Fields," </title> <booktitle> Jour. of Visual Communication and Image Representation, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 225-229, </pages> <month> Sep. </month> <year> 1992. </year>
Reference-contexts: Two important aspects of multiresolution approaches are: (1) divide and conquer and (2) action at a distance [21]. Research efforts on multiresolution models and analysis can be found in [22],[2],[10],[4] and, [16]. We elaborate on the following, because of their relevance to our work. Jeng, in <ref> [12] </ref>, discusses the effect of subsampling resolution transformation on MRFs and presents two results: first, the Markov property is not preserved for a general subsampling scheme and, second, it is preserved under some specific subsampling schemes depending on the size and shape of the neighborhood.
Reference: [13] <author> R. L. Kashyap, </author> <title> "Analysis and Synthesis of Image Patterns by Spatial Interaction Models," in Progress in Pattern Recognition (L. </title> <editor> N. Kanal and A. Rosenfeld, eds.), </editor> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1981. </year>
Reference-contexts: For the first order case, = f (1; 0); (0; 1); (1; 0); (0; 1)g, and s = fs + r : r 2 g. If X is modeled by a GMRF with a symmetric neighborhood , then X can be written as <ref> [13] </ref>: X r X s+r + e s where e is a zero mean, Gaussian noise, with autocorrelation given by : E [e s e s+r ] = &gt; &lt; 2 if r = (0; 0) 0 otherwise. (1) Hence the GMRF can be completely characterized by the set of parameters <p> The individual elements of s are given by: cos ( M N r 2 r 2 : The first condition is necessary to ensure stationarity and the second to ensure that the covariance matrix of X is positive definite. X exhibits the Markov property <ref> [13] </ref>, p (x s jx t ; 8t 6= s; t 2 ) = p (x s jx s+r ; r 2 ) 1 2 2 expf [x s r2 r x s+r ] 2 The power spectrum S x (!) of X can be shown to be [13]: S x <p> Markov property <ref> [13] </ref>, p (x s jx t ; 8t 6= s; t 2 ) = p (x s jx s+r ; r 2 ) 1 2 2 expf [x s r2 r x s+r ] 2 The power spectrum S x (!) of X can be shown to be [13]: S x (!) = 1 r2 r cos [ 2 N r 2 ! 2 ] where ! = f! 1 ; ! 2 g; and 0 ! 1 M 1; 0 ! 2 N 1. 2.2: GMRFs and Resolution Transformation Let (0) = = f (s 1 ; s <p> X (k) (0) E p (k) [X (k) (k) (0) (0) For any two lattice sites u and v in (0) the correlation is given by <ref> [13] </ref>: E p (0) [X (0) v ] = M N s2 (0) M s 2 u 2 M s 2 v 2 1 [ (0) ] T (13) where i n = exp ( 1 2i Under the assumption that the covariance matrix with respect to p measure is positive
Reference: [14] <author> S. Krishnamachari, </author> <title> Hierarchical Markov Random Field Models for Image Analysis, </title> <type> Ph.D. dissertation, </type> <institution> University of Maryland, College Park, </institution> <year> 1995. </year>
Reference-contexts: For a second order GMRF at the fine resolution, the only set of parameters that result in the same power spectrum at (1) is, ( (1;0) ; (0;1) ; (1;1) ; (1;1) ), ( (1;0) ; (0;1) ; (1;1) ; (1;1) ); Proof: The proof can be found in <ref> [14] </ref>. Similar results can be obtained for higher order cases. 5: Texture Segmentation Computer vision and image analysis algorithms use various visual cues to analyze and interpret an image of a complex scene. These visual cues include, among others, photometric and geometric cues.
Reference: [15] <author> S. Lakshmanan and H. Derin, </author> <title> "Gaussian Markov Random Fields at Multiple Resolutions," in Markov Random Fields: Theory and Applications (R. Chellappa, </title> <publisher> ed.), </publisher> <pages> pp. 131-157, </pages> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: Jeng, in [12], discusses the effect of subsampling resolution transformation on MRFs and presents two results: first, the Markov property is not preserved for a general subsampling scheme and, second, it is preserved under some specific subsampling schemes depending on the size and shape of the neighborhood. In <ref> [15] </ref> Lakshmanan and Derin present an excellent discussion on multiresolution GMRF models. It is shown that the GMRFs lose their Markov property under subsampling and expressions for the power spectral density functions at coarser resolution are obtained. <p> In addition, a covariance invariance approximation is presented to approximate the coarser resolution data by GMRFs. Many interesting properties of this estimator such as maximizing the entropy and minimizing the Kullback-Leibler (KL) distance can be found in <ref> [15] </ref>. We present a multiresolution model based on a KL distance measure. Given that the data at the fine resolution is a GMRF, the goal is to obtain suitable models at coarser resolutions. Data at coarser resolutions are obtained by subsampling the fine resolution data. <p> Section 7 concludes the paper. 2: GMRFs and Resolution Transformation In this section we introduce basic notations used in the rest of the chapter and also present results on loss of the Markov property under resolution transformation <ref> [15] </ref>. 2.1: The GMRF Model We use the following notation : t = (t 1 ; t 2 ); s = (s 1 ; s 2 ) : coordinates of grid points on a 2-D lattice = fs : 0 s 1 M 1; 0 s 2 N 1g : a <p> The power spectrum of X (k) can be shown to be <ref> [15] </ref>: S (k) 1 X S (0) 0 where r 0 2 k r 1 ; N It can be observed that S (k) x (!) cannot be written in the form of Eq. (4) with a finite neighborhood. <p> Therefore, the subsampled fields X (k) are non-Markov, except for the special case of second order separable correlation GMRFs <ref> [15] </ref>. 3: Local Conditional Distribution Invariance Approximation As mentioned in the last section, GMRFs become non-Markov when subsampled. However, if the coarser resolution data are modeled by the exact non-Markov Gaussian measures, conventional optimization techniques based on Markov properties cannot be applied.
Reference: [16] <author> M. R. Luettgen, W. C. Karl, A. S. Willsky, and R. R. Tenney, </author> <title> "Multiscale Representations of Markov Random Fields," </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> Vol. 41, </volume> <pages> pp. 3377-3397, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The second approach is to use multiresolution techniques. Two important aspects of multiresolution approaches are: (1) divide and conquer and (2) action at a distance [21]. Research efforts on multiresolution models and analysis can be found in [22],[2],[10],[4] and, <ref> [16] </ref>. We elaborate on the following, because of their relevance to our work.
Reference: [17] <author> B. S. Manjunath and R. Chellappa, </author> <title> "A Note on Unsupervised Texture Segmentation," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 13, </volume> <pages> pp. 478-483, </pages> <month> May </month> <year> 1991. </year>
References-found: 17

