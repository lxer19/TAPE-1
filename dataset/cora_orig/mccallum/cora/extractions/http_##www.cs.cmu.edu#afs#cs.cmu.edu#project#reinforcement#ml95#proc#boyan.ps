URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/reinforcement/ml95/proc/boyan.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/reinforcement/ml95/proceedings.html
Root-URL: 
Email: jab@cs.cmu.edu, awm@cs.cmu.edu  
Title: Robust Value Function Approximation by Working Backwards Computing an accurate value function is the key
Author: Justin A. Boyan and Andrew W. Moore 
Keyword: LEARNING CONTROL  
Note: BACKWARDS  O(AX 2 O(AX log X), and O(AX), respectively.  
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: In this paper, we examine the intuition that TD() is meant to operate by approximating asynchronous value iteration. We note that on the important class of discrete acyclic stochastic tasks, value iteration is inefficient compared with the DAG-SP algorithm, which essentially performs only one sweep instead of many by working backwards from the goal. The question we address in this paper is whether there is an analogous algorithm that can be used in large stochastic state spaces requiring function approximation. We present such an algorithm, analyze it, and give comparative results to TD on several domains. the state). Using VI to solve MDPs belonging to either of these special classes can be quite inefficient, since VI performs backups over the entire space, whereas the only backups useful for improving V fl are those on the "frontier" between already-correct and not-yet-correct V fl values. In fact, there are classical algorithms for both problem classes which compute V fl more efficiently by explicitly working backwards: for the deterministic class, Dijkstra's shortest-path algorithm; and for the acyclic class, Directed-Acyclic-Graph-Shortest-Paths (DAG-SP) [6]. 1 DAG-SP first topologically sorts the MDP, producing a linear ordering of the states in which every state x precedes all states reachable from x. Then, it runs through that list in reverse, performing one backup per state. Worst-case bounds for VI, Dijkstra, and DAG-SP in deterministic domains with X states and A actions/state are 1 Although [6] presents DAG-SP only for deterministic acyclic problems, it applies straightforwardly to the 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <journal> AI Journal, </journal> <year> 1995. </year>
Reference-contexts: We have therefore investigated generalizations of the Dijkstra and DAG-SP algorithms specifically modified to accommodate huge state spaces and value function 2 This algorithm also bears a close resemblance to the RTDP algorithm <ref> [1] </ref>. approximation. Our variant of Dijkstra's algorithm, called Grow-Support, was presented in [5] and will not be discussed further here. Our variant of DAG-SP is an algorithm we call ROUT, introduced below. Table 2 summarizes the relationships among these algorithms.
Reference: [2] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference: [3] <author> D. A. Berry and B. Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <year> 1985. </year>
Reference-contexts: By contrast, ROUT completed successfully in under 1 million evaluations, and performed at the significantly higher level of -0.09. ROUT's adaptively generated training set contained only 133 states. Task 3: Multi-armed Bandit Problem Our third test domain is the finite-horizon k-armed bandit problem <ref> [3, 8] </ref>. While an optimal solution in the infinite-horizon case can be found efficiently using Gittins indices, solving the finite-horizon problem is equivalent to solving an acyclic, stochastic MDP in belief space [3]. The size of this MDP is O (n 2k ) for a horizon of length n. <p> Task 3: Multi-armed Bandit Problem Our third test domain is the finite-horizon k-armed bandit problem [3, 8]. While an optimal solution in the infinite-horizon case can be found efficiently using Gittins indices, solving the finite-horizon problem is equivalent to solving an acyclic, stochastic MDP in belief space <ref> [3] </ref>. The size of this MDP is O (n 2k ) for a horizon of length n. We show results for k = 3 arms and a horizon of n = 25 pulls, where the resulting MDP has 736,281 states.
Reference: [4] <author> D. Bertsekas. </author> <title> A counterexample to temporal differences learning. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 270-9, </pages> <year> 1995. </year>
Reference-contexts: In the case of undiscounted, absorbing MDPs and linear function approximators, TD () will converge [7], but even then not necessarily to a good approximation of V fl when 6= 1|as was recently demonstrated by an example of Bertsekas <ref> [4] </ref>. Moreover, in the general function-approximation case, repeatedly applying one-step backups may propagate and enlarge approximation errors, leading to instability [5].
Reference: [5] <author> J. A. Boyan and A. W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Te-sauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Moreover, in the general function-approximation case, repeatedly applying one-step backups may propagate and enlarge approximation errors, leading to instability <ref> [5] </ref>. Thus, we have presented two reasons why working strictly backwards may be desirable: efficiency, because updates need only be done on the "frontier" rather than all over state space; and robustness, because correct V fl values, once assigned, need never again be changed. <p> We have therefore investigated generalizations of the Dijkstra and DAG-SP algorithms specifically modified to accommodate huge state spaces and value function 2 This algorithm also bears a close resemblance to the RTDP algorithm [1]. approximation. Our variant of Dijkstra's algorithm, called Grow-Support, was presented in <ref> [5] </ref> and will not be discussed further here. Our variant of DAG-SP is an algorithm we call ROUT, introduced below. Table 2 summarizes the relationships among these algorithms.
Reference: [6] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Although <ref> [6] </ref> presents DAG-SP only for deterministic acyclic problems, it applies straightforwardly to the stochastic case. Another difference between VI and working back-wards is that VI repeatedly re-estimates the values at every state, using old predictions to generate new training values.
Reference: [7] <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <type> 8(3/4), </type> <month> May </month> <year> 1992. </year>
Reference-contexts: Nevertheless, it is important to note that when function approximators are used, TD () provides no guarantees of optimality. In the case of undiscounted, absorbing MDPs and linear function approximators, TD () will converge <ref> [7] </ref>, but even then not necessarily to a good approximation of V fl when 6= 1|as was recently demonstrated by an example of Bertsekas [4]. Moreover, in the general function-approximation case, repeatedly applying one-step backups may propagate and enlarge approximation errors, leading to instability [5].
Reference: [8] <author> M. O. Duff. </author> <title> Q-learning for bandit problems. </title> <type> Technical Report CMPSCI 95-26, </type> <institution> University of Massachusetts, </institution> <year> 1995. </year>
Reference-contexts: By contrast, ROUT completed successfully in under 1 million evaluations, and performed at the significantly higher level of -0.09. ROUT's adaptively generated training set contained only 133 states. Task 3: Multi-armed Bandit Problem Our third test domain is the finite-horizon k-armed bandit problem <ref> [3, 8] </ref>. While an optimal solution in the infinite-horizon case can be found efficiently using Gittins indices, solving the finite-horizon problem is equivalent to solving an acyclic, stochastic MDP in belief space [3]. The size of this MDP is O (n 2k ) for a horizon of length n.
Reference: [9] <author> R. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: Perhaps the most successful application of VI-based algorithms with function approximation has been in the domain of backgammon [10]. Tesauro modified Sut-ton's TD () algorithm <ref> [9] </ref>, which is normally thought of as a model-free algorithm for learning to predict, into a model-based algorithm for learning to control. Table 1 shows a TD (0) variant of Tesauro's algorithm adapted for the general MDP case.
Reference: [10] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8(3/4), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Computing V fl requires generalization: a natural technique is to encode the states as real-valued feature vectors and to use a function approximator to fit V fl over this feature space. Perhaps the most successful application of VI-based algorithms with function approximation has been in the domain of backgammon <ref> [10] </ref>. Tesauro modified Sut-ton's TD () algorithm [9], which is normally thought of as a model-free algorithm for learning to predict, into a model-based algorithm for learning to control. Table 1 shows a TD (0) variant of Tesauro's algorithm adapted for the general MDP case.
Reference: [11] <author> C. Watkins and P. Dayan. </author> <title> Technical note: </title> <journal> Q-Learning. Machine Learning, </journal> <volume> 8(3/4), </volume> <month> May </month> <year> 1992. </year>
Reference: [12] <author> W. Zhang and T. G. Dietterich. </author> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of IJCAI-95 (to appear), </booktitle> <year> 1995. </year> <month> 6 </month>
Reference-contexts: It is closely related to VI; the key difference is that its backups are done along sample trajectories through the process, rather than along sweeps of the entire state space. 2 Tesauro's combination of TD () and neural networks has been applied successfully to other domains, including combinatorial optimization <ref> [12] </ref>. Nevertheless, it is important to note that when function approximators are used, TD () provides no guarantees of optimality.
References-found: 12

