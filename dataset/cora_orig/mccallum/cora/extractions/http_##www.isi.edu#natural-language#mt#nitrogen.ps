URL: http://www.isi.edu/natural-language/mt/nitrogen.ps
Refering-URL: http://www.isi.edu/natural-language/mt/
Root-URL: http://www.isi.edu
Email: ilangkil@isi.edu and knight@isi.edu  
Title: Generating Word Lattices from Abstract Meaning Representation  
Author: Irene Langkilde and Kevin Knight 
Address: Marina del Rey, CA 90292  
Affiliation: Information Sciences Institute University of Southern California  
Abstract: Large-scale generation of natural language requires an abstract meaning representation and a mechanism for integrating immense amounts of lexical, morphological, grammatical, and conceptual knowledge. The availability of corpus-based statistical knowledge motivates the invention of a new style of generation in which word lattices compactly encode many possible sentence renderings and a statistical extractor chooses the best ones. The focus of generation thus shifts to how word lattices can be generated from abstract meaning representation. This paper presents a flexible meaning representation scheme and generation mechanism. It includes an efficient generation algorithm and grammar formalism that maps from a meaning representation to a lattice. This mapping is flexible enough to allow meaning representation along a continuum of semantic depth. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Elhadad, M., and Robin, J. </author> <year> 1992. </year> <title> Controlling content realization with functional unification grammars. </title> <editor> In Dale, R.; Hovy, E.; Roesner, D.; and Stock, O., eds., </editor> <booktitle> Aspects of Automated Natural Language Generation. </booktitle> <publisher> Springler Verlag. </publisher> <pages> 89-104. </pages>
Reference-contexts: This mode of operation can unnecessarily duplicate work at run time, unless sophisticated control directives are included in the search engine <ref> (Elhadad & Robin 1992) </ref>. In contrast, in bottom-up parsing and in our generation algorithm, a special data structure (a chart or a lattice respectively) is used to efficiently encode multiple analyses, and to allow structure sharing.
Reference: <author> Elhadad, M. </author> <year> 1993a. </year> <title> FUF: The universal unifier|user manual, version 5.2. </title> <type> Technical Report CUCS-038-91, </type> <institution> Columbia University. </institution>
Reference: <author> Elhadad, M. </author> <year> 1993b. </year> <title> Using Argumentation to Control Lexical Choice: A Unification-Based Implementation. </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science Department, Columbia University, </institution> <address> New York. </address>
Reference-contexts: In top-down parsing, backtracking is employed to exhaustively examine the space of possible alternatives. Similarly, traditional control mechanisms in generation operate top-down, either deterministically (Meteer et al. 1987; Tomita & Nyberg 1988; Penman 1989) or by backtracking to previous choice points <ref> (Elhadad 1993b) </ref>. This mode of operation can unnecessarily duplicate work at run time, unless sophisticated control directives are included in the search engine (Elhadad & Robin 1992).
Reference: <author> Grishman, R.; Macleod, C.; and Meyers, A. </author> <year> 1994. </year> <title> Comlex syntax: Building a computational lexicon. </title> <booktitle> In Proc. Coling. </booktitle>
Reference-contexts: These cached lattices are reused for each role/syntactic-category reference in a grammar specification. We are currently expanding our syntactic categories and features, e.g., to those available in COMLEX <ref> (Grishman, Macleod, & Meyers 1994) </ref>, but we do not anticipate an increase in matching cost. In addition to improving the coverage and accuracy of the grammar, we plan to produce lattices that include not only words, but also intended parts of speech.
Reference: <author> Kay, M. </author> <year> 1996. </year> <title> Chart generation. </title> <booktitle> In Proc. ACL. </booktitle>
Reference-contexts: In contrast, in bottom-up parsing and in our generation algorithm, a special data structure (a chart or a lattice respectively) is used to efficiently encode multiple analyses, and to allow structure sharing. Recently, <ref> (Kay 1996) </ref> has explored a bottom-up approach to generation as well, using a chart rather than a word lattice. Our generation is quite efficient; its matching is much less expensive than graph unification, and it caches the lattices generated for each sub-AMR.
Reference: <author> Knight, K., and Hatzivassiloglou, V. </author> <year> 1995. </year> <title> Two-level, many-paths generation. </title> <booktitle> In Proc. ACL. </booktitle>
Reference-contexts: Introduction Large-scale natural language generation (NLG) faces immense knowledge acquisition problems, as does any large-scale AI enterprise. NLG operating on a scale of 200,000 entities (concepts, relations, and words) requires large and sophisticated lexicons, grammars, ontologies, collocation lists, and morphological tables. <ref> (Knight & Hatzivassiloglou 1995) </ref> suggested overcoming this knowledge acquisition bottleneck in NLG by tapping the vast knowledge inherent in English text corpora. Corpus-based knowledge such as n-gram statistical frequencies can be used to help sort out good sentences from bad ones.
Reference: <author> Knight, K., and Luk, S. K. </author> <year> 1994. </year> <title> Building a large-scale knowledge base for machine translation. </title> <booktitle> In Proc. </booktitle> <publisher> AAAI. </publisher>
Reference-contexts: We also present a technique that adds powerful flexibility to the grammar formalism. We finish with a discussion of our generation system and comparison to other work. Abstract Meaning Representation The AMR language is composed of concepts from the SENSUS knowledge base <ref> (Knight & Luk 1994) </ref>, including all of WordNet 1.5 (Miller 1990), and keywords relating these concepts to each other. 1 An AMR is a labeled directed graph, or feature structure, written using the syntax of the PENMAN Sentence Plan Language (Penman 1989).
Reference: <author> Meteer, M. W.; McDonald, D. D.; Anderson, S. D.; Forster, D.; Gay, L. S.; Huettner, A. K.; and Sibun, P. </author> <year> 1987. </year> <title> Mumble-86: Design and implementation. </title> <type> Technical Report COINS 87-87, </type> <institution> University of Mas-sachussets at Amherst, </institution> <address> Ahmerst, Ma. </address>
Reference: <author> Miller, G. </author> <year> 1990. </year> <title> Wordnet: An on-line lexical database. </title> <journal> International Journal of Lexicography 3(4). </journal> <volume> Penman. </volume> <year> 1989. </year> <title> The Penman documentation. </title> <type> Techni--cal report, </type> <institution> USC/Information Sciences Institute. </institution>
Reference-contexts: We finish with a discussion of our generation system and comparison to other work. Abstract Meaning Representation The AMR language is composed of concepts from the SENSUS knowledge base (Knight & Luk 1994), including all of WordNet 1.5 <ref> (Miller 1990) </ref>, and keywords relating these concepts to each other. 1 An AMR is a labeled directed graph, or feature structure, written using the syntax of the PENMAN Sentence Plan Language (Penman 1989).
Reference: <author> Shieber, S. M.; van Noord, G.; Moore, R. C.; and Pereira, F. C. N. </author> <year> 1989. </year> <title> A semantic-head-driven generation algorithm for unification based formalisms. </title> <booktitle> In Proc. ACL, </booktitle> <pages> 7-17. </pages>
Reference: <author> Tomita, M., and Nyberg, E. </author> <year> 1988. </year> <title> The GenKit and Transformation Kit User's Guide. </title> <type> Technical Report CMU-CMT-88-MEMO, </type> <institution> Center for Machine Translation, Carnegie Mellon University. </institution>
References-found: 11

