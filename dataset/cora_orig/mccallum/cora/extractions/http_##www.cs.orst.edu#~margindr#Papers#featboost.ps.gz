URL: http://www.cs.orst.edu/~margindr/Papers/featboost.ps.gz
Refering-URL: http://www.cs.orst.edu/~margindr/Data/publications.html
Root-URL: 
Email: margindr@cs.orst.edu  
Title: LEARNING BY USING DYNAMIC FEATURE COMBINATION AND SELECTION  
Author: DRAGOS D. MARGINEANTU 
Note: on different features of the underlying data.  
Address: 303 Dearborn Hall  Corvallis, OR 97331-3202, USA  
Affiliation: Department of Computer Science  Oregon State University  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. </author> <year> (1990). </year> <title> Learning from hints in neural networks. </title> <journal> Journal of Complexity, </journal> <volume> 6, </volume> <pages> 192-198. </pages>
Reference: <author> Breiman, L. </author> <year> (1996). </year> <title> Bias, variance, and arcing classi-fiers. </title> <type> Tech. rep., </type> <institution> Department of Statistics, University of California, Berkeley. </institution>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Brodley, C. E., & Utgoff, P. E. </author> <year> (1995). </year> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19 (1), </volume> <pages> 45-77. </pages>
Reference: <author> Cherkauer, K. J. </author> <year> (1996). </year> <title> Human expert-level per-formance on a scientific image analysis task by a system using combined artificial neural net-works. </title> <editor> In Chan, P. (Ed.), </editor> <booktitle> Working Notes of the AAAI Workshop on Integrating Multiple Learned Models, </booktitle> <pages> pp. 15-21. </pages>
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1991). </year> <title> Errorcorrecting output codes: A general method for improving multiclass inductive learning pro-grams. </title> <booktitle> In Proc. of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 572-577. </pages> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: Based on how the hypotheses are build, we could classify the ensembles techniques into five groups: - subsampling the training examples (Breiman, 1996; Freund & Schapire, 1995); manipulating the output targets <ref> (Dietterich & Bakiri, 1991) </ref>; manipulating the input features (Cherkauer, 1996; Tumer & Ghosh, 1996); injecting randomness into the learning algorithm (Parmanto, Munro, & Doyle, 1996); algorithm specific methods (Abu-Mostafa, 1990; Rosen, 1996; Opitz & Shavlik, 1996).
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1995). </year> <title> A decisiontheoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Proceedings ECML-95 San Francisco, </booktitle> <address> CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This means that, for data with k features, p has a value of p = 4 fi 2 . Each step, the weights of the features are adjusted according to the accuracy of the hypothesis. By using weights for voting the final hypothesis, FeatBoost resembles adaptive boosting <ref> (Freund & Schapire, 1995) </ref>. The main difference is that FeatBoost doesn't attach weights to the instances, but instead, attaches weights to the different feature spaces where the classifiers operate, and then outputs a voted hypothesis.
Reference: <author> Ittner, A., & Schlosser, M. </author> <year> (1996). </year> <title> Non-linear decision trees. </title> <booktitle> In Proceedings of the International Conference in Machine Learning San Francisco, </booktitle> <address> CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: partitioning of the feature space) (Breiman et al., 1984; Utgoff & Brodley, 1991; Murthy, Kasif, & Salzberg, 1993; Brodley & Utgoff, 1995); testing non-linear combinations of features at each internal node of the decision tree (this approach performs a partitioning of the feature space in the form of curved hypersurfaces) <ref> (Ittner & Schlosser, 1996) </ref>. These approaches have the disadvantage that they grow a single decision tree which sometimes, for real applications, leads to a low accuracy of prediction due to the complicated form of the decision boundaries.
Reference: <author> Merz, C. J., & Murphy, P. M. </author> <year> (1996). </year> <title> UCI repository of machine learning databases. </title> <type> Tech. rep., </type> <institution> U.C. </institution> <address> Irvine, Irvine, CA. [http://www.ics.uci.edu/~ mlearn/MLRepository.html]. </address>
Reference-contexts: FeatBoost is practically voting among linear and non-linear decision trees. 3 Experiments and results We tested FeatBoost on eight real application datasets from the UCI ML Repository <ref> (Merz & Murphy, 1996) </ref>, Table 1: The experimental results. The table presents the errors of the different learning methods in percents.
Reference: <author> Murthy, S., Kasif, S., & Salzberg, S. </author> <year> (1993). </year> <title> Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence Cambridge, </booktitle> <address> MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Opitz, D. W., & Shavlik, J. W. </author> <year> (1996). </year> <title> Generating ac-curate and diverse members of a neural-network ensemble. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> 8, </volume> <pages> pp. </pages> <address> 535-541 Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1989). </year> <title> Two algorithms that learn dnf by discovering relevant features. </title> <booktitle> In Proceedings of the Sixth International Machine Learning Workshop. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There are several approaches that apply this idea: constructing new boolean features of the data by using logical operators <ref> (Pagallo & Haussler, 1989) </ref>; testing linear combinations of the features at each non-leaf node of the decision tree (this approach performs an oblique partitioning of the feature space) (Breiman et al., 1984; Utgoff & Brodley, 1991; Murthy, Kasif, & Salzberg, 1993; Brodley & Utgoff, 1995); testing non-linear combinations of features at
Reference: <author> Parmanto, B., Munro, P. W., & Doyle, H. R. </author> <year> (1996). </year> <title> Reducing variance of committee prediction with resampling techniques. </title> <journal> Connection Science, </journal> <volume> 8 (3-4), </volume> <pages> 405-426. </pages>
Reference-contexts: on how the hypotheses are build, we could classify the ensembles techniques into five groups: - subsampling the training examples (Breiman, 1996; Freund & Schapire, 1995); manipulating the output targets (Dietterich & Bakiri, 1991); manipulating the input features (Cherkauer, 1996; Tumer & Ghosh, 1996); injecting randomness into the learning algorithm <ref> (Parmanto, Munro, & Doyle, 1996) </ref>; algorithm specific methods (Abu-Mostafa, 1990; Rosen, 1996; Opitz & Shavlik, 1996).
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1). </volume>
Reference-contexts: Unseen data is labeled with the value of the leaf node reached by following the path in the tree according to the feature values of the data. Initially, the decision trees were used for classification in domains with symbolic-valued features <ref> (Quinlan, 1986) </ref>, and later their use was extended to data with numeric features (Quinlan, 1993) where the tests at the internal leaves are of the form x i &gt; C (x i is a feature, and C is a constant).
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Initially, the decision trees were used for classification in domains with symbolic-valued features (Quinlan, 1986), and later their use was extended to data with numeric features <ref> (Quinlan, 1993) </ref> where the tests at the internal leaves are of the form x i &gt; C (x i is a feature, and C is a constant). In these approaches, only one feature is tested at the level of an internal node, thus we may call them univariate (axis-parallel). <p> 7.91 glass 31.82 20.36 27.13 19.11 heart-c 23.12 21.85 22.73 21.25 iris 4.96 6.23 5.21 4.05 labor 19.12 14.21 17.79 15.32 lympho 22.03 16.92 22.38 16.59 xd6 18.29 11.70 20.04 12.21 and two synthetic dataset (expr, and xd6), and compared them with the performance of C4.5 (axis-parallel decision tree classifier <ref> (Quinlan, 1993) </ref>), AdaBoosted C4.5 (adaptive boosting using C4.5 as weak learner), NDT (non-linear decison tree classifier). In our experiments, we employed C4.5 as decision tree learner, and assigned T the value of 50. The results are shown in table 1.
Reference: <author> Rosen, B. E. </author> <year> (1996). </year> <title> Ensemble learning using decorrelated neural networks. </title> <journal> Connection Science, </journal> <volume> 8 (34), </volume> <pages> 373-384. </pages>
Reference: <author> Tumer, K., & Ghosh, J. </author> <year> (1996). </year> <title> Error correlation and error reduction in ensemble classifiers. </title> <journal> Connection Science, </journal> <volume> 8 (3-4), </volume> <pages> 385-404. </pages>
Reference: <author> Utgoff, P. E., & Brodley, C. E. </author> <year> (1991). </year> <title> Linear ma-chine decision trees. </title> <type> Tech. rep., </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
References-found: 18

