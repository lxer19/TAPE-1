URL: http://www.cs.gatech.edu/computing/Database/students/vibby/papers/scalable-arch.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/Database/students/vibby/vibby.html
Root-URL: 
Title: A Scalable Sharing Architecture for a Parallel Database System  
Author: Vibby Gottemukkala Edward Omiecinski Umakishore Ramachandran 
Address: Atlanta, GA 30332  
Affiliation: College of Computing Georgia Institute of Technology,  
Abstract: Exploiting parallelism is a key to building high-performance database systems. Several approaches to building database systems that support both inter- and intra-query parallelism have been proposed. These approaches can be broadly classified as either Shared Nothing (SN) or Shared Everything (SE). Although the SN approach is highly scalable, it requires complex data partitioning and tuning to achieve good performance whereas the SE approach suffers from non-scalability. We propose a scalable sharing approach which combines the advantages of both SN and SE. We propose a comprehensive database architecture that includes the underlying hardware, and data partitioning and scheduling strategies, to promote scalable sharing. We analyze the performance and scalability of our approach and compare with that of a SN system. We find that for a variety of workloads and data skew our approach performs and scales at least as well as a SN system that uses the best possible data partitioning strategy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Balakrishnan, R. Jain, and C. S. Raghavendra. </author> <title> On array storage for conflict-free memory access for parallel processors. </title> <booktitle> In Proceedings of the In ternational Conference on Parallel Processing, </booktitle> <pages> pages 103-107, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: To overcome this deficiency of the diagonal assignment strategy we could use the Magic Squares strategy which guarantees that even the diagonal fragments get assigned to different processors <ref> [1] </ref>. This property of the Magic Squares strategy allows it to trivially handle the presence of skew due to correlation between attributes. In the case of more general data skew, where the distri bution of tuples has no logical implications, the diagonal and Magic Squares strategies may be equivalent.
Reference: [2] <author> M. Bellew, M. Hsu, and V. Tam. </author> <title> Update propagation in distributed mem ory hierarchy. </title> <booktitle> In Proceedings of the 6th International Conference on Data Engineering, </booktitle> <pages> pages 521-28, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The SN and SE approaches represent the extremes of the spectrum of possible approaches. We propose a Distributed Shared Memory (DSM) based architecture that provides the flexibility of a SE system and the scalability of a SN system (see Fig. 1). In <ref> [2] </ref>, the authors present a DSM based database architecture. However, the thrust of their work was efficient coherence maintenance and concurrency control. DSM was also used in [25] to enhance join strategies to efficiently handle data skew.
Reference: [3] <author> B. Bergsten, M. Couprie, and P. Valduriez. </author> <title> Prototyping DBS3, a shared memory parallel database system. </title> <booktitle> In Proceedings of the 1st International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 226-234, </pages> <year> 1991. </year>
Reference-contexts: These include both SN systems (e.g. BUBBA and GAMMA) [4, 7] and SE systems (e.g. XPRS and DBS3) <ref> [27, 3] </ref>. SN systems are typically built by interconnecting nodes of processor/disk pairs. Data is split into disjoint sets and assigned to different nodes in the system.
Reference: [4] <author> H. Boral, W. Alexander, L. Clay, G. Copeland, S. Danforth, M. Franklin, B. Hart, M. Smith, and P. Valduriez. </author> <title> Prototyping Bubba, a highly parallel database system. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 4-23, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In the remainder of this section we will describe previous work related to each of these components and the innovations and extensions we propose. 2.1 Architecture Several parallel database architectures have been proposed and built. These include both SN systems (e.g. BUBBA and GAMMA) <ref> [4, 7] </ref> and SE systems (e.g. XPRS and DBS3) [27, 3]. SN systems are typically built by interconnecting nodes of processor/disk pairs. Data is split into disjoint sets and assigned to different nodes in the system. <p> Logical partitioning divides a relation into a collection of disjoint subsets of tuples. Physical partitioning maps these subsets to processors. 2.2.1 Logical Partitioning A relation can be declustered by applying a partitioning function (e.g. hash, range) to an attribute to decide the mapping between a tuple and a fragment <ref> [7, 4] </ref>. However, when a single attribute is used as the basis for declustering, only queries that involve the declus-tering attribute gain the full benefits of the available parallelism. This problem can be overcome by using multi-attribute declustering [10, 16].
Reference: [5] <author> J. B. Carter, J. K. Bennet, and W. Zwaenepoel. </author> <title> Implementation and per formance of Munin. </title> <booktitle> In Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In the SE approach the global accessibility of data allows uniform utilization of resources. However, the uniformity of a SE system makes communication a bottleneck which hinders the scalability of such a system. Distributed Shared Memory (DSM) <ref> [17, 22, 5] </ref> can be used to overcome the drawbacks of both SN and SE. In a DSM system, memory is physically distributed among the processors. When data is partitioned among processors a DSM system essentially behaves as a SN system. <p> The architecture provides shared-memory 1 That is to say that more data repositories (e.g. disks) can be added without having to add processors and vice versa. in hardware and not as a software abstraction built on top of a message-passing environment such as those in <ref> [17, 22, 5] </ref>. The reason for choosing a hardware implementation of DSM, despite the ease of implementation of the software-based DSM, is the higher cost of sharing in a software-based DSM. Our design choice is justified by the results in Section 4.
Reference: [6] <author> D. DeWitt and J. Gray. </author> <title> Parallel database systems: The future of high performance database systems. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 85-98, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In Section 4 we present the results of our simulation experiments and discuss their implications. Concluding remarks are given in Section 5. 2 Overview Our objective is to build a database system that captures the advantages of both the SN and SE approaches. From the discussions in the literature <ref> [26, 8, 6] </ref> we know that the main drawback to sharing is scalability. We identify the following simple guidelines to building a scalable system: 1. Eliminate central resources that can become bot tlenecks. 2. Minimize communication and thus avoid making the interconnect a bottleneck. 3. <p> The effect of the per relation degree of parallelism on transaction response time. 2. The effect of skew on the scheduling strategies (load balancing vs no load balancing). 3. Scalability of the different schemes, both in terms of scaleup and speedup <ref> [6] </ref>. 4. The effect of the cost of sharing on the SS system. 5. The effectiveness of the assignment strategies. We have developed a process-oriented simulation model for our experiments based on CSIM [24]. The simulation model consists of a transaction generator, transactions, processors and work queues (see Figure 3).
Reference: [7] <author> D. J. DeWitt, S. Ghandeharizadeh, D. A. Schneider, A. Bricker, H. Hsiao, and R. Rasmussen. </author> <title> The Gamma Database Machine Project. </title> <journal> IEEE Transac tions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 44-61, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In the remainder of this section we will describe previous work related to each of these components and the innovations and extensions we propose. 2.1 Architecture Several parallel database architectures have been proposed and built. These include both SN systems (e.g. BUBBA and GAMMA) <ref> [4, 7] </ref> and SE systems (e.g. XPRS and DBS3) [27, 3]. SN systems are typically built by interconnecting nodes of processor/disk pairs. Data is split into disjoint sets and assigned to different nodes in the system. <p> Logical partitioning divides a relation into a collection of disjoint subsets of tuples. Physical partitioning maps these subsets to processors. 2.2.1 Logical Partitioning A relation can be declustered by applying a partitioning function (e.g. hash, range) to an attribute to decide the mapping between a tuple and a fragment <ref> [7, 4] </ref>. However, when a single attribute is used as the basis for declustering, only queries that involve the declus-tering attribute gain the full benefits of the available parallelism. This problem can be overcome by using multi-attribute declustering [10, 16].
Reference: [8] <author> D. J. DeWitt and J. Gray. </author> <title> Parallel database systems: The future of database processing or a passing fad? SIGMOD RECORD, </title> <booktitle> 19(4) </booktitle> <pages> 104-112, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: In Section 4 we present the results of our simulation experiments and discuss their implications. Concluding remarks are given in Section 5. 2 Overview Our objective is to build a database system that captures the advantages of both the SN and SE approaches. From the discussions in the literature <ref> [26, 8, 6] </ref> we know that the main drawback to sharing is scalability. We identify the following simple guidelines to building a scalable system: 1. Eliminate central resources that can become bot tlenecks. 2. Minimize communication and thus avoid making the interconnect a bottleneck. 3.
Reference: [9] <author> S. Ghandeharizadeh and D. J. DeWitt. </author> <title> MAGIC amulti-attribute declus tering mechanism for multiprocessor database machines. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, University of Southern California, </institution> <address> Los Angeles, </address> <year> 1992. </year>
Reference-contexts: Furthermore, our strategy greatly simplifies the process of assigning fragments to nodes in the system. Another problem in physical partitioning is the presence of data skew. In SN systems the basic technique for skew avoidance is to tune the assignment of fragments to processors <ref> [14, 20, 9] </ref> such that each node gets roughly the same number of fragments. Typically, these strategies are static in nature. For example, in [9] the authors present a heuristic to avoid imbalances in the amount of data assigned to different processors. <p> In SN systems the basic technique for skew avoidance is to tune the assignment of fragments to processors [14, 20, 9] such that each node gets roughly the same number of fragments. Typically, these strategies are static in nature. For example, in <ref> [9] </ref> the authors present a heuristic to avoid imbalances in the amount of data assigned to different processors. This strategy, however, works only for data skew resulting from high correlation between declustering attributes.
Reference: [10] <author> S. Ghandeharizadeh, D. J. DeWitt, and W. Qureshi. </author> <title> A performance ananl ysis of alternative multi-attribute declustering strategies. </title> <booktitle> In Proceedings of the 1992 ACM SIGMOD, </booktitle> <pages> pages 29-38, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: However, when a single attribute is used as the basis for declustering, only queries that involve the declus-tering attribute gain the full benefits of the available parallelism. This problem can be overcome by using multi-attribute declustering <ref> [10, 16] </ref>. First, all the attributes to be used in the declustering are identified. Then the domain of values for each such attribute is divided into a set of sub-domains and tuples are assigned to sub-domains based on their attribute values. <p> The number of sub-domains along each dimension is dependent on the degree of parallelism that is desired. For example, the MAGIC declustering method <ref> [10] </ref> applies range partitioning to each of the attributes involved in the declustering to create a multi-attribute grid structure. <p> For example, the MAGIC declustering method [10] applies range partitioning to each of the attributes involved in the declustering to create a multi-attribute grid structure. The desired degree of parallelism, P , can be determined when the following cost model is assumed <ref> [28, 10] </ref>: R = P P opt = c t T ave (2) where R is the response time for a query, c t is the cost of processing a tuple, T ave is the average number of tuples accessed by a query and c p is the overhead per unit <p> However, in the case where the number of fragments is greater than the number of processors this strategy will no longer work and a more complex assignment strategy is required. In <ref> [10] </ref> the authors present a set of complex heuristics to assign the fragments to processors. The reasons for the complexity of the strategy are discussed in detail in [11]. <p> P fi P in a row-major order then, the members of the ith slice can be determined using the equation: slice i = ((i 1)P + 1 + j (P + 1)) mod n (3) 8j 2 [0; P ); 1 i P ; n = P fi P In <ref> [10] </ref> the number of ranges in each dimension (N i ) is determined using the frequency with which each dimension is accessed and the degree of parallelism for each dimension (P i ) serves as a lower bound for this number.
Reference: [11] <author> V. Gottemukkala, E. Omiecienski, and U. Ramachandran. </author> <title> A Scalable Shar ing Architecture for a Parallel Database System. </title> <type> Technical Report GIT-CC 93-24, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: In [10] the authors present a set of complex heuristics to assign the fragments to processors. The reasons for the complexity of the strategy are discussed in detail in <ref> [11] </ref>. We propose a simple algorithm, the diagonal assignment strategy, for assigning fragments to processors that does not make use of any heuristics.
Reference: [12] <author> D. B. Gustavson. </author> <title> The Scalable Coherent Interface and Related Standards Projects. </title> <journal> IEEE Micro, </journal> <volume> 12(1), </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: The DSM architecture differs from the SE architecture in that all nodes are not identical with respect to the cost of accessing a particular data item. The architecture we propose is an extension to existing scalable, shared-memory multiprocessor architectures <ref> [15, 12] </ref>. The architecture provides shared-memory 1 That is to say that more data repositories (e.g. disks) can be added without having to add processors and vice versa. in hardware and not as a software abstraction built on top of a message-passing environment such as those in [17, 22, 5].
Reference: [13] <author> Y. Hirano, T. Satoh, U. Inoue, and K. Teranaka. </author> <title> Load balancing algorithms for parallel database processing on shared memory multiprocessors. </title> <booktitle> In Pro ceedings of the 1st International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 210-217. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Since all processors are uniform in a SE system the sub-queries can be executed on any processor. Several strategies have been proposed to determine when and what a node chooses to process. For example, in <ref> [13] </ref> the authors present a processor-initiated load-balancing algorithm. In this approach, queries are decomposed into work-units and placed in a central queue. Each idle processor picks some number of work-units and does the required processing.
Reference: [14] <author> K. A. Hua and C. Lee. </author> <title> Handling data skew in multiprocessor database computer using partition tuning. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Databases, </booktitle> <pages> pages 525-535, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Furthermore, our strategy greatly simplifies the process of assigning fragments to nodes in the system. Another problem in physical partitioning is the presence of data skew. In SN systems the basic technique for skew avoidance is to tune the assignment of fragments to processors <ref> [14, 20, 9] </ref> such that each node gets roughly the same number of fragments. Typically, these strategies are static in nature. For example, in [9] the authors present a heuristic to avoid imbalances in the amount of data assigned to different processors.
Reference: [15] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: The DSM architecture differs from the SE architecture in that all nodes are not identical with respect to the cost of accessing a particular data item. The architecture we propose is an extension to existing scalable, shared-memory multiprocessor architectures <ref> [15, 12] </ref>. The architecture provides shared-memory 1 That is to say that more data repositories (e.g. disks) can be added without having to add processors and vice versa. in hardware and not as a software abstraction built on top of a message-passing environment such as those in [17, 22, 5].
Reference: [16] <author> J. Li, J. Srivastava, and D. Rotem. CMD: </author> <title> A multidimensional declustering method for parallel database systems. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 3-14, </pages> <year> 1992. </year>
Reference-contexts: However, when a single attribute is used as the basis for declustering, only queries that involve the declus-tering attribute gain the full benefits of the available parallelism. This problem can be overcome by using multi-attribute declustering <ref> [10, 16] </ref>. First, all the attributes to be used in the declustering are identified. Then the domain of values for each such attribute is divided into a set of sub-domains and tuples are assigned to sub-domains based on their attribute values.
Reference: [17] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM TOCS, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: In the SE approach the global accessibility of data allows uniform utilization of resources. However, the uniformity of a SE system makes communication a bottleneck which hinders the scalability of such a system. Distributed Shared Memory (DSM) <ref> [17, 22, 5] </ref> can be used to overcome the drawbacks of both SN and SE. In a DSM system, memory is physically distributed among the processors. When data is partitioned among processors a DSM system essentially behaves as a SN system. <p> The architecture provides shared-memory 1 That is to say that more data repositories (e.g. disks) can be added without having to add processors and vice versa. in hardware and not as a software abstraction built on top of a message-passing environment such as those in <ref> [17, 22, 5] </ref>. The reason for choosing a hardware implementation of DSM, despite the ease of implementation of the software-based DSM, is the higher cost of sharing in a software-based DSM. Our design choice is justified by the results in Section 4.
Reference: [18] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Load balancing vs. locality management in shared-memory multiprocessors. </title> <booktitle> In Proceedings of the International Con ference on Parallel Processing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: For our experiments we use the diagonal assignment strategy because of our assumptions about the data skew (see Section 3). 2.3 Scheduling Following the classification of scheduling given in <ref> [18] </ref> the scheduling strategies in SN and SE systems can be classified as data-affinity and load-balancing scheduling, respectively. In SN systems, queries are decomposed into sub-queries based on the the degree of decluster-ing of the data being accessed. <p> Our approach to scheduling decomposes queries into sub-queries and is processor-initiated. However, our approach takes data-affinity into consideration and thus is similar to the scheduling approach advocated in <ref> [18] </ref>. Each node has a private work queue, similar to a SN scheduling strategy. However, any node can access any other node's work queue through DSM. When a query is submitted to the DBMS the declustering directory is consulted to determine the fragments that need to be accessed.
Reference: [19] <author> E. Omiecinski. </author> <title> Performance analysis of a load-balancing hash -join algo rithm for a shared memory multiprocessor. </title> <booktitle> In Proceedings of the 17th In ternational Conference on Very Large Databases, </booktitle> <pages> pages 375-85, </pages> <year> 1991. </year>
Reference-contexts: The objective of the algorithm is to optimize in two dimensions, namely, load balancing time and idle time. To achieve this objective the number of work-units that are acquired by a processor are reduced in each iteration. In <ref> [19] </ref> a load-balancing scheme is proposed for hash-based join processing in the presence of data skew. The strategy takes advantage of two-phase join processing. In the first phase tuples are assigned to buckets. The size of the buckets can vary widely due to data skew.
Reference: [20] <author> E. Omiecinski and E. Lin. </author> <title> The adaptive hash join algorithm for a Hypercube Multicomputer. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3) </volume> <pages> 334-49, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Furthermore, our strategy greatly simplifies the process of assigning fragments to nodes in the system. Another problem in physical partitioning is the presence of data skew. In SN systems the basic technique for skew avoidance is to tune the assignment of fragments to processors <ref> [14, 20, 9] </ref> such that each node gets roughly the same number of fragments. Typically, these strategies are static in nature. For example, in [9] the authors present a heuristic to avoid imbalances in the amount of data assigned to different processors.
Reference: [21] <author> H. Pirahesh, C. Mohan, J. Cheng, T. S. Liu, and P. Selinger. </author> <title> Parallelism in relational database systems: </title> <booktitle> Architectural issues and design approaches. In Proceedings of the 2nd International Symposium on Databases in Parallel and Distributed Systems, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The rise in the complexity of databases in terms of physical size, query complexity and query volume demands enormous amounts of processing power which can only be satisfied using parallel systems <ref> [21] </ref>. An area of debate in parallel database design is the Shared Nothing (SN) approach versus the Shared Everything (SE) approach. Both approaches have their pros and cons. The SN approach lends itself well to large, scalable systems but load-balancing requires complex data partitioning and assignment strategies.
Reference: [22] <author> U. Ramachandran and M. Y. A. Khalidi. </author> <title> An implementation of distributed shared memory. </title> <journal> Software Practice & Experience, </journal> <volume> 21(5) </volume> <pages> 443-64, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In the SE approach the global accessibility of data allows uniform utilization of resources. However, the uniformity of a SE system makes communication a bottleneck which hinders the scalability of such a system. Distributed Shared Memory (DSM) <ref> [17, 22, 5] </ref> can be used to overcome the drawbacks of both SN and SE. In a DSM system, memory is physically distributed among the processors. When data is partitioned among processors a DSM system essentially behaves as a SN system. <p> The architecture provides shared-memory 1 That is to say that more data repositories (e.g. disks) can be added without having to add processors and vice versa. in hardware and not as a software abstraction built on top of a message-passing environment such as those in <ref> [17, 22, 5] </ref>. The reason for choosing a hardware implementation of DSM, despite the ease of implementation of the software-based DSM, is the higher cost of sharing in a software-based DSM. Our design choice is justified by the results in Section 4.
Reference: [23] <author> U. Ramachandran, G. Shah, S. Ravikumar, and J. Muthukumarasamy. </author> <title> Scal ability Study of the KSR-1. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: The architecture is scaled, not by adding more nodes to a single ring, but by hierarchically coupling such rings, i.e. rings of nodes are connected through a higher level ring (see Figure 1). The scalability of such an architecture has been studied in <ref> [23] </ref>. Before we discuss how to build a scalable database system given the proposed hardware architecture, we have to recognize that the SN mode of functioning is acceptable as long as all the nodes are busy processing assigned tasks that access data local to the nodes.
Reference: [24] <author> H. Schwetman. </author> <title> CSIM Users' Guide, </title> <month> March </month> <year> 1990. </year>
Reference-contexts: Scalability of the different schemes, both in terms of scaleup and speedup [6]. 4. The effect of the cost of sharing on the SS system. 5. The effectiveness of the assignment strategies. We have developed a process-oriented simulation model for our experiments based on CSIM <ref> [24] </ref>. The simulation model consists of a transaction generator, transactions, processors and work queues (see Figure 3). The transaction generator process generates transactions with a frequency that models the inter-arrival time between transactions.
Reference: [25] <author> A. Shatdal and J. F. Naughton. </author> <title> Using shared virtual memory for parallel join processing. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD, </booktitle> <pages> pages 119-128, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In [2], the authors present a DSM based database architecture. However, the thrust of their work was efficient coherence maintenance and concurrency control. DSM was also used in <ref> [25] </ref> to enhance join strategies to efficiently handle data skew. However, these studies did not examine the impact of the architectural assumptions on the scalability of the database system. The DSM architecture we propose consists of interconnected processors, each with a large amount of main-memory and a disk. <p> The reason for this is two fold. First, the major performance differences between joins in SN and SS would show up in the `build' phase of join processing. This aspect of join performance is captured by the various select queries in the workload. Second, <ref> [25] </ref> shows how DSM can be exploited to eliminate load imbalances that could arise in the `join' phase due to skewed distribution of data by the build phase. 4 Results In this section we present the results of our experiments to determine the effects of declustering and skew, the scalability of
Reference: [26] <author> M. Stonebraker. </author> <title> The case for shared nothing. </title> <journal> IEEE Database Engineering, </journal> <volume> 9(1), </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: In Section 4 we present the results of our simulation experiments and discuss their implications. Concluding remarks are given in Section 5. 2 Overview Our objective is to build a database system that captures the advantages of both the SN and SE approaches. From the discussions in the literature <ref> [26, 8, 6] </ref> we know that the main drawback to sharing is scalability. We identify the following simple guidelines to building a scalable system: 1. Eliminate central resources that can become bot tlenecks. 2. Minimize communication and thus avoid making the interconnect a bottleneck. 3.
Reference: [27] <author> M. Stonebraker, R. Katz, D. Patterson, and J. Ousterhout. </author> <booktitle> The design of XPRS. In Proceedings of the 14th VLDB Conference, </booktitle> <pages> pages 318-330, </pages> <year> 1988. </year>
Reference-contexts: These include both SN systems (e.g. BUBBA and GAMMA) [4, 7] and SE systems (e.g. XPRS and DBS3) <ref> [27, 3] </ref>. SN systems are typically built by interconnecting nodes of processor/disk pairs. Data is split into disjoint sets and assigned to different nodes in the system.
Reference: [28] <author> A. N. Wilschut, J. Flokstra, and P. M. G. Apers. </author> <title> Parallelism in a main memory DBMS: </title> <booktitle> The performance of PRISMA/DB. In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 521-532, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: For example, the MAGIC declustering method [10] applies range partitioning to each of the attributes involved in the declustering to create a multi-attribute grid structure. The desired degree of parallelism, P , can be determined when the following cost model is assumed <ref> [28, 10] </ref>: R = P P opt = c t T ave (2) where R is the response time for a query, c t is the cost of processing a tuple, T ave is the average number of tuples accessed by a query and c p is the overhead per unit
References-found: 28

