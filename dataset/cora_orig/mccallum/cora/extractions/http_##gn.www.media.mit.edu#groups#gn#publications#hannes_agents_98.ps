URL: http://gn.www.media.mit.edu/groups/gn/publications/hannes_agents_98.ps
Refering-URL: http://gn.www.media.mit.edu/groups/gn/publications.html
Root-URL: http://www.media.mit.edu
Title: BodyChat: Autonomous Communicative Behaviors in Avatars  
Author: Hannes Hgni Vilhjlmsson Justine Cassell 
Address: 20 Ames Street Cambridge, MA 02139  20 Ames Street Cambridge, MA 02139  
Affiliation: MIT Media Laboratory  MIT Media Laboratory  
Note: In ACM Proceedings of the Second International Conference on Autonomous Agents, Minneapolis,  
Email: hannes@media.mit.edu  justine@media.mit.edu  
Phone: 1-617-253-7211  1-617-253-4899  
Date: May 9-13, 1998 (269-276)  
Abstract: 1. ABSTRACT 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, D.B., Barrus, J.W., Brogan, D., Casey M., McKeown, S., Sterns, I., Waters, R., Yerazunis, W. </author> <title> Diamond Park and Spline: A Social Virtual Reality System with 3D Animation, Spoken Interaction, and Runtime Modifiability. </title> <type> Technical Report at MERL, </type> <address> Cambridge, </address> <year> 1996. </year>
Reference: [2] <author> Argyle, M., Cook, M. </author> <title> Gaze and Mutual Gaze. </title> <publisher> Cambridge University Press, </publisher> <year> 1976. </year>
Reference-contexts: For example, when giving feedback one can avoid overlapping a partner by giving it over a secondary channel, such as by facial expression, while receiving information over the speech channel <ref> [2] </ref>. The channels can also work together, supplementing or complementing each other by emphasizing salient points [11][19], directing the listeners attention [13] or providing additional information or elaboration [17][8]. When multiple channels are employed in a conversation, we refer to it as being multimodal. <p> She looks back up at Paul when she is within 10 [for initiating a close salutation], meeting his gaze, smiling again ([14], 188; <ref> [2] </ref>, 113). Paul tilts his head to the side slightly and says Paul, as he offers Susan his hand, which she shakes lightly while facing him and replying Susan [close salutation] ([14], 188, 193). Then she steps a little to the side to face Paul at an angle ([14], 193; [2], <p> <ref> [2] </ref>, 113). Paul tilts his head to the side slightly and says Paul, as he offers Susan his hand, which she shakes lightly while facing him and replying Susan [close salutation] ([14], 188, 193). Then she steps a little to the side to face Paul at an angle ([14], 193; [2], 101). A conversation starts. During the conversation both Paul and Susan display appropriate gaze behavior, such as looking away when starting a long utterance ([14], 63; [2], 115; [11], 177; [24]), marking various syntactic events in their speech with appropriate facial expressions, such as raising their eyebrows while reciting a <p> Then she steps a little to the side to face Paul at an angle ([14], 193; <ref> [2] </ref>, 101). A conversation starts. During the conversation both Paul and Susan display appropriate gaze behavior, such as looking away when starting a long utterance ([14], 63; [2], 115; [11], 177; [24]), marking various syntactic events in their speech with appropriate facial expressions, such as raising their eyebrows while reciting a question or nodding and raising eyebrows on an emphasized word ([3]; [11], 177; [10]), giving feedback while listening in the form of nods, low mhms and eyebrow <p> reciting a question or nodding and raising eyebrows on an emphasized word ([3]; [11], 177; [10]), giving feedback while listening in the form of nods, low mhms and eyebrow action ([11], 187; [20]; [10]) and finally giving the floor to the other person using gaze ([14], 85; [11], 177; [3]; <ref> [2] </ref>, 118). Speakers choose conversational partners but do not choose to raise their eyebrows along with an emphasis word, or to look at the other person when giving over the floor. Yet we attend to these clues as listeners, and are thrown off by their absence.
Reference: [3] <author> Argyle, M., Ingham, R., Alkema, F., McCallin, M. </author> <title> The Different Functions of Gaze. </title> <address> Semiotica, </address> <year> 1973. </year>
Reference-contexts: while reciting a question or nodding and raising eyebrows on an emphasized word (<ref> [3] </ref>; [11], 177; [10]), giving feedback while listening in the form of nods, low mhms and eyebrow action ([11], 187; [20]; [10]) and finally giving the floor to the other person using gaze ([14], 85; [11], 177; [3]; [2], 118). Speakers choose conversational partners but do not choose to raise their eyebrows along with an emphasis word, or to look at the other person when giving over the floor. Yet we attend to these clues as listeners, and are thrown off by their absence.
Reference: [4] <author> Bates, J., Loyall, </author> <title> A.B., Reilley, W.S. Broad Agents. </title> <journal> SIGART Bulletin, </journal> <volume> 4 (2), </volume> <year> 1991. </year>
Reference-contexts: Real-time external control of animated autonomous actors has called for methods to direct animated behavior on a number of different levels such as in ALIVE [6] and in the OZ Project <ref> [4] </ref>. In this sense, the goals of BodyChat are similar, but the set of behaviors is different. Here we focus on those behaviors that accompany language. We have also introduced, for the first time, a distinction between conversational Phenomena and Communicative Behaviors. 6.
Reference: [5] <author> Benford, S., Bowers, J., Fahlen, L.E., Greenhalgh, C., Snowdon, D. </author> <title> User Embodiment in Collaborative Virtual Environments. </title> <booktitle> In Proceedings of CHI95, </booktitle> <volume> 242 249. </volume>
Reference-contexts: RELATED WORK Embodiment in Distributed Virtual Environments has been a research issue in systems such as MASSIVE at CRG Nottingham University, UK where various techniques and design issues have been proposed <ref> [5] </ref>. There it is made clear that involuntary facial expression and gesture are important but hard to capture. Avatar autonomy however is not suggested.
Reference: [6] <author> Blumberg, B. M., Galyean, T. A. </author> <title> MultiLevel Direction of Autonomous Creatures for Real-Time Virtual Environments. </title> <booktitle> Proceedings of SIGGRAPH 95. </booktitle>
Reference-contexts: However, automatically generating the appropriate communicative behaviors and synchronizing them with an actual conversation between users has not been addressed yet in these systems. Real-time external control of animated autonomous actors has called for methods to direct animated behavior on a number of different levels such as in ALIVE <ref> [6] </ref> and in the OZ Project [4]. In this sense, the goals of BodyChat are similar, but the set of behaviors is different. Here we focus on those behaviors that accompany language. We have also introduced, for the first time, a distinction between conversational Phenomena and Communicative Behaviors. 6.
Reference: [7] <author> Cary, M. S. </author> <title> The Role of Gaze in the Initiation of Conversation. </title> <journal> Social Psychology, </journal> <volume> 41(3), </volume> <year> 1978. </year>
Reference-contexts: Paul is standing by himself at a cocktail party, looking out for interesting people. Susan (unaquainted with Paul) walks by, mutual glances are exchanged, Paul nods smiling, Susan looks at Paul and smiles [distance salutation] ([14], 173; <ref> [7] </ref>, 269) Susan touches the hem of her shirt [grooming] as she dips her head, ceases to smile and approaches Paul ([14], 186, 177). She looks back up at Paul when she is within 10 [for initiating a close salutation], meeting his gaze, smiling again ([14], 188; [2], 113).
Reference: [8] <author> Cassell, J. </author> <title> (forthcoming). A Framework For Gesture Generation And Interpretation. </title> <editor> In R. Cipolla and A. Pentland (eds.), </editor> <title> Computer Vision in Human-Machine Interaction. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: We tend to take communicative behaviors such as gaze and head movements for granted, as their spontaneous nature and nonvoluntary fluid execution makes them easy to overlook when recalling a previous encounter <ref> [8] </ref>. This is a serious oversight when creating avatars or humanoid agents since emotion displays do not account for the majority of displays that occur in a human to human interaction [11]. 3.
Reference: [9] <author> Cassell, J., Pelachaud, C., Badler, N., Steedman, M., Achorn, B., Becket, T., Douville, B., Prevost, S., Stone, M. </author> <title> Animated Conversation: Rule-based Generation of Facial Expression, Gesture & Spoken Intonation for Multiple Conversational Agents. </title> <booktitle> Proceedings of SIGGRAPH 94. </booktitle>
Reference-contexts: Creating fully autonomous agents capable of natural multimodal interaction entails integrating speech, gesture and facial expression. By applying knowledge from discourse analysis and studies of social cognition, systems like Animated Conversation <ref> [9] </ref> and Gandalf [22] have been developed. Animated Conversation renders a graphical representation of two autonomous agents engaged in conversation. The systems dialogue planner generates the conversation and its accompanying communicative signals, based on the agents initial goals and knowledge.
Reference: [10] <author> Cassell, J., Stone, M., Douville, B., Prevost, S., Achorn, B., Steedman, M., Badler, N., Pelachaud, C. </author> <title> Modeling the Interaction between Speech and Gesture. </title> <booktitle> Proceedings of the Cognitive Science Society Annual Conference, </booktitle> <year> 1994. </year>
Reference-contexts: gaze behavior, such as looking away when starting a long utterance ([14], 63; [2], 115; [11], 177; [24]), marking various syntactic events in their speech with appropriate facial expressions, such as raising their eyebrows while reciting a question or nodding and raising eyebrows on an emphasized word ([3]; [11], 177; <ref> [10] </ref>), giving feedback while listening in the form of nods, low mhms and eyebrow action ([11], 187; [20]; [10]) and finally giving the floor to the other person using gaze ([14], 85; [11], 177; [3]; [2], 118). <p> marking various syntactic events in their speech with appropriate facial expressions, such as raising their eyebrows while reciting a question or nodding and raising eyebrows on an emphasized word ([3]; [11], 177; <ref> [10] </ref>), giving feedback while listening in the form of nods, low mhms and eyebrow action ([11], 187; [20]; [10]) and finally giving the floor to the other person using gaze ([14], 85; [11], 177; [3]; [2], 118). Speakers choose conversational partners but do not choose to raise their eyebrows along with an emphasis word, or to look at the other person when giving over the floor.
Reference: [11] <author> Chovil, N. </author> <title> Discourse-Oriented Facial Displays in Conversation. Research on Language and Social Interaction, </title> <booktitle> 25, </booktitle> <pages> 163-194, </pages> <year> 1992. </year>
Reference-contexts: This is a serious oversight when creating avatars or humanoid agents since emotion displays do not account for the majority of displays that occur in a human to human interaction <ref> [11] </ref>. 3. AUTOMATING AVATAR BEHAVIOR Many believe that employing trackers to map certain key parts of the users body or face onto the graphical representation will solve the problem of having to explicitly control the avatars every move. As the user moves, the avatar imitates the motion. <p> Then she steps a little to the side to face Paul at an angle ([14], 193; [2], 101). A conversation starts. During the conversation both Paul and Susan display appropriate gaze behavior, such as looking away when starting a long utterance ([14], 63; [2], 115; <ref> [11] </ref>, 177; [24]), marking various syntactic events in their speech with appropriate facial expressions, such as raising their eyebrows while reciting a question or nodding and raising eyebrows on an emphasized word ([3]; [11], 177; [10]), giving feedback while listening in the form of nods, low mhms and eyebrow action ([11], <p> display appropriate gaze behavior, such as looking away when starting a long utterance ([14], 63; [2], 115; <ref> [11] </ref>, 177; [24]), marking various syntactic events in their speech with appropriate facial expressions, such as raising their eyebrows while reciting a question or nodding and raising eyebrows on an emphasized word ([3]; [11], 177; [10]), giving feedback while listening in the form of nods, low mhms and eyebrow action ([11], 187; [20]; [10]) and finally giving the floor to the other person using gaze ([14], 85; [11], 177; [3]; [2], 118). <p> their eyebrows while reciting a question or nodding and raising eyebrows on an emphasized word ([3]; <ref> [11] </ref>, 177; [10]), giving feedback while listening in the form of nods, low mhms and eyebrow action ([11], 187; [20]; [10]) and finally giving the floor to the other person using gaze ([14], 85; [11], 177; [3]; [2], 118). Speakers choose conversational partners but do not choose to raise their eyebrows along with an emphasis word, or to look at the other person when giving over the floor. Yet we attend to these clues as listeners, and are thrown off by their absence. <p> This mapping is based on previous work in human communicative behavior, mainly <ref> [11] </ref> and [14]. Finally, each Communicative Behavior starts an animation engine that manipulates the corresponding avatar geometry in order change the visual appearance.
Reference: [12] <author> Donath, J. </author> <title> The Illustrated Conversation. </title> <booktitle> Multimedia Tools and Applications, </booktitle> <volume> 1, </volume> <pages> 79-88, </pages> <year> 1995. </year>
Reference-contexts: However these systems have not been able to naturally integrate the graphics with the communication that is taking place. Studies of human communicative behavior have seldom been considered in the design of believable avatars. Significant work includes Judith Donaths Collaboration-at-a-Glance <ref> [12] </ref>, where on-screen participants gaze direction changes to display their attention, and Microsofts Comic Chat [16], where illustrative comicstyle images are automatically generated from the interaction. In Collaboration-at-a-Glance the users lack a body and the system only implements a few functions of the head.
Reference: [13] <author> Goodwin, C. </author> <title> Gestures as a Resource for the Organization of Mutual Orientation. </title> <journal> Semiotica, </journal> <volume> 62(1/2), </volume> <year> 1986. </year>
Reference-contexts: The channels can also work together, supplementing or complementing each other by emphasizing salient points [11][19], directing the listeners attention <ref> [13] </ref> or providing additional information or elaboration [17][8]. When multiple channels are employed in a conversation, we refer to it as being multimodal. The current work focuses on gaze and communicative facial expression mainly because these are fundamental in establishing and maintaining a live link between participants in a conversation.
Reference: [14] <author> Kendon, A. </author> <title> Conducting Interaction: Patterns of behavior in focused encounters. </title> <publisher> Cambridge University Press. </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: This mapping is based on previous work in human communicative behavior, mainly [11] and <ref> [14] </ref>. Finally, each Communicative Behavior starts an animation engine that manipulates the corresponding avatar geometry in order change the visual appearance.
Reference: [15] <author> Kendon, A. </author> <title> The negotiation of context in faceto-face interaction. </title> <editor> In A. Duranti and C. Goodwin (eds.), </editor> <title> Rethinking context: language as interactive phenomenon. </title> <publisher> Cambridge University Press. </publisher> <address> NY, </address> <year> 1990. </year>
Reference: [16] <author> Kurlander, D., Skelly, T., Salesin, D. Comic Chat. </author> <booktitle> Proceedings of SIGGRAPH 96 </booktitle>
Reference-contexts: Studies of human communicative behavior have seldom been considered in the design of believable avatars. Significant work includes Judith Donaths Collaboration-at-a-Glance [12], where on-screen participants gaze direction changes to display their attention, and Microsofts Comic Chat <ref> [16] </ref>, where illustrative comicstyle images are automatically generated from the interaction. In Collaboration-at-a-Glance the users lack a body and the system only implements a few functions of the head.
Reference: [17] <author> McNeill, D. </author> <title> Hand and Mind: What Gestures Reveal about Thought. </title> <institution> University of Chicago, </institution> <year> 1992. </year>
Reference: [18] <author> Perlin, K., Goldberg, A. Improv: </author> <title> A System for Scripting Interactive Actors in Virtual Worlds. </title> <note> SIGGRAPH 1996 Course Notes #25. </note>
Reference-contexts: The real-time animation of lifelike 3D humanoid figures has been greatly improved in recent years. The Improv system <ref> [18] </ref> demonstrates a visually appealing humanoid animation and provides tools for scripting complex behaviors, ideal for agents as well as avatars. Similarly the Humanoid 2 project deals with virtual actors performing scripts as well as improvising role-related behavior [25].
Reference: [19] <author> Prevost, S. </author> <title> Modeling Contrast in the Generation and Synthesis of Spoken Language. </title> <booktitle> In Proceedings of ICSLP 96. </booktitle>
Reference: [20] <author> Schegloff, E. </author> <title> Sequencing in Conversational Openings. </title> <journal> American Anthropologist, </journal> <volume> 70, </volume> <pages> 1075-1095, </pages> <year> 1968. </year>
Reference-contexts: [24]), marking various syntactic events in their speech with appropriate facial expressions, such as raising their eyebrows while reciting a question or nodding and raising eyebrows on an emphasized word ([3]; [11], 177; [10]), giving feedback while listening in the form of nods, low mhms and eyebrow action ([11], 187; <ref> [20] </ref>; [10]) and finally giving the floor to the other person using gaze ([14], 85; [11], 177; [3]; [2], 118). Speakers choose conversational partners but do not choose to raise their eyebrows along with an emphasis word, or to look at the other person when giving over the floor.
Reference: [21] <author> Schegloff, E., Sacks, H. </author> <title> Opening up closings. </title> <journal> Semiotica, </journal> <volume> 8, </volume> <pages> 289-327, </pages> <year> 1973. </year>
Reference: [22] <author> Thrisson, K. R. </author> <title> Gandalf: An Embodied Humanoid Capable of Real-Time Multimodal Dialogue with People. </title> <booktitle> Proceedings of Agents'97, </booktitle> <pages> 536-537. </pages>
Reference-contexts: Creating fully autonomous agents capable of natural multimodal interaction entails integrating speech, gesture and facial expression. By applying knowledge from discourse analysis and studies of social cognition, systems like Animated Conversation [9] and Gandalf <ref> [22] </ref> have been developed. Animated Conversation renders a graphical representation of two autonomous agents engaged in conversation. The systems dialogue planner generates the conversation and its accompanying communicative signals, based on the agents initial goals and knowledge.
Reference: [23] <author> Thrisson, K.R., Cassell, J. </author> <title> Why Put an Agent in a Human Body: The Importance of Communicative Feedback in Human-Humanoid Dialogue. </title> <booktitle> (abstract) In Proceedings of Lifelike Computer Characters '96, Snowbird, Utah, </booktitle> <pages> 44-45. </pages>
Reference-contexts: However, lively emotional expression in interaction is in vain if mechanisms for establishing and maintaining mutual focus and attention are not in place <ref> [23] </ref>. We tend to take communicative behaviors such as gaze and head movements for granted, as their spontaneous nature and nonvoluntary fluid execution makes them easy to overlook when recalling a previous encounter [8].
Reference: [24] <author> Torres, O., Cassell, J., Prevost, S. </author> <title> Modeling Gaze Behavior as a Function of Discourse Structure. </title> <booktitle> In Proceedings of the First International Workshop on Human-Computer Conversations 1997. </booktitle> <address> Bellagio, Italy. </address>
Reference-contexts: Then she steps a little to the side to face Paul at an angle ([14], 193; [2], 101). A conversation starts. During the conversation both Paul and Susan display appropriate gaze behavior, such as looking away when starting a long utterance ([14], 63; [2], 115; [11], 177; <ref> [24] </ref>), marking various syntactic events in their speech with appropriate facial expressions, such as raising their eyebrows while reciting a question or nodding and raising eyebrows on an emphasized word ([3]; [11], 177; [10]), giving feedback while listening in the form of nods, low mhms and eyebrow action ([11], 187; [20];
Reference: [25] <author> Wavish, P., Connah, D. </author> <title> Virtual actors that can perform scripts and improvise roles. </title> <booktitle> Proceedings of Agents'97, </booktitle> <pages> 317-322. </pages>
Reference-contexts: The Improv system [18] demonstrates a visually appealing humanoid animation and provides tools for scripting complex behaviors, ideal for agents as well as avatars. Similarly the Humanoid 2 project deals with virtual actors performing scripts as well as improvising role-related behavior <ref> [25] </ref>. However, automatically generating the appropriate communicative behaviors and synchronizing them with an actual conversation between users has not been addressed yet in these systems.
References-found: 25

