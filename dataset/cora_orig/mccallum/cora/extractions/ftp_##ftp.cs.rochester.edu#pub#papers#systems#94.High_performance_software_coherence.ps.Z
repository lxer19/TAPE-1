URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.High_performance_software_coherence.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/scott/pubs.html
Root-URL: 
Email: fkthanasi,scottg@cs.rochester.edu  
Title: High Performance Software Coherence for Current and Future Architectures  
Author: Leonidas I. Kontothanassis and Michael L. Scott 
Note: This work was supported in part by NSF Institutional Infrastructure grant no. CDA-8822724 and ONR research grant no. N00014-92-J-1801 (in conjunction with the DARPA Research in Information Science and Technology|High Performance Computing, Software Science and Technology program, ARPA Order no. 8930).  
Date: September 1994  
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> N. M. Aboulenein, J. R. Goodman, S. Gjessing, and P. J. Woest. </author> <title> Hardware Support for Synchronization in the Scalable Coherent Interface (SCI). </title> <booktitle> In Proceedings of the Eighth International Parallel Processing Symposium, </booktitle> <pages> pages 141-150, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: data and locks aggravates an adverse interaction between the application locks and the locks protecting coherent map entries at 4 In systems with contention-free hardware locks, co-location of data and locks may actually be desirable, because it allows the data to migrate along with lock ownership, in a single message <ref> [1] </ref>. In most systems, however, separation is a good idea. 13 levels of restructuring levels of restructuring levels of restructuring levels of restructuring 14 the OS level.
Reference: [2] <author> A. Agarwal and others. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <editor> In M. Dubois and S. S. Thakkar, editors, </editor> <booktitle> Scalable Shared Memory Multiprocessors, </booktitle> <pages> pages 239-261. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others [5, 22]. Coherence is substantially harder to achieve on large-scale multiprocessors <ref> [2, 27, 32] </ref>; it increases both the cost of the machine and the time and intellectual effort required to bring it to market. Given the speed of advances in microprocessor technology, long development times generally lead to machines with out-of-date processors.
Reference: [3] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Sor computes the steady state temperature of a metal sheet using a banded parallelization of red-black successive overrelaxation on a 640 fi 640 grid. Fft computes a one-dimensional FFT on a 65536-element array of complex numbers, using the algorithm described by Akl <ref> [3] </ref>. Mp3d and water are part of the SPLASH suite [42]. Mp3d is a wind-tunnel airflow simulation. We simulated 40000 particles for 10 steps in our studies. Water is a molecular dynamics simulation computing inter- and intra-molecule forces for a set of water molecules.
Reference: [4] <author> T. E. Anderson, H. M. Levy, B. N. Bershad, and E. D. Lazowska. </author> <title> The Interaction of Architecture and Operating System Design. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: To avoid the complexities of instruction-level simulation of interrupt handlers, we assume a constant overhead for page faults. Table I summarizes the default parameters used in our simulations. They are in agreement with those published by Anderson et al. <ref> [4] </ref>, and in several hardware manuals. Some of the transactions required by our coherence protocols require a collection of the operations shown in table I and therefore incur the aggregate cost of their constituents.
Reference: [5] <author> J. Archibald and J. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others <ref> [5, 22] </ref>. Coherence is substantially harder to achieve on large-scale multiprocessors [2, 27, 32]; it increases both the cost of the machine and the time and intellectual effort required to bring it to market.
Reference: [6] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: We simulated 40000 particles for 10 steps in our studies. Water is a molecular dynamics simulation computing inter- and intra-molecule forces for a set of water molecules. We used 256 molecules and 3 times steps. Finally appbt is from the NASA parallel benchmarks suite <ref> [6] </ref>. It computes an approximation to Navier-Stokes equations. It was translated to shared memory from the original message-based form by Doug Burger and Sanjay Mehta at the University of Wisconsin.
Reference: [7] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. </author> <title> Adaptive Software Cache Management for Distributed Shared Memory Architectures. </title> <booktitle> In Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin <ref> [16, 8, 7] </ref>, Treadmarks [26], and the work of Petersen and Li [37, 36, 38], we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data
Reference: [8] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence. </title> <booktitle> In Proceedings of the Second ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-176, </pages> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin <ref> [16, 8, 7] </ref>, Treadmarks [26], and the work of Petersen and Li [37, 36, 38], we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data
Reference: [9] <author> B. N. Bershad and M. J. Zekauskas. Midway: </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors. </title> <institution> CMU-CS-91-170, Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: This fix was not available to us because our programming model provides no means of identifying acquire and release operations except through a pre-defined set of synchronization operations. 15 release. This style of annotation is reminiscent of entry consistency <ref> [9] </ref>, but with a critical difference: Entry consistency requires the programmer to identify the data protected by particular locks|in effect, to identify all situations in which the protocol must not skip coherence operations. Errors of omission affect the correctness of the program.
Reference: [10] <author> B. N. Bershad, M. J. Zekauskas, and W. A. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In Proceedings of the IEEE COMPCON '93. </booktitle> <pages> 28 </pages>
Reference-contexts: Unfortunately, the current state of the art in software coherence for message-passing machines provides performance nowhere close to that of hardware cache coherence. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations <ref> [35, 16, 10, 26] </ref>. First, because they are based on messages, DSM systems must interrupt the execution of remote processors in order to perform any time-critical inter-processor operations.
Reference: [11] <author> R. Bianchini and T. J. LeBlanc. </author> <title> Can High Bandwidth and Latency Justify Large Cache Blocks in Scalable Multiprocessors? In Proceedings of the 1994 International Conference on Parallel Processing, </title> <address> St. Charles, IL, </address> <month> August </month> <year> 1994. </year> <note> Expanded version available as TR 486, </note> <institution> Computer Science Department, University of Rochester, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: We believe that the degradation seen at larger sizes is due to a lack of bandwidth in our system. We note that the performance improvements seen by increasing the line size are progressively smaller for each increase. This is in agreement with the results in <ref> [11] </ref>. The exception to the above observations is mp3d under software coherence, where increases in line size hurt performance. The reason for this anomalous behavior is the interaction between cache accesses and remote references (uncached accesses).
Reference: [12] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Machines in this non-cache-coherent, non-uniform memory access (NCC-NUMA) class include the Cray Research T3D and the Princeton Shrimp <ref> [12] </ref>. In comparison to hardware-coherent machines, NCC-NUMAs can more easily be built from commodity parts, with only a small incremental cost per processor for large systems, and can follow improvements in microprocessors and other hardware technologies closely. <p> Moreover, experience with machines such as the IBM RP3, the BBN Butterfly series, and the current Cray Research T3D suggests that a memory-mapped interface to the network (without coherence) is not much more expensive than a message-passing interface. Memory-mapped interfaces for ATM networks are likely to be available soon <ref> [12] </ref>; we see our work as ideally suited to machines equipped with such an interface. We are currently pursuing software protocol optimizations that should improve performance for important classes of programs.
Reference: [13] <author> W. J. Bolosky, R. P. Fitzgerald, and M. L. Scott. </author> <title> Simple But Effective Techniques for NUMA Memory Management. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Our use of remote reference to reduce the overhead of coherence management can also be found in systems for NUMA memory management <ref> [13, 14, 20, 29, 30] </ref>. Designed for machines without caches, these systems migrate and replicate pages in the manner of distributed shared memory systems, but also make on-line decisions between page movement and remote reference.
Reference: [14] <author> W. J. Bolosky, M. L. Scott, R. P. Fitzgerald, R. J. Fowler, and A. L. Cox. </author> <title> NUMA Policies and Their Relation to Memory Architecture. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 212-221, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Hardware coherence may also be affected, due to the placement of pages in memory modules, but this is a secondary effect we have chosen to ignore in our study. Previous studies on the impact of page size on the performance of Software DSM systems <ref> [14] </ref> indicate that the smaller pages can provide significant performance improvements. The main reason for this result is the reduction in false sharing achieved by smaller coherence units. <p> Our use of remote reference to reduce the overhead of coherence management can also be found in systems for NUMA memory management <ref> [13, 14, 20, 29, 30] </ref>. Designed for machines without caches, these systems migrate and replicate pages in the manner of distributed shared memory systems, but also make on-line decisions between page movement and remote reference.
Reference: [15] <author> W. J. Bolosky and M. L. Scott. </author> <title> False Sharing and its Effect on Shared Memory Performance. </title> <booktitle> In Proceedings of the Fourth USENIX Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 57-71, </pages> <month> September </month> <year> 1993. </year> <note> Also available as MSR-TR-93-1, </note> <institution> Microsoft Research Laboratory, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Second, because they are based on virtual memory, DSM systems must copy entire pages from one processor to another, regardless of the true granularity of sharing. Third, in order to maximize concurrency in the face of false sharing in page-size blocks <ref> [15, 23] </ref>, the fastest DSM systems permit multiple writable copies of a page, forcing them to compute diffs with older versions in order to merge the changes [16, 26].
Reference: [16] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Unfortunately, the current state of the art in software coherence for message-passing machines provides performance nowhere close to that of hardware cache coherence. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations <ref> [35, 16, 10, 26] </ref>. First, because they are based on messages, DSM systems must interrupt the execution of remote processors in order to perform any time-critical inter-processor operations. <p> Third, in order to maximize concurrency in the face of false sharing in page-size blocks [15, 23], the fastest DSM systems permit multiple writable copies of a page, forcing them to compute diffs with older versions in order to merge the changes <ref> [16, 26] </ref>. Hardware cache coherence avoids these problems by working at the granularity of cache lines, and by allowing inter-node communication without interrupting normal processor execution. <p> As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin <ref> [16, 8, 7] </ref>, Treadmarks [26], and the work of Petersen and Li [37, 36, 38], we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data <p> Delayed write notices were shown to improve performance in the Munin distributed shared memory system <ref> [16] </ref>, which runs on networks of workstations and communicates solely via messages. Though 6 the relative costs of operations are quite different, experiments indicate (see section 4) that delayed transitions are generally beneficial in our environment as well. <p> We have also examined architectural alternatives and program-structuring issues that were not addressed by Petersen and Li. Our work resembles Munin <ref> [16] </ref> and lazy release consistency [25] in its use of delayed write notices, but we take advantage of the globally accessible physical address space for cache fills and for access to the coherent map and the local weak lists.
Reference: [17] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: In reality, most pages are shared by a modest number of processors <ref> [17] </ref>, and so local weak lists make sense when the number of processors is large.
Reference: [18] <author> Y. Chen and A. Veidenbaum. </author> <title> An Effective Write Policy for Software Coherence Schemes. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: We ran our experiments under three different assumptions: write-through caches, write-back caches with per-word hardware dirty bits in the cache, and write-through caches with a write-merge buffer <ref> [18] </ref> that hangs onto recently-written lines (16 in our experiments) and coalesces any writes that are directed to the same line. <p> Specifically, we compare the performance obtained with a write-through cache, a write-back cache, and a write-through cache with a buffer for merging writes <ref> [18] </ref>. The policy is applied on only shared data. Private data uses by default a write-back policy. Write-back caches impose the minimum load on the memory and network, since they write blocks back only on eviction, or when explicitly flushed. <p> A write completes when it is acknowledged by the memory system. With a large amount of write traffic we may have simply replaced waiting for the write-back with waiting for missing acknowledgments. Write-through caches with a write-merge buffer <ref> [18] </ref> employ a small (16 entries in our case) fully associative buffer between the cache and the interconnection network. The buffer merges writes to the same cache line, and allocates a new entry for a write to a non-resident cache line.
Reference: [19] <author> H. Cheong and A. V. Veidenbaum. </author> <title> Compiler-Directed Cache Management in Multiprocessors. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 39-47, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Coherence for distributed memory with per-processor caches can also be maintained entirely by a compiler <ref> [19, 21] </ref>. Under this approach the compiler inserts the appropriate cache flush and invalidation instructions in the code, to enforce data consistency.
Reference: [20] <author> A. L. Cox and R. J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: The state of a page is a property of the system as a whole, not (as in most protocols) the viewpoint of a single processor. Borrowing terminology from Platinum <ref> [20] </ref>, the distributed data structure consisting of this information stored at home nodes is called the coherent map. <p> Our use of remote reference to reduce the overhead of coherence management can also be found in systems for NUMA memory management <ref> [13, 14, 20, 29, 30] </ref>. Designed for machines without caches, these systems migrate and replicate pages in the manner of distributed shared memory systems, but also make on-line decisions between page movement and remote reference.
Reference: [21] <author> E. Darnell, J. M. Mellor-Crummey, and K. Kennedy. </author> <title> Automatic Software Cache Coherence Through Vectorization. </title> <booktitle> In 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washing-ton, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Coherence for distributed memory with per-processor caches can also be maintained entirely by a compiler <ref> [19, 21] </ref>. Under this approach the compiler inserts the appropriate cache flush and invalidation instructions in the code, to enforce data consistency.
Reference: [22] <author> S. J. Eggers and R. H. Katz. </author> <title> Evaluation of the Performance of Four Snooping Cache Coherency Protocols. </title> <booktitle> In Proceedings of the Sixteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 2-15, </pages> <month> May </month> <year> 1989. </year> <month> 29 </month>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others <ref> [5, 22] </ref>. Coherence is substantially harder to achieve on large-scale multiprocessors [2, 27, 32]; it increases both the cost of the machine and the time and intellectual effort required to bring it to market.
Reference: [23] <author> S. J. Eggers and R. H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 257-270, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: Second, because they are based on virtual memory, DSM systems must copy entire pages from one processor to another, regardless of the true granularity of sharing. Third, in order to maximize concurrency in the face of false sharing in page-size blocks <ref> [15, 23] </ref>, the fastest DSM systems permit multiple writable copies of a page, forcing them to compute diffs with older versions in order to merge the changes [16, 26].
Reference: [24] <author> M. D. Hill and J. R. Larus. </author> <title> Cache Considerations for Multiprocessor Programmers. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 97-102, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Of the techniques we have used, two are simple program modifications and require no additions to the coherence protocol. They can also be used in the context of hardware coherent systems and are advocated in the literature <ref> [24] </ref>. Our contribution lies in quantifying their impact on performance in the context of a software coherent system and attributing the performance loss observed in the unmodified applications to specific interactions between the application and the coherence protocol. <p> Gauss enjoys the greatest improvement due to this change, though noticeable improvements occur in water, appbt and mp3d as well. Data structure alignment and padding are well-known methods of reducing false sharing <ref> [24] </ref>. Since coherence blocks in software coherent systems are large (4K bytes in our case), it is unreasonable to require padding of data structures to that size. However we can often pad data structures to subpage boundaries so that a collection of them will fit exactly in a page.
Reference: [25] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: We have also examined architectural alternatives and program-structuring issues that were not addressed by Petersen and Li. Our work resembles Munin [16] and lazy release consistency <ref> [25] </ref> in its use of delayed write notices, but we take advantage of the globally accessible physical address space for cache fills and for access to the coherent map and the local weak lists.
Reference: [26] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <institution> COMP TR93-214, Department of Computer Science, Rice University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Unfortunately, the current state of the art in software coherence for message-passing machines provides performance nowhere close to that of hardware cache coherence. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations <ref> [35, 16, 10, 26] </ref>. First, because they are based on messages, DSM systems must interrupt the execution of remote processors in order to perform any time-critical inter-processor operations. <p> Third, in order to maximize concurrency in the face of false sharing in page-size blocks [15, 23], the fastest DSM systems permit multiple writable copies of a page, forcing them to compute diffs with older versions in order to merge the changes <ref> [16, 26] </ref>. Hardware cache coherence avoids these problems by working at the granularity of cache lines, and by allowing inter-node communication without interrupting normal processor execution. <p> As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin [16, 8, 7], Treadmarks <ref> [26] </ref>, and the work of Petersen and Li [37, 36, 38], we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) 1
Reference: [27] <author> Kendall Square Research. </author> <title> KSR1 Principles of Operation. </title> <address> Waltham MA, </address> <year> 1992. </year>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others [5, 22]. Coherence is substantially harder to achieve on large-scale multiprocessors <ref> [2, 27, 32] </ref>; it increases both the cost of the machine and the time and intellectual effort required to bring it to market. Given the speed of advances in microprocessor technology, long development times generally lead to machines with out-of-date processors.
Reference: [28] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The best performance, clearly, will be obtained by systems that combine the speed and concur-rency of existing hardware coherence mechanisms with the flexibility of software coherence. This goal may be achieved by a new generation of machines with programmable network controllers <ref> [28, 39] </ref>. It is not yet clear whether the additional performance of such machines will justify their design time and cost. Our suspicion, based on our results, is that less elaborate hardware will be more cost effective.
Reference: [29] <author> R. P. LaRowe Jr. and C. S. Ellis. </author> <title> Experimental Comparison of Memory Management Policies for NUMA Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Our use of remote reference to reduce the overhead of coherence management can also be found in systems for NUMA memory management <ref> [13, 14, 20, 29, 30] </ref>. Designed for machines without caches, these systems migrate and replicate pages in the manner of distributed shared memory systems, but also make on-line decisions between page movement and remote reference.
Reference: [30] <author> R. P. LaRowe Jr., C. S. Ellis, and L. S. Kaplan. </author> <title> The Robustness of NUMA Memory Management. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 137-151, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Our use of remote reference to reduce the overhead of coherence management can also be found in systems for NUMA memory management <ref> [13, 14, 20, 29, 30] </ref>. Designed for machines without caches, these systems migrate and replicate pages in the manner of distributed shared memory systems, but also make on-line decisions between page movement and remote reference.
Reference: [31] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: In reality, however, most of the messages between processors can be pipelined if lock operations are properly designed. If we employ a distributed queue-based lock [34], 1 Under release consistency <ref> [31] </ref>, memory references are classified as acquires, releases, or ordinary references. A release indicates that the processor is completing an operation on which other processor (s) may depend; all of the processor's previous writes must be made visible to any processor that performs a subsequent acquire. <p> improved, with more major restructuring of access to the space cell data structure, but this would require effort out of keeping with the current study. 4.3 Hardware v. software coherence Figures 8 and 9 compare the performance of our best software protocol to that of a relaxed-consistency DASH-like hardware protocol <ref> [31] </ref> on 16 and 64 processors respectively. The unit line in the graphs represents the running time of each application under a sequentially consistent hardware coherence protocol.
Reference: [32] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others [5, 22]. Coherence is substantially harder to achieve on large-scale multiprocessors <ref> [2, 27, 32] </ref>; it increases both the cost of the machine and the time and intellectual effort required to bring it to market. Given the speed of advances in microprocessor technology, long development times generally lead to machines with out-of-date processors. <p> In fact, early experiments we have conducted with on-line NUMA policies and relaxed consistency have failed badly in their attempt to determine when to use remote reference. On the hardware side our work bears a resemblance to the Stanford Dash project <ref> [32] </ref> in the use of a relaxed consistency model, and to the Georgia Tech Beehive project [41] in the use of relaxed consistency and per-word dirty bits for successful merging of inconsistent cache lines.
Reference: [33] <author> M. Marchetti, L. Kontothanassis, R. Bianchini, and M. L. Scott. </author> <title> Using Simple Page Placement Policies to Reduce the Cost of Cache Fills in Coherent Shared-Memory Systems. </title> <type> TR 535, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> September </month> <year> 1994. </year> <note> Submitted for publication. </note>
Reference-contexts: Designed for machines without caches, these systems migrate and replicate pages in the manner of distributed shared memory systems, but also make on-line decisions between page movement and remote reference. We have experimented with dynamic page movement in conjunction with software coherence on NCC-NUMA machines <ref> [33] </ref>, and have found that while appropriate placement of a unique page copy reduces the average cache fill cost appreciably, replication of pages provides no significant benefit in the presence of hardware caches. Moreover, we have found that relaxed consistency greatly reduces the opportunities for profitable remote data access.
Reference: [34] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: In reality, however, most of the messages between processors can be pipelined if lock operations are properly designed. If we employ a distributed queue-based lock <ref> [34] </ref>, 1 Under release consistency [31], memory references are classified as acquires, releases, or ordinary references.
Reference: [35] <author> B. Nitzberg and V. Lo. </author> <title> Distributed Shared Memory: A Survey of Issues and Algorithms. </title> <journal> Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Unfortunately, the current state of the art in software coherence for message-passing machines provides performance nowhere close to that of hardware cache coherence. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations <ref> [35, 16, 10, 26] </ref>. First, because they are based on messages, DSM systems must interrupt the execution of remote processors in order to perform any time-critical inter-processor operations.
Reference: [36] <author> K. Petersen and K. Li. </author> <title> Cache Coherence for Shared Memory Multiprocessors Based on Virtual Memory Support. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <address> Newport Beach, CA, </address> <month> April </month> <year> 1993. </year> <month> 30 </month>
Reference-contexts: As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin [16, 8, 7], Treadmarks [26], and the work of Petersen and Li <ref> [37, 36, 38] </ref>, we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) 1 As in the work of Petersen and Li, <p> This is the protocol of Petersen and 2 Mp3d does not scale to 64 processor but we use it as a stress test to compare the performance of different coherence mechanisms 10 different software protocols on 64 processors software protocols on 64 processors Li <ref> [36, 37] </ref>, with the exception that while the weak list is conceptually centralized, its entries are distributed physically among the nodes of the machine. rel.centr.del: Same as rel.distr.del, except that write notices are propagated by inserting weak pages in a global list which is traversed on acquires. <p> It is interesting to notice that in water the centralized relaxed consistency protocols are badly beaten by the sequentially consistent software protocol. This agrees to some extent with the results reported by Petersen and Li <ref> [36] </ref>, but the advantage of the sequentially consistent protocol was less pronounced in their work. We believe there are two reasons for our difference in results. <p> In the absence of reduced memory startup costs for remote reference the performance of mp3d under software coherence degrades rapidly and is as much as twice worse than that of the hardware systems. 6 Related Work Our work is most closely related to that of Petersen and Li <ref> [36, 37] </ref>: we both use the notion of weak pages, and purge caches on acquire operations.
Reference: [37] <author> K. Petersen. </author> <title> Operating System Support for Modern Memory Hierarchies. </title> <publisher> Ph. </publisher> <address> D. </address> <institution> dissertation, CS-TR-431-93, Department of Computer Science, Princeton University, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin [16, 8, 7], Treadmarks [26], and the work of Petersen and Li <ref> [37, 36, 38] </ref>, we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) 1 As in the work of Petersen and Li, <p> Finally, in section 4.3, we compare the best of the software results to the corresponding results on sequentially-consistent and release-consistent hardware. 4.1 Software coherence protocol alternatives This section compares our software protocol (presented in section 2) to the protocol devised by Petersen and Li <ref> [37] </ref> (modified to distribute the centralized weak list among the memories of the machine), and to a sequentially consistent page-based cache coherence protocol. <p> This is the protocol of Petersen and 2 Mp3d does not scale to 64 processor but we use it as a stress test to compare the performance of different coherence mechanisms 10 different software protocols on 64 processors software protocols on 64 processors Li <ref> [36, 37] </ref>, with the exception that while the weak list is conceptually centralized, its entries are distributed physically among the nodes of the machine. rel.centr.del: Same as rel.distr.del, except that write notices are propagated by inserting weak pages in a global list which is traversed on acquires. <p> In the absence of reduced memory startup costs for remote reference the performance of mp3d under software coherence degrades rapidly and is as much as twice worse than that of the hardware systems. 6 Related Work Our work is most closely related to that of Petersen and Li <ref> [36, 37] </ref>: we both use the notion of weak pages, and purge caches on acquire operations.
Reference: [38] <author> K. Petersen and K. Li. </author> <title> An Evaluation of Multiprocessor Cache Coherence Based on Virtual Memory Support. </title> <booktitle> In Proceedings of the Eighth International Parallel Processing Symposium, </booktitle> <pages> pages 158-164, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: We then describe our experimental methodology and application suite in section 3 and present results in section 4. We compare our protocol to a variety of existing alternatives, including sequentially-consistent hardware, release-consistent hardware, straightforward sequentially-consistent software, and a coherence scheme for small-scale NCC-NUMAs due to Petersen and Li <ref> [38] </ref>. We show that certain simple program modifications can improve the performance of software coherence substantially. <p> As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin [16, 8, 7], Treadmarks [26], and the work of Petersen and Li <ref> [37, 36, 38] </ref>, we allow more than one processor to write a page concurrently, and we use a variant of release consistency to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) 1 As in the work of Petersen and Li, <p> The main reason for this result is the reduction in false sharing achieved by smaller coherence units. Moving to relaxed consistency, however, and to an architecture that uses pages for the coherence but cache lines for the data fetch unit, reverses the decision in favor of large pages <ref> [38] </ref>. Relaxed consistency mitigates the impact of false sharing, and the larger page size reduces the length of the weak list that needs to be traversed on an acquire operation.
Reference: [39] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level Shared-Memory. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The best performance, clearly, will be obtained by systems that combine the speed and concur-rency of existing hardware coherence mechanisms with the flexibility of software coherence. This goal may be achieved by a new generation of machines with programmable network controllers <ref> [28, 39] </ref>. It is not yet clear whether the additional performance of such machines will justify their design time and cost. Our suspicion, based on our results, is that less elaborate hardware will be more cost effective.
Reference: [40] <author> I. Schoinas, B. Falsafi, A. R. Lebeck, S. K. Reinhardt, J. R. Larus, and D. A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, CA, </address> <note> October 1994 (to appear). </note>
Reference-contexts: Hardware protocols also have the advantage of smaller block sizes, and therefore less false sharing, but improvements in program structuring techniques, and the use of relaxed consistency, are eroding this advantage too. Recent work also suggests <ref> [45, 40] </ref> that software-coherent systems may be able to enforce consistency on sub-page-size blocks efficiently by using binary editing techniques to embed coherence operations in the program text itself.
Reference: [41] <author> G. Shah and U. Ramachandran. </author> <title> Towards Exploiting the Architectural Features of Beehive. </title> <institution> GIT-CC-91/51, College of Computing, Georgia Institute of Technology, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: On the hardware side our work bears a resemblance to the Stanford Dash project [32] in the use of a relaxed consistency model, and to the Georgia Tech Beehive project <ref> [41] </ref> in the use of relaxed consistency and per-word dirty bits for successful merging of inconsistent cache lines.
Reference: [42] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Fft computes a one-dimensional FFT on a 65536-element array of complex numbers, using the algorithm described by Akl [3]. Mp3d and water are part of the SPLASH suite <ref> [42] </ref>. Mp3d is a wind-tunnel airflow simulation. We simulated 40000 particles for 10 steps in our studies. Water is a molecular dynamics simulation computing inter- and intra-molecule forces for a set of water molecules. We used 256 molecules and 3 times steps. <p> We were unable to establish the access pattern of appbt from the source code; it uses linear arrays to represent higher dimensional data structures and the computation of offsets often uses several levels of indirection. Mp3d <ref> [42] </ref> has very wide-spread sharing. We modified the program slightly (prior to the current studies) to ensure that colliding molecules belong with high probability to either the same processor or neighboring processors. Therefore the molecule data structures exhibit limited pairwise sharing. The main problem is the space cell data structures.
Reference: [43] <author> J. E. Veenstra. </author> <title> Mint Tutorial and User Manual. </title> <type> TR 452, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Our simulator consists of two parts: a front end, Mint <ref> [43, 44] </ref>, that simulates the execution of the processors, and a back end that simulates the memory system. The front end calls the back end on every data reference (instruction fetches are assumed to always be cache hits).
Reference: [44] <author> J. E. Veenstra and R. J. Fowler. Mint: </author> <title> A Front End for Efficient Simulation of Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Second International Workshop on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS '94), </booktitle> <pages> pages 201-207, </pages> <address> Durham, NC, </address> <month> January - February </month> <year> 1994. </year>
Reference-contexts: Our simulator consists of two parts: a front end, Mint <ref> [43, 44] </ref>, that simulates the execution of the processors, and a back end that simulates the memory system. The front end calls the back end on every data reference (instruction fetches are assumed to always be cache hits).
Reference: [45] <author> M. J. Zekauskas, W. A. Sawdon, and B. N. Bershad. </author> <title> Software Write Detection for Distributed Shared Memory. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <note> November 1994 (to appear). 31 </note>
Reference-contexts: Hardware protocols also have the advantage of smaller block sizes, and therefore less false sharing, but improvements in program structuring techniques, and the use of relaxed consistency, are eroding this advantage too. Recent work also suggests <ref> [45, 40] </ref> that software-coherent systems may be able to enforce consistency on sub-page-size blocks efficiently by using binary editing techniques to embed coherence operations in the program text itself.
References-found: 45

