URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3676/3676.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Phone: 2  3  4  
Title: Performance of On-Line Learning Methods in Predicting Multiprocessor Memory Access Patterns  
Author: Majd F. Sakr , Steven P. Levitan Donald M. Chiarulli Bill G. Horne C. Lee Giles , 
Keyword: Prediction; Learning; Multiprocessors; Memory; Markov Predictor; Linear Predictor; Time Delay Neural Network  
Address: 4 Independence Way, Princeton NJ 08540  Pittsburgh PA 15261  Pittsburgh PA 15260  College Park, MD 20742  
Affiliation: 1 NEC Research Institute,  University of Pittsburgh, Electrical Engineering Department,  University of Pittsburgh, Computer Science Department,  UMIACS, University of Maryland,  
Abstract: Technical Report UMIACS-TR-96-59 and CS-TR-3676 Institute for Advanced Computer Studies University of Maryland College Park, MD 20742 Abstract Shared memory multiprocessors require reconfigurable interconnection networks (INs) for scalability. These INs are reconfigured by an IN control unit. However, these INs are often plagued by undesirable reconfiguration time that is primarily due to control latency, the amount of time delay that the control unit takes to decide on a desired new IN configuration. To reduce control latency, a trainable prediction unit (PU) was devised and added to the IN controller. The PUs job is to anticipate and reduce control configuration time, the major component of the control latency. Three different on-line prediction techniques were tested to learn and predict repetitive memory access patterns for three typical parallel processing applications, the 2-D relaxation algorithm, matrix multiply and Fast Fourier Transform. The predictions were then used by a routing control algorithm to reduce control latency by configuring the IN to provide needed memory access paths before they were requested. Three prediction techniques were used and tested: 1). a Markov predictor, 2). a linear predictor and 3). a time delay neural network (TDNN) predictor. As expected, different predictors performed best on different applications, however, the TDNN produced the best overall results. 
Abstract-found: 1
Intro-found: 1
Reference: [Bell85] <author> C.G. Bell, Multis: </author> <title> A new class of multiprocessor computers, </title> <journal> Science, </journal> <volume> vol. 228, </volume> <pages> pp. 462-467, </pages> <year> 1985. </year>
Reference-contexts: In section 3, we describe the three prediction methods used and in section 4 we present experimental results of the predictors. The final section interprets our results and discusses future directions of research. 3 2 Multiprocessor Model Shared memory parallel computers are commonly referred to as multiprocessor systems <ref> [Bell85, Kumar94] </ref>. Our shared memory multiprocessor (SMM) system consists of 8 processors (P0-P7), 32 memory modules (M0-M31), a reconfigurable IN and an IN controller (Figure 2). This SMM model uses a state-sequence router [Chiarulli94] as the reconfigurable interconnection network controller.
Reference: [Chiarulli94] <author> D.M. Chiarulli, S.P. Levitan, R.G. Melhem, C. Qiao, </author> <title> Locality Based Control Algorithms for Reconfigurable Interconnection Networks, </title> <journal> Applied Optics, </journal> <volume> vol. 33, </volume> <pages> pp. 1528-1537, </pages> <year> 1994. </year>
Reference-contexts: Our shared memory multiprocessor (SMM) system consists of 8 processors (P0-P7), 32 memory modules (M0-M31), a reconfigurable IN and an IN controller (Figure 2). This SMM model uses a state-sequence router <ref> [Chiarulli94] </ref> as the reconfigurable interconnection network controller. In addition, we use a SMM simulator which allows us to record the memory access traces of parallel applications. <p> A group of compatible (non-blocking) paths are called an IN configuration or a state. Because of contention for paths, the IN must be dynamically reconfigured to satisfy the set of current processor-memory accesses. This SMM model employs an IN control system based on the state sequence routing (SSR) paradigm <ref> [Chiarulli94] </ref> which takes advantage of the locality characteristics exhibited in memory access patterns [Johnson92] and reconfigures the network through a fixed set of configurations in a repetitive manner. The IN controller, used for state sequence routing, consists of a state generator which is controlled by a state transformer.
Reference: [Dormans95] <author> M. Dormans, H.-U. Heiss, </author> <title> Partitioning and Mapping of Large FEM-Graphs by Self-Organization, </title> <booktitle> Proceedings Euromicro Workshop on Parallel and Distributed Processing, </booktitle> <address> San Romeo, Italy, </address> <pages> pp. 227-235, </pages> <year> 1995. </year>
Reference-contexts: For multicomputer systems, genetic algorithms have been applied as a distributed task scheduling technique [Wang95]. Solutions to the problem of mapping parallel programs onto multicomputer systems to provide load balancing and minimize interprocessor communication have been proposed using genetic algorithms [Seredynski94] and self organizing maps <ref> [Dormans95] </ref> as well as variants of the Growing Cell Structures network [Tumuluri96]. In uniprocessor environments, Sti-gal et. al. [Stigal91] propose a neural network cache replacement algorithm. Their technique predicts which cache block will be accessed furthest in the future and therefore should be replaced, thus lowering the cache miss rate.
Reference: [Fritsch91] <author> T. Fritsch, W. Mandel, </author> <title> Communication Network Routing Using Neural Nets-Numerical Aspects and Alternative Approaches, </title> <booktitle> IEEE International Joint Conference on Neural Networks, </booktitle> <pages> pp 752-757, </pages> <year> 1991. </year>
Reference-contexts: The effect is a significant reduction in the communications latency for multiprocessor systems. Learning methods have been applied in various areas of computing and communication systems. For instance, neural networks have been applied to learn both network topology and traffic patterns for routing and control of communication networks <ref> [Fritsch91, Jensen90, Thomopoulos91] </ref>. Recent work on using neurocomputing in high speed communication networks was the subject of a special issue of Communications [Habib95].
Reference: [Funabiki93] <author> N. Funabiki, Y. Takefuji, K.C. Lee, </author> <title> Comparisons of Seven Neural Network Models on Traffic Control Problems in Multistage Interconnection Networks, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 42, no. 4, </volume> <pages> pp 497-501, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Recent work on using neurocomputing in high speed communication networks was the subject of a special issue of Communications [Habib95]. Other applications of neural networks are for the control of switching elements of a multistage interconnection network for parallel computers <ref> [Funabiki93, Giles95] </ref> and for learning the structure of interconnection networks [Goudreau95]. For multicomputer systems, genetic algorithms have been applied as a distributed task scheduling technique [Wang95].
Reference: [Giles95] <author> C.L. Giles, M.W. Goudreau, </author> <title> Routing in Optical Multistage Interconnection Networks: a Neural Network Solution, </title> <journal> Journal of Lightwave Technology, </journal> <volume> vol. 13, no. 6, </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: Recent work on using neurocomputing in high speed communication networks was the subject of a special issue of Communications [Habib95]. Other applications of neural networks are for the control of switching elements of a multistage interconnection network for parallel computers <ref> [Funabiki93, Giles95] </ref> and for learning the structure of interconnection networks [Goudreau95]. For multicomputer systems, genetic algorithms have been applied as a distributed task scheduling technique [Wang95].
Reference: [Gornish90] <author> E.H. Gornish, E.D. Granston, </author> <title> A.V. Veidenbaum, Compiler-directed Data Prefetch-ing in Multiprocessors with Memory Hierarchies, </title> <booktitle> Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pp. 354-368, </pages> <month> Sep. </month> <year> 1990. </year> <month> 11 </month>
Reference-contexts: Therefore, control time dominates the communication latency. However, in a multiprocessor system executing a parallel scientific application, the memory-access requests made by the processors follow a repetitive pattern based on the application. Compilers can analyze an application and attempt to predict its access patterns <ref> [Gornish90] </ref>, but often the pattern is dynamic and thus hard to predict. The goal of this work is to employ a technique that learns these patterns online, predicts the processor requests, and performs IN configuration prior to the requests being issued, thus hiding the control latency.
Reference: [Goudreau95] <author> M.W. Goudreau, C.L. Giles, </author> <title> Using Recurrent Neural Networks to Learn the Structure of Interconnection Networks, </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, no. 5, </volume> <pages> pp. </pages> <month> 793-804 </month> <year> (1995). </year>
Reference-contexts: Other applications of neural networks are for the control of switching elements of a multistage interconnection network for parallel computers [Funabiki93, Giles95] and for learning the structure of interconnection networks <ref> [Goudreau95] </ref>. For multicomputer systems, genetic algorithms have been applied as a distributed task scheduling technique [Wang95].
Reference: [Habib95] <author> I.W. Habib, </author> <title> Guest editor of the Special Issue on Neurocomputing in High-Speed Networks, </title> <journal> IEEE Communications Magazine, </journal> <volume> vol. 33, </volume> <month> October </month> <year> 1995. </year>
Reference-contexts: For instance, neural networks have been applied to learn both network topology and traffic patterns for routing and control of communication networks [Fritsch91, Jensen90, Thomopoulos91]. Recent work on using neurocomputing in high speed communication networks was the subject of a special issue of Communications <ref> [Habib95] </ref>. Other applications of neural networks are for the control of switching elements of a multistage interconnection network for parallel computers [Funabiki93, Giles95] and for learning the structure of interconnection networks [Goudreau95]. For multicomputer systems, genetic algorithms have been applied as a distributed task scheduling technique [Wang95].
Reference: [HechtN91] <author> R. Hecht-Nielsen, </author> <title> Neurocomputing, </title> <publisher> Addison Wesley, </publisher> <year> 1991. </year>
Reference-contexts: In order to compare the results of this predictor with that of the TDNN we use one bias weight for each output value, hence the Linear predictor is actually an affine predictor <ref> [HechtN91] </ref>: 6 where is a binary vector of dimension 32, x i denotes the i th component and is the prediction.
Reference: [Isaacson76] <author> D.L. Isaacson, R.W. Madsen, </author> <title> Markov Chains Theory and Applications, R.E. </title> <publisher> Krieger, </publisher> <year> 1976. </year>
Reference-contexts: The three prediction methods tested are considered appropriate for this dynamic system since the training and prediction is performed on-line. 3.1 Markov Predictor There are many ways one could consider using a Markov predictor <ref> [Isaacson76] </ref>.
Reference: [Jensen90] <author> J.E. Jensen, M.A. Eshera, </author> <title> S.C. Barash, Neural Network Controller for Adaptive Routing in Survivable Communication Networks, </title> <booktitle> IEEE International Joint Conference on Neural Networks, </booktitle> <pages> pp 2693-702, </pages> <year> 1990. </year>
Reference-contexts: The effect is a significant reduction in the communications latency for multiprocessor systems. Learning methods have been applied in various areas of computing and communication systems. For instance, neural networks have been applied to learn both network topology and traffic patterns for routing and control of communication networks <ref> [Fritsch91, Jensen90, Thomopoulos91] </ref>. Recent work on using neurocomputing in high speed communication networks was the subject of a special issue of Communications [Habib95].
Reference: [Johnson92] <author> K.L. Johnson, </author> <title> The Impact of Communication Locality on Large-Scale Multiprocessor Performance, </title> <journal> Computer Architecture News, </journal> <volume> vol. 20, </volume> <pages> pp 392-402, </pages> <year> 1992. </year>
Reference-contexts: Because of contention for paths, the IN must be dynamically reconfigured to satisfy the set of current processor-memory accesses. This SMM model employs an IN control system based on the state sequence routing (SSR) paradigm [Chiarulli94] which takes advantage of the locality characteristics exhibited in memory access patterns <ref> [Johnson92] </ref> and reconfigures the network through a fixed set of configurations in a repetitive manner. The IN controller, used for state sequence routing, consists of a state generator which is controlled by a state transformer.
Reference: [Jordan94] <author> M.I. Jordan, R.A. Jacobs, </author> <title> Hierarchical Mixtures of Experts and the EM Algorithm, </title> <journal> Neural Computation, </journal> <volume> vol. 6, </volume> <pages> pp. 181-214, </pages> <year> 1994. </year>
Reference-contexts: Thus, we hypothesize that the TDNN has the best chance of adapting to different memory access patterns from the variety of real applications. However, it could be feasible to use all prediction methods in a mixture of experts model <ref> [Jordan94] </ref> and use the best predictor available. Our future work will address more realistic simulation of the multiprocessor environment, such as the effects of incorporating runtime delays due to memory and network contention in the memory access patterns and how these prediction methods affect actual performance.
Reference: [Kumar94] <author> V. Kumar, A. Grama, A. Gupta, G. Karypis, </author> <title> Introduction to Parallel Computing, </title> <address> Benjamin/Cummings, CA, </address> <year> 1994. </year>
Reference-contexts: In section 3, we describe the three prediction methods used and in section 4 we present experimental results of the predictors. The final section interprets our results and discusses future directions of research. 3 2 Multiprocessor Model Shared memory parallel computers are commonly referred to as multiprocessor systems <ref> [Bell85, Kumar94] </ref>. Our shared memory multiprocessor (SMM) system consists of 8 processors (P0-P7), 32 memory modules (M0-M31), a reconfigurable IN and an IN controller (Figure 2). This SMM model uses a state-sequence router [Chiarulli94] as the reconfigurable interconnection network controller.
Reference: [Maggini94] <author> M. Maggini, </author> <type> Personal Communication, </type> <year> 1994. </year>
Reference-contexts: The learning algorithm is a simple on-line gradient descent algorithm using the following adaptive learning rate <ref> [Maggini94] </ref>, starting value is set to 0.01: if (present error - previous error &gt; previous error allowed % increment [default 10%])- reduce learning rate (multiply by a decrease factor &lt; 1 [default 0.5]) and move back in the weight space to the previous point else - keep the updated weights and
Reference: [Sakr95a] <author> M.F. Sakr, </author> <title> Predicting Multiprocessor Communication Patterns with Neural Networks, M.S. </title> <type> Thesis, </type> <institution> Electrical Engineering Department, University of Pittsburgh, </institution> <year> 1995. </year>
Reference-contexts: Since, processor-memory access patterns change dynamically and thus can be modeled as a time series, for this preliminary investigation, we chose to study three simple on-line time series prediction methods: a Markov predictor, a linear predictor and a TDNN <ref> [Sakr95a] </ref>. 3 Prediction Method Experiments To evaluate the performance of various prediction methods, we test how well each technique can predict the next memory access pattern as the SMM executes three typical parallelized scientific applications.
Reference: [Sakr95b] <author> M.F. Sakr, S.P. Levitan, C.L. Giles, B.G. Horne, M. Maggini, </author> <title> D.M. Chiarulli, Predictive Control of Opto-Electronic Reconfigurable Interconnection Networks using Neural Networks, </title> <booktitle> Proceedings of the Second IEEE International Conference on Massively Parallel Processing Using Optical Interconnections, </booktitle> <pages> pp. 326-335, </pages> <year> 1995. </year>
Reference-contexts: For each experiment we use the 32 memory module access pattern of a single processor, these patterns are shown in Figures 5a, 6a, and 7a. The applications are symmetrically partitioned to execute the same code on all processors while each processor uses different parts of the data <ref> [Sakr95b] </ref>. Hence, the access patterns of all other processors are very similar to the one used. Second, we use the processors memory access patterns as input to the PU to perform on-line training and one-step ahead prediction of the next memory access. <p> in Figures 5c, 6c, and 7c where we compare the performance of the Markov predictor to that of the Linear and TDNN. 3.2 Linear Predictor For the Linear PU, the input data is transformed from a processors raw 32 memory module traces into a sequence of 32 bit binary vectors <ref> [Sakr95b] </ref>. The i th component of the binary vector is set to 1 when an access to the i th memory module takes place. All other values in that vector are set to zero, this data encoding is shown in Figure 4.
Reference: [Sakr96] <author> M.F. Sakr, C.L. Giles, S.P. Levitan, B.G. Horne, M. Maggini, </author> <title> D.M. Chiarulli, On-Line Prediction of Multiprocessor Memory Access Patterns, </title> <booktitle> Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pp. 1564-1569, </pages> <year> 1996. </year>
Reference-contexts: Again, since we are implementing one-step-ahead prediction, the TDNN takes as input the current binary vector and attempts to predict the access vector at the next time step <ref> [Sakr96] </ref>. Therefore there are 32 inputs and 32 outputs for the network. For each input, we experiment with a tapped delay line of length 1, 5 or 10.
Reference: [Seredynski94] <author> F. Seredynski, </author> <title> Dynamic Mapping and Load Balancing with Parallel Genetic Algorithms, </title> <booktitle> Proceedings of the First IEEE Conference on Evolutionary Computation, </booktitle> <volume> vol. II, </volume> <pages> pp. 834-839, </pages> <year> 1994. </year>
Reference-contexts: For multicomputer systems, genetic algorithms have been applied as a distributed task scheduling technique [Wang95]. Solutions to the problem of mapping parallel programs onto multicomputer systems to provide load balancing and minimize interprocessor communication have been proposed using genetic algorithms <ref> [Seredynski94] </ref> and self organizing maps [Dormans95] as well as variants of the Growing Cell Structures network [Tumuluri96]. In uniprocessor environments, Sti-gal et. al. [Stigal91] propose a neural network cache replacement algorithm.
Reference: [Siegel90] <author> H.J. Siegel, </author> <title> Interconnection Networks for Large-Scale Parallel Processing, </title> <publisher> McGraw-Hill, </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Large scale multiprocessor systems will require low-cost, highly-scalable, and dynamically reconfigurable interconnection networks (INs) <ref> [Siegel90] </ref>. Such INs offer a limited number of communication channels that are configured on demand to satisfy required processor-memory accesses. In this demand driven environment, a processor accessing a memory module makes a request to an IN controller to establish a path (reconfigure the IN) that satisfies the processors request.
Reference: [Stigal91] <author> P.D. Stigal, C.H. Dagli, </author> <title> C.F. Sen, A Neural Network Cache Controller, Intelligent Engineering Systems Through Artificial Neural Networks, </title> <editor> C. Dagli, S. Kumara and Y. Shin editors, </editor> <booktitle> pp. </booktitle> <pages> 561-566, </pages> <publisher> ASME Press, </publisher> <year> 1991. </year>
Reference-contexts: Solutions to the problem of mapping parallel programs onto multicomputer systems to provide load balancing and minimize interprocessor communication have been proposed using genetic algorithms [Seredynski94] and self organizing maps [Dormans95] as well as variants of the Growing Cell Structures network [Tumuluri96]. In uniprocessor environments, Sti-gal et. al. <ref> [Stigal91] </ref> propose a neural network cache replacement algorithm. Their technique predicts which cache block will be accessed furthest in the future and therefore should be replaced, thus lowering the cache miss rate.
Reference: [Thomopoulos91] <author> S.C.A. Thomopoulos, L. Zhang, </author> <title> C.D. Wann, Neural Network Application of the Shortest Path Algorithm for Traffic Control in Communication Networks, </title> <booktitle> IEEE International Joint Conference on Neural Networks, </booktitle> <pages> pp. 2693-702, </pages> <year> 1991. </year>
Reference-contexts: The effect is a significant reduction in the communications latency for multiprocessor systems. Learning methods have been applied in various areas of computing and communication systems. For instance, neural networks have been applied to learn both network topology and traffic patterns for routing and control of communication networks <ref> [Fritsch91, Jensen90, Thomopoulos91] </ref>. Recent work on using neurocomputing in high speed communication networks was the subject of a special issue of Communications [Habib95].
Reference: [Tumuluri96] <author> C. Tumuluri, C.K. Mohan, </author> <title> A.N. Choudhary, Unsupervised Algorithms for Learning Spatio-Temporal Correlations, </title> <institution> Syracuse University Technical Report #SU-CIS-96-1, </institution> <year> 1996. </year>
Reference-contexts: Solutions to the problem of mapping parallel programs onto multicomputer systems to provide load balancing and minimize interprocessor communication have been proposed using genetic algorithms [Seredynski94] and self organizing maps [Dormans95] as well as variants of the Growing Cell Structures network <ref> [Tumuluri96] </ref>. In uniprocessor environments, Sti-gal et. al. [Stigal91] propose a neural network cache replacement algorithm. Their technique predicts which cache block will be accessed furthest in the future and therefore should be replaced, thus lowering the cache miss rate.

References-found: 24

