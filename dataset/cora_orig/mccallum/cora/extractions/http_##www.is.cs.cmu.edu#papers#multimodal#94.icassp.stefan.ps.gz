URL: http://www.is.cs.cmu.edu/papers/multimodal/94.icassp.stefan.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.multimodal.publications.html
Root-URL: 
Title: A CONNECTIONIST RECOGNIZER FOR ON-LINE CURSIVE HANDWRITING RECOGNITION  
Author: Stefan Manke and Ulrich Bodenhausen 
Address: D-76128 Karlsruhe, Germany  
Affiliation: University of Karlsruhe, Computer Science Department,  
Abstract: In this paper we show how the Multi-State Time Delay Neural Network (MS-TDNN), which is already used successfully in continuous speech recognition tasks, can be applied both to online single character and cursive (continuous) handwriting recognition. The MS-TDNN integrates the high accuracy single character recognition capabilities of a TDNN with a non-linear time alignment procedure (dynamic time warping algorithm) for finding stroke and character boundaries in isolated, handwritten characters and words. In this approach each character is modelled by up to 3 different states and words are represented as a sequence of these characters. We describe the basic MS-TDNN architecture and the input features used in this paper, and present results (up to 97.7% word recognition rate) both on writer dependent/ independent, single character recognition tasks and writer dependent, cursive handwriting tasks with varying vocabulary sizes up to 20000 words. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Schenkel, H. Weissman, I. Guyon, C. Nohl, and D. Hend-erson. </author> <title> Recognition-based Segmentation of Online Hand-printed Words. </title> <booktitle> Advances in Neural Network Information Processing Systems (NIPS-5). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: coordinates with varying speed and pressure in each coordinate. .As in speech recognition, the main problem of recognizing continuous words is that character or stroke boundaries are not known (in particular if no pen lifts or white space indicate these boundaries) and an optimal time alignment has to be found <ref> [1] </ref>. The connectionist recognizer, described in this paper, integrates the recognition and segmentation into a single network architecture, the Multi-State Time Delay Neural Network (MS-TDNN), which was originally proposed for continuous speech recognition tasks [2,3,4].
Reference: [2] <author> P. Haffner, M. Franzini, and A. Waibel. </author> <title> Integrating Time Alignment and Neural Networks for High Performance Continuous Speech Recognition. </title> <booktitle> Proceedings of the ICASSP-91. </booktitle>
Reference: [3] <author> H. Hild and A. Waibel. </author> <title> Connected Letter Recognition with a Multi-State Time Delay Neural Network. </title> <booktitle> Advances in Neural Network Information Processing Systems (NIPS-5). </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Now training starts at the word level of the MS-TDNN and is performed on unsegmented training data. During training in forced alignment mode the McClelland objective function <ref> [3] </ref> is used to avoid the problem with 1-out-of-n codings for large n which appear with the Mean Squared Error.
Reference: [4] <author> C. Bregler, H. Hild, S. Manke, and A. W aibel. </author> <title> Improving Connected Letter Recognition by Lipreading. </title> <booktitle> Proceedings of the ICASSP-93, </booktitle> <address> Minneapolis, </address> <month> April </month> <year> 1993. </year>
Reference: [5] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shiano, and K. Lang. </author> <title> Phoneme Recognition using TimeDelay Neural Networks. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <month> March </month> <year> 1989. </year>
Reference-contexts: The connectionist recognizer, described in this paper, integrates the recognition and segmentation into a single network architecture, the Multi-State Time Delay Neural Network (MS-TDNN), which was originally proposed for continuous speech recognition tasks [2,3,4]. For on-line single character recognition, the Time Delay Neural Network (TDNN) <ref> [5] </ref> with its time-shift invariant architecture has been applied successfully [6].
Reference: [6] <author> I. Guyon, P. Albrecht, Y. Le Cun, W. Denker, and W. Hub-bard. </author> <title> Design of a Neural Network Character Recognizer for a Touch Terminal. </title> <journal> Pattern Recognition, </journal> <volume> 24(2), </volume> <year> 1991. </year>
Reference-contexts: For on-line single character recognition, the Time Delay Neural Network (TDNN) [5] with its time-shift invariant architecture has been applied successfully <ref> [6] </ref>. The Multi-State Time Delay Neural Network (MS-TDNN), an extension of the TDNN, combines the high accuracy character recognition capabilities of a TDNN with a non-linear time alignment procedure (Dynamic Time Warping) [7] for finding an optimal alignment between strokes and characters in handwritten continuous words.
Reference: [7] <author> H. Ney. </author> <title> The Use of a OneStage Dynamic Programming Algorithm for Connected Word Recognition. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Pr ocessing, </journal> <month> March </month> <year> 1984. </year>
Reference-contexts: The Multi-State Time Delay Neural Network (MS-TDNN), an extension of the TDNN, combines the high accuracy character recognition capabilities of a TDNN with a non-linear time alignment procedure (Dynamic Time Warping) <ref> [7] </ref> for finding an optimal alignment between strokes and characters in handwritten continuous words. The following section describes the basic network architecture and training method of the MS-TDNN, followed by a description of the input features used in this paper (section 3).
Reference: [8] <author> U. Bodenhausen, S. Manke, and A. W aibel. </author> <title> Connectionist Architectural Learning for High Performance Character and Speech Recognition. </title> <booktitle> Proceedings of the ICASSP-93 , Minne-apolis, </booktitle> <month> April </month> <year> 1993. </year>
Reference: [9] <author> U. Bodenhausen and S. Manke. </author> <title> Automatically Structured Neural Networks for Handwritten Character and Word Recognition. </title> <booktitle> Proceedings of the ICANN-93, </booktitle> <address> Amsterdam, </address> <month> Sep-tember </month> <year> 1993. </year>
Reference: [10] <author> J. Hampshire and A. Waibel. </author> <title> A Novel Objective Function for Improved Phoneme Recognition. </title> <journal> IEEE Transactions on Neural Networks, </journal> <month> June </month> <year> 1990. </year>
Reference-contexts: For word level training (where back-propagation starts at the output units) we use an objective function (Classification Figure of Merit <ref> [10] </ref>), which tries to maximize the distance between the activation of the correct output unit and the activation of the best incorrect output unit. 3. DATA COLLECTION AND PREPROCESSING The databases used for training and testing of the MS-TDNN were collected at the University of Karlsruhe.
References-found: 10

