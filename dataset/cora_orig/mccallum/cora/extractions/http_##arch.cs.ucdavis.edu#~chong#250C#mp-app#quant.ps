URL: http://arch.cs.ucdavis.edu/~chong/250C/mp-app/quant.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C.html
Root-URL: http://www.cs.ucdavis.edu
Title: A Quantitative Study of Parallel Scientific Applications with Explicit Communication  
Author: R. Cypher A. Ho flfl S. Konstantinidou P. Messina 
Affiliation: Computer Science Department, Johns Hopkins University flfl IBM Research Division, Almaden Research Center Caltech Concurrent Supercomputing Facilities, California Institute of Technology  
Abstract: This paper studies the behavior of scientific applications running on distributed memory parallel computers. Our goal is to quantify the floating point, memory, I/O and communication requirements of highly parallel scientific applications that perform explicit communication. In addition to quantifying these requirements for fixed problem sizes and numbers of processors, we develop analytical models for the effects of changing the problem size and the degree of parallelism for several of the applications. The contribution of our paper is that it provides quantitative data about real parallel scientific applications in a manner that is largely independent of the specific machine on which the application was run. Such data are clearly very valuable to an architect who is designing a new parallel computer, and they were not previously available. For example, the majority of research papers in interconnection networks have used simulated communication loads consisting of fixed size messages. The data presented in this paper show that this is unrealistic, and they can be used to generate more realistic communication loads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and A. Gupta. </author> <title> Memory-reference characteristics of multiprocessor applications under MACH. </title> <booktitle> In Proc. 1988 ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 215-225. </pages>
Reference-contexts: In addition, the specific design of an I/O system or a communication system depends on the traffic patterns that the system must support. Several studies of the architectural requirements of parallel applications that use a shared-memory model have been performed <ref> [1, 3, 13, 15] </ref>. The characteristics of applications with explicit communication only recently have come under scrutiny. The communication characteristics of CAD applications and numeric algorithms running on a 16 node hypercube have been considered previously [5].
Reference: [2] <author> C. Baillie and D. Walker. </author> <title> Lattice QCD as a large scale scientific computation. </title> <type> Technical Report C 3 P-641, </type> <institution> California Institute of Technology, </institution> <year> 1988. </year>
Reference-contexts: The data were provided by Roy Williams at the Caltech Concurrent Supercomputing Facilities. Quantum Chromodynamics. Quantum chromodynamics (QCD) is the theory of quarks and gluons and how they interact to form particles such as protons and neutrons. The QCD application <ref> [11, 4, 2, 10] </ref> studied here operates on a 4-dimensional periodic lattice (torus). The QCD code performs numerous stages of simulation, where each stage loops through the lattice edges and updates them.
Reference: [3] <author> F. Darema-Rogers, G. Pfister and K. </author> <title> So. Memory access patterns of parallel scientific programs. </title> <booktitle> In Proc. 1987 ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 46-57. </pages>
Reference-contexts: In addition, the specific design of an I/O system or a communication system depends on the traffic patterns that the system must support. Several studies of the architectural requirements of parallel applications that use a shared-memory model have been performed <ref> [1, 3, 13, 15] </ref>. The characteristics of applications with explicit communication only recently have come under scrutiny. The communication characteristics of CAD applications and numeric algorithms running on a 16 node hypercube have been considered previously [5].
Reference: [4] <author> J. Flower. </author> <title> Lattice Gauge Theory on a Parallel Computer. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <year> 1987. </year>
Reference-contexts: The data were provided by Roy Williams at the Caltech Concurrent Supercomputing Facilities. Quantum Chromodynamics. Quantum chromodynamics (QCD) is the theory of quarks and gluons and how they interact to form particles such as protons and neutrons. The QCD application <ref> [11, 4, 2, 10] </ref> studied here operates on a 4-dimensional periodic lattice (torus). The QCD code performs numerous stages of simulation, where each stage loops through the lattice edges and updates them.
Reference: [5] <author> J.-M. Hsu and P. Banerjee. </author> <title> Performance measurement and trace driven simulation of parallel CAD and numeric applications on a hypercube multicomputer. </title> <booktitle> In Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pp. 260-269, </pages> <year> 1990. </year>
Reference-contexts: The characteristics of applications with explicit communication only recently have come under scrutiny. The communication characteristics of CAD applications and numeric algorithms running on a 16 node hypercube have been considered previously <ref> [5] </ref>.
Reference: [6] <author> D. Kotz and N. </author> <title> Nieuwejaar Dynamic File-Access Characteristics of a Production Parallel Scientific Workload. </title> <booktitle> In Proc. Supercomputing '94, </booktitle> <pages> pp. 640-649, </pages> <year> 1994. </year>
Reference-contexts: Finally, some applications could be restructured to take advantage of improved I/O performance. For example, REACT currently performs duplicate floating point operations in order to limit the amount of I/O performed. The I/O behavior of parallel aplications have been studied extensively in <ref> [6, 14] </ref>. 4.4 Communication Tables 5 through 7 give information about the number of messages sent between processors and their lengths. The volume of communication per Mflop performed varies from 5KB to 1956KB and averages 329KB.
Reference: [7] <author> A. Leonard. </author> <title> Vortex methods for flow simulation. </title> <journal> J. Computational Physics, </journal> <volume> 37, </volume> <month> 289 </month> <year> (1980). </year>
Reference-contexts: The simulation is run for 100 stages, at which point the resulting matrices are written to disk. 2-D Fluid Flow Using the Vortex Method. The application VORTEX <ref> [7] </ref> models the evolution of vortices in a 2-dimensional fluid. All vortices have an effect on all other vortices, so each stage of the simulation calculates fi (N 2 ) interactions among the N vortices. The vortices are evenly distributed to the processors.
Reference: [8] <author> R. Lucas, K. Wu and R. Dutton. </author> <title> A parallel 3-D Poisson solver on a hypercube multiprocessor. </title> <booktitle> In Proc. IEEE Intl. Conf. on Computer-Aided Design, </booktitle> <pages> pp. 442-445, </pages> <year> 1987. </year>
Reference-contexts: Because the results of the climate studies have not yet been published, we are unable to give the names and affiliations of the application's authors. 3-D Semiconductor Device Simulation. This application, SEMI, is a 3-D semiconductor device simulator that utilizes a parallelized 3-D Poisson solver <ref> [8, 17] </ref>. The simulation domain is approximated by an irregular 3-D grid which is decomposed into contiguous blocks that are allocated to the processors. An iterative method is employed to solve a system of equations.
Reference: [9] <author> P. Li and D. Curkendall. </author> <title> Parallel 3-D perspective rendering. </title> <booktitle> In Proc. First Intel Delta Applications Workshop, Technical Report CCSF-14-92, California Institute of Technology, </booktitle> <pages> pp. 52-58, </pages> <year> 1992. </year>
Reference-contexts: The data were provided by Steve Plimpton at Sandia National Laboratories. 3-D Perspective Rendering. In order to obtain a 3-dimensional perspective view of a planet's surface, it is necessary to integrate satellite image data with surface elevation data. The application RENDER <ref> [9] </ref> integrates a 6000 x 6000 pixel 24-bit color image with 16-bits of elevation data per pixel to obtain 480 x 640 pixel 24-bit output images. The run considered here produces 30 frames of output, which corresponds to approximately 1 second of animation.
Reference: [10] <author> P. Messina, C. Baillie, E. Felten, P. Hipes, R. Williams, A. Alagar, A. Kamrath, R. Leary, W. Pfeiffer, J. Rogers and D. Walker. </author> <title> Benchmarking advanced architecture computers. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 2(3), </volume> <pages> pp. 195-255, </pages> <month> Sept. </month> <year> 1990. </year> <month> 16 </month>
Reference-contexts: The data were provided by Roy Williams at the Caltech Concurrent Supercomputing Facilities. Quantum Chromodynamics. Quantum chromodynamics (QCD) is the theory of quarks and gluons and how they interact to form particles such as protons and neutrons. The QCD application <ref> [11, 4, 2, 10] </ref> studied here operates on a 4-dimensional periodic lattice (torus). The QCD code performs numerous stages of simulation, where each stage loops through the lattice edges and updates them.
Reference: [11] <author> S. Otto. </author> <title> Monte Carlo Methods in Lattice Gauge Theories. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <year> 1983. </year>
Reference-contexts: The data were provided by Roy Williams at the Caltech Concurrent Supercomputing Facilities. Quantum Chromodynamics. Quantum chromodynamics (QCD) is the theory of quarks and gluons and how they interact to form particles such as protons and neutrons. The QCD application <ref> [11, 4, 2, 10] </ref> studied here operates on a 4-dimensional periodic lattice (torus). The QCD code performs numerous stages of simulation, where each stage loops through the lattice edges and updates them.
Reference: [12] <author> S. Plimpton and G. Heffelfinger. </author> <title> Scalable parallel molecular dynamics on MIMD supercomputers. </title> <booktitle> In Proc. Scalable High Performance Computing Conf., </booktitle> <pages> pp. 246-251, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: The application studied here, MOLECULE, assumes that each atom interacts only with other atoms that are within a fixed cut-off distance <ref> [12] </ref>. The application assigns the N atoms to the P processors, with each processor receiving N=P atoms. In each time step, the states of all of the atoms are sent to all of the processors using a hypercube pattern of communication.
Reference: [13] <author> A. Reddy and P. Banerjee. </author> <title> A study of I/O behavior of perfect benchmarks on a multiprocessor. </title> <booktitle> In Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pp. 312-321, </pages> <year> 1990. </year>
Reference-contexts: In addition, the specific design of an I/O system or a communication system depends on the traffic patterns that the system must support. Several studies of the architectural requirements of parallel applications that use a shared-memory model have been performed <ref> [1, 3, 13, 15] </ref>. The characteristics of applications with explicit communication only recently have come under scrutiny. The communication characteristics of CAD applications and numeric algorithms running on a 16 node hypercube have been considered previously [5].
Reference: [14] <author> J.M. del Rosario and A. Choudhary. </author> <title> High Performance I/O for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> vol. 27, no. 3, </volume> <pages> pp. 59-68, </pages> <year> 1994. </year>
Reference-contexts: Finally, some applications could be restructured to take advantage of improved I/O performance. For example, REACT currently performs duplicate floating point operations in order to limit the amount of I/O performed. The I/O behavior of parallel aplications have been studied extensively in <ref> [6, 14] </ref>. 4.4 Communication Tables 5 through 7 give information about the number of messages sent between processors and their lengths. The volume of communication per Mflop performed varies from 5KB to 1956KB and averages 329KB.
Reference: [15] <author> E. Rothberg, J.P. Singh and A. Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> In Proc. 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 14-26, </pages> <year> 1993. </year>
Reference-contexts: In addition, the specific design of an I/O system or a communication system depends on the traffic patterns that the system must support. Several studies of the architectural requirements of parallel applications that use a shared-memory model have been performed <ref> [1, 3, 13, 15] </ref>. The characteristics of applications with explicit communication only recently have come under scrutiny. The communication characteristics of CAD applications and numeric algorithms running on a 16 node hypercube have been considered previously [5].
Reference: [16] <author> R. Williams. </author> <title> Performance of dynamic load balancing algorithms for unstructured mesh calculations. </title> <journal> Concurrency, </journal> <volume> vol. 3, </volume> <pages> pp. </pages> <month> 457-481 </month> <year> (1991). </year>
Reference-contexts: The resulting image is then sent to disk and the process is repeated with a different viewpoint to obtain the next output image. The data were provided by Peggy Li at the Jet Propulsion Laboratory. 3-D Fluid Flow Using Adaptive Grids. The application EXFLOW <ref> [16] </ref> uses adaptive grids to simulate 3-D fluid flow around a fixed body. EXFLOW begins with a grid of 160 tetrahedra which is distributed among the processors, refined to a working resolution of 1000 tetrahedra, and load-balanced.
Reference: [17] <author> K. Wu, G. Chin and R. Dutton. </author> <title> A STRIDE towards practical 3-D device simulation numerical and visualization considerations. </title> <journal> IEEE Trans. on Computer-Aided Design, </journal> <volume> vol. 10, no. 9, </volume> <pages> pp. 1132-1140, </pages> <year> 1991. </year>
Reference-contexts: Because the results of the climate studies have not yet been published, we are unable to give the names and affiliations of the application's authors. 3-D Semiconductor Device Simulation. This application, SEMI, is a 3-D semiconductor device simulator that utilizes a parallelized 3-D Poisson solver <ref> [8, 17] </ref>. The simulation domain is approximated by an irregular 3-D grid which is decomposed into contiguous blocks that are allocated to the processors. An iterative method is employed to solve a system of equations.
Reference: [18] <author> M. Wu, S. Cuccaro, P. Hipes and A. Kuppermann. </author> <title> Quantum mechanical reactive scattering using a high-performance distributed-memory parallel computer. </title> <journal> Chem. Phys. Lett., </journal> <volume> 168, </volume> <month> 429-440 </month> <year> (1990). </year>
Reference-contexts: Quantum Chemical Reaction Dynamics. Quantum chemical reaction dynamics simulations predict the behavior of chemical reactions based on first principles. The quantum chemical reaction application studied here, REACT <ref> [18, 19, 20] </ref>, simulates a collision between a hydrogen atom and a hydrogen molecule. The application consists of four major phases. In the first phase, a set of 2K eigenvalues and eigenvectors is calculated for each of 200 10K x 10K tridiagonal matrices.
Reference: [19] <author> M. Wu, A. Kuppermann and B. Lepetit. </author> <title> Theoretical calculation experimentally observable consequences of the geometric phase on chemical reaction cross sections. </title> <journal> Chem. Phys. Lett., </journal> <volume> 186, </volume> <month> 319-328 </month> <year> (1991). </year>
Reference-contexts: Quantum Chemical Reaction Dynamics. Quantum chemical reaction dynamics simulations predict the behavior of chemical reactions based on first principles. The quantum chemical reaction application studied here, REACT <ref> [18, 19, 20] </ref>, simulates a collision between a hydrogen atom and a hydrogen molecule. The application consists of four major phases. In the first phase, a set of 2K eigenvalues and eigenvectors is calculated for each of 200 10K x 10K tridiagonal matrices.
Reference: [20] <author> M. Wu and A. Kuppermann. </author> <title> Prediction of the effect of the geometric phase on product rotational state distributions and integral cross sections. </title> <journal> Chem. Phys. Lett., </journal> <volume> 201, </volume> <month> 178-186 </month> <year> (1993). </year> <month> 17 </month>
Reference-contexts: Quantum Chemical Reaction Dynamics. Quantum chemical reaction dynamics simulations predict the behavior of chemical reactions based on first principles. The quantum chemical reaction application studied here, REACT <ref> [18, 19, 20] </ref>, simulates a collision between a hydrogen atom and a hydrogen molecule. The application consists of four major phases. In the first phase, a set of 2K eigenvalues and eigenvectors is calculated for each of 200 10K x 10K tridiagonal matrices.
References-found: 20

