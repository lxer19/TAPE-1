URL: http://polaris.cs.uiuc.edu/newhome/ftp/Parafrase2/papers/mohammad1.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/newhome/ftp/Parafrase2/papers/
Root-URL: http://www.cs.uiuc.edu
Title: Symbolic Analysis for Parallelizing Compilers  
Author: Mohammad R. Haghighat and Constantine D. Polychronopoulos 
Keyword: General terms: Algorithms, Experimentation, Languages, Performance Additional Key Words and Phrases: Dependence analysis, parallelization, symbolic analysis  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: The notion of dependence captures the most important properties of a program for efficient execution on parallel computers. The dependence structure of a program defines the necessary constraints of the order of execution of the program components, and provides sufficient information for the exploitation of the available parallelism. Static discovery and management of the dependence structure of programs saves a tremendous amount of execution time, and dynamic utilization of dependence information results in a significant performance gain on parallel computers. However, experiments with supercomputers indicate that existing multiprocessing environments are unable to deliver the desired performance over a wide range of real applications, mainly due to lack of precision of their dependence information. This calls for an effective compilation scheme capable of understanding the dependence structure of complicated application programs. This paper describes a methodology for the capturing and analyzing program properties that are essential in the effective detection and efficient exploitation of parallelism on parallel computers. Based on this methodology, a symbolic analysis framework is developed for the Parafrase-2 parallelizing compiler. This framework extends the scope of a variety of important program analysis problems, and solves them in a unified way. The attained solution space of these problems is much larger than that handled by existing compiler technology. Such a powerful approach is required for the effective compilation of a large class of application programs. Categories and Subject Descriptors: C.1.2 [Processor Architectures]: Multiple Data Stream Architectures|Array and vector processors, Parallel processors; D.2.6 [Software Engineering]: Programming Environments; D.3.4 [Programming Languages]: Processors|Compilers, Optimization; D.4.1 [Operating Systems]: Process Management|Scheduling; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs; I.2.2 [Artificial Intelligence]: Automatic Programming|Program transformation, Automatic analysis of algorithms 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <month> March </month> <year> 1986. </year>
Reference-contexts: Expressions involving procedure calls with side effects are examples of such cases. The effect of a sequence of assignment operations is the functional composition of the effects of the individual assignments. Thus, a node of a control flow graph <ref> [1] </ref> can be interpreted as an abstract operator which maps an input environment to Symbolic Analysis 5 Commutativity: 8a; b 2 E, S [[a + b]] e = fl S [[b + a]] e = fl S [[a]] e + fl S [[b]] e, Associativity: 8a; b; c 2 E, S <p> Loops may be defined by the notion of dominance relation <ref> [1] </ref>, or by doing a depth first search traversal of the program control flow graph [48]. By loops, we mean natural loops as defined by the dominance relation. Derivation of functional behavior of a given program which has a cyclic flow graph is a non-trivial task. <p> It can be shown that when two natural loops have different headers, they are either disjoint or one is entirely contained (nested) within the other <ref> [1] </ref>. However, when two natural loops have the same header, the loop nesting is ambiguous. In such cases, structure normalization can be used to transform the loops in such a way that each loop is uniquely identified by its header. <p> This scheme of induction variable recognition is a fundamental component of our symbolic analysis framework; it is discussed in the Section 3. 3. INDUCTION VARIABLES Induction variables are of particular interest in optimizing compilers <ref> [1] </ref>; a subclass of them that form arithmetic progressions has a vital role in parallelizing compilers [5, 90]. Induction variables are usually introduced to improve program performance on sequential computers. <p> Replacement of expensive instructions by fast operations is called strength reduction and has been extensively studied for optimizing compilers <ref> [1, 3, 19, 37] </ref>. The classical method of strength reduction is based on recognition of induction variables. Computation via strength reduction is an inherently sequential procedure and is not suitable for parallel machines. <p> In fact, in our symbolic analysis framework, recognition of loop-invariant expressions is a byproduct of induction expression analysis. This approach to loop-invariant expressions is more precise than traditional methods using reaching definitions based on def-use chains <ref> [1] </ref>. To illustrate this, consider the code shown in Fig. 28. Although variable k is modified within the loop, Parafrase-2 has discovered Symbolic Analysis 27 that all occurrences of k inside the loop are invariant. Syntactical methods are not able to recognize this class of loop-invariant expressions. 5. <p> In the general case, finding an optimal solution of code generation has been proven to be NP-Complete; however, for the cases where the DAG representation of expressions is a tree, efficient algorithms exist that find an optimal solution for machines that have a single instruction stream <ref> [1] </ref>. In many cases where the DAG representation of expressions is not a tree, symbolic values of expressions may be used to find an equivalent tree of the DAG and thus generate the optimal code. 9.
Reference: 2. <author> F. E. Allen. </author> <title> Control flow analysis. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 5(7) </volume> <pages> 1-19, </pages> <month> July </month> <year> 1970. </year>
Reference-contexts: Note that the resulting loop nesting, grounded upon natural loops, might be different from that determined by other definitions of loops, such as those based on interval analysis <ref> [2, 18] </ref>, T1-T2 analysis [84], and control dependence analysis [34]. Wolfe has studied the implication of various definitions of loops on loop nesting [92]. Making loops uniquely identifiable by their headers results in a simpler scheme for interpretation of loops. Parafrase-2 normalizes the structure of loops of the following categories.
Reference: 3. <author> F. E. Allen, J. Cocke, and K. Kennedy. </author> <title> Reduction of operator strength. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 79-101. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Replacement of expensive instructions by fast operations is called strength reduction and has been extensively studied for optimizing compilers <ref> [1, 3, 19, 37] </ref>. The classical method of strength reduction is based on recognition of induction variables. Computation via strength reduction is an inherently sequential procedure and is not suitable for parallel machines. <p> PROGRAM OPTIMIZATION Symbolic analysis can be used as a basis for a wide range of program optimizations such as strength reduction, restructuring of arithmetic computations, and Symbolic Analysis 31 elimination of redundant computations. 8.1 Generalized Strength Reduction Strength reduction technique <ref> [3] </ref> can be generalized by considering computations in their entirety rather than their partial computations that appear in their functional definitions [45]. Using the information gathered by symbolic analysis, the compiler can basically perform the inverse of induction variable substitution in sequential loops.
Reference: 4. <author> R. Allen. </author> <title> Dependence analysis for subscripted variables and its application to program transformations. </title> <type> PhD dissertation, </type> <institution> Rice University, Houston, Texas, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: Problems that are solved interprocedurally in our symbolic analysis framework include symbolic constant propagation, generalized induction variable substitution, global forward substitution, and detection of loop invariant computations. These problems have a vital role in dependence analysis <ref> [4, 8, 91] </ref>, the fundamental component of parallelizing compilers. Some of the analytical tools used effectively in our compilation scheme are mathematical methods such as computer algebra, calculus of finite differences, and theorem proving techniques based on number theory. <p> In many cases in which dependence equations contain unknown symbolic terms, a simple symbolic manipulation is sufficient for the purpose of dependence analysis, provided that the unknown symbolic terms are invariant in the loops under consideration <ref> [4, 44] </ref>. k = 0 a (i + k) = b (i) a (i + k) = c (i) end do print*, k CDOALL i = 1,n,2 a (i) = b (i) END DO PRINT *, 0 Fig. 28. Example of loop-invariant expressions recognized by Parafrase-2. <p> Nevertheless, standard dependence tests cannot be applied in many of the unresolved cases, owing to lack of information about the value of certain variables at compile time. Symbolic techniques are required for effective dependence analysis of these cases <ref> [4, 51, 56, 64, 82] </ref>. Our proposed symbolic analysis framework [44] supports a very accurate dependence analysis scheme in the presence of unknown symbolic terms. The code segment shown in Fig. 18 is an example that requires symbolic dependence analysis.
Reference: 5. <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4), </volume> <month> October </month> <year> 1987. </year>
Reference-contexts: 1. INTRODUCTION High performance on parallel computers can be achieved only when they are pro grammed effectively. The complexity of this task and portability issues make auto matic support of parallel program development highly desirable <ref> [5, 67, 69] </ref>. Despite all the effort spent on the automatic parallelization of programs, existing paralleliz ing compilers are not able to deliver the desired performance over a wide range of real applications [12]. <p> INDUCTION VARIABLES Induction variables are of particular interest in optimizing compilers [1]; a subclass of them that form arithmetic progressions has a vital role in parallelizing compilers <ref> [5, 90] </ref>. Induction variables are usually introduced to improve program performance on sequential computers. To illustrate this, we consider the loop of Fig. 15. do i = 1, n a (j) = 1 CDOALL i = 1,n a (2 * i) = 1 Fig. 15.
Reference: 6. <author> Z. Ammarguellat and W. L. Harrison III. </author> <title> Automatic recognition of induction variables and recurrence relations by abstract interpretation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 283-295, </pages> <address> White Plains, New York, </address> <month> June 20-22 </month> <year> 1990. </year>
Reference-contexts: They have been named Generalized Induction Variables or GIVs <ref> [6, 31, 32, 45, 93] </ref>. The sequence of values that a GIV receives throughout the execution of a loop does not necessarily form an arithmetic progression.
Reference: 7. <author> K. R. Apt and E. Olderog. </author> <title> Verification of Sequential and Concurrent Programs. </title> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: In some cases, however, the simplified loop exit equation can be solved symbolically, and the method of constraint propagation [26, 44] may be used to verify the existence of a non-negative solution. For example, consider the source code shown in Fig. 12. This program is given in <ref> [7] </ref>, in studying complications raised in the verification of programs with loops, as an example of a simple loop that is difficult to understand. The program computes, in a very primitive fashion, the integer cubic root of x, b 3 p xc, given that x 0.
Reference: 8. <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer, </publisher> <year> 1988. </year> <note> Symbolic Analysis 45 </note>
Reference-contexts: Problems that are solved interprocedurally in our symbolic analysis framework include symbolic constant propagation, generalized induction variable substitution, global forward substitution, and detection of loop invariant computations. These problems have a vital role in dependence analysis <ref> [4, 8, 91] </ref>, the fundamental component of parallelizing compilers. Some of the analytical tools used effectively in our compilation scheme are mathematical methods such as computer algebra, calculus of finite differences, and theorem proving techniques based on number theory.
Reference: 9. <author> U. Banerjee. </author> <title> Loop Transformations for Restructuring Compilers, Volume II: Loop Paral-lelization. </title> <publisher> Kluwer, </publisher> <year> 1994. </year>
Reference-contexts: We plan to investigate these problems using our symbolic analysis system. As the first application, we are studying the effectiveness of Banerjee's theory of loop transformations <ref> [9] </ref>, extended with symbolic dependence information provided by our techniques. We are also studying techniques to optimize the compilation time of programs in our framework. Currently, Parafrase-2 compiles real-life Fortran applications with a speed of 3000 lines per wall clock minute on Sparc/10 workstations.
Reference: 10. <author> I. Barani and Z. Furedi. </author> <title> Computing the volume is difficult. </title> <booktitle> In 18th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 442-447, </pages> <address> Berkeley, California, </address> <year> 1986. </year>
Reference-contexts: Computing the volume of convex sets, in the general case, is proven to be an exponential problem in terms of the dimension of the convex set <ref> [10] </ref>. Even the probabilistic algorithm for finding the Euclidian volume of convex bodies, proposed in [30], which has a polynomial time complexity is computationally very expensive. However, the convex polytopes that we consider have a nice property that makes computation of their volumes much easier.
Reference: 11. <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Within this framework, symbolic analysis is used as an abstract interpretation technique to solve a variety of interprocedural flow analysis problems in a unified way. We studied the effectiveness of our techniques on the Perfect Benchmarks r fl <ref> [11] </ref>, a suite of Fortran programs representative of real engineering and scientific codes. Problematic cases, identified by other studies on automatic parallelization [31, 32, 80], were the focus of our experimentation.
Reference: 12. <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect benchmark programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Despite all the effort spent on the automatic parallelization of programs, existing paralleliz ing compilers are not able to deliver the desired performance over a wide range of real applications <ref> [12] </ref>. The challenging problem confronting designers of paralleliz ing compilers is the difficulty of collecting sufficient information in an efficient way to utilize the underlying architecture. This work was supported in part by the National Science Foundation under Grant No. NSF-CCR-89-57310 and Texas Instruments.
Reference: 13. <author> W. J. Blume. </author> <title> Success and limitations in automatic parallelization of the Perfect Benchmark programs. M.S. </title> <type> thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: In fact, one of the reasons for the poor performance of parallelizing compilers on program SPEC77 is the perceived, but nonexistent, data dependences as a result of compilers limitations on handling coupled induction variables with multiple assignments <ref> [13] </ref>. Derivation of induction variables in these subroutines would further help in identifying the range of array accesses, proving privatizability of array a, and parallelizing the caller loops. Subroutines CPASS, CPASSM, RPASS, and RPASSM from the program MG3D have also loops with similar structure.
Reference: 14. <author> B. Buchberger and R. Loos. </author> <title> Algebraic simplification. </title> <editor> In B. Buchberger, G. E. Collins, and R. Loos, editors, </editor> <booktitle> Computer Algebra: Symbolic and Algebraic Computation, </booktitle> <pages> pages 11-43. </pages> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: In [76] Richardson showed that the zero equivalence problem is recursively unsolvable for a sufficiently rich class of transcendental expressions. Theoretical limitations and other unsolvability results concerning transcendental terms in computer algebra may be found in <ref> [14, 16, 66] </ref>. The zero equivalence problem, however, can be solved in many cases of practical interest. In particular, the case of polynomials does not pose any difficulties. There is a variety of choices for canonical representation of polynomials [28, 39].
Reference: 15. <author> D. Callahan, J. Dongarra, and D. Levine. </author> <title> Vectorizing compilers: A test suite and results. </title> <booktitle> In Supercomputing 88, </booktitle> <year> 1988. </year>
Reference-contexts: We also studied performance of three of the state-of-the-art parallelizing compilers: Cedar Fortran compiler, based on a 1991 version of KAP, from Kuck and Associates [62]; the Titan 1500/3000 compilation system from Kubota Pacific Computer Inc.; and the Alliant FX/Fortran compiler, based on VAST-2 preprocessor, found in <ref> [15] </ref> to be one of the best commercial vectorizers. In this paper we do not attempt a comparative study of these compilers, but instead, we focus on cases from real applications where the techniques employed by these compilers were ineffective. <p> Other considered compilers are the 1991 version of KAP, from Kuck and Associates [62], the Titan 1500/3000 compilation system from Kubota Pacific Computer Inc., and the Alliant Fortran compiler, found in <ref> [15] </ref> to be one of the best commercial vectorizers. While Titan compiler was good at single basic block symbolic analysis, Parafrase-2 did an excellent job in a global frame. A summary of this study is shown in Table II. Table II. Comparison of program analysis capabilities of parallelizing compilers.
Reference: 16. <author> B. F. Caviness. </author> <title> On canonical forms and simplification. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 17(2) </volume> <pages> 385-396, </pages> <month> April </month> <year> 1970. </year>
Reference-contexts: In [76] Richardson showed that the zero equivalence problem is recursively unsolvable for a sufficiently rich class of transcendental expressions. Theoretical limitations and other unsolvability results concerning transcendental terms in computer algebra may be found in <ref> [14, 16, 66] </ref>. The zero equivalence problem, however, can be solved in many cases of practical interest. In particular, the case of polynomials does not pose any difficulties. There is a variety of choices for canonical representation of polynomials [28, 39].
Reference: 17. <author> C. L. Chang and R. C. T. Lee. </author> <title> Symbolic Logic and Mechanical Theorem Proving. </title> <publisher> Academic Press, </publisher> <year> 1973. </year>
Reference-contexts: External side effects such as input/output statements are also taken into account. The derived knowledge about actual parameters of procedures can be used to simplify [87] or partially evaluate [54] the procedures. Procedure specialization <ref> [17] </ref>, also known as procedure cloning [22], can be very beneficial in the exploitation and enhancement of the parallelism available in programs. subroutine test (a, b, n) real a (*), b (2 * n, *) implicit integer (a-z) m = 1 k = 2 last = k do i = 1,
Reference: 18. <author> J. Cocke. </author> <title> Global common subexpression elimination. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 5(7) </volume> <pages> 20-24, </pages> <month> July </month> <year> 1970. </year>
Reference-contexts: Note that the resulting loop nesting, grounded upon natural loops, might be different from that determined by other definitions of loops, such as those based on interval analysis <ref> [2, 18] </ref>, T1-T2 analysis [84], and control dependence analysis [34]. Wolfe has studied the implication of various definitions of loops on loop nesting [92]. Making loops uniquely identifiable by their headers results in a simpler scheme for interpretation of loops. Parafrase-2 normalizes the structure of loops of the following categories.
Reference: 19. <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the Association for Computing Machinery, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: Replacement of expensive instructions by fast operations is called strength reduction and has been extensively studied for optimizing compilers <ref> [1, 3, 19, 37] </ref>. The classical method of strength reduction is based on recognition of induction variables. Computation via strength reduction is an inherently sequential procedure and is not suitable for parallel machines.
Reference: 20. <author> J. Cocke and J. T. Schwartz. </author> <title> Programming Languages and Their Compilers. </title> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York, </address> <note> second revised edition edition, </note> <year> 1970. </year>
Reference-contexts: Parafrase-2 selects a loop stride that simplifies induction variables. 4. RECOGNITION OF LOOP-INVARIANT COMPUTATIONS Loop-invariant expressions are of particular interest in optimizing compilers. Their recognition is a basis for code motion <ref> [20, 74] </ref>; an important loop optimization technique that improves program performance by reducing frequency of execution of some instructions. In parallelizing compilers, on the other hand, recognition of loop-invariant expressions has an important role in dependence analysis.
Reference: 21. <author> J. Cohen and J. Katcoff. </author> <title> Symbolic solution of finite-difference equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 3(3) </volume> <pages> 261-271, </pages> <month> September </month> <year> 1977. </year>
Reference-contexts: Symbolic Analysis 13 interdependences between different recurrences might be cyclic; x may depend on y, which depends on x. Due to its importance in complexity analysis of algorithms, automatic solution of difference equations has been the subject of several studies <ref> [21, 42, 52, 57, 68] </ref>. Parafrase-2 uses the method of finite differences to solve the system of recurrence relations obtained by symbolic interpretation of loops.
Reference: 22. <author> K. D. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proceedings of the Fourth IEEE International Conference on Computer Languages, </booktitle> <pages> pages 96-105, </pages> <address> Oakland, California, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: External side effects such as input/output statements are also taken into account. The derived knowledge about actual parameters of procedures can be used to simplify [87] or partially evaluate [54] the procedures. Procedure specialization [17], also known as procedure cloning <ref> [22] </ref>, can be very beneficial in the exploitation and enhancement of the parallelism available in programs. subroutine test (a, b, n) real a (*), b (2 * n, *) implicit integer (a-z) m = 1 k = 2 last = k do i = 1, 2 * n call f (a
Reference: 23. <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <address> Los Angeles, California, </address> <month> January </month> <year> 1977. </year>
Reference-contexts: This can be achieved by specifying the relationship between actual values and their descriptions, and by establishing a connection between the operations on the actual values and those on the corresponding abstract values. This theory of semantics approximation is called abstract interpretation <ref> [23, 24, 25] </ref>. The term approximate does not imply any possibilities of incorrectness, but rather the lack of full information is meant. Roughly speaking, in the method of abstract interpretation the compiler employs an interpreter to execute programs in an abstract domain, in order to discover their properties.
Reference: 24. <author> P. Cousot and R. Cousot. </author> <title> Systematic design of program analysis frameworks. </title> <booktitle> In Proceedings of the 6th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 269-282, </pages> <address> San Antonio, Texas, </address> <month> January </month> <year> 1979. </year>
Reference-contexts: This can be achieved by specifying the relationship between actual values and their descriptions, and by establishing a connection between the operations on the actual values and those on the corresponding abstract values. This theory of semantics approximation is called abstract interpretation <ref> [23, 24, 25] </ref>. The term approximate does not imply any possibilities of incorrectness, but rather the lack of full information is meant. Roughly speaking, in the method of abstract interpretation the compiler employs an interpreter to execute programs in an abstract domain, in order to discover their properties.
Reference: 25. <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation frameworks. </title> <journal> Journal of Logic and Computation, </journal> <volume> 2(4) </volume> <pages> 511-547, </pages> <year> 1992. </year>
Reference-contexts: This can be achieved by specifying the relationship between actual values and their descriptions, and by establishing a connection between the operations on the actual values and those on the corresponding abstract values. This theory of semantics approximation is called abstract interpretation <ref> [23, 24, 25] </ref>. The term approximate does not imply any possibilities of incorrectness, but rather the lack of full information is meant. Roughly speaking, in the method of abstract interpretation the compiler employs an interpreter to execute programs in an abstract domain, in order to discover their properties.
Reference: 26. <author> P. Cousot and N. Halbwachs. </author> <title> Automatic discovery of linear restraints among variables of a program. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 84-97, </pages> <address> Tucson, Arizona, </address> <month> January </month> <year> 1978. </year>
Reference-contexts: Example of analysis of loop termination by Parafrase-2. In some cases, however, the simplified loop exit equation can be solved symbolically, and the method of constraint propagation <ref> [26, 44] </ref> may be used to verify the existence of a non-negative solution. For example, consider the source code shown in Fig. 12. <p> The problem of determining number of iterations of a loop is of great importance in handling multiply nested loops. The technique of constraint propagation <ref> [26, 44] </ref> can often be used to prove the execution of loops for at least one iteration, and thus avoid the complications that arise from having max and min functions in the expressions.
Reference: 27. <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Currently, Parafrase-2 compiles real-life Fortran applications with a speed of 3000 lines per wall clock minute on Sparc/10 workstations. Although this speed is cost effective, it can be further improved by employing more sparse representations suitable for symbolic analysis. Global Value Graph [75] and Static Single Assignment <ref> [27] </ref> have been shown to be bases for efficient compiler algorithms, and a number of studies have used the Static Single Assignment form for solving induction variables [93] and symbolic analysis [46]. 12.
Reference: 28. <author> J. H. Davenport, Y. Siret, and E. Tournier. </author> <title> Computer Algebra: Systems and Algorithms for Algebraic Computation. </title> <publisher> Academic, </publisher> <address> second edition, </address> <year> 1993. </year>
Reference-contexts: The zero equivalence problem, however, can be solved in many cases of practical interest. In particular, the case of polynomials does not pose any difficulties. There is a variety of choices for canonical representation of polynomials <ref> [28, 39] </ref>. We represent multivariate polynomials in a sparse distributive canonical form using the lexicographical ordering of program variables. In this form, only non zero elements are represented, with the exception of zero polynomial. For example, j fl (3k + 2i) will be represented as 2ij + 3jk.
Reference: 29. <author> S. K. Debray and N. W. Lin. </author> <title> Cost analysis of logic programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(5) </volume> <pages> 826-875, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Such a metric for analysis of algorithms is proposed by Knuth [58] and used by Wegbreit [85, 88], Ramshaw [73], and Hickey and Cohen [49]. Semi-automatic worst-case analysis of a large class of logic program is studied by Debray and Lin <ref> [29] </ref>. The performance measure should also take into account architectural issues such as the type of memory accesses in computers with hierarchical memories. In par-allelizing compilers, the performance measure is especially useful for choosing the appropriate choice of transformations and scheduling strategies.
Reference: 30. <author> M. Dyer, A. Frieze, and R. Kannan. </author> <title> A random polynomial time algorithm for approximating the volume of convex bodies. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 38(1) </volume> <pages> 1-17, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Computing the volume of convex sets, in the general case, is proven to be an exponential problem in terms of the dimension of the convex set [10]. Even the probabilistic algorithm for finding the Euclidian volume of convex bodies, proposed in <ref> [30] </ref>, which has a polynomial time complexity is computationally very expensive. However, the convex polytopes that we consider have a nice property that makes computation of their volumes much easier. In the mathematical sense, these polytopes are special cases of cylinders. An algebraic 38 M. R. Haghighat and C. D.
Reference: 31. <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Z. Li, and D. Padua. </author> <title> Restructuring Fortran programs for cedar. </title> <booktitle> In Proceedings of the 1991 ICPP, </booktitle> <volume> volume I, </volume> <pages> pages 57-66, </pages> <address> St. Charles, Illinois, </address> <month> August 12-17 </month> <year> 1991. </year> <note> 46 M. </note> <author> R. Haghighat and C. D. </author> <note> Polychronopoulos </note>
Reference-contexts: We studied the effectiveness of our techniques on the Perfect Benchmarks r fl [11], a suite of Fortran programs representative of real engineering and scientific codes. Problematic cases, identified by other studies on automatic parallelization <ref> [31, 32, 80] </ref>, were the focus of our experimentation. <p> The attained solution space of these problems in our framework was much larger than that handled by the existing compiler technology. In particular, our symbolic analyzer was able to automatically solve the most complicated cases of induction analysis identified in <ref> [31, 32, 80] </ref>. Problems that are solved interprocedurally in our symbolic analysis framework include symbolic constant propagation, generalized induction variable substitution, global forward substitution, and detection of loop invariant computations. These problems have a vital role in dependence analysis [4, 8, 91], the fundamental component of parallelizing compilers. <p> They have been named Generalized Induction Variables or GIVs <ref> [6, 31, 32, 45, 93] </ref>. The sequence of values that a GIV receives throughout the execution of a loop does not necessarily form an arithmetic progression. <p> In the program OCEAN, one loop that performs 40% of the operations of the whole program could be parallelized after recognition and substitution of the corresponding generalized induction variable. This results in a speedup of 8:1 on the Cedar multiprocessor <ref> [31] </ref>. 3.2 Generalized Induction Expressions The notion of GIVs can further be generalized to what we call Generalized Induction Expressions.
Reference: 32. <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect-Benchmark programs. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 65-82. </pages> <publisher> Springer-Verlag, LNCS 589, </publisher> <month> August 7-9 </month> <year> 1991. </year>
Reference-contexts: We studied the effectiveness of our techniques on the Perfect Benchmarks r fl [11], a suite of Fortran programs representative of real engineering and scientific codes. Problematic cases, identified by other studies on automatic parallelization <ref> [31, 32, 80] </ref>, were the focus of our experimentation. <p> The attained solution space of these problems in our framework was much larger than that handled by the existing compiler technology. In particular, our symbolic analyzer was able to automatically solve the most complicated cases of induction analysis identified in <ref> [31, 32, 80] </ref>. Problems that are solved interprocedurally in our symbolic analysis framework include symbolic constant propagation, generalized induction variable substitution, global forward substitution, and detection of loop invariant computations. These problems have a vital role in dependence analysis [4, 8, 91], the fundamental component of parallelizing compilers. <p> This complicates the simplification of expressions that use the final value of the induction variables. This problem, called zero-trip loop problem <ref> [32] </ref>, can cause problems in the analysis of nested loops and will be addressed in Section 9.3.1. Fig. 13 shows the result of loop analysis and induction expression substitution, performed by Parafrase-2, in the case of a loop with multiple exits. <p> They have been named Generalized Induction Variables or GIVs <ref> [6, 31, 32, 45, 93] </ref>. The sequence of values that a GIV receives throughout the execution of a loop does not necessarily form an arithmetic progression. <p> Transformation of the code segment of Fig. 18 by Parafrase-2. The code segment shown in Fig. 18 account for 29% of the overall execution time of the program TRFD. Its parallelization requires recognition of generalized induction variables, and results in a loop speedup of 12:3 on the Cedar multiprocessor <ref> [32] </ref>. The most important loop of the program TRFD accounts for 69% of the overall execution time of the program. Parallelization of that loop, also, requires capability of handling non-linear induction variables, and results in a loop speedup of 16:4 on the Cedar multiprocessor [32]. <p> of 12:3 on the Cedar multiprocessor <ref> [32] </ref>. The most important loop of the program TRFD accounts for 69% of the overall execution time of the program. Parallelization of that loop, also, requires capability of handling non-linear induction variables, and results in a loop speedup of 16:4 on the Cedar multiprocessor [32]. In the program OCEAN, one loop that performs 40% of the operations of the whole program could be parallelized after recognition and substitution of the corresponding generalized induction variable.
Reference: 33. <author> M. Eisenberg. </author> <title> Axiomatic Theory of Sets and Classes. </title> <publisher> Holt, Rinehart and winston, Inc., </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: It should be noted that 0 0 is one of the indeterminate forms in analysis, but is defined to be one in axiomatic number theory <ref> [33] </ref>. All the computers that we are aware of evaluate the integer expression 0 0 to one.
Reference: 34. <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Note that the resulting loop nesting, grounded upon natural loops, might be different from that determined by other definitions of loops, such as those based on interval analysis [2, 18], T1-T2 analysis [84], and control dependence analysis <ref> [34] </ref>. Wolfe has studied the implication of various definitions of loops on loop nesting [92]. Making loops uniquely identifiable by their headers results in a simpler scheme for interpretation of loops. Parafrase-2 normalizes the structure of loops of the following categories.
Reference: 35. <author> L. E. Flynn and S. Flynn Hummel. </author> <title> Scheduling variable-length parallel subtasks. </title> <type> Technical Report RC15492, </type> <institution> IBM T.J. Watson Research Center, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: Between these two schemes lie other approaches that attempt to compromise load balancing and minimizing overhead. Guided self-scheduling [71], factoring <ref> [35, 36] </ref>, and trapezoidal self-scheduling [83] are examples of such schemes. n = 50 DOALL i = 1, n*n INTEGER k DO k = i, n*n END DO END DOALL Fig. 31. Adjoint-convolution program and its parallel work.
Reference: 36. <author> S. Flynn Hummel, E. Schonberg, and L. E. Flynn. </author> <title> Factoring: A method for scheduling parallel loops. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Between these two schemes lie other approaches that attempt to compromise load balancing and minimizing overhead. Guided self-scheduling [71], factoring <ref> [35, 36] </ref>, and trapezoidal self-scheduling [83] are examples of such schemes. n = 50 DOALL i = 1, n*n INTEGER k DO k = i, n*n END DO END DOALL Fig. 31. Adjoint-convolution program and its parallel work. <p> In the case of coarse-grained iterations with variable execution times, self-scheduling performs much better than chunk scheduling, while fine-grained iterations with constant execution time favor chunk scheduling. To compare the performance of balanced chunk scheduling against other schemes, we have selected two examples from <ref> [36] </ref> that cover a wide spectrum of characteristics. The benchmark program shown in Fig. 31 is an adjoint-convolution. The granularity of the parallel work is large, with a great deal of variance among Symbolic Analysis 33 the parallel iterations.
Reference: 37. <author> A. C. Fong and J. D. Ullman. </author> <title> Induction variables in very high level languages. </title> <booktitle> Conf. Rec. Third ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 104-112, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: Replacement of expensive instructions by fast operations is called strength reduction and has been extensively studied for optimizing compilers <ref> [1, 3, 19, 37] </ref>. The classical method of strength reduction is based on recognition of induction variables. Computation via strength reduction is an inherently sequential procedure and is not suitable for parallel machines.
Reference: 38. <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability, A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, California, </address> <year> 1979. </year>
Reference-contexts: This problem is clearly unsolvable in the general case, otherwise the halting problem <ref> [38] </ref> would have been decidable. However, many instances of this problem are solvable. For example, consider the code shown in Fig. 37. This subroutine, taken from Numerical Recipes [72], is a complete Cholesky decomposition for solving linear equations.
Reference: 39. <author> K. O. Geddes, S. R. Czapor, and G. Labahn. </author> <title> Algorithms for Computer Algebra. </title> <publisher> Kluwer, </publisher> <year> 1992. </year>
Reference-contexts: The zero equivalence problem, however, can be solved in many cases of practical interest. In particular, the case of polynomials does not pose any difficulties. There is a variety of choices for canonical representation of polynomials <ref> [28, 39] </ref>. We represent multivariate polynomials in a sparse distributive canonical form using the lexicographical ordering of program variables. In this form, only non zero elements are represented, with the exception of zero polynomial. For example, j fl (3k + 2i) will be represented as 2ij + 3jk.
Reference: 40. <author> M. B. Girkar. </author> <title> Functional Parallelism : Theoretical Foundations and Implementation. </title> <type> PhD dissertation, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: In the case of dynamic scheduling, the compiler can use the generalized strength reduction scheme 32 M. R. Haghighat and C. D. Polychronopoulos of Section 8 to instrument the code in such a way that each task of the program HTG <ref> [40] </ref> has its own metric computed in an efficient way.
Reference: 41. <author> T. Gonzalez and J. Ja'Ja'. </author> <title> Evaluation of arithmetic expressions with algebraic identities. </title> <journal> SIAM J. of Computing, </journal> <volume> 11(4) </volume> <pages> 633-662, </pages> <month> November 82. </month>
Reference-contexts: the operations in the sequential loops, while it does not prohibit the concurrent execution of parallel loops. 8.2 Restructuring of Arithmetic Computations To evaluate an arithmetic expression under a set of algebraic laws, the compiler can generate code for any equivalent expression, obtained by successive applications of the algebraic laws <ref> [41] </ref>. An optimality criterion can be defined based on the cost of computations.
Reference: 42. <author> R. W. Gosper Jr. </author> <title> Decision procedure for indefinite hypergeometric summation. </title> <journal> In Proc. Nat. Acad. Sci. </journal> <volume> 75, 1, </volume> <pages> pages 40-42, </pages> <month> January </month> <year> 1978. </year>
Reference-contexts: Symbolic Analysis 13 interdependences between different recurrences might be cyclic; x may depend on y, which depends on x. Due to its importance in complexity analysis of algorithms, automatic solution of difference equations has been the subject of several studies <ref> [21, 42, 52, 57, 68] </ref>. Parafrase-2 uses the method of finite differences to solve the system of recurrence relations obtained by symbolic interpretation of loops.
Reference: 43. <author> M. R. Haghighat. </author> <title> Symbolic Analysis for High Performance Parallelizing Compilers. </title> <type> PhD dissertation, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: This test is based on the following lemma. Lemma 2.0.1. A polynomial p n over ZZ is divisible by k!, k 2 IN, if and only if p n is divisible by k! when n assumes a sequence of k consecutive integers. Proof. See <ref> [43] </ref>. Lemma 2.0.1 indicates that the divisibility by 2 of p 0 and p 1 implies the divisibility by 2 of p n for all n 2 ZZ. <p> It can be shown that the upper bound of k is max i p i n i , where Q n i is the prime factorization of m <ref> [43] </ref>. For example, the divisibility by 60 of p n is implied by the divisibility by 60 of p 0 ; p 1 ; p 2 ; p 3 ; and p 4 , since 60 divides 5! = 120. <p> In fact, the problem remains undecidable, even if the loop exit equation is restricted to be a polynomial in the loop index variable, of arbitrary degree, with constant coefficients. The proof <ref> [43] </ref> is a trivial implication of the Matijasevic's result on the unsolvability of the Hilbert's Tenth Problem [53]. 14 M. R. Haghighat and C. D. <p> In such a case, is called the characteristic function of the induction expression ". It can be shown that, in the general case, it is impossible to decide whether a given expression of a loop is an induction expression <ref> [43] </ref>. An induction variable of a loop may have different characteristic functions at different points of the loop. <p> Proof. See <ref> [43] </ref>. In the code segment of Fig. 19, let the value of the subscript expression of the array xijkl at the iteration (mi; mj; mk; ml) be denoted by e (mi; mj; mk; ml). <p> Corollary 9.0.1. For any a; b 2 ffalse, trueg, the following hold: (1) t (a) + t (a) = 1. (3) t (a ^ b) = t (a)t (b). Proof. See <ref> [43] </ref>. 2 The function t is simple, but very powerful. Many mathematical functions, including abs, max, min, sign, positive part, negative part, and Kronecker delta, can easily be defined by t . A special case of t function is especially useful for mechanical manipulation of relations between program variables. <p> Proof. See <ref> [43] </ref>. 2 Many instances of the zero trip problem can be solved using the above algebra. Example 9.1.
Reference: 44. <author> M. R. Haghighat and C. D. Polychronopoulos. </author> <title> Symbolic dependence analysis for high-performance parallelizing compilers. </title> <editor> In A. Nicolau, D. Gelernter, T. Gross, and D. Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 310-330. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: Example of analysis of loop termination by Parafrase-2. In some cases, however, the simplified loop exit equation can be solved symbolically, and the method of constraint propagation <ref> [26, 44] </ref> may be used to verify the existence of a non-negative solution. For example, consider the source code shown in Fig. 12. <p> The problem of determining number of iterations of a loop is of great importance in handling multiply nested loops. The technique of constraint propagation <ref> [26, 44] </ref> can often be used to prove the execution of loops for at least one iteration, and thus avoid the complications that arise from having max and min functions in the expressions. <p> In many cases in which dependence equations contain unknown symbolic terms, a simple symbolic manipulation is sufficient for the purpose of dependence analysis, provided that the unknown symbolic terms are invariant in the loops under consideration <ref> [4, 44] </ref>. k = 0 a (i + k) = b (i) a (i + k) = c (i) end do print*, k CDOALL i = 1,n,2 a (i) = b (i) END DO PRINT *, 0 Fig. 28. Example of loop-invariant expressions recognized by Parafrase-2. <p> Nevertheless, standard dependence tests cannot be applied in many of the unresolved cases, owing to lack of information about the value of certain variables at compile time. Symbolic techniques are required for effective dependence analysis of these cases [4, 51, 56, 64, 82]. Our proposed symbolic analysis framework <ref> [44] </ref> supports a very accurate dependence analysis scheme in the presence of unknown symbolic terms. The code segment shown in Fig. 18 is an example that requires symbolic dependence analysis.
Reference: 45. <author> M. R. Haghighat and C. D. Polychronopoulos. </author> <title> Symbolic program analysis and optimization for parallelizing compilers. </title> <booktitle> In Proceedings of the 5th Annual Workshop on Languages and Compilers for Parallel Computing, volume 757 of Lecture Notes in Computer Science, </booktitle> <pages> pages 538-562, </pages> <address> New Haven, Connecticut, </address> <month> August </month> <year> 1992. </year> <note> Springer-Verlag. </note>
Reference-contexts: A single back edge goes from the new node to the header. Fig. 8 shows this transformation together with induction variable substitution, performed by Parafrase-2. Fixed do loops that have if terminators belong to this category <ref> [45] </ref>. (2) Loops that are contained in each other and share the loop header in such cases it is natural to assume that the loop that is contained in another one is nested within it (although from the point of view of frequency of execution, it could Symbolic Analysis 11 k <p> They have been named Generalized Induction Variables or GIVs <ref> [6, 31, 32, 45, 93] </ref>. The sequence of values that a GIV receives throughout the execution of a loop does not necessarily form an arithmetic progression. <p> Parafrase-2 is able to handle even more complicated cases where the loop bounds are not necessarily linear expressions of enclosing loop index variables, but rather polynomials of arbitrary degrees in terms of the enclosing loop index variables and invariants <ref> [45] </ref>. <p> a wide range of program optimizations such as strength reduction, restructuring of arithmetic computations, and Symbolic Analysis 31 elimination of redundant computations. 8.1 Generalized Strength Reduction Strength reduction technique [3] can be generalized by considering computations in their entirety rather than their partial computations that appear in their functional definitions <ref> [45] </ref>. Using the information gathered by symbolic analysis, the compiler can basically perform the inverse of induction variable substitution in sequential loops.
Reference: 46. <author> P. Havlak. </author> <title> Interprocedural Symbolic Analysis. </title> <type> PhD dissertation, </type> <institution> Department of Computer Science, Rice University, </institution> <year> 1994. </year>
Reference-contexts: Global Value Graph [75] and Static Single Assignment [27] have been shown to be bases for efficient compiler algorithms, and a number of studies have used the Static Single Assignment form for solving induction variables [93] and symbolic analysis <ref> [46] </ref>. 12. CONCLUSION Existing parallelizing compilers are unable to detect and exploit effectively the available parallelism in real programs, mainly due to lack of accuracy in their analysis. The most important factor for the effective detection and management of parallelism is the precision of dependence information.
Reference: 47. <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <year> 1991. </year>
Reference-contexts: Since there does not remain any dependence between various iterations of the loop, it can run in parallel. This approach is especially useful in interprocedural dependence analysis techniques that summarize array accesses in terms of regular sections <ref> [47] </ref>. 6. DEAD-CODE ELIMINATION Exploitation of information that reaches a conditional statement may show that the condition is always false on all incoming control flow paths. In such a case, an unreachable code is identified.
Reference: 48. <author> M. S. Hecht. </author> <title> Flow Analysis of Computer Programs. </title> <publisher> Elsevier North-Holland, </publisher> <year> 1977. </year>
Reference-contexts: Loops may be defined by the notion of dominance relation [1], or by doing a depth first search traversal of the program control flow graph <ref> [48] </ref>. By loops, we mean natural loops as defined by the dominance relation. Derivation of functional behavior of a given program which has a cyclic flow graph is a non-trivial task. Loops might introduce an infinite number of execution paths in a given program.
Reference: 49. <author> T. Hickey and J. Cohen. </author> <title> Automating program analysis. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(1) </volume> <pages> 185-220, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Symbolic analysis can even provide statistical information such as minimum, maximum, average, and standard deviation of the execution time of iterations of program loops. Such a metric for analysis of algorithms is proposed by Knuth [58] and used by Wegbreit [85, 88], Ramshaw [73], and Hickey and Cohen <ref> [49] </ref>. Semi-automatic worst-case analysis of a large class of logic program is studied by Debray and Lin [29]. The performance measure should also take into account architectural issues such as the type of memory accesses in computers with hierarchical memories.
Reference: 50. <author> T. W. Hungerford. </author> <title> Algebra. </title> <publisher> Springer-Verlag, </publisher> <year> 1974. </year>
Reference-contexts: The reason is that we need to know the ranges of i, for which g (i) f (i). That means, we have to be able to solve g (i) - f (i) 0. But, it is a well known fact in algebra <ref> [50] </ref> that for any n 5, there are polynomials of degree n with integer coefficients whose roots cannot be expressed in terms of radicals of roots of polynomials of degrees 4. k = 0 do j = f (i), g (i) end do Fig. 39.
Reference: 51. <author> F. Irigoin. </author> <title> Interprocedural analyses for programming environments. </title> <editor> In J. J. Dongarra and B. Tourancheau, editors, </editor> <booktitle> Environments and Tools for Parallel Scientific Computing, </booktitle> <pages> pages 333-350. </pages> <publisher> Elsevier Science, </publisher> <year> 1993. </year>
Reference-contexts: Nevertheless, standard dependence tests cannot be applied in many of the unresolved cases, owing to lack of information about the value of certain variables at compile time. Symbolic techniques are required for effective dependence analysis of these cases <ref> [4, 51, 56, 64, 82] </ref>. Our proposed symbolic analysis framework [44] supports a very accurate dependence analysis scheme in the presence of unknown symbolic terms. The code segment shown in Fig. 18 is an example that requires symbolic dependence analysis.
Reference: 52. <author> J. Ivie. </author> <title> Some MACSYMA programs for solving recurrence relations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 4(1) </volume> <pages> 24-33, </pages> <month> March </month> <year> 1978. </year>
Reference-contexts: Symbolic Analysis 13 interdependences between different recurrences might be cyclic; x may depend on y, which depends on x. Due to its importance in complexity analysis of algorithms, automatic solution of difference equations has been the subject of several studies <ref> [21, 42, 52, 57, 68] </ref>. Parafrase-2 uses the method of finite differences to solve the system of recurrence relations obtained by symbolic interpretation of loops.
Reference: 53. <author> J. P. Jones and Y. Matijasevic. </author> <title> Proof of recursive unsolvability of Hilbert's tenth problem. </title> <journal> The American Mathematical Monthly, </journal> <volume> 98(8) </volume> <pages> 689-709, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In fact, the problem remains undecidable, even if the loop exit equation is restricted to be a polynomial in the loop index variable, of arbitrary degree, with constant coefficients. The proof [43] is a trivial implication of the Matijasevic's result on the unsolvability of the Hilbert's Tenth Problem <ref> [53] </ref>. 14 M. R. Haghighat and C. D.
Reference: 54. <author> N. D. Jones, C. K. Gomard, and P. Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <publisher> Prentice-Hall, </publisher> <year> 1993. </year>
Reference-contexts: Procedures are abstracted by closed form expressions that are solutions to the system of recurrences defined by their bodies. External side effects such as input/output statements are also taken into account. The derived knowledge about actual parameters of procedures can be used to simplify [87] or partially evaluate <ref> [54] </ref> the procedures.
Reference: 55. <author> C. Jordan. </author> <title> Calculus of Finite Differences. </title> <publisher> Chelsea, </publisher> <address> New York, third edition, </address> <year> 1965. </year>
Reference-contexts: Assuming h = x i x i1 ; 8i; 1 i n, the polynomial p, interpolating f at x 0 ; x 1 ; : : : ; x n , is given by p (x) = P n r , where r = (x x 0 )=h <ref> [55] </ref>. j = 0 j = j + i end do CDOALL i = 1,n a (i,i) = b ((i + i * i) / 2) Fig. 23. Example of a nonlinear induction variable. To illustrate the above method, consider the source program of Fig. 23.
Reference: 56. <author> P. Jouvelot and B. Dehbonei. </author> <title> A unified semantic approach for the vectorization and paral-lelization of generalized reductions. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June 5-9 </month> <year> 1989. </year> <note> Symbolic Analysis 47 </note>
Reference-contexts: Nevertheless, standard dependence tests cannot be applied in many of the unresolved cases, owing to lack of information about the value of certain variables at compile time. Symbolic techniques are required for effective dependence analysis of these cases <ref> [4, 51, 56, 64, 82] </ref>. Our proposed symbolic analysis framework [44] supports a very accurate dependence analysis scheme in the presence of unknown symbolic terms. The code segment shown in Fig. 18 is an example that requires symbolic dependence analysis.
Reference: 57. <author> M. Karr. </author> <title> Summation in finite terms. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 28(2) </volume> <pages> 305-350, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: Symbolic Analysis 13 interdependences between different recurrences might be cyclic; x may depend on y, which depends on x. Due to its importance in complexity analysis of algorithms, automatic solution of difference equations has been the subject of several studies <ref> [21, 42, 52, 57, 68] </ref>. Parafrase-2 uses the method of finite differences to solve the system of recurrence relations obtained by symbolic interpretation of loops.
Reference: 58. <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming, Vol. 1 / Fundamental Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <note> second edition, </note> <year> 1973. </year>
Reference-contexts: Symbolic analysis can even provide statistical information such as minimum, maximum, average, and standard deviation of the execution time of iterations of program loops. Such a metric for analysis of algorithms is proposed by Knuth <ref> [58] </ref> and used by Wegbreit [85, 88], Ramshaw [73], and Hickey and Cohen [49]. Semi-automatic worst-case analysis of a large class of logic program is studied by Debray and Lin [29].
Reference: 59. <author> D. E. Knuth. </author> <title> Two notes on notation. </title> <journal> The American Mathematical Monthly, </journal> <volume> 99(5) </volume> <pages> 403-422, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It should be noted that 0 0 is one of the indeterminate forms in analysis, but is defined to be one in axiomatic number theory [33]. All the computers that we are aware of evaluate the integer expression 0 0 to one. Knuth <ref> [59] </ref> surveys the history of the mathematical debate on whether 0 0 should be defined as one or undefined, and mentions several cases related to integers in which it is desirable to have 0 0 defined as one. <p> As we shall see later, these constraints may be guards of such values as results of finite summations. For systematic manipulation of summations, a special case of t function is defined as follows: 2 Following Iverson's APL convention, Knuth <ref> [59] </ref> suggests the notation [b] for t (b). 40 M. R. Haghighat and C. D. Polychronopoulos Definition 9.0.2. The unit step function : IR ! f0,1g is defined as: (x) = t (x &gt; 0) : Fig. 40. The truth and unit step functions.
Reference: 60. <author> D. Kozen. </author> <title> Semantics of probabilistic programs. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 22(3) </volume> <pages> 328-350, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: The slight difference is the result of approximations in the cases of data dependent conditional branches. Timing analysis of programs with data dependent conditions requires statistical knowledge of distribution of the input data <ref> [60] </ref>, and is beyond the scope of this paper. Suppose that the compiler wants to compute the total number of floating point multiplications in the example shown in Fig. 38: a code that contains a structural conditional statement.
Reference: 61. <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(10), </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: However, self-scheduling is not a good scheme if the overhead involved with each dispatch is comparable to the average execution time of the iterations. At the other extreme, another scheme, chunk scheduling, <ref> [61] </ref> allocates a fixed number of iterations (a chunk) to each idle processor. This method reduces the scheduling overhead but may result in poor performance when there is a considerable variation between execution times of loop iterations.
Reference: 62. <institution> Kuck & Associates, Inc., Champaign, Illinois. </institution> <note> KAP User's Guide, </note> <year> 1988. </year>
Reference-contexts: Problematic cases, identified by other studies on automatic parallelization [31, 32, 80], were the focus of our experimentation. We also studied performance of three of the state-of-the-art parallelizing compilers: Cedar Fortran compiler, based on a 1991 version of KAP, from Kuck and Associates <ref> [62] </ref>; the Titan 1500/3000 compilation system from Kubota Pacific Computer Inc.; and the Alliant FX/Fortran compiler, based on VAST-2 preprocessor, found in [15] to be one of the best commercial vectorizers. <p> Parafrase-2 recognizes induction variables under conditional statements. able with the appropriate induction variable [93]. Wraparound variables have been given special consideration by researchers and commercial compilers <ref> [62, 63] </ref> because they appear so often in real application programs. In our experiments with Perfect Benchmarks r fl , we observed occurrences of wraparound variables in programs TRFD and ADM. <p> We compared the current program analysis capabilities of Parafrase-2 with those of other state-of-the-art parallelizing compilers using a test suite including Perfect Benchmarks r fl . Other considered compilers are the 1991 version of KAP, from Kuck and Associates <ref> [62] </ref>, the Titan 1500/3000 compilation system from Kubota Pacific Computer Inc., and the Alliant Fortran compiler, found in [15] to be one of the best commercial vectorizers. While Titan compiler was good at single basic block symbolic analysis, Parafrase-2 did an excellent job in a global frame.
Reference: 63. <author> B. Leasure. </author> <title> The Parafrase project's Fortran analyzer major module documentation. </title> <type> Technical Report CSRD 504, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, </institution> <year> 1985. </year>
Reference-contexts: II in Section 10) that we tried could handle this case. 3.5 Wraparound Expressions Traditionally, wraparound variables of order n of a loop are defined to be those variables that are not induction variables of the loop, but will become induction variables when n iterations of the loop are unrolled <ref> [63, 67, 93] </ref>. Typically, wraparound variables are of the first order. For instance, this may occur when in a loop a variable is used before it is defined in terms of the loop index variable. <p> Parafrase-2 recognizes induction variables under conditional statements. able with the appropriate induction variable [93]. Wraparound variables have been given special consideration by researchers and commercial compilers <ref> [62, 63] </ref> because they appear so often in real application programs. In our experiments with Perfect Benchmarks r fl , we observed occurrences of wraparound variables in programs TRFD and ADM.
Reference: 64. <author> A. Lichnewsky and F. Thomasset. </author> <title> Introducing symbolic problem solving techniques in the dependence testing phases of a vectorizer. </title> <booktitle> In Supercomputing 88. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> July </month> <year> 1988. </year>
Reference-contexts: Nevertheless, standard dependence tests cannot be applied in many of the unresolved cases, owing to lack of information about the value of certain variables at compile time. Symbolic techniques are required for effective dependence analysis of these cases <ref> [4, 51, 56, 64, 82] </ref>. Our proposed symbolic analysis framework [44] supports a very accurate dependence analysis scheme in the presence of unknown symbolic terms. The code segment shown in Fig. 18 is an example that requires symbolic dependence analysis.
Reference: 65. <author> J. E. Moreira. </author> <title> Auto-scheduling Compilers. </title> <type> PhD dissertation, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illinois, </institution> <month> January </month> <year> 1995. </year> <note> In preparation. </note>
Reference-contexts: We call this new scheme balanced chunk scheduling (or BCS). The performance of balanced chunk scheduling will be compared with that of chunk scheduling and self-scheduling. The reported performance is the result of compiling and running our examples on a system developed at the University of Illinois <ref> [65] </ref>. It is oftentimes the characteristics of loop iterations that make one scheduling scheme perform better than the other. In the case of coarse-grained iterations with variable execution times, self-scheduling performs much better than chunk scheduling, while fine-grained iterations with constant execution time favor chunk scheduling.
Reference: 66. <author> J. Moses. </author> <title> Algebraic simplification: A guide for the perplexed. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 14(8) </volume> <pages> 527-537, </pages> <month> August </month> <year> 1971. </year>
Reference-contexts: In [76] Richardson showed that the zero equivalence problem is recursively unsolvable for a sufficiently rich class of transcendental expressions. Theoretical limitations and other unsolvability results concerning transcendental terms in computer algebra may be found in <ref> [14, 16, 66] </ref>. The zero equivalence problem, however, can be solved in many cases of practical interest. In particular, the case of polynomials does not pose any difficulties. There is a variety of choices for canonical representation of polynomials [28, 39].
Reference: 67. <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: 1. INTRODUCTION High performance on parallel computers can be achieved only when they are pro grammed effectively. The complexity of this task and portability issues make auto matic support of parallel program development highly desirable <ref> [5, 67, 69] </ref>. Despite all the effort spent on the automatic parallelization of programs, existing paralleliz ing compilers are not able to deliver the desired performance over a wide range of real applications [12]. <p> Thus, no dependences remain between iterations of the loop, and it can run in parallel. The above transformation, called induction variable substitution, is the inverse of strength reduction and has been used in parallelizing compilers for a number of years <ref> [67] </ref>. <p> II in Section 10) that we tried could handle this case. 3.5 Wraparound Expressions Traditionally, wraparound variables of order n of a loop are defined to be those variables that are not induction variables of the loop, but will become induction variables when n iterations of the loop are unrolled <ref> [63, 67, 93] </ref>. Typically, wraparound variables are of the first order. For instance, this may occur when in a loop a variable is used before it is defined in terms of the loop index variable.
Reference: 68. <author> M. Petkovsek. </author> <title> Finding Closed-Form Solutions of Difference Equations by Symbolic Methods. </title> <type> PhD dissertation, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <year> 1991. </year>
Reference-contexts: Symbolic Analysis 13 interdependences between different recurrences might be cyclic; x may depend on y, which depends on x. Due to its importance in complexity analysis of algorithms, automatic solution of difference equations has been the subject of several studies <ref> [21, 42, 52, 57, 68] </ref>. Parafrase-2 uses the method of finite differences to solve the system of recurrence relations obtained by symbolic interpretation of loops.
Reference: 69. <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer, </publisher> <year> 1988. </year>
Reference-contexts: 1. INTRODUCTION High performance on parallel computers can be achieved only when they are pro grammed effectively. The complexity of this task and portability issues make auto matic support of parallel program development highly desirable <ref> [5, 67, 69] </ref>. Despite all the effort spent on the automatic parallelization of programs, existing paralleliz ing compilers are not able to deliver the desired performance over a wide range of real applications [12].
Reference: 70. <author> C. D. Polychronopoulos, M. B. Girkar, M. R. Haghighat, C. L. Lee, B. P. Leung, and D. A. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing and scheduling programs on multiprocessors. </title> <booktitle> In Proceedings of the 1989 ICPP, </booktitle> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: R. Haghighat and C. D. Polychronopoulos This paper describes a methodology for the discovery of certain program properties that are essential in the effective detection and efficient exploitation of parallelism. Based on this methodology, we have built a compiler analysis framework for Parafrase-2 <ref> [70] </ref>, a source-to-source multilingual parallelizer developed at the University of Illinois. Within this framework, symbolic analysis is used as an abstract interpretation technique to solve a variety of interprocedural flow analysis problems in a unified way.
Reference: 71. <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> De-cember </month> <year> 1987. </year>
Reference-contexts: Between these two schemes lie other approaches that attempt to compromise load balancing and minimizing overhead. Guided self-scheduling <ref> [71] </ref>, factoring [35, 36], and trapezoidal self-scheduling [83] are examples of such schemes. n = 50 DOALL i = 1, n*n INTEGER k DO k = i, n*n END DO END DOALL Fig. 31. Adjoint-convolution program and its parallel work.
Reference: 72. <author> W. H. Press, S. A. Teukolsky, Vetterling W. T., and B. P. Flannery. </author> <title> Numerical Recipes in FORTRAN: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> second edition, </address> <year> 1992. </year>
Reference-contexts: This problem is clearly unsolvable in the general case, otherwise the halting problem [38] would have been decidable. However, many instances of this problem are solvable. For example, consider the code shown in Fig. 37. This subroutine, taken from Numerical Recipes <ref> [72] </ref>, is a complete Cholesky decomposition for solving linear equations.
Reference: 73. <author> L. H. Ramshaw. </author> <title> Formalizing the analysis of algorithms. </title> <type> Technical Report SL-79-5, </type> <institution> Xerox Palo Alto Research Center, Palo Alto, California, </institution> <year> 1979. </year>
Reference-contexts: Symbolic analysis can even provide statistical information such as minimum, maximum, average, and standard deviation of the execution time of iterations of program loops. Such a metric for analysis of algorithms is proposed by Knuth [58] and used by Wegbreit [85, 88], Ramshaw <ref> [73] </ref>, and Hickey and Cohen [49]. Semi-automatic worst-case analysis of a large class of logic program is studied by Debray and Lin [29]. The performance measure should also take into account architectural issues such as the type of memory accesses in computers with hierarchical memories.
Reference: 74. <author> J. H. Reif. </author> <title> Code motion. </title> <journal> SIAM J. of Computing, </journal> <volume> 9(2) </volume> <pages> 375-395, </pages> <month> May </month> <year> 1980. </year>
Reference-contexts: Parafrase-2 selects a loop stride that simplifies induction variables. 4. RECOGNITION OF LOOP-INVARIANT COMPUTATIONS Loop-invariant expressions are of particular interest in optimizing compilers. Their recognition is a basis for code motion <ref> [20, 74] </ref>; an important loop optimization technique that improves program performance by reducing frequency of execution of some instructions. In parallelizing compilers, on the other hand, recognition of loop-invariant expressions has an important role in dependence analysis.
Reference: 75. <author> J. H. Reif and R. E. Tarjan. </author> <title> Symbolic program analysis in almost linear time. </title> <journal> SIAM J. of Computing, </journal> <volume> 11(1) </volume> <pages> 81-93, </pages> <month> February 81. </month>
Reference-contexts: Currently, Parafrase-2 compiles real-life Fortran applications with a speed of 3000 lines per wall clock minute on Sparc/10 workstations. Although this speed is cost effective, it can be further improved by employing more sparse representations suitable for symbolic analysis. Global Value Graph <ref> [75] </ref> and Static Single Assignment [27] have been shown to be bases for efficient compiler algorithms, and a number of studies have used the Static Single Assignment form for solving induction variables [93] and symbolic analysis [46]. 12.
Reference: 76. <author> D. Richardson. </author> <title> Some unsolvable problems involving elementary functions of a real variable. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 33 </volume> <pages> 511-520, </pages> <year> 1968. </year>
Reference-contexts: First, efficient procedures can be built for carrying out operations on the objects when they are in a canonical form. Second, the zero equivalence problem, that is, the problem of deciding equivalence between two objects, is immediately solved when equivalent objects have identical representations. In <ref> [76] </ref> Richardson showed that the zero equivalence problem is recursively unsolvable for a sufficiently rich class of transcendental expressions. Theoretical limitations and other unsolvability results concerning transcendental terms in computer algebra may be found in [14, 16, 66].
Reference: 77. <author> V. Sarkar. </author> <title> Determining average program execution times and their variance. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 298-312, </pages> <address> Portland, Oregon, </address> <month> June 21-23 </month> <year> 1989. </year>
Reference-contexts: Sarkar has studied partitioning of parallel programs using cost estimates computed from the profile information <ref> [77, 78] </ref>. 9.2 Derivation of Symbolic Cost Estimates Deriving cost estimates of program tasks at any level in terms of their inputs is essentially the same as finding the net effect of execution of the tasks on a variable, which is initially zero, and is incremented by the cost of each
Reference: 78. <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: Sarkar has studied partitioning of parallel programs using cost estimates computed from the profile information <ref> [77, 78] </ref>. 9.2 Derivation of Symbolic Cost Estimates Deriving cost estimates of program tasks at any level in terms of their inputs is essentially the same as finding the net effect of execution of the tasks on a variable, which is initially zero, and is incremented by the cost of each
Reference: 79. <author> D. A. Schouten. </author> <title> An overview of interprocedural analysis techniques for high performance parallelizing compilers. M.S. </title> <type> thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, </institution> <year> 1990. </year>
Reference-contexts: Exploitation 28 M. R. Haghighat and C. D. Polychronopoulos of symbolic information about parameters of procedure f has enabled the inter-procedural dependence analyzer <ref> [79] </ref> of Parafrase-2 to prove lack of dependence between calls to procedure f. Furthermore, after derivation of the exact side effects of procedure ndx the call to this procedure becomes useless and is eliminated by the compiler.
Reference: 80. <author> J. P. Singh and J. L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <editor> In N. Suzuki, editor, </editor> <booktitle> Shared Memory Multiprocessing, </booktitle> <pages> pages 213-240. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: We studied the effectiveness of our techniques on the Perfect Benchmarks r fl [11], a suite of Fortran programs representative of real engineering and scientific codes. Problematic cases, identified by other studies on automatic parallelization <ref> [31, 32, 80] </ref>, were the focus of our experimentation. <p> The attained solution space of these problems in our framework was much larger than that handled by the existing compiler technology. In particular, our symbolic analyzer was able to automatically solve the most complicated cases of induction analysis identified in <ref> [31, 32, 80] </ref>. Problems that are solved interprocedurally in our symbolic analysis framework include symbolic constant propagation, generalized induction variable substitution, global forward substitution, and detection of loop invariant computations. These problems have a vital role in dependence analysis [4, 8, 91], the fundamental component of parallelizing compilers. <p> Parafrase-2 could handle all those cases automatically and without any user assertions. In their empirical investigation of the effectiveness and limitations of automatic parallelization <ref> [80] </ref>, Singh and Hennessy find the need for a fairly sophisticated induction variable analysis to be able to parallelize the middle loop of the source program shown in Fig. 21. This code is the prediction phase of the predictor-corrector method used in program MDG: an N-body molecular dynamics simulation.
Reference: 81. <author> P. Tang and P.-C. Yew. </author> <title> Processor self-scheduling for multiple-nested parallel loops. </title> <booktitle> In Proceedings of the 1986 ICPP, </booktitle> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1986. </year> <note> 48 M. </note> <author> R. Haghighat and C. D. </author> <note> Polychronopoulos </note>
Reference-contexts: At one extreme, a strategy called self-scheduling <ref> [81] </ref> schedules loop iterations one at a time, and thus achieves the best possible load balancing. However, self-scheduling is not a good scheme if the overhead involved with each dispatch is comparable to the average execution time of the iterations.
Reference: 82. <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of Call statements. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, California, </address> <year> 1986. </year>
Reference-contexts: Nevertheless, standard dependence tests cannot be applied in many of the unresolved cases, owing to lack of information about the value of certain variables at compile time. Symbolic techniques are required for effective dependence analysis of these cases <ref> [4, 51, 56, 64, 82] </ref>. Our proposed symbolic analysis framework [44] supports a very accurate dependence analysis scheme in the presence of unknown symbolic terms. The code segment shown in Fig. 18 is an example that requires symbolic dependence analysis.
Reference: 83. <author> T. H. Tzen and L. M. Ni. </author> <title> Dynamic loop scheduling for shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1991 ICPP, volume 2, </booktitle> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Between these two schemes lie other approaches that attempt to compromise load balancing and minimizing overhead. Guided self-scheduling [71], factoring [35, 36], and trapezoidal self-scheduling <ref> [83] </ref> are examples of such schemes. n = 50 DOALL i = 1, n*n INTEGER k DO k = i, n*n END DO END DOALL Fig. 31. Adjoint-convolution program and its parallel work. All the above schemes can benefit from the cost estimates provided by the symbolic analysis.
Reference: 84. <author> J. D. Ullman. </author> <title> Fast algorithms for the elimination of common subexpressions. </title> <journal> Acta Informat-ica, </journal> <volume> 2:3:191-213, </volume> <month> July </month> <year> 1973. </year>
Reference-contexts: Note that the resulting loop nesting, grounded upon natural loops, might be different from that determined by other definitions of loops, such as those based on interval analysis [2, 18], T1-T2 analysis <ref> [84] </ref>, and control dependence analysis [34]. Wolfe has studied the implication of various definitions of loops on loop nesting [92]. Making loops uniquely identifiable by their headers results in a simpler scheme for interpretation of loops. Parafrase-2 normalizes the structure of loops of the following categories.
Reference: 85. <author> B. Wegbreit. </author> <title> Mechanical program analysis. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 18(9) </volume> <pages> 528-539, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: Symbolic analysis can even provide statistical information such as minimum, maximum, average, and standard deviation of the execution time of iterations of program loops. Such a metric for analysis of algorithms is proposed by Knuth [58] and used by Wegbreit <ref> [85, 88] </ref>, Ramshaw [73], and Hickey and Cohen [49]. Semi-automatic worst-case analysis of a large class of logic program is studied by Debray and Lin [29]. The performance measure should also take into account architectural issues such as the type of memory accesses in computers with hierarchical memories.
Reference: 86. <author> B. Wegbreit. </author> <title> Property extraction in well-founded property sets. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 1(3) </volume> <pages> 270-285, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: Wegman and Zadeck [89] have presented an algorithm for constant propagation combined with dead-code elimination. Their algorithm recognizes the same class of constants as that identified by Wegbreit's general global flow analysis algorithm <ref> [86] </ref>.
Reference: 87. <author> B. Wegbreit. </author> <title> Goal-directed program transformation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-2(2):69-80, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: Procedures are abstracted by closed form expressions that are solutions to the system of recurrences defined by their bodies. External side effects such as input/output statements are also taken into account. The derived knowledge about actual parameters of procedures can be used to simplify <ref> [87] </ref> or partially evaluate [54] the procedures.
Reference: 88. <author> B. Wegbreit. </author> <title> Verifying program performance. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 23(4) </volume> <pages> 691-699, </pages> <month> October </month> <year> 1976. </year>
Reference-contexts: Symbolic analysis can even provide statistical information such as minimum, maximum, average, and standard deviation of the execution time of iterations of program loops. Such a metric for analysis of algorithms is proposed by Knuth [58] and used by Wegbreit <ref> [85, 88] </ref>, Ramshaw [73], and Hickey and Cohen [49]. Semi-automatic worst-case analysis of a large class of logic program is studied by Debray and Lin [29]. The performance measure should also take into account architectural issues such as the type of memory accesses in computers with hierarchical memories.
Reference: 89. <author> M. N. Wegman and F. K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This is particularly important in interprocedural analysis schemes such as in-line expansion and procedure cloning where properties of actual parameters are used to improve the performance of called procedures. Wegman and Zadeck <ref> [89] </ref> have presented an algorithm for constant propagation combined with dead-code elimination. Their algorithm recognizes the same class of constants as that identified by Wegbreit's general global flow analysis algorithm [86].
Reference: 90. <author> M. J. Wolfe. </author> <title> Techniques for improving the inherent parallelism in programs. </title> <type> Technical Report 78-929, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1978. </year>
Reference-contexts: INDUCTION VARIABLES Induction variables are of particular interest in optimizing compilers [1]; a subclass of them that form arithmetic progressions has a vital role in parallelizing compilers <ref> [5, 90] </ref>. Induction variables are usually introduced to improve program performance on sequential computers. To illustrate this, we consider the loop of Fig. 15. do i = 1, n a (j) = 1 CDOALL i = 1,n a (2 * i) = 1 Fig. 15.
Reference: 91. <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: Problems that are solved interprocedurally in our symbolic analysis framework include symbolic constant propagation, generalized induction variable substitution, global forward substitution, and detection of loop invariant computations. These problems have a vital role in dependence analysis <ref> [4, 8, 91] </ref>, the fundamental component of parallelizing compilers. Some of the analytical tools used effectively in our compilation scheme are mathematical methods such as computer algebra, calculus of finite differences, and theorem proving techniques based on number theory.
Reference: 92. <author> M. J. Wolfe. </author> <title> Flow graph anomalies: What's in a loop? Technical report, </title> <institution> Oregon Graduate Institute, Beaverton, Oregon, </institution> <year> 1990. </year>
Reference-contexts: Wolfe has studied the implication of various definitions of loops on loop nesting <ref> [92] </ref>. Making loops uniquely identifiable by their headers results in a simpler scheme for interpretation of loops. Parafrase-2 normalizes the structure of loops of the following categories.
Reference: 93. <author> M. J. Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <address> San Francisco, California, </address> <month> June 17-19 </month> <year> 1992. </year>
Reference-contexts: They have been named Generalized Induction Variables or GIVs <ref> [6, 31, 32, 45, 93] </ref>. The sequence of values that a GIV receives throughout the execution of a loop does not necessarily form an arithmetic progression. <p> II in Section 10) that we tried could handle this case. 3.5 Wraparound Expressions Traditionally, wraparound variables of order n of a loop are defined to be those variables that are not induction variables of the loop, but will become induction variables when n iterations of the loop are unrolled <ref> [63, 67, 93] </ref>. Typically, wraparound variables are of the first order. For instance, this may occur when in a loop a variable is used before it is defined in terms of the loop index variable. <p> Parafrase-2 recognizes induction variables under conditional statements. able with the appropriate induction variable <ref> [93] </ref>. Wraparound variables have been given special consideration by researchers and commercial compilers [62, 63] because they appear so often in real application programs. In our experiments with Perfect Benchmarks r fl , we observed occurrences of wraparound variables in programs TRFD and ADM. <p> This point has also been observed by Wolfe <ref> [93] </ref>. On the other hand, the value of k at the first iteration is not known, but the symbolic analyzer of Parafrase-2 has been able to derive a formula that covers all iterations of the loop. <p> Global Value Graph [75] and Static Single Assignment [27] have been shown to be bases for efficient compiler algorithms, and a number of studies have used the Static Single Assignment form for solving induction variables <ref> [93] </ref> and symbolic analysis [46]. 12. CONCLUSION Existing parallelizing compilers are unable to detect and exploit effectively the available parallelism in real programs, mainly due to lack of accuracy in their analysis. The most important factor for the effective detection and management of parallelism is the precision of dependence information.
Reference: 94. <author> M. J. Wolfe. </author> <title> Engineering a data dependence test. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(7) </volume> <pages> 603-622, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Loop normalization not only does not solve the dependence problem, but also complicates subscript expressions and can raise difficulties for dependence analysis. However, there are cases where loop normalization becomes necessary. The advantage and disadvantages of loop normalization for dependence testing has been studied by Wolfe <ref> [94] </ref>. Parafrase-2 tries to avoid loop normalizations that complicate subscript expressions. Even in the cases where normalization becomes necessary, it is done in a such a way that usually simplifies subscript expressions and thereafter dependence analysis.
References-found: 94

