URL: http://www.cs.dartmouth.edu/~nils/thesis.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~nils/galley.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: nils@cs.dartmouth.edu  
Title: Galley: A New Parallel File System For Scientific Workloads  
Author: Nils Nieuwejaar 
Address: College, Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth  
Abstract-found: 0
Intro-found: 1
Reference: [Are91] <author> James W. Arendt. </author> <title> Parallel genome sequence comparison using a concurrent file system. </title> <type> Technical Report UIUCDCS-R-91-1674, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: This problem requires searching a large database to find approximate matches between strings <ref> [Are91] </ref>. The raw database used in [Are91] contains thousands of genetic sequences, each of which is composed of hundreds or thousands of bases. To reduce the amount of time required to identify potential matches, the authors constructed an index of the database that was specific to their needs. <p> This problem requires searching a large database to find approximate matches between strings <ref> [Are91] </ref>. The raw database used in [Are91] contains thousands of genetic sequences, each of which is composed of hundreds or thousands of bases. To reduce the amount of time required to identify potential matches, the authors constructed an index of the database that was specific to their needs.
Reference: [BBD + 94] <author> Brian Bershad, David Black, David DeWitt, Garth Gibson, Kai Li, Larry Peterson, and Marc Snir. </author> <title> Operating system support for high-performance parallel I/O systems, 1994. Scalable I/O Initiative Working Paper Number 4. </title>
Reference-contexts: While the SIO Initiative has many goals, the most relevant to our work is the design of a new low-level interface for parallel I/O <ref> [BBD + 94] </ref>. The long-term goal of the Operating Systems Working Group is to design a set of interfaces that may be added to the standard X/Open 4.2 interfaces.
Reference: [BBDS92] <author> D. H. Bailey, E. Barszcz, L. Dagum, and H. D. Simon. </author> <title> NAS parallel benchmark results. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 386-393, </pages> <year> 1992. </year>
Reference-contexts: the application's I/O patterns and the characteristics of the underlying file system. 107 7.3 BT I/O Benchmark Several years ago, NASA Ames Research Center released a set of benchmarks, called the NAS Parallel Benchmarks (NPB), which has become a de facto standard for comparing the performance of high performance computers <ref> [BBDS92] </ref>. NPB was initially a set of paper benchmarks. That is, a set of problems were described in detail, but the implementations were left to the benchmarker.
Reference: [BBH95] <author> Sandra Johnson Baylor, Caroline B. Benveniste, and Yarson Hsu. </author> <title> Performance evaluation of a parallel I/O architecture. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 404-413, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Indeed, most do not even examine the performance of requests of fewer than many kilobytes <ref> [Nit92, BBH95, KR94] </ref>. As discussed earlier, multiprocessor filesystem workloads frequently include many small requests.
Reference: [BBS + 94] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: There has also been quite a bit of work done in supporting irregular data structures: [CSBS94, PSC + 95] discuss the issues relating to interprocessor distribution of these structures, and Jovian explores the issues relating to their persistent storage <ref> [BBS + 94] </ref>.
Reference: [BGMZ92] <author> Peter Brezany, Michael Gernt, Piyush Mehotra, and Hans Zima. </author> <title> Concurrent file operations in a High Performance FORTRAN. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 230-237, </pages> <year> 1992. </year>
Reference-contexts: These interfaces are sometimes tightly integrated into a particular language such as HPF <ref> [Lov93, BGMZ92, HPF93] </ref> or CMF [Thi94]. There has also been great deal of research in designing libraries to support parallel I/O. The focus of much of this research has been the support of distributed matrices, particularly for data-parallel variants of Fortran [CBH + 94, TBC + 94, TG96].
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: interfaces and policies, implemented in libraries, * allow easy and efficient implementations of libraries, * be scalable enough to run well on multiprocessors with tens or hundreds of nodes, * minimize memory and performance overhead. 5.2 File Structure Most parallel file systems are based on a Unix-like, linear file model <ref> [BGST93, LIN + 93, Pie89] </ref>. Under this model, a file is seen as an addressable, linear sequence of bytes. Applications can issue 42 requests to read or write contiguous subranges of that sequence of bytes. <p> Users were expected to access it through the I/O support built in to one of their data-parallel languages or through the CMMD library, described below. 8.1.3 CMMD The CMMD library allows CM-5 applications to be written in a control parallel style <ref> [BGST93] </ref>. CMMD maintains the traditional stream-of-bytes abstraction of a file and supports all the standard Unix-like operations, which may be executed by each compute node individually. In addition to the standard operations, CMMD includes support for parallel applications in the form of access modes like those in Intel's CFS.
Reference: [BHK + 91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <year> 1991. </year>
Reference-contexts: General-purpose uniprocessor workloads. Uniprocessor file-access patterns have been measured many times. Floyd and Ellis [Flo86, FE89] and Ousterhout et al. [OCH + 85] measured isolated Unix machines. Baker et al. studied the workload of a cluster of workstations running the Sprite file system, which is a distributed, Unix-like system <ref> [BHK + 91] </ref>. Ramakrishnan et al. studied access patterns in a commercial computing environment on a VAX/VMS platform [RBK92]. These studies all cover general-purpose (engineering and office) workloads with uniprocessor applications. <p> Although these files were larger than those in a general-purpose file system <ref> [BHK + 91] </ref>, they were smaller than we would expect to see in a scientific supercomputing environment [MK91]. <p> We define a consecutive request to be a sequential request that begins precisely one byte beyond where the previous request ended. A common characteristic of file workloads, particularly scientific workloads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. Frequently, files are also accessed in their entirety. Figures 3.7 and 3.8 show the amount of sequential and consecutive access (on a per-node basis) to files with more than one request in our workload. <p> It is concurrently shared if the opens overlap in time. It is write-shared if at least one of the processors writes to the file. In uniprocessor and distributed-system workloads, concurrent sharing is known to be rare <ref> [BHK + 91] </ref>. In a parallel file system, concurrent file sharing among processes within a job is presumably the norm, while concurrent file sharing between jobs is likely to be rare. <p> A common characteristic of many file-system workloads, particularly scientific file-system work-loads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. In the parallel file-system workload, we found that while almost 93% of all files were accessed sequentially, consecutive access was primarily limited to those files that were only opened by one compute node. <p> The CP then fills the appropriate buffer with zeros. Not only does this approach avoid a disk access, it avoids sending a block of data across the network. The Unix approach is optimized for small files. The workload studies discussed in <ref> [OCH + 85, BHK + 91] </ref> show that most files in a Unix workload can be addressed using just the direct-mapping entries in the inode. Few files will require an indirect block and fewer still will require a doubly-indirect block.
Reference: [BHS + 95] <author> D. H. Bailey, Tim Harris, William Saphir, Rob van der Wijngaart, Alex Woo, and Maurice Yarrow. </author> <title> NAS parallel benchmarks 2.0. </title> <type> Technical Report NAS-95-020, </type> <institution> NASA Ames Research Center, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: With the rise of MPI and HPF, standards for both message-passing and data-parallel programming have become available. As a result, version 2 of NPB is an MPI-based collection of codes, rather than a paper benchmark <ref> [BHS + 95] </ref>. There are plans to release a set of HPF-based benchmarks as well. Following the success of NPB, NASA Ames attempted to devise a similar suite of benchmarks for I/O.
Reference: [BSP + 95] <author> Brian Bershad, Stefan Savage, Przemys law Pardyak, Emin Gun Sirer, Marc E. Fi-uczynski, David Becker, Craig Chambers, and Susan Eggers. </author> <title> Extensibility, safety 132 and performance in the SPIN operating system. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 267-284, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: This approach is similar in spirit to the techniques suggested by the SPIN <ref> [BSP + 95] </ref> and Exokernel [EKO95] projects, which involve uniprocessor operating systems. The ability to run application code at the IOPs could be used to allow applications to perform data-dependent filtering or distribution, possibly reducing network traffic.
Reference: [BW96] <author> Sandra Johnson Baylor and C. Eric Wu. </author> <title> Parallel I/O workload characteristics using Vesta. </title> <editor> In Jain et al. [JWB96], </editor> <volume> chapter 7, </volume> <pages> pages 167-185. </pages>
Reference-contexts: There was a wide distribution in request sizes, though few were larger than 1 MB, and a wide variation in spatial and temporal access patterns. Baylor et al. performed a similar analysis of several applications on the IBM SP-2 <ref> [BW96] </ref>. Galbreath et al. present a high-level characterization of multiprocessor file-system workloads based on anecdotal evidence [GGL93].
Reference: [CACR95] <author> Phyllis E. Crandall, Ruth A. Aydt, Andrew A. Chien, and Daniel A. Reed. </author> <title> Input/output characteristics of scalable parallel applications. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Cypher et al. [CHKM93] studied individual parallel scientific applications, measuring temporal patterns in I/O rates. Crandall et al. instrumented three parallel applications to study their file-access activity in detail <ref> [CACR95] </ref>. Although they found primarily sequential access patterns, the patterns were often cyclical (e.g., applications repeatedly opened and closed the same file, each time accessing it in the same pattern).
Reference: [CBF93] <author> Peter F. Corbett, Sandra Johnson Baylor, and Dror G. Feitelson. </author> <title> Overview of the Vesta parallel file system. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 1-16, </pages> <year> 1993. </year>
Reference-contexts: This restriction includes the number of I/O nodes, the number of compute nodes, the disk-block size, the unit-of-transfer size, and, for some data distributions, the matrix dimensions. This interface was implemented, but never released. 121 8.2.3 Vesta The Vesta file system breaks away from the traditional one-dimensional file structure <ref> [CBF93, CF96, FCHP95] </ref>. Files in Vesta are two-dimensional, and are composed of multiple cells, each of which is a sequence of basic striping units. BSUs are essentially records, or fixed-sized sequences of bytes. Like Galley's subfiles, each cell resides on a single disk.
Reference: [CBH + 94] <author> Alok Choudhary, Rajesh Bordawekar, Michael Harry, Rakesh Krishnaiyer, Ravi Pon-nusamy, Tarvinder Singh, and Rajeev Thakur. </author> <title> Passion: Parallel and scalable software for input-output. </title> <type> Technical Report SCCS-636, </type> <institution> ECE Dept., NPAC and CASE Center, Syracuse University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: There has also been great deal of research in designing libraries to support parallel I/O. The focus of much of this research has been the support of distributed matrices, particularly for data-parallel variants of Fortran <ref> [CBH + 94, TBC + 94, TG96] </ref>. Other libraries support multidimensional matrix I/O under C++ [SW94, SCJ + 95, CWS + 96].
Reference: [CC94] <author> Thomas H. Cormen and Alex Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: He also found that Galley's nested-strided interfaces allowed him to achieve significantly better performance than the two-phase I/O strategy he had 115 originally employed. Finally, a group at Dartmouth is developing a compiler for ViC*, a variant of the data-parallel C* <ref> [CC94] </ref>. ViC* is designed to easily support out-of-core programming methods. Galley is one of the file systems that they are targeting.
Reference: [CCFN92] <author> Russell Carter, Bob Ciotti, Sam Fineberg, and Bill Nitzberg. </author> <title> NHT-1 I/O benchmarks. </title> <type> Technical Report RND-92-016, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Simple benchmarking of the instrumented library revealed that the overhead added by our instrumentation was virtually undetectable in many cases. The worst case we found was a 7% increase in execution time on one run of the NAS NHT-1 Application-I/O Benchmark <ref> [CCFN92] </ref>. After the instrumented library was put into production use, anecdotal evidence suggests that there was no noticeable performance loss. 3.2.1 Postprocessing The raw trace files required some simple postprocessing before they could be easily analyzed. This postprocessing included data realignment, clock synchronization, and chronological sorting.
Reference: [CF96] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> The Vesta parallel file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(3) </volume> <pages> 225-264, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: To find the IOP that manages a given file's header block, a simple hash function is applied to the file name. Given the simplicity of Galley's naming system, this approach is reasonably simple to implement efficiently. Vesta uses a similar hashing scheme for their naming system <ref> [CF96] </ref>. A subfile header block contains information about all the forks within the subfile. In particular, a fork's entry in the subfile header block contains its name, its size, and the disk addresses of its mapping blocks, which are discussed below. <p> Most parallel file systems provide applications with little more than a standard Unix interface. Specifically, they only allow applications to accesses contiguous regions of the file in a single request. The most notable exceptions to this rule are Vesta <ref> [CF96] </ref> and MPI-IO [The96], which we discuss further in Chapter 8. Both of these systems allow applications to describe logical views of a file. That is, each process in a parallel application describes the subset of the file that it wishes to access. <p> CMMD also offers two global modes in which there is a single shared file pointer. 8.1.4 PIOFS PIOFS, a parallel file system for IBM's SP-2, allows users and applications to interact with it exactly as they would interact with any AIX file system <ref> [CF96] </ref>. Indeed, the parallel file system is mounted on each of the nodes of the SP-2 using AIX's standard Virtual File System interface. Although PIOFS may appear as a standard sequential file system, it is implemented on top of the Vesta parallel file system (discussed below). <p> This restriction includes the number of I/O nodes, the number of compute nodes, the disk-block size, the unit-of-transfer size, and, for some data distributions, the matrix dimensions. This interface was implemented, but never released. 121 8.2.3 Vesta The Vesta file system breaks away from the traditional one-dimensional file structure <ref> [CBF93, CF96, FCHP95] </ref>. Files in Vesta are two-dimensional, and are composed of multiple cells, each of which is a sequence of basic striping units. BSUs are essentially records, or fixed-sized sequences of bytes. Like Galley's subfiles, each cell resides on a single disk.
Reference: [CFP + 95] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, George S. Almasi, Sandra John-son Baylor, Anthony S. Bolmarcich, Yarsun Hsu, Julian Satran, Marc Snir, Robert Colao, Brian Herr, Joseph Kavaky, Thomas R. Morgan, and Anthony Zlotek. </author> <title> Parallel file systems for the IBM SP computers. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 222-248, </pages> <month> January </month> <year> 1995. </year>
Reference: [CH96] <author> Thomas H. Cormen and Melissa Hirschl. </author> <title> Early experiences in evaluating the parallel disk model with the ViC* implementation. </title> <type> Technical Report PCS-TR96-293, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1996. </year> <note> To appear in Parallel Computing. </note>
Reference-contexts: ViC* provides nearly-transparent support for out-of-core applications written in C*, a data-parallel dialect of C <ref> [CH96] </ref>. 125 Chapter 9 Conclusion During the course of this research, we have explored two related areas: how scientific applications use current parallel file systems, and how parallel file systems can be designed to better meet the needs of those applications.
Reference: [CHKM93] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <year> 1993. </year>
Reference-contexts: This study is interesting, but far from what we need: the sample size is small; the programs are parallelized sequential programs, not parallel programs per se; and the I/O itself was not parallelized. Cypher et al. <ref> [CHKM93] </ref> studied individual parallel scientific applications, measuring temporal patterns in I/O rates. Crandall et al. instrumented three parallel applications to study their file-access activity in detail [CACR95].
Reference: [CK93] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <note> Revised as Dartmouth PCS-TR93-188 on 9/20/94. 133 </note>
Reference-contexts: The number in each square indicates the disk on which that block is stored. on an application's data. Instead, Galley provides applications with the ability to fully control this declustering according to their own needs. This control is particularly important when implementing I/O-optimal algorithms <ref> [CK93] </ref>. Applications are also able to explicitly indicate which disks they wish to access in each request. To allow this behavior, files are composed of one or more subfiles, which may be directly addressed by the application. <p> It was designed to support libraries for out-of-core applications [SW95b, SWC + 95], and it implements many of the recommendations for parallel I/O discussed in <ref> [CK93] </ref>. Whiptail provides no byte-level operations; all file operations must be done in units that are multiples of the block size.
Reference: [CPD + 96] <author> Peter F. Corbett, Jean-Pierre Prost, Chris Demetriou, Garth Gibson, Erik Reidel, Jim Zelenka, Yuqun Chen, Ed Felten, Kai Li, John Hartman, Larry Peterson, Brian Bershad, Alec Wolman, and Ruth Aydt. </author> <title> Proposal for a common parallel file system programming interface. </title> <note> WWW http://www.cs.arizona.edu/sio/api1.0.ps, September 1996. Version 1.0. </note>
Reference-contexts: These interfaces may be considered independently of Galley's new three-dimensional file structure; they may be added to existing parallel or sequential file systems, giving applications the opportunity to achieve higher performance, but without sacrificing backwards compatibility. These interfaces have influenced the Scalable I/O Initiative's low-level application programming interface <ref> [CPD + 96] </ref>. Galley's design was deliberately kept simple, to facilitate the task of developing an efficient, high-performance implementation. We will discuss our implementation, and show that we achieved our goal of efficiency and high-performance. Our implementation was designed with the goal of being easily portable to other platforms. <p> That is, the interface presented by the group is intended to be an extension to the Unix file system, rather than a replacement of it. The core of the Scalable I/O Iniative's interface allows applications to submit lists of simple-strided requests <ref> [CPD + 96] </ref>. Earlier proposals borrowed our nested-batched requests for the core of their interface, but this level of functionality is now described as an "extension" to the core interface. 8.1.9 Scotch Scotch is a parallel file system from Carnegie Mellon University [GSC + 95].
Reference: [Cro89] <author> Thomas W. Crockett. </author> <title> File concepts for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: Baylor et al. performed a similar analysis of several applications on the IBM SP-2 [BW96]. Galbreath et al. present a high-level characterization of multiprocessor file-system workloads based on anecdotal evidence [GGL93]. Crockett <ref> [Cro89] </ref> hypothesizes about the character of a parallel scientific file-system workload. 14 In the next two chapters we present our own study of the workloads on two parallel machines in use in a production environment. 15 Chapter 3 Workload Characterization Perhaps the most important information needed by the designer of a
Reference: [CSBS94] <author> Frederick T. Chong, Shamik D. Sharma, Eric Brewer, and Joel Saltz. </author> <title> Multiprocessor runtime support for fine-grained, irregular DAGs. </title> <type> Technical Report CS-TR-3266, </type> <institution> Department of Computer Science, University of Maryland, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Other libraries support multidimensional matrix I/O under C++ [SW94, SCJ + 95, CWS + 96]. There has also been quite a bit of work done in supporting irregular data structures: <ref> [CSBS94, PSC + 95] </ref> discuss the issues relating to interprocessor distribution of these structures, and Jovian explores the issues relating to their persistent storage [BBS + 94].
Reference: [CWS + 96] <author> Y. Chen, M. Winslett, K. E. Seamons, S. Kuo, Y. Cho, and M. Subramaniam. </author> <title> Scalable message passing in Panda. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 109-121, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The focus of much of this research has been the support of distributed matrices, particularly for data-parallel variants of Fortran [CBH + 94, TBC + 94, TG96]. Other libraries support multidimensional matrix I/O under C++ <ref> [SW94, SCJ + 95, CWS + 96] </ref>. There has also been quite a bit of work done in supporting irregular data structures: [CSBS94, PSC + 95] discuss the issues relating to interprocessor distribution of these structures, and Jovian explores the issues relating to their persistent storage [BBS + 94].
Reference: [dC94] <author> Juan Miguel del Rosario and Alok Choudhary. </author> <title> High performance I/O for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Some studies examined scientific workloads on vector machines. In <ref> [dC94] </ref>, del Rosario and Choud-hary provide an informal characterization of grand-challenge applications. Powell measured a set of static characteristics (file sizes) of a Cray-1 file system [Pow77]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [MK91], focusing primarily on access rates.
Reference: [Dib90] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: failure of one disk. 8.2 Non-Unix Parallel File Systems Although most commercially available parallel file systems are based on the Unix model, there has been a great deal of interesting research done using alternative interfaces and file models. 120 8.2.1 Bridge Bridge was one of the earliest parallel file systems <ref> [DSE88, Dib90] </ref>, and is unusual in not separating I/O nodes from compute nodes. Bridge provides three interfaces. The simplest, designed to be used by a single compute node, is similar to a traditional Unix interface and transparently distributes the data across the system's disks in blocks.
Reference: [DJ93] <author> Erik P. DeBenedictis and Stephen C. Johnson. </author> <title> Extending Unix for scalable computing. </title> <journal> IEEE Computer, </journal> <volume> 26(11) </volume> <pages> 43-53, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: that this interface be used primarily for manipulating data on disk (e.g., copying or sorting files) rather than for the transfer of data to and from compute nodes. 8.2.2 nCUBE A file-system interface proposed for the nCUBE is based on a two-step mapping of a file into the compute-node memories <ref> [DJ93] </ref>. The first step is to provide a mapping from subfiles stored on multiple disks to an abstract dataset (a traditional one-dimensional I/O stream). The second step is mapping the abstract dataset into the compute-node memories.
Reference: [DSE88] <author> Peter Dibble, Michael Scott, and Carla Ellis. </author> <title> Bridge: A high-performance file system for parallel processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: failure of one disk. 8.2 Non-Unix Parallel File Systems Although most commercially available parallel file systems are based on the Unix model, there has been a great deal of interesting research done using alternative interfaces and file models. 120 8.2.1 Bridge Bridge was one of the earliest parallel file systems <ref> [DSE88, Dib90] </ref>, and is unusual in not separating I/O nodes from compute nodes. Bridge provides three interfaces. The simplest, designed to be used by a single compute node, is similar to a traditional Unix interface and transparently distributes the data across the system's disks in blocks.
Reference: [EK93] <author> Rudiger Esser and Renate Knecht. </author> <title> Intel Paragon XP/S | architecture and software environment. </title> <type> Technical Report KFA-ZAM-IB-9305, </type> <institution> Central Institute for Applied Mathematics, Research Center Julich, Germany, </institution> <month> April 26 </month> <year> 1993. </year>
Reference-contexts: CFS was written for the iPSC family of parallel machines. Its successor, PFS, is similar and was written for the Paragon <ref> [EK93, RP95] </ref>. CFS and PFS provide a simple, Unix-like interface to the application. The blocks of a file are declustered across all the disks in round-robin order. CFS and PFS extend the conventional Unix interface to provide support for parallel applications by introducing several varieties of shared file pointer.
Reference: [EKO95] <author> Dawson R. Engler, M. Frans Kaashoek, and James W. O'Toole Jr. Exokernel: </author> <title> An operating system architecture for application-level resource management. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 251-266, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: This approach is similar in spirit to the techniques suggested by the SPIN [BSP + 95] and Exokernel <ref> [EKO95] </ref> projects, which involve uniprocessor operating systems. The ability to run application code at the IOPs could be used to allow applications to perform data-dependent filtering or distribution, possibly reducing network traffic.
Reference: [FCHP95] <author> Dror G. Feitelson, Peter F. Corbett, Yarson Hsu, and Jean-Pierre Prost. </author> <title> Parallel I/O systems and interfaces for parallel computers. In Multiprocessor Systems | Design and Integration. </title> <publisher> World Scientific, </publisher> <year> 1995. </year>
Reference-contexts: This restriction includes the number of I/O nodes, the number of compute nodes, the disk-block size, the unit-of-transfer size, and, for some data distributions, the matrix dimensions. This interface was implemented, but never released. 121 8.2.3 Vesta The Vesta file system breaks away from the traditional one-dimensional file structure <ref> [CBF93, CF96, FCHP95] </ref>. Files in Vesta are two-dimensional, and are composed of multiple cells, each of which is a sequence of basic striping units. BSUs are essentially records, or fixed-sized sequences of bytes. Like Galley's subfiles, each cell resides on a single disk.
Reference: [FE89] <author> Richard Allen Floyd and Carla Schlatter Ellis. </author> <title> Directory reference patterns in hierarchical file systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(2) </volume> <pages> 238-247, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: General-purpose uniprocessor workloads. Uniprocessor file-access patterns have been measured many times. Floyd and Ellis <ref> [Flo86, FE89] </ref> and Ousterhout et al. [OCH + 85] measured isolated Unix machines. Baker et al. studied the workload of a cluster of workstations running the Sprite file system, which is a distributed, Unix-like system [BHK + 91].
Reference: [Flo86] <author> Rick Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical Report 177, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> March </month> <year> 1986. </year> <month> 134 </month>
Reference-contexts: General-purpose uniprocessor workloads. Uniprocessor file-access patterns have been measured many times. Floyd and Ellis <ref> [Flo86, FE89] </ref> and Ousterhout et al. [OCH + 85] measured isolated Unix machines. Baker et al. studied the workload of a cluster of workstations running the Sprite file system, which is a distributed, Unix-like system [BHK + 91]. <p> Note also that there were very few files that were read and written in the same open. This latter behavior is common in Unix file systems <ref> [Flo86] </ref> and may be accentuated here by the difficulty in coordinating concurrent reads and writes to the same file (note that the CFS file-access modes are of little help for read-write access).
Reference: [FPD93] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference-contexts: The I/O nodes are based on the Intel i386 processor, and each has a single bus for SCSI disk drives. There may also be one or more service 16 nodes that handle Ethernet connections or interactive shells [NAS93]. Intel's Concurrent File System (CFS) <ref> [Pie89, FPD93, Nit92] </ref> provides a Unix-like interface to the user with the addition of four I/O modes, as discussed in the previous chapter. CFS generally stripes each file across all I/O nodes in 4 KB blocks.
Reference: [GGL93] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: Baylor et al. performed a similar analysis of several applications on the IBM SP-2 [BW96]. Galbreath et al. present a high-level characterization of multiprocessor file-system workloads based on anecdotal evidence <ref> [GGL93] </ref>.
Reference: [GL91] <author> Andrew S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: object-oriented extensible file systems. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> page 177, </pages> <year> 1991. </year>
Reference-contexts: It appears that MPI-IO could also feasibly be implemented on top of a nested-batched interface. 8.2.5 ELFS The ELFS system, from the University of Virginia, is an object-oriented file system that has tight ties to the Mentat programming language <ref> [GP91, GL91] </ref>. Files in ELFS are instances of object classes, which provide a high-level interface to an abstract data structure and encapsulate the access patterns and the actual structure of the file.
Reference: [GP91] <author> Andrew S. Grimshaw and Jeff Prem. </author> <title> High performance parallel file objects. </title> <booktitle> In Sixth Annual Distributed-Memory Computer Conference, </booktitle> <pages> pages 720-723, </pages> <year> 1991. </year>
Reference-contexts: It appears that MPI-IO could also feasibly be implemented on top of a nested-batched interface. 8.2.5 ELFS The ELFS system, from the University of Virginia, is an object-oriented file system that has tight ties to the Mentat programming language <ref> [GP91, GL91] </ref>. Files in ELFS are instances of object classes, which provide a high-level interface to an abstract data structure and encapsulate the access patterns and the actual structure of the file.
Reference: [GP94] <author> Gregory R. Ganger and Yale N. Patt. </author> <title> Metadata update performance in file systems. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 49-60, </pages> <year> 1994. </year>
Reference-contexts: Indeed, 129 it is likely that the fork will actually contain data from a fork that had been deleted, which clearly raises security concerns. It should be possible to improve reliability without greatly sacrificing performance by implementing a scheme such as soft metadata updates <ref> [GP94] </ref>. * Heterogeneity The current implementation of Galley does not support a heterogeneous environment. The assumption that the CPs and IOPs all use the same basic data representation pervades the implementation.
Reference: [Gro96] <author> NAS Scientific Consulting Group. </author> <title> Hitchhiker's guide to using the NAS SP2, </title> <year> 1996. </year>
Reference-contexts: Furthermore, 256 bytes is far smaller than the request sizes needed to achieve peak performance in most other parallel file systems that provide linear file models <ref> [Nit92, LIN + 93, Gro96] </ref>. Finally, although the peak speed is lower than that available through Galley's low-level interface, the increase in performance gained by using a strided interface is even greater for LFM than for the raw Galley 98 32 KB striping unit.
Reference: [GSC + 95] <author> Garth A. Gibson, Daniel Stodolsky, Pay W. Chang, William V. Courtright II, Chris G. Demetriou, Eka Ginting, Mark Holland, Qingming Ma, LeAnn Neal, R. Hugo Patter-son, Jiawen Su, Rachad Youssef, and Jim Zelenka. </author> <title> The Scotch parallel storage system. </title> <booktitle> In Proceedings of 40th IEEE Computer Society International Conference (COMPCON 95), </booktitle> <pages> pages 403-410, </pages> <address> San Francisco, </address> <month> Spring </month> <year> 1995. </year>
Reference-contexts: Earlier proposals borrowed our nested-batched requests for the core of their interface, but this level of functionality is now described as an "extension" to the core interface. 8.1.9 Scotch Scotch is a parallel file system from Carnegie Mellon University <ref> [GSC + 95] </ref>. It appears that they eventually intend Scotch to support the SIO low-level interface. Scotch differs from other Unix-like parallel file systems in two significant ways. The first difference is Scotch's reliance on application-influenced prefetching for high performance.
Reference: [HER + 95] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Furthermore, the designers explicitly did not intend for OSF/1 to satisfy the I/O performance needs of scientific, supercomputer applications [Roy93]. 8.1.7 PPFS Like the systems mentioned above, PPFS provides the end user with a linear file that is accessed with primitives similar to the traditional read ()/write () interface <ref> [HER + 95] </ref>. In PPFS, however, the basic transfer unit is an application-defined record, rather than a byte. PPFS maps requests against the logical, linear stream of records to an underlying two-dimensional model, indexed with a (disk, record) pair.
Reference: [HP91] <institution> Hewlett Packard. </institution> <note> HP97556/58/60 5.25-inch SCSI Disk Drives Technical Reference Manual, second edition, </note> <month> June </month> <year> 1991. </year> <title> HP Part number 5960-0115. </title>
Reference-contexts: The HP 97560 has an average seek time of 13.5 ms and a maximum sustained throughput of 2.2 MB/s <ref> [HP91] </ref>. Our implementation of the disk model was based on earlier implementations described in [RW94, 69 KTR94]. Among the factors simulated by our model are head-switch time, track-switch time, SCSI--bus overhead, controller overhead, rotational latency, and the disk cache.
Reference: [HPF93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <address> 1.0 edition, </address> <month> May 3 </month> <year> 1993. </year>
Reference-contexts: These interfaces are sometimes tightly integrated into a particular language such as HPF <ref> [Lov93, BGMZ92, HPF93] </ref> or CMF [Thi94]. There has also been great deal of research in designing libraries to support parallel I/O. The focus of much of this research has been the support of distributed matrices, particularly for data-parallel variants of Fortran [CBH + 94, TBC + 94, TG96].
Reference: [IBM94] <author> IBM. </author> <title> AIX Version 3.2 General Programming Concepts, </title> <booktitle> twelfth edition, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: List requests int gfs_read_listio (int fid, void *buf, struct gfs_list *vec, int quant) Finally, in addition to these structured operations, Galley provides a list interface, which has functionality similar to the POSIX lio listio () interface <ref> [IBM94] </ref>. This interface allows an application to simply specify an array of (file offset, memory offset, size) triples that it would like transferred between memory and disk.
Reference: [JWB96] <editor> Ravi Jain, John Werth, and James C. Browne, editors. </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference: [KE93a] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):140-145, January and February 1993. </note>
Reference-contexts: Overall, the amount of block sharing implies strong interprocess spatial locality, and suggests that caching may be successful. 3.4 Caching Buffering and caching are common in traditional file systems, and with the right policies can be successful in multiprocessor file systems <ref> [KE93b, KE93a] </ref>. One advantage of caching is that multiple small requests (which were common in this workload) may be combined into a few larger requests that can be more efficiently served by disk hardware.
Reference: [KE93b] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Overall, the amount of block sharing implies strong interprocess spatial locality, and suggests that caching may be successful. 3.4 Caching Buffering and caching are common in traditional file systems, and with the right policies can be successful in multiprocessor file systems <ref> [KE93b, KE93a] </ref>. One advantage of caching is that multiple small requests (which were common in this workload) may be combined into a few larger requests that can be more efficiently served by disk hardware.
Reference: [KFG94] <author> John F. Karpovich, James C. French, and Andrew S. Grimshaw. </author> <title> High performance access to radio astronomy data: A case study. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 240-249, </pages> <month> September </month> <year> 1994. </year> <note> Also available as UVA TR CS-94-25. </note>
Reference-contexts: A library that was capable of handling many kinds of queries and FITS files is a perfect example of the type of domain-specific library we expect to be implemented on Galley. 7.1.1 FITS at NRAO One specific example of how FITS files are used in practice is described in <ref> [KFG94, KGF93] </ref>. This type of FITS file contains records with 6 keys, describing the frequency domain (U; V; W ), the baseline, and the time the data was collected. The baseline is a single number that indicates 86 which antenna or combination of antennas generated that record. <p> For example, a user may want to examine all the records within a given time range, sorted along the U axis. Previous work on these files has focused on increasing locality along several dimensions simultaneously. In <ref> [KFG94, KGF93] </ref>, the authors examine studied the effectiveness of Piecewise Linear Order-Preserving Hashing (PLOP) files at reducing the amount of time required to perform common queries, by increasing certain kinds of locality within the files.
Reference: [KGF93] <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French. Breaking the I/O bottleneck at the National Radio Astronomy Observatory (NRAO). </title> <type> Technical Report CS-94-37, </type> <institution> University of Virginia, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: A library that was capable of handling many kinds of queries and FITS files is a perfect example of the type of domain-specific library we expect to be implemented on Galley. 7.1.1 FITS at NRAO One specific example of how FITS files are used in practice is described in <ref> [KFG94, KGF93] </ref>. This type of FITS file contains records with 6 keys, describing the frequency domain (U; V; W ), the baseline, and the time the data was collected. The baseline is a single number that indicates 86 which antenna or combination of antennas generated that record. <p> For example, a user may want to examine all the records within a given time range, sorted along the U axis. Previous work on these files has focused on increasing locality along several dimensions simultaneously. In <ref> [KFG94, KGF93] </ref>, the authors examine studied the effectiveness of Piecewise Linear Order-Preserving Hashing (PLOP) files at reducing the amount of time required to perform common queries, by increasing certain kinds of locality within the files. <p> To evaluate the efficacy of their PLOP-file implementation, the authors performed several queries, which were intended to be representative of those that were most commonly used in practice at NRAO <ref> [KGF93] </ref>. Their tests were performed on a single-processor, single-disk system. We performed the same set of queries, using the same data set, on our implementation. Our tests were performed on a cluster of IBM RS/6000s connected by an FDDI network. <p> The specific queries performed in both cases are briefly described below. More detail about each query, and why it is commonly used at NRAO, may be found in <ref> [KGF93] </ref>. 1. Read the full data set. 2. Read the full data set, sorting records by time. 3. Read the full data set, sorting records by baseline. 4. Read a sub-volume of the data including 10% of the time range. 5.
Reference: [Kim86] <author> Michelle Y. Kim. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(11):978-988, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: This practice is typically called striping or declustering. This striping is performed by breaking the file into smaller units, typically measured in kilobytes, and assigning these units to disks in round-robin order. A simple example of such a declustering may be seen in Figure 2.3. Kim <ref> [Kim86] </ref> and Salem [SGM86] were among the earliest researchers to demonstrate the usefulness of transparent striping. This approach is simple to implement, and it is easy to identify on which disk a given file block is stored, without requiring an expensive lookup operation.
Reference: [KN96] <author> David Kotz and Nils Nieuwejaar. </author> <title> Flexibility and performance of parallel file systems. </title> <journal> ACM Operating Systems Review, </journal> <volume> 30(2) </volume> <pages> 63-73, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: applications can provide to the file system to increase performance further? How should applications provide this information? One of the more radical methods of addressing this issue is to allow applications to "download" code to the IOP, where it would run in the same address space as the I/O server <ref> [KN96] </ref>. This approach is similar in spirit to the techniques suggested by the SPIN [BSP + 95] and Exokernel [EKO95] projects, which involve uniprocessor operating systems.
Reference: [KR94] <author> Thomas T. Kwan and Daniel A. Reed. </author> <title> Performance of the CM-5 scalable file system. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 156-165, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Indeed, most do not even examine the performance of requests of fewer than many kilobytes <ref> [Nit92, BBH95, KR94] </ref>. As discussed earlier, multiprocessor filesystem workloads frequently include many small requests.
Reference: [Kri94] <author> Orran Krieger. </author> <title> HFS: A flexible file system for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Since applications manipulate files only via the external interface to the objects, applications are robust under architectural changes. 8.2.6 HFS The Hurricane File System (HFS) is a part of the Hurricane operating system and was designed at the University of Toronto to run on the Hector shared-memory multiprocessor <ref> [KS93, Kri94, SVW + 93] </ref>. While HFS shares our goal of providing a flexible, high-performance file system, it adopts an entirely different approach. HFS is based on a complex, highly-structured, object-oriented model.
Reference: [KS93] <author> Orran Krieger and Michael Stumm. </author> <title> HFS: a flexible file system for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 6-14, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution>
Reference-contexts: Since applications manipulate files only via the external interface to the objects, applications are robust under architectural changes. 8.2.6 HFS The Hurricane File System (HFS) is a part of the Hurricane operating system and was designed at the University of Toronto to run on the Hector shared-memory multiprocessor <ref> [KS93, Kri94, SVW + 93] </ref>. While HFS shares our goal of providing a flexible, high-performance file system, it adopts an entirely different approach. HFS is based on a complex, highly-structured, object-oriented model.
Reference: [KSU94] <author> Orran Krieger, Michael Stumm, and Ronald Unrau. </author> <title> The Alloc Stream Facility: A redesign of application-level stream I/O. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 75-82, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: To achieve this flexibility, each sub-object must fit exacting specifications to fulfill the assumptions of other sub-objects. Most applications interact with the file system through a user-level application library such as the Alloc Stream Facility <ref> [KSU94] </ref>. Note that the ideas underlying HFS could be used to build class libraries on top of Galley. 8.2.7 Whiptail Whiptail is a file system developed for the Intel Paragon, and was built on top of Intel's PFS.
Reference: [KTR94] <author> David Kotz, Song Bac Toh, and Sriram Radhakrishnan. </author> <title> A detailed simulation model of the HP 97560 disk drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: For example, it was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>. Instead, our observations show that many scientific applications make many small, regular, but non-consecutive requests to the file system. Using the results from our workload characterizations and from performance evaluations of 1 existing multiprocessor file systems, we have developed Galley. <p> interfaces and policies, implemented in libraries, * allow easy and efficient implementations of libraries, * be scalable enough to run well on multiprocessors with tens or hundreds of nodes, * minimize memory and performance overhead. 5.2 File Structure Most parallel file systems are based on a Unix-like, linear file model <ref> [BGST93, LIN + 93, Pie89] </ref>. Under this model, a file is seen as an addressable, linear sequence of bytes. Applications can issue 42 requests to read or write contiguous subranges of that sequence of bytes. <p> The most common method of implementing a linear file model on a parallel file system is to break the file into a series of blocks, all the same size, and decluster them across the disks in the system, in round-robin order <ref> [Pie89, LIN + 93, SGM86] </ref>. This method is relatively simple to implement, and can lead to good performance in some cases. Specifically, when individual nodes in an application access a linear file in large chunks, each access will involve data stored on multiple disks. <p> Furthermore, 256 bytes is far smaller than the request sizes needed to achieve peak performance in most other parallel file systems that provide linear file models <ref> [Nit92, LIN + 93, Gro96] </ref>. Finally, although the peak speed is lower than that available through Galley's low-level interface, the increase in performance gained by using a strided interface is even greater for LFM than for the raw Galley 98 32 KB striping unit. <p> To efficiently support data-parallel I/O operations, Thinking Machines developed a file system called sfs, which was derived from SunOS <ref> [LIN + 93] </ref>. Files in sfs are distributed across logical devices, which are groups of physical disks, clustered into a level 3 RAID. <p> While sfs is capable of providing high bandwidth for large transfers that span multiple blocks, a high start-up latency leads to poor performance for small requests. sfs also has structural problems that lead to poor performance when multiple files within a single cylinder group are used <ref> [LIN + 93] </ref>. sfs was designed as a low-level file system.
Reference: [LMKQ89] <author> Samuel J. Le*er, Marshall Kirk McKusick, Michael J. Karels, and John S. Quar-terman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: CFS and PFS extend the conventional Unix interface to provide support for parallel applications by introducing several varieties of shared file pointer. The simplest form of shared file pointer is similar to the "atomic append" of BSD 4.3 <ref> [LMKQ89] </ref>. The remaining two types of shared file pointer are similar to the first, but enforce round-robin access to the file from all the nodes. The second type of pointer allows arbitrary sized records while the third, and fastest, shared mode requires that all records be of the same size.
Reference: [Lov93] <author> David B. Loveman. </author> <title> High Performance Fortran. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(1) </volume> <pages> 25-42, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: These interfaces are sometimes tightly integrated into a particular language such as HPF <ref> [Lov93, BGMZ92, HPF93] </ref> or CMF [Thi94]. There has also been great deal of research in designing libraries to support parallel I/O. The focus of much of this research has been the support of distributed matrices, particularly for data-parallel variants of Fortran [CBH + 94, TBC + 94, TG96].
Reference: [MHQ96] <author> Jason A. Moore, Phil Hatcher, and Michael J. Quinn. </author> <title> Efficient data-parallel files via automatic mode detection. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1-14, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year> <month> 136 </month>
Reference-contexts: Under Galley, this index could be stored in one fork, while the database itself could be stored in a second fork. A final example of the potential use of forks is Stream*, a parallel file abstraction for the data-parallel language, C* <ref> [MHQ96] </ref>. Stream* divides a file into three distinct segments, each of which corresponds to a particular set of access semantics. Although the current implementation of Stream* stores all the segments in a single file, one could use a different fork for each segment.
Reference: [MJLF84] <author> Marshall K. McKusick, William N. Joy, Samuel J. Le*er, and Robert S. Fabry. </author> <title> A fast file system for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: The Berkeley Fast File System (FFS), a higher-performance Unix file system released with BSD 4.2, used a block size of at least 4096 bytes, and the inode typically contained 5 to 13 direct entries <ref> [MJLF84] </ref>. Thus, FFS is able to access files at least 20 KB in size using only the direct entries in the inode. If the file is too large to be mapped by the direct entries in the inode, the file system uses an indirect block.
Reference: [MK91] <author> Ethan L. Miller and Randy H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: For example, it was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>. Instead, our observations show that many scientific applications make many small, regular, but non-consecutive requests to the file system. Using the results from our workload characterizations and from performance evaluations of 1 existing multiprocessor file systems, we have developed Galley. <p> In [dC94], del Rosario and Choud-hary provide an informal characterization of grand-challenge applications. Powell measured a set of static characteristics (file sizes) of a Cray-1 file system [Pow77]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns <ref> [MK91] </ref>, focusing primarily on access rates. Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [MK93], giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [PP93, PP94a, PP94b]. <p> Although these files were larger than those in a general-purpose file system [BHK + 91], they were smaller than we would expect to see in a scientific supercomputing environment <ref> [MK91] </ref>. We suspect that users limited their file sizes due to the small disk capacity (7.2 GB) and limited disk bandwidth (10 MB/s peak). 3.3.3 I/O Request Sizes Figures 3.5 and 3.6 show that the vast majority of reads are small, but that most bytes are transferred through large reads. <p> We define a consecutive request to be a sequential request that begins precisely one byte beyond where the previous request ended. A common characteristic of file workloads, particularly scientific workloads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. Frequently, files are also accessed in their entirety. Figures 3.7 and 3.8 show the amount of sequential and consecutive access (on a per-node basis) to files with more than one request in our workload. <p> This further suggests that when caching was limited to the I/O nodes, most of the hits were indeed a result of interprocess locality because, as Figure 3.10 shows, the limited intraprocess locality was filtered out by the compute-node cache. Note the contrast with Miller and Katz's tracing study <ref> [MK91] </ref>, which found little benefit from caching. They did notice a benefit from prefetching and write-behind. <p> A common characteristic of many file-system workloads, particularly scientific file-system work-loads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. In the parallel file-system workload, we found that while almost 93% of all files were accessed sequentially, consecutive access was primarily limited to those files that were only opened by one compute node. <p> Since previous studies have shown that scientific applications rarely access files randomly <ref> [MK91] </ref>, the fact that a large number of multi-node files have many different interval sizes suggests that these files are being accessed in some complex, but possibly regular, pattern. 4.1.1 Strided Accesses Although files may be opened by multiple nodes simultaneously, we are only interested here in the accesses generated by
Reference: [MK93] <author> Ethan L. Miller and Randy H. Katz. </author> <title> An analysis of file migration in a UNIX supercomputing environment. </title> <booktitle> In Proceedings of the 1993 Winter USENIX Conference, </booktitle> <pages> pages 421-434, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Powell measured a set of static characteristics (file sizes) of a Cray-1 file system [Pow77]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [MK91], focusing primarily on access rates. Miller and Katz also measured secondary-tertiary file migration patterns on a Cray <ref> [MK93] </ref>, giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [PP93, PP94a, PP94b]. All of these studies are limited to single-process applications on vector supercomputers.
Reference: [MMRW94] <author> Arthur B. Maccabe, Kevin S. McCurley, Rolf Riesen, and Stephen R. Wheat. </author> <title> SUNMOS for the Intel Paragon: A brief user's guide. </title> <booktitle> In Proceedings of the Intel Supercomputer Users Group Conference, </booktitle> <pages> pages 245-251, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: This power does come at some cost; on a Paragon, SUNMOS requires only 256KB of memory on each node, but OSF/1 AD occupies nearly 10MB <ref> [MMRW94] </ref>. File system access in OSF/1 is built on top of Mach Memory Objects [Roy93]. Since memory objects are restricted to multiples of the page size, small, non-contiguous requests, such as those common in parallel scientific workloads, may lead to poor performance.
Reference: [MPI94] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <address> 1.0 edition, </address> <month> May 5 </month> <year> 1994. </year>
Reference-contexts: hand, the same results show that in many cases, the strict partitioning offered by nCUBE and Vesta may match the applications' needs for write-only files. 122 8.2.4 MPI-IO MPI-IO [The96] is a draft standard for parallel I/O, which derives much of its philosophy and interface from the MPI message-passing standard <ref> [MPI94] </ref>. In MPI-IO, file I/O is modeled as message passing. That is, reading from a file is analogous to receiving a message and writing to a file is analogous to sending a message.
Reference: [NAS93] <institution> NASA Ames Research Center, Moffet Field, CA. NAS User Guide, </institution> <address> 6.1 edition, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: The I/O nodes are based on the Intel i386 processor, and each has a single bus for SCSI disk drives. There may also be one or more service 16 nodes that handle Ethernet connections or interactive shells <ref> [NAS93] </ref>. Intel's Concurrent File System (CFS) [Pie89, FPD93, Nit92] provides a Unix-like interface to the user with the addition of four I/O modes, as discussed in the previous chapter. CFS generally stripes each file across all I/O nodes in 4 KB blocks.
Reference: [NAS94] <institution> NASA/Science Office of Standards and Technology, NASA Goddard Space Flight Center, </institution> <month> Greenbelt, </month> <title> MD 020771. A User's Guide for the Flexible Image Transport System (FITS), </title> <address> 3.1 edition, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: We will discuss, in detail, an application written directly to Galley's API, a user-level library implemented on top of Galley, and an application implemented on top of the user-level library. 7.1 FITS The Flexible Image Transport System (FITS) data format is a standard format for astronomical data <ref> [NAS94] </ref>. A FITS file begins with an ASCII header that describes the contents of the file and structure of the records in the file. The remainder of the file is a series of records, stored in binary form.
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: The I/O nodes are based on the Intel i386 processor, and each has a single bus for SCSI disk drives. There may also be one or more service 16 nodes that handle Ethernet connections or interactive shells [NAS93]. Intel's Concurrent File System (CFS) <ref> [Pie89, FPD93, Nit92] </ref> provides a Unix-like interface to the user with the addition of four I/O modes, as discussed in the previous chapter. CFS generally stripes each file across all I/O nodes in 4 KB blocks. <p> Similarly, 89% of all writes were for fewer than 100 bytes, but those writes transferred only about 2.5% of all data written. The number of small requests is surprising due to their poor performance in CFS <ref> [Nit92] </ref>. <p> Prefetching is an attempt to reduce the latency of file access perceived by the process. Galley's DiskManager does not attempt to prefetch data for two reasons. First, indiscriminate prefetching can cause thrashing in the buffer cache <ref> [Nit92] </ref>. Second, prefetching is based on the assumption that the system can intelligently guess what an application is going to request next. <p> Indeed, most do not even examine the performance of requests of fewer than many kilobytes <ref> [Nit92, BBH95, KR94] </ref>. As discussed earlier, multiprocessor filesystem workloads frequently include many small requests. <p> Furthermore, 256 bytes is far smaller than the request sizes needed to achieve peak performance in most other parallel file systems that provide linear file models <ref> [Nit92, LIN + 93, Gro96] </ref>. Finally, although the peak speed is lower than that available through Galley's low-level interface, the increase in performance gained by using a strided interface is even greater for LFM than for the raw Galley 98 32 KB striping unit. <p> Some simply provide a Unix-like interface, and some provide the full semantics required by Unix standards. 8.1.1 CFS/PFS Intel's Concurrent File System <ref> [Pie89, Nit92] </ref> is frequently cited as the canonical first-generation parallel file system. CFS was written for the iPSC family of parallel machines. Its successor, PFS, is similar and was written for the Paragon [EK93, RP95]. CFS and PFS provide a simple, Unix-like interface to the application.
Reference: [NK96] <author> Nils Nieuwejaar and David Kotz. </author> <title> Performance of the Galley parallel file system. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 83-94, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: When testing an earlier version of Galley we found that with large numbers of IOPs, the network congestion at the CPs was so great that the CPs were unable to receive data and issue new requests to the IOPs in a timely fashion <ref> [NK96] </ref>. This congestion was also responsible for the limited TCP/IP throughput with large numbers of nodes.
Reference: [OCH + 85] <author> John Ousterhout, Herve Da Costa, David Harrison, John Kunze, Mike Kupfer, and James Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: General-purpose uniprocessor workloads. Uniprocessor file-access patterns have been measured many times. Floyd and Ellis [Flo86, FE89] and Ousterhout et al. <ref> [OCH + 85] </ref> measured isolated Unix machines. Baker et al. studied the workload of a cluster of workstations running the Sprite file system, which is a distributed, Unix-like system [BHK + 91]. Ramakrishnan et al. studied access patterns in a commercial computing environment on a VAX/VMS platform [RBK92]. <p> We define a consecutive request to be a sequential request that begins precisely one byte beyond where the previous request ended. A common characteristic of file workloads, particularly scientific workloads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. Frequently, files are also accessed in their entirety. Figures 3.7 and 3.8 show the amount of sequential and consecutive access (on a per-node basis) to files with more than one request in our workload. <p> A common characteristic of many file-system workloads, particularly scientific file-system work-loads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. In the parallel file-system workload, we found that while almost 93% of all files were accessed sequentially, consecutive access was primarily limited to those files that were only opened by one compute node. <p> The CP then fills the appropriate buffer with zeros. Not only does this approach avoid a disk access, it avoids sending a block of data across the network. The Unix approach is optimized for small files. The workload studies discussed in <ref> [OCH + 85, BHK + 91] </ref> show that most files in a Unix workload can be addressed using just the direct-mapping entries in the inode. Few files will require an indirect block and fewer still will require a doubly-indirect block.
Reference: [PEK + 95] <author> Apratim Purakayastha, Carla Schlatter Ellis, David Kotz, Nils Nieuwejaar, and Michael Best. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 165-172, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: nodes leads to interprocess spatial locality, both of which could be successfully captured by caching. 32 3.5 Workload Characterization of a CM-5 Working with researchers from Duke University and Thinking Machines Corporation, we performed a similar workload characterization on a Thinking Machines CM-5 at the National Center for Supercomputing Applications <ref> [PEK + 95, Pur96] </ref>. The CM-5 had 512 compute nodes and a much more powerful I/O subsystem than the iPSC/860. <p> In the CMMD workload, modes with shared file pointers were used, but only because those modes were substantially faster than the independent modes. Anecdotal evidence collected from those users indicated that they would have preferred to use the independent modes <ref> [PEK + 95] </ref>. 7.2.2 Data Access Interface The standard Unix interface provides only simple primitives for accessing the data in files. These primitives are limited to read ()ing and write ()ing consecutive regions of a file.
Reference: [PEK96] <author> Apratim Purakayastha, Carla Schlatter Ellis, and David Kotz. </author> <title> ENWRICH: a compute-processor write caching scheme for parallel file systems. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 55-68, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Other people have examined the possibility of caching data for write-only files at the compute node <ref> [PEK96] </ref>. The results of a simple trace-driven simulation of read-only compute-node caching, with 4 KB (one block) buffers and LRU replacement, are shown in with no request sent to an I/O node).
Reference: [PFDJ89] <author> Terrence W. Pratt, James C. French, Phillip M. Dickens, and Stanley A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 161-166, </pages> <year> 1989. </year>
Reference-contexts: For example, it was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>. Instead, our observations show that many scientific applications make many small, regular, but non-consecutive requests to the file system. Using the results from our workload characterizations and from performance evaluations of 1 existing multiprocessor file systems, we have developed Galley.
Reference: [PGK88] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: One of the most common types of parallel I/O system is the RAID (Redundant Array of Inexpensive Disks) system <ref> [PGK88] </ref>. In a RAID system multiple disks are clustered together, and the system can deliver high performance by exploiting the high aggregate bandwidth of the multiple disks. A RAID is usually connected to a computer system by a high-speed channel, such as SCSI-2, Fiber Channel, or HiPPI.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: For example, it was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>. Instead, our observations show that many scientific applications make many small, regular, but non-consecutive requests to the file system. Using the results from our workload characterizations and from performance evaluations of 1 existing multiprocessor file systems, we have developed Galley. <p> The I/O nodes are based on the Intel i386 processor, and each has a single bus for SCSI disk drives. There may also be one or more service 16 nodes that handle Ethernet connections or interactive shells [NAS93]. Intel's Concurrent File System (CFS) <ref> [Pie89, FPD93, Nit92] </ref> provides a Unix-like interface to the user with the addition of four I/O modes, as discussed in the previous chapter. CFS generally stripes each file across all I/O nodes in 4 KB blocks. <p> interfaces and policies, implemented in libraries, * allow easy and efficient implementations of libraries, * be scalable enough to run well on multiprocessors with tens or hundreds of nodes, * minimize memory and performance overhead. 5.2 File Structure Most parallel file systems are based on a Unix-like, linear file model <ref> [BGST93, LIN + 93, Pie89] </ref>. Under this model, a file is seen as an addressable, linear sequence of bytes. Applications can issue 42 requests to read or write contiguous subranges of that sequence of bytes. <p> The most common method of implementing a linear file model on a parallel file system is to break the file into a series of blocks, all the same size, and decluster them across the disks in the system, in round-robin order <ref> [Pie89, LIN + 93, SGM86] </ref>. This method is relatively simple to implement, and can lead to good performance in some cases. Specifically, when individual nodes in an application access a linear file in large chunks, each access will involve data stored on multiple disks. <p> Some simply provide a Unix-like interface, and some provide the full semantics required by Unix standards. 8.1.1 CFS/PFS Intel's Concurrent File System <ref> [Pie89, Nit92] </ref> is frequently cited as the canonical first-generation parallel file system. CFS was written for the iPSC family of parallel machines. Its successor, PFS, is similar and was written for the Paragon [EK93, RP95]. CFS and PFS provide a simple, Unix-like interface to the application.
Reference: [Pow77] <author> Michael L. Powell. </author> <title> The DEMOS File System. </title> <booktitle> In Proceedings of the Sixth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 33-42, </pages> <month> November </month> <year> 1977. </year>
Reference-contexts: Some studies examined scientific workloads on vector machines. In [dC94], del Rosario and Choud-hary provide an informal characterization of grand-challenge applications. Powell measured a set of static characteristics (file sizes) of a Cray-1 file system <ref> [Pow77] </ref>. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [MK91], focusing primarily on access rates. Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [MK93], giving a good picture of long-term, whole-file access patterns.
Reference: [PP93] <author> Barbara K. Pasquale and George C. Polyzos. </author> <title> A static analysis of I/O characteristics of scientific applications in a production workload. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 388-397, </pages> <year> 1993. </year>
Reference-contexts: Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [MK93], giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate <ref> [PP93, PP94a, PP94b] </ref>. All of these studies are limited to single-process applications on vector supercomputers. These studies identify several characteristics that are common among supercomputer file-system workloads.
Reference: [PP94a] <author> Barbara K. Pasquale and George C. Polyzos. </author> <title> A case study of a scientific application I/O behavior. </title> <booktitle> In Proceedings of the International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, </booktitle> <pages> pages 101-106, </pages> <year> 1994. </year>
Reference-contexts: Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [MK93], giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate <ref> [PP93, PP94a, PP94b] </ref>. All of these studies are limited to single-process applications on vector supercomputers. These studies identify several characteristics that are common among supercomputer file-system workloads.
Reference: [PP94b] <author> Barbara K. Pasquale and George C. Polyzos. </author> <title> Dynamic I/O characterization of I/O-intensive scientific applications. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 660-669, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [MK93], giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate <ref> [PP93, PP94a, PP94b] </ref>. All of these studies are limited to single-process applications on vector supercomputers. These studies identify several characteristics that are common among supercomputer file-system workloads.
Reference: [PSC + 95] <author> Ravi Ponnusamy, Joel Saltz, Alok Choudhary, Yuan-Shin Hwang, and Geoffrey Fox. </author> <title> Runtime support and compilation methods for user-specified data distributions. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(8) </volume> <pages> 815-831, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Other libraries support multidimensional matrix I/O under C++ [SW94, SCJ + 95, CWS + 96]. There has also been quite a bit of work done in supporting irregular data structures: <ref> [CSBS94, PSC + 95] </ref> discuss the issues relating to interprocessor distribution of these structures, and Jovian explores the issues relating to their persistent storage [BBS + 94].
Reference: [Pur96] <author> Apratim Purakayastha. </author> <title> Characterizing and Optimizing Parallel File Systems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Duke University, Durham, NC, </institution> <month> June </month> <year> 1996. </year> <note> Also available as technical report CS-1996-10. </note>
Reference-contexts: nodes leads to interprocess spatial locality, both of which could be successfully captured by caching. 32 3.5 Workload Characterization of a CM-5 Working with researchers from Duke University and Thinking Machines Corporation, we performed a similar workload characterization on a Thinking Machines CM-5 at the National Center for Supercomputing Applications <ref> [PEK + 95, Pur96] </ref>. The CM-5 had 512 compute nodes and a much more powerful I/O subsystem than the iPSC/860.
Reference: [RB90] <author> A. L. Narasimha Reddy and Prithviraj Banerjee. </author> <title> A study of I/O behavior of Perfect benchmarks on a multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 312-321, </pages> <year> 1990. </year>
Reference-contexts: Scientific parallel applications. Although there has been no other study of the full workload of a multiprocessor file system, people have studied individual applications. Reddy et al. chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns <ref> [RB90] </ref>. This study is interesting, but far from what we need: the sample size is small; the programs are parallelized sequential programs, not parallel programs per se; and the I/O itself was not parallelized. Cypher et al. [CHKM93] studied individual parallel scientific applications, measuring temporal patterns in I/O rates.
Reference: [RBK92] <author> K. K. Ramakrishnan, P. Biswas, and Ramakrishna Karedla. </author> <title> Analysis of file I/O traces in commercial computing environments. </title> <booktitle> In Proceedings of ACM SIGMETRICS and PERFORMANCE '92, </booktitle> <pages> pages 78-90, </pages> <year> 1992. </year>
Reference-contexts: Baker et al. studied the workload of a cluster of workstations running the Sprite file system, which is a distributed, Unix-like system [BHK + 91]. Ramakrishnan et al. studied access patterns in a commercial computing environment on a VAX/VMS platform <ref> [RBK92] </ref>. These studies all cover general-purpose (engineering and office) workloads with uniprocessor applications.
Reference: [Roy93] <author> Paul J. Roy. </author> <title> Unix file access and caching in a multicomputer environment. </title> <booktitle> In Proceedings of the Usenix Mach III Symposium, </booktitle> <pages> pages 21-37, </pages> <year> 1993. </year>
Reference-contexts: This power does come at some cost; on a Paragon, SUNMOS requires only 256KB of memory on each node, but OSF/1 AD occupies nearly 10MB [MMRW94]. File system access in OSF/1 is built on top of Mach Memory Objects <ref> [Roy93] </ref>. Since memory objects are restricted to multiples of the page size, small, non-contiguous requests, such as those common in parallel scientific workloads, may lead to poor performance. There is no support for a multiprocessing environment at the user interface. <p> There is no support for a multiprocessing environment at the user interface. Furthermore, the designers explicitly did not intend for OSF/1 to satisfy the I/O performance needs of scientific, supercomputer applications <ref> [Roy93] </ref>. 8.1.7 PPFS Like the systems mentioned above, PPFS provides the end user with a linear file that is accessed with primitives similar to the traditional read ()/write () interface [HER + 95]. In PPFS, however, the basic transfer unit is an application-defined record, rather than a byte.
Reference: [RP95] <author> Brad Rullman and David Payne. </author> <title> An efficient file I/O interface for parallel applications. DRAFT presented at the Workshop on Scalable I/O, </title> <booktitle> Frontiers '95, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: CFS was written for the iPSC family of parallel machines. Its successor, PFS, is similar and was written for the Paragon <ref> [EK93, RP95] </ref>. CFS and PFS provide a simple, Unix-like interface to the application. The blocks of a file are declustered across all the disks in round-robin order. CFS and PFS extend the conventional Unix interface to provide support for parallel applications by introducing several varieties of shared file pointer.
Reference: [RT78] <author> D. M. Ritchie and K. Thompson. </author> <title> The UNIX time-sharing system. </title> <journal> The Bell System Technical Journal, </journal> <volume> 6(2) </volume> <pages> 1905-1930, </pages> <month> July-August </month> <year> 1978. </year>
Reference-contexts: Under Unix, each file is represented as a linear, addressable stream of bytes <ref> [Tho78, RT78] </ref>. In other words, files are collections of bytes, arranged in a one-dimensional structure. <p> A direct mapping is a list of blocks containing actual file data. This direct mapping is generally kept right in the inode. In the original Unix file system, the inode contained direct mapping entries for the first 10 blocks of the file <ref> [Tho78, RT78] </ref>. Since each block in that system was 512 bytes, files that were 5120 bytes or less were mapped completely by the direct entries in the inode.
Reference: [RW94] <author> Chris Ruemmler and John Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year> <month> 138 </month>
Reference-contexts: The HP 97560 has an average seek time of 13.5 ms and a maximum sustained throughput of 2.2 MB/s [HP91]. Our implementation of the disk model was based on earlier implementations described in <ref> [RW94, 69 KTR94] </ref>. Among the factors simulated by our model are head-switch time, track-switch time, SCSI--bus overhead, controller overhead, rotational latency, and the disk cache.
Reference: [SCJ + 95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Galley has also been used by other people. The Panda Array I/O library was designed at the University of Illinois, to support high performance I/O for multidimensional matrices <ref> [SCJ + 95, SW94] </ref>. While most such projects are based on some variant of Fortran, this group chose to examine the issue of supporting distributed matrices under C++. <p> The focus of much of this research has been the support of distributed matrices, particularly for data-parallel variants of Fortran [CBH + 94, TBC + 94, TG96]. Other libraries support multidimensional matrix I/O under C++ <ref> [SW94, SCJ + 95, CWS + 96] </ref>. There has also been quite a bit of work done in supporting irregular data structures: [CSBS94, PSC + 95] discuss the issues relating to interprocessor distribution of these structures, and Jovian explores the issues relating to their persistent storage [BBS + 94].
Reference: [SCO90] <author> Margo Seltzer, Peter Chen, and John Ousterhout. </author> <title> Disk scheduling revisited. </title> <booktitle> In Proceedings of the 1990 Winter USENIX Conference, </booktitle> <pages> pages 313-324, </pages> <year> 1990. </year>
Reference-contexts: The DiskManager maintains a list of pending block requests. As new requests arrive from the CacheManager, they are placed into the list according to the disk scheduling algorithm. The DiskManager currently uses a Cyclical Scan algorithm <ref> [SCO90] </ref>. When a block has been read from disk, the DiskManager updates the cache status of that block's buffer from `not ready' to `ready' and adds it to the requesting threads' ready queues.
Reference: [SGM86] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference-contexts: This practice is typically called striping or declustering. This striping is performed by breaking the file into smaller units, typically measured in kilobytes, and assigning these units to disks in round-robin order. A simple example of such a declustering may be seen in Figure 2.3. Kim [Kim86] and Salem <ref> [SGM86] </ref> were among the earliest researchers to demonstrate the usefulness of transparent striping. This approach is simple to implement, and it is easy to identify on which disk a given file block is stored, without requiring an expensive lookup operation. <p> The most common method of implementing a linear file model on a parallel file system is to break the file into a series of blocks, all the same size, and decluster them across the disks in the system, in round-robin order <ref> [Pie89, LIN + 93, SGM86] </ref>. This method is relatively simple to implement, and can lead to good performance in some cases. Specifically, when individual nodes in an application access a linear file in large chunks, each access will involve data stored on multiple disks.
Reference: [SVW + 93] <author> Michael Stumm, Zvonko Vranesic, Ron White, Ronald Unrau, and Keith Farkas. </author> <title> Experiences with the Hector multiprocessor. </title> <booktitle> In Proceedings of the Parallel Systems Fair at the International Parallel Processing Symposium, </booktitle> <pages> pages 10-17, </pages> <year> 1993. </year>
Reference-contexts: Since applications manipulate files only via the external interface to the objects, applications are robust under architectural changes. 8.2.6 HFS The Hurricane File System (HFS) is a part of the Hurricane operating system and was designed at the University of Toronto to run on the Hector shared-memory multiprocessor <ref> [KS93, Kri94, SVW + 93] </ref>. While HFS shares our goal of providing a flexible, high-performance file system, it adopts an entirely different approach. HFS is based on a complex, highly-structured, object-oriented model.
Reference: [SW94] <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Galley has also been used by other people. The Panda Array I/O library was designed at the University of Illinois, to support high performance I/O for multidimensional matrices <ref> [SCJ + 95, SW94] </ref>. While most such projects are based on some variant of Fortran, this group chose to examine the issue of supporting distributed matrices under C++. <p> The focus of much of this research has been the support of distributed matrices, particularly for data-parallel variants of Fortran [CBH + 94, TBC + 94, TG96]. Other libraries support multidimensional matrix I/O under C++ <ref> [SW94, SCJ + 95, CWS + 96] </ref>. There has also been quite a bit of work done in supporting irregular data structures: [CSBS94, PSC + 95] discuss the issues relating to interprocessor distribution of these structures, and Jovian explores the issues relating to their persistent storage [BBS + 94].
Reference: [SW95a] <author> K. E. Seamons and M. Winslett. </author> <title> A data management approach for handling large compressed arrays in high performance computing. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 119-128, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: In addition to storing data in the traditional sense, many libraries also need to store persistent, library-specific "metadata" independently of the data proper. One example of such a library would be a compression library similar to that described in <ref> [SW95a] </ref>. Rather than compressing the whole file at once, making it difficult to modify or extract data in the middle of the file, the file is broken into a series of chunks, which are then compressed independently.
Reference: [SW95b] <author> Elizabeth A. M. Shriver and Leonard F. Wisniewski. </author> <title> An API for choreographing data accesses. </title> <type> Technical Report PCS-TR95-267, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: Note that the ideas underlying HFS could be used to build class libraries on top of Galley. 8.2.7 Whiptail Whiptail is a file system developed for the Intel Paragon, and was built on top of Intel's PFS. It was designed to support libraries for out-of-core applications <ref> [SW95b, SWC + 95] </ref>, and it implements many of the recommendations for parallel I/O discussed in [CK93]. Whiptail provides no byte-level operations; all file operations must be done in units that are multiples of the block size.
Reference: [SWC + 95] <author> Elizabeth A. M. Shriver, Leonard F. Wisniewski, Bruce G. Calder, David Greenberg, Ryan Moore, and David Womble. </author> <title> Parallel disk access using the Whiptail File System: Design and implementation. </title> <type> Unpublished Manuscript, </type> <year> 1995. </year>
Reference-contexts: Note that the ideas underlying HFS could be used to build class libraries on top of Galley. 8.2.7 Whiptail Whiptail is a file system developed for the Intel Paragon, and was built on top of Intel's PFS. It was designed to support libraries for out-of-core applications <ref> [SW95b, SWC + 95] </ref>, and it implements many of the recommendations for parallel I/O discussed in [CK93]. Whiptail provides no byte-level operations; all file operations must be done in units that are multiples of the block size.
Reference: [TBC + 94] <author> Rajeev Thakur, Rajesh Bordawekar, Alok Choudhary, Ravi Ponnusamy, and Tarvinder Singh. </author> <title> PASSION runtime library for parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: There has also been great deal of research in designing libraries to support parallel I/O. The focus of much of this research has been the support of distributed matrices, particularly for data-parallel variants of Fortran <ref> [CBH + 94, TBC + 94, TG96] </ref>. Other libraries support multidimensional matrix I/O under C++ [SW94, SCJ + 95, CWS + 96].
Reference: [TG96] <author> Sivan Toledo and Fred G. Gustavson. </author> <title> The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 28-40, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: One such example is the two-dimensional, cyclically-shifted block layout scheme for matrices, shown in Figure 5.1, which was designed for SOLAR, a portable, out-of-core linear-algebra library <ref> [TG96] </ref>. This data layout is intended to efficiently support a wide variety of out-of-core algorithms. In particular, it allows blocks of rows and columns to be transferred efficiently, with a high degree of I/O parallelism, as well as square or nearly-square sub-matrices. <p> In particular, it allows blocks of rows and columns to be transferred efficiently, with a high degree of I/O parallelism, as well as square or nearly-square sub-matrices. To avoid the limitations of the linear file model, Galley does not impose a declustering strategy 43 described in <ref> [TG96] </ref>. In this example there are 6 disks, logically arranged into a 2-by-3 grid, and a 6-by-12 block matrix. The number in each square indicates the disk on which that block is stored. on an application's data. <p> In particular, Galley's subfiles and forks simplified the implementation of Vesta's two-dimensional file structure, and Galley's nested-strided interface supports Vesta's logical views. Vesta is described in greater detail in the next chapter. SOLAR is a library of routines to support applications that use out-of-core, dense, matrix computations <ref> [TG96] </ref>. SOLAR relies on existing high-performance in-core subroutine libraries to do much of the computation, and it provides its own optimized matrix I/O library. SOLAR's author ported the package to Galley, and found that the subfile model provided him with a useful degree of control over the distribution of data. <p> There has also been great deal of research in designing libraries to support parallel I/O. The focus of much of this research has been the support of distributed matrices, particularly for data-parallel variants of Fortran <ref> [CBH + 94, TBC + 94, TG96] </ref>. Other libraries support multidimensional matrix I/O under C++ [SW94, SCJ + 95, CWS + 96].
Reference: [The96] <author> The MPI-IO Committee. </author> <title> MPI-IO: a parallel file I/O interface for MPI, </title> <month> April </month> <year> 1996. </year> <note> Version 0.5. </note>
Reference-contexts: Most parallel file systems provide applications with little more than a standard Unix interface. Specifically, they only allow applications to accesses contiguous regions of the file in a single request. The most notable exceptions to this rule are Vesta [CF96] and MPI-IO <ref> [The96] </ref>, which we discuss further in Chapter 8. Both of these systems allow applications to describe logical views of a file. That is, each process in a parallel application describes the subset of the file that it wishes to access. <p> On the other hand, the same results show that in many cases, the strict partitioning offered by nCUBE and Vesta may match the applications' needs for write-only files. 122 8.2.4 MPI-IO MPI-IO <ref> [The96] </ref> is a draft standard for parallel I/O, which derives much of its philosophy and interface from the MPI message-passing standard [MPI94]. In MPI-IO, file I/O is modeled as message passing.
Reference: [Thi94] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, Mass. </address> <note> CM Fortran User's Guide, 2.1 edition, </note> <month> January </month> <year> 1994. </year>
Reference-contexts: These interfaces are sometimes tightly integrated into a particular language such as HPF [Lov93, BGMZ92, HPF93] or CMF <ref> [Thi94] </ref>. There has also been great deal of research in designing libraries to support parallel I/O. The focus of much of this research has been the support of distributed matrices, particularly for data-parallel variants of Fortran [CBH + 94, TBC + 94, TG96].
Reference: [Tho78] <author> K. Thompson. </author> <title> UNIX implementation. </title> <journal> The Bell System Technical Journal, </journal> <volume> 6(2) </volume> <pages> 1931-1946, </pages> <month> July-August </month> <year> 1978. </year>
Reference-contexts: Under Unix, each file is represented as a linear, addressable stream of bytes <ref> [Tho78, RT78] </ref>. In other words, files are collections of bytes, arranged in a one-dimensional structure. <p> A direct mapping is a list of blocks containing actual file data. This direct mapping is generally kept right in the inode. In the original Unix file system, the inode contained direct mapping entries for the first 10 blocks of the file <ref> [Tho78, RT78] </ref>. Since each block in that system was 512 bytes, files that were 5120 bytes or less were mapped completely by the direct entries in the inode.
Reference: [Tho96] <author> Joel T. Thomas. </author> <title> The Panda array I/O library on the Galley parallel file system. </title> <type> Technical Report PCS-TR96-288, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> June </month> <year> 1996. </year> <type> Senior Honors Thesis. 139 </type>
Reference-contexts: While most such projects are based on some variant of Fortran, this group chose to examine the issue of supporting distributed matrices under C++. Joel Thomas, a Dartmouth undergraduate, redesigned and modified the Panda Array Library to run on top of the Galley Parallel File System <ref> [Tho96] </ref>. The Vesta file system, from IBM Research, uses a two-dimensional file model. Vesta provides applications with a concise method of describing how these files should be partitioned among the compute nodes in an application.
Reference: [vdW93] <author> Rob van der Wijngaart. </author> <title> Efficient implementation of a 3-dimensional ADI Method on the iPSC/860. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 102-111, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: A simple, two-dimensional multi-partitioned matrix is shown in Figure 7.11. This distribution is designed to give good locality, which reduces inter-processor communication, and to lead to good load balancing, which increases processor utilization and reduces total execution time <ref> [vdW93] </ref>. Since the solution for each cell relies on knowledge about points on the borders of each surrounding cell, the submatrices on each processor are slightly larger than the size of the cells for which that processor is responsible. CPs exchange messages between each iteration to update this boundary information.
Reference: [WMR + 94] <author> Stephen R. Wheat, Arthur B. Maccabe, Rolf Riesen, David W. van Dresser, and T. Mack Stallcup. PUMA: </author> <title> An operating system for massively parallel systems. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 56-65, </pages> <year> 1994. </year>
Reference-contexts: Using Unix's ioctl () facility, administrators and advanced users may interact with the underlying parallel file system. 118 8.1.5 SUNMOS and PUMA SUNMOS and its successor, PUMA, are operating systems that were developed by Sandia National Laboratory and the University of New Mexico for the Intel Paragon <ref> [WMR + 94] </ref>. The design goal behind both SUNMOS and PUMA was to make as much of the power in the hardware available to the user as possible. For this reason, the designers adopted simple interfaces that could be implemented efficiently.
Reference: [ZRB + 93] <author> Roman Zajcew, Paul Roy, David Black, Chris Peak, Paulo Guedes, Bradford Kemp, John LoVerso, Michael Leibensperger, Michael Barnett, FaraMarz Rabii, and Durriya Netterwala. </author> <title> An OSF/1 UNIX for massively parallel multicomputers. </title> <booktitle> In Proceedings of the 1993 Winter USENIX Conference, </booktitle> <pages> pages 449-468, </pages> <month> January </month> <year> 1993. </year> <month> 140 </month>
Reference-contexts: file system; both are built on top of the underlying PFS and UFS file systems. 8.1.6 OSF/1 AD While SUNMOS and PUMA offer a "lean and mean" operating system to users of the Intel Paragon, OSF/1 AD provides the full power of the Unix operating system to each compute node <ref> [ZRB + 93] </ref>. This power does come at some cost; on a Paragon, SUNMOS requires only 256KB of memory on each node, but OSF/1 AD occupies nearly 10MB [MMRW94]. File system access in OSF/1 is built on top of Mach Memory Objects [Roy93].
References-found: 105

