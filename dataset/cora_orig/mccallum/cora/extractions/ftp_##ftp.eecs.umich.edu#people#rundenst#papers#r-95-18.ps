URL: ftp://ftp.eecs.umich.edu/people/rundenst/papers/r-95-18.ps
Refering-URL: ftp://ftp.eecs.umich.edu/people/rundenst/papers/INDEX.html
Root-URL: http://www.cs.umich.edu
Email: rundenst)@eecs.umich.edu.  
Title: A VISUAL MULTIMEDIA QUERY LANGUAGE FOR TEMPORAL ANALYSIS OF VIDEO DATA  
Author: Stacie Hibino and Elke A. Rundensteiner, 
Address: (hibino,  
Affiliation: University of Michigan,  
Note: [by  To appear in an upcoming Multimedia DBMS book, edited by Kingsley Nwosu.]  
Abstract: The storage of various media in multimedia databases poses new challenges to query techniques | challenges that exceed the expressive power of traditional text-based query languages. New query interfaces should take advantage of characteristics inherent in multimedia data, such as the dynamic temporal nature of video, the visual and spatial characteristics of images, the pitch of audio, etc. The focus of our research is to exploit the temporal continuity and combined spatio-temporal characteristics of video data for the purpose of video analysis. We do so by integrating a visual query paradigm with a dynamic visual presentation of results into a user-friendly interactive visualization environment. In this chapter, we present our overall approach for identifying trends in video data via querying for relationships between video annotations. Our approach allows users to analyze the video in terms of temporal relationships between events (e.g., events of type A frequently follow events of type B). We present a temporal visual query language (TVQL) for specifying relative temporal queries between sets of annotations. This query language builds on the notion of dynamic query filters and significantly extends them. It is tailored for temporal analysis | allowing users to pose queries, as well as to browse the data in a temporally continuous manner, thereby aiding them in the discovery of temporal trends. The TVQL is augmented with complementary temporal diagrams, which provide intuitive visual feedback for quickly and qualitatively verifying the temporal query specified. This chapter includes the complete specification of our TVQL, a transformation function for generating corresponding temporal diagrams, and the process for mapping TVQL queries to system queries. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Allen, J.F. </author> <year> (1983). </year> <title> Maintaining knowledge about temporal intervals. </title> <journal> Communications of the ACM, </journal> <volume> 26(11), </volume> <pages> 832-843. </pages>
Reference-contexts: It describes information such as whether A1 starts before B1 or whether A1 and B1 finish at the same time. In order to be complete, a temporal visual query language needs to be able to specify any primitive Visual Multimedia Query Language 17 temporal relationship <ref> [1] </ref> as well as any combination of these primitives. This section describes how our TVQL can be used to accomplish this. Primitive Temporal Relationships Allen [1] describes thirteen primitive temporal relationships between two events: before, meets, during, starts, finishes, overlaps, the symmetric counterparts to these six relationships, and the equals relationship <p> In order to be complete, a temporal visual query language needs to be able to specify any primitive Visual Multimedia Query Language 17 temporal relationship <ref> [1] </ref> as well as any combination of these primitives. This section describes how our TVQL can be used to accomplish this. Primitive Temporal Relationships Allen [1] describes thirteen primitive temporal relationships between two events: before, meets, during, starts, finishes, overlaps, the symmetric counterparts to these six relationships, and the equals relationship (see first column of Table 2).
Reference: [2] <author> Ahlberg, C., Williamson, C., & Shneiderman, B. </author> <year> (1992). </year> <title> Dynamic Queries for Information Exploration: An Implementation and Evaluation. </title> <booktitle> CHI'92 Conference Proceedings, </booktitle> <pages> 619-626: </pages> <publisher> ACM Press. </publisher>
Reference-contexts: This chapter describes our overall environment (Section 3) and then presents the details of our temporal visual query language (TVQL | see Section 5). Our TVQL can be used to specify relative temporal queries between sets of annotations. This query language builds on the notion of dynamic query filters <ref> [2] </ref> and significantly extends them. It is tailored for temporal analysis | allowing users to pose queries, as well as to browse the data in a temporally continuous manner, thereby aiding them in the discovery of temporal trends. <p> Users can then explore and analyze the video through iteratively specifying queries using a visual query language (VQL) and reviewing the visualization of results presented. Similar to VIS, our interface will also use dynamic query filters <ref> [2] </ref>. Our filters, however, will be customized to handle relative temporal queries. The advantage of query filters is that they allow rapid incremental adjustment of query parameters via the use of buttons and sliders. <p> We hypothesize, however, that our visual query approach will be more effective than a forms-based language for the purpose of searching for temporal trends in the video data. This hypothesis is supported by a study comparing the use of dynamic queries to a forms-based query language <ref> [2] </ref>. In this study, the researchers found that users performed significantly better using dynamic queries over a forms-based approach for three out of five tasks, one of which involved looking for trends in the data. <p> Our temporal query filters are consistent with the original DQ filters in terms of interlinking the filters together and maintaining the notion of progressive refinement through continuous value range selections. Besides the work in dynamic query filters by Ahlberg et al <ref> [2] </ref> and others (e.g., [11]), previous work in graphical or visual query languages has focused on diagrammatic, forms-based, or iconic approaches. Diagrammatic query languages allow users to specify queries by using direct manipulation of schema-type constructs to represent a set of objects and their relationships (e.g., [28, 23]).
Reference: [3] <author> Ahlberg, C., & Shneiderman, B. </author> <year> (1994). </year> <title> Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays. </title> <booktitle> CHI'94 Conference Proceedings, </booktitle> <pages> 313-317: </pages> <publisher> ACM Press. </publisher>
Reference-contexts: We postulate that visually presenting the results retrieved from relative queries will facilitate this searching for trends. The query and review part of the video analysis process is similar to the process used in visual information seeking (VIS) <ref> [3] </ref>. The VIS process is a process for browsing database information; a process characterized by rapid filtering, progressive refinement, continuous reformulation of goals, and visual scanning to identify results. These characteristics make VIS particularly well suited for searching for trends in the video annotations. <p> While we can use the VIS approach to identify and specify interesting subsets of data, we cannot use the approach to specify relationships between these subsets <ref> [3] </ref>. <p> Note that while value ranges for discrete predicate domains such as "Name" and "Category" are selected from lists, numerically valued ranges such as "Dur" (for duration) are selected using double-thumbed slider filters. This type of filter was introduced in <ref> [3] </ref> for specifying ranges of values. Each thumb has an arrow, pointing towards the range being specified. The thumb arrow is filled to indicate that the endpoint of a range is included, or empty to indicate that the endpoint of a range is excluded. <p> Ranges are filled in the sliders for further clarification. The text above the slider indicates exact values. Figure 6 identifies each of these components of the double-thumbed slider. Visual Multimedia Query Language 13 In addition to double-thumbed sliders, Ahlberg and Shneiderman <ref> [3] </ref> introduce a number of other interfaces for different types of query filters. For example, they use a single-thumbed slider (with no arrow) for specifying a single value, or a single-thumbed slider with an arrow to indicate a range towards one of the extremes of the slider. <p> by using both temporal and spatial characteristics of the media, as well as by addressing needs for both retrieval and analysis. 10 CONCLUSION, CURRENT STATUS AND FUTURE WORK In this chapter, we have presented a framework for video analysis based on applying and adapting a visual information seeking (VIS) approach <ref> [3] </ref> to spatio-temporal video data. In our extensions to VIS, we have defined a temporal visual query language (TVQL) for specifying relative temporal queries, including relative temporal position and duration between video annotations.
Reference: [4] <author> Chakravarthy, A.S. </author> <year> (1994). </year> <title> Toward Semantic Retrieval of Pictures and Video. </title> <booktitle> AAAI'94 Workshop on Indexing and Reuse in Multimedia Systems, </booktitle> <pages> 12-18. </pages>
Reference-contexts: On the other hand, databases which handle temporal media tend to focus on semantic or text-based queries as well as on locating information rather than analyzing it (e.g., <ref> [17, 4] </ref>). In these types of databases, media are typically images or short clips with textual 34 Chapter 1 captions or descriptions. Information is then located by semantic inferencing on these textual descriptions (e.g., [17]).
Reference: [5] <author> Chu, </author> <title> W.W., Ieong, I.T., Taira, R.K., & Breant, C.M. (1992). A Temporal Evolutionary Object-Oriented Data Model and Its Query Language for Medical Image Management. </title> <booktitle> Proceedings of the 18th VLDB Conference, </booktitle> <month> 53-64: </month> <title> Very Large Data Base Endowment. </title> <note> 36 Chapter 1 </note>
Reference-contexts: Historical databases, such as those used to manage medical images <ref> [5] </ref> are different from databases of temporal media in that they focus on discrete changes entered into the databases as an effect of changes made in the real world (e.g., John broke his leg on April 10, 1994).
Reference: [6] <author> Chua, T.-S., Lim, S.-K., & Pung, H.-K. </author> <year> (1994). </year> <title> Content-Based Retrieval of Segmented Images. </title> <booktitle> ACM Multimedia'94 Proceedings: </booktitle> <publisher> ACM Press. </publisher>
Reference: [7] <author> Davis, M. </author> <year> (1994). </year> <title> Knowledge Representation for Video. </title> <booktitle> Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 120-127: </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: While some work has begun in temporal queries of continuous, dynamic media, such work has tended to focus on issues other than analysis. For example, work by [18] focuses on synchronization and multimedia playback. Work by Davis <ref> [7] </ref> focuses on repurposing video, using iconic annotations to represent objects and events in a video and collecting these icons in a parallel timeline format. While his iconic language supports temporal encodings including relative ones, it requires users to select icons for each type of temporal relationship individually.
Reference: [8] <author> Freksa, C. </author> <year> (1992). </year> <title> Temporal reasoning based on semi- intervals. </title> <journal> Artificial Intelligence, </journal> <volume> 54(1992), </volume> <pages> 199-227. </pages>
Reference-contexts: Moreover, because the filters can specify ranges and because they are bound to one another to prevent invalid combinations, we can use the filters to specify single "neighborhoods" of related temporal primitives. Freksa <ref> [8] </ref> defines two primitive temporal relationships between two events to be (conceptual) neighbors if a continuous change (e.g., shortening, lengthening, or moving of the duration of the events) to the events can be used to transform either relation to the other [without passing through an additional primitive temporal relationship]. <p> In the diagrams, the rectangle refers to B events, while the connected circles are used to designate potential relative positions of A events. The open circles represent possible starting points for set A, while filled circles represent possible ending points. Freksa <ref> [8] </ref> uses similar diagrams to describe individual relationships but then uses different icons to describe combinations of temporal relationships (see columns 2 and 3 of Table 2). While the use of Freksa's icons provides a compact visual description, it is more difficult to decipher than our temporal diagrams.
Reference: [9] <author> Fishkin, K. and Stone, M.C. </author> <year> (1995). </year> <title> Enhanced Dynamic Queries via Movable Filters. </title> <booktitle> CHI'95 Conference Proceedings, </booktitle> <pages> 415-420. </pages> <publisher> ACM Press. </publisher>
Reference-contexts: That is, these databases focus on discrete changes to static objects rather than changes in continuous, dynamic media (such as video). Although other researchers are working on extensions to dynamic query filters, this work has focused on aggregation extensions to the interface <ref> [9, 11] </ref>. While this recent work can be added to our system for the purpose of forming subsets, they are not sufficient to meet the needs of video analysis described in this chapter.
Reference: [10] <author> Gevers, T. and Smeulders, A.W.M. </author> <year> (1992). </year> <title> Indexing of Images by Pictorial Information. Visual Database Systems, </title> <editor> II (E. Knuth and L.M. Wegner, Eds.), </editor> <publisher> North Holland: Amsterdam, </publisher> <pages> 93-100. </pages>
Reference-contexts: While some image databases allow users to search using spatial information, such approaches have either not dealt with temporal media, or have focused on object extraction (e.g., <ref> [10] </ref>).
Reference: [11] <author> Goldstein, J. & Roth, S. </author> <year> (1994). </year> <title> Using Aggregation and Dynamic Queries for Exploring Large Data Sets. </title> <booktitle> CHI'94 Conference Proceedings, </booktitle> <pages> 23-29. </pages> <publisher> ACM Press. </publisher>
Reference-contexts: That is, these databases focus on discrete changes to static objects rather than changes in continuous, dynamic media (such as video). Although other researchers are working on extensions to dynamic query filters, this work has focused on aggregation extensions to the interface <ref> [9, 11] </ref>. While this recent work can be added to our system for the purpose of forming subsets, they are not sufficient to meet the needs of video analysis described in this chapter. <p> Our temporal query filters are consistent with the original DQ filters in terms of interlinking the filters together and maintaining the notion of progressive refinement through continuous value range selections. Besides the work in dynamic query filters by Ahlberg et al [2] and others (e.g., <ref> [11] </ref>), previous work in graphical or visual query languages has focused on diagrammatic, forms-based, or iconic approaches. Diagrammatic query languages allow users to specify queries by using direct manipulation of schema-type constructs to represent a set of objects and their relationships (e.g., [28, 23]).
Reference: [12] <author> Hampapur, A., Weymouth, T., & Jain, R. </author> <year> (1994). </year> <title> Digital Video Segmentation. </title> <booktitle> ACM Multimedia'94 Proceedings, </booktitle> <pages> 357-364: </pages> <publisher> ACM Press. </publisher>
Reference: [13] <author> Harrison, B.L., Owen, R., & Baecker, R.M. </author> <year> (1994). </year> <title> Timelines: An Interactive System for the Collection of Visualization of Temporal Data. </title> <booktitle> Proceedings of Graphics Interface '94. Canadian Information Processing Society. </booktitle>
Reference-contexts: In the case of video analysis, we can then examine temporal and/or spatial relationships between objects and events. While some work in video databases and analysis 1 <ref> [19, 21, 27, 13] </ref> has used the temporal characteristics of video, little work in temporal query techniques of dynamic media has been done to take advantage of the temporal continuity and/or the combined spatio-temporal characteristics of the medium. <p> Similar to previous work in video analysis <ref> [13, 19, 22] </ref>, we are also using annotations for abstracting information and coding the video data.
Reference: [14] <author> Hibino, S. & Rundensteiner, E. </author> <year> (1995a). </year> <title> A Visual Query Language for Identifying Temporal Trends in Video Data. </title> <booktitle> To appear in Proceedings of the First International Workship on Multimedia Database Management Systems. </booktitle>
Reference-contexts: This ability to incrementally adjust queries allows users to temporally browse the data in a new and powerful way. Visual Multimedia Query Language 5 1.4 Relationship to Previous Work Preliminary descriptions of this work have been presented elsewhere <ref> [14, 16] </ref>. This chapter expands this previous work, providing a more refined and complete description and derivation of our temporal visual query language (TVQL).
Reference: [15] <author> Hibino, S. & Rundensteiner, E. </author> <year> (1995b). </year> <title> Interactive Visualizations for Exploration and Spatio-Temporal Analysis of Video Data. </title> <note> To appear in IJ-CAI'95 Workshop on Intelligent Multimedia Information Retrieval. </note>
Reference-contexts: These characteristics make VIS particularly well suited for searching for trends in the video annotations. We are thus adopting this methodology as our underlying framework. problem of video data analysis <ref> [15] </ref>. In our MultiMedia VIS (MMVIS), users first code the video data with annotations. These annotations are stored in an underlying database (also known as the Annotation Server).
Reference: [16] <author> Hibino, S. & Rundensteiner, E. </author> <month> (Dec </month> <year> 1994). </year> <title> A Graphical Query Language for Identifying Temporal Trends in Video Data. </title> <institution> University of Michigan, EECS Technical Report CSE-TR-225-94. </institution>
Reference-contexts: This ability to incrementally adjust queries allows users to temporally browse the data in a new and powerful way. Visual Multimedia Query Language 5 1.4 Relationship to Previous Work Preliminary descriptions of this work have been presented elsewhere <ref> [14, 16] </ref>. This chapter expands this previous work, providing a more refined and complete description and derivation of our temporal visual query language (TVQL).
Reference: [17] <author> Lenat, D. & Guha, R.V. </author> <year> (1994). </year> <title> Strongly Semantic Information Retrieval. </title> <booktitle> AAAI'94 Workshop on Indexing and Reuse in Multimedia Systems, </booktitle> <pages> 58-68. </pages>
Reference-contexts: On the other hand, databases which handle temporal media tend to focus on semantic or text-based queries as well as on locating information rather than analyzing it (e.g., <ref> [17, 4] </ref>). In these types of databases, media are typically images or short clips with textual 34 Chapter 1 captions or descriptions. Information is then located by semantic inferencing on these textual descriptions (e.g., [17]). <p> In these types of databases, media are typically images or short clips with textual 34 Chapter 1 captions or descriptions. Information is then located by semantic inferencing on these textual descriptions (e.g., <ref> [17] </ref>). The drawback of this type of approach is that it does not take advantage of the temporal and/or spatial characteristics inherent in the media.
Reference: [18] <author> Little, T.D.C. & Ghafoor, A. </author> <year> (1993). </year> <title> Interval-Based Conceptual Models for Time-Dependent Multimedia Data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(4), </volume> <month> 551-563. </month> <title> Visual Multimedia Query Language 37 </title>
Reference-contexts: While some work has begun in temporal queries of continuous, dynamic media, such work has tended to focus on issues other than analysis. For example, work by <ref> [18] </ref> focuses on synchronization and multimedia playback. Work by Davis [7] focuses on repurposing video, using iconic annotations to represent objects and events in a video and collecting these icons in a parallel timeline format.
Reference: [19] <author> Mackay, W. E. </author> <year> (1989). </year> <title> EVA: An experimental video annotator for symbolic analysis of video data. </title> <journal> SIGCHI Bulletin, </journal> <volume> 21(2), </volume> <pages> 68-71. </pages>
Reference-contexts: In the case of video analysis, we can then examine temporal and/or spatial relationships between objects and events. While some work in video databases and analysis 1 <ref> [19, 21, 27, 13] </ref> has used the temporal characteristics of video, little work in temporal query techniques of dynamic media has been done to take advantage of the temporal continuity and/or the combined spatio-temporal characteristics of the medium. <p> Similar to previous work in video analysis <ref> [13, 19, 22] </ref>, we are also using annotations for abstracting information and coding the video data.
Reference: [20] <author> Nagasaka, A. and Tanaka, A. </author> <year> (1992). </year> <title> Automatic Video Indexing and Full-Video Search for Object Appearances. Visual Database Systems, </title> <editor> II (E. Knuth and L.M. Wegner, Eds.), </editor> <address> 113-127. </address> <publisher> Elsevier Science Publishers. </publisher>
Reference-contexts: Query interfaces to multimedia databases should thus take advantage of such inherent characteristics in order to properly allow the exploitation of the richness of information captured in the media. Recent work in image and video databases illustrate the use of query techniques which exploit inherent visual and spatial characteristics <ref> [20, 26] </ref>. For example, Ueda, et al [26] have developed techniques to find images based on the user's selection of an area in an existing image. <p> This will aid users in coding the video data in a consistent manner. Although we are currently requiring researchers to input annotations manually, we expect that previous work by others in the area of object extraction (e.g., <ref> [20] </ref>) will eventually be used to generate annotations automatically. We also expect, however, that it will be too inefficient to operate on the raw data directly.
Reference: [21] <author> Oomoto, E. & Tanaka, K. </author> <year> (1993). </year> <title> OVID: Design and Implementation of a Video-Object Database System. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(4), </volume> <pages> 629-643. </pages>
Reference-contexts: In the case of video analysis, we can then examine temporal and/or spatial relationships between objects and events. While some work in video databases and analysis 1 <ref> [19, 21, 27, 13] </ref> has used the temporal characteristics of video, little work in temporal query techniques of dynamic media has been done to take advantage of the temporal continuity and/or the combined spatio-temporal characteristics of the medium. <p> While such an interface could be used to build temporal queries, it would lack the continuous browsing support provided by our TVQL. Similarly, while novel techniques have been used to combine forms-based queries with automated object extraction from videos (e.g., <ref> [21] </ref>), such an interface would also require users to specify temporal primitives as well as ranges of temporal values by hand, thus disrupting the ability to review the video data in a temporally continuous manner.
Reference: [22] <author> Roschelle, J., Pea, R., & Trigg, R. </author> <year> (1990). </year> <title> VIDEONOTER: A tool for exploratory analysis (Research Rep. No. </title> <address> IRL90- 0021). Palo Alto, CA: </address> <booktitle> Institute for Research on Learning. </booktitle>
Reference-contexts: Similar to previous work in video analysis <ref> [13, 19, 22] </ref>, we are also using annotations for abstracting information and coding the video data.
Reference: [23] <author> Santucci, G. </author> & <title> Sottile, </title> <address> P.A. </address> <year> (1993). </year> <title> Query by Diagram: a Visual Environment for Querying Databases. </title> <journal> Software | Practice and Experience, </journal> <volume> 23(3), </volume> <pages> 317-340. </pages>
Reference-contexts: Diagrammatic query languages allow users to specify queries by using direct manipulation of schema-type constructs to represent a set of objects and their relationships (e.g., <ref> [28, 23] </ref>). Essentially, you could build a query graphically by representing each object by a circle, attaching predicates to the circles, and using lines to denote relationships between the circles.
Reference: [24] <author> Snodgrass, R. </author> <year> (1987). </year> <title> The Temporal Query Language TQuel. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 12(2), </volume> <pages> 247-298. </pages>
Reference-contexts: Note that this one query palette now specifies a disjunctive combination of related primitive temporal relationships, corresponding to the disjunction of several primitive predicates in typical textual query languages (e.g., <ref> [24] </ref>). That is, in the example, setting startA-startB = 0 and endA-endB = any is equivalent to requesting events of type A which "start", "equal", or are "started by" events of type B.
Reference: [25] <author> Snodgrass, R. </author> <year> (1992). </year> <title> Temporal Databases. Theories and Methods of Spatio-Temporal Reasoning in Geographic Space (A.U. </title> <editor> Frank, I. Campari, and U. Formentini, Eds.), </editor> <publisher> Springer-Verlag: </publisher> <address> New York, </address> <pages> 22-64. </pages>
Reference-contexts: Preliminary results indicate that the dynamically updated temporal diagrams are strong visual aids explaining the semantics of the query to the users. 9 RELATED WORK Research in temporal queries has focused more on the context of historical databases rather than on databases of temporal media (e.g., <ref> [25] </ref>). Historical databases, such as those used to manage medical images [5] are different from databases of temporal media in that they focus on discrete changes entered into the databases as an effect of changes made in the real world (e.g., John broke his leg on April 10, 1994).
Reference: [26] <author> Ueda, H., Miyatake, T., Sumino, S., & Nagasaka, A. </author> <year> (1993). </year> <title> Automatic Structure Visualization for Video Editing. </title> <booktitle> InterCHI'93 Proceedings, </booktitle> <pages> (pp. 137-141): </pages> <publisher> ACM Press. </publisher>
Reference-contexts: Query interfaces to multimedia databases should thus take advantage of such inherent characteristics in order to properly allow the exploitation of the richness of information captured in the media. Recent work in image and video databases illustrate the use of query techniques which exploit inherent visual and spatial characteristics <ref> [20, 26] </ref>. For example, Ueda, et al [26] have developed techniques to find images based on the user's selection of an area in an existing image. <p> Recent work in image and video databases illustrate the use of query techniques which exploit inherent visual and spatial characteristics [20, 26]. For example, Ueda, et al <ref> [26] </ref> have developed techniques to find images based on the user's selection of an area in an existing image.
Reference: [27] <author> Weber, K. & Poon, A. </author> <year> (1994). </year> <title> Marquee: A Tool for Real-Time Video Logging. </title> <booktitle> CHI'94 Conference Proceedings, </booktitle> <pages> 58-64: </pages> <publisher> ACM Press. </publisher>
Reference-contexts: In the case of video analysis, we can then examine temporal and/or spatial relationships between objects and events. While some work in video databases and analysis 1 <ref> [19, 21, 27, 13] </ref> has used the temporal characteristics of video, little work in temporal query techniques of dynamic media has been done to take advantage of the temporal continuity and/or the combined spatio-temporal characteristics of the medium.
Reference: [28] <author> Whang, K., Malhotra, A., Sockut, G., Burns, L., & Coi, K-S. </author> <year> (1992). </year> <title> Two-Dimensional Specification of Universal Quantification in a Graphical Database Query Language, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3), </volume> <pages> 216-224. </pages>
Reference-contexts: Diagrammatic query languages allow users to specify queries by using direct manipulation of schema-type constructs to represent a set of objects and their relationships (e.g., <ref> [28, 23] </ref>). Essentially, you could build a query graphically by representing each object by a circle, attaching predicates to the circles, and using lines to denote relationships between the circles.
References-found: 28

