URL: ftp://ftp.cs.washington.edu/tr/1992/06/UW-CSE-92-06-03.PS.Z
Refering-URL: http://www.cs.washington.edu/research/arch/latency-nb-pre.html
Root-URL: 
Title: Reducing Memory Latency via Non-blocking and Prefetching Caches  
Author: Tien-Fu Chen and Jean-Loup Baer 
Note: June 1992  
Abstract: Technical Report 92-06-03 Department of Computer Science and Engineering University of Washington Seattle, WA 98195 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 176-186, </pages> <year> 1991. </year> <note> Also TR 91-03-07, </note> <institution> Department of Computer Science and Engineering, University of Washington. </institution>
Reference-contexts: A non-blocking (or lockup-free) cache [15, 20] allows execution to proceed concurrently with one (or more) cache misses until an instruction that actually needs a value to be returned is reached. Such caches exploit the overlap of memory access time with post-miss computations. Prefetching <ref> [1, 8, 13, 17, 18] </ref> hardware and/or software techniques can eliminate the cache miss penalty by generating prefetch requests to the memory system to bring the data into the cache before the data value is actually used. These techniques exploit the overlap of computations prior to an actual cache miss. <p> Prefetching can be either hardware-based <ref> [1, 12] </ref> or software-directed [8, 13, 17, 18], or a combination of both. The main advantages of the hardware-based approach are that prefetches are handled dynamically without compiler intervention and that code compatibility is preserved. However, extra hardware resources are required and unwanted data could be prefetched. <p> 6 6 B B fi fi branch target unit execution stride prev address effective d a PC Cache Instruction write buf Cache Data Reference Prediction Table Branch Prediction match ? LA-PC target addressTable ORL inc The prefetching scheme used in this paper is derived from the hardware-based approach proposed in <ref> [1] </ref>. It consists of a support unit (cf. Figure 1) for a conventional data cache whose design is based on the prediction of the execution of the instruction stream and associated operand references in load/store instructions. <p> A 256-entry reference prediction table is used to record and generate data prediction streams in the case of prefetching caches. This RPT and its associated complexity require roughly as much real estate on the chip as a 2 Kbyte data cache <ref> [1] </ref>. Branch predictions are performed by the Branch Target Buffer with two-bit state transition design [16]. Like the baseline caches, the prefetching caches will cause the processor to stall on each cache miss. We experimented with various architectural choices summarized in Table 1.
Reference: [2] <author> J.-L. Baer and W.-H. Wang. </author> <title> Multi-level cache hierarchies: Organizations, protocols and performance. </title> <journal> Journal of Parallel and Distributed computing, </journal> <volume> 6(3) </volume> <pages> 451-476, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction As the gap between processor cycle time and memory latency increases, the cache miss penalty becomes more severe and thus results in lower processor utilization. Several enhancements to cache designs have been proposed to reduce the miss penalty. Multi-level cache hierarchies <ref> [2] </ref> provide a cost-effective way to reduce the average memory access times. Hit ratios can be improved by complementing caches with small buffers or specialized caching structures [3, 12]. Another possibility is to hide the memory latency of a thread or of a process by providing fast context-switching [21].
Reference: [3] <author> B. K. Bray and M. J. Flynn. </author> <title> Writes caches as an alternative to write buffers. </title> <type> Technical Report CSL-TR-91-470, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Several enhancements to cache designs have been proposed to reduce the miss penalty. Multi-level cache hierarchies [2] provide a cost-effective way to reduce the average memory access times. Hit ratios can be improved by complementing caches with small buffers or specialized caching structures <ref> [3, 12] </ref>. Another possibility is to hide the memory latency of a thread or of a process by providing fast context-switching [21]. Still another approach is to exploit the overlap of processor computations with data accesses within one process by using write buffers, non-blocking caches, and prefetching caches. <p> Write buffers in conjunction with write-through caches are especially useful in reducing the write penalty. For write-back caches (with write-allocate), write buffers are used to temporarily store the written value until the data line is returned. Some implementations <ref> [3] </ref> allow multiple writes on the same line to be combined, thus reducing the total number of writes to the next level of the memory hierarchy. Another example of write buffers is the write-back buffer used for temporary storage of replaced dirty blocks in a write-back cache.
Reference: [4] <author> F. C. Chow and J. L. Hennessy. </author> <title> The priority-based coloring approach to register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(4) </volume> <pages> 501-536, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Although the results of our register renaming procedure are optimistic since we do not limit the number of registers, the approach is still feasible if the compiler identifies the unused registers or performs a priority-based register allocation <ref> [4] </ref> by taking into account the cost of data access penalty. 5.2 Effect of Instruction Scheduling Table 3 shows the effect of the reordering of data accesses. All the data is in weighted average" form where the weight is the execution frequency of the individual basic blocks.
Reference: [5] <author> K. Gharachorloo, A. Gupta, and H. Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Proc. ASPLOS-IV, </booktitle> <pages> pages 245-259, </pages> <year> 1991. </year>
Reference-contexts: We consider ways to improve the approaches by compiler-based optimizations (e.g., code rescheduling, software register renaming). We also propose a hybrid design which is a combination of these techniques. Our results confirm previous studies <ref> [5] </ref> indicating that buffering writes can remove most of the write miss penalty when reads are allowed to bypass writes. <p> Subsequently, lockup-free caches have been mentioned often in the literature <ref> [5] </ref> but there is some confusion on what part of the processor-cache-memory interface should support a given feature.
Reference: [6] <author> K. Gharachorloo, A. Gupta, and H. Hennessy. </author> <title> Hiding memory latency using dynamic scheduling in shared-memory multiprocessors. </title> <booktitle> In Proc. of the 19th Annual Int. Symp. on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Dynamic instruction scheduling (out of order execution) obtained at a significant increased cost in hardware complexity, can provide a larger non-blocking distance. However, the effectiveness is still subject to data dependence effects, branch prediction, and the size of the lookahead window provided by the architecture <ref> [6] </ref>. By comparison, non-blocking writes can be more advantageous in reducing the write miss penalty because the non-blocking distance is usually equal to the memory access time 2 . Moreover, the write buffer, a FIFO queue buffering pending writes, does not need a supporting unit in the processor.
Reference: [7] <author> P. B. Gibbons and S. S. Muchnick. </author> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> In Proc. of SIGPLAN Symp. on Compiler Construction, </booktitle> <month> July </month> <year> 1986. </year>
Reference-contexts: The instruction scheduling that we study here, based on the scheme given by Gibbons and Muchnick <ref> [7] </ref>, is performed after register allocation. The goal of the scheduling algorithm is to create as much distance as possible between a load and the first instruction dependent on that load.
Reference: [8] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler-directed data prefetching in multiprocessor with memory hierarchies. </title> <booktitle> In Proc. 1990 Int. Conf. on Supercomputing, </booktitle> <pages> pages 354-368, </pages> <year> 1990. </year>
Reference-contexts: A non-blocking (or lockup-free) cache [15, 20] allows execution to proceed concurrently with one (or more) cache misses until an instruction that actually needs a value to be returned is reached. Such caches exploit the overlap of memory access time with post-miss computations. Prefetching <ref> [1, 8, 13, 17, 18] </ref> hardware and/or software techniques can eliminate the cache miss penalty by generating prefetch requests to the memory system to bring the data into the cache before the data value is actually used. These techniques exploit the overlap of computations prior to an actual cache miss. <p> Prefetching can be either hardware-based [1, 12] or software-directed <ref> [8, 13, 17, 18] </ref>, or a combination of both. The main advantages of the hardware-based approach are that prefetches are handled dynamically without compiler intervention and that code compatibility is preserved. However, extra hardware resources are required and unwanted data could be prefetched.
Reference: [9] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: In the case of dynamic instruction scheduling, introducing out-of-order execution, some scoreboarding mechanism is required. Both instruction scheduling strategies need interrupt handling routines that can deal with interrupts generated by the non-blocking operations <ref> [9] </ref>. Write buffers are used to eliminate stalls on write operations. They permit the processor to continue executing even though there may be outstanding writes. Write buffers in conjunction with write-through caches are especially useful in reducing the write penalty.
Reference: [10] <author> R. Jain. </author> <title> The Art of Computer System Performance Anaylsis. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: is defined as: CP I data access = total data access penalty number of instructions executed In the discussions of the following sections, when an average reduction of CP I data access is summarized, a geometric mean is used to average the percentages of the penalty reduction for the benchmarks <ref> [10] </ref>.
Reference: [11] <author> S. Jain. </author> <title> Circular scheduling: a new technique to perform software pipelining. </title> <booktitle> In Proc. SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 219-228, </pages> <year> 1991. </year>
Reference-contexts: An intelligent compiler could take this into account when assigning edge latencies. Register renaming at compile time has been used in conjunction with software pipelining <ref> [11] </ref>. The purpose of register renaming, similar to that of dynamic instruction scheduling, is to remove write-after-read (WAR) and write-after-write (WAW) dependencies, thus allowing greater freedom in moving instructions around.
Reference: [12] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proc. of the 17th Annual Int. Symp. on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Several enhancements to cache designs have been proposed to reduce the miss penalty. Multi-level cache hierarchies [2] provide a cost-effective way to reduce the average memory access times. Hit ratios can be improved by complementing caches with small buffers or specialized caching structures <ref> [3, 12] </ref>. Another possibility is to hide the memory latency of a thread or of a process by providing fast context-switching [21]. Still another approach is to exploit the overlap of processor computations with data accesses within one process by using write buffers, non-blocking caches, and prefetching caches. <p> Prefetching can be either hardware-based <ref> [1, 12] </ref> or software-directed [8, 13, 17, 18], or a combination of both. The main advantages of the hardware-based approach are that prefetches are handled dynamically without compiler intervention and that code compatibility is preserved. However, extra hardware resources are required and unwanted data could be prefetched.
Reference: [13] <author> A. C. Klaiber and H. M. Levy. </author> <title> An architecture for software-controlled data prefetching. </title> <booktitle> In Proc. of the 18th Annual Int. Symp. on Computer Architecture, </booktitle> <pages> pages 43-53, </pages> <year> 1991. </year>
Reference-contexts: A non-blocking (or lockup-free) cache [15, 20] allows execution to proceed concurrently with one (or more) cache misses until an instruction that actually needs a value to be returned is reached. Such caches exploit the overlap of memory access time with post-miss computations. Prefetching <ref> [1, 8, 13, 17, 18] </ref> hardware and/or software techniques can eliminate the cache miss penalty by generating prefetch requests to the memory system to bring the data into the cache before the data value is actually used. These techniques exploit the overlap of computations prior to an actual cache miss. <p> Prefetching can be either hardware-based [1, 12] or software-directed <ref> [8, 13, 17, 18] </ref>, or a combination of both. The main advantages of the hardware-based approach are that prefetches are handled dynamically without compiler intervention and that code compatibility is preserved. However, extra hardware resources are required and unwanted data could be prefetched.
Reference: [14] <author> S. M. Krishnamurthy. </author> <title> A brief survey of papers on scheduling for pipelined processors. </title> <journal> SIGPLAN Notices, </journal> <volume> 25(7) </volume> <pages> 97-106, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: kinds of optimization: instruction scheduling for exploiting a possibly large non-blocking distance within a basic block and register renaming for removing false dependencies before the instruction scheduling is applied. 5.1 Instruction Scheduling Several instruction scheduling techniques based on the architecture of the target machine have been proposed in the literature <ref> [14] </ref>. The instruction scheduling that we study here, based on the scheme given by Gibbons and Muchnick [7], is performed after register allocation. The goal of the scheduling algorithm is to create as much distance as possible between a load and the first instruction dependent on that load.
Reference: [15] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proc. of the 8th Annual Int. Symp. on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <year> 1981. </year> <month> 20 </month>
Reference-contexts: The basic idea in non-blocking and prefetching caches is to hide the latency of (read and write) data misses by the overlap of data accesses and computations to the extent allowed by the data dependencies and consistency requirements. A non-blocking (or lockup-free) cache <ref> [15, 20] </ref> allows execution to proceed concurrently with one (or more) cache misses until an instruction that actually needs a value to be returned is reached. Such caches exploit the overlap of memory access time with post-miss computations. <p> We then discuss performance issues and the extra processor requirements that are needed to support the features found in these caches. 1 2.1 Non-blocking Caches Lockup-free caches were originally proposed by Kroft <ref> [15] </ref>. In his design, the following three features are included: 1. Load operations are non-blocking. 2. Write operations are non-blocking. 3. The cache is capable of servicing multiple cache miss requests.
Reference: [16] <author> J. K. F. Lee and A. J. Smith. </author> <title> Branch prediction strategies and branch target buffer design. </title> <booktitle> Com--puter, </booktitle> <pages> pages 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: This RPT and its associated complexity require roughly as much real estate on the chip as a 2 Kbyte data cache [1]. Branch predictions are performed by the Branch Target Buffer with two-bit state transition design <ref> [16] </ref>. Like the baseline caches, the prefetching caches will cause the processor to stall on each cache miss. We experimented with various architectural choices summarized in Table 1.
Reference: [17] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessor. </title> <journal> Journal of Parallel and Distributed computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: A non-blocking (or lockup-free) cache [15, 20] allows execution to proceed concurrently with one (or more) cache misses until an instruction that actually needs a value to be returned is reached. Such caches exploit the overlap of memory access time with post-miss computations. Prefetching <ref> [1, 8, 13, 17, 18] </ref> hardware and/or software techniques can eliminate the cache miss penalty by generating prefetch requests to the memory system to bring the data into the cache before the data value is actually used. These techniques exploit the overlap of computations prior to an actual cache miss. <p> Prefetching can be either hardware-based [1, 12] or software-directed <ref> [8, 13, 17, 18] </ref>, or a combination of both. The main advantages of the hardware-based approach are that prefetches are handled dynamically without compiler intervention and that code compatibility is preserved. However, extra hardware resources are required and unwanted data could be prefetched.
Reference: [18] <author> A. K. Porterfield. </author> <title> Software methods for improvement of cache performance on supercomputer application. </title> <type> Technical Report COMP TR 89-93, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: A non-blocking (or lockup-free) cache [15, 20] allows execution to proceed concurrently with one (or more) cache misses until an instruction that actually needs a value to be returned is reached. Such caches exploit the overlap of memory access time with post-miss computations. Prefetching <ref> [1, 8, 13, 17, 18] </ref> hardware and/or software techniques can eliminate the cache miss penalty by generating prefetch requests to the memory system to bring the data into the cache before the data value is actually used. These techniques exploit the overlap of computations prior to an actual cache miss. <p> Prefetching can be either hardware-based [1, 12] or software-directed <ref> [8, 13, 17, 18] </ref>, or a combination of both. The main advantages of the hardware-based approach are that prefetches are handled dynamically without compiler intervention and that code compatibility is preserved. However, extra hardware resources are required and unwanted data could be prefetched.
Reference: [19] <author> A. J. Smith. </author> <title> Cache memories. </title> <journal> ACM Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: An ORL with N entries is associated with each module. - Pipelined (N) : A request can be issued at every cycle. This model is representative of processor-cache pairs being linked to memory modules through a pipelined packet-switched interconnection network. We assume a load through mechanism <ref> [19] </ref>, i.e., the desired word is available as soon as the first data response arrives. An N -entry ORL is associated with the cache. The memory latency used is denoted by ffi, which usually is equal to 30.
Reference: [20] <author> G. S. Sohi and M. Franklin. </author> <title> High-bandwidth data memory systems for superscalar processor. </title> <booktitle> In Proc. ASPLOS-IV, </booktitle> <pages> pages 53-62, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The basic idea in non-blocking and prefetching caches is to hide the latency of (read and write) data misses by the overlap of data accesses and computations to the extent allowed by the data dependencies and consistency requirements. A non-blocking (or lockup-free) cache <ref> [15, 20] </ref> allows execution to proceed concurrently with one (or more) cache misses until an instruction that actually needs a value to be returned is reached. Such caches exploit the overlap of memory access time with post-miss computations. <p> We give now a brief qualitative view of the expected benefits for both types of overlap. Non-blocking loads delay processor stalls until the necessary data dependence is encountered. Non-blocking loads will become necessary for processors, such as superscalar processors, capable of issuing multiple instructions per cycle <ref> [20] </ref>. However, the non-blocking distance, which is the number of instructions that can be overlapped with the memory access (e.g., instructions between the reference and the first dependent instruction), is likely to be small in the case of static scheduling.
Reference: [21] <author> W.-D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results. </title> <booktitle> In Proc. 1989 Int. Conf. on Supercomputing, </booktitle> <pages> pages 273-280, </pages> <year> 1989. </year>
Reference-contexts: Hit ratios can be improved by complementing caches with small buffers or specialized caching structures [3, 12]. Another possibility is to hide the memory latency of a thread or of a process by providing fast context-switching <ref> [21] </ref>. Still another approach is to exploit the overlap of processor computations with data accesses within one process by using write buffers, non-blocking caches, and prefetching caches. The focus of this paper is on this latter approach.
References-found: 21

