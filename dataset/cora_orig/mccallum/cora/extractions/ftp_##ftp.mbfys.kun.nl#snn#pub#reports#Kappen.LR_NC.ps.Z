URL: ftp://ftp.mbfys.kun.nl/snn/pub/reports/Kappen.LR_NC.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00383.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Efficient learning in Boltzmann Machines using linear response theory  
Author: H.J. Kappen and F. B. Rodrguez 
Date: October 29, 1997  
Abstract: The learning process in Boltzmann Machines is computationally very expensive. The computational complexity of the exact algorithm is exponential in the number of neurons. We present a new approximate learning algorithm for Boltzmann Machines, which is based on mean field theory and the linear response theorem. The computational complexity of the algorithm is cubic in the number of neurons. In the absence of hidden units, we show how the weights can be directly computed from the fixed point equation of the learning rules. Thus, in this case we do not need to use a gradient descent procedure for the learning process. We show that the solutions of this method are close to the optimal solutions and give a significant improvement when correlations play a significant role. Finally, we apply the method to a pattern completion task and show good performance for networks up to 100 neurons.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D., Hinton, G., and Sejnowski, T. </author> <year> (1985). </year> <title> A learning algorithm for Boltzmann Machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169. </pages>
Reference-contexts: 1 Introduction Boltzmann Machines (BMs) <ref> (Ackley et al., 1985) </ref>, are networks of binary neurons with a stochastic neuron dynamics, known as Glauber dynamics. Assuming symmetric connections between neurons, the probability distribution over neuron states ~s will become stationary and will be given by the Boltzmann-Gibbs distribution P (~s). <p> Monte Carlo methods can be more effective than the summation of all terms because the sampling is biased towards states with lower E. These terms will give the dominant contribution to the sum over states. This is the approach chosen for learning in the original Boltzmann Machine <ref> (Ackley et al., 1985) </ref>. Practical use requires that the Markov process converges sufficiently fast, i.e. in polynomial time, to the equilibrium distribution. This property is known as rapid mixing and does probably not hold in general for Glauber dynamics (Sinclair, 1993). <p> (thermal equilibrium) and is given by the Boltzmann distribution p (~s) = Z expfE (~s)g: (4) Z = ~s expfE (~s)g is the partition function which normalizes the probability distribution. 2 2.2 Slow learning in Boltzmann Machines A learning rule for Boltzmann Machines was introduced by Ackley, Hinton and Sejnowski <ref> (Ackley et al., 1985) </ref>. Let us partition the neurons in a set of n v visible units and n h hidden units (n v + n h = n). Let ff and fi label the 2 n v visible and 2 n h hidden states of the network, respectively.
Reference: <author> Dayan, P., Hinton, G., Neal, R., and Zemel, R. </author> <year> (1995). </year> <title> The Helmholtz Machine. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 889-904. </pages>
Reference-contexts: Such an approach can be applied to architectures whose probability distribution does not contain a sum over all states for normalization, such as the Helmholz Machine <ref> (Dayan et al., 1995) </ref> and the sigmoid belief network (Saul et al., 1996). The application of such an approach to Boltzmann Machines is not as simple because it requires in addition an upper bound on Z, which is computationally more complex (Jaakkola and Jordan, 1996).
Reference: <author> Durbin, R. and Willshaw, D. </author> <year> (1987). </year> <title> An analogue approach to the travelling salesman problem using an elastic net method. </title> <journal> Nature, </journal> <volume> 326 </volume> <pages> 689-691. </pages>
Reference: <author> Fischer, K. and Hertz, J. </author> <year> (1991). </year> <title> Spin glasses. Cambridge Studies in Magnetism. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: The effect of this additional term, but in the absence of the linear response correction, was studied by (Galland, 1993). In general there is an infinite sum of terms, each involving a higher power of the couplings w ij <ref> (Fischer and Hertz, 1991) </ref>. It is interesting to note that all higher order terms in the fixed point equation are proportional to m i and thus represent corrections to the self-coupling term.
Reference: <author> Galland, C. </author> <year> (1993). </year> <title> The limitations of deterministic boltzmann machine learning. </title> <journal> Network, </journal> <volume> 4 </volume> <pages> 355-380. </pages>
Reference-contexts: It describes how the mean firing of neuron i affects the polarization of the surrounding spins and thus affect the local field of spin i. The effect of this additional term, but in the absence of the linear response correction, was studied by <ref> (Galland, 1993) </ref>. In general there is an infinite sum of terms, each involving a higher power of the couplings w ij (Fischer and Hertz, 1991).
Reference: <author> Ginzburg, I. and Sompolinsky, H. </author> <year> (1994). </year> <title> Theory of correlations in stochastic neural networks. </title> <journal> Physical Review E, </journal> <volume> 50 </volume> <pages> 3171-3191. </pages>
Reference-contexts: We will argue, that in the correct treatment of mean field theory for BMs, the correlations can be computed using the linear response theorem (Parisi, 1988). In the context of neural networks this approach was first introduced by <ref> (Ginzburg and Sompolinsky, 1994) </ref> for the computation of time-delayed correlations and later by (Kappen, 1997) for the computation of stimulus dependent correlations. We will show, that this approximation can be used succesfully to approximate the gradients in the Boltzmann Machine. This paper is organized as follows.
Reference: <author> Hertz, J., Krogh, A., and Palmer, R. </author> <year> (1991). </year> <title> Introduction to the theory of neural computation, </title> <booktitle> volume 1 of Santa Fe Institute. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Redwood City. </address>
Reference-contexts: The effect of this additional term, but in the absence of the linear response correction, was studied by (Galland, 1993). In general there is an infinite sum of terms, each involving a higher power of the couplings w ij <ref> (Fischer and Hertz, 1991) </ref>. It is interesting to note that all higher order terms in the fixed point equation are proportional to m i and thus represent corrections to the self-coupling term.
Reference: <author> Hinton, G. </author> <year> (1989). </year> <title> Deterministic Boltzmann learning performs steepest descent in weight-space. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 143-150. </pages>
Reference-contexts: The mean fields are given by the solution of a set of n coupled mean field equations, with n the number of neurons. The solution can be efficiently obtained by fixed point iteration. The method was further elaborated in <ref> (Hinton, 1989) </ref>. In this paper, we will show that the naive mean field approximation of the learning rules does not converge in general and explain why.
Reference: <author> Hopfield, J. and Tank, D. </author> <year> (1985). </year> <title> Neural computation of decision in optimization problems. </title> <journal> Biological Cybernetics, </journal> <volume> 52 </volume> <pages> 141-152. </pages>
Reference: <author> Itzykson, C. and Drouffe, J.-M. </author> <year> (1989). </year> <title> Statistical Field Theory. Cambridge monographs on mathematical physics. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK. </address>
Reference-contexts: A well-known approximate method to compute correlations is the Monte Carlo method <ref> (Itzykson and Drouffe, 1989) </ref>, which is a stochastic sampling of the state space. Glauber dynamics is an example of such a method. The terms in the sum over states are proportional to a 'Boltzmann factor' exp (E). <p> The real partition function Z, Eq. 4, can be computed in the mean field approximation <ref> (Itzykson and Drouffe, 1989) </ref>: Z = ~s X exp (E mf + E mf E) = Z mf hexp (E mf E)i mf Z mf exp (hE mf Ei mf ) = Z 0 : (17) The mean field approximation is in the last step and is related to the convexity <p> mf + E mf E) = Z mf hexp (E mf E)i mf Z mf exp (hE mf Ei mf ) = Z 0 : (17) The mean field approximation is in the last step and is related to the convexity of the exponential function hexp f i exp hfi <ref> (Itzykson and Drouffe, 1989) </ref>. Note that hi mf denotes expectation with respect to the mean field distribution Eq. 13 and not with respect to the Boltzmann distribution Eq. 4.
Reference: <author> Jaakkola, T. and Jordan, M. </author> <year> (1996). </year> <title> Recorsive algorithms for approximating probabilities in graphical models. </title> <institution> MIT Computational Cognitive Science Technical Report 9604. </institution>
Reference-contexts: The application of such an approach to Boltzmann Machines is not as simple because it requires in addition an upper bound on Z, which is computationally more complex <ref> (Jaakkola and Jordan, 1996) </ref>. We will argue, that in the correct treatment of mean field theory for BMs, the correlations can be computed using the linear response theorem (Parisi, 1988).
Reference: <author> Kappen, H. </author> <year> (1995). </year> <title> Deterministic learning rules for Boltzmann machines. </title> <booktitle> Neural Networks, </booktitle> <volume> 8 </volume> <pages> 537-548. </pages>
Reference-contexts: This is because the BM learning rule requires the computation of correlations between neurons. Thus, learning in BMs requires exponential time. For specific architectures, learning can be dramatically accelerated. For instance (Saul and Jordan, 1994) discuss how learning times become linear in the number of neurons for tree-like architectures. <ref> (Kappen, 1995) </ref> show how strong inhibition between hidden neurons reduces the computation time to polynomial in the number of neurons. A well-known approximate method to compute correlations is the Monte Carlo method (Itzykson and Drouffe, 1989), which is a stochastic sampling of the state space.
Reference: <author> Kappen, H. </author> <year> (1997). </year> <title> Stimulus dependent correlations in stochastic networks. </title> <journal> Physical Review E, </journal> <volume> 55 </volume> <pages> 5849-5858. </pages>
Reference-contexts: In the context of neural networks this approach was first introduced by (Ginzburg and Sompolinsky, 1994) for the computation of time-delayed correlations and later by <ref> (Kappen, 1997) </ref> for the computation of stimulus dependent correlations. We will show, that this approximation can be used succesfully to approximate the gradients in the Boltzmann Machine. This paper is organized as follows.
Reference: <author> Kosowsky, J. and Yuille, A. </author> <year> (1994). </year> <title> The invisible hand algorithm: solving the assignment problem with statistical physics. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 477-490. </pages>
Reference: <author> Kullback, S. </author> <year> (1959). </year> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Onsager, L. </author> <year> (1936). </year> <title> Electric moments of molecules in liquids. </title> <journal> Journal of the American Chemical Society, </journal> <volume> 58 </volume> <pages> 1486-1493. </pages>
Reference-contexts: m i + 2 i;j 1 X w 2 i )(1 m 2 and the corresponding mean field equations become the TAP equations: m i = tanh X w ij m j + i m i j6=i ij (1 m 2 The additional term is called the Onsager reaction term <ref> (Onsager, 1936) </ref>. It describes how the mean firing of neuron i affects the polarization of the surrounding spins and thus affect the local field of spin i. The effect of this additional term, but in the absence of the linear response correction, was studied by (Galland, 1993).
Reference: <author> Parisi, G. </author> <year> (1988). </year> <title> Statistical Field Theory. </title> <booktitle> Frontiers in Physics. </booktitle> <publisher> Addison-Wesley. </publisher>
Reference-contexts: We will argue, that in the correct treatment of mean field theory for BMs, the correlations can be computed using the linear response theorem <ref> (Parisi, 1988) </ref>. In the context of neural networks this approach was first introduced by (Ginzburg and Sompolinsky, 1994) for the computation of time-delayed correlations and later by (Kappen, 1997) for the computation of stimulus dependent correlations. <p> The last step in Eq. 21 follows when we use the mean field equations Eq. 19. Thus, there are no linear response corrections to the mean firing rate. Eq. 22 is known as the linear response theorem <ref> (Parisi, 1988) </ref>. The inverse of the matrix A can be directly obtained by differentiating Eq. 10 with respect to i .
Reference: <author> Peterson, C. and Anderson, J. </author> <year> (1987). </year> <title> A mean field theory learning algorithm for neural networks. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 995-1019. </pages>
Reference-contexts: This property is known as rapid mixing and does probably not hold in general for Glauber dynamics (Sinclair, 1993). Useful results can be obtained with Glauber dynamics when the network is not too large and has small weights. In <ref> (Peterson and Anderson, 1987) </ref>, an acceleration method for learning in BMs is proposed. <p> This is intractible as well, but usually less expensive than the free expectation values. As a result, the BM learning algorithm can not be applied to practical problems. 2.3 The naive mean field approximation Peterson and Anderson <ref> (Peterson and Anderson, 1987) </ref> proposed an approximation to calculate the expectation values based on mean field theory.
Reference: <author> Plefka, T. </author> <year> (1982). </year> <title> Convergence condition of the TAP equation for the infinite-range Ising spin glass model. </title> <journal> Journal of Physics A, 24:2173. </journal>
Reference-contexts: It is interesting to note that all higher order terms in the fixed point equation are proportional to m i and thus represent corrections to the self-coupling term. In the case of the SK model, it can be shown that all terms beyond the Onsager term are negligible <ref> (Plefka, 1982) </ref>. (For unfrustrated systems, like the Ising model, the Onsager term itself is negligible). One can obtain the linear response corrections for TAP and higher order mean field corrections in a similar way as was described above, i.e. by variation around the TAP equations.
Reference: <author> Saul, L., Jaakkola, T., and Jordan, M. </author> <year> (1996). </year> <title> Mean field theory for sigmoid belief networks. </title> <journal> Journal of artificial intelligence research, </journal> <volume> 4 </volume> <pages> 61-76. </pages>
Reference-contexts: Such an approach can be applied to architectures whose probability distribution does not contain a sum over all states for normalization, such as the Helmholz Machine (Dayan et al., 1995) and the sigmoid belief network <ref> (Saul et al., 1996) </ref>. The application of such an approach to Boltzmann Machines is not as simple because it requires in addition an upper bound on Z, which is computationally more complex (Jaakkola and Jordan, 1996).
Reference: <author> Saul, L. and Jordan, M. </author> <year> (1994). </year> <title> Learning in boltzmann trees. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 1174-1184. </pages>
Reference-contexts: This problem is particularly important for BM learning. This is because the BM learning rule requires the computation of correlations between neurons. Thus, learning in BMs requires exponential time. For specific architectures, learning can be dramatically accelerated. For instance <ref> (Saul and Jordan, 1994) </ref> discuss how learning times become linear in the number of neurons for tree-like architectures. (Kappen, 1995) show how strong inhibition between hidden neurons reduces the computation time to polynomial in the number of neurons.
Reference: <author> Sinclair, A. </author> <year> (1993). </year> <title> Algorithms for Random Generation & Counting. A Markov Chain Approach. </title> <booktitle> Progress in theoretical computer science. </booktitle> <publisher> Birkhauser. </publisher>
Reference-contexts: Practical use requires that the Markov process converges sufficiently fast, i.e. in polynomial time, to the equilibrium distribution. This property is known as rapid mixing and does probably not hold in general for Glauber dynamics <ref> (Sinclair, 1993) </ref>. Useful results can be obtained with Glauber dynamics when the network is not too large and has small weights. In (Peterson and Anderson, 1987), an acceleration method for learning in BMs is proposed.
Reference: <author> Thouless, D., Anderson, P., and Palmer, R. </author> <year> (1977). </year> <title> Solution of 'solvable model of a spin glass'. </title> <journal> Philisophical Magazine, </journal> <volume> 35 </volume> <pages> 593-601. </pages>
Reference-contexts: In general, terms involving higher powers of the coupling matrix w ij must be included. For example, 6 for the Sherrington-Kirkpatrick (SK) model the appropriate mean field free energy becomes <ref> (Thouless et al., 1977) </ref> X log (2 cosh ( i + W i )) X W i m i + 2 i;j 1 X w 2 i )(1 m 2 and the corresponding mean field equations become the TAP equations: m i = tanh X w ij m j + i
Reference: <author> Young, A. </author> <year> (1983). </year> <title> Direct deterrmination of the probability distriubtion for the spin-glass order parameter. </title> <journal> Physical Review Letters, </journal> <volume> 51 </volume> <pages> 1206-1209. </pages>
Reference-contexts: Clearly, the situation is different here, since one is mainly concerned with the quality of the solution 'at the end' of the annealing schedule, i.e. when T ! 0. Correlation vanish in this limit in unfrustrated systems but can be quite complex in spin glasses (see for instance <ref> (Young, 1983) </ref> for numerical results). Whether the linear response correction can improve deterministic annealing is an open question. As mentioned in the introduction, the naive mean field approach arises as a special case of the variational techniques that have been recently proposed.
Reference: <author> Yuille, A., Geiger, D., and Bulthoff, H. </author> <year> (1991). </year> <title> Stereo integration, mean field theory and psychophysics. </title> <journal> Network, </journal> <volume> 2 </volume> <pages> 423-442. </pages>

References-found: 25

