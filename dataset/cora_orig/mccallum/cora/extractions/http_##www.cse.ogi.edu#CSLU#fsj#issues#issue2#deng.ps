URL: http://www.cse.ogi.edu/CSLU/fsj/issues/issue2/deng.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/fsj/html/pastvolumes.html
Root-URL: http://www.cse.ogi.edu
Email: (deng@crg5.uwaterloo.ca)  
Title: Submitted to Free Speech Journal Speech Recognition Using Autosegmental Representation of Phonological Units with Interface
Author: Li Deng 
Address: Waterloo, Waterloo, Ontario, Canada  
Affiliation: Department of Electrical and Computer Engineering University of  
Abstract: A novel speech recognizer is described which capitalizes on multi-dimensional articulatory structures and incorporates key ideas from autosegmental phonology and articulatory phonology. The novelty has been in the design of the atomic units of speech so as to arrive at a unified and parsimonious way to account for the context-dependent behaviors of speech acoustics. At the heart of the recognizer is a procedure developed to automatically convert a probabilistic overlap pattern over five articulatory feature dimensions into a finite-state automaton which serves as the phonological construct of the recognizer. The phonetic-interface component of the recognizer, based on the nonstationary-state hidden Markov model or the trended HMM, is also described. Some phonetic recognition results using the TIMIT database are reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bakis. </author> <title> "Coarticulation modeling with continuous-state HMMs," </title> <booktitle> Proc. IEEE Workshop Automatic Speech Recognition, </booktitle> <address> Harriman, New York, </address> <year> 1991, </year> <pages> pp. 20-21. 20 </pages>
Reference-contexts: A phonological model motivated by an articulation-based feature-geometric theory is reported in [3, 23, 31], providing preliminary evidence for its value in classification of limited phonetic classes. Although no positive results have been reported yet, one interesting phonological model is the one used in the Bakis-type speech recognizer <ref> [1, 2, 4] </ref>, where abstract phoneme-specific 5 control commands, or targets, serve as the phonological construct. Yet another phonological model, in a rather different spirit than the previous ones, is based on the idea of quantizing articulatory variables (called articulatory or multi-valued phonetic features) [27, 7, 19].
Reference: [2] <author> R. Bakis. </author> <title> "An articulatory-like speech production model with controlled use of prior knowl-edge," </title> <booktitle> notes from Frontiers in Speech Processing, CD-ROM, </booktitle> <year> 1993. </year>
Reference-contexts: A phonological model motivated by an articulation-based feature-geometric theory is reported in [3, 23, 31], providing preliminary evidence for its value in classification of limited phonetic classes. Although no positive results have been reported yet, one interesting phonological model is the one used in the Bakis-type speech recognizer <ref> [1, 2, 4] </ref>, where abstract phoneme-specific 5 control commands, or targets, serve as the phonological construct. Yet another phonological model, in a rather different spirit than the previous ones, is based on the idea of quantizing articulatory variables (called articulatory or multi-valued phonetic features) [27, 7, 19].
Reference: [3] <author> N. Bitar, and C. Espy-Wilson. </author> <title> "Speech parameterization based on phonetic features: Application to speech recognition," </title> <booktitle> Proc. Eurospeech, vol.2, </booktitle> <year> 1995, </year> <pages> pp. 1411-1414. </pages>
Reference-contexts: A phonological model motivated by an articulation-based feature-geometric theory is reported in <ref> [3, 23, 31] </ref>, providing preliminary evidence for its value in classification of limited phonetic classes.
Reference: [4] <author> C. Blackburn and S. Young. </author> <title> "Towards improved speech recognition using a speech production model," </title> <booktitle> Proc. Eurospeech, </booktitle> <volume> vol. 2, </volume> <year> 1995, </year> <pages> pp. 1623-1626. </pages>
Reference-contexts: A phonological model motivated by an articulation-based feature-geometric theory is reported in [3, 23, 31], providing preliminary evidence for its value in classification of limited phonetic classes. Although no positive results have been reported yet, one interesting phonological model is the one used in the Bakis-type speech recognizer <ref> [1, 2, 4] </ref>, where abstract phoneme-specific 5 control commands, or targets, serve as the phonological construct. Yet another phonological model, in a rather different spirit than the previous ones, is based on the idea of quantizing articulatory variables (called articulatory or multi-valued phonetic features) [27, 7, 19].
Reference: [5] <author> C. Browman and L. Goldstein. </author> <title> "Articulatory phonology: An overview," </title> <journal> Phonetica, </journal> <volume> Vol.49, </volume> <pages> 155-180, </pages> <year> 1992. </year>
Reference-contexts: The overlapping nature of the articulatory features in this model is directly motivated by the phonological representations proposed in autosegmental phonology [15] and by the way gestural scores are constructed in articulatory phonology <ref> [5] </ref>. In the next section, we provide a detailed account of a most updated version of this overlapping-feature based phonological model. <p> Articulatory phonology proposes that the relatively low-level articulatory process can be used to account for many phonological rules <ref> [5] </ref>. The phonological component of the speech recognizer described here, called the overlapping articulatory feature model, is grounded on the articulatory mis-alignment model. However, the constraints on the mis-alignment are implemented carefully using the statistical means offered by the HMM-based interface component.
Reference: [6] <author> L. Deng, M. Lennig, and P. Mermelstein. </author> <title> "Modeling microsegments of stop consonants in a hidden Markov model based word recognizer," </title> <journal> J. Acoust. Soc. Am., </journal> <volume> vol. 87, </volume> <year> 1990, </year> <pages> pp. 2738-2747. </pages>
Reference-contexts: Some of simple sub-phonemic phonological models used with varying degrees of effectiveness in speech recognition include the microsegment model <ref> [6] </ref>, the locus model focusing on CV and VC transitions [12], and that which directly takes Chomsky-Halle binary distinctive features as the recognition object [14, 25].
Reference: [7] <author> L. Deng and K. Erler. </author> <title> "Structural design of an HMM speech recognizer using multi-valued phonetic features: Comparison with segmental speech units," </title> <journal> J. Acoust. Soc. Am., </journal> <volume> vol.92, </volume> <pages> pp. 3058-3067. </pages>
Reference-contexts: Yet another phonological model, in a rather different spirit than the previous ones, is based on the idea of quantizing articulatory variables (called articulatory or multi-valued phonetic features) <ref> [27, 7, 19] </ref>. Our previous research experience with this type of model showed that the model can be made effective for classification of limited phonetic classes but extending the effectiveness to broader classes of speech sounds is difficult.
Reference: [8] <author> L. Deng. </author> <title> "A generalized hidden Markov model with state-conditioned trend functions of time for the speech signal," </title> <booktitle> Signal Processing, Vol.27, No.1, </booktitle> <pages> pp. 65-78, </pages> <year> 1992. </year>
Reference-contexts: The phonetic interface model will also be described, which is based on a rigorous formulation of the nonstationary-state or trended hidden Markov model (HMM) developed earlier <ref> [8, 10] </ref>. We shall emphasize the interplay between the design of the phonological model and that of the interface model according to the underlying physical relationship between the phonological units and their phonetic correlates (e.g., static or dynamic acoustic patterning). <p> On the other hand, if the assimilated feature (s) are all in the secondary articulatory feature dimension (s), then the corresponding feature bundle defines 14 a static unit. 5.2 Model implementation The interface model has been implemented using the trended HMM theory <ref> [8] </ref>. For each of the transitional units or states, a nonstationary process consisting of a mixture of polynomial functions of time embedded in white noise is used to represent the acoustic trajectory of speech.
Reference: [9] <author> L. Deng and D. Sun. </author> <title> "A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features," </title> <journal> J. Acoust. Soc. Am., </journal> <volume> Vol. 95, </volume> <pages> pp. 2702-2719, </pages> <year> 1994. </year>
Reference-contexts: Overcoming the above weakness and again motivated by the articulatory organization of speech, the overlapping articulatory feature model reported in <ref> [9, 11] </ref> treats each articulatory feature as a symbolic entity (i.e. with no partial ordering) embodying phonological contrasts together with acoustic and possibly auditory correlates 4 . <p> in the state construction (especially useful for multilingual speech recognition including Asian languages) is currently underway. 8 without constraints on the size of the vocabulary. 4.3 Articulatory state construction At the heart of the recognizer is an algorithm for automatic conversion of any probabilistic and fractional articulatory feature overlap pattern <ref> [9] </ref> into a finite state automaton (or equivalently a Markov-state transition graph), which is described below. Let the phonemic transcription be f 1 ; f 2 ; ; f T for a speech utterance. <p> The process of mapping phonemic representation to the articulatory states (i.e. finite-state automata) involves the following four steps: (1) Map each phoneme f i to a context-independent composite articulatory state s i consisting of five-tupled feature values (See details in <ref> [9] </ref>).
Reference: [10] <author> L. Deng, M. Aksmanovic, D. Sun, and J. </author> <title> Wu "Speech recognition using hidden Markov models with polynomial regression functions as nonstationary states," </title> <journal> IEEE Trans. Speech Audio Proc., </journal> <volume> 2(4), </volume> <pages> 507-520, </pages> <year> 1994. </year>
Reference-contexts: The phonetic interface model will also be described, which is based on a rigorous formulation of the nonstationary-state or trended hidden Markov model (HMM) developed earlier <ref> [8, 10] </ref>. We shall emphasize the interplay between the design of the phonological model and that of the interface model according to the underlying physical relationship between the phonological units and their phonetic correlates (e.g., static or dynamic acoustic patterning).
Reference: [11] <author> L. Deng and H. Sameti. </author> <title> "Transitional speech units and their representation by the regressive Markov states: Applications to speech recognition," </title> <journal> IEEE Trans. Speech Audio Proc., Vol.4, </journal> <volume> No.4, </volume> <month> July </month> <year> 1996, </year> <pages> pp. 301-306. </pages>
Reference-contexts: Overcoming the above weakness and again motivated by the articulatory organization of speech, the overlapping articulatory feature model reported in <ref> [9, 11] </ref> treats each articulatory feature as a symbolic entity (i.e. with no partial ordering) embodying phonological contrasts together with acoustic and possibly auditory correlates 4 . <p> TIMIT phonetic recognition task is used, except a smaller amount of training data (40 speakers only) and a new N-best list were used. (A significantly greater computation is required for the trended HMM with the polynomial order higher than zero; see further details in <ref> [11] </ref>.) Table 3 shows the top-one re-scored phonetic recognition results, again in terms of percent correct, percent accurate, percent substitution error, percent deletion and insertion errors, as a function of the polynomial order used for transitional units in the trended HMM.
Reference: [12] <author> L. Deng and D. Braam. </author> <title> "Context-dependent Markov model structured by locus equations: Application to phonetic classification," </title> <journal> J. Acoust. Soc. Am., </journal> <volume> Vol. 96, </volume> <pages> pp. 2008-2025, </pages> <year> 1994. </year> <month> 21 </month>
Reference-contexts: how words and word sequences W can be expressed in terms of a particular organization of a small set of fundamental phonological units; P (OjF ) is the probability that 1 This can either be speech waveforms [29], or continuous-valued acoustic vectors (such as cepstral vectors [32] or formant frequencies <ref> [12] </ref>), or discrete-valued vector-quantized codes [24]. 2 A reviewer alerted us the existence of some underlying assumptions leading to Eqn.(2). <p> Some of simple sub-phonemic phonological models used with varying degrees of effectiveness in speech recognition include the microsegment model [6], the locus model focusing on CV and VC transitions <ref> [12] </ref>, and that which directly takes Chomsky-Halle binary distinctive features as the recognition object [14, 25]. A phonological model motivated by an articulation-based feature-geometric theory is reported in [3, 23, 31], providing preliminary evidence for its value in classification of limited phonetic classes.
Reference: [13] <author> L. Deng, J. Wu, and H. Sameti. </author> <title> "Improved speech modeling and recognition using multi-dimensional articulatory states as primitive speech units," </title> <booktitle> Proc. ICASSP, </booktitle> <year> 1995, </year> <pages> pp. 385-388. </pages>
Reference-contexts: However, since the former requires more complex search methods, we in this study adopt the latter easy-to-implement strategy. 10 See details of the implementation in <ref> [13] </ref>. 16 The performance of the new speech recognizer is compared with that of a system con-structed using conventional generalized triphone models. Each generalized triphone is modeled by a three-state, left-to-right HMM with no state skipping.
Reference: [14] <author> E. Eide. </author> <title> "A linguistic feature representation of the speech waveform," </title> <booktitle> Proc. ICASSP, </booktitle> <volume> vol. 2, </volume> <year> 1993, </year> <pages> pp. 483-486. </pages>
Reference-contexts: Some of simple sub-phonemic phonological models used with varying degrees of effectiveness in speech recognition include the microsegment model [6], the locus model focusing on CV and VC transitions [12], and that which directly takes Chomsky-Halle binary distinctive features as the recognition object <ref> [14, 25] </ref>. A phonological model motivated by an articulation-based feature-geometric theory is reported in [3, 23, 31], providing preliminary evidence for its value in classification of limited phonetic classes.
Reference: [15] <author> J. A. Goldsmith. </author> <title> Autosegmental and Metrical Phonology, </title> <publisher> Blackwell: Oxford, </publisher> <year> 1990. </year>
Reference-contexts: The overlapping nature of the articulatory features in this model is directly motivated by the phonological representations proposed in autosegmental phonology <ref> [15] </ref> and by the way gestural scores are constructed in articulatory phonology [5]. In the next section, we provide a detailed account of a most updated version of this overlapping-feature based phonological model. <p> That is, the different features characterizing a segment often do not take new values synchronously when moving to the next segment. Autosegmental phonology has been developed largely by recognizing this failing <ref> [15] </ref>.
Reference: [16] <author> M. Hwang, X. Huang, and F. Alleva. </author> <title> "Predicting unseen triphones with senones," </title> <booktitle> Proc. ICASSP, vol.1, </booktitle> <year> 1993, </year> <pages> pp. 311-315. </pages>
Reference-contexts: While use of context-dependent allophones [24] and of sub-phonemic units constructed systematically by training from acoustic data (e.g., <ref> [16] </ref>) has achieved promising results, it is the author's belief that more challenging, unconstrained tasks with performance approaching the human capability will require sub-phonemic models grounded solidly on modern phonological theories.
Reference: [17] <author> M. Huckvale. </author> <title> "Tiered segmentation of speech: Opportunities, methods, problems and challenges," in Speech, hearing, </title> <booktitle> and language: work in progress, </booktitle> <volume> Vol. </volume> <pages> 7, </pages> <address> University College London, </address> <pages> pp. 133-152. </pages>
Reference-contexts: We also note a related hierarchical approach to segmenting speech in <ref> [17] </ref> and a model focusing on the level of articulators in [30]. 3 A review of phonological models used in statistical speech recognition The phonological (symbolic) representations of speech that have been used in speech recognition include words, syllables, demisyllables, diphones, phonemes, context-dependent allo-phones, and a number of different forms of
Reference: [18] <author> B.H. Juang and L. R. Rabiner. </author> <title> "The segmental k-means algorithm for estimating parameters of hidden Markov models," </title> <journal> IEEE Trans. Speech Audio Proc., </journal> <volume> Vol. 38, pp.1639-1641, </volume> <year> 1990. </year>
Reference-contexts: An efficient algorithm has been developed for automatic learning of the parameters of the above interface model. The algorithm is motivated by and is an extension of the segmental K-means algorithm developed in the past for training conventional HMMs <ref> [18] </ref>.
Reference: [19] <author> P. Kenny, et al. </author> <title> "Articulatory Markov models," </title> <booktitle> Proc. IEEE Workshop Automatic Speech Recognition, </booktitle> <address> Harriman, New York, </address> <year> 1991, </year> <pages> pp. 22-23. </pages>
Reference-contexts: Yet another phonological model, in a rather different spirit than the previous ones, is based on the idea of quantizing articulatory variables (called articulatory or multi-valued phonetic features) <ref> [27, 7, 19] </ref>. Our previous research experience with this type of model showed that the model can be made effective for classification of limited phonetic classes but extending the effectiveness to broader classes of speech sounds is difficult.
Reference: [20] <author> J. Keyser and K. Stevens. </author> <title> "Feature geometry and the vocal tract," </title> <journal> Phonology, </journal> <volume> Vol. 11(2), </volume> <year> 1994, </year> <pages> pp. 207-236. </pages>
Reference-contexts: This design has been partially motivated by a modern version of the feature geometry theory that involves the critical notion of active articulator <ref> [20, 28] </ref>. This notion is connected to the view that for each consonant there is one active articulator forming the main vocal tract constriction. The active articulator is defined from a general principle | the most anterior articulator below the supranasal node that dominates terminal features. <p> The active articulator is defined from a general principle | the most anterior articulator below the supranasal node that dominates terminal features. Under the supranasal node, there are three possible active articulatory features: lips, tongue blade, and tongue dorsum. According to the theory outlined in <ref> [20] </ref>, for a speech segment with the dominant node being supranasal, the acoustic correlates of the corresponding active articulatory features that characterize the segment are spread over a region near the time when there is a rapid spectral change, exhibiting a strongly dynamic pattern.
Reference: [21] <editor> N. Lass (ed.) </editor> <title> Principles of Experimental Phonetics, </title> <editor> Ed. N. Lass, Mosby: </editor> <address> London, </address> <year> 1995. </year>
Reference-contexts: The max approximation in Eqn.(2) is a convention adopted by many speech recognition researchers (most of whom use phonetic sequences as the model for F ) for simplifying the development of training and decoding algorithms. According to the phonetic theory (rf. <ref> [21] </ref>), the interface model ideally should consist of at least three hierarchical levels of mapping: from phonological symbols to motor commands (M), from motor commands to articulation (A), and from articulation to acoustics (O).
Reference: [22] <author> K. Lee and H. Hon, </author> <title> "Speaker-independent phone recognition using hidden Markov models," </title> <journal> IEEE Transactions on Signal Processing, Vol.37, </journal> <volume> No.11, </volume> <year> 1989, </year> <month> pp.1641-1648. </month>
Reference-contexts: Some preliminary results are reported here while more comprehensive evaluations are underway. The first evaluation task is phonetic recognition of 39 standard folded phone classes <ref> [22] </ref> in continuous TIMIT sentences. To reduce computational complexity in the recognition experiment, we adopt the strategy of re-evaluating N-best 9 phonetic label hypotheses for each TIMIT sentence using the computation intensive feature-based, long-span context dependent models.
Reference: [23] <author> S. Liu. </author> <title> Landmark detection for distinctive feature-based speech recognition, </title> <type> Ph.D. thesis, </type> <institution> MIT, </institution> <year> 1995. </year>
Reference-contexts: A phonological model motivated by an articulation-based feature-geometric theory is reported in <ref> [3, 23, 31] </ref>, providing preliminary evidence for its value in classification of limited phonetic classes.
Reference: [24] <author> J. Mahkoul and R. </author> <title> Schwartz "State of the art in continuous speech recognition," </title> <journal> Proc. Natl. Acad. Sci. USA, </journal> <volume> Vol. 92, </volume> <pages> 9956-9963, </pages> <year> 1995. </year>
Reference-contexts: W can be expressed in terms of a particular organization of a small set of fundamental phonological units; P (OjF ) is the probability that 1 This can either be speech waveforms [29], or continuous-valued acoustic vectors (such as cepstral vectors [32] or formant frequencies [12]), or discrete-valued vector-quantized codes <ref> [24] </ref>. 2 A reviewer alerted us the existence of some underlying assumptions leading to Eqn.(2). <p> consisting of linear phonetic sequences as the phonological model, and of the interface model established by one-level mapping from phonological symbols to acoustics; this one-level mapping is implemented very simply by the use of state-conditioned i.i.d. (independent and identically distributed) output distributions associated with the (stationary) states of the HMM <ref> [32, 24] </ref>. One important point to note is that in the conventional technology, the HMM as a one-level mapping device has been developed largely independent of the phonological model. <p> While use of context-dependent allophones <ref> [24] </ref> and of sub-phonemic units constructed systematically by training from acoustic data (e.g., [16]) has achieved promising results, it is the author's belief that more challenging, unconstrained tasks with performance approaching the human capability will require sub-phonemic models grounded solidly on modern phonological theories.
Reference: [25] <author> H. Meng and V. Zue. </author> <title> "Signal representation comparison for phonetic classification," </title> <booktitle> Proc. ICASSP, </booktitle> <volume> vol. 1, </volume> <year> 1991, </year> <pages> pp. 285-288. </pages>
Reference-contexts: Some of simple sub-phonemic phonological models used with varying degrees of effectiveness in speech recognition include the microsegment model [6], the locus model focusing on CV and VC transitions [12], and that which directly takes Chomsky-Halle binary distinctive features as the recognition object <ref> [14, 25] </ref>. A phonological model motivated by an articulation-based feature-geometric theory is reported in [3, 23, 31], providing preliminary evidence for its value in classification of limited phonetic classes.
Reference: [26] <author> C. Pereira and M. Riley. </author> <title> "Speech recognition by composition of weighted finite automata," </title> <type> CMP-LG archive paper 9603001, </type> <year> 1996. </year> <month> 22 </month>
Reference-contexts: Hence, at least theoretically, our one-level interface model (the trended HMMs, see Section 4) is a better approximation to the realistic multi-level hierarchy (from phonology to acoustics) than the conventional stationary-state HMM. A multi-level approach to speech recognition has also been proposed in <ref> [26] </ref>, where substantially more coarse levels (acoustics to phones to words and to sentences) of the speech representation were presented compared with the multi-level representation (acoustics to articulation to motor commands to features to words and to sentences) discussed in this section.
Reference: [27] <author> M. Randolph. </author> <title> "Speech analysis based on articulatory behavior," </title> <journal> J. Acoust. Soc. Am., </journal> <volume> vol. 95, </volume> <year> 1994, </year> <pages> pp. </pages> <month> 1aSP15. </month>
Reference-contexts: Yet another phonological model, in a rather different spirit than the previous ones, is based on the idea of quantizing articulatory variables (called articulatory or multi-valued phonetic features) <ref> [27, 7, 19] </ref>. Our previous research experience with this type of model showed that the model can be made effective for classification of limited phonetic classes but extending the effectiveness to broader classes of speech sounds is difficult.
Reference: [28] <author> E. Sagey. </author> <title> "The representation of features and relations in nonlinear phonology," </title> <type> PH.D. dissertation, </type> <address> 1986, </address> <publisher> M.I.T., </publisher> <address> Cambridge, MA. </address>
Reference-contexts: This design has been partially motivated by a modern version of the feature geometry theory that involves the critical notion of active articulator <ref> [20, 28] </ref>. This notion is connected to the view that for each consonant there is one active articulator forming the main vocal tract constriction. The active articulator is defined from a general principle | the most anterior articulator below the supranasal node that dominates terminal features.
Reference: [29] <author> H. Sheikhzadeh and L. </author> <title> Deng "Waveform-based speech recognition using hidden filter models: Parameter selection and sensitivity to power normalization," </title> <journal> IEEE Trans. Speech Audio Proc., </journal> <volume> 2, </volume> <pages> 80-91, </pages> <year> 1994. </year>
Reference-contexts: a discrete-valued phonological construct and specifies, according to probability P (F jW ), how words and word sequences W can be expressed in terms of a particular organization of a small set of fundamental phonological units; P (OjF ) is the probability that 1 This can either be speech waveforms <ref> [29] </ref>, or continuous-valued acoustic vectors (such as cepstral vectors [32] or formant frequencies [12]), or discrete-valued vector-quantized codes [24]. 2 A reviewer alerted us the existence of some underlying assumptions leading to Eqn.(2).
Reference: [30] <author> A. J. H. Simons. </author> <title> "A quantitative model of the articulators," </title> <booktitle> Proc. the 8th European Conf. Artificial Intelligence, </booktitle> <year> 1988, </year> <pages> pp. 464-466. </pages>
Reference-contexts: We also note a related hierarchical approach to segmenting speech in [17] and a model focusing on the level of articulators in <ref> [30] </ref>. 3 A review of phonological models used in statistical speech recognition The phonological (symbolic) representations of speech that have been used in speech recognition include words, syllables, demisyllables, diphones, phonemes, context-dependent allo-phones, and a number of different forms of sub-phonemes.
Reference: [31] <author> K. Stevens, et al. </author> <title> "Implementation of a model for lexical access based on features," </title> <booktitle> Proc. ICSLP, </booktitle> <volume> vol. 1, </volume> <year> 1992, </year> <pages> pp. 499-502. </pages>
Reference-contexts: A phonological model motivated by an articulation-based feature-geometric theory is reported in <ref> [3, 23, 31] </ref>, providing preliminary evidence for its value in classification of limited phonetic classes.
Reference: [32] <author> S. Young. </author> <title> "Large vocabulary continuous speech recognition: a review," </title> <booktitle> Proc. IEEE Workshop on Automatic Speech Recognition, Snowbird, Utah, </booktitle> <pages> 3-28, </pages> <year> 1995. </year> <month> 23 </month>
Reference-contexts: P (F jW ), how words and word sequences W can be expressed in terms of a particular organization of a small set of fundamental phonological units; P (OjF ) is the probability that 1 This can either be speech waveforms [29], or continuous-valued acoustic vectors (such as cepstral vectors <ref> [32] </ref> or formant frequencies [12]), or discrete-valued vector-quantized codes [24]. 2 A reviewer alerted us the existence of some underlying assumptions leading to Eqn.(2). <p> consisting of linear phonetic sequences as the phonological model, and of the interface model established by one-level mapping from phonological symbols to acoustics; this one-level mapping is implemented very simply by the use of state-conditioned i.i.d. (independent and identically distributed) output distributions associated with the (stationary) states of the HMM <ref> [32, 24] </ref>. One important point to note is that in the conventional technology, the HMM as a one-level mapping device has been developed largely independent of the phonological model.
References-found: 32

