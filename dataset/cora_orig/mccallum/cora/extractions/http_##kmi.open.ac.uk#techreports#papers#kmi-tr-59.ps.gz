URL: http://kmi.open.ac.uk/techreports/papers/kmi-tr-59.ps.gz
Refering-URL: http://kmi.open.ac.uk/techreports/kmi-tr-list.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225, </pages> <year> 1994. </year>
Reference-contexts: However, the statistical roots of bbns soon prompted for the use of learning methods to extract them directly from databases of cases rather than from the insight of human domain experts <ref> [2, 1, 8] </ref>. This choice can be extremely rewarding when the domain of application generates large amounts of statistical information and aspects of the domain knowledge are still unknown or controversial, or too complex to be encoded as subjective probabilities of few experts.
Reference: [2] <author> G.F. Cooper and E. Herskovitz. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: However, the statistical roots of bbns soon prompted for the use of learning methods to extract them directly from databases of cases rather than from the insight of human domain experts <ref> [2, 1, 8] </ref>. This choice can be extremely rewarding when the domain of application generates large amounts of statistical information and aspects of the domain knowledge are still unknown or controversial, or too complex to be encoded as subjective probabilities of few experts. <p> Unfortunately, when the database is incomplete, i.e. some data are reported as unknown, simplicity and efficiency of conjugate analysis is lost, and exact Bayesian updating becomes infeasible: its complexity is exponential in the number of missing data <ref> [2] </ref>. <p> As the number of incomplete cases increases, exact updating becomes apparently infeasible: its complexity is in fact exponential in the number of missing data <ref> [2] </ref>, and approximate methods are therefore required. gs is currently the most popular method for Bayesian estimation in complex problems such us inference from incomplete samples. gs is an iterative, stochastic method in which missing data are treated as unknown parameters.
Reference: [3] <author> R.G. Cowell, A.P. Dawid, and P. Sebastiani. </author> <title> A comparison of sequential learning methods for incomplete data. </title> <booktitle> In Bayesian Statistics 5, </booktitle> <pages> pages 533-542. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1996. </year>
Reference-contexts: During the past few years, several methods have been proposed to learn conditional probabilities from incomplete databases, such as sequential updating <ref> [15, 3] </ref>, the em algorithm [4], or Gibbs Sampling (gs) [7]. gs is a stochastic, iterative method and it is currently becoming the most popular in the statistical community, although its limitations are well known: it is highly resource demanding, the convergence rate may be slow, and the execution time heavily <p> If we want to approximate the marginal posterior distribution of ijk , we can use a moment-matching approximation <ref> [3] </ref>, such as ijk jD; ijk ~ D ( ~ff ijk1 ; ~ff ijk2 ); where ~ff ijk1 ; ~ff ijk2 are such that ^p (x ik j ij ; D; ijk ) = ~ff ijk1 + ~ff ijk2 ~ff ijk1 ~ff ijk2 : From these two equations it is easy
Reference: [4] <author> A. Dempster, D. Laird, and D. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: During the past few years, several methods have been proposed to learn conditional probabilities from incomplete databases, such as sequential updating [15, 3], the em algorithm <ref> [4] </ref>, or Gibbs Sampling (gs) [7]. gs is a stochastic, iterative method and it is currently becoming the most popular in the statistical community, although its limitations are well known: it is highly resource demanding, the convergence rate may be slow, and the execution time heavily depends on the number of
Reference: [5] <author> D Geiger and D Heckerman. </author> <title> A characterization of Dirichlet distributions through local and global independence. </title> <journal> Ann. Statist., </journal> <volume> 25 </volume> <pages> 1344-1368, </pages> <year> 1997. </year>
Reference-contexts: The situation of initial ignorance can be represented by assuming ff ijk = ff=(c i q i ) for all i, j and k, so that the prior probability of (x ik j ij ) is simply 1=c i <ref> [5] </ref>. [15] shows that parameter independence and prior Dirichlet distributions imply that the posterior density of is still a product of Dirichlet densities and ij jD ~ D (ff ij1 + n (x i1 j ij ); : : : ; ff ijc i + n (x ic i j ij
Reference: [6] <author> A Gelman, J B Carlin, H S Stern, and D B Rubin. </author> <title> Bayesian Data Analysis. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1995. </year>
Reference-contexts: According to the classification of missing data mechanisms proposed by [13] and further developed by [10] and <ref> [6] </ref>, when the database consists of values of a single variable X, unreported data are said to be "Missing at Random" (mar) if the probability that an observation on X is missing does not depend on X; if this probability depends on X, then the missing data mechanism is said to
Reference: [7] <author> S. Geman and D. Geman. </author> <title> Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741, </pages> <year> 1984. </year>
Reference-contexts: During the past few years, several methods have been proposed to learn conditional probabilities from incomplete databases, such as sequential updating [15, 3], the em algorithm [4], or Gibbs Sampling (gs) <ref> [7] </ref>. gs is a stochastic, iterative method and it is currently becoming the most popular in the statistical community, although its limitations are well known: it is highly resource demanding, the convergence rate may be slow, and the execution time heavily depends on the number of missing data.
Reference: [8] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning Bayesian networks: The combinations of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: However, the statistical roots of bbns soon prompted for the use of learning methods to extract them directly from databases of cases rather than from the insight of human domain experts <ref> [2, 1, 8] </ref>. This choice can be extremely rewarding when the domain of application generates large amounts of statistical information and aspects of the domain knowledge are still unknown or controversial, or too complex to be encoded as subjective probabilities of few experts.
Reference: [9] <author> S L Lauritzen. </author> <title> Graphical Models. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1996. </year>
Reference-contexts: A popular approach, called Maximum Likelihood (ml), calculates the parameter values that maximize l (). It is well known <ref> [9] </ref> that the ml estimate of ijk is the relative frequency of relevant cases in the database: ^ ijk = n (x ik j ij )=n ( ij ).
Reference: [10] <author> R.J.A. Little and D.B. Rubin. </author> <title> Statistical Analysis with Missing Data. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: According to the classification of missing data mechanisms proposed by [13] and further developed by <ref> [10] </ref> and [6], when the database consists of values of a single variable X, unreported data are said to be "Missing at Random" (mar) if the probability that an observation on X is missing does not depend on X; if this probability depends on X, then the missing data mechanism is <p> This particular case is considered in [14]. If ff ijk = 0, then (7) is the classical Maximum Likelihood Estimate of ijk <ref> [10] </ref>. gs would complete the database by guessing the missing data from the current estimate of p (x ik j ij ), use the completed database to learn the parameters and iterate the procedure until convergence is reached.
Reference: [11] <author> M. Ramoni and P. Sebastiani. </author> <title> Robust learning with missing data. </title> <type> Technical Report KMi-TR-28, </type> <institution> Knowledge Media Institute, The Open University, </institution> <year> 1996. </year> <note> Available at http://kmi.open.ac.uk/techreports/KMi-TR-28. </note>
Reference-contexts: This paper will present a new method to estimate conditional distributions defining a bbn from an incomplete database, which, with a minimum amount of information about the missing data process, yields accurate estimates. Ramoni and Sebastiani <ref> [11] </ref> introduce a deterministic method to learn conditional probabilities from incomplete databases which does not assume any information on the missing data mechanism. <p> 1 2 x 5 1 ? ? n * (x 31 j (1; 1)) = 2 n * (x 31 j (1; 2)) = 2 n * (x 32 j (1; 1)) = 2 n * (x 32 j (1; 2)) = 1 available data using a technique given in <ref> [11] </ref>, and then collapses the resulting interval to a point via a convex combination of the extreme estimates using information on the pattern of missing data. <p> As in <ref> [11] </ref>, we define virtual frequencies: n * (x ik j ij ) = n (?j ij ) + n (x ik j?) + n (?j?) c i X n (x ih j?) + n (?j?): Thus, n * (x ik j ij ) is the maximum achievable frequency of (x ik <p> Furthermore, n (x 32 j?) = 1 from x 3 , so that n * (x 31 j (1; 1)) = 2 and n * (x 32 j (1; 1)) = n * (x 31 j (1; 1)). Ramoni and Sebastiani <ref> [11] </ref> show that the maximum Bayesian estimate of p (x ik j ij ) is: p * (x ik j ij ; D) = ff ij + n ( ij ) + n * (x ik j ij ) and the minimum Bayesian estimate is 6 Parameter Estimation in Bayesian Networks
Reference: [12] <author> M. Ramoni and P. Sebastiani. </author> <title> Learning Bayesian networks from incomplete databases. </title> <booktitle> In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> San Mateo, CA, 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: ~ff ijk1 ~ff ijk2 : From these two equations it is easy to derive ~ff ijk1 = ^ V ( ijk jD) ~ff ijk2 = ^ V ( ijk jD) An alternative approximation could be based on the use of bc to estimate the expected posterior precision, as described in <ref> [12] </ref>. 11 Parameter Estimation in Bayesian Networks from Incomplete Databases p (X 1 = 1) = 0:5 p (X 2 = 1) = 0:4 p (X 3 = 1j (X 1 = 1; X 2 = 2)) = 0:05 p (X 3 = 2j (X 1 = 1; X 2 = <p> However, it is worth recalling that, while this assumption is embedded in the methodological nature of gs, bc can easily encode different patterns of missing data, if known. 4.1 Materials We compared the implementation of bc provided by bkd <ref> [12] </ref> to the implementation of gs provided by the BUGS version 0.5 [16] on a Sun Sparc 5 under SunOS 5.5.
Reference: [13] <author> D B Rubin. </author> <title> Inference and missing data. </title> <journal> Biometrika, </journal> <volume> 63 </volume> <pages> 581-592, </pages> <year> 1976. </year>
Reference-contexts: According to the classification of missing data mechanisms proposed by <ref> [13] </ref> and further developed by [10] and [6], when the database consists of values of a single variable X, unreported data are said to be "Missing at Random" (mar) if the probability that an observation on X is missing does not depend on X; if this probability depends on X, then
Reference: [14] <author> P. Sebastiani and M. Ramoni. </author> <title> Bayesian inference from data subject to non response using bound and collapse. </title> <type> Technical Report KMi-TR-48, </type> <institution> Knowledge Media Institute, The Open University, </institution> <year> 1997. </year> <note> Available at http://kmi.open.ac.uk/techreports/KMi-TR-48. </note>
Reference-contexts: This particular case is considered in <ref> [14] </ref>.
Reference: [15] <author> D.J. Spiegelhalter and S.L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 157-224, </pages> <year> 1990. </year> <title> 22 Parameter Estimation in Bayesian Networks from Incomplete Databases </title>
Reference-contexts: During the past few years, several methods have been proposed to learn conditional probabilities from incomplete databases, such as sequential updating <ref> [15, 3] </ref>, the em algorithm [4], or Gibbs Sampling (gs) [7]. gs is a stochastic, iterative method and it is currently becoming the most popular in the statistical community, although its limitations are well known: it is highly resource demanding, the convergence rate may be slow, and the execution time heavily <p> The standard Bayesian estimate of is the posterior expectation of : E (jD). A common assumption, which leads to local computations, is that the parameter vectors ij and i 0 j 0 are independent for i 6= i 0 (global independence) and for j 6= j 0 (local independence) <ref> [15] </ref>. <p> The situation of initial ignorance can be represented by assuming ff ijk = ff=(c i q i ) for all i, j and k, so that the prior probability of (x ik j ij ) is simply 1=c i [5]. <ref> [15] </ref> shows that parameter independence and prior Dirichlet distributions imply that the posterior density of is still a product of Dirichlet densities and ij jD ~ D (ff ij1 + n (x i1 j ij ); : : : ; ff ijc i + n (x ic i j ij )) <p> : : ; ff ijc i + n (x ic i j ij ) + ffi c i (k))p (x ik ; ij jD o ) where ffi i (k) = 1 if i = k, ffi i (k) = 0 otherwise, and the frequencies are based on D o <ref> [15] </ref>. The first term in the mixture computes the possible completions of the child variable X i given the parent configuration ij . The last term is conditioned on completing the parent configuration to a state different from ij , so that the distribution of ij is not updated.
Reference: [16] <author> A. Thomas, D.J. Spiegelhalter, and W.R. Gilks. </author> <title> Bugs: A program to perform Bayesian inference using Gibbs Sampling. </title> <booktitle> In Bayesian Statistics 4, </booktitle> <pages> pages 837-42. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1992. </year> <month> 23 </month>
Reference-contexts: Suppose that the conditional distributions of fi i j (fi 1 ; ; fi i1 ; fi i+1 ; ; fi p ) are known for each i. This assumption is easily satisfied in bbns with discrete data and Dirichlet priors <ref> [16] </ref>. <p> This process is repeated several times and it is known <ref> [16] </ref> that, under broad conditions, it provides a sample from the joint posterior distribution of fi, from which a sample from the posterior distribution of can be extracted. This sample is used to compute empirical estimates of the posterior means and any other function of the parameters. <p> it is worth recalling that, while this assumption is embedded in the methodological nature of gs, bc can easily encode different patterns of missing data, if known. 4.1 Materials We compared the implementation of bc provided by bkd [12] to the implementation of gs provided by the BUGS version 0.5 <ref> [16] </ref> on a Sun Sparc 5 under SunOS 5.5. The graphical structure of the bbn used in this experimental evaluation is the same as the one used in our previous description in Section 2 and depicted in Figure 1.
References-found: 16

