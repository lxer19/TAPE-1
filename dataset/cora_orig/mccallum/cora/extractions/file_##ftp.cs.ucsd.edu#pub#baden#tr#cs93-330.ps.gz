URL: file://ftp.cs.ucsd.edu/pub/baden/tr/cs93-330.ps.gz
Refering-URL: http://www.cs.ucsd.edu/groups/hpcl/scg/tr.html
Root-URL: http://www.cs.ucsd.edu
Title: Portable Parallel Programming of Numerical Problems Under the LPAR System  
Author: Scott B. Baden Scott R. Kohn 
Address: La Jolla, California 92093-0114 USA  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Date: November 1993  
Pubnum: CSE Technical Report Number CS93-330  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Almgren, T. Buttke, and P. Colella, </author> <title> A fast vortex method in three dimensions, </title> <booktitle> in Proceedings of the 10th AIAA Computational Fluid Dynamics Conference, </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1991, </year> <pages> pp. 446-455. </pages>
Reference-contexts: The computation repeats for the desired number of time steps. The force calculation typically dominates the computation time. In general, each particle may be influenced by every other particle. A naive implementation would require O (n 2 ) particle-particle interactions to determine the force on every particle. Approximation methods <ref> [1] </ref> [13] [31] trade some accuracy for speed by computing interactions that involve only those particles lying within a specified cut-off distance r, as shown in Figure 10.
Reference: [2] <author> C. R. Anderson, </author> <title> A method of local corrections for computing the velocity field due to a distribution of vortex blobs, </title> <journal> JCP, </journal> <volume> 62 (1986), </volume> <pages> pp. 111-123. </pages>
Reference-contexts: The vortex dynamics application is typical of the particle methods described in Section 5.1. It solves the two-dimensional vorticity-stream formulation of the Euler equations. Particle velocities are calculated using a fast N -body solver due to Anderson known as the Method of Local Corrections <ref> [2] </ref>, which divides the computation into local and non-local interactions, and spends most of the time computing local interactions. The application provided LPAR with an estimate of the workload distribution, and the LPAR libraries managed the decomposition of the problem and all communication between maps.
Reference: [3] <author> I. Ashok and J. Zahorjan, Adhara: </author> <title> Runtime support for dynamic space-based applications on distributed memory mimd multiprocessors, </title> <type> Tech. Report UW-CSE-93-04-01, </type> <institution> University of Washington, </institution> <address> Seattle, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Crutchfield et al. [17], have independently developed abstractions that employ FIDIL's domain concept, which are intended for single processor architectures. Other programming models and tools for non-uniform structured problems include: block structured PARTI [14] [30], Adhara <ref> [3] </ref>, DINO [27], P++ [24], and Fortran-90 dialects such as Fortran-90 D [33], CM-Fortran [32], and HPF [20]. DINO hides many of the details of message passing, but requires that the programmer be aware of message passing activity.
Reference: [4] <author> S. B. Baden, </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12 (1991), </volume> <pages> pp. 145-157. </pages>
Reference-contexts: Moreover, this model is robust with respect to low-level performance tuning. For example, the logical structure of LPAR application software is independent of the processor to data mapping or the presence of tightly-coupled processing clusters. LPAR is based in part on the GenMP programming model <ref> [4] </ref> which may be used to develop portable implementations of particle methods on MIMD multiprocessors [6] and in part on the FIDIL programming language [21]. Crutchfield et al. [17], have independently developed abstractions that employ FIDIL's domain concept, which are intended for single processor architectures.
Reference: [5] <author> S. B. Baden and S. Kohn, </author> <title> Lattice parallelism: A parallel programming model for nonuniform, structured scientific computations, </title> <type> Tech. Report CS92-261, </type> <institution> University of California - San Diego, CSE 0114, </institution> <address> 9500 Gilman Drive, La Jolla, CA 92092-0114, </address> <month> August </month> <year> 1992. </year> <note> Also available via anonymous ftp from cs.ucsd.edu in directory pub/baden/lpar. 25 [6] , The reference guide to genmp the generic multiprocessor, Tech. Report CS92-243, </note> <institution> University of California, San Diego, Dept. of Computer Science and Engineering, CSE 0114, </institution> <address> 9500 Gilman Drive, La Jolla, CA 92092-0114, </address> <month> June </month> <year> 1992. </year> <note> Also available via anonymous ftp from cs.ucsd.edu in directory pub/baden/genmp. </note>
Reference-contexts: While some Fortran-90 dialects support irregular array decompositions, these are fine-grained and unstructured and thus incur substantial run-time overhead. The LPAR abstractions, described in the following section, extend the Fortran-90 uniform array model to include dynamic, non-uniform arrays. 3 (b) an irregular block decomposition. 3 An LPAR Tutorial LPAR <ref> [5] </ref> [23] is a coarse-grain programming model for managing complex, dynamic calculations on distributed memory MIMD architectures. It provides high-level abstractions which exploit the locality inherent to many scientific computations.
Reference: [7] <author> S. B. Baden and E. G. Puckett, </author> <title> A fast vortex code for computing 2d viscous flow, </title> <journal> JCP, </journal> <volume> 91 (1990), </volume> <pages> pp. 278-297. </pages>
Reference-contexts: We present a domain specific programming model, called Lattice Parallelism (LPAR), which is intended for a significant class of non-uniform numerical algorithms that possess local structure. These "non-uniform structured algorithms," or NUSA's, play a significant role in modeling diverse complex phenomena such as turbulence (e.g. wakes) <ref> [15, 7] </ref>, combustion [16, 29], and shock hydrodynamics [9]. They are also expected to play an important role in semiconductor device simulation, in the study of flows in porous media (e.g. petroleum reservoirs and toxic waste cleanup), and in theoretical materials science.
Reference: [8] <author> M. J. Berger and S. H. Bokhari, </author> <title> A partitioning strategy for nonuniform problems on multiprocessors, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36 (1987), </volume> <pages> pp. 570-580. </pages>
Reference-contexts: Collections are intended to be coarse-grained decompositions and typically only a few map components are assigned to each processor. Figure 5 shows two common decompositions and mappings. Figure 5a illustrates the decomposition of a rectangular region into four non-uniform maps using recursive bisection <ref> [8] </ref>. The collection of maps in Figure 5b is typical of a single level refinement structure for adaptive mesh methods. LPAR expresses parallel computation through distributed loops over the maps in a collection. The loops compute on each map as if it were assigned its own processor.
Reference: [9] <author> M. J. Berger and P. Colella, </author> <title> Local adaptive mesh refinement for shock hydrodynamics, </title> <journal> JCP, </journal> <volume> 82 (1989), </volume> <pages> pp. 64-84. </pages>
Reference-contexts: These "non-uniform structured algorithms," or NUSA's, play a significant role in modeling diverse complex phenomena such as turbulence (e.g. wakes) [15, 7], combustion [16, 29], and shock hydrodynamics <ref> [9] </ref>. They are also expected to play an important role in semiconductor device simulation, in the study of flows in porous media (e.g. petroleum reservoirs and toxic waste cleanup), and in theoretical materials science. NUSA's play a particularly crucial role in treating three-dimensional problems. <p> NUSA's play a particularly crucial role in treating three-dimensional problems. They work by locally concentrating computational effort non-uniformly over "interesting" regions of the solution, placing the effort only where it is "needed." Typical examples of NUSA's include multilevel [25], adaptive grid <ref> [9] </ref>, particle [22, 13] and multifluid methods [26]. 1 This work was supported in part by NSF contract ASC-9110793 and in part by ONR contract N00014-93-1-0152.
Reference: [10] <author> M. J. Berger and J. Oliger, </author> <title> Adaptive mesh refinement for hyperbolic partial differential equations, </title> <journal> Journal of Computational Physics, </journal> <volume> 53 (1984), </volume> <pages> pp. 484-512. </pages>
Reference-contexts: DINO hides many of the details of message passing, but requires that the programmer be aware of message passing activity. P++ is a C ++ run time library which supports a virtual shared grid abstraction for structured grid methods such as adaptive mesh refinement <ref> [10] </ref>. Adhara provides abstractions for handling single level uniform mesh and particle methods.
Reference: [11] <author> A. Brandt, </author> <title> Multigrid techniques: 1984 guide, </title> <type> Tech. Report GMD Studien 85, </type> <institution> Gesselschaft fuer Mathematik un Datenverarbeitung, </institution> <address> St. Augustin, Germany, </address> <year> 1984. </year>
Reference-contexts: This code is similar to the loop (6) of rebalance (): for every pair of regions I and J, the ghost cell regions of bins [I] are filled with data from the non-ghost cell regions of bins [J]. 5.2 Multigrid Multigrid <ref> [11] </ref> is a fast method for solving partial differential equations and sparse systems of linear equations. It represents a problem over a hierarchy of successively coarsened meshes to accelerate the communication of information across the computational box and hence reduce the time to solution.
Reference: [12] <author> W. L. Briggs, </author> <title> Multigrid Tutorial, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1987. </year>
Reference-contexts: Since multigrid is simpler to describe, we use it to illustrate the management of multilevel mesh structures in LPAR. We will only discuss the communication of information between meshes; details about the numerical calculations can be found in Briggs's informative tutorial <ref> [12] </ref>.
Reference: [13] <author> J. Carrier, L. Greengard, and V. Rokhlin, </author> <title> A fast adaptive multipole algorithm for particle simulations, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <year> (1988). </year>
Reference-contexts: NUSA's play a particularly crucial role in treating three-dimensional problems. They work by locally concentrating computational effort non-uniformly over "interesting" regions of the solution, placing the effort only where it is "needed." Typical examples of NUSA's include multilevel [25], adaptive grid [9], particle <ref> [22, 13] </ref> and multifluid methods [26]. 1 This work was supported in part by NSF contract ASC-9110793 and in part by ONR contract N00014-93-1-0152. <p> The force calculation typically dominates the computation time. In general, each particle may be influenced by every other particle. A naive implementation would require O (n 2 ) particle-particle interactions to determine the force on every particle. Approximation methods [1] <ref> [13] </ref> [31] trade some accuracy for speed by computing interactions that involve only those particles lying within a specified cut-off distance r, as shown in Figure 10.
Reference: [14] <author> C. Chase, K. Crowley, J. Saltz, and A. Reeves, </author> <title> Parallelization of irregularly coupled regular meshes, </title> <type> Tech. Report 92-1, </type> <institution> ICASE, Hampton, VA, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Crutchfield et al. [17], have independently developed abstractions that employ FIDIL's domain concept, which are intended for single processor architectures. Other programming models and tools for non-uniform structured problems include: block structured PARTI <ref> [14] </ref> [30], Adhara [3], DINO [27], P++ [24], and Fortran-90 dialects such as Fortran-90 D [33], CM-Fortran [32], and HPF [20]. DINO hides many of the details of message passing, but requires that the programmer be aware of message passing activity.
Reference: [15] <author> A. J. Chorin, </author> <title> Numerical modeling of turbulent flow in a combustion tunnel, </title> <journal> Journal of Fluid Mech., </journal> <volume> 57 (1973), </volume> <pages> pp. 785-796. </pages>
Reference-contexts: We present a domain specific programming model, called Lattice Parallelism (LPAR), which is intended for a significant class of non-uniform numerical algorithms that possess local structure. These "non-uniform structured algorithms," or NUSA's, play a significant role in modeling diverse complex phenomena such as turbulence (e.g. wakes) <ref> [15, 7] </ref>, combustion [16, 29], and shock hydrodynamics [9]. They are also expected to play an important role in semiconductor device simulation, in the study of flows in porous media (e.g. petroleum reservoirs and toxic waste cleanup), and in theoretical materials science.
Reference: [16] <author> A. J. Chorin and A. K. Oppenheim, </author> <title> Numerical modeling of turbulent flow in a combustion tunnel, </title> <journal> J. Phil. Trans. R. Soc. Lond. A, </journal> <volume> 304 (1982), </volume> <pages> pp. 303-325. </pages>
Reference-contexts: We present a domain specific programming model, called Lattice Parallelism (LPAR), which is intended for a significant class of non-uniform numerical algorithms that possess local structure. These "non-uniform structured algorithms," or NUSA's, play a significant role in modeling diverse complex phenomena such as turbulence (e.g. wakes) [15, 7], combustion <ref> [16, 29] </ref>, and shock hydrodynamics [9]. They are also expected to play an important role in semiconductor device simulation, in the study of flows in porous media (e.g. petroleum reservoirs and toxic waste cleanup), and in theoretical materials science. NUSA's play a particularly crucial role in treating three-dimensional problems.
Reference: [17] <author> W. Y. Crutchfield and M. L. </author> <title> Welcome, Object oriented implementation of adaptive mesh refinement algorithms, </title> <type> Tech. Report UCRL-JC-113502, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: LPAR is based in part on the GenMP programming model [4] which may be used to develop portable implementations of particle methods on MIMD multiprocessors [6] and in part on the FIDIL programming language [21]. Crutchfield et al. <ref> [17] </ref>, have independently developed abstractions that employ FIDIL's domain concept, which are intended for single processor architectures.
Reference: [18] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu, </author> <title> Fortran d language specification, </title> <type> Tech. Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, Houston, TX, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: LPAR is based in part on the scientific programming language FIDIL [21], from which it borrows the concept of admitting structural information as a first class object. It generalizes the Fortran-90 family of programming languages (which we will refer to collectively as "Fortran-90"), for example CM Fortran [32], Fortran-D <ref> [18] </ref>, and HPF [20], to coarse grained data parallelism involving dynamic irregular data structures. LPAR provides a self-contained dynamic memory management facility for distributed memory architectures, and may be embedded within existing programming languages.
Reference: [19] <author> G. H. Golub and C. F. V. Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: A second LPAR application solves a three dimensional Poisson's equation using preconditioned conjugate gradient (PCG) <ref> [19] </ref>, a single-level method for solving sparse linear systems. The communication structure of PCG is identical to the intralevel communication structure of multigrid. iPSC and nCUBE. Memory limitations on the iPSC prevented runs with fewer than sixteen nodes (We plan to repeat the iPSC/860 runs on the Paragon.
Reference: [20] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification, </title> <institution> Rice University, Houston, Texas, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: It generalizes the Fortran-90 family of programming languages (which we will refer to collectively as "Fortran-90"), for example CM Fortran [32], Fortran-D [18], and HPF <ref> [20] </ref>, to coarse grained data parallelism involving dynamic irregular data structures. LPAR provides a self-contained dynamic memory management facility for distributed memory architectures, and may be embedded within existing programming languages. A prototype, implemented as a C++ class library, is running on the iPSC/860, the nCUBE/2, and on workstations. <p> This is consistent with each loop iteration pursuing its own execution thread. Indeed we can think of foreach loops as logically executing in parallel (although there is no physical parallelism) under the restriction of the owner stores rule (The semantics of foreach are similar to HPF's INDEPENDENT forall loop <ref> [20] </ref>). 4.5 Distributed Foreach LPAR provides a distributed foreach loop to complement the Distributed Map facility: distributed foreach I in &lt;Domain&gt; do &lt;body&gt; end I This is a parallel version of the (non-distributed) foreach loop. <p> Other programming models and tools for non-uniform structured problems include: block structured PARTI [14] [30], Adhara [3], DINO [27], P++ [24], and Fortran-90 dialects such as Fortran-90 D [33], CM-Fortran [32], and HPF <ref> [20] </ref>. DINO hides many of the details of message passing, but requires that the programmer be aware of message passing activity. P++ is a C ++ run time library which supports a virtual shared grid abstraction for structured grid methods such as adaptive mesh refinement [10].
Reference: [21] <author> P. N. Hilfinger and P. Colella, Fidil: </author> <title> A language for scientific programming, </title> <type> Tech. Report UCRL-98057, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: These abstractions support an economical programming style that hides inappropriate incidental details from application software, all for only a modest overhead; the programmer can manage program locality explicitly to limit overheads such as communication. LPAR is based in part on the scientific programming language FIDIL <ref> [21] </ref>, from which it borrows the concept of admitting structural information as a first class object. <p> However, LPAR does provide powerful tools for hiding most of the details, and applications written using these tools are portable across a wide range of MIMD architectures with little loss in performance. LPAR is based in part on the scientific programming language FIDIL <ref> [21] </ref> and is designed to be embedded within a standard programming language such as C ++ or Fortran-90. LPAR borrows from FIDIL two abstract data types, the map and the domain. Maps are dynamically defined arrays with arbitrary bounds. <p> The physical significance of a processor is implementation dependent, and the distinction is largely hidden to the programmer. 4.2 Domains, Maps, and the Domain Calculus LPAR's principal contribution are two abstract types|Domain and Map. These types are based on those defined in the FIDIL programming language <ref> [21] </ref>, with extensions for treating MIMD parallelism. A domain represents an index set. It is a first class object which may be assigned and manipulated using intrinsic operations defined on domains. A map is a generalized array whose index set is defined over a domain-valued expression. <p> LPAR is based in part on the GenMP programming model [4] which may be used to develop portable implementations of particle methods on MIMD multiprocessors [6] and in part on the FIDIL programming language <ref> [21] </ref>. Crutchfield et al. [17], have independently developed abstractions that employ FIDIL's domain concept, which are intended for single processor architectures.
Reference: [22] <author> R. W. Hockney and J. W. Eastwood, </author> <title> Computer Simulation Using Particles, </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year> <month> 26 </month>
Reference-contexts: NUSA's play a particularly crucial role in treating three-dimensional problems. They work by locally concentrating computational effort non-uniformly over "interesting" regions of the solution, placing the effort only where it is "needed." Typical examples of NUSA's include multilevel [25], adaptive grid [9], particle <ref> [22, 13] </ref> and multifluid methods [26]. 1 This work was supported in part by NSF contract ASC-9110793 and in part by ONR contract N00014-93-1-0152. <p> Map element types are not limited to floating point numbers or integers but may be any user-defined type such as a linked list. For example, in addition to representing a finite difference grid, a map may also represent the spatial data structure <ref> [22] </ref> common in particle codes (see Section 5.1). Maps and domains are defined within an underlying global coordinate system called the lattice. Even though maps may logically overlap within this coordinate system, they do not physically share map elements. <p> Approximation methods [1] [13] [31] trade some accuracy for speed by computing interactions that involve only those particles lying within a specified cut-off distance r, as shown in Figure 10. A common method for maintaining neighbor relationships for interacting particles is to sort the particles into a binning mesh <ref> [22] </ref>, which avoids the costly O (n 2 ) search for neighbors. Each bin of this mesh contains a list of the particles assigned to that region of space.
Reference: [23] <author> S. R. Kohn and S. B. Baden, </author> <title> An implementation of the lpar parallel programming model for scientific computations, </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: The LPAR abstractions, described in the following section, extend the Fortran-90 uniform array model to include dynamic, non-uniform arrays. 3 (b) an irregular block decomposition. 3 An LPAR Tutorial LPAR [5] <ref> [23] </ref> is a coarse-grain programming model for managing complex, dynamic calculations on distributed memory MIMD architectures. It provides high-level abstractions which exploit the locality inherent to many scientific computations.
Reference: [24] <author> M. Lemke and D. Quinlan, </author> <title> P++: a c++ virtual shared grids based programming environment for architecture-independent development of structured grid applications, </title> <type> tech. report, </type> <institution> Computational Mathematics Group, University of Colorado at Boulder, </institution> <address> Denver, Colorado, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: Crutchfield et al. [17], have independently developed abstractions that employ FIDIL's domain concept, which are intended for single processor architectures. Other programming models and tools for non-uniform structured problems include: block structured PARTI [14] [30], Adhara [3], DINO [27], P++ <ref> [24] </ref>, and Fortran-90 dialects such as Fortran-90 D [33], CM-Fortran [32], and HPF [20]. DINO hides many of the details of message passing, but requires that the programmer be aware of message passing activity.
Reference: [25] <author> S. McCormick, </author> <title> Multilevel Adaptive Methods for Partial Differential Equations, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1989. </year>
Reference-contexts: NUSA's play a particularly crucial role in treating three-dimensional problems. They work by locally concentrating computational effort non-uniformly over "interesting" regions of the solution, placing the effort only where it is "needed." Typical examples of NUSA's include multilevel <ref> [25] </ref>, adaptive grid [9], particle [22, 13] and multifluid methods [26]. 1 This work was supported in part by NSF contract ASC-9110793 and in part by ONR contract N00014-93-1-0152.
Reference: [26] <author> J. E. Pilliod and E. G. Puckett, </author> <title> Second-order volume-of-fluid interface tracking algorithms, </title> <address> JCP, </address> <year> (1993). </year>
Reference-contexts: NUSA's play a particularly crucial role in treating three-dimensional problems. They work by locally concentrating computational effort non-uniformly over "interesting" regions of the solution, placing the effort only where it is "needed." Typical examples of NUSA's include multilevel [25], adaptive grid [9], particle [22, 13] and multifluid methods <ref> [26] </ref>. 1 This work was supported in part by NSF contract ASC-9110793 and in part by ONR contract N00014-93-1-0152.
Reference: [27] <author> M. Rosing, R. B. Schnabel, and R. P. Weaver, </author> <title> Expressing complex parallel algorithms in dino, </title> <type> Tech. Report CU-CS-430-88, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <address> Denver, Colorado, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: Crutchfield et al. [17], have independently developed abstractions that employ FIDIL's domain concept, which are intended for single processor architectures. Other programming models and tools for non-uniform structured problems include: block structured PARTI [14] [30], Adhara [3], DINO <ref> [27] </ref>, P++ [24], and Fortran-90 dialects such as Fortran-90 D [33], CM-Fortran [32], and HPF [20]. DINO hides many of the details of message passing, but requires that the programmer be aware of message passing activity.
Reference: [28] <author> L. Semenzato and P. Hilfinger, </author> <booktitle> Arrays in fidil, in Proceedings of the First International Workshop on Arrays, Functional Programming, and Parallel Systems, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: Furthermore, most interactions between maps are empty. LPAR is able to reduce communication overheads and improve performance by communicating only necessary data values and eliminating messages for spurious dependencies. LPAR avoids unnecessary communication through run-time optimizations known as live transmission analysis <ref> [28] </ref>. Live transmission analysis determines if communication between processors is necessary and, if so, calculates the minimum set of data needed to satisfy the data dependencies. Analysis must be performed at run-time since map structures are not usually known at compile time.
Reference: [29] <author> J. Sethian, </author> <title> Turbulent combustion in open and closed vessels, </title> <journal> JCP, </journal> <volume> 54 (1984), </volume> <pages> pp. 425-456. </pages>
Reference-contexts: We present a domain specific programming model, called Lattice Parallelism (LPAR), which is intended for a significant class of non-uniform numerical algorithms that possess local structure. These "non-uniform structured algorithms," or NUSA's, play a significant role in modeling diverse complex phenomena such as turbulence (e.g. wakes) [15, 7], combustion <ref> [16, 29] </ref>, and shock hydrodynamics [9]. They are also expected to play an important role in semiconductor device simulation, in the study of flows in porous media (e.g. petroleum reservoirs and toxic waste cleanup), and in theoretical materials science. NUSA's play a particularly crucial role in treating three-dimensional problems.
Reference: [30] <author> A. Sussman and J. Saltz, </author> <title> A manual for the multiblock parti runtime primitives revision 4, </title> <type> Tech. Report CS-TR-3070, </type> <institution> University of Maryland | College Park, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Crutchfield et al. [17], have independently developed abstractions that employ FIDIL's domain concept, which are intended for single processor architectures. Other programming models and tools for non-uniform structured problems include: block structured PARTI [14] <ref> [30] </ref>, Adhara [3], DINO [27], P++ [24], and Fortran-90 dialects such as Fortran-90 D [33], CM-Fortran [32], and HPF [20]. DINO hides many of the details of message passing, but requires that the programmer be aware of message passing activity.
Reference: [31] <author> P. Tamayo, J. P. Mesirov, and B. M. Boghosian, </author> <title> Parallel approaches to short range molecular dynamics simulations, </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: The force calculation typically dominates the computation time. In general, each particle may be influenced by every other particle. A naive implementation would require O (n 2 ) particle-particle interactions to determine the force on every particle. Approximation methods [1] [13] <ref> [31] </ref> trade some accuracy for speed by computing interactions that involve only those particles lying within a specified cut-off distance r, as shown in Figure 10.
Reference: [32] <author> Thinking Machines, Inc., </author> <title> CM Fortran User's Guide, </title> <address> Cambridge, Massachusetts, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: LPAR is based in part on the scientific programming language FIDIL [21], from which it borrows the concept of admitting structural information as a first class object. It generalizes the Fortran-90 family of programming languages (which we will refer to collectively as "Fortran-90"), for example CM Fortran <ref> [32] </ref>, Fortran-D [18], and HPF [20], to coarse grained data parallelism involving dynamic irregular data structures. LPAR provides a self-contained dynamic memory management facility for distributed memory architectures, and may be embedded within existing programming languages. <p> Other programming models and tools for non-uniform structured problems include: block structured PARTI [14] [30], Adhara [3], DINO [27], P++ [24], and Fortran-90 dialects such as Fortran-90 D [33], CM-Fortran <ref> [32] </ref>, and HPF [20]. DINO hides many of the details of message passing, but requires that the programmer be aware of message passing activity. P++ is a C ++ run time library which supports a virtual shared grid abstraction for structured grid methods such as adaptive mesh refinement [10].

References-found: 31

