URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/94-16.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Title: Nonmonotone Curvilinear Line Search Methods for Unconstrained Optimization  
Author: M.C. Ferris S. Lucidi M. Roma 
Date: March 20, 1995  
Abstract: We present a new algorithmic framework for solving unconstrained minimization problems that incorporates a curvilinear linesearch. The search direction used in our framework is a combination of an approximate Newton direction and a direction of negative curvature. Global convergence to a stationary point where the Hessian matrix is positive semidefinite is exhibited for this class of algorithms by means of a nonmonotone stabilization strategy. An implementation using the Bunch-Parlett decomposition is shown to outperform several other techniques on a large class of test problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Arioli, T. F. Chan, I. S. Duff, N. I. M. Gould, and J. K. Reid. </author> <title> Computing a search direction for large-scale linearly-constrained nonlinear optimization calculations. </title> <type> Technical Report TR/PA/93/34, </type> <institution> CERFACS, </institution> <year> 1993. </year>
Reference-contexts: We believe that some promising approaches to compute both a Newton-type direction and a negative curvature direction are * the use of the conjugate gradient method <ref> [1, 5, 24] </ref>, * the use of Lanczos decomposition [18, 20], * and several new modifications of Cholesky factorization [9, 10, 11, 21, 22]. Acknowledgement We thank the referees for their constructive comments and remarks that helped improve the paper. 13
Reference: [2] <author> I. Bongartz, A. R. Conn, N. Gould, and Ph. L. Toint. CUTE: </author> <title> Constrained and unconstrained testing environment. </title> <institution> Publications du Department de Mathematique Report 93/10, Facultes Universitaires De Namur, </institution> <year> 1993. </year> <note> To appear in ACM Trans. Math. Soft. </note>
Reference-contexts: This is because these test problems present just a few points where the objective function has negative curvature, as was noted in [16, 17]. The advent of the CUTE collection of test problems <ref> [2] </ref> allows us to obtain a good evaluation of globally convergent algorithms, since it includes a variety of functions with negative curvature. The main aim of our work is to encourage the development of new research in this area. <p> Step 2: If f (x k + ff 2 s k + ffd k ) F + flff 2 h 2 d T i set ff k = ff and stop. Step 3: Choose 2 <ref> [ 1 ; 2 ] </ref>, ff = ff and go to Step 2. This approach has been tested in [13] and proven successful on many ill-conditioned problems. However, it does enforce a monotonic decrease in the sequence fF j g. <p> search procedure, and hence there exists an index ^ k such that, for all k ^ k, k 2 K 1 : f x k + ff k 2 ff k d k &gt; f (x k ) + fl ff k 2 1 d T for some k 2 <ref> [ 1 ; 2 ] </ref>. <p> minimum eigenvalue of the Hessian matrix. 10 5 Computational Results In order to evaluate the behavior of our new algorithm, we have used the Bunch-Parlett decomposition as discussed in the previous section and we have tested the resulting algorithm on all the small unconstrained problems available from the CUTE collection <ref> [2] </ref>. This test set covers the classical test problems along with a large number of nonlinear optimization problems of various difficulty representing both "academic" and "real world" applications. We have compared the results with those obtained by the algorithm proposed in [14].
Reference: [3] <author> J. R. Bunch and B. N. Parlett. </author> <title> Direct methods for solving symmetric indefinite systems of linear equations. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 8 </volume> <pages> 639-655, </pages> <year> 1971. </year>
Reference-contexts: In practice, we think of s k as an approximation of the Newton step and d k as a direction of negative curvature. Both of these can be calculated using the Bunch-Parlett factorization <ref> [3] </ref>; this was first considered in [17]. In order that our convergence theorems remain applicable to more general algorithms than the ones we test numerically, we list below the general conditions that we require of the search directions. <p> The Bunch-Parlett decomposition is very easy to implement and it gives information about the distribution of the eigenvalues of the Hessian matrix (for details, see <ref> [3, 17] </ref>). We recall that (at iteration k) the Bunch-Parlett decomposition gives H = W DW T ; where W is a n fi n non singular matrix and D is a symmetric n fi n block diagonal matrix with one by one and two by two diagonal blocks.
Reference: [4] <author> A. R. Conn, N. I. M. Gould, and Ph. L. Toint. LANCELOT: </author> <title> A Fortran package for Large Scale Nonlinear Optimization (Release A). </title> <booktitle> Number 17 in Springer Series in Computational Mathematics. </booktitle> <publisher> Springer Verlag, </publisher> <address> Heidelberg, Berlin, </address> <year> 1992. </year>
Reference-contexts: Since the new algorithm uses essentially the same stabilization technique, the comparison between these two determines the effect of exploiting information on the curvature of the objective function via the curvilinear linesearch. The second algorithm we compare against is LANCELOT <ref> [4] </ref>, which is a trust region based code. The numerical results which we report show that a significant improvement in terms of the number of iterations and function-gradient evaluations can be obtained by using an algorithm that exploits curvature effects. <p> The failures are caused by excessive number of iterations or functions evaluations. On the basis of these results, the new method generates a considerable computational savings, along with an increase in robustness. 11 We have also compared our algorithm to LANCELOT <ref> [4] </ref>, a trust region based code for large-scale nonlinear optimization, written in Fortran. We believe that LANCELOT is close to the state of the art for nonlinear optimization codes.
Reference: [5] <author> R. S. Dembo and T. Steihaug. </author> <title> Truncated Newton method algorithms for large-scale uncon strained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 26 </volume> <pages> 190-212, </pages> <year> 1983. </year>
Reference-contexts: We believe that some promising approaches to compute both a Newton-type direction and a negative curvature direction are * the use of the conjugate gradient method <ref> [1, 5, 24] </ref>, * the use of Lanczos decomposition [18, 20], * and several new modifications of Cholesky factorization [9, 10, 11, 21, 22]. Acknowledgement We thank the referees for their constructive comments and remarks that helped improve the paper. 13
Reference: [6] <author> N.Y. Deng, Y. Xiao, and F.J. Zhou. </author> <title> Nonmonotonic trust region algorithm. </title> <journal> Journal of Opti mization Theory and Applications, </journal> <volume> 76 </volume> <pages> 259-285, </pages> <year> 1993. </year>
Reference-contexts: Computational results [14] have shown this technique to be even better than the standard nonmonotone line search. This technique has also proven useful in other applications and extensions <ref> [6, 7, 8, 25, 26, 27] </ref>. Of course, sometimes it may lead to regions where the function is poorly behaved. In these cases, a backtracking scheme is incorporated into our algorithm.
Reference: [7] <author> S. P. Dirkse and M. C. Ferris. </author> <title> The PATH solver: A non-monotone stabilization scheme for mixed complementarity problems. Optimization Methods & Software, 1995, </title> <publisher> forthcoming. </publisher>
Reference-contexts: Computational results [14] have shown this technique to be even better than the standard nonmonotone line search. This technique has also proven useful in other applications and extensions <ref> [6, 7, 8, 25, 26, 27] </ref>. Of course, sometimes it may lead to regions where the function is poorly behaved. In these cases, a backtracking scheme is incorporated into our algorithm.
Reference: [8] <author> M. C. Ferris and S. Lucidi. </author> <title> Nonmonotone stabilization methods for nonlinear equations. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 81 </volume> <pages> 53-74, </pages> <year> 1994. </year>
Reference-contexts: Computational results [14] have shown this technique to be even better than the standard nonmonotone line search. This technique has also proven useful in other applications and extensions <ref> [6, 7, 8, 25, 26, 27] </ref>. Of course, sometimes it may lead to regions where the function is poorly behaved. In these cases, a backtracking scheme is incorporated into our algorithm.
Reference: [9] <author> A. Forsgren, P.E. Gill, and W. Murray. </author> <title> A modified Newton method for unconstrained optimiza tion. </title> <type> Technical Report SOL 89-12, </type> <institution> Department of Operations Research, Stanford University, Stanford, California, </institution> <year> 1989. </year>
Reference-contexts: We believe that some promising approaches to compute both a Newton-type direction and a negative curvature direction are * the use of the conjugate gradient method [1, 5, 24], * the use of Lanczos decomposition [18, 20], * and several new modifications of Cholesky factorization <ref> [9, 10, 11, 21, 22] </ref>. Acknowledgement We thank the referees for their constructive comments and remarks that helped improve the paper. 13
Reference: [10] <author> A. Forsgren, P. E. Gill, and W. Murray. </author> <title> Computing modified Newton directions using a partial Cholesky factorization. </title> <type> Technical Report TRITA-MAT-1993-9, </type> <institution> Department of Mathematics, Royal Institute of Technology, Stockholm, Sweden, </institution> <year> 1993. </year>
Reference-contexts: We believe that some promising approaches to compute both a Newton-type direction and a negative curvature direction are * the use of the conjugate gradient method [1, 5, 24], * the use of Lanczos decomposition [18, 20], * and several new modifications of Cholesky factorization <ref> [9, 10, 11, 21, 22] </ref>. Acknowledgement We thank the referees for their constructive comments and remarks that helped improve the paper. 13
Reference: [11] <author> P. E. Gill, W. Murray, D. B. Poncelon, and M. A. Saunders. </author> <title> Preconditioners for indefinite systems arising in optimization. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 13 </volume> <pages> 292-311, </pages> <year> 1992. </year>
Reference-contexts: We believe that some promising approaches to compute both a Newton-type direction and a negative curvature direction are * the use of the conjugate gradient method [1, 5, 24], * the use of Lanczos decomposition [18, 20], * and several new modifications of Cholesky factorization <ref> [9, 10, 11, 21, 22] </ref>. Acknowledgement We thank the referees for their constructive comments and remarks that helped improve the paper. 13
Reference: [12] <author> P. E. Gill and W. Murray. </author> <title> Newton-type methods for unconstrained and linearly constrained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 7 </volume> <pages> 311-350, </pages> <year> 1974. </year>
Reference-contexts: As a second step we test the new algorithm against two Newton-type algorithms. The first of these follows the algorithm model proposed in [14] which uses a classical perturbation of the Newton direction (based on the modified Cholesky factorization <ref> [12] </ref>) as the search direction. Since the new algorithm uses essentially the same stabilization technique, the comparison between these two determines the effect of exploiting information on the curvature of the objective function via the curvilinear linesearch.
Reference: [13] <author> L. Grippo, F. Lampariello, and S. Lucidi. </author> <title> A nonmonotone line search technique for Newton's method. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 23 </volume> <pages> 707-716, </pages> <year> 1986. </year>
Reference-contexts: Step 3: Choose 2 [ 1 ; 2 ], ff = ff and go to Step 2. This approach has been tested in <ref> [13] </ref> and proven successful on many ill-conditioned problems. However, it does enforce a monotonic decrease in the sequence fF j g.
Reference: [14] <author> L. Grippo, F. Lampariello, and S. Lucidi. </author> <title> A class of nonmonotone stabilization methods in unconstrained optimization. </title> <journal> Numerische Mathematik, </journal> <volume> 59 </volume> <pages> 779-805, </pages> <year> 1991. </year>
Reference-contexts: As a first step we show that the linesearch approach proposed in [17] can be embedded in a general nonmonotone globalization strategy based on the model of <ref> [14] </ref>. This allows us to define a general algorithm model that inherits the following properties: * the capability to efficiently exploit potential local non convexity of the objective function (from [17]) * the capability to efficiently solve highly nonlinear and ill-conditioned minimization problems (from the nonmonotone approach of [14]). <p> model of <ref> [14] </ref>. This allows us to define a general algorithm model that inherits the following properties: * the capability to efficiently exploit potential local non convexity of the objective function (from [17]) * the capability to efficiently solve highly nonlinear and ill-conditioned minimization problems (from the nonmonotone approach of [14]). As a second step we test the new algorithm against two Newton-type algorithms. The first of these follows the algorithm model proposed in [14] which uses a classical perturbation of the Newton direction (based on the modified Cholesky factorization [12]) as the search direction. <p> local non convexity of the objective function (from [17]) * the capability to efficiently solve highly nonlinear and ill-conditioned minimization problems (from the nonmonotone approach of <ref> [14] </ref>). As a second step we test the new algorithm against two Newton-type algorithms. The first of these follows the algorithm model proposed in [14] which uses a classical perturbation of the Newton direction (based on the modified Cholesky factorization [12]) as the search direction. <p> the fact that eventually the step length will decrease to zero when we converge to a solution and by the fact that always enforcing the point x k to be located in nested level sets may deteriorate the efficiency of the Newton-like method (see the discussion in Section 5.1 of <ref> [14] </ref>). Computational results [14] have shown this technique to be even better than the standard nonmonotone line search. This technique has also proven useful in other applications and extensions [6, 7, 8, 25, 26, 27]. Of course, sometimes it may lead to regions where the function is poorly behaved. <p> eventually the step length will decrease to zero when we converge to a solution and by the fact that always enforcing the point x k to be located in nested level sets may deteriorate the efficiency of the Newton-like method (see the discussion in Section 5.1 of <ref> [14] </ref>). Computational results [14] have shown this technique to be even better than the standard nonmonotone line search. This technique has also proven useful in other applications and extensions [6, 7, 8, 25, 26, 27]. Of course, sometimes it may lead to regions where the function is poorly behaved. <p> The first of these establishes three important facts regarding the reference values and the iterates of the algorithm. The proof of the first two lemmas easily follows, with minor modifications, from the proof of Lemma 1 and Lemma 2 in <ref> [14] </ref>. <p> Moreover, by Lemma 3.1 and Lemma 3.3, we have that x fl 2 0 . The proof of assertion (a) can be easily completed similar to the proof of Theorem 1 of <ref> [14] </ref>. Finally, assertion (b) follows from known results [19, p.478], by taking into account Lemma 3.3 (b). 4 Computation of the search directions In this section, we describe an example of how to determine search directions that satisfy our assumptions using the Bunch-Parlett decomposition. <p> This test set covers the classical test problems along with a large number of nonlinear optimization problems of various difficulty representing both "academic" and "real world" applications. We have compared the results with those obtained by the algorithm proposed in <ref> [14] </ref>. For both the methods the stopping criterion is kgk 10 5 and also the parameters of the stabilization scheme are the same: M = 20, N = 20 and 0 = 10 3 . <p> In order to give a summary of this extensive numerical testing, in Table 1 we report the number of times each method performs the best in terms of number of iterations, function and gradient evaluations: NEW ALGORITHM ALGORITHM <ref> [14] </ref> tie iterations 69 17 91 function evaluations 155 18 4 gradient evaluations 69 17 91 Table 1: number of times each method performs the best Table 2 shows the cumulative results for all the problems solved by both algorithms. <p> In this table iterations stands for the total number of iterations needed to solve all these problems; the same for function and gradient evaluations. NEW ALGORITHM ALGORITHM <ref> [14] </ref> iterations 3979 10330 function evaluations 4293 17678 gradient evaluations 4148 10499 Table 2: cumulative results Moreover, there are 8 problems solved by our new algorithm while the algorithm proposed in [14] fails on these problems. The failures are caused by excessive number of iterations or functions evaluations. <p> NEW ALGORITHM ALGORITHM <ref> [14] </ref> iterations 3979 10330 function evaluations 4293 17678 gradient evaluations 4148 10499 Table 2: cumulative results Moreover, there are 8 problems solved by our new algorithm while the algorithm proposed in [14] fails on these problems. The failures are caused by excessive number of iterations or functions evaluations. <p> The results above show that this should lead to more robust and faster solution of these minimization problems. 6 Conclusions In this work we propose an algorithmic model based on a modified Newton method [17] and a non-monotone stabilization strategy <ref> [14] </ref>. <p> We have compared this algorithm on a large set of test problems to a similar one proposed in <ref> [14] </ref> that does not use any curvature information of the objective function. The results indicate that the new algorithm outperforms the old one. We believe that they should rekindle interest in defining new methods based on the curvilinear linesearch approach and using directions of negative curvature.
Reference: [15] <author> G. P. McCormick. </author> <title> A modification of Armijo's step-size rule for negative curvature. </title> <journal> Mathe matical Programming, </journal> <volume> 13 </volume> <pages> 111-115, </pages> <year> 1977. </year> <month> 14 </month>
Reference-contexts: In this context some interesting results have been proposed in <ref> [15] </ref> and [17]. In these papers, linesearch algorithms that generate sequences fx k g which converge to points x fl where the Hessian H (x fl ) is positive semidefinite are defined.
Reference: [16] <author> G. P. McCormick. </author> <title> Nonlinear Programming: Theory, Algorithms and Applications. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: This is because these test problems present just a few points where the objective function has negative curvature, as was noted in <ref> [16, 17] </ref>. The advent of the CUTE collection of test problems [2] allows us to obtain a good evaluation of globally convergent algorithms, since it includes a variety of functions with negative curvature. The main aim of our work is to encourage the development of new research in this area.
Reference: [17] <author> J. J. More and D. C. Sorensen. </author> <title> On the use of directions of negative curvature in a modified Newton method. </title> <journal> Mathematical Programming, </journal> <volume> 16 </volume> <pages> 1-20, </pages> <year> 1979. </year>
Reference-contexts: In this context some interesting results have been proposed in [15] and <ref> [17] </ref>. In these papers, linesearch algorithms that generate sequences fx k g which converge to points x fl where the Hessian H (x fl ) is positive semidefinite are defined. <p> This is because these test problems present just a few points where the objective function has negative curvature, as was noted in <ref> [16, 17] </ref>. The advent of the CUTE collection of test problems [2] allows us to obtain a good evaluation of globally convergent algorithms, since it includes a variety of functions with negative curvature. The main aim of our work is to encourage the development of new research in this area. <p> In particular, we believe that further investigation is needed to define efficient new linesearch algorithms that exploit the curvature information of the objective function more deeply. As a first step we show that the linesearch approach proposed in <ref> [17] </ref> can be embedded in a general nonmonotone globalization strategy based on the model of [14]. This allows us to define a general algorithm model that inherits the following properties: * the capability to efficiently exploit potential local non convexity of the objective function (from [17]) * the capability to efficiently <p> the linesearch approach proposed in <ref> [17] </ref> can be embedded in a general nonmonotone globalization strategy based on the model of [14]. This allows us to define a general algorithm model that inherits the following properties: * the capability to efficiently exploit potential local non convexity of the objective function (from [17]) * the capability to efficiently solve highly nonlinear and ill-conditioned minimization problems (from the nonmonotone approach of [14]). As a second step we test the new algorithm against two Newton-type algorithms. <p> In practice, we think of s k as an approximation of the Newton step and d k as a direction of negative curvature. Both of these can be calculated using the Bunch-Parlett factorization [3]; this was first considered in <ref> [17] </ref>. In order that our convergence theorems remain applicable to more general algorithms than the ones we test numerically, we list below the general conditions that we require of the search directions. We assume that the directions fs k g and fd k g satisfy the following conditions: Condition 1. <p> In Section 4, we show that a particular implementation satisfies these conditions. The key difference between our approach and that of <ref> [17] </ref> is in the line search acceptance criterion. We allow possible increases to the sequence of function values by keeping track of a reference value (which we label F ) in order to relax the normal Armijo acceptance criterion. <p> More specifically, the updating takes into account a prefixed number m (j) M of previous function values, which is called the "memory". Remark Notice that if we set M = 0 and 0 = 0, we obtained the same algorithm proposed in <ref> [17] </ref>. Moreover, it is important to note that we obtain the convergence results under weaker conditions than those ones required in [17]. This small difference allows us to use a particular pair of directions that do not satisfy the assumptions of [17], but do satisfy our conditions. 3 Convergence analysis To <p> Remark Notice that if we set M = 0 and 0 = 0, we obtained the same algorithm proposed in <ref> [17] </ref>. Moreover, it is important to note that we obtain the convergence results under weaker conditions than those ones required in [17]. This small difference allows us to use a particular pair of directions that do not satisfy the assumptions of [17], but do satisfy our conditions. 3 Convergence analysis To establish the convergence properties of Algorithm NMS, we employ some technical lemmas. <p> = 0, we obtained the same algorithm proposed in <ref> [17] </ref>. Moreover, it is important to note that we obtain the convergence results under weaker conditions than those ones required in [17]. This small difference allows us to use a particular pair of directions that do not satisfy the assumptions of [17], but do satisfy our conditions. 3 Convergence analysis To establish the convergence properties of Algorithm NMS, we employ some technical lemmas. The first of these establishes three important facts regarding the reference values and the iterates of the algorithm. <p> The Bunch-Parlett decomposition is very easy to implement and it gives information about the distribution of the eigenvalues of the Hessian matrix (for details, see <ref> [3, 17] </ref>). We recall that (at iteration k) the Bunch-Parlett decomposition gives H = W DW T ; where W is a n fi n non singular matrix and D is a symmetric n fi n block diagonal matrix with one by one and two by two diagonal blocks. <p> Proof The proof follows immediately from the definitions of d + and d . On the basis of this result, we set s = d + in our algorithm. In order to find a negative curvature direction d satisfying Condition 3, we recall the following direction from <ref> [17] </ref>: d MS = sgn g T v j m (D)j v; (17) where v is the solution of the following system W T v = z: Here, z is the eigenvector of D corresponding to the minimum eigenvalue m (D). It is shown in [17] that d MS satisfies Condition <p> recall the following direction from <ref> [17] </ref>: d MS = sgn g T v j m (D)j v; (17) where v is the solution of the following system W T v = z: Here, z is the eigenvector of D corresponding to the minimum eigenvalue m (D). It is shown in [17] that d MS satisfies Condition 3. <p> Using Proposition 4.1 and Lemma 4.3 of <ref> [17] </ref>, it can be easily proved that this direction satisfies Condition 3. The motivating property of our direction d is as follows. If the norm of the gradient is large then d almost coincides with d and hence approximately minimizes the quadratic model of the objective function. <p> The results above show that this should lead to more robust and faster solution of these minimization problems. 6 Conclusions In this work we propose an algorithmic model based on a modified Newton method <ref> [17] </ref> and a non-monotone stabilization strategy [14].
Reference: [18] <author> S. G. Nash. </author> <title> Newton-type minimization via Lanczos method. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 21 </volume> <pages> 770-788, </pages> <year> 1984. </year>
Reference-contexts: We believe that some promising approaches to compute both a Newton-type direction and a negative curvature direction are * the use of the conjugate gradient method [1, 5, 24], * the use of Lanczos decomposition <ref> [18, 20] </ref>, * and several new modifications of Cholesky factorization [9, 10, 11, 21, 22]. Acknowledgement We thank the referees for their constructive comments and remarks that helped improve the paper. 13
Reference: [19] <author> J. M. Ortega and W. C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press, </publisher> <address> San Diego, California, </address> <year> 1970. </year>
Reference-contexts: Moreover, by Lemma 3.1 and Lemma 3.3, we have that x fl 2 0 . The proof of assertion (a) can be easily completed similar to the proof of Theorem 1 of [14]. Finally, assertion (b) follows from known results <ref> [19, p.478] </ref>, by taking into account Lemma 3.3 (b). 4 Computation of the search directions In this section, we describe an example of how to determine search directions that satisfy our assumptions using the Bunch-Parlett decomposition.
Reference: [20] <author> C. C. Paige and M. A. Saunders. </author> <title> Solution of sparse indefinite systems of linear equations. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 12 </volume> <pages> 617-629, </pages> <year> 1975. </year>
Reference-contexts: We believe that some promising approaches to compute both a Newton-type direction and a negative curvature direction are * the use of the conjugate gradient method [1, 5, 24], * the use of Lanczos decomposition <ref> [18, 20] </ref>, * and several new modifications of Cholesky factorization [9, 10, 11, 21, 22]. Acknowledgement We thank the referees for their constructive comments and remarks that helped improve the paper. 13
Reference: [21] <author> T. Schlick. </author> <title> Modified Cholesky factorization for sparse preconditioners. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 14 </volume> <pages> 424-445, </pages> <year> 1993. </year>
Reference-contexts: We believe that some promising approaches to compute both a Newton-type direction and a negative curvature direction are * the use of the conjugate gradient method [1, 5, 24], * the use of Lanczos decomposition [18, 20], * and several new modifications of Cholesky factorization <ref> [9, 10, 11, 21, 22] </ref>. Acknowledgement We thank the referees for their constructive comments and remarks that helped improve the paper. 13
Reference: [22] <author> R. B. Schnabel and E. Eskow. </author> <title> A new modified Cholesky factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 1136-1158, </pages> <year> 1990. </year>
Reference-contexts: We believe that some promising approaches to compute both a Newton-type direction and a negative curvature direction are * the use of the conjugate gradient method [1, 5, 24], * the use of Lanczos decomposition [18, 20], * and several new modifications of Cholesky factorization <ref> [9, 10, 11, 21, 22] </ref>. Acknowledgement We thank the referees for their constructive comments and remarks that helped improve the paper. 13
Reference: [23] <author> G. A. Shultz, R. B. Schnabel, and R. H. Byrd. </author> <title> A family of trust-region-based algorithms for unconstrained minimization. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 22 </volume> <pages> 44-67, </pages> <year> 1985. </year>
Reference-contexts: Trust region algorithms intrinsically have this capability due to the fact that they are based on the idea of minimizing the quadratic model of the objective function over a sphere (see, for example <ref> [23] </ref>). Linesearch algorithms do not enjoy this particular feature. In fl Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706.
Reference: [24] <author> Ph. L. Toint. </author> <title> Towards an efficient sparsity exploiting Newton method for Minimization. </title> <editor> In I. S. Duff, editor, </editor> <title> Sparse matrices and their uses, </title> <address> pages 57-88. </address> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: We believe that some promising approaches to compute both a Newton-type direction and a negative curvature direction are * the use of the conjugate gradient method <ref> [1, 5, 24] </ref>, * the use of Lanczos decomposition [18, 20], * and several new modifications of Cholesky factorization [9, 10, 11, 21, 22]. Acknowledgement We thank the referees for their constructive comments and remarks that helped improve the paper. 13
Reference: [25] <author> Ph. L. Toint. </author> <title> An assesment of non-monotone linesearch techniques for unconstrained opti mization. </title> <type> Technical Report 94/14, </type> <institution> Department of Mathematics, FUNDP, </institution> <address> Namur, Belgium, </address> <year> 1994. </year>
Reference-contexts: Computational results [14] have shown this technique to be even better than the standard nonmonotone line search. This technique has also proven useful in other applications and extensions <ref> [6, 7, 8, 25, 26, 27] </ref>. Of course, sometimes it may lead to regions where the function is poorly behaved. In these cases, a backtracking scheme is incorporated into our algorithm.
Reference: [26] <author> Ph. L. Toint. </author> <title> A non-monotone trust-region algorithm for nonlinear optimization subject to convex constraints. </title> <type> Technical Report 94/24, </type> <institution> Department of Mathematics, FUNDP, </institution> <address> Namur, Belgium, </address> <year> 1994. </year>
Reference-contexts: Computational results [14] have shown this technique to be even better than the standard nonmonotone line search. This technique has also proven useful in other applications and extensions <ref> [6, 7, 8, 25, 26, 27] </ref>. Of course, sometimes it may lead to regions where the function is poorly behaved. In these cases, a backtracking scheme is incorporated into our algorithm.
Reference: [27] <author> Y. Xiao, and F. J. Zhou. </author> <title> Nonmonotone trust region methods with curvilinear path in uncon strained minimization. </title> <journal> Computing, </journal> <volume> 48 </volume> <pages> 303-317, </pages> <year> 1992. </year> <month> 15 </month>
Reference-contexts: Computational results [14] have shown this technique to be even better than the standard nonmonotone line search. This technique has also proven useful in other applications and extensions <ref> [6, 7, 8, 25, 26, 27] </ref>. Of course, sometimes it may lead to regions where the function is poorly behaved. In these cases, a backtracking scheme is incorporated into our algorithm.
References-found: 27

