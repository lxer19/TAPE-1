URL: http://www.cs.ucsd.edu/~calder/pfdc/papers/pfdc-bodin.ps.Z
Refering-URL: http://www.cs.ucsd.edu/~calder/pfdc/program.html
Root-URL: http://www.cs.ucsd.edu
Phone: 2  3  
Title: Iterative Compilation in a Non-Linear Optimisation Space  
Author: F. Bodin T. Kisuki P.M.W. Knijnenburg M.F.P. O'Boyle E. Rohou 
Address: 35042 Rennes, France  Netherlands  
Affiliation: 1 IRISA, Campus Universitaire de Beaulieu,  Dept. of Computer Science, Leiden University, The  Division of Informatics, The University, Edinburgh  
Abstract: This paper investigates the applicability of iterative search techniques in program optimisation. Iterative compilation is usually considered too expensive for general purpose computing but is applicable to embedded applications where the cost is easily amortised over the number of embedded systems produced. This paper presents a case study, where an iterative search algorithm is used to investigate a nonlinear transformation space and find the fastest execution time within a fixed number of evaluations. By using execution time as feedback, it searches a large but restricted transformation space and shows performance improvement over existing approaches. We show that in the case of large transformation spaces, we can achieve within 0.3% of the best possible time by visiting less then 0.25% of the space using a simple algorithm and find the minimum after visiting less than 1% of the space. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Aarts, M. Barreteau, F. Bodin, P. Brinkhaus, Z. Chamski, H.-P. Charles, C. Eisenbeis, J.R. Gurd, J. Hoogerbrugge, P. Hu, W. Jalby, P.M.W. Knijnenburg, M.F.P. O'Boyle, E. Ro-hou, R. Sakellariou, H. Schepers, A. Seznec, E.A. Stohr, M. Verhoeven, H.A.G. Wijshoff, </author> <title> OCEANS: Optimizing Compilers for Embedded Applications. </title> <note> Proc. EuroPar97, </note> <year> 1997. </year>
Reference-contexts: Other factors generally ignored, include the introduction of spill code and instruction cache misses. Iterative compilation by definition consider all parts of the system when deciding on the best optimisation. The Oceans project is an ESPRIT funded project, concerned with developing an iterative compiler for embedded systems <ref> [1] </ref>. In particular, we are targeting general purpose VLIW processors of which the Philips TriMedia TM1000 [2] is typ ical. Economies of scale allow the production of cheaper and faster general purpose processors over custom embedded processors. <p> Further analysis techniques could be used to reduce this number. In this paper we have considered execution time as the metric for evaluating goodness of a transformation. As our system <ref> [1] </ref> provides additional information, such as code size, register pressure, slot utilisation etc., it is possible to statically evaluate the goodness of a transformation after code generation.
Reference: [2] <author> B. </author> <title> Case, Philips Hope to Displace DSPs with VLIW. </title> <type> Microprocessor Report 8(16), </type> <month> 5 Dec. </month> <year> 1994, </year> <pages> pp. 12-15. </pages> <note> See also http://www.trimedia-philips.com/ </note>
Reference-contexts: This is repeated for a larger transformation space. The algorithm is applied to the TriMedia TM1000 simulator (a VLIW processor produced by Philips aimed at the embedded processor market <ref> [2] </ref>) and its behaviour evaluated. The paper finishes with a brief survey of related work and some concluding remarks. Bodin et al. 1 2. <p> Iterative compilation by definition consider all parts of the system when deciding on the best optimisation. The Oceans project is an ESPRIT funded project, concerned with developing an iterative compiler for embedded systems [1]. In particular, we are targeting general purpose VLIW processors of which the Philips TriMedia TM1000 <ref> [2] </ref> is typ ical. Economies of scale allow the production of cheaper and faster general purpose processors over custom embedded processors. However, such processors rely on efficient software implementations of the embedded applications but can afford long compilation times, hence our interest in iterative compilation.
Reference: [3] <author> S. Coleman and K. McKinley, </author> <title> Tile Size Selection using Cache Organization and Data Layout. </title> <booktitle> Proc. Programming Language Design and Implementation, </booktitle> <publisher> ACM Press, </publisher> <year> 1995. </year>
Reference-contexts: Related Work and Discussion There is a large body of working considering program transformations to improve uniprocessor performance. In <ref> [3] </ref>, an analytic algorithm to give a good tile size to minimise interference and exploit locality is presented. This work considered rectangular tiles whose dimensions are a function of the iteration space and the cache organisation. <p> This work considered rectangular tiles whose dimensions are a function of the iteration space and the cache organisation. This work gives good performance improvements over existing techniques but does not consider the impact of tiling on unrolling or other transformations. For example, in <ref> [3] </ref> a tile size of 170 fi 2 was shown to give the best performance on the Al Bodin et al. 5 Alpha N = 400 Alpha N = 512 min = 1.237 max = 38.186 orig = 12.041 min = 3.347 max = 81.403 orig = 31.726 avg = 2.854
Reference: [4] <author> E. Hansen, </author> <title> Global Optimization Using Interval Analysis, </title> <publisher> Marcel Dekker Inc. </publisher> <address> New York, </address> <year> 1992 </year>
Reference-contexts: In this paper we have used a very simple search algorithm as a basis for iterative compilation There is in fact a large literature on non-linear optimisation <ref> [4] </ref>, though it is based on a continuous underlying optimisation function rather than the discrete space we consider. Techniques such as polynomial fitting could be applied to help improve the performance of the search algorithm.
Reference: [5] <author> S.F. Hummel, I. Banicesu, C.-T. Wang and J. Wein, </author> <title> Load Balancing and Data Locality via Fractiling: An Experimental Study. </title> <publisher> Kluwer Academic Press, </publisher> <address> LCR, </address> <year> 1995. </year>
Reference-contexts: Several researchers have considered using runtime information to select the best implementation. They, however, define one or more options statically which are then considered at runtime. For example, in <ref> [5] </ref>, whether or not a portion of the iteration space should be tiled depends on run-time characteristics and in [7], different synchronisation algorithms are called depending on runtime behaviour. The work in this paper, however, considers a much larger space of optimisations at compile time without incurring runtime overhead.
Reference: [6] <author> A. Nisbet, </author> <title> GAPS: Genetic Algorithm Optimised Paralleli-sation. </title> <booktitle> Proc. 7th Workshop on Compilers for Parallel Computing, </booktitle> <year> 1998. </year>
Reference-contexts: Later work could combine the approaches by including dynamic monitoring to select, at runtime, one of a number of optimisations programs that were determined (at compile time) to perform well under certain circumstances. In <ref> [6] </ref>, genetic algorithms are used to create and select transformations for parallel optimisation.
Reference: [7] <author> P. Diniz and M. Rinard, </author> <title> Dynamic Feedback: An Effective Technique for Adaptive Computing. </title> <booktitle> Proc. Programming Languages Design and Implementation, </booktitle> <year> 1997. </year>
Reference-contexts: Several researchers have considered using runtime information to select the best implementation. They, however, define one or more options statically which are then considered at runtime. For example, in [5], whether or not a portion of the iteration space should be tiled depends on run-time characteristics and in <ref> [7] </ref>, different synchronisation algorithms are called depending on runtime behaviour. The work in this paper, however, considers a much larger space of optimisations at compile time without incurring runtime overhead.
Reference: [8] <author> M. Fernadez, </author> <title> Simple and effecient link-time optimizations of modula-2 programs. </title> <booktitle> Proc. Programming Languages Design and Implementation, </booktitle> <year> 1995. </year>
Reference-contexts: 1. Introduction The use of transformations to improve program performance has been extensively studied for over 30 years. Such work is based primarily on static analysis, possibly with some profile information to determine the significant regions of the code <ref> [8] </ref> and runtime dependent control-flow. Each technique is characterised by trying (i) to determine how a program would perform on a particular processor and (ii) developing a program transformation such that the code is likely to execute more efficiently.

References-found: 8

