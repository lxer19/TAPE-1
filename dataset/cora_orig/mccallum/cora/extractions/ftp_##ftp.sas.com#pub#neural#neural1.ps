URL: ftp://ftp.sas.com/pub/neural/neural1.ps
Refering-URL: http://atlas.otago.ac.nz:800/nnweb/FAQ1.html
Root-URL: 
Title: Neural Networks and Statistical Models Proceedings of the Nineteenth Annual SAS Users Group International Conference,
Author: Warren S. Sarle, SAS 
Address: Cary, NC, USA  
Affiliation: Institute Inc.,  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderberg, </author> <title> M.R. (1973), Cluster Analysis for Applications, </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Bates, D.M. and Watts, D.G. </author> <year> (1988), </year> <title> Nonlinear Regression Analysis and Its Applications, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Borowiak, D.S. </author> <year> (1989), </year> <title> Model Discrimination for Nonlinear Regression Models, </title> <address> New York: Marcel-Dekker. </address>
Reference: <author> Carpenter, G.A., Grossberg, S., Reynolds, J.H. </author> <year> (1991), </year> <title> ARTMAP: Supervised real-time learning and classification of nonstationary data by a self-organizing neural network, </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 565-588. </pages>
Reference-contexts: It differs from ART 1 mainly in having an elaborate iterative scheme for normalizing the inputs. The normalization is supposed to reduce the cluster proliferation that plagues ART 1 and to allow for varying background levels in visual pattern recognition. Fuzzy ART <ref> (Carpenter, Grossberg, and Rosen 1991) </ref> is for bounded quantitative data. It is similar to ART 1 but uses the fuzzy operators min and max in place of the logical and and or operators. ARTMAP (Carpenter, Grossberg, and Reynolds 1991) is an ARTistic variant of counterpropagation for supervised learning. <p> Fuzzy ART (Carpenter, Grossberg, and Rosen 1991) is for bounded quantitative data. It is similar to ART 1 but uses the fuzzy operators min and max in place of the logical and and or operators. ARTMAP <ref> (Carpenter, Grossberg, and Reynolds 1991) </ref> is an ARTistic variant of counterpropagation for supervised learning. ART has its own jargon. For example, data are called an arbitrary sequence of input patterns. The current observation is stored in short term memory and cluster seeds are long term memory.
Reference: <author> Carpenter, G.A., Grossberg, S., Rosen, D.B. </author> <year> (1991), </year> <title> Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system, </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 759-771. </pages> <editor> Chen et al (1991), </editor> <title> Orthogonal least squares algorithm for radial-basis-function networks IEEE Transactions on Neural Networks, </title> <booktitle> 2, </booktitle> <pages> 302-309. </pages>
Reference-contexts: It differs from ART 1 mainly in having an elaborate iterative scheme for normalizing the inputs. The normalization is supposed to reduce the cluster proliferation that plagues ART 1 and to allow for varying background levels in visual pattern recognition. Fuzzy ART <ref> (Carpenter, Grossberg, and Rosen 1991) </ref> is for bounded quantitative data. It is similar to ART 1 but uses the fuzzy operators min and max in place of the logical and and or operators. ARTMAP (Carpenter, Grossberg, and Reynolds 1991) is an ARTistic variant of counterpropagation for supervised learning. <p> Fuzzy ART (Carpenter, Grossberg, and Rosen 1991) is for bounded quantitative data. It is similar to ART 1 but uses the fuzzy operators min and max in place of the logical and and or operators. ARTMAP <ref> (Carpenter, Grossberg, and Reynolds 1991) </ref> is an ARTistic variant of counterpropagation for supervised learning. ART has its own jargon. For example, data are called an arbitrary sequence of input patterns. The current observation is stored in short term memory and cluster seeds are long term memory.
Reference: <author> Cooper, L.N., Elbaum, C. and Reilly, D.L. </author> <year> (1982), </year> <title> Self Organizing General Pattern Class Separator and Identifier, </title> <type> U.S. Patent 4,326,259. </type>
Reference-contexts: RBF neurons are also called localized receptive fields, locally tuned processing units, or potential functions. RBF networks are closely related to regularization networks. The modified Kanerva model (Prager and Fallside 1989) is an RBF network with a threshhold activation function. The Restricted Coulomb Energy TM System <ref> (Cooper, Elbaum and Reilly 1982) </ref> is another threshold RBF network used for classification. There is a discrete variant of RBF networks called the cerebellum model articulation controller (CMAC) (Miller, Glanz and Kraft 1990).
Reference: <author> Cramer, J. S. </author> <year> (1986), </year> <title> Econometric Applications of Maximum Likelihood Methods, </title> <address> Cambridge, UK: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Edwards, </author> <month> A.W.F </month> <year> (1972), </year> <title> Likelihood, </title> <address> Cambridge, UK: </address> <publisher> Cam-bridge University Press. </publisher>
Reference: <author> Everitt, B.S. </author> <year> (1980), </year> <title> Cluster Analysis, 2nd Edition, </title> <publisher> London: Heineman Educational Books Ltd. </publisher>
Reference: <author> Fortier, J.J. </author> <year> (1966), </year> <title> Simultaneous Linear Prediction, </title> <journal> Psy-chometrika, </journal> <volume> 31, </volume> <pages> 369-381. </pages>
Reference: <author> Friedman, J.H. and Stuetzle, W. </author> <year> (1981), </year> <title> Projection pursuit regression, </title> <journal> Journal of the American Statistical Association, </journal> <volume> 76, </volume> <pages> 817-823. </pages>
Reference-contexts: With a small number of hidden neurons, an MLP is a parametric model that provides a useful alternative to polynomial regression. With a moderate number of hidden neurons, an MLP can be considered a quasi-parametric model similar to projection pursuit regression <ref> (Friedman and Stuetzle 1981) </ref>. An MLP with one hidden layer is essentially the same as the projection pursuit regression model except that an MLP uses a predetermined functional form for the activation function in the hidden layer, whereas projection pursuit uses a flexible nonlinear smoother. <p> This network could be reduced to a single hidden layer, but the additional hidden layers aid interpretation of the results. By adding another linear hidden layer to the GAM network, a projection pursuit network can be constructed as shown in <ref> (Friedman and Stuetzle 1981) </ref> except that subnetworks provide the nonlinearities instead of nonlinear smoothers. Conclusion The goal of creating artificial intelligence has lead to some fundamental differences in philosophy between neural engineers and statisticians.
Reference: <author> Gallant, A.R. </author> <year> (1987), </year> <title> Nonlinear Statistical Models, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Geiger, H. </author> <year> (1990), </year> <title> Storing and Processing Information in Connectionist Systems, </title> <editor> in Eckmiller, R., ed., </editor> <booktitle> Advanced Neural Computers, </booktitle> <pages> 271-277, </pages> <address> Amsterdam: </address> <publisher> North-Holland. </publisher>
Reference-contexts: In a generalized additive model (GAM) (Hastie and Tibshirani 1990), a nonlinear transformation estimated by a nonparametric smoother is applied to each input, and these values are added together. The TRANSREG procedure fits nonlinear additive models using B splines. Topologically distributed encoding (TDE) <ref> (Geiger 1990) </ref> uses Gaussian basis functions. A nonlinear additive model can also be implemented as a NN as shown in Figure 17. Each input is connected to a small subnetwork to provide the nonlinear transformations. The outputs of the subnetworks are summed to give the output of the complete network.
Reference: <author> Gifi, A. </author> <year> (1990), </year> <title> Nonlinear Multivariate Analysis, </title> <address> Chichester, UK: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Hand, D.J. </author> <year> (1981), </year> <title> Discrimination and Classification, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: LVQ is a variation of nearest-neighbor discriminant analysis. Rather than finding the nearest neighbor in the entire training set to classify an input vector, LVQ finds the nearest point in a set of prototype vectors, with several protypes for each class. LVQ differs from edited and condensed k-nearest-neighbor methods <ref> (Hand 1981) </ref> in that the prototypes are not members of the training set but are computed using algorithms similar to AVQ. A somewhat similar method proceeds by clustering each class separately and then using the cluster centers as prototypes.
Reference: <author> H a rdle, W. </author> <year> (1990), </year> <title> Applied Nonparametric Regression, </title> <address> Cam-bridge, UK: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: If the number of hidden neurons is allowed to increase with the sample size, an MLP becomes a nonparametric sieve (White 1992) that provides a useful alternative to methods such as kernel regression <ref> (H a rdle 1990) </ref> and smoothing splines (Eubank 1988; Wahba 1990). MLPs are especially valuable because you can vary the complexity of the model from a simple parametric model to a highly flexible, nonparametric model. <p> In a radial basis function (RBF) network (Wasserman 1993), as shown in Figure 13, the hidden neurons compute radial basis functions of the inputs, which are similar to kernel functions in kernel regression <ref> (H ardle 1990) </ref>. The net input to the hidden layer is the distance from the input vector to the weight vector. The weight vectors are also called centers. The distance is usually computed in the Euclidean metric, although it is sometimes a weighted Euclidean distance or an inner product metric. <p> For kernel regression estimators, the error in prediction typically decreases in proportion to n p=(2p+d) , where p is the number of derivatives of the regression function and d is the number of inputs <ref> (H a rdle 1990, 93) </ref>. Hence, kernel methods tend to require larger sample sizes than paramteric methods, especially in multidimensional spaces. <p> Nonlinear additive models provide a compromise in complexity between multiple linear regression and a fully flexible nonlinear model such as an MLP, a high-order polynomial, or a tensor spline model. In a generalized additive model (GAM) <ref> (Hastie and Tibshirani 1990) </ref>, a nonlinear transformation estimated by a nonparametric smoother is applied to each input, and these values are added together. The TRANSREG procedure fits nonlinear additive models using B splines. Topologically distributed encoding (TDE) (Geiger 1990) uses Gaussian basis functions.
Reference: <author> Hartigan, J.A. </author> <year> (1975), </year> <title> Clustering Algorithms, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: However, the normalization requirement greatly limits the applicability of the network. It is generally more useful to define the net input as the Euclidean distance between the synaptic weights and the input values, in which case the competitive learning network is very similar to k-means clustering <ref> (Hartigan 1975) </ref> except that the usual training algorithms are slow and nonconver-gent.
Reference: <author> Hastie, T.J. and Tibshirani, R.J. </author> <year> (1990), </year> <title> Generalized Additive Models, </title> <publisher> London: Chapman & Hall. </publisher>
Reference-contexts: Nonlinear additive models provide a compromise in complexity between multiple linear regression and a fully flexible nonlinear model such as an MLP, a high-order polynomial, or a tensor spline model. In a generalized additive model (GAM) <ref> (Hastie and Tibshirani 1990) </ref>, a nonlinear transformation estimated by a nonparametric smoother is applied to each input, and these values are added together. The TRANSREG procedure fits nonlinear additive models using B splines. Topologically distributed encoding (TDE) (Geiger 1990) uses Gaussian basis functions.
Reference: <author> Hertz, J., Krogh, A. and Palmer, R.G. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation, </title> <address> Redwood City, CA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Hinton, G.E. </author> <year> (1992), </year> <title> How Neural Networks Learn from Experience, </title> <publisher> Scientific American, </publisher> <month> 267 (September), </month> <pages> 144-151. </pages>
Reference: <author> Hotelling, H. </author> <year> (1933), </year> <title> Analysis of a Complex of Statistical Variables into Principal Components, </title> <journal> Journal of Educational Psychology, </journal> <volume> 24, </volume> <pages> 417-441, 498-520. </pages>
Reference: <author> Hosmer, D.W. and Lemeshow, S. </author> <year> (1989), </year> <title> Applied Logistic Regression, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: Input X 1 X 3 Independent Variables Output Predicted Values Target Y 1 Dependent Variables Linear Regression A perceptron with a logistic activation function is a logistic regression model <ref> (Hosmer and Lemeshow 1989) </ref> as shown in Input X 1 X 3 Independent Variables Output Predicted Value Target Y Dependent Variable A perceptron with a threshold activation function is a linear discriminant function (Hand 1981; McLachlan 1992; Weiss and Kulikowski 1991).
Reference: <author> Huber, P.J. </author> <year> (1985), </year> <title> Projection pursuit, </title> <journal> Annals of Statistics, </journal> <volume> 13, </volume> <pages> 435-475. </pages>
Reference: <author> Jackson, J.E. </author> <year> (1991), </year> <title> A User's Guide to Principal Components, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Jolliffe, I.T. </author> <year> (1986), </year> <title> Principal Component Analysis, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Jones, M.C. and Sibson, R. </author> <year> (1987), </year> <title> What is projection pursuit? Journal of the Royal Statistical Society, </title> <journal> Series A, </journal> <volume> 150, </volume> <pages> 1-38. </pages>
Reference: <author> Kaufmann, L. and Rousseeuw, P.J. </author> <year> (1990), </year> <title> Finding Groups in Data, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> McCullagh, P. and Nelder, J.A. </author> <year> (1989), </year> <title> Generalized Linear Models, 2nd ed., </title> <publisher> London: Chapman & Hall. </publisher>
Reference-contexts: Input X 1 X 3 Independent Variables Output Predicted Value Target Y Binary Class Variable The activation function in a perceptron is analogous to the inverse of the link function in a generalized linear model (GLIM) <ref> (McCullagh and Nelder 1989) </ref>. Activation functions are usually bounded, whereas inverse link functions, such as the identity, reciprocal, and exponential functions, often are not. Inverse link functions are required to be monotone in most implementations of GLIMs, although this restriction is only for computational convenience.
Reference: <author> McLachlan, G.J. </author> <year> (1992), </year> <title> Discriminant Analysis and Statistical Pattern Recognition, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> McLachlan, G.J. and Basford, K.E. </author> <year> (1988), </year> <title> Mixture Models, </title> <address> New York: </address> <publisher> Marcel Dekker, Inc. </publisher>
Reference: <author> Massart, D.L. and Kaufman, L. </author> <year> (1983), </year> <title> The Interpretation of Analytical Chemical Data by the Use of Cluster Analysis, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Masters, T. </author> <year> (1993), </year> <title> Practical Neural Network Recipes in C++, </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Miller III, W.T., Glanz, F.H. and Kraft III, L.G. </author> <year> (1990), </year> <title> CMAC: an associative neural network alternative to backpropagation, </title> <journal> Proceedings IEEE, </journal> <volume> 78, </volume> <pages> 1561-1567. </pages>
Reference-contexts: The Restricted Coulomb Energy TM System (Cooper, Elbaum and Reilly 1982) is another threshold RBF network used for classification. There is a discrete variant of RBF networks called the cerebellum model articulation controller (CMAC) <ref> (Miller, Glanz and Kraft 1990) </ref>. Sometimes the hidden layer values are normalized to sum to 1 (Moody and Darken 1988) as is commonly done in kernel regression (Nadaraya 1964; Watson 1964).
Reference: <author> Moore, B. </author> <year> (1988), </year> <title> ART 1 and Pattern Clustering, </title> <editor> in Touretzky, D., Hinton, G. and Sejnowski, T., eds., </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> 174-185, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Myers, R.H. </author> <year> (1986), </year> <title> Classical and Modern Regression with Applications, </title> <address> Boston: </address> <publisher> Duxbury Press Nadaraya, E.A. </publisher> <year> (1964), </year> <title> On estimating regression, </title> <journal> Theory Probab. Applic. </journal> <volume> 10, </volume> <pages> 186-90. Pao, </pages> <month> Y </month> <year> (1989), </year> <title> Adaptive Pattern Recognition and Neural Networks, </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Any two neurons that are neighbors in the output array would correspond to two sets of points in the input space that are close to each other. Hybrid Networks Hybrid networks combine supervised and unsupervised learning. Principal component regression <ref> (Myers 1986) </ref> is an example of a well-known statistical method that can be viewed as a hybrid network with three layers. The independent variables are the input layer, and the principal components of the independent variables are the hidden, unsupervised layer.
Reference: <author> Prager, R.W. and Fallside, F. </author> <year> (1989), </year> <title> The Modified Kanerva Model for Automatic Speech Recognition, </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 3, </volume> <pages> 61-81. </pages>
Reference-contexts: RBF neurons are also called localized receptive fields, locally tuned processing units, or potential functions. RBF networks are closely related to regularization networks. The modified Kanerva model <ref> (Prager and Fallside 1989) </ref> is an RBF network with a threshhold activation function. The Restricted Coulomb Energy TM System (Cooper, Elbaum and Reilly 1982) is another threshold RBF network used for classification.
Reference: <author> Rao, C.R. </author> <year> (1964), </year> <title> The Use and Interpretation of Principal Component Analysis in Applied Research, </title> <journal> Sankya, Series A, </journal> <volume> 26, </volume> <pages> 329-358. </pages>
Reference: <author> Ripley, B.D. </author> <year> (1993), </year> <title> Statistical Aspects of Neural Networks, </title> <editor> in Barndorff-Nielsen, O.E., Jensen, J.L. and Kendall, W.S., eds., </editor> <title> Networks and Chaos: Statistical and Probabilistic Aspects, </title> <publisher> London: Chapman & Hall. </publisher>
Reference: <author> Ross, G.J.S. </author> <year> (1990), </year> <title> Nonlinear Estimation, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Schiler, H. and Hartmann, U. </author> <year> (1992), </year> <title> Mapping Neural Network Derived from the Parzen Window Estimator, </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 903-909. </pages>
Reference: <author> Seber, G.A.F and Wild, C.J. </author> <year> (1989), </year> <title> Nonlinear Regression, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Sneath, P.H.A. and Sokal, R.R. </author> <year> (1973), </year> <title> Numerical Taxonomy, </title> <address> San Francisco: </address> <publisher> W.H. Freeman. </publisher>
Reference: <author> Sou cek, B. </author> <title> and The IRIS Group (1992), Fast Learning and Invariant Object Recognotion: The Sixth-Generation Breakthrough, </title> <address> New York, </address> <publisher> John Wiley & Sons, Spath, H. </publisher> <year> (1980), </year> <title> Cluster Analysis Algorithms, </title> <address> Chichester, UK: </address> <publisher> Ellis Horwood. </publisher>
Reference-contexts: Elaborate functional link networks are used in applications such as image processing to perform a variety of impressive tasks <ref> (Sou cek and The IRIS Group 1992) </ref>. Multilayer Perceptrons A functional link network introduces an extra hidden layer of neurons, but there is still only one layer of weights to be estimated.
Reference: <author> Specht, D.F. </author> <year> (1991), </year> <title> A Generalized Regression Neural Network, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2, </volume> <month> Nov. </month> <year> 1991, </year> <pages> 568-576. </pages>
Reference-contexts: Many NN researchers are engineers, physicists, neurophysi-ologists, psychologists, or computer scientists who know little about statistics and nonlinear optimization. NN researchers routinely reinvent methods that have been known in the statistical or mathematical literature for decades or centuries, but they often fail to understand how these methods work <ref> (e.g., Specht 1991) </ref>. The common implementations of NNs are based on biological or engineering criteria, such as how easy it is to fit the net on a chip, rather than on well-established statistical and optimization criteria. <p> The NN literature usually uses interpolation, but kernel smoothing would be superior in most cases. Kernel-smoothed counterpropagation would be a variety of binned kernel regression estimation using clusters for the bins, similar to the clustered form of GRNN <ref> (Specht 1991) </ref>. 7 Learning vector quantization (LVQ) (Kohonen 1989) has both supervised and unsupervised aspects, although it is not a hybrid network in the strict sense of having separate supervised and unsupervised layers. LVQ is a variation of nearest-neighbor discriminant analysis.
Reference: <author> Titterington, D.M., Smith, A.F.M., and Makov, U.E. </author> <year> (1985), </year> <title> Statistical Analysis of Finite Mixture Distributions, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Tukey, J.W. </author> <year> (1961), </year> <title> Curves as Parameters and Touch Estimation, </title> <booktitle> Proceedings of the 4th Berkeley Symposium, </booktitle> <pages> 681-694. </pages> <editor> van den Wollenberg, A.L. </editor> <year> (1977), </year> <title> Redundancy AnalysisAn Alternative to Canonical Correlation Analysis, </title> <journal> Psychometrika, </journal> <volume> 42, </volume> <pages> 207-219. </pages>
Reference-contexts: Hence, counterpropagation is usually used for prediction in only one direction. As such, coun-terpropagation is a form of nonparametric regression in which the smoothing parameter is the number of clusters. If training is unidirectional, then counterpropagation is a regressogram estimator <ref> (Tukey 1961) </ref> with the bins determined clustering the input cases.
Reference: <author> Wasserman, </author> <title> P.D. </title> <booktitle> (1993), Advanced Methods in Neural Computing, </booktitle> <address> New York: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: Radial Basis Functions In an MLP, the net input to the hidden layer is a linear combination of the inputs as specified by the weights. In a radial basis function (RBF) network <ref> (Wasserman 1993) </ref>, as shown in Figure 13, the hidden neurons compute radial basis functions of the inputs, which are similar to kernel functions in kernel regression (H ardle 1990). The net input to the hidden layer is the distance from the input vector to the weight vector.
Reference: <author> Watson, G.S. </author> <year> (1964), </year> <title> Smooth regression analysis, </title> <journal> Sankhya, Series A, </journal> <volume> 26, </volume> <pages> 359-72. </pages>
Reference: <author> Weisberg, S. </author> <year> (1985), </year> <title> Applied Linear Regression, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Weiss, </author> <title> S.M. and Kulikowski, </title> <address> C.A. </address> <year> (1991), </year> <title> Computer Systems That Learn, </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> White, H. </author> <year> (1992), </year> <title> Artificial Neural Networks: Approximation and Learning Theory, </title> <publisher> Oxford, UK: Blackwell. </publisher> <address> SAS/ETS, SAS/IML, </address> <month> SAS/OR, </month> <title> and SAS/STAT are registered trademarks of SAS Institute Inc. in the USA and other countries. R fl indicates USA registration. Other brand and product names are trademerks of their respective companies. </title> <type> 13 </type>
Reference-contexts: MLPs are general-purpose, flexible, nonlinear models that, given enough hidden neurons and enough data, can approximate virtually any function to any desired degree of accuracy. In other words, MLPs are universal approximators <ref> (White 1992) </ref>. MLPs can be used when you have little knowledge about the form of the relationship between the independent and dependent variables. You can vary the complexity of the MLP model by varying the number of hidden layers and the number of hidden neurons in each hidden layer. <p> If the number of hidden neurons is allowed to increase with the sample size, an MLP becomes a nonparametric sieve <ref> (White 1992) </ref> that provides a useful alternative to methods such as kernel regression (H a rdle 1990) and smoothing splines (Eubank 1988; Wahba 1990). MLPs are especially valuable because you can vary the complexity of the model from a simple parametric model to a highly flexible, nonparametric model.
References-found: 51

