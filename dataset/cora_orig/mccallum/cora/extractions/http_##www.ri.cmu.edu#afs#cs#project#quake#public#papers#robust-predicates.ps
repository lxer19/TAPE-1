URL: http://www.ri.cmu.edu/afs/cs/project/quake/public/papers/robust-predicates.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/quake/public/www/tripaper/triangle8.html
Root-URL: 
Email: jrs@cs.cmu.edu  
Title: Robust Adaptive Floating-Point Geometric Predicates  
Author: Jonathan Richard Shewchuk 
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Fast C implementations of four geometric predicates, the 2D and 3D orientation and incircle tests, are publicly available. Their inputs are ordinary single or double precision floating-point numbers. They owe their speed to two features. First, they employ new fast algorithms for arbitrary precision arithmetic that have a strong advantage over other software techniques in computations that manipulate values of extended but small precision. Second, they are adaptive; their running time depends on the degree of uncertainty of the result, and is usually small. These algorithms work on computers whose floating-point arithmetic uses radix two and exact rounding, including machines that comply with the IEEE 754 floating-point standard. Timings of the predicates, in isolation and embedded in 2D and 3D Delaunay triangulation programs, verify their effectiveness. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Francis Avnaim, Jean-Daniel Boissonnat, Olivier Devillers, Franco P. Preparata, and Mariette Yvinec. </author> <title> Evaluating Signs of Determinants Using Single-Precision Arithmetic. </title> <year> 1995. </year>
Reference-contexts: Clarkson's algorithm is naturally adaptive; its running time is small for matrices whose determinants are not near zero 1 . Recently, Avnaim, Boissonnat, Devillers, Preparata, and Yvinec <ref> [1] </ref> proposed an algorithm to evaluate signs of determinants of 2 fi 2 and 3 fi 3 matrices of p-bit integers using only p and (p + 1)-bit arithmetic, respectively. <p> This paper does not address issues of overflow and underflow, so I allow the exponent to be an integer in the range <ref> [1; 1] </ref>. (Fortunately, many applications have inputs that fall within a circumscribed exponent range and will not overflow or underflow.) See the survey by Goldberg [7] for a detailed explanation of floating-point storage formats, particularly the IEEE 754 standard. <p> Such a tool might even be able to automate the process of breaking an expression into adaptive stages as described in Section 4. It might be fruitful to explore whether the methods described by Clarkson [3] and Avnaim et al. <ref> [1] </ref> can be extended by fast multiprecision methods to handle arbitrary double precision floating-point inputs.
Reference: [2] <author> David H. Bailey. </author> <title> A Portable High Performance Multiprecision Package. </title> <type> Technical Report RNR-90-022, </type> <institution> NASA Ames Research Center, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Most arbitrary precision libraries store numbers in a multiple-digit format, consisting of a sequence of digits (usually of large radix, like 2 32 ) coupled with a single exponent. A freely available example of the multiple-digit approach is Bailey's MPFUN package <ref> [2] </ref>, a sophisticated portable multi-precision library that uses digits of machine-dependent radix (usually 2 24 ) stored as single precision floating-point values. An alternative is the multiple-term format, wherein a number is expressed as a sum of ordinary floating-point words, each with its own significand and exponent [12]. <p> Table 1 lists timings for ORIENT2D, given random inputs. Observe that the adaptive test, when it stops at the approximate result A, takes nearly twice as long as the approximate test because of the need to compute an error bound. The table includes a comparison with Bailey's MPFUN <ref> [2] </ref>, chosen because it is the fastest portable and freely available arbitrary precision package I know of. ORIENT2D coded with my (nonadaptive) algorithms is roughly thirteen times faster than ORIENT2D coded with MPFUN.
Reference: [3] <author> Kenneth L. Clarkson. </author> <title> Safe and Effective Determinant Evaluation. </title> <booktitle> 33rd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 387-395, </pages> <year> 1992. </year>
Reference-contexts: This adaptive approach is described in Section 4, and its application to the orientation and incircle tests is described in Section 5. 2 Related Work There are several exact arithmetic schemes designed specifically for computational geometry; most are methods of exactly evaluating the sign of a determinant. Clarkson <ref> [3] </ref> proposes an algorithm for using floating-point arithmetic to evaluate the sign of the determinant of a small matrix of integers. A variant of the modified Gram-Schmidt procedure is used to improve the conditioning of the matrix, so that the determinant can subsequently be evaluated safely by Gaussian elimination. <p> Such a tool might even be able to automate the process of breaking an expression into adaptive stages as described in Section 4. It might be fruitful to explore whether the methods described by Clarkson <ref> [3] </ref> and Avnaim et al. [1] can be extended by fast multiprecision methods to handle arbitrary double precision floating-point inputs.
Reference: [4] <author> T. J. Dekker. </author> <title> A Floating-Point Technique for Extending the Available Precision. </title> <journal> Numerische Mathematik 18 </journal> <pages> 224-242, </pages> <year> 1971. </year>
Reference-contexts: Theorems 3 and 6 are the key new results. 3.2 Addition An important basic operation in all the algorithms for performing arithmetic with expansions is the addition of two p-bit values to form a nonoverlapping expansion (of length two). Two such algorithms follow. Theorem 1 (Dekker <ref> [4] </ref>) Let a and b be p-bit floating-point numbers such that jaj jbj. <p> The trick is to find a way to split a floating-point value in two. Theorem 4 (Dekker <ref> [4] </ref>) Let a be a p-bit floating-point number, where p 3. <p> By subtracting them from a b in a proper order, one is assured the subtractions are exact and the result is the roundoff error of computing a b. Dekker <ref> [4] </ref> attributes the following method to G. W. Veltkamp. Theorem 5 (Veltkamp) Let a and b be p-bit floating-point numbers, where p 4.
Reference: [5] <author> Steven Fortune and Christopher J. Van Wyk. </author> <title> Efficient Exact Arithmetic for Computational Geometry. </title> <booktitle> Ninth Annual Symposium on Computational Geometry, </booktitle> <pages> pages 163-172, </pages> <month> May </month> <year> 1993. </year> <title> [6] . Static Analysis Yields Efficient Exact Integer Arithmetic for Computational Geometry. </title> <note> To appear in Transactions on Mathematical Software, </note> <year> 1996. </year>
Reference-contexts: The orientation and incircle tests evaluate the sign of a matrix determinant. It is significant that only the sign, and not the magnitude, of the determinant is needed. Fortune and Van Wyk <ref> [5] </ref> take advantage of this fact by using a floating-point filter: the determinant is first evaluated approximately, and only if forward error analysis indicates that the sign of the approximate result cannot be trusted does one use an exact test. <p> Fortune and Van Wyk <ref> [5, 6] </ref> propose a more general approach (not specific to determinants, or even to predicates) that represents integers using a standard exact arithmetic technique with digits of radix 2 23 stored as double precision floating-point values. (53-bit double precision significands make it possible to add several products of 23-bit integers before <p> Because of the nonoverlapping property, APPROXIMATE produces an approximation having error less than the magnitude of the least significant bit of the approximation's significand. 4 Adaptive Precision Arithmetic Exact arithmetic is expensive, and should be avoided when possible. The floating-point filter suggested by Fortune and Van Wyk <ref> [5] </ref>, which tries to verify the correctness of the approximate result (using error analysis) before resorting to exact arithmetic, is quite effective. If the exact test is only needed occasionally, an application can be made robust at only a small cost in speed. <p> Because exact translation is the common case, my adaptive geometric predicates test for and exploit this case. Once a determinant has been chosen for evaluation, there are several methods to evaluate it. A few are surveyed by Fortune and Van Wyk <ref> [5] </ref>; only their conclusion is repeated here. The cheapest method of evaluating the determinant of a 5 fi 5 or smaller matrix seems to be dynamic programming applied to cofactor expansion. <p> The techniques Priest and I have developed are simple enough to be coded directly in numerical algorithms, avoiding function call overhead and conversion costs. A useful tool in coding such algorithms would be an expression compiler similar to Fortune and Van Wyk's <ref> [5] </ref>, which converts an expression into exact arithmetic code, complete with error bound derivation and floating-point filters. Such a tool might even be able to automate the process of breaking an expression into adaptive stages as described in Section 4. <p> Nevertheless, Clarkson's approach looks promising for larger determinants. Although my methods work well for small determinants, they are unlikely to work well for sizes much larger than 5 fi 5. Even if one uses Gaussian elimination rather than cofactor expansion (an important adjustment for larger matrices <ref> [5, 9] </ref>), the adaptivity technique does not scale well with determinants, because of the large number of terms in the expanded polynomial. Clarkson's technique may be the only economical approach for matrices larger than 10 fi 10.
Reference: [7] <author> David Goldberg. </author> <title> What Every Computer Scientist Should Know About Floating-Point Arithmetic. </title> <journal> ACM Computing Surveys 23(1) </journal> <pages> 5-48, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: This paper does not address issues of overflow and underflow, so I allow the exponent to be an integer in the range [1; 1]. (Fortunately, many applications have inputs that fall within a circumscribed exponent range and will not overflow or underflow.) See the survey by Goldberg <ref> [7] </ref> for a detailed explanation of floating-point storage formats, particularly the IEEE 754 standard. Most arbitrary precision libraries store numbers in a multiple-digit format, consisting of a sequence of digits (usually of large radix, like 2 32 ) coupled with a single exponent.
Reference: [8] <author> Leonidas J. Guibas and Jorge Stolfi. </author> <title> Primitives for the Manipulation of General Subdivisions and the Computation of Voronoi Diagrams. </title> <journal> ACM Transactions on Graphics 4(2) </journal> <pages> 74-123, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: The incircle test determines whether a point lies inside, outside, or on a circle or sphere, and is used for Delaunay triangulation <ref> [8] </ref>. Inexact versions of these tests are vulnerable to roundoff error, and the wrong answers they produce can cause geometric algorithms to hang, crash, or produce incorrect output. <p> Fortunately, C is usually accurate enough. 5.4 Triangulation To evaluate the effectiveness of the adaptive tests in applications, I tested them in two of my Delaunay triangulation codes. Triangle [14] is a 2D Delaunay triangulator and mesh generator, publicly available from Netlib, that uses a divide-and-conquer algorithm <ref> [11, 8] </ref>. Pyramid is a 3D Delaunay tetrahedralizer that uses an incremental algorithm [15].
Reference: [9] <author> Michael Karasick, Derek Lieber, and Lee R. Nackman. </author> <title> Efficient De-launay Triangulation Using Rational Arithmetic. </title> <journal> ACM Transactions on Graphics 10(1) </journal> <pages> 71-91, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Fortune and Van Wyk report an order-of-magnitude speed improvement over the use of libraries (for equal bit complexity). Furthermore, the expression compiler garners another speed improvement by installing floating-point filters wherever appropriate, calculating static error bounds automatically. Karasick, Lieber, and Nackman <ref> [9] </ref> report their experiences optimizing a method for determinant evaluation using rational inputs. Their approach reduces the bit complexity 1 The method presented in Clarkson's paper does not work correctly if the determinant is exactly zero, but Clarkson (personal communication) notes that it is easily fixed. <p> Nevertheless, Clarkson's approach looks promising for larger determinants. Although my methods work well for small determinants, they are unlikely to work well for sizes much larger than 5 fi 5. Even if one uses Gaussian elimination rather than cofactor expansion (an important adjustment for larger matrices <ref> [5, 9] </ref>), the adaptivity technique does not scale well with determinants, because of the large number of terms in the expanded polynomial. Clarkson's technique may be the only economical approach for matrices larger than 10 fi 10.
Reference: [10] <author> Donald Ervin Knuth. </author> <booktitle> The Art of Computer Programming: Seminu-merical Algorithms, second edition, </booktitle> <volume> volume 2. </volume> <publisher> Addison Wesley, </publisher> <year> 1981. </year>
Reference-contexts: The difficulty with using FAST-TWO-SUM is the requirement that jaj jbj. If the relative sizes of a and b are unknown, a comparison is required to order the addends before invoking FAST-TWO-SUM. In practice, it is faster on most processors to use the following algorithm. Theorem 2 (Knuth <ref> [10] </ref>) Let a and b be p-bit floating-point numbers, where p 3.
Reference: [11] <author> D. T. Lee and B. J. Schachter. </author> <title> Two Algorithms for Constructing a Delaunay Triangulation. </title> <journal> Int. J. Comput. Inf. Sci. </journal> <volume> 9 </volume> <pages> 219-242, </pages> <year> 1980. </year>
Reference-contexts: Fortunately, C is usually accurate enough. 5.4 Triangulation To evaluate the effectiveness of the adaptive tests in applications, I tested them in two of my Delaunay triangulation codes. Triangle [14] is a 2D Delaunay triangulator and mesh generator, publicly available from Netlib, that uses a divide-and-conquer algorithm <ref> [11, 8] </ref>. Pyramid is a 3D Delaunay tetrahedralizer that uses an incremental algorithm [15].
Reference: [12] <author> Douglas M. Priest. </author> <title> Algorithms for Arbitrary Precision Floating Point Arithmetic. </title> <booktitle> Tenth Symposium on Computer Arithmetic, </booktitle> <pages> pages 132-143, </pages> <year> 1991. </year> <title> [13] . On Properties of Floating Point Arithmetics: Numerical Stability and the Cost of Accurate Computations. </title> <type> Ph.D. thesis, </type> <institution> Department of Mathematics, University of California at Berkeley, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Second, there is no cost to convert floating-point numbers to a specialized extended precision format. The method has its greatest advantage in computations that process values of extended but small precision (several hundred or thousand bits), and seems ideal for computational geometry. The method was largely developed by Priest <ref> [12, 13] </ref>, who designed similar algorithms that run on a wide variety of floating-point architectures, with different radices and rounding behavior. I have made significant speed improvements by relaxing Priest's normalization requirement and optimizing for radix two with exact rounding. <p> An alternative is the multiple-term format, wherein a number is expressed as a sum of ordinary floating-point words, each with its own significand and exponent <ref> [12] </ref>. <p> Algorithms for addition and multiplication of expansions follow. The (rather lengthy) proofs of all theorems are omitted, but are available in a full-length version of this paper. 2 Note that this definition of expansion is slightly different from that used by Priest <ref> [12] </ref>; whereas Priest requires that the exponents of any two components of an expansion differ by at least p, no such requirement is made here. 3 Formally, x and y are nonoverlapping if there exist integers r and s such that x = r2 s and jyj &lt; 2 s , <p> For simplicity, versions without zero elimination are presented here, but my implementations eliminate zeros. Priest <ref> [12] </ref> presents a similar algorithm (for processors with arbitrary floating-point radix) that guarantees that the components of the output expansion overlap by at most one digit (i.e. one bit in binary arithmetic). An expensive renor-malization step is required afterward to remove the overlap.
Reference: [14] <author> Jonathan Richard Shewchuk. </author> <title> Triangle: Engineering a 2D Quality Mesh Generator and Delaunay Triangulator. </title> <booktitle> First Workshop on Applied Computational Geometry. Association for Computing Machinery, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: All determinants are Expression 1 or 3, or the more stable Expression 2 or 4, as indicated. Fortunately, C is usually accurate enough. 5.4 Triangulation To evaluate the effectiveness of the adaptive tests in applications, I tested them in two of my Delaunay triangulation codes. Triangle <ref> [14] </ref> is a 2D Delaunay triangulator and mesh generator, publicly available from Netlib, that uses a divide-and-conquer algorithm [11, 8]. Pyramid is a 3D Delaunay tetrahedralizer that uses an incremental algorithm [15].
Reference: [15] <author> David F. Watson. </author> <title> Computing the n-dimensional Delaunay Tessellation with Application to Voronoi Polytopes. </title> <journal> Computer Journal 24 </journal> <pages> 167-172, </pages> <year> 1981. </year>
Reference-contexts: Triangle [14] is a 2D Delaunay triangulator and mesh generator, publicly available from Netlib, that uses a divide-and-conquer algorithm [11, 8]. Pyramid is a 3D Delaunay tetrahedralizer that uses an incremental algorithm <ref> [15] </ref>. For both 2D and 3D, three types of inputs were tested: uniform random points, points lying (approximately) on the boundary of a circle or sphere, and a square or cubic grid of lattice points, tilted so as not to be aligned with the coordinate axes.
References-found: 13

