URL: http://suif.stanford.edu/papers/lim97.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Email: faimee, lamg@cs.stanford.edu  
Title: Maximizing Parallelism and Minimizing Synchronization with Affine Transforms  
Author: Amy W. Lim and Monica S. Lam 
Address: Stanford, CA 94305  
Affiliation: Computer Systems Laboratory Stanford University  
Abstract: This paper presents the first algorithm to find the optimal affine transform that maximizes the degree of parallelism while minimizing the degree of synchronization in a program with arbitrary loop nestings and affine data accesses. The problem is formulated without the use of imprecise data dependence abstractions such as data dependence vectors. The algorithm presented subsumes previously proposed program transformation algorithms that are based on unimod-ular transformations, loop fusion, fission, scaling, reindexing and/or statement reordering. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Conference Record of the Fourteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 63-76, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination [20] can be used to generate the desired SPMD (Single Program Multiple Data) code. The technique of affine partitioning can find all the loop level parallelism exposed through a combination of many previously defined transformations <ref> [1, 4, 6, 7, 16, 17, 19, 21, 22, 23] </ref>, including * loop fission (or loop distribution), * loop fusion, * unimodular transforms (interchange, reversal, skew ing), * loop scaling, * loop reindexing (or index set shifting), and * statement reordering. The rest of the paper is organized as follows. <p> Example 5: for l 1 = 1 to 100 do a [l 1 ; l 2 ] = a [l 1 ; l 2 ] + b [l 1 1; l 2 ]; <ref> [1] </ref> In Figure 2, we show four iterations from each of the loops. Each iteration of the innermost loop is represented by a pair of nodes, with the white node representing an operation of instruction 1 and the black representing an operation of instruction 2. <p> The SPMD code generated from the space-partition mapping is as follows: (p denotes the processor's id) if (p = 99) then a <ref> [1; 100] </ref> = a [1; 100] + b [0; 100]; if (98 p 99) then if (1 p 99) then b [p; 1] = a [p; 0] fl b [p; 1]; 5 Each equation is normalized using the GCD of its coefficients. 7 for l 1 = max (1; 1 + <p> The SPMD code generated from the space-partition mapping is as follows: (p denotes the processor's id) if (p = 99) then a <ref> [1; 100] </ref> = a [1; 100] + b [0; 100]; if (98 p 99) then if (1 p 99) then b [p; 1] = a [p; 0] fl b [p; 1]; 5 Each equation is normalized using the GCD of its coefficients. 7 for l 1 = max (1; 1 + p) to min (100; <p> The SPMD code generated from the space-partition mapping is as follows: (p denotes the processor's id) if (p = 99) then a [1; 100] = a [1; 100] + b [0; 100]; if (98 p 99) then if (1 p 99) then b <ref> [p; 1] </ref> = a [p; 0] fl b [p; 1]; 5 Each equation is normalized using the GCD of its coefficients. 7 for l 1 = max (1; 1 + p) to min (100; 99 + p) b [l 1 ; l 1 p + 1] = a [l 1 ; <p> SPMD code generated from the space-partition mapping is as follows: (p denotes the processor's id) if (p = 99) then a [1; 100] = a [1; 100] + b [0; 100]; if (98 p 99) then if (1 p 99) then b <ref> [p; 1] </ref> = a [p; 0] fl b [p; 1]; 5 Each equation is normalized using the GCD of its coefficients. 7 for l 1 = max (1; 1 + p) to min (100; 99 + p) b [l 1 ; l 1 p + 1] = a [l 1 ; l 1 p] fl b [l 1 ; <p> 1 p + 1] = a [l 1 ; l 1 p] fl b [l 1 ; l 1 p + 1]; if (98 p 0) then a [100 + p; 100] = a [100 + p; 100] + b [99 + p; 100]; if (p = 100) then b <ref> [100; 1] </ref> = a [100; 0] fl b [100; 1]; 6 Parallelism With O (1) Synchronizations To find parallelism that requires only a constant amount of synchronization, we use a program dependence graph, where all dynamic instances of the same instruction are represented by a single node. <p> ; l 1 p] fl b [l 1 ; l 1 p + 1]; if (98 p 0) then a [100 + p; 100] = a [100 + p; 100] + b [99 + p; 100]; if (p = 100) then b <ref> [100; 1] </ref> = a [100; 0] fl b [100; 1]; 6 Parallelism With O (1) Synchronizations To find parallelism that requires only a constant amount of synchronization, we use a program dependence graph, where all dynamic instances of the same instruction are represented by a single node. <p> ID is denoted by p; the ith wait (q) executed by processor p stalls its execution until processor q executes the ith signal (p). if (1 p n 1) then if p &gt; 1 then wait (p 1) a [p; 0] += b [0; 0] fl a [p 1; 0]; <ref> [1] </ref> if p &lt; n 1 then signal (p + 1) for k 1 = 1 to n 1 do if p &gt; 1 then wait (p 1) for k 2 = 0 to k 1 1 do a [p; k 1 ] -= b [n 1 k 2 ; k
Reference: [2] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In Proceedings of the Third ACM/SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 39-50, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Our code generation algorithm is based on Ancourt and Irigoin's polyhedron-scanning code generation technique <ref> [2] </ref>; details and examples can be found in [18]. <p> Consider the following example of an outer sequential loop: Example 6: for l 1 = 1 to n 1 do for l 3 = 0 to n 1 l 2 do for l 0 a [l 1 ; l 0 3 ] fl a [l 1 ; l 2 ]; <ref> [2] </ref> Instruction 1 modifies each l 1 th row of matrix a using the lower triangular elements in matrix b, and instruction 2 updates a [l 1 ] using the upper triangular portion of b. Let us first consider how previous loop transformation algorithms would parallelize this code. <p> + 1) for k 1 = 1 to n 1 do if p &gt; 1 then wait (p 1) for k 2 = 0 to k 1 1 do a [p; k 1 ] -= b [n 1 k 2 ; k 1 ] fl a [p; k 2 ]; <ref> [2] </ref> if p &lt; n 1 then signal (p + 1) 11 The computation assigned to processor p is simply the pth iteration of the outermost loop. The execution order on each processor is illustrated in Figure 3.
Reference: [3] <author> J. M. Anderson and M. S. Lam. </author> <title> Global optimiza tions for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112-125, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Huang and Sadayappan find one degree of communication-free parallelism from a sequence of perfectly nested loops [12]. Anderson and Lam apply unimodular transforms to loop nests to increase the granularity of parallelism, find the maximum degree of communication-free parallelism across loops, and heuristically introduce communication where necessary <ref> [3] </ref>. Bau et al. propose an improved algorithm to find communi-cation-free parallelism [8]. While all the algorithms above treat operations within each loop iteration as indivisible, we previously presented a communication-free paralleliza-tion algorithm that transforms instructions individually [18].
Reference: [4] <author> E. Ayguade and J. Torres. </author> <title> Partitioning the statement per iteration space using non-singular matrices. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <pages> pages 407-415, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination [20] can be used to generate the desired SPMD (Single Program Multiple Data) code. The technique of affine partitioning can find all the loop level parallelism exposed through a combination of many previously defined transformations <ref> [1, 4, 6, 7, 16, 17, 19, 21, 22, 23] </ref>, including * loop fission (or loop distribution), * loop fusion, * unimodular transforms (interchange, reversal, skew ing), * loop scaling, * loop reindexing (or index set shifting), and * statement reordering. The rest of the paper is organized as follows.
Reference: [5] <author> D. Bacon, S. Graham, and O. Sharp. </author> <title> Compiler trans formations for high-performance computing. </title> <journal> Computing Surveys, </journal> <volume> 26(4) </volume> <pages> 345-420, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: The domain of such techniques is generally limited to loops, whose loop bounds and array accesses are affine functions of loop indices. Many forms of program transformations have been proposed, a catalog of which can be found in a survey by Bacon, Graham, and Sharp <ref> [5] </ref>. The key challenge that remains is how to combine these transformations optimally to achieve particular goals. This paper presents an algorithm that finds the maximum degree of parallelism in the general program domain of arbitrarily nested loops and array accesses with affine index expressions.
Reference: [6] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 192-219, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination [20] can be used to generate the desired SPMD (Single Program Multiple Data) code. The technique of affine partitioning can find all the loop level parallelism exposed through a combination of many previously defined transformations <ref> [1, 4, 6, 7, 16, 17, 19, 21, 22, 23] </ref>, including * loop fission (or loop distribution), * loop fusion, * unimodular transforms (interchange, reversal, skew ing), * loop scaling, * loop reindexing (or index set shifting), and * statement reordering. The rest of the paper is organized as follows. <p> Loop interchanges, reversals, and skewing, and combinations thereof, have been modeled as unimodular transforma-tions <ref> [6, 21, 22] </ref>, and various algorithms based on this frame work have been developed. Under this framework, the desired combination of loop transformations is achieved by finding the suitable unimodular matrix, which can then be used to generate the desired SPMD code in a straightforward manner.
Reference: [7] <author> U. Banerjee. </author> <title> Loop Transformations for Restructuring Compilers. </title> <publisher> Kluwer Academic, </publisher> <year> 1993. </year>
Reference-contexts: From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination [20] can be used to generate the desired SPMD (Single Program Multiple Data) code. The technique of affine partitioning can find all the loop level parallelism exposed through a combination of many previously defined transformations <ref> [1, 4, 6, 7, 16, 17, 19, 21, 22, 23] </ref>, including * loop fission (or loop distribution), * loop fusion, * unimodular transforms (interchange, reversal, skew ing), * loop scaling, * loop reindexing (or index set shifting), and * statement reordering. The rest of the paper is organized as follows.
Reference: [8] <author> D. Bau, I. Kodukula, V. Kotlyar, K. Pingali, and P. Stodghill. </author> <title> Solving alignment using elementary linear algebra. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 46-60. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: Anderson and Lam apply unimodular transforms to loop nests to increase the granularity of parallelism, find the maximum degree of communication-free parallelism across loops, and heuristically introduce communication where necessary [3]. Bau et al. propose an improved algorithm to find communi-cation-free parallelism <ref> [8] </ref>. While all the algorithms above treat operations within each loop iteration as indivisible, we previously presented a communication-free paralleliza-tion algorithm that transforms instructions individually [18].
Reference: [9] <author> P. Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, part I, one dimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(5) </volume> <pages> 313-348, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The objective is to find affine mappings that yield maximum parallelism among operations within each of the time partitions. The space-partition and time-partition constraints are similar in many ways and are amenable to the same kind of techniques. We use the affine form of the Farkas lemma <ref> [9] </ref> to transform the constraints into systems of linear inequalities. The problem of finding a partition mapping that gives the maximum degree of loop-level and pipelined parallelism while satisfying the space-partition or time-partition constraints reduces to finding the null space of a system of equations. <p> G 0 ~y 0 1 = 0 (4) where vector ~y 0 has less variables than ~y. Step 2: Reduce the system in (4) to a set of linear equations, using the Affine Form of the Farkas Lemma <ref> [9] </ref>. Lemma 5.1 (Affine Form of the Farkas Lemma) 3 For synchronization-free parallelism, it is not necessary to distinguish between the direction of the data dependence in R. The redundancy is included in the constraints to emphasize the similarity with the definition in Section 7. <p> Unimodular transformations cannot achieve the effects that can be obtained via loop fission, fusion, scaling, reindexing, or statement reordering. Feautrier finds a piece-wise affine schedule that minimizes the overall execution time of a program <ref> [9, 10] </ref>, and tries to minimize communication after parallelization [11]. For each instruction, he finds a piece-wise affine mapping that maps its loop indices to a multi-dimensional time index. He uses a heuristic based on parametric integer programming to minimize the dimensionality of time.
Reference: [10] <author> P. Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, part II, multidimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(6), </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: Unimodular transformations cannot achieve the effects that can be obtained via loop fission, fusion, scaling, reindexing, or statement reordering. Feautrier finds a piece-wise affine schedule that minimizes the overall execution time of a program <ref> [9, 10] </ref>, and tries to minimize communication after parallelization [11]. For each instruction, he finds a piece-wise affine mapping that maps its loop indices to a multi-dimensional time index. He uses a heuristic based on parametric integer programming to minimize the dimensionality of time.
Reference: [11] <author> P. Feautrier. </author> <title> Towards automatic distribution. </title> <type> Technical Report 92.95, </type> <institution> Institut Blaise Pascal/Laboratoire MASI, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Unimodular transformations cannot achieve the effects that can be obtained via loop fission, fusion, scaling, reindexing, or statement reordering. Feautrier finds a piece-wise affine schedule that minimizes the overall execution time of a program [9, 10], and tries to minimize communication after parallelization <ref> [11] </ref>. For each instruction, he finds a piece-wise affine mapping that maps its loop indices to a multi-dimensional time index. He uses a heuristic based on parametric integer programming to minimize the dimensionality of time. Minimizing the dimensionality of a time schedule corresponds to maximizing the degree of parallelism.
Reference: [12] <author> C. H. Huang and P. Sadayappan. </author> <title> Communication-free hyperplane partitioning of nested loops. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 19(2) </volume> <pages> 90-102, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Several algorithms have been proposed for solving the subproblem of finding communication-free parallelism. Huang and Sadayappan find one degree of communication-free parallelism from a sequence of perfectly nested loops <ref> [12] </ref>. Anderson and Lam apply unimodular transforms to loop nests to increase the granularity of parallelism, find the maximum degree of communication-free parallelism across loops, and heuristically introduce communication where necessary [3]. Bau et al. propose an improved algorithm to find communi-cation-free parallelism [8].
Reference: [13] <author> W. Kelly and W. Pugh. </author> <title> Determining schedules based on performance estimation. </title> <type> Technical Report CS-TR-3108, </type> <institution> University of Maryland, </institution> <year> 1993. </year>
Reference-contexts: Code generation in our framework is straightforward because the schedules we find are more regular. Kelly and Pugh presented a framework for unifying iteration reordering transformations [14]. They described a way to find an affine mapping for each instruction based on performance estimation <ref> [13] </ref>. Recently, they described an algorithm to minimize communication while preserving parallelism [15]. Their algorithm finds only one dimension of parallelism, rather than all the possible degrees of parallelism. It estimates the parallelism and the communication cost for all legal loop permutations and reversals for each instruction.
Reference: [14] <author> W. Kelly and W. Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report CS-TR-2995.1, </type> <institution> University of Maryland, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: In addition, we can also generate pipeline parallelism that has better locality and less synchronization overhead. Code generation in our framework is straightforward because the schedules we find are more regular. Kelly and Pugh presented a framework for unifying iteration reordering transformations <ref> [14] </ref>. They described a way to find an affine mapping for each instruction based on performance estimation [13]. Recently, they described an algorithm to minimize communication while preserving parallelism [15]. Their algorithm finds only one dimension of parallelism, rather than all the possible degrees of parallelism.
Reference: [15] <author> W. Kelly and W. Pugh. </author> <title> Minimizing communication while preserving parallelism. </title> <booktitle> In Proceedings of the 1996 ACM International Conference on Supercomputing, </booktitle> <pages> pages 52-60, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Kelly and Pugh presented a framework for unifying iteration reordering transformations [14]. They described a way to find an affine mapping for each instruction based on performance estimation [13]. Recently, they described an algorithm to minimize communication while preserving parallelism <ref> [15] </ref>. Their algorithm finds only one dimension of parallelism, rather than all the possible degrees of parallelism. It estimates the parallelism and the communication cost for all legal loop permutations and reversals for each instruction.
Reference: [16] <author> K. Kennedy and K. S. McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <pages> pages 323-334, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination [20] can be used to generate the desired SPMD (Single Program Multiple Data) code. The technique of affine partitioning can find all the loop level parallelism exposed through a combination of many previously defined transformations <ref> [1, 4, 6, 7, 16, 17, 19, 21, 22, 23] </ref>, including * loop fission (or loop distribution), * loop fusion, * unimodular transforms (interchange, reversal, skew ing), * loop scaling, * loop reindexing (or index set shifting), and * statement reordering. The rest of the paper is organized as follows.
Reference: [17] <author> K. Kennedy and K. S. McKinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 301-320. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination [20] can be used to generate the desired SPMD (Single Program Multiple Data) code. The technique of affine partitioning can find all the loop level parallelism exposed through a combination of many previously defined transformations <ref> [1, 4, 6, 7, 16, 17, 19, 21, 22, 23] </ref>, including * loop fission (or loop distribution), * loop fusion, * unimodular transforms (interchange, reversal, skew ing), * loop scaling, * loop reindexing (or index set shifting), and * statement reordering. The rest of the paper is organized as follows.
Reference: [18] <author> A. W. Lim and M. S. Lam. </author> <title> Communication-free par allelization via affine transformations. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 92-106. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: Our code generation algorithm is based on Ancourt and Irigoin's polyhedron-scanning code generation technique [2]; details and examples can be found in <ref> [18] </ref>. <p> Bau et al. propose an improved algorithm to find communi-cation-free parallelism [8]. While all the algorithms above treat operations within each loop iteration as indivisible, we previously presented a communication-free paralleliza-tion algorithm that transforms instructions individually <ref> [18] </ref>. The synchronization-free algorithm presented in this paper is simpler and more direct, made possible by the application of the Farkas lemma. 12 10 Conclusion This paper presents an algorithm to find the optimal affine mapping that maximizes the degree of parallelism while minimizing the degree of synchronization.
Reference: [19] <author> V. Sarkar and R. Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 175-187, </pages> <month> June </month> <year> 1992. </year> <month> 13 </month>
Reference-contexts: From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination [20] can be used to generate the desired SPMD (Single Program Multiple Data) code. The technique of affine partitioning can find all the loop level parallelism exposed through a combination of many previously defined transformations <ref> [1, 4, 6, 7, 16, 17, 19, 21, 22, 23] </ref>, including * loop fission (or loop distribution), * loop fusion, * unimodular transforms (interchange, reversal, skew ing), * loop scaling, * loop reindexing (or index set shifting), and * statement reordering. The rest of the paper is organized as follows.
Reference: [20] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Program ming. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1986. </year>
Reference-contexts: The algorithm can be used to find all the parallelism at the coarsest granularity, or just up to the degree needed to exploit a particular parallel hardware configuration. From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination <ref> [20] </ref> can be used to generate the desired SPMD (Single Program Multiple Data) code. <p> Since (7) holds for all ~y 0 , XE = 1 m 0 0 0 1 fi fl T The unknowns in the new constraints (8) are X, 0 ; , m . We are interested only in X, so we apply Fourier-Motzkin elimination <ref> [20] </ref> to eliminate all the Farkas multipliers to obtain a set of constraints in the form of XA ~ 0 (9) Renaming X in (6) as X 0 and applying the Farkas Lemma and Fourier Motzkin elimination as above, we get X A ~ 0 XA ~ 0 (10) Combining (10)
Reference: [21] <author> M. E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year> <note> Published as CSL-TR-92-538. </note>
Reference-contexts: From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination [20] can be used to generate the desired SPMD (Single Program Multiple Data) code. The technique of affine partitioning can find all the loop level parallelism exposed through a combination of many previously defined transformations <ref> [1, 4, 6, 7, 16, 17, 19, 21, 22, 23] </ref>, including * loop fission (or loop distribution), * loop fusion, * unimodular transforms (interchange, reversal, skew ing), * loop scaling, * loop reindexing (or index set shifting), and * statement reordering. The rest of the paper is organized as follows. <p> Loop interchanges, reversals, and skewing, and combinations thereof, have been modeled as unimodular transforma-tions <ref> [6, 21, 22] </ref>, and various algorithms based on this frame work have been developed. Under this framework, the desired combination of loop transformations is achieved by finding the suitable unimodular matrix, which can then be used to generate the desired SPMD code in a straightforward manner.
Reference: [22] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation the ory and an algorithm to maximize parallelism. </title> <journal> Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-470, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination [20] can be used to generate the desired SPMD (Single Program Multiple Data) code. The technique of affine partitioning can find all the loop level parallelism exposed through a combination of many previously defined transformations <ref> [1, 4, 6, 7, 16, 17, 19, 21, 22, 23] </ref>, including * loop fission (or loop distribution), * loop fusion, * unimodular transforms (interchange, reversal, skew ing), * loop scaling, * loop reindexing (or index set shifting), and * statement reordering. The rest of the paper is organized as follows. <p> Loop interchanges, reversals, and skewing, and combinations thereof, have been modeled as unimodular transforma-tions <ref> [6, 21, 22] </ref>, and various algorithms based on this frame work have been developed. Under this framework, the desired combination of loop transformations is achieved by finding the suitable unimodular matrix, which can then be used to generate the desired SPMD code in a straightforward manner.
Reference: [23] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercom puters. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: From these affine partition mappings, a straightforward algorithm based on Fourier-Motzkin elimination [20] can be used to generate the desired SPMD (Single Program Multiple Data) code. The technique of affine partitioning can find all the loop level parallelism exposed through a combination of many previously defined transformations <ref> [1, 4, 6, 7, 16, 17, 19, 21, 22, 23] </ref>, including * loop fission (or loop distribution), * loop fusion, * unimodular transforms (interchange, reversal, skew ing), * loop scaling, * loop reindexing (or index set shifting), and * statement reordering. The rest of the paper is organized as follows.
References-found: 23

