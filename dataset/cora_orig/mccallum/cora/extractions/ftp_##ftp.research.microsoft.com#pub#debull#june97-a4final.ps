URL: ftp://ftp.research.microsoft.com/pub/debull/june97-a4final.ps
Refering-URL: http://www.research.microsoft.com/research/db/debull/issues-list.htm
Root-URL: http://www.research.microsoft.com
Title: Parallel Processing Capabilities of Sybase Adaptive Server Enterprise 11.5  
Author: Jhingran, Timothy Malkemus, and Sriram Padmanabhan Eugene Ding, Lucien Dimino, Ganesan Gopal, and T.K. Rengarajan 
Note: Bulletin of the Technical Committee on Data Engineering June, 1997 Vol. 20 No. 2 IEEE Computer Society Letters Letter from the Editor-in-Chief. .David Lomet 1 Letter from the Special Issue Editor. .Betty Salzberg 2 Special Issue on Commercial Parallel Systems Anant  Conference and Journal Notices TCDE Nomination Form 44 1997 Very Large Data Bases Conference back cover  
Abstract: Born to be Parallel. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Carrie Ballinger and Ron Fryer 3 Parallel Solutions in ClustRa . . . . Svein Erik Bratsberg, Svein-Olaf Hvassovd, and ystein Torbjtrnsen 13 XPS: A High Performance Parallel Database Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Chendong Zou 21 Query Optimization in DB2 Parallel Edition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bratsberg, S.E., Hvasshovd, S.-O., and Torbjtrnsen, </author> . <title> Location and replication independent recovery in a highly available database. </title> <booktitle> In 15th British National Conference on Databases. </booktitle> <publisher> Springer-Verlag LNCS, </publisher> <month> July </month> <year> 1997. </year>
Reference-contexts: To achieve this flexibility, we cannot impose any restrictions on the physical allocation and representation of data inside a node. Therefore, the tuples and tuple log records are completely location transparent. This is done by using logical logging and state identifiers connected to tuples <ref> [1] </ref>. The fragment replicas consist of both the tuples themselves and the associated tuple log records. The log records are at any time only generated at the primary replica and sent to the hot standbys, ensuring sequence preservation in the logs.
Reference: [2] <author> Copeland, G., and Keller, T. </author> <title> A comparison of high-availability media recovery techniques. </title> <booktitle> In proceedings of the ACM SIGMOD Conference, </booktitle> <pages> pages 98-109, </pages> <month> June </month> <year> 1989. </year>
Reference: [3] <author> El Abbadi, A., Skeen, D., and Cristian, F. </author> <title> An efficient, fault-tolerant protocol for replicated data management. </title> <editor> In M. Stonebraker, editor, </editor> <booktitle> Readings in Database Systems, </booktitle> <pages> pages 259-273. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: If node 11 in Figure 2 failed, node 0 and 3 will take over its primary responsibility (for fragments 35 and 48). This balances the extra load between two nodes. Before takeover, the virtual node set protocol <ref> [3] </ref> informs the other nodes that a node has failed. The update channels from the failed node will be closed and all enqueued log records from the failed node will be redone.
Reference: [4] <author> Garcia-Molina, H., Polyzois, C.A., and Hagmann, R.B. </author> <title> Two epoch algorithms for disaster recovery. </title> <booktitle> In proceedings of the 16th International Conference on Very Large Databases, </booktitle> <pages> pages 222-230, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: This will require concatenation of subfragments and their corresponding logs. Creating a transaction consistent image of a backed-up database is a problem quite similar to obtaining a 1-safe consistent database after a site crash. We use the single mark variant of the epoch algorithm <ref> [4] </ref> method to obtain consistency. The same problem occurs between fragments of a table in case a table consistent backup is to be produced. 8 Scaling and Refragmentation It is of great importance for continuously available database servers to be able to do management operations like scaleup without interrupting services.
Reference: [5] <author> Gray, J., and Reuter, A. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference: [6] <author> Hvasshovd, S.-O. </author> <title> Recovery in Parallel Database Systems. </title> <publisher> Vieweg, </publisher> <year> 1996. </year>
Reference-contexts: After a following takeover, all new log records are entered into a separate log and they are given log sequence numbers that are higher than the log sequence numbers in all the original fragment logs. The old logs will be removed as part of the checkpointing policy <ref> [6] </ref>. 5 Parallel Takeover When a node has failed, takeover will happen. This means that one or more nodes at another site will take over the primary responsibility for the fragments which resided at the failed node.
Reference: [7] <author> Hvasshovd, S.-O., Torbjtrnsen, ., Bratsberg, S.E., and Holager, P. </author> <title> The ClustRa telecom database: High availability, high throughput, and real-time response. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Databases, </booktitle> <pages> pages 469-477, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Parallelism is used to achieve scalability, where much focus has been put into query processing and optimization. ClustRa is aimed at telecommunication applications, where transactions are simple, but where there is a demand for high availability and soft real-time response <ref> [7] </ref>. In addition to catering for increased load and data volume, ClustRa uses parallelism in all situations requiring a highly available database. Parallelism is important for high availability, because failure masking situations must be done as fast and smoothly as possible. <p> Therefore, parallel takeover makes the unavailability interval shorter. However, ClustRa does eager redo at hot standbys so that takeover may be performed as fast as possible. It has been measured to be on the order of 1/3 second using off-the-shelf workstations <ref> [7] </ref>. 6 Online Self-Repair When a node has failed and is not able to recover, online self-repair is started. The purpose of online self-repair is to reestablish the availability level without degrading the response time and the current availability of the database.
Reference: [8] <author> Sockut, G.H., and Iyer, B.R. </author> <title> A survey of online reorganization in IBM products and research. </title> <journal> IEEE Data Engineering Bulletin, </journal> <volume> 19(2) </volume> <pages> 4-11, </pages> <year> 1996. </year>
Reference-contexts: RAIDs) and shared disk solutions to mask single component failures. DB2 for MVS/ESA moves in the direction of supporting self-repair, since it allows for online reorganization to restore clustering of tables <ref> [8] </ref>. Both reads and writes are allowed during copy. The new copy is made up-to-date by replaying the log created during the copy phase.

Reference: [DG90] <author> D. DeWitt and J. Gray. </author> <title> Parallel database systems: The future of database processing or a passing fad? sigmod, </title> <booktitle> 19(4) </booktitle> <pages> 104-112, </pages> <month> December </month> <year> 1990. </year>
Reference: [Gra90] <author> Goetz Graefe. Volcano, </author> <title> an Extensible and Parallel Query Evaluation System. </title> <type> Technical Report CU-CS-481-90, </type> <institution> University of Colorado at Boulder, </institution> <year> 1990. </year>
Reference: [Sch96] <author> Bob Schaller. </author> <title> The origin, nature, and implications of "moore's law" the benchmark of progress in semiconductor electronics, </title> <note> 1996 http://131.107.1.182/research/BARC/Gray/Moore Law.html </note>

Reference: [1] <author> C. K. Baru et al. </author> <title> DB2 Parallel Edition. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 292-322, </pages> <year> 1995. </year>
Reference-contexts: To achieve this flexibility, we cannot impose any restrictions on the physical allocation and representation of data inside a node. Therefore, the tuples and tuple log records are completely location transparent. This is done by using logical logging and state identifiers connected to tuples <ref> [1] </ref>. The fragment replicas consist of both the tuples themselves and the associated tuple log records. The log records are at any time only generated at the primary replica and sent to the hot standbys, ensuring sequence preservation in the logs.
Reference: [2] <author> C.K. Baru and S. Padmanabhan. </author> <title> Join and Data Redistribution algorithms for Hypercubes. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(1) </volume> <pages> 161-168, </pages> <month> February </month> <year> 1993. </year>
Reference: [3] <author> H. Boral et al. </author> <title> Prototyping Bubba, a highly parallel database system. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 4-24, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: If node 11 in Figure 2 failed, node 0 and 3 will take over its primary responsibility (for fragments 35 and 48). This balances the extra load between two nodes. Before takeover, the virtual node set protocol <ref> [3] </ref> informs the other nodes that a node has failed. The update channels from the failed node will be closed and all enqueued log records from the failed node will be redone.
Reference: [4] <author> G. Copeland et al. </author> <title> Data placement in Bubba. </title> <booktitle> In Proceedings of the 1988 ACM SIGMOD Conference, </booktitle> <pages> pages 99-108, </pages> <address> Chicago, IL, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: This will require concatenation of subfragments and their corresponding logs. Creating a transaction consistent image of a backed-up database is a problem quite similar to obtaining a 1-safe consistent database after a site crash. We use the single mark variant of the epoch algorithm <ref> [4] </ref> method to obtain consistency. The same problem occurs between fragments of a table in case a table consistent backup is to be produced. 8 Scaling and Refragmentation It is of great importance for continuously available database servers to be able to do management operations like scaleup without interrupting services.
Reference: [5] <author> D.J. DeWitt et al. </author> <title> The Gamma database machine project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 44-62, </pages> <month> March </month> <year> 1990. </year>
Reference: [6] <author> S. Ganguly, W. Hasan, and R. Krishnamurthy. </author> <title> Query optimization for parallel execution. </title> <booktitle> In Proceedings of the 1992 ACM SIGMOD COnference, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: After a following takeover, all new log records are entered into a separate log and they are given log sequence numbers that are higher than the log sequence numbers in all the original fragment logs. The old logs will be removed as part of the checkpointing policy <ref> [6] </ref>. 5 Parallel Takeover When a node has failed, takeover will happen. This means that one or more nodes at another site will take over the primary responsibility for the fragments which resided at the failed node.
Reference: [7] <author> S. Ghandeharizadeh and D.J. Dewitt. </author> <title> Performance analysis of alternative declustering strategies. </title> <booktitle> In Proceedings of 6th Intl. Conference on Data Engineering, </booktitle> <pages> pages 466-475, </pages> <address> Los Angeles, CA, </address> <month> February </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Parallelism is used to achieve scalability, where much focus has been put into query processing and optimization. ClustRa is aimed at telecommunication applications, where transactions are simple, but where there is a demand for high availability and soft real-time response <ref> [7] </ref>. In addition to catering for increased load and data volume, ClustRa uses parallelism in all situations requiring a highly available database. Parallelism is important for high availability, because failure masking situations must be done as fast and smoothly as possible. <p> Therefore, parallel takeover makes the unavailability interval shorter. However, ClustRa does eager redo at hot standbys so that takeover may be performed as fast as possible. It has been measured to be on the order of 1/3 second using off-the-shelf workstations <ref> [7] </ref>. 6 Online Self-Repair When a node has failed and is not able to recover, online self-repair is started. The purpose of online self-repair is to reestablish the availability level without degrading the response time and the current availability of the database.
Reference: [8] <author> T. Haerder and A. Reuter. </author> <title> Principles of transaction-oriented database recovery. </title> <journal> ACM Computing Surveys, </journal> <volume> 15(4), </volume> <year> 1983. </year>
Reference-contexts: RAIDs) and shared disk solutions to mask single component failures. DB2 for MVS/ESA moves in the direction of supporting self-repair, since it allows for online reorganization to restore clustering of tables <ref> [8] </ref>. Both reads and writes are allowed during copy. The new copy is made up-to-date by replaying the log created during the copy phase.
Reference: [9] <author> W. Hong and M. Stonebraker. </author> <title> Optimization of parallel query execution plans in XPRS. </title> <booktitle> In Proceedings of the 1991 PDIS Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Other declustering strategies lead to more complicated allocation schemes where fragment replicas stored at one node at one site is distributed over multiple nodes at the other. Figure 2 illustrates minimum intersecting sets declustering <ref> [9] </ref>. This strategy is easiest to understand by viewing the identifiers of fragment replicas as columns in matrixes. When comparing the matrixes for the two sites, the first column is equal, the second column is rotated one position down, the third column is rotated two positions down, and so on.
Reference: [10] <author> K. A. Hua and C. Lee. </author> <title> An adaptive data placement scheme for parallel database computer systems. </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <pages> pages 493-506. </pages> <publisher> Morgan Kaufman, </publisher> <month> August </month> <year> 1990. </year>
Reference: [11] <author> S. Khoshafian and P. Valduriez. </author> <title> Parallel execution strategies for declustered databases. </title> <editor> In M. Kit-suregawa and H. Tanaka, editors, </editor> <booktitle> Database Machines and Knowledge base Machines, </booktitle> <pages> pages 458-471. </pages> <publisher> Kluwer Acad. Publishers (Boston, </publisher> <address> MA), </address> <year> 1988. </year>
Reference: [12] <author> S. Padmanabhan. </author> <title> Data Placement in Shared-Nothing Parallel Database Systems. </title> <type> PhD thesis, </type> <institution> EECS Department, The University of Michigan, </institution> <address> Ann Arbor, MI 48109, </address> <year> 1992. </year>
Reference: [13] <author> D. Schneider and D.J. DeWitt. </author> <title> Tradeoffs in processing complex join queries via hashing in multiprocessor database machines. </title> <booktitle> In Proceedings of the 16th International Conference on Very Large Data Bases, </booktitle> <pages> pages 469-481, </pages> <address> Brisbane, Australia, 1990. </address> <publisher> Morgan Kaufman. </publisher>
Reference: [14] <author> D.A. Schneider and D.J. DeWitt. </author> <title> A performance evaluation of four parallel join algorithms in a shared-nothing multiprocessor environment. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference, </booktitle> <pages> pages 110-121, </pages> <address> Portland, Oregon, </address> <month> May </month> <year> 1989. </year>

Reference: [DG] <author> D.J.DeWitt and J. Gray. </author> <title> Parallel Database Systems: The Future of High Performance Database Systems. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 85-98, </pages> <month> June </month> <year> 1992. </year>
Reference: [Syb] <institution> Sybase System 11 SQL Server documentation. </institution>
Reference: [RDC] <author> T.K.Rengarajan, Lucien A. Dimino, Dwayne Chung. </author> <title> Sybase System11 Online Capabilities. </title> <journal> Data Engineering Bulletin Volume 19(2) </journal> <pages> 19-24, </pages> <month> June </month> <year> 1996. </year> <booktitle> 43 44 45 46 IEEE Computer Society 1730 Massachusetts Ave, </booktitle> <address> NW Washington, D.C. 20036-1903 Non-profit Org. U.S. Postage PAID Silver Spring, MD Permit 1398 </address>
References-found: 28

