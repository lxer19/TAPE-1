URL: http://www.neci.nj.nec.com/homepages/giles/papers/fault_tolerant.recurrent.net-dfa.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Title: Fault-Tolerant Implementation of Finite-State Automata in Recurrent Neural Networks  
Author: C.W. Omlin a;b C.L. Giles a;c 
Address: 4 Independence Way, Princeton, NJ 08540  Troy, NY 12180  College Park, MD 20742  
Affiliation: a NEC Research Institute,  b CS Department, Rensselaer Polytechnic Institute,  c UMIACS, University of Maryland,  
Abstract: Recently, we have proven that the dynamics of any deterministic finite-state automata (DFA) with n states and m input symbols can be implemented in a sparse second-order recurrent neural network (SORNN) with n + 1 state neurons and O(mn) second-order weights and sigmoidal discriminant functions [5]. We investigate how that constructive algorithm can be extended to fault-tolerant neural DFA implementations where faults in an analog implementation of neurons or weights do not affect the desired network performance. We show that tolerance to weight perturbation can be achieved easily; tolerance to weight and/or neuron stuck-at-zero faults, however, requires duplication of the network resources. This result has an impact on the construction of neural DFAs with a dense internal representation of DFA states.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda, "Injecting nondeterministic finite state automata into recurrent networks," </title> <type> tech. rep., </type> <institution> Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy, Florence, Italy, </institution> <year> 1993. </year>
Reference-contexts: Furthermore, for x &lt; 0 and 0 &lt; x; h p (x; H) converges to and + , respectively. Stability of 0 is obvious. and + can shown to be the minima of an appropriate Lyapunov function <ref> [1] </ref>. From the symmetry of h (x; H), it follows that + + = 1. 7 We can now define a new function h t (x; H) which takes the residual inputs into consideration. We assume that each neuron receives residual inputs from all other neurons.
Reference: [2] <author> J. Hopcroft and J. Ullman, </author> <title> Introduction to Automata Theory, </title> <booktitle> Languages, and Computation. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1979. </year>
Reference-contexts: In particular, some of the results in section 8 will demonstrate the limitations of the representation of DFAs in SORNNs. A summary and directions for future research conclude this paper. 2 FINITE STATE AUTOMATA Regular languages represent the smallest class of formal languages in the Chomsky hierarchy <ref> [2] </ref>. Regular languages are generated by regular grammars. A regular grammar G is a quadruple G =&lt; S; N; T; P &gt; where S is the start symbol, N and T are non-terminal and terminal symbols, respectively.
Reference: [3] <author> L. Leerink. </author> <type> Personal Communication. </type>
Reference-contexts: All faults are likely to occur during the fabrication of an analog implementation. If the implementation of the weights is analog and no special circuitry is added to the design, then the weights will also be sensitive to temperature variations <ref> [3] </ref>. 5 WEIGHT PERTURBATION Recall the network state transitions and their equations where preservation of low and high signals implied preservation of all signals and thus stability of the internal DFA state representation: low ! low: i = h (S t X S t low ! high: S t+1 j +
Reference: [4] <author> N. May and D. Hammerstrom, </author> <title> "Fault simulation of a wafer-scale integrated neural network," </title> <type> tech. rep., </type> <institution> Oregon Graduate Center, </institution> <year> 1988. </year>
Reference-contexts: The results of a study on fault models in VLSI wafer neural networks can be found in <ref> [4] </ref>. In this paper, we are only concerned with faults as they occur in analog implementations. Analog implementations of neural networks have the advantage of lower power consumption compared to digital implementations.
Reference: [5] <author> C. Omlin and C. Giles, </author> <title> "Constructing deterministic finite-state automata in sparse recurrent neural networks," </title> <booktitle> in IEEE International Conference on Neural Networks (ICNN'94), </booktitle> <pages> pp. 1732-1737, </pages> <year> 1994. </year>
Reference: [6] <author> C. Omlin and C. Giles, </author> <title> "Stable encoding of large finite-state automata in recurrent neural networks with sigmoid discriminants," </title> <type> Tech. Rep. </type> <institution> UMIACS-TR-94-101, Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, </institution> <year> 1994. </year>
Reference-contexts: Since constructed SORNNs are able to regenerate their internal signals and since typical DFAs do not have the worst case properties assumed in this analysis, the conditions guaranteeing stable low and high signals are generally much too strong for some given DFA. Scaling issues are discussed elsewhere <ref> [6] </ref>. 3.3 Stable Finite-State Dynamics Stability of low and high signals is defined as follows: Definition 3.1 An encoding of DFA states in a SORNN is called stable if all the low and high signals are less and larger than 0.5, respectively.
Reference: [7] <author> D. Phatak and I. Koren, </author> <title> "Complete and partial fault tolerance of feedforward neural nets," </title> <type> Tech. Rep. </type> <institution> TR-92-CSE-26, Department of Electrical and Computer Engineering, University of Masssachusetts, </institution> <address> Amherst, MA, </address> <year> 1992. </year>
Reference-contexts: 1 INTRODUCTION Fault-tolerance is often mentioned as a desirable property of neural networks. However, neural networks are not inherently tolerant to faults in their internal structure; they can be made tolerant to certain types of faults either by providing multiple copies of the network resources <ref> [7] </ref> or by training them under conditions which emulate faults [8]. Neural networks may be able to recover from faults in their internal structure through retraining.
Reference: [8] <author> C. Sequin and R. Clay, </author> <title> "Fault tolerance training improves generalization and robustness," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks (IJCNN'92), Baltimore, MD, </booktitle> <volume> vol. 1, </volume> <pages> pp. 769774, </pages> <month> June </month> <year> 1992. </year> <month> 32 </month>
Reference-contexts: However, neural networks are not inherently tolerant to faults in their internal structure; they can be made tolerant to certain types of faults either by providing multiple copies of the network resources [7] or by training them under conditions which emulate faults <ref> [8] </ref>. Neural networks may be able to recover from faults in their internal structure through retraining.
References-found: 8

