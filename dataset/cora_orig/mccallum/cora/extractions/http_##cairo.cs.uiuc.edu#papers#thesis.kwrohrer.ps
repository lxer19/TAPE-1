URL: http://cairo.cs.uiuc.edu/papers/thesis.kwrohrer.ps
Refering-URL: http://cairo.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Note: c Copyright by Keith W. Rohrer, 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Dwight J. Makaroff and Raymond T. Ng. </author> <title> Buffer sharing schemes for continuous-media systems. </title> <type> Technical Report 1, </type> <institution> Department of Computer Science, University of British Columbia, </institution> <year> 1995. </year>
Reference-contexts: Both scenarios can benefit from reduction of the VOD server's memory needs, by sharing buffers between media streams. Such a sharing scheme has been described algorithmically in <ref> [1] </ref>, but an implementation and the associated details have not been described. This work describes a buffer sharing scheme adapted from CES, presents a VOD server which uses this algorithm to reduce buffer requirements, and analyzes the viability and performance of the server and algorithm. <p> While this seems impractical, or at least imprudent, the result is useful nonetheless. 2.2 The CES Algorithm The buffer sharing algorithm on which this thesis is based was developed by Makaroff and Ng. <ref> [1] </ref> The algorithm, called CES, applies to a fixed-period server and groups the data to be read for a stream during a given period down into portions, which are read into buffer slots; these slots are re-used by subsequent streams once their contents are consumed. <p> formula for the size of a slot (or portion) becomes ffiT p=n, except that one slot per stream must be oversized (by (1 ffi)T p=n) to contain the data the stream will consume during the idle time. 2 A similar figure, without the denotations of reading and consumption, appears in <ref> [1] </ref>. 5 This idle time is the only time new streams may be added to the set of streams being served. 3 The n + 1 st stream requires the allocation of n + 1 new slots, one of which is oversized. <p> as group sweeping and SCAN-EDF re-order batches of concurrent requests to improve disk performance while still allowing real time guarantees. [3] However, control over the placement of files in the filesystem can also improve disk performance. 3 No details of how to retire a stream from service are given in <ref> [1] </ref>, so perhaps a new stream may also be added to the schedule in place of a stream which ended during the previous period. 6 A paper by Ozden, Rastogi, and Silberschatz discusses these issues. [4] Beyond suggesting a stan-dard circular elevator algorithm, 4 the authors suggest placing the files on <p> It is intended to provide smooth, jitter-free service in the absence of conflicts for disk or network access. While it evolved from the CES algorithm described in <ref> [1] </ref>, it allows each stream to use a different bandwidth and improves the average response time to start a new streams. 3.1 Read Scheduling The BRS algorithm uses time quanta called subperiods; based on a maximum period T (some number of seconds) and a constant service granularity factor S (a number), <p> Once a stream has been read for, enough 2 A more timing dependent variant of CES called NCES actually relies on the reading rate of the disk and the transmission rate of the network to safely fill buffer slots with new portions while the old portions are still being consumed. <ref> [1] </ref> BRS is much more conservative about predictions of disk bandwidth and response times. 9 data has been buffered to last until S s i SU of other streams have been read for. <p> Meanwhile, each established stream will consume one buffer block per SU of bandwidth it requires. In the steady state the consumption and reading rates will be equal; at full load, data will be read for streams 6 As the proper handling of terminated streams was not described in <ref> [1] </ref>, it may be possible for a stream which was terminated last period to yield its place to a new stream during this period. 7 Slightly more data may be required; see Section 3.3. 13 just in time to prevent underflow. <p> first SU thus requires S blocks of buffer space, but by the start of the next subperiod at least one of those blocks will have been consumed, so the next SU requires only S 1 blocks of additional buffer space because it can re-use that block from the previous SU. <ref> [1] </ref> This linear trend continues until the S th SU requires only one additional block of buffer space, for a basic buffer space requirement of T P S X b (3.1) when s SU are in use. 8 The number of buffer blocks is available by the same formula, when the <p> The server should therefore allocate the usual number of blocks for a stream which requires 5 SU, filling each only 96% full, thus preserving the 1:1 consumption ratio. 3.2.3 Buffer Locations and Allocations While Makaroff and Ng suggest in <ref> [1] </ref> that data should be located within the buffer according to a given formula to avoid the space overhead of bookkeeping, I disagree.
Reference: [2] <author> Raymond T. Ng and Jinhai Yang. </author> <title> An analysis of buffer sharing and prefetching techniques in multimedia systems. </title> <type> Technical Report 20, </type> <institution> Department of Computer Science, University of British Columbia, </institution> <year> 1994. </year>
Reference-contexts: Ideally, an inexpensive server will be able to exceed the response time of household video tape equipment, even serving several clients at once. 2 Chapter 2 Background and Related Work 2.1 Theoretical Results Ng and Yang analyzed the theoretical buffer space requirements of a group of media streams. <ref> [2] </ref> They determined that when the disk is constantly busy, the buffer space required by a group of streams is only half the space which would be required to buffer each stream independently for the same period. <p> This corresponds to 960 or 975 kilobytes of buffer space, a mere 54% of the 1800 kilobytes of buffer which would be required to buffer all six streams separately for a two-second server period; this is quite close to the 50% theoretical minimum given by Ng and Yang. <ref> [2] </ref> Fortunately, extra space requirements are much easier to explain than unexpectedly small space requirements: it is likely that at least some of the Senders fell briefly behind schedule, as they could not release their fully consumed buffers without first contending for access to the network. 7.3 Background and Foreground Streams
Reference: [3] <author> Ralf Steinmetz and Klara Nahrstedt. </author> <title> Multimedia: Computing, Communications, & Applications. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: and does not seem to attempt to minimize response time when starting streams, nor to minimize jitter from the disk subsystem. 2.3 Storage System Optimizations Well-established disk reading algorithms such as group sweeping and SCAN-EDF re-order batches of concurrent requests to improve disk performance while still allowing real time guarantees. <ref> [3] </ref> However, control over the placement of files in the filesystem can also improve disk performance. 3 No details of how to retire a stream from service are given in [1], so perhaps a new stream may also be added to the schedule in place of a stream which ended during
Reference: [4] <author> Banu Ozden, Rajeev Rastogi, and Avi Silberschatz. </author> <title> A framework for the storage and retrieval of continuous media data. </title> <booktitle> In Proceedings of the IEEE International Conference on Multimedia Computing and Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: performance. 3 No details of how to retire a stream from service are given in [1], so perhaps a new stream may also be added to the schedule in place of a stream which ended during the previous period. 6 A paper by Ozden, Rastogi, and Silberschatz discusses these issues. <ref> [4] </ref> Beyond suggesting a stan-dard circular elevator algorithm, 4 the authors suggest placing the files on the disk starting at track or cylinder boundaries; the effects of rotational latency and head switch or track-to-track seeking can then be minimized by reading whole tracks at a time.
Reference: [5] <author> Abraham Silberschatz and Peter B. Galvin. </author> <title> Operating System Concepts. </title> <publisher> Addison-Wesley, </publisher> <address> fourth edition, </address> <year> 1994. </year> <note> Reprinted with corrections in January, </note> <year> 1995. </year>
Reference-contexts: of these factors complicate disk optimizations, better SCSI disks are able to report the relevant information. [7] These drawbacks put the use of such optimizations beyond the scope of this research. 4 The authors of the paper use the name C-LOOK, which is consistent with Silberschatz's textbook on operating systems. <ref> [5] </ref> Deitel's textbook on operating systems uses the name C-SCAN for the same algorithm, which does not automatically seek to the extremes of the disk during each pass, unlike Silberschatz's description of SCAN and C-SCAN. [6] 7 Chapter 3 The Block-Release Sharing (BRS) Algorithm The Block-Release Sharing (BRS) algorithm manages a
Reference: [6] <author> Harvey M. Deitel. </author> <title> Operating Systems. Addison-Wesley, </title> <note> second edition, 1990. [7] ftp://tsx-11.mit.edu:/pub/linux/ALPHA/scsi/scsiinfo-1.6.tar.gz. </note>
Reference-contexts: the paper use the name C-LOOK, which is consistent with Silberschatz's textbook on operating systems. [5] Deitel's textbook on operating systems uses the name C-SCAN for the same algorithm, which does not automatically seek to the extremes of the disk during each pass, unlike Silberschatz's description of SCAN and C-SCAN. <ref> [6] </ref> 7 Chapter 3 The Block-Release Sharing (BRS) Algorithm The Block-Release Sharing (BRS) algorithm manages a central buffer with fixed-size blocks for a variable-period continuous media server.
Reference: [8] <author> Balaji Srinivasan, Raghavan Menon, et al. </author> <title> Utime (microsecond resolution timers for linux). </title> <address> http://hegel.ittc.ukans.edu/projects/utime/. </address>
Reference-contexts: Projects like UTIME, which provides microsecond-resolution timers for Linux, should be investigated to minimize the oversleeping (and consequent jitter) caused by the scheduling granularity, which was chosen without real time guarantees in mind. <ref> [8] </ref> 9.1.2 Realtime Scheduling for Senders This server design totally ignored scheduling for the network device, allowing the streams to compete in the scheduler when they were ready to transmit and to sleep when they were not 59 ready.
Reference: [9] <author> Yasuhiko Miyazaki. </author> <title> Hierarchical client-server multimedia systems. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1996. </year> <month> 68 </month>
Reference-contexts: streams could share buffers either with the other incoming streams (in a similar manner to the way BRS sharing works) or with a paired outgoing stream (which would not be able to share much space, if any, with other outgoing ). 9.3.3 Service Hierarchies Hierarchical service architectures such as Miyazaki's <ref> [9] </ref> can take advantage of buffer sharing. While servers at all levels can use BRS-style buffer sharing, top- and mid-level servers will likely use a different reading and service algorithm.
References-found: 8

