URL: http://www.research.att.com/~stubblebine/97fse.ps
Refering-URL: http://www.research.att.com/~stubblebine/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: prem,stubblebine@research.att.com  
Title: Cryptographic Verification of Test Coverage Claims  
Author: Prem Devanbu Stuart G. Stubblebine 
Keyword: Testing, Verification, Cryptography, Components  
Date: 12, 1997  
Note: February  
Address: 2b417, 600 Mountain Ave, Murray Hill, NJ 07974, USA  
Affiliation: Information Systems and Services Research Center,  AT&T Labs Research  
Abstract: The market for software components is growing, driven on the "demand side" by the need for rapid deployment of highly functional products, and on the "supply side" by distributed object standards. As components and component vendors proliferate, there is naturally a growing concern about quality, and the effectiveness of testing processes. White box testing, particularly the use of coverage criteria, is a widely used method for measuring the "thoroughness" of testing efforts. High levels of test coverage are used as indicators of good quality control procedures. Software vendors who can demonstrate high levels of test coverage have a credible claim to high quality. However, verifying such claims involves knowledge of the source code. In applications where reliability and quality are critical, it would be desirable to verify test coverage claims without forcing vendors to give up valuable technical secrets. In this paper, we explore cryptographic techniques that can be used to verify such claims. Our techniques have some limitations; however, if such methods can be perfected and popularized, they can have an important "leveling" effect on the software market place: small, relatively unknown software vendors with limited resources can provide credible evidence of high-quality processes, and thus compete with much larger corporations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Delta Software Testing (accredited by Danish Accreditation Authority-DANAK). </institution> <note> http://www.delta.dk/se/ats.htm. </note>
Reference-contexts: As the number and types of components proliferate, and smaller, newer vendors enter the market, there is a natural concern about quality. Traditionally, systems with stringent quality requirements undergo a rigorous verification process, often under the auspices of third party verification agents <ref> [1, 17, 18] </ref>. One common testing technique used is white box testing; The goal is to ensure that a system has been adequately exercised during testing.
Reference: [2] <author> H. Agrawal. </author> <title> Dominators, super blocks and program coverage. </title> <booktitle> In Proceedings, POPL 94, </booktitle> <year> 1986. </year>
Reference-contexts: In cases where this trade-off is acceptable, this technique is applicable. There is a threat to the validity of the confidence level calculation described above: the sampling process is not really a series of independent events. Executions of coverage points (blocks, branches, or functions) are often strongly correlated. Agrawal <ref> [2] </ref> shows how to determine the set of statically independent coverage points from the control flow graph by computing the post-dominator and pre-dominator trees of basic blocks. The leaves of such trees could be used to form an indepedent set of coverage points.
Reference: [3] <author> T. Ball and J. Larus. </author> <title> Efficient path profiling. In Micro '96. </title> <publisher> IEEE Press, </publisher> <month> December </month> <year> 1996. </year>
Reference-contexts: Without a physically protected hardware platform, a determined adversary can reverse-engineer a good deal of information about software. Techniques and tools to support reverse engineering are an area of active research. In fact, previous research <ref> [3, 5, 19, 22] </ref> demonstrates how control-flow graphs, profile information, compiler-generated binary idioms, and even slices can be derived by analyzing and instrumenting binaries. Decompilation (converting binary to source code) and binary porting (converting binaries from one machine architecture to another) are typical goals of binary analysis. <p> In addition, by instrumentation, and 6 For simplicity, we assume trials can be repeated. 17 dynamic analysis. C A can detect which paths of the control path are activated for different input conditions. Some recent work by Ball & Larus <ref> [3] </ref> show how it is possible to trace and profile control flow path execution using just the binary. Additional information can be gained by tracing memory references and building dynamic slices. <p> For simplicity, in the above discussion, we have assumed basic block coverage, since it very simple to check e.g., with a debugger. If a far more stringent coverage is desired, such as all paths [10] then a tool based on <ref> [3] </ref> could be used to monitor the various paths during coverage verification. However, this level of coverage testing is extremely rare in practice. Another limitation is the manner in which the challenge coverage points are selected.
Reference: [4] <author> C. Cifuentes. </author> <title> Partial automation of an integrated reverse engineering environment for binary code. </title> <booktitle> In Third Working Conference on Reverse Engineering, </booktitle> <year> 1996. </year>
Reference-contexts: As binary decompilation tools <ref> [4] </ref> mature and become more widely available, they can be used by customers to build confidence about areas of the binary that V claims to be non-testable for the reasons listed above. 4 Towards Eliminating the trusted third party All the approaches described above rely upon a trusted third party (T
Reference: [5] <author> C. Cifuentes and J. Gough. </author> <title> Decompilation of binary programs. </title> <journal> Software Practice and Experience, </journal> <month> July </month> <year> 1995. </year>
Reference-contexts: Without a physically protected hardware platform, a determined adversary can reverse-engineer a good deal of information about software. Techniques and tools to support reverse engineering are an area of active research. In fact, previous research <ref> [3, 5, 19, 22] </ref> demonstrates how control-flow graphs, profile information, compiler-generated binary idioms, and even slices can be derived by analyzing and instrumenting binaries. Decompilation (converting binary to source code) and binary porting (converting binaries from one machine architecture to another) are typical goals of binary analysis.
Reference: [6] <author> P. Devanbu. </author> <title> Genoa- a language and front-end independent source code analyzer generator. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Software Engineering, </booktitle> <year> 1992. </year>
Reference-contexts: The physical enclosure is a guarantee of integrity; if tampered with, the processor will erase its memory and cease to function. We envision placing customizable, trusted source code analyzers <ref> [7, 6] </ref> in secure co-processors. Such a device can be installed as a co-processor at the site of the vendor; it can be sent an authenticated message which describes the type of analysis to be conducted. The smart-card resident tool can perform the analysis, and sign the results.
Reference: [7] <author> P. Devanbu and S. G. Stubblebine. </author> <title> Building software with certified properties. </title> <type> Unpublished Manuscript, </type> <note> available from the authors, February 97. </note>
Reference-contexts: The physical enclosure is a guarantee of integrity; if tampered with, the processor will erase its memory and cease to function. We envision placing customizable, trusted source code analyzers <ref> [7, 6] </ref> in secure co-processors. Such a device can be installed as a co-processor at the site of the vendor; it can be sent an authenticated message which describes the type of analysis to be conducted. The smart-card resident tool can perform the analysis, and sign the results.
Reference: [8] <author> P.G. Frankl, R. Hamlet, B. Littlewood, and L. Strigini. </author> <title> Choosing a testing method to deliver reliability. </title> <booktitle> In Proceedings of the 19th International Conference on Software Engineering (To Appear). IEEE Computer Society, </booktitle> <year> 1997. </year>
Reference-contexts: Earlier work [9] had yielded inconclusive results; however, the programs used in [9] were substantially smaller than [13, 20]. On the analytic front, rigorous probabilistic models of the relationship between increasing white-box coverage and the likelihood of fault detection have been developed <ref> [8, 11] </ref>. However, no known testing process is perfect; all known methods, white box or black box will let some faults slip! The best current experimental work [13, 20] suggests that high levels of white box test coverage can guarantee high levels of fault detection.
Reference: [9] <author> P.G. Frankl and S. N. Weiss. </author> <title> An experimental comparison of the effectiveness of branch testing and data flow testing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> August </month> <year> 1993. </year>
Reference-contexts: Most recent empirical work [13, 20] has found that test sets with coverage levels in the range of of 80-90% have a high chance of exposing failures. Earlier work <ref> [9] </ref> had yielded inconclusive results; however, the programs used in [9] were substantially smaller than [13, 20]. On the analytic front, rigorous probabilistic models of the relationship between increasing white-box coverage and the likelihood of fault detection have been developed [8, 11]. <p> Most recent empirical work [13, 20] has found that test sets with coverage levels in the range of of 80-90% have a high chance of exposing failures. Earlier work <ref> [9] </ref> had yielded inconclusive results; however, the programs used in [9] were substantially smaller than [13, 20]. On the analytic front, rigorous probabilistic models of the relationship between increasing white-box coverage and the likelihood of fault detection have been developed [8, 11].
Reference: [10] <author> P.G. Frankl and E. J. Weyuker. </author> <title> An applicable family of data flow testing criteria. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> August </month> <year> 1988. </year>
Reference-contexts: For simplicity, in the above discussion, we have assumed basic block coverage, since it very simple to check e.g., with a debugger. If a far more stringent coverage is desired, such as all paths <ref> [10] </ref> then a tool based on [3] could be used to monitor the various paths during coverage verification. However, this level of coverage testing is extremely rare in practice. Another limitation is the manner in which the challenge coverage points are selected.
Reference: [11] <author> P.G. Frankl and E. J. Weyuker. </author> <title> A formal analysis of the fault-detecting ability of testing methods. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> March </month> <year> 1993. </year> <month> 23 </month>
Reference-contexts: Earlier work [9] had yielded inconclusive results; however, the programs used in [9] were substantially smaller than [13, 20]. On the analytic front, rigorous probabilistic models of the relationship between increasing white-box coverage and the likelihood of fault detection have been developed <ref> [8, 11] </ref>. However, no known testing process is perfect; all known methods, white box or black box will let some faults slip! The best current experimental work [13, 20] suggests that high levels of white box test coverage can guarantee high levels of fault detection.
Reference: [12] <author> A. Freier, P. Karlton, and P. Kocher. </author> <title> The ssl protocol, </title> <note> version 3.0 (internet draft), </note> <month> March </month> <year> 1996. </year>
Reference-contexts: Our focus of security is not at the session layer but at the application layer. Thus we assume the testing protocols occur over secure channels such as those provided by Secure Socket Layer ( SSL <ref> [12] </ref>). In the following scenarios, V refers to the vendor who claims to have achieved fl test coverage on a system X, and C refers to the skeptical customer who wants to be convinced. Basic Third Party Method 1.
Reference: [13] <author> M. Hutchins, H. Foster, T. Goradia, and T. </author> <title> Ostrand. Experiments on the effectiveness of dataflow- and controlflow-based test adequacy criteria. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering. IEEE Computer Society, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: who undertakes the cost of developing an adequate set T X fl for some stringent fl can reasonably expect that the system is less likely to fail 1 in the field due to undetected 1 Providing, of course, that the system passes the tests! 2 faults in in system X <ref> [13] </ref>. Often, in fields with exacting reliability requirements (such as transportation, telecommunications or health) software users demand high quality standards, and expect vendors to use testing processes that achieve high levels of coverage with stringent coverage criteria. <p> Thus, at the 95% confidence level, T can reasonably conclude that an estimate of p = 0:95 is no more than 0:09 too high with about 25 samples. Experimental work <ref> [20, 13] </ref> indicates that branch coverage levels in the range of 80-90% have a high likelihood of exposing faults in the software. <p> With a suitable instrumentation facility, the binary can be instrumented and coverage verified. It should be noted here that while coverage testing has widely used in industry, some researchers dispute the effectiveness of whitebox (or "clearbox") coverage methods. Most recent empirical work <ref> [13, 20] </ref> has found that test sets with coverage levels in the range of of 80-90% have a high chance of exposing failures. Earlier work [9] had yielded inconclusive results; however, the programs used in [9] were substantially smaller than [13, 20]. <p> Most recent empirical work <ref> [13, 20] </ref> has found that test sets with coverage levels in the range of of 80-90% have a high chance of exposing failures. Earlier work [9] had yielded inconclusive results; however, the programs used in [9] were substantially smaller than [13, 20]. On the analytic front, rigorous probabilistic models of the relationship between increasing white-box coverage and the likelihood of fault detection have been developed [8, 11]. <p> However, no known testing process is perfect; all known methods, white box or black box will let some faults slip! The best current experimental work <ref> [13, 20] </ref> suggests that high levels of white box test coverage can guarantee high levels of fault detection. However, since white box testing is not perfect, there are several complications. Given a coverage point that has faults, there may be several test cases that exercise that point. <p> To summarize, our work rests on the assumption (again, supported by <ref> [13, 20] </ref>) that comprehensive coverage testing tends to expose faults, and on the assumption that vendors will most often find it more profitable to fix faults exposed by a covering test (rather than searching for a test that covers but hides faults).
Reference: [14] <author> Mondex Inc. </author> <note> http:/www.mondex.com. </note>
Reference-contexts: One way to avoid this problem is through the use of a physically secure co-processor. Various forms of these processors are available in tamper-proof enclosures, running secure operating systems; they are expected to become ubiquitous in the form of smart cards <ref> [27, 14] </ref>, which are expected to become quite powerful in a few years. The physical enclosure is a guarantee of integrity; if tampered with, the processor will erase its memory and cease to function. We envision placing customizable, trusted source code analyzers [7, 6] in secure co-processors.
Reference: [15] <author> Plum Hall Inc. </author> <note> http:/www.plumhall.com. </note>
Reference-contexts: A comprehensive consideration of all these special cases is a valuable piece of intellectual property that demands protection. Indeed, there are vendors who make it their business to develop and sell comprehensive test suites <ref> [15, 25] </ref>. Our goal is to protect this information, while allowing the vendor to make credible test coverage claims. 3 Caveats and Assumptions. First, we assume that vendors are strongly motivated by market forces 2 to provide the highest quality software.
Reference: [16] <author> M. J. Kearns and U. V. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: With a small number of random challenges, T can bound V's test coverage. If V's responses cover a proportion p s of T 's challenges, T can estimate V's actual coverage p (using Hoeffding's version of Chernoff bounds for the "positive tail" of a binomial distribution, see <ref> [16] </ref>, pp 190-191): P (p s p *) e 2p (1p) when p 0:5 (1) 4 Or a string indicating the lack of a test case for this coverage point. 8 For a 95% confidence level, we can bound *: e 2p (1p) 0:05 s 0:05 )p (1 p) Clearly, as
Reference: [17] <institution> National Software Testing Labs. </institution> <note> http://www.nstl.com. </note>
Reference-contexts: As the number and types of components proliferate, and smaller, newer vendors enter the market, there is a natural concern about quality. Traditionally, systems with stringent quality requirements undergo a rigorous verification process, often under the auspices of third party verification agents <ref> [1, 17, 18] </ref>. One common testing technique used is white box testing; The goal is to ensure that a system has been adequately exercised during testing.
Reference: [18] <institution> Software Testing Labs. </institution> <note> http://www.stlabs.com. </note>
Reference-contexts: As the number and types of components proliferate, and smaller, newer vendors enter the market, there is a natural concern about quality. Traditionally, systems with stringent quality requirements undergo a rigorous verification process, often under the auspices of third party verification agents <ref> [1, 17, 18] </ref>. One common testing technique used is white box testing; The goal is to ensure that a system has been adequately exercised during testing.
Reference: [19] <author> J. Larus and E. Schnarr. Eel: </author> <title> Machine-independent executable editing. </title> <booktitle> In ACM SIG-PLAN PLDI. </booktitle> <publisher> ACM Press, </publisher> <year> 1995. </year>
Reference-contexts: Without a physically protected hardware platform, a determined adversary can reverse-engineer a good deal of information about software. Techniques and tools to support reverse engineering are an area of active research. In fact, previous research <ref> [3, 5, 19, 22] </ref> demonstrates how control-flow graphs, profile information, compiler-generated binary idioms, and even slices can be derived by analyzing and instrumenting binaries. Decompilation (converting binary to source code) and binary porting (converting binaries from one machine architecture to another) are typical goals of binary analysis. <p> V sends T : B X s , C X fl and the test suite T X fl , with the locations in the source files corresponding the coverage points. 3. T uses a binary instrumentation tool, either interactive (e.g., a debugger, or batch oriented (e.g., ATOM [24], EEL <ref> [19] </ref>) to instrument B X s , using the line number/file information sent by T , and the symbol table information embedded in B X s . For example, a debugger, can set break points at the appropriate locations (e.g., line numbers in files). 4. <p> Interactive debuggers such as gdb can readily perform such a task; a command such as break 0x89ABC to gdb will set a break point at the machine address 0x89ABC in the program. A batch-oriented tool like EEL <ref> [19] </ref> can also be used. Such a tool will be used by T to insert instrumentation at coverage points, and verify coverage. We also use a binary location finder (blf ), which uses the symbol table to find binary addresses for the coverage points.
Reference: [20] <author> Y. Malaiya, N. Li, J. Bieman, R. Karcich, and B. Skibbe. </author> <title> Software test coverage and reliability. </title> <type> Technical report, </type> <institution> Colorado State University, </institution> <year> 1996. </year>
Reference-contexts: Thus, at the 95% confidence level, T can reasonably conclude that an estimate of p = 0:95 is no more than 0:09 too high with about 25 samples. Experimental work <ref> [20, 13] </ref> indicates that branch coverage levels in the range of 80-90% have a high likelihood of exposing faults in the software. <p> With a suitable instrumentation facility, the binary can be instrumented and coverage verified. It should be noted here that while coverage testing has widely used in industry, some researchers dispute the effectiveness of whitebox (or "clearbox") coverage methods. Most recent empirical work <ref> [13, 20] </ref> has found that test sets with coverage levels in the range of of 80-90% have a high chance of exposing failures. Earlier work [9] had yielded inconclusive results; however, the programs used in [9] were substantially smaller than [13, 20]. <p> Most recent empirical work <ref> [13, 20] </ref> has found that test sets with coverage levels in the range of of 80-90% have a high chance of exposing failures. Earlier work [9] had yielded inconclusive results; however, the programs used in [9] were substantially smaller than [13, 20]. On the analytic front, rigorous probabilistic models of the relationship between increasing white-box coverage and the likelihood of fault detection have been developed [8, 11]. <p> However, no known testing process is perfect; all known methods, white box or black box will let some faults slip! The best current experimental work <ref> [13, 20] </ref> suggests that high levels of white box test coverage can guarantee high levels of fault detection. However, since white box testing is not perfect, there are several complications. Given a coverage point that has faults, there may be several test cases that exercise that point. <p> To summarize, our work rests on the assumption (again, supported by <ref> [13, 20] </ref>) that comprehensive coverage testing tends to expose faults, and on the assumption that vendors will most often find it more profitable to fix faults exposed by a covering test (rather than searching for a test that covers but hides faults).
Reference: [21] <author> Doug McIlroy. </author> <type> Personal e-mail communication, </type> <year> 1996. </year>
Reference-contexts: Sometimes generated code may contain additional control flow that represent different cases that can occur in the field, and 12 V can legitimately be expected to supply covering test cases. When the generated code is genuinely unreachable <ref> [21] </ref>, V can claim it as such, and supply source code that C can compile to create similar binaries. Occurrences of dead code in the binary are really bugs in the compiler, and are likely to be rare.
Reference: [22] <author> N. Ramsey and M. Fernandez. </author> <title> Specifying representations of machine instructions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <year> 1997. </year>
Reference-contexts: Without a physically protected hardware platform, a determined adversary can reverse-engineer a good deal of information about software. Techniques and tools to support reverse engineering are an area of active research. In fact, previous research <ref> [3, 5, 19, 22] </ref> demonstrates how control-flow graphs, profile information, compiler-generated binary idioms, and even slices can be derived by analyzing and instrumenting binaries. Decompilation (converting binary to source code) and binary porting (converting binaries from one machine architecture to another) are typical goals of binary analysis.
Reference: [23] <author> B. Schneier. </author> <title> Applied Cryptography. </title> <publisher> John Wiley & Sons, </publisher> <year> 1995. </year>
Reference-contexts: In this paper, we apply several cryptographic techniques to address some common scenarios that may arise in practice. We now briefly describe the techniques we have used in our work; more complete descriptions can be found in <ref> [23] </ref>. All of these techniques are used to build assurance in the customer (C) that the vendor (V) has high test coverage, while attempting to protect V's secrets.
Reference: [24] <author> A. Srivastava and A. Eustace. </author> <title> Atom: A tool for building customized program analysis tools. </title> <type> Technical Report 1994/2, </type> <institution> DEC Western Research Labs, </institution> <year> 1994. </year>
Reference-contexts: V sends T : B X s , C X fl and the test suite T X fl , with the locations in the source files corresponding the coverage points. 3. T uses a binary instrumentation tool, either interactive (e.g., a debugger, or batch oriented (e.g., ATOM <ref> [24] </ref>, EEL [19]) to instrument B X s , using the line number/file information sent by T , and the symbol table information embedded in B X s . For example, a debugger, can set break points at the appropriate locations (e.g., line numbers in files). 4.
Reference: [25] <institution> Applied Testing and Technology Inc. </institution> <note> http://www.aptest.com. </note>
Reference-contexts: A comprehensive consideration of all these special cases is a valuable piece of intellectual property that demands protection. Indeed, there are vendors who make it their business to develop and sell comprehensive test suites <ref> [15, 25] </ref>. Our goal is to protect this information, while allowing the vendor to make credible test coverage claims. 3 Caveats and Assumptions. First, we assume that vendors are strongly motivated by market forces 2 to provide the highest quality software.
Reference: [26] <author> E. J. Weyuker. </author> <title> On testing non-testable programs. </title> <journal> The Computer Journal, </journal> <volume> 25(4) </volume> <pages> 465-470, </pages> <year> 1982. </year>
Reference-contexts: Third, we assume that all parties involved in coverage verification protocols have access to a test oracle <ref> [26] </ref> that decides, at low cost, whether the output for any test case is right or not.
Reference: [27] <author> Bennet Yee and Doug Tygar. </author> <title> Secure coprocessors in electronic commerce applications. </title> <booktitle> In Proceedings of The First USENIX Workshop on Electronic Commerce, </booktitle> <address> New York, New York, </address> <month> July </month> <year> 1995. </year> <month> 24 </month>
Reference-contexts: One way to avoid this problem is through the use of a physically secure co-processor. Various forms of these processors are available in tamper-proof enclosures, running secure operating systems; they are expected to become ubiquitous in the form of smart cards <ref> [27, 14] </ref>, which are expected to become quite powerful in a few years. The physical enclosure is a guarantee of integrity; if tampered with, the processor will erase its memory and cease to function. We envision placing customizable, trusted source code analyzers [7, 6] in secure co-processors.
References-found: 27

