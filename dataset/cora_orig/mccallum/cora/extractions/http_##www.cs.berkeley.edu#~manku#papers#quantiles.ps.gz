URL: http://www.cs.berkeley.edu/~manku/papers/quantiles.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~manku/papers.html
Root-URL: 
Email: manku@almaden.ibm.com  sridhar@almaden.ibm.com  bruce@almaden.ibm.com  
Title: Approximate Medians and other Quantiles in One Pass and with Limited Memory  
Author: Gurmeet Singh Manku Sridhar Rajagopalan Bruce G. Lindsay 
Affiliation: IBM Almaden Research Center  IBM Almaden Research Center  IBM Almaden Research Center  
Abstract: We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply without regard to the value distribution or the arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude. We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e., they apply with respect to a (user controlled) confidence parameter. We present the algorithms, their theoretical analysis and simulation results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. G. Selinger, M. M. Astrahan, R. A. Lories, and T. G. Price, </author> <title> "Access Path Selection in a Relational Database Management System", </title> <booktitle> in ACM SIGMOD 79, </booktitle> <month> June </month> <year> 1979. </year>
Reference-contexts: 1 Introduction This article studies the problem of computing order statistics of large sequences of online or disk-resident data using as little main memory as possible. We focus on computing quantiles, which are elements at specific positions in the sorted order of the input. The -quantile, for 2 <ref> [0; 1] </ref>, is defined to be the element in position dNe in the sorted sequence of the input. Here, and in the rest of this paper N denotes the number of elements in the input. For = 0:5, the quantile is called the median. <p> They characterize distributions of real world data sets and are less sensitive to outliers than the moments (mean and variance). They can be used by business intelligence applications to distill summary information from huge data sets. Obtaining an accurate estimate of predicate selectivity is valuable for query optimization <ref> [1] </ref>. To better estimate the cardinality of query result sets, quantiles can be, and are, used to characterize the distribution of stored data [2].
Reference: [2] <author> G. Piatetsky-Shapiro, </author> <title> "Accurate Estimation of the Number of Tuples Satisfying a Condition", </title> <booktitle> in ACM SIGMOD 84, </booktitle> <address> Boston, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: Obtaining an accurate estimate of predicate selectivity is valuable for query optimization [1]. To better estimate the cardinality of query result sets, quantiles can be, and are, used to characterize the distribution of stored data <ref> [2] </ref>. Equi-depth histograms [3], for instance, are simply i p -quantiles for i 2 f1; 2; : : : p1g, computed over column values of database tables for a suitable p.
Reference: [3] <author> V. Poosala, Y. E. Ioannidis, P. J. Haas, and E. J. Shekita, </author> <title> "Improved Histograms for Selectivity Estimation of Range Predicates", </title> <booktitle> in ACM SIGMOD 96, </booktitle> <pages> pp. 294-305, </pages> <address> Montreal, </address> <month> June </month> <year> 1996. </year> <type> [4] "DB2 MVS", . [5] "Informix", </type> . 
Reference-contexts: Obtaining an accurate estimate of predicate selectivity is valuable for query optimization [1]. To better estimate the cardinality of query result sets, quantiles can be, and are, used to characterize the distribution of stored data [2]. Equi-depth histograms <ref> [3] </ref>, for instance, are simply i p -quantiles for i 2 f1; 2; : : : p1g, computed over column values of database tables for a suitable p.
Reference: [6] <author> D. DeWitt, J. Naughton, and D. Schneider, </author> <title> "Parallel Sorting on a Shared-Nothing Architecture using Probabilistic Splitting", </title> <booktitle> in Proc. Intl. Conf. on Parallel and Distributed Inf. Sys., </booktitle> <pages> pp. 280-291, </pages> <address> Miami Beach, </address> <year> 1991. </year>
Reference-contexts: Parallel database systems [4, 5] employ value range data partitioning that requires generation of splitters to divide the data into approximately equal parts. Distributed parallel sorting can also use splitter values to assign data elements to the nodes where they will be sorted <ref> [6] </ref>. Approximate quantiles can be substituted for exact quan-tiles in all three applications, namely statistical data analysis, database query optimization and value range data partitioning. <p> Alsabti, Ranka and Singh [18] propose a data independent single pass algorithm with guaranteed error bounds. We describe this algorithm in more detail in Section 4.4. Random sampling can be a very useful tool in this context. It has been used by DeWitt et al <ref> [6] </ref> for distributed sorting. We discuss this idea and various applications in detail in Section 5. 2.3 Bridging the gap The ideas in the paper by Munro and Paterson can be applied to develop a one-pass algorithm for approximate quantiles.
Reference: [7] <author> M. Blum, R. W. Floyd, V. R. Pratt, R. L. Rivest, and R. E. Tarjan, </author> <title> "Time Bounds for Selection", </title> <journal> in J. Com-put. Syst. Sci., </journal> <volume> vol. 7, </volume> <pages> pp. 448-461, </pages> <year> 1973. </year>
Reference-contexts: The reader will judge the simplicity and understandability of our algorithms. 2 Antecedents 2.1 The Theory Literature The theory literature has primarily focused on counting the number of comparisons needed to find the exact median (quantile). The celebrated paper of Blum, Floyd, Pratt, Rivest and Tarjan <ref> [7] </ref>, shows that selection of the kth largest element out of N can be done using at most 5:43N comparisons. This paper also shows that at least 1:5N comparisons are required in the computation of the exact median.
Reference: [8] <author> M. R. Paterson, </author> <title> "Progress in Selection", </title> <institution> Deptt. of Computer Science, University of Warwick, Coventry, UK, </institution> <year> 1997. </year>
Reference-contexts: This paper also shows that at least 1:5N comparisons are required in the computation of the exact median. For an account of progress since then, see the survey by Mike Paterson <ref> [8] </ref>. The current best bounds are much tighter and are the product of sophisticated and deep analysis (see [8, 9, 10, 11, 12]). <p> This paper also shows that at least 1:5N comparisons are required in the computation of the exact median. For an account of progress since then, see the survey by Mike Paterson [8]. The current best bounds are much tighter and are the product of sophisticated and deep analysis (see <ref> [8, 9, 10, 11, 12] </ref>). The upper bound is 2:9423N comparisons, and the lower bound (2 + ff)N , where ff is of the order of 2 40 , an extremely small number by most standards. <p> The naive randomized algorithm, which outputs the median of a random sample of size O ( 1 * 2 log ffi 1 ), uses a number of comparisons independent of N . For a comprehensive survey of this aspect of the literature, see the survey by Paterson <ref> [8] </ref>. Ira Pohl [14] established that any deterministic algorithm that computes the exact median in one pass needs to store at least N=2 data elements.
Reference: [9] <author> D. Dor, </author> <title> Selection Algorithms, </title> <type> PhD thesis, </type> <institution> Tel-Aviv University, </institution> <year> 1995. </year>
Reference-contexts: This paper also shows that at least 1:5N comparisons are required in the computation of the exact median. For an account of progress since then, see the survey by Mike Paterson [8]. The current best bounds are much tighter and are the product of sophisticated and deep analysis (see <ref> [8, 9, 10, 11, 12] </ref>). The upper bound is 2:9423N comparisons, and the lower bound (2 + ff)N , where ff is of the order of 2 40 , an extremely small number by most standards.
Reference: [10] <author> D. Dor and U. Zwick, </author> <title> "Selecting the Median", </title> <booktitle> in Proc. 6th Annual ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pp. 28-37, </pages> <year> 1995. </year>
Reference-contexts: This paper also shows that at least 1:5N comparisons are required in the computation of the exact median. For an account of progress since then, see the survey by Mike Paterson [8]. The current best bounds are much tighter and are the product of sophisticated and deep analysis (see <ref> [8, 9, 10, 11, 12] </ref>). The upper bound is 2:9423N comparisons, and the lower bound (2 + ff)N , where ff is of the order of 2 40 , an extremely small number by most standards.
Reference: [11] <author> D. Dor and U. Zwick, </author> <title> "Finding the ffn th Largest Element", </title> <journal> Combinatorica, </journal> <volume> vol. 16, </volume> <pages> pp. 41-58, </pages> <year> 1996. </year>
Reference-contexts: This paper also shows that at least 1:5N comparisons are required in the computation of the exact median. For an account of progress since then, see the survey by Mike Paterson [8]. The current best bounds are much tighter and are the product of sophisticated and deep analysis (see <ref> [8, 9, 10, 11, 12] </ref>). The upper bound is 2:9423N comparisons, and the lower bound (2 + ff)N , where ff is of the order of 2 40 , an extremely small number by most standards.
Reference: [12] <author> D. Dor and U. Zwick, </author> <title> "Median Selection Requires (2 + *)n Comparisons", </title> <type> Technical Report 312/96, </type> <institution> Department of Computer Science, Tel-Aviv University, </institution> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: This paper also shows that at least 1:5N comparisons are required in the computation of the exact median. For an account of progress since then, see the survey by Mike Paterson [8]. The current best bounds are much tighter and are the product of sophisticated and deep analysis (see <ref> [8, 9, 10, 11, 12] </ref>). The upper bound is 2:9423N comparisons, and the lower bound (2 + ff)N , where ff is of the order of 2 40 , an extremely small number by most standards.
Reference: [13] <author> F. F. Yao, </author> <title> "On Lower Bounds for Selection Problems", </title> <type> Technical Report MAC TR-121, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1974. </year>
Reference-contexts: The upper bound is 2:9423N comparisons, and the lower bound (2 + ff)N , where ff is of the order of 2 40 , an extremely small number by most standards. Frances Yao <ref> [13] </ref> showed that computing an approximate median requires (N ) comparisons for any deterministic algorithm. Curiously, this lower bound is easily beaten by resorting to randomization.
Reference: [14] <author> I. Pohl, </author> <title> "A Minimum Storage Algorithm for Computing the Median", </title> <type> Technical Report IBM Research Report RC 2701 (# 12713), </type> <institution> IBM T J Watson Center, </institution> <month> Nov. </month> <year> 1969. </year>
Reference-contexts: For a comprehensive survey of this aspect of the literature, see the survey by Paterson [8]. Ira Pohl <ref> [14] </ref> established that any deterministic algorithm that computes the exact median in one pass needs to store at least N=2 data elements.
Reference: [15] <author> J. I. Munro and M. S. Paterson, </author> <title> "Selection and Sorting with Limited Storage", </title> <journal> Theoretical Computer Science, </journal> <volume> vol. 12, </volume> <pages> pp. 315-323, </pages> <year> 1980. </year>
Reference-contexts: For a comprehensive survey of this aspect of the literature, see the survey by Paterson [8]. Ira Pohl [14] established that any deterministic algorithm that computes the exact median in one pass needs to store at least N=2 data elements. Munro and Paterson <ref> [15] </ref> generalized this and showed that memory to store O (N 1 p ) elements is necessary and sufficient for finding the exact median in p passes. The same bound holds for any -quantile for a constant . <p> Output is invoked on the final set of full buffers. Different buffer collapsing policies correspond to different algorithms. We now describe three interesting policies. 3.4 Collapse Policies Munro and Paterson <ref> [15] </ref> If there is an empty buffer, invoke New; otherwise, in voke Collapse on two buffers having the same weight. Alsabti, Ranka and Singh [18] Fill b=2 empty buffers by invoking New and then invoke Collapse on them. Repeat this b=2 times and invoke Output on the resulting buffers.
Reference: [16] <author> R. Jain and I. Chlamtac, </author> <title> "The P 2 Algorithm for Dynamic Calculation for Quantiles and Histograms without Storing Observations", </title> <journal> CACM, </journal> <volume> vol. 28, </volume> <pages> pp. 1076-1085, </pages> <year> 1985. </year>
Reference-contexts: The same bound holds for any -quantile for a constant . This result motivates a search for algorithms that produce approximate quantiles in a single pass with less memory. 2.2 The Database Literature Jain and Chlamtac <ref> [16] </ref> proposed a simple algorithm for computing quantiles in a single pass using only a constant amount of memory. However, there are no a-priori guarantees on the error. Agrawal and Swami [17] proposed another one pass algorithm.
Reference: [17] <author> R. Agrawal and A. Swami, </author> <title> "A One-Pass Space-Efficient Algorithm for Finding Quantiles", </title> <booktitle> in Proc. 7th Intl. Conf. Management of Data (COMAD-95), </booktitle> <address> Pune, India, </address> <year> 1995. </year>
Reference-contexts: However, there are no a-priori guarantees on the error. Agrawal and Swami <ref> [17] </ref> proposed another one pass algorithm. The idea here is to adjust equi-depth histogram boundaries on the fly when they do not appear to be in balance. Again, there are no strong and a-priori guarantees on error.
Reference: [18] <author> K. Alsabti, S. Ranka, and V. Singh, </author> <title> "A One-Pass Algorithm for Accurately Estimating Quantiles for Disk-Resident Data", </title> <booktitle> in Proc. 23rd VLDB Conference, </booktitle> <address> Athens, Greece, </address> <year> 1997. </year>
Reference-contexts: Agrawal and Swami [17] proposed another one pass algorithm. The idea here is to adjust equi-depth histogram boundaries on the fly when they do not appear to be in balance. Again, there are no strong and a-priori guarantees on error. Alsabti, Ranka and Singh <ref> [18] </ref> propose a data independent single pass algorithm with guaranteed error bounds. We describe this algorithm in more detail in Section 4.4. Random sampling can be a very useful tool in this context. It has been used by DeWitt et al [6] for distributed sorting. <p> Different buffer collapsing policies correspond to different algorithms. We now describe three interesting policies. 3.4 Collapse Policies Munro and Paterson [15] If there is an empty buffer, invoke New; otherwise, in voke Collapse on two buffers having the same weight. Alsabti, Ranka and Singh <ref> [18] </ref> Fill b=2 empty buffers by invoking New and then invoke Collapse on them. Repeat this b=2 times and invoke Output on the resulting buffers. New Algorithm Associate with each buffer X an integer `(X) that denotes its level.
Reference: [19] <author> W. Hoeffding, </author> <title> "Probability Inequalities for Sums of Bounded Random Variables", </title> <journal> American Statistical Association Jornal, </journal> <pages> pp. 13-30, </pages> <month> Mar. </month> <year> 1963. </year>
Reference-contexts: We require the following inequality due to Hoeffding <ref> [19] </ref>: Lemma 6 (Hoeffding's Inequality) Let X 1 ; X 2 ; : : : ; X n be independent random variables with 0 X i 1 for i = 1; 2; : : : ; n. Let X = i=1 X i .
References-found: 17

