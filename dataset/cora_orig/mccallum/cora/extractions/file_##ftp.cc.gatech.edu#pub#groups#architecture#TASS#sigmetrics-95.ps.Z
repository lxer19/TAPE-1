URL: file://ftp.cc.gatech.edu/pub/groups/architecture/TASS/sigmetrics-95.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/Architecture/TASS/tass.html
Root-URL: 
Email: fanand, aman, rama, venkatg@cc.gatech.edu  
Title: On Characterizing Bandwidth Requirements of Parallel Applications  
Author: Anand Sivasubramaniam Aman Singla Umakishore Ramachandran H. Venkateswaran 
Date: May 1995.  
Note: To appear in Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems,  
Address: Atlanta, GA 30332-0280.  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: Synthesizing architectural requirements from an application viewpoint can help in making important architectural design decisions towards building large scale parallel machines. In this paper, we quantify the link bandwidth requirement on a binary hypercube topology for a set of five parallel applications. We use an execution-driven simulator called SPASM to collect data points for system sizes that are feasible to be simulated. These data points are then used in a regression analysis for projecting the link bandwidth requirements for larger systems. The requirements are projected as a function of the following system parameters: number of processors, CPU clock speed, and problem size. These results are also used to project the link bandwidths for other network topologies. Our study quantifies the link bandwidth that has to be made available to limit the network overhead in an application to a specified tolerance level. The results show that typical link bandwidths (200-300 MBytes/sec) found in current commercial parallel architectures (such as Intel Paragon and Cray T3D) would have fairly low network overhead for the applications considered in this study. For two of the applications, this overhead is negligible. For the other applications, this overhead can be limited to about 30% of the execution time provided the problem sizes are increased commensurate with the processor clock speed. The technique presented can be useful to a system architect to synthesize the bandwidth requirements for realizing well-balanced parallel architectures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. S. Adve and M. K. Vernon. </author> <title> Performance analysis of mesh interconnection networks with deterministic routing. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(3) </volume> <pages> 225-246, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The results suggest that low-dimensional networks are preferred (based on physical and technological constraints) when the network contention is ignored or when the workload (the application) exhibits sufficient network locality; and that higher dimensional networks may be needed otherwise. Adve and Vernon <ref> [1] </ref> show using analytical models that network locality has an important role to play in the performance of the mesh. Since network requirements are sensitive to the workload, it is necessary to study them in the context of real applications. <p> Such a projection assumes that the communication in an application is devoid of any network locality and that each message crosses the bisection. But we know that applications normally tend to exploit network locality and the projection can thus become very pessimistic <ref> [1] </ref>. With a little knowledge about the communication behavior of applications, one may be able to reduce the degree of pessimism. In both FFT and IS, every processor communicates with every other processor, and thus only 50% of the messages cross the bisection.
Reference: [2] <author> A. Agarwal. </author> <title> Limits on Interconnection Network Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Hence, in order to quantify requirements that limit network overheads (latency and contention) to an acceptable level, it is necessary to study the impact of link bandwidth and network connectivity on these overheads. Dally [8] and Agarwal <ref> [2] </ref> present analytical models to study the impact of network connectivity and link bandwidth for k-ary n-cube networks. <p> The cube represents a highly scalable network where the bisection bandwidth grows linearly with the number of processors. Even though cubes of 1024 nodes have been built [11], cost and technology factors often play an important role in its physical realization. Agarwal <ref> [2] </ref> and Dally [8] show that wire delays (due to increased wire lengths associated with planar layouts) of higher dimensional networks make low dimensional networks more viable.
Reference: [3] <editor> R. Alverson et al. </editor> <booktitle> The Tera Computer System. In Proceedings of the ACM 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <address> Amsterdam, Netherlands, </address> <year> 1990. </year>
Reference-contexts: Agarwal [2] and Dally [8] show that wire delays (due to increased wire lengths associated with planar layouts) of higher dimensional networks make low dimensional networks more viable. The 2-dimensional [15] and 3--dimensional <ref> [17, 3] </ref> toroids are common topologies used in current day networks, and it would be interesting to project link bandwidth requirements for these topologies. A metric that is often used to compare different networks is the bisection bandwidth available per processor.
Reference: [4] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: We have chosen five parallel applications in this study that exhibit different characteristics and are representative of many scientific computations. Three of the applications (EP, IS and CG) are from the NAS parallel benchmark suite <ref> [4] </ref>; CHOLESKY is from the SPLASH benchmark suite [23]; and FFT is the well-known Fast Fourier Transform algorithm. EP and FFT are well-structured applications with regular communication patterns determinable at compile-time, with the difference that EP has a higher computation to communication ratio. <p> As a result, the bandwidth requirements are expected to decrease linearly with problem size. Given that real world problem sizes for this application are of the order of n = 2 28 <ref> [4] </ref>, a very low link bandwidth (less than 1 MByte/sec) would suffice to yield an efficiency close to 100%. 50% ovhd. 30% ovhd. 10% ovhd. p=4 &lt; 1.0 &lt; 1.0 &lt; 1.0 p=16 &lt; 1.0 &lt; 1.0 &lt; 1.0 p=64 &lt; 1.0 &lt; 1.0 1.17 p=1024 - Table 1: EP: Impact <p> Using these results, the bandwidth requirements for IS are projected in Table 6 for a 1024 node system and a problem size of 2 23 that is representative of a real world problem <ref> [4] </ref>. This table shows that bandwidth requirements of IS are considerably high. We may at best be able to operate currently at around 50% network overhead range with 33 MHz processors given that link bandwidth of state-of-the-art networks is around 200-300 MBytes/sec. <p> There is also a decreased probability of reusing a fetched data item for computing another row. These complicated interactions are to a large extent dependent on the input data and are difficult to analyze statically. We use the data sets supplied with the NAS benchmarks <ref> [4] </ref>. The results from our simulation are given in Table 11. We observe that the effect of lower local computation, and lesser data reuse has a more significant impact in increasing the communication requirements for larger systems. <p> Table 13 shows the requirements for two different problem sizes. For the 1400 fi 1400 problem, the sparsity factor is 0.04, while the sparsity factor for the 5600 fi 5600 problem is 0.02. The corresponding factor for the 14000 fi 14000 problem suggested in <ref> [4] </ref> is 0.1 and we scale down the bandwidth requirements accordingly in Table 14 for a 1024 node system. The results suggest that we may be able to limit the overheads to within 50% of the execution time with existing technology.
Reference: [5] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. </author> <title> PROTEUS : A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT-LCS-TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA 02139, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: The input to the simulator are parallel applications written in C. These programs are preprocessed (to label shared memory accesses), the compiled assembly code is augmented with cycle counting instructions, and the assembled binary is linked with the simulator code. As with other recent simulators <ref> [5, 9, 6, 19] </ref>, bulk of the instructions is executed at the speed of the native processor (the SPARC in this case) and only instructions (such as LOADs and STOREs on a shared memory platform or SENDs and RECEIVEs on a message-passing platform) that may potentially involve a network access are
Reference: [6] <author> R. G. Covington, S. Madala, V. Mehta, J. R. Jump, and J. B. Sinclair. </author> <title> The Rice parallel processing testbed. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1988 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <address> Santa Fe, NM, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: The input to the simulator are parallel applications written in C. These programs are preprocessed (to label shared memory accesses), the compiled assembly code is augmented with cycle counting instructions, and the assembled binary is linked with the simulator code. As with other recent simulators <ref> [5, 9, 6, 19] </ref>, bulk of the instructions is executed at the speed of the native processor (the SPARC in this case) and only instructions (such as LOADs and STOREs on a shared memory platform or SENDs and RECEIVEs on a message-passing platform) that may potentially involve a network access are
Reference: [7] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Since network requirements are sensitive to the workload, it is necessary to study them in the context of real applications. The RISC ideology clearly illustrates the importance of using real applications in synthesizing architectural requirements. Several researchers have used this approach for parallel architectural studies <ref> [20, 7, 13] </ref>. Cypher et al. [7] use a range of scientific applications in quantifying the processing, memory, communication and I/O requirements. They present the communication requirements in terms of the number of messages exchanged between processors and the volume (size) of these messages. <p> The RISC ideology clearly illustrates the importance of using real applications in synthesizing architectural requirements. Several researchers have used this approach for parallel architectural studies [20, 7, 13]. Cypher et al. <ref> [7] </ref> use a range of scientific applications in quantifying the processing, memory, communication and I/O requirements. They present the communication requirements in terms of the number of messages exchanged between processors and the volume (size) of these messages. <p> As identified in [22], communication in parallel applications may be categorized by the following attributes: communication volume, the communication pattern, the communication frequency and the ability to overlap communication with computation. A static analysis of the communication as conducted in <ref> [7] </ref> fails to capture the last two attributes, making it very difficult to quantify the contention in the system. The importance of simulation in capturing the dynamics of parallel system (an application-architecture combination) behavior has been clearly illustrated in [12, 22, 25].
Reference: [8] <author> W. J. Dally. </author> <title> Performance analysis of k-ary n-cube interconnection networks. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 39(6) </volume> <pages> 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Hence, in order to quantify requirements that limit network overheads (latency and contention) to an acceptable level, it is necessary to study the impact of link bandwidth and network connectivity on these overheads. Dally <ref> [8] </ref> and Agarwal [2] present analytical models to study the impact of network connectivity and link bandwidth for k-ary n-cube networks. <p> The cube represents a highly scalable network where the bisection bandwidth grows linearly with the number of processors. Even though cubes of 1024 nodes have been built [11], cost and technology factors often play an important role in its physical realization. Agarwal [2] and Dally <ref> [8] </ref> show that wire delays (due to increased wire lengths associated with planar layouts) of higher dimensional networks make low dimensional networks more viable.
Reference: [9] <author> H. Davis, S. R. Goldschmidt, and J. L. Hennessy. </author> <title> Multiprocessor Simulation and Tracing Using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages II 99-107, </pages> <year> 1991. </year>
Reference-contexts: The input to the simulator are parallel applications written in C. These programs are preprocessed (to label shared memory accesses), the compiled assembly code is augmented with cycle counting instructions, and the assembled binary is linked with the simulator code. As with other recent simulators <ref> [5, 9, 6, 19] </ref>, bulk of the instructions is executed at the speed of the native processor (the SPARC in this case) and only instructions (such as LOADs and STOREs on a shared memory platform or SENDs and RECEIVEs on a message-passing platform) that may potentially involve a network access are
Reference: [10] <author> A. Gupta and V. Kumar. </author> <title> The scalability of FFT on parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(8) </volume> <pages> 922-932, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Hence, in projecting the requirements for a 1024-node system, link bandwidths of around 100-150 MBytes/sec would suffice to limit the network overheads to less than 10% of the execution time (see Table 10). The results shown in the above tables agree with theoretical results presented in <ref> [10] </ref> where the authors show that FFT is scalable on the cube topology and the achievable efficiency is only limited by the ratio of the CPU clock speed and the link bandwidth. 50% ovhd. 30% ovhd. 10% ovhd. p=4 &lt; 1.0 6.40 16.35 p=16 &lt; 1.0 7.52 16.75 p=64 &lt; 1.0
Reference: [11] <author> J. L. Gustafson, G. R. Montry, and R. E. Benner. </author> <title> Development of Parallel Methods for a 1024-node Hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference-contexts: All of the above link bandwidth results have been presented for the binary hypercube network topology. The cube represents a highly scalable network where the bisection bandwidth grows linearly with the number of processors. Even though cubes of 1024 nodes have been built <ref> [11] </ref>, cost and technology factors often play an important role in its physical realization. Agarwal [2] and Dally [8] show that wire delays (due to increased wire lengths associated with planar layouts) of higher dimensional networks make low dimensional networks more viable.
Reference: [12] <author> W. B. Ligon III and U. Ramachandran. </author> <title> Simulating interconnection networks in RAW. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: A static analysis of the communication as conducted in [7] fails to capture the last two attributes, making it very difficult to quantify the contention in the system. The importance of simulation in capturing the dynamics of parallel system (an application-architecture combination) behavior has been clearly illustrated in <ref> [12, 22, 25] </ref>. In particular, using an execution-driven simulator, one can faithfully capture all the attributes of communication that are important to network requirements synthesis. <p> The importance of simulation in capturing the dynamics of parallel system (an application-architecture combination) behavior has been clearly illustrated in [12, 22, 25]. In particular, using an execution-driven simulator, one can faithfully capture all the attributes of communication that are important to network requirements synthesis. For example, in <ref> [12] </ref> the authors use an execution driven simulator to study k-ary n-cube networks in the context of applications drawn from image understanding, and show the impact of application characteristics on the choice of the network topology. We take a similar approach to deriving the network requirements in this study. <p> Tolerance is the ability of an application to hide network overheads by overlapping computation with communication. Modeling all these attributes of communication in a parallel application is extremely difficult by simple static analysis. Further, the dynamic access patterns exhibited by many applications makes modeling more complex. Several researchers <ref> [22, 25, 12] </ref> have observed the importance of simulation for capturing the communication behavior of applications.
Reference: [13] <author> W. B. Ligon III and U. Ramachandran. </author> <title> Evaluating multigauge architectures for computer vision. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21 </volume> <pages> 323-333, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Since network requirements are sensitive to the workload, it is necessary to study them in the context of real applications. The RISC ideology clearly illustrates the importance of using real applications in synthesizing architectural requirements. Several researchers have used this approach for parallel architectural studies <ref> [20, 7, 13] </ref>. Cypher et al. [7] use a range of scientific applications in quantifying the processing, memory, communication and I/O requirements. They present the communication requirements in terms of the number of messages exchanged between processors and the volume (size) of these messages.
Reference: [14] <author> Intel Corporation, </author> <title> Oregon. Paragon User's Guide, </title> <year> 1993. </year>
Reference-contexts: Using regression analysis and application knowledge, we extrapolate requirements for larger systems of 1024 processors and other network topologies. The results suggest that existing link bandwidth of 200-300 MBytes/sec available on machines like Intel Paragon <ref> [14] </ref> and Cray T3D [17] can easily sustain the requirements of two applications (EP and FFT) even on high-speed processors of the future. For the other three, one may be able to maintain network overheads at an acceptable level if the problem size is increased commensurate with the processing speed. <p> Using regression analysis and analytical techniques, we projected requirements for large scale parallel systems with 1024 processors and other network topologies. The results show that existing link bandwidth of 200-300 MBytes/sec available on machines like Intel Paragon <ref> [14] </ref> and Cray T3D [17] can sustain high speed applications with fairly low network overhead. For applications like EP and FFT, this overhead is negligible.
Reference: [15] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W-D Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Agarwal [2] and Dally [8] show that wire delays (due to increased wire lengths associated with planar layouts) of higher dimensional networks make low dimensional networks more viable. The 2-dimensional <ref> [15] </ref> and 3--dimensional [17, 3] toroids are common topologies used in current day networks, and it would be interesting to project link bandwidth requirements for these topologies. A metric that is often used to compare different networks is the bisection bandwidth available per processor.
Reference: [16] <institution> Microelectronics and Computer Technology Corporation, Austin, TX 78759. </institution> <note> CSIM User's Guide, </note> <year> 1990. </year>
Reference-contexts: In this study, we use an execution-driven simulator called SPASM (Simulator for Parallel Architectural Scalability Measurements) [27, 25] that enables us to accurately model the behavior of applications on a number of simulated hardware platforms. SPASM has been written using CSIM <ref> [16] </ref>, a process oriented sequential simulation package, and currently runs on SPARCstations. The input to the simulator are parallel applications written in C.
Reference: [17] <author> W. Oed. </author> <title> The Cray Research Massively Parallel Processor System Cray T3D, </title> <year> 1993. </year>
Reference-contexts: Using regression analysis and application knowledge, we extrapolate requirements for larger systems of 1024 processors and other network topologies. The results suggest that existing link bandwidth of 200-300 MBytes/sec available on machines like Intel Paragon [14] and Cray T3D <ref> [17] </ref> can easily sustain the requirements of two applications (EP and FFT) even on high-speed processors of the future. For the other three, one may be able to maintain network overheads at an acceptable level if the problem size is increased commensurate with the processing speed. <p> Agarwal [2] and Dally [8] show that wire delays (due to increased wire lengths associated with planar layouts) of higher dimensional networks make low dimensional networks more viable. The 2-dimensional [15] and 3--dimensional <ref> [17, 3] </ref> toroids are common topologies used in current day networks, and it would be interesting to project link bandwidth requirements for these topologies. A metric that is often used to compare different networks is the bisection bandwidth available per processor. <p> Using regression analysis and analytical techniques, we projected requirements for large scale parallel systems with 1024 processors and other network topologies. The results show that existing link bandwidth of 200-300 MBytes/sec available on machines like Intel Paragon [14] and Cray T3D <ref> [17] </ref> can sustain high speed applications with fairly low network overhead. For applications like EP and FFT, this overhead is negligible. For the other applications, this overhead can be limited to about 30% of the execution time provided the problem sizes are increased commensurate with the processor clock speed.
Reference: [18] <author> U. Ramachandran, G. Shah, S. Ravikumar, and J. Muthuku-marasamy. </author> <title> Scalability study of the KSR-1. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages I-237-240, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: On the other hand, if the number of buckets is maintained constant, it may be possible to sustain bandwidth requirements by increasing the problem size linearly with the processing speed. In <ref> [18] </ref>, the authors show that the applications EP, IS, and CG scale well on a 32-node KSR-1. Although our results suggest that these applications may incur overheads affecting their scalability, this does not contradict the results presented in [18] since the implications of our study are for larger systems built with <p> In <ref> [18] </ref>, the authors show that the applications EP, IS, and CG scale well on a 32-node KSR-1. Although our results suggest that these applications may incur overheads affecting their scalability, this does not contradict the results presented in [18] since the implications of our study are for larger systems built with much faster processors. All of the above link bandwidth results have been presented for the binary hypercube network topology. The cube represents a highly scalable network where the bisection bandwidth grows linearly with the number of processors.
Reference: [19] <author> S. K. Reinhardt et al. </author> <title> The Wisconsin Wind Tunnel : Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1993 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <address> Santa Clara, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The input to the simulator are parallel applications written in C. These programs are preprocessed (to label shared memory accesses), the compiled assembly code is augmented with cycle counting instructions, and the assembled binary is linked with the simulator code. As with other recent simulators <ref> [5, 9, 6, 19] </ref>, bulk of the instructions is executed at the speed of the native processor (the SPARC in this case) and only instructions (such as LOADs and STOREs on a shared memory platform or SENDs and RECEIVEs on a message-passing platform) that may potentially involve a network access are
Reference: [20] <author> E. Rothberg, J. P. Singh, and A. Gupta. </author> <title> Working sets, cache sizes and node granularity issues for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Since network requirements are sensitive to the workload, it is necessary to study them in the context of real applications. The RISC ideology clearly illustrates the importance of using real applications in synthesizing architectural requirements. Several researchers have used this approach for parallel architectural studies <ref> [20, 7, 13] </ref>. Cypher et al. [7] use a range of scientific applications in quantifying the processing, memory, communication and I/O requirements. They present the communication requirements in terms of the number of messages exchanged between processors and the volume (size) of these messages. <p> Each node in the system has a piece of the globally shared memory and a 2-way set-associative private cache (64KBytes with 32 byte blocks). The cache is maintained sequentially consistent using an invalidation-based fully-mapped directory-based cache coherence scheme. Rothberg et al. <ref> [20] </ref> 1 Efficiency is defined as speedup (p)=p where p is the number of processors.
Reference: [21] <institution> SAS Institute Inc., Cary, </institution> <address> NC 27512. </address> <note> SAS/STAT User's Guide, </note> <year> 1988. </year>
Reference-contexts: In cases where such a static analysis is not possible (due to the dynamic nature of the execution), we perform a non-linear regression analysis of the simulation results using a multivariate secant method with a 95% confidence interval in the SAS <ref> [21] </ref> statistics package.
Reference: [22] <author> J. P. Singh, E. Rothberg, and A. Gupta. </author> <title> Modeling communication in parallel algorithms: </title> <booktitle> A fruitful interaction between theory and systems? In Proceedings of the Sixth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1994. </year>
Reference-contexts: Cypher et al. [7] use a range of scientific applications in quantifying the processing, memory, communication and I/O requirements. They present the communication requirements in terms of the number of messages exchanged between processors and the volume (size) of these messages. As identified in <ref> [22] </ref>, communication in parallel applications may be categorized by the following attributes: communication volume, the communication pattern, the communication frequency and the ability to overlap communication with computation. <p> A static analysis of the communication as conducted in [7] fails to capture the last two attributes, making it very difficult to quantify the contention in the system. The importance of simulation in capturing the dynamics of parallel system (an application-architecture combination) behavior has been clearly illustrated in <ref> [12, 22, 25] </ref>. In particular, using an execution-driven simulator, one can faithfully capture all the attributes of communication that are important to network requirements synthesis. <p> briefly describes the hardware platform and applications used in this study; section 4 presents results from our experiments along with an analysis of bandwidth requirements as a function of system parameters; section 5 summarizes the implication of these results; and section 6 presents concluding remarks. 2 Approach As observed in <ref> [22] </ref>, communication in an application may be characterized by four attributes. Volume refers to the number and size of messages. The communication pattern in the application determines the source-destination pairs for the messages, and reflects on the application's ability to exploit network locality. <p> Tolerance is the ability of an application to hide network overheads by overlapping computation with communication. Modeling all these attributes of communication in a parallel application is extremely difficult by simple static analysis. Further, the dynamic access patterns exhibited by many applications makes modeling more complex. Several researchers <ref> [22, 25, 12] </ref> have observed the importance of simulation for capturing the communication behavior of applications.
Reference: [23] <author> J. P. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stan-ford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stan-ford University, </institution> <year> 1991. </year>
Reference-contexts: We have chosen five parallel applications in this study that exhibit different characteristics and are representative of many scientific computations. Three of the applications (EP, IS and CG) are from the NAS parallel benchmark suite [4]; CHOLESKY is from the SPLASH benchmark suite <ref> [23] </ref>; and FFT is the well-known Fast Fourier Transform algorithm. EP and FFT are well-structured applications with regular communication patterns determinable at compile-time, with the difference that EP has a higher computation to communication ratio.
Reference: [24] <author> A. Sivasubramaniam, U. Ramachandran, and H. Venkat-eswaran. </author> <title> A comparative evaluation of techniques for studying parallel system performance. </title> <type> Technical Report GIT-CC-94/38, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: We can thus vary a range of system parameters and study their impact on application performance. The main problem with the execution-driven simulation approach is the tremendous resource (both space and time) requirement in simulating large parallel systems. Related studies <ref> [28, 24] </ref> address this problem and show how it may be alleviated by augmenting simulation with other evaluation techniques. SPASM gives a wide range of statistical information about the execution of the program.
Reference: [25] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> An Approach to Scalability Study of Shared Memory Parallel Systems. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1994 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: A static analysis of the communication as conducted in [7] fails to capture the last two attributes, making it very difficult to quantify the contention in the system. The importance of simulation in capturing the dynamics of parallel system (an application-architecture combination) behavior has been clearly illustrated in <ref> [12, 22, 25] </ref>. In particular, using an execution-driven simulator, one can faithfully capture all the attributes of communication that are important to network requirements synthesis. <p> We take a similar approach to deriving the network requirements in this study. Using an execution-driven simulation platform called SPASM <ref> [27, 25] </ref>, we simulate the execution of five parallel applications on an architectural platform with a binary hypercube network topology. We vary the link bandwidth on the hypercube and quantify its impact on application performance. <p> Tolerance is the ability of an application to hide network overheads by overlapping computation with communication. Modeling all these attributes of communication in a parallel application is extremely difficult by simple static analysis. Further, the dynamic access patterns exhibited by many applications makes modeling more complex. Several researchers <ref> [22, 25, 12] </ref> have observed the importance of simulation for capturing the communication behavior of applications. <p> Further, the dynamic access patterns exhibited by many applications makes modeling more complex. Several researchers [22, 25, 12] have observed the importance of simulation for capturing the communication behavior of applications. In this study, we use an execution-driven simulator called SPASM (Simulator for Parallel Architectural Scalability Measurements) <ref> [27, 25] </ref> that enables us to accurately model the behavior of applications on a number of simulated hardware platforms. SPASM has been written using CSIM [16], a process oriented sequential simulation package, and currently runs on SPARCstations. The input to the simulator are parallel applications written in C. <p> Hence, in both these phases, the communication is expected to grow as O (p) with increase in processors. Further, the computation performed by a processor decreases with an increase in processors, but the rate is less than linear owing to algorithmic deficiencies in the problem <ref> [25] </ref>. These factors combine to yield a considerable bandwidth requirement for larger systems (see Table 3), if we are willing to tolerate less than 10% network overheads.
Reference: [26] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> On characterizing bandwidth requirements of parallel applications. </title> <type> Technical Report GIT-CC-94/31, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: While a certain number of rows of the matrix in CG is assigned to a processor at compile time (static scheduling), CHOLESKY uses a dynamically maintained queue of runnable tasks. Further details on the applications can be found in <ref> [26] </ref>. 4 Results and Analysis In this section, we present results from our simulation experiments. Using these results, we quantify the link bandwidth necessary to support the efficient performance of these applications and project the bandwidth requirements for building large-scale parallel systems with a binary hypercube topology. <p> There are two dominant phases in the execution that account for the bulk of the communication <ref> [26] </ref>. In the first, a processor accesses a part of the local buckets of every other processor in making updates to the global buckets allotted to it. <p> FFT: Impact of Problem Size on Link Bandwidth (in MBytes/sec) 50% ovhd. 30% ovhd. 10% ovhd. s=33MHz - 5.15 21.64 s=300MHz - 17.38 144.50 Table 10: FFT: Link Bandwidth Projections (in MBytes/sec) CG The main communication in CG occurs in the multiplication of a sparse matrix with a dense vector <ref> [26] </ref>. Each processor performs this operation for a contiguous set of rows allocated to it. The elements of the vector that are needed by a processor to perform this operation depend on the distribution of non-zero elements in the matrix and may involve external accesses. <p> p=64, s=33 MHz Table 13: CG: Impact of Problem Size on Link Bandwidth (in MBytes/sec) 50% ovhd. 30% ovhd. 10% ovhd. s=33MHz 34.87 120.04 247.33 s=300MHz 269.7 684.41 2035.64 Table 14: CG: Link Bandwidth Projections (in MBytes/sec) CHOLESKY This application performs a Cholesky factorization of a sparse positive definite matrix <ref> [26] </ref>. Each processor while working on a column will need to access the non-zero elements in the same row position of other columns. Once a non-local element is fetched, the processor can reuse it for the next column that it has to process.
Reference: [27] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> A Simulation-based Scalability Study of Parallel Systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 411-426, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: We take a similar approach to deriving the network requirements in this study. Using an execution-driven simulation platform called SPASM <ref> [27, 25] </ref>, we simulate the execution of five parallel applications on an architectural platform with a binary hypercube network topology. We vary the link bandwidth on the hypercube and quantify its impact on application performance. <p> Further, the dynamic access patterns exhibited by many applications makes modeling more complex. Several researchers [22, 25, 12] have observed the importance of simulation for capturing the communication behavior of applications. In this study, we use an execution-driven simulator called SPASM (Simulator for Parallel Architectural Scalability Measurements) <ref> [27, 25] </ref> that enables us to accurately model the behavior of applications on a number of simulated hardware platforms. SPASM has been written using CSIM [16], a process oriented sequential simulation package, and currently runs on SPARCstations. The input to the simulator are parallel applications written in C.
Reference: [28] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> Abstracting network characteristics and locality properties of parallel systems. </title> <booktitle> In Proceedings of the First International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 54-63, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: We can thus vary a range of system parameters and study their impact on application performance. The main problem with the execution-driven simulation approach is the tremendous resource (both space and time) requirement in simulating large parallel systems. Related studies <ref> [28, 24] </ref> address this problem and show how it may be alleviated by augmenting simulation with other evaluation techniques. SPASM gives a wide range of statistical information about the execution of the program. <p> In an earlier study <ref> [28] </ref>, we show that for an invalidation-based protocol with a full-map directory, the coherence maintenance overhead due to the protocol is not significant for a range of applications.
Reference: [29] <author> H. Sullivan and T. R. Bashkow. </author> <title> A large scale, homogenous, </title> <booktitle> fully-distributed parallel machine. In Proceedings of the 4th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 105-117, </pages> <month> March </month> <year> 1977. </year>
Reference-contexts: The synchronization primitive supported in hardware is a test-and-set operation and applications use a test-test-and-set to implement higher level synchronization. The study is conducted for a binary hypercube interconnect. The hypercube is assumed to have serial (1-bit wide) unidirectional links and uses the e-cube routing algorithm <ref> [29] </ref>. Messages are circuit-switched using a wormhole routing strategy and the switching delay is assumed to be zero. We simulate the network in its entirety in the context of the given applications.
Reference: [30] <author> D. A. Wood et al. </author> <title> Mechanisms for cooperative shared memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Speedup (p) is the ratio of the time taken to execute the parallel application on 1 processor to the time taken to execute the same on p processors. show that a cache of moderate size (64KBytes) suffices to capture the working set in many applications, and Wood et al. <ref> [30] </ref> show that for several applications, the execution time is not significantly different across cache coherence protocols. In an earlier study [28], we show that for an invalidation-based protocol with a full-map directory, the coherence maintenance overhead due to the protocol is not significant for a range of applications.
Reference: [31] <author> J. C. Wyllie. </author> <title> The Complexity of Parallel Computations. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1979. </year>
Reference-contexts: This is the time that would be taken by an execution of the parallel program on the target parallel machine. SPASM also gives the ideal time, which is the time taken by the parallel program to execute on an ideal machine such as the PRAM <ref> [31] </ref>. This metric includes the algorithmic overheads but does not include any overheads arising from architectural limitations.
References-found: 31

