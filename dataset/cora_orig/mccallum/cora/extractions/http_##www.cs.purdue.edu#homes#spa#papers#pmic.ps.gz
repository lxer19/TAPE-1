URL: http://www.cs.purdue.edu/homes/spa/papers/pmic.ps.gz
Refering-URL: http://www.cs.purdue.edu/homes/spa/current.html
Root-URL: http://www.cs.purdue.edu
Author: Mikhail Atallah Yann Genin Wojciech Szpankowski 
Keyword: Index Terms: Lossy Lempel-Ziv scheme, approximate pattern matching, image compression, generalized Renyi entropy, Hamming and square-root distortion, mixing probabilistic model, string editing and algorithms on words, Fast Fourier Transform, image derivative, JPEG, wavelets and fractal image compression.  
Address: W. Lafayette, IN 47907 31057 Toulouse W. Lafayette, IN 47907 U.S.A. France U.S.A.  
Affiliation: Dept. of Computer Science Ecole de la Meteorologie Dept. of Computer Science Purdue University 42, av. Coriolis Purdue University  
Date: 26, 1996  
Note: July  Research supported by the National Science Foundation under Grant CCR-9202807. This work was supported by NSF Grants NCR-9415491 and CCR-9201078, and in part by NATO Collaborative Grant CGR.950060.  
Abstract: PATTERN MATCHING IMAGE COMPRESSION: Algorithmic and Empirical Results Abstract We propose a non-transform image compression scheme based on approximate pattern matching, that we name Pattern Matching Image Compression (PMIC). The main idea behind it is a lossy extension of the Lempel-Ziv data compression scheme in which one searches for the longest prefix of an uncompressed image that approximately occurs in the already processed imager (e.g., in the sense of the Hamming distance or, alternatively, of the square error distortion). This main algorithm is enhanced with several new features such as searching for reverse approximate matching, recognizing substrings in images that are additively shifted versions of each other, introducing a variable and adaptive maximum distortion level D, and so forth. These enhancements are crucial to the overall quality of our scheme, and their efficient implementation leads to algorithmic results of interest in their own right. Both algorithmic and experimental results are presented. Our scheme turns out to be competitive with JPEG and wavelet compression for graphical and photographical images. A unique feature of the proposed algorithm is that an asymptotic performance of the scheme can be theoretically established. More precisely, under stationary mixing probabilistic model of an image and fixed maximum distortion level D, it is shown that the compression ratio is asymptotically equal to the so called generalized Renyi entropy r 0 (D). This entropy is in general smaller than the optimal rate distortion function R(D), but there is numerical evidence that these two quantities do not differ too much for small and medium values of D. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Abrahamson, </author> <title> Generalized String Matching, </title> <journal> SIAM J. Comput., </journal> <volume> 16, </volume> <pages> 1039-1051, </pages> <year> 1987. </year>
Reference-contexts: Observe that one can use Fast Fourier Transform (FFT) to find the "compare-add" convolution of two strings (this is similar to the usual convolution of two sequences, except that the "product" of two symbols is 1 if they are equal and zero otherwise) in time O (jAjn log n) (cf. <ref> [1] </ref>) where jAj is the size of the alphabet. This was used in [1] to obtain an O (n 1:5 polylog (n)) time algorithm for the case when jAj = n. <p> convolution of two strings (this is similar to the usual convolution of two sequences, except that the "product" of two symbols is 1 if they are equal and zero otherwise) in time O (jAjn log n) (cf. <ref> [1] </ref>) where jAj is the size of the alphabet. This was used in [1] to obtain an O (n 1:5 polylog (n)) time algorithm for the case when jAj = n. Here we seek to achieve O (n polylog (n)) time irrespective of the size of the alphabet, so we propose below an approximate algorithm.
Reference: [2] <author> A. Apostolico and W. Szpankowski, </author> <title> Self-Alignments in Words and Their Applications, </title> <journal> J. Algorithms, </journal> <volume> 13, </volume> <month> 446-467 </month> <year> (1992). </year>
Reference-contexts: an additional O (log ^ k) factor comes from the binary search (see end of Sec. 3.2 for a comment on the approximate nature of such a binary search). 3.4 Suffix Tree Algorithm Finally, we briefly mention an O (N 2 ) approximate algorithm based on a suffix tree (cf. <ref> [2, 20] </ref>). The main idea is quite simple. We first build a suffix tree of the whole image N fi N (either using McCreight's algorithm in O (N 2 ) time [20] or by a brute force approach in O (N 2 log N ) time [2]). <p> The main idea is quite simple. We first build a suffix tree of the whole image N fi N (either using McCreight's algorithm in O (N 2 ) time [20] or by a brute force approach in O (N 2 log N ) time <ref> [2] </ref>). Once the suffix tree is constructed, one can find exact longest prefix in a row starting at any position in O (N ) steps or with high probability in O (log N ) steps.
Reference: [3] <author> R. Arratia and M. Waterman, </author> <title> The Erdos-Renyi Strong Law for Pattern Matching with Given Proportion of Mismatches, </title> <journal> Annals of Probability, </journal> <volume> 17, </volume> <month> 1152-1169 </month> <year> (1989). </year>
Reference-contexts: In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. We should mention here the paper of Steinberg and Gutman [24], and Luczak and Szpankowski [18, 19], as well as recent results of Yang and Kieffer [29]. Arratia and Waterman <ref> [3] </ref> also analyzed an approximate pattern matching problem in the context of molecular biology. The reader is referred to the survey [11] and/or a recent book by Crochemore and Rytter [6] on string matching algorithms. As will become apparent soon (cf.
Reference: [4] <author> M. Atallah, P. Jacquet and W. Szpankowski, </author> <title> Pattern matching with mismatches: A probabilistic analysis and a randomized algorithm, </title> <journal> Proc. Combinatorial Pattern Matching, Tucson, </journal> <volume> LNCS 644, </volume> <pages> 27-40, </pages> <publisher> Springer-Verlag 1992. </publisher>
Reference: [5] <author> T. Berger, </author> <title> Rate Distortion Theory: A Mathematical Basis for Data Compression, </title> <address> En-glewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1971. </year>
Reference-contexts: We only consider single-letter fidelity measures, that is, such that d (x n ; ^x n ) = n i=1 Furthermore, in order to use properly the rate distortion theory we impose the following two conditions on the fidelity measure (cf. <ref> [5, 14, 24] </ref>): (F1) Subadditivity. For any two integers n; m, and given vectors x n+m , y n+m we postu late that d (x n+m ; y n+m ) n + m m d (x n+m n+1 ) : (1) (F2) Finiteness.
Reference: [6] <author> M. Crochemore and W. Rytter, </author> <title> Text Algorithms, </title> <publisher> Oxford University Press, </publisher> <address> New York (1995). </address>
Reference-contexts: Arratia and Waterman [3] also analyzed an approximate pattern matching problem in the context of molecular biology. The reader is referred to the survey [11] and/or a recent book by Crochemore and Rytter <ref> [6] </ref> on string matching algorithms. As will become apparent soon (cf. Section 3), most of the algorithms for approximate pattern matching proposed in the literature so far will not be applicable to our situation of lossy image compression. The paper is organized as follows.
Reference: [7] <author> T. Cornsweet, </author> <title> Visual Perception, </title> <publisher> Academic Press, </publisher> <address> New York (1970). </address>
Reference-contexts: Variable Maximum Distortion. It is well known that human eyes can easily distinguish an "undesired" pattern in a low frequency (constant) background while the same pattern might be almost invisible to human eyes in high frequency (quickly varying) background (cf. <ref> [7, 13] </ref>). Therefore, we used a low value of maximum distortion, say D 1 , for slowly varying background, and a high value, say D 2 , for quickly varying background. To recognize these two different situations, we review the notion of the derivative D of an image I. <p> For example, in our experimental study, the latter situation occured when comparing "variable-D" versus "constant-D" methods. In clearly indicates that "variable-D" method is much better. In such a situation, we either must resort to sophisticated visual tests such as Contrast Sensitivity Function (CSF) (cf. 21 <ref> [7] </ref>) or use another measure of comparison. We choose the latter, which is basically a weighted L b norm for some b &gt; 0 (cf. [13]). We first observe that visual distortion is most visible for low frequency (slowly varying) background.
Reference: [8] <author> R. DeVore, B. Jawerth, and B. Lucier, </author> <title> Image Compression Through Wavelet Transform Coding, </title> <journal> IEEE Trans. Information Theory, </journal> <volume> 38, </volume> <month> 719-746 </month> <year> (1992). </year>
Reference-contexts: The algorithms we give for the Hamming distance case are quite different from those we give for the square error distortion. Our general practical conclusion can be summarized as follows: The proposed non-transform scheme achieves compression ratios comparable to JPEG (UNIX implementation), wavelet compression (implementation based on <ref> [8] </ref>), and better than fractal image compression (implemented according to [10]). PMIC works particularly well for images with high frequencies (e.g., containing sharp egdes, etc). <p> No inverse transforms, no multiplications it is indeed a very simple decompression algorithm. 20 4.2 Experimental Results In order to assess the quality of our PMIC scheme, we performed a series of experiments, and compared PMIC with the standard JPEG (UNIX implementation), wavelet compression (cf. <ref> [8] </ref>), and also fractal compression (implemented as described in [10]). Before presenting our experimental results, one must decide about metrics of comparison. It is natural to use (real) compression (scale) = (size of the original file)/(size of the compressed file) as one metric.
Reference: [9] <author> W. Finamore, M. Carvalho, and J. Kieffer, </author> <title> Lossy Compression with the Lempel-Ziv Algorithm, </title> <booktitle> 11th Brasilian Telecommunication Conference, </booktitle> <month> 141-146 </month> <year> (1993). </year>
Reference-contexts: Unfortunately, all of these schemes were prohibitively expensive from the computational complexity point of view. Recently, a quest for asymptotically optimal and computationally attractive lossy data compression based on approximate pattern matching has begun <ref> [9, 23, 24, 30] </ref>. But one may wonder whether a practical and optimal lossy compression exists at all. <p> There is a huge volume of knowledge on image processing (cf. [12, 13, 22]) but the majority of image compression techniques are based on transform methods. On the other 3 hand, lossy Lempel-Ziv schemes based on approximate pattern matching were discussed in <ref> [9, 16, 18, 19, 23, 24, 29] </ref>, however, to the best of our knowledge (with a possible exception of [9]) no real and successful image compression implementation was reported so far. In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. <p> On the other 3 hand, lossy Lempel-Ziv schemes based on approximate pattern matching were discussed in [9, 16, 18, 19, 23, 24, 29], however, to the best of our knowledge (with a possible exception of <ref> [9] </ref>) no real and successful image compression implementation was reported so far. In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. <p> Searching for an approximate prefix it much more complicated and usually leads to an explosion in the search time. To circumvent this problem, we explore from every node of the suffix tree only a fixed number, say g, of paths (cf. <ref> [9] </ref>). Our preliminary experimental results indicate that such a restriction does not lead to significant deterioration of the compression ratio and quality of the image. 4.
Reference: [10] <author> Y. Fisher, E. Jacobs, and R. Boss, </author> <title> Fractal Image Compression Using Iterated Transforms, in Image and Text Compression (ed. </title> <journal> J. </journal> <volume> Storer), </volume> <pages> 35-62, </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston (1992). </address>
Reference-contexts: Our general practical conclusion can be summarized as follows: The proposed non-transform scheme achieves compression ratios comparable to JPEG (UNIX implementation), wavelet compression (implementation based on [8]), and better than fractal image compression (implemented according to <ref> [10] </ref>). PMIC works particularly well for images with high frequencies (e.g., containing sharp egdes, etc). <p> multiplications it is indeed a very simple decompression algorithm. 20 4.2 Experimental Results In order to assess the quality of our PMIC scheme, we performed a series of experiments, and compared PMIC with the standard JPEG (UNIX implementation), wavelet compression (cf. [8]), and also fractal compression (implemented as described in <ref> [10] </ref>). Before presenting our experimental results, one must decide about metrics of comparison. It is natural to use (real) compression (scale) = (size of the original file)/(size of the compressed file) as one metric.
Reference: [11] <author> Z. Galil and R. Giancarlo, </author> <title> Data Structures and Algorithms for Approximate String Matching, </title> <journal> J. Complexity, </journal> <volume> 4, </volume> <pages> 33-72, </pages> <year> (1988). </year>
Reference-contexts: Arratia and Waterman [3] also analyzed an approximate pattern matching problem in the context of molecular biology. The reader is referred to the survey <ref> [11] </ref> and/or a recent book by Crochemore and Rytter [6] on string matching algorithms. As will become apparent soon (cf. Section 3), most of the algorithms for approximate pattern matching proposed in the literature so far will not be applicable to our situation of lossy image compression.
Reference: [12] <author> A. Gersho and R. Gray, </author> <title> Vector Quantization and Signal Compression, </title> <publisher> Kluwer (1992). </publisher> <pages> 27 </pages>
Reference-contexts: We believe that our compression time will become competitive with transform based methods once we implement the faster compression schemes of Section 3. There is a huge volume of knowledge on image processing (cf. <ref> [12, 13, 22] </ref>) but the majority of image compression techniques are based on transform methods.
Reference: [13] <author> A. K. Jain, </author> <title> Fundamentals of Digital Image Processing, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood (1989). </address>
Reference-contexts: We believe that our compression time will become competitive with transform based methods once we implement the faster compression schemes of Section 3. There is a huge volume of knowledge on image processing (cf. <ref> [12, 13, 22] </ref>) but the majority of image compression techniques are based on transform methods. <p> Variable Maximum Distortion. It is well known that human eyes can easily distinguish an "undesired" pattern in a low frequency (constant) background while the same pattern might be almost invisible to human eyes in high frequency (quickly varying) background (cf. <ref> [7, 13] </ref>). Therefore, we used a low value of maximum distortion, say D 1 , for slowly varying background, and a high value, say D 2 , for quickly varying background. To recognize these two different situations, we review the notion of the derivative D of an image I. <p> We also use the compression ratio r in bpp (bits per pixel), as defined in Section 2. There is, however, more controversy on how to measure the quality of compression (cf. <ref> [13, 22] </ref>). In Section 2 we discussed a measure of distortion d (; ) which usually for image compression becomes square-root measure or root-mean-squared-error (RMSE) defined as RM SE = u t 1 N X (x ij ^x ij ) 2 where ^x represents the compressed/decompressed image. <p> In fact, traditionally this measure is redefined as P SN R = 20 log 10 255 One also observes that another possible measure is simply the average-gray-level-error (AGLE), that is AGLE = 1 N X jx ij ^x ij j : It is well known (cf. <ref> [13] </ref>), and cofirmed by our experiments, that either all the above measures were doing well or none of them was good. For example, in our experimental study, the latter situation occured when comparing "variable-D" versus "constant-D" methods. In clearly indicates that "variable-D" method is much better. <p> In such a situation, we either must resort to sophisticated visual tests such as Contrast Sensitivity Function (CSF) (cf. 21 [7]) or use another measure of comparison. We choose the latter, which is basically a weighted L b norm for some b &gt; 0 (cf. <ref> [13] </ref>). We first observe that visual distortion is most visible for low frequency (slowly varying) background. This suggests the introduction of weights that are inversely proportional to the derivative D ij at pixel (i; j).
Reference: [14] <author> J.C. Kieffer, </author> <title> A Survey of the Theory of Source Coding with a Fidelity Criterion, </title> <journal> IEEE Trans. Information Theory, </journal> <volume> 39, </volume> <month> 1473-1490 </month> <year> (1993). </year>
Reference-contexts: It must be said that early attempts on lossy compression based on pattern matching were rather unsuccessful. Already in 1980 Ziv [31] (cf. also [28]) proposed an optimal lossy compression scheme at fixed rate level, while Ornstein and Shields [21], and independently Kieffer <ref> [14] </ref> gave a universal lossy compression for coding at fixed distortion level. Unfortunately, all of these schemes were prohibitively expensive from the computational complexity point of view. Recently, a quest for asymptotically optimal and computationally attractive lossy data compression based on approximate pattern matching has begun [9, 23, 24, 30]. <p> We only consider single-letter fidelity measures, that is, such that d (x n ; ^x n ) = n i=1 Furthermore, in order to use properly the rate distortion theory we impose the following two conditions on the fidelity measure (cf. <ref> [5, 14, 24] </ref>): (F1) Subadditivity. For any two integers n; m, and given vectors x n+m , y n+m we postu late that d (x n+m ; y n+m ) n + m m d (x n+m n+1 ) : (1) (F2) Finiteness.
Reference: [15] <author> J.C. Kieffer, </author> <title> Private Correspondence. </title>
Reference-contexts: In the enlarged-database scheme discussed in this section, the compression ratio r can be approximated by r = length of the overhead information length of repeated subword = L n However, Kieffer in a private correspondence <ref> [15] </ref> pointed out that a precise estimation of the compression ratio is more complicated. Indeed, let X (k) be the database after the kth application of the above procedure. Observe that X (k+1) = X (k) fl X jX (k) j+1 (3) where fl denotes concatenation. <p> The almost sure convergence of log N ` =` is proved in [29], while the lack of almost sure convergence of L n = log n is established in [19]. Finally, (10) is a simple consequence of (2) and (8). We conjecture after Kieffer <ref> [15] </ref> that the compression ratio as defined in (4) also converges almost surely to r 0 (D). In [18, 19] the Renyi entropy r 0 (D) was computed for memoryless sources and Hamming distance.
Reference: [16] <author> H. Koga and S. Arimoto, </author> <title> Asymptotic Properties of algorithms of Data Compression with Fidelity Criterion Based on String Matching, </title> <booktitle> Proc. 1994 IEEE Information Symposium on Information Theory, </booktitle> <month> 24-25 </month> <year> (1994). </year>
Reference-contexts: There is a huge volume of knowledge on image processing (cf. [12, 13, 22]) but the majority of image compression techniques are based on transform methods. On the other 3 hand, lossy Lempel-Ziv schemes based on approximate pattern matching were discussed in <ref> [9, 16, 18, 19, 23, 24, 29] </ref>, however, to the best of our knowledge (with a possible exception of [9]) no real and successful image compression implementation was reported so far. In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too.
Reference: [17] <author> Knuth, D.E., </author> <booktitle> The Art of Computer Programming. Seminumerical Algorithms, </booktitle> <volume> Vol. 2, </volume> <publisher> Addison-Wesley (1981). </publisher>
Reference-contexts: They do so by giving algorithms for faster (but approximate) implementations of the PREFIX procedure. Two of these approximation schemes are based on the Fast Fourier Transform (cf. <ref> [17] </ref>, pp. 290-294), and are discussed in the sequel. 3.2 Faster Algorithm for Square Error This sub-section deals with a fast approximate implementation of PREFIX for the square error, without the max-difference enhancement but with the additive-shift enhancement described earlier (we later explain how to handle max-difference).
Reference: [18] <author> T. Luczak and W. Szpankowski, </author> <title> A Lossy Data Compression Based on String Matching: Preliminary Analysis and Suboptimal Algorithms, </title> <booktitle> Proc. Combinatorial Pattern Matching, Asilomar, </booktitle> <volume> LNCS 807, </volume> <pages> 102-112, </pages> <month> Springer-Verla </month> <year> (1994). </year>
Reference-contexts: In this paper, we believe we can give an affirmative answer to these questions. After recalling (and somewhat extending) some recent theoretical results of Luczak and Szpankowski <ref> [18, 19] </ref> which constitute a theoretical basis for the lossy compression based on approximate pattern matching, we present our experimental results with pattern matching image compression that support the above claim. It must be said that early attempts on lossy compression based on pattern matching were rather unsuccessful. <p> In view of this, Luczak and Szpankowski <ref> [18, 19] </ref> (cf. also [24]) constructed a simple, computationally attractive lossy data compression based on approximate pattern matching. 2 The main idea was to search for the longest prefix of the uncompressed file that approxi-mately occurs in the already compressed file (the so called "training sequence" or "database sequence"). <p> There is a huge volume of knowledge on image processing (cf. [12, 13, 22]) but the majority of image compression techniques are based on transform methods. On the other 3 hand, lossy Lempel-Ziv schemes based on approximate pattern matching were discussed in <ref> [9, 16, 18, 19, 23, 24, 29] </ref>, however, to the best of our knowledge (with a possible exception of [9]) no real and successful image compression implementation was reported so far. In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. <p> In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. We should mention here the paper of Steinberg and Gutman [24], and Luczak and Szpankowski <ref> [18, 19] </ref>, as well as recent results of Yang and Kieffer [29]. Arratia and Waterman [3] also analyzed an approximate pattern matching problem in the context of molecular biology. The reader is referred to the survey [11] and/or a recent book by Crochemore and Rytter [6] on string matching algorithms. <p> Our goal is to find a code that represents the rest of the file, X M n+1 , in as few bits as possible, and to assure that the construction of the code is computationally efficient. As in Luczak and Szpankowski <ref> [18, 19] </ref> we define the depth L n as the length of the longest prefix of X 1 n+1 that approximately occurs in the database. <p> Finally, (10) is a simple consequence of (2) and (8). We conjecture after Kieffer [15] that the compression ratio as defined in (4) also converges almost surely to r 0 (D). In <ref> [18, 19] </ref> the Renyi entropy r 0 (D) was computed for memoryless sources and Hamming distance. In Figure 1 we compared it to the optimal rate distortion R (D) = h + D log D + (1 D) log (1 D), where h is the source entropy rate.
Reference: [19] <author> T. Luczak and W. Szpankowski, </author> <title> A Suboptimal Lossy Data Compression Based in Approximate Pattern Matching, </title> <booktitle> IEEE International Symposium on Information Theory, </booktitle> <address> Whistler, </address> <note> 1995; also under revision in IEEE Trans. Information Theory, </note> <institution> and Purdue University, </institution> <month> CSD-TR-94-072 </month> <year> (1994). </year>
Reference-contexts: In this paper, we believe we can give an affirmative answer to these questions. After recalling (and somewhat extending) some recent theoretical results of Luczak and Szpankowski <ref> [18, 19] </ref> which constitute a theoretical basis for the lossy compression based on approximate pattern matching, we present our experimental results with pattern matching image compression that support the above claim. It must be said that early attempts on lossy compression based on pattern matching were rather unsuccessful. <p> In view of this, Luczak and Szpankowski <ref> [18, 19] </ref> (cf. also [24]) constructed a simple, computationally attractive lossy data compression based on approximate pattern matching. 2 The main idea was to search for the longest prefix of the uncompressed file that approxi-mately occurs in the already compressed file (the so called "training sequence" or "database sequence"). <p> It was proved in <ref> [19] </ref> that under a stationary mixing probabilistic model of an image, the compression ratio can asymptotically achieve the so called generalized Renyi entropy r 0 (D) (cf. next section for a precise definition). <p> It must be stressed that the scheme we shall propose in this paper, henceforth called Pattern Matching Image Compression (PMIC for short), is a major modification of the basic idea described above and analyzed in <ref> [19] </ref>. A straightforward implementation of the basic scheme on real images (structured data) seems not to be attractive from a practical point of view, so that the enhancements we describe in what follows play an important role in the quality of the experimental results we obtained. <p> There is a huge volume of knowledge on image processing (cf. [12, 13, 22]) but the majority of image compression techniques are based on transform methods. On the other 3 hand, lossy Lempel-Ziv schemes based on approximate pattern matching were discussed in <ref> [9, 16, 18, 19, 23, 24, 29] </ref>, however, to the best of our knowledge (with a possible exception of [9]) no real and successful image compression implementation was reported so far. In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. <p> In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. We should mention here the paper of Steinberg and Gutman [24], and Luczak and Szpankowski <ref> [18, 19] </ref>, as well as recent results of Yang and Kieffer [29]. Arratia and Waterman [3] also analyzed an approximate pattern matching problem in the context of molecular biology. The reader is referred to the survey [11] and/or a recent book by Crochemore and Rytter [6] on string matching algorithms. <p> Section 3), most of the algorithms for approximate pattern matching proposed in the literature so far will not be applicable to our situation of lossy image compression. The paper is organized as follows. In the next section we recall some concepts and results from Luczak and Szpankowski <ref> [19] </ref>. Next we discuss algorithmic issues, namely fast algorithms to identify a longest prefix that occurs approximately in the database, and we present several variations of image compression algorithms. Finally, we discuss our implementation of the PMIC scheme, and present several results on graphic and photographic images. 2. <p> PROBLEM FORMULATION AND THEORETICAL RESULTS In this section, we describe in general terms a lossy data compression based on approximate pattern matching, and review and slightly generalize theoretical results of Luczak and Szpankowski <ref> [19] </ref> (cf. also [24, 29]) that constitute a foundation for the performance of the lossy compression scheme. <p> Our goal is to find a code that represents the rest of the file, X M n+1 , in as few bits as possible, and to assure that the construction of the code is computationally efficient. As in Luczak and Szpankowski <ref> [18, 19] </ref> we define the depth L n as the length of the longest prefix of X 1 n+1 that approximately occurs in the database. <p> time N ` (cf. [24, 26, 29, 27]), defined as the smallest N 2` such that d (X ` 1 ; X N In this paper, we focus on L n and variable length compression codes. 2.2 Main Theoretical Results We review and slightly generalize results of Luczak and Szpankowski <ref> [19] </ref> to demonstrate the quality of the lossy data compression outlined in the previous subsection. In order to formulate the results we must adopt a suitable probabilistic model. <p> Result (7) follows directly from the subadditivity of the distortion measure (F1) and mixing model assumption (M) by an application of the Subadditive Ergodic Theorem along the lines of arguments presented in <ref> [19] </ref>. We finally can present our main theoretical results that provides a basis for the image compression discussed in the next section. Theorem 1. <p> The results (8) and (9) follow directly from Lemma 1 and the first and second moment methods along the lines of arguments used in <ref> [19] </ref>. The almost sure convergence of log N ` =` is proved in [29], while the lack of almost sure convergence of L n = log n is established in [19]. Finally, (10) is a simple consequence of (2) and (8). <p> (9) follow directly from Lemma 1 and the first and second moment methods along the lines of arguments used in <ref> [19] </ref>. The almost sure convergence of log N ` =` is proved in [29], while the lack of almost sure convergence of L n = log n is established in [19]. Finally, (10) is a simple consequence of (2) and (8). We conjecture after Kieffer [15] that the compression ratio as defined in (4) also converges almost surely to r 0 (D). In [18, 19] the Renyi entropy r 0 (D) was computed for memoryless sources and Hamming distance. <p> Finally, (10) is a simple consequence of (2) and (8). We conjecture after Kieffer [15] that the compression ratio as defined in (4) also converges almost surely to r 0 (D). In <ref> [18, 19] </ref> the Renyi entropy r 0 (D) was computed for memoryless sources and Hamming distance. In Figure 1 we compared it to the optimal rate distortion R (D) = h + D log D + (1 D) log (1 D), where h is the source entropy rate. <p> In Section 2 we indicated that for the basic scheme the compression ratio achieves the rate of the generalized Renyi entropy r 0 (D) introduced in <ref> [19] </ref>. While r 0 (D) is greater than the optimal rate-distortion function R (D), numerical evaluation (cf.
Reference: [20] <author> E.M. McCreight, </author> <title> A Space Economical Suffix Tree Construction Algorithm, </title> <journal> JACM, </journal> <volume> 23, </volume> <month> 262-272 </month> <year> (1976). </year>
Reference-contexts: an additional O (log ^ k) factor comes from the binary search (see end of Sec. 3.2 for a comment on the approximate nature of such a binary search). 3.4 Suffix Tree Algorithm Finally, we briefly mention an O (N 2 ) approximate algorithm based on a suffix tree (cf. <ref> [2, 20] </ref>). The main idea is quite simple. We first build a suffix tree of the whole image N fi N (either using McCreight's algorithm in O (N 2 ) time [20] or by a brute force approach in O (N 2 log N ) time [2]). <p> The main idea is quite simple. We first build a suffix tree of the whole image N fi N (either using McCreight's algorithm in O (N 2 ) time <ref> [20] </ref> or by a brute force approach in O (N 2 log N ) time [2]). Once the suffix tree is constructed, one can find exact longest prefix in a row starting at any position in O (N ) steps or with high probability in O (log N ) steps.
Reference: [21] <author> D. Ornstein and P. Shields, </author> <title> Universal Almost Sure Data Compression, </title> <journal> Annals of Probability, </journal> <volume> 18, </volume> <month> 441-452 </month> <year> (1990). </year>
Reference-contexts: It must be said that early attempts on lossy compression based on pattern matching were rather unsuccessful. Already in 1980 Ziv [31] (cf. also [28]) proposed an optimal lossy compression scheme at fixed rate level, while Ornstein and Shields <ref> [21] </ref>, and independently Kieffer [14] gave a universal lossy compression for coding at fixed distortion level. Unfortunately, all of these schemes were prohibitively expensive from the computational complexity point of view.
Reference: [22] <author> M. Rabbani and P. Jones, </author> <title> Digital Image Compression Techniques, </title> <publisher> SPIE Optical Engineering Press, </publisher> <address> Bellingham (1991). </address>
Reference-contexts: We believe that our compression time will become competitive with transform based methods once we implement the faster compression schemes of Section 3. There is a huge volume of knowledge on image processing (cf. <ref> [12, 13, 22] </ref>) but the majority of image compression techniques are based on transform methods. <p> We also use the compression ratio r in bpp (bits per pixel), as defined in Section 2. There is, however, more controversy on how to measure the quality of compression (cf. <ref> [13, 22] </ref>). In Section 2 we discussed a measure of distortion d (; ) which usually for image compression becomes square-root measure or root-mean-squared-error (RMSE) defined as RM SE = u t 1 N X (x ij ^x ij ) 2 where ^x represents the compressed/decompressed image.
Reference: [23] <author> I. Sadeh, </author> <title> On Approximate String Matching, </title> <booktitle> Proc. of Data Compression Conference, </booktitle> <month> 148-157 </month> <year> (1993). </year>
Reference-contexts: Unfortunately, all of these schemes were prohibitively expensive from the computational complexity point of view. Recently, a quest for asymptotically optimal and computationally attractive lossy data compression based on approximate pattern matching has begun <ref> [9, 23, 24, 30] </ref>. But one may wonder whether a practical and optimal lossy compression exists at all. <p> There is a huge volume of knowledge on image processing (cf. [12, 13, 22]) but the majority of image compression techniques are based on transform methods. On the other 3 hand, lossy Lempel-Ziv schemes based on approximate pattern matching were discussed in <ref> [9, 16, 18, 19, 23, 24, 29] </ref>, however, to the best of our knowledge (with a possible exception of [9]) no real and successful image compression implementation was reported so far. In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too.
Reference: [24] <author> Y. Steinberg and M. Gutman, </author> <title> An Algorithm for Source Coding Subject to a Fidelity Criterion, Based on String Matching, </title> <journal> IEEE Trans. Information Theory, </journal> <volume> 39, </volume> <month> 877-886 </month> <year> (1993). </year>
Reference-contexts: Unfortunately, all of these schemes were prohibitively expensive from the computational complexity point of view. Recently, a quest for asymptotically optimal and computationally attractive lossy data compression based on approximate pattern matching has begun <ref> [9, 23, 24, 30] </ref>. But one may wonder whether a practical and optimal lossy compression exists at all. <p> In view of this, Luczak and Szpankowski [18, 19] (cf. also <ref> [24] </ref>) constructed a simple, computationally attractive lossy data compression based on approximate pattern matching. 2 The main idea was to search for the longest prefix of the uncompressed file that approxi-mately occurs in the already compressed file (the so called "training sequence" or "database sequence"). <p> There is a huge volume of knowledge on image processing (cf. [12, 13, 22]) but the majority of image compression techniques are based on transform methods. On the other 3 hand, lossy Lempel-Ziv schemes based on approximate pattern matching were discussed in <ref> [9, 16, 18, 19, 23, 24, 29] </ref>, however, to the best of our knowledge (with a possible exception of [9]) no real and successful image compression implementation was reported so far. In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. <p> In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. We should mention here the paper of Steinberg and Gutman <ref> [24] </ref>, and Luczak and Szpankowski [18, 19], as well as recent results of Yang and Kieffer [29]. Arratia and Waterman [3] also analyzed an approximate pattern matching problem in the context of molecular biology. <p> PROBLEM FORMULATION AND THEORETICAL RESULTS In this section, we describe in general terms a lossy data compression based on approximate pattern matching, and review and slightly generalize theoretical results of Luczak and Szpankowski [19] (cf. also <ref> [24, 29] </ref>) that constitute a foundation for the performance of the lossy compression scheme. <p> We only consider single-letter fidelity measures, that is, such that d (x n ; ^x n ) = n i=1 Furthermore, in order to use properly the rate distortion theory we impose the following two conditions on the fidelity measure (cf. <ref> [5, 14, 24] </ref>): (F1) Subadditivity. For any two integers n; m, and given vectors x n+m , y n+m we postu late that d (x n+m ; y n+m ) n + m m d (x n+m n+1 ) : (1) (F2) Finiteness. <p> computed as r = jX (k+1) j In this paper, we adopt (2) to simplify the presentation. 6 Finally, we should mention in passing that one can design a block coding (i.e., fixed length code) for lossy data compression based on another parameter, namely the waiting time N ` (cf. <ref> [24, 26, 29, 27] </ref>), defined as the smallest N 2` such that d (X ` 1 ; X N In this paper, we focus on L n and variable length compression codes. 2.2 Main Theoretical Results We review and slightly generalize results of Luczak and Szpankowski [19] to demonstrate the quality
Reference: [25] <author> W. Szpankowski, </author> <title> A Typical Behavior of Some Data Compression Schemes, </title> <booktitle> Proc. of Data Compression Conference, </booktitle> <address> Snowbird, </address> <month> 247-256 </month> <year> (1991). </year>
Reference: [26] <author> W. Szpankowski, </author> <title> A Generalized Suffix Tree and Its (Un)Expected Asymptotic Behaviors, </title> <journal> SIAM J. Computing, </journal> <volume> 22, </volume> <month> 1176-1198 </month> <year> (1993). </year>
Reference-contexts: computed as r = jX (k+1) j In this paper, we adopt (2) to simplify the presentation. 6 Finally, we should mention in passing that one can design a block coding (i.e., fixed length code) for lossy data compression based on another parameter, namely the waiting time N ` (cf. <ref> [24, 26, 29, 27] </ref>), defined as the smallest N 2` such that d (X ` 1 ; X N In this paper, we focus on L n and variable length compression codes. 2.2 Main Theoretical Results We review and slightly generalize results of Luczak and Szpankowski [19] to demonstrate the quality
Reference: [27] <author> A. Wyner and J. Ziv, </author> <title> Some Asymptotic Properties of the Entropy of a Stationary Ergodic Data Source with Applications to Data Compression, </title> <journal> IEEE Trans. Information Theory, </journal> <volume> 35, </volume> <month> 1250-1258 </month> <year> (1989). </year> <month> 28 </month>
Reference-contexts: Such a scheme can be called the enlarged-database scheme. In another implementation one can keep the database fixed so that the longest prefix is not added to the database. The latter is called fixed-database scheme 9cf. <ref> [27] </ref>).. Finally, in a sliding window implementation one adds L n symbols to the database and simultaneously the first L n symbols of the database are deleted keeping the size of the databased constant. In our implementation (cf. <p> computed as r = jX (k+1) j In this paper, we adopt (2) to simplify the presentation. 6 Finally, we should mention in passing that one can design a block coding (i.e., fixed length code) for lossy data compression based on another parameter, namely the waiting time N ` (cf. <ref> [24, 26, 29, 27] </ref>), defined as the smallest N 2` such that d (X ` 1 ; X N In this paper, we focus on L n and variable length compression codes. 2.2 Main Theoretical Results We review and slightly generalize results of Luczak and Szpankowski [19] to demonstrate the quality
Reference: [28] <author> E.H. Yang, and J. Kieffer, </author> <title> Simple Universal Lossy Data Compression Schemes Derived From Lempel-Ziv algorithm, </title> <note> preprint (1995). </note>
Reference-contexts: It must be said that early attempts on lossy compression based on pattern matching were rather unsuccessful. Already in 1980 Ziv [31] (cf. also <ref> [28] </ref>) proposed an optimal lossy compression scheme at fixed rate level, while Ornstein and Shields [21], and independently Kieffer [14] gave a universal lossy compression for coding at fixed distortion level. Unfortunately, all of these schemes were prohibitively expensive from the computational complexity point of view. <p> Recently, a quest for asymptotically optimal and computationally attractive lossy data compression based on approximate pattern matching has begun [9, 23, 24, 30]. But one may wonder whether a practical and optimal lossy compression exists at all. Yang and Kieffer in their recent paper <ref> [28] </ref> expressed the following opinion: "... it is our belief that a universal lossy source coding scheme with attractive computational complexity aspects will never be found." We share this view, and we believe that investigations of suboptimal and practical heuristics for lossy compression are needed.
Reference: [29] <author> E.H. Yang, and J. Kieffer, </author> <title> On the Performance of Data Compression Algorithms Based upon String Matching, </title> <note> preprint (1995). </note>
Reference-contexts: There is a huge volume of knowledge on image processing (cf. [12, 13, 22]) but the majority of image compression techniques are based on transform methods. On the other 3 hand, lossy Lempel-Ziv schemes based on approximate pattern matching were discussed in <ref> [9, 16, 18, 19, 23, 24, 29] </ref>, however, to the best of our knowledge (with a possible exception of [9]) no real and successful image compression implementation was reported so far. In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. <p> In fact, a literature on the probabilistic analysis of approximate pattern matching is rather scarce, too. We should mention here the paper of Steinberg and Gutman [24], and Luczak and Szpankowski [18, 19], as well as recent results of Yang and Kieffer <ref> [29] </ref>. Arratia and Waterman [3] also analyzed an approximate pattern matching problem in the context of molecular biology. The reader is referred to the survey [11] and/or a recent book by Crochemore and Rytter [6] on string matching algorithms. As will become apparent soon (cf. <p> PROBLEM FORMULATION AND THEORETICAL RESULTS In this section, we describe in general terms a lossy data compression based on approximate pattern matching, and review and slightly generalize theoretical results of Luczak and Szpankowski [19] (cf. also <ref> [24, 29] </ref>) that constitute a foundation for the performance of the lossy compression scheme. <p> computed as r = jX (k+1) j In this paper, we adopt (2) to simplify the presentation. 6 Finally, we should mention in passing that one can design a block coding (i.e., fixed length code) for lossy data compression based on another parameter, namely the waiting time N ` (cf. <ref> [24, 26, 29, 27] </ref>), defined as the smallest N 2` such that d (X ` 1 ; X N In this paper, we focus on L n and variable length compression codes. 2.2 Main Theoretical Results We review and slightly generalize results of Luczak and Szpankowski [19] to demonstrate the quality <p> The results (8) and (9) follow directly from Lemma 1 and the first and second moment methods along the lines of arguments used in [19]. The almost sure convergence of log N ` =` is proved in <ref> [29] </ref>, while the lack of almost sure convergence of L n = log n is established in [19]. Finally, (10) is a simple consequence of (2) and (8). We conjecture after Kieffer [15] that the compression ratio as defined in (4) also converges almost surely to r 0 (D).
Reference: [30] <author> Z. Zhang and V. Wei, </author> <title> An On-Line Universal Lossy Data Compression Algorithm via Continuous Codebook Refinement, </title> <type> preprint. </type>
Reference-contexts: Unfortunately, all of these schemes were prohibitively expensive from the computational complexity point of view. Recently, a quest for asymptotically optimal and computationally attractive lossy data compression based on approximate pattern matching has begun <ref> [9, 23, 24, 30] </ref>. But one may wonder whether a practical and optimal lossy compression exists at all.
Reference: [31] <author> J. Ziv, </author> <title> Distortion-rate Theory for Individual Sequences, </title> <journal> IEEE Trans. Information Theory, </journal> <volume> 26, </volume> <month> 137-143 </month> <year> (1980). </year>
Reference-contexts: 1. INTRODUCTION Data compression based on exact pattern matching can be traced back to seminal papers of Lempel and Ziv <ref> [31, 32, 33] </ref>, but recently there has been a resurgence of interest in this type of data compression. This might be a consequence of rapid growth in digital representation of multimedia (e.g., text, audio, image, video, etc.) which are particularly amenable to pattern matching manipulations. <p> It must be said that early attempts on lossy compression based on pattern matching were rather unsuccessful. Already in 1980 Ziv <ref> [31] </ref> (cf. also [28]) proposed an optimal lossy compression scheme at fixed rate level, while Ornstein and Shields [21], and independently Kieffer [14] gave a universal lossy compression for coding at fixed distortion level. Unfortunately, all of these schemes were prohibitively expensive from the computational complexity point of view.
Reference: [32] <author> J. Ziv and A. Lempel, </author> <title> A Universal Algorithm for Sequential Data Compression, </title> <journal> IEEE Trans. Information Theory, </journal> <volume> 23, 3, </volume> <month> 337-343 </month> <year> (1977). </year>
Reference-contexts: 1. INTRODUCTION Data compression based on exact pattern matching can be traced back to seminal papers of Lempel and Ziv <ref> [31, 32, 33] </ref>, but recently there has been a resurgence of interest in this type of data compression. This might be a consequence of rapid growth in digital representation of multimedia (e.g., text, audio, image, video, etc.) which are particularly amenable to pattern matching manipulations. <p> This might be a consequence of rapid growth in digital representation of multimedia (e.g., text, audio, image, video, etc.) which are particularly amenable to pattern matching manipulations. It was known for a along time that pattern matching based data compression such as Lempel-Ziv schemes LZ77 <ref> [32] </ref> and LZ78 [33] is very attractive for text compression. For example, such schemes were used in the UNIX compress and gunzip commands, and in a CCITT standard for data compression for modems.
Reference: [33] <author> J. Ziv and A. Lempel, </author> <title> Compression of Individual Sequences via Variable-rate Coding, </title> <journal> IEEE Trans. Information Theory, </journal> <volume> 24, </volume> <pages> 530-536, </pages> <year> 1978. </year> <month> 29 </month>
Reference-contexts: 1. INTRODUCTION Data compression based on exact pattern matching can be traced back to seminal papers of Lempel and Ziv <ref> [31, 32, 33] </ref>, but recently there has been a resurgence of interest in this type of data compression. This might be a consequence of rapid growth in digital representation of multimedia (e.g., text, audio, image, video, etc.) which are particularly amenable to pattern matching manipulations. <p> This might be a consequence of rapid growth in digital representation of multimedia (e.g., text, audio, image, video, etc.) which are particularly amenable to pattern matching manipulations. It was known for a along time that pattern matching based data compression such as Lempel-Ziv schemes LZ77 [32] and LZ78 <ref> [33] </ref> is very attractive for text compression. For example, such schemes were used in the UNIX compress and gunzip commands, and in a CCITT standard for data compression for modems.
References-found: 33

