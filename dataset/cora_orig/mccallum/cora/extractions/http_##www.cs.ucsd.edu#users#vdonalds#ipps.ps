URL: http://www.cs.ucsd.edu/users/vdonalds/ipps.ps
Refering-URL: http://www.cs.ucsd.edu/users/vdonalds/
Root-URL: http://www.cs.ucsd.edu
Email: fvdonalds,ferranteg@cs.ucsd.edu  
Title: Determining Asynchronous Acyclic Pipeline Execution Times  
Author: Val Donaldson and Jeanne Ferrante 
Address: La Jolla, California 92093-0114  
Affiliation: Computer Science and Engineering Department University of California, San Diego  
Abstract: Pipeline execution is a form of parallelism in which sub-computations of a repeated computation, such as statements in the body of a loop, are executed in parallel. A measure of the execution time of a pipeline is needed to determine if pipelining is an effective form of parallelism for a loop, and to evaluate alternative scheduling choices. We derive a formula for precisely determining the asynchronous pipeline execution time of a loop modeled as iterated execution of an acyclic task graph. The formula can be evaluated in time linear in the number of tasks and edges in the graph. We assume that computation and communication times are fixed and known, interprocessor communication and buffering capability are unbounded, and each task is assigned to a distinct processor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Banerjee, T. Hamada, P. M. Chau, and R. D. Fellman. </author> <title> Macro pipelining based scheduling on high performance heterogeneous multiprocessor systems. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 43(8) </volume> <pages> 1468-1484, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Given a pipeline schedule, it is important to be able to predict the execution time of the schedule, to determine whether or not the loop should be pipelined, and to chose between alternative schedules. Existing pipeline scheduling algorithms <ref> [1, 3, 4, 5] </ref> use conservative estimates of the execution time of a schedule. We derive a formula which precisely characterizes the execution time of an asynchronous pipeline schedule when the loop body is represented as an acyclic task graph, and each task is assigned to a distinct processor. <p> The execution time estimates used by existing large-grain pipeline scheduling algorithms generally account for the effects of G:vmax and task parallelism, but overestimate the effects of nonzero communication times. In [2], Theorem 1 is used to show that several example pipeline schedules from <ref> [1, 4, 5] </ref> execute in less time than otherwise estimated.
Reference: [2] <author> V. Donaldson and J. Ferrante. </author> <title> Determining asynchronous acyclic pipeline execution times. </title> <type> Technical Report CS96-466, </type> <institution> Computer Science and Engineering Dept., University of California, </institution> <address> San Diego, La Jolla, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Any loop may be modeled as iterative execution of an acyclic task graph, by localizing loop carried dependences (dependences which cross loop iterations) within a task. An example of deriving an acyclic task graph from a loop is given in <ref> [2] </ref>. Abstractly, a task graph is a directed acyclic graph G = (V; E), where V is a set of vertices or tasks, and E is a set of directed edges between tasks. <p> Then G:time n = max X2V fG:time 1 X:slack + (n 1)X:timeg: Proof The proof is by induction on both the iteration number and the level of a task in G. See <ref> [2] </ref> for the full proof. 2 The formula in Theorem 1 can be evaluated for a given value of n in O (jV j + jEj) time, using O (jV j) space beyond the storage required for graph G itself. <p> The execution time estimates used by existing large-grain pipeline scheduling algorithms generally account for the effects of G:vmax and task parallelism, but overestimate the effects of nonzero communication times. In <ref> [2] </ref>, Theorem 1 is used to show that several example pipeline schedules from [1, 4, 5] execute in less time than otherwise estimated.
Reference: [3] <author> F. Gasperoni and U. Schwiegelshohn. </author> <title> Scheduling loops on parallel processors: a simple algorithm with close to optimum performance. </title> <booktitle> In Proc. Second Joint International Conference on Vector and Parallel Processing (Parallel Processing: CONPAR 92-VAPP V), </booktitle> <pages> pages 625-636, </pages> <address> Lyon, France, </address> <month> Septem-ber </month> <year> 1992. </year>
Reference-contexts: Given a pipeline schedule, it is important to be able to predict the execution time of the schedule, to determine whether or not the loop should be pipelined, and to chose between alternative schedules. Existing pipeline scheduling algorithms <ref> [1, 3, 4, 5] </ref> use conservative estimates of the execution time of a schedule. We derive a formula which precisely characterizes the execution time of an asynchronous pipeline schedule when the loop body is represented as an acyclic task graph, and each task is assigned to a distinct processor.
Reference: [4] <author> P. D. Hoang and J. M. Rabaey. </author> <title> Scheduling of DSP programs onto multiprocessors for maximum throughput. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 41(6) </volume> <pages> 2225-2235, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Given a pipeline schedule, it is important to be able to predict the execution time of the schedule, to determine whether or not the loop should be pipelined, and to chose between alternative schedules. Existing pipeline scheduling algorithms <ref> [1, 3, 4, 5] </ref> use conservative estimates of the execution time of a schedule. We derive a formula which precisely characterizes the execution time of an asynchronous pipeline schedule when the loop body is represented as an acyclic task graph, and each task is assigned to a distinct processor. <p> The execution time estimates used by existing large-grain pipeline scheduling algorithms generally account for the effects of G:vmax and task parallelism, but overestimate the effects of nonzero communication times. In [2], Theorem 1 is used to show that several example pipeline schedules from <ref> [1, 4, 5] </ref> execute in less time than otherwise estimated.
Reference: [5] <author> T. Yang, C. Fu, A. Gerasoulis, and V. Sarkar. </author> <title> Mapping iterative task graphs on distributed memory machines. </title> <booktitle> In Proc. 24th International Conference on Parallel Processing, </booktitle> <volume> Volume II, </volume> <pages> pages 151-158, </pages> <address> Oconomowoc, WI, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Given a pipeline schedule, it is important to be able to predict the execution time of the schedule, to determine whether or not the loop should be pipelined, and to chose between alternative schedules. Existing pipeline scheduling algorithms <ref> [1, 3, 4, 5] </ref> use conservative estimates of the execution time of a schedule. We derive a formula which precisely characterizes the execution time of an asynchronous pipeline schedule when the loop body is represented as an acyclic task graph, and each task is assigned to a distinct processor. <p> The execution time estimates used by existing large-grain pipeline scheduling algorithms generally account for the effects of G:vmax and task parallelism, but overestimate the effects of nonzero communication times. In [2], Theorem 1 is used to show that several example pipeline schedules from <ref> [1, 4, 5] </ref> execute in less time than otherwise estimated.
References-found: 5

