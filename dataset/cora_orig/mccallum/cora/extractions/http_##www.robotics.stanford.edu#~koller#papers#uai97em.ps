URL: http://www.robotics.stanford.edu/~koller/papers/uai97em.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/uai97em.html
Root-URL: http://www.robotics.stanford.edu
Email: ebauer@cs.stanford.edu  koller@cs.stanford.edu  singer@research.att.com  
Title: Update rules for parameter estimation in Bayesian networks  
Author: Eric Bauer Daphne Koller Yoram Singer 
Affiliation: Stanford University  Stanford University  AT&T Labs  
Date: August 1-3, 1997  
Address: Providence, Rhode Island,  
Note: In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-97), pages 3-13,  
Abstract: This paper re-examines the problem of parameter estimation in Bayesian networks with missing values and hidden variables from the perspective of recent work in on-line learning [12]. We provide a unified framework for parameter estimation that encompasses both on-line learning, where the model is continuously adapted to new data cases as they arrive, and the more traditional batch learning, where a pre-accumulated set of samples is used in a one-time model selection process. In the batch case, our framework encompasses both the gradient projection algorithm [2, 3] and the EM algorithm [14] for Bayesian networks. The framework also leads to new on-line and batch parameter update schemes, including a parameterized version of EM. We provide both empirical and theoretical results indicating that parameterized EM allows faster convergence to the maximum likelihood parame ters than does standard EM.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Beinlich, H. Suermondt, R. Chavez, and G. Cooper. </author> <title> The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks. </title> <booktitle> In Proceedings of the Second European Conference on Artificial Intelligence in Medicine, </booktitle> <pages> pages 247256. </pages> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference-contexts: We therefore begin by analyzing these eigen values. We first show that the eigen values of R all lie in <ref> [0; 1] </ref>. We use the following lemmas [11]: Lemma 2: Let A and B be two matrices of dimensions nfim and m fi n, respectively, where m n. Then, every eigen value of AB is also an eigen value of BA. <p> Then, any vector is within the space , and our convergence rate argument applies. 5 Experimental results We tested the performance of the EM () algorithm in practice by using it for parameter estimation in two well-known networks: the Alarm network for ICU ventilator management <ref> [1] </ref> and the Insurance network for car insurance underwriting [2]. Since the results on the two networks were fairly comparable, we present major results for Alarm.
Reference: [2] <author> J. Binder, D. Koller, S.J. Russell, and K. </author> <title> Kanazawa. Adaptive probabilistic networks with hidden variables. </title> <booktitle> Machine Learning, </booktitle> <year> 1997. </year> <note> To appear. Available via WWW from http://robotics.stanford.edu/koller. </note>
Reference-contexts: Surprisingly, it turns out that well-known parameter estimation algorithms are special cases of our framework. For example, our update rule for L 2 -norm results in the gradient-projection scheme of <ref> [3, 21, 2] </ref>. Our update rule for the 2 distance results in a family of update rules with varying learning rates . This family, which we denote EM (), includes the standard EM algorithm [6, 8] as the special case EM (1). <p> from , we can replace the L D ( ~ ) term in (1), thereby changing our task to one of maximizing: ^ F ( ~ ) = [L D ( ) + rL D ( ) ( ~ )] d ( ~ ; ): (2) It turns out (see <ref> [2] </ref>) that the decomposition of the distribution P implied by the network structure results in a particularly simple expression for the gradient vector: r ijk L D () = ijk l=1 P (x k j N E (x k j ijk where E (x k i ; pa i j D) <p> (5), we obtain the following update rule for ~ : ~ ijk = ijk + (r ijk L D ( ) r i k 0 If we substitute for r ijk L D ( ) its value according to (3), we obtain precisely the standard gradient projection algorithm described in <ref> [2] </ref>. 2.3 Relative entropy The two parameter vectors, with the associated network structure, define two probability distributions over the the same spacethe joint probability space of the variables in the Bayesian network. <p> Therefore, we can instantiate the three update rules described above in the context of this problem. We have already seen that the update rule for the L 2 -norm leads directly to the gradient projection algorithm of <ref> [3, 2] </ref>. In order to apply the other two update rules in this context, we must only decide on the estimate ^ P (pa ij ), introduced as an approximation to P ~ (Pa i = pa j i ). <p> within the space , and our convergence rate argument applies. 5 Experimental results We tested the performance of the EM () algorithm in practice by using it for parameter estimation in two well-known networks: the Alarm network for ICU ventilator management [1] and the Insurance network for car insurance underwriting <ref> [2] </ref>. Since the results on the two networks were fairly comparable, we present major results for Alarm.
Reference: [3] <author> Wray L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2:159225, </volume> <month> De-cember </month> <year> 1994. </year>
Reference-contexts: Surprisingly, it turns out that well-known parameter estimation algorithms are special cases of our framework. For example, our update rule for L 2 -norm results in the gradient-projection scheme of <ref> [3, 21, 2] </ref>. Our update rule for the 2 distance results in a family of update rules with varying learning rates . This family, which we denote EM (), includes the standard EM algorithm [6, 8] as the special case EM (1). <p> Therefore, we can instantiate the three update rules described above in the context of this problem. We have already seen that the update rule for the L 2 -norm leads directly to the gradient projection algorithm of <ref> [3, 2] </ref>. In order to apply the other two update rules in this context, we must only decide on the estimate ^ P (pa ij ), introduced as an approximation to P ~ (Pa i = pa j i ).
Reference: [4] <author> N. Cesa-Bianchi, Y. Freund, D.P. Helmbold, D. Haussler, R.E. Schapire, and M.K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 382391, </pages> <year> 1993. </year> <note> To appear, Journal of the Association for Computing Machinery. </note>
Reference-contexts: Intuitively, it seems that a new sample should have less effect on a well-established model (one based on many prior samples) than on a new one. Therefore, we may want to adapt our learning rate over time, based on the number of examples seen so far (see, for instance, <ref> [4] </ref>). This is, in fact, precisely the behavior we would get from a full Bayesian updating scheme for our model (a scheme which is unfortunately infeasible in the presence of partially-observable data cases [8]).
Reference: [5] <author> T.M. Cover and J.A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: ) log ijk ! After some simple algebraic manipulation, and plugging in the value for the gradient as in (3), we get: ~ ijk = ijk i ;pa j ^ P (pa j k ijk i ;pa j ^ P (pa j : (8) 2.4 2 distance The 2 distance <ref> [5] </ref> between two distributions p and q as above is: 2 (pkq) = 2 x This distance function is, in fact, a linear approximation of the relative entropy distance [5]. <p> ^ P (pa j k ijk i ;pa j ^ P (pa j : (8) 2.4 2 distance The 2 distance <ref> [5] </ref> between two distributions p and q as above is: 2 (pkq) = 2 x This distance function is, in fact, a linear approximation of the relative entropy distance [5]. We use the 2 distance to approximate the relative entropy by first decomposing the relative entropy as in (7), and then using the 2 distance to approximate each of the distributions distributions P ~ (X i j Pa i = pa i ) which appear in the summation.
Reference: [6] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B39:138, </volume> <year> 1977. </year>
Reference-contexts: For example, our update rule for L 2 -norm results in the gradient-projection scheme of [3, 21, 2]. Our update rule for the 2 distance results in a family of update rules with varying learning rates . This family, which we denote EM (), includes the standard EM algorithm <ref> [6, 8] </ref> as the special case EM (1). From the relative entropy distance, we derive an analogous family of multiplicative update rules which, following [12, 10], we call EG (). <p> By contrast, the standard EM update rule is guaranteed to converge to a local maximum from any point in the space (except perhaps for pathological cases <ref> [6] </ref>). A similar guarantee cannot be made for other values of . However, our experimental results in the next section show that, in practice, we do get convergence from random starting positions for &gt; 1.
Reference: [7] <author> A. Dvoretzky. </author> <title> On stochastic approximation. </title> <booktitle> In Proceedings of the Third Berkley Symposium on Mathematical Statistics and Probability. </booktitle> <institution> University of California Press, </institution> <year> 1956. </year>
Reference-contexts: We therefore call this parameterized update rule EM (). For &lt; 1, EM () instantiates the new parameter values to be a weighted combination between the EM update and the current vector of parameters. (A form of parameter update that belongs to the family of stochastic approximation algorithms <ref> [19, 7] </ref>.) The new parameters are therefore somewhere between the old ones and the ones induced by the data. Thus, EM () updates the parameter values more slowly than stan-dard EM.
Reference: [8] <author> D. Heckerman. </author> <title> A tutorial on learning with Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <address> Red-mond, Washington, </address> <year> 1995. </year> <month> Revised November </month> <year> 1996. </year>
Reference-contexts: We show, both theoretically and empirically, that the convergence properties of these new algorithms can be significantly better than those of the current state-of-the-art algorithms for this problem such as EM <ref> [14, 8] </ref>. We then use the same framework to provide an initial foundation for on-line parameter estimation for Bayesian networks. In many real-world domains, the data available for learning is incomplete: some of the variables may be difficult or even impossible to observe. <p> Thus, we focus on the problem of learning from a data set consisting of data cases where some of the variables may be permanently or occasionally unobserved. In the presence of missing data, the problem of Bayesian network learning becomes much more difficult <ref> [8] </ref>. In fact, even the relatively simple task of adapting the numerical parameters (conditional probability table entries) for a given network structure becomes nontrivial. <p> Parameter estimation is an important task both because of the difficulty of eliciting accurate numerical estimates from people, and because parameter learning is part of the inner loop of more general algorithms that also learn the structure (see <ref> [8] </ref> for a survey). <p> For example, our update rule for L 2 -norm results in the gradient-projection scheme of [3, 21, 2]. Our update rule for the 2 distance results in a family of update rules with varying learning rates . This family, which we denote EM (), includes the standard EM algorithm <ref> [6, 8] </ref> as the special case EM (1). From the relative entropy distance, we derive an analogous family of multiplicative update rules which, following [12, 10], we call EG (). <p> We conclude in Section 7 with some discussion and open questions. 2 The framework In this section, we present a basic framework which can be used to interpret both the on-line learning and the more standard batch learning tasks. Our presentation and notation follow that of <ref> [8] </ref>. Recall that our task is to learn the numerical parameters for a discrete valued Bayesian network of a given structure S. <p> This is, in fact, precisely the behavior we would get from a full Bayesian updating scheme for our model (a scheme which is unfortunately infeasible in the presence of partially-observable data cases <ref> [8] </ref>). On the other hand, most of the worst case analyses of online learning algorithms do, in fact, employ a fixed learning rate to derive bounds on the performance of the on-line algorithm (see for instance [12] and the references therein).
Reference: [9] <author> D. P. Helmbold, R. E. Schapire, Y. Singer, and M. K. War-muth. </author> <title> On-line portfolio selection using multiplicative updates. </title> <booktitle> In Proc. of the Thirteenth Intl. Conf. on Machine Learning, </booktitle> <year> 1996. </year>
Reference-contexts: Furthermore, a fixed learning rate yielded very good results for other learning tasks, even when applied to natural data <ref> [9] </ref>. It is therefore an interesting and challenging research problem to determine whether an adaptive learning rate or a fixed one should be employed in on-line learning of Bayesian networks. 7 Conclusion and future work In this paper, we re-examined the problem of parameter esti mation in Bayesian networks.
Reference: [10] <author> D.P. Helmbold, R.E. Schapire, Y. Singer, and M.K. Warmuth. </author> <title> A comparison of new and old algorithms for a mixture estimation problem. </title> <booktitle> Machine Learning, </booktitle> <address> 27:97119, </address> <year> 1997. </year> <note> 4 Available from http://www.sgi.com/Technology/mlc/. </note>
Reference-contexts: This family, which we denote EM (), includes the standard EM algorithm [6, 8] as the special case EM (1). From the relative entropy distance, we derive an analogous family of multiplicative update rules which, following <ref> [12, 10] </ref>, we call EG (). We provide both theoretical and empirical evidence showing that EM () can lead to much faster convergence than standard EM while still using a very simple update rule. (In contrast to more complex and expensive second order methods such as those of [22]). <p> Intuitively, larger differences cause the parameter to be updated more rapidly. Again, serves to guide the rate at which the parameters are changed. This update rule is called EG () (see <ref> [12, 10] </ref>), due to its use of an exponentiated gradient as its main term. We note that, in our experimental results, the batch version of EG () performed quite poorly.
Reference: [11] <author> R.A. Horn and C.R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: We therefore begin by analyzing these eigen values. We first show that the eigen values of R all lie in [0; 1]. We use the following lemmas <ref> [11] </ref>: Lemma 2: Let A and B be two matrices of dimensions nfim and m fi n, respectively, where m n. Then, every eigen value of AB is also an eigen value of BA. Lemma 3: Denote by max (A) the largest eigen value of a matrix A.
Reference: [12] <author> J. Kivinen and M. K. Warmuth. </author> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <type> Technical Report UCSC-CRL-94-16, </type> <institution> University of California, Santa Cruz, </institution> <year> 1994. </year> <note> Extended abstract appeared in Proc. </note> <editor> Twenty-Seventh Ann. </editor> <booktitle> ACM Symp. Theory of Comp., </booktitle> <year> 1995. </year>
Reference-contexts: On-line learning is designed to deal with situations where we want to fine-tune an existing model, either because the model was initially inaccurate or because the environment has changed. We derive a precise formulation for these tasks using the new framework introduced by Kivinen and Warmuth <ref> [12] </ref>. Our analysis leads to the definition of a family of new algorithms for the traditional task of batch parameter estimation for Bayesian networks. <p> This family, which we denote EM (), includes the standard EM algorithm [6, 8] as the special case EM (1). From the relative entropy distance, we derive an analogous family of multiplicative update rules which, following <ref> [12, 10] </ref>, we call EG (). We provide both theoretical and empirical evidence showing that EM () can lead to much faster convergence than standard EM while still using a very simple update rule. (In contrast to more complex and expensive second order methods such as those of [22]). <p> Intuitively, larger differences cause the parameter to be updated more rapidly. Again, serves to guide the rate at which the parameters are changed. This update rule is called EG () (see <ref> [12, 10] </ref>), due to its use of an exponentiated gradient as its main term. We note that, in our experimental results, the batch version of EG () performed quite poorly. <p> Thus, the learner faces contradictory demands: it has to keep track of what has been learned so far while adjusting its hypothesis based on the new examples. The on-line learning framework of Kivinen and Warmuth <ref> [12] </ref> provides an analytical tool to balance these two requirements. In Section 2, we laid the foundation for applying these tools in the context of Bayesian networks. In this section, we interpret the resulting rules in the on-line setting. <p> On the other hand, most of the worst case analyses of online learning algorithms do, in fact, employ a fixed learning rate to derive bounds on the performance of the on-line algorithm (see for instance <ref> [12] </ref> and the references therein). Furthermore, a fixed learning rate yielded very good results for other learning tasks, even when applied to natural data [9]. <p> By applying a recently devel oped theoretical framework for this task <ref> [12] </ref>, we derive two families of update rules EM () and EG (), where EM (1) is simply the standard EM algorithm for Bayesian networks.
Reference: [13] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <address> New-York, </address> <year> 1959. </year>
Reference-contexts: Thus, we can compare the distance between the two parameter vectors using the distance between the distributions they induce over this space. One of the widely-used distance measures for two distributions over the same space is the relative entropy (also known as KL-divergence <ref> [13] </ref>).
Reference: [14] <author> S. L. Lauritzen. </author> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 19:191201, </volume> <year> 1995. </year>
Reference-contexts: We show, both theoretically and empirically, that the convergence properties of these new algorithms can be significantly better than those of the current state-of-the-art algorithms for this problem such as EM <ref> [14, 8] </ref>. We then use the same framework to provide an initial foundation for on-line parameter estimation for Bayesian networks. In many real-world domains, the data available for learning is incomplete: some of the variables may be difficult or even impossible to observe.
Reference: [15] <author> Y. Le Cun, I. Kanter, and S. A. Solla. </author> <title> Eigenvalues of covari-ance matrices: Application to neural- network learning. </title> <journal> Physical Review Letters, </journal> <volume> 66(18):23962399, </volume> <year> 1991. </year>
Reference-contexts: However, if we assume that our current parameter vector is fairly close to ? , then min and max at the current position can be used to provide a fairly good estimate for fl . These eigenvalues can be estimated fairly efficiently using techniques similar to those used in <ref> [15] </ref>. Of course, empirical testing would be necessary to check whether the additional computational cost of approximating fl is worthwhile in practice. Finally, we note that our convergence theorem and the result on the optimal rate of convergence hold only in a neighborhood of the local maximum.
Reference: [16] <author> K. G. Olesen, S. L. Lauritzen, and F. V. Jensen. aHUGIN: </author> <title> A system for creating adaptive causal probabilistic networks. </title> <booktitle> In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence (UAI-92). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: We would like the system to fine-tune itself based on these cases, adapting its network model to the environment. Nevertheless, the problem of online learning has been left largely unexplored in the context of Bayesian networks (with the exception of <ref> [16] </ref> and the recent work of [?] on structure learning with fully observable data). In this section, we take a first step towards providing a formal model for this task. The key assumption in the on-line setting is that the learner cannot store past examples.
Reference: [17] <author> B.C. Peters and H.F. Walker. </author> <title> An iterative procedure for obtaining maximum-likelihood estimates of the parameters for a mixture of normal distributions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35:362378, </volume> <year> 1978. </year>
Reference-contexts: It then speeds up the update process, going even further in that direction than what is implied by the data. While this extrapolation might seem somewhat counterintuitive, it has been used successfully in density estimation problems <ref> [17, 18] </ref>. As we show in the next two sections, this faster update rate can speed up convergence considerably. Finally, we consider the relative entropy update rule (8). <p> Our analysis generalizes a technique first used by Peters and Walker <ref> [17] </ref> in the context of estimating the parameters of a mixture of normal distributions.
Reference: [18] <author> B.C. Peters and H.F. Walker. </author> <title> The numerical evaluation of the maximum-likelihood estimates of a subset of mixture proportions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35:447452, </volume> <year> 1978. </year>
Reference-contexts: It then speeds up the update process, going even further in that direction than what is implied by the data. While this extrapolation might seem somewhat counterintuitive, it has been used successfully in density estimation problems <ref> [17, 18] </ref>. As we show in the next two sections, this faster update rate can speed up convergence considerably. Finally, we consider the relative entropy update rule (8).
Reference: [19] <author> H. Robbins and S. Monro. </author> <title> A stochastic approximation model. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22, </volume> <year> 1951. </year>
Reference-contexts: We therefore call this parameterized update rule EM (). For &lt; 1, EM () instantiates the new parameter values to be a weighted combination between the EM update and the current vector of parameters. (A form of parameter update that belongs to the family of stochastic approximation algorithms <ref> [19, 7] </ref>.) The new parameters are therefore somewhere between the old ones and the ones induced by the data. Thus, EM () updates the parameter values more slowly than stan-dard EM.
Reference: [20] <author> D. Rubin. </author> <title> Inference and missing data. </title> <journal> Biometrika, </journal> <volume> 3:581592, </volume> <year> 1976. </year>
Reference-contexts: output nodes given the (partial) instantiation of the input nodes in the training case. 2 We measured these parameters 1 Since the process of obscuring values does not depend on the actual values of any of the variables in the data case, this process is ignorable as defined by Rubin <ref> [20] </ref>. 2 If the probability of the relevant output node given the observed input nodes is p in the learned network and p fl in the correct network, then the absolute error is jp p fl and the relative error is (a) (b) cases, using 20% unknown input values in both.
Reference: [21] <author> S.J. Russell, J. Binder, D. Koller, and K. </author> <title> Kanazawa. Local learning in probabilistic networks with hidden variables. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI '95), </booktitle> <pages> pages 11461152, </pages> <year> 1995. </year>
Reference-contexts: Surprisingly, it turns out that well-known parameter estimation algorithms are special cases of our framework. For example, our update rule for L 2 -norm results in the gradient-projection scheme of <ref> [3, 21, 2] </ref>. Our update rule for the 2 distance results in a family of update rules with varying learning rates . This family, which we denote EM (), includes the standard EM algorithm [6, 8] as the special case EM (1).
Reference: [22] <author> B. Thiesson. </author> <title> Accelerated quantification of bayesian networks with incomplete data. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD-95)., </booktitle> <pages> pages 306311, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 2021 1995. </year> <note> AAAI Press. </note>
Reference-contexts: We provide both theoretical and empirical evidence showing that EM () can lead to much faster convergence than standard EM while still using a very simple update rule. (In contrast to more complex and expensive second order methods such as those of <ref> [22] </ref>). In particular, we show in Section 4 that, while 1 is the largest value of for which convergence to a local maximum is guaranteed, some value fl which is bigger than 1 provides the optimal convergence rate.
References-found: 22

