URL: http://www.cs.washington.edu/homes/kapu/552/report.ps
Refering-URL: http://www.cs.washington.edu/homes/kapu/552/proj.html
Root-URL: 
Title: An Implementation of Entry Consistency on the Cray T3D  
Author: Kari Pulli Peter Van Vleet 
Date: January 12, 1995  
Abstract: This paper presents several strategies for the implementation of an entry consistent, distributed shared memory model on the Cray T3D. The Cray T3D supports a very fast interconnect and allows direct remote memory accesses. We found that with such fast communication, many optimizations useful in the typical distributed environment (a relatively slow interconnect) are counter productive compared to a simple and straightforward implementation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Carter, J. Bennett, W. Zwaenepoel, </author> <title> "Techniques for Reducing Consistency Related Communication in Distributed Shared Memory Systems", </title> <note> to appear in Transactions on Computer Systems. </note>
Reference-contexts: However, these frequent synchronizations can be a source of great inefficiency. Weak consistency models relax consistency requirements to certain points of program execution and thus greatly reduce the frequency of synchronization and updating. For example, the release consistency model (Munin <ref> [1] </ref>) requires the user to explicitly acquire and release the shared memory. <p> Since DASH is a hardware implementation, it is fixed, and therefore has to be the one generic solution for every application. The hardware implementation also limits the number of bites userd in DASH representation, thus limiting its overall size. Munin <ref> [1] </ref> is similar to Ivy in its implementation: the DSM is implemented at the page level through page faults and the location of valid data is found using very similar data structures. However, the consistency model used by Munin is release consistency.
Reference: [2] <author> B. Bershad, M. Zekauskas, W. Sawdon, </author> <title> "The Midway Distributed Shared Memory System", </title> <journal> IEEE Com-pcon, </journal> <year> 1993. </year>
Reference-contexts: By the acquire, it is guaranteed to see the updates that other processors have released, and conversely a release makes its changes available to others. 1.2 The entry consistency model The entry consistency model (Midway <ref> [2] </ref>, [7]) is even weaker than the release consistency model. This enables higher performance since less data has to be updated. Each data segment is guarded by its own synchronization object. When that object is acquired, the consistency of the corresponding data, and only that data, is guaranteed. <p> Object migration implies much more than a way to implement a DSM, it subsumes both sharing a data and moving of processes. This provides a unified framework to implement, e.g., dynamic load balancing and increasing availability in the presence of failures or reconfiguration. Midway <ref> [2] </ref>, [7] is a more recent DSM that introduces entry consistency, although allows the user to run under processor consistency and release consistency as well as entry consistency.
Reference: [3] <author> E. Jul, H. Levy, N. Hutchinson, A. Black, </author> <title> "Fine-Grained Mobility in the Emerald System", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year> <month> 10 </month>
Reference-contexts: A write-share protocol avoids false sharing if processors divide the workload between themselves, the results can be merged in the end using a barrier. The basic method, however, is to broadcast all the changes to all participants once the data is released. Emerald <ref> [3] </ref> has similar aspects to Orca and Munin. Like Orca, it is object-based, like Munin, it has several implementations for a single paradigm. Emerald is based on object migration, and since there is no data replication, the system can be considered to be sequentially consistent.
Reference: [4] <author> L. Lamport, </author> <title> "Time, Clocks and the Ordering of Events in a Distributed System", </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: A solution is to tag each data element (or cache line) with a Lamport clock <ref> [4] </ref> which provides an ordering on the updates. A processor needs only update the elements whose remote Lamport clock values are less than the local ones, i.e., the elements that were updated after that processor last saw them.
Reference: [5] <author> D. Lenoski, J. Laudon, J. Truman, D. Nakahira, L. Stevens, A. Gupta, J. Hennessy, </author> <title> "The DASH Prototype: Implementation and Performance", </title> <booktitle> Proc. 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Thus, while it is possible for coarser grained Lamport Clocks to be faster then our straightforward implementation, their exact performance is going to be highly dependent on the nature of the application. 5 A pipelined approach We had also considered adopting the fined grained pipelining of updates that the DASH <ref> [5] </ref> used in its implementation. This is appealing, because like the DASH, the T3D has a dedicated, low latency network. <p> For example, if an increment is not atomic and two processors read the value of a variable and then increment it, one of the increments may get lost. The major drawback of the system is that it won't scale due to the central sequentializer. DASH <ref> [5] </ref> is a complete hardware implementation of the release consistency model. The granularity of sharing is a second level cache line. Cache coherence is maintained by a directory-based point-to-point snooping write-invalidate protocol between a processor and a remote processor's memory. Updates of shared data are pipelined across a dedicated network.
Reference: [6] <author> K. Li, P. Hudak, </author> <title> "Memory Coherence in Shared Virtual Memory Systems", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: This task can be facilitated by providing the user with an illusion of a global shared memory. In order to guarantee a fully consistent view of memory at all times, writes to shared data have to be synchronized (Ivy <ref> [6] </ref> and Orca [8]). However, these frequent synchronizations can be a source of great inefficiency. Weak consistency models relax consistency requirements to certain points of program execution and thus greatly reduce the frequency of synchronization and updating. <p> Ivy <ref> [6] </ref> was a very influential system that explored ways to implement a page base sequentially consistent DSM. Several processors can share and read a page, but only one writer is allowed at a time.
Reference: [7] <author> M. Zekauskas, W. Sawdon, B. Bershad, </author> <title> "Software Write Detection for a Distributed Shared Memory", </title> <note> to appear in OSDI Conference, </note> <year> 1994. </year>
Reference-contexts: By the acquire, it is guaranteed to see the updates that other processors have released, and conversely a release makes its changes available to others. 1.2 The entry consistency model The entry consistency model (Midway [2], <ref> [7] </ref>) is even weaker than the release consistency model. This enables higher performance since less data has to be updated. Each data segment is guarded by its own synchronization object. When that object is acquired, the consistency of the corresponding data, and only that data, is guaranteed. <p> These trade-offs are considered in detail for a standard distributed system in Midway <ref> [7] </ref>. The conclusions that we draw about the feasibility of various implementations are directly based on these measurements, and we will show that the normal assumptions of distributed computing do not necessarily hold in more tightly coupled systems. We start with an example of programming in the entry consistency model. <p> Object migration implies much more than a way to implement a DSM, it subsumes both sharing a data and moving of processes. This provides a unified framework to implement, e.g., dynamic load balancing and increasing availability in the presence of failures or reconfiguration. Midway [2], <ref> [7] </ref> is a more recent DSM that introduces entry consistency, although allows the user to run under processor consistency and release consistency as well as entry consistency. <p> And having non-paged based sharing exchanges infrequently but costly write collection in the relatively slow MMU hardware for more frequent but much less expensive software write detection. <ref> [7] </ref> explores this trade-off in detail and shows that software write detection can be profitable. 8 Conclusions In this paper, we have examined several strategies in implementing the entry consistency memory model on the Cray T3D.
Reference: [8] <author> A. Tanenbaum, M. Kaashoek, H. Bal, </author> <title> "Parallel Programming Using Shared Objects and Broadcasting", </title> <journal> IEEE Computer Magazine, </journal> <pages> pp. 10-19, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This task can be facilitated by providing the user with an illusion of a global shared memory. In order to guarantee a fully consistent view of memory at all times, writes to shared data have to be synchronized (Ivy [6] and Orca <ref> [8] </ref>). However, these frequent synchronizations can be a source of great inefficiency. Weak consistency models relax consistency requirements to certain points of program execution and thus greatly reduce the frequency of synchronization and updating. <p> One of the main disadvantages of page based sharing is that such large granularity creates false sharing, i.e., several processors swap a page back and forth as they access unrelated data that happens to be in the same page. Orca <ref> [8] </ref> is an object-based sequentially consistent DSM. Since the unit of sharing is an object, the false sharing problem is avoided. The data is replicated which makes the reads very fast.
Reference: [9] <author> C. Thekkath, H. Levy, E. Lazowska, </author> <title> "Separating Data and Control Transfer in Distributed Operating Systems", </title> <booktitle> Proc. 6th Annual International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: We have shown that in this environment, the traditional distributed methods don't perform as well and as consistently as more direct and simpler methods. This work suggests that as new network technologies become available, such as 622 Mb/s ATM, and new communication paradigms emerge <ref> [9] </ref>, distributed systems will become more tightly coupled. And with these more tightly coupled distributed systems, we may have to re-evaluate the effectiveness of standard distributed techniques and implementations.
Reference: [10] <author> R. </author> <title> Numrich, "The Cray T3D Address Space and How to Use It", Cray Research Inc. </title> <type> Technical Report, </type> <month> August </month> <year> 1994. </year>
Reference-contexts: Other operations such as locks, barriers, broadcasts, messages with interrupts, and cache management are also provided to the user via libraries. A good overview of the T3D and its communication paradigms can be found in <ref> [10] </ref>. Because the processors on the T3D are only running at 150MHz and have small direct mapped caches, they are relatively slow compared to state of the art workstations that one might use in a truly distributed system.
References-found: 10

