URL: http://www.cs.wustl.edu/~jxh/research/papers/proposal.ps.gz
Refering-URL: http://www.cs.wustl.edu/~jxh/research/research.html
Root-URL: http://www.cs.wustl.edu
Email: jxh@cs.wustl.edu  
Title: Creating a Framework for Developing High-performance Web Servers over ATM  
Author: Id: aa-main.tex,v . // :: jxh Exp James C. Hu 
Address: St. Louis, MO 63130, USA  
Affiliation: Department of Computer Science Washington University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Jussara Almeida, Virg ilio Almeida, and David J. Yates. </author> <title> Measuring the Behavior of a World-Wide Web Server. </title> <type> Technical Report TR-CS-96-025, </type> <institution> Department of Computer Science, Boston University, </institution> <month> October 29 </month> <year> 1996. </year>
Reference-contexts: As shown in Section 2, existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques [32] or protocol optimizations [17, 20, 4]. Moreover, measurements of server performance have focused largely on low-speed networks <ref> [1, 15] </ref>. However, I am unaware of any publication, aside from unofficial accounts [22], that describes the design principles and techniques necessary to develop high-performance Web servers. To my knowledge, there is no related work on achieving Web server performance over high speed networks. <p> Workload has been analytically modeled at the University of Saskatchewan [14], but similar results were observed by Trent [10]. Additional performance tools have been developed at Boston University to analyze the Web server performance overheads of Apache on a Pentium-based machine <ref> [1] </ref>. 2.3 Optimization Strategies One of the purposes for performance measurements is to discover areas for optimization in order to improve performance. One way to improve performance is by removing overhead in the protocol itself.
Reference: [2] <author> M. Stella Atkins, Samuel T. Chanson, and James B. Robinson. </author> <title> LNTP An Efficient Transport Protocol for Local Area Networks. </title> <booktitle> In Proceedings of the Conference on Global Communications (GLOBECOM), </booktitle> <pages> pages 705-710, </pages> <year> 1988. </year>
Reference-contexts: Solutions to this problem have included protocol delayering, protocol parallelizing, and using specialized hardware. Protocol delayering is the process where several layers of a protocol stack are implemented as a single layer (or eliminated completely as in <ref> [2] </ref>), thus avoiding some data copying overhead. Protocol parallelizing implements protocol handling by processing multiple layers of the protocol concurrently [30], e.g., marshalling and de-marshalling (presentation layer), connection multiplexing (transport layer), and directory lookup (application layer).
Reference: [3] <author> Anselm Baird-Smith. </author> <title> Jigsaw performance evaluation. </title> <note> Available from http://www.w3.org/, October 1996. </note>
Reference-contexts: The first was market penetration, in order to gauge the performance of servers that are widely used. The second was reported performance, as reported by benchmark results published by Jigsaw and NCSA, as well as information available from WebCompare <ref> [3, 15, 35] </ref>. The third was source/object code availability, which is essential for white-box analysis. Based on these criteria, we selected a mixture of commercial and free server implementations, with source available for all of the free servers.
Reference: [4] <author> Azer Bestavros. </author> <title> Using speculation to reduce server load and service time on the WWW. In Proceedings of CIKM'95: </title> <booktitle> The Fourth ACM International Conference on Information and Knowledge Management, </booktitle> <month> Novem-ber </month> <year> 1995. </year>
Reference-contexts: As shown in Section 2, existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques [32] or protocol optimizations <ref> [17, 20, 4] </ref>. Moreover, measurements of server performance have focused largely on low-speed networks [1, 15]. However, I am unaware of any publication, aside from unofficial accounts [22], that describes the design principles and techniques necessary to develop high-performance Web servers.
Reference: [5] <author> Alexander Carlton. </author> <title> An Explanation of the SPECweb96 Benchmark. Standard Performance Evaluation Corporation whitepaper, </title> <note> 1996. Available from http://www.specbench.org/. </note>
Reference-contexts: The growing number of available servers prompted the need for assessing their relative performance. The current standard benchmarks available are WebStone (by SGI) and SPECweb96 (by SPEC), both heavily influenced by the design of LADDIS <ref> [10, 5] </ref>. Both WebStone and SPECweb96 attempt to measure overall performance, and rate the performance of a server with a single number, where a higher number indicates better performance. <p> There were two motivations for this design: * Predictability: The client traffic generator permits us to control the request rate per second to within 3 significant digits. For example, 2:15 requests per second corresponds to 129 requests per minute. Conventional benchmarking tools <ref> [10, 5] </ref> also issue requests at various rates. <p> The content requested by clients was a 5 megabyte GIF file, which is the equivalent of a large desktop-sized 256-color picture. 2. Small file transfer: This black-box experiment consisted of requests for a single small file. This design is motivated by previous workload studies <ref> [14, 5] </ref>. These studies show that more than 80% of all requested files from a typical Web server are 10 kB or smaller, and that 85% of the requested files are from 15% of the files available on the Web server. <p> Apply additional optimizations: There are additional strategies that have not yet been incorporated into JAWS. These include prioritized request handling, and parallelized protocol processing. * Workload studies have indicated that the most frequently requested files from a Web server are small (less than 10 Kbytes) <ref> [14, 5] </ref>. Thus, Web servers should adaptively optimize themselves to provide higher priorities for smaller requests. These techniques 20 combined could potentially produce a server capable of being highly responsive and maximizes throughput. * It is possible to parallelize some of the processing of the HTTP protocol.
Reference: [6] <author> Steve Crane, Jeff Magee, and Nat Pryce. </author> <title> Design Patterns for Binding in Distributed Systems. </title> <booktitle> In The OOPSLA '95 Workshop on Design Patterns for Concurrent, Parallel, and Distributed Object-Oriented Systems, </booktitle> <address> Austin, TX, </address> <month> October </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: A design pattern is intended to solve a general design problem for a specific context. Many patterns have been observed in the context of concurrent, parallel and distributed systems <ref> [29, 6, 26] </ref>. Many of these ideas are applicable to Web server design. Katz presents an NCSA prototype of a scalable Web server design [13]. This design was prompted by the growing number of requests being made to the NCSA server machine.
Reference: [7] <author> Zubin D. Dittia, Jr. Jerome R. Cox, and Guru M. Parulkar. </author> <title> Design of the APIC: A High Performance ATM Host-Network Interface Chip. </title> <booktitle> In IEEE INFOCOM '95, </booktitle> <pages> pages 179-187, </pages> <address> Boston, USA, April 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Specialized hardware, which understands the nature of the protocol, can be used to avoid protocol processing by the CPU altogether, and handle the protocol without passing any data to memory in user space (zero data copies) <ref> [7] </ref>. Each of these solutions can improve overall performance. It may be impractical, however, to substantially alter the network device drivers or create new hardware for every new protocol that may arise, such as HTTP/99.3. Further, the techniques do not directly address the issue of adaptive protocol processing.
Reference: [8] <author> J.R. Eykholt, S.R. Kleiman, S. Barton, R. Faulkner, A Shivalingiah, M. Smith, D. Stein, J. Voll, M. Weeks, and D. Williams. </author> <title> Beyond Multiprocessing... Multithreading the SunOS Kernel. </title> <booktitle> In Proceedings of the Summer USENIX Conference, </booktitle> <address> San Antonio, Texas, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: However, bound threads are more expensive to create <ref> [8] </ref>. This cost becomes negligible, however, when pre-spawned thread pools are used. requests into a message queue, from which free threads from the pool can dequeue a request and begin processing. * Single-threaded concurrent: Threads are a good choice overall, since they scale well to multiple CPU platforms.
Reference: [9] <author> Erich Gamma, Richard Helm, Ralph John-son, and John Vlissides. </author> <title> Design Patterns: Elements of Reusable Object-Oriented Software. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1995. </year>
Reference-contexts: Latency can also be improved by using caching proxies and caching clients, although the removal policy must be considered carefully [32]. 2.4 Web Server Design and Implementation Design patterns are re-usable software abstractions that have been observed to occur many times in actual solutions <ref> [9] </ref>. A design pattern is intended to solve a general design problem for a specific context. Many patterns have been observed in the context of concurrent, parallel and distributed systems [29, 6, 26]. Many of these ideas are applicable to Web server design.
Reference: [10] <author> Gene Trent and Mark Sake. WebSTONE: </author> <title> The First Generation in HTTP Server Benchmarking. Silicon Graphics, </title> <publisher> Inc. </publisher> <address> whitepaper, </address> <month> February </month> <year> 1995. </year> <note> Available from http://www.sgi.com/. </note>
Reference-contexts: The growing number of available servers prompted the need for assessing their relative performance. The current standard benchmarks available are WebStone (by SGI) and SPECweb96 (by SPEC), both heavily influenced by the design of LADDIS <ref> [10, 5] </ref>. Both WebStone and SPECweb96 attempt to measure overall performance, and rate the performance of a server with a single number, where a higher number indicates better performance. <p> In addition to gauging the relative performance of Web servers, studies have been done to characterize the Web server workload and performance overheads. Workload has been analytically modeled at the University of Saskatchewan [14], but similar results were observed by Trent <ref> [10] </ref>. Additional performance tools have been developed at Boston University to analyze the Web server performance overheads of Apache on a Pentium-based machine [1]. 2.3 Optimization Strategies One of the purposes for performance measurements is to discover areas for optimization in order to improve performance. <p> There were two motivations for this design: * Predictability: The client traffic generator permits us to control the request rate per second to within 3 significant digits. For example, 2:15 requests per second corresponds to 129 requests per minute. Conventional benchmarking tools <ref> [10, 5] </ref> also issue requests at various rates.
Reference: [11] <author> Aniruddha Gokhale and Douglas C. Schmidt. </author> <title> Measuring the Performance of Communication Middleware on High-Speed Networks. </title> <booktitle> In Proceedings of SIGCOMM '96, </booktitle> <pages> pages 306-317, </pages> <address> Stanford, CA, </address> <month> August </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: A maximum of 32 Kbytes is allotted per ATM virtual circuit connection for receiving and transmitting frames (for a total of 64 K). This allows up to eight switched virtual connections per card. This testbed is similar to the one used in <ref> [11] </ref>. 3.1.2 Client Traffic Generators To provide a thorough and accurate understanding of server performance bottlenecks, I designed a new benchmarking technique. In this technique, a client traffic generator sends HTTP GET requests to a Web server at a particular rate.
Reference: [12] <author> Tim Harrison and Douglas C. Schmidt. </author> <title> Thread-Specific Storage: A Pattern for Reducing Locking Overhead in Concurrent Programs. </title> <booktitle> In OOPSLA Workshop on Design Patterns for Concurrent, Parallel, and Distributed Systems. ACM, </booktitle> <month> October </month> <year> 1995. </year>
Reference-contexts: Surprisingly, the literature lack any discussion of the relative performance between concurrency strategies. This may be due to the difficulty of decoupling the concur-rency strategy from the rest of server implementation. Various sources discuss the tradeoffs between multi-threaded and single-threaded solutions <ref> [12, 28] </ref>. These are understood to be simplicity of design and implementation and overlapped I/O versus synchronization and context switching overhead. 3 Experimental Results of Web Server Performance over ATM 3.1 Web Server/ATM Testbed Environment The performance measurements were done in collaboration with Sumedh Mungee.
Reference: [13] <author> Eric Dean Katz, Michelle Butler, and Robert McGrath. </author> <title> A Scalable HTTP Server: The NCSA Prototype. </title> <booktitle> In Proceedings of the First International Conference on the World-Wide Web, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: The need for scalable Web servers, i.e. servers which can be scaled up to handle increasingly larger capacities, was observed even in the early stages of the World Wide Web. The need was noted in <ref> [13] </ref>, where they demonstrate a scalable decentralized distributed server. The need for high-capacity servers spurred commercial sector activity, and many server implementations are available on the market [35]. The growing number of available servers prompted the need for assessing their relative performance. <p> Many patterns have been observed in the context of concurrent, parallel and distributed systems [29, 6, 26]. Many of these ideas are applicable to Web server design. Katz presents an NCSA prototype of a scalable Web server design <ref> [13] </ref>. This design was prompted by the growing number of requests being made to the NCSA server machine. Many commercial server implementations arose to meet the demand for high-performance Web servers. Higher end implementations, such as Netscape Enterprise and Zeus, use multi-threading to scale for high-end architectures with multiple CPUs.
Reference: [14] <author> Martin F. Arlitt and Carey L. Williamson. </author> <title> Intenet Web Servers: Workload Characterization and Performance Implications. </title> <type> Technical report, </type> <institution> University of Saskatchewan, </institution> <month> December </month> <year> 1996. </year> <note> A shorter version of this paper appeared in ACM SIGMETRICS '96. </note>
Reference-contexts: In addition to gauging the relative performance of Web servers, studies have been done to characterize the Web server workload and performance overheads. Workload has been analytically modeled at the University of Saskatchewan <ref> [14] </ref>, but similar results were observed by Trent [10]. <p> The content requested by clients was a 5 megabyte GIF file, which is the equivalent of a large desktop-sized 256-color picture. 2. Small file transfer: This black-box experiment consisted of requests for a single small file. This design is motivated by previous workload studies <ref> [14, 5] </ref>. These studies show that more than 80% of all requested files from a typical Web server are 10 kB or smaller, and that 85% of the requested files are from 15% of the files available on the Web server. <p> Apply additional optimizations: There are additional strategies that have not yet been incorporated into JAWS. These include prioritized request handling, and parallelized protocol processing. * Workload studies have indicated that the most frequently requested files from a Web server are small (less than 10 Kbytes) <ref> [14, 5] </ref>. Thus, Web servers should adaptively optimize themselves to provide higher priorities for smaller requests. These techniques 20 combined could potentially produce a server capable of being highly responsive and maximizes throughput. * It is possible to parallelize some of the processing of the HTTP protocol.
Reference: [15] <author> Robert E. McGrath. </author> <title> Performance of Several HTTP Demons on an HP 735 Workstation. </title> <note> Avaiable from http://www.ncsa.uiuc.edu/, April 25 1995. </note>
Reference-contexts: As shown in Section 2, existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques [32] or protocol optimizations [17, 20, 4]. Moreover, measurements of server performance have focused largely on low-speed networks <ref> [1, 15] </ref>. However, I am unaware of any publication, aside from unofficial accounts [22], that describes the design principles and techniques necessary to develop high-performance Web servers. To my knowledge, there is no related work on achieving Web server performance over high speed networks. <p> The first was market penetration, in order to gauge the performance of servers that are widely used. The second was reported performance, as reported by benchmark results published by Jigsaw and NCSA, as well as information available from WebCompare <ref> [3, 15, 35] </ref>. The third was source/object code availability, which is essential for white-box analysis. Based on these criteria, we selected a mixture of commercial and free server implementations, with source available for all of the free servers.
Reference: [16] <author> Marshall Kirk McKusick, Keith Bostic, Michael J. Karels, and John S. Quarter-man. </author> <title> The Design and Implementation of the 4.4BSD Operating System. </title> <publisher> Addison Wesley, </publisher> <year> 1996. </year>
Reference-contexts: To speed up common operations like directory listings, newer versions of UNIX implement stat more efficiently (i.e., requiring less than 50 s on average) by leaving the pointer at the name of the last successful stat call <ref> [16] </ref>. The information returned in a struct stat buffer must still be parsed, however. For instance, once stat returns, a Web server must immediately test to see if the requested file is a directory.
Reference: [17] <author> Jeffrey C. Mogul. </author> <note> The Case for Persistent-Connection HTTP. Technical Report 95/4, </note> <institution> Digital Equipment Corporation Western Research Laboratory, </institution> <month> May </month> <year> 1995. </year> <note> Available online at http://www.research.digital.com/. </note>
Reference-contexts: As shown in Section 2, existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques [32] or protocol optimizations <ref> [17, 20, 4] </ref>. Moreover, measurements of server performance have focused largely on low-speed networks [1, 15]. However, I am unaware of any publication, aside from unofficial accounts [22], that describes the design principles and techniques necessary to develop high-performance Web servers. <p> The W 3 C is currently standardizing HTTP/1.1, which multiplexes multiple requests over a single connection. This "connection-caching" strategy can significantly enhance the performance over HTTP/1.0 by reducing unnecessary connection set up and termination [36, 20]. The need for persistent connections to improve latency was noted by Mogul in <ref> [17] </ref>. Latency can also be improved by using caching proxies and caching clients, although the removal policy must be considered carefully [32]. 2.4 Web Server Design and Implementation Design patterns are re-usable software abstractions that have been observed to occur many times in actual solutions [9].
Reference: [18] <author> Nancy J. Yeager and Robert E. McGrath. </author> <title> Web Server Technology: The Advanced Guide for World Wide Web Information Providers. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year> <month> 23 </month>
Reference-contexts: An informal discussion about Web server performance issues can be found in [22]. Yeager and McGrath of NCSA discuss the tradeoffs of some Web server designs and their performance in <ref> [18] </ref>. 4 2.5 Concurrency Strategies A concurrency strategy specifies the policy and mecahnism for allowing multiple tasks to execute simultaneously. In the case of a server, as task is an object which encapsulates the handling of server requests.
Reference: [19] <author> Bill Nowicki. </author> <title> Transport Issues in the Net--work File System. </title> <journal> ACM Computer Communication Review, </journal> <volume> 19(2) </volume> <pages> 16-20, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The issues surrounding NFS such as reliability and cache co 3 herency are discussed in detail in the literature <ref> [19, 23] </ref>. To measure the performance of filesystems, the Standard Performance Evaluation Corporation (SPEC) designed the LAD-DIS benchmark [40]. The need for scalable Web servers, i.e. servers which can be scaled up to handle increasingly larger capacities, was observed even in the early stages of the World Wide Web.
Reference: [20] <author> R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee. </author> <title> Hypertext Transfer Protocol - HTTP/1.1. Standards Track RFC 2068, </title> <institution> Network Working Group, </institution> <month> January </month> <year> 1997. </year> <note> Available from http://www.w3.org/. </note>
Reference-contexts: As shown in Section 2, existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques [32] or protocol optimizations <ref> [17, 20, 4] </ref>. Moreover, measurements of server performance have focused largely on low-speed networks [1, 15]. However, I am unaware of any publication, aside from unofficial accounts [22], that describes the design principles and techniques necessary to develop high-performance Web servers. <p> One way to improve performance is by removing overhead in the protocol itself. The W 3 C is currently standardizing HTTP/1.1, which multiplexes multiple requests over a single connection. This "connection-caching" strategy can significantly enhance the performance over HTTP/1.0 by reducing unnecessary connection set up and termination <ref> [36, 20] </ref>. The need for persistent connections to improve latency was noted by Mogul in [17].
Reference: [21] <author> Dennis Ritchie. </author> <title> A Stream Input-Output System. </title> <journal> AT&T Bell Labs Technical Journal, </journal> <volume> 63(8) </volume> <pages> 311-324, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: The SunOS 5.5.1 TCP/IP protocol stack is implemented using the STREAMS communication framework <ref> [21] </ref>. Each UltraSparc-2 has 256 Mbytes of RAM and an ENI-155s-MF ATM adaptor card, which supports 155 Megabits per-sec (Mbps) SONET multimode fiber. The Maximum Transmission Unit (MTU) on the ENI ATM adaptor is 9,180 bytes. Each ENI card has 512 Kbytes of on-board memory.
Reference: [22] <author> James Rubarth-Lay. </author> <title> Keeping the 400lb. Gorilla at Bay: Optimizing Web Performance, </title> <note> May 9 1996. Class paper for UT Austin LIS385T.6, Spring 96. </note>
Reference-contexts: Moreover, measurements of server performance have focused largely on low-speed networks [1, 15]. However, I am unaware of any publication, aside from unofficial accounts <ref> [22] </ref>, that describes the design principles and techniques necessary to develop high-performance Web servers. To my knowledge, there is no related work on achieving Web server performance over high speed networks. <p> Other implementations (e.g., Roxen, BOA and thttpd) use a single thread of control to optimize performance on single CPU architectures. An informal discussion about Web server performance issues can be found in <ref> [22] </ref>. Yeager and McGrath of NCSA discuss the tradeoffs of some Web server designs and their performance in [18]. 4 2.5 Concurrency Strategies A concurrency strategy specifies the policy and mecahnism for allowing multiple tasks to execute simultaneously.
Reference: [23] <author> R. Sandberg. </author> <title> The Sun Network Filesystem: Design, Implementation, and Experience. </title> <booktitle> In Proceedings of the Summer 1986 USENIX Technical Conference and Exhibition, </booktitle> <year> 1986. </year>
Reference-contexts: The issues surrounding NFS such as reliability and cache co 3 herency are discussed in detail in the literature <ref> [19, 23] </ref>. To measure the performance of filesystems, the Standard Performance Evaluation Corporation (SPEC) designed the LAD-DIS benchmark [40]. The need for scalable Web servers, i.e. servers which can be scaled up to handle increasingly larger capacities, was observed even in the early stages of the World Wide Web.
Reference: [24] <author> Douglas C. Schmidt. GPERF: </author> <title> A Perfect Hash Function Generator. </title> <booktitle> In Proceedings of the 2 nd C++ Conference, </booktitle> <pages> pages 87-102, </pages> <address> San Francisco, California, </address> <month> April </month> <year> 1990. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: Perfect hashing is feasible since the password file is relatively static and changes infrequently, thereby amortizing the cost of reconstructing the perfect hash. Moreover, since the perfect hash function can be computed "off-line," sophisticated tools <ref> [24] </ref> can be used to generate the perfect hash table. A JAWS server can be notified on-demand as the password file changes, in order to reload the cache.
Reference: [25] <author> Douglas C. Schmidt. </author> <title> ACE: an Object-Oriented Framework for Developing Distributed Applications. </title> <booktitle> In Proceedings of the 6 th USENIX C++ Technical Conference, </booktitle> <address> Cambridge, Massachusetts, </address> <month> April </month> <year> 1994. </year> <institution> USENIX Association. </institution>
Reference-contexts: Holding network and file I/O constant (by using memory-cached files) allowed me to precisely pinpoint the request rates where a server starts to overload. These results help to create an accurate behavioral model for Web servers. 6 The client traffic generator is based on the ACE network programming toolkit <ref> [25] </ref>, which is an object-oriented framework composed of strategic and tactical design patterns that simplify the development of high-performance, concurrent communication software.
Reference: [26] <author> Douglas C. Schmidt. </author> <title> Flexible configuration of high-performance object-oriented distributed communication systems. </title> <booktitle> In 9 th OOPSLA Conference, Workshop on Flexibility in Systems Software, </booktitle> <address> Portland, Ore-gon, </address> <month> October </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: A design pattern is intended to solve a general design problem for a specific context. Many patterns have been observed in the context of concurrent, parallel and distributed systems <ref> [29, 6, 26] </ref>. Many of these ideas are applicable to Web server design. Katz presents an NCSA prototype of a scalable Web server design [13]. This design was prompted by the growing number of requests being made to the NCSA server machine.
Reference: [27] <author> Douglas C. Schmidt. </author> <title> Reactor: An Object Behavioral Pattern for Concurrent Event Demultiplexing and Dispatching. </title> <booktitle> In Proceedings of the 1 st Annual Conference on the Pattern Languages of Programs, </booktitle> <pages> pages 1-10, </pages> <address> Monticello, Illinois, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The use of these strategies are prevalent enough to have been identified as patterns. For example, the solution to designing a single-threaded concurrent server is to apply the reactor pattern <ref> [27] </ref>. Surprisingly, the literature lack any discussion of the relative performance between concurrency strategies. This may be due to the difficulty of decoupling the concur-rency strategy from the rest of server implementation. Various sources discuss the tradeoffs between multi-threaded and single-threaded solutions [12, 28].
Reference: [28] <author> Douglas C. Schmidt. </author> <title> Reactor: An Object Behavioral Pattern for Concurrent Event Demultiplexing and Event Handler Dispatching. </title> <editor> In James O. Coplien and Dou-glas C. Schmidt, editors, </editor> <booktitle> Pattern Languages of Program Design. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1995. </year>
Reference-contexts: Surprisingly, the literature lack any discussion of the relative performance between concurrency strategies. This may be due to the difficulty of decoupling the concur-rency strategy from the rest of server implementation. Various sources discuss the tradeoffs between multi-threaded and single-threaded solutions <ref> [12, 28] </ref>. These are understood to be simplicity of design and implementation and overlapped I/O versus synchronization and context switching overhead. 3 Experimental Results of Web Server Performance over ATM 3.1 Web Server/ATM Testbed Environment The performance measurements were done in collaboration with Sumedh Mungee.
Reference: [29] <author> Douglas C. Schmidt and Charles D. Cra-nor. Half-Sync/Half-Async: </author> <title> an Architectural Pattern for Efficient and Well-structured Concurrent I/O. </title> <editor> In James O. Coplien, John Vlissides, and Norm Kerth, editors, </editor> <booktitle> Pattern Languages of Program Design. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1996. </year>
Reference-contexts: A design pattern is intended to solve a general design problem for a specific context. Many patterns have been observed in the context of concurrent, parallel and distributed systems <ref> [29, 6, 26] </ref>. Many of these ideas are applicable to Web server design. Katz presents an NCSA prototype of a scalable Web server design [13]. This design was prompted by the growing number of requests being made to the NCSA server machine.
Reference: [30] <author> Douglas C. Schmidt and Tatsuya Suda. </author> <title> The Performance of Alternative Threading Architectures for Parallel Communication Subsystems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1997. to appear. </note>
Reference-contexts: Protocol delayering is the process where several layers of a protocol stack are implemented as a single layer (or eliminated completely as in [2]), thus avoiding some data copying overhead. Protocol parallelizing implements protocol handling by processing multiple layers of the protocol concurrently <ref> [30] </ref>, e.g., marshalling and de-marshalling (presentation layer), connection multiplexing (transport layer), and directory lookup (application layer).
Reference: [31] <institution> Pure Software. </institution> <note> Quantify User's Guide, </note> <year> 1996. </year>
Reference-contexts: The timer for latency measurement is started just before the client benchmarking software sends the HTTP request and stops just after the client receives the first response from the server. Several white-box methods (such as thread creation and synchronization overhead) were collected using Quantify v2.2 <ref> [31] </ref>. Quantify analyzes server performance by identifying the functions and system calls that contribute significantly to server performance degradation at high-frequency of requests. <p> 25%-30% Virtual filesystem Read/send file 70%-75% 50%-55% Virtual filesystem Cleanup operations 2% 2% Table 3: White-box Analysis of the Apache Web server the throughput of the JAWS and Zeus servers. 2 3.2.4 White-box Analysis of Web Server Performance * Description: In this experiment, the Apache server was instrumented with Quantify <ref> [31] </ref>. Apache was then subjected to workloads identical to those described in Section 3.2.2 and Section 3.2.3. For the purposes of this presentation, only Apache was analyzed using white-box analysis.
Reference: [32] <author> Stephen Williams, Marc Abrams, Charles R. Standridge, Ghaleb Abdhulla, and Ed-ward A. Fox. </author> <title> Removal Policies in Network Caches for World-Wide Web Documents. </title> <booktitle> In Proceedings of ACM SIGCOMM'96 Conference: Applications, Technologies, Architectures, and Protocols for Computer Communications, </booktitle> <volume> volume 26, </volume> <month> October </month> <year> 1996. </year>
Reference-contexts: Thus, my work will be to provide a basis for this framework, which involves an abstraction for concurrency strategies and an abstraction for protocol construction and configuration. As shown in Section 2, existing research on improving Web performance has focused largely on reducing network latency, primarily through caching techniques <ref> [32] </ref> or protocol optimizations [17, 20, 4]. Moreover, measurements of server performance have focused largely on low-speed networks [1, 15]. However, I am unaware of any publication, aside from unofficial accounts [22], that describes the design principles and techniques necessary to develop high-performance Web servers. <p> The need for persistent connections to improve latency was noted by Mogul in [17]. Latency can also be improved by using caching proxies and caching clients, although the removal policy must be considered carefully <ref> [32] </ref>. 2.4 Web Server Design and Implementation Design patterns are re-usable software abstractions that have been observed to occur many times in actual solutions [9]. A design pattern is intended to solve a general design problem for a specific context.
Reference: [33] <author> W. Richard Stevens. </author> <title> Advanced Programming in the UNIX Environment. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: This process is repeated until the file is transferred. While this technique is straightforward to implement, it is not very efficient <ref> [33] </ref>. The problem is that the data path of the transfer copies the bytes twice: once from the filesystem into main memory and once again from main memory to the network adapter.
Reference: [34] <author> W. Richard Stevens. </author> <title> UNIX Network Programming, Second Edition. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1997. </year>
Reference-contexts: Such a server simply receives each request, and iterates through them one at a time, handling them in FIFO order. This kind of server is called an iterative server. This type of server does not perform well except in very low load servers <ref> [34] </ref>. None of the servers in our test suite uses an iterative design. * Process-per-request: A common concurrency strategy for Web servers is to fork a new process to handle each new request, Unfortunately, this strategy does not scale well on most OS implementations. <p> In Solaris 2.5, accept is not a system call, and is not atomic. In BSD 4.4, accept is a system call. More information is available in <ref> [34] </ref>. 22
Reference: [35] <author> David Strom. </author> <note> Web Compare. Available from http://webcompare.iworld.com/, 1997. </note>
Reference-contexts: The need was noted in [13], where they demonstrate a scalable decentralized distributed server. The need for high-capacity servers spurred commercial sector activity, and many server implementations are available on the market <ref> [35] </ref>. The growing number of available servers prompted the need for assessing their relative performance. The current standard benchmarks available are WebStone (by SGI) and SPECweb96 (by SPEC), both heavily influenced by the design of LADDIS [10, 5]. <p> The first was market penetration, in order to gauge the performance of servers that are widely used. The second was reported performance, as reported by benchmark results published by Jigsaw and NCSA, as well as information available from WebCompare <ref> [3, 15, 35] </ref>. The third was source/object code availability, which is essential for white-box analysis. Based on these criteria, we selected a mixture of commercial and free server implementations, with source available for all of the free servers.
Reference: [36] <author> T. Berners-Lee, R. T. Fielding, and H. Frystyk. </author> <title> Hypertext Transfer Protocol - HTTP/1.0. Informational RFC 1945, </title> <institution> Network Working Group, </institution> <month> May </month> <year> 1996. </year> <note> Available from http://www.w3.org/. </note>
Reference-contexts: One way to improve performance is by removing overhead in the protocol itself. The W 3 C is currently standardizing HTTP/1.1, which multiplexes multiple requests over a single connection. This "connection-caching" strategy can significantly enhance the performance over HTTP/1.0 by reducing unnecessary connection set up and termination <ref> [36, 20] </ref>. The need for persistent connections to improve latency was noted by Mogul in [17].
Reference: [37] <author> Andrew S. Tanenbaum. </author> <title> Computer Networks (Second Edition). </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: The networks then were slow, less than 57.6 Kbps. The TCP/IP layered protocol provided the functionality necessary for interconnections and communication between hosts. The three services provided were FTP for file transfer, SMTP for electronic mail, and TELNET for remote login <ref> [37] </ref>. These services were layered on top of TCP/IP, but the slow networks hid the costs of pushing data through every protocol layer.
Reference: [38] <author> Jonathan Turner. </author> <title> Design of a Broadcast Packet Switching Network. </title> <journal> IEEE Transactions on Communication, </journal> <volume> 4(8), </volume> <month> June </month> <year> 1988. </year>
Reference-contexts: These services were layered on top of TCP/IP, but the slow networks hid the costs of pushing data through every protocol layer. As the speed of networks has increased, due to the advent of fiber optics and high bandwidth switches <ref> [38] </ref>, it became clear that layered protocol handling was a bottleneck, due to the "data copying" problem. Solutions to this problem have included protocol delayering, protocol parallelizing, and using specialized hardware.
Reference: [39] <author> J.S. Turner. </author> <title> An optimal nonblocking mul-ticast virtual circuit switch. </title> <booktitle> In Proceedings of the Conference on Computer Communications (INFOCOM), </booktitle> <pages> pages 298-305, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: To accomplish this, we constructed a hardware and software testbed consisting of a high-performance server and client connected by a high-speed ATM switch <ref> [39] </ref> (shown in Figure 1). 1 The experiments in this paper were conducted using a Bay Networks LattisCell 10114 ATM switch connected to two dual-processor UltraSparc-2s running SunOS 5.5.1. The LattisCell 10114 is a 16 Port, OC3 155 Mbs/port switch.
Reference: [40] <author> Mark Wittle and Bruce E. Keith. LADDIS: </author> <title> The Next Generation in NFS File Server Benchmarking. </title> <booktitle> In USENIX Association Conference Proceedings '93, </booktitle> <month> April </month> <year> 1993. </year> <month> 24 </month>
Reference-contexts: The issues surrounding NFS such as reliability and cache co 3 herency are discussed in detail in the literature [19, 23]. To measure the performance of filesystems, the Standard Performance Evaluation Corporation (SPEC) designed the LAD-DIS benchmark <ref> [40] </ref>. The need for scalable Web servers, i.e. servers which can be scaled up to handle increasingly larger capacities, was observed even in the early stages of the World Wide Web. The need was noted in [13], where they demonstrate a scalable decentralized distributed server.
References-found: 40

