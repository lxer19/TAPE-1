URL: http://polaris.cs.uiuc.edu/reports/965.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Supercomputer Performance Evaluation and the Perfect Benchmarks T M  
Author: George Cybenko Lyle Kipp Lynn Pointer David Kuck 
Date: May 21, 1993  
Abstract: In the past three years, the Perfect Benchmark TM Suite has evolved from a supercomputer performance evaluation plan, presented by Kuck and Sameh at the 1987 International Conference on Supercomputing, to a vigorous international activity. This paper surveys the current state of supercomputer performance evaluation with particular focus on the methodology adopted by the Perfect effort. While there has been considerable success in achieving the goals of the plan, some issues remain unresolved, and new questions have surfaced.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bailey and J. Barton, </author> <title> The NAS kernel benchmark program, </title> <type> Tech. Rep. 86711, </type> <institution> NASA Ames Technical Memorandum, </institution> <year> 1985. </year>
Reference-contexts: Sections 6 and 7 summarize current and future directions of the Perfect benchmarking effort. Finally, we present our concluding remarks in Section 8. 2 Benchmarking with Kernels and Applications A number of general benchmarks such as the Livermore Fortran Kernels, NAS Kernels, the Linpack Benchmark, and Whetstones <ref> [11, 1, 6, 3] </ref> have emerged during the past two decades and have had considerable influence in the performance evaluation community. These benchmarks are certainly improvements over peak execution rates commonly quoted by vendors but still fall short of stressing large computer systems across the full array of architectural features.
Reference: [2] <author> M. Berry et al., </author> <title> The Perfect Club Benchmarks: Effective performance evaluation of supercomputers, </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 3 (1989), </volume> <pages> pp. 5-40. </pages>
Reference-contexts: The goal of assembling site independent, scientifically relevant complete applications codes for the purposes of supercomputer benchmark-ing was initiated by the Perfect effort participants in 1987 <ref> [2] </ref>. In 1988, an 3 industrial consortium named SPEC (Systems Performance Evaluation Co--operative) was formed to apply similar applications-based benchmarking to high performance workstations [16]. An applications-based benchmarking effort was also started at the California Institute of Technology to evaluate and compare different hypercube multiprocessor systems [12]. <p> The goals were to define an applications-based methodology for supercomputer performance evaluation along the lines described in [5], and in the process assemble and 5 port a suite of scientifically relevant codes to numerous high performance computing machines. A summary of that activity can be found in <ref> [2] </ref>. The resulting set of codes, known as the Perfect Benchmarks T M , consists of 13 programs drawn from a variety of scientific and engineering fields (see Table 1).
Reference: [3] <author> H. Curnow and B. Wichman, </author> <title> A synthetic benchmark, </title> <journal> Comput. J., </journal> <volume> 19 (1976), </volume> <pages> pp. 43-49. </pages>
Reference-contexts: Sections 6 and 7 summarize current and future directions of the Perfect benchmarking effort. Finally, we present our concluding remarks in Section 8. 2 Benchmarking with Kernels and Applications A number of general benchmarks such as the Livermore Fortran Kernels, NAS Kernels, the Linpack Benchmark, and Whetstones <ref> [11, 1, 6, 3] </ref> have emerged during the past two decades and have had considerable influence in the performance evaluation community. These benchmarks are certainly improvements over peak execution rates commonly quoted by vendors but still fall short of stressing large computer systems across the full array of architectural features.
Reference: [4] <author> G. Cybenko, </author> <title> Networking performance databases: A proposal, </title> <type> Tech. Rep. 965, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana, </institution> <type> Technical Report, </type> <year> 1990. </year>
Reference-contexts: Ideas about the design of such a networkable database facility have been investigated <ref> [4] </ref>. Comparing Benchmarks: In an area as quantitative as computer science, it is embarrassing that debates about which approach to benchmark-ing and which benchmarking programs are "better" resemble philosophical discussions more than technical analyses.
Reference: [5] <author> David J. Kuck and Ahmed H. Sameh, </author> <title> A supercomputing performance evaluation plan, </title> <type> Tech. Rep. 692, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: When the machines being compared all have a single, scalar processor, one level of memory, and similar peripheral characteristics, it is at least feasible to give a single-number performance statistic. With more advanced architectures, a single number no longer suffices. At ICS '87, Kuck and Sameh <ref> [5] </ref> presented a paper proposing a new plan to evaluate the performance of supercomputers. In this paper, we present an update on what has resulted in the three years since that paper was presented. Section 2 presents an overview of various benchmarking efforts. <p> We examine our experiences with these difficulties in a later section. 3 A Review of the Supercomputer Performance Evaluation Plan, ICS '87 In their paper, Kuck and Sameh <ref> [5] </ref> identify three basic purposes for using benchmarks to evaluate existing machines: 1. Selecting a new machine, 2. Tuning of an existing system, and 3. Determining which strengths to incorporate and which weaknesses to avoid in the design of a new machine. <p> The goals were to define an applications-based methodology for supercomputer performance evaluation along the lines described in <ref> [5] </ref>, and in the process assemble and 5 port a suite of scientifically relevant codes to numerous high performance computing machines. A summary of that activity can be found in [2].
Reference: [6] <author> J. Dongarra, </author> <title> Performance of various computers using standard linear equations software in a Fortran environment, </title> <type> Tech. Rep. 23, </type> <institution> Argonne National Lab Technical Report, </institution> <year> 1988. </year>
Reference-contexts: Sections 6 and 7 summarize current and future directions of the Perfect benchmarking effort. Finally, we present our concluding remarks in Section 8. 2 Benchmarking with Kernels and Applications A number of general benchmarks such as the Livermore Fortran Kernels, NAS Kernels, the Linpack Benchmark, and Whetstones <ref> [11, 1, 6, 3] </ref> have emerged during the past two decades and have had considerable influence in the performance evaluation community. These benchmarks are certainly improvements over peak execution rates commonly quoted by vendors but still fall short of stressing large computer systems across the full array of architectural features.
Reference: [7] <author> J. Dongarra, J. Martin, and J. Worlton, </author> <title> Computer benchmark-ing: Paths and pitfalls, </title> <journal> IEEE Spectrum, </journal> <year> (1987). </year>
Reference-contexts: Thus using a large number of kernel measurements risks giving one the illusion of excellent predictability if the methodology is not tested on a very large sample of both programs and machines. These and other observations (see for example <ref> [7, 17, 15, 8] </ref>) have lead to a growing recognition that supercomputer performance cannot be reduced to a single number nor to a set of isolated kernels or high-level program segments. <p> At present, the performance evaluation of hypercubes and large SIMD machines has largely been restricted to kernel and algorithm benchmarking. David Walker (University of South Carolina) is monitoring progress being made in porting the codes to other classes of machines. 10 6.7 Reporting Performance Numerous authors <ref> [15, 7, 17] </ref> have bemoaned the use of a single number to report supercomputer performance, and we sympathize with their arguments and conclusions.
Reference: [8] <author> R. H. Hill, </author> <title> The art of benchmarking, </title> <booktitle> The Spang Robinson Report on Supercomputing and Parallel Processing, </booktitle> <month> 3 </month> <year> (1989). </year>
Reference-contexts: Thus using a large number of kernel measurements risks giving one the illusion of excellent predictability if the methodology is not tested on a very large sample of both programs and machines. These and other observations (see for example <ref> [7, 17, 15, 8] </ref>) have lead to a growing recognition that supercomputer performance cannot be reduced to a single number nor to a set of isolated kernels or high-level program segments.
Reference: [9] <author> P. F. Koss, </author> <title> Application performance on supercomputers, </title> <type> Tech. Rep. 847, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: It can therefore be argued that they provide a simple representation for much more complex programs. In spite of such arguments, we are not aware of any positive analytic or empirical results demonstrating that such kernels explain or predict with reasonable accuracy supercomputer performance on other, more general workloads <ref> [9] </ref>. Accordingly, the identification of a small set of easy to port kernels or algorithms that characterize performance of a broad class of scientific codes remains the elusive Holy Grail of high end computer benchmarking.
Reference: [10] <author> A. Malony, </author> <title> Performance measurement intrusion and perturbation analysis, </title> <type> Tech. Rep. 923, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana, </institution> <type> Technical Report, </type> <year> 1990. </year>
Reference-contexts: There are of course limitations to the level at which finer timing results can be obtained while preserving accuracy and meaningfulness. Too much timing instrumentation can lead to undesired performance perturbation, although there is now some research suggesting that such perturbations can be factored out through post-processing <ref> [10] </ref>. Bearing in mind that instrumentation needs to be at the source code level for portability considerations, such instrumentation will be meaningful in baseline executions but runs the risk of being compromised in optimized codes since the timing points can disappear altogether.
Reference: [11] <author> F. McMahon, </author> <title> The Livermore Fortran Kernels: A test of the numerical performance range, </title> <type> tech. rep., </type> <institution> Lawrence Livermore Lab, </institution> <year> 1986. </year>
Reference-contexts: Sections 6 and 7 summarize current and future directions of the Perfect benchmarking effort. Finally, we present our concluding remarks in Section 8. 2 Benchmarking with Kernels and Applications A number of general benchmarks such as the Livermore Fortran Kernels, NAS Kernels, the Linpack Benchmark, and Whetstones <ref> [11, 1, 6, 3] </ref> have emerged during the past two decades and have had considerable influence in the performance evaluation community. These benchmarks are certainly improvements over peak execution rates commonly quoted by vendors but still fall short of stressing large computer systems across the full array of architectural features.
Reference: [12] <author> P. Messina et al., </author> <title> Benchmarking advanced architecture computers, </title> <type> Tech. Rep. </type> <institution> C3P712, Caltech Concurrent Supercomputing Facility, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: In 1988, an 3 industrial consortium named SPEC (Systems Performance Evaluation Co--operative) was formed to apply similar applications-based benchmarking to high performance workstations [16]. An applications-based benchmarking effort was also started at the California Institute of Technology to evaluate and compare different hypercube multiprocessor systems <ref> [12] </ref>. Benchmarking that involves complete scientific and engineering codes together with standardized data sets is referred to as applications-based bench-marking. As argued above, such benchmarking may be the most meaningful approach today.
Reference: [13] <author> L. </author> <title> Pointer, </title> <type> Perfect Report 2, Tech. Rep. 964, </type> <institution> Center for Supercom--puting Research and Development, University of Illinois at Urbana, </institution> <type> Technical Report, </type> <year> 1990. </year>
Reference-contexts: These portability constraints have been largely reduced over the past three years, but new ones are still being discovered and corrected, albeit with diminishing frequency. These experiences have lead to a short set of coding guidelines that are stricter than the ANSI Fortran standards. (See <ref> [13] </ref> for the current Perfect Fortran standard.) On a different front, the baseline Perfect executions have exposed bugs in compilers, particularly in newer machines and with high levels of compiler optimization settings.
Reference: [14] <author> R. Saavedra-Barrera, A. Smith, and E. Miya, </author> <title> Machine characterization based on an abstract high-level language machine, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 38 (1989), </volume> <pages> pp. 1659-1679. </pages>
Reference-contexts: The inner product of the timing vector and the kernel quantification vector is then computed and serves as an estimate of the execution time of the target application program on the target machine. See <ref> [14] </ref> for an example of this approach. Caution, particularly from the point of view of statistical soundness, must be strictly exercised when apparent successes of this approach are encountered.
Reference: [15] <author> J. E. Smith, </author> <title> Characterizing computer performance with a single number, </title> <journal> JACM, </journal> <year> (1988). </year>
Reference-contexts: Thus using a large number of kernel measurements risks giving one the illusion of excellent predictability if the methodology is not tested on a very large sample of both programs and machines. These and other observations (see for example <ref> [7, 17, 15, 8] </ref>) have lead to a growing recognition that supercomputer performance cannot be reduced to a single number nor to a set of isolated kernels or high-level program segments. <p> At present, the performance evaluation of hypercubes and large SIMD machines has largely been restricted to kernel and algorithm benchmarking. David Walker (University of South Carolina) is monitoring progress being made in porting the codes to other classes of machines. 10 6.7 Reporting Performance Numerous authors <ref> [15, 7, 17] </ref> have bemoaned the use of a single number to report supercomputer performance, and we sympathize with their arguments and conclusions.
Reference: [16] <author> J. Uniejewski, </author> <title> Spec benchmark suite: Designed for today's advanced systems. </title> <note> SPEC Newsletter, Fall 1989. Volume 1, Issue 1. </note>
Reference-contexts: In 1988, an 3 industrial consortium named SPEC (Systems Performance Evaluation Co--operative) was formed to apply similar applications-based benchmarking to high performance workstations <ref> [16] </ref>. An applications-based benchmarking effort was also started at the California Institute of Technology to evaluate and compare different hypercube multiprocessor systems [12]. Benchmarking that involves complete scientific and engineering codes together with standardized data sets is referred to as applications-based bench-marking.
Reference: [17] <author> J. Worlton, </author> <title> Understanding supercomputer benchmarks, </title> <journal> Datamation, </journal> <year> (1984), </year> <pages> pp. 121-129. </pages>
Reference-contexts: Thus using a large number of kernel measurements risks giving one the illusion of excellent predictability if the methodology is not tested on a very large sample of both programs and machines. These and other observations (see for example <ref> [7, 17, 15, 8] </ref>) have lead to a growing recognition that supercomputer performance cannot be reduced to a single number nor to a set of isolated kernels or high-level program segments. <p> At present, the performance evaluation of hypercubes and large SIMD machines has largely been restricted to kernel and algorithm benchmarking. David Walker (University of South Carolina) is monitoring progress being made in porting the codes to other classes of machines. 10 6.7 Reporting Performance Numerous authors <ref> [15, 7, 17] </ref> have bemoaned the use of a single number to report supercomputer performance, and we sympathize with their arguments and conclusions.
References-found: 17

