URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/link/www/papers/ps/gmm.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/link/www/homepage.html
Root-URL: 
Title: GIBBS-MARKOV MODELS  
Author: John D. Lafferty 
Address: 5000 Forbes Avenue Pittsburgh, PA 15217  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: In this paper we present a framework for building probabilistic automata parameterized by context-dependent probabilities. Gibbs distributions are used to model state transitions and output generation, and parameter estimation is carried out using an EM algorithm where the M-step uses a generalized iterative scaling procedure. We discuss relations with certain classes of stochastic feedforward neural networks, a geometric interpretation for parameter estimation, and a simple example of a statistical language model constructed using this methodology. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amari, </author> <title> Information geometry of the EM and em algorithms for neural networks, </title> <type> preprint, </type> <institution> Department of Mathematical Engineering and Information Physics, University of Tokyo, </institution> <year> 1995. </year>
Reference-contexts: The EM algorithm can in general be viewed in geometric terms <ref> [1, 8] </ref>. When estimating within a (closed) family Q of probabilities, given an incomplete-data density ~p, the E-step is given by the projection E (q) = arg min D (p k q) where P is the convex subset of all probabilities whose incomplete-data marginal agrees with ~p.
Reference: [2] <author> L. Bahl, F. Jelinek, and R. Mercer. </author> <title> A maximum likelihood approach to continuous speech recognition, </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(2):179-190, </volume> <month> March </month> <year> 1983. </year>
Reference-contexts: Stochastic Networks and Grammars One of the most common approaches to constructing statistical models of language for applications such as speech and language processing is to design and train a hidden Markov model (HMM) or stochastic context-free grammar <ref> [2, 3, 10] </ref>. This methodology is also currently being applied in various problems in computational biology [14, 20]. HMM's provide a simple framework for assigning probabilities to the output of an arbitrary finite- state machine.
Reference: [3] <author> J. Baker, </author> <title> Trainable grammars for speech recogni-tion, </title> <booktitle> in Proceedings of the Spring Conference of the Acoustical Society of America, </booktitle> <pages> 547-550, </pages> <address> Boston, MA, </address> <year> 1979. </year>
Reference-contexts: Stochastic Networks and Grammars One of the most common approaches to constructing statistical models of language for applications such as speech and language processing is to design and train a hidden Markov model (HMM) or stochastic context-free grammar <ref> [2, 3, 10] </ref>. This methodology is also currently being applied in various problems in computational biology [14, 20]. HMM's provide a simple framework for assigning probabilities to the output of an arbitrary finite- state machine.
Reference: [4] <author> V. Balasubramanian, </author> <title> Equivalence and reduction of hidden Markov models, </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <type> issued as AI Technical Report 1370, </type> <year> 1993. </year>
Reference-contexts: As an indication of the simplicity of HMM's, it is known that the problem of determining when two HMM's are probabilistically equivalent can be decided in polynomial time <ref> [4] </ref>. A thorough discussion of the mechanics of training and decoding HMM's, as well as a discussion of HMM/neural network hybrids related to Gibbs-Markov models is given in [18]. HMM's and stochastic grammars are fundamentally limited, however, in their modeling capabilities.
Reference: [5] <author> A. Berger, S. Della Pietra and V. Della Pietra, </author> <title> A maximum entropy approach to natural language pro-cessing, </title> <note> Computational Linguistics, to appear. </note>
Reference-contexts: The models that we describe are related to certain classes of stochastic feed- foward neural networks. They are also closely related to the recent use of exponential models and maximum entropy methods in a variety of natural language processing problems <ref> [5, 11, 16] </ref>. This paper presents one approach to extending these techniques to incomplete data problems for such applications.
Reference: [6] <author> E. Black, F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, and S. Roukos, </author> <title> Towards history-based gram-mars: Using richer models for probabilistic parsing, </title> <booktitle> Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <publisher> Arden House, Morgan Kaufman Publishers, </publisher> <year> 1992. </year>
Reference-contexts: As a consequence, stochastic grammars cannot model lexical and long-distance dependencies. While the independence assumption is convenient for encoding the grammar, it is often a gross approximation when extended to the probabilistic model. As one example of an attempt to relax the Markov assumption, Black et al. <ref> [6] </ref> used CART to estimate context-dependent rewrite probabilities. In this paper we present another approach to parameterizing and estimating context-dependent probabilities. Certain classes of stochastic neural networks offer advantages over hidden Markov models because of their inherent ability to model contextual dependencies.
Reference: [7] <author> I. Csiszar, </author> <title> A geometric interpretation of Darroch and Ratcliff 's generalized iterative scaling, </title> <journal> The Annals of Statistics, </journal> <volume> 17, No. 3, </volume> <pages> 1409-1413, </pages> <year> 1989. </year>
Reference-contexts: Because the auxiliary function can be maximized directly, there is little distinction between the E-steps and M-steps. 5. A Geometric Interpretation In this section we give a geometric interpretation of the parameter estimation algorithm just outlined. In particular, we adopt Csiszar's formulation <ref> [7] </ref> of iterative scaling, using the techniques of alternating projection and divergence geometry, to the situation where the features are not normalized. Let = (O fi S) n be the complete data event space, and let I = fffg be the set of indices for the features ff ff g.
Reference: [8] <author> I. Csiszar and G. Tusnady, </author> <title> Information geometry and alternating minimization procedures, </title> <journal> Statistics & Decisions, Supplement Issue, </journal> <volume> 1, </volume> <pages> 205-237, </pages> <year> 1984. </year>
Reference-contexts: The EM algorithm can in general be viewed in geometric terms <ref> [1, 8] </ref>. When estimating within a (closed) family Q of probabilities, given an incomplete-data density ~p, the E-step is given by the projection E (q) = arg min D (p k q) where P is the convex subset of all probabilities whose incomplete-data marginal agrees with ~p.
Reference: [9] <author> J. Darroch and D. Ratcliff, </author> <title> Generalized iterative scal-ing for log-linear models, </title> <journal> Ann. Math. Statist. </journal> <volume> 43, </volume> <pages> 1470-1480, </pages> <year> 1972. </year>
Reference-contexts: The resulting algorithm for iteratively updating the parameters ff is a generalization of the Darroch-Ratcliff iterative scaling procedure <ref> [9] </ref>; it is discussed in detail in [11]. It is an improvement over the Darroch-Ratcliff algorithm for problems where a large number of parameters need to be estimated because it does not require that the features f ff sum to a constant.
Reference: [10] <author> S. Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, H. Printz, L. Ures, </author> <title> Inference and estimation of a long-range trigram model, </title> <booktitle> Proceedings of the Second International Colloquium on Grammatical Inference and Applications, Lecture Notes in Artificial Intelligence, </booktitle> <volume> 862, </volume> <publisher> Springer-Verlag, </publisher> <pages> 78-92, </pages> <year> 1994. </year>
Reference-contexts: Stochastic Networks and Grammars One of the most common approaches to constructing statistical models of language for applications such as speech and language processing is to design and train a hidden Markov model (HMM) or stochastic context-free grammar <ref> [2, 3, 10] </ref>. This methodology is also currently being applied in various problems in computational biology [14, 20]. HMM's provide a simple framework for assigning probabilities to the output of an arbitrary finite- state machine.
Reference: [11] <author> S. Della Pietra, V. Della Pietra and J. Lafferty, </author> <title> Inducing features of random fields, </title> <institution> Carnegie Mellon technical report CMU-CS-95-144, </institution> <year> 1995. </year>
Reference-contexts: The models that we describe are related to certain classes of stochastic feed- foward neural networks. They are also closely related to the recent use of exponential models and maximum entropy methods in a variety of natural language processing problems <ref> [5, 11, 16] </ref>. This paper presents one approach to extending these techniques to incomplete data problems for such applications. <p> For language processing problems, Gibbs sampling is typically not a feasible alternative. The values that the hidden and visible states can assume is given by vocabularies that are typically on the order of 10,000-100,000 entries in size. Even for character-based models, Gibbs sampling can be computationally intensive <ref> [11] </ref>. Fur-- thermore, the mean field approximation is not appropriate for such models because the number of states is so large. Such approximations may be more suitable to modeling nucleotide sequences in computational biology. 3. <p> The resulting algorithm for iteratively updating the parameters ff is a generalization of the Darroch-Ratcliff iterative scaling procedure [9]; it is discussed in detail in <ref> [11] </ref>. It is an improvement over the Darroch-Ratcliff algorithm for problems where a large number of parameters need to be estimated because it does not require that the features f ff sum to a constant.
Reference: [12] <author> J. Godfrey, E. Holliman, and J. McDaniel, </author> <title> Switchboard: Telephone speech corpus for research devel-opment, </title> <booktitle> Proc. </booktitle> <address> ICASSP-92, I-517-520, </address> <year> 1992. </year>
Reference-contexts: We have built a language model of this form on the Switchboard corpus <ref> [12] </ref> for use in a speech recognition system. This corpus includes approximately three million words of text, transcribed from more than 150 hours of speech collected from telephone conversations restricted to 70 different topics.
Reference: [13] <author> F. Jelinek, J. Lafferty, and R. Mercer, </author> <title> Basic meth-ods of probabilistic context-free grammars, Speech Recognition and Understanding: Recent Advances, Trends, and Applications, </title> <editor> P. Laface and R. De Mori, editors, Springer Verlag, </editor> <title> Series F: </title> <journal> Computer and Systems Sciences, </journal> <volume> Volume 75, </volume> <year> 1992. </year>
Reference: [14] <author> A. Krogh, M. Brown, I. S. Mian, K. Sjolander, and D. Haussler, </author> <title> Hidden Markov models in compu-tational biology: Applications to protein modeling, </title> <journal> Journal of Molecular Biology, </journal> <volume> 235 </volume> <pages> 1501-1531, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: This methodology is also currently being applied in various problems in computational biology <ref> [14, 20] </ref>. HMM's provide a simple framework for assigning probabilities to the output of an arbitrary finite- state machine.
Reference: [15] <author> J. Lafferty and B. Suhm, </author> <title> Cluster expansions and it-erative scaling of maximum entropy language models, </title> <booktitle> in proceedings of the Fifteenth International Workshop on Maximum Entropy and Bayesian Methods, </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995, </year> <note> to appear. </note>
Reference-contexts: The the equations for this model can be efficiently computed and solved using a technique that we call the cluster expansion, borrowing the name of a closely related method for calculating partition functions in statistical mechanics. This is described in detail in <ref> [15] </ref>. While the technique does nothing to address the inherent computational difficulty of carrying out the E-step for a highly connected network, it can be a very useful tool for efficient estimation of a large class of models that arise in applications.
Reference: [16] <author> R. Lau, R. Rosenfeld, and S. Roukos, </author> <title> Adaptive lan-guage modeling using the maximum entropy prin-ciple, </title> <booktitle> Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <publisher> Morgan Kaufman Publishers, </publisher> <pages> 108-113, </pages> <year> 1993. </year>
Reference-contexts: The models that we describe are related to certain classes of stochastic feed- foward neural networks. They are also closely related to the recent use of exponential models and maximum entropy methods in a variety of natural language processing problems <ref> [5, 11, 16] </ref>. This paper presents one approach to extending these techniques to incomplete data problems for such applications. <p> = hO t = yihS t1 = sihv appears in last 100 wordsi: If the state S t encodes information such as the "part-of-- speech" of the word being generated, then such a feature allows a more plausible notion of "trigger words" compared to the use of long-distance features in <ref> [16] </ref>, by allowing words to be triggered only in the appropriate grammatical contexts. An advantage of such a model is that the embedded HMM can be trained first, and then used as the starting point for the full EM training.
Reference: [17] <author> S. Lauritzen and D. Spiegelhalter, </author> <title> Local computa-tions with probabilities on graphical structures and their application to expert systems, </title> <journal> J. Roy. Stat. Soc. </journal> <volume> B 50: </volume> <pages> 157-224, </pages> <year> 1988. </year>
Reference-contexts: In this paper we present another approach to parameterizing and estimating context-dependent probabilities. Certain classes of stochastic neural networks offer advantages over hidden Markov models because of their inherent ability to model contextual dependencies. For example, sigmoid belief networks, a special case of more general graphical models <ref> [17] </ref>, are feed-forward networks with pairwise interactions and binary values taken by the hidden and visible units.
Reference: [18] <author> A. Nadas and R. Mercer, </author> <title> Hidden Markov models and some connections with artificial neural networks, in Mathematical Perspectives on Neural Networks, </title> <editor> P. Smolensky, M. Mozer, and D. Rumelhart, ed., </editor> <publisher> Erlbaum, </publisher> <year> 1994. </year>
Reference-contexts: A thorough discussion of the mechanics of training and decoding HMM's, as well as a discussion of HMM/neural network hybrids related to Gibbs-Markov models is given in <ref> [18] </ref>. HMM's and stochastic grammars are fundamentally limited, however, in their modeling capabilities. The Markov assumption, which enables the E-step to be carried out efficiently, forces all predictive information to be encoded into the states.
Reference: [19] <author> R. Neal, </author> <title> Connectionist learning of belief networks, </title> <booktitle> Artificial Intelligence 56, </booktitle> <pages> 71-113, </pages> <year> 1992. </year>
Reference-contexts: Training the weights ij can be carried out using the EM algorithm. However, when the network is highly connected, the E-step is intractable since it involves summing over an exponentially large number of hidden configurations. To surmount this difficulty, both Gibbs sampling <ref> [19] </ref> and mean field approximation techniques [21] have been proposed for approximating the expectations that must be computed. For language processing problems, Gibbs sampling is typically not a feasible alternative.
Reference: [20] <author> Y. Sakakibara, M. Brown, R. Hughey, I. S. Mian, K. Sjolander, R. Underwood and D. Haussler, </author> <title> Stochastic context-free grammars for tRNA modeling, </title> <journal> Nucleaic Acids Research, </journal> <volume> 22(23) </volume> <pages> 5112-5120, </pages> <year> 1994. </year>
Reference-contexts: This methodology is also currently being applied in various problems in computational biology <ref> [14, 20] </ref>. HMM's provide a simple framework for assigning probabilities to the output of an arbitrary finite- state machine.
Reference: [21] <author> L. Saul, T. Jaakkola, and M. Jordan, </author> <title> Mean field theory for sigmoid belief networks, </title> <booktitle> Computational Cognitive Science Technical Report 9501, </booktitle> <publisher> MIT, </publisher> <year> 1995. </year>
Reference-contexts: Training the weights ij can be carried out using the EM algorithm. However, when the network is highly connected, the E-step is intractable since it involves summing over an exponentially large number of hidden configurations. To surmount this difficulty, both Gibbs sampling [19] and mean field approximation techniques <ref> [21] </ref> have been proposed for approximating the expectations that must be computed. For language processing problems, Gibbs sampling is typically not a feasible alternative. The values that the hidden and visible states can assume is given by vocabularies that are typically on the order of 10,000-100,000 entries in size.
References-found: 21

