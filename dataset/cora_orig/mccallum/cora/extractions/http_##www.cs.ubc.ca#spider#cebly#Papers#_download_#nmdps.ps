URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/nmdps.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: fbacchus@logos.uwaterloo.ca  cebly@cs.ubc.ca  grove@research.nj.nec.com  
Title: Structured Solution Methods for Non-Markovian Decision Processes  
Author: Fahiem Bacchus Craig Boutilier Adam Grove 
Address: Waterloo, Ontario Canada, N2L 3G1  Vancouver, B.C. Canada, V6T 1Z4  4 Independence Way Princeton NJ 08540, USA  
Affiliation: Dept. of Computer Science University of Waterloo  Dept. of Computer Science University of British Columbia  NEC Research Institute  
Date: August, 1997  
Note: To appear, Proc. 14th National Conf. on AI (AAAI-97),Providence,  
Abstract: Markov Decision Processes (MDPs), currently a popular method for modeling and solving decision theoretic planning problems, are limited by the Markovian assumption: rewards and dynamics depend on the current state only, and not on previous history. Non-Markovian decision processes (NMDPs) can also be defined, but then the more tractable solution techniques developed for MDP's cannot be directly applied. In this paper, we show how an NMDP, in which temporal logic is used to specify history dependence, can be automatically converted into an equivalent MDP by adding appropriate temporal variables. The resulting MDP can be represented in a structured fashion and solved using structured policy construction methods. In many cases, this offers significant computational advantages over previous proposals for solving NMDPs. 
Abstract-found: 1
Intro-found: 1
Reference: [BBG96] <author> Fahiem Bacchus, Craig Boutilier, and Adam Grove. </author> <title> Rewarding behaviors. </title> <booktitle> In Proceedings of the Thirteenth National Conferenceon Artificial Intelligence, </booktitle> <pages> pages 1160-1167, </pages> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: This has shown up in work on planning [HH92, Dru89, Kab90, GK91] (e.g., in the use of maintenance goals); and in <ref> [BBG96] </ref> we have argued that many reward functions for process-oriented prob fl The work of Fahiem Bacchus and Craig Boutilier was supported by the Canadian government through their NSERC and IRIS programs. 1 Copyright c fl1997, American Association for Artificial Intelligence (www.aaai.org). <p> For instance, rewarding an agent for achieving a goal within k steps of a request being issued is a natural, yet history-dependent, specification of desirable behavior. Similarly, process dynamics (action effects) are sometimes most naturally expressed in a history dependent fashion. In <ref> [BBG96] </ref> we examined Non-Markovian decision processes (NMDPs) and identified two key issues, namely, the specification of non-Markovian properties and the solution of NMDPs. 2 A temporal logic called PLTL was used as a mechanism for specifying the non-Markovian aspects of a system, and we will adopt the same approach here. <p> As in the earlier paper, we develop a method for automatically converting an NMDP into an equivalent MDP, solutions to which can be re-interpreted to yield solutions to the original NMDP. The key difference between the two papers is that <ref> [BBG96] </ref> presents a state-based construction, while this paper works with structured representations of (N)MDPs. Each approach has advantages and disadvantages. <p> In either case, one can think of each state in the original NMDP as leading to multiple states in the resulting MDP, the new states being distinguished by various relevant histories. The difference, in essence, is how the new MDP is represented. The algorithm in <ref> [BBG96] </ref> takes an NMDP and a reward function specified using PLTL formulas, and produces a new MDP whose states are listed explicitly. <p> The resultant MDPs have as states all possible assignments of values to these variables. In such cases, a major difficulty with any state-based algorithm for MDPs is that 2 In <ref> [BBG96] </ref> we considered non-Markovian reward functions only, although the approach could easily be extended to deal with history dependence in the system dynamics as well. In this paper we consider both rewards and dynamics explicitly. <p> For such problems, the NMDP conversion algorithm proposed in <ref> [BBG96] </ref> has some obvious drawbacks. First, being state-based, its complexity is exponential in the number of problem variables, because even if the original NMDP has a compact representation in terms of variables the final MDP will not. <p> Finally, as we will see, the history that must be encoded itself exhibits significant structure, but this is not exploited in <ref> [BBG96] </ref>. Our approach to NMDP conversion in this paper assumes the existence of a compact variable-based description of the given NMDP, and works by adding temporal variables to this description. We thus expand the state space without regard to minimality, but this expansion is implicitwe never enumerate all states. <p> As such, unlike <ref> [BBG96] </ref>, we need not be directly concerned about only adding the relevant history. To a significant extent, relevance can be detected during optimization by appropriate SPC algorithms. This points to a related advantage of our technique, which is that relevant history is dynamically determined in SPC algorithms. The algorithm of [BBG96] <p> <ref> [BBG96] </ref>, we need not be directly concerned about only adding the relevant history. To a significant extent, relevance can be detected during optimization by appropriate SPC algorithms. This points to a related advantage of our technique, which is that relevant history is dynamically determined in SPC algorithms. The algorithm of [BBG96] is based on a static analysis of the NMDP, before optimal policy construction is attempted. That algorithm must encode enough history at a given state s to determine the reward at any future reachable state t. <p> The irrelevance of the required history cannot be detected a priori. A final contrast with <ref> [BBG96] </ref> is the relative simplicity of our proposal in the current paper, which is based on well-known properties of temporal logic and the observation that these can be made to integrate well with SPC. <p> Although the technical aspects of our contribution are straightforward, it has considerable potential for improving our ability to solve history-dependent decision problems. We begin with a brief description of MDPs, NMDPs and the temporal logic used in <ref> [BBG96] </ref> to specify trajectory properties. We also give an overview of the SPC algorithm of [BDG95]. Then we present our technique of adding temporal variables to convert an NMDP to an MDP. <p> We note that SPC offers another advantage, namely its amenability to approximation; see [BD96]. 2.4 Non Markovian Decision Processes Following <ref> [BBG96] </ref>, we use a temporal logic called PLTL (Past Linear Temporal Logic) to specify the history dependence of rewards and action effects. PLTL is a past version of LTL [Eme90]. <p> Simi larly, rewarding states satisfying G ^ fl k C would reward behaviors that achieve G within k steps its request. <ref> [BBG96] </ref> gives further examples of how the logic can be used to specify useful historical dependencies. Using a logic for making such specifications provides all of the usual advantages gained from logical representations. <p> Consider any temporal formula appearing as a decision node in one of the 6 Intuitively, equivalence implies that an optimal policy for the constructed MDP can be re-interpreted as an optimal policy for the original NMDP. See <ref> [BBG96] </ref> for a formal definition. decision trees of N . It follows that any state in M must contain enough information to decide if is true. <p> In fact, it is easy to see that minimal state space size is sometimes not achievable by adding variables at all. The reason is that the required history can vary from state to state (indeed, this fact motivates much of <ref> [BBG96] </ref>). Our approach cannot make such fine distinctions: every state has the same set of variables added to it. On the other hand, the dynamic irrelevance feature of SPC may compensate for this. <p> On the other hand, the dynamic irrelevance feature of SPC may compensate for this. That is, although the temporal variables can potentially cause us to make unnecessary distinctions between histories at certain states (as compared to <ref> [BBG96] </ref>), SPC attempts to focus only on the variables whose values influence the choices made by the optimal policy, and so will avoid some of these irrelevant distinctions. Furthermore, SPC can avoid dynamic irrelevances that cannot be detected by the approach of [BBG96], and is much more amenable to approximation in <p> between histories at certain states (as compared to <ref> [BBG96] </ref>), SPC attempts to focus only on the variables whose values influence the choices made by the optimal policy, and so will avoid some of these irrelevant distinctions. Furthermore, SPC can avoid dynamic irrelevances that cannot be detected by the approach of [BBG96], and is much more amenable to approximation in the solution of the resulting MDP. Empirical studies are, of course, needed to quantify this tradeoff. <p> Empirical studies are, of course, needed to quantify this tradeoff. We suspect that there will be a range of domains where the SPC approach we are suggesting here will be superior to the state-space based approach of <ref> [BBG96] </ref> and vice versa. The interesting question will be to attempt to characterize domain features that tend to favor one approach over the other. Not surprisingly, NMDPs (even in the structured framework we are considering) can be much harder to solve than a comparably sized MDP.
Reference: [BD94] <author> Craig Boutilier and Richard Dearden. </author> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1016-1022, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: A recent focus in DTP research has been the development of MDP representation and solution techniques that do not require an explicit enumeration of the state space. For instance, the use of STRIPS <ref> [BD94, DF95, KHW94] </ref> or Bayes nets [BDG95, BD96] to represent actions in MDPs, and structured policy construction (SPC) methods that exploit such representations [DF95, BDG95, BD96] to avoid explicit state-based computations when solving MDPs, promise to make MDPs more effective for such DTP problems. <p> V is called the value To appear, Proc. 14th National Conf. on AI (AAAI-97),Providence, August, 1997 function. We refer to [Put94] for an excellent treatment of MDPs and associated computational methods. See <ref> [DW91, BD94, BDG95] </ref> on the use of MDPs for DTP. 2.2 Structured Representations For DTP problems, we are often interested in MDPs that can be represented concisely in terms of a set of features or variables.
Reference: [BD96] <author> Craig Boutilier and Richard Dearden. </author> <title> Approximating value trees in structured dynamic programming. </title> <booktitle> In Proceedings of the Thirteenth International Conferenceon Machine Learning, </booktitle> <pages> pages 54-62, </pages> <address> Bari, Italy, </address> <year> 1996. </year>
Reference-contexts: A recent focus in DTP research has been the development of MDP representation and solution techniques that do not require an explicit enumeration of the state space. For instance, the use of STRIPS [BD94, DF95, KHW94] or Bayes nets <ref> [BDG95, BD96] </ref> to represent actions in MDPs, and structured policy construction (SPC) methods that exploit such representations [DF95, BDG95, BD96] to avoid explicit state-based computations when solving MDPs, promise to make MDPs more effective for such DTP problems. <p> For instance, the use of STRIPS [BD94, DF95, KHW94] or Bayes nets [BDG95, BD96] to represent actions in MDPs, and structured policy construction (SPC) methods that exploit such representations <ref> [DF95, BDG95, BD96] </ref> to avoid explicit state-based computations when solving MDPs, promise to make MDPs more effective for such DTP problems. For such problems, the NMDP conversion algorithm proposed in [BBG96] has some obvious drawbacks. <p> Such modifications are entirely compatible with our proposals here. 3 In particular, we have Pr (s; a; s ) = p : s 0 j=p Tree (a; p)[s] Y (1 Tree (a; p)[s]): 2.3 Structured Policy Construction The basic idea behind the SPC algorithms described in <ref> [BDG95, BD96] </ref> is the use of tree-structured representations during policy construction. In particular, a policy can be represented as a decision tree Tree where the leaves are labeled by actions. That is, Tree [s] specifies the action to execute in state s. <p> We note that SPC offers another advantage, namely its amenability to approximation; see <ref> [BD96] </ref>. 2.4 Non Markovian Decision Processes Following [BBG96], we use a temporal logic called PLTL (Past Linear Temporal Logic) to specify the history dependence of rewards and action effects. PLTL is a past version of LTL [Eme90].
Reference: [BDG95] <author> Craig Boutilier, Richard Dearden, and Moises Gold-szmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1104-1111, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: A recent focus in DTP research has been the development of MDP representation and solution techniques that do not require an explicit enumeration of the state space. For instance, the use of STRIPS [BD94, DF95, KHW94] or Bayes nets <ref> [BDG95, BD96] </ref> to represent actions in MDPs, and structured policy construction (SPC) methods that exploit such representations [DF95, BDG95, BD96] to avoid explicit state-based computations when solving MDPs, promise to make MDPs more effective for such DTP problems. <p> For instance, the use of STRIPS [BD94, DF95, KHW94] or Bayes nets [BDG95, BD96] to represent actions in MDPs, and structured policy construction (SPC) methods that exploit such representations <ref> [DF95, BDG95, BD96] </ref> to avoid explicit state-based computations when solving MDPs, promise to make MDPs more effective for such DTP problems. For such problems, the NMDP conversion algorithm proposed in [BBG96] has some obvious drawbacks. <p> We begin with a brief description of MDPs, NMDPs and the temporal logic used in [BBG96] to specify trajectory properties. We also give an overview of the SPC algorithm of <ref> [BDG95] </ref>. Then we present our technique of adding temporal variables to convert an NMDP to an MDP. <p> V is called the value To appear, Proc. 14th National Conf. on AI (AAAI-97),Providence, August, 1997 function. We refer to [Put94] for an excellent treatment of MDPs and associated computational methods. See <ref> [DW91, BD94, BDG95] </ref> on the use of MDPs for DTP. 2.2 Structured Representations For DTP problems, we are often interested in MDPs that can be represented concisely in terms of a set of features or variables. <p> See [DW91, BD94, BDG95] on the use of MDPs for DTP. 2.2 Structured Representations For DTP problems, we are often interested in MDPs that can be represented concisely in terms of a set of features or variables. We adapt the Bayes net/decision tree representation used by <ref> [DK89, BDG95] </ref>, representing an MDP with a set of variables P, a reward decision tree Tree R , the set of actions A, and a set of action decision trees fTree (a; p) j a 2 A; p 2 Pg. <p> These events are assumed to be independent so we can obtain the entire distribution over the successor states, Pr (s; a; ), by simply multiplying these probabilities. 3 Although <ref> [BDG95] </ref> describes their representation in terms of Bayes nets, it is nevertheless equivalent to ours, including the independence assumption. One advantage of the Bayes net representation is that it suggests an easy way of relaxing this assumption. (This possibility was not explored in [BDG95], but see [Bou97] for details.) Thus, SPC <p> by simply multiplying these probabilities. 3 Although <ref> [BDG95] </ref> describes their representation in terms of Bayes nets, it is nevertheless equivalent to ours, including the independence assumption. One advantage of the Bayes net representation is that it suggests an easy way of relaxing this assumption. (This possibility was not explored in [BDG95], but see [Bou97] for details.) Thus, SPC algorithms can be modified to deal with dependence between present variables, although they become somewhat more complex. <p> Such modifications are entirely compatible with our proposals here. 3 In particular, we have Pr (s; a; s ) = p : s 0 j=p Tree (a; p)[s] Y (1 Tree (a; p)[s]): 2.3 Structured Policy Construction The basic idea behind the SPC algorithms described in <ref> [BDG95, BD96] </ref> is the use of tree-structured representations during policy construction. In particular, a policy can be represented as a decision tree Tree where the leaves are labeled by actions. That is, Tree [s] specifies the action to execute in state s. <p> Intuitively, these algorithms dynamically detect the relevance of particular variables, under specific conditions, to the current value function or policy. The particular tree-manipulation steps necessary are not trivial, but the details are not directly relevant here (they are presented in <ref> [BDG95] </ref>). We have already alluded to some of SPC's properties. If the dynamics and reward function of the MDP are simple, the optimal policy very often has a simple structure as well (see [BDG95] for examples). <p> steps necessary are not trivial, but the details are not directly relevant here (they are presented in <ref> [BDG95] </ref>). We have already alluded to some of SPC's properties. If the dynamics and reward function of the MDP are simple, the optimal policy very often has a simple structure as well (see [BDG95] for examples). SPC will find this policy in just as many iterations as modified policy iteration (a popular and time-efficient algorithm), but often without ever considering large treeseven though it is (implicitly) optimizing over exponentially many states.
Reference: [Bou97] <author> Craig Boutilier. </author> <title> Correlated action effects in decision-theoretic regression. </title> <type> (manuscript), </type> <year> 1997. </year>
Reference-contexts: One advantage of the Bayes net representation is that it suggests an easy way of relaxing this assumption. (This possibility was not explored in [BDG95], but see <ref> [Bou97] </ref> for details.) Thus, SPC algorithms can be modified to deal with dependence between present variables, although they become somewhat more complex.
Reference: [DF95] <author> Thomas G. Dietterich and Nicholas S. Flann. </author> <title> Explanation-based learning and reinforcement learning: A unified approach. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 176-184, </pages> <address> Lake Tahoe, </address> <year> 1995. </year>
Reference-contexts: A recent focus in DTP research has been the development of MDP representation and solution techniques that do not require an explicit enumeration of the state space. For instance, the use of STRIPS <ref> [BD94, DF95, KHW94] </ref> or Bayes nets [BDG95, BD96] to represent actions in MDPs, and structured policy construction (SPC) methods that exploit such representations [DF95, BDG95, BD96] to avoid explicit state-based computations when solving MDPs, promise to make MDPs more effective for such DTP problems. <p> For instance, the use of STRIPS [BD94, DF95, KHW94] or Bayes nets [BDG95, BD96] to represent actions in MDPs, and structured policy construction (SPC) methods that exploit such representations <ref> [DF95, BDG95, BD96] </ref> to avoid explicit state-based computations when solving MDPs, promise to make MDPs more effective for such DTP problems. For such problems, the NMDP conversion algorithm proposed in [BBG96] has some obvious drawbacks.
Reference: [DK89] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: See [DW91, BD94, BDG95] on the use of MDPs for DTP. 2.2 Structured Representations For DTP problems, we are often interested in MDPs that can be represented concisely in terms of a set of features or variables. We adapt the Bayes net/decision tree representation used by <ref> [DK89, BDG95] </ref>, representing an MDP with a set of variables P, a reward decision tree Tree R , the set of actions A, and a set of action decision trees fTree (a; p) j a 2 A; p 2 Pg.
Reference: [Dru89] <author> M. Drummond. </author> <title> Situated control rules. </title> <booktitle> In Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 103-113, </pages> <address> Toronto, </address> <year> 1989. </year>
Reference-contexts: For instance, it is often natural to specify desirable behaviors by referring to trajectory properties (properties of the sequence of states passed through, i.e., the system's history) in addition to just the current state. This has shown up in work on planning <ref> [HH92, Dru89, Kab90, GK91] </ref> (e.g., in the use of maintenance goals); and in [BBG96] we have argued that many reward functions for process-oriented prob fl The work of Fahiem Bacchus and Craig Boutilier was supported by the Canadian government through their NSERC and IRIS programs. 1 Copyright c fl1997, American Association
Reference: [DW91] <author> Thomas Dean and Michael Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1991. </year>
Reference-contexts: V is called the value To appear, Proc. 14th National Conf. on AI (AAAI-97),Providence, August, 1997 function. We refer to [Put94] for an excellent treatment of MDPs and associated computational methods. See <ref> [DW91, BD94, BDG95] </ref> on the use of MDPs for DTP. 2.2 Structured Representations For DTP problems, we are often interested in MDPs that can be represented concisely in terms of a set of features or variables.
Reference: [Eme90] <author> E. A. Emerson. </author> <title> Temporal and modal logic. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume B, chapter 16, </booktitle> <pages> pages 997-1072. </pages> <publisher> MIT, </publisher> <year> 1990. </year>
Reference-contexts: We note that SPC offers another advantage, namely its amenability to approximation; see [BD96]. 2.4 Non Markovian Decision Processes Following [BBG96], we use a temporal logic called PLTL (Past Linear Temporal Logic) to specify the history dependence of rewards and action effects. PLTL is a past version of LTL <ref> [Eme90] </ref>.
Reference: [GK91] <author> P. Godefroid and F. Kabanza. </author> <title> An efficient reactive planner for synthesizing reactive plans. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 640-645, </pages> <year> 1991. </year>
Reference-contexts: For instance, it is often natural to specify desirable behaviors by referring to trajectory properties (properties of the sequence of states passed through, i.e., the system's history) in addition to just the current state. This has shown up in work on planning <ref> [HH92, Dru89, Kab90, GK91] </ref> (e.g., in the use of maintenance goals); and in [BBG96] we have argued that many reward functions for process-oriented prob fl The work of Fahiem Bacchus and Craig Boutilier was supported by the Canadian government through their NSERC and IRIS programs. 1 Copyright c fl1997, American Association
Reference: [HH92] <author> Peter Haddawy and Steve Hanks. </author> <title> Representations for decision-theoretic planning: Utility functions for deadline goals. </title> <booktitle> In Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 71-82, </pages> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: For instance, it is often natural to specify desirable behaviors by referring to trajectory properties (properties of the sequence of states passed through, i.e., the system's history) in addition to just the current state. This has shown up in work on planning <ref> [HH92, Dru89, Kab90, GK91] </ref> (e.g., in the use of maintenance goals); and in [BBG96] we have argued that many reward functions for process-oriented prob fl The work of Fahiem Bacchus and Craig Boutilier was supported by the Canadian government through their NSERC and IRIS programs. 1 Copyright c fl1997, American Association
Reference: [How60] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1960. </year>
Reference-contexts: We also give an overview of the SPC algorithm of [BDG95]. Then we present our technique of adding temporal variables to convert an NMDP to an MDP. We close the paper with some further observations. 2 Background 2.1 Markov Decision Processes A fully observable Markov Decision Process <ref> [How60, Put94] </ref> can be characterized by a finite set of states S, a set of actions A, and a reward function R. <p> The expected value of a fixed policy at any state s can be shown to satisfy <ref> [How60] </ref>: V (s) = R (s) + fi t2S The value of at any state s can be computed by solving this system of linear equations. A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 .
Reference: [Kab90] <author> F. Kabanza. </author> <title> Synthesis of reactive plans for multi-path environments. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 164-169, </pages> <year> 1990. </year>
Reference-contexts: For instance, it is often natural to specify desirable behaviors by referring to trajectory properties (properties of the sequence of states passed through, i.e., the system's history) in addition to just the current state. This has shown up in work on planning <ref> [HH92, Dru89, Kab90, GK91] </ref> (e.g., in the use of maintenance goals); and in [BBG96] we have argued that many reward functions for process-oriented prob fl The work of Fahiem Bacchus and Craig Boutilier was supported by the Canadian government through their NSERC and IRIS programs. 1 Copyright c fl1997, American Association
Reference: [KHW94] <author> Nicholas Kushmerick, Steve Hanks, and Daniel Weld. </author> <title> An algorithm for probabilistic least-commitment planning. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1073-1078, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: A recent focus in DTP research has been the development of MDP representation and solution techniques that do not require an explicit enumeration of the state space. For instance, the use of STRIPS <ref> [BD94, DF95, KHW94] </ref> or Bayes nets [BDG95, BD96] to represent actions in MDPs, and structured policy construction (SPC) methods that exploit such representations [DF95, BDG95, BD96] to avoid explicit state-based computations when solving MDPs, promise to make MDPs more effective for such DTP problems.
Reference: [Put94] <author> Martin L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This allows computation-ally effective dynamic programming techniques to be used to solve decision problems <ref> [Put94] </ref>. Nevertheless, the Markovian requirement is often not met by planning problems that are encoded in the obvious way. <p> We also give an overview of the SPC algorithm of [BDG95]. Then we present our technique of adding temporal variables to convert an NMDP to an MDP. We close the paper with some further observations. 2 Background 2.1 Markov Decision Processes A fully observable Markov Decision Process <ref> [How60, Put94] </ref> can be characterized by a finite set of states S, a set of actions A, and a reward function R. <p> A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . V is called the value To appear, Proc. 14th National Conf. on AI (AAAI-97),Providence, August, 1997 function. We refer to <ref> [Put94] </ref> for an excellent treatment of MDPs and associated computational methods. See [DW91, BD94, BDG95] on the use of MDPs for DTP. 2.2 Structured Representations For DTP problems, we are often interested in MDPs that can be represented concisely in terms of a set of features or variables.
References-found: 16

