URL: http://www.cs.unc.edu/~stuerzl/papers/vision.ps.gz
Refering-URL: http://www.cs.unc.edu/~stuerzl/publications.html
Root-URL: http://www.cs.unc.edu
Email: wilbur@cast.uni-linz.ac.at  
Phone: 2  3  
Title: Immersive Simulation for Computer Vision  
Author: Wilhelm Burger, Matthew J. Barth and Wolfgang Sturzlinger 
Address: A-4040 Linz, Austria  Riverside, CA 92521-0425, U.S.A.  A-4040 Linz, Austria  
Affiliation: 1 Johannes Kepler Univ., Dept. of Systems Science,  University of California, College of Engineering,  Johannes Kepler Univ., Dept. f. Graphics Parall. Process.,  
Abstract: Synthetic imagery has often been considered unsuitable for demonstrating the performance of vision algorithms and systems. We argue that (despite many remaining difficulties) simulation and computer graphics are at a point today that make them extremely useful for evaluation and training, even for complex outdoor applications. This is particularly valuable for autonomous and robotic applications, where the lack of suitable training data and ground truth information is a severe bottleneck. Extensive testing in a simulated environment should become an integral part of the systems development and evaluation process to reduce the possibility of failure in the real world. We describe ongoing efforts towards the development of an "Immersive Perception Simulator" and discuss some of the specific problems involved.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Y. Aloimonos. </author> <title> Purposive and qualitative vision. </title> <booktitle> In Proc. DARPA Image Understanding Workshop, </booktitle> <pages> pages 816-825, </pages> <year> 1990. </year>
Reference-contexts: of active vision, adaptation and learning, and performance evaluation deserve some additional thoughts: Active Vision: The idea of active vision is to control the sensor (s) in a goal-direction fashion, thus allowing, e.g., to focus on specific parts of the scene, track moving objects, or to compensate for platform motion <ref> [1] </ref>. Since the resulting systems are inherently closed-loop, testing is usually only possible when the whole setup is complete and operational. Simulation allows to perform meaningful experiments at a much earlier phase and without finalizing the mechanical design.
Reference: 2. <author> M. Asada. </author> <title> Map building for a mobile robot from sensory data. </title> <journal> IEEE Trans. Systems, Man, and Cybernetics, </journal> <volume> 37(6) </volume> <pages> 1326-1336, </pages> <year> 1990. </year>
Reference-contexts: Specific vision examples are the ALV testbed at the University of Maryland <ref> [2] </ref>, the map-based simulation environment developed at Hughes [8], and the IUA simulator at UMASS [15]. Yet, until now, there has been no simulation environment available that could support the complexity of complete perception and control systems, and provide the required realism at the same time.
Reference: 3. <author> B. Bhanu and T. Poggio. </author> <title> Special section on learning in computer vision. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(9) </volume> <pages> 865-919, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: There are many forms of learning applicable for vision, including statistical parameter estimation, clustering, function approximation, structural learning, self-organization, and neural network training. Existing applications include low-level processing, feature selection and grouping, model acquisition from examples, map learning, and 3-D object recognition <ref> [4, 3] </ref>. The use of immersive simulation can provide several important solutions with respect to the learning problem in vision: a. Large sets of realistic examples can be created and processed with reasonable effort. b.
Reference: 4. <author> K.W. Bowyer, L.O. Hall, P. Langley, B. Bhanu, and B. Draper. </author> <booktitle> Report of the AAAI fall symposium on machine learning and computer vision: What, why and how? In Proc. DARPA Image Understanding Workshop, </booktitle> <pages> pages 727-731, </pages> <year> 1994. </year>
Reference-contexts: There are many forms of learning applicable for vision, including statistical parameter estimation, clustering, function approximation, structural learning, self-organization, and neural network training. Existing applications include low-level processing, feature selection and grouping, model acquisition from examples, map learning, and 3-D object recognition <ref> [4, 3] </ref>. The use of immersive simulation can provide several important solutions with respect to the learning problem in vision: a. Large sets of realistic examples can be created and processed with reasonable effort. b.
Reference: 5. <author> R.L. Cook. </author> <title> Stochastic sampling and distributed ray tracing. In A.S. Glassner, editor, An Introduction to Ray Tracing, </title> <address> pages 161-199. </address> <publisher> Academic Press, </publisher> <year> 1989. </year>
Reference-contexts: This is mainly important for close-range viewing, such as in robotic applications, but can probably be ignored in many outdoor tasks. Practical solutions for simulating the depth-of-field effect exist <ref> [11, 5] </ref>. Motion blur: Image motion (and thus motion blur) is induced by a moving camera, a moving object, or a combination of both. Due to occlusion effects, moving shadows, etc., motion blur is an extremely complex phenomenon and its simulation time-consuming [12]. <p> Motion blur: Image motion (and thus motion blur) is induced by a moving camera, a moving object, or a combination of both. Due to occlusion effects, moving shadows, etc., motion blur is an extremely complex phenomenon and its simulation time-consuming [12]. Distributed ray-tracing, as proposed in <ref> [5] </ref>, appears to be the only solution that allows arbitrary object shapes and motion paths.
Reference: 6. <author> E.D. Dickmanns and F.R. Schell. </author> <title> Autonomous landing of airplanes by dynamic machine vision. </title> <booktitle> In Proc. IEEE Workshop on Applications of Computer Vision, </booktitle> <pages> pages 172-179, </pages> <address> Palm Springs, CA, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: Although flight simulators have actually been used with computer vision systems (closed-loop experiments for automatic aircraft landing <ref> [6] </ref>), their sophisticated mechanical design (i.e., hydraulics for emulating aircraft motion) is unnecessary for most perception tasks. 3 The Immersive Perception Simulator (IPS) The IPS is a new software environment intended to support a wide range of simulation and evaluation tasks in computer vision.
Reference: 7. <author> R. Dutta, R. Manmatha, L R. Williams, </author> <title> and E.M. Riseman. A data set for quantitative motion analysis. </title> <booktitle> In Proc. Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 159-164, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: outdoor environments: Since Radiance is not primarily aimed at simulating outdoor scenes, we are enhancing the system to support the generation of fractal terrain models, plant shapes, and corresponding textures. 3.5 Ground-Truth Data The acquisition of ground-truth data is usually expensive and tedious (e.g., through separate theodolite or radar measurements <ref> [7, 13] </ref>), therefore the pos sibility to access highly reliable ground-truth data is a key motive for using simulation in many vision applications.
Reference: 8. <author> K.E. Olin and D.Y. Tseng. </author> <title> Autonomous cross-country navigation: an integrated perception and planning system. </title> <journal> IEEE Expert, </journal> <pages> pages 16-30, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Specific vision examples are the ALV testbed at the University of Maryland [2], the map-based simulation environment developed at Hughes <ref> [8] </ref>, and the IUA simulator at UMASS [15]. Yet, until now, there has been no simulation environment available that could support the complexity of complete perception and control systems, and provide the required realism at the same time.
Reference: 9. <author> K. Pimentel and K. Teixeira. </author> <title> Virtual Reality: Through the New Looking Glass. </title> <publisher> Windcrest Books, </publisher> <year> 1993. </year>
Reference-contexts: The environment model is used, in conjunction with the appropriate sensor models, to produce realistic images of the scene viewed by the robot's sensors. The structure of the actual simulator resembles that of contemporary "virtual reality" (VR) systems <ref> [9] </ref> and, in fact, we are borrowing several technological ingredients from VR, such as the modeling of object dynamics and collision detection.
Reference: 10. <author> J. Pino, S. Ha, E. Lee, and J.T. Buck. Ptolemy: </author> <title> A framework for simulating and prototyping heterogeneous systems. </title> <type> Technical report, </type> <institution> UC Berkeley, </institution> <month> August </month> <year> 1992. </year> <institution> EECS Dept. </institution>
Reference-contexts: Specific system components may be gradually replaced by real-time hardware components as during the development process. Tools that support this kind of complex and heterogeneous engineering process are urgently needed and emerging just now <ref> [10] </ref>. 5 Summary In the past, experiments on synthetic imagery have generally not been considered conclusive for a system's performance under real operating conditions.
Reference: 11. <author> M. Potmesil and I. Chakravarty. </author> <title> Synthetic image generation with a lens and aperture camera model. </title> <journal> ACM Trans. Graphics, </journal> <volume> 1(2) </volume> <pages> 85-108, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: This is mainly important for close-range viewing, such as in robotic applications, but can probably be ignored in many outdoor tasks. Practical solutions for simulating the depth-of-field effect exist <ref> [11, 5] </ref>. Motion blur: Image motion (and thus motion blur) is induced by a moving camera, a moving object, or a combination of both. Due to occlusion effects, moving shadows, etc., motion blur is an extremely complex phenomenon and its simulation time-consuming [12].
Reference: 12. <author> M. Potmesil and I. Chakravarty. </author> <title> Modeling motion blur in computer-generated images. </title> <journal> Computer Graphics, </journal> <volume> 17(3) </volume> <pages> 389-399, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Motion blur: Image motion (and thus motion blur) is induced by a moving camera, a moving object, or a combination of both. Due to occlusion effects, moving shadows, etc., motion blur is an extremely complex phenomenon and its simulation time-consuming <ref> [12] </ref>. Distributed ray-tracing, as proposed in [5], appears to be the only solution that allows arbitrary object shapes and motion paths.
Reference: 13. <author> B. Sridhar, R. Suorsa, P. Smith, and B. Hussien. </author> <title> Vision-based obstacle detection for rotorcraft flight. </title> <journal> Journal of Robotic Systems, </journal> <volume> 9(6) </volume> <pages> 709-727, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: outdoor environments: Since Radiance is not primarily aimed at simulating outdoor scenes, we are enhancing the system to support the generation of fractal terrain models, plant shapes, and corresponding textures. 3.5 Ground-Truth Data The acquisition of ground-truth data is usually expensive and tedious (e.g., through separate theodolite or radar measurements <ref> [7, 13] </ref>), therefore the pos sibility to access highly reliable ground-truth data is a key motive for using simulation in many vision applications.
Reference: 14. <author> G.J. Ward. </author> <title> The RADIANCE lighting simulation and rendering system. </title> <booktitle> In Proc. SIGGRAPH Conference, </booktitle> <pages> pages 459-472, </pages> <address> Orlando, FL, </address> <month> July </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: On these platforms, limited availability of ground-truth data is provided by reading the z-buffer contents. Also, hardware-accelerated techniques do generally not allow to implement non-standard sensor models, but this may be tolerable at the early test stages. 3.4 The Radiance Rendering System Radiance <ref> [14] </ref>, a physics-based rendering system for producing photo-realistic images of complex scenes, is the main rendering tool in the IPS. Although initially developed for applications in architecture and lighting design, Radiance is currently the most widely used non-commercial tool for general photo-realistic image synthesis.
Reference: 15. <author> C.C. Weems, C. Brown, J.A. Webb, T. Poggio, and J.R. Kender. </author> <title> Parallel processing in the DARPA Strategic Computing vision program. </title> <journal> IEEE Expert, </journal> <volume> 6(5) </volume> <pages> 23-38, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Specific vision examples are the ALV testbed at the University of Maryland [2], the map-based simulation environment developed at Hughes [8], and the IUA simulator at UMASS <ref> [15] </ref>. Yet, until now, there has been no simulation environment available that could support the complexity of complete perception and control systems, and provide the required realism at the same time.
References-found: 15

