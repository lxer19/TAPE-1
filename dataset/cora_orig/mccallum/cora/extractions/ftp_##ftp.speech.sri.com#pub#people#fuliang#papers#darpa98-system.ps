URL: ftp://ftp.speech.sri.com/pub/people/fuliang/papers/darpa98-system.ps
Refering-URL: http://www.speech.sri.com/people/fuliang/fuliang.html
Root-URL: 
Title: Development of SRI's 1997 Broadcast News Transcription System  
Author: Ananth Sankar, Fuliang Weng, Ze'ev Rivlin, Andreas Stolcke, and Ramana Rao Gadde 
Address: 333 Ravenswood Avenue Menlo Park, CA 94025  
Affiliation: Speech Technology and Research Laboratory SRI International  
Abstract: This paper describes SRI's 1997 broadcast news transcription system used for the 1997 DARPA H4 evaluations. Our system had several novel components. These include automatic segmentation of entire broadcast shows, word-internal and crossword acoustic models robustly estimated with a new Gaussian Merging-Splitting (GMS) algorithm, the use of trigram language models (LMs) in lattices instead of for rescoring N-best lists, and an LM pruning algorithm that allows efficient representation of high-order (like 4- or 5-gram) LMs. We briefly describe these features and give comparative experimental results. We achieved a 18.7% relative improvement in performance on our 1996 H4 partitioned evaluation (PE) development test set as compared to our 1996 H4 PE evaluation system. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> V. Digalakis, P. Monaco, and H. Murveit, Genones: </author> <title> Generalized Mixture Tying in Continuous Hidden Markov Model-Based Speech Recognizers, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 281289, </pages> <year> 1996. </year>
Reference-contexts: In addition, we segmented at all gender changes located by the male/female recognition system. The segments were clustered into acoustically similar groups using bottom-up agglomera tive clustering. 2. First Pass Recognition: The segments were recognized using gender-dependent, speaker-independent (SI) Genonic hidden Markov models HMMs <ref> [1] </ref> trained using a new Gaussian Merging-Splitting (GMS) algorithm [2], and the 100 hours of H4 training data. This recognition was done with a 48,000-word bigram language model. 3. <p> The distance between two segments is then defined as the weighted-by-counts increase in entropy of the mixture weight distribution due to clustering two segments. This is identical to the approach we use for HMM state clustering <ref> [1] </ref>. The performance of the new clustering algorithm was found to be slightly better than that of the approach we used last year. 6. Acoustic Modeling For the 1997 broadcast news transcription system, we trained gender-dependent Genonic HMM models [1] using only the nominally 100 hours of H4 acoustic training data. <p> is identical to the approach we use for HMM state clustering <ref> [1] </ref>. The performance of the new clustering algorithm was found to be slightly better than that of the approach we used last year. 6. Acoustic Modeling For the 1997 broadcast news transcription system, we trained gender-dependent Genonic HMM models [1] using only the nominally 100 hours of H4 acoustic training data. <p> Since training a single H4 model is also easier, we chose to use this approach for our 1997 broadcast news system. We have recently developed the GMS algorithm to train state-clustered HMMs. We use Genonic HMMs <ref> [1] </ref>, where each HMM state cluster shares the same set of Gaussians (or Genone), and a separate mixture weight is used for each state. The GMS algorithm uses iterative Gaussian splitting and training to generate the required number of Gaussians per Genone.
Reference: 2. <author> A. Sankar, </author> <title> Experiments with a Gaussian Merging-Splitting Algorithm for HMM training for Speech Recognition, </title> <booktitle> in Proceedings of DARPA Speech Recognition Workshop, </booktitle> <address> (Lans-downe, VA), </address> <month> February </month> <year> 1998. </year>
Reference-contexts: The segments were clustered into acoustically similar groups using bottom-up agglomera tive clustering. 2. First Pass Recognition: The segments were recognized using gender-dependent, speaker-independent (SI) Genonic hidden Markov models HMMs [1] trained using a new Gaussian Merging-Splitting (GMS) algorithm <ref> [2] </ref>, and the 100 hours of H4 training data. This recognition was done with a 48,000-word bigram language model. 3. Adapting Models: The data from each segment cluster was used along with the hypotheses generated in Step 2 to adapt the gender-dependent HMMs to each test segment cluster. <p> At each stage of training the Gaussians are iteratively merged until all Gaussians have at least a threshold of data. For the HMM parameters, this technique was found to give more robust estimates than our previous training algorithm. The GMS algorithm is described elsewhere in these proceedings <ref> [2] </ref>. We used the GMS algorithm to train a separate non-crossword H4 model for each gender. We used 7761 triphones for the males and 6723 triphones for the females. <p> An explanation for this is given in <ref> [2] </ref>. We used Genonic crossword models to rescore n-best lists. The triphones in the crossword models are word-position in SI Adapted 31.78 29.97 Table 3: Comparison of SI and mean-adapted models dependent. There were 16,728 triphones for the males and 13,368 triphones for the females.
Reference: 3. <author> V. Digalakis, D. Rtischev, and L. Neumeyer, </author> <title> Speaker Adaptation Using Constrained Reestimation of Gaussian Mixtures, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 3, no. 5, </volume> <pages> pp. 357366, </pages> <year> 1995. </year>
Reference-contexts: This recognition was done with a 48,000-word bigram language model. 3. Adapting Models: The data from each segment cluster was used along with the hypotheses generated in Step 2 to adapt the gender-dependent HMMs to each test segment cluster. We used maximum-likelihood (ML) transformation-based adaptation <ref> [3, 4] </ref>, with a block-diagonal affine matrix transformation of the HMM mean vectors [5]. 4. Trigram Lattice Generation: The test-segment-cluster adapted models were used to create bigram lattices, which were then expanded to trigram lattices. 5. <p> Test-Segment-Cluster Adaptation As in our 1996 system, we adapted the SI HMMs to each test-segment-cluster by using unsupervised transcription-mode ML transformation-based adaptation <ref> [3, 4] </ref>. The transformations were a block diagonal affine matrix transform of the HMM mean vectors [5], and a scaling transform for the variance vector [4, 5]. We used three separate transforms, one of them being tied to the non-speech (silence) model.
Reference: 4. <author> A. Sankar and C.-H. Lee, </author> <title> A Maximum-Likelihood Approach to Stochastic Matching for Robust Speech Recognition, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 4, </volume> <pages> pp. 190 202, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: This recognition was done with a 48,000-word bigram language model. 3. Adapting Models: The data from each segment cluster was used along with the hypotheses generated in Step 2 to adapt the gender-dependent HMMs to each test segment cluster. We used maximum-likelihood (ML) transformation-based adaptation <ref> [3, 4] </ref>, with a block-diagonal affine matrix transformation of the HMM mean vectors [5]. 4. Trigram Lattice Generation: The test-segment-cluster adapted models were used to create bigram lattices, which were then expanded to trigram lattices. 5. <p> Adapting Models and Creating N-Best Lists: The recognition hypotheses from Step 5 were used to adapt models for each of the test segment clusters. Both a block diagonal affine matrix transform of the HMM means and a variance scaling transform <ref> [4, 5] </ref> were used in this step. The adapted models were then used to run forward-backward recognition on the trigram lattices to create word-dependent n-best lists. 7. Adapting Crossword Models: The hypotheses from Step 5 were used to adapt crossword models which were trained using the GMS algorithm. <p> Test-Segment-Cluster Adaptation As in our 1996 system, we adapted the SI HMMs to each test-segment-cluster by using unsupervised transcription-mode ML transformation-based adaptation <ref> [3, 4] </ref>. The transformations were a block diagonal affine matrix transform of the HMM mean vectors [5], and a scaling transform for the variance vector [4, 5]. We used three separate transforms, one of them being tied to the non-speech (silence) model. <p> Test-Segment-Cluster Adaptation As in our 1996 system, we adapted the SI HMMs to each test-segment-cluster by using unsupervised transcription-mode ML transformation-based adaptation [3, 4]. The transformations were a block diagonal affine matrix transform of the HMM mean vectors [5], and a scaling transform for the variance vector <ref> [4, 5] </ref>. We used three separate transforms, one of them being tied to the non-speech (silence) model. The other transforms are tied to phone classes determined by a human expert. Table 3 shows the performance gain from mean adaptation for the 1996 H4 development test set.
Reference: 5. <author> L. Neumeyer, A. Sankar, and V. Digalakis, </author> <title> A Comparative Study of Speaker Adaptation Techniques, </title> <booktitle> in Proceedings of EUROSPEECH, </booktitle> <pages> pp. 11271130, </pages> <year> 1995. </year>
Reference-contexts: Adapting Models: The data from each segment cluster was used along with the hypotheses generated in Step 2 to adapt the gender-dependent HMMs to each test segment cluster. We used maximum-likelihood (ML) transformation-based adaptation [3, 4], with a block-diagonal affine matrix transformation of the HMM mean vectors <ref> [5] </ref>. 4. Trigram Lattice Generation: The test-segment-cluster adapted models were used to create bigram lattices, which were then expanded to trigram lattices. 5. Recognition from Trigram Lattices: The gender-dependent SI models were used to run recognition for all the segments using the trigram lattices. <p> Adapting Models and Creating N-Best Lists: The recognition hypotheses from Step 5 were used to adapt models for each of the test segment clusters. Both a block diagonal affine matrix transform of the HMM means and a variance scaling transform <ref> [4, 5] </ref> were used in this step. The adapted models were then used to run forward-backward recognition on the trigram lattices to create word-dependent n-best lists. 7. Adapting Crossword Models: The hypotheses from Step 5 were used to adapt crossword models which were trained using the GMS algorithm. <p> Test-Segment-Cluster Adaptation As in our 1996 system, we adapted the SI HMMs to each test-segment-cluster by using unsupervised transcription-mode ML transformation-based adaptation [3, 4]. The transformations were a block diagonal affine matrix transform of the HMM mean vectors <ref> [5] </ref>, and a scaling transform for the variance vector [4, 5]. We used three separate transforms, one of them being tied to the non-speech (silence) model. The other transforms are tied to phone classes determined by a human expert. <p> Test-Segment-Cluster Adaptation As in our 1996 system, we adapted the SI HMMs to each test-segment-cluster by using unsupervised transcription-mode ML transformation-based adaptation [3, 4]. The transformations were a block diagonal affine matrix transform of the HMM mean vectors [5], and a scaling transform for the variance vector <ref> [4, 5] </ref>. We used three separate transforms, one of them being tied to the non-speech (silence) model. The other transforms are tied to phone classes determined by a human expert. Table 3 shows the performance gain from mean adaptation for the 1996 H4 development test set.
Reference: 6. <author> A. Sankar, L. Heck, and A. Stolcke, </author> <title> Acoustic Modeling for the SRI Hub4 Partitioned Evaluation Continuous Speech Recognition System, </title> <booktitle> in Proceedings of the 1997 DARPA Speech Recognition Workshop, </booktitle> <address> (Chantilly, VA), </address> <year> 1997. </year>
Reference-contexts: We used version 0.3 of the CMU dictionary modified at SRI to make sure that pronunciations existed for all the 48,000 vocabulary words. 5. Acoustic Segmentation and Clustering Our acoustic segmentation algorithm is a modification of the algorithm we used in 1997 to segment long PE segments <ref> [6] </ref>. In the 1996 PE data, each stream was guaranteed to contain only speech, and to come from a single speaker. <p> The segments were clustered using bottom-up agglomerative clustering as in our 1996 system <ref> [6, 7] </ref>. However, we modified the way in which the distances were computed between the segments. In our previous work [6, 7], a separate Gaussian mixture model (GMM) was trained for each segment, and the distance between the segments was given by the symmetric relative entropy computed using these GMMs [6, <p> The segments were clustered using bottom-up agglomerative clustering as in our 1996 system <ref> [6, 7] </ref>. However, we modified the way in which the distances were computed between the segments. In our previous work [6, 7], a separate Gaussian mixture model (GMM) was trained for each segment, and the distance between the segments was given by the symmetric relative entropy computed using these GMMs [6, 7]. Since some segments have very little data, it is difficult to estimate a full GMM for each segment. <p> <ref> [6, 7] </ref>. However, we modified the way in which the distances were computed between the segments. In our previous work [6, 7], a separate Gaussian mixture model (GMM) was trained for each segment, and the distance between the segments was given by the symmetric relative entropy computed using these GMMs [6, 7]. Since some segments have very little data, it is difficult to estimate a full GMM for each segment. We modified our approach by training a single GMM for all the segments, and using a separate mixture weight distribution for each segment to these shared Gaussians. <p> For these runs, we used trigram lattices, which we recently implemented [10]. From the table, we see a relative 5.7% improvement from using adaptation. This is less than the 8.3% improvement we reported using last year's system <ref> [6] </ref>. This could be explained by the fact that our SI models have improved over last year. Thus, the further improvement possible from adaptation may decrease. We also experimented with iterative adaptation. We tried two different approaches. <p> Trigram Lattice Generation In previous years, we have used trigram and higher-order LMs to rescore n-best lists <ref> [12, 13, 6] </ref>. However, it is well known that trigram LMs give a drastic improvement over bigram LMs. Thus, it makes sense to use them earlier in the search.
Reference: 7. <author> L. Heck and A. Sankar, </author> <title> Acoustic Clustering and Adaptation for Robust Speech Recognition, </title> <booktitle> in Proceedings of EU-ROSPEECH, </booktitle> <year> 1997. </year>
Reference-contexts: The segments were clustered using bottom-up agglomerative clustering as in our 1996 system <ref> [6, 7] </ref>. However, we modified the way in which the distances were computed between the segments. In our previous work [6, 7], a separate Gaussian mixture model (GMM) was trained for each segment, and the distance between the segments was given by the symmetric relative entropy computed using these GMMs [6, <p> The segments were clustered using bottom-up agglomerative clustering as in our 1996 system <ref> [6, 7] </ref>. However, we modified the way in which the distances were computed between the segments. In our previous work [6, 7], a separate Gaussian mixture model (GMM) was trained for each segment, and the distance between the segments was given by the symmetric relative entropy computed using these GMMs [6, 7]. Since some segments have very little data, it is difficult to estimate a full GMM for each segment. <p> <ref> [6, 7] </ref>. However, we modified the way in which the distances were computed between the segments. In our previous work [6, 7], a separate Gaussian mixture model (GMM) was trained for each segment, and the distance between the segments was given by the symmetric relative entropy computed using these GMMs [6, 7]. Since some segments have very little data, it is difficult to estimate a full GMM for each segment. We modified our approach by training a single GMM for all the segments, and using a separate mixture weight distribution for each segment to these shared Gaussians.
Reference: 8. <author> R. Stern, </author> <title> Specification of the 1996 Hub4 Broadcast News Evaluation, </title> <booktitle> in Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <address> (Chantilly, VA), </address> <year> 1997. </year>
Reference-contexts: This is a deviation from our 1996 system, where we adapted models, trained with the Wall Street Journal or Switchboard training data, to each of the seven individual acoustic focus conditions defined by the H4 evaluation committee <ref> [8] </ref>, using the first 50 hours of acoustic training data. This creates seven condition-specific models for each gender. In 1996, we participated in the PE test, where the acoustic focus condition was given for each test segment.
Reference: 9. <author> F. Kubala, H. Jin, S. Matsoukas, L. Nguyen, R. Schwartz, and J. Makhoul, </author> <title> The 1996 BBN BYBLOS HUB-4 Transcription System, </title> <booktitle> in Proceedings of the 1997 DARPA Speech Recognition Workshop, </booktitle> <address> (Chantilly, VA), </address> <year> 1997. </year>
Reference-contexts: After the 1996 evaluations, we trained a single gender-dependent H4 model using the first 50 hours of training data. This approach was taken by BBN in the 1996 evaluations <ref> [9] </ref>. Table 2 gives recognition word error rates with the 20,000 word bigram LM we used for the 1996 evaluations for the male subset of the 1996 development test set. The single H4 model gave a relative 6.1% lower word error rate than the condition-specific models.
Reference: 10. <author> F. Weng, A. Stolcke, and A. Sankar, </author> <title> New developments in lattice-based search strategies in SRI's H4 system, </title> <booktitle> in Proceedings of DARPA Speech Recognition Workshop, </booktitle> <address> (Lansdowne, VA), </address> <month> February </month> <year> 1998. </year>
Reference-contexts: The other transforms are tied to phone classes determined by a human expert. Table 3 shows the performance gain from mean adaptation for the 1996 H4 development test set. For these runs, we used trigram lattices, which we recently implemented <ref> [10] </ref>. From the table, we see a relative 5.7% improvement from using adaptation. This is less than the 8.3% improvement we reported using last year's system [6]. This could be explained by the fact that our SI models have improved over last year. <p> However, it is well known that trigram LMs give a drastic improvement over bigram LMs. Thus, it makes sense to use them earlier in the search. This year, we developed new lattice-based search capability by implementing a new bigram lattice algorithm and algorithms to expand these to trigram lattices <ref> [10] </ref>. We achieved about 5% relative improvement in performance on a male subset of the 1996 H4 development test set by running recognition from our new trigram lattices as compared to trigram rescoring of n-best lists generated using our 1996 H4 PE evaluation system. 9.
Reference: 11. <author> M. J. F. Gales, </author> <title> The Generation and Use of Regression Class Trees for MLLR Adaptation, </title> <type> Tech. Rep. </type> <institution> CUED/F-INFENG/TR263, Cambridge University, </institution> <year> 1996. </year>
Reference-contexts: In the second approach, we started with a single transform, increasing it to two, three, six, and eleven transforms in subsequent iterations. In addition, in this approach, adaptation is stopped if the transcriptions do not change from one iteration to the next <ref> [11] </ref>. Table 4 shows that we did not achieve any improvement by using iterative adaptation, and hence we did not use it in our final system.
Reference: 12. <author> V. Digalakis, M. Weintraub, A. Sankar, H. Franco, L. Neumeyer, and H. Murveit, </author> <title> Continuous Speech Dictation on ARPA's North American Business News Domain, </title> <booktitle> in Proceedings of the Spoken Language Systems Technology Workshop, </booktitle> <pages> pp. 8893, </pages> <year> 1995. </year>
Reference-contexts: Trigram Lattice Generation In previous years, we have used trigram and higher-order LMs to rescore n-best lists <ref> [12, 13, 6] </ref>. However, it is well known that trigram LMs give a drastic improvement over bigram LMs. Thus, it makes sense to use them earlier in the search.
Reference: 13. <author> A. Sankar, A. Stolcke, T. Chung, L. Neumeyer, M. Weintraub, H. Franco, and F. Beaufays, </author> <title> Noise-resistant Feature Extraction and Model Training for Robust Speech Recognition, </title> <booktitle> in Proceedings of the 1996 DARPA CSR Workshop, </booktitle> <address> Ardenhouse, NY, </address> <year> 1996. </year>
Reference-contexts: Trigram Lattice Generation In previous years, we have used trigram and higher-order LMs to rescore n-best lists <ref> [12, 13, 6] </ref>. However, it is well known that trigram LMs give a drastic improvement over bigram LMs. Thus, it makes sense to use them earlier in the search.
Reference: 14. <author> A. Stolcke, </author> <title> Entropy-based Pruning of N-gram Backoff Language Models, </title> <booktitle> in Proceedings of DARPA Speech Recognition Workshop, </booktitle> <address> (Lansdowne, VA), </address> <month> February </month> <year> 1998. </year>
Reference-contexts: The 5-gram LM was pruned using a newly developed entropy-based pruning technique that drastically reduces the number of n-grams in the model without altering its performance <ref> [14] </ref>. In all the LMs, multiple corpora were used by training separate LMs for each and then interpolating the language models. The interpolation weights were estimated so as to minimize the perplexity on the 1996 development test transcriptions. All LMs used Katz backoff [15] and Good-Turing discounting. 10.
Reference: 15. <author> S. M. Katz, </author> <title> Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer, </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 35, no. 3, </volume> <pages> pp. 400401, </pages> <year> 1987. </year>
Reference-contexts: In all the LMs, multiple corpora were used by training separate LMs for each and then interpolating the language models. The interpolation weights were estimated so as to minimize the perplexity on the 1996 development test transcriptions. All LMs used Katz backoff <ref> [15] </ref> and Good-Turing discounting. 10. N-best List Rescoring The trigram lattices generated in Step 4 were used to generate n-best lists using the test-segment-cluster-adapted non-crossword models and the word-dependent n-best algorithm [16]. These n-best lists were then rescored with 5-gram LMs, test-segment-cluster-adapted crossword models, and a word insertion penalty.
Reference: 16. <author> R. Schwartz and Y.-L. Chow, </author> <title> A Comparison of Several Approximate Algorithms for Finding Multiple (N-Best) Sentence Hypotheses, </title> <booktitle> in Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. 701704, </pages> <year> 1991. </year>
Reference-contexts: All LMs used Katz backoff [15] and Good-Turing discounting. 10. N-best List Rescoring The trigram lattices generated in Step 4 were used to generate n-best lists using the test-segment-cluster-adapted non-crossword models and the word-dependent n-best algorithm <ref> [16] </ref>. These n-best lists were then rescored with 5-gram LMs, test-segment-cluster-adapted crossword models, and a word insertion penalty. For crossword rescoring, the n-best lists were represented in the form of a tree lattice, resulting in very fast and memory-efficient rescoring.
Reference: 17. <author> A. Stolcke, Y. Konig, and M. Weintraub, </author> <title> Explicit Word Error Minimization In N-Best List Rescoring, </title> <booktitle> in Proceedings of EUROSPEECH, </booktitle> <pages> pp. 163166, </pages> <year> 1997. </year>
Reference-contexts: The hypothesis with the lowest expected word error count was chosen to be the output of the recognition system <ref> [17] </ref>. Table 5 gives the word error rates using different knowledge sources. In all cases, we used word insertion penalty. The table gives the word errors for both the 1996 and 1997 systems.
References-found: 17

