URL: ftp://borneo.gmd.de/pub/as/janus/ref94_2.ps
Refering-URL: http://borneo.gmd.de/AS/janus/publi/publi.html
Root-URL: 
Title: The Pandemonium System of Reflective Agents  
Author: Frank Smieja 
Note: Report number: 1994/2  
Abstract: In IEEE Transactions on Neural Networks, 7(1):97-106, 1996 Also available as GMD report #794 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Beyer and F. J. Smieja. </author> <title> Data exploration with reflective adaptive models. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 22 </volume> <pages> 193-211, </pages> <year> 1996. </year>
Reference-contexts: In Figure 2 the information produced by reflection has been denoted by c. It is called confidence and is defined in the following way. Definition 1 (confidence) Let the accuracy of a prediction f (x) be given by A (f (x); y); 2 <ref> [0; 1] </ref>. The confidence function c ( x), 0 c ( x) 1, is an estimate of the accuracy of the prediction f (x) for y: The function c (x) attempts to establish the zones of competence of the agent in the x-space. <p> Figure 5c is the "batch" solution|where all the 400 patterns are presented before the weights are updated with the accumulated back-propagation changes, and online back-propagation learning. The incremental solution is more desirable for dynamic open problems <ref> [1] </ref>, since the agent adapts after each pattern is seen. A characteristic difference in solution (the generalization) between the two methods is the sharper edges in the incremental method, that separate the MINOS modules' contributions.
Reference: [2] <author> U. Beyer and F. J. Smieja. </author> <title> Learning from examples, agent teams and the concept of reflection. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 10(3) </volume> <pages> 251-272, </pages> <year> 1996. </year>
Reference-contexts: Two basic types of modular systems may be identified: those that divide the input space among modules and those that consist of modules each of which learns the entire set of examples. The latter type is also referred to as a team architecture <ref> [2] </ref>. The Pandemonium system is designed to carry out the former modularization technique. <p> A fundamental problem to be faced by such modular systems is known as the decomposition-recomposition problem [28, 14]: how is the input space to be (automatically) divided and then recombined after 1 2 Smieja processing through the agents? This turns out, as do so many aspects of data-driven information processing <ref> [2, 7] </ref> to be problem dependent. As a result three different methods of dynamic decomposition are developed for the Pandemonium architecture. The paper is organized as follows. <p> Our method partly distributes the knowledge, in the adaptive modules themselves, through the coarser-grained nature of the processing elements. In order that it be possible to use such coarse Pandemonium 3 grained modules in a Pandemonium architecture, a necessary requirement is that they are reflective <ref> [28, 2] </ref>. This means they have the property illustrated in Figure 2. Reflection is the method by which an agent observes its own behavior and produces information relating to the quality of its output. <p> The function is adaptable and is fitted to the learning set data L. Reflection is discussed in more detail in <ref> [2] </ref>; we concentrate in this paper neural network learns the function f (x) for those (x; y) tuples it has been allocated. <p> The modules in our Pandemonium system are called MINOS, and they generate confidence in ways depending on the problem type. In this paper we focus on three problem types: continuous problems, discrete (boolean) problems and discrete (classification) problems. The latter was studied extensively with Pandemonium in <ref> [28, 2, 12] </ref>, and we will be discussing the results of those experiments in this paper. The main strength of Pandemonium is shown to lie in the recognition of large-scale structure and in dynamic breakdown of a problem with nonoverlapping regions. A MINOS reagent is shown schematically in Figure 3. <p> The borders between classes have a form typified in Figure 8. A performance of around 80% is generally possible with linear separation, after which nonlinear decision regions have to be constructed. The number of regions required for further improvement increases the better the performance becomes <ref> [2] </ref>. The "last few" difficult patterns become ever more entwined in other classes, and ever further from their fellow patterns. For such problems the Pandemonium type of mo-dularization will not help significantly in the so lution of the problem. <p> The "last few" difficult patterns become ever more entwined in other classes, and ever further from their fellow patterns. For such problems the Pandemonium type of mo-dularization will not help significantly in the so lution of the problem. Experiments carried out in <ref> [12, 28, 2] </ref> indicated that the Pandemonium system was significantly faster than a (large) single net in converging to a good solution [27], but did not produced higher accuracy than other, non-modular, systems. <p> Pandemonium is most likely to help, as for the parity problem, in higher convergence speeds and for incremental learning of limited numbers of examples. It was shown in <ref> [2] </ref> how marked improvements may be gained in a team method of solution. The performance of a reflective team improved on the best single agent performance of 88.4% by 4.9% for a difficult hand-written upper-case letters problem. <p> For such problems any of the decomposition methods could be used, but due to the overlapping nature of the classes one would expect to find significant improvements through using team methods (as described in <ref> [2] </ref>) rather than decomposition methods.
Reference: [3] <author> L. Bochereau, P. Bourgine, and H. E. Priso. </author> <title> Generalist vs. specialist neural networks. </title> <type> Technical report, </type> <institution> CEMAGREF, France, </institution> <year> 1991. </year>
Reference: [4] <author> S. Eberlein. </author> <title> Developing a decision-making network for a rover. </title> <journal> Journal of Neural Computation, </journal> <volume> 1(2), </volume> <year> 1989. </year>
Reference: [5] <author> G.M. Edelman. </author> <title> Neural Darwinism. The theory of neuronal group selection. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1987. </year>
Reference: [6] <author> J. Franke and E. Mandler. </author> <title> A comparison of two approaches for combining the votes of cooperating classifiers. </title> <booktitle> In Proceedings of the 11th IAPR, </booktitle> <pages> pages 611-614, </pages> <address> Den Haag, Netherlands, </address> <year> 1992. </year>
Reference: [7] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: A fundamental problem to be faced by such modular systems is known as the decomposition-recomposition problem [28, 14]: how is the input space to be (automatically) divided and then recombined after 1 2 Smieja processing through the agents? This turns out, as do so many aspects of data-driven information processing <ref> [2, 7] </ref> to be problem dependent. As a result three different methods of dynamic decomposition are developed for the Pandemonium architecture. The paper is organized as follows.
Reference: [8] <author> L. K. Hanson and P. Salamon. </author> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(10) </volume> <pages> 993-1001, </pages> <year> 1990. </year>
Reference-contexts: In team methods the input space is not necessarily decomposed among agents, but the agents themselves have different ways of looking at the problem and modeling it. Their range of suggestions for each input is used in a committee decision <ref> [8] </ref> from members that each see the entire input space.
Reference: [9] <author> R. Hecht-Nielsen. </author> <title> Theory of the backpropagation neural network. </title> <booktitle> In Proceedings of the First International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, San Diego, </address> <year> 1989. </year> <journal> IEEE, IEEE TAB Neural Network Committee. </journal>
Reference-contexts: The idea is to approach processing flexibility through combining diverse specializations of the comprising modules (agents). Such diversity complements the inherent generality in the modeling process of the agents themselves, Although theory implies <ref> [11, 9, 15] </ref> that a neural network is always capable of representing the solution. It does not show how the solution may be found, whether it is the best solution, how long it may take to find, and how much data need to be seen before it is found.
Reference: [10] <author> G. E. Hinton. </author> <title> Learning distributed representations of concepts. </title> <booktitle> In Eighth Annual Conference of the Cognitive Science Society, </booktitle> <year> 1986. </year>
Reference-contexts: Thus the decision daemon chooses the A-daemon. It is clear that we can translate the daemons used by Selfridge into our present-day notion of "grandmother cells" (GC), and indeed the entire Pandemonium system can be seen as a GC neural network <ref> [10] </ref>. We take the general idea employed in Pandemonium, and extend it to specifying a modular system of neural networks. Each of the daemons in figure 1 is replaced by an adaptive module, and the system is run using the divide-and-conquer principle [26, 28].
Reference: [11] <author> K. M. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feed-forward networks 14 Smieja are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: The idea is to approach processing flexibility through combining diverse specializations of the comprising modules (agents). Such diversity complements the inherent generality in the modeling process of the agents themselves, Although theory implies <ref> [11, 9, 15] </ref> that a neural network is always capable of representing the solution. It does not show how the solution may be found, whether it is the best solution, how long it may take to find, and how much data need to be seen before it is found.
Reference: [12] <author> S. Hubrig-Schaumburg. </author> <title> Handwritten character recognition using a reflective modular neural network system, </title> <month> November </month> <year> 1992. </year>
Reference-contexts: The modules in our Pandemonium system are called MINOS, and they generate confidence in ways depending on the problem type. In this paper we focus on three problem types: continuous problems, discrete (boolean) problems and discrete (classification) problems. The latter was studied extensively with Pandemonium in <ref> [28, 2, 12] </ref>, and we will be discussing the results of those experiments in this paper. The main strength of Pandemonium is shown to lie in the recognition of large-scale structure and in dynamic breakdown of a problem with nonoverlapping regions. A MINOS reagent is shown schematically in Figure 3. <p> The "last few" difficult patterns become ever more entwined in other classes, and ever further from their fellow patterns. For such problems the Pandemonium type of mo-dularization will not help significantly in the so lution of the problem. Experiments carried out in <ref> [12, 28, 2] </ref> indicated that the Pandemonium system was significantly faster than a (large) single net in converging to a good solution [27], but did not produced higher accuracy than other, non-modular, systems.
Reference: [13] <author> R. A. Jacobs and M. I. Jordan. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference: [14] <author> K. Joe, Y. Mori, and S. Miyake. </author> <title> Construction of a large-scale neural network: simulation of handwritten Japanese character recognition on NCUBE. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 2(2) </volume> <pages> 79-107, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The latter type is also referred to as a team architecture [2]. The Pandemonium system is designed to carry out the former modularization technique. A fundamental problem to be faced by such modular systems is known as the decomposition-recomposition problem <ref> [28, 14] </ref>: how is the input space to be (automatically) divided and then recombined after 1 2 Smieja processing through the agents? This turns out, as do so many aspects of data-driven information processing [2, 7] to be problem dependent.
Reference: [15] <author> Vera K o urkova. </author> <title> Kolmogorov's theorem and multilayer neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 501-506, </pages> <year> 1992. </year>
Reference-contexts: The idea is to approach processing flexibility through combining diverse specializations of the comprising modules (agents). Such diversity complements the inherent generality in the modeling process of the agents themselves, Although theory implies <ref> [11, 9, 15] </ref> that a neural network is always capable of representing the solution. It does not show how the solution may be found, whether it is the best solution, how long it may take to find, and how much data need to be seen before it is found.
Reference: [16] <author> K. Lang and M. Witbrock. </author> <title> Learning to tell two spirals apart. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Proceedings of the 1988 Connectionist Summer School, </booktitle> <pages> pages 52-59. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1988. </year>
Reference-contexts: The gray regions represent "undecided" areas, defined to be when the output is in [0:4; 0:6]. The sharing of the input space between the 6 MINOS modules (10 hidden units each) is shown in (c). Pandemonium 9 4.1 The two-spirals problem The two-spirals problem is defined in <ref> [16] </ref> by the 192 points making up the learning set. This is shown graphically in Figure 7a. The black points are to be mapped to 1 and the white points to 0.
Reference: [17] <author> H. Muhlenbein. </author> <title> Limitations of multilayer perceptrons steps towards genetic neural networks. </title> <journal> Parallel Computing, </journal> <volume> 14(3) </volume> <pages> 249-260, </pages> <year> 1990. </year>
Reference-contexts: It does not show how the solution may be found, whether it is the best solution, how long it may take to find, and how much data need to be seen before it is found. These are fundamental limitations <ref> [17] </ref> to a "universal modeling" philosophy. Modular systems are in no respect a new idea.
Reference: [18] <author> J. M. Murre, R. H. Pfaf, and G. Wolters. </author> <title> CALM networks: A modular approach to supervised and unsupervised learning. </title> <booktitle> In Proceedings of the IJCNN-89. IEEE, </booktitle> <year> 1989. </year>
Reference: [19] <author> Y. Nishikawa, H. Kita, and A. Kawamura. NN/I: </author> <title> a neural network which divides and learns environments. </title> <booktitle> In Proceedings of the IJCNN-90, </booktitle> <address> Washington D.C., 1990. </address> <publisher> IEEE. </publisher>
Reference: [20] <author> L. Y. Pratt and C. A. Kamm. </author> <title> Improving a phoneme classification neural network through problem decomposition. </title> <booktitle> In Proceedings of the IJCNN-91. IEEE, </booktitle> <year> 1991. </year>
Reference: [21] <author> D. L. Reilly, C. Scofield, C. Elbaum, and L. N. Cooper. </author> <title> Learning system archictectu-res composed of multiple learning modules. </title> <booktitle> In IEEE First International Conference on Neural Networks, </booktitle> <pages> pages 495-503, </pages> <year> 1987. </year>
Reference: [22] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <journal> Nature, </journal> <volume> 323(533), </volume> <year> 1986. </year>
Reference-contexts: It is constructed from two agent modules, called the Worker and Monitor. The Worker learns the mapping x 7! y and the Monitor's job is to generate the confidence c (x). The Worker is a feed-forward neural network, trained using the back-propagation learning procedure <ref> [22] </ref>. A Pandemonium system has the following structure. A number of MINOS modules are chosen to comprise a system with a common task. They are not connected to one another, and can be trained and run independently.
Reference: [23] <author> M. Sekiguchi, S. Nagata, and K. Asakawa. </author> <title> Behaviour control for a mobile robot by multi-hierarchical neural network. </title> <booktitle> In Proceedings of the IEEE international conference on robotics and automation, </booktitle> <year> 1989. </year>
Reference: [24] <author> O. G. Selfridge. Pandemonium: </author> <title> a paradigm for learning. </title> <booktitle> In The Mechanisation of Thought Processes: Proceedings of a Symposium Held at the National Physical Laboratory, </booktitle> <month> November </month> <year> 1958, </year> <pages> pages 511-527, </pages> <address> Lon-don: HMSO, </address> <year> 1958. </year>
Reference-contexts: Finally, in section 5 the performance of a Pandemonium system on optical character recognition (OCR) problems is discussed with reference to previous experimental results. 2 The Pandemonium system The Pandemonium architecture was originally proposed in 1958 by Selfridge <ref> [24] </ref>. The idea is to divide and conquer a complex problem domain, through use of a number of specialized agents working in parallel. All these agents, or daemons, process the same signal in parallel, and each provides a possible answer.
Reference: [25] <author> O. G. Selfridge and U. Neisser. </author> <title> Pattern recognition by machine. </title> <journal> Scientific American, </journal> 203(2) 60-68, August 1960. 
Reference-contexts: Each module will become specialized to a greater or lesser degree on a particular aspect of the task. With this extension we retain the divide-and-conquer specialization problem-solving strategy. However, Selfridge's division resulted in local knowledge distribution (GC), or in his later applications <ref> [25] </ref> distributions of a number of nonadaptive filters. Our method partly distributes the knowledge, in the adaptive modules themselves, through the coarser-grained nature of the processing elements.
Reference: [26] <author> F. J. Smieja. </author> <title> Multiple network systems (MI-NOS) modules: Task division and module discrimination. </title> <booktitle> In Proceedings of the 8th AISB conference on Artificial Intelligence, </booktitle> <address> Leeds, </address> <month> 16-19 April, </month> <year> 1991, 1991. </year> <note> Also available as GMD technical report 638. </note>
Reference-contexts: We take the general idea employed in Pandemonium, and extend it to specifying a modular system of neural networks. Each of the daemons in figure 1 is replaced by an adaptive module, and the system is run using the divide-and-conquer principle <ref> [26, 28] </ref>. Each module will become specialized to a greater or lesser degree on a particular aspect of the task. With this extension we retain the divide-and-conquer specialization problem-solving strategy.
Reference: [27] <author> F. J. Smieja. </author> <title> Optical character recognition using Pandemonium and a genetically optimized polynomial classifier. </title> <editor> In Jon Geist, editor, </editor> <booktitle> The First Census Optical Character Recognition System conference, </booktitle> <pages> pages 145-168, </pages> <address> Gaithersburg MD, </address> <month> August </month> <year> 1992. </year> <institution> National Institute of Standards and Technology. </institution>
Reference-contexts: For such problems the Pandemonium type of mo-dularization will not help significantly in the so lution of the problem. Experiments carried out in [12, 28, 2] indicated that the Pandemonium system was significantly faster than a (large) single net in converging to a good solution <ref> [27] </ref>, but did not produced higher accuracy than other, non-modular, systems. Pandemonium is most likely to help, as for the parity problem, in higher convergence speeds and for incremental learning of limited numbers of examples.
Reference: [28] <author> F. J. Smieja and H. Muhlenbein. </author> <title> Reflective modular neural network systems. </title> <type> Technical Report 633, </type> <institution> GMD German National Research Center for Information Technology, </institution> <address> Sankt Augustin, Germany, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: Whether or not a universal modeler is a viable prospect, the practical application of neural networks and other approximation methodologies requires a modular perspective. Numerous suggestions have been made in the literature regarding modular systems of neural networks, examples are to be found in the citations [21]-[26]. In <ref> [28] </ref> these methods of modularization are briefly compared and contrasted. Two basic types of modular systems may be identified: those that divide the input space among modules and those that consist of modules each of which learns the entire set of examples. <p> The latter type is also referred to as a team architecture [2]. The Pandemonium system is designed to carry out the former modularization technique. A fundamental problem to be faced by such modular systems is known as the decomposition-recomposition problem <ref> [28, 14] </ref>: how is the input space to be (automatically) divided and then recombined after 1 2 Smieja processing through the agents? This turns out, as do so many aspects of data-driven information processing [2, 7] to be problem dependent. <p> We take the general idea employed in Pandemonium, and extend it to specifying a modular system of neural networks. Each of the daemons in figure 1 is replaced by an adaptive module, and the system is run using the divide-and-conquer principle <ref> [26, 28] </ref>. Each module will become specialized to a greater or lesser degree on a particular aspect of the task. With this extension we retain the divide-and-conquer specialization problem-solving strategy. <p> Our method partly distributes the knowledge, in the adaptive modules themselves, through the coarser-grained nature of the processing elements. In order that it be possible to use such coarse Pandemonium 3 grained modules in a Pandemonium architecture, a necessary requirement is that they are reflective <ref> [28, 2] </ref>. This means they have the property illustrated in Figure 2. Reflection is the method by which an agent observes its own behavior and produces information relating to the quality of its output. <p> The modules in our Pandemonium system are called MINOS, and they generate confidence in ways depending on the problem type. In this paper we focus on three problem types: continuous problems, discrete (boolean) problems and discrete (classification) problems. The latter was studied extensively with Pandemonium in <ref> [28, 2, 12] </ref>, and we will be discussing the results of those experiments in this paper. The main strength of Pandemonium is shown to lie in the recognition of large-scale structure and in dynamic breakdown of a problem with nonoverlapping regions. A MINOS reagent is shown schematically in Figure 3. <p> The "last few" difficult patterns become ever more entwined in other classes, and ever further from their fellow patterns. For such problems the Pandemonium type of mo-dularization will not help significantly in the so lution of the problem. Experiments carried out in <ref> [12, 28, 2] </ref> indicated that the Pandemonium system was significantly faster than a (large) single net in converging to a good solution [27], but did not produced higher accuracy than other, non-modular, systems.
Reference: [29] <author> A. Waibel. </author> <title> Modular construction of time-delay neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <year> 1989. </year>
Reference: [30] <author> D. H. Wolpert. </author> <title> Stacked generalization. </title> <type> Technical Report LA-UR-90-3460, </type> <address> LANL, Los Alamos, NM, </address> <year> 1990. </year>
References-found: 30

