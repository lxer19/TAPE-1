URL: http://www.cs.wustl.edu/cs/techreports/1991/wucs-91-36.part3.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: http://www.cs.wustl.edu
Title: Theorem 15.10 Suppose A is a polynomial time learning algorithm for learning monomials using monomials,
Note: can be obtained. For each 1 i n, let ~p i be the n-bit vector that is 1 everywhere except  r(n) 2C PCopt At that  A. This can be avoided by first using the greedy approximation 120  
Abstract: times the optimal cost, where n is the number of sets. Proof: We will give a reduction showing how algorithm A can be used as a subroutine to obtain the desired set cover approximation algorithm by associating the weighted set covers with monomials. Let J opt 2 f1; . . . ; ng be an optimal cover of T = f1; . . . ; mg, and let PCcost(J opt ) = C PCopt . Each cover fi 1 ; . . . ; i l g is associated with monomial x i 1 ^ . . . ^ x i l . Let M opt be the monomial corresponding to the optimal cover J opt . The basic idea is for A 0 to run A with M opt as the target concept. We then force the output of algorithm A to be a monomial with error close to M opt . This monomial will correspond to a cover that is close to optimal. We will use N EG fi M to force the output of A to correspond to a cover. To achieve this goal we put a uniform distribution over the following set of m negative examples. For each i 2 T , define ~e i to be an n-bit vector whose j th bit is 0 if and only if i 2 S j . For example, let m = 5 and S 1 = f1; 3; 5g, S 2 = f1; 2g, S 3 = f1; 4; 5g, and S 4 = f1; 3g. Then ~e 1 = 0000, ~e 2 = 1011, ~e 3 = 0110, ~e 4 = 1101, and ~e 5 = 0101. Let E = [ i2T ~e i . Observe that J 0 = fi 1 ; . . . ; i l g is a cover of T if and only if every vector in E is a negative example of the corresponding monomial. Thus since J opt is a cover, M opt is consistent with all the negative examples. Finally, we let * &lt; 1 m . Note that 1 * is polynomial in the size of the set cover instance. Since we force A to output a monomial consistent with the sample, A 0 outputs a cover. Note that at this point we have used the assumption that A learns monomials by monomials because we do not have a natural mapping between non-monomials and possible covers. Next we use P OS fi M to force the cost of the cover to be low. First, without loss of generality, we assume the costs are scaled so that P n r(n) &lt; 1. Next we define the induced distribution I + on the positive examples. We will then show that this distribution A good hypothesis output by algorithm A must have a rate of disagreement of at most 2* with M opt on I + . One factor of * comes from the disagreement with D + , and the other factor comes from the error fi &lt; * induced by the malicious oracle. Finally, we do not know the actual value of C PCopt . So, we repeatedly divide * by 2 and run A with the new value of *, but without changing I + . This forces A to output better and better monomials, until it reaches a point where C PCopt &lt; * 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Learning k-term DNF formulas using queries and counterexamples. </title> <type> Technical Report YALEU/DCS/RR-559, </type> <institution> Yale University Department of Computer Science, </institution> <year> 1987. </year>
Reference-contexts: We now discuss how to reduce the problem of counting the number of extensions of a partial order on n elements to that of computing the volume of a convex n-dimensional polyhedron given by a separation oracle. The polyhedron built in the reduction will be a subset of the <ref> [0; 1] </ref> n (i.e. the unit hypercube in &lt; n ) where each dimension corresponds to one of the n elements. Observe that any inequality x i &gt; x j defines a halfspace in [0; 1] n . <p> The polyhedron built in the reduction will be a subset of the <ref> [0; 1] </ref> n (i.e. the unit hypercube in &lt; n ) where each dimension corresponds to one of the n elements. Observe that any inequality x i &gt; x j defines a halfspace in [0; 1] n . <p> Let T n be the set of all n! total orders on n elements. Then <ref> [0; 1] </ref> n = t2T n 152 (0,0,0) (0,1,1) (1,0,0) (1,1,1) x From equation (19.1) and the observation that the volumes of the polyhedra formed by each total order is equal, it follows that the volume of the polyhedron defined by any total order is 1=n!.
Reference: [2] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference: [3] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference: [4] <author> Dana Angluin. </author> <title> Equivalence queries and approximate fingerprints. </title> <booktitle> In Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 134-145, </pages> <month> August </month> <year> 1989. </year>
Reference: [5] <author> Dana Angluin, Michael Frazier, and Leonard Pitt. </author> <title> Learning conjunctions of horn clauses. </title> <booktitle> In Proceedings of the Thirty First Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 186-192, </pages> <month> October </month> <year> 1990. </year>
Reference: [6] <author> Dana Angluin, Lisa Hellerstein, and Marek Karpinski. </author> <title> Learning read-once formulas with queries. </title> <type> Technical Report UCB/CSD 89/528, </type> <institution> University of California Berkeley Computer Science Division, </institution> <year> 1989. </year>
Reference: [7] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference: [8] <author> Dana Angluin and Leslie G. Valiant. </author> <title> Fast probabilistic algorithms for hamiltonian circuits and matchings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18(2) </volume> <pages> 155-193, </pages> <month> April </month> <year> 1979. </year>
Reference: [9] <author> Javed A. Aslam and Ronald L. Rivest. </author> <title> Inferring graphs from walks. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 359-370, </pages> <year> 1990. </year>
Reference-contexts: The material presented here comes from the paper, "Inferring Graphs From Walks," by Javed Aslam and Ronald Rivest <ref> [9] </ref>. Consider an undirected, edge-colored multigraph G = (V; E; c) with vertex set V , edge set E, and edge coloring c : E ! . <p> In general, does performing any applicable reduction lead to a consistent and optimal solution? We shall prove that for this problem the order of reductions does not matter. Such questions have been asked in other contexts for other kinds of reductions. See the Aslam, Rivest paper <ref> [9] </ref> for references. 16.3 Preliminaries Before proving that any order of reductions produces a consistent and optimal linear chain, we first need some definitions. Let S be a set. A binary relation on S is a subset of S fi S. Let ! be a binary relation on S. <p> This property can be used to prove the following key theorem. Theorem 16.3 If y is a walk, then the shortest linear chain that can produce y is ^y, the normal form of y with respect to the relation ! HBT . See the Aslam, Rivest paper <ref> [9] </ref> for the details of these proofs. To further simplify the algorithm it would be nice if we could perform the three type of reductions one at a time. We now prove that this approach can be applied.
Reference: [10] <author> Avrim Blum. </author> <title> Learning boolean functions in an infinite attribute space. </title> <booktitle> In Proceedings of the Twenty Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 64-72, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: One can think of attributes not in a particular object's representation as either attributes the object does not possess or as unknown but irrelevant attributes. The material presented here comes from the paper "Learning Boolean Functions in an Infinite Attribute Space," by Avrim Blum <ref> [10] </ref>. Using the notation in Blum: * A is the set of all attributes. For simplicity enumerate the attributes, a 1 , a 2 , a 3 ,... * x A is an instance if jxj &lt; 1.
Reference: [11] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Oc-cam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <month> April </month> <year> 1987. </year> <month> 167 </month>
Reference: [12] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference: [13] <author> M. Dyer, A. Frieze, and R. Kannan. </author> <title> A random polynomial time algorithm for estimating the volumes of convex bodies. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 375-381, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: If a does not satisfy some inequality then reply that a 62 S and return that inequality as the separating hyperplane. Otherwise, if a satisfies all inequalities, reply that a 2 S. Dyer, Frieze and Kannan <ref> [13] </ref> give a fully-polynomial randomized approximation scheme (fpras) to approximate the volume of a polyhedron given a separation oracle. <p> Proof Sketch: We apply the results of Theorem 19.2 using the fpras for counting the number of extensions of a partial order given independently by Dyer, Frieze and Kannan <ref> [13] </ref>, and by Matthews [31]. We know that with probability at least 1 ffi, the number of mistakes is at most lg jC n j i n . Since jC n j = n! the desired result is obtained.
Reference: [14] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference: [15] <author> E. Mark Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference: [16] <author> Sally A. Goldman, Ronald L. Rivest, and Robert E. Schapire. </author> <title> Learning binary relations and total orders. </title> <booktitle> In Proceedings of the Thirtieth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 46-51, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: We will apply this technique to the problem of learning a total order in the mistake bound learning model. The material presented in this lecture comes from the paper, "Learning Binary Relations and Total Orders," by Sally Goldman, Ronald Rivest, and Robert Schapire <ref> [16] </ref>. These scribe notes are an abbreviated version of Chapter 4 of Goldman's thesis [18]. We formalize the problem of learning a total order as follows. The instance space X n = f1; . . . ; ng fi f1; . . . ; ng.
Reference: [17] <author> Sally A. Goldman and Robert H. Sloan. </author> <title> The difficulty of random attribute noise. </title> <type> Technical Report WUCS-91-29, </type> <institution> Washington University, Department of Computer Science, </institution> <month> June </month> <year> 1991. </year>
Reference: [18] <author> Sally Ann Goldman. </author> <title> Learning Binary Relations, Total Orders, and Read-once Formulas. </title> <type> PhD thesis, </type> <institution> MIT Dept. of Electrical Engineering and Computer Science, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: The material presented in this lecture comes from the paper, "Learning Binary Relations and Total Orders," by Sally Goldman, Ronald Rivest, and Robert Schapire [16]. These scribe notes are an abbreviated version of Chapter 4 of Goldman's thesis <ref> [18] </ref>. We formalize the problem of learning a total order as follows. The instance space X n = f1; . . . ; ng fi f1; . . . ; ng.
Reference: [19] <author> Oded Goldreich, Shafi Goldwasser, and Silvio Micali. </author> <title> How to construct random functions. </title> <journal> Journal of the ACM, </journal> <volume> 33(4) </volume> <pages> 792-807, </pages> <year> 1986. </year>
Reference: [20] <author> G.H. Hardy, J.E. Littlewood, and G. Polya. </author> <title> Inequalities. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1959. </year>
Reference-contexts: Definition 18.1 A function f : R ! R is concave (respectively convex) over an interval D of R if for all x 2 D, f 00 (x) 0 (f 00 (x) 0). The following to standard lemmas <ref> [20, 32] </ref> are often useful. Lemma 18.1 Let f be a function from R to R that is concave over some interval D of R. Let q be a natural number, and let x 1 ; x 2 ; . . . ; x q 2 D.
Reference: [21] <author> David Haussler, Michael Kearns, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation. </journal> <note> To appear. A preliminary version is available in Proceedings of the 1988 Workshop on Computational Learning Theory, pages 42-55, </note> <year> 1988. </year>
Reference: [22] <author> Wassily Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> March </month> <year> 1963. </year>
Reference: [23] <author> Gerard Huet. </author> <title> Confluent reductions: Abstract properties and applications to term rewrit-ting systems. </title> <booktitle> In Proceedings of the 18th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 30-45, </pages> <year> 1977. </year>
Reference-contexts: The following well known results (they can be found in Huet <ref> [23] </ref>) will be very useful here. Theorem 16.1 A noetherian relation is confluent if and only if it is locally confluent. Theorem 16.2 If a binary relation is confluent, then the normal form of any element, if it exists, is unique.
Reference: [24] <author> Jeff Kahn and Michael Saks. </author> <title> Balancing poset extensions. </title> <booktitle> Order 1, </booktitle> <pages> pages 113-126, </pages> <year> 1984. </year>
Reference-contexts: We first prove an (n lg n) lower bound on the number of mistakes made by any learning algorithm. We use the following result of Kahn and Saks <ref> [24] </ref>: Given any partial order P that is not a total order there exists an incomparable pair of elements x i ,x j such that 3 number of extensions of P with x i x j number of extensions of P 11 So the adversary can always pick a pair of
Reference: [25] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <month> May </month> <year> 1988. </year> <month> 168 </month>
Reference: [26] <author> Michael Kearns and Leslie Valiant. </author> <title> Cryptographic limitations on learning Boolean for-mulae and finite automata. </title> <booktitle> In Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <month> May </month> <year> 1989. </year>
Reference: [27] <author> Michael J. Kearns. </author> <title> The Computational Complexity of Machine Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference: [28] <author> Nathan Linial, Yishay Mansour, and Ronald L. Rivest. </author> <title> Results on learnability and the Vapnik-Chervonenkis dimension. </title> <booktitle> In Proceedings of the Twenty-Ninth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 120-129, </pages> <month> October </month> <year> 1988. </year>
Reference: [29] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference: [30] <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <booktitle> In Proceedings of the Thirtieth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 256-261, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: This algorithm is shown to be robust with respect to errors in the data. We then discuss other situations in which such a compound algorithm can be applied. The material presented here comes from the paper "The Weighted Majority Algorithm," by Nick Littlestone and Manfred Warmuth <ref> [30] </ref>. Before describing the weighted majority algorithm in detail we briefly discuss some learning problems in which the above scenario applies. The first case is one that we have seen before.
Reference: [31] <author> Peter Matthews. </author> <title> Generating a random linear extension of a partial order. </title> <type> Unpublished manuscript, </type> <year> 1989. </year>
Reference-contexts: From equation (19.3) we see that this fpras for estimating the volume of a polyhedron can be easily applied to estimate the number of extensions of a partial order. (A similar result was given independently by Matthews <ref> [31] </ref>.) 153 19.2 Learning a Total Order In this section we show how to use a fpras to implement a randomized version of the approximate halving algorithm, and apply this result for the problem of learning a total order on a set of n elements. <p> Proof Sketch: We apply the results of Theorem 19.2 using the fpras for counting the number of extensions of a partial order given independently by Dyer, Frieze and Kannan [13], and by Matthews <ref> [31] </ref>. We know that with probability at least 1 ffi, the number of mistakes is at most lg jC n j i n . Since jC n j = n! the desired result is obtained.
Reference: [32] <author> D.S. Mitrinovic. </author> <title> Analytic Inequalities. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Definition 18.1 A function f : R ! R is concave (respectively convex) over an interval D of R if for all x 2 D, f 00 (x) 0 (f 00 (x) 0). The following to standard lemmas <ref> [20, 32] </ref> are often useful. Lemma 18.1 Let f be a function from R to R that is concave over some interval D of R. Let q be a natural number, and let x 1 ; x 2 ; . . . ; x q 2 D.
Reference: [33] <author> Leonard Pitt and Leslie G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference: [34] <author> Leonard Pitt and Manfred Warmuth. </author> <title> Reductions among prediction problems: On the difficulty of predicting automata (extended abstract). </title> <booktitle> In 3rd IEEE Conference on Structure in Complexity Theory, </booktitle> <pages> pages 60-69, </pages> <month> June </month> <year> 1988. </year>
Reference: [35] <author> Ronald L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference: [36] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <booktitle> In Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 411-420, </pages> <month> May </month> <year> 1989. </year>
Reference: [37] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <booktitle> Machine Learning, </booktitle> <year> 1990. </year> <note> Special issue for COLT 89, to appear. A preliminary version is available in Proceedings of the Thirtieth Annual Symposium on Foundations of Computer Science, pages 28-33,1989. </note>
Reference: [38] <author> George Shackelford and Dennis Volper. </author> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> In First Workshop on Computatinal Learning Theory, </booktitle> <pages> pages 97-103, </pages> <address> Cambridge, Mass. August 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [39] <author> Alistair Sinclair. </author> <title> Randomised Algorithms for Counting and Generating Combinatorial Structures. </title> <type> PhD thesis, </type> <institution> University of Edinburgh, Department of Computer Science, </institution> <month> November </month> <year> 1988. </year> <month> 169 </month>
Reference-contexts: In other words, with high probability, R estimates jS x j within a factor of 1 + *. Such a scheme is fully polynomial if it runs in time polynomial in jxj; 1 * ; and lg 1 ffi . For further discussion see Sinclair <ref> [39] </ref>. We now review work on counting the number of linear extensions of a partial order. That is, given a partial order on a set of n elements, the goal is to compute the number of total orders that are linear extensions of the given partial order.
Reference: [40] <author> Robert H. Sloan. </author> <title> Some notes on Chernoff bounds. </title> <type> (Unpublished), </type> <year> 1987. </year>
Reference: [41] <author> Robert H. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In First Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference: [42] <author> Leslie Valiant. </author> <title> The complexity of computing the permanent. </title> <journal> Theoretical Computer Science, </journal> <volume> 8 </volume> <pages> 198-201, </pages> <year> 1979. </year>
Reference-contexts: Such a scheme is polynomial if it runs in time polynomial in jxj. Sometimes exact counting can be done in polynomial time; however, many counting problems are #P-complete and thus assumed to be intractable. (For a discussion of the class #P see Valiant <ref> [42] </ref>.) For many #P-complete problems good approximations are possible.
Reference: [43] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference: [44] <author> Leslie Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of Ninth International Joint Conference on Artificial Intelleigence, </booktitle> <year> 1985. </year> <month> 170 </month>
References-found: 44

