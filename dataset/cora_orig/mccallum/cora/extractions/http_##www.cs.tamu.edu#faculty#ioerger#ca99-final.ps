URL: http://www.cs.tamu.edu/faculty/ioerger/ca99-final.ps
Refering-URL: http://www.cs.tamu.edu/people/magys/conf/publication.html
Root-URL: http://www.cs.tamu.edu
Title: Emotionally Expressive Agents  
Author: Magy Seif El-Nasr Thomas R. Ioerger, John Yen Donald H. House, Frederic I. Parke 
Address: Texas A&M University  
Affiliation: Department of Computer Science Texas A&M University  Visualization Laboratory  
Abstract: The ability to express emotions is important for creating believable interactive characters. To simulate emotional expressions in an interactive environment, an intelligent agent needs both an adaptive model for generating believable responses, and a visualization model for mapping emotions into facial expressions. Recent advances in intelligent agents and in facial modeling have produced effective algorithms for these tasks independently. In this paper, we describe a method for integrating these algorithms to create an interactive simulation of an agent that produces appropriate facial expressions in a dynamic environment. Our approach to combining a model of emotions with a facial model represents a first step towards developing the technology of a truly believable interactive agent, which has a wide range of applications from designing intelligent training systems to video games and animation tools. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Bates. </author> <title> The role of emotion in believable agents. </title> <journal> Communications of ACM, </journal> <volume> 37(7):122125, </volume> <year> 1992. </year>
Reference-contexts: There has been considerable work in the Intelligent Agents' community (with contributions from psychologists) on devising computational models of emotions <ref> [1] </ref>. Some models have demonstrated how to map events into emotional responses based on how they affect the agent's 1 goals [15], while other models have focused more on inter-nal motivational states, such as pain, thirst, and tiredness or fatigue [2]. <p> Creating the illusion that an agent is actually intelligent or alive is useful for a wide range of applications, from computer games to public information kiosks to computer-aided training systems [20, 7]. Emotions are a very important component in enhancing the believability of intelligent agents <ref> [1] </ref>. To create a believable agent, the agent needs to be able to simulate emotional responses to its environment. For example, character modeling studies in theater often focus on developing ways of acting that are effective at conveying emotions [22]. <p> Within the Intelligent Agents community, these models of emotions have been implemented in various agent-based systems. One of the best-known systems that includes emotional agents is the OZ project developed at CMU <ref> [1, 20] </ref>. The OZ project simulated believable emotional and social agents; each agent initially has some predetermined goals, different strategies to achieve each goal, and attitudes towards certain objects in the environment.
Reference: [2] <author> R. Bolles and M. Fanselow. </author> <title> A perceptual defensive recuperative model of fear and pain. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 3:291301, </volume> <year> 1980. </year>
Reference-contexts: Some models have demonstrated how to map events into emotional responses based on how they affect the agent's 1 goals [15], while other models have focused more on inter-nal motivational states, such as pain, thirst, and tiredness or fatigue <ref> [2] </ref>. <p> Most models treat emotions as internal states of an agent which can be activated to varying degrees. Event-appraisal models [15, 21] study the effect of events on emotions by evaluating how they impact the agent's goals. Other models link emotions to motivational states, such as pain or hunger <ref> [2] </ref>. Still other models have examined the connection between emotions and physiological conditions, which is useful for various applications such as affective wearable computers [19], or studied emotions from the perspective of neurobiology [3]. Within the Intelligent Agents community, these models of emotions have been implemented in various agent-based systems. <p> Thus, to trigger an emotion, we need both the event desirability measure and the expectation measure. The mixture of triggered emotions will then be filtered to produce a coherent emotional state. The filtering process uses a variation of Bolles and Fanslow's <ref> [2] </ref> model to produce a consistent set of emotional intensities that co-exist. Filtering the triggered emotions involves a variety of com plex suppression and enhancement relationships involving motivational states. <p> Filtering the triggered emotions involves a variety of com plex suppression and enhancement relationships involving motivational states. For example, sadness tends to override happiness when the intensities are similar, and pain tends to (temporarily) inhibit the degree of fear <ref> [2] </ref>. The filtered emotional state is used to select a consistent emotional behavior, such as laughing or crying. These are determined according to the emotional state and the current situation using fuzzy rules.
Reference: [3] <author> A. Damasio. </author> <title> Descartes' Error: Emotion, Reason, and the Human Brain. </title> <address> New York: Putnam, </address> <year> 1994. </year>
Reference-contexts: Other models link emotions to motivational states, such as pain or hunger [2]. Still other models have examined the connection between emotions and physiological conditions, which is useful for various applications such as affective wearable computers [19], or studied emotions from the perspective of neurobiology <ref> [3] </ref>. Within the Intelligent Agents community, these models of emotions have been implemented in various agent-based systems. One of the best-known systems that includes emotional agents is the OZ project developed at CMU [1, 20].
Reference: [4] <author> P. Ekman. </author> <title> The argument and evidence about universals in facial expressions of emotion. </title> <editor> In H. Wagner and A. Mon-stead, editors, </editor> <booktitle> Handbook of Social Psychophysiology, </booktitle> <pages> pages 143146. </pages> <publisher> Wiley, </publisher> <year> 1989. </year> <month> 9 </month>
Reference-contexts: Faces provide good interface elements in computers because they take advantage of people's capability for recognizing expressions, and hence can be used to effectively communicate certain types of information, particularly emotions <ref> [4] </ref>. Believable agents could enhance interactive applications such as information kiosks by making them more accessible to the general public. There are also many applications in training and education that could ben fl Correspondence may be addressed to: Magy Seif El-Nasr, 301 H.R. <p> There are specific facial feature postures that correspond to these states [8], as shown in Figure 4 and summarized below. The first five of these fear, sadness, anger, joy, and laughter correspond to four of the six universal facial expressions <ref> [4] </ref>, laughter being an extreme expression of joy. The last four listed pain, tiredness, thirst and hunger are physical rather than emotional states. Pain and tiredness have specific corresponding facial expressions.
Reference: [5] <author> M. El-Nasr. </author> <title> Modeling emotion dynamics in intelligent agents. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, Texas A&M University, College Station, TX, </institution> <year> 1998. </year>
Reference-contexts: In our recent work, we have developed a new computational model of emotions in intelligent agents that can be used to generate believable behavior in a dynamic environment using several learning algorithms <ref> [5, 6] </ref>. In this paper, we integrate our model of emotional response with a model for facial expressions to produce a simulated agent that is capable of both generating and expressing believable emotions in real time. <p> To generate a believable behavior for an interactive agent, the model of emotions must take these types of adaptation into account. In our previous work, we have developed a model of emotions called FLAME, for Fuzzy Logic Adaptive Model of Emotions <ref> [5] </ref>. FLAME uses fuzzy logic to represent the relationships among events, goals, and emotions in a flexible way that helps produces smooth transitions in behavior. FLAME also incorporates several learning algorithms that it uses for the agent to adapt to its environment. <p> In FLAME, we also include feedback mechanisms to model time-dependent effects (such as moods), and we decay emotional intensities over time to improve the realism of the simulation. For more details on the emotional component of the FLAME model, see <ref> [5] </ref>. 2.4 Learning Algorithms in FLAME Learning and memory have a major impact on emotional behavior [11]. In FLAME, we incorporated algorithms for learning several different types of things about the environment, including the likelihood of certain events to occur in any given situation, and the desirability of various actions.
Reference: [6] <author> M. El-Nasr, T. Ioerger, and J. Yen. PETEEI: </author> <title> A pet with evolving emotional intelligence. </title> <booktitle> In Third International Conference on Autonomous Agents, </booktitle> <year> 1999. </year>
Reference-contexts: In our recent work, we have developed a new computational model of emotions in intelligent agents that can be used to generate believable behavior in a dynamic environment using several learning algorithms <ref> [5, 6] </ref>. In this paper, we integrate our model of emotional response with a model for facial expressions to produce a simulated agent that is capable of both generating and expressing believable emotions in real time. <p> Variants of these formulas can be used to handle the inherent non-determinism introduced by the human in the interaction <ref> [6] </ref>. Using this approach, the agent will be able to formulate its expectations of rewards according to the situations it faces.
Reference: [7] <author> C. Elliot and J. Brzezinki. </author> <title> Autonomous agents as synthetic character. </title> <journal> AI Magazine, </journal> <volume> 19(2):1330, </volume> <year> 1998. </year>
Reference-contexts: 1 Introduction The ability to simulate believable characters or agents in an interactive system is a challenging problem that has applications in many areas <ref> [20, 7] </ref>. These types of agents present a visual interface, typically a human face, which is familiar and non-intimidating, making users feel comfortable when interacting with a computer. <p> A particularly important capability for systems that interact with humans is believability. Creating the illusion that an agent is actually intelligent or alive is useful for a wide range of applications, from computer games to public information kiosks to computer-aided training systems <ref> [20, 7] </ref>. Emotions are a very important component in enhancing the believability of intelligent agents [1]. To create a believable agent, the agent needs to be able to simulate emotional responses to its environment.
Reference: [8] <author> G. Faigin. </author> <title> The Artist's Complete Guide to Facial Expression. </title> <address> Watson-Guptill, New York, </address> <year> 1990. </year>
Reference-contexts: The expressive capabilities of this style are a good match to the relatively simple emotional model for the baby. This facial model supports the range of expressions that correspond to the physical and emotional states of the baby. There are specific facial feature postures that correspond to these states <ref> [8] </ref>, as shown in Figure 4 and summarized below. The first five of these fear, sadness, anger, joy, and laughter correspond to four of the six universal facial expressions [4], laughter being an extreme expression of joy.
Reference: [9] <author> N. Jennings, K. Sycara, and M. Wooldridge. </author> <title> A roadmap of agent research and development. </title> <booktitle> Journal of Autonomous Agents and Multi-Agent Systems, </booktitle> <address> 1:275306, </address> <year> 1998. </year>
Reference-contexts: capable of capturing the intent of the character designer, and allowing the character's personality to be expressed as the designer intended, even in complex interactive situations such as in a video game. 2 Modeling Emotional Dynamics 2.1 Believable Agents Intelligent agents are software programs that exhibit autonomous, goal-oriented, adaptive behavior <ref> [9] </ref>. Generally, agents receive inputs from the external environment and take actions to achieve their goals.
Reference: [10] <author> L. Kaelbling, M. Littman, and A. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4:237285, </volume> <year> 1996. </year>
Reference-contexts: Identifying this connection has often been considered to be a very difficult task [20]. However, in FLAME, we use reinforcement learning <ref> [10] </ref> to learn how various events impact the agent's goals, based on experiences accumulated over sequences of events. In reinforcement learning, the agent represents the problem space using a table of numbers called Q-values which are associated with each state-action pair. The table is initially filled with random values.
Reference: [11] <author> J. LeDoux. </author> <title> The Emotional Brain. </title> <address> New York: </address> <publisher> Simon & Schuster, </publisher> <year> 1996. </year>
Reference-contexts: For more details on the emotional component of the FLAME model, see [5]. 2.4 Learning Algorithms in FLAME Learning and memory have a major impact on emotional behavior <ref> [11] </ref>. In FLAME, we incorporated algorithms for learning several different types of things about the environment, including the likelihood of certain events to occur in any given situation, and the desirability of various actions.
Reference: [12] <author> N. Magnenat Thalmann, N. Primeau, and D. Thalmann. </author> <title> Abstract muscle action procedures for human face animation. Visual Computer, </title> <address> 3(5):290297, </address> <year> 1988. </year>
Reference-contexts: This control is usually in terms of a set of facial parameters. These parameters may mimic the anatomical actions of the facial muscles [25] or may directly manipulate feature postures using pseudo-muscle <ref> [12] </ref> or ad hoc parameterizations [16]. The control parameterizations used for these facial models do not usually correspond directly to the emotional state of the character. Multiple facial parameters contribute to the visual expressions associated with each emotional or physical state.
Reference: [13] <author> N. Magnenat Thalmann and D. Thalmann. </author> <title> Computer animation. </title> <editor> In A. Tucker, editor, </editor> <booktitle> The Computer Science and Engineering Handbook, </booktitle> <pages> pages 13001318. </pages> <publisher> CRC Press, </publisher> <year> 1996. </year>
Reference-contexts: Visualization of faces and expressions is often employed in animations and computer games. From inverse kinematics to artificially-generated textures to simulations of fabric dynamics, there has been a powerful drive to automate the animation of various subjects with increasing speed and realism <ref> [13] </ref>, and incorporating an emotional model in characters is another key step toward creating effective behavior-based models. An important aspect of systems that incorporate believable agents is the ability to simulate realistic emotional responses.
Reference: [14] <author> T. Mitchell. </author> <title> Machine Learning. </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1997. </year>
Reference-contexts: The agent may obtain a reward R i from the environment for this action (such as being told Good Baby). After the agent makes the transition, it updates the corresponding Q-value in the table using the following formula <ref> [14] </ref>: Q (S; A i ) = R i + fl max A j Q (S i ; A j ) to reflect the immediate reward plus the maximum expected reward achievable from the new state based on its Q-values, discounted by a constant factor fl.
Reference: [15] <author> A. Ortony, G. Clore, and A. Collins. </author> <title> The Cognitive Structure of Emotions. </title> <publisher> Cambridge: Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: There has been considerable work in the Intelligent Agents' community (with contributions from psychologists) on devising computational models of emotions [1]. Some models have demonstrated how to map events into emotional responses based on how they affect the agent's 1 goals <ref> [15] </ref>, while other models have focused more on inter-nal motivational states, such as pain, thirst, and tiredness or fatigue [2]. <p> Most models treat emotions as internal states of an agent which can be activated to varying degrees. Event-appraisal models <ref> [15, 21] </ref> study the effect of events on emotions by evaluating how they impact the agent's goals. Other models link emotions to motivational states, such as pain or hunger [2]. <p> When the agent 2 perceives an event in the environment, it is then evaluated according to the agent's goals, standards and attitudes. After the event is evaluated, special rules are used to produce an emotion with a specific intensity. These rules are based on Ortony et al.'s event-appraisal model <ref> [15] </ref>. The emotions triggered are then mapped, according to their intensity, to a specific behavior. This behavior is then expressed using text or animation [20]. <p> The calculated desirability measure will be passed to an event appraisal model to determine the emotional state of the agent. The event appraisal model used in FLAME is based on Ortony et al.'s model <ref> [15] </ref>. The appraisal model is essentially a set of rules that produces emotions according to the agent's expectations and the desirability of expected or perceived events. To illustrate how the appraisal model works, we use Joy as an example.
Reference: [16] <author> F. Parke. </author> <title> A parametric model for human faces. </title> <type> Technical Report UTEC-CSc-75-047, </type> <institution> University of Utah, </institution> <address> Salt Lake City, </address> <year> 1975. </year>
Reference-contexts: This control is usually in terms of a set of facial parameters. These parameters may mimic the anatomical actions of the facial muscles [25] or may directly manipulate feature postures using pseudo-muscle [12] or ad hoc parameterizations <ref> [16] </ref>. The control parameterizations used for these facial models do not usually correspond directly to the emotional state of the character. Multiple facial parameters contribute to the visual expressions associated with each emotional or physical state.
Reference: [17] <author> F. Parke and K. Waters. </author> <title> Computer Facial Animation. A.K. </title> <publisher> Peters: </publisher> <address> Wellesley, Mass., </address> <year> 1996. </year>
Reference-contexts: Producing emotional responses requires both the ability to generate facial expressions, and a model that the agent can use for synthesizing appropriate emotions in a dynamic environment. There has been a great deal of work done on generating believable facial expressions <ref> [17] </ref>. A number of basic facial motions, often derived from unique muscle groups, have been identified, such as a specific twist of the eyebrow or pucker of the lips, which can be combined to form most facial expressions. <p> The development of expressive visual facial models has progressed over the last 25 years and is now quite well understood <ref> [17] </ref>. The basic idea is to manipulate a geometric representation of the face so that it mimics the expressive feature postures of the human face. The facial geometry is usually a three-dimensional surface or set of surfaces that conform to the face of interest.
Reference: [18] <author> K. Perlin and A. Goldberg. Improv: </author> <title> A system for scripting interactive actors in virtual worlds. </title> <booktitle> In Proceedings of SIGGRAPH '96, </booktitle> <year> 1996. </year>
Reference-contexts: There have been a number of other attempts to automate the animation of facial (and other) expressions, such the Improv project by Perlin <ref> [18] </ref>. These systems often provide the capability of generating a range of expressions or behaviors, but do not specify a controller for dynamically generating believable responses to events in real-time (other than through simple scripts and triggers).
Reference: [19] <author> R. </author> <title> Picard. Affective Computing. </title> <publisher> Cambridge: MIT press, </publisher> <year> 1997. </year>
Reference-contexts: Other models link emotions to motivational states, such as pain or hunger [2]. Still other models have examined the connection between emotions and physiological conditions, which is useful for various applications such as affective wearable computers <ref> [19] </ref>, or studied emotions from the perspective of neurobiology [3]. Within the Intelligent Agents community, these models of emotions have been implemented in various agent-based systems. One of the best-known systems that includes emotional agents is the OZ project developed at CMU [1, 20].
Reference: [20] <author> W. Reilly. </author> <title> Believable social and emotional agents. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction The ability to simulate believable characters or agents in an interactive system is a challenging problem that has applications in many areas <ref> [20, 7] </ref>. These types of agents present a visual interface, typically a human face, which is familiar and non-intimidating, making users feel comfortable when interacting with a computer. <p> A particularly important capability for systems that interact with humans is believability. Creating the illusion that an agent is actually intelligent or alive is useful for a wide range of applications, from computer games to public information kiosks to computer-aided training systems <ref> [20, 7] </ref>. Emotions are a very important component in enhancing the believability of intelligent agents [1]. To create a believable agent, the agent needs to be able to simulate emotional responses to its environment. <p> Within the Intelligent Agents community, these models of emotions have been implemented in various agent-based systems. One of the best-known systems that includes emotional agents is the OZ project developed at CMU <ref> [1, 20] </ref>. The OZ project simulated believable emotional and social agents; each agent initially has some predetermined goals, different strategies to achieve each goal, and attitudes towards certain objects in the environment. <p> These rules are based on Ortony et al.'s event-appraisal model [15]. The emotions triggered are then mapped, according to their intensity, to a specific behavior. This behavior is then expressed using text or animation <ref> [20] </ref>. One of the limitations of OZ is that it treats the outward behaviors in a crisp fashion, such that, with sufficient activation, the agent either becomes happy or sad, without intermediate states. <p> Therefore, the link between events and the goals they affect is no longer a simple one-to-one relationship, but rather we need to identify more general relationships between events and goals through a sequence of actions. Identifying this connection has often been considered to be a very difficult task <ref> [20] </ref>. However, in FLAME, we use reinforcement learning [10] to learn how various events impact the agent's goals, based on experiences accumulated over sequences of events. In reinforcement learning, the agent represents the problem space using a table of numbers called Q-values which are associated with each state-action pair.
Reference: [21] <author> I. Roseman, J. P.E., and M. Spindel. </author> <title> Appraisals of emotion-eliciting events: Testing a theory of discrete emotions. </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> 59(5):899915, </volume> <year> 1990. </year>
Reference-contexts: Most models treat emotions as internal states of an agent which can be activated to varying degrees. Event-appraisal models <ref> [15, 21] </ref> study the effect of events on emotions by evaluating how they impact the agent's goals. Other models link emotions to motivational states, such as pain or hunger [2]. <p> Velasquez [24] has also implemented an agent that simulates emotions, based on the event-appraisal model by Roseman et al. <ref> [21] </ref>, in a robot named Yuppy. One aspect that is not frequently addressed in these computational models of emotions is how to adapt to an environment dynamically. <p> The work reported in this paper is more similar to the work of Velasquez [23]. Velasquez describes the simulation of an interactive baby, called Simon, with various facial expressions. The implementation of Simon is based on Roseman et al.'s <ref> [21] </ref> event-appraisal model, which is used to generate emotions and then select a corresponding face image to display. However, we extend Velasquez's approach by replacing his single-mode emotional response with one that allows for a multidimensional complex of physical and emotional states.
Reference: [22] <author> F. Thomas and O. Johnston. </author> <title> The Illusion of Life. </title> <address> New York: </address> <publisher> Abbeville Press, </publisher> <year> 1981. </year>
Reference-contexts: To create a believable agent, the agent needs to be able to simulate emotional responses to its environment. For example, character modeling studies in theater often focus on developing ways of acting that are effective at conveying emotions <ref> [22] </ref>. Thus, to be truly convincing, the agent must have the capability of generating and expressing emotions. Many studies have been done on visualizing static expressions, which are sufficient for many applications in the entertainment industry, such as movies. <p> Painstaking attention is paid to how situations would affect the character's emotional state, and how the character would react in terms of facial expression, body language and overt actions <ref> [22] </ref>. For instance, compare the mild manner and philosophical reactions of Mickey Mouse to the hair-trigger highly emotional reactions of Donald Duck.
Reference: [23] <author> J. Velasquez. </author> <title> Modeling emotions and other motivations in synthetic agents. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1015, </pages> <year> 1997. </year>
Reference-contexts: The work reported in this paper is more similar to the work of Velasquez <ref> [23] </ref>. Velasquez describes the simulation of an interactive baby, called Simon, with various facial expressions. The implementation of Simon is based on Roseman et al.'s [21] event-appraisal model, which is used to generate emotions and then select a corresponding face image to display.
Reference: [24] <author> J. Velasquez. </author> <title> When robots weep: Emotional memories and decision-making. </title> <booktitle> In Proceedings of the Fifteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 7075, </pages> <year> 1998. </year>
Reference-contexts: This behavior is then expressed using text or animation [20]. One of the limitations of OZ is that it treats the outward behaviors in a crisp fashion, such that, with sufficient activation, the agent either becomes happy or sad, without intermediate states. Velasquez <ref> [24] </ref> has also implemented an agent that simulates emotions, based on the event-appraisal model by Roseman et al. [21], in a robot named Yuppy. One aspect that is not frequently addressed in these computational models of emotions is how to adapt to an environment dynamically.
Reference: [25] <author> K. Waters. </author> <title> A muscle model for animating three-dimensional facial expressions. </title> <journal> Computer Graphics, </journal> <volume> 21(4):1724, </volume> <year> 1987. </year>
Reference-contexts: The shape of the facial surfaces or lines are then controlled to take on the feature postures which represent the various facial expressions. This control is usually in terms of a set of facial parameters. These parameters may mimic the anatomical actions of the facial muscles <ref> [25] </ref> or may directly manipulate feature postures using pseudo-muscle [12] or ad hoc parameterizations [16]. The control parameterizations used for these facial models do not usually correspond directly to the emotional state of the character. Multiple facial parameters contribute to the visual expressions associated with each emotional or physical state.
Reference: [26] <author> J. Yen and R. Langari. </author> <title> Fuzzy Logic: </title> <booktitle> Intelligence, Control, and Information. </booktitle> <address> Upper Saddle River, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1998. </year>
Reference-contexts: The various antecedents may be combined using the SupMin rule, in which the satisfaction of the rule is determined by the minimum over all the antecedents, and strength of a conclusion is determined by taking the maximum over all relevant rules <ref> [26] </ref>. The calculated desirability measure will be passed to an event appraisal model to determine the emotional state of the agent. The event appraisal model used in FLAME is based on Ortony et al.'s model [15].
References-found: 26

