URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P637.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/preprints.htm
Root-URL: http://www.mcs.anl.gov
Title: Automatic Differentiation of a Parallel Molecular Dynamics Application  
Author: P. Hovland C. Bischof L. Roh 
Date: December 31, 1996  
Abstract: The ADIC and ADIFOR automatic differentiation tools have proven useful for obtaining the derivatives needed in many scientific applications written in Fortran 77 or ANSI C. But many new scientific programs are written for or ported to parallel platforms to achieve maximal performance. We provide an overview of our approach to the complex task of applying automatic differentiation techniques to parallel programming environments, especially as applied to a parallel molecular dynamics application written in C++ with PVM message passing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Bischof, A. Carle, G. Corliss, A. Griewank, and P. Hovland, ADIFOR: </author> <title> Generating derivative codes from Fortran programs, </title> <booktitle> Scientific Programming, 1 (1992), </booktitle> <pages> pp. 11-29. </pages>
Reference: [2] <author> C. Bischof, A. Carle, P. Khademi, and A. Mauer, ADIFOR 2.0: </author> <title> Automatic differentiation of Fortran 77 programs, </title> <journal> IEEE Computational Science & Engineering, </journal> <volume> 3 (1996), </volume> <pages> pp. 18-32. </pages>
Reference-contexts: We conclude with a synopsis of our research and a description of planned future work. 2 Automatic Differentiation Automatic differentiation can be used to transform a program for computing some mathematical function into a new program capable of computing not only the function, but also the derivatives of that function <ref> [2, 3] </ref>. Automatic differentiation relies upon the fact that all programs, no matter how complicated, use a limited set of elementary operations and functions, as defined by the language. The function computed by the program is simply the composition of these elementary functions. <p> While this example is very simple, automatic differentiation can be applied to complex programs of arbitrary length. The ADIC tool has processed programs of over 10,000 lines and the ADIFOR tool has been applied to programs of over 100,000 lines <ref> [2] </ref>. 3 NAMD NAMD is a parallel, object-oriented molecular dynamics program designed for high performance molecular dynamics simulations of large biomolecular systems [12]. <p> Finally, the derivative matrices being computed are sparse. Tools such as ADIC and ADIFOR provide support for automatic exploitation of sparsity, without prior knowledge of the sparsity structure of the derivative matrices <ref> [2, 3] </ref>. 4 AD of NAMD NAMD is written in C++ with PVM message passing. A port to MPI is planned for the future.
Reference: [3] <author> C. Bischof, L. Roh, and A. Mauer, </author> <title> ADIC | An extensible automatic differentiation tool for ANSI-C, </title> <type> Preprint ANL/MCS-P626-1196, </type> <year> 1996. </year>
Reference-contexts: We conclude with a synopsis of our research and a description of planned future work. 2 Automatic Differentiation Automatic differentiation can be used to transform a program for computing some mathematical function into a new program capable of computing not only the function, but also the derivatives of that function <ref> [2, 3] </ref>. Automatic differentiation relies upon the fact that all programs, no matter how complicated, use a limited set of elementary operations and functions, as defined by the language. The function computed by the program is simply the composition of these elementary functions. <p> Finally, the derivative matrices being computed are sparse. Tools such as ADIC and ADIFOR provide support for automatic exploitation of sparsity, without prior knowledge of the sparsity structure of the derivative matrices <ref> [2, 3] </ref>. 4 AD of NAMD NAMD is written in C++ with PVM message passing. A port to MPI is planned for the future.
Reference: [4] <author> I. Foster, R. Olson, and S. Tuecke, </author> <title> Programming in Fortran M, </title> <type> Tech. Rep. ANL-93/26, Rev. 1, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> October </month> <year> 1993. </year>
Reference: [5] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam, </author> <title> PVM Parallel Virtual Machine: A Users' Guide and Tutorial for Network Parallel Computing, </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1994. </year>
Reference: [6] <author> A. Griewank, </author> <title> On automatic differentiation, </title> <booktitle> in Mathematical Programming: Recent Developments and Applications, </booktitle> <address> Amsterdam, 1989, </address> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 83-108. </pages>
Reference-contexts: Thus, we can compute the partial derivatives of the elementary functions using formulas obtained via table lookup, then compute the overall derivatives using the chain rule. This process can be completely automated, and is thus termed automatic differentiation <ref> [6] </ref>. For example, consider the following program to compute the function y = f (x), where f (x) = (sin (x) x)=x. B = sqrt (X) C = A * B Using automatic differentiation, we can generate code to compute y and dy=dx.
Reference: [7] <author> A. Griewank, D. Juedes, and J. Utke, ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 22 (1996), </volume> <pages> pp. 131-167. </pages>
Reference: [8] <author> W. Gropp, E. Lusk, and A. Skjellum, </author> <title> Using MPI Portable Parallel Programming with the Message Passing Interface, </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1994. </year>
Reference: [9] <author> P. Hovland, C. Bischof, and L. Roh, </author> <title> Automatic differentiation of parallel reduction operations, </title> <type> Preprint ANL/MCS-P632-1296, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: In general, there are other issues that may need to be addressed in applying AD to parallel programs. In addition to preserving variable-derivative matrix associations, we should correctly differentiate reduction operations and attempt to avoid unnecessary derivative computations. These issues are discussed elsewhere <ref> [9, 10] </ref>. 4.3 AD of NAMD Due to some of the limitations mentioned in Section 4.1, we were unable to process NAMD in its entirety. <p> We have addressed several of the important issues in this task, including maintaining the association between derivative vectors and variables, improving efficiency through intertask dependence analysis, properly differentiating reduction operations, and utilizing the added potential for parallelism created by the automatic differentiation process <ref> [9, 10] </ref>. Based on the experience gained from the development of prototype AD tools for Fortran M and Fortran with MPI message passing as well as the application of AD to NAMD, we plan to build a tool for the automatic differentiation of C/C++ with PVM/MPI message passing.
Reference: [10] <author> P. D. Hovland, </author> <title> Automatic Differentiation of Parallel Programs, </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign. </institution> <note> In preparation. </note>
Reference-contexts: In general, there are other issues that may need to be addressed in applying AD to parallel programs. In addition to preserving variable-derivative matrix associations, we should correctly differentiate reduction operations and attempt to avoid unnecessary derivative computations. These issues are discussed elsewhere <ref> [9, 10] </ref>. 4.3 AD of NAMD Due to some of the limitations mentioned in Section 4.1, we were unable to process NAMD in its entirety. <p> We have addressed several of the important issues in this task, including maintaining the association between derivative vectors and variables, improving efficiency through intertask dependence analysis, properly differentiating reduction operations, and utilizing the added potential for parallelism created by the automatic differentiation process <ref> [9, 10] </ref>. Based on the experience gained from the development of prototype AD tools for Fortran M and Fortran with MPI message passing as well as the application of AD to NAMD, we plan to build a tool for the automatic differentiation of C/C++ with PVM/MPI message passing.
Reference: [11] <author> M. Lopez-Marcos, J. M. Sanz-Serna, and R. D. Skeel, </author> <title> Explicit symplectic integrators using Hessian-vector products, </title> <note> SIAM J. Sci. Comput., 18 (1997). To appear. </note>
Reference-contexts: For appropriate values of k, the error due to holding the forces constant for a few time steps is small compared to the errors incurred from using a finite timestep. The developers of NAMD hope to improve the integrator using an approach that requires Hessian-vector products <ref> [11] </ref>. The Hessian required corresponds to the derivatives of forces with respect to position. The availability of these correction terms is expected to increase the smallest time step by a factor of nearly three. Another proposed method would increase performance by decreasing the frequency of long-range force evaluations.
Reference: [12] <author> M. Nelson, W. Humphrey, A. Gursoy, A. Dalke, L. Kale, R. D. Skeel, and K. Schulten, </author> <title> NAMD a parallel, object-oriented molecular dynamics program, Journal of Supercomputing Applications and High Performance Computing. </title> <publisher> In Press. </publisher>
Reference-contexts: The ADIC tool has processed programs of over 10,000 lines and the ADIFOR tool has been applied to programs of over 100,000 lines [2]. 3 NAMD NAMD is a parallel, object-oriented molecular dynamics program designed for high performance molecular dynamics simulations of large biomolecular systems <ref> [12] </ref>. Important features include scalable parallelism, an efficient implementation of full electrostatics, modifiability, portability, and compatibility with X-PLOR (a program for determining three-dimensional structures from crystallographic diffraction or NMR data). Full electrostatics are computed using the Distributed Parallel Multipole Tree Algorithm (DPMTA) developed at Duke University [13].
Reference: [13] <author> W. T. Rankin and J. A. Board Jr., </author> <title> A portable distributed implementation of the parallel multipole tree algorithm, </title> <booktitle> in Proceedings of the Fourth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <address> Los Alamitos, CA, 1995, </address> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 17-22. </pages>
Reference-contexts: Important features include scalable parallelism, an efficient implementation of full electrostatics, modifiability, portability, and compatibility with X-PLOR (a program for determining three-dimensional structures from crystallographic diffraction or NMR data). Full electrostatics are computed using the Distributed Parallel Multipole Tree Algorithm (DPMTA) developed at Duke University <ref> [13] </ref>. NAMD is written in C++, using an object-oriented and highly modular design. This design facilitates modification of algorithms and techniques. Communication in NAMD is accomplished via PVM, making it portable across a wide range of computing platforms.
Reference: [14] <author> N. Rostaing, S. Dalmas, and A. Galligo, </author> <title> Automatic differentiation in Odyssee, </title> <booktitle> Tellus, 45a (1993), </booktitle> <pages> pp. 558-568. </pages>
References-found: 14

