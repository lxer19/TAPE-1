URL: file://ftp.t.soka.ac.jp/users/unemi/papers/ALifeIV.ps.Z
Refering-URL: http://ai.iit.nrc.ca/baldwin/bibliography.html
Root-URL: 
Email: e-mail: funemi,rorry,nobu,toshi,papino,yasg@iss.soka.ac.jp  
Title: Evolutionary Differentiation of Learning Abilities a case study on optimizing parameter values in Q-learning by
Author: Tatsuo Unemi Masahiro Nagayoshi, Nobumasa Hirayama, Toshiaki Nade, Kiyoshi Yano, and Yasuhiro Masujima 
Address: 1-236, Tangi-machi, Hachioji, 192 Tokyo, JAPAN  
Affiliation: Department of Information Systems Science, Soka University  
Abstract: This paper describes the first stage of our study on evolution of learning abilities. We use a simple maze exploration problem designed by R. Sut-ton as the task of each individual, and encode the inherent learning parameters on the genome. The learning architecture we use is a one step Q-learning using look-up table, where the inherent parameters are initial Q-values, learning rate, discount rate of rewards, and exploration rate. Under the fitness measure proportioning to the number of times it achieves at the goal in the later half of life, learners evolve through a genetic algorithm. The results of computer simulation indicated that learning ability emerge when the environment changes every generation, and that the inherent map for the optimal path can be acquired when the environment doesn't change. These results suggest that emergence of learning ability needs environmental change faster than alternate generation. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D. and M. Littman. </author> <year> 1992. </year> <title> Interactions between Learning and Evolution. In Artificial Life II, edited by C. </title> <editor> G. Langton, C. Taylor, J. D. Farmer, </editor> <publisher> and S. </publisher>
Reference-contexts: On the third combination, evolution of learners, Todd and Miller have been pursuing evolutionary process to organize associative neural networks that learn by a simple Hebbian rule (Todd and Miller, 1990). Ackley and Littman mentioned genetic acquisition of evaluation network in a neural network based reinforcement learning method <ref> (Ackley and Littman, 1992) </ref> and a distributed Lamarckian evolution (Ackley and Littman, 1992). Their studies provided us fruitful suggestion to understand a process of emergence of intelligent creatures. <p> Ackley and Littman mentioned genetic acquisition of evaluation network in a neural network based reinforcement learning method <ref> (Ackley and Littman, 1992) </ref> and a distributed Lamarckian evolution (Ackley and Littman, 1992). Their studies provided us fruitful suggestion to understand a process of emergence of intelligent creatures.
Reference: <editor> Rasmussen. </editor> <publisher> Addison-Wesley, </publisher> <pages> 487-509. </pages>
Reference: <author> Ackley, D. and M. Littman. </author> <year> 1992. </year> <title> A Case of Distributed Lamarckian Evolution. </title> <booktitle> Presented at the Third International Workshop on Artificial Life, </booktitle> <address> Santa Fe, NM. </address>
Reference-contexts: On the third combination, evolution of learners, Todd and Miller have been pursuing evolutionary process to organize associative neural networks that learn by a simple Hebbian rule (Todd and Miller, 1990). Ackley and Littman mentioned genetic acquisition of evaluation network in a neural network based reinforcement learning method <ref> (Ackley and Littman, 1992) </ref> and a distributed Lamarckian evolution (Ackley and Littman, 1992). Their studies provided us fruitful suggestion to understand a process of emergence of intelligent creatures. <p> Ackley and Littman mentioned genetic acquisition of evaluation network in a neural network based reinforcement learning method <ref> (Ackley and Littman, 1992) </ref> and a distributed Lamarckian evolution (Ackley and Littman, 1992). Their studies provided us fruitful suggestion to understand a process of emergence of intelligent creatures.
Reference: <author> Nade, T., M, Nagayoshi, N, Hirayama, Y. Masujima, K. Yano, and T. Unemi. </author> <year> 1994. </year> <title> A Simple Development System on 3D Euclidean Space and its Evolution. IPSJ SIG Notes 94:20:25-30, in Japanese. </title>
Reference-contexts: One of the methods for fruitful studies is on combination of two or three of these layers, such as evolutionary development system, collective behavior of learners. We already finished the first stage of above two kinds of combinations <ref> (Nade et al, 1994, Unemi, 1993) </ref>. On the third combination, evolution of learners, Todd and Miller have been pursuing evolutionary process to organize associative neural networks that learn by a simple Hebbian rule (Todd and Miller, 1990).
Reference: <author> Sutton, R. S. </author> <year> 1990. </year> <title> Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> 216-224. </pages>
Reference-contexts: The rest sections of this paper describe the specifications of the simulator, the experiments, and their results. 2 Task for individual The task for each individual is a simple maze exploration designed by Sutton as a testbed for his Dyna learning architecture <ref> (Sutton, 1990) </ref>. The maze an individual creature lives in is a two-dimensional grid world of nine columns and six rows. Some of the grids are occupied by obstacles so the individual cannot position there. Figure 1 shows a typical map of the maze used in the experiments described later.
Reference: <author> Todd, P. M. and G. F. Miller. </author> <year> 1990. </year> <title> Exploring Adaptive Agency II: Simulating the Evolution of Associative Learning. </title> <booktitle> From Animals to Animats Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> 306-315. </pages>
Reference-contexts: We already finished the first stage of above two kinds of combinations (Nade et al, 1994, Unemi, 1993). On the third combination, evolution of learners, Todd and Miller have been pursuing evolutionary process to organize associative neural networks that learn by a simple Hebbian rule <ref> (Todd and Miller, 1990) </ref>. Ackley and Littman mentioned genetic acquisition of evaluation network in a neural network based reinforcement learning method (Ackley and Littman, 1992) and a distributed Lamarckian evolution (Ackley and Littman, 1992). Their studies provided us fruitful suggestion to understand a process of emergence of intelligent creatures.
Reference: <author> Unemi, T. </author> <year> 1993. </year> <title> Collective Behavior of Reinforcement Learning Agents. </title> <booktitle> Proceedings of the 1993 IEEE/Nagoya University WWW on Learning and Adaptive System, </booktitle> <pages> 92-97. </pages>
Reference: <author> Watkins, C. J. C. H. and P. Dayan. </author> <year> 1993. </year> <note> Technical Note Q-Learning. In Reinforcement Learning, edited by R. </note> <editor> S. Sutton, </editor> <publisher> Kluwer Academic Pub., </publisher> <month> 55-68. </month> <title> changing environment. </title>
References-found: 8

