URL: ftp://ftp.cwi.nl/pub/pdg/jmp.ps
Refering-URL: http://www.cwi.nl/~pdg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Model Selection based on Minimum Description Length  
Author: P. Grunwald 
Note: submitted to Journal of Mathematical Psychology. Please do not cite without permission. `It is vain to do with more what can be done with fewer.' William of Ockham (1290?-1349?)  
Address: P.O. Box 94079 1009 AB Amsterdam The Netherlands  
Affiliation: CWI  
Abstract-found: 0
Intro-found: 1
Reference: <author> Barron, A. </author> <year> (1990). </year> <title> Complexity regularization with application to artificial neural networks. </title> <editor> In Roussas, G., editor, </editor> <booktitle> Nonparametric Functional Estimation and Related Topics, </booktitle> <pages> pages 561-576, </pages> <address> Dordrecht. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Examples of `strange' model classes for which two-part codes have already been constructed are context-free grammars (Grunwald, 1996), various neural networks <ref> (Barron, 1990) </ref> and several models arising in the analysis of DNA and protein structure (Dowe et al., 1996). 6 Comparison to other approaches Bayesian Evidence and Stochastic Complexity In Bayesian statistics (Berger, 1985) we always use a prior distribution w for all elements in the chosen model class M.
Reference: <author> Barron, A. and Cover, T. </author> <year> (1991). </year> <title> Minimum complexity density estimation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(4) </volume> <pages> 1034-1054. </pages>
Reference: <author> Berger, J. </author> <year> (1985). </year> <title> Statistical Decision Theory and Bayesian Analysis. </title> <booktitle> Springer Series in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <note> revised and expanded second edition. </note>
Reference-contexts: `strange' model classes for which two-part codes have already been constructed are context-free grammars (Grunwald, 1996), various neural networks (Barron, 1990) and several models arising in the analysis of DNA and protein structure (Dowe et al., 1996). 6 Comparison to other approaches Bayesian Evidence and Stochastic Complexity In Bayesian statistics <ref> (Berger, 1985) </ref> we always use a prior distribution w for all elements in the chosen model class M.
Reference: <author> Chater, N. </author> <year> (1996). </year> <title> Reconciling simplicity and likelihood principles in perceptual organization. </title> <journal> Psychological Review, </journal> (103):566-581. 
Reference-contexts: This result, proved independently by Solomonoff, Kolmogorov and Chaitin has generated a large body of research on Kolmogorov Complexity (Li and Vitanyi, 1997), which has found its way into psychology before in a context very different from model selection <ref> (Chater, 1996) </ref>. 1 By this we mean that a universal Turing Machine can be implemented in it (Li and Vitanyi, 1997). 2 Unfortunately, the Kolmogorov Complexity as such cannot be computed there can be no computer program that, for every set of data D, when given D as input, returns the
Reference: <author> Cover, T. and Thomas, J. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> Wiley Interscience, </publisher> <address> New York. </address>
Reference-contexts: It is easy to show that there is a simple algorithm which for any encoded sequence C (x) retrieves the original sequence x. Moreover, in the encodings C (x), no comma's are needed to separate the codewords. Codes with this latter property are called `uniquely decodable' <ref> (Cover and Thomas, 1991) </ref>. For various reasons (Rissanen, 1989), all codes considered in any application of MDL are uniquely decodable; henceforth whenever we speak of a code, we actually mean a uniquely decodable one. <p> This is also the reason why we can compress sequence (3): the number of all sequences of length n with four times as many 1s as 0s is extremely (exponentially) small compared to 2 n , the total number of sequences of length n <ref> (Cover and Thomas, 1991) </ref>. Codes `are' Probability Distributions Many model classes encountered in practice are probabilistic. A very simple example is the class of Bernoulli distributions M B . M B is indexed by a single parameter . <p> Since this term can be interpreted as a code length, we abbreviate it 2 Strictly speaking, it does not follow from Kraft's original inequality but from McMillan's extension <ref> (Cover and Thomas, 1991) </ref> for uniquely decodable codes. 6 to L (Dj).
Reference: <author> Dowe, D., Allison, L., Dix, T., Hunter, L., Wallace, C., and Edgoose, T. </author> <year> (1996). </year> <title> Circular 4 The only book about MDL is `Stochastic Complexity in Statistical Inquiry' (Rissanen, 1989) which we very much recommend reading. However, it is a little outdated equation (8) was not known in 1989. 12 clustering of protein dihedral angles by minimum message length. </title> <booktitle> In Proceedings Pacific Symposium on Biocomputing '96. </booktitle> <publisher> World Scientific. </publisher>
Reference-contexts: Examples of `strange' model classes for which two-part codes have already been constructed are context-free grammars (Grunwald, 1996), various neural networks (Barron, 1990) and several models arising in the analysis of DNA and protein structure <ref> (Dowe et al., 1996) </ref>. 6 Comparison to other approaches Bayesian Evidence and Stochastic Complexity In Bayesian statistics (Berger, 1985) we always use a prior distribution w for all elements in the chosen model class M.
Reference: <author> Grunwald, P. </author> <year> (1996). </year> <title> A minimum description length approach to grammar inference. </title> <editor> In S. Wermter, E. Riloff, G. S., editor, </editor> <title> Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing, </title> <booktitle> number 1040 in Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 203-216. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Examples of `strange' model classes for which two-part codes have already been constructed are context-free grammars <ref> (Grunwald, 1996) </ref>, various neural networks (Barron, 1990) and several models arising in the analysis of DNA and protein structure (Dowe et al., 1996). 6 Comparison to other approaches Bayesian Evidence and Stochastic Complexity In Bayesian statistics (Berger, 1985) we always use a prior distribution w for all elements in the chosen
Reference: <author> Grunwald, P. D. </author> <year> (1998). </year> <type> Ph.d. thesis, forthcoming. </type>
Reference-contexts: Since this observation is so important, it has been repeated in figure 2. It turns out that one can even make this observation (partially) mathematically precise <ref> (Grunwald, 1998) </ref>! The fact that, in MDL, all inductive inference is done with respect to a coding system leads to a kind of `generic avoidance' of overfitting.
Reference: <author> Kearns, M., Mansour, Y., Ng, A., and Ron, D. </author> <year> (1997). </year> <title> An experimental and theoretical comparison of model selection methods. </title> <journal> Machine Learning, </journal> <volume> 27 </volume> <pages> 7-50. </pages>
Reference-contexts: Though theoretically speaking, there are situations in which cross-validation `overfits' whereas MDL does not (Rissanen, 1989), it is not clear to the present author whether this implies much for practical settings, for which cross-validation seems to perform comparable to MDL <ref> (Kearns et al., 1997) </ref>. 7 On Overfitting and Underfitting There have been several studies which indicate that MDL performs well both in theory and practice (Barron and Cover, 1991; Rissanen, 1989; Kearns et al., 1997; Kontkanen et al., 1996).
Reference: <author> Kontkanen, P., Myllymaki, P., and Tirri, H. </author> <year> (1996). </year> <title> Comparing Bayesian model class selection criteria by discrete finite mixtures. </title> <editor> In Dowe, D., Korb, K., and Oliver, J., editors, </editor> <booktitle> Information, Statistics and Induction in Science (Proceedings of the ISIS'96 Conference in Melbourne, Australia), </booktitle> <pages> pages 364-374, </pages> <address> Singapore. </address> <publisher> World Scientific. </publisher>
Reference: <author> Li, M. and Vitanyi, P. </author> <year> (1997). </year> <title> An Introduction to Kolmogorov Complexity and Its Applications. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <note> revised and expanded second edition. </note>
Reference-contexts: Clearly, it is does nothing more than repeating the sequence. It is easy to show that there exists quite a short program generating sequence (3) too, as we will make plausible in the next section. Kolmogorov Complexity We now define the Kolmogorov Complexity <ref> (Li and Vitanyi, 1997) </ref> of a sequence as the length of the shortest program that prints the sequence and then halts. The lower the Kolmogorov complexity of a sequence, the more regular or equivalently, the less random, or, yet equivalently, the simpler it is. <p> This result, proved independently by Solomonoff, Kolmogorov and Chaitin has generated a large body of research on Kolmogorov Complexity <ref> (Li and Vitanyi, 1997) </ref>, which has found its way into psychology before in a context very different from model selection (Chater, 1996). 1 By this we mean that a universal Turing Machine can be implemented in it (Li and Vitanyi, 1997). 2 Unfortunately, the Kolmogorov Complexity as such cannot be computed <p> Kolmogorov and Chaitin has generated a large body of research on Kolmogorov Complexity <ref> (Li and Vitanyi, 1997) </ref>, which has found its way into psychology before in a context very different from model selection (Chater, 1996). 1 By this we mean that a universal Turing Machine can be implemented in it (Li and Vitanyi, 1997). 2 Unfortunately, the Kolmogorov Complexity as such cannot be computed there can be no computer program that, for every set of data D, when given D as input, returns the length of the shortest program that prints D: assuming such a program exists leads to a contradiction <p> and Vitanyi, 1997). 2 Unfortunately, the Kolmogorov Complexity as such cannot be computed there can be no computer program that, for every set of data D, when given D as input, returns the length of the shortest program that prints D: assuming such a program exists leads to a contradiction <ref> (Li and Vitanyi, 1997) </ref>. Another problem is that in many realistic settings, we are often confronted with very small data sets for which the `invariance theorem' does not say too much.
Reference: <author> Myung, I. and Pitt, M. </author> <year> (1997). </year> <title> Issues in selecting mathematical models of cognition. </title> <note> To appear in: </note> <editor> J. Grainger and A.M. Jacobs (eds.), </editor> <title> Localist Connectionist Approaches to Human Cognition. </title> <publisher> Lawrence Erlbaum associates. </publisher>
Reference-contexts: This version of SC may be of particular interest to psychologists since one can compute it for just about any model class one can think of in psychological applications <ref> (Myung and Pitt, 1997) </ref>, one more often than not uses complicated models for which the other approximations to SC mentioned in this paper are very hard to compute.
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by the shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference: <author> Rissanen, J. </author> <year> (1987). </year> <title> Stochastic complexity. </title> <journal> J. Royal Stat. Soc., series B, </journal> <volume> 49 </volume> <pages> 223-239. Discussion: pages 252-265. </pages>
Reference-contexts: By discretizing to a finite precision we can do the same for continuous data: for any probability density function f (x), log f (x) can be interpreted as a code length <ref> (Rissanen, 1987) </ref>. Maximum Likelihood = Minimal Code Length Suppose now we are given a probabilistic (e.g. Bernoulli) model class M containing models . <p> We illustrate this for the polynomials and the squared error. It is possible <ref> (Rissanen, 1987) </ref> to construct a code C 2 such that for all D = ((x 1 ; y 1 ); : : : ; (x n ; y n )) and all polynomials H: L C 2 (x n ; y n jH) = se (y n jx n ; H)
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company. </publisher>
Reference-contexts: We can define stochastic complexity even for such broad classes and select the one which allows for more compression <ref> (Rissanen, 1989) </ref> of the data at hand. 3 Codes, Probability Distributions and Hypotheses In the next section we give a formal definition of stochastic complexity. <p> We will sometimes use the notation x i for (x 1 ; : : : ; x i ). Similarly, whenever the length n is not clear from the context, 4 we write x n in stead of D. It is easy to see <ref> (Rissanen, 1989) </ref> that without loss of generality we may always describe our data sequences as binary sequences: Definition 3.1 Let A be some data alphabet. A code or description method C is a one-to-one map from A fl into B fl , the set of binary strings. <p> Moreover, in the encodings C (x), no comma's are needed to separate the codewords. Codes with this latter property are called `uniquely decodable' (Cover and Thomas, 1991). For various reasons <ref> (Rissanen, 1989) </ref>, all codes considered in any application of MDL are uniquely decodable; henceforth whenever we speak of a code, we actually mean a uniquely decodable one. <p> We can perform the same trick for just about any other class of models and any other error measure <ref> (Rissanen, 1989) </ref>. It follows that we can turn any such class of models into an equivalent class of codes: any model, probabilistic or not, may be identified with a code. <p> Neglecting this constant term and selecting M k by choosing the k which minimizes L (Dj ^ (D)) + k 2 log n has confusingly-been called the MDL Model Selection Criterion <ref> (Rissanen, 1989) </ref>. One can use it as a first approximation, but it will only work well if one really has a lot of data, since the neglected constant can be very large. <p> Though theoretically speaking, there are situations in which cross-validation `overfits' whereas MDL does not <ref> (Rissanen, 1989) </ref>, it is not clear to the present author whether this implies much for practical settings, for which cross-validation seems to perform comparable to MDL (Kearns et al., 1997). 7 On Overfitting and Underfitting There have been several studies which indicate that MDL performs well both in theory and practice
Reference: <author> Rissanen, J. </author> <year> (1996). </year> <title> Fisher information and stochastic complexity. </title> <journal> IEEE Trans. Inf. Theory, </journal> <volume> 1(42) </volume> <pages> 40-47. </pages>
Reference-contexts: However, in contrast to the computer language case, it is very hard to define `with the help of' formally. Indeed, a completely satisfactory formal definition has only been found very recently <ref> (Rissanen, 1996) </ref>. We can use stochastic complexity (SC) in many ways, two important ones being prediction of future data and model selection. <p> term K fl and as such embodies the trade-off between fit and complexity referred to in figure 3 Actually, such a code C 2 can be created only for model classes with a fixed number of parameters; the definition for classes like the class of all polynomials is more involved <ref> (Rissanen, 1996) </ref>. 8 1. To see this for the polynomial example, let us compare the polynomials in fig. 1.1: let M i be the class of all polynomials of the i-th degree. <p> Computing K fl however is very difficult. For sufficiently regular model classes (which include, for example, any class of functions with squared loss), we can however approximate K fl k (i.e. the K fl for a model class with k parameters) extremely well <ref> (Rissanen, 1996) </ref> as follows: K fl k log n + complicated constant (9) The first term grows linearly in the number of parameters k and logarithmically in n, the size of the data set. <p> In this case, again for many model classes there exists a prior w () with which log P av (DjM) approximates I (DjM) extremely well but it is not the uniform prior <ref> (Rissanen, 1996) </ref>. For most other priors which give probability &gt; 0 to all models in M, the approximation to I (DjM) will still be reasonable but only for large datasets. <p> By taking the model fl 2 M and the precision d for which the sum L (Dj) + L () is as low as possible, we arrive at a reasonable approximation of stochastic complexity: I (DjM k ) L (Dj fl ) + L ( fl ) <ref> (Rissanen, 1996) </ref>.
Reference: <author> Solomonoff, R. </author> <year> (1964). </year> <title> A formal theory of inductive inference, part 1 and part 2. </title> <journal> Inform. Contr., </journal> <volume> 7 </volume> <pages> 1-22, 224-254. </pages>
Reference: <author> Wallace, C. and Boulton, D. </author> <year> (1968). </year> <title> An information measure for classification. </title> <journal> Computing Journal, </journal> <volume> 11 </volume> <pages> 185-195. 13 </pages>
References-found: 18

