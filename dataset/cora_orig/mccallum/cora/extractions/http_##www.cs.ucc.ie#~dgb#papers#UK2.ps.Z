URL: http://www.cs.ucc.ie/~dgb/papers/UK2.ps.Z
Refering-URL: http://www.cs.ucc.ie/~dgb/publist.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: ftony|dgbg@minster.york.ac.uk  
Title: A Yardstick for the Evaluation of Case-Based Classifiers  
Author: A D Griffiths and D G Bridge 
Address: York, YORK YO1 5DD, UK  
Affiliation: Department of Computer Science, University of  
Abstract: This paper proposes that the generalisation capabilities of a case-based reasoning system can be evaluated by comparison with a `rote-learning' algorithm which uses a very simple generalisation strategy. Two such algorithms are defined, and expressions for their classification accuracy are derived as a function of the size of training sample. A series of experiments using artificial and `natural' data sets is described in which the learning curve for a case-based learner is compared with those for the apparently trivial rote-learning learning algorithms. The results show that in a number of `plausible' situations, the learning curves for a simple case-based learner and the `majority' rote-learner can barely be distinguished, although a domain is demonstrated where favourable performance from the case-based learner is observed. This suggests that the maxim of case-based reasoning that `similar problems have similar solutions' may be useful as the basis of a generalisation strategy only in selected domains.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D W Aha. </author> <title> Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 36 </volume> <pages> 267-287, </pages> <year> 1992. </year>
Reference-contexts: Since it is known that the presence of irrelevant attributes adversely affects case-based classification, the success of CB1 is even more encouraging. Furthermore, there are methods for introducing appropriate weightings into the similarity measure to counteract this effect <ref> [1] </ref>, and were these to be used here, even higher accuracies might be expected from the case-based classifier. This data-set would then seem a clear indication that in appropriately chosen domains, case-based classification can have significant generalisation power.
Reference: 2. <author> A D Griffiths. </author> <title> Machine Learning using a Case-Based Representation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of York, UK, </institution> <note> In Preparation. </note>
Reference-contexts: The equations for the learning curves require the following lemma, restating the overall expected accuracy of a consistent learning algorithm in terms of the expected coverage (m). Proofs below are sketched or omitted; details will be available in <ref> [2] </ref>. Lemma 4. The expected accuracy of a consistent learning algorithm L may be re-expressed in terms of (m) as follows: EA L 1 t * T x * (X 1 ) m Proposition 5. Expected Accuracy of L 1 .
Reference: 3. <author> A D Griffiths and D G Bridge. </author> <title> Formalising the knowledge content of case memory systems. </title> <booktitle> To appear in Proceedings of the First UK Workshop on Case-Based Reasoning, </booktitle> <address> Salford UK, Jan 1995. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Since case-based classification will also in practice give perfect recall of the training sample, subject only to very weak constraints on the similarity measure <ref> [3] </ref>, this comparison allows the contribution of the nearest neighbour generalisation strategy to be isolated. As a minimum criterion for success, it is proposed that a case-based generalisation strategy should outperform the two `nave' strategies used by the two rote-learners. <p> In this work, we assume that a `preference ordering' is defined over the solution space to resolve any ties that might occur, although other options are available <ref> [3] </ref>. i.e. In this paper the function defined by a case-base CB and similarity measure is the function f where f (x) = max w fx 2 jhx 1 ; x 2 i * N N (x; CB; )g (4) for some preference ordering w. <p> The case-based learning algorithm will be CB1 as defined in [5] [4] <ref> [3] </ref>, namely a straightforward case-based classifier which collects all available cases into its case-base, which calculates similarity by counting the proportion of features of the representation on which two descriptions agree, and which resolves ties between equally near neighbours using a fixed preference ordering defined over the space of solution values <p> namely a straightforward case-based classifier which collects all available cases into its case-base, which calculates similarity by counting the proportion of features of the representation on which two descriptions agree, and which resolves ties between equally near neighbours using a fixed preference ordering defined over the space of solution values <ref> [3] </ref>. <p> Earlier work <ref> [3] </ref> has shown that the `preference ordering' plays an important part in defining the decision function for case-based classification.
Reference: 4. <author> A D Griffiths and D G Bridge. </author> <title> Inductive bias in case-based reasoning systems. </title> <type> Technical Report YCS 95/259, </type> <institution> Department of Computer Science, University of York, </institution> <address> York YO1 5DD, UK, </address> <year> 1995. </year> <note> Available by WWW via [http://www.cs.york.ac.uk/~tony/]. </note>
Reference-contexts: The case-based learning algorithm will be CB1 as defined in [5] <ref> [4] </ref> [3], namely a straightforward case-based classifier which collects all available cases into its case-base, which calculates similarity by counting the proportion of features of the representation on which two descriptions agree, and which resolves ties between equally near neighbours using a fixed preference ordering defined over the space of solution <p> There are 3 N such functions defined on an example space of N -dimensional binary-valued feature vectors, and 2 k k such functions defined by conjunctions of exactly k literals. Previous work has considered in detail the behaviour of the case-based learning of monomial target functions [5] <ref> [4] </ref>. The performance of case-based learning was shown to be poor compared to learning algorithms which directly manipulated a representation of a monomial function, especially when the classifier uses a fixed similarity measure.
Reference: 5. <author> A D Griffiths and D G Bridge. </author> <title> On concept space and hypothesis space in case-based learning algorithms. </title> <editor> In N Lavrac and S Wrobel, eds, ECML-95: </editor> <booktitle> Proc. 8th European Conf. on Machine Learning, 1995, </booktitle> <volume> LNAI Volume 914, </volume> <pages> pages 161 - 173, </pages> <address> 1995. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: The case-based learning algorithm will be CB1 as defined in <ref> [5] </ref> [4] [3], namely a straightforward case-based classifier which collects all available cases into its case-base, which calculates similarity by counting the proportion of features of the representation on which two descriptions agree, and which resolves ties between equally near neighbours using a fixed preference ordering defined over the space of <p> There are 3 N such functions defined on an example space of N -dimensional binary-valued feature vectors, and 2 k k such functions defined by conjunctions of exactly k literals. Previous work has considered in detail the behaviour of the case-based learning of monomial target functions <ref> [5] </ref> [4]. The performance of case-based learning was shown to be poor compared to learning algorithms which directly manipulated a representation of a monomial function, especially when the classifier uses a fixed similarity measure.
Reference: 6. <author> R Kohavi. </author> <title> The power of decision tables. </title> <editor> In N Lavrac and S Wrobel, eds, ECML-95: </editor> <booktitle> Proc. 8th European Conf. on Machine Learning, 1995, </booktitle> <volume> LNAI Volume 914, </volume> <pages> pages 174-189, </pages> <address> 1995. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: rote-learning algorithms which will provide the yardstick for the evaluation of these classifiers. 3 Learning Curves for Rote-Learning Algorithms The representation used by the rote learning algorithms defined here are very closely related to the decision tables used by learners such as the `Decision Table Majority' algorithm studied by Kohavi <ref> [6] </ref>. The classification rule for Decision Table Majority is, "given an unlabelled instance, a decision table classifier searches for exact matches in the decision table [...].
Reference: 7. <author> P M Murphy and D W Aha. </author> <title> Uci repository of machine learning databases, </title> <institution> [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA, </address> <year> 1994. </year>
Reference-contexts: Figures 4 and 5 show the results of applying the yardstick proposed here to case-based classification used in `real-world' domains. The two data sets used were chosen from the UCI Machine Learning Repository <ref> [7] </ref> on the basis of having nominal feature values, no missing values in the data sets and a solution set of size greater than two. Characteristics of the two data sets are given in Table 1. <p> Acknowledgements. This work was mostly carried out while the first author was supported by an EPSRC grant. Additional funding was provided by Michelin Tyres PLC. We are indebted to Messrs Murphy and Aha for the availability of the UCI Machine Learning Repository <ref> [7] </ref>.
Reference: 8. <author> C Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In ML94: Proc. International Conference on Machine Learning, </booktitle> <address> New Brunswick, New Jersey, </address> <pages> pages 259-265. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: In Figures 1 and 2 the curves for case-based learning and the rote-learning algorithms coincide exactly; this is no surprise since in the maximum entropy case where all possible target mappings are equally likely, then no generalisation strategy can do better than random guessing and all are essentially equivalent [9] <ref> [8] </ref>. The success of the equations derived in the previous section in describing this case however lends confidence to the accuracy of the analysis. Rote-learning of Monomial Target Functions Monomial functions are those boolean functions which can be defined by a simple conjunction of (possibly negated) literals. <p> The two `rote-learning' algorithms defined in this paper, and the equations derived for their average-case learning curves, allow these two factors to be separated in the evaluation of a case-based classifier. Figures 1 and 2 give direct experimental confirmation of the results in [9] <ref> [8] </ref> that no generalisation strategy can outperform any other if all possible classifications of the example space are equally likely.
Reference: 9. <author> D H Wolpert. </author> <title> On the connection between in-sample testing and generalisation error. </title> <journal> Complex Systems, </journal> <volume> 6 </volume> <pages> 47-94, </pages> <year> 1992. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: In Figures 1 and 2 the curves for case-based learning and the rote-learning algorithms coincide exactly; this is no surprise since in the maximum entropy case where all possible target mappings are equally likely, then no generalisation strategy can do better than random guessing and all are essentially equivalent <ref> [9] </ref> [8]. The success of the equations derived in the previous section in describing this case however lends confidence to the accuracy of the analysis. Rote-learning of Monomial Target Functions Monomial functions are those boolean functions which can be defined by a simple conjunction of (possibly negated) literals. <p> The two `rote-learning' algorithms defined in this paper, and the equations derived for their average-case learning curves, allow these two factors to be separated in the evaluation of a case-based classifier. Figures 1 and 2 give direct experimental confirmation of the results in <ref> [9] </ref> [8] that no generalisation strategy can outperform any other if all possible classifications of the example space are equally likely.
References-found: 9

