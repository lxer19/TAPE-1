URL: http://www.cse.ucsc.edu/research/compbio/papers/HughKrogh96.ps.Z
Refering-URL: http://www.cse.ucsc.edu/research/compbio/sam.html
Root-URL: http://www.cse.ucsc.edu
Title: Hidden Markov models for sequence analysis: extension and analysis of the basic method Running title:
Author: Richard Hughey and Anders Krogh 
Date: CABIOS 12(2):95-107, 1996  
Pubnum: REPRINT  
Abstract: Hidden Markov models (HMMs) are a highly effective means of modeling a family of unaligned sequences or a common motif within a set of unaligned sequences. The trained HMM can then be used for discrimination or multiple alignment. The basic mathematical description of an HMM and its expectation-maximization training procedure is relatively straight-forward. In this paper, we review the mathematical extensions and heuristics that move the method from the theoretical to the practical. Then, we experimentally analyze the effectiveness of model regularization, dynamic model modification, and optimization strategies. Finally it is demonstrated on the SH2 domain how a domain can be found from unaligned sequences using a special model type. The experimental work was completed with the aid of the Sequence Alignment and Modeling software suite.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bairoch, A. & Boeckmann, B. </author> <year> (1994). </year> <title> The SWISS-PROT protein sequence data bank: current status. </title> <journal> Nucleic Acids Research, </journal> <volume> 22, </volume> <pages> 3578-3580. </pages> <note> 20 Baldi, </note> <author> P., Chauvin, Y., Hunkapillar, T., & McClure, M. </author> <year> (1994). </year> <title> Hidden Markov models of biological primary sequence information. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 91, </volume> <pages> 1059-1063. </pages>
Reference-contexts: When searching a database like SWISS-PROT <ref> (Bairoch & Boeckmann, 1994) </ref> with an HMM, the smooth average and the Z-scores are calculated as follows. For a fixed sequence length we assume that the NLL scores are distributed as a normal distribution with some outliers representing the sequences in the modeled family.
Reference: <author> Berger, J. </author> <year> (1985). </year> <title> Statistical Decision Theory and Bayesian Analysis. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Brown, M. P., Hughey, R., Krogh, A., Mian, I. S., Sjolander, K., & Haussler, D. </author> <year> (1993). </year> <title> Using Dirichlet mixture priors to derive hidden Markov models for protein families. </title> <booktitle> In: Proc. of First Int. Conf. on Intelligent Systems for Molecular Biology, </booktitle> <editor> (Hunter, L., Searls, D., & Shavlik, J., eds) pp. </editor> <address> 47-55, Menlo Park, CA: </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: The SAM system can also use the more complicated Dirichlet mixture priors for regularization. These priors include several different distributions for some number of different types of columns, such as hydrophobic and hydrophilic positions. For more information on these distributions, please refer to our previous work <ref> (Brown et al., 1993) </ref>. By normalizing the regularizer as in (6), a valid model is obtained. Since this model represents prior beliefs, it is natural to use it as the initial model for the estimation process. <p> SAM supports both Dirichlet and Dirichlet mixture regularization. To gauge the effects of regularization, we compared no regularization to four other choices: the default simple regularizer, the original 9-component mixture <ref> (Brown et al., 1993) </ref>, and more recent 9- and 13 (a) (b) (c) on 50 other globin sequences with (a) default noise, (b) random starting model lengths and (c) all heuristics including surgery.
Reference: <author> Bucher, P. & Bairoch, A. </author> <year> (1994). </year> <title> A generalized profile syntax for biomolecular sequence motifs and its function in automatic sequence interpretation. </title> <booktitle> In: Proc. Int. Conf. on Intelligent Systems for Molecular Biology pp. </booktitle> <pages> 53-61, </pages> <address> Stanford, CA: </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Eddy, S. </author> <year> (1995). </year> <title> Multiple alignment using hidden Markov models. </title> <booktitle> In: Proc. Int. Conf. on Intelligent Systems for Molecular Biology pp. </booktitle> <pages> 114-120, </pages> <address> Cambridge, England: </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Exponentially: If r is less than 1, the noise is decreased exponentially by multiplying the noise with r in each iteration, N i = N 0 r i : An alternative and very elegant way of using simulated annealing is described in <ref> (Eddy, 1995) </ref>. 3.3 Multiple sequence alignment The probability (1) or the score (3) can (in principle) be calculated for all possible alignments to a model, and thus the most probable (i.e., the best) can be found.
Reference: <author> Eddy, S., Mitchison, G., & Durbin, R. </author> <year> (1995). </year> <title> Maximum discrimination hidden Markov models of sequence consensus. </title> <journal> J. Comput. Biol. </journal> <volume> 2, </volume> <pages> 9-23. </pages>
Reference-contexts: Exponentially: If r is less than 1, the noise is decreased exponentially by multiplying the noise with r in each iteration, N i = N 0 r i : An alternative and very elegant way of using simulated annealing is described in <ref> (Eddy, 1995) </ref>. 3.3 Multiple sequence alignment The probability (1) or the score (3) can (in principle) be calculated for all possible alignments to a model, and thus the most probable (i.e., the best) can be found.
Reference: <author> Gribskov, M., McLachlan, A. D., & Eisenberg, D. </author> <year> (1987). </year> <title> Profile analysis: Detection of distantly related proteins. </title> <booktitle> Proceedings of the National Academy of Sciences of the United States of America, </booktitle> <volume> 84, </volume> <pages> 4355-4358. </pages>
Reference: <author> Grice, J. A., Hughey, R., & Speck, D. </author> <year> (1995). </year> <title> Parallel sequence alignment in limited space. </title> <booktitle> In: Proc. of Third Int. Conf. on Intelligent Systems for Molecular Biology , Menlo Park, </booktitle> <address> CA: </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Future additions will include repeated motif location, alternate scoring methods using null models, subsequence to submodel training, sequence weighting, an improved user interface, and use of our new algorithm for parallel sequence alignment in limited space to greatly extend the capabilities of the MasPar code <ref> (Grice et al., 1995) </ref>. 7 Acknowledgments We would like to thank Saira Mian for many valuable suggestions, and David Haussler and the rest of the computational biology group at the Baskin Center for Computer Engineering and Information Sciences at UCSC for many suggestions and contributions.
Reference: <author> Haussler, D., Krogh, A., Mian, I. S., & Sjolander, K. </author> <year> (1993). </year> <title> Protein modeling using hidden Markov models: Analysis of globins. </title> <booktitle> In: Proceedings of the Hawaii International Conference on System Sciences volume 1 pp. </booktitle> <pages> 792-802, </pages> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Karplus, K. </author> <year> (1995). </year> <title> Regularizers for estimating distributions of amino acids from small samples. </title> <booktitle> In: Proc. of Third Int. Conf. on Intelligent Systems for Molecular Biology , Menlo Park, </booktitle> <address> CA: </address> <note> AAAI/MIT Press. Full version available as UCSC Technical Report UCSC-CRL-95-11. </note>
Reference-contexts: The solid vertical line at 334 is the average test sequence score without any random heuristics (in this case, surgery has no effect on the non-random training routine). 21-component regularizers <ref> (Karplus, 1995) </ref>. Histograms of the test set NLL scores for these experiments, which used the noise settings determined in the previous experiments, are shown in Figure 6. Clearly, regularization is needed.
Reference: <author> Kirkpatrick, S., Jr., C. G., & Vecchi, M. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220. </volume>
Reference-contexts: By default, R is set to 50 random sequences. One can also add noise to the models during their estimation and decrease the noise level gradually in a technique similar to the general optimization method called simulated annealing <ref> (Kirkpatrick et al., 1983) </ref>. The initial level of the noise in this annealing process is called N 0 as above. During the estimation process the annealing noise is decreased by a speed determined by r.
Reference: <author> Krogh, A., Brown, M., Mian, I. S., Sjolander, K., & Haussler, D. </author> <year> (1994a). </year> <title> Hidden Markov models in computational biology: Applications to protein modeling. </title> <journal> Journal of Molecular Biology, </journal> <volume> 235, </volume> <pages> 1501-1531. </pages> <note> 21 Krogh, </note> <author> A., Mian, I. S., & Haussler, D. </author> <year> (1994b). </year> <title> A hidden Markov model that finds genes in e. coli DNA. </title> <journal> Nucleic Acids Research, </journal> <volume> 22, </volume> <pages> 4768-4778. </pages>
Reference-contexts: The most basic mathematical method of training an HMM does not work well without the addition of several mathematical extensions and heuristics. The body of this paper is a study of three of the most important extensions to the basic model presented previously <ref> (Krogh et al., 1994a) </ref>: regu-larizers, dynamic model modification, and `free insertion modules'. After reviewing these in turn, we present an experimental evaluation of the utility of these and other extensions. 2 System and methods The Sequence Alignment and Modeling (SAM) software suite is written in ANSI C for Unix workstations. <p> Questions concerning the software and its distribution can be mailed to sam-info@cse.ucsc.edu. 3 Algorithm This section discusses the basic theory and use of HMMs. Although most aspects of our linear HMMs have been described previously <ref> (Krogh et al., 1994a) </ref>, this section reexamines several operational features as the foundation for the experimental evaluation that follows. 3.1 The hidden Markov model This section briefly describes the structure and basic theory of the type of hidden Markov model used in this work. <p> For a general introduction to hidden Markov models, refer to (Rabiner, 1989). For additional details on our hidden Markov models, refer to <ref> (Krogh et al., 1994a) </ref>. The basic model topology is shown in Figure 1. Each position, or module, in the model has three states. A state shown as a rectangular box is a match state that models the distribution of letters in the corresponding column of an alignment. <p> The path specifying the particular alignment is denoted q 1 : : : q L , where q k is the kth state in the path. To simplify notation, as compared to <ref> (Krogh et al., 1994a) </ref>, only the match and insert states are included in the path; for two states not directly connected, delete states are filled in as needed. <p> We call this the maximum a posteriori (MAP) estimate, although the correct MAP formula has ff (xjm k ) 1 instead of ff (xjm k ). Equation (5) is really a least squares estimate (see <ref> (Krogh et al., 1994a) </ref>), but one can also view it as a MAP estimate with redefined ffs. Even without the theoretical justification this formula is appealing. For each parameter in the model a number (ff) is added to the corresponding n before the new parameter is found. <p> The second method is to use the full sequences and train with the FIMs. The second method is generally easier, but also slower in terms of CPU time, especially if the sequences are much longer than the subsequences. In <ref> (Krogh et al., 1994a) </ref> the first method was used to model the protein kinase catalytic domain and the EF hand domain, and in this paper the second method will be demonstrated on the SH2 domain. <p> For practical purposes, however, various problems arise, of which the most important ones were discussed in this paper. A variety of heuristics and extensions to overcome the problems which are all part of the SAM software package were presented. Most of these techniques were mentioned in our original paper <ref> (Krogh et al., 1994a) </ref>, but here we have discussed them in more detail, and treated it from a more practical perspective, whereas we have not focussed on the biological results at all.
Reference: <author> Krogh, A. & Mitchison, G. </author> <year> (1995). </year> <title> Maximum entropy weighting of aligned sequences of proteins or DNA. </title> <booktitle> In: Proc. of Third Int. Conf. on Intelligent Systems for Molecular Biology , Menlo Park, </booktitle> <address> CA: </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: We choose the globin family for the first of these illustrative experiments because of our previous familiarity with the family. From a set of 624 globins, close homologues were removed using a maximum entropy weighting scheme <ref> (Krogh & Mitchison, 1995) </ref> by removing all sequences with a very small weight (&lt; 10 5 ), which left us with 167 globins.
Reference: <author> Kuriyan, J. & Cowburn, D. </author> <year> (1993). </year> <title> Structures of SH2 and SH3 domains. </title> <booktitle> Current Opinion in Structural Biology, </booktitle> <volume> 3, </volume> <pages> 828-837. </pages>
Reference-contexts: We use the SH2 domain, which is found in a variety of proteins involved in signal transduction, where it mediates protein-protein interactions. For a review see <ref> (Kuriyan & Cowburn, 1993) </ref>. The domain has a length of about 100. <p> The differences in test set scores between the available regularizers is shown in the blowup to the right. 15 and ended about 20 amino acids early as compared to the the alignment in <ref> (Kuriyan & Cowburn, 1993) </ref>. Some of the other models also covered part of the domain, whereas others had picked up a different signal. This signal was probably the kinase catalytic domain of some of the proteins in the file. <p> The unshaded modules correspond to secondary structure elements as given in <ref> (Kuriyan & Cowburn, 1993) </ref>. These elements are fiA, ffA, fiB, fiC, fiD, fiE, fiF , ffB, and fiG. The figure (except the shading) was produced with the program drawmodel in SAM.
Reference: <author> Lawrence, C., Altschul, S., Boguski, M., Liu, J., Neuwald, A., & Wootton, J. </author> <year> (1993). </year> <title> Detecting subtle sequence signals: a Gibbs sampling strategy for multiple alignment. </title> <journal> Science, </journal> <volume> 262 (5131), </volume> <pages> 208-14. </pages>
Reference-contexts: Gibbs sampling is an alternate approach that does not allow arbitrary gaps within aligned blocks <ref> (Lawrence et al., 1993) </ref>. Alignment of a sequence to a model means that each letter in the sequence is associated with a match or insert state in the model. <p> In (Krogh et al., 1994a) the first method was used to model the protein kinase catalytic domain and the EF hand domain, and in this paper the second method will be demonstrated on the SH2 domain. In (Lawrence & Reilly, 1990) and <ref> (Lawrence et al., 1993) </ref>, two related methods of automatically finding common patterns in a set of sequences are described. Those papers deal only with gap-less alignments, i.e., patterns without insertions or deletions.
Reference: <author> Lawrence, C. & Reilly, A. </author> <year> (1990). </year> <title> An expectation maximization (EM) algorithm for the identification and characterization of common sites in unaligned biopolymer sequences. </title> <journal> Proteins, </journal> <volume> 7 (1), </volume> <pages> 41-51. </pages>
Reference-contexts: In (Krogh et al., 1994a) the first method was used to model the protein kinase catalytic domain and the EF hand domain, and in this paper the second method will be demonstrated on the SH2 domain. In <ref> (Lawrence & Reilly, 1990) </ref> and (Lawrence et al., 1993), two related methods of automatically finding common patterns in a set of sequences are described. Those papers deal only with gap-less alignments, i.e., patterns without insertions or deletions.
Reference: <author> Nickolls, J. R. </author> <year> (1990). </year> <title> The design of the Maspar MP-1: A cost effective massively parallel computer. </title> <booktitle> In: Proc. COMPCON Spring 1990 pp. </booktitle> <pages> 25-28, </pages> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: 3000/500 0 107 2.9 C-Linda, 7 Decstation 5000s (240 and 125) 4 147 4.0 Cray Y-MP, 1 CPU, vectorized 8 167 4.5 8K MP-1, optimized 60 1530 41 4K MP-2, optimized 60 1580 43 The MasPar parallel processor features a 2-dimensional mesh of processing elements (PEs) and a global router <ref> (Nickolls, 1990) </ref>. For large numbers of sequences, the best algorithm mapping can be to perform a complete model-sequence dynamic programming in each PE; unfortunately, the 64 KBytes of local memory per PE is too small for such coarse-grain parallelism.
Reference: <author> Rabiner, L. R. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <journal> Proc. IEEE, </journal> <volume> 77 (2), </volume> <pages> 257-286. </pages>
Reference-contexts: For a general introduction to hidden Markov models, refer to <ref> (Rabiner, 1989) </ref>. For additional details on our hidden Markov models, refer to (Krogh et al., 1994a). The basic model topology is shown in Figure 1. Each position, or module, in the model has three states. <p> The sequences used to estimate or train the model are called the training sequences, and any reserved sequences used to evaluate the model are called the test sequences. The model estimation is done with the forward-backward algorithm, also known as the Baum-Welch algorithm, which is described in <ref> (Rabiner, 1989) </ref>. It is an iterative algorithm that maximizes the likelihood of the training sequences. We have modified the algorithm to implement maximum a posteriori (MAP) estimation. <p> It is an iterative algorithm that maximizes the likelihood of the training sequences. We have modified the algorithm to implement maximum a posteriori (MAP) estimation. In the basic algorithm the expected number of times a certain transition or letter emis sion is used by the training sequences is calculated <ref> (Rabiner, 1989) </ref>. For a letter emission probability P (xjq) this is called n (xjq). Then the reestimated parameter is ^ P (xjq) = P ; (4) where the sum is over all the characters x 0 in the alphabet, such as the 20 amino acids. <p> There exists a dynamic programming technique, called the Viterbi algorithm <ref> (Rabiner, 1989) </ref>, that can find the best alignment and its probability without going through all the possible alignments. It is this best alignment to the model that is used to produce multiple alignments of a set of sequences. For each of the sequences the alignment to the model is found. <p> This probability can be calculated efficiently without having to explicitly consider all the possible alignments by the forward algorithm <ref> (Rabiner, 1989) </ref>. The negative logarithm of this probability is called the negative log-likelihood score, NLL (x 1 : : : x L jmodel) = log Prob (x 1 : : : x L jmodel) (8) Any sequence can be compared to a model by calculating this NLL score.
Reference: <author> Santner, T. J. & Duffy, D. E. </author> <year> (1989). </year> <title> The Statistical Analysis of Discrete Data. </title> <address> New York: </address> <publisher> Springer Verlag. </publisher> <pages> 22 </pages>
References-found: 19

