URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3833/3833.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: g@umiacs.umd.edu  
Title: Sorting on Clusters of SMPs  
Author: David R. Helman Joseph JaJa fhelman, joseph 
Keyword: Parallel Algorithms, Generalized Sorting, Integer Sorting, Sorting by Regular Sam pling, Parallel Performance.  
Note: Supported in part by NSF grant No. CCR-9627210 and NSF HPCC/GCAG grant No. BIR-9318183.  
Date: August 28, 1997  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies Department of Electrical Engineering, University of Maryland,  
Abstract: Clusters of symmetric multiprocessors (SMPs) have emerged as the primary candidates for large scale multiprocessor systems. In this paper, we introduce an efficient sorting algorithm for clusters of SMPs. This algorithm relies on a novel scheme for stably sorting on a single SMP coupled with balanced regular communication on the cluster. Our SMP algorithm seems to be asymptotically faster than any of the published algorithms we are aware of. The algorithms were implemented in C using Posix Threads and the SIMPLE library of communication primitives and run on a cluster of DEC AlphaServer 2100A systems. Our experimental results verify the scalability and efficiency of our proposed solution and illustrate the importance of considering both memory hierarchy and the overhead of shifting to multiple nodes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal, B. Alpern, A. Chandra, and M. Snir. </author> <title> A Model for Heirarchical Memory. </title> <booktitle> In Proceedings of the 19th Annual ACM Symposium of Theory of Computing, </booktitle> <pages> pages 305-314, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Alternatively, there are a number of models which allow for any arbitrary number of memory levels. Focusing on the fact that access to different levels of memory are achieved at differing costs, Aggarwal et al. <ref> [1] </ref> introduced the Hierarchical Memory Model (HMM), in which access to location x requires time f (x), where f (x) is any monotonic nondecreasing function. <p> Randomized Duplicates [RD], an input of duplicates in which each processor fills an array T with some constant number range (range is 32 for our work) of random values between 0 and (range 1) whose sum is S. The first T <ref> [1] </ref> p values of the input are then set to a random value between 0 and (range 1), the next T [2] p values of the input are then set to another random value between 0 and (range 1), and so forth.
Reference: [2] <author> A. Aggarwal, A. Chandra, and M. Snir. </author> <title> Heirarchical Memory with Block Transfer. </title> <booktitle> In Proceedings of the 28th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 204-216, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Taking note of the fact that the latency of memory access makes it economical to fetch a block of data, Aggarwal, Chandra, and Snir <ref> [2] </ref> extended this model to the Hierarchical Memory with Block Transfer Model (BT). In this model, accessing t consecutive locations beginning with location x requires time f (x) + t. Finally. <p> The first T [1] p values of the input are then set to a random value between 0 and (range 1), the next T <ref> [2] </ref> p values of the input are then set to another random value between 0 and (range 1), and so forth.
Reference: [3] <author> A. Aggarwal and G. Plaxton. </author> <title> Optimal Parallel Sorting in Multi-Level Storage. </title> <booktitle> In Proceedings of the Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 659-668, </pages> <year> 1994. </year>
Reference-contexts: On the other hand, from the perspective of the individual SMP, there are fewer choices for sorting on hierarchical shared memory machines. These include distribution sort [4], greed sort [13], balance sort [14], sharesort <ref> [3] </ref>, simple randomized merge sort [8], radix sort, and the sorting algorithm of Varman et al. [16, 15]. Unfortunately, none of these algorithms by themselves is sufficient to achieve efficient performance on an SMP cluster. <p> These are sophisticated versions of merge sort, designed to make optimal use of multiple independent disks. However, straightforward merging is inherently sequential, so without modifications these algorithms would not be expected to perform efficiently with multiple processors. Finally, there is sharesort <ref> [3] </ref>, which is a hybrid of the first two approaches. Briefly, the input of n elements is evenly divided into n fl subsets, where fl is some constant between 0 and 1, and the resulting subsets are recursively sorted.
Reference: [4] <author> A. Aggarwal and J. Vitter. </author> <title> The Input/Output Complexity of Sorting and Related Problems. </title> <journal> Communications of the ACM, </journal> <volume> 31 </volume> <pages> 1116-1127, </pages> <year> 1988. </year>
Reference-contexts: The first is a variation on sample sort and the other is a variation on the approach of sorting by regular sampling. On the other hand, from the perspective of the individual SMP, there are fewer choices for sorting on hierarchical shared memory machines. These include distribution sort <ref> [4] </ref>, greed sort [13], balance sort [14], sharesort [3], simple randomized merge sort [8], radix sort, and the sorting algorithm of Varman et al. [16, 15]. Unfortunately, none of these algorithms by themselves is sufficient to achieve efficient performance on an SMP cluster. <p> It is instructive to start with a brief overview of a number of models that have been proposed to capture the performance of multilevel hierarchical memories. Many of the models in the literature are specifically limited to two-level memories. Aggarwal and Vitter <ref> [4] </ref> first proposed a simple model for main memory and disks which recognized the importance of spatial locality. In their uniprocessor model, D blocks of B contiguous records can be transferred between primary and secondary memory in a single I/O. <p> For example, distribution sort <ref> [4] </ref> and its more refined version balance sort [14] partition the input elements into buckets using a set of approximately evenly spaced splitters and then sort the contents of each bucket recursively.
Reference: [5] <author> B. Alpern, L. Carter, E. Feig, and T. Selker. </author> <title> The Uniform Memory Hierarchy Model of Compu-atation. </title> <journal> Algorithmica, </journal> <volume> 12 </volume> <pages> 72-109, </pages> <year> 1994. </year>
Reference-contexts: These models all assume that while the buses which connect the various levels of memory might be simultaneously active, this only occurs in order to cooperate on a single transfer. Partly in response to this limitation, Alpern et al. <ref> [5] </ref> proposed the Uniform Memory Hierarchy Model (UMH). <p> This model has 3 been extended to the parallel domain in two ways as described in <ref> [5] </ref> and [13]. However, the UMH model is unnecessarily complicated for use with clusters of SMPs and requires significant refinements to capture the corresponding hybrid architecture.
Reference: [6] <author> D.A. Bader and J. JaJa. </author> <title> Practical Parallel Algorithms for Dynamic Data Redistribution, Median Finding, and Selection. </title> <institution> Technical Report CS-TR-3494 and UMIACS-TR-95-74, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> July </month> <year> 1995. </year> <booktitle> Presented at the 10th International Parallel Processing Symposium, </booktitle> <pages> pages 292-301, </pages> <address> Honolulu, HI, </address> <month> April 15-19, </month> <year> 1996. </year>
Reference-contexts: We repeat the pseudocode here for convenience, where we replace each sequential step where appropriate by a multithreaded SMP implementation. Note that the communication primitives mentioned are described in detail in <ref> [6] </ref>. * Step (1): Using p threads, each node N i (1 i N ) randomly assigns each of its n N elements to one of N buckets. <p> Finally, Steps (2), (5), and (7) call the communication primitives transpose, bcast, and transpose, respectively. The analysis of these primitives in <ref> [6] </ref> shows that with high probability these three steps require T comm (n; p) 2n (N1) , T comm (n; p) 2 (N1) and T comm (n; p) 5:24n (N1) , respectively.
Reference: [7] <author> D.A. Bader and J. JaJa. </author> <title> SIMPLE: A Methodology for Programming High Performance Algorithms on Clusters of Symmetric Multiprocessors. </title> <institution> CS-TR-3798 and UMIACS-TR-97-48 Technical Report, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: Each Alpha 21064A processor has a 16KB primary data cache and a 4MB secondary data cache. The AlphaServers are connected using the Digital Gigaswitch/ATM and OC-3c adapter cards, which have a peak bandwidth rating of 155.52 Mbps. Internode communication is effected by calls to the SIMPLE collective communication primitives <ref> [7] </ref>. 4.1 Sorting Benchmarks We tested our code on a variety of benchmarks, each of which had both a 32-bit integer version and a 64-bit double precision floating point number (double) version.
Reference: [8] <author> R. Barve, E. Grove, and J. Vitter. </author> <title> Simple Randomized Mergesort on Parallel Disks. </title> <booktitle> In Proceedings of the Eighth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 109-118, </pages> <address> Padua, Italy, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: On the other hand, from the perspective of the individual SMP, there are fewer choices for sorting on hierarchical shared memory machines. These include distribution sort [4], greed sort [13], balance sort [14], sharesort [3], simple randomized merge sort <ref> [8] </ref>, radix sort, and the sorting algorithm of Varman et al. [16, 15]. Unfortunately, none of these algorithms by themselves is sufficient to achieve efficient performance on an SMP cluster. Efficient algorithms for distributed memory machines tend to confine interprocessor communication to a minimum number of regular balanced exchanges. <p> While they are efficient in memory access, they are not optimal in their computational costs since at each level of recursion they sort the elements in blocks corresponding to the size of the cache. Other sorting algorithms such as greed sort [13] and simple randomized merge sort <ref> [8] </ref> are bottom-up sorting algorithms. These are sophisticated versions of merge sort, designed to make optimal use of multiple independent disks. However, straightforward merging is inherently sequential, so without modifications these algorithms would not be expected to perform efficiently with multiple processors.
Reference: [9] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: &lt; n Note that the analysis indicates that the bus bandwidth is a more serious limiting factor than the cluster interconnect bandwidth, especially in view of the fact that the bus bandwidth is not scalable like the cluster interconnect bandwidth. 4 Performance Evaluation Our algorithms were implemented using Posix Threads <ref> [9] </ref> and run on a DEC Alpha Cluster. Our DEC Alpha cluster consists of 10 AlphaServer 2100A systems, each of which holds 4 Alpha 21064A proces 9 sors running each at 275 MHz. Each Alpha 21064A processor has a 16KB primary data cache and a 4MB secondary data cache.
Reference: [10] <author> D.R. Helman, D.A. Bader, and J. JaJa. </author> <title> A Randomized Parallel Sorting Algorithm With an Experimental Study. </title> <institution> Technical Report CS-TR-3669 and UMIACS-TR-96-53, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> August </month> <year> 1996. </year> <note> Submitted to jpdc. 20 </note>
Reference-contexts: From the perspective of the cluster, any algorithm which performs well for distributed memory machines would be a reasonable candidate. In particular, we have identified two such algorithms in our previous work <ref> [11, 10, 12] </ref>. The first is a variation on sample sort and the other is a variation on the approach of sorting by regular sampling. On the other hand, from the perspective of the individual SMP, there are fewer choices for sorting on hierarchical shared memory machines. <p> By contrast, the transfer of w noncontiguous words would require w time. We capture performance on an SMP cluster in exactly the same way as described in <ref> [11, 10, 12] </ref>. We view a parallel algorithm as a sequence of local SMP computations interleaved with communication steps, where we allow computation and communication to overlap. <p> that the optimal choices for m and z are actually slightly smaller than suggested by our analysis, since they depend on a variety of factors besides those captured in our model. 3.2 Sorting On a Cluster of SMPs We have already developed both a deterministic [11, 12] and a randomized <ref> [11, 10] </ref> sample sort algorithm which we have shown to be very efficient for message passing platforms. Either would be an appropriate choice for a cluster of SMPs, but we chose the randomized sample sort because it proved to be slightly faster in its implementation. <p> Note that, with high probability, no node has received more than ff 2 n N elements, where ff 2 is a constant to be defined later. Recalling that we established in <ref> [10] </ref> that with high probability c 1 2, ff 2 2:62, and c 2 5:42, the analysis of our sample sort algorithm is as follows. <p> The first T [1] p values of the input are then set to a random value between 0 and (range 1), the next T [2] p values of the input are then set to another random value between 0 and (range 1), and so forth. See <ref> [10] </ref> for a detailed justification of these benchmarks. 4.2 Experimental Results for a Single SMP We begin by experimentally determining the optimal values of the parameters m and z, where m is the block size in Step (1A) and z is the z-way merge used in Step (1B).
Reference: [11] <author> D.R. Helman, D.A. Bader, and J. JaJa. </author> <title> Parallel Algorithms for Personalized Communication and Sorting With an Experimental Study. </title> <booktitle> In Proceedings of the Eighth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 211-220, </pages> <address> Padua, Italy, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: From the perspective of the cluster, any algorithm which performs well for distributed memory machines would be a reasonable candidate. In particular, we have identified two such algorithms in our previous work <ref> [11, 10, 12] </ref>. The first is a variation on sample sort and the other is a variation on the approach of sorting by regular sampling. On the other hand, from the perspective of the individual SMP, there are fewer choices for sorting on hierarchical shared memory machines. <p> By contrast, the transfer of w noncontiguous words would require w time. We capture performance on an SMP cluster in exactly the same way as described in <ref> [11, 10, 12] </ref>. We view a parallel algorithm as a sequence of local SMP computations interleaved with communication steps, where we allow computation and communication to overlap. <p> Yet another algorithm is an adaptation of our sorting by regular sampling <ref> [11, 12] </ref>, which we originally developed for distributed memory machines. As modified for an SMP, this algorithm is similar to the parallel sorting by regular sampling (PSRS) algorithm, except that our algorithm can be easily implemented as a stable sort. <p> Finally, stability can be preserved in the merging of Step (5) in exactly the same fashion as Step (1B). Hence, our algorithm results in a stable integer sort. Before establishing the complexity of this algorithm, we need the results of the following theorem developed in <ref> [11, 12] </ref>: Theorem 1: At the completion of the partitioning in Step (4), no more than p + n elements will be associated with any splitter, for n p 2 and p 2 . <p> However, our experimental investigation shows that the optimal choices for m and z are actually slightly smaller than suggested by our analysis, since they depend on a variety of factors besides those captured in our model. 3.2 Sorting On a Cluster of SMPs We have already developed both a deterministic <ref> [11, 12] </ref> and a randomized [11, 10] sample sort algorithm which we have shown to be very efficient for message passing platforms. Either would be an appropriate choice for a cluster of SMPs, but we chose the randomized sample sort because it proved to be slightly faster in its implementation. <p> that the optimal choices for m and z are actually slightly smaller than suggested by our analysis, since they depend on a variety of factors besides those captured in our model. 3.2 Sorting On a Cluster of SMPs We have already developed both a deterministic [11, 12] and a randomized <ref> [11, 10] </ref> sample sort algorithm which we have shown to be very efficient for message passing platforms. Either would be an appropriate choice for a cluster of SMPs, but we chose the randomized sample sort because it proved to be slightly faster in its implementation.
Reference: [12] <author> D.R. Helman, J. JaJa, and D.A. Bader. </author> <title> A New Deterministic Parallel Sorting Algorithm With an Experimental Evaluation. </title> <institution> Technical Report CS-TR-3670 and UMIACS-TR-96-54, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> August </month> <year> 1996. </year> <note> Submitted to jea. </note>
Reference-contexts: From the perspective of the cluster, any algorithm which performs well for distributed memory machines would be a reasonable candidate. In particular, we have identified two such algorithms in our previous work <ref> [11, 10, 12] </ref>. The first is a variation on sample sort and the other is a variation on the approach of sorting by regular sampling. On the other hand, from the perspective of the individual SMP, there are fewer choices for sorting on hierarchical shared memory machines. <p> By contrast, the transfer of w noncontiguous words would require w time. We capture performance on an SMP cluster in exactly the same way as described in <ref> [11, 10, 12] </ref>. We view a parallel algorithm as a sequence of local SMP computations interleaved with communication steps, where we allow computation and communication to overlap. <p> Yet another algorithm is an adaptation of our sorting by regular sampling <ref> [11, 12] </ref>, which we originally developed for distributed memory machines. As modified for an SMP, this algorithm is similar to the parallel sorting by regular sampling (PSRS) algorithm, except that our algorithm can be easily implemented as a stable sort. <p> Finally, stability can be preserved in the merging of Step (5) in exactly the same fashion as Step (1B). Hence, our algorithm results in a stable integer sort. Before establishing the complexity of this algorithm, we need the results of the following theorem developed in <ref> [11, 12] </ref>: Theorem 1: At the completion of the partitioning in Step (4), no more than p + n elements will be associated with any splitter, for n p 2 and p 2 . <p> However, our experimental investigation shows that the optimal choices for m and z are actually slightly smaller than suggested by our analysis, since they depend on a variety of factors besides those captured in our model. 3.2 Sorting On a Cluster of SMPs We have already developed both a deterministic <ref> [11, 12] </ref> and a randomized [11, 10] sample sort algorithm which we have shown to be very efficient for message passing platforms. Either would be an appropriate choice for a cluster of SMPs, but we chose the randomized sample sort because it proved to be slightly faster in its implementation.
Reference: [13] <author> M. Nodine and J. Vitter. </author> <title> Large-Scale Sorting in Parallel Memories. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 29-39, </pages> <address> Newport, RI, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: On the other hand, from the perspective of the individual SMP, there are fewer choices for sorting on hierarchical shared memory machines. These include distribution sort [4], greed sort <ref> [13] </ref>, balance sort [14], sharesort [3], simple randomized merge sort [8], radix sort, and the sorting algorithm of Varman et al. [16, 15]. Unfortunately, none of these algorithms by themselves is sufficient to achieve efficient performance on an SMP cluster. <p> This model has 3 been extended to the parallel domain in two ways as described in [5] and <ref> [13] </ref>. However, the UMH model is unnecessarily complicated for use with clusters of SMPs and requires significant refinements to capture the corresponding hybrid architecture. <p> While they are efficient in memory access, they are not optimal in their computational costs since at each level of recursion they sort the elements in blocks corresponding to the size of the cache. Other sorting algorithms such as greed sort <ref> [13] </ref> and simple randomized merge sort [8] are bottom-up sorting algorithms. These are sophisticated versions of merge sort, designed to make optimal use of multiple independent disks. However, straightforward merging is inherently sequential, so without modifications these algorithms would not be expected to perform efficiently with multiple processors.
Reference: [14] <author> M. Nodine and J. Vitter. </author> <title> Deterministic Distribution Sort in Shared and Distributed Memory Multiprocessors. </title> <booktitle> In Proceedings of the Fifth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 120-129, </pages> <address> Velen, Germany, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: On the other hand, from the perspective of the individual SMP, there are fewer choices for sorting on hierarchical shared memory machines. These include distribution sort [4], greed sort [13], balance sort <ref> [14] </ref>, sharesort [3], simple randomized merge sort [8], radix sort, and the sorting algorithm of Varman et al. [16, 15]. Unfortunately, none of these algorithms by themselves is sufficient to achieve efficient performance on an SMP cluster. <p> In this model, accessing t consecutive locations beginning with location x requires time f (x) + t. Finally. Nodine and Vitter <ref> [14] </ref> generalized this model to the parallel domain in exactly the same fashion as the D-disk model. The result is the Parallel Hierarchical Model with Block Transfer (P-BT), in which the individual memory hierarchies are linked at the CPUs by some appropriate network topology. <p> For example, distribution sort [4] and its more refined version balance sort <ref> [14] </ref> partition the input elements into buckets using a set of approximately evenly spaced splitters and then sort the contents of each bucket recursively.
Reference: [15] <author> P. Varman, B. Iyer, D. Haderle, and S. Dunn. </author> <title> Parallel Merging: Algorithm and Implementation Results. </title> <journal> Parallel Computing, </journal> <volume> 15 </volume> <pages> 165-177, </pages> <year> 1990. </year>
Reference-contexts: These include distribution sort [4], greed sort [13], balance sort [14], sharesort [3], simple randomized merge sort [8], radix sort, and the sorting algorithm of Varman et al. <ref> [16, 15] </ref>. Unfortunately, none of these algorithms by themselves is sufficient to achieve efficient performance on an SMP cluster. Efficient algorithms for distributed memory machines tend to confine interprocessor communication to a minimum number of regular balanced exchanges. <p> However, depending on the input, 5 this data rearrangement could cause very poor cache performance, which would make this algorithm uncompetitive with other possible choices. Another approach proposed by Varman et al. <ref> [16, 15] </ref>. starts by having each processor sorts n p elements from the input set using C-way merge sort, where C is the size of the cache.
Reference: [16] <author> P. Varman, B. Iyer, and S. scheufler. </author> <title> A Multiprocessor Algorithm for Merging Multiple Sorted Lists. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages 22-26. </pages>
Reference-contexts: These include distribution sort [4], greed sort [13], balance sort [14], sharesort [3], simple randomized merge sort [8], radix sort, and the sorting algorithm of Varman et al. <ref> [16, 15] </ref>. Unfortunately, none of these algorithms by themselves is sufficient to achieve efficient performance on an SMP cluster. Efficient algorithms for distributed memory machines tend to confine interprocessor communication to a minimum number of regular balanced exchanges. <p> However, depending on the input, 5 this data rearrangement could cause very poor cache performance, which would make this algorithm uncompetitive with other possible choices. Another approach proposed by Varman et al. <ref> [16, 15] </ref>. starts by having each processor sorts n p elements from the input set using C-way merge sort, where C is the size of the cache.
Reference: [17] <author> J. Vitter and E. Shriver. </author> <title> Algorithms for Parallel Memory I: Two-Level Memories. </title> <journal> Algorithmica, </journal> <volume> 12 </volume> <pages> 110-147, </pages> <year> 1994. </year>
Reference-contexts: Aggarwal and Vitter [4] first proposed a simple model for main memory and disks which recognized the importance of spatial locality. In their uniprocessor model, D blocks of B contiguous records can be transferred between primary and secondary memory in a single I/O. Vitter and Shriver <ref> [17] </ref> proposed the more realistic D-disk model, in which secondary storage is partitioned into D physically distinct disk drives. In this model, D blocks can be transfered in a single I/O but only if no two blocks are from the same disk. Finally, Vitter and Shriver [17] extended the D-disk model <p> Vitter and Shriver <ref> [17] </ref> proposed the more realistic D-disk model, in which secondary storage is partitioned into D physically distinct disk drives. In this model, D blocks can be transfered in a single I/O but only if no two blocks are from the same disk. Finally, Vitter and Shriver [17] extended the D-disk model to the parallel domain. In the so-called parallel disk model, the processors are directly connected to one another by some interconnection network which can be modeled by any of the standard models (e.g. PRAM, fixed-connection network).
References-found: 17

