URL: http://www.research.microsoft.com/users/jckrumm/papers/easyliving/smartspaces.ps
Refering-URL: http://www.research.microsoft.com/research/vision/
Root-URL: http://www.research.microsoft.com
Title: Joint DARPA/NIST Smart Spaces  
Author: Steve Shafer, John Krumm, Barry Brumitt, Brian Meyers, Mary Czerwinski, Daniel Robbins 
Address: One Microsoft Way Redmond, WA 98052  
Affiliation: Gaithersburg, Maryland The New EasyLiving Project at Microsoft Research  Microsoft Research Microsoft Corporation  
Date: July 30-31, 1998,  
Note: Workshop,  
Abstract: EasyLiving is a new project in intelligent environments at Microsoft Research. We are working to make computing more accessible and more pervasive than todays desktop computer. More specifically, our goal is to develop a prototype architecture and technologies for building intelligent environments that facilitate the unencumbered interaction of people with other people, with computers, and with devices. This paper describes our goals, design decisions, and applications of EasyLiving. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. H. Coen, </author> <title> Design Principals for Intelligent Environments, </title> <booktitle> presented at AAAI Spring Symposium on Intelligent Environments, </booktitle> <address> Stanford, CA, </address> <year> 1998. </year>
Reference-contexts: Cameras give rich data that can be used for tracking and identifying people and objects and for measuring them in 3D. In the context of intelligent environments, cameras have been used to track people in Michael Coens Intelligent Room at MITs AI Lab <ref> [1] </ref> and to understand gestures in Mark Lucentes Visualization Space at IBM Research [2]. To accommodate the video sensing demands of EasyLiving, we are building a vision module that gives both color and range images.
Reference: [2] <author> M. Lucente, G.-J. Zwart, and A. George, </author> <title> Visualization Space: A Testbed for Deviceless Multimodal User Interface, </title> <booktitle> presented at AAAI Spring Symposium on Intelligent Environments, </booktitle> <address> Stanford, CA, </address> <year> 1998. </year>
Reference-contexts: In the context of intelligent environments, cameras have been used to track people in Michael Coens Intelligent Room at MITs AI Lab [1] and to understand gestures in Mark Lucentes Visualization Space at IBM Research <ref> [2] </ref>. To accommodate the video sensing demands of EasyLiving, we are building a vision module that gives both color and range images. It will consist of between two and four cameras, packaged together, with control and processing done on one PC.
Reference: [3] <author> M. J. Swain and D. H. Ballard, </author> <title> Color Indexing, </title> <journal> International Journal of Computer Vision, </journal> <volume> vol. 7, </volume> <pages> pp. 11-32, </pages> <year> 1991. </year>
Reference-contexts: It will consist of between two and four cameras, packaged together, with control and processing done on one PC. We will use the color image to make color histograms, which have been shown to work well for identifying objects <ref> [3] </ref>. Our own experiments show that color histograms are effective at reidentifying people that have already been seen as long as their clothing doesnt change. The range images will come from passive stereo, and they will be used primarily for image segmentation.
Reference: [4] <author> K. A. Bharat and L. Cardelli, </author> <title> Migratory Applications, </title> <booktitle> presented at UIST '95, </booktitle> <address> Pittsburgh, PA, </address> <year> 1995. </year>
Reference-contexts: The migrating user interface, such as the family-room-to-kitchen example above, requires that an application, or at least its user interface, be able to move smoothly from one room to another. In their work on the Obliq distributed scripting language, Bharat and Cardelli <ref> [4] </ref> used a software architecture that moves whole applications between computers using agents. The application, including its user interface, is packaged as an agent, and each computer contains software that can receive such an agent and start it running.
Reference: [5] <author> S. Oviatt, </author> <title> Multimodal Interactive Maps: Designing for Human Performance, </title> <journal> Human-Computer Interaction, </journal> <volume> vol. 12, </volume> <pages> pp. 93-129, </pages> <year> 1997. </year> <title> with three hotspots. The user interface migrates between the three screens (including the large one on the back wall) as the user moves from hotspot to hotspot. Based on color histograms, the program has identified the contents of the hotspots as object 1, background, and object 2. </title>
Reference-contexts: The use of speech and gestures simultaneously, however, is a relatively new area of research. Sharon Oviatt has studied the use of speech and gesture in a pen-based, geographic map application <ref> [5] </ref>. When given the choice to issue commands with a pen and/or voice, users preferred to convey locatives (i.e. points, lines, and areas) with the pen, while they preferred speech for describing objects and giving commands.
References-found: 5

