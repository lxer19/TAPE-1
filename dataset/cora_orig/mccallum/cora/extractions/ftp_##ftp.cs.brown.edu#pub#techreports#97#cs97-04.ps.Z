URL: ftp://ftp.cs.brown.edu/pub/techreports/97/cs97-04.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-97-04.html
Root-URL: http://www.cs.brown.edu
Abstract-found: 0
Intro-found: 1
Reference: [BPS70] <author> L. E. Baum et al., </author> <title> A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains, </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 41 , no. 1, </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference-contexts: In contrast, we directly incorporate odometric information into the Baum-Welch algorithm. To prove the correctness of our extension to the Baum-Welch algorithm, in terms of local maximization of the likelihood, we follow the proof techniques given by Levinson et al [LRS83], Baum et al <ref> [BPS70] </ref> and Juang et al [JLS86]. Levinson et al provide an extensive and coherent introduction to learning standard hmms with discrete states and observations. They follow Baum's proof for showing that the learned model locally maximizes the likelihood function P (datajmodel). <p> They follow Baum's proof for showing that the learned model locally maximizes the likelihood function P (datajmodel). They also show how to handle simple linear constraints specified over single parameters (x ij *). Baum et al <ref> [BPS70] </ref> provide and prove the reestimation formulae for hmms with continuous observations, where the density function for the observations in each state must be log-concave. Their reestimation formulae were later proved by Dempster et al [DLR77] to be a special case of the Expectation Maximization family of algorithms. <p> The latter must satisfy geometrical constraints. Under such conditions an extension of the update formulae and a proof of their correctness is required. The proof is along the lines of those given by Baum et al <ref> [BPS70] </ref>, and by Juang [Jua85], and is given in section 4.3. 4.1 Computing State-Occupation Probabilities Following Rabiner [Rab89], we first compute the forward (ff) and backward (fi) matrices. <p> There are several proof techniques for the correctness of the reestimation formulae for the standard Baum-Welch algorithm (under various kinds of observation matrix B) <ref> [BPS70, DLR77, LRS83, Jua85, JLS86] </ref>. Our proof uses the same approach as the latter two. It is straightforward to show that maximization of the likelihood function with respect to each of the parameters separately is equivalent to its maximization with respect the complete model. <p> Hence the above theorem can not be applied here. Instead, we use the technique of maximizing Baum's auxiliary function, following Section 4 of the paper by Baum et al <ref> [BPS70] </ref>. We shall denote by x , where x is some relation matrix, the model whose A and B matrices are the same as those of , but whose relation matrix R is replaced by the matrix x. <p> We also use the notation: f ij (r t ) = Q D ij (r k Baum et al <ref> [BPS70] </ref> introduce an auxiliary function, Q, and prove that maximizing it is the same as increasing the likelihood. <p> X D X [log (f s t1 ;s t (r k s t1 ;s t )] : (12) 2 The independence assumption is not necessary, but it makes for a clearer proof. 8 Since the normal distribution is strictly log-concave, a slight adaptation to the proof of Theorem 4:1 in <ref> [BPS70] </ref> is sufficient for showing that Q above has a unique global maximum as a function of k ij and k ij , which is the unique point in which the partial derivatives of Q according to k ij and k ij are 0.
Reference: [BS68] <author> L. E. Baum and G. R. Sell, </author> <title> Growth Transformations for Functions on Manifolds, </title> <journal> Pacific Journal of Mathematics, </journal> <volume> 27 , no. 2, </volume> <pages> 211-227, </pages> <year> 1968. </year>
Reference-contexts: Hence, we break the proof, and prove that each of the reestimation formulae indeed maximizes the likelihood function with respect to the associated reestimated parameter. 4.3.1 Transitions and Observations To prove the correctness of formulae (3) and (4), we use the central theorem of Baum and Sell <ref> [BS68] </ref>, which claims 1 that for x = fx ij g s.t. x ij &gt; 0; 1 j N , and P N given a homogeneous polynomial P in the variable x ij , with nonnegative coefficients, the transformation x ij = @P N X x ik @x ik satisfies P
Reference: [CKK96] <author> A. R. Cassandra, L. P. Kaelbling and J. A. Kurien, </author> <title> Acting Under Uncertainty: Discrete Bayesian Models for Mobile-Robot Navigation, </title> <booktitle> In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <year> 1996. </year>
Reference-contexts: These are appropriate in contexts such as office building, road network, or sewerage system modeling. Specifically, such pomdp models have proven particularly useful as a basis for robot navigation in buildings, providing a sound method for localization and planning <ref> [SK95, NPB95, CKK96] </ref>. Much of the previous work on planning assumed that the pomdp model is acquired manually; such manual acquisition can be very tedious and it is often difficult to obtain correct probabilities.
Reference: [CKS90] <author> P. Cheeseman et al., </author> <title> AutoClass: A Bayesian Classification System, </title> <editor> In J. W. Shavlik and T. G. Dietterich, editors, </editor> <booktitle> Readings in Machine Learning, </booktitle> <pages> pp. 296-306. </pages> <publisher> Morgan-Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: We then compute fl and ~ values, with the fl values being 0 or 1, since we use a deterministic clustering algorithm (it might be beneficial to use a stochastic clustering algorithm, such as Autoclass <ref> [CKS90] </ref>). The A, B, and R matrices are all estimated from fl and ~ as described in the previous section.
Reference: [DLR77] <author> A. P. Dempster, N. M. Laird and D. B. Rubin, </author> <title> Maximum Likelihood from Incomplete Data via the EM Algorithm, </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 39 , no. 1, </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Baum et al [BPS70] provide and prove the reestimation formulae for hmms with continuous observations, where the density function for the observations in each state must be log-concave. Their reestimation formulae were later proved by Dempster et al <ref> [DLR77] </ref> to be a special case of the Expectation Maximization family of algorithms. Baum's work was further extended by Liporace [Lip82], to the case of observations whose density function is multivariate and elliptically symmetric. <p> There are several proof techniques for the correctness of the reestimation formulae for the standard Baum-Welch algorithm (under various kinds of observation matrix B) <ref> [BPS70, DLR77, LRS83, Jua85, JLS86] </ref>. Our proof uses the same approach as the latter two. It is straightforward to show that maximization of the likelihood function with respect to each of the parameters separately is equivalent to its maximization with respect the complete model.
Reference: [JLS86] <author> B. H. Juang, S. E. Levinson and M. M. Sondhi, </author> <title> Maximum Likelihood Estimation for Multivariate Mixture Observations of Markov Chains, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 32 , no. 2, </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: In contrast, we directly incorporate odometric information into the Baum-Welch algorithm. To prove the correctness of our extension to the Baum-Welch algorithm, in terms of local maximization of the likelihood, we follow the proof techniques given by Levinson et al [LRS83], Baum et al [BPS70] and Juang et al <ref> [JLS86] </ref>. Levinson et al provide an extensive and coherent introduction to learning standard hmms with discrete states and observations. They follow Baum's proof for showing that the learned model locally maximizes the likelihood function P (datajmodel). <p> Their reestimation formulae were later proved by Dempster et al [DLR77] to be a special case of the Expectation Maximization family of algorithms. Baum's work was further extended by Liporace [Lip82], to the case of observations whose density function is multivariate and elliptically symmetric. Juang et al <ref> [Jua85, JLS86] </ref> provide an additional extension to handle multivariate general Gaussian mixtures, which are not necessarily symmetric. In all of the above work the model consists of two matrices|the transition probability matrix and the observation distribution matrix, as well as an initial-state distribution vector. <p> There are several proof techniques for the correctness of the reestimation formulae for the standard Baum-Welch algorithm (under various kinds of observation matrix B) <ref> [BPS70, DLR77, LRS83, Jua85, JLS86] </ref>. Our proof uses the same approach as the latter two. It is straightforward to show that maximization of the likelihood function with respect to each of the parameters separately is equivalent to its maximization with respect the complete model.
Reference: [Jua85] <author> B. H. Juang, </author> <title> Maximum Likelihood Estimation for Mixture Multivariate Stochastic Observations of Markov Chains, </title> <journal> AT&T Technical Journal, </journal> <volume> 64 , no. 6, </volume> <month> July-August </month> <year> 1985. </year>
Reference-contexts: Their reestimation formulae were later proved by Dempster et al [DLR77] to be a special case of the Expectation Maximization family of algorithms. Baum's work was further extended by Liporace [Lip82], to the case of observations whose density function is multivariate and elliptically symmetric. Juang et al <ref> [Jua85, JLS86] </ref> provide an additional extension to handle multivariate general Gaussian mixtures, which are not necessarily symmetric. In all of the above work the model consists of two matrices|the transition probability matrix and the observation distribution matrix, as well as an initial-state distribution vector. <p> The latter must satisfy geometrical constraints. Under such conditions an extension of the update formulae and a proof of their correctness is required. The proof is along the lines of those given by Baum et al [BPS70], and by Juang <ref> [Jua85] </ref>, and is given in section 4.3. 4.1 Computing State-Occupation Probabilities Following Rabiner [Rab89], we first compute the forward (ff) and backward (fi) matrices. <p> There are several proof techniques for the correctness of the reestimation formulae for the standard Baum-Welch algorithm (under various kinds of observation matrix B) <ref> [BPS70, DLR77, LRS83, Jua85, JLS86] </ref>. Our proof uses the same approach as the latter two. It is straightforward to show that maximization of the likelihood function with respect to each of the parameters separately is equivalent to its maximization with respect the complete model.
Reference: [KL51] <author> S. Kullback and R. A. Leibler, </author> <title> On Information and Sufficiency, </title> <journal> Annals of Mathemat--ical Statistics, </journal> <volume> 22 , no. 1, </volume> <pages> 79-86, </pages> <year> 1951. </year>
Reference-contexts: Each of the models induces a probability distribution on strings of observations; the asymmetric Kullback-Leibler divergence <ref> [KL51] </ref> between the two distributions is a measure of how good the learned model is with respect 3 Observations are omitted for clarity. 11 ment. way environment. to the true model.
Reference: [KS96a] <author> S. Koenig and R. G. Simmons, </author> <title> Passive Distance Learning for Robot Navigation, </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 266-274, </pages> <year> 1996. </year>
Reference-contexts: We prefer to learn a combined model of the world and the robot's perception and interaction with the world; this allows robust planning that takes into account likelihood of error in sensing and action. The work most closely related to ours is by Koenig and Simmons <ref> [KS96b, KS96a] </ref>, who learn pomdp models (stochastic topological maps) of a robot hallway environment. They also recognize the impossibility of learning such a model without initial information; they 2 solve the problem by using a human-provided topological map, together with further constraints on the shared structure of the model.
Reference: [KS96b] <author> S. Koenig and R. G. Simmons, </author> <title> Unsupervised Learning of Probabilistic Models for Robot Navigation, </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1996. </year>
Reference-contexts: We prefer to learn a combined model of the world and the robot's perception and interaction with the world; this allows robust planning that takes into account likelihood of error in sensing and action. The work most closely related to ours is by Koenig and Simmons <ref> [KS96b, KS96a] </ref>, who learn pomdp models (stochastic topological maps) of a robot hallway environment. They also recognize the impossibility of learning such a model without initial information; they 2 solve the problem by using a human-provided topological map, together with further constraints on the shared structure of the model.
Reference: [Lip82] <author> L. A. Liporace, </author> <title> Maximum Likelihood Estimation for Multivariate Observations of Markov Sources, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 28 , no. 5, </volume> <year> 1982. </year>
Reference-contexts: Their reestimation formulae were later proved by Dempster et al [DLR77] to be a special case of the Expectation Maximization family of algorithms. Baum's work was further extended by Liporace <ref> [Lip82] </ref>, to the case of observations whose density function is multivariate and elliptically symmetric. Juang et al [Jua85, JLS86] provide an additional extension to handle multivariate general Gaussian mixtures, which are not necessarily symmetric.
Reference: [LRS83] <author> S. E. Levinson, L. R. Rabiner and M. M. Sondhi, </author> <title> An Introduction to the Application of the Theory of Probabilistic Functions of a Markov Process to Automatic Speech Recognition, </title> <journal> The Bell System Technical Journal, </journal> <volume> 62 , no. 4, </volume> <pages> 1035-1074, </pages> <year> 1983. </year>
Reference-contexts: In contrast, we directly incorporate odometric information into the Baum-Welch algorithm. To prove the correctness of our extension to the Baum-Welch algorithm, in terms of local maximization of the likelihood, we follow the proof techniques given by Levinson et al <ref> [LRS83] </ref>, Baum et al [BPS70] and Juang et al [JLS86]. Levinson et al provide an extensive and coherent introduction to learning standard hmms with discrete states and observations. They follow Baum's proof for showing that the learned model locally maximizes the likelihood function P (datajmodel). <p> There are several proof techniques for the correctness of the reestimation formulae for the standard Baum-Welch algorithm (under various kinds of observation matrix B) <ref> [BPS70, DLR77, LRS83, Jua85, JLS86] </ref>. Our proof uses the same approach as the latter two. It is straightforward to show that maximization of the likelihood function with respect to each of the parameters separately is equivalent to its maximization with respect the complete model.
Reference: [NPB95] <author> I. Nourbakhsh, R. Powers and S. Birchfield, DERVISH: </author> <title> An Office-Navigating Robot, </title> <journal> AI Magazine, </journal> <volume> 16 , no. 1, </volume> <pages> 53-60, </pages> <year> 1995. </year>
Reference-contexts: These are appropriate in contexts such as office building, road network, or sewerage system modeling. Specifically, such pomdp models have proven particularly useful as a basis for robot navigation in buildings, providing a sound method for localization and planning <ref> [SK95, NPB95, CKK96] </ref>. Much of the previous work on planning assumed that the pomdp model is acquired manually; such manual acquisition can be very tedious and it is often difficult to obtain correct probabilities.
Reference: [Rab89] <author> L. R. Rabiner, </author> <title> A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77 , no. 2, </volume> <pages> 257-285, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: An ultimate goal is to be able to learn such models automatically, both for robustness and in order to cope with new and changing environments. Since pomdp models are a 1 simple extension of hmms, they can, theoretically, be learned with a simple extension to the Baum-Welch algorithm <ref> [Rab89] </ref> for learning hmms. However, without a strong prior constraint on the structure of the model, the Baum-Welch algorithm does not perform very well: it is slow to converge, requires a great deal of data, and often becomes stuck in local maxima. <p> sequence E is of length T ; each element is a pair hr t ; V t i, where r t is the observed relation vector between q t1 and q t and V t is the observation vector at time t. 4 Algorithm Our algorithm extends the Baum-Welch algorithm <ref> [Rab89] </ref> to deal with the relational information and the factored observation sets. The Baum-Welch algorithm has been proven to provide a monotonically increasing convergence of P (Ej) to a local maximum, for discrete observations as well as for continuous ones (with some restrictions), as discussed in Section 2. <p> Under such conditions an extension of the update formulae and a proof of their correctness is required. The proof is along the lines of those given by Baum et al [BPS70], and by Juang [Jua85], and is given in section 4.3. 4.1 Computing State-Occupation Probabilities Following Rabiner <ref> [Rab89] </ref>, we first compute the forward (ff) and backward (fi) matrices. <p> These are essentially the same formulae appearing in <ref> [Rab89] </ref>, but taking into account the density of the relational observation. 4.2 Updating Model Parameters In this phase of the algorithm, the goal is to find a new model, , that maximizes P (Ej; fl). <p> The smaller this measure is, the closer the learned model is to the true one, and the better is the result. We report our results in terms of a sampled version of the kl-divergence, as described by Rabiner <ref> [Rab89] </ref>. It is based on generating at random sequences of sufficient length (5 sequences of 1000 observations in our case) according to the distribution induced by the true model, and comparing their likelihoods according to the learned model with the true model likelihoods.
Reference: [SK95] <author> R. G. Simmons and S. Koenig, </author> <title> Probabilistic Navigation in Partially Observable Environments, </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: These are appropriate in contexts such as office building, road network, or sewerage system modeling. Specifically, such pomdp models have proven particularly useful as a basis for robot navigation in buildings, providing a sound method for localization and planning <ref> [SK95, NPB95, CKK96] </ref>. Much of the previous work on planning assumed that the pomdp model is acquired manually; such manual acquisition can be very tedious and it is often difficult to obtain correct probabilities.
Reference: [SK97] <author> H. Shatkay and L. P. Kaelbling, </author> <title> Learning Topological Maps with Weak Local Odometric Information, </title> <note> Submitted to IJCAI97, 1997. 16 </note>
Reference-contexts: Our work focuses on showing how weak metric information about the relationship between states can be used to significantly improve model learning. Such information is readily available in many cases and is often ignored during the process of learning topological maps. We have empirically shown <ref> [SK97] </ref> that the odometric ability of the robot, which allows measuring relative transformations between its configurations, can be used to learn maps of environments in a faster and more accurate fashion. In this paper we provide two extensions to the previous work. <p> In a previous paper <ref> [SK97] </ref> we have demonstrated that the use of odometry readings significantly reduces the number of iterations required for convergence, and results in significantly better models both in terms of the resulting geometric map and in terms of the measured kl-divergence from the true model. <p> In the rest of this section we describe the experimental setting in a simulated robot domain. For the sake of completeness we briefly survey the results from our previous paper <ref> [SK97] </ref>, and then empirically show that our algorithm performs well with less data. 5.1 Robot Domain The robot moves through the corridors in an office environment. <p> In addition, the number of iterations required for convergence when learning using odometry information is roughly half that required when ignoring odometry information. We used the t-test to verify the statistical significance of these results. Deeper analysis of both the qualitative and the quantitative results is given in <ref> [SK97] </ref>. To examine the influence of the amount of data on the quality of the learned models, we took one of the 5 sequences (Seq. #1) and used its prefixes of length 100 to 1000 (the complete sequence), in increments of 100, as individual sequences.
References-found: 16

