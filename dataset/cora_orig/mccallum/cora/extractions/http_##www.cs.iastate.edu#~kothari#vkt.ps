URL: http://www.cs.iastate.edu/~kothari/vkt.ps
Refering-URL: http://www.cs.iastate.edu/~kothari/
Root-URL: http://www.cs.iastate.edu
Email: @cs.iastate.edu  turner@ameslab.gov  
Title: Parallelization of Waveguide Program and Performance on a Cluster of PCs  
Author: Bogdan Vasiliu Suraj C. Kothari Dave Turner 
Address: (bogdan, kothari)  Ames, IA 50011  Ames, IA 50011  
Affiliation: Department of Computer Science  Iowa State University  Ames Laboratory  
Abstract: This paper reports on parallelization of a finite difference Waveguide program and its performance on a cluster of 64 PCs connected by a Fast Ethernet switch. This simulation program is used to study photonic band structures. A stencil-exchange communication library and a tool for automatic parallelization are used to facilitate parallelization of this complex program.. The performance study includes problems ranging in memory requirement from 50 megabytes to 5 gigabytes. The results show remarkable overall performance including super-linear speedups due to the cache memory effect. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. M. Sigalas, R. Biswas, K.M. Ho, W. Leung, G. Tuttle, and D. Crouch, </author> <title> ``Applications of Photonic Band Gap Materials,'' </title> <booktitle> 13th Annual Review of Progress in Applied Computational Electromagnetics, </booktitle> <pages> pp. 412, </pages> <address> Monterey, CA, </address> <year> 1997. </year> . 
Reference-contexts: This paper reports on the performance study using the Waveguide program [1-3] that simulates the transmission of electromagnetic waves through dielectric and/or metallic media. A group of physicists at the Ames Laboratory uses the Waveguide program as an important tool to study photonic band structures <ref> [1] </ref>. It is of great interest to explore the possibility of developing photonic devices based on photonic band structures in much the same way the components of modern microelectronics are built upon the interaction of the electronic band structures of differing materials. <p> Section 3 describes the parallelization effort. Section 4 reports the performance study. Section 5 gives conclusions. 2. The Waveguide Problem Electromagnetic (EM) waves propagating through structures of periodically modulated dielectric constants are organized into photonic bands that are analogous to the electronic band structures in crystals <ref> [1] </ref>. The Waveguide program is based on the Finite Difference Time Domain (FDTD) method.
Reference: 2. <author> K. Kunz and R. Luebbers, </author> <title> "The Finite Difference Time Domain Method for Electromagnetics," </title> <publisher> CRC Press, </publisher> <address> Boca Raton, Florida (1993). 3. http://cmp.ameslab.gov/ercap/pbg99.html </address>
Reference-contexts: The FDTD method solves the Maxwell curl equations numerically on a finite 3-dimensional grid of points <ref> [2] </ref>. The derivatives are approximated with finite differences, and time is discretized so that the EM wave propagates less than the distance between grid points for each time step.
Reference: 4. <author> Suraj Kothari and Simanta Mitra "Parallelization Agent: </author> <title> A New Approach to Parallelization of Legacy Codes," </title> <booktitle> Eight SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1997. </year> <note> 5. http://hpc00.cs.iastate.edu/WWW/paragent.html </note>
Reference: 6. <author> Michalakes, J., </author> <year> 1997c: </year> <title> "RSL: A Parallel Runtime System Library for Regional Atmospheric Models with Nesting," </title> <type> preprint ANL/MCS-P663-0597. </type>
Reference: 7. <author> Marc Snir, Steve W. Otto, Steven Huss-Lederman, David W. Walker, and Jack Dongara, </author> <title> MPI: The Complete Reference, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, London, England. </address>
Reference-contexts: The 64 PCs are connected using a Fast Ethernet switch as shown in the Figure 5. The machines were running Linux 2.0.35 with a few modifications to the kernel to get rid of some dropouts in the TCP performance. The message-passing library was MPICH <ref> [7] </ref> which provided a maximum communication rate of 8.5 MB/sec over the Fast Ethernet links with a latency of around 300 microseconds. The Fast Ethernet switch provides enough backplane switching capacity to guarantee that no bottleneck occurs.
Reference: 8. <author> Gene M. </author> <title> Amdahl, "Validity of the Single processor approach to achieving large scale computing capabilities," </title> <booktitle> In AFIPS Conference Proceedings, </booktitle> <pages> pp. 483-485, </pages> <year> 1967. </year>
Reference-contexts: Another aspect of scalability is "fixed problem size" scalability which implies that an increasing number of processors can be used to solve the same size problem in smaller and smaller amount of time. Amdahl's law <ref> [8] </ref> implies that "fixed problem size" scalability is not possible. On the contrary, Gustafson [9,10] essentially observes that "fixed problem size" scalability is possible as long as the problem size is large enough. Our results show "fixed problem size" scalability that supports Gustafson's observations.
Reference: 9. <author> John L. Gustafson, "Reevaluating Amdahls Law," </author> <booktitle> Communications of the ACM 31(5) </booktitle> <pages> 532-533, </pages> <year> 1988. </year>
Reference: 10. <author> John L. Gustafson, Gary R. Montry, and Robert E. Benner, </author> <title> "Development of parallel methods for a 1024-processor hypercube," </title> <journal> SIAM Journal on Scientific and Statistical Computing 9(4) </journal> <pages> 609-638, </pages> <year> 1988. </year>
References-found: 8

