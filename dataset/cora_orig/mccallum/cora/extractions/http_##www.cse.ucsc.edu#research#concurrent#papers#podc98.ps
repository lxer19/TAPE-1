URL: http://www.cse.ucsc.edu/research/concurrent/papers/podc98.ps
Refering-URL: http://www.cse.ucsc.edu/research/concurrent/reports/
Root-URL: http://www.cse.ucsc.edu
Email: randal@almaden.ibm.com darrell@cs.ucsc.edu  
Title: In-Place Reconstruction of Delta Compressed Files  
Author: Randal C. Burns Darrell D. E. Long 
Address: Santa Cruz  
Affiliation: Department of Computer Science Department of Computer Science IBM Almaden Research Center University of California,  
Abstract: We present and algorithm for modifying delta compressed files so that the compressed versions may be reconstructed without requiring additional memory or storage space. This allows network clients with limited resources to efficiently update software by downloading delta compressed versions over a network. Delta compression for binary files, compactly encoding a version of data with only the changed bytes from a previous version, may be used to efficiently distribute software over low bandwidth channels, such as the Internet. Traditional methods of rebuilding these delta files require memory or storage space on the target machine for both the old and new version of the file to be reconstructed. With the advent of network computing and Internet set-top boxes, many of these network attached target machines have limited additional scratch space in memory or storage. We provide an algorithm for modifying a delta compressed version file so that it may rebuild the new file version in the space that the current version occupies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Miklos Ajtai, Randal Burns, Ronald Fagin, and Larry Stockmeyer. </author> <title> Compactly encoding arbitrary inputs with differential compression. </title> <journal> IBM Research: </journal> <note> In Preparation, </note> <year> 1997. </year>
Reference-contexts: However, low bandwidth channels to network devices often makes the time to perform software update prohibitive. Differential or delta compression <ref> [5, 1] </ref>, compactly encoding a new version of a file using only the changed bytes from a previous version, can be used to reduce the size of the file to be transmitted and consequently the time to perform software update. <p> Recent advances in differencing algorithms have produced fine granularity differencing algorithms that run using time linear in the length of the input files <ref> [1, 5] </ref>. The improved algorithms allow large files without alignment or known structure to be efficiently differenced and permits the application of delta compression to backup and restore [4], file system replication, and software distribution. <p> order C sorted = fc 1 ; c 2 ; :::; c n g where c i = hf i ; t i ; l i i is an ordered triple describing the source offset, destination offset and the length, and write order is en forced, for i; j 2 <ref> [1; n] </ref> : i &lt; j () t i &lt; t j . 3. We next construct a digraph from the copy commands. <p> Previous studies have indicated that delta compression algorithms can compact data for the transmission of software <ref> [1] </ref>. Delta compression algorithms compatible with in-place reconstruction, compress a large body of distributed software by a factor of 4 to 10. This translates to a commensurate 4 to 10 times factor in the amount of time required to transmit these files over low bandwidth channel. <p> This translates to a commensurate 4 to 10 times factor in the amount of time required to transmit these files over low bandwidth channel. Over all software distributions, we found that the delta algorithm <ref> [1] </ref> we used compressed data, on average, to 8.71% it's original size and that after running our algorithm to make these files suitable for in-place reconstruction we lost only 1.56% compression for total compression of 10.27%.
Reference: [2] <author> Philip A. Berrnstein, Vassos Hadzilacos, and Nathan Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> AddisonWesley Publishing Co., </publisher> <year> 1987. </year>
Reference-contexts: Consider that we attempt to read from a memory offset that has already been written. This will result in an incorrect reconstruction since the reference file data we are attempting to read has been overwritten. This is termed a write before read (WR) conflict <ref> [2] </ref>. It turns out that write before read conflicts are the only type of data conflict that concerns us.
Reference: [3] <author> Andrew P. Black and Charles H. Burris, Jr. </author> <title> A compact representation for file versions: A preliminary report. </title> <booktitle> In Proceedings of the 5th International Conference on Data Engineering, </booktitle> <pages> pages 321329. </pages> <publisher> IEEE, </publisher> <year> 1989. </year>
Reference-contexts: Previous work that studied efficiently reconstructing versions has focused on the problem of storing many versions of the same file for versioning file systems, versioning editors or source code control <ref> [6, 3] </ref>. These applications have many versions of the same file present locally. In contrast, we look at delta reconstruction with the goal of distributing a single version of data over a network. Our application need consider only a single version at a time.
Reference: [4] <author> Randal C. Burns and Darrell D. E. </author> <title> Long. Efficient distributed backup with delta compression. </title> <booktitle> In Proceedings of the 1997 I/O in Parallel and Distributed Systems (IOPADS'97), 17 November 1997, </booktitle> <address> San Jose, CA, USA, </address> <month> November </month> <year> 1997. </year>
Reference-contexts: The improved algorithms allow large files without alignment or known structure to be efficiently differenced and permits the application of delta compression to backup and restore <ref> [4] </ref>, file system replication, and software distribution. Previous work that studied efficiently reconstructing versions has focused on the problem of storing many versions of the same file for versioning file systems, versioning editors or source code control [6, 3]. These applications have many versions of the same file present locally.
Reference: [5] <author> Randal C. Burns and Darrell D. E. </author> <title> Long. A linear time, constant space differencing algorithm. </title> <booktitle> In Proceedings of the 1997 International Performance, Computing and Communications Conference (IPCCC'97), </booktitle> <month> Feb. </month> <pages> 5-7, </pages> <address> Tempe/Phoenix, Arizona, USA, </address> <month> February </month> <year> 1997. </year>
Reference-contexts: However, low bandwidth channels to network devices often makes the time to perform software update prohibitive. Differential or delta compression <ref> [5, 1] </ref>, compactly encoding a new version of a file using only the changed bytes from a previous version, can be used to reduce the size of the file to be transmitted and consequently the time to perform software update. <p> Recent advances in differencing algorithms have produced fine granularity differencing algorithms that run using time linear in the length of the input files <ref> [1, 5] </ref>. The improved algorithms allow large files without alignment or known structure to be efficiently differenced and permits the application of delta compression to backup and restore [4], file system replication, and software distribution.
Reference: [6] <author> Christopher W. Fraser and Eugene W. Myers. </author> <title> An editor for revision control. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2):277295, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Previous work that studied efficiently reconstructing versions has focused on the problem of storing many versions of the same file for versioning file systems, versioning editors or source code control <ref> [6, 3] </ref>. These applications have many versions of the same file present locally. In contrast, we look at delta reconstruction with the goal of distributing a single version of data over a network. Our application need consider only a single version at a time.
Reference: [7] <author> S.P. De Jong. </author> <title> Combining of changes to a source file. </title> <journal> IBM Technical Disclosure Bulletin, </journal> <volume> 15(4):11861188, </volume> <month> September </month> <year> 1972. </year>
Reference-contexts: The first applications of delta compression found changed lines in text data for analyzing the recent modifications to files <ref> [7] </ref>. Considering data as lines of text fails to encode minimum sized delta files as it does not look at data at a minimum granularity or alignment. Granularity describes the minimum sized element or string that can be detected as a change between file versions.
Reference: [8] <author> R. M. Karp. </author> <title> Reducibility among combinatorial problems. </title> <editor> In R. E. Miller and J. W. Thatcher, editors, </editor> <booktitle> Complexity of Computer Computations, </booktitle> <pages> pages 85104. </pages> <publisher> Plenum Press, </publisher> <year> 1972. </year>
Reference-contexts: Furthermore, well known NP-complete problems can be reduced to this decision problem. For example, the following graph optimization problem is known to be NP-complete <ref> [8] </ref> and trivially reduces to the previously stated problem. Input: Digraph G = hV; Ei and positive integer k.
Reference: [9] <author> Donald E. Knuth. </author> <title> Fundamental Algorithms, </title> <booktitle> volume 1 of The Art of Computer Programming. </booktitle> <publisher> Addison Wesley Publishing Co., </publisher> <year> 1968. </year>
Reference-contexts: expressions by our bound on jEj; jV j; jAj and jCj, our algorithm runs in time O (n log n) and space O (n). 7 Topologically Sorting a Digraph with Cy cles The algorithm we use for topological sorting is a slight modification of a depth-first search on a digraph <ref> [9] </ref>. This sort orders vertices based on the edge relation The resultant topologically sorted digraph has no back edges, i.e. no node may have an edge to a node that precedes it in topological order. No valid topological ordering exists for cycles in a digraph.
Reference: [10] <author> Webb Miller and Eugene W. Myers. </author> <title> A file comparison program. </title> <journal> Software Practice and Experience, </journal> <volume> 15(11):10251040, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: Efforts to generalize delta compression to data that are not aligned and to minimize the granularity of the smallest change resulted in algorithms for compressing data at the granularity of a byte. Early algorithms were based upon either dynamic programming <ref> [10] </ref> or the greedy method [11] and performed this task using time quadratic in length of the input files. Recent advances in differencing algorithms have produced fine granularity differencing algorithms that run using time linear in the length of the input files [1, 5].
Reference: [11] <author> Christoph Reichenberger. </author> <title> Delta storage for arbitrary non-text files. </title> <booktitle> In Proceedings of the 3rd International Workshop on Software Configuration Management, </booktitle> <address> Trondheim, Norway, </address> <month> 12-14 June </month> <year> 1991, </year> <pages> pages 144152. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: Efforts to generalize delta compression to data that are not aligned and to minimize the granularity of the smallest change resulted in algorithms for compressing data at the granularity of a byte. Early algorithms were based upon either dynamic programming [10] or the greedy method <ref> [11] </ref> and performed this task using time quadratic in length of the input files. Recent advances in differencing algorithms have produced fine granularity differencing algorithms that run using time linear in the length of the input files [1, 5].
Reference: [12] <author> Marc J. Rochkind. </author> <title> The source code control system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(4):364370, </volume> <month> December </month> <year> 1975. </year>
Reference-contexts: Even though the general problem of detecting and encoding version to version modifications was well defined, delta compression applications continued to rely on the alignment of data, as in database records [13], and the grouping of data into block or line granularity, as in source code control systems <ref> [12, 15] </ref>, to limit the amount of required metadata and simplify the combinatorial task of finding the common and different strings between files.
Reference: [13] <author> Dennis G. Severance and Guy M. Lohman. </author> <title> Differential files: Their application to the maintenance of large databases. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 1(2):256267, </volume> <month> September </month> <year> 1976. </year>
Reference-contexts: Even though the general problem of detecting and encoding version to version modifications was well defined, delta compression applications continued to rely on the alignment of data, as in database records <ref> [13] </ref>, and the grouping of data into block or line granularity, as in source code control systems [12, 15], to limit the amount of required metadata and simplify the combinatorial task of finding the common and different strings between files.
Reference: [14] <author> Walter F. Tichy. </author> <title> The string-to-string correction problem with block move. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4), </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: Alignment describes the boundaries at which changes between files are found. For line-oriented text differencing, algorithms can only find changes when they are line-aligned, i.e. they occur at the start of a line. The problem of compactly representing version to version changes in data was formalized by string-to-string correction <ref> [16, 14] </ref>, which generalized this problem to detecting changed regions of a file at an arbitrarily fine granularity and alignment.
Reference: [15] <author> Walter F. Tichy. </author> <title> RCS A system for version control. </title> <journal> Software Practice and Experience, </journal> <volume> 15(7):637654, </volume> <month> July </month> <year> 1985. </year>
Reference-contexts: Even though the general problem of detecting and encoding version to version modifications was well defined, delta compression applications continued to rely on the alignment of data, as in database records [13], and the grouping of data into block or line granularity, as in source code control systems <ref> [12, 15] </ref>, to limit the amount of required metadata and simplify the combinatorial task of finding the common and different strings between files.
Reference: [16] <author> R.A. Wagner and M.J. Fischer. </author> <title> The string-to-string correction problem. </title> <journal> Journal of the ACM, </journal> <volume> 21(1):168173, </volume> <month> January </month> <year> 1973. </year>
Reference-contexts: Alignment describes the boundaries at which changes between files are found. For line-oriented text differencing, algorithms can only find changes when they are line-aligned, i.e. they occur at the start of a line. The problem of compactly representing version to version changes in data was formalized by string-to-string correction <ref> [16, 14] </ref>, which generalized this problem to detecting changed regions of a file at an arbitrarily fine granularity and alignment.
References-found: 16

