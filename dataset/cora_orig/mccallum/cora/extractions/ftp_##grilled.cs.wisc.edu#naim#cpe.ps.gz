URL: ftp://grilled.cs.wisc.edu/naim/cpe.ps.gz
Refering-URL: http://www.cs.wisc.edu/~naim/publications.html
Root-URL: 
Title: Do-Loop-Surface: An Abstract Representation of Parallel Program Performance  
Author: Oscar Nam, Tony Hey, and Ed Zaluska 
Note: PUBLISHED IN CPE, APRIL 1996. REVISED VERSION 24-2-95. 1  
Abstract: Performance is a critical issue in current massively parallel processors. However, delivery of adequate performance is not automatic and performance evaluation tools are required in order to help the programmer to understand the behaviour of a parallel program. In recent years, a wide variety of tools have been developed for this purpose including tools for monitoring and evaluating performance and visualization tools. However, these tools do not provide an abstract representation of performance. Massively parallel processors can generate a huge amount of performance data and sophisticated methods for representing and displaying this data (e.g. visual and aural) are required. Performance views are not scalable in general and do not represent an abstraction of the performance data. The Do-Loop-Surface display is proposed as an abstract representation of the performance of a particular do-loop in a program. It has been used to improve the performance of a Matrix Multiply parallel algorithm as well as to understand the behaviour of the following applications: Matrix Transposition (TRANS1) and Fast Fourier Transform (FFT1) from the Genesis Benchmarks, and the kernel of a fluid dynamics package (FIRE). These experiments were performed on a CM-5, Meiko CS-1, and a PARSYS Supernode. The examples demonstrate that the Do-Loop-Surface display is an useful way to represent performance. It is implemented using AVS (Application Visualization System), a standard data visualization package. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Parasoft Corporation, ParaSoft Express. </institution> <note> User's Guide, </note> <year> 1990. </year>
Reference-contexts: Popular views like space NAM, HEY AND ZALUSKA: DO-LOOP-SURFACE... 23 Fig. 20. TRANS1 Genesis Benchmark. DLS display of a 100x100 matrix example on a 4x4 grid of processors (PARSYS Supernode, 16 processors). time/Feynman diagram (ParaGraph [4], [5]) or event timelines (Express Etool <ref> [1] </ref>) often provide some insight about program behaviour. Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views [10].
Reference: [2] <author> Bernd Mohr, </author> <title> "SIMPLE: A Performance Evaluation Tool Environment for Parallel and Distributed Systems", </title> <booktitle> in EDMCC2, </booktitle> <address> Munich, </address> <month> April </month> <year> 1991. </year>
Reference: [3] <author> Pallas, </author> <title> GmbH, PA-Tools, Performance Analysis Tools, </title> <year> 1991. </year>
Reference: [4] <author> M. Heath and J. Etheridge, </author> <title> "Visualizing the performance of parallel programs", </title> <journal> IEEE Software, </journal> <pages> pp. 29-39, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: FFT1 and TRANS1 are two benchmarks from the Genesis Benchmark Suite [36]. FFT1 computes a Fast Fourier Transform and it is used to compare the Feynman display of the ParaGraph Visualization System <ref> [4] </ref> against a DLS display. TRANS1 is a matrix transposition algorithm and demonstrates how to improve performance by understanding the behaviour of a parallel program (with the help of a DLS). Both experiments were made on a PARSYS Supernode. <p> Popular views like space NAM, HEY AND ZALUSKA: DO-LOOP-SURFACE... 23 Fig. 20. TRANS1 Genesis Benchmark. DLS display of a 100x100 matrix example on a 4x4 grid of processors (PARSYS Supernode, 16 processors). time/Feynman diagram (ParaGraph <ref> [4] </ref>, [5]) or event timelines (Express Etool [1]) often provide some insight about program behaviour. Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views [10].
Reference: [5] <author> Michael Heath, </author> <title> "Recent Developments and Case Studies in Performance Visualization using ParaGraph", </title> <booktitle> in Workshop on Performance Measurement and Visualization of Parallel Systems, </booktitle> <address> Moravany, Czecho-Slovakia, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Popular views like space NAM, HEY AND ZALUSKA: DO-LOOP-SURFACE... 23 Fig. 20. TRANS1 Genesis Benchmark. DLS display of a 100x100 matrix example on a 4x4 grid of processors (PARSYS Supernode, 16 processors). time/Feynman diagram (ParaGraph [4], <ref> [5] </ref>) or event timelines (Express Etool [1]) often provide some insight about program behaviour. Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views [10].
Reference: [6] <author> Arndt Bode and Peter Braun, </author> <title> "Monitoring and visualisation in topsys", </title> <booktitle> in Workshop on Performance Measurement and Visualization of Parallel Systems, </booktitle> <address> Moravany, Czecho-Slovakia, </address> <month> October </month> <year> 1992. </year>
Reference: [7] <author> Daniel Reed, Ruth Aydt, Tara Madhyastha, Roger Noe, Keith Shields, and Bradley Schwartz, </author> <title> "The PABLO performance analysis environment", </title> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: Not only visual but also aural methods are being explored in order to represent parallel performance data. Visual and aural portrayals of parallel program execution are used to gain insight into how a program is working (e.g. PABLO <ref> [7] </ref>). The combination of portrayals in a coordinated performance environment provides the user with multiple perspectives and stimuli to comprehend complex, multidimensional run-time information [19]. Reasons for using "auralization" in general and in parallel computing in particular are documented by Francioni [20], [21].
Reference: [8] <author> Oscar Nam, </author> <title> "Performance Analysis of Parallel Programs", </title> <type> Internal Report. </type> <institution> Electronics and Computer Science, University of Southampton, Southampton SO17 1BJ, UK., </institution> <month> June </month> <year> 1993. </year>
Reference: [9] <author> Allen Malony and Gregory Wilson, </author> <title> "Future directions in parallel performance environments", </title> <booktitle> in Workshop on Performance Measurement and Visualization of Parallel Systems, </booktitle> <address> Moravany, Czecho-Slovakia, </address> <month> October </month> <year> 1992. </year>
Reference: [10] <author> Sekhar Sarukkai, Doug Kimelman, and Larry Rudolph, </author> <title> "A methodology for visualizing performance of loosely synchronous programs", </title> <booktitle> in Scalable High Performance Computing Conference, SHPCC-92. </booktitle> <month> April </month> <year> 1992, </year> <pages> pp. 424-432, </pages> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: In general, visual-aural representations could be used effectively on large systems. However, suitable approaches must support a hierarchical presentation and/or logical grouping of information [19]. Another interesting line of research is proposed by Sarukkai et al <ref> [10] </ref>. They suggest that a programmer should begin the investigation into the causes of performance degradation with high level and abstract views, so that global trends can be seen. <p> A set of guidelines for methodical application of views, proceeding from the highest level to the lowest level, is also presented in <ref> [10] </ref>. While a collection of views, animations, and general performance data can help uncover performance bugs, some guidelines or strategies are needed to direct the order which views are examined. <p> Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views <ref> [10] </ref>.
Reference: [11] <author> Barton Miller, </author> <title> "What to draw? when to draw? An essay on parallel program visualization", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 18, no. 2, </volume> <pages> pp. 265-269, </pages> <year> 1993. </year>
Reference-contexts: 1 Drawing useful pictures is difficult. As Miller states in <ref> [11] </ref>, visualizations should guide, not rationalize. To guide means that it leads you to discover things that you did not already know and rationalize means that it lets you illustrate things that you already know. NAM, HEY AND ZALUSKA: DO-LOOP-SURFACE... 3 it is presented via a hierarchy of views.
Reference: [12] <author> Larry Carter, </author> <title> "Private Electronic Mail Communication", </title> <month> January </month> <year> 1993. </year>
Reference: [13] <author> G. Almasi, B. Alpern, C. Berman, Larry Carter, and D. Hale, </author> <title> "A Case-study in performance programming: Seismic Migration", </title> <booktitle> in 2nd Symposium on High Performance Computing. </booktitle> <month> October </month> <year> 1991, </year> <pages> pp. 195-206, </pages> <publisher> North Holland. </publisher>
Reference: [14] <author> John Merlin, </author> <title> "HPF Visualization Tools Proposal", </title> <type> Tech. Rep., </type> <institution> University of Southampton, </institution> <month> April </month> <year> 1993. </year>
Reference: [15] <author> Brian Wylie, Michael Norman, and Lyndon Clarke, </author> <title> "High Performance Fortran: A Perspective", </title> <institution> The University of Edinburgh, EPCC-TN92-05.04, </institution> <month> May </month> <year> 1992. </year>
Reference: [16] <author> Oscar Nam and Tony Hey, "Do-Loop-Surface: </author> <title> An Abstract Performance Data Visualization", </title> <booktitle> in HPCN Europe'94, </booktitle> <address> Munich, Germany, April 1994, </address> <publisher> Springer-Verlag Publishers. </publisher>
Reference: [17] <author> Margaret Simmons and Rebecca Koskela, </author> <title> Performance Instrumentation and Visualization, </title> <publisher> ACM Press, Frontier Series, </publisher> <year> 1990. </year>
Reference: [18] <author> Diane Rover and Charles Wright, </author> <title> "Visualizing the Performance of SPMD and Data-Parallel Programs", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 18, </volume> <pages> pp. 129-146, </pages> <year> 1993. </year>
Reference-contexts: The activity of any component becomes part of an aggregate activity of the system. These levels form a useful hierarchy which must be exploited for larger systems. A key and essential area of study is analyzing the microscopic versus macroscopic performance of large systems, where there is little experience <ref> [18] </ref>. Not only visual but also aural methods are being explored in order to represent parallel performance data. Visual and aural portrayals of parallel program execution are used to gain insight into how a program is working (e.g. PABLO [7]).
Reference: [19] <author> Joan Francioni and Diane Rover, </author> <title> "Visual-Aural representations of performance for a scalable application program", </title> <booktitle> in Scalable High Performance Computing Conference, SHPCC-92. </booktitle> <month> April </month> <year> 1992, </year> <pages> pp. 433-440, </pages> <publisher> IEEE Computer Society. NAM, HEY AND ZALUSKA: </publisher> <address> DO-LOOP-SURFACE... 27 Fig. </address> <month> 24. </month> <title> FFT1 Genesis Benchmark. Comparison between a Feynman display (ParaGraph) and a DLS. 4x4 Grid. </title> <note> 28 PUBLISHED IN CPE, APRIL 1996. REVISED VERSION 24-2-95. Fig. </note> <month> 25. </month> <title> FFT1 Genesis Benchmark. Comparison between a Feynman display (ParaGraph) and a DLS. 4x4 Grid plus some additional links. NAM, HEY AND ZALUSKA: </title> <journal> DO-LOOP-SURFACE... </journal> <volume> 29 </volume>
Reference-contexts: Visual and aural portrayals of parallel program execution are used to gain insight into how a program is working (e.g. PABLO [7]). The combination of portrayals in a coordinated performance environment provides the user with multiple perspectives and stimuli to comprehend complex, multidimensional run-time information <ref> [19] </ref>. Reasons for using "auralization" in general and in parallel computing in particular are documented by Francioni [20], [21]. <p> In general, visual-aural representations could be used effectively on large systems. However, suitable approaches must support a hierarchical presentation and/or logical grouping of information <ref> [19] </ref>. Another interesting line of research is proposed by Sarukkai et al [10]. They suggest that a programmer should begin the investigation into the causes of performance degradation with high level and abstract views, so that global trends can be seen.
Reference: [20] <author> Joan Francioni, L. Albright, and J. Jackson, </author> <title> "Debugging parallel programs using sound", </title> <journal> in SIGPLAN Notices, 1991, </journal> <volume> vol. 26, </volume> <pages> pp. 68-75. </pages>
Reference-contexts: PABLO [7]). The combination of portrayals in a coordinated performance environment provides the user with multiple perspectives and stimuli to comprehend complex, multidimensional run-time information [19]. Reasons for using "auralization" in general and in parallel computing in particular are documented by Francioni <ref> [20] </ref>, [21].
Reference: [21] <author> Joan Francioni, L. Albright, and J. Jackson, </author> <title> "The sounds of parallel programs", </title> <booktitle> in Proceedings of the Sixth Distributed Memory Computing Conference. 1991, </booktitle> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: PABLO [7]). The combination of portrayals in a coordinated performance environment provides the user with multiple perspectives and stimuli to comprehend complex, multidimensional run-time information [19]. Reasons for using "auralization" in general and in parallel computing in particular are documented by Francioni [20], <ref> [21] </ref>.
Reference: [22] <author> Alva Couch, </author> <title> "Categories and Context in Scalable Execution Visualization", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 18, </volume> <pages> pp. 195-204, </pages> <year> 1993. </year>
Reference-contexts: These guidelines that they suggest are: * Dissatisfied with performance? * Compare progress with ideal behaviour. * Examine deviation in overall behaviour. * Examine individual sections/PEs. * Low Level investigation. The work done by Couch in <ref> [22] </ref> deals with the problem of scalability. In that paper, global context is described by scalable execution views that do not change in format, size, meaning, or clarity as processors are added to an execution. <p> Categorical views are particularly useful when there is an inverse mapping for an arbitrary view region to the subset of processors whose behaviour was described in the region. The execution visualization tool Seeplex implements this form of category management to provide scalable execution views <ref> [22] </ref>. Seeplex manages view relationships using a data-flow visualization environment in the spirit of scientific visualization systems such as AVS. Two important issues are also addressed by Couch. The first one is that a scalable view is less useful if one cannot trace back from view features to raw data. <p> REVISED VERSION 24-2-95. "..visualization takes much time, effort, and experience to provide results. We hope to be made obsolete by an automatic analysis method that avoids the global search problem and all its difficulties. However, we do not think this likely" <ref> [22] </ref>. Finally, another interesting research effort is the approach to parallel program analysis by LeBlanc, Mellor-Crummey and Fowler [23], which is based on a multiplicity of views of an execution. A synchronization trace, captured during execution, is used to construct a graph representation of the behaviour of the program.
Reference: [23] <author> T. LeBlanc, J. Mellor-Crummey, and R. Fowler, </author> <title> "Analyzing parallel program executions using multiple views", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 9, no. 2, </volume> <pages> pp. 203-217, </pages> <year> 1990. </year>
Reference-contexts: We hope to be made obsolete by an automatic analysis method that avoids the global search problem and all its difficulties. However, we do not think this likely" [22]. Finally, another interesting research effort is the approach to parallel program analysis by LeBlanc, Mellor-Crummey and Fowler <ref> [23] </ref>, which is based on a multiplicity of views of an execution. A synchronization trace, captured during execution, is used to construct a graph representation of the behaviour of the program. The user then manipulates this representation to create and tune visualizations using an integrated, programmable toolkit. <p> This figure shows that some nodes execute faster than others. Additionally, the figure shows 2 A visualization makes some details of the view manifest, while obscuring others. A view defines what information is presented; a visualization describes how the information is displayed <ref> [23] </ref>. NAM, HEY AND ZALUSKA: DO-LOOP-SURFACE... 5 how the user can get information from the display by "clicking" the mouse on the desired position). Fig. 1. DLS display showing differences in execution time of floating point units on a CM-5.
Reference: [24] <institution> Advanced Visual Systems Inc., </institution> <note> AVS User's Guide, Release 4, </note> <month> May </month> <year> 1992. </year>
Reference-contexts: A new and evocative vocabulary that explains the particular features of the display is also incorporated (e.g. mountains, valleys, hilly, flat). This surface is displayed using AVS <ref> [24] </ref> (Application Visualization System), which allows rotation of the figure in several ways as well as zooming when necessary and some other interesting transformations. <p> Experimentation with such tools will allow us to learn more about how to use visualization techniques to represent performance data and to design, modify, and produce new tools which incorporate this new knowledge. A. AVS: A Data Visualization Tool AVS, Application Visualization System <ref> [24] </ref>, is a data visualization environment designed to analyze scientific and engineering data in areas like fluid dynamics, computer-aided engineering, and molecular modeling. AVS users can construct their own visualization applications, by combining software components into executable flow networks. <p> These subsystems are: * The Geometric Viewer: displays 3D geometric objects. * The Image Viewer: displays 2D images. * The Graph Viewer: creates XY and contour graphs of data. For more detailed information about AVS, see <ref> [24] </ref>. V. DLS: EXPERIMENTS AND CASE STUDIES The purpose of the following experiments is to demonstrate the usefulness of a DLS in analyzing the performance of a parallel algorithm.
Reference: [25] <author> Oscar Nam and Tony Hey, </author> <title> "Visualization of cache/memory effects using Do-Loop-Surface displays", </title> <type> Technical Report HPCC94-003, </type> <institution> Dept. of Electronics and Computer Science, University of Southampton, Southampton S017 1BJ, UK, </institution> <year> 1994. </year>
Reference-contexts: By analyzing a DLS, the user can identify performance features like load balance and communication delays. Previous work has shown that DLS displays are also useful in identifying cache/memory effects <ref> [25] </ref>, comparing different communication strategies, and even identifying hardware irregularities (figure 1 illustrates a DLS display that enabled us to discover performance differences in the floating point units of the SPARC-2 chip set on a CM-5. This figure shows that some nodes execute faster than others.
Reference: [26] <author> R. Hempel, </author> <title> The ANL/GMD Macros (PARMACS) in FORTRAN for Portable Parallel Programming using the Message Passing Programming Model. User's Guide and Reference Manual, </title> <publisher> Pallas GmbH, </publisher> <year> 1991. </year>
Reference-contexts: A. Trace Generation The procedure to be followed in order to generate the necessary trace data for a DLS is straightforward. A small trace library has been implemented on top of PARMACS <ref> [26] </ref>, PVM [27], and MPI [28], (both for FORTRAN and C) on the following architectures: * PARMACS version: CM-5, PARSYS Supernode, and MEIKO CS-1. * PVM version: CM-5, Meiko CS-2, and Sun workstation network. * MPI version: Sun workstation network. 6 PUBLISHED IN CPE, APRIL 1996. REVISED VERSION 24-2-95. <p> The hardware platform consisted of a 64 T800 Parsys Supernode and the program was written in C using PARMACS <ref> [26] </ref>. A.1 Results for a 100x100 matrix on 32 processors The DLS of figure 9 represents the matrix multiplication algorithm for two matrices of dimension 100x100, running on 32 processors (initialization and terminating values have been removed in order to get a clearer picture).
Reference: [27] <author> Al Geist, Adam Beguelin, Jack Dongarra, Kiang Weicheng, Robert Manchek, and Vaidy Sunderam, </author> <title> "PVM 3 Users's Guide and Reference Manual", </title> <type> Tech. Rep. </type> <institution> ORNL/TM-12187, Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year> <title> [28] "Message Passing INterface Forum", </title> <type> Tech. Rep. Technical Report CS-93-214, </type> <month> November </month> <year> 1993, </year> <title> Document for a standard message-passing interface. </title>
Reference-contexts: A. Trace Generation The procedure to be followed in order to generate the necessary trace data for a DLS is straightforward. A small trace library has been implemented on top of PARMACS [26], PVM <ref> [27] </ref>, and MPI [28], (both for FORTRAN and C) on the following architectures: * PARMACS version: CM-5, PARSYS Supernode, and MEIKO CS-1. * PVM version: CM-5, Meiko CS-2, and Sun workstation network. * MPI version: Sun workstation network. 6 PUBLISHED IN CPE, APRIL 1996. REVISED VERSION 24-2-95.
Reference: [29] <author> Oscar Nam and Tony Hey, </author> <title> "Invasiveness of Performance Instrumentation Measurements on Multiprocessors", </title> <journal> IFIP TRANSACTIONS A-COMPUTER SCIENCE AND TECHNOLOGY, </journal> <volume> vol. 44, </volume> <pages> pp. 319-328, </pages> <year> 1994. </year>
Reference-contexts: Output from the dls filter program. In this example, the number of iterations has been reduced from 678 to 52 or 92%. The overhead, or invasiveness <ref> [29] </ref>, generated by this tracing mechanism, is low and the difference in time between running a program with and without such tracing is often negligible.
Reference: [30] <author> Oscar Nam and Alejandro Teruel, </author> <title> "ANDES: A Performance Analyzer for Parallel Programs", in Transputer and Occam Research: New Directions, </title> <booktitle> 16th Technical Meeting of the World Occam and Transputer User Group (WoTUG-16). </booktitle> <address> Sheffield, UK, </address> <month> March </month> <year> 1993, </year> <journal> vol. </journal> <volume> 33, </volume> <pages> pp. 91-99, </pages> <publisher> IOS Press, Amsterdam. </publisher>
Reference-contexts: In fact, the overhead generated by DLSGETT (which is the only routine that really makes an effect in the execution time of the do-loop being measured) is, approximately, 0.0001 seconds (for each do-loop iteration). However, there are other systems (e.g. ANDES <ref> [30] </ref>), where the invasiveness may become a significant factor. Some undesirable effects due to invasiveness are: execution slowdown, changes in memory reference patterns, and event reordering. For this reason, it is important to keep the invasiveness low.
Reference: [31] <author> B. Miller, M. Clark, J. Hollingsworth, S. Kierstead, S. Lim, and T. Torzewski, "IPS-2: </author> <title> The second generation of a parallel program measurement system", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pp. 206-217, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: ANDES [30]), where the invasiveness may become a significant factor. Some undesirable effects due to invasiveness are: execution slowdown, changes in memory reference patterns, and event reordering. For this reason, it is important to keep the invasiveness low. In general, based on empirical results (Miller <ref> [31] </ref>, Goldberg [32], Mohr [33]), values below 10% are considered acceptable if any reordering of events does not affect the final result of the application. Greater invasiveness implies longer execution times and less accuracy.
Reference: [32] <author> A. Goldberg and J. Hennessy, </author> <title> "Mtool an integrated system for performance debugging shared memory multiprocessor applications", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 28-40, </pages> <year> 1993. </year>
Reference-contexts: ANDES [30]), where the invasiveness may become a significant factor. Some undesirable effects due to invasiveness are: execution slowdown, changes in memory reference patterns, and event reordering. For this reason, it is important to keep the invasiveness low. In general, based on empirical results (Miller [31], Goldberg <ref> [32] </ref>, Mohr [33]), values below 10% are considered acceptable if any reordering of events does not affect the final result of the application. Greater invasiveness implies longer execution times and less accuracy. From a performance evaluation perspective, instrumentation perturbations must be balanced against the need for detailed performance data [34].
Reference: [33] <author> Bernd Mohr, </author> <title> "Private electronic mail communication", Subject: Measuring Invasiveness, </title> <month> March </month> <year> 1993. </year>
Reference-contexts: ANDES [30]), where the invasiveness may become a significant factor. Some undesirable effects due to invasiveness are: execution slowdown, changes in memory reference patterns, and event reordering. For this reason, it is important to keep the invasiveness low. In general, based on empirical results (Miller [31], Goldberg [32], Mohr <ref> [33] </ref>), values below 10% are considered acceptable if any reordering of events does not affect the final result of the application. Greater invasiveness implies longer execution times and less accuracy. From a performance evaluation perspective, instrumentation perturbations must be balanced against the need for detailed performance data [34]. IV.
Reference: [34] <author> Allen Malony, Daniel Reed, and H. Wijshoff, </author> <title> "Performance-measurement intrusion and perturbation analysis", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 433-450, </pages> <year> 1992. </year>
Reference-contexts: Greater invasiveness implies longer execution times and less accuracy. From a performance evaluation perspective, instrumentation perturbations must be balanced against the need for detailed performance data <ref> [34] </ref>. IV. PERFORMANCE ANALYSIS USING A DATA VISUALIZATION TOOL The amount of trace information generated by a program running on a large number of processors can be enormous. However, data visualization environments can represent this NAM, HEY AND ZALUSKA: DO-LOOP-SURFACE... 9 Fig. 5.
Reference: [35] <author> Brandes, Thomas, </author> <title> "Automatic Parallelization/Partitioning: A Case Study on the AVL Fire Benchmark Codes", </title> <booktitle> in RAPS: Open Workshop on Parallel Benchmarks and Programming Models. Childworth Manor Conference Centre, </booktitle> <address> Southampton., </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Matrix Multiply), but others are parts of real applications (e.g. FIRE). The parallel Matrix Multiply algorithm was selected as a case study for its simplicity and also because it illustrates how performance can be improved by using the DLS displays. The FIRE Benchmark, a fluid dynamics code <ref> [35] </ref>, is a set of kernels from a real application. It is used to make comparisons between different equation NAM, HEY AND ZALUSKA: DO-LOOP-SURFACE... 11 Fig. 7. The dls transform module controls the current iterations and processors being displayed.
Reference: [36] <author> V. Getov, Tony. Hey, R. Hockney, and I. Wolton, </author> <title> "The GENESIS Distributed-Memory Benchmark Suite - Release 2.1", </title> <type> Technical report, </type> <institution> Southampton Novel Architecture Research Centre, University of Southampton, </institution> <year> 1993. </year>
Reference-contexts: In this figure the first 10 iterations has been deleted. solvers on different architectures (CM-5 and Meiko CS-1) and to show the importance of the filtering technique on large trace files. FFT1 and TRANS1 are two benchmarks from the Genesis Benchmark Suite <ref> [36] </ref>. FFT1 computes a Fast Fourier Transform and it is used to compare the Feynman display of the ParaGraph Visualization System [4] against a DLS display. <p> NAM, HEY AND ZALUSKA: DO-LOOP-SURFACE... 21 Fig. 17. FIRE Benchmark. Filtered DLS using the gaus2 solver (CM-5, 31 processors). Only "important" changes in the picture are displayed. C. Case Study III: Genesis Benchmarks FFT1 (Fast Fourier Transform) and TRANS1 (Matrix Transpose) are two programs from the Genesis Benchmarks Suite <ref> [36] </ref>. TRANS1 transposes a square matrix by dividing the original matrix in sub-matrices and exchanging them between opposite processors (see figure 18).
Reference: [37] <author> C.A.R. Hoare, </author> <title> Communicating Sequential Processes, </title> <publisher> Prentice Hall International, </publisher> <year> 1985. </year>
Reference-contexts: For these machines, low level communications are based on CSP <ref> [37] </ref> (i.e. communications are synchronous). However, software libraries such as PARMACS can implement asynchronous communications by using buffering techniques. It is also important to notice that the distance between two points increases the latency of a message, since this message has to travel through every intermediate node.
Reference: [38] <author> Cliff Addison, James Allwright, Norman Binsted, Nigel Bishop, Bryan Carpenter, Peter Dalloz, David Gee, Vladimir Getov, Tony Hey, Roger Hockney, Max Lemke, John Merlin, Mark Pinches, Chris Scott, and Ivan Wolton, </author> <title> "The Genesis Distributed Memory Benchmarks-I. Methodology and General Relativity benchmark with results for the SUPRENUM Computer", </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 1-22, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The benchmark performance is defined by the following equation <ref> [38] </ref>: R B (N ; p) = T (N ; p) where: * p: is the number of processors. * N : stands for a set of parameters characterizing the size of the problem. * F B (N ): is the quantity of arithmetic performed 3 . 3 For simple problems <p> These solvers constitute the kernel of the benchmark. performance has the advantage that it is possible to conclude that, for a given benchmark, the implementation that has the highest benchmark performance is the best because it executes in the least time. For more details, see <ref> [38] </ref>. NAM, HEY AND ZALUSKA: DO-LOOP-SURFACE... 17 Fig. 12. Matrix Multiply: Benchmark Performance (1, 4, 8, 16, 32 and 48 processors).
Reference: [39] <author> Alistair Dunlop, Emilio Hernandez, Oscar Nam, Tony Hey, and Denis Nicole, </author> <title> "Collaborative Tools for Parallel Performance Optimization", </title> <type> Technical Report HPCC94-011, </type> <institution> Dept. of Electronics and Computer Science, University of Southampton, Southampton S017 1BJ, UK, </institution> <month> November </month> <year> 1994, </year> <note> Submitted to Scientific Programming. </note>
Reference-contexts: being measured during the experiment. * Identifying different sort of system irregularities such as unexpected operating system overhead and memory behaviour. * Integrating parallel performance visualization and scientific data visualization. * Validating results of other performance tools (collaborative work using Lebep, a Parallel Benchmark Language Especification, and a Performance Estimator <ref> [39] </ref>). * Identifying load balancing problems. Another interesting issue is the representation of the Benchmark Performance metric by using a 3D display. This representation has shown to be very helpful, allowing us to learn more about a picture and reducing the possibility of missing information.
References-found: 38

