URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-394.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: bobick@media.mit.edu  
Title: Seeing Action  
Author: Aaron F. Bobick 
Web: http://www.media.mit.edu/~bobick  
Address: 20 Ames Street Cambridge, MA 02139 USA  
Affiliation: MIT Media Laboratory  
Note: Computers  American football) are briefly mentioned.  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 394 Appears in the British Machine Vision Conference, Edinburgh, Scotland | September 1996 Abstract As research in computer vision has shifted from only processing single, static images to the manipulation of video sequences, the concept of action recognition has become important. Fundamental to understanding action is reasoning about time, in either an implicit or explicit framework. In this paper I describe several specific examples of incorporating time into representations of action and how those representations are used to recognize actions. The approaches differ on whether variation over time is considered a continuous mapping, a state-based trajectory, or a qualitative, semantically labeled sequence. For two of the domains | whole body actions and hand gestures | I describe the approaches in detail while two others | constrained semantic domains (e.g. watching someone cooking) and labeling dynamic events (e.g. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Black, M. and Y. Yacoob. </author> <title> Tracking and recognizing rigid and non-rigid facial motion using local parametric models of image motion. </title> <booktitle> In ICCV, </booktitle> <year> 1995. </year>
Reference-contexts: The prior work in this area has addressed either periodic or gross motion detection and recognition [17, 21, 24] or the understanding of facial expressions <ref> [23, 1, 10] </ref>. In [2, 5] we propose a representation and recognition theory that decomposes motion-based recognition into first describing where there is motion (the spatial pattern) and then describing how the motion is moving. <p> The basic idea is that we project the temporal pattern of motion into a single, image-based representation | a temporal template. This approach is a natural extension of Black and Yacoob's work on facial expression recognition <ref> [1] </ref>. 1 That is, the temporal segmentation and recognition tasks are performed simultaneously. 1 Frame 5 15 25 35 frame people can trivially recognize the action as someone sitting. 2.1 Motion images Consider the example of someone sitting, as shown in Figure 2a.
Reference: [2] <author> A. Bobick and J. Davis. </author> <title> Real-time recognition of activity using temporal templates. </title> <booktitle> In Proceedings of WACV 96, </booktitle> <year> 1996. </year>
Reference-contexts: The prior work in this area has addressed either periodic or gross motion detection and recognition [17, 21, 24] or the understanding of facial expressions [23, 1, 10]. In <ref> [2, 5] </ref> we propose a representation and recognition theory that decomposes motion-based recognition into first describing where there is motion (the spatial pattern) and then describing how the motion is moving. <p> Fortunately, in the recognition section we derive a backward-looking (in time) algorithm which can dynamically search over a range of t . In Figure 2b we display the MEIs of viewing a sitting action across 90 ffi . In <ref> [2] </ref> we exploited the smooth variation of motion over angle to compress the entire view circle into a low order representation. Here we simply note that because of the slow variation across angle, we only need to sample the view sphere coarsely to recognize all directions.
Reference: [3] <author> A. F. Bobick and A. D. Wilson. </author> <title> A state-based technique for the summarization and recognition of gesture. </title> <booktitle> Proc. Int. Conf. Comp. Vis., </booktitle> <year> 1995. </year>
Reference-contexts: The program cannot see him during this time, but it knows he's there. Understanding time can be either explicit, as in the above example, or implicit, captured in the representation of action. One example that we will expand upon later is our work in gesture recognition <ref> [3, 22] </ref>. In this work gesture is represented either deterministically by an explicit sequence of states through which the hand must move, or probabilistically by a hidden Markov model. <p> Quek [18] has observed that it is rare for both the pose and the position of the hand to simultaneously change in a meaningful way during a gesture. Recently we have presented an approach that represents gesture as a sequence of states in a particular observation space <ref> [3] </ref>. <p> The parsing of the entire gesture is accomplished by finding a likely sequence of states given the memberships and the learned transition probabilities between the states. The details of the techniques are presented in <ref> [3, 22] </ref>. The approach is based upon state models that define a residual | how well a given model can represent the current sensor input.
Reference: [4] <author> Aaron Bobick and Claudio Pinhanez. </author> <title> Using approximate models as source of contextual information for vision processing. </title> <booktitle> In Proc. of the ICCV'95 Workshop on Context-Based Vision, </booktitle> <pages> pages 13-21, </pages> <address> Cambridge, Massachusetts, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Borrowing from the object recognition literature, the iterative approach is to use some visual features to reduce the space of possible plays, which in turn constrain the events that need be detected, which further constrain the solution. For more details see: http://www-white.media.mit.edu/vismod/demos/ football/football.html A different focus is taken in <ref> [16, 4] </ref> where we introduce SmartCams | cameraman-less cameras | that respond to 4 gesture. The mean image for each state is shown in the middle. On the right is a plot of membership (solid line) and residual (dotted line) for each state for one training sequence.
Reference: [5] <author> Bobick, A. and J. Davis. </author> <title> Real-time recognition of activity using temporal templates. </title> <note> In Submitted to WACV, </note> <month> December </month> <year> 1996. </year>
Reference-contexts: The prior work in this area has addressed either periodic or gross motion detection and recognition [17, 21, 24] or the understanding of facial expressions [23, 1, 10]. In <ref> [2, 5] </ref> we propose a representation and recognition theory that decomposes motion-based recognition into first describing where there is motion (the spatial pattern) and then describing how the motion is moving. <p> Our current system uses a backward looking variable time window. Because of the simple nature of the replacement operator we can construct a highly efficient algorithm for approximating a search over a wide range of t <ref> [5] </ref>. After computing the various MEIs and MHIs, we compute the Hu moments for each image. We then check the Mahalanobis distance of the MEI parameters against the known view/action pairs. Any action found to be within a threshold distance of the input is tested for agreement of the MHI.
Reference: [6] <author> Campbell, L. and A. Bobick. </author> <title> Recognition of human body motion using phase space constraints. </title> <booktitle> In ICCV, </booktitle> <year> 1995. </year>
Reference-contexts: Recently a number of approaches have appeared attempting the full three-dimensional reconstruction of the human form from image sequences, with the presumption that such information would be useful and perhaps even necessary to understand the action taking place (e.g. <ref> [6, 12, 19, 20] </ref>). Consider, however, an extremely blurred sequence of action; a few frames on one such example is shown in Figure 1. Even with almost no structure present in each frame people can trivially recognize the action as someone sitting.
Reference: [7] <author> Y. Cui and J. Weng. </author> <title> Learning-based hand sign recognition. </title> <booktitle> In Proc. of the Intl. Workshop on Automatic Face- and Gesture-Recognition, </booktitle> <address> Zurich, </address> <year> 1995. </year>
Reference-contexts: First, we note that gestures are embedded within communication. As such, the gesturer typically orients the movements towards the recipient of the gesture. Visual gestures are therefore viewpoint-dependent <ref> [8, 7] </ref>. Second, in the space of motions allowed by the body's degrees of freedom, there is a small subspace that we use in the making of a gesture. Taken together, these observations argue for a view-based approach in which only a small subspace of human motions is represented.
Reference: [8] <author> T.J. Darrell and A.P. Pentland. </author> <title> Space-time gestures. </title> <booktitle> Proc. Comp. Vis. and Pattern Rec., </booktitle> <pages> pages 335-340, </pages> <year> 1993. </year>
Reference-contexts: First, we note that gestures are embedded within communication. As such, the gesturer typically orients the movements towards the recipient of the gesture. Visual gestures are therefore viewpoint-dependent <ref> [8, 7] </ref>. Second, in the space of motions allowed by the body's degrees of freedom, there is a small subspace that we use in the making of a gesture. Taken together, these observations argue for a view-based approach in which only a small subspace of human motions is represented.
Reference: [9] <author> Darrell, T., P. Maes, B. Blumberg, and A. Pentland. </author> <title> A novel environment for situated vision and behavior. In IEEE Wkshp. for Visual Behaviors (CVPR-94), </title> <year> 1994. </year>
Reference-contexts: Finally, in the conclusion I will refer to additional work where knowledge about time and actions is explicitly expressed in rules that are used by the system to interpret the imagery. 2 Recognizing motion: temporal templates The lure of wireless interfaces (e.g. [11]) and interactive environments <ref> [9] </ref> has heightened interest in understanding human actions.
Reference: [10] <author> Essa, I. and S. Pentland. </author> <title> Facial expression recognition using a dynamic model and motion energy. </title> <booktitle> In ICCV, </booktitle> <year> 1995. </year>
Reference-contexts: The prior work in this area has addressed either periodic or gross motion detection and recognition [17, 21, 24] or the understanding of facial expressions <ref> [23, 1, 10] </ref>. In [2, 5] we propose a representation and recognition theory that decomposes motion-based recognition into first describing where there is motion (the spatial pattern) and then describing how the motion is moving.
Reference: [11] <author> Freeman, W. </author> <title> Orientation histogram for hand gesture recognition. </title> <booktitle> In Int'l Workshop on Automatic Face-and Gesture-Recognition, </booktitle> <year> 1995. </year>
Reference-contexts: Finally, in the conclusion I will refer to additional work where knowledge about time and actions is explicitly expressed in rules that are used by the system to interpret the imagery. 2 Recognizing motion: temporal templates The lure of wireless interfaces (e.g. <ref> [11] </ref>) and interactive environments [9] has heightened interest in understanding human actions.
Reference: [12] <author> Hogg, D. </author> <title> Model-based vision: a paradigm to see a walking person. </title> <journal> Image and Vision Computing, </journal> <volume> 1(1), </volume> <year> 1983. </year>
Reference-contexts: Recently a number of approaches have appeared attempting the full three-dimensional reconstruction of the human form from image sequences, with the presumption that such information would be useful and perhaps even necessary to understand the action taking place (e.g. <ref> [6, 12, 19, 20] </ref>). Consider, however, an extremely blurred sequence of action; a few frames on one such example is shown in Figure 1. Even with almost no structure present in each frame people can trivially recognize the action as someone sitting.
Reference: [13] <author> Hu, M. </author> <title> Visual pattern recognition by moment invariants. </title> <journal> IRE Trans. Information Theory, </journal> <volume> IT-8(2), </volume> <year> 1962. </year>
Reference-contexts: We first collect training examples of each action from a variety of viewing angles. Given a set of MEIs and MHIs for each view/action combination, we compute statistical descriptions of the these images using moment-based features. Our current choice are 7 Hu moments <ref> [13] </ref> which are known to yield reasonable shape discrimination in a translation- and scale-invariant manner. For each view of each action a statistical model (mean and covariance matrix) is generated for both the MEI and MHI.
Reference: [14] <author> S.S. Intille and A.F. Bobick. </author> <title> Closed-world tracking. </title> <booktitle> In Proc. Int. Conf. Comp. Vis., </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: One research effort in our lab is video annotation, in particular labeling American football plays. In <ref> [14] </ref> we developed closed-world tracking, a technique that reasons about local contexts at a semantic level (e.g. "grass", "players", "field lines") to build robust templates to track players . We are currently developing context sensitive methods for recognizing the plays themselves.
Reference: [15] <author> R. W. Picard and T. P. Minka. </author> <title> Vision texture for annotation. </title> <journal> Journal of Multimedia Systems, </journal> <volume> 3 </volume> <pages> 3-14, </pages> <year> 1995. </year>
Reference-contexts: We then extended that work and developed a technique for learning visual behaviors that 1) incorporates the notion of multiple models | multiple ways of describing a set of sensor data <ref> [15] </ref>; 2) makes explicit the idea that a given phase of a gesture is constrained to be within some small subspace of possible human motions; and 3) represents time as a probabilistic trajectory through states [22].
Reference: [16] <author> Claudio S. Pinhanez and Aaron F. Bobick. </author> <title> Approximate world models: Incorporating qualitative and linguistic information into vision systems. </title> <note> To appear in AAAI'96, </note> <year> 1996. </year>
Reference-contexts: Borrowing from the object recognition literature, the iterative approach is to use some visual features to reduce the space of possible plays, which in turn constrain the events that need be detected, which further constrain the solution. For more details see: http://www-white.media.mit.edu/vismod/demos/ football/football.html A different focus is taken in <ref> [16, 4] </ref> where we introduce SmartCams | cameraman-less cameras | that respond to 4 gesture. The mean image for each state is shown in the middle. On the right is a plot of membership (solid line) and residual (dotted line) for each state for one training sequence.
Reference: [17] <author> Polana, R. and R. Nelson. </author> <title> Low level recognition of human motion. </title> <booktitle> In IEEE Workshop on Non-rigid and Articulated Motion, </booktitle> <year> 1994. </year>
Reference-contexts: Such capabilities argue for recognizing action from the motion itself, as opposed to first reconstructing a 3-dimensional model of a person, and then recognizing the action of the model. The prior work in this area has addressed either periodic or gross motion detection and recognition <ref> [17, 21, 24] </ref> or the understanding of facial expressions [23, 1, 10]. In [2, 5] we propose a representation and recognition theory that decomposes motion-based recognition into first describing where there is motion (the spatial pattern) and then describing how the motion is moving.
Reference: [18] <author> F. Quek. </author> <title> Hand gesture interface for human-machine interaction. </title> <booktitle> In Proc. of Virtual Reality Systems, volume Fall, </booktitle> <year> 1993. </year>
Reference-contexts: Quek <ref> [18] </ref> has observed that it is rare for both the pose and the position of the hand to simultaneously change in a meaningful way during a gesture. Recently we have presented an approach that represents gesture as a sequence of states in a particular observation space [3].
Reference: [19] <author> Rehg, J. and T. Kanade. </author> <title> Model-based tracking of self-occluding articulated objects. </title> <booktitle> In ICCV, </booktitle> <year> 1995. </year>
Reference-contexts: Recently a number of approaches have appeared attempting the full three-dimensional reconstruction of the human form from image sequences, with the presumption that such information would be useful and perhaps even necessary to understand the action taking place (e.g. <ref> [6, 12, 19, 20] </ref>). Consider, however, an extremely blurred sequence of action; a few frames on one such example is shown in Figure 1. Even with almost no structure present in each frame people can trivially recognize the action as someone sitting.
Reference: [20] <author> Rohr, K. </author> <title> Towards model-based recognition of human movements in image sequences. CVGIP, Image Understanding, </title> <type> 59(1), </type> <year> 1994. </year>
Reference-contexts: Recently a number of approaches have appeared attempting the full three-dimensional reconstruction of the human form from image sequences, with the presumption that such information would be useful and perhaps even necessary to understand the action taking place (e.g. <ref> [6, 12, 19, 20] </ref>). Consider, however, an extremely blurred sequence of action; a few frames on one such example is shown in Figure 1. Even with almost no structure present in each frame people can trivially recognize the action as someone sitting.
Reference: [21] <author> Shavit, E. and A. Jepson. </author> <title> Motion understanding using phase portraits. </title> <booktitle> In IJCAI Workshop: Looking at People, </booktitle> <year> 1995. </year>
Reference-contexts: Such capabilities argue for recognizing action from the motion itself, as opposed to first reconstructing a 3-dimensional model of a person, and then recognizing the action of the model. The prior work in this area has addressed either periodic or gross motion detection and recognition <ref> [17, 21, 24] </ref> or the understanding of facial expressions [23, 1, 10]. In [2, 5] we propose a representation and recognition theory that decomposes motion-based recognition into first describing where there is motion (the spatial pattern) and then describing how the motion is moving.
Reference: [22] <author> A. D. Wilson and A. F. Bobick. </author> <title> Learning visual behavior for gesture analysis. </title> <booktitle> In Proc. IEEE Int'l. Symp. on Comp. Vis., Coral Gables, </booktitle> <address> Florida, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: The program cannot see him during this time, but it knows he's there. Understanding time can be either explicit, as in the above example, or implicit, captured in the representation of action. One example that we will expand upon later is our work in gesture recognition <ref> [3, 22] </ref>. In this work gesture is represented either deterministically by an explicit sequence of states through which the hand must move, or probabilistically by a hidden Markov model. <p> the notion of multiple models | multiple ways of describing a set of sensor data [15]; 2) makes explicit the idea that a given phase of a gesture is constrained to be within some small subspace of possible human motions; and 3) represents time as a probabilistic trajectory through states <ref> [22] </ref>. The basic idea is that the different models need to approximate the (small) subspace associated with a particular state and membership in a state is determined by how well the state models can represent the current observation. <p> The parsing of the entire gesture is accomplished by finding a likely sequence of states given the memberships and the learned transition probabilities between the states. The details of the techniques are presented in <ref> [3, 22] </ref>. The approach is based upon state models that define a residual | how well a given model can represent the current sensor input.
Reference: [23] <author> Yacoob, Y. and L. Davis. </author> <title> Computing spatio-temporal representations of human faces. </title> <booktitle> In CVPR, </booktitle> <year> 1994. </year>
Reference-contexts: The prior work in this area has addressed either periodic or gross motion detection and recognition [17, 21, 24] or the understanding of facial expressions <ref> [23, 1, 10] </ref>. In [2, 5] we propose a representation and recognition theory that decomposes motion-based recognition into first describing where there is motion (the spatial pattern) and then describing how the motion is moving.
Reference: [24] <author> Yamato, J., J. Ohya, and K. Ishii. </author> <title> Recognizing human action in time sequential images using hidden markov models. </title> <booktitle> In CVPR, </booktitle> <year> 1992. </year> <month> 6 </month>
Reference-contexts: Such capabilities argue for recognizing action from the motion itself, as opposed to first reconstructing a 3-dimensional model of a person, and then recognizing the action of the model. The prior work in this area has addressed either periodic or gross motion detection and recognition <ref> [17, 21, 24] </ref> or the understanding of facial expressions [23, 1, 10]. In [2, 5] we propose a representation and recognition theory that decomposes motion-based recognition into first describing where there is motion (the spatial pattern) and then describing how the motion is moving.
References-found: 24

