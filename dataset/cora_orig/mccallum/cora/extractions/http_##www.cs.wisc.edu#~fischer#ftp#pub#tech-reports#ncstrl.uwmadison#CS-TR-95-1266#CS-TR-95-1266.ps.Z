URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1266/CS-TR-95-1266.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1266/
Root-URL: http://www.cs.wisc.edu
Email: -alain, dburger, goodman-@cs.wisc.edu, nagi@cs.uno.edu  
Title: An Analysis of the Interactions of Overhead-Reducing Techniques for Shared-Memory Multiprocessors The fine-grain nature of
Author: Alain Kgi, Nagi Aboulenein, Douglas C. Burger, James R. Goodman 
Keyword: simulations) shows a large and consistent improvement, much  
Note: Abstract  tion  larger than that predicted by Mellor-Crummey and Scott [19]. The  substantial  
Address: 1210 West Dayton Street Madison, Wisconsin 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: This work is supported in part by NSF Grant CCR-9207971, an unrestricted grant from the Apple Computer Advanced Technology Group, and donations from Thinking Machines Corporation. Our Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618, with matching funding from the University of Wisconsin Graduate School. 1995 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works, requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept, ACM Inc., 1515 Broadway, New York, NY 10036 USA, fax +1 (212) 869-0481, or &lt;permissions@acm.org&gt;. A version of this paper appears in: International Conference on Supercomputing, July 1995. Reprinted by permission of ACM. and the SPLASH benchmark suite for applications, we analyze data. We perform simulations both for current technology and technology that we anticipate will be available five years hence. We find that QOLB (of which this study performs the first detailed tent performance improvement. In accordance with prior results, we show that a more aggressive memory model produces more node sharing shows mixed results, correlating unsurprisingly with the presence of that sharing pattern in an application. Our most important results are (i) that the overheads eliminated with these three techniques to reduce this overhead: (i) efficient synchroniza
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Scalable Coherent Interface (SCI). ANSI/IEEE Standard 1596-1992, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: In order to establish a complete and detailed environment, we simulate a target system of 32 nodes providing hardware-guaranteed cache coherence by means of the ANSI/IEEE standard 1596 Scalable Coherent Interface (SCI) <ref> [1] </ref>. The target system consists of workstation-like nodes possessing a processor, cache memory, transaction queue (similar to a functionally-extended write buffer), network interface, and some fraction of the distributed, globally-shared memory with the associated directory entries (see Figure 1). <p> The internal details of the simulated network correspond closely to those of the SCI transport layer standard. A messages delay through the network includes staging time at the source and target nodes, parsing and wire delay through each intermediate node, and possibly a delay through an agent queue <ref> [1] </ref>, if the message switches dimensions. Table 2 lists the specific times for these delays, for both current and future networks. Table 3 shows the errors (in terms of target execution time) that the constant latency network model suffers when compared against the detailed network simulation.
Reference: [2] <author> Nagi M. Aboulenein, James R. Goodman, Stein Gjessing, and Philip J. Woest. </author> <title> Hardware Support for Synchronization in the Scalable Coherent Interface (SCI). </title> <booktitle> In Proc. of the 8th International Parallel Processing Symposium, </booktitle> <pages> pp. 141150, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These algorithms are also unable to prefetch data without extending them and adding significantly to their complexity. Aboulenein et al. <ref> [2] </ref> showed that Andersons solution performs no better than the MCS solution; therefore in this study we restrict ourselves to comparing MCS with QOLB. <p> If we combine the speedups resulting from release consistency with those resulting from sim ple sequentially consistent transaction queues (we support asynchronous ushes in the base case), we obtain results comparable to those reported by Gupta et al. Aboulenein et al. <ref> [2] </ref> present a detailed analysis study of the QOLB synchronization primitive. A QOLB implementation in the framework of the Scalable Coherent Interface (SCI) is presented.
Reference: [3] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering A New Definition. </title> <booktitle> In Proc. of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 214, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Most memory operations could in fact be completed out of order without affecting the program result. The identification of those which could affect the result is difficult, however, as synchronization through shared variables can be extremely subtle. Memory modelssuch as data-race-free-0 <ref> [3] </ref> and release consistency [13]allow the system to relax the constraints of sequential consistency by creating a contract between the software and the hardware, defining what memory orderings are legal. Implementations may then be conservative or aggressive in supporting the memory model.
Reference: [4] <author> Thomas E. Anderson. </author> <title> The Performance Implications of Spin-Waiting Alternatives for Shared-Memory Multiprocessors. </title> <booktitle> In Proc. of the 1989 International Conference on Parallel Processing, vol. II Software, </booktitle> <pages> pp. 170174, </pages> <month> Aug </month> <year> 1989. </year>
Reference-contexts: Mellor-Crummey and Scott (MCS) implement a queue as a linked list, and use atomic operations such as swap or compare-and-swap to update the list correctly [18]. Anderson presented a scheme that implements a queue as a circular array <ref> [4] </ref>. Inspired by QOLB, these algorithms also reduce the network traffic to a constant number of traversals per synchronization access, allowing processors to spin locally while waiting for the release of the lock.
Reference: [5] <author> Douglas C. Burger and James R. Goodman. </author> <title> Simulation of the SCI Transport Layer on the Wisconsin Wind Tunnel. </title> <type> Technical Report 1265, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: To validate this process, we used a detailed, event-driven SCI network simulator (based on the original WWT network simulator [6]) that accurately simulates message buffering, message retransmission, and ow control <ref> [5] </ref>. The implementation serializes the network simulation at a central node, making simulation performance suffer by roughly a factor of 15. The target network that we used to derive the validation was an mesh of rings.
Reference: [6] <author> Douglas C. Burger and David A. Wood. </author> <title> Accuracy vs. Performance in Parallel Simulation of Interconnection Networks. </title> <booktitle> In Proc. of the 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: This assumption of constant latency provides sufficient lookahead at each node to allow efficient parallel simulation. Reducing the minimum end-to-end network latency reduces the node lookahead, which causes severe increases in simulation time <ref> [6] </ref>. The constant latency assumption ignores network contention, which can play a pivotal role in evaluating various optimizations. Optimizations that reduce target execution time without a corresponding reduction in communication raise the effective load on the network. Other optimizations that reduce the number of messages lower the offered load. <p> We iterated this process until the difference between the network latency constant and the value produced by the model for that run converged to within one cycle per message. To validate this process, we used a detailed, event-driven SCI network simulator (based on the original WWT network simulator <ref> [6] </ref>) that accurately simulates message buffering, message retransmission, and ow control [5]. The implementation serializes the network simulation at a central node, making simulation performance suffer by roughly a factor of 15. The target network that we used to derive the validation was an mesh of rings.
Reference: [7] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proc. of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 152164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Implementations may then be conservative or aggressive in supporting the memory model. The final optimization we study reduces overhead for one particular pattern of data sharing. General cache-coherence protocols may not optimally handle the majority of common data sharing patterns. Consequently, researchers have investigated many protocol extensions <ref> [7, 9, 11, 24] </ref> that allow existing protocols to perform better under specific classes of data sharing patterns. These extensions attempt to reduce network traversals and/or memory accesses. Two common classes of sharing patterns are migratory data [16] and producer-consumer data sharing.
Reference: [8] <institution> Convex Computer Corporation, Richardson, Texas. SPP1000 Systems Overview, </institution> <year> 1994. </year>
Reference-contexts: The establishment of standardssuch as the IEEE Scalable Coherent Interface [1]has resulted from this growing industrial inertia. Consequently, parts are becoming available that integrate entire aspects of these standards, reducing system design complexity, time-to-market, and total system cost. Convex based their Exemplar system <ref> [8] </ref>, for instance, largely on third-party components. The growing number of bus-based shared-memory systems will further strengthen the success of the shared-memory processing paradigm. The increasing prevalence of these systems will create a large base of parallel applications, which should ease the acceptance of larger-scale shared-memory systems.
Reference: [9] <author> Alan L. Cox and Robert J. Fowler. </author> <title> Adaptive Cache Coherency for Detecting Migratory Shared Data. </title> <booktitle> In Proc. of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 98108, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Implementations may then be conservative or aggressive in supporting the memory model. The final optimization we study reduces overhead for one particular pattern of data sharing. General cache-coherence protocols may not optimally handle the majority of common data sharing patterns. Consequently, researchers have investigated many protocol extensions <ref> [7, 9, 11, 24] </ref> that allow existing protocols to perform better under specific classes of data sharing patterns. These extensions attempt to reduce network traversals and/or memory accesses. Two common classes of sharing patterns are migratory data [16] and producer-consumer data sharing. <p> This analysis shows that QOLB outperforms both of these algorithms, both in terms of interconnect messages and memory accesses needed to gain access to a critical section. Cox and Fowler <ref> [9] </ref>, and Stenstrm, Brorsson, and Sandberg [24] present studies that propose different solutions for dealing with the problem of migratory sharing patterns. Both studies present adaptive schemes that can be implemented by a hardware cache coherence protocol.
Reference: [10] <institution> Cypress Semiconductor, </institution> <address> San Jose, California. </address> <note> CY7C601 SPARC RISC Users Guide, second edition, </note> <year> 1990. </year>
Reference-contexts: The first class of optimizations we study is improved synchronization primitives. A straightforward approach to building synchronization functions uses instructions provided by the commodity microprocessor (such as the atomic swap instruction in the SPARC instruction set <ref> [10] </ref>) in much the same way as uniprocessor platforms use them. Typically, the processor accesses a lock repeatedly until the processor finds it unlocked. On a multiprocessor, these repeated accesses often translate directly into network traffic that leads to heavy network contention and potentially severe performance degradation. <p> The simulation target is a 32-node shared-memory multiprocessor supporting the SCI cache-coherence protocol. WWT executes SPARC binaries, and assumes fixed execution time for the instructions (the actual values correspond to the instruction delays listed by the CY7C601 SPARC users guide <ref> [10] </ref>). The execution times for EnQOLB and DeQOLB (executed at the entry and the exit of a critical section) are 3 and 2 cycles respectively. Both instruction and stack accesses to the cache are not simulated in WWT; they are assumed always to hit.
Reference: [11] <author> Frederick Dahlgren, Michel Dubois, and Per Stenstrm. </author> <title> Combined Performance Gains of Simple Cache Protocol Extensions. </title> <booktitle> In Proc. of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 187197, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Implementations may then be conservative or aggressive in supporting the memory model. The final optimization we study reduces overhead for one particular pattern of data sharing. General cache-coherence protocols may not optimally handle the majority of common data sharing patterns. Consequently, researchers have investigated many protocol extensions <ref> [7, 9, 11, 24] </ref> that allow existing protocols to perform better under specific classes of data sharing patterns. These extensions attempt to reduce network traversals and/or memory accesses. Two common classes of sharing patterns are migratory data [16] and producer-consumer data sharing.
Reference: [12] <author> Kourosh Gharachorloo, Sarita V. Adve, Anoop Gupta, John L. Hen-nessy, and Mark D. Hill. </author> <title> Programming for Different Memory Consistency Models. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4):399407, </volume> <year> 1992. </year>
Reference-contexts: We labeled all memory accesses as aggressively as possible according to the structure proposed by Gharachorloo et al. [13]. We then inserted memory fences to achieve release consistency on our simulated hardware platform. The memory fences are consistent with those proposed by Gharachorloo <ref> [12, 13] </ref>. We performed additional optimizations on each benchmark to maximize performance on the simulated hardware. For simulations evaluating QOLB, data structures were modified to couple locks in the same line with the data that they protect. We padded data in each benchmark, where necessary, to eliminate false sharing [14].
Reference: [13] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Philip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory. </title> <booktitle> In Proc. of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 15 26, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Sequential consistency is overly restrictive with respect to multiprocessor memory orderings. Systems can achieve higher performance by relaxing the memory orderings, without compromising the correctness of the program. The class of weakened consistency models that we implemented belong to the release consistency model <ref> [13] </ref>, which divides groups of memory accesses with acquire, release, and special accesses. So long as a program obeys the rules specified by this model, many memory accesses can bypass others, allowing the processor to tolerate the longer latencies associated with remote transactions. <p> A description of these benchmarks appears in the original article [23]. We focus the following discussion on specifics related to our study. We labeled all memory accesses as aggressively as possible according to the structure proposed by Gharachorloo et al. <ref> [13] </ref>. We then inserted memory fences to achieve release consistency on our simulated hardware platform. The memory fences are consistent with those proposed by Gharachorloo [12, 13]. We performed additional optimizations on each benchmark to maximize performance on the simulated hardware. <p> We labeled all memory accesses as aggressively as possible according to the structure proposed by Gharachorloo et al. [13]. We then inserted memory fences to achieve release consistency on our simulated hardware platform. The memory fences are consistent with those proposed by Gharachorloo <ref> [12, 13] </ref>. We performed additional optimizations on each benchmark to maximize performance on the simulated hardware. For simulations evaluating QOLB, data structures were modified to couple locks in the same line with the data that they protect. We padded data in each benchmark, where necessary, to eliminate false sharing [14].
Reference: [14] <author> James R. Goodman and Philip J. </author> <type> Woest. </type> <institution> The Wisconsin Multicube: </institution>
Reference-contexts: We performed additional optimizations on each benchmark to maximize performance on the simulated hardware. For simulations evaluating QOLB, data structures were modified to couple locks in the same line with the data that they protect. We padded data in each benchmark, where necessary, to eliminate false sharing <ref> [14] </ref>. We compiled the benchmarks using GCC version 2.6.2 with the option -O3. Barnes originally used locks to protect the higher levels of the tree during its tree-building phase, which results in an often-locked root.
References-found: 14

