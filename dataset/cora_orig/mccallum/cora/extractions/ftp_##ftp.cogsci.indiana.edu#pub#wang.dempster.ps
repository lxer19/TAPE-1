URL: ftp://ftp.cogsci.indiana.edu/pub/wang.dempster.ps
Refering-URL: http://www.cogsci.indiana.edu/farg/peiwang/papers.html
Root-URL: 
Email: E-mail: pwang@cogsci.indiana.edu  
Title: A Defect in Dempster-Shafer Theory  
Author: Pei Wang 
Address: 510 North Fess Street, Bloomington, IN 47408, USA  
Affiliation: Indiana University  
Note: Center for Research on Concepts and Cognition,  
Abstract: By analyzing the relationships among chance, weight of evidence and degree of belief, it is shown that the assertion "chances are special cases of belief functions" and the assertion "Dempster's rule can be used to combine belief functions based on distinct bodies of evidence" together lead to an inconsistency in Dempster-Shafer theory. To solve this problem, some fundamental postulates of the theory must be rejected. A new approach for uncertainty management is introduced, which shares many intuitive ideas with D-S theory, while avoiding this problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Dempster. </author> <title> Upper and lower probabilities induced by a multivalued mapping. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 38 </volume> <pages> 325-339, </pages> <year> 1967. </year>
Reference-contexts: 1 Introduction Evidence theory, or Dempster-Shafer (D-S) theory, is developed as an attempt to generalize probability theory by introducing a rule for combining distinct bodies of evidence <ref> [1, 7] </ref>. The most influential version of the theory is presented by Shafer in his book A Mathematical Theory of Evidence [7]. In the book, the following postulates are assumed, which form the foundation of D-S theory. <p> Since fi is exhaustive and exclusive by definition, we have H 0 = H (the negation of H). In such a situation, all the information about the system's beliefs can be represented by a pair of real numbers on <ref> [0, 1] </ref>: the degree of belief and the degree of plausibility of fHg, &lt; Bel (fHg); P l (fHg) &gt;, and the belief about H can be derived from them. To simplify our notation, in the following the two numbers are referred to as Bel and P l. <p> P l 1 P l 2 (1) To specify the meaning of "evidence combination", Shafer introduces weight of evidence, w, with the following properties [7, pages 8, 88]: 1. w is a measurement defined on bodies of evidence, with respect to a subset of fi, and it takes values on <ref> [0; 1] </ref>. 2. When two entirely distinct bodies of evidence are combined, the weight of the pooled evidence (for the same subset of fi) is the sum of the original ones. <p> Mathematically speaking, a certain measurement of the evidence (call it weight) is additive during the process. Of course, the rule cannot be applied anywhere. We need to make sure that no evidence is repeatedly counted. This is what Dempster calls "independent sources of information" <ref> [1] </ref> and Shafer calls "distinct body of evidence" [7]. According to D-S theory, belief functions are determined by available evidence. <p> Generally, we have Bel P l, or identically, Bel (fHg) + Bel (f Hg) 1. When Bel = P l, Bel (fHg) become a probability function, because then Bel (fHg) + Bel (f Hg) = 1. In <ref> [1] </ref>, Dempster calls such a belief function "sharp," and treats it as "an ordinary probability measure." In [7], Shafer calls it "Bayesian," and writes it as Bel 1 (fHg). <p> If Postulate 2 were rejected, it would be invalid to interpret Bel and P l as "lower and upper probability" <ref> [1, 3, 4, 12] </ref>. It is true that there are probability functions P (x) satisfying Bel (fxg) P (x) P l (fxg) , for all x 2 fi: However, as demonstrated above, these functions may be unrelated to P r (H). <p> or the theory will fail to meet its original purpose, that is, to extend probability theory by introducing an evidence combination rule. 5 An Alternative Approach In spite of the problems, some intuitions behind D-S theory are still attractive, such as the first three postulates, the idea of lower-upper probabilities <ref> [1] </ref>, and the distinction between disbelief and lack of belief [7]. From previous discussion, we have seen that the core of evidence combination is the relationships among degree of belief, chance, and weight of evidence. The combination rule can be derived from these relationships.
Reference: [2] <author> D. Dubois and H. Prade. </author> <title> Updating with belief functions, ordinal conditional functions and possibility measures. </title> <editor> In P. Bonissone, M. Henrion, L. Kanal, and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 6, </booktitle> <pages> pages 311-329. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: In D-S theory, however, they are combined to get a different Bayesian belief function, except for some special points. Such a result is counter-intuitive [18] and inconsistent with Shafer's interpretation of chance. There are already many papers on the justification of Dempster's rule <ref> [2, 5, 6, 11, 14, 19] </ref>, but few of them addresses the relationships among degree of belief, weight of evidence, and chance. As a result, the mathematical properties of D-S theory are explored in detail, but its usage of notions, such as "chance" and "evidence combination", lacks a careful analysis.
Reference: [3] <author> D. Dubois and H. Prade. </author> <title> Evidence, knowledge, and belief functions. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 6 </volume> <pages> 295-319, </pages> <year> 1992. </year>
Reference-contexts: If Postulate 2 were rejected, it would be invalid to interpret Bel and P l as "lower and upper probability" <ref> [1, 3, 4, 12] </ref>. It is true that there are probability functions P (x) satisfying Bel (fxg) P (x) P l (fxg) , for all x 2 fi: However, as demonstrated above, these functions may be unrelated to P r (H).
Reference: [4] <author> R. Fagin and J. Halpern. </author> <title> A new approach to updating beliefs. </title> <editor> In P. Bonissone, M. Hen-rion, L. Kanal, and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 6, </booktitle> <pages> pages 347-374. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: According to many, if not all, textbooks and introductory papers, D-S theory is a generalization of probability theory, and a chance can be used as a degree of belief. 2. The "lower-upper bounds of probability" interpretation for belief functions is still ac cepted by some authors <ref> [4] </ref>. 3. Some other authors, including Shafer himself, reject the above interpretation, but they still refer to a probability function as a special type (or a limit) of belief functions [9]. 6 4. <p> If Postulate 2 were rejected, it would be invalid to interpret Bel and P l as "lower and upper probability" <ref> [1, 3, 4, 12] </ref>. It is true that there are probability functions P (x) satisfying Bel (fxg) P (x) P l (fxg) , for all x 2 fi: However, as demonstrated above, these functions may be unrelated to P r (H).
Reference: [5] <author> H. Kyburg. </author> <title> Bayesian and non-Bayesian evidential updating. </title> <journal> Artificial Intelligence, </journal> <volume> 31 </volume> <pages> 271-293, </pages> <year> 1987. </year>
Reference-contexts: In D-S theory, however, they are combined to get a different Bayesian belief function, except for some special points. Such a result is counter-intuitive [18] and inconsistent with Shafer's interpretation of chance. There are already many papers on the justification of Dempster's rule <ref> [2, 5, 6, 11, 14, 19] </ref>, but few of them addresses the relationships among degree of belief, weight of evidence, and chance. As a result, the mathematical properties of D-S theory are explored in detail, but its usage of notions, such as "chance" and "evidence combination", lacks a careful analysis.
Reference: [6] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: In D-S theory, however, they are combined to get a different Bayesian belief function, except for some special points. Such a result is counter-intuitive [18] and inconsistent with Shafer's interpretation of chance. There are already many papers on the justification of Dempster's rule <ref> [2, 5, 6, 11, 14, 19] </ref>, but few of them addresses the relationships among degree of belief, weight of evidence, and chance. As a result, the mathematical properties of D-S theory are explored in detail, but its usage of notions, such as "chance" and "evidence combination", lacks a careful analysis.
Reference: [7] <author> G. Shafer. </author> <title> A Mathematical Theory of Evidence. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1976. </year>
Reference-contexts: 1 Introduction Evidence theory, or Dempster-Shafer (D-S) theory, is developed as an attempt to generalize probability theory by introducing a rule for combining distinct bodies of evidence <ref> [1, 7] </ref>. The most influential version of the theory is presented by Shafer in his book A Mathematical Theory of Evidence [7]. In the book, the following postulates are assumed, which form the foundation of D-S theory. <p> 1 Introduction Evidence theory, or Dempster-Shafer (D-S) theory, is developed as an attempt to generalize probability theory by introducing a rule for combining distinct bodies of evidence [1, 7]. The most influential version of the theory is presented by Shafer in his book A Mathematical Theory of Evidence <ref> [7] </ref>. In the book, the following postulates are assumed, which form the foundation of D-S theory. Postulate 1: Chance is the limit of the proportion of "positive" outcomes among all out comes [7, pages 9, 202]. <p> In the book, the following postulates are assumed, which form the foundation of D-S theory. Postulate 1: Chance is the limit of the proportion of "positive" outcomes among all out comes <ref> [7, pages 9, 202] </ref>. Postulate 2: Chances, if known, should be used as belief functions [7, pages 16, 201]. Postulate 3: Evidence combination refers to the pooling, or accumulating, of distinct bodies of evidence [7, pages 8, 77]. <p> In the book, the following postulates are assumed, which form the foundation of D-S theory. Postulate 1: Chance is the limit of the proportion of "positive" outcomes among all out comes [7, pages 9, 202]. Postulate 2: Chances, if known, should be used as belief functions <ref> [7, pages 16, 201] </ref>. Postulate 3: Evidence combination refers to the pooling, or accumulating, of distinct bodies of evidence [7, pages 8, 77]. Postulate 4: Dempster's rule can be used on belief functions for evidence combination [7, pages 6, 57]. <p> Postulate 2: Chances, if known, should be used as belief functions [7, pages 16, 201]. Postulate 3: Evidence combination refers to the pooling, or accumulating, of distinct bodies of evidence <ref> [7, pages 8, 77] </ref>. Postulate 4: Dempster's rule can be used on belief functions for evidence combination [7, pages 6, 57]. In this paper, we show, by discussing a simple situation, that there is an inconsistency among the postulates. <p> Postulate 2: Chances, if known, should be used as belief functions [7, pages 16, 201]. Postulate 3: Evidence combination refers to the pooling, or accumulating, of distinct bodies of evidence [7, pages 8, 77]. Postulate 4: Dempster's rule can be used on belief functions for evidence combination <ref> [7, pages 6, 57] </ref>. In this paper, we show, by discussing a simple situation, that there is an inconsistency among the postulates. Then, we argue that though there are several possible solutions of this problem within the framework of D-S theory, each of them has serious disadvantages. <p> l &gt; value, according to Dempster's rule (Postulate 4): Bel = 1 Bel 1 (1 P l 2 ) Bel 2 (1 P l 1 ) P l 1 P l 2 (1) To specify the meaning of "evidence combination", Shafer introduces weight of evidence, w, with the following properties <ref> [7, pages 8, 88] </ref>: 1. w is a measurement defined on bodies of evidence, with respect to a subset of fi, and it takes values on [0; 1]. 2. <p> Of course, the rule cannot be applied anywhere. We need to make sure that no evidence is repeatedly counted. This is what Dempster calls "independent sources of information" [1] and Shafer calls "distinct body of evidence" <ref> [7] </ref>. According to D-S theory, belief functions are determined by available evidence. <p> According to D-S theory, belief functions are determined by available evidence. Given (1) and (2), Shafer shows that the relationship between &lt; Bel; P l &gt; and &lt; w + ; w &gt; can be 2 derived <ref> [7, page 84] </ref>: Bel = 1 + e w P l = e w + 1 It is also possible to derive (1) from (2) and (3), or derive (2) from (1) and (3). <p> When Bel = P l, Bel (fHg) become a probability function, because then Bel (fHg) + Bel (f Hg) = 1. In [1], Dempster calls such a belief function "sharp," and treats it as "an ordinary probability measure." In <ref> [7] </ref>, Shafer calls it "Bayesian," and writes it as Bel 1 (fHg). <p> P l happens if and only if the weight of all evidence, w (w = w + + w ), goes to infinite: Bel 1 (fHg) = lim Bel = lim P l (4) Shafer interprets the above relationship as indicating that probability functions is a subset of belief functions <ref> [7, pages 19] </ref>, and degree of belief converges to chance when the available evidence goes to infinite (Postulate 2), that is, Bel 1 (fHg) = P r (H) (5) where P r (H) is the chance, or aleatory probability, of H [7, pages 16, 33, 201]. <p> that probability functions is a subset of belief functions [7, pages 19], and degree of belief converges to chance when the available evidence goes to infinite (Postulate 2), that is, Bel 1 (fHg) = P r (H) (5) where P r (H) is the chance, or aleatory probability, of H <ref> [7, pages 16, 33, 201] </ref>. <p> The conclusion Bel 1 = 1=(1 + e ) is proven by Shafer himself <ref> [7, page 198] </ref>. However, he does not relate it to chance. <p> For example, Shafer describes chance as "essentially hypothetical rather than empirical," and unreachable by collecting (finite) evidence <ref> [7, page 202] </ref>. According to this interpretation, combining the evidence of two different Bayesian belief functions becomes invalid or nonsense, because they are chances and therefore not supported by finite empirical evidence. <p> This is the case because of the one-to-one correspondence between the weight of evidence and the belief function. The rejection of Postulate 2 seems more plausible than the previous alternatives. Very few authors actually use Bel 1 (fHg) to represent the chance of H. Even in Shafer's classic book <ref> [7] </ref>, in which Postulate 2 is made or assumed at several places, Bel 1 is not directly applied to represent statistical evidence. However, there is not a consensus in the "Uncertainty in AI" community that Bel 1 (fxg) and P r (x) are unequal. The following phenomena shows this: 1. <p> For the same reason, the assertion that "the Bayesian theory is a limiting case of D-S theory" <ref> [7, page 32] </ref> may be misleading. From a mathematical point of view, this assertion is true, since Bel 1 (fHg) is a probability function. But as discussed previously, it is not the probability, or chance, of H. <p> In general, the relationship between D-S theory and probability theory will be very loose. It is still possible to put different possible probability distributions into fi and to assign belief function to them, as Shafer did <ref> [7, 8] </ref>. For example, the knowledge "P r (H) = 0:51" can be represented as "Bel (fP r (H) = 0:51g) = 1." However, here the probability function is evaluated by the belief function, rather than being a special case of it. The two are at different levels. <p> that is, to extend probability theory by introducing an evidence combination rule. 5 An Alternative Approach In spite of the problems, some intuitions behind D-S theory are still attractive, such as the first three postulates, the idea of lower-upper probabilities [1], and the distinction between disbelief and lack of belief <ref> [7] </ref>. From previous discussion, we have seen that the core of evidence combination is the relationships among degree of belief, chance, and weight of evidence. The combination rule can be derived from these relationships. Let us continue with the previous example. <p> The latter, being the probability of H, is more helpful and important in most situations than the former is. In fact, Shafer acknowledges the problem when he writes, "It is difficult to imagine a belief function such as Bel 1 being useful for the representation of actual evidence <ref> [7, page 199] </ref>." However, the result seems to be accepted without further analysis, since it follows from Dempster's rule. Let us apply the paradigm to infinite evidence. <p> Here we are even more faithful to Shafer's interpretation of (aleatory) probability than D-S theory is. Being "essentially hypothetical rather than empirical," probability cannot be evaluated with less than infinite evidence <ref> [7, page 201] </ref>. For the same reason, it should not be changed by less than infinite evidence. 10 In summary, though many of the intuitive ideas of D-S theory are preserved, the problem in D-S theory discussed above no longer exists in the "lower-upper frequency" approach.
Reference: [8] <author> G. Shafer. </author> <title> Belief functions and parametric models. </title> <journal> Journal of the Royal Statistical Society. Series B, </journal> <volume> 44 </volume> <pages> 322-352, </pages> <year> 1982. </year>
Reference-contexts: In general, the relationship between D-S theory and probability theory will be very loose. It is still possible to put different possible probability distributions into fi and to assign belief function to them, as Shafer did <ref> [7, 8] </ref>. For example, the knowledge "P r (H) = 0:51" can be represented as "Bel (fP r (H) = 0:51g) = 1." However, here the probability function is evaluated by the belief function, rather than being a special case of it. The two are at different levels.
Reference: [9] <author> G. Shafer. </author> <title> Perspectives on the theory and practice of belief functions. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 4 </volume> <pages> 323-362, </pages> <year> 1990. </year>
Reference-contexts: Another way to reject Postulate 3 is to remove the concept of weight of evidence from D-S theory. Actually weight of evidence is seldom mentioned in the literature of D-S theory. Shafer, in his later papers (for example, <ref> [9, 10] </ref>), tends to relate belief functions to reliability of testimony and randomly coded message, rather than to weight of evidence. One problem of such a solution is the loss of the intuition in the notion of "evidence combination". <p> The "lower-upper bounds of probability" interpretation for belief functions is still ac cepted by some authors [4]. 3. Some other authors, including Shafer himself, reject the above interpretation, but they still refer to a probability function as a special type (or a limit) of belief functions <ref> [9] </ref>. 6 4. Though some authors have gone so far to the conclusion that Bayesian belief functions do not generally correspond to Bayesian measures of belief, they still view a belief function as the lower bound of probability [18]. 5.
Reference: [10] <author> G. Shafer and A. Tversky. </author> <title> Languages and designs for probability judgment. </title> <journal> Cognitive Science, </journal> <volume> 12 </volume> <pages> 177-210, </pages> <year> 1985. </year>
Reference-contexts: Another way to reject Postulate 3 is to remove the concept of weight of evidence from D-S theory. Actually weight of evidence is seldom mentioned in the literature of D-S theory. Shafer, in his later papers (for example, <ref> [9, 10] </ref>), tends to relate belief functions to reliability of testimony and randomly coded message, rather than to weight of evidence. One problem of such a solution is the loss of the intuition in the notion of "evidence combination".
Reference: [11] <author> Ph. Smets. </author> <title> The combination of evidence in the transferable belief model. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 447-458, </pages> <year> 1990. </year>
Reference-contexts: In D-S theory, however, they are combined to get a different Bayesian belief function, except for some special points. Such a result is counter-intuitive [18] and inconsistent with Shafer's interpretation of chance. There are already many papers on the justification of Dempster's rule <ref> [2, 5, 6, 11, 14, 19] </ref>, but few of them addresses the relationships among degree of belief, weight of evidence, and chance. As a result, the mathematical properties of D-S theory are explored in detail, but its usage of notions, such as "chance" and "evidence combination", lacks a careful analysis. <p> Though some authors have gone so far to the conclusion that Bayesian belief functions do not generally correspond to Bayesian measures of belief, they still view a belief function as the lower bound of probability [18]. 5. In the transferable belief model of D-S theory <ref> [11, 12, 13] </ref>, Smets shows that it is possible "for quantified beliefs developed independently of any underlying probabilistic model," though he still believes that "it seems reasonable to defend the idea that the belief of an event should be numerically equal to the probability of that event" [13].
Reference: [12] <author> Ph. Smets. </author> <title> The transferable belief model and other interpretations of Dempster-Shafer's model. </title> <editor> In P. Bonissone, M. Henrion, L. Kanal, and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 6, </booktitle> <pages> pages 375-383. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: Though some authors have gone so far to the conclusion that Bayesian belief functions do not generally correspond to Bayesian measures of belief, they still view a belief function as the lower bound of probability [18]. 5. In the transferable belief model of D-S theory <ref> [11, 12, 13] </ref>, Smets shows that it is possible "for quantified beliefs developed independently of any underlying probabilistic model," though he still believes that "it seems reasonable to defend the idea that the belief of an event should be numerically equal to the probability of that event" [13]. <p> If Postulate 2 were rejected, it would be invalid to interpret Bel and P l as "lower and upper probability" <ref> [1, 3, 4, 12] </ref>. It is true that there are probability functions P (x) satisfying Bel (fxg) P (x) P l (fxg) , for all x 2 fi: However, as demonstrated above, these functions may be unrelated to P r (H).
Reference: [13] <author> Ph. Smets. </author> <title> Belief induced by the partial knowledge of the probabilities. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 523-530. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, California, </address> <year> 1994. </year>
Reference-contexts: For example, according to Smets, "distinctness" is not satisfied in the present context because of the existence of a underlying probability function P r that create a link among the outcomes of the experiment <ref> [13] </ref>. Accepting such an opinion, however, means that Postulate 2 is rejected. <p> Though some authors have gone so far to the conclusion that Bayesian belief functions do not generally correspond to Bayesian measures of belief, they still view a belief function as the lower bound of probability [18]. 5. In the transferable belief model of D-S theory <ref> [11, 12, 13] </ref>, Smets shows that it is possible "for quantified beliefs developed independently of any underlying probabilistic model," though he still believes that "it seems reasonable to defend the idea that the belief of an event should be numerically equal to the probability of that event" [13]. <p> of D-S theory [11, 12, 13], Smets shows that it is possible "for quantified beliefs developed independently of any underlying probabilistic model," though he still believes that "it seems reasonable to defend the idea that the belief of an event should be numerically equal to the probability of that event" <ref> [13] </ref>. Although it is possible to get rid of the inconsistency by give up the equality of Bel 1 (fxg) and P r (x), such a solution will make the relationship between probability theory and D-S theory complicated. <p> For instance, we can say that Dempster's rule does not apply to evidence combination, but can be used for some other purposes. Even so, the initial goal of D-S theory will be missed. Another suggestion is to use Dempster's rule only on non-Bayesian belief functions <ref> [13, 18] </ref>. However, the problem remains under the constraint, because in the previous demonstration Dempster's rule is only applied to non-Bayesian belief functions to make equations (3) true.
Reference: [14] <author> F. Voorbraak. </author> <title> On the justification of Dempster's rule of combination. </title> <journal> Artificial Intelligence, </journal> <volume> 48 </volume> <pages> 171-197, </pages> <year> 1991. </year> <month> 13 </month>
Reference-contexts: In D-S theory, however, they are combined to get a different Bayesian belief function, except for some special points. Such a result is counter-intuitive [18] and inconsistent with Shafer's interpretation of chance. There are already many papers on the justification of Dempster's rule <ref> [2, 5, 6, 11, 14, 19] </ref>, but few of them addresses the relationships among degree of belief, weight of evidence, and chance. As a result, the mathematical properties of D-S theory are explored in detail, but its usage of notions, such as "chance" and "evidence combination", lacks a careful analysis.
Reference: [15] <author> P. Wang. </author> <title> Belief revision in probability theory. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 519-526. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: Though the criticism of D-S theory to Bayes approach is justifiable, and the "lower-upper frequency" approach is motivated by similar theoretical considerations <ref> [15] </ref>, the two approaches solve the problem differently. The "lower-upper frequency" approach is not specially designed to replace D-S theory in general, but it does suggest a better way to represent and process uncertainty.
Reference: [16] <author> P. Wang. </author> <title> Non-axiomatic reasoning system (version 2.2). </title> <type> Technical Report 75, </type> <institution> Center for Research on Concepts and Cognition, Indiana University, Bloomington, Indiana, </institution> <year> 1993. </year> <note> Available via WWW at http://www.cogsci.indiana.edu/farg/peiwang/papers.html. </note>
Reference-contexts: This approach is used in the Non-Axiomatic Reasoning System (NARS) project. As an intelligent reasoning system, NARS can adapt to its environment and answer questions with insufficient knowledge and resources <ref> [16, 17] </ref>. A complete comparison of NARS and D-S theory is beyond the scope of this paper. <p> The new approach sets up a more natural relation among the various measurements of uncertainty, including probability. It can combine evidence from distinct sources. Therefore, it makes the system capable of carrying out multiple types of inference, such as deduction, induction, and abduction <ref> [16, 17] </ref>. 11 APPENDIX: Detailed derivation of (7) * If w + 0 (1 P r), then lim e w + 1 0 t + 0 t = lim e w + 0 t 0 t 0 t + w + 1 e w = lim e t [w t )w
Reference: [17] <author> P. Wang. </author> <title> From inheritance relation to nonaxiomatic logic. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 11(4) </volume> <pages> 281-319, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Though it is possible, in theory, to directly use w and w + as measurements of uncertainty, it is often unnatural and inconvenient <ref> [17] </ref>. Can we capture this kind of information without recording w and w + directly? Yes, we can. From the viewpoint of evidence combination, the influence of w appears in the stability of a frequence evaluation based on it. <p> In general, the interval bounds the frequence until the weight of new evidence reaches a constant unit. For the current purpose, the 1 that appears in the definitions of l and u can be substituted by any positive number <ref> [17] </ref>. 1 is used here to simplify the discussion. As bounds of frequency, l and u share intuitions with Dempster's P fl and P fl , as well as Shafer's Bel and P l. <p> This approach is used in the Non-Axiomatic Reasoning System (NARS) project. As an intelligent reasoning system, NARS can adapt to its environment and answer questions with insufficient knowledge and resources <ref> [16, 17] </ref>. A complete comparison of NARS and D-S theory is beyond the scope of this paper. <p> The new approach sets up a more natural relation among the various measurements of uncertainty, including probability. It can combine evidence from distinct sources. Therefore, it makes the system capable of carrying out multiple types of inference, such as deduction, induction, and abduction <ref> [16, 17] </ref>. 11 APPENDIX: Detailed derivation of (7) * If w + 0 (1 P r), then lim e w + 1 0 t + 0 t = lim e w + 0 t 0 t 0 t + w + 1 e w = lim e t [w t )w
Reference: [18] <author> N. Wilson. </author> <title> The combination of belief: when and how fast? International Journal of Approximate Reasoning, </title> <booktitle> 6 </booktitle> <pages> 377-388, </pages> <year> 1992. </year>
Reference-contexts: If Bel 11 (fHg) and Bel 12 (fHg) are equal, then they are the same convention made from different considerations. In D-S theory, however, they are combined to get a different Bayesian belief function, except for some special points. Such a result is counter-intuitive <ref> [18] </ref> and inconsistent with Shafer's interpretation of chance. There are already many papers on the justification of Dempster's rule [2, 5, 6, 11, 14, 19], but few of them addresses the relationships among degree of belief, weight of evidence, and chance. <p> Though some authors have gone so far to the conclusion that Bayesian belief functions do not generally correspond to Bayesian measures of belief, they still view a belief function as the lower bound of probability <ref> [18] </ref>. 5. <p> For instance, we can say that Dempster's rule does not apply to evidence combination, but can be used for some other purposes. Even so, the initial goal of D-S theory will be missed. Another suggestion is to use Dempster's rule only on non-Bayesian belief functions <ref> [13, 18] </ref>. However, the problem remains under the constraint, because in the previous demonstration Dempster's rule is only applied to non-Bayesian belief functions to make equations (3) true.
Reference: [19] <author> N. Wilson. </author> <title> The assumptions behind Dempster's rule. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 527-534. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1993. </year> <month> 14 </month>
Reference-contexts: In D-S theory, however, they are combined to get a different Bayesian belief function, except for some special points. Such a result is counter-intuitive [18] and inconsistent with Shafer's interpretation of chance. There are already many papers on the justification of Dempster's rule <ref> [2, 5, 6, 11, 14, 19] </ref>, but few of them addresses the relationships among degree of belief, weight of evidence, and chance. As a result, the mathematical properties of D-S theory are explored in detail, but its usage of notions, such as "chance" and "evidence combination", lacks a careful analysis.
References-found: 19

