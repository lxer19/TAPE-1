URL: ftp://ftp.cs.colorado.edu/users/alw/papers/icsp3.ps.Z
Refering-URL: http://www.cs.colorado.edu/~arcadia/Papers/metrics/cu_metrics_papers.html
Root-URL: http://www.cs.colorado.edu
Email: fjcook,alwg@cs.colorado.edu  
Title: Toward Metrics for Process Validation  
Author: Jonathan E. Cook and Alexander L. Wolf 
Affiliation: Department of Computer Science University of Colorado  
Date: October 10-11, 1994  
Address: Reston, Virginia, USA,  Boulder, CO 80309 USA  
Note: From the Proc. of the 3rd Inter. Conf. on the Software Process,  
Abstract: To a great extent, the usefulness of a formal model of a software process lies in its ability to accurately predict the behavior of the executing process. Similarly, the usefulness of an executing process lies largely in its ability to fulfill the requirements embodied in a formal model of the process. When process models and process executions diverge, something significant is happening. We are developing techniques for uncovering discrepancies between models and executions under the rubric of process validation. Further, we are developing met-rics for process validation that give engineers a feel for the severity of the discrepancy. We view the metrics presented here as a first step toward a suite of useful metrics for process validation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.S. Avrunin, U.A. Buy, J.C. Corbett, L.K. Dil-lon, and J.C. Wileden. </author> <title> Automated Analysis of Concurrent Systems with the Constrained Expression Toolset. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(11) </volume> <pages> 1204-1222, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: There are elegant methods that, given a model, a current simulation state of that model, and a desired event, can answer the question "Can this event be produced in the future?". One such method is Constrained Expressions <ref> [1] </ref>. If the answer is "yes", then, in the case of Constrained Expressions, heuristics are used to produce a plausible behavior that leads to the event. This behavior constitutes the next sequence of model events.
Reference: [2] <author> S. Bandinelli, A. Fuggetta, and C. Ghezzi. </author> <title> Software Process Model Evolution in the SPADE En-vironement. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1128-1144, December 1993. 
Reference-contexts: Even if one could completely enforce a process, there still remains the issue of managing change in a process, which might lead to a discrepancy between the model and the execution. There has, in fact, been considerable recent work that addresses process evolution <ref> [2, 17] </ref>. Commensurate with the historical approach mentioned above, that work is concerned more with the problem of effecting changes to a process model used for automation, than it is with the problem of uncovering inconsistencies between the model and the execution.
Reference: [3] <author> S. Bandinelli, C. Ghezzi, and A. Morzenti. </author> <title> A Multi-Paradigm Petri Net Based Approach to Process Description. </title> <booktitle> In Proceedings of the 7th International Software Process Workshop, </booktitle> <pages> pages 41-43, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Several formal models suitable for our analyses have been used to describe software processes. These in 34 clude models based on state machines (e.g., State-mate [16]), Petri nets (e.g., Slang <ref> [3] </ref> and FUN-SOFT Nets [15]), and procedural languages (e.g., APPL/A [24]). Techniques for process validation clearly depend on an ability to collect data about an executing process, but we do not address this topic here. Fortunately, a variety of methods for collecting process execution data have been devised.
Reference: [4] <author> V.R. Basili and D.M. Weiss. </author> <title> A Methodology for Collecting Valid Software Engineering Data. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(6):728-738, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: Fortunately, a variety of methods for collecting process execution data have been devised. Basili and Weiss describe a method for manual, forms-based collection of data for use in evaluating and comparing software development methods <ref> [4] </ref>. Amadeus is a system for automated collection and analysis of process metrics [22]. Wolf and Rosenblum use a hybrid of manual and automated collection methods [25]. <p> Their system, Amadeus, can automatically collect measurement data (currently focused primarily on product data) that can then be used to guide development efforts. * Basili and Weiss <ref> [4] </ref> describe a methodology for selecting metrics and data collection techniques based on the goals that are desired of the measurement activity.
Reference: [5] <author> P. Bates. </author> <title> Debugging Heterogenous Systems Using Event-Based Models of Behavior. </title> <booktitle> In Proceedings of a Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 11-22. </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: In using event-based data to compare an execution with a formal model, the most closely related work to ours is in the areas of distributed debugging and history checking. * Bates <ref> [5] </ref> uses "event-based behavioral abstraction" to characterize the behavior of programs. He then attempts to match the event data to a model based on regular expressions.
Reference: [6] <author> I. Bhandari, M. Halliday, E. Tarver, D. Brown, J. Chaar, and R. Chillarege. </author> <title> A Case Study of Software Process Improvement During Development. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1157-1170, December 1993. 
Reference-contexts: In particular, most previous work has used product data metrics to guide process changes that refocus effort onto specific problem areas of a project. Below, we summarize some of this work. 41 * Chmura et al. [8] and Bhandari et al. <ref> [6] </ref> try to de-duce problems in the process by looking at defect data in the products. * Selby et al. [22] take the approach of providing automated support for empirically guided software development.
Reference: [7] <author> M.G. Bradac, D.E. Perry, and L.G. Votta. </author> <title> Pro-totyping a Process Monitoring Experiment. </title> <booktitle> In Proceedings of the 15th International Conference on Software Engineering, </booktitle> <pages> pages 155-165. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: centers on using a rule base and goals to derive a generalized execution flow from a specific process history. * Garg et al. [14] employ a manual process history analysis in the context of a meta-process for creating and validating domain-specific process models and software toolkits. * Bradac et al. <ref> [7] </ref> describe the beginnings of a process monitoring experiment in which their goal is to model the process as a queuing network, and use actual data about the time spent by the agents in specific tasks and states to determine the real parameters (i.e., service times and probabilities, and branch path
Reference: [8] <author> L.J. Chmura, A.F. Norcio, and T.J. Wicinski. </author> <title> Evaluating Software Design Processes by Analyzing Change Data Over Time. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(7) </volume> <pages> 729-739, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: In particular, most previous work has used product data metrics to guide process changes that refocus effort onto specific problem areas of a project. Below, we summarize some of this work. 41 * Chmura et al. <ref> [8] </ref> and Bhandari et al. [6] try to de-duce problems in the process by looking at defect data in the products. * Selby et al. [22] take the approach of providing automated support for empirically guided software development.
Reference: [9] <author> J. Cuny, G. Forman, A. Hough, J. Kundu, C. Lin, L. Snyder, and D. Stemple. </author> <title> The Adriane Debugger: Scalable Application of Event-Based Abstraction. </title> <booktitle> In Proceedings of the ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 85-95. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: He then attempts to match the event data to a model based on regular expressions. However, he only marks the points at which the data and model did not match, not attempting to provide aggregate measures of disparity. * Cuny et al. <ref> [9] </ref> builds on the work of Bates, attempting to deal with large amounts of event data by providing query mechanisms for event relationships.
Reference: [10] <author> J.L. Devore. </author> <title> Probability and Statistics for Engineering and the Sciences. </title> <address> Brooks/Cole, Pacific Grove, California, </address> <note> 3rd edition, </note> <year> 1991. </year>
Reference-contexts: Thus, one might pick the standard statistical correlation rules of thumb <ref> [10] </ref> and say that any measurement less than 0:2 is a strong correspondence, less than 0:5 is a moderate correspondence, and greater than 0:5 is a weak correspondence. (Actually, these are inversions of the standard statistical rules of thumb, but their effect is the same.) 3.3 Non-linear String Distance Metric A
Reference: [11] <author> M. Felder, D. Mandrioli, and A. Morzenti. </author> <title> Proving Properties of Real-time Systems Through Logical Specifications and Petri Net Models. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 20(2) </volume> <pages> 127-141, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: They assume that there is some problem somewhere in the event stream and that one is trying to locate that problem. * Felder et al. <ref> [11, 12] </ref> describe a method and tool by which one can compare an execution history against a temporal logic specification to decide the correctness of that execution with respect to the model. Our immediate goal is to quantify discrepancies, with correctness being a separate issue.
Reference: [12] <author> M. Felder and A. Morzenti. </author> <title> Validating Real-time Systems by History-checking TRIO Specifications. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 199-211. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: They assume that there is some problem somewhere in the event stream and that one is trying to locate that problem. * Felder et al. <ref> [11, 12] </ref> describe a method and tool by which one can compare an execution history against a temporal logic specification to decide the correctness of that execution with respect to the model. Our immediate goal is to quantify discrepancies, with correctness being a separate issue.
Reference: [13] <author> P.K. Garg and S. Bhansali. </author> <title> Process Programming by Hindsight. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 280-293. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: Though there is no attempt to relate this to real data, "what-if" analyses are powerful tools in their own right. Some recent efforts have begun to look at process data itself, but still not for the purpose of process valida tion. * Garg and Bhansali <ref> [13] </ref> describe a method that uses explanation-based learning to discover aspects and fragments of the underlying process model from process history data and rules of operations and their effects.
Reference: [14] <author> P.K. Garg, M. Jazayeri, </author> <title> and M.L. Creech. A Meta-Process for Software Reuse, Process Discovery, and Evolution. </title> <booktitle> In Proceedings of the 6th International Workshop on Software Reuse, </booktitle> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: This work centers on using a rule base and goals to derive a generalized execution flow from a specific process history. * Garg et al. <ref> [14] </ref> employ a manual process history analysis in the context of a meta-process for creating and validating domain-specific process models and software toolkits. * Bradac et al. [7] describe the beginnings of a process monitoring experiment in which their goal is to model the process as a queuing network, and use
Reference: [15] <author> V. Gruhn and R. Jegelka. </author> <title> An Evaluation of FUN-SOFT Nets. </title> <booktitle> In Proceedings of the Second Euro-pean Workshop on Software Process Technology, number 635 in Lecture Notes in Computer Science, </booktitle> <pages> pages 196-214. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: Several formal models suitable for our analyses have been used to describe software processes. These in 34 clude models based on state machines (e.g., State-mate [16]), Petri nets (e.g., Slang [3] and FUN-SOFT Nets <ref> [15] </ref>), and procedural languages (e.g., APPL/A [24]). Techniques for process validation clearly depend on an ability to collect data about an executing process, but we do not address this topic here. Fortunately, a variety of methods for collecting process execution data have been devised.
Reference: [16] <author> D. Harel, H. Lachover, A. Naamad, A. Pnueli, M. Politi, R. Sherman, and A. Shtul-Trauring. STATEMATE: </author> <title> A Working Environment for the Development of Complex Reactive Systems. </title> <booktitle> In Proceedings of the 10th International Conference on Software Engineering, </booktitle> <pages> pages 396-406. </pages> <publisher> IEEE Computer Society, </publisher> <month> April </month> <year> 1988. </year>
Reference-contexts: Thus, possible paths through the specification|that is, possible behaviors specified by the model|can be represented by event streams produced by a simulation. Several formal models suitable for our analyses have been used to describe software processes. These in 34 clude models based on state machines (e.g., State-mate <ref> [16] </ref>), Petri nets (e.g., Slang [3] and FUN-SOFT Nets [15]), and procedural languages (e.g., APPL/A [24]). Techniques for process validation clearly depend on an ability to collect data about an executing process, but we do not address this topic here.
Reference: [17] <author> M.L. Jaccheri and R. Conradi. </author> <title> Techniques for Process Model Evolution in EPOS. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1145-1156, December 1993. 
Reference-contexts: Even if one could completely enforce a process, there still remains the issue of managing change in a process, which might lead to a discrepancy between the model and the execution. There has, in fact, been considerable recent work that addresses process evolution <ref> [2, 17] </ref>. Commensurate with the historical approach mentioned above, that work is concerned more with the problem of effecting changes to a process model used for automation, than it is with the problem of uncovering inconsistencies between the model and the execution.
Reference: [18] <author> M.I. Kellner. </author> <title> Software Process Modeling Support for Management Planning and Control. </title> <booktitle> In Proceedings of the First International Conference on the Software Process, </booktitle> <pages> pages 8-28. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: Their work also focuses on using product data, such as code modifications and change classification. * Kellner <ref> [18] </ref> shows the usefulness of simulation and "what-if" analyses in forecasting the schedule and outcome of a specific execution of a process. He uses deterministic and stochastic modeling, along with resource constraints, to derive schedule, work effort, and staffing estimations.
Reference: [19] <author> M.I. Kellner, P.H. Feiler, A. Finkelstein, T. Katayama, L.J. Osterweil, M.H. Penedo, and H.D. Rombach. </author> <title> Software Process Modeling Example Problem. </title> <booktitle> In Proceedings of the 6th International Software Process Workshop, </booktitle> <pages> pages 19-29, </pages> <month> October </month> <year> 1990. </year> <month> 43 </month>
Reference-contexts: We view the metrics presented here as a first step toward a suite of useful metrics for process validation. The next section of the paper describes the framework within which the techniques have been developed. Section 3 introduces the process validation met-rics. A portion of the ISPW6/7 example <ref> [19] </ref> is used in Section 4 to demonstrate the application of the met-rics to a process. <p> Constrained Expressions is just one example of the kinds of techniques available to generate a complete model event stream. 4 Example Use of the Metrics To illustrate the various metrics introduced above, we use the Test Unit task from the ISPW 6/7 process problem <ref> [19] </ref>. This is a very simple and small process fragment, but it should give the reader a feeling for how the metrics are applied to a process. In this task, a developer and a tester are involved in testing a module that has undergone some change.
Reference: [20] <author> J.B. Kruskal. </author> <title> An Overview of Sequence Com--parison. </title> <editor> In D. Sankoff and J.B. Kruskal, editors, </editor> <title> Time Warps, String Edits, and Macromolecules: </title> <booktitle> The Theory and Practice of Sequence Comparison, </booktitle> <pages> pages 1-44. </pages> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1983. </year>
Reference-contexts: We can then apply a well-known method for calculating the distance between strings <ref> [20] </ref> and use distance as the metric of difference between the process model and process execution. String distance metrics have been used profitably as measures of correspondence in a wide variety of other domains, including parsing, DNA/RNA sequencing, and text recognition [21]. <p> Given two strings, one of length n and the other of length m, the minimal total cost of operations can be computed in O (nm) time using a well-known dynamic program <ref> [20] </ref>. In some applications of this method, such as DNA/RNA sequencing or text recognition, token substitution in the string distance metric makes sense. For process validation, however, it is not clear that a substituted event should contribute in any way to the goodness of the correspondence.
Reference: [21] <author> D. Sankoff and J.B. Kruskal, </author> <title> editors. Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1983. </year>
Reference-contexts: String distance metrics have been used profitably as measures of correspondence in a wide variety of other domains, including parsing, DNA/RNA sequencing, and text recognition <ref> [21] </ref>. The basic Levenshtein distance between two strings is measured by counting the minimal number of token insertions, deletions, and substitutions needed to transform one string into the other. Figure 2 shows two event streams, represented as strings, and one possible correspondence between their events.
Reference: [22] <author> R.W. Selby, A.A. Porter, D.C. Schmidt, and J. Berney. </author> <title> Metric-Driven Analysis and Feedback Systems for Enabling Empirically Guided Software Development. </title> <booktitle> In Proceedings of the 13th International Conference on Software Engineering, </booktitle> <pages> pages 288-298. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: Fortunately, a variety of methods for collecting process execution data have been devised. Basili and Weiss describe a method for manual, forms-based collection of data for use in evaluating and comparing software development methods [4]. Amadeus is a system for automated collection and analysis of process metrics <ref> [22] </ref>. Wolf and Rosenblum use a hybrid of manual and automated collection methods [25]. <p> Below, we summarize some of this work. 41 * Chmura et al. [8] and Bhandari et al. [6] try to de-duce problems in the process by looking at defect data in the products. * Selby et al. <ref> [22] </ref> take the approach of providing automated support for empirically guided software development.
Reference: [23] <author> S.M. Sutton, Jr. </author> <title> Accommodating Manual Activities in Automated Process Programs. </title> <booktitle> In Proceedings of the 7th International Software Process Workshop, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: This approach, however, suffers from a fundamental flaw. In particular, it assumes that virtually the entire process is executed within the context of the automated environment. In fact, critical aspects of the process occur off the computer and, therefore, not under the watchful eye of the environment <ref> [23, 25, 26] </ref>. That being the case, there is no effective way to enforce the process using this approach nor to guarantee the mutual consistency of a process model and a process execution.
Reference: [24] <author> S.M. Sutton, Jr., D. Heimbigner, and L.J. Oster-weil. </author> <title> Language Constructs for Managing Change in Process-Centered Environments. </title> <booktitle> In SIGSOFT '90: Proceedings of the Fourth Symposium on Software Development Environments, </booktitle> <pages> pages 206-217. </pages> <booktitle> ACM SIGSOFT, </booktitle> <month> December </month> <year> 1990. </year>
Reference-contexts: Several formal models suitable for our analyses have been used to describe software processes. These in 34 clude models based on state machines (e.g., State-mate [16]), Petri nets (e.g., Slang [3] and FUN-SOFT Nets [15]), and procedural languages (e.g., APPL/A <ref> [24] </ref>). Techniques for process validation clearly depend on an ability to collect data about an executing process, but we do not address this topic here. Fortunately, a variety of methods for collecting process execution data have been devised.
Reference: [25] <author> A.L. Wolf and D.S. Rosenblum. </author> <title> A Study in Software Process Data Capture and Analysis. </title> <booktitle> In Proceedings of the Second International Conference on the Software Process, </booktitle> <pages> pages 115-124. </pages> <publisher> IEEE Computer Society, </publisher> <month> February </month> <year> 1993. </year>
Reference-contexts: This approach, however, suffers from a fundamental flaw. In particular, it assumes that virtually the entire process is executed within the context of the automated environment. In fact, critical aspects of the process occur off the computer and, therefore, not under the watchful eye of the environment <ref> [23, 25, 26] </ref>. That being the case, there is no effective way to enforce the process using this approach nor to guarantee the mutual consistency of a process model and a process execution. <p> Of course, this does not mean that the static aspects of a process are irrelevant to validation. It is simply that the metrics we have chosen to first investigate are those having to do with behavior rather than structure. Following Wolf and Rosenblum <ref> [25] </ref>, we use an event-based model of process actions, where an event is used to characterize the dynamic behavior of a process in terms of identifiable, instantaneous actions, such as invoking a development tool or deciding upon the next activity to be performed. <p> Basili and Weiss describe a method for manual, forms-based collection of data for use in evaluating and comparing software development methods [4]. Amadeus is a system for automated collection and analysis of process metrics [22]. Wolf and Rosenblum use a hybrid of manual and automated collection methods <ref> [25] </ref>. We also do not address the issue of data integrity; we assume that the data are correct (i.e., the events that are collected have actually occurred) and consistent (e.g., all "begin" events for an activity have a corresponding "end" event). <p> model the process as a queuing network, and use actual data about the time spent by the agents in specific tasks and states to determine the real parameters (i.e., service times and probabilities, and branch path probabilities) of the queuing network that can then be analyzed. * Wolf and Rosenblum <ref> [25] </ref> demonstrate how to collect event-based process data and use basic statistical and visual techniques to find interesting relationships among the data in order to uncover possible areas of process improvement.
Reference: [26] <editor> A.L. Wolf and D.S. Rosenblum. </editor> <booktitle> Process-centered Environments (Only) Support Environment-centered Processes. In Proceedings of the 8th International Software Process Workshop, </booktitle> <pages> pages 148-149, </pages> <month> March </month> <year> 1993. </year> <month> 44 </month>
Reference-contexts: This approach, however, suffers from a fundamental flaw. In particular, it assumes that virtually the entire process is executed within the context of the automated environment. In fact, critical aspects of the process occur off the computer and, therefore, not under the watchful eye of the environment <ref> [23, 25, 26] </ref>. That being the case, there is no effective way to enforce the process using this approach nor to guarantee the mutual consistency of a process model and a process execution.
References-found: 26

