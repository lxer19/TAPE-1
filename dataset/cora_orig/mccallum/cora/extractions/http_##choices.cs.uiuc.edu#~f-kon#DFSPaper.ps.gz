URL: http://choices.cs.uiuc.edu/~f-kon/DFSPaper.ps.gz
Refering-URL: http://choices.cs.uiuc.edu/~f-kon/dfs.html
Root-URL: http://www.cs.uiuc.edu
Email: f-kon@cs.uiuc.edu  
Title: Distributed File Systems Past, Present and Future A Distributed File System for 2006  
Author: Fabio Kon 
Date: March 6, 1996  
Abstract-found: 0
Intro-found: 1
Reference: [Bak94] <author> Mary Baker. </author> <title> Fast Crash Recovery in Distributed File Systems. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA 94720, </address> <month> January </month> <year> 1994. </year> <note> Technical Report UCB/CSD 94/787. </note>
Reference-contexts: This can cause problems when a server crashes because it must 4 rebuild its state upon rebooting. Mary Baker has worked hard on this topic and her PhD thesis <ref> [Bak94] </ref> presents some mechanisms that allowed the rebooting time for a SPARCStation 2 with 40 clients to be reduced to just 30 seconds (including the file system verification). Name Space Differently from the previously described systems, the SPRITE name space is the same for all system clients.
Reference: [BHJ + 93] <author> Andrew D. Birrell, Andy Hisgen, Chuck Jerian, Timothy Mann, and Garret Swart. </author> <title> The echo distributed file system. </title> <type> Technical Report #111, </type> <institution> DIGITAL Equipment Corporation Systems Research Center, </institution> <address> Palo Alto, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Experiments on a wide area Engineering company network [GZS94] showed major improvements on lowering the intra-cluster traffic. 3 Indeed, it can be rewritten much later, after a "garbage collection" operation has been performed. 6 2.8 ECHO The ECHO distributed file system <ref> [BHJ + 93] </ref> was a piece of an ambitious project carried since 1988 at the DEC research center in Palo Alto. ECHO joined good qualities of systems like AFS (scalability), SPRITE (UNIX semantics), HARP (fault tolerance) and presented some new features like a coherent distributed file cache with directory write-behind.
Reference: [BHK + 91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <pages> pages 198-212, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: When a user saves a file using a text editor, it can take up to 2 minutes to get to a safe place (the hard disk). The updates to the file may be completely lost if, meanwhile, the client or the server crashes. Exhaustive experiments <ref> [BHK + 91, NWO88] </ref> have shown that SPRITE presented very good performance on local networks outperforming both NFS and AFS. <p> When caching for a particular file is disabled all read and write operations must be done directly with the server and the UNIX semantics is obviously achieved. It is clear that when caching is disabled the performance is strongly affected. However, studies have shown <ref> [BHK + 91] </ref> that concurrent write sharing rarely occurs and therefore the gain of having a file system in which one can trust can compensate the loss in performance. Differently from NFS, SPRITE servers are not stateless, they must keep track of which clients are doing what with its files.
Reference: [DO91] <author> F. Douglis and J. Ousterhout. </author> <title> Transparent process migration: Design alternatives and the Sprite implementation. </title> <journal> Software-Practice & Experience, </journal> <volume> 21(8), </volume> <month> August </month> <year> 1991. </year> <month> 10 </month>
Reference-contexts: It is not only a file system or a collection of network 3 services but something closer to what one can call a Distributed Operating System. SPRITE has a transparently distributed file system and a mechanism for process migration <ref> [DO91] </ref> which can be transparent both to the process and to its user. Since this paper is on distributed file systems, we will only address SPRITE's file system. The two main goals of the SPRITE file system designers were high performance and UNIX semantics on file sharing.
Reference: [Dou93] <author> Fred Douglis. </author> <title> On the role of compression in distributed systems. </title> <journal> ACM Operating Systems Review, </journal> <volume> 27(2) </volume> <pages> 88-93, </pages> <year> 1993. </year>
Reference-contexts: single files. * Aggressive caching: increases system performance by making the cache hit ratio as big as possible. * Log-structure: increase data writing performance and decrease file system verification time. * Automatic compression: by making use of idle CPU cycles decreases the storage space requirements and the data transfer times <ref> [Dou93] </ref>. * Multimedia support: multimedia applications deal with huge amounts of information which can currently get to terabytes of data and transfer rates of hundreds of megabytes per second. UNIX-like file systems are not proper for this kind of task.
Reference: [GC89] <author> Cary G. Gray and David R. Cheriton. Leases: </author> <title> An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 202-210, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The cache consistency is maintained by a very efficient and fault tolerant mechanism called leases <ref> [GC89, KM95] </ref>. This mechanism allows accessing cached files even when they are being write-shared by different clients. Two of the most interesting ECHO features, however, were not completely implemented. The first one is the integration of the local naming service with the Internet domain name service (DNS).
Reference: [GZS94] <author> Deepinder S. Gill, Songnian Zhou, and Harjinder S. Sandhu. </author> <title> A case study of file system workload in a large-scale distributed environment. </title> <type> Technical Report 296, </type> <institution> Computer Science Research Institute University of Toronto, </institution> <year> 1994. </year>
Reference-contexts: Servers, however, can perform inter-cluster communication in order to fulfill client requests. Servers dynamically create local replicas of remote files and the consistency control between replicas on different clusters obeys session semantics. Experiments on a wide area Engineering company network <ref> [GZS94] </ref> showed major improvements on lowering the intra-cluster traffic. 3 Indeed, it can be rewritten much later, after a "garbage collection" operation has been performed. 6 2.8 ECHO The ECHO distributed file system [BHJ + 93] was a piece of an ambitious project carried since 1988 at the DEC research center
Reference: [HKM + 88] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satya-narayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The user level implementation based on RPC/XDR and the client caching policy make NFS's performance worse than most of the systems we describe bellow. Currently however, NFS is undoubtfully the most used distributed file system on networks of workstations. 2.2 ANDREW The ANDREW project <ref> [HKM + 88, Sat90, Tan92] </ref> started at the Carnegie Mellon University in 1983 with IBM's support. Its goal was to design and implement an ideal distributed file system for the academic environment which would allow the sharing of a common directory structure among thousands of client machines.
Reference: [HO92] <author> John H. Hartman and John K. Ousterhout. </author> <title> Zebra: A striped network file system. </title> <booktitle> In Proceedings of the USENIX File Systems Workshop, </booktitle> <pages> pages 71-78, </pages> <address> Ann Arbor, Michi-gan, </address> <month> May </month> <year> 1992. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: One of these weaknesses arises from the fact that prefix tables allow a user to access a directory, say /a/b/c/ without checking for the UNIX permission bits for directories /a/ and /a/b/ if the prefix /a/b/c was previously added to the prefix table. 2.5 ZEBRA The ZEBRA system <ref> [HO92, HO93] </ref> has been developed at University of California at Berkeley since 1990. It joined two powerful ideas originally conceived for local file systems on a new distributed file system concept. These two concepts were Redundant Arrays of Inexpensive Disks (RAIDs) [L + 92] and Log-structured File Systems [RO91].
Reference: [HO93] <author> John H. Hartman and John K. Ousterhout. </author> <title> The zebra striped network file system. </title> <booktitle> In Proceedings of the 14th Symposium on Operating System Principles, </booktitle> <pages> pages 29-43, </pages> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: One of these weaknesses arises from the fact that prefix tables allow a user to access a directory, say /a/b/c/ without checking for the UNIX permission bits for directories /a/ and /a/b/ if the prefix /a/b/c was previously added to the prefix table. 2.5 ZEBRA The ZEBRA system <ref> [HO92, HO93] </ref> has been developed at University of California at Berkeley since 1990. It joined two powerful ideas originally conceived for local file systems on a new distributed file system concept. These two concepts were Redundant Arrays of Inexpensive Disks (RAIDs) [L + 92] and Log-structured File Systems [RO91].
Reference: [KM95] <author> Fabio Kon and Arnaldo Mandel. </author> <title> SODA: A lease-based consistent distributed file system. </title> <editor> In Sociedade Brasileira de Computa~c~ao, editor, </editor> <booktitle> XIII Brazilian Symposium on Computer Networks, </booktitle> <year> 1995. </year> <note> Also Technical Report #9503 (MAC-IME/USP). Available at ftp://ftp.ime.usp.br:/pub/reports/comp/rt-mac-9503.ps.gz. </note>
Reference-contexts: The cache consistency is maintained by a very efficient and fault tolerant mechanism called leases <ref> [GC89, KM95] </ref>. This mechanism allows accessing cached files even when they are being write-shared by different clients. Two of the most interesting ECHO features, however, were not completely implemented. The first one is the integration of the local naming service with the Internet domain name service (DNS).
Reference: [L + 92] <author> Edward K. Lee et al. </author> <title> RAID-II: A scalable storage architecture for high bandwidth network file service. </title> <month> February </month> <year> 1992. </year>
Reference-contexts: It joined two powerful ideas originally conceived for local file systems on a new distributed file system concept. These two concepts were Redundant Arrays of Inexpensive Disks (RAIDs) <ref> [L + 92] </ref> and Log-structured File Systems [RO91]. In the last years, mainly after the RISC technology was introduced, the microprocessor speeds have increased much more than the disk access speeds [Ous90].
Reference: [LGJ + 91] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber Paul Johnson, Liuba Shrira, and Michael Williams. </author> <title> Replication in the harp file system. </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 226-38, </pages> <year> 1991. </year>
Reference-contexts: More details about ZEBRA implementation can be found on the cited papers. 2.6 HARP HARP (Highly Available, Reliable, Persistent file system) <ref> [LGJ + 91] </ref> is an experimental system developed at MIT in the early nineties. It offers a highly fault tolerant file service by adopting a primary-backup-witness pessimistic replication scheme.
Reference: [MBH + 93] <author> Timothy Mann, Andrew Birrell, Andy Hisgen, Charles Jerian, and Garret Swart. </author> <title> A coherent distributed file cache with directory write-behind. </title> <type> Technical Report #103, </type> <institution> DIGITAL Equipment Corporation Systems Research Center, </institution> <address> Palo Alto, CA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: A pessimistic replication algorithm - similar to the one implemented by HARP is used. ECHO was the first system to implement write-behind on all kind of data including directories <ref> [MBH + 93] </ref>.
Reference: [MP91] <author> Keith Muller and Joseph Pasquale. </author> <title> A high performance multi-structured file system design. </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 56-67, </pages> <year> 1991. </year>
Reference-contexts: UNIX-like file systems are not proper for this kind of task. Alternatives like <ref> [MP91] </ref> designed at UCSD must be considered in order to decrease latency and increase transfer speed and file sizes. * Adaptation: by monitoring the workload, the system should try to make future accesses the most efficient as possible. * Protocol flexibility: as done by ECHO's junctions shown in 2.8 the system
Reference: [NWO88] <author> M. Nelson, B. Welch, and J. Ousterhout. </author> <title> Caching in the Sprite network file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: When a user saves a file using a text editor, it can take up to 2 minutes to get to a safe place (the hard disk). The updates to the file may be completely lost if, meanwhile, the client or the server crashes. Exhaustive experiments <ref> [BHK + 91, NWO88] </ref> have shown that SPRITE presented very good performance on local networks outperforming both NFS and AFS.
Reference: [O + 85] <author> J. Ousterhout et al. </author> <title> A trace-driven analysis of the Unix 4.2 BSD file system. </title> <booktitle> In Proceedings of the 10th Symposium on Operating System Principles, </booktitle> <pages> pages 15-24, </pages> <address> Orcas Island, WA, </address> <month> December </month> <year> 1985. </year> <note> ACM. </note>
Reference-contexts: The achieved performance was one of the best of their times, certainly better than NFS and AFS. The UNIX semantics was also achieved in the case where no failures occur. High Performance Previous experiments <ref> [O + 85] </ref> had shown that huge client and server caches could improve the performance considerably. The same experiments had shown that a large amount of file system work is devoted to temporary files whose lifetime is no more than some seconds.
Reference: [OCD + 88] <author> J. Ousterhout, A. Cherenson, F. Douglis, M. Nelson, and B. Welch. </author> <title> The Sprite network operating system. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year> <month> 11 </month>
Reference-contexts: We can see that CODA does not even respect the session semantics. But this is something one must tolerate if one wants to have the extra availability provided by CODA. 2.4 SPRITE The SPRITE Network Operating System <ref> [OCD + 88] </ref> has been developed in the University of Cali-fornia at Berkeley since the middle eighties. It is not only a file system or a collection of network 3 services but something closer to what one can call a Distributed Operating System.
Reference: [Ous90] <author> John K. Ousterhout. </author> <title> Why aren't operating systems getting faster as fast as hardware? In Summer USENIX '90, </title> <address> pages 247-256, Anaheim, CA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: These two concepts were Redundant Arrays of Inexpensive Disks (RAIDs) [L + 92] and Log-structured File Systems [RO91]. In the last years, mainly after the RISC technology was introduced, the microprocessor speeds have increased much more than the disk access speeds <ref> [Ous90] </ref>.
Reference: [PGZ92] <author> James Y.C. Pang, Deepinder S. Gill, and Songnian Zhou. </author> <title> Implementation and performance of cluster-based file replication in large-scale distributed systems. </title> <type> Technical report, </type> <institution> Computer Science Research Institute University of Toronto, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Since any file system operation must be approved by a majority of servers the system performance is obviously worse than all previously described systems. Therefore, such a scheme should only be used if the integrity of the file system data is really essential. 2.7 FROLIC FROLIC <ref> [PGZ92, SZ92] </ref> has been developed in the nineties at the University of Toronto. It is based on the assumption that file sharing on wide are networks may be much larger than what was expected by the AFS team. The system is divided on clusters.
Reference: [RO91] <author> M. Rosenblum and J. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <booktitle> In Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <pages> pages 1-15, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: It joined two powerful ideas originally conceived for local file systems on a new distributed file system concept. These two concepts were Redundant Arrays of Inexpensive Disks (RAIDs) [L + 92] and Log-structured File Systems <ref> [RO91] </ref>. In the last years, mainly after the RISC technology was introduced, the microprocessor speeds have increased much more than the disk access speeds [Ous90]. <p> On log-structured file systems like SPRITE LFS <ref> [RO91] </ref> a large amount of small write operations can be grouped together and sequentially transferred to the disk avoiding unwanted seek times. Once a disk block is written it is not updated anymore 3 .
Reference: [Sat90] <author> Mahadev Satyanarayanan. </author> <title> Scalable, Secure, and Highly Available Distributed File Access. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 9-21, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The user level implementation based on RPC/XDR and the client caching policy make NFS's performance worse than most of the systems we describe bellow. Currently however, NFS is undoubtfully the most used distributed file system on networks of workstations. 2.2 ANDREW The ANDREW project <ref> [HKM + 88, Sat90, Tan92] </ref> started at the Carnegie Mellon University in 1983 with IBM's support. Its goal was to design and implement an ideal distributed file system for the academic environment which would allow the sharing of a common directory structure among thousands of client machines.
Reference: [SBHM93] <author> Garret Swart, Andrew Birrell, Andy Hisgen, and Timothy Mann. </author> <title> Availability in the echo file system. </title> <type> Technical Report #112, </type> <institution> DIGITAL Equipment Corporation Systems Research Center, </institution> <address> Palo Alto, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: ECHO was used by 60 users from January, 91 to middle 92. The fault tolerance scheme <ref> [SBHM93] </ref> is based on the replication of the disks of a single server which increases data integrity and on the "replication of the servers of a single disk" which increases the system availability.
Reference: [SKK + 90] <author> Mahadev Satyanarayanan, James J. Kistler, Puneet Kumar, Maria E. Okasai, Ellen H. Siegel, and David C. Steere. Coda: </author> <title> A highly available file system for a distributed workstation environment. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4) </volume> <pages> 447-59, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Currently there are more than 100 AFS cells all over the world giving its users the possibility of sharing files across different continents using a UNIX-like file system interface. 2.3 CODA The CODA system <ref> [SKK + 90] </ref>, implemented in the early nineties, is a descendent of AFS. Its main goal was to provide access to a distributed file system from portable computers. CODA also implements automatic replication mechanisms not present on AFS.
Reference: [SMB79] <author> Daniel Swinehart, Gene McDaniel, and David Boggs. WFS: </author> <title> A simple shared file system for a distributed envoronment. </title> <booktitle> In Proceedings of the 7th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 9-17, </pages> <month> December </month> <year> 1979. </year>
Reference-contexts: It started running on a PDP-10 in the end of 1973. The Interim File Server (IFS) was created two years later in the XEROX PARC. It was able to organize public and personal files on a hierarchical directory tree. The same research center brought the Woodstock File Server <ref> [SMB79] </ref>. In this system it was possible to access single pages of a file which allowed the use of diskless clients and over the network virtual memory. In the following years, a lot of different systems like XDFS, LOCUS, SWALLOW, ACORN, and CMU's VICE were designed and implemented [Svo84].
Reference: [SNS88] <author> Jennifer G. Steiner, Clifford Neuman, and Jeffrey I. Schiller. </author> <title> Kerberos: An authentication service for open network systems. </title> <note> Available at ftp://athena-dist.mit.edu, file /pub/kerberos/doc/usenix.ftxt,PSg, </note> <month> March </month> <year> 1988. </year>
Reference-contexts: With so many clients and users, it would be impossible to trust on every client and not weaken the security of the whole system. The security problem was solved or, at least, restricted by using the Kerberos protocol <ref> [SNS88] </ref> for mutual authentication between client and servers. The scalability problem was address by taking a large amount of work from the server to the client. When a client opens a file under the ANDREW File System (AFS), the whole file 1 is fetched from the server.
Reference: [SS90] <author> M. Satyanarayanan and Ellen H. Siegel. </author> <title> Parallel communication in a large distributed environment. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39 </volume> <pages> 328-48, </pages> <year> 1990. </year>
Reference-contexts: The coherency among the several copies of a file is maintained using AFS-like callbacks. When a client sends an updated file to a server, the update is forwarded to all AVSG servers using a multiRPC <ref> [SS90] </ref> mechanism. If a server that was down recovers from a crash, nothing is done at this moment to update its files.
Reference: [SUN89] <author> SUN Microsystems, Inc. XDR: </author> <title> External data representation standard. </title> <type> RFC 1014, </type> <institution> Network Information Center, SRI International, </institution> <year> 1989. </year>
Reference-contexts: Upon releasing the first versions of NFS in 1985, SUN made public the NFS protocol specification [SUN94] which allowed the implementation of NFS servers and clients by other vendors. This protocol defines an RPC interface using the external data representation standard XDR <ref> [SUN89] </ref>. Today it is possible to find NFS implementations for almost all existing 1 computers and operating systems including non-UNIX systems like MVS, MacOS, OS/2 and even MS-DOS. NFS servers are, by definition, stateless, i.e., they do not store information about the state of client accesses to its files.
Reference: [SUN94] <author> SUN Microsystems, Inc. NFS: </author> <title> Network file system version 3 protocol specification. </title> <note> Available at ftp://ftp.uu.net/networking/ip/nfs/NFS3.spec.ps.Z, </note> <month> February </month> <year> 1994. </year>
Reference-contexts: Upon releasing the first versions of NFS in 1985, SUN made public the NFS protocol specification <ref> [SUN94] </ref> which allowed the implementation of NFS servers and clients by other vendors. This protocol defines an RPC interface using the external data representation standard XDR [SUN89].
Reference: [Svo84] <author> Liba Svobodova. </author> <title> File servers for network-based distributed systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 16(4) </volume> <pages> 353-398, </pages> <month> December </month> <year> 1984. </year>
Reference-contexts: Nowadays there are tens of different implementations of this kind of system. In the very beginning of the Distributed Systems history one can find the Datacomputer which used to support an FTP-like service for clients which did not have a large amount of local storage <ref> [Svo84] </ref>. It started running on a PDP-10 in the end of 1973. The Interim File Server (IFS) was created two years later in the XEROX PARC. It was able to organize public and personal files on a hierarchical directory tree. The same research center brought the Woodstock File Server [SMB79]. <p> In this system it was possible to access single pages of a file which allowed the use of diskless clients and over the network virtual memory. In the following years, a lot of different systems like XDFS, LOCUS, SWALLOW, ACORN, and CMU's VICE were designed and implemented <ref> [Svo84] </ref>. After that, it was the time for the creation of the currently most used distributed file systems like Sun's NFS and CMU's AFS.
Reference: [SZ92] <author> Harjinder S. Sandhu and Songnian Zhou. </author> <title> Cluster-based file replication in large-scale distributed systems. </title> <journal> ACM SIGMETRICS Performance Evaluation Review, </journal> <volume> 20(1) </volume> <pages> 91-102, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Since any file system operation must be approved by a majority of servers the system performance is obviously worse than all previously described systems. Therefore, such a scheme should only be used if the integrity of the file system data is really essential. 2.7 FROLIC FROLIC <ref> [PGZ92, SZ92] </ref> has been developed in the nineties at the University of Toronto. It is based on the assumption that file sharing on wide are networks may be much larger than what was expected by the AFS team. The system is divided on clusters.
Reference: [Tan92] <author> Andrew S. Tanenbaum. </author> <title> Modern Operating Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: The user level implementation based on RPC/XDR and the client caching policy make NFS's performance worse than most of the systems we describe bellow. Currently however, NFS is undoubtfully the most used distributed file system on networks of workstations. 2.2 ANDREW The ANDREW project <ref> [HKM + 88, Sat90, Tan92] </ref> started at the Carnegie Mellon University in 1983 with IBM's support. Its goal was to design and implement an ideal distributed file system for the academic environment which would allow the sharing of a common directory structure among thousands of client machines.
Reference: [WO86] <author> B. B. Welch and J. K. Ousterhout. </author> <title> Prefix tables: A simple mechanism for locating files in a distributed filesystem. </title> <booktitle> In Proc. of the 6th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 184-189, </pages> <address> Boston, Mass., </address> <month> May </month> <year> 1986. </year> <journal> IEEE. </journal> <volume> 12 </volume>
Reference-contexts: Name Space Differently from the previously described systems, the SPRITE name space is the same for all system clients. The pathname resolution scheme is based on prefix tables, a mechanism presented in <ref> [WO86] </ref>. When a client needs to resolve a pathname, it looks for the largest prefix of this pathname on a local prefix table. There, it can find the server which handles the requested file or directory.
References-found: 33

