URL: http://www.cs.uchicago.edu/~swain/pubs/TR-96-14.ps.Z
Refering-URL: http://infolab.cs.uchicago.edu/webseer/
Root-URL: 
Title: WebSeer: An Image Search Engine for the World Wide Web 1  
Author: Charles Frankel, Michael J. Swain, and Vassilis Athitsos 
Date: August 1, 1996  
Address: 1100 East 58th Street Chicago, Illinois 60637  
Affiliation: The University of Chicago Computer Science Department  
Pubnum: Technical Report 96-14  
Abstract: Because of the size of the World Wide Web and its inherent lack of structure, finding what one is looking for can be a challenge. PC-Meters March, 1996, survey found that three of the five most visited Web sites were search engines. However, while Web pages typically contain both text and images, all the currently available search engines only index text. This paper describes WebSeer, a system for locating images on the Web. WebSeer uses image content in addition to associated text to index images, presenting the user with a selection that potentially fits her needs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ogle and Stonebraker (1995). Chabot: </author> <title> Retrieval from a Relational Database of Images. </title> <journal> IEEE Computer, </journal> <volume> 28(2), </volume> <pages> 49-56. </pages>
Reference: [2] <author> Nancy A. Van House, Mark H. Butler, Virginia Ogle, and Lisa Schiff. </author> <title> User-Centered Iterative Design for Digital Libraries: </title> <journal> The Cypress Experience. D-lib Magazine, </journal> <month> Feb </month> <year> 1996, </year> <pages> ISSN 1082-9873. </pages>
Reference: [3] <author> Srihari, Rohini K. </author> <year> (1995). </year> <title> Automatic Indexing and Content-Based Retrieval of Captioned Images. </title> <journal> IEEE Computer, </journal> <volume> 28(2), </volume> <pages> 49-56. </pages>
Reference-contexts: One approach has been to supplement the image content with other types of information associated with the image. Ogle and Stonebrakers Cypress system [1][2] uses information contained in hand-keyed database fields to supplement image content information. Sriharis Piction system <ref> [3] </ref> uses the captions of newspaper photographs containing human faces to help locate the faces. IBMs QBIC system [4] relies on the user specifying specific visual cues or providing an example image (e.g. a sketch) to begin a query for an image.
Reference: [4] <author> Flickner, Myron, et. al. </author> <year> (1995). </year> <title> Query by Image and Video Content: The QBIC System. </title> <journal> IEEE Computer. September, </journal> <volume> 1995 Volume 28, pgs. </volume> <pages> 23-32. </pages>
Reference-contexts: Ogle and Stonebrakers Cypress system [1][2] uses information contained in hand-keyed database fields to supplement image content information. Sriharis Piction system [3] uses the captions of newspaper photographs containing human faces to help locate the faces. IBMs QBIC system <ref> [4] </ref> relies on the user specifying specific visual cues or providing an example image (e.g. a sketch) to begin a query for an image. In Picard and Minkas Foureyes system [5][6] close interaction with a human user supplements information derived from the image content.
Reference: [5] <author> R.W. </author> <title> Picard (1996). A Society of Models for Video and Image Libraries. Media Laboratory Perceptual Computing Section Technical Report No. </title> <type> 360. </type>
Reference: [6] <author> T.P. Minka and R.W. </author> <title> Picard (1995). Interactive Learning using a society of models. MIT Media Laboratory Perceptual Computing Section Technical Report No. </title> <type> 349. </type>
Reference: [7] <author> Kah-Kay Sung and Tomaso Poggio (1994). </author> <title> Example-based learning for view-based human face detection. </title> <type> Technical Report A.I. Memo 1521, CBCL Paper 112, </type> <institution> MIT, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The color transitions are very abrupt, and the image is unusually narrow. Locating Faces We are testing face finders written by Sung and Poggio <ref> [7] </ref> and Rowley, et. al. [8]. Both of these approaches look for upright faces facing the camera. The results shown for Rebecca De Mornay were obtained using Sung and Poggios code.
Reference: [8] <author> Henry A. Rowley, Shumeet Baluja, </author> <title> and Takeo Kanade (1995). Human Face Detection in Visual Scenes. </title> <institution> Carnegie Mellon University Technical Report CMU-CS-95-158. </institution>
Reference-contexts: The color transitions are very abrupt, and the image is unusually narrow. Locating Faces We are testing face finders written by Sung and Poggio [7] and Rowley, et. al. <ref> [8] </ref>. Both of these approaches look for upright faces facing the camera. The results shown for Rebecca De Mornay were obtained using Sung and Poggios code. <p> We have also experienced problems with false positives using Sung and Poggios code with its default settings. 18 Rowley et. al.s code is considerably more efficient, as has been reported in <ref> [8] </ref>, and so is more suitable for our application. Published results indicate that the effectiveness is about the same as Sung and Poggios on grayscale images.
Reference: [9] <author> Markus Stricker and Alexander Dimai (1996). </author> <title> Color Indexing with Weak Spatial Constraints. </title> <booktitle> SPIE Conference, February 1996, </booktitle> <address> San Jose, CA. </address>
Reference-contexts: Portraits can be close-ups, head-and-shoulders shots, upper-body, or full-body shots; the size and location of the face can differentiate among these choices. 6 2. Horizon: A number of systems search images for the presence of a horizon <ref> [9] </ref> (e.g. the Cypress System). Detecting a horizon in a photograph tells us that the image has been taken outdoors and allows us to roughly classify it as a landscape photograph. Face recognition is another possible task one might think of assigning the system. <p> We are working on identifying a taxonomy that fits users needs and is constructed of image classes that can be reliably identified. Some of these categories may include advertisements, geographic maps, landscapes <ref> [9] </ref>, city/country scenes [5][10], night scenes, sunsets, scenes with foliage, and so on. Tomasi and Guibis descibe a system which classifies types of images based on image content alone [11].
Reference: [10] <author> R.W. </author> <title> Picard and T.P. Minka (1995). Vision texture for annotation. </title> <journal> Journal of Multimedia Systems, </journal> <volume> vol. 3, </volume> <pages> pp 3-14. </pages>
Reference: [11] <author> C. Tomasi and L. </author> <title> Guibas (1994). Image Descriptions for Browsing and Retrieval. </title> <booktitle> Proceedings of the ARPA Image Understanding Workshop, </booktitle> <month> November </month> <year> 1994, </year> <pages> pp. 165-168. </pages>
Reference-contexts: Some of these categories may include advertisements, geographic maps, landscapes [9], city/country scenes [5][10], night scenes, sunsets, scenes with foliage, and so on. Tomasi and Guibis descibe a system which classifies types of images based on image content alone <ref> [11] </ref>. We believe that we need a close interaction between the image understanding algorithms and the associated text indexing algorithms in order to successfully categorize images. Sriharis theory of visual semantics [12] provides useful insight into some of the challenges of integrating text indexing with image understanding algorithms.
Reference: [12] <author> Srihari, Rohini K. </author> <year> (1995). </year> <title> Use of Multimedia Input in Automated Image Annotation and Content-Based Retrieval. Presented at Conference on Storage and Retrieval Techniques for Image Databases, </title> <booktitle> SPIE '95, </booktitle> <address> San Jose, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Tomasi and Guibis descibe a system which classifies types of images based on image content alone [11]. We believe that we need a close interaction between the image understanding algorithms and the associated text indexing algorithms in order to successfully categorize images. Sriharis theory of visual semantics <ref> [12] </ref> provides useful insight into some of the challenges of integrating text indexing with image understanding algorithms. A closer interaction between text and image processing will also provide significant improvements in indexing speed.
Reference: [13] <author> Beecher, </author> <type> Ben (1996) Personal Communication. </type> <institution> AcIS R&D Columbia University. </institution>
Reference-contexts: Note the entries containing /acis/gif/, /cu/libraries/events/sw25/gifs/, /cu/libraries/gifs/, and /gif/. The author of this file has informed us that these restrictions were included since he believes that current robots are only interested in indexing textual information, and these directories contain no text <ref> [13] </ref>. http://www.columbia.edu/robots.txt # this file is read by any robot that conforms to the WWW robot # guidelines described here: # http://www.nexor.co.uk/mak/doc/robots/norobots.html User-agent: * Disallow: /acis/eds/triarc Disallow: /acis/gif/ Disallow: /acis/poetry/ Disallow: /cgi-bin/ Disallow: /cp/ Disallow: /cu/libraries/events/sw25/gifs/ Disallow: /cu/libraries/gifs/ Disallow: /cu/libraries/inside/ Disallow: /cu/spectator/ Disallow: /cu/distrib/ Disallow: /experimental/ Disallow: /gif/ Disallow: /httpd/reports/ Disallow:
References-found: 13

