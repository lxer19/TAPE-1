URL: ftp://ftp.ai.mit.edu/pub/users/oded/papers/piecewise-long.ps.Z
Refering-URL: http://www.ai.mit.edu/people/oded/
Root-URL: 
Title: Using Errors to Create Piecewise Learnable Partitions  
Author: Oded Maron 
Keyword: decision trees, instance-based learning  
Web: oded ai.mit.edu  
Address: 545 Technology Sq., Room #755 Cambridge, MA 02139  
Affiliation: Artificial Intelligence Laboratory Massachusetts Institute of Technology  
Abstract: In this paper we describe an algorithm which exploits the error distribution generated by a learning algorithm in order to break up the domain which is being approximated into piecewise learnable partitions. Traditionally, the error distribution has been neglected in favor of a lump error measure such as RMS. By doing this, however, we lose a lot of important information. The error distribution tells us where the algorithm is doing badly, and if there exists a "ridge" of errors, also tells us how to partition the space so that one part of the space will not interfere with the learning of another. The algorithm builds a variable arity k-d tree whose leaves contain the partitions. Using this tree, new points can be predicted using the correct partition by traversing the tree. We instantiate this algorithm using memory based learners and cross-validation. 
Abstract-found: 1
Intro-found: 1
Reference: [ Aha, 1990 ] <author> D. W. Aha. </author> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks: Mathematical, Empirical and Psychological Evaluations. </title> <type> PhD. Thesis; Technical Report No. 90-42, </type> <institution> University of California, Irvine, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: In fact, one technique of lowering the space requirements for memory-based learning algorithms is to just keep the instances with high error, and therefore just remember where the boundaries are <ref> [ Aha, 1990 ] </ref> . The idea of creating a tree of learners is similiar to [ Jordan and Jacobs, 1992 ] where a tree of experts is constructed. The partitioning idea is an integral part of any decision tree algorithms (such as CART or MARS).
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: While techniques from vision research exist for d = 2; 3, it appears that the problem is difficult for arbitrary dimensions. This assumption exists for most popular partitioning algorithms such as CART <ref> [ Breiman et al., 1984 ] </ref> or MARS [ Friedman, 1988 ] . The approach described here was brought about by looking at functions which have some 3 continuous attributes and some discrete attributes.
Reference: [ Friedman, 1988 ] <author> Jerome H. Friedman. </author> <title> Multivariate Adaptive Regression Splines. </title> <type> Technical Report No. 102, </type> <institution> Stanford University, Department of Statistics, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: While techniques from vision research exist for d = 2; 3, it appears that the problem is difficult for arbitrary dimensions. This assumption exists for most popular partitioning algorithms such as CART [ Breiman et al., 1984 ] or MARS <ref> [ Friedman, 1988 ] </ref> . The approach described here was brought about by looking at functions which have some 3 continuous attributes and some discrete attributes. In most of those functions, the output changes drastically when one of the discrete attributes changes value.
Reference: [ Jordan and Jacobs, 1992 ] <author> Michael I. Jordan and Robert A. Jacobs. </author> <title> Hierarchies of adaptive experts. </title> <editor> In J. Moody, S. Hanson, and R. Lippman, editors, </editor> <booktitle> Neural Information Systems 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: In fact, one technique of lowering the space requirements for memory-based learning algorithms is to just keep the instances with high error, and therefore just remember where the boundaries are [ Aha, 1990 ] . The idea of creating a tree of learners is similiar to <ref> [ Jordan and Jacobs, 1992 ] </ref> where a tree of experts is constructed. The partitioning idea is an integral part of any decision tree algorithms (such as CART or MARS). Like this algorithm, CART can only create partitions which are axis-parallel. <p> The second difference is that this algorithm breaks the space into learnable partitions, which is much more general than trying to break it into constant or linear sections [ Quinlan, 1992 ] . <ref> [ Jordan and Jacobs, 1992 ] </ref> The algorithm presented in this paper is meant to take a learning algorithm with some training data and see where it performs badly.
Reference: [ Maron, 1994 ] <author> Oded Maron. </author> <title> Hoeffding Races: Accelerating model selection search for clas-sification and function approximation. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: It can be as cheap as just using the original learning box at each node, or as expensive as doing a full search for the best fit for each partition. For some examples of memory based model selection see <ref> [ Maron, 1994 ] </ref> or [ Moore et al., 1992 ] . 10 5.2 Finding H eight and Length Unfortunately, it doesn't seem that we can rigorously determine an appropriate hill size for arbitrary learning algorithms with arbitrary training distributions.
Reference: [ Moore et al., 1992 ] <author> A. W. Moore, D. J. Hill, and M. P. Johnson. </author> <title> An empirical investigation of brute force to choose features, smoothers and function approximators. </title> <editor> In S. Hanson, S. Judd, and T. Petsche, editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Volume 3. </volume> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: It can be as cheap as just using the original learning box at each node, or as expensive as doing a full search for the best fit for each partition. For some examples of memory based model selection see [ Maron, 1994 ] or <ref> [ Moore et al., 1992 ] </ref> . 10 5.2 Finding H eight and Length Unfortunately, it doesn't seem that we can rigorously determine an appropriate hill size for arbitrary learning algorithms with arbitrary training distributions.
Reference: [ Quinlan, 1992 ] <author> J. Ross Quinlan. C4.5: </author> <title> programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: The second difference is that this algorithm breaks the space into learnable partitions, which is much more general than trying to break it into constant or linear sections <ref> [ Quinlan, 1992 ] </ref> . [ Jordan and Jacobs, 1992 ] The algorithm presented in this paper is meant to take a learning algorithm with some training data and see where it performs badly.
Reference: [ Schaffer, 1993 ] <author> Cullen Schaffer. </author> <title> Overfitting avoidance as bias. </title> <booktitle> Machine Learning, </booktitle> <address> 10:153178, </address> <year> 1993. </year>
Reference-contexts: We immediately choose to break at the place which alligns with a hill of errors. Neither one of these approaches guarantees that the choice is an optimal cut, despite the fact that they are both intuitive approaches (see <ref> [ Schaffer, 1993 ] </ref> and [ Wolpert, 1993 ] for criticisms of the CART approach).
Reference: [ Stanfill and Waltz, 1986 ] <author> C. Stanfill and D. Waltz. </author> <title> Towards memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: It seems unlikely that a proof of this claim is tractable unless some simplifying assumptions are made about the learning box (e.g. it uses polynomials of degree m to approximate the function). Finally, the ideas here have been tested using the effective combination of memory-based learning boxes <ref> [ Stanfill and Waltz, 1986 ] </ref> and leave-one-out cross validation [ Wahba and Wold, 1975 ] . Memory-based methods are highly efficient with partitioning algorithms since once we have partitioned the training set, we do not need to retrain.
Reference: [ Wahba and Wold, 1975 ] <author> G. Wahba and S. </author> <title> Wold. A completely automatic french curve: Fitting spline functions by cross-validation. </title> <journal> Communications in Statistics, </journal> <volume> 4(1), </volume> <year> 1975. </year>
Reference-contexts: Finally, the ideas here have been tested using the effective combination of memory-based learning boxes [ Stanfill and Waltz, 1986 ] and leave-one-out cross validation <ref> [ Wahba and Wold, 1975 ] </ref> . Memory-based methods are highly efficient with partitioning algorithms since once we have partitioned the training set, we do not need to retrain. The only work occurs during prediction, and training simply involves dumping all the points into memory where they already are.
Reference: [ Wolpert, 1993 ] <author> David H. Wolpert. </author> <title> On overfitting avoidance as bias. </title> <type> Technical Report No. </type> <institution> 92-03-5001, The Santa Fe Institute, </institution> <year> 1993. </year> <month> 12 </month>
Reference-contexts: We immediately choose to break at the place which alligns with a hill of errors. Neither one of these approaches guarantees that the choice is an optimal cut, despite the fact that they are both intuitive approaches (see [ Schaffer, 1993 ] and <ref> [ Wolpert, 1993 ] </ref> for criticisms of the CART approach).
References-found: 11

