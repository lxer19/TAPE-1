URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-316.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: 
Email: EMAIL gopinath@cs.rutgers.edu  EMAIL phalke@paul.rutgers.edu  
Phone: TEL (908) 932 5394  TEL (908) 932-4635  
Title: Using Spatial Locality for Trace Compression  
Author: B. Gopinath Vidyadhar Phalke 
Keyword: Key Words Trace Compression, Locality of Reference, Cache Memory.  
Address: New Brunswick, NJ 08903.  New Brunswick, NJ 08903.  
Affiliation: Dept. of Electrical Engineering Rutgers University  Dept. of Computer Science Rutgers University  
Abstract: Performance of most cache memories, virtual paging systems, TLB's, and disk caches are analyzed using trace-driven simulations. These require large amounts of storage for the traces. In this paper we present a paging based trace compression mechanism which is loss less and improves upon the mache method of Samples [5], up to a factor of two. The key idea is to split up a trace of main memory references into two levels. The top level is the page reference stream and the lower is the string of offset references for each of the pages. Then we compress the two levels separately and obtain the final compaction. In addition, unlike the monolithic compression of mache, this method provides random access to individual page traces. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal and Minor Huffman. </author> <title> Blocking: Exploiting spatial locality for trace compaction. </title> <booktitle> In Proceedings of ACM SIGMETRICS 1990 Conference on Measurement & Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: In the lossy method data loss is tolerated as long as the features needed for a particular application are saved in some approximate fashion, e.g. as done by Agarwal and Huffman's blocking method <ref> [1] </ref>- all but one references to each of the tight (small period) loops within a trace are removed. <p> This method does not introduce errors in simulations with caches containing more sets than those in the filter. Finally, Agarwal and Huffman <ref> [1] </ref> proposed a method called blocking, where first they apply Puzak's cache filter, followed by a block filter which removes spatially nearby references by doing a div operation and removing low order bits from the address.
Reference: [2] <author> J.K. Flanagan, B. Nelson, J. Archibald, and K. Grimsrud. BACH: </author> <title> BYU address collection hardware; the collection of complete traces. </title> <booktitle> In International Workshop on Modeling Techniques and Tools for Computer Performance Evaluation, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: Each is a half-second snapshot execution on a machine the speed of a VAX-11/780. KenSparc is a four million references long kenbus1 trace from the SPEC benchmark suite. The number of simulated users is 20. It was collected on a SPARC 2 1+ using the BACH <ref> [2] </ref> system. Table 1 gives more details about these traces. All traces have 32 bit wide addresses, and labels of the three types mentioned earlier. Table 1 shows the compression results for the UNIX compress and the mache method in bytes.
Reference: [3] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture : A Quantitative Approach. </title> <publisher> Morgan Kaufman, </publisher> <year> 1990. </year>
Reference-contexts: The relative improvement over mache ranged from 9 to 57%. Improvement was measured as (C m C pm )/C m ; C m , C pm being the compression ratios of mache and page-mache respectively. The cc1 trace is a Gnu C compilation process from the dinero suite <ref> [3] </ref>. The spice trace, a circuit simulation, is also from the dinero suite. Fsxzz, ivex, and lisp, each have approximately quarter of a million references and were obtained by the ATUM process [7]. Each is a half-second snapshot execution on a machine the speed of a VAX-11/780.
Reference: [4] <author> Thomas R. Puzak. </author> <title> Analysis of Cache Replacement Algorithms. </title> <type> PhD thesis, </type> <institution> University of Massachusetts Department of Electrical and Computer Engineering, </institution> <month> February </month> <year> 1985. </year>
Reference-contexts: The first one removed the most frequent hits in a cache, assuming all caching algorithms perform equally well for the highly referred addresses. The second method took samples of a trace at regular intervals with the underlying assumption that locality does not change very rapidly. Puzak <ref> [4] </ref> proposed a method called trace stripping in which a direct-mapped cache (called a cache filter) with a fixed block size is simulated, only the misses are stored in the final compaction. This method does not introduce errors in simulations with caches containing more sets than those in the filter.
Reference: [5] <author> A. Dain Samples. Mache: </author> <title> No-loss trace compaction. </title> <booktitle> In Proceedings of ACM SIGMETRICS 1989 Conference on Measurement & Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: It only requires O (1) preprocessing, the rest of the complexity being that of the UNIX 1 compress program this would be approximately of the same order as compress on the original trace. Section 2 describes some related work and the mache method of Samples <ref> [5] </ref>, which we improve upon. Section 3 describes our method. Section 4 gives the results and examines why the technique works so well. <p> Their method can produce trace size reductions of one to two orders of magnitude, and introduces simulation errors of the order of 10%. The simplest starting points for loss less trace compaction are the standard Ziv Lempel [8][9] based methods like the UNIX compress and gzip schemes. Samples <ref> [5] </ref> proposed a method called mache which improves upon UNIX compress by a factor of at least three. a 1 UNIX is a trademark of AT&T Bell Laboratories. 2 His basic idea (depicted pictorially above) is to use spatial locality among consecutive addresses of the same label in the trace. <p> Similar arguments would hold even if the input to the compress program is generated by taking successive differences (as done in mache described before). 3. Cache-differencing : The technique used in mache <ref> [5] </ref> works because for the same value of the label field, the addresses referred to are spatially near by (within ffi) and change slowly over time for most part of the trace. So the stream of address differences is more regular than the original stream of actual addresses.
Reference: [6] <author> Alan Jay Smith. </author> <title> Two methods for the efficient analysis of memory address trace data. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(1), </volume> <month> January </month> <year> 1977. </year>
Reference-contexts: Section 4 gives the results and examines why the technique works so well. Finally, section 5 has conclusions and directions for future work. 2 Related Work Main objective of the lossy compaction methods has been to reduce cache algorithm simulation time. Among them, two methods were proposed by Smith <ref> [6] </ref>. The first one removed the most frequent hits in a cache, assuming all caching algorithms perform equally well for the highly referred addresses. The second method took samples of a trace at regular intervals with the underlying assumption that locality does not change very rapidly.
Reference: [7] <author> Alan Jay Smith. </author> <title> Cache memories. </title> <journal> Computing Surveys, </journal> <volume> 3(14), </volume> <month> September </month> <year> 1982. </year>
Reference-contexts: The cc1 trace is a Gnu C compilation process from the dinero suite [3]. The spice trace, a circuit simulation, is also from the dinero suite. Fsxzz, ivex, and lisp, each have approximately quarter of a million references and were obtained by the ATUM process <ref> [7] </ref>. Each is a half-second snapshot execution on a machine the speed of a VAX-11/780. KenSparc is a four million references long kenbus1 trace from the SPEC benchmark suite. The number of simulated users is 20. It was collected on a SPARC 2 1+ using the BACH [2] system.
Reference: [8] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-23(3), </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: a a a aaaaa a a a a a a a a aaaaa a lisp a a a a a a Imp = 36% a aaaaaaa Instead of the compress program as the backend for page-mache, we tried the gzip UNIX utility which is based on the LZ77 Ziv-Lempel algorithm <ref> [8] </ref>. We used page size of 4096 and threshold of 32 for all traces. Table 3 describes these results.
Reference: [9] <author> J. Ziv and A. Lempel. </author> <title> Compression of individual sequences via variable-rate coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-24(5), </volume> <month> September </month> <year> 1978. </year> <month> 10 </month>
Reference-contexts: Identical properties are also seen in other traces used for the experiments presented in this paper. 2. Ziv Lempel coding : The UNIX compress program uses a variation of the Ziv-Lempel <ref> [9] </ref> algorithm. This method maintains a dynamic dictionary of codes (substrings of the source). So at each step of compression, the longest prefix of the remaining string, which matches a code, is found.
References-found: 9

