URL: http://www.cs.cornell.edu/Info/People/weichen/research/FDpapers/TR98-1676.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/weichen/research/FDpapers.html
Root-URL: http://www.cs.cornell.edu
Email: aguilera,weichen,sam@cs.cornell.edu  
Title: Failure Detection and Consensus in the Crash-Recovery Model  
Author: Marcos Kawazoe Aguilera Wei Chen Sam Toueg 
Date: April 27, 1998  
Address: Upson Hall,  Ithaca, NY 14853-7501, USA.  
Affiliation: Department of Computer Science  Cornell University  
Abstract: We study the problems of failure detection and consensus in asynchronous systems in which processes may crash and recover, and links may lose messages. We first propose new failure detectors that are particularly suitable to the crash-recovery model. We next determine under what conditions stable storage is necessary to solve consensus in this model. Using the new failure detectors, we give two consensus algorithms that match these conditions: one requires stable storage and the other does not. Both algorithms tolerate link failures and are particularly efficient in the runs that are most likely in practice those with no failures or failure detector mistakes. In such runs, consensus is achieved within 3ffi time and with 4n messages, where ffi is the maximum message delay and n is the number of processes in the system.
Abstract-found: 1
Intro-found: 1
Reference: [ACT97] <author> Marcos Kawazoe Aguilera, Wei Chen, and Sam Toueg. Heartbeat: </author> <title> a timeout-free failure detector for quiescent reliable communication. </title> <booktitle> In Proceedings of the 11th International Workshop on Distributed Algorithms, Lecture Notes on Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1997. </year> <note> A full version is also available as Technical Report 97-1631, </note> <institution> Computer Science Department, Cornell University, </institution> <address> Ithaca, New York, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: In contrast, in nice executions the consensus algorithms of [HMR97, OGS97] reach decision within 2ffi and with O (n 2 ) messages. 8.2 Quiescence An algorithm is quiescent if eventually all processes stop sending messages <ref> [ACT97] </ref>. It is clear that no consensus algorithm can be quiescent in the presence of unstable processes (each time such a process recovers, it must be sent the decision value, at which point it may crash again and lose this message; this scenario can be repeated infinitely often).
Reference: [CHT96] <author> Tushar Deepak Chandra, Vassos Hadzilacos, and Sam Toueg. </author> <title> The weakest failure detector for solving consensus. </title> <journal> Journal of the ACM, </journal> <volume> 43(4):685722, </volume> <month> July </month> <year> 1996. </year>
Reference-contexts: 1 Introduction The problem of solving consensus in asynchronous systems with unreliable failure detectors (i.e., failure detectors that make mistakes) was first investigated in <ref> [CT96, CHT96] </ref>. But these works only considered systems where process crashes are permanent and links are reliable (i.e., they do not lose messages). In real systems, however, processes may recover after crashing and links may lose messages.
Reference: [CT96] <author> Tushar Deepak Chandra and Sam Toueg. </author> <title> Unreliable failure detectors for reliable distributed systems. </title> <journal> Journal of the ACM, </journal> <volume> 43(2):225267, </volume> <month> March </month> <year> 1996. </year> <month> 27 </month>
Reference-contexts: 1 Introduction The problem of solving consensus in asynchronous systems with unreliable failure detectors (i.e., failure detectors that make mistakes) was first investigated in <ref> [CT96, CHT96] </ref>. But these works only considered systems where process crashes are permanent and links are reliable (i.e., they do not lose messages). In real systems, however, processes may recover after crashing and links may lose messages. <p> For example, in the rotating coordinator consensus algorithms of <ref> [CT96, DFKM96, HMR97] </ref> if a process kept suspecting all processes then consensus would never be reached. <p> This essentially reduces the problem to the case where process crashes are permanent and a majority of processes do not crash (and then an algorithm such as the one in <ref> [CT96] </ref> can be used). Is it possible to solve consensus without stable storage if 1 n a n=2? To answer this question, assume that in every execution of consensus at most n b processes are bad. <p> Prima facie, this seems to contradict the fact that if a majority of processes may crash then consensus cannot be solved even with 3P <ref> [CT96] </ref>. There is no contradiction, however, since [CT96] assumes that all process crashes are permanent, while in our case some of the processes that crash do recover: even though they completely lost their state, they can still provide some help. 4 In Appendix B, we consider the partial synchrony models in <p> Prima facie, this seems to contradict the fact that if a majority of processes may crash then consensus cannot be solved even with 3P <ref> [CT96] </ref>. There is no contradiction, however, since [CT96] assumes that all process crashes are permanent, while in our case some of the processes that crash do recover: even though they completely lost their state, they can still provide some help. 4 In Appendix B, we consider the partial synchrony models in [DLS88, CT96]. <p> There is no contradiction, however, since [CT96] assumes that all process crashes are permanent, while in our case some of the processes that crash do recover: even though they completely lost their state, they can still provide some help. 4 In Appendix B, we consider the partial synchrony models in <ref> [DLS88, CT96] </ref>. <p> We define the Consensus problem in Section 4. In Section 5, we determine under what conditions consensus requires stable storage. We then give two matching consensus algorithms: one does 6 If the good processes are not a majority, a simple partitioning argument as the one in <ref> [CT96] </ref> shows that consensus cannot be solved even with 3P. 4 not require stable storage (Section 6), and the other uses stable storage (Section 7). In Section 8, we briefly consider the performance of these algorithms. The issue of repeated consensus is discussed in Section 9. <p> Although many algorithms are designed to tolerate such failure detector mistakes, the erroneous suspicions of some good processes may hurt the performance of these algorithms. For example, the erroneous suspicions of good coordinators can delay the termination of the consensus algorithms in <ref> [CT96, DFKM96, HMR97, OGS97] </ref>. Thus, requiring Strong Completeness should be avoided if possible. <p> This algorithm, shown in Fig. 1, is based on the rotating coordinator paradigm <ref> [CT96] </ref> and uses 3S 0 e . It must deal with unstable processes and link failures. <p> It requires a majority of good processes and works in systems with lossy links. The basic structure of the algorithm (given in Fig. 3) is as in <ref> [CT96, DFKM96] </ref> and consists of rounds of 4 phases each (task 4phases). <p> Theorem 23 If a majority of processes are good, then the algorithm in Fig. 4 transforms 3S e to 3S u . 17 As explained in <ref> [CT96] </ref>, a transformation algorithm T D!D 0 uses failure detector D to maintain at each process p a variable D 0 p that emulates the output of D 0 at p. 25 We now proceed to prove this theorem. Assume that a majority of processes are good.
Reference: [DFKM96] <author> Danny Dolev, Roy Friedman, Idit Keidar, and Dahlia Malkhi. </author> <title> Failure detectors in omission failure environments. </title> <type> Technical Report 96-1608, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: In real systems, however, processes may recover after crashing and links may lose messages. In this paper, we focus on solving consensus with failure detectors in such systems, a problem that was first considered in <ref> [DFKM96, OGS97, HMR97] </ref> (a brief comparison with these works is in Section 1.3). <p> Note that the completeness property of 3S e does not require predicting the future (to determine if a process is unstable), and so it does not force implementations to have anomalous behaviors. To illustrate this, in 1 In fact, this property is assumed in [OGS97, HMR97]. 2 In <ref> [DFKM96] </ref>, crash-recovery is regarded as a special case of omission failures, and the algorithm is not designed to handle unstable processes that can send and receive messages to and from good processes. 3 In such a system, processes execute in synchronized rounds, and all messages are received in the round they <p> For example, in the rotating coordinator consensus algorithms of <ref> [CT96, DFKM96, HMR97] </ref> if a process kept suspecting all processes then consensus would never be reached. <p> losses, provided that links are fair lossy, i.e., if p sends messages to a good process q infinitely often, then q receives messages from p infinitely often. 1.3 Related Work The problem of solving consensus with failure detectors in systems where processes may recover from crashes was first addressed in <ref> [DFKM96] </ref> (with crash-recovery as a form of omission failures) and more recently studied in [OGS97, HMR97]. <p> In <ref> [DFKM96, HMR97, OGS97] </ref>, the question of whether stable storage is always necessary is not addressed, and all the algorithms use stable storage: in [DFKM96, OGS97], the entire state of the algorithm is recorded into stable storage at every state transition; in [HMR97], only a small part of the state is recorded, <p> In [DFKM96, HMR97, OGS97], the question of whether stable storage is always necessary is not addressed, and all the algorithms use stable storage: in <ref> [DFKM96, OGS97] </ref>, the entire state of the algorithm is recorded into stable storage at every state transition; in [HMR97], only a small part of the state is recorded, and writing to stable storage is done at most once per round. <p> In the one that uses stable storage, only a small part of the state is recorded and this occurs twice per round. The algorithms in [OGS97, HMR97] use failure detectors that require that unstable processes be eventually suspected forever. The algorithm in <ref> [DFKM96] </ref> is not designed to deal with unstable processes which may intermittently communicate with good ones. 1.4 Summary of Contributions We study the problems of failure detection and consensus in asynchronous systems with process crashes and recoveries, and lossy links. 1. <p> Although many algorithms are designed to tolerate such failure detector mistakes, the erroneous suspicions of some good processes may hurt the performance of these algorithms. For example, the erroneous suspicions of good coordinators can delay the termination of the consensus algorithms in <ref> [CT96, DFKM96, HMR97, OGS97] </ref>. Thus, requiring Strong Completeness should be avoided if possible. <p> To deal with the message loss problem, each process p has a task retransmit that periodically retransmits the last message sent to each process (only the last message really matters, just as in <ref> [DFKM96, GOS96, HMR97] </ref>). This task is terminated once p decides. We now describe the algorithm in more detail. When a process recovers from a crash, it stops participating in the algorithm, except that it periodically broadcasts a RECOVERED message until it receives the decision value. <p> It requires a majority of good processes and works in systems with lossy links. The basic structure of the algorithm (given in Fig. 3) is as in <ref> [CT96, DFKM96] </ref> and consists of rounds of 4 phases each (task 4phases).
Reference: [DLS88] <author> Cynthia Dwork, Nancy A. Lynch, and Larry Stockmeyer. </author> <title> Consensus in the presence of partial synchrony. </title> <journal> Journal of the ACM, </journal> <volume> 35(2):288323, </volume> <month> April </month> <year> 1988. </year>
Reference-contexts: There is no contradiction, however, since [CT96] assumes that all process crashes are permanent, while in our case some of the processes that crash do recover: even though they completely lost their state, they can still provide some help. 4 In Appendix B, we consider the partial synchrony models in <ref> [DLS88, CT96] </ref>.
Reference: [GOS96] <author> Rachid Guerraoui, Rui Oliveira, and Andre Schiper. </author> <title> Stubborn communication channels. </title> <type> Technical report, </type> <institution> Departement d'Informatique, Ecole Polytechnique Federale, Lausanne, Switzerland, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: To deal with the message loss problem, each process p has a task retransmit that periodically retransmits the last message sent to each process (only the last message really matters, just as in <ref> [DFKM96, GOS96, HMR97] </ref>). This task is terminated once p decides. We now describe the algorithm in more detail. When a process recovers from a crash, it stops participating in the algorithm, except that it periodically broadcasts a RECOVERED message until it receives the decision value.
Reference: [HMR97] <author> Michel Hurfin, Achour Mostefaoui, and Michel Raynal. </author> <title> Consensus in asynchronous systems where processes can crash and recover. </title> <type> Technical Report 1144, </type> <institution> Institut de Recherche en Infor-matique et Systemes Aleatoires, Universite de Rennes, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: In real systems, however, processes may recover after crashing and links may lose messages. In this paper, we focus on solving consensus with failure detectors in such systems, a problem that was first considered in <ref> [DFKM96, OGS97, HMR97] </ref> (a brief comparison with these works is in Section 1.3). <p> We also need to determine if and when stable-storage is necessary. 1.1 Failure Detectors for the Crash-Recovery Model We first focus on the problem of failure detection in the crash-recovery model. Previous solutions require unstable processes to be eventually suspected forever <ref> [OGS97, HMR97] </ref>. 2 We first prove that this requirement has a serious drawback: it forces failure detector implementations to have undesirable behaviors even in perfectly synchronous systems. More precisely, consider a synchronous round-based system with no message losses, 3 where up to n u processes may be unstable. <p> Note that the completeness property of 3S e does not require predicting the future (to determine if a process is unstable), and so it does not force implementations to have anomalous behaviors. To illustrate this, in 1 In fact, this property is assumed in <ref> [OGS97, HMR97] </ref>. 2 In [DFKM96], crash-recovery is regarded as a special case of omission failures, and the algorithm is not designed to handle unstable processes that can send and receive messages to and from good processes. 3 In such a system, processes execute in synchronized rounds, and all messages are received <p> For example, in the rotating coordinator consensus algorithms of <ref> [CT96, DFKM96, HMR97] </ref> if a process kept suspecting all processes then consensus would never be reached. <p> good process q infinitely often, then q receives messages from p infinitely often. 1.3 Related Work The problem of solving consensus with failure detectors in systems where processes may recover from crashes was first addressed in [DFKM96] (with crash-recovery as a form of omission failures) and more recently studied in <ref> [OGS97, HMR97] </ref>. <p> In <ref> [DFKM96, HMR97, OGS97] </ref>, the question of whether stable storage is always necessary is not addressed, and all the algorithms use stable storage: in [DFKM96, OGS97], the entire state of the algorithm is recorded into stable storage at every state transition; in [HMR97], only a small part of the state is recorded, <p> In [DFKM96, HMR97, OGS97], the question of whether stable storage is always necessary is not addressed, and all the algorithms use stable storage: in [DFKM96, OGS97], the entire state of the algorithm is recorded into stable storage at every state transition; in <ref> [HMR97] </ref>, only a small part of the state is recorded, and writing to stable storage is done at most once per round. In this paper, we determine when stable storage is necessary, and give two matching consensus algorithms with and without stable storage. <p> In this paper, we determine when stable storage is necessary, and give two matching consensus algorithms with and without stable storage. In the one that uses stable storage, only a small part of the state is recorded and this occurs twice per round. The algorithms in <ref> [OGS97, HMR97] </ref> use failure detectors that require that unstable processes be eventually suspected forever. <p> Crash-Recovery Model In this section, we first consider the failure detectors that were previously proposed for solving consensus in the crash-recovery model, and then propose a new type of failure detector for this model. 3.1 Limitations of Existing Failure Detectors To solve consensus in the crash-recovery model, Hurfin et al. <ref> [HMR97] </ref> and Oliveira et al. [OGS97] assume that processes have failure detectors that output lists of processes suspected to be bad, and that these failure detectors satisfy the following property: * Strong Completeness: Eventually every bad process is permanently suspected by all good processes. <p> Although many algorithms are designed to tolerate such failure detector mistakes, the erroneous suspicions of some good processes may hurt the performance of these algorithms. For example, the erroneous suspicions of good coordinators can delay the termination of the consensus algorithms in <ref> [CT96, DFKM96, HMR97, OGS97] </ref>. Thus, requiring Strong Completeness should be avoided if possible. <p> Note that 3P is stronger than the other failure detectors considered in this paper and in <ref> [OGS97, HMR97] </ref>. Theorem 3 If n a n b consensus cannot be solved without stable storage even in systems where there are no unstable processes, links do not lose messages, and processes can use 3P. <p> How does a rotating coordinator algorithm cope with an unstable coordinator? In <ref> [HMR97, OGS97] </ref> the burden is entirely on the failure detector: it is postulated that every unstable process is eventually suspected forever. <p> To deal with the message loss problem, each process p has a task retransmit that periodically retransmits the last message sent to each process (only the last message really matters, just as in <ref> [DFKM96, GOS96, HMR97] </ref>). This task is terminated once p decides. We now describe the algorithm in more detail. When a process recovers from a crash, it stops participating in the algorithm, except that it periodically broadcasts a RECOVERED message until it receives the decision value. <p> In contrast, in nice executions the consensus algorithms of <ref> [HMR97, OGS97] </ref> reach decision within 2ffi and with O (n 2 ) messages. 8.2 Quiescence An algorithm is quiescent if eventually all processes stop sending messages [ACT97].
Reference: [Lyn96] <author> Nancy A. Lynch. </author> <title> Distributed Algorithms. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1996. </year>
Reference: [NT90] <author> Gil Neiger and Sam Toueg. </author> <title> Automatically increasing the fault-tolerance of distributed algorithms. </title> <journal> Journal of Algorithms, </journal> <volume> 11(3):374419, </volume> <year> 1990. </year>
Reference-contexts: A stronger version of consensus, called uniform consensus <ref> [NT90] </ref>, requires: * Uniform Agreement: No two processes decide differently. The algorithms that we provide solve uniform consensus, and the lower bounds that we prove hold even for consensus.
Reference: [OGS97] <author> Rui Oliveira, Rachid Guerraoui, and Andre Schiper. </author> <title> Consensus in the crash-recover model. </title> <type> Technical Report 97-239, </type> <institution> Departement d'Informatique, Ecole Polytechnique Federale, Lausanne, Switzerland, </institution> <month> August </month> <year> 1997. </year> <month> 28 </month>
Reference-contexts: In real systems, however, processes may recover after crashing and links may lose messages. In this paper, we focus on solving consensus with failure detectors in such systems, a problem that was first considered in <ref> [DFKM96, OGS97, HMR97] </ref> (a brief comparison with these works is in Section 1.3). <p> We also need to determine if and when stable-storage is necessary. 1.1 Failure Detectors for the Crash-Recovery Model We first focus on the problem of failure detection in the crash-recovery model. Previous solutions require unstable processes to be eventually suspected forever <ref> [OGS97, HMR97] </ref>. 2 We first prove that this requirement has a serious drawback: it forces failure detector implementations to have undesirable behaviors even in perfectly synchronous systems. More precisely, consider a synchronous round-based system with no message losses, 3 where up to n u processes may be unstable. <p> Note that the completeness property of 3S e does not require predicting the future (to determine if a process is unstable), and so it does not force implementations to have anomalous behaviors. To illustrate this, in 1 In fact, this property is assumed in <ref> [OGS97, HMR97] </ref>. 2 In [DFKM96], crash-recovery is regarded as a special case of omission failures, and the algorithm is not designed to handle unstable processes that can send and receive messages to and from good processes. 3 In such a system, processes execute in synchronized rounds, and all messages are received <p> good process q infinitely often, then q receives messages from p infinitely often. 1.3 Related Work The problem of solving consensus with failure detectors in systems where processes may recover from crashes was first addressed in [DFKM96] (with crash-recovery as a form of omission failures) and more recently studied in <ref> [OGS97, HMR97] </ref>. <p> In <ref> [DFKM96, HMR97, OGS97] </ref>, the question of whether stable storage is always necessary is not addressed, and all the algorithms use stable storage: in [DFKM96, OGS97], the entire state of the algorithm is recorded into stable storage at every state transition; in [HMR97], only a small part of the state is recorded, <p> In [DFKM96, HMR97, OGS97], the question of whether stable storage is always necessary is not addressed, and all the algorithms use stable storage: in <ref> [DFKM96, OGS97] </ref>, the entire state of the algorithm is recorded into stable storage at every state transition; in [HMR97], only a small part of the state is recorded, and writing to stable storage is done at most once per round. <p> In this paper, we determine when stable storage is necessary, and give two matching consensus algorithms with and without stable storage. In the one that uses stable storage, only a small part of the state is recorded and this occurs twice per round. The algorithms in <ref> [OGS97, HMR97] </ref> use failure detectors that require that unstable processes be eventually suspected forever. <p> we first consider the failure detectors that were previously proposed for solving consensus in the crash-recovery model, and then propose a new type of failure detector for this model. 3.1 Limitations of Existing Failure Detectors To solve consensus in the crash-recovery model, Hurfin et al. [HMR97] and Oliveira et al. <ref> [OGS97] </ref> assume that processes have failure detectors that output lists of processes suspected to be bad, and that these failure detectors satisfy the following property: * Strong Completeness: Eventually every bad process is permanently suspected by all good processes. <p> Although many algorithms are designed to tolerate such failure detector mistakes, the erroneous suspicions of some good processes may hurt the performance of these algorithms. For example, the erroneous suspicions of good coordinators can delay the termination of the consensus algorithms in <ref> [CT96, DFKM96, HMR97, OGS97] </ref>. Thus, requiring Strong Completeness should be avoided if possible. <p> Note that 3P is stronger than the other failure detectors considered in this paper and in <ref> [OGS97, HMR97] </ref>. Theorem 3 If n a n b consensus cannot be solved without stable storage even in systems where there are no unstable processes, links do not lose messages, and processes can use 3P. <p> How does a rotating coordinator algorithm cope with an unstable coordinator? In <ref> [HMR97, OGS97] </ref> the burden is entirely on the failure detector: it is postulated that every unstable process is eventually suspected forever. <p> In contrast, in nice executions the consensus algorithms of <ref> [HMR97, OGS97] </ref> reach decision within 2ffi and with O (n 2 ) messages. 8.2 Quiescence An algorithm is quiescent if eventually all processes stop sending messages [ACT97].
References-found: 10

