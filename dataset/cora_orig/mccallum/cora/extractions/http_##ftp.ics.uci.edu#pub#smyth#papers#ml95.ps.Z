URL: http://ftp.ics.uci.edu/pub/smyth/papers/ml95.ps.Z
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: fpjs,agray,fayyadg@aig.jpl.nasa.gov  
Title: Retrofitting Decision Tree Classifiers Using Kernel Density Estimation  
Author: Padhraic Smyth, Alexander Gray, Usama M. Fayyad 
Address: M/S 525-3660  4800 Oak Grove Drive Pasadena, CA 91109-8099  
Affiliation: Jet Propulsion Laboratory  California Institute of Technology  
Abstract: A novel method for combining decision trees and kernel density estimators is proposed. Standard classification trees, or class probability trees, provide piecewise constant estimates of class posterior probabilities. Kernel density estimators can provide smooth non-parametric estimates of class probabilities, but scale poorly as the dimensionality of the problem increases. This paper discusses a hybrid scheme which uses decision trees to find the relevant structure in high-dimensional classification problems and then uses local kernel density estimates to fit smooth probability estimates within this structure. Experimental results on simulated data indicate that the method provides substantial improvement over trees or density methods alone for certain classes of problems. The paper briefly discusses various extensions of the basic approach and the types of application for which the method is best suited.
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. Friedman, J. H., Olshen, R. A. and Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees, </title> <address> Bel-mont, CA: </address> <publisher> Wadsworth. </publisher>
Reference: <author> Buntine, W. and Caruana, R. </author> <year> (1992). </year> <title> An Introduction to IND and Recursive Partitioning. </title> <type> Version 2.1, </type> <institution> NASA Ames Research Center. </institution>
Reference-contexts: Thus, to accurately assess the performance of a class probability estimator one needs to use simulated data for which the true posterior probabilities are known. (Note that an alternative approach is to estimate the difference between the probability estimates and the true probabilities via the half-Brier score <ref> (Buntine and Caruana 1992) </ref>, which essentially substitutes "1" or "0" for the true probability depending on which class is true | however, this can be an inaccurate estimate when the sample size is small and the probabilities themselves are not near 0 or 1). <p> p (! j jx i ) ^p (! j jx i ) fi fi ; (6) where N test is the number of test data points. 6.3 CLASSIFIERS USED For our standard decision tree classifiers we used both the CART and C4 algorithms as implemented in the IND software package <ref> (Buntine and Caruana 1992) </ref>, using default settings. For density estimation we used the product kernel density method described in Sections 2 and 3 (and cross-validation method as in the Appendix).
Reference: <author> Buntine, W. </author> <year> (1993). </year> <title> Learning classification trees. </title> <booktitle> In Artificial Inteligence Frontiers in Statistics: AI and Statistics III, </booktitle> <address> London, UK: </address> <publisher> Chapman and Hall, </publisher> <pages> 183-201. </pages>
Reference: <author> Cacoullos, T. </author> <year> (1966). </year> <title> Estimation of a multivariate density. </title> <journal> Ann. Inst. Statist. Math., </journal> <volume> 18: </volume> <pages> 178-189. </pages>
Reference: <author> Hand, D. J. </author> <year> (1982). </year> <title> Kernel Discriminant Analysis. </title> <address> Chichester, UK: </address> <publisher> Research Studies Press (John Wiley and Sons). </publisher>
Reference: <author> Friedman, J. H. </author> <year> (1995). </year> <title> Flexible metric nearest-neighbor classification. </title> <institution> Department of Statistics, Stanford University, </institution> <type> preprint. </type>
Reference: <author> Izenmann, A. J. </author> <year> (1991). </year> <title> Recent developments in non-parametric density estimation. </title> <journal> J. Am. Stat. Ass., </journal> <volume> 86: </volume> <pages> 205-224. </pages>
Reference: <author> Lauder, I. J. </author> <year> (1983). </year> <title> Direct kernel asessment of diagnostic probabilities. </title> <journal> Biometrika, </journal> <volume> 70(1): </volume> <pages> 251-6. </pages>
Reference-contexts: Bayesian averaging over option trees or smoothing over internal nodes could also be incorporated directly. Alternative density estimation methods are possible, such as locally adaptive methods or kernel techniques which avoid Bayes' rule and seek to estimate p (! j jx) directly <ref> (Lauder 1983) </ref> but still use the information in the tree structure.
Reference: <author> Quinlan, J. R. </author> <year> (1992). </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Los Gatos, </publisher> <address> CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Scott, D. W. </author> <year> (1992). </year> <title> Multivariate Density Estimation: Theory, Practice, and Visualization. </title> <address> New York: </address> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: A more fundamental problem is the fact that density estimation tends to scale poorly as the dimensionality d of the problem increases. In particular, it can be shown theoretically <ref> (Scott 1992) </ref> that to achieve a constant approximation error as the number of dimensions grows one needs exponentially many more examples. Thus, in practice, density estimation techniques are rarely used directly for high-dimensional problems.
Reference: <author> Silverman, B. </author> <year> (1986). </year> <title> Density Estimation for Statistics and Data Analysis. </title> <address> New York: </address> <publisher> Chapman and Hall. </publisher>
Reference-contexts: For density estimation we used the product kernel density method described in Sections 2 and 3 (and cross-validation method as in the Appendix). We experimented with both Gaussian and Cauchy kernel shapes <ref> (Silverman 1986) </ref> to get a rough idea of the sensitivity of the method to kernel shape. We also included a maximum-likelihood Gaussian classifier using separate full covariance matrices which are estimated from the data for each class. Other decision tree methods were experimented with, such as ID3. <p> From a density estimation viewpoint, the proposed method is probably most closely related to projection pursuit density estimation <ref> (Silverman 1986) </ref>: in this method, "interesting" low-dimensional projections of a high-dimensional dataset are found and density estimation is performed in this low-dimensional projection. This technique is usually carried out in the context of unsupervised learning or clustering.
Reference: <author> Walker, M. G. </author> <year> (1992). </year> <title> Probability Estimation for Classification Trees and DNA Sequence Analysis. </title> <type> PhD thesis, </type> <institution> Departments of Computer Science and Medicine, Stanford University. </institution>
References-found: 12

