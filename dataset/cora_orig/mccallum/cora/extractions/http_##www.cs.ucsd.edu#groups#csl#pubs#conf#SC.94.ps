URL: http://www.cs.ucsd.edu/groups/csl/pubs/conf/SC.94.ps
Refering-URL: http://www.cs.ucsd.edu/groups/csl/pubs/author.html
Root-URL: http://www.cs.ucsd.edu
Email: bittel, polyzos-@cs.ucsd.edu  
Title: system levels, including applications, system software and architectures. as programs whose availability is limited to
Author: Barbara K. Pasquale and George C. Polyzos 
Keyword: Key Words: empirical I/O behavior, supercomputer applications  2.0 Methodology private user applications  
Date: November 14-18, 1994  
Address: La Jolla, CA 92093-0114  D.C.  
Affiliation: Computer Systems Laboratory Department of Computer Science and Engineering University of California, San Diego  Washington,  
Note: 1.0 Introduction issues must be addressed at all  vate user applications  This work is funded in part by grants from DEC, the U.C. Micro program, and the Sequoia 2000 Project. ISSN 1063-9535. Copyright (c) 1994 IEEE. All rights reserved. Proceedings of Supercomputing 94  
Abstract: Understanding the characteristic I/O behavior of scientific applications is an integral part of the research and development efforts for the improvement of high performance I/O systems. This study focuses on application level I/O behavior with respect to both static and dynamic characteristics. We observed the San Diego Supercomputer Centers Cray C90 workload and isolated the most I/O intensive applications. The combination of a low-level description of physical resource usage and the high-level functional composition of applications and scientific disciplines for this set reveals the major sources of I/O demand in the workload. We selected two applications from the I/O intensive set and performed a detailed analysis of their dynamic I/O behavior. These applications exhibited a high degree of regularity in their I/O activity over time and their characteristic I/O behaviors can be precisely described by one and two, respectively, recurring sequences of data accesses and com putation periods Research investigations in many scientific disciplines require enormous computational power and, consequently, have fueled the need for larger and faster computer systems. However, combined with the ability to perform GFLOPS computations, scientific research also requires the ability to store, access and generate massive data sets. The growing I/O needs of scientific applications and the existing performance gap between processing power and the I/O system has intensified I/O demand within the local system and communication requirements across networks [17]. As a result, attention is now focused on research and development in the area of high performance I/O systems [5, 8, 10, 14]. Lower level I/O system improvements have been achieved by partitioning data across large disk arrays, enabling parallel data access [14], yet many issues remain regarding interconnection networks, operating system support, and file system structure for high performance I/O systems. Finding effective solutions to potential I/O bottlenecks in high perfor mance systems will require an integrated approach [6]. I/O We focus on high performance I/O at the application level. Specifically, we are interested in isolating an I/O intensive set of scientific applications from which a small subset of representative applications can be selected and their dynamic I/O behavior characterized. Recent studies indicate that scientific applications have a high degree of regularity in I/O resource usage (e.g., stable I/O rates across different execution times, consistent main memory usage, cyclical patterns of I/O activity, sequential file accesses, and fixed-sized data transfers) [9, 10, 11]. Such regularities can be exploited to improve I/O performance at all levels, but first must be characterized and quantified across a wide spectrum of applications. The research presented in this paper describes our efforts in studying the dynamic I/O behavior of two scientific applications representative of the most I/O intensive applications in a typical high performance workload. We observed a Cray C90 workload and isolated the I/O intensive population. We analyzed this population at two different levels [3]: the low-level physical resource usage and the higher-level functional composition of applications and scientific disciplines. The results of this analysis allowed us to describe the I/O intensive population in terms of individual applications, scientific disciplines, and I/O demands, i.e., volume and rate. From this description, we selected two interesting applications and studied their dynamic I/O behavior. In the sections that follow, we present our research methodology, workload characterization study, selection of an I/O intensive set, and the dynamic I/O analy sis of the two selected scientific applications. To isolate a representative set of I/O intensive applica tions, we first conduct a general workload characterization study. This allows us to observe the characteristic features of the workload and to focus our attention on one important component, the . We regard 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Calzarossa, M., and Serazzi, G., </author> <title> Workload Characterization for Supercomputers, Performance Evaluation of Supercomputers, </title> <editor> Ed. J.. L. </editor> <publisher> Martin, </publisher> <pages> pp. 283-315, </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: This not only provides a system-independent description of the major I/O disciplines [3], but also serves to highlight the demands particular scientific disciplines place on system I/O. By combining the low-level description of physical resource usage and the higher level functional composition of applications and scientific disciplines <ref> [1, 2, 4, 16] </ref>, we define the most I/O intensive applications and characterize the relationship between the individual applications, the resources they consume, and the scientific disciplines they represent. <p> 0.05 0.04 37.55 Astronomy 64 464,258.44 7.31 2.29 8,226.63 0.01 0.01 unknown Physics 1547 145,771.64 2.29 0.72 2,471.26 0.00 0.00 unknown Biology 1 94.98 0.00 0.00 67.65 0.00 0.00 .71 Electrical Eng. 308 12,301.85 0.19 0.06 15.34 0.00 0.00 unknown Oceanography 6 60.08 0.00 0.00 5.35 0.00 0.00 unknown in <ref> [1] </ref>, the construction of a representative workload model relies on the number of elements of each particular class being either (1) proportional to the number of elements in each class, (2) a function of the number and weight of elements in each class, or (3) proportional to the influence of each
Reference: [2] <author> Ferrari, D., </author> <title> Workload Characterization and Selection in Computer Performance Measurement, </title> <booktitle> Computer, </booktitle> <pages> pp. 18 24, </pages> <month> July/August </month> <year> 1972. </year>
Reference-contexts: This not only provides a system-independent description of the major I/O disciplines [3], but also serves to highlight the demands particular scientific disciplines place on system I/O. By combining the low-level description of physical resource usage and the higher level functional composition of applications and scientific disciplines <ref> [1, 2, 4, 16] </ref>, we define the most I/O intensive applications and characterize the relationship between the individual applications, the resources they consume, and the scientific disciplines they represent. <p> The elements, once selected, can be taken from the workload directly without manipulations (i.e., natural workload elements) or they may be designed and implemented independent of the real workload (i.e., artificial workload elements) <ref> [2] </ref>. We believe it is important to use the real or natural applications, but we cannot hope to monitor the runtime behavior and analyze the resulting trace of every single application in the I/O intensive set.
Reference: [3] <author> Ferrari, D., Serazzi, G., and Zeigner, A., </author> <title> Measurement and Tuning of Computer Systems, </title> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference-contexts: We observed a Cray C90 workload and isolated the I/O intensive population. We analyzed this population at two different levels <ref> [3] </ref>: the low-level physical resource usage and the higher-level functional composition of applications and scientific disciplines. The results of this analysis allowed us to describe the I/O intensive population in terms of individual applications, scientific disciplines, and I/O demands, i.e., volume and rate. <p> Next we perform a functional level characterization of the I/O intensive set with respect to scientific discipline. Each application is mapped to its respective scientific discipline and resource usage totals are calculated for each discipline. This not only provides a system-independent description of the major I/O disciplines <ref> [3] </ref>, but also serves to highlight the demands particular scientific disciplines place on system I/O. <p> The most common and important are: the physical, the virtual and the functional levels <ref> [3] </ref>. At the physical level, the description of the workload is architecture-dependent and represents the consumption of hardware and software resources (e.g., CPU time, memory).
Reference: [4] <author> Heidelberger, P., and Lavenberg, S. S., </author> <title> Computer Perfor mance Evaluation Methodology, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 33, No. 12, </volume> <pages> pp. 1195-1220, </pages> <month> December </month> <year> 1984 </year>
Reference-contexts: This not only provides a system-independent description of the major I/O disciplines [3], but also serves to highlight the demands particular scientific disciplines place on system I/O. By combining the low-level description of physical resource usage and the higher level functional composition of applications and scientific disciplines <ref> [1, 2, 4, 16] </ref>, we define the most I/O intensive applications and characterize the relationship between the individual applications, the resources they consume, and the scientific disciplines they represent.
Reference: [5] <author> Hennessy, J. L., and Patterson, D. A., </author> <title> Computer Architec ture: A Quantitative Approach, </title> <publisher> Morgan Kaufmann Publish ers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: As a result, attention is now focused on research and development in the area of high performance I/O systems <ref> [5, 8, 10, 14] </ref>. Lower level I/O system improvements have been achieved by partitioning data across large disk arrays, enabling parallel data access [14], yet many issues remain regarding interconnection networks, operating system support, and file system structure for high performance I/O systems.
Reference: [6] <author> Jain, R., </author> <title> Requirements and Heuristics for Scheduling Paral lel I/O Operations, </title> <booktitle> Proceedings of IPPS 93, Workshop on Input/Output in Parallel Computer Systems, </booktitle> <address> Newport Beach, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Finding effective solutions to potential I/O bottlenecks in high perfor mance systems will require an integrated approach <ref> [6] </ref>. I/O issues must be addressed at all system levels, including applications, system software and architectures. We focus on high performance I/O at the application level.
Reference: [7] <author> Karin, S., </author> <title> The Evolving Supercomputer Environment at the San Diego Supercomputer Center, </title> <booktitle> Supercomputing, NATO ASI Series, </booktitle> <volume> Vol. F62, </volume> <pages> pp. 97-108. </pages> <note> Edited by J. </note> <editor> S. Kowalik, </editor> <publisher> Springer-Verlag Berlin Heidelberg, </publisher> <year> 1990. </year>
Reference-contexts: There is 1 GB of main memory (128 million 64-bit words) and a 4 GB SSD used principally for caching frequently accessed disk files and for very fast swapping of jobs in and out of main memory. The broad base of this systems workload <ref> [7] </ref>, its volume of I/O, and the scientific significance of its applications, make it an important and appealing environment to study. Measurements of resource usage were obtained from the Cray System Accounting (CSA) utility [19] and collected over a one-month period during January 1994.
Reference: [8] <author> Katz, R. H., Gibson, G.A., and Patterson, D. A., </author> <title> Disk Sys tem Architectures for High Performance Computing, </title> <journal> Pro ceedings of the IEEE, </journal> <volume> Vol. 77, No. 12, </volume> <pages> pp. 1842-1858, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: As a result, attention is now focused on research and development in the area of high performance I/O systems <ref> [5, 8, 10, 14] </ref>. Lower level I/O system improvements have been achieved by partitioning data across large disk arrays, enabling parallel data access [14], yet many issues remain regarding interconnection networks, operating system support, and file system structure for high performance I/O systems.
Reference: [9] <author> Lim, S. B. and Condry, M. W., </author> <title> Supercomputing Applica tion Access Characteristics, </title> <type> Technical Report No. </type> <institution> UIUCDCS-R-91-1708,University of Illinois at Urbana Champaign, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Recent studies indicate that scientific applications have a high degree of regularity in I/O resource usage (e.g., stable I/O rates across different execution times, consistent main memory usage, cyclical patterns of I/O activity, sequential file accesses, and fixed-sized data transfers) <ref> [9, 10, 11] </ref>. Such regularities can be exploited to improve I/O performance at all levels, but first must be characterized and quantified across a wide spectrum of applications.
Reference: [10] <author> Miller, E. L. and Katz, R. H., </author> <booktitle> Input/Output Behavior of Supercomputing Applications, Proceedings of Supercom puting 91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: As a result, attention is now focused on research and development in the area of high performance I/O systems <ref> [5, 8, 10, 14] </ref>. Lower level I/O system improvements have been achieved by partitioning data across large disk arrays, enabling parallel data access [14], yet many issues remain regarding interconnection networks, operating system support, and file system structure for high performance I/O systems. <p> Recent studies indicate that scientific applications have a high degree of regularity in I/O resource usage (e.g., stable I/O rates across different execution times, consistent main memory usage, cyclical patterns of I/O activity, sequential file accesses, and fixed-sized data transfers) <ref> [9, 10, 11] </ref>. Such regularities can be exploited to improve I/O performance at all levels, but first must be characterized and quantified across a wide spectrum of applications.
Reference: [11] <author> Pasquale, B. K. and Polyzos, G. C., </author> <title> A Static Analysis of I/ O Characteristics of Scientific Applications in a Production Workload, </title> <booktitle> Proceedings of Supercomputing 93, </booktitle> <address> Portland, OR, </address> <pages> pp. 388-397, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Recent studies indicate that scientific applications have a high degree of regularity in I/O resource usage (e.g., stable I/O rates across different execution times, consistent main memory usage, cyclical patterns of I/O activity, sequential file accesses, and fixed-sized data transfers) <ref> [9, 10, 11] </ref>. Such regularities can be exploited to improve I/O performance at all levels, but first must be characterized and quantified across a wide spectrum of applications. <p> Even though it represents an extremely small portion of all executed jobs (4%), it accounts for 93% of total CPU time and 96% of total bytes transferred. We compared these results to our previous study of SDSCs Cray YMP workload of February 1992 <ref> [11] </ref>. The characterizations are extremely similiar with the exception of cumulative bytes transferred. In February 1992, cumulative bytes transferred for the entire workload was in excess of 47TB and user processes accounted for 88% of total workload bytes transferred. <p> Figure 1 shows the top I/O ranked processes cumulative contribution to total workload resources. (It should be noted that all processes considered in the I/O rate ordering have average CPU execution times of 100 seconds or more. This threshold was determined in <ref> [11] </ref> as a result of the observation that short jobs do not have significant impact when considering their contribution to total workload bytes transferred.) The Cumulative Contribution to Total Workload Bytes Transferred graph in Figure 1 clearly delineates the I/O intensive processes.
Reference: [12] <author> Pasquale, B. K. and Polyzos, G. C., </author> <title> A Case Study of Scien tific Application I/O Behavior, </title> <booktitle> Proceedings of MASCOTS 94, International Workshop on Modeling, Analysis and Simulation of Computer and Telecommunication Systems, Durham, NC, </booktitle> <pages> pp. 101-106, </pages> <month> Jan. 31 - Feb 2, </month> <year> 1994. </year> <note> (Also appears as Sequoia 2000 Technical Report #93/36, </note> <institution> Univer sity of California Berkeley, </institution> <month> December </month> <year> 1993.) </year>
Reference-contexts: During the second period of read I/O to file 4, transfers also complete within one microsecond and the time between successive transfers averages 5082.01 microseconds. The second computation takes an average of 5.13 seconds. Statistics summarizing interevent times and com putation periods can be found in <ref> [12] </ref>. 0 2 0 4 0 6 0 8 0 Read Transfers Write Transfers I / O r a s f e e e n 16 secs. from last I/O CPU Seconds 4 secs. between I/Os 16 secs. to next I/O 55 I/O transfers 10.8 MB transferred 1 second 11 I/O <p> The second computation phase follows the read I/O to file 4 and lasts for an average of 16.70 seconds. Additional statistics on interevent times and computation periods can be found in <ref> [12] </ref>. 4.2 OGCMs Dynamic I/O Behavior OGCM is executed with a set of runtime parameters which regulate the degree of resolution in the simulation process. As a result, total CPU time and I/O volume are commensurate with simulation granularity.
Reference: [13] <author> Pasquale, B. K. and Polyzos, G. C., </author> <title> Dynamic I/O Characterization of I/O Intensive Scientific Applications, </title> <type> Techni cal Report No. </type> <institution> CS94-364, University of California, </institution> <address> San Diego, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Based on this analysis, we conclude that the named processes in the I/O intensive set and their resource usage are representative of I/O intensive applications and will refer to them as applications. A complete listing of these I/O intensive applications ordered by average virtual I/O rate is given in <ref> [13] </ref>. The listing also provides the execution frequency, scientific discipline, average CPU time, and average bytes trans ferred for each application. Figures 2 and 3 detail the actual distributions of average virtual I/O rate and average CPU time for this I/O intensive set. <p> Five cgcm suffix applications appeared for February 1992 and 13 appeared for January 1994. We also compared the cumulative resource usage totals for the two top 80 ranked orderings. (The actual table of values can be found in <ref> [13] </ref>.) With the exception of the top ranked application, there is a general increase in both cumulative CPU and cumulative bytes transferred from the rank ordering of February 1992 to that of January 1994.
Reference: [14] <author> Patterson, D. A., Gibson, G. A., and Katz, R. H., </author> <title> A Case for Redundant Arrays of Inexpensive Disks, </title> <booktitle> Proceedings of ACM SIGMOD Conference, </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: As a result, attention is now focused on research and development in the area of high performance I/O systems <ref> [5, 8, 10, 14] </ref>. Lower level I/O system improvements have been achieved by partitioning data across large disk arrays, enabling parallel data access [14], yet many issues remain regarding interconnection networks, operating system support, and file system structure for high performance I/O systems. <p> As a result, attention is now focused on research and development in the area of high performance I/O systems [5, 8, 10, 14]. Lower level I/O system improvements have been achieved by partitioning data across large disk arrays, enabling parallel data access <ref> [14] </ref>, yet many issues remain regarding interconnection networks, operating system support, and file system structure for high performance I/O systems. Finding effective solutions to potential I/O bottlenecks in high perfor mance systems will require an integrated approach [6].
Reference: [15] <author> Roads, J. O., et al., </author> <title> A Preliminary Description of the West ern U.S. </title> <booktitle> Climatology, Proceedings of the Ninth Annual Pacific Climate (PAClim) Workshop, </booktitle> <month> September 8, </month> <year> 1992. </year>
Reference-contexts: The first application, is an analysis application used in the Climate Research Division at Scripps Institution of Oceanography <ref> [15] </ref>. used for the verification of a fine-grained regional atmo spheric climate model over the Western United States with measured large-scale, global climate and weather station site data.
Reference: [16] <author> Serazzi, G., </author> <title> A Functional and Resource-Oriented Proce dure for Workload Modeling, Performance '81, </title> <journal> pp. </journal> <volume> 345 361, </volume> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference-contexts: This not only provides a system-independent description of the major I/O disciplines [3], but also serves to highlight the demands particular scientific disciplines place on system I/O. By combining the low-level description of physical resource usage and the higher level functional composition of applications and scientific disciplines <ref> [1, 2, 4, 16] </ref>, we define the most I/O intensive applications and characterize the relationship between the individual applications, the resources they consume, and the scientific disciplines they represent.
Reference: [17] <author> Stonebraker, M., and Dozier, J., </author> <title> Overview of the Sequoia 2000 Project, </title> <booktitle> Proceedings of COMPCON 93, </booktitle> <address> San Fran cisco, CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: The growing I/O needs of scientific applications and the existing performance gap between processing power and the I/O system has intensified I/O demand within the local system and communication requirements across networks <ref> [17] </ref>. As a result, attention is now focused on research and development in the area of high performance I/O systems [5, 8, 10, 14].
Reference: [18] <institution> Corporation for National Research Initiatives, </institution> <note> CASA Giga bit Testbed: 1991 Annual Report, </note> <month> June </month> <year> 1991. </year>
Reference-contexts: The second application, OGCM , is a simulation application used in the Department of Atmospheric Sci ences at the University of California, Los Angeles <ref> [18] </ref>. OGCM , the ocean global circulation model, simulates pat terns in water temperature, velocity, etc. in the Pacific Ocean basins.
Reference: [19] <institution> [UNICOS System Administration Manual, </institution> <note> Vol. 1 (SG-2113 8.0), </note> <institution> Cray Research Inc., </institution> <year> 1994. </year>
Reference-contexts: The broad base of this systems workload [7], its volume of I/O, and the scientific significance of its applications, make it an important and appealing environment to study. Measurements of resource usage were obtained from the Cray System Accounting (CSA) utility <ref> [19] </ref> and collected over a one-month period during January 1994. On a per process-basis, the CSA utility records resource usage through the use of kernel probes and creates a process account record that summarizes the total resource usage.
References-found: 19

