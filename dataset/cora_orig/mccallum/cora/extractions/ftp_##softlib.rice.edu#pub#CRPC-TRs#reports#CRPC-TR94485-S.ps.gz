URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94485-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A Linear-Time Algorithm for Computing the Memory Access Sequence in Data-Parallel Programs  
Author: Ken Kennedy Nenad Nedeljkovic Ajay Sethi 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  Subimtted to the 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP '95).  
Date: October, 1994  
Pubnum: CRPC-TR94485-S  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> C. Ancourt, F. Coelho, F. Irigoin, and R. Keryell. </author> <title> A linear algebra framework for static HPF code distribution. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Since this point belongs to processor 1 no adjustment to M [0] is necessary. In the next iteration of the outer loop, we visit index 76 in the inner loop, setting M <ref> [1] </ref> = 12 (lines 30-31) After terminating the inner loop, the next index visited is 103, which does not belong to processor 1, and therefore we move to the point 139, setting M [2] = 15 (lines 38-39). <p> In an approach similar to the virtual-cyclic scheme Stichnoth et al. use intersections of array slices for communication generation [13]. As mentioned above, a disadvantage of this method is that array accesses are reordered. Ancourt et al. use a linear algebra framework for compiling independent loops in HPF <ref> [1] </ref>. Because of the independent parallelism they assume that loop iterations can be enumerated in any order.
Reference: [2] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Using this data mapping specification, the compiler must partition the arrays and generate SPMD code which will be executed on each processor. Several variants of data-parallel Fortran that preceded HPF, such as Fortran D [9] and Vienna Fortran <ref> [2] </ref>, also provided ways for the programmer to specify mapping of array data onto processors. Implementations of these languages included the support for block and cyclic distributions. <p> the next iteration of the outer loop, we visit index 76 in the inner loop, setting M [1] = 12 (lines 30-31) After terminating the inner loop, the next index visited is 103, which does not belong to processor 1, and therefore we move to the point 139, setting M <ref> [2] </ref> = 15 (lines 38-39).
Reference: [3] <author> S. Chatterjee. </author> <title> Private communication, </title> <month> October </month> <year> 1994. </year>
Reference-contexts: The process is continued until we reach the first point of the next sequence, index 301, and at the end, M = <ref> [3, 12, 15, 12, 3, 12, 3, 12] </ref>. 9 5.1 Complexity The running time of the extended Euclid's algorithm is O (log min (s; pk)) [5]. The loops in lines 4-9 and 17-22 of Figure 4 are both O (k). <p> In order to perform a correct comparison with the algorithm from [4], we modified the code provided to us by Siddhartha Chatterjee <ref> [3] </ref> so that the segments common to both methods (lines 3-9 in Figure 4) were coded identically. <p> It should be noted here that the implementation from <ref> [3] </ref> uses the linear-time radix sort for sorting the initial sequence when k 64, which causes the relative performance gain achieved by our algorithm to be constant.
Reference: [4] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This problem and its solution were first described by Chatterjee et al. <ref> [4] </ref>. They give an algorithm for solving the memory address problem in O (k log k + min (log s; log p)) time, and show that any algorithm for this problem takes (k) time. <p> In this paper we describe an improved algorithm that computes the memory address sequence for the general case in O (k + min (log s; log p)) time. Experimental comparison with the method presented in <ref> [4] </ref> shows that the theoretically proven lower complexity of our algorithm is mirrored by its superior performance in practice. The proposed algorithm allows for simple and efficient implementation and, as such, is suitable for inclusion in compilers and run-time systems for HPF-like languages. <p> The proposed algorithm allows for simple and efficient implementation and, as such, is suitable for inclusion in compilers and run-time systems for HPF-like languages. The remainder of this paper is organized as follows: In Section 2 we describe the problem and the solution proposed in <ref> [4] </ref> in some detail. In Section 3 we show how regular section accesses fit in the framework of integer lattice theory. This is further explored in Section 4, where we lay the theoretical foundation for our algorithm. <p> in Section 8 by summarizing our contributions and indicating directions for future research. 2 Problem Statement For array A distributed across p processors using cyclic (k) distribution, the layout of its elements in local processor memories can be visualized as a two-dimensional matrix, with each row divided into p blocks <ref> [4] </ref>. The location of array element A (i) is determined by the processor holding A (i), the block within this processor containing A (i), and the offset of A (i) within the block. <p> Chatterjee et al. visualize the offset and memory gap sequence as the transition diagram of a finite state machine <ref> [4] </ref>. State transitions depend only on p, k, and s, whereas a processor's start state in the transition table also depends on the lower bound of the array section l and that processor's number m. <p> offset relative to the beginning of its row (i mod pk) lies in the range [km; k (m + 1)), finding the starting location is equivalent to finding the smallest nonnegative integer j such that km (l + sj) mod pk k (m + 1) 1: It is shown in <ref> [4] </ref> that this is equivalent to solving a set of k linear Diophantine equations fsj pkq = i j km l i km l + k 1g in variables j and q. Each individual equation has solutions if and only if gcd (s; pk) divides i. <p> elements of the array section with lower bound l = 0, and stride s = 9. 2 For each solvable equation, Chatterjee et al. find the solution having the smallest nonnegative j and use the minimum of these solutions to get the starting array section element A (l + js) <ref> [4] </ref>. The last array section element can be found in a similar way using the upper bound u. <p> Furthermore, we assume that s &gt; 0, as the case when s is negative can be treated analogously. While we follow the approach from <ref> [4] </ref> for finding the starting location, our method differs in the way we find the offset and memory gap sequences. <p> After finding the set of smallest positive solutions as described above, Chatterjee et al. sort this set to produce the sequence of array section indices that will be successively accessed by the processor <ref> [4] </ref>. Memory gap sequence can then be found by a simple linear scan through the sorted sequence of array indices. Since sorting the sequence requires O (k log k) time, it represents the dominating term in the overall complexity of the algorithm. <p> Perfect alignment of an array to a template is given by a = 1; b = 0. Chatterjee et al. show that the memory access problem for any affine alignment can be solved by two applications of the algorithm for the perfect alignment <ref> [4] </ref>. Therefore, we present our algorithm only for the case of perfect alignment without loss of generality. <p> As mentioned in Section 2, we use the approach described in <ref> [4] </ref> to compute the starting location for a given processor. First, we use the extended form of Euclid's algorithm [5] to compute d = gcd (s; pk). Using this value, we can find the smallest regular section index for each offset in the range of processor m (lines 3-9). <p> First, we use the extended form of Euclid's algorithm [5] to compute d = gcd (s; pk). Using this value, we can find the smallest regular section index for each offset in the range of processor m (lines 3-9). Unlike the algorithm in <ref> [4] </ref>, which stores all these locations and later sorts them, we are only interested in the first location for the processor. <p> Therefore, we conclude that the running time of the algorithm is O (log min (s; pk)) + O (k), which reduces to O (k + min (log s; log p)). Since it was shown in <ref> [4] </ref> that the problem is (k), our algorithm is O (min (log s; log p)) away from being theoretically optimal. <p> We now describe our implementation experience, which shows that our algorithm is more efficient in practice than the method described in <ref> [4] </ref>. We also discuss the impact that the shape of the node code has on the overall performance. 6.1 Table Construction The description of the algorithm in Figure 4 provides enough low-level details to directly convert the algorithm into working code. <p> In order to perform a correct comparison with the algorithm from <ref> [4] </ref>, we modified the code provided to us by Siddhartha Chatterjee [3] so that the segments common to both methods (lines 3-9 in Figure 4) were coded identically. <p> Moreover, since the method by Chatterjee et al. requires sorting of the initial sequence of memory accesses, we tried to use the most efficient sorting routines available to us, so as not to obtain an unfair advantage over the algorithm in <ref> [4] </ref>. If input parameters p, k, l, and s for our algorithm are compile-time constants, then the compiler could compute the table of memory gaps (M) for each processor. <p> In that case the code that computes the parameters of the two line families (lines 16-24 in Figure 4) would have to be executed only once, and values of (b r ; a r ) and (b l ; a l ) could be reused. Furthermore, as noted in <ref> [4] </ref>, if gcd (s; pk) = 1, then the local M 10 sequences are cyclic shifts of one another, and after computing the table once, only the starting locations for all the processors need to be found. <p> While the difference in performance of the two algorithms is not significant for small values of k (k = 4, k = 8), as k increases the algorithm in Figure 4 clearly outperforms the algorithm described by Chatterjee et al. in <ref> [4] </ref>. It should be noted here that the implementation from [3] uses the linear-time radix sort for sorting the initial sequence when k 64, which causes the relative performance gain achieved by our algorithm to be constant. <p> 60 138 76 145 65 134 60 140 54 133 k = 64 122 775 140 749 132 747 124 735 109 727 k = 256 332 2708 394 2814 340 2730 368 2713 325 2776 Table 1 Execution times in microseconds for our algorithm (Lines), and the algorithm in <ref> [4] </ref> (Sorting). 11 4 8 16 32 200 * * * * * s * * Sorting * * Lines 64 128 256 512 3000 * * * * * s * * Sorting * * Lines 6.2 Code Generation After the table of local memory gaps is constructed, each processor <p> In Figure 7 we show four different ways to generate the node code based on the memory sequence table. The C code fragments correspond to the simple array assignment statement A (l : u : s) = 100:0. The code in 7 (a) is identical to that proposed in <ref> [4] </ref>. In an attempt to remove the usually expensive mod operation, we replace it with a simple test in 7 (b). A slight modification of the same idea is shown in 7 (c). <p> 18541 3916 3823 3065 k = 32 s = 15 18070 3504 2845 2547 s = 3 18122 3316 2573 2357 s = 99 18567 4000 3294 3149 Table 2 Execution times in microseconds for different node code versions. 13 7 Related Work Besides the work by Chatterjee et al. <ref> [4] </ref> that has been extensively cited throughout this paper, several other researchers have also dealt with issues of compiling programs with cyclic (k) distribution. <p> Only after these problems are fully and efficiently solved, can the use of such novel features of HPF become commonplace. Acknowledgments We would like to thank Siddhartha Chatterjee for providing us the code that implements the algorithm described in <ref> [4] </ref> and for insightful comments on an earlier draft of this paper. We also thank Debbie Campbell and Lani Granston for their help in proofreading various drafts of the paper. 14
Reference: [5] <author> T.H. Cormen, C.E. Leiserson, and R.L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: As mentioned in Section 2, we use the approach described in [4] to compute the starting location for a given processor. First, we use the extended form of Euclid's algorithm <ref> [5] </ref> to compute d = gcd (s; pk). Using this value, we can find the smallest regular section index for each offset in the range of processor m (lines 3-9). <p> The process is continued until we reach the first point of the next sequence, index 301, and at the end, M = [3, 12, 15, 12, 3, 12, 3, 12]. 9 5.1 Complexity The running time of the extended Euclid's algorithm is O (log min (s; pk)) <ref> [5] </ref>. The loops in lines 4-9 and 17-22 of Figure 4 are both O (k). Finally, the doubly nested loop in lines 28-42 does only O (k) work; in fact, we show that, in the worst case, at most 2k + 1 points are examined.
Reference: [6] <author> S.K.S. Gupta, S.D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <type> Technical Report OSE-CISRC-4/94-TR19, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Gupta et al. address the problem of array statements (A (l a : u a : s a ) = B (l b : u b : s b )) involving block-cyclic distributions <ref> [6] </ref>. In their virtual-cyclic scheme, array elements are accessed in an order different from the order in a sequential program. While this is not a problem in perfectly parallel array assignments, the scheme cannot be used for arbitrary loops accessing block-cyclically distributed arrays.
Reference: [7] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction High Performance Fortran (HPF) <ref> [7, 12] </ref> incorporates a set of Fortran extensions for portable data-parallel programming on distributed-memory machines. The most important of these extensions are align and distribute directives, which are used to describe how data should be distributed across processors in a parallel computer.
Reference: [8] <author> S. Hiranandani, K. Kennedy, J. Mellor-Crummey, and A. Sethi. </author> <title> Compilation techniques for block-cyclic distributions. </title> <booktitle> In Proceedings of the 1994 ACM International Conference on Supercomputing, </booktitle> <address> Manchester, England, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Hiranandani et al. present an algorithm that works in O (k) time, but only if some special conditions are satisfied (s mod pk &lt; k) <ref> [8] </ref>. In this paper we describe an improved algorithm that computes the memory address sequence for the general case in O (k + min (log s; log p)) time.
Reference: [9] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Commu nications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Using this data mapping specification, the compiler must partition the arrays and generate SPMD code which will be executed on each processor. Several variants of data-parallel Fortran that preceded HPF, such as Fortran D <ref> [9] </ref> and Vienna Fortran [2], also provided ways for the programmer to specify mapping of array data onto processors. Implementations of these languages included the support for block and cyclic distributions.
Reference: [10] <author> R. Kannan. </author> <title> Algorithmic geometry of numbers. </title> <editor> In J. Traub, editor, </editor> <booktitle> Annual Review of Computer Science. </booktitle> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1987. </year>
Reference-contexts: Proof: Every discrete subset of R n closed under subtraction is a lattice <ref> [10] </ref>. Let (b 1 ; a 1 ) : pk a 1 + b 1 = i 1 s and (b 2 ; a 2 ) : pk a 2 + b 2 = i 2 s (i 1 ; i 2 2 Z) be two arbitrary points in fl.
Reference: [11] <author> A. Knies, M. O'Keefe, and T. MacDonald. </author> <title> High Performance Fortran: A practical analysis. </title> <journal> Scientific Programming, </journal> <volume> 3(3) </volume> <pages> 187-199, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: &gt; lastmem) goto done; g done: (c) base = startmem; i = startoffset; while (base &lt;= lastmem) f *base = 100.0; base += deltaM [i]; i = nextoffset [i]; g It was noted by Knies et al. that the code based on table lookup makes a time versus space tradeoff <ref> [11] </ref>. This is particularly true for the code in Figure 7 (d), which while being the fastest, requires two tables to be stored. <p> In this way the memory overhead is eliminated, but the performance will still be much better than if we used the full run-time generation of addresses as described in <ref> [11] </ref>.
Reference: [12] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction High Performance Fortran (HPF) <ref> [7, 12] </ref> incorporates a set of Fortran extensions for portable data-parallel programming on distributed-memory machines. The most important of these extensions are align and distribute directives, which are used to describe how data should be distributed across processors in a parallel computer. <p> The process is continued until we reach the first point of the next sequence, index 301, and at the end, M = <ref> [3, 12, 15, 12, 3, 12, 3, 12] </ref>. 9 5.1 Complexity The running time of the extended Euclid's algorithm is O (log min (s; pk)) [5]. The loops in lines 4-9 and 17-22 of Figure 4 are both O (k).
Reference: [13] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 150-159, </pages> <month> April </month> <year> 1994. </year> <month> 15 </month>
Reference-contexts: In the virtual-block scheme array accesses are not reordered, but if the array section stride is larger than the block size, their method effectively reduces to run-time address resolution. In an approach similar to the virtual-cyclic scheme Stichnoth et al. use intersections of array slices for communication generation <ref> [13] </ref>. As mentioned above, a disadvantage of this method is that array accesses are reordered. Ancourt et al. use a linear algebra framework for compiling independent loops in HPF [1]. Because of the independent parallelism they assume that loop iterations can be enumerated in any order.
References-found: 13

