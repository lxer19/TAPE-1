URL: http://drl.cs.uiuc.edu/pubs/mahesh-thesis.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/mahesh-thesis.html
Root-URL: http://www.cs.uiuc.edu
Title: HIGH PERFORMANCE IMPLEMENTATION OF SERVER DIRECTED I/O  
Author: BY MAHESH SUBRAMANIAM 
Degree: B.E, Birla Institute of Technology Science, 1993 M.Sc, Birla Institute of Technology Science, 1993 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1996 Urbana, Illinois  
Affiliation: University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Application Working Group of the Scalable I/O Initiative. </institution> <note> Preliminary Survey of I/O Intensive Applications. Scalabale I/O Initiative Working Paper No. 1, </note> <year> 1994. </year>
Reference-contexts: A collective I/O operation is one where multiple pro 1 cesses of the application cooperate to perform the I/O operation. The collective I/O operations are useful in a wide variety of applications including several applications targeted by the Scalable I/O Initiative project <ref> [1] </ref>, computational fluid dynamics (CFD) applications, astrophysics applications and weather modeling applications.
Reference: [2] <author> Fern E. Bassow. </author> <title> IBM AIX Parallel I/O File System: Installation, Administration, and Use. </title> <institution> IBM, Kingston, </institution> <address> N.Y., </address> <month> May </month> <year> 1995. </year> <title> Document Number SH34-6065-00. </title>
Reference: [3] <author> Rajesh Bordawekar, Alok Choudhary, Ken Kennedy, Charles Koelbel, and Mike Paleczny. </author> <title> A model and compilation strategy for out-of-core data parallel programs. </title> <type> Technical Report CRPC-TR94507-S, </type> <note> Center for Research on Parallel Computation, </note> <month> December </month> <year> 1994. </year>
Reference: [4] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel i/o. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference: [5] <author> Peter Brezany, Michael Gernt, Piyush Mehrotra, and Hans Zima. </author> <title> Concurrent file operations in High Performance Fortran. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 230-237, </pages> <month> November </month> <year> 1992. </year>
Reference: [6] <author> Y. Chen, M. Winslett, K. E. Seamons, S. Kuo, Y. Cho, M. Subramaniam. </author> <title> Scalable Message Passing in Panda. </title> <booktitle> Fourth Annual Workshop on I/O in Parallel and Distributed Systems, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The design of the Panda library must be extensible and allow the implementation of new collective I/O strategies in a quick and efficient manner. * The Panda array I/O library must also support the implementation of the collective I/O operations using the reduced messages server directed I/O strategy described in <ref> [6] </ref>. * The collective I/O strategies must be extended to support "shared" I/O nodes. In this mode, the I/O servers are shared by multiple applications and are able to process multiple collective I/O requests simultaneously. * The Panda array I/O library must support "part time" I/O nodes.
Reference: [7] <author> Alok Choudhary, Rajesh Bordawekar, Michael Harry, Rakesh Krishnaiyer, Ravi Pon-nusamy, Tarvinder Singh, and Rajeev Thakur. </author> <title> PASSION: parallel and scalable software for input-output. </title> <type> Technical Report SCCS-636, </type> <institution> ECE Dept., NPAC and CASE Center, Syracuse University, </institution> <month> September </month> <year> 1994. </year> <month> 114 </month>
Reference: [8] <author> Peter Corbett, Dror Feitelson, Yarson Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, and Parkson Wong. </author> <title> MPI-IO: A parallel file i/o interface for MPI. </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year> <note> Version 0.3. </note>
Reference-contexts: It allows users to specify intended access patterns and control parameters like caching and prefetching. Another important area of research is the design of I/O interfaces and the strategies to maximize the performance of the I/O subsystem. The MPI-IO <ref> [8] </ref> proposal aims to extend the current standard for message passing to describe parallel I/O operations. The proposal provides interfaces for I/O at a level higher than that provided by existing parallel file systems. The user is allowed to define derived data types to access fragmented subsets of a file.
Reference: [9] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference-contexts: The Paragon has specialized I/O nodes and the file data is striped across the I/O nodes. The PFS provides additional access modes and allows control over the striping parameters. The IBM Vesta <ref> [9] </ref> file system is an experimental system targeted for the IBM SP1 and SP2 [36]. The data is interleaved across multiple I/O nodes. A file on the Vesta file system is a two dimensional structure with the file divided into physical partitions.
Reference: [10] <author> Thomas W. Crockett. </author> <title> File concepts for parallel i/o. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference: [11] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improving parallel i/o performance using a two-phase access strategy. </title> <type> Technical Report SCCS-406, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <year> 1993. </year>
Reference-contexts: This results in poor overall I/O performance. There are techniques like the two-phase I/O strategy <ref> [11] </ref> to alleviate this problem. During the first phase of the 11 two-phase I/O strategy, a subset of the compute nodes cooperate and re-order the data such that it conforms to the distribution on the I/O nodes. <p> Since the Panda interfaces are at a higher level than the ones provided by MPI-IO, it is possible to build Panda on top of MPI-IO in order to provide portability. The two-phase run-time access strategy <ref> [11] </ref> is a collective I/O strategy for arrays in a distributed memory environment. Like Panda, it allows the array to have different distributions on the I/O nodes and the compute nodes.
Reference: [12] <author> Juan Miguel del Rosario and Alok Choudhary. </author> <title> High performance i/o for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference: [13] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference: [14] <author> Craig S. Freedman, Josef Burger, and David J. Dewitt. </author> <title> SPIFFI | a scalable parallel file system for the Intel Paragon. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <year> 1995. </year>
Reference: [15] <author> James C. French, Terrence W. Pratt, and Mrigganka Das. </author> <title> Performance measurement of a Parallel Input/Output System for the Intel iPSC/2 Hypercube. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> San Diego, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: The extensions supported are allowing for concurrent accesses to the file and striping the file across multiple disks in order to facilitate concurrent accesses. Scalability is an important factor in designing these parallel file systems. The Intel Concurrent File System (CFS) [27] is used on the Intel iPSC <ref> [15] </ref> machines and the Touchstone Delta [18]. It stripes the file data across multiple disks and provides four different access modes. The first access mode allows processors to access the file data independently with no coordination among the accesses.
Reference: [16] <author> James V. Huber Jr., Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The processors can logically 100 partition the file into subfiles and access them. The IBM Vesta file systems also allows for concurrent independent accesses to the data on the I/O nodes. The Portable Parallel File System (PPFS) <ref> [16] </ref> is a user level library built on top of the standard Unix file system. Like Panda, it uses C++, MPI and standard Unix file system calls to ensure portability of the system. It allows users to specify intended access patterns and control parameters like caching and prefetching.
Reference: [17] <author> IBM. </author> <title> AIX Version 3.2 and 4.1 Performance tuning guide. </title> <institution> IBM Customer Publications, IBM corporation, </institution> <month> August </month> <year> 1994. </year> <month> 115 </month>
Reference-contexts: The AIX operating system allocates the prefetch buffer from the amount of free memory available and can allocate up to 90% of the physical memory on each node for the prefetch buffer <ref> [17] </ref>. Thus when the size of the compute chunk increases, the amount of free memory available to the AIX operating system on the part time I/O nodes decreases, thereby decreasing the size of the prefetch buffer.
Reference: [18] <author> Intel. </author> <title> Touchstone Delta System Description. </title> <type> Technical report, </type> <institution> Intel Advanced Information , Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: Scalability is an important factor in designing these parallel file systems. The Intel Concurrent File System (CFS) [27] is used on the Intel iPSC [15] machines and the Touchstone Delta <ref> [18] </ref>. It stripes the file data across multiple disks and provides four different access modes. The first access mode allows processors to access the file data independently with no coordination among the accesses. The second mode provides a file pointer which is shared by all the processors.
Reference: [19] <author> Intel. </author> <title> Paragon XP/S System Description. </title> <type> Technical report, </type> <institution> Intel Advanced Information , Intel Corporation, </institution> <year> 1992. </year>
Reference-contexts: The fourth mode is similar to the previous mode except that the individual accesses by the processors are of the same size. The Intel Parallel File System (PFS) is an improved version of CFS and is targeted for the Intel Paragon <ref> [19] </ref>. The Paragon has specialized I/O nodes and the file data is striped across the I/O nodes. The PFS provides additional access modes and allows control over the striping parameters. The IBM Vesta [9] file system is an experimental system targeted for the IBM SP1 and SP2 [36].
Reference: [20] <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French. Extensible file systems ELFS: An object-oriented approach to high performance file i/o. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 191-204, </pages> <month> October </month> <year> 1994. </year>
Reference: [21] <author> David Kotz. </author> <title> Disk-directed i/o for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year> <month> Revised November 8, </month> <year> 1994. </year>
Reference-contexts: Since the disk performance is the dominant cost, the large disk write operations provide a better overall I/O performance as compared to the traditional approach. Similar problems exist for collective read operations. The server directed I/O algorithm which is derived from disk directed I/O <ref> [21] </ref> overcomes the disadvantages of the traditional approach to performing I/O. In this algorithm, the I/O servers direct the flow of I/O requests. The I/O servers are aware of the data layouts on the compute nodes and on the I/O nodes. <p> This provides additional disk I/O performance. When array chunks are small and the network latency becomes the dominant message passing cost, the two phase strategy can be expected to provide better performance than Panda's server directed I/O strategy. Disk directed I/O <ref> [21] </ref> is a collective I/O strategy where the compute nodes inform the I/O nodes of the collective I/O request and the I/O nodes direct the flow of data. The I/O nodes are aware of the physical disk blocks in which the data is stored. <p> Then the data is gathered/scattered such that 101 the disk blocks are accessed in sorted order. Simulations show that disk directed I/O provides better performance than traditional caching techniques and the two phase I/O strategy <ref> [21] </ref>. Simulations also show that disk-directed I/O provides high performance for the out-of-core LU decomposition problem [22] and other irregularly structured data requests [23].
Reference: [22] <author> David Kotz. </author> <title> Disk-directed i/o for an out-of-core computation. </title> <booktitle> In Proceedings of the Fourth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 159-166, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Simulations show that disk directed I/O provides better performance than traditional caching techniques and the two phase I/O strategy [21]. Simulations also show that disk-directed I/O provides high performance for the out-of-core LU decomposition problem <ref> [22] </ref> and other irregularly structured data requests [23]. Since the disk directed I/O strategy requires knowledge of the actual physical disk layouts, each I/O node needs to have the information about the size of disk blocks and the disk blocks that comprise each file.
Reference: [23] <author> David Kotz. </author> <title> Expanding the potential for disk-directed i/o. </title> <type> Technical Report PCS-TR95-254, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Simulations show that disk directed I/O provides better performance than traditional caching techniques and the two phase I/O strategy [21]. Simulations also show that disk-directed I/O provides high performance for the out-of-core LU decomposition problem [22] and other irregularly structured data requests <ref> [23] </ref>. Since the disk directed I/O strategy requires knowledge of the actual physical disk layouts, each I/O node needs to have the information about the size of disk blocks and the disk blocks that comprise each file.
Reference: [24] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: Being a distributed memory environment, all communication between the nodes is via messages. These messages travel through an inter-connection network. The Panda servers communicate with each other and with the Panda clients using messages. In order to ensure portability, MPI <ref> [24] </ref> is used as the standard for message passing. 2.1 Array I/O interfaces The high level array I/O interface provided by Panda has two major advantages. <p> on disk (i.e., the internal subchunking should be performed such that each server chunk is still stored on disk in same order as if there were no subchunking). 22 Chapter 3 Panda 2.1 Implementation The Panda 2.1 array I/O library is implemented using C++ as the programming language and MPI <ref> [24] </ref> for message passing. This is in keeping with the philosophy of using standard software components to build the I/O library. 3.1 Programming language and messaging interface While choosing C++ as the programming language, the following factors were considered. * The implementation should be clean and simple.
Reference: [25] <author> S. A. Moyer and V. S. Sunderam. </author> <title> PIOUS: A scalable parallel i/o system for distributed computing environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <month> May </month> <year> 1994. </year>
Reference: [26] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference: [27] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: The extensions supported are allowing for concurrent accesses to the file and striping the file across multiple disks in order to facilitate concurrent accesses. Scalability is an important factor in designing these parallel file systems. The Intel Concurrent File System (CFS) <ref> [27] </ref> is used on the Intel iPSC [15] machines and the Touchstone Delta [18]. It stripes the file data across multiple disks and provides four different access modes. The first access mode allows processors to access the file data independently with no coordination among the accesses.
Reference: [28] <author> M. Rosenblum and J. K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <month> February </month> <year> 1992. </year> <month> 116 </month>
Reference: [29] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference: [30] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server directed collective i/o in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, California, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: portable across multiple platforms. * The Panda array I/O library should provide high level interfaces to specify collective I/O operations on multi-dimensional arrays and must provide high I/O performance. * The Panda array I/O library must support the implementation of the collective I/O operations using the server directed I/O strategy <ref> [30] </ref>. * The design of the Panda library must be extensible and allow the implementation of new collective I/O strategies in a quick and efficient manner. * The Panda array I/O library must also support the implementation of the collective I/O operations using the reduced messages server directed I/O strategy described <p> case, before each checkpoint operation the density array is inserted into the simulation object and after the completion of the checkpoint operation the density array is removed from the simulation object. 2.2 Server directed collective I/O The collective I/O operations in Panda are implemented using the server directed I/O technique <ref> [30] </ref>. The basic philosophy of the server directed I/O technique is that I/O servers actively direct the I/O as opposed to passively responding to requests from the clients. Let us consider the execution of a write operation on a large multidimensional array using the traditional approach.
Reference: [31] <author> K. E. Seamons, Y. Chen, M. Winslett, Y. Cho, S. Kuo, P. Jones, J. Jozwiak, and M. Sub-ramaniam. </author> <title> Fast and easy i/o for arrays in large-scale applications. </title> <booktitle> In Seventh IEEE Symposium on Parallel and Distributed Information Systems, Workshop on Modeling and Specification of I/O, </booktitle> <month> October </month> <year> 1995. </year>
Reference: [32] <author> Kent E. Seamons and Marianne Winslett. </author> <title> An efficient abstract interface for multidimensional array i/o. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <address> Washington, D.C., </address> <month> November </month> <year> 1994. </year>
Reference: [33] <author> Kent E. Seamons and Marianne Winslett. </author> <title> Physical schemas for large multidimensional arrays in scientific computing applications. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 218-227, </pages> <address> Char-lottesville, Virginia, </address> <month> September </month> <year> 1994. </year>
Reference: [34] <author> Kent E. Seamons and Marianne Winslett. </author> <title> A data management approach for handling large compressed arrays in high performance computing. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 119-128, </pages> <address> McLean, Virginia, </address> <month> February </month> <year> 1995. </year>
Reference: [35] <author> Kent E. Seamons. Panda: </author> <title> Fast Access to Persistent Arrays using High Level Interfaces and Server Directed Input/Output. </title> <type> PhD Thesis, </type> <institution> University of Illinois, </institution> <month> May </month> <year> 1996. </year>
Reference: [36] <author> C. Stunkel, D. Shea, B. Abali, M. Atkins, C. Bender, D. Grice, P. Hochschild, D. Joseph, B. Nathanson, R. Swetz, R. Stucke, M. Taso, and P. Varker. </author> <title> The SP2 communiation system. </title> <type> Technical Report, </type> <institution> IBM T.J. Watson Research Center and IBM Highly Parallel Supercomputing Systems Laboratory, Yorktown Heights and Kingston, </institution> <address> NY, </address> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: The Paragon has specialized I/O nodes and the file data is striped across the I/O nodes. The PFS provides additional access modes and allows control over the striping parameters. The IBM Vesta [9] file system is an experimental system targeted for the IBM SP1 and SP2 <ref> [36] </ref>. The data is interleaved across multiple I/O nodes. A file on the Vesta file system is a two dimensional structure with the file divided into physical partitions. The number of partitions is fixed at file creation time, but each partition can grow linearly.
Reference: [37] <author> R. Thakur, R. Bordawekar, and A. Choudhary. </author> <title> Compiler and runtime support for out-of-core HPF programs. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 382-391, </pages> <month> July </month> <year> 1994. </year> <month> 117 </month>
Reference: [38] <author> Rajeev Thakur, Rajesh Bordawekar, Alok Choudhary, Ravi Ponnusamy, and Tarvinder Singh. </author> <title> PASSION runtime library for parallel i/o. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year>
Reference: [39] <author> Rajeev Thakur and Alok Choudhary. </author> <title> Accessing sections of out-of-core arrays using an extended two-phase method. </title> <type> Technical Report SCCS-685, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <month> January </month> <year> 1995. </year>
Reference: [40] <author> Rajeev Thakur, Ewing Lusk, and William Gropp. </author> <title> I/O characterization of a portable astrophysics application on the IBM SP and Intel Paragon. </title> <type> Technical Report MCS-P534-0895, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1995. </year> <month> 118 </month>
References-found: 40

