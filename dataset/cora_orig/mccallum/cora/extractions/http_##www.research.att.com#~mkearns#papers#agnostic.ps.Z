URL: http://www.research.att.com/~mkearns/papers/agnostic.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Title: Toward Efficient Agnostic Learning  
Author: MICHAEL J. KEARNS ROBERT E. SCHAPIRE LINDA M. SELLIE Editor: Lisa Hellerstein 
Keyword: machine learning, agnostic learning, PAC learning, computational learning theory  
Address: 600 Mountain Avenue, Murray Hill, NJ 07974-0636  Chicago, Chicago, IL 60637  
Affiliation: AT&T Bell Laboratories,  Department of Computer Science, University of  
Note: Small Journal Name,  c 1992 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: mkearns@research.att.com  schapire@research.att.com  sellie@research.att.com  
Date: 9, 275-302 (1992)  
Abstract: In this paper we initiate an investigation of generalizations of the Probably Approximately Correct (PAC) learning model that attempt to significantly weaken the target function assumptions. The ultimate goal in this direction is informally termed agnostic learning, in which we make virtually no assumptions on the target function. The name derives from the fact that as designers of learning algorithms, we give up the belief that Nature (as represented by the target function) has a simple or succinct explanation. We give a number of positive and negative results that provide an initial outline of the possibilities for agnostic learning. Our results include hardness results for the most obvious generalization of the PAC model to an agnostic setting, an efficient and general agnostic learning method based on dynamic programming, relationships between loss functions for agnostic learning, and an algorithm for a learning problem that involves hidden variables. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aldous, D. & Vazirani, U. </author> <year> (1990). </year> <title> A Markovian extension of Valiant's learning model. </title> <booktitle> 31st Annual Symposium on Foundations of Computer Science (pp. </booktitle> <pages> 392-404). </pages>
Reference: <author> Blum, A. & Chalasani, P. </author> <year> (1992). </year> <title> Learning switching concepts. </title> <booktitle> Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (pp. </booktitle> <pages> 231-242). </pages>
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. </author> <year> (1989). </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36, </volume> <pages> 929-965. </pages>
Reference-contexts: The correct output for the instance S is the h fl 2 T that minimizes d S (h) = jfi : h (x i ) 6= b i gj over all h 2 T . It follows from standard arguments <ref> (Blumer, Ehrenfeucht, Haussler & Warmuth, 1989) </ref> that if the Vapnik-Chervonenkis dimension of T is polynomially bounded by the complexity parameter n, an algorithm that efficiently solves the disagreement minimization problem for T can be used as a subroutine by an efficient algorithm for learning T in the agnostic PAC model. (See
Reference: <author> Duda, R. O. & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley. </publisher>
Reference: <author> Dudley, R. M. </author> <year> (1978). </year> <title> Central limit theorems for empirical measures. </title> <journal> The Annals of Probability, </journal> <volume> 6, </volume> <pages> 899-929. </pages> <note> 302 M.J. </note> <author> KEARNS, R.E. SCHAPIRE AND L.M. SELLIE Freund, Y. </author> <year> (1990). </year> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> Proceedings of the Third Annual Workshop on Computational Learning Theory (pp. </booktitle> <pages> 202-216). </pages>
Reference-contexts: KEARNS, R.E. SCHAPIRE AND L.M. SELLIE 3. Thus, Q P is a subset of a (2d +3)-dimensional vector space of functions. Therefore, its pseudo dimension is at most 2d+3 <ref> (Dudley, 1978) </ref> (reproved by Haussler (1992, Theorem 4)). 4. By Theorem 5, this implies that the pseudo dimension of pw s (d+1) (Q P ) is at most s (d + 1)(2d + 4).
Reference: <author> Freund, Y. </author> <year> (1992). </year> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (pp. </booktitle> <pages> 391-398). </pages>
Reference: <author> Garey, M. & Johnson, D. </author> <year> (1979). </year> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <address> San Francisco: </address> <publisher> W. H. Freeman. </publisher>
Reference-contexts: Proof: Suppose to the contrary of the theorem's statement that there exists an efficient algorithm for the stated learning problem. We show how such an algorithm can be used probabilistically to solve the minimum set cover problem <ref> (Garey & Johnson, 1979) </ref> in polynomial time, thus implying that RP = NP.
Reference: <author> Haussler, D. </author> <year> (1992). </year> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100, </volume> <pages> 78-150. </pages>
Reference: <author> Helmbold, D. P. & Long, P. M. </author> <year> (1994). </year> <title> Tracking drifting concepts by minimizing disagreements. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> 27-45. </pages>
Reference: <author> Hoeffding, W. </author> <year> (1963). </year> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58, </volume> <pages> 13-30. </pages>
Reference: <author> Izenman, A. J. </author> <year> (1991). </year> <title> Recent developments in nonparametric density estimation. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 86, </volume> <pages> 205-224. </pages>
Reference: <author> Kearns, M. & Li, M. </author> <year> (1993). </year> <title> Learning in the presence of malicious errors. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22, </volume> <pages> 807-837. </pages>
Reference-contexts: demonstrate the equivalence of the problem of learning T in the agnostic PAC model and a natural combinatorial optimization problem based on T , the disagreement minimization problem for T , a problem known to be equivalent (up to constant approximation factors) to the problem of learning with malicious errors <ref> (Kearns & Li, 1993) </ref>. <p> of agnostic PAC learning that may not be directly covered by Theorem 1, we essentially interpret the result as negative evidence for hopes of efficient agnostic PAC learning algorithms, because previous results indicate that a fi (*) malicious error rate can be achieved for only the most limited classes T <ref> (Kearns & Li, 1993) </ref> (such as the class of symmetric functions on n boolean variables). Other results for agnostic PAC learning may be obtained via Theorem 1 and the previous work on learning in the presence of malicious errors.
Reference: <author> Kearns, M., Li, M., Pitt, L., & Valiant, L. </author> <year> (1987). </year> <title> On the learnability of Boolean formulae. </title> <booktitle> Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing (pp. </booktitle> <pages> 285-295). </pages>
Reference-contexts: If we retain the condition T = F but allow H F , we obtain the standard PAC model <ref> (Kearns et al., 1987) </ref>, where the hypothesis class may be more powerful than the target class.
Reference: <author> Kearns, M. & Valiant, L. G. </author> <year> (1994). </year> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 41, </volume> <pages> 67-95. </pages>
Reference: <author> Kearns, M. J. & Schapire, R. E. </author> <year> (1990). </year> <title> Efficient distribution-free learning of probabilistic concepts. </title> <booktitle> 31st Annual Symposium on Foundations of Computer Science (pp. </booktitle> <pages> 382-391). </pages> <note> To appear, Journal of Computer and System Sciences. </note>
Reference-contexts: Next, if A is the p-concept decomposition using a class F of p-concepts, T = F , and H F , then we obtain the p-concept learning model <ref> (Kearns & Schapire, 1990) </ref>, and there are at least two interesting choices of loss functions. <p> Note that if r &lt; 1=2 then the Bayes optimal is the constant function 0; otherwise, it is just the conjunction S. It has been shown <ref> (Kearns & Schapire, 1990) </ref> that we can approximate the Bayes optimal predictor by applying Valiant's (1984) algorithm for conjunctions to approximate the conjunction S, and by then estimating r using this approximation for S. <p> We will show that p f can be represented as a k-probabilistic decision list with increasing probabilities, a class of p-concepts for which there is known to exist an efficient algorithm for approximating the Bayes optimal predictor <ref> (Kearns & Schapire, 1990) </ref>. A similar technique is used by Blum and Chalasani (1992).
Reference: <author> Linial, N., Mansour, Y., & Nisan, N. </author> <year> (1993). </year> <title> Constant depth circuits, Fourier transform, and learnability. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 40, </volume> <pages> 607-620. </pages>
Reference: <author> Pitt, L. & Valiant, L. G. </author> <year> (1988). </year> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35, </volume> <pages> 965-984. </pages>
Reference: <author> Pollard, D. </author> <year> (1984). </year> <title> Convergence of Stochastic Processes. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Rissanen, J., Speed, T. P., & Yu, B. </author> <year> (1992). </year> <title> Density estimation by stochastic complexity. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38, </volume> <pages> 315-323. </pages>
Reference: <author> Schapire, R. E. </author> <year> (1990). </year> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 197-227. </pages>
Reference-contexts: Next, if A is the p-concept decomposition using a class F of p-concepts, T = F , and H F , then we obtain the p-concept learning model <ref> (Kearns & Schapire, 1990) </ref>, and there are at least two interesting choices of loss functions. <p> Note that if r &lt; 1=2 then the Bayes optimal is the constant function 0; otherwise, it is just the conjunction S. It has been shown <ref> (Kearns & Schapire, 1990) </ref> that we can approximate the Bayes optimal predictor by applying Valiant's (1984) algorithm for conjunctions to approximate the conjunction S, and by then estimating r using this approximation for S. <p> We will show that p f can be represented as a k-probabilistic decision list with increasing probabilities, a class of p-concepts for which there is known to exist an efficient algorithm for approximating the Bayes optimal predictor <ref> (Kearns & Schapire, 1990) </ref>. A similar technique is used by Blum and Chalasani (1992).
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27, </volume> <pages> 1134-1142. </pages>
Reference-contexts: 1. Introduction One of the major limitations of the Probably Approximately Correct (or PAC) learning model <ref> (Valiant, 1984) </ref> (and related models) is the strong assumptions placed on the so-called target function that the learning algorithm is attempting to approximate from examples. <p> First of all, if F is any class of boolean functions, A is the functional decomposition using F, H = T = F, and L is the prediction loss function Z, then we obtain the restricted PAC model <ref> (Valiant, 1984) </ref>, where the hypothesis class is the same as the target class. If we retain the condition T = F but allow H F , we obtain the standard PAC model (Kearns et al., 1987), where the hypothesis class may be more powerful than the target class.
Reference: <author> Valiant, L. G. </author> <year> (1985). </year> <title> Learning disjunctions of conjunctions. </title> <booktitle> Proceedings of the 9th International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 560-566). </pages>
Reference: <author> Vapnik, V. N. </author> <year> (1982). </year> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> White, H. </author> <year> (1989). </year> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 425-464. </pages>
Reference: <author> Yamanishi, K. </author> <year> (1992a). </year> <title> A learning criterion for stochastic rules. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 165-203. </pages>
Reference: <author> Yamanishi, K. </author> <year> (1992b). </year> <title> Learning nonparametric densities in terms of finite dimensional parametric hypotheses. </title> <journal> IEICE Transactions: D Information and Systems, </journal> <volume> E75D, </volume> <pages> 459-469. </pages>
References-found: 26

