URL: ftp://ftp.ensae.fr/pub/labo_stat/CPRobert/Recycled_AR.ps.gz
Refering-URL: http://www.stats.bris.ac.uk/MCMC/pages/list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Post-Processing Accept-Reject Samples: Recycling and Rescaling  
Author: George Casella Christian P. Robert 
Keyword: Key words and phrases Metropolis-Hastings algorithm, Rao-Black wellization, importance sampling, mean squared error.  
Date: May 29, 1996  
Affiliation: Cornell University  CREST, INSEE and Universite de Rouen  
Abstract: This paper proposes alternative methods for constructing estimators from accept-reject samples by incorporating the variables rejected by the algorithm. The resulting estimators are quick to compute, and turn out to be variations of importance sampling estimators, although their derivations are quite different. We show that these estimators are superior asymptotically to the classical accept-reject estimator, which ignores the rejected variables. In addition, we consider the issue of rescaling of estimators, a topic that has implications beyond accept-reject and importance sampling. We also show how rescaling can improve an estimator, and illustrate the domination of the standard importance sampling techniques in different setups. Extensions to MCMC algorithms are mentioned, although an analytical treatment seems quite involved. fl This research was supported by NSF Grant No. DMS93-05547 and NSF/CNRS Grant No. INT-9216784. We are thankful to Gerard N. Grancher for pointing out an error in an earlier version of this paper and to the city of Belgrade Lakes, Maine, for providing a convivial working environment in Summer 1995. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Casella, G. and Robert, C. P. </author> <year> (1996). </year> <title> Rao-Blackwellization of sampling schemes, </title> <journal> Biometrika, </journal> <pages> 83 81-94. </pages>
Reference-contexts: Indeed, accept-reject samples are based on the generation of ancillary uniform variables and are thus dependent on a randomization mechanism, which allows for improvement. A complete removal of ancillary variables can be done by conditioning and we show in Casella and Robert <ref> (1996) </ref> that global conditioning on the entire sample is feasible, although computationally costly. The alternative approaches considered in this paper are computationally feasible, and can be interpreted as partial conditioning. <p> In addition, it appears from the simulations that there is no visible loss in using termwise conditioning together with rescaling, since the mean squared errors of the alternative estimator are equivalent to those of the Rao-Blackwell estimator of Casella and Robert <ref> (1996) </ref>. This paper is organized as follows. Section 2 shows how improvement can be brought on the usual accept-reject estimator when the corresponding bound is too large, and Section 3 indicates how to take advantage of the rejected random variables through importance sampling. <p> One might think that the smaller values of % are the most likely to allow for an improvement. However, the zone where ffi 1 has a smaller variance than ffi AR is, surprisingly, the rightmost part of <ref> [0; 1] </ref>, including % = 1 since lim %(2; 1 %) = 1 and lim log (%)(2; 1 %) = 0 : This result suggests that small values of % indicate a wide difference between f and g, thus of a large variance of the factor f (y i )=g (y <p> Therefore, the convex combination ffi 2w should not be used as the main estimate in such settings. Since accept-reject does not take into account the rejected random variables, there is room for improvement, as shown by Casella and Robert <ref> (1996) </ref> through Rao-Blackwellization. However, the Rao-Blackwell estimate cannot be easily implemented in large sample settings because of space and time requirements, as it involves complex permutations of the overall sample. <p> A Rao-Blackwellized estimate <ref> (Casella and Robert 1996) </ref> was obtained by integrating the accept-reject estimate conditionally on the informative part, namely y 1 ; : : : ; y N ; N . <p> Although domination in simulation theory can be a murky subject (see Casella and Robert <ref> (1996) </ref> for a discussion), the form of the dominating estimator is so simple that one may wonder about the practical value of such an improvement both in terms of the number of simula tions required to achieve domination and of the magnitude of the improvement. <p> Given the original motivation for using ffi CAR and ffi IAR , one may wonder about the relation between these estimators and the full conditional expectation ffi RB = n i=1 of Casella and Robert <ref> (1996) </ref> in terms of variance reduction.
Reference: 2. <author> Devroye, L. </author> <year> (1985). </year> <title> Non-Uniform Random Variate Generation. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: 3. <author> Gelfand, A. and Smith, A. F. M. </author> <year> (1990). </year> <title> Sampling based approaches to calculating marginal densities. </title> <journal> Journal of the American Statistical Society, </journal> <volume> 85, </volume> <year> 1990, </year> <pages> p. 398-409. </pages>
Reference: 4. <author> Liu, J. S., Wong, W. H. and Kong, A. </author> <year> (1994). </year> <title> Covariance structure of the Gibbs sampler with applications to the comparison of estimators and augmentation schemes, </title> <journal> Biometrika, </journal> <volume> 81, </volume> <year> 1994, </year> <pages> 27-40. </pages>
Reference-contexts: We therefore consider improvements of the form ffi fl = n + t i=1 where the weight ! i only depends on the current observation z i . This form of estimator is similar to that treated by Liu, Wong, and Kong <ref> (1994, 1995) </ref> in a Gibbs sampling setup. There, the termwise weighting arose naturally through the calculation of conditional expectations.
Reference: 5. <author> Liu, J. S., Wong, W. H. and Kong, A. </author> <year> (1995). </year> <title> Correlation structure and convergence rate of the Gibbs sampler with various scans. </title> <journal> Journal of the Royal Statistical Society 57, </journal> <pages> 157-169. </pages>
Reference: 6. <author> Ripley, B. </author> <year> (1987). </year> <title> Stochastic Simulation. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: 7. <author> Robert, C. P. </author> <year> (1994). </year> <title> The Bayesian Choice. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: 8. <author> Rubinstein, R.Y. </author> <title> (1981) Simulation and the Monte Carlo Method. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: This major difficulty is common to all importance sampling schemes and prohibits uniform domination results; it appears in the following sections as well. (Note that optimality results such as those found in Rubinstein <ref> (1981) </ref> about the optimal choice of g lead to g = f in the case of constant functions.) The solution to this drawback is to force the estimators to estimate correctly constant functions.
Reference: 9. <author> Tanner, M. </author> <title> (1991) Tools for Statistical Inference: Observed Data and Data Augmentation Methods. </title> <booktitle> Lecture Notes in Statistics 67. </booktitle> <address> New York: </address> <publisher> Springer-Verlag. </publisher>

Reference: 1. <institution> Generate Z ~ g, U ~ U [0;1] </institution> ; 
Reference-contexts: One might think that the smaller values of % are the most likely to allow for an improvement. However, the zone where ffi 1 has a smaller variance than ffi AR is, surprisingly, the rightmost part of <ref> [0; 1] </ref>, including % = 1 since lim %(2; 1 %) = 1 and lim log (%)(2; 1 %) = 0 : This result suggests that small values of % indicate a wide difference between f and g, thus of a large variance of the factor f (y i )=g (y

References-found: 10

