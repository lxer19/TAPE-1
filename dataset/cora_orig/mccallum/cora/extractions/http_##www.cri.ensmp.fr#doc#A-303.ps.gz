URL: http://www.cri.ensmp.fr/doc/A-303.ps.gz
Refering-URL: http://www.cri.ensmp.fr/rapports.html
Root-URL: 
Title: Interprocedural Analyses of Fortran Programs  
Author: Beatrice Creusillet and Fran~cois Irigoin 
Address: 35, rue Saint-Honore, 77305 Fontainebleau cedex, France  
Affiliation: Centre de recherche en informatique, Ecole des mines de Paris  
Abstract: Interprocedural analyses (IPA) are becoming more and more common in commercial compilers. But research on the analysis of Fortran programs is still going on, as a number of problems are not yet satisfactorily solved and others are emerging with new language dialects. This paper presents a survey of the main interprocedural analysis techniques, with an emphasis on the suitability of the analysis framework for the characteristics of the original semantic problem. Our experience with the pips interprocedural compiler workbench is then described. pips includes a make-like mechanism, PipsMake, which takes care of the interleavings between top-down and bottom-up analyses and allows a quick prototyping of new interprocedural analyses. Intensive summarization is used to reduce storage requirements and achieve reasonable analysis times when dealing with real-life applications. The speed/accuracy tradeoffs made for pips are discussed in the light of other interprocedural tools. Key words: Interprocedural analysis, parallelization, re-engineering, pips. fl E-mail: fcreusillet,irigoing@cri.ensmp.fr 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agrawal and J. Saltz. </author> <title> Interprocedural partial redundancy elimination and its application to distributed memory compilation. </title> <booktitle> In International Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 258-269, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: But research prototypes are still ahead, in particular for symbolic analyses [40, 54] and flow sensitive array region analyses [37, 42, 78, 31] which are mainly used for array privatization in parallelizing tools. Also, the compilation of fortran dialects such as hpf raises new interproce-dural problems <ref> [1, 44, 9] </ref>. 1.5 Conclusion A wide variety of interprocedural frameworks exists. They are more or less adapted to a specific interprocedural problem.
Reference: [2] <author> F. E. Allen. </author> <title> Interprocedural data flow analysis. </title> <booktitle> In Proceedings of the IFIP Congress, </booktitle> <pages> pages 398-402, </pages> <year> 1974. </year>
Reference-contexts: They are also more and more necessary for sequential machines. Interprocedural techniques have been introduced to cope with this problem. To our knowledge, the oldest article about interprocedural analyses was written more than twenty years ago by Allen <ref> [2] </ref>. Since then, research has focused on three aspects of interprocedural techniques: Frameworks (how to perform these analyses), semantic problems (what information to propagate), and the representation of problem solutions, which is critical because the amount of gathered information is much larger than with intraprocedural analyses.
Reference: [3] <author> B. Alpern, M.N. Wegman, and F.K. Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-11, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: In this last case, the size of the representation of each procedure is reduced. Several sparse interprocedural representations have been designed for particular classes of problems, such as the program summary graph [19], the system dependence graph [34, 10], : : : Intraprocedural sparse representations include the ssa form <ref> [3, 32, 76] </ref> and the sparse data flow evaluation graph [24]. The unified interprocedural graph [48] provides a demand-driven unified representation which combines the advantages of the sparse representations without restricting the scope of the possible analyses.
Reference: [4] <author> M. Alt and F. Martin. </author> <title> Generation of efficient interprocedural analyzers with PAG. </title> <booktitle> In Static Analysis Symposium, Lecture Notes in Computer Science, </booktitle> <pages> pages 33-50. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: In particular, the speed/accuracy ratio is still an issue, but has not yet been extensively studied [38]. For that purpose, more generic tools for the experimentation of interprocedural analyses, such as fiat [46], pag <ref> [4] </ref>, or pips, should be made available. Many issues linked to interprocedural analyses have been dealt with by pips, the in-terprocedural Fortran 77 source-to-source parallelizer developed at Ecole des mines. It supports a comprehensive set of interprocedural analyses, each with a large number of accuracy options.
Reference: [5] <author> American National Standard Institute. </author> <title> Programming Language FORTRAN, ANSI X3.9-1978, </title> <type> ISO 1539-1980, </type> <year> 1983. </year>
Reference-contexts: Effects, transformers, preconditions and array region analyses are described below. No alias analysis is performed, because the fortran 77 standard forbids interprocedural aliasing of modified variables (see <ref> [5] </ref>, paragraph 15.9.3.6). Memory Effects This abstraction describes the memory operations performed by statements or procedures.
Reference: [6] <author> C. Ancourt, F. Coelho, B. Creusillet, and R. Keryell. </author> <title> How to add a new phase in PIPS: The case of dead code elimination. </title> <booktitle> In Sixth International Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 19-30, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: The latest developments essentially focus on these program transformations: Array privatization for loops and procedures (see Section 2.3); or partial evaluation, dead code elimination <ref> [6] </ref> and code restructuring based on preconditions or use-def chains, for example. The recursive application of transformations and their interaction with the analyses on which they rely, creates a new problem in an interprocedural setup.
Reference: [7] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In International Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 41-53, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: They range from constant propagation [18, 36, 10, 40, 23], to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others <ref> [21, 7, 50, 57] </ref>. Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler.
Reference: [8] <author> J. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1979. </year> <month> 25 </month>
Reference-contexts: Two main criteria have been proposed to classify interprocedural problems: Flow sensitivity and context sensitivity (also called path specificity ) <ref> [8, 64, 60] </ref>. An interprocedural problem is said to be flow sensitive if it cannot be solved precisely without considering the internal control flows of procedures. The above mentioned may be modified problem is flow insensitive, while the must be modified problem is flow sensitive.
Reference: [9] <author> S. Benkner and H.P. Brezany, P.and Zima. </author> <title> Processing array statements and procedure interfaces in the PREPARE High Performance Fortran compiler. </title> <booktitle> In International Conference on Compiler Construction, Lecture Notes in Computer Science, </booktitle> <pages> pages 324-338, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: But research prototypes are still ahead, in particular for symbolic analyses [40, 54] and flow sensitive array region analyses [37, 42, 78, 31] which are mainly used for array privatization in parallelizing tools. Also, the compilation of fortran dialects such as hpf raises new interproce-dural problems <ref> [1, 44, 9] </ref>. 1.5 Conclusion A wide variety of interprocedural frameworks exists. They are more or less adapted to a specific interprocedural problem.
Reference: [10] <author> D. Binkley. </author> <title> Interprocedural constant propagation using dependence graphs and a data-flow model. </title> <booktitle> In International Conference on Compiler Construction, </booktitle> <pages> pages 374-388, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In this last case, the size of the representation of each procedure is reduced. Several sparse interprocedural representations have been designed for particular classes of problems, such as the program summary graph [19], the system dependence graph <ref> [34, 10] </ref>, : : : Intraprocedural sparse representations include the ssa form [3, 32, 76] and the sparse data flow evaluation graph [24]. The unified interprocedural graph [48] provides a demand-driven unified representation which combines the advantages of the sparse representations without restricting the scope of the possible analyses. <p> To enhance the results of these analyses, and to enable other useful code transformations 6 Recursivity and formal procedures are not often used in scientific programs. 9 such as partial redundancy elimination [63], many other interprocedural scalar analyses have also been introduced. They range from constant propagation <ref> [18, 36, 10, 40, 23] </ref>, to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others [21, 7, 50, 57].
Reference: [11] <author> W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoeflinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, B. Pottenger, L. Rauchwerger, and P. Tu. </author> <title> Parallel programming with Polaris. </title> <journal> Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Many flow sensitive analyses are still too complex for commercial products, which only implement flow-insensitive interprocedural analyses. But as good experimental results are published <ref> [43, 11] </ref>, and as the power and the memory sizes of computers increase, such analyses will undoubtedly appear in commercial tools. Not long ago, even flow insensitive array region analyses were considered too time and space consuming. <p> tradeoff in the accuracy/speed space and that their potential has not yet been measured. 2.5 PIPS and Other Research Tools Many research Fortran optimizing tools include some kind of interprocedural analyses, but the closest ones to pips certainly are fiat/suif [46, 47], ParaScope [26] and the D system [44], Polaris <ref> [11] </ref>, Parafrase-2 [68], and Panorama [65, 37, 38]. FIAT/SUIF suif is an intraprocedural compiler for parallel machines developed at Stanford. <p> But this effort must be pursued, in particular for interdependent analyses which may override the existing classification. The current interprocedural frameworks for scientific programming in Fortran are surveyed in this paper. Few experimental results about effectiveness <ref> [51, 43, 11] </ref> have been published and additional research in the interprocedural analysis area is necessary before a really formal comparison can be made. In particular, the speed/accuracy ratio is still an issue, but has not yet been extensively studied [38].
Reference: [12] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Introduction Real life applications are split into several procedures to factorize code as much as possible and to improve readability. Traditional compilers only provide separate compilation, and procedure boundaries prevent many analyses and optimizations, such as constant propagation for instance, although they are of paramount importance in par-allelizing compilers <ref> [12, 66] </ref> to achieve reasonable performances. They are also more and more necessary for sequential machines. Interprocedural techniques have been introduced to cope with this problem. To our knowledge, the oldest article about interprocedural analyses was written more than twenty years ago by Allen [2].
Reference: [13] <author> W. Blume and R. Eigenmann. </author> <title> The range test: A dependence test for symbolic, nonlinear expressions. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 528-537, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: They range from constant propagation [18, 36, 10, 40, 23], to subexpression availability and variable values [47], ranges <ref> [13] </ref>, or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others [21, 7, 50, 57].
Reference: [14] <author> W. Blume and R. Eigenmann. </author> <title> Symbolic analysis techniques needed for the effective parallelization of the Perfect Benchmarks. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Representation independence could be used to move pips away from its original linear algebra framework and into polynomial representations as in Parafrase-2 (see next section) or into Presburger-based representations. Such a move has been shown necessary in <ref> [14] </ref>, but we feel that linear algebra and polyhedra are an interesting tradeoff in the accuracy/speed space and that their potential has not yet been measured. 2.5 PIPS and Other Research Tools Many research Fortran optimizing tools include some kind of interprocedural analyses, but the closest ones to pips certainly are
Reference: [15] <author> M. Burke. </author> <title> An interval-based approach to exhaustive and incremental interprocedu-ral data-flow analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 341-395, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The rationale of these approaches is to avoid computing irrelevant intermediate results, while still performing a global analysis. Reducing the cost of interprocedural data flow analyses can also be achieved by demand-driven techniques, such as those presented in [33] and [69]. Incremental analysis <ref> [15, 59, 75] </ref> addresses the problem of avoiding unnecessary recomputations of data flow solutions in case of local program changes; but it requires an initial exhaustive solution.
Reference: [16] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In ACM SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 162-175, </pages> <month> July </month> <year> 1986. </year> <month> 26 </month>
Reference-contexts: And finally, the translation mechanisms may reduce the efficiency of subsequent intraprocedural analyses, for example by generating nonlinear terms in array subscript expressions <ref> [16] </ref>. Partial inlining is more suitable to actually optimize programs. User-controlled inlining is usually available in compilers. Another solution is to let the compiler automatically inline some functions or some call sites, according to a heuristic. <p> At first, the main purpose was therefore to analyze interprocedural dependences. And alias analysis <ref> [16, 29, 72, 62] </ref> as well as summary data flow information or inter-procedural side-effects (sdfi) [28] on scalar variables were among the main analyses.
Reference: [17] <author> Michael Burke and Linda Torczon. </author> <title> Interprocedural optimization: Eliminating unnec-essary recompilation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(3) </volume> <pages> 367-399, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: A make-like mechanism, called PipsMake, automatically determines the order in which the different phases must be scheduled for the different procedures to have the resources ready when needed initially or after some minor changes of sources <ref> [17] </ref> or analysis options. The ordering is automatically deduced from a specification file, written by the phase designers, using a call-by-need interpretation.
Reference: [18] <author> C. Callahan, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In ACM SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 152-161, </pages> <year> 1986. </year>
Reference-contexts: To enhance the results of these analyses, and to enable other useful code transformations 6 Recursivity and formal procedures are not often used in scientific programs. 9 such as partial redundancy elimination [63], many other interprocedural scalar analyses have also been introduced. They range from constant propagation <ref> [18, 36, 10, 40, 23] </ref>, to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others [21, 7, 50, 57]. <p> Transformer analysis is a backward analysis, and its results are mainly used by two other types of analyses: preconditions and array region analyses. Transformers are called return functions in <ref> [18] </ref>. Preconditions Many program analyses and transformations can be improved when the relations between variable values are known. This is the case for loop parallelization which benefits from the knowledge of loop bound values, as well as array subscript symbolic constants. <p> The information available in initialization routines is moved upwards to the main procedure using transformers, and propagated downwards towards the leaf procedures by preconditions. The whole analysis is thus completed in two traversals of the call graph. 16 Summary preconditions are similar to jump functions <ref> [18] </ref>. Array Region Analyses Effects often fail to give precise information about the way array element sets are used and defined by procedures. And this lack of accuracy often prevents subsequent program transformations such as loop parallelization.
Reference: [19] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data-flow analysis. </title> <booktitle> International Conference on Programming Language Design and Implementation, ACM SIGPLAN Notices, </booktitle> <volume> 23(7) </volume> <pages> 47-56, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: This can be done either at the inter-procedural level, or even at the intraprocedural level. In this last case, the size of the representation of each procedure is reduced. Several sparse interprocedural representations have been designed for particular classes of problems, such as the program summary graph <ref> [19] </ref>, the system dependence graph [34, 10], : : : Intraprocedural sparse representations include the ssa form [3, 32, 76] and the sparse data flow evaluation graph [24].
Reference: [20] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4), </volume> <month> Winter </month> <year> 1988. </year>
Reference-contexts: In 1991, the convex Application Compiler was the first on the market [58]; it includes several complex analyses, such as interprocedural pointer tracking or array section analysis. The acknowledged origins for this compiler have to be found in several research projects: ptran [72], ParaScope <ref> [20] </ref>, Parafrase-2 [68], and pat [74]. Other commercial products, such as the product line developed by Applied Parallel Research, the xlf compiler from ibm (option -qipa), or the sgi, foresys, kap, and vast compilers, have since included various interprocedural analyses. But most interprocedural techniques are still in the research domain.
Reference: [21] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(10) </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference-contexts: However there are only two possible paths, namely E-R and E'-R'. Of course, these two criteria must be taken into account when chosing or implementing an interprocedural analysis framework. 1.3 Interprocedural Analysis Frameworks Many interprocedural analysis frameworks have been defined (some examples can be found in <ref> [64, 21, 46, 79] </ref>). They differ in several ways: The degree of summarization and the accuracy of the translation across procedure calls are two important issues, and directly depend on the specific semantic problem and the representation of its solutions. <p> They range from constant propagation [18, 36, 10, 40, 23], to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others <ref> [21, 7, 50, 57] </ref>. Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler. <p> Parascope and the D System Parascope and the D system are being both developed at Rice University. They use fiat as an interprocedural engine, like fiat/suif. Parascope provides several interprocedural analyses: mod/ref analysis (i.e. effects), array region analyses based on regular section descriptors (RSD) <ref> [21] </ref>, alias analysis, constant propagation and symbolic value analysis. The D system is built in the context of Parascope, and aims at compiling Fortran D programs. Fortran D is an hpf-like dialect, and requires specific interprocedural analyses, such as reaching decomposition or overlap analyses.
Reference: [22] <author> P. Carini. </author> <title> Automatic inlining. </title> <type> Research Report RC 10286, </type> <institution> IBM, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Another solution is to let the compiler automatically inline some functions or some call sites, according to a heuristic. The most advanced compilers seem to be the convex Application Compiler and the ibm xlf compiler. However, the best heuristics are usually based on results from previous interprocedural phases <ref> [22] </ref>. 1 This occurs when an array is not similarly declared in the caller and the callee. 2 Before any subsequent optimization such as dead code elimination. 4 A second approach 3 , is to compute summaries of the procedure behaviors, to trans-late them as accurately as possible into the callers'
Reference: [23] <author> P. Carini. </author> <title> Flow-sensitive interprocedural constant propagation. </title> <institution> Research report RC 20290, IBM, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: To enhance the results of these analyses, and to enable other useful code transformations 6 Recursivity and formal procedures are not often used in scientific programs. 9 such as partial redundancy elimination [63], many other interprocedural scalar analyses have also been introduced. They range from constant propagation <ref> [18, 36, 10, 40, 23] </ref>, to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others [21, 7, 50, 57].
Reference: [24] <author> J. Choi, R. Cytron, and J Ferrante. </author> <title> Automatic construction of sparse data flow evaluation graphs. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 55-66, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Several sparse interprocedural representations have been designed for particular classes of problems, such as the program summary graph [19], the system dependence graph [34, 10], : : : Intraprocedural sparse representations include the ssa form [3, 32, 76] and the sparse data flow evaluation graph <ref> [24] </ref>. The unified interprocedural graph [48] provides a demand-driven unified representation which combines the advantages of the sparse representations without restricting the scope of the possible analyses. The rationale of these approaches is to avoid computing irrelevant intermediate results, while still performing a global analysis.
Reference: [25] <author> F. Coelho. </author> <title> Compilation of I/O communications for HPF. </title> <booktitle> In Frontiers'95, </booktitle> <pages> pages 102-109, </pages> <month> February </month> <year> 1995. </year> <note> Available via http://www.cri.ensmp.fr/~coelho. 27 </note>
Reference-contexts: They are mainly used by the dependence analysis phase, but also by hpfc, the pips hpf compiler <ref> [25] </ref>.
Reference: [26] <author> K. Cooper, M. Hall, R. Hood, K. Kennedy, K. McKinley, J. Mellor-Crummey, L. Torc--zon, and S. Waren. </author> <title> The Parascope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2), </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: linear algebra and polyhedra are an interesting tradeoff in the accuracy/speed space and that their potential has not yet been measured. 2.5 PIPS and Other Research Tools Many research Fortran optimizing tools include some kind of interprocedural analyses, but the closest ones to pips certainly are fiat/suif [46, 47], ParaScope <ref> [26] </ref> and the D system [44], Polaris [11], Parafrase-2 [68], and Panorama [65, 37, 38]. FIAT/SUIF suif is an intraprocedural compiler for parallel machines developed at Stanford.
Reference: [27] <author> K. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In IEEE International Conference on Computer Language, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: However, much precision may be lost, due to (1) summarization techniques, (2) translation processes, and (3) simplification of the program structure representation. Between these two approaches, a recent technique lessens the drawbacks of inter-procedural analysis, but preserves a reasonable complexity. Selective cloning <ref> [41, 27] </ref> duplicates procedures on the basis of calling contexts. For example, a heuristic based on integer variable values has proved useful in an automatic parallelizer, without increasing the program size dramatically [41]. <p> However, this may result in an exponential growth of the number of solutions at each node, and thus in overwhelming execution times and memory requirements. Selective cloning <ref> [41, 27] </ref> also appears as a partial solution, since it reduces the number of unrealizable paths taken into account. From our experience with the pips parallelizer, combining results from several analyses can also lessen the problem. This is the case with preconditions and transformers in pips (see Section 2.3).
Reference: [28] <author> K. D. Cooper and K. Kennedy. </author> <title> Efficient computation of flow insensitive interprocedural summary information. </title> <booktitle> In ACM SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 247-258, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: At first, the main purpose was therefore to analyze interprocedural dependences. And alias analysis [16, 29, 72, 62] as well as summary data flow information or inter-procedural side-effects (sdfi) <ref> [28] </ref> on scalar variables were among the main analyses.
Reference: [29] <author> K. D. Cooper and K. Kennedy. </author> <title> Fast interprocedural alias analysis. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 49-59, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: At first, the main purpose was therefore to analyze interprocedural dependences. And alias analysis <ref> [16, 29, 72, 62] </ref> as well as summary data flow information or inter-procedural side-effects (sdfi) [28] on scalar variables were among the main analyses.
Reference: [30] <author> B. Creusillet. </author> <title> Array Region Analyses and Applications. </title> <type> PhD thesis, </type> <institution> Ecole des mines de Paris, </institution> <month> December </month> <year> 1996. </year> <note> Available at http://www.cri.ensmp.fr/doc/A-295.ps.gz. </note>
Reference-contexts: And this lack of accuracy often prevents subsequent program transformations such as loop parallelization. To obtain better accuracy, and to enable advanced program transformations such as array privatization, several flow sensitive array region analyses have been implemented in pips <ref> [31, 30] </ref>. read and write regions represent the effects of statements and procedures on array elements as sets. They are mainly used by the dependence analysis phase, but also by hpfc, the pips hpf compiler [25]. <p> They are also used to privatize array regions in loops <ref> [30] </ref>. New types of program transformations based on in and out regions are currently being implemented, such as the re-engineering of procedure declarations. This transformation spots global arrays used as temporary variables in procedures and reallocates them as stack-allocated local variables. <p> Handling such problems involves an extension of PipsMake to deal with the fix points of analysis and transformation phases. Also, recent developments in the array region area have shown that representation independence could be improved <ref> [30] </ref>. For instance, different array region representations such as intervals, convex polyhedra, list of convex polyhedra or Presburger formulae, have been made usable by a generic interprocedural array region engine.
Reference: [31] <author> B. Creusillet and F. Irigoin. </author> <title> Interprocedural array region analyses. </title> <journal> International Journal of Parallel Programming (special issue on LCPC), </journal> <volume> 24(6) </volume> <pages> 513-546, </pages> <year> 1996. </year>
Reference-contexts: Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler. But research prototypes are still ahead, in particular for symbolic analyses [40, 54] and flow sensitive array region analyses <ref> [37, 42, 78, 31] </ref> which are mainly used for array privatization in parallelizing tools. Also, the compilation of fortran dialects such as hpf raises new interproce-dural problems [1, 44, 9]. 1.5 Conclusion A wide variety of interprocedural frameworks exists. They are more or less adapted to a specific interprocedural problem. <p> And this lack of accuracy often prevents subsequent program transformations such as loop parallelization. To obtain better accuracy, and to enable advanced program transformations such as array privatization, several flow sensitive array region analyses have been implemented in pips <ref> [31, 30] </ref>. read and write regions represent the effects of statements and procedures on array elements as sets. They are mainly used by the dependence analysis phase, but also by hpfc, the pips hpf compiler [25].
Reference: [32] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> Efficiently computing static single assignement form and the control dependence graph. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 25-35, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: In this last case, the size of the representation of each procedure is reduced. Several sparse interprocedural representations have been designed for particular classes of problems, such as the program summary graph [19], the system dependence graph [34, 10], : : : Intraprocedural sparse representations include the ssa form <ref> [3, 32, 76] </ref> and the sparse data flow evaluation graph [24]. The unified interprocedural graph [48] provides a demand-driven unified representation which combines the advantages of the sparse representations without restricting the scope of the possible analyses.
Reference: [33] <author> E. Duesterwald, R. Gupta, and M. L. Soffa. </author> <title> Demand-driven computation of interproce-dural data-flow. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 37-48, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The rationale of these approaches is to avoid computing irrelevant intermediate results, while still performing a global analysis. Reducing the cost of interprocedural data flow analyses can also be achieved by demand-driven techniques, such as those presented in <ref> [33] </ref> and [69]. Incremental analysis [15, 59, 75] addresses the problem of avoiding unnecessary recomputations of data flow solutions in case of local program changes; but it requires an initial exhaustive solution.
Reference: [34] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <year> 1987. </year>
Reference-contexts: In this last case, the size of the representation of each procedure is reduced. Several sparse interprocedural representations have been designed for particular classes of problems, such as the program summary graph [19], the system dependence graph <ref> [34, 10] </ref>, : : : Intraprocedural sparse representations include the ssa form [3, 32, 76] and the sparse data flow evaluation graph [24]. The unified interprocedural graph [48] provides a demand-driven unified representation which combines the advantages of the sparse representations without restricting the scope of the possible analyses.
Reference: [35] <author> S. Graham, S. Lucco, and O. Sharp. </author> <title> Orchestrating interactions among parallel compu-tations. </title> <booktitle> In International Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 100-111, </pages> <month> June </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices. </journal>
Reference-contexts: The control structure of the code and comments are preserved as long as possible. However, the use of summarization, to keep summaries small, combined with a pure interprocedural framework without cloning often prevents interprocedural path differentiation, whereas other parallelizers <ref> [35, 42] </ref> partially achieve this goal through automatic selective procedure cloning. Since pips is an interactive tool, manual cloning was tried and more accurate results were obtained, especially for forward interprocedural analyses, such as preconditions and out regions, and for intraprocedural transformations.
Reference: [36] <author> D. Grove and L. Torczon. </author> <title> Interprocedural constant propagation: A study of jump functions implementations. </title> <booktitle> In International Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 90-99, </pages> <month> June </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices. </journal>
Reference-contexts: To enhance the results of these analyses, and to enable other useful code transformations 6 Recursivity and formal procedures are not often used in scientific programs. 9 such as partial redundancy elimination [63], many other interprocedural scalar analyses have also been introduced. They range from constant propagation <ref> [18, 36, 10, 40, 23] </ref>, to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others [21, 7, 50, 57].
Reference: [37] <author> J. Gu, Z. Li, and G. Lee. </author> <title> Symbolic array dataflow analysis for array privatization and program parallelization. </title> <booktitle> In Supercomputing, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler. But research prototypes are still ahead, in particular for symbolic analyses [40, 54] and flow sensitive array region analyses <ref> [37, 42, 78, 31] </ref> which are mainly used for array privatization in parallelizing tools. Also, the compilation of fortran dialects such as hpf raises new interproce-dural problems [1, 44, 9]. 1.5 Conclusion A wide variety of interprocedural frameworks exists. They are more or less adapted to a specific interprocedural problem. <p> This transformation spots global arrays used as temporary variables in procedures and reallocates them as stack-allocated local variables. It is useful for the maintenance and parallelization of dusty deck programs. 9 in regions are similar to upward exposed read regions computed by others <ref> [72, 42, 37, 78] </ref>. 10 out regions are different from downward exposed regions, because they also depend on the future of the computation. 17 2.4 Recent and Planned Developments We have only described some of the many analyses available in pips. <p> and that their potential has not yet been measured. 2.5 PIPS and Other Research Tools Many research Fortran optimizing tools include some kind of interprocedural analyses, but the closest ones to pips certainly are fiat/suif [46, 47], ParaScope [26] and the D system [44], Polaris [11], Parafrase-2 [68], and Panorama <ref> [65, 37, 38] </ref>. FIAT/SUIF suif is an intraprocedural compiler for parallel machines developed at Stanford.
Reference: [38] <author> J. Gu, Z. Li, and G. Lee. </author> <title> Experience with efficient data flow analysis for array privatiza-tion. </title> <booktitle> In Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 157-167, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: and that their potential has not yet been measured. 2.5 PIPS and Other Research Tools Many research Fortran optimizing tools include some kind of interprocedural analyses, but the closest ones to pips certainly are fiat/suif [46, 47], ParaScope [26] and the D system [44], Polaris [11], Parafrase-2 [68], and Panorama <ref> [65, 37, 38] </ref>. FIAT/SUIF suif is an intraprocedural compiler for parallel machines developed at Stanford. <p> Few experimental results about effectiveness [51, 43, 11] have been published and additional research in the interprocedural analysis area is necessary before a really formal comparison can be made. In particular, the speed/accuracy ratio is still an issue, but has not yet been extensively studied <ref> [38] </ref>. For that purpose, more generic tools for the experimentation of interprocedural analyses, such as fiat [46], pag [4], or pips, should be made available. Many issues linked to interprocedural analyses have been dealt with by pips, the in-terprocedural Fortran 77 source-to-source parallelizer developed at Ecole des mines.
Reference: [39] <author> R. Gupta, L. Pollock, and M. L. Soffa. </author> <title> Parallelizing data flow analysis. </title> <booktitle> In Workshop on Parallel Compilation, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Another solution to reduce the cost of interprocedural analyses is to perform 5 Array region analyses collect information about the way array elements are used and defined by the program. 8 them in parallel <ref> [39, 56] </ref>. When the representation of the program interprocedural control flow is approximative, and when the problem is context sensitive, the ability to avoid taking into account unrealizable paths [55, 70] is an issue.
Reference: [40] <author> M. Haghighat. </author> <title> Symbolic Analysis for Parallelizing Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: To enhance the results of these analyses, and to enable other useful code transformations 6 Recursivity and formal procedures are not often used in scientific programs. 9 such as partial redundancy elimination [63], many other interprocedural scalar analyses have also been introduced. They range from constant propagation <ref> [18, 36, 10, 40, 23] </ref>, to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others [21, 7, 50, 57]. <p> Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler. But research prototypes are still ahead, in particular for symbolic analyses <ref> [40, 54] </ref> and flow sensitive array region analyses [37, 42, 78, 31] which are mainly used for array privatization in parallelizing tools. Also, the compilation of fortran dialects such as hpf raises new interproce-dural problems [1, 44, 9]. 1.5 Conclusion A wide variety of interprocedural frameworks exists. <p> Parafrase-2 Parafrase-2 is an optimizing and parallelizing source-to-source compiler developed at the University of Illinois at Urbana-Champaign. It includes many phases, some of which are interprocedural, especially a powerful symbolic analysis based on polynomials <ref> [40] </ref>. Polaris Polaris too is being developed at the University of Illinois. Most Polaris phases are intraprocedural and require inline expansion to parallelize loops efficiently.
Reference: [41] <author> M. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, Texas, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The main advantage is that usual intraprocedural analyses 3 and transformations can be applied on the resulting code, hopefully with the same precision. However, there are several drawbacks <ref> [41, 61] </ref>: 1. Inlining is not always possible, due to array reshaping 1 , or recursive procedures in languages other than fortran 77. 2. The resulting code is usually much larger than the original 2 . <p> However, much precision may be lost, due to (1) summarization techniques, (2) translation processes, and (3) simplification of the program structure representation. Between these two approaches, a recent technique lessens the drawbacks of inter-procedural analysis, but preserves a reasonable complexity. Selective cloning <ref> [41, 27] </ref> duplicates procedures on the basis of calling contexts. For example, a heuristic based on integer variable values has proved useful in an automatic parallelizer, without increasing the program size dramatically [41]. <p> Selective cloning [41, 27] duplicates procedures on the basis of calling contexts. For example, a heuristic based on integer variable values has proved useful in an automatic parallelizer, without increasing the program size dramatically <ref> [41] </ref>. However, even if this approach alleviates some precision problems (see Section 1.3), its decision heuristics are usually based on another interprocedural information, and subsequent program analyses and transformations may require an interprocedural propagation of information. <p> However, this may result in an exponential growth of the number of solutions at each node, and thus in overwhelming execution times and memory requirements. Selective cloning <ref> [41, 27] </ref> also appears as a partial solution, since it reduces the number of unrealizable paths taken into account. From our experience with the pips parallelizer, combining results from several analyses can also lessen the problem. This is the case with preconditions and transformers in pips (see Section 2.3).
Reference: [42] <author> M. Hall, S. Amarasinghe, B. Murphy, S.-W. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Supercomputing, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler. But research prototypes are still ahead, in particular for symbolic analyses [40, 54] and flow sensitive array region analyses <ref> [37, 42, 78, 31] </ref> which are mainly used for array privatization in parallelizing tools. Also, the compilation of fortran dialects such as hpf raises new interproce-dural problems [1, 44, 9]. 1.5 Conclusion A wide variety of interprocedural frameworks exists. They are more or less adapted to a specific interprocedural problem. <p> This transformation spots global arrays used as temporary variables in procedures and reallocates them as stack-allocated local variables. It is useful for the maintenance and parallelization of dusty deck programs. 9 in regions are similar to upward exposed read regions computed by others <ref> [72, 42, 37, 78] </ref>. 10 out regions are different from downward exposed regions, because they also depend on the future of the computation. 17 2.4 Recent and Planned Developments We have only described some of the many analyses available in pips. <p> The control structure of the code and comments are preserved as long as possible. However, the use of summarization, to keep summaries small, combined with a pure interprocedural framework without cloning often prevents interprocedural path differentiation, whereas other parallelizers <ref> [35, 42] </ref> partially achieve this goal through automatic selective procedure cloning. Since pips is an interactive tool, manual cloning was tried and more accurate results were obtained, especially for forward interprocedural analyses, such as preconditions and out regions, and for intraprocedural transformations.
Reference: [43] <author> M. Hall, J. Anderson, S. Amarasinghe, B. Murphy, S.-W. Liao, E. Bugnion, and M. Lam. </author> <title> Maximizing multiprocessor performance with the SUIF compiler. </title> <journal> Computer, </journal> <volume> 29(12) </volume> <pages> 84-89, </pages> <month> December </month> <year> 1996. </year> <month> 29 </month>
Reference-contexts: Many flow sensitive analyses are still too complex for commercial products, which only implement flow-insensitive interprocedural analyses. But as good experimental results are published <ref> [43, 11] </ref>, and as the power and the memory sizes of computers increase, such analyses will undoubtedly appear in commercial tools. Not long ago, even flow insensitive array region analyses were considered too time and space consuming. <p> But this effort must be pursued, in particular for interdependent analyses which may override the existing classification. The current interprocedural frameworks for scientific programming in Fortran are surveyed in this paper. Few experimental results about effectiveness <ref> [51, 43, 11] </ref> have been published and additional research in the interprocedural analysis area is necessary before a really formal comparison can be made. In particular, the speed/accuracy ratio is still an issue, but has not yet been extensively studied [38].
Reference: [44] <author> M. Hall, S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Interprocedural compilation of Fortran D. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 38(2) </volume> <pages> 114-129, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: But research prototypes are still ahead, in particular for symbolic analyses [40, 54] and flow sensitive array region analyses [37, 42, 78, 31] which are mainly used for array privatization in parallelizing tools. Also, the compilation of fortran dialects such as hpf raises new interproce-dural problems <ref> [1, 44, 9] </ref>. 1.5 Conclusion A wide variety of interprocedural frameworks exists. They are more or less adapted to a specific interprocedural problem. <p> an interesting tradeoff in the accuracy/speed space and that their potential has not yet been measured. 2.5 PIPS and Other Research Tools Many research Fortran optimizing tools include some kind of interprocedural analyses, but the closest ones to pips certainly are fiat/suif [46, 47], ParaScope [26] and the D system <ref> [44] </ref>, Polaris [11], Parafrase-2 [68], and Panorama [65, 37, 38]. FIAT/SUIF suif is an intraprocedural compiler for parallel machines developed at Stanford.
Reference: [45] <author> M. Hall and K. Kennedy. </author> <title> Efficient call graph analysis. </title> <journal> Letters on Programming Languages and Systems, </journal> <volume> 1(3) </volume> <pages> 227-242, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Each edge is labeled with the actual parameters associated to the corresponding call site. 4 Mathematically, this is a multigraph, since two nodes can be connected by several edges. 7 Call graphs can be constructed very efficiently <ref> [71, 45] </ref>, and can provide sufficient information for many program optimizations, including parallelization. However, they do not allow flow sensitive interprocedural analyses, because they do not take into account the intraprocedural control flow of individual procedures.
Reference: [46] <author> M. Hall, J. Mellor-Crummey, A. Carle, and R. Rodrguez. FIAT: </author> <title> A framework for inter-procedural analysis and transformation. </title> <booktitle> In Sixth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: However there are only two possible paths, namely E-R and E'-R'. Of course, these two criteria must be taken into account when chosing or implementing an interprocedural analysis framework. 1.3 Interprocedural Analysis Frameworks Many interprocedural analysis frameworks have been defined (some examples can be found in <ref> [64, 21, 46, 79] </ref>). They differ in several ways: The degree of summarization and the accuracy of the translation across procedure calls are two important issues, and directly depend on the specific semantic problem and the representation of its solutions. <p> But the representation of the program structure on which the information is conveyed and the ability to perform or not flow sensitive or context sensitive analyses are also of importance. These last two problems are related <ref> [48, 46] </ref>, as shown in this section. Interprocedural analyses can be performed on the program call graph, where nodes represent individual procedures, and edges represent call sites 4 . <p> we feel that linear algebra and polyhedra are an interesting tradeoff in the accuracy/speed space and that their potential has not yet been measured. 2.5 PIPS and Other Research Tools Many research Fortran optimizing tools include some kind of interprocedural analyses, but the closest ones to pips certainly are fiat/suif <ref> [46, 47] </ref>, ParaScope [26] and the D system [44], Polaris [11], Parafrase-2 [68], and Panorama [65, 37, 38]. FIAT/SUIF suif is an intraprocedural compiler for parallel machines developed at Stanford. <p> FIAT/SUIF suif is an intraprocedural compiler for parallel machines developed at Stanford. To enable the parallelization of loops containing procedure calls, fiat <ref> [46] </ref>, an interproce-dural engine, has been added on top of it. fiat is a framework which allows prototyping of flow insensitive and flow sensitive interprocedural data flow analyses. The programmer has only to provide the initialization functions, the meet operator, the transfer function, and the direction of the analysis. <p> In particular, the speed/accuracy ratio is still an issue, but has not yet been extensively studied [38]. For that purpose, more generic tools for the experimentation of interprocedural analyses, such as fiat <ref> [46] </ref>, pag [4], or pips, should be made available. Many issues linked to interprocedural analyses have been dealt with by pips, the in-terprocedural Fortran 77 source-to-source parallelizer developed at Ecole des mines. It supports a comprehensive set of interprocedural analyses, each with a large number of accuracy options.
Reference: [47] <author> M. Hall, B. Murphy, S. Amarasinghe, S.-W. Liao, and M. Lam. </author> <title> Interprocedural analysis for parallelization. </title> <booktitle> In Languages and Compilers for Parallel Computing, Lecture Notes in Computer Science, </booktitle> <pages> pages 61-80. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: They range from constant propagation [18, 36, 10, 40, 23], to subexpression availability and variable values <ref> [47] </ref>, ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others [21, 7, 50, 57]. <p> we feel that linear algebra and polyhedra are an interesting tradeoff in the accuracy/speed space and that their potential has not yet been measured. 2.5 PIPS and Other Research Tools Many research Fortran optimizing tools include some kind of interprocedural analyses, but the closest ones to pips certainly are fiat/suif <ref> [46, 47] </ref>, ParaScope [26] and the D system [44], Polaris [11], Parafrase-2 [68], and Panorama [65, 37, 38]. FIAT/SUIF suif is an intraprocedural compiler for parallel machines developed at Stanford.
Reference: [48] <author> M. J. Harrold and B. Malloy. </author> <title> A unified interprocedural program representation for a maintenance environment. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(6) </volume> <pages> 584-593, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: But the representation of the program structure on which the information is conveyed and the ability to perform or not flow sensitive or context sensitive analyses are also of importance. These last two problems are related <ref> [48, 46] </ref>, as shown in this section. Interprocedural analyses can be performed on the program call graph, where nodes represent individual procedures, and edges represent call sites 4 . <p> The unified interprocedural graph <ref> [48] </ref> provides a demand-driven unified representation which combines the advantages of the sparse representations without restricting the scope of the possible analyses. The rationale of these approaches is to avoid computing irrelevant intermediate results, while still performing a global analysis.
Reference: [49] <author> M. J. Harrold and M. L. Soffa. </author> <title> Efficient computation of interprocedural definition-use chains. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(2) </volume> <pages> 175-204, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Panorama The Panorama parallelizing compiler has been initiated at the University of Minnesota, to support systems with memory hierarchies. It performs several interprocedural analyses, such as use-def chains <ref> [49] </ref> and flow-sensitive array region analysis based on lists of guarded regular section descriptors (gRSDs). Interprocedural analyses are performed on the program hierarchical supergraph, which is an extension of Myers' supergraph. As such, it provides the necessary framework to perform flow and context sensitive analyses.
Reference: [50] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: They range from constant propagation [18, 36, 10, 40, 23], to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others <ref> [21, 7, 50, 57] </ref>. Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler.
Reference: [51] <author> M. Hind, M. Burke, P. Carini, and S. Midkiff. </author> <title> Interprocedural array analysis : How much precision do we need ? In Third Workshop on Compilers for Parallel Computers, </title> <address> pages 48-64, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: But this effort must be pursued, in particular for interdependent analyses which may override the existing classification. The current interprocedural frameworks for scientific programming in Fortran are surveyed in this paper. Few experimental results about effectiveness <ref> [51, 43, 11] </ref> have been published and additional research in the interprocedural analysis area is necessary before a really formal comparison can be made. In particular, the speed/accuracy ratio is still an issue, but has not yet been extensively studied [38].
Reference: [52] <author> F. Irigoin. </author> <title> Interprocedural analyses for programming environments. </title> <booktitle> In Workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <pages> pages 333-350. </pages> <publisher> North-Holland, </publisher> <month> September </month> <year> 1992. </year> <month> 30 </month>
Reference-contexts: They range from constant propagation [18, 36, 10, 40, 23], to subexpression availability and variable values [47], ranges [13], or preconditions <ref> [53, 52] </ref> propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others [21, 7, 50, 57].
Reference: [53] <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical interprocedural parallelization: An overview of the PIPS project. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 144-151, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: They range from constant propagation [18, 36, 10, 40, 23], to subexpression availability and variable values [47], ranges [13], or preconditions <ref> [53, 52] </ref> propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others [21, 7, 50, 57].
Reference: [54] <author> S. Johnson, M. Cross, and M. Everett. </author> <title> Exploitation of symbolic information in inter-procedural dependence analysis. </title> <journal> Parallel Computing, </journal> <volume> 22 </volume> <pages> 197-226, </pages> <year> 1996. </year>
Reference-contexts: Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler. But research prototypes are still ahead, in particular for symbolic analyses <ref> [40, 54] </ref> and flow sensitive array region analyses [37, 42, 78, 31] which are mainly used for array privatization in parallelizing tools. Also, the compilation of fortran dialects such as hpf raises new interproce-dural problems [1, 44, 9]. 1.5 Conclusion A wide variety of interprocedural frameworks exists.
Reference: [55] <author> W. Landi and B. Ryder. </author> <title> A safe approximation algorithm for interprocedural pointer aliasing. </title> <booktitle> In International Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 235-248, </pages> <year> 1992. </year>
Reference-contexts: When the representation of the program interprocedural control flow is approximative, and when the problem is context sensitive, the ability to avoid taking into account unrealizable paths <ref> [55, 70] </ref> is an issue. Several solutions have been proposed for this problem, the most common approach being to tag solutions with path history or path specific information [64, 73, 55]. <p> Several solutions have been proposed for this problem, the most common approach being to tag solutions with path history or path specific information <ref> [64, 73, 55] </ref>. However, this may result in an exponential growth of the number of solutions at each node, and thus in overwhelming execution times and memory requirements. Selective cloning [41, 27] also appears as a partial solution, since it reduces the number of unrealizable paths taken into account.
Reference: [56] <author> Y.-F. Lee, B. Ryder, and M. Fiuczynski. </author> <title> Region analysis: a parallel elimination method for data flow analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(11) </volume> <pages> 913-926, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Another solution to reduce the cost of interprocedural analyses is to perform 5 Array region analyses collect information about the way array elements are used and defined by the program. 8 them in parallel <ref> [39, 56] </ref>. When the representation of the program interprocedural control flow is approximative, and when the problem is context sensitive, the ability to avoid taking into account unrealizable paths [55, 70] is an issue.
Reference: [57] <author> Z. Li and P.-C. Yew. </author> <title> Efficient interprocedural analysis and program parallelization and restructuring. </title> <booktitle> In Symposium on Parallel Processing: Experience with Applications, Languages and Systems, </booktitle> <pages> pages 85-99, </pages> <year> 1988. </year>
Reference-contexts: They range from constant propagation [18, 36, 10, 40, 23], to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet [77], followed by many others <ref> [21, 7, 50, 57] </ref>. Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler.
Reference: [58] <author> J. Loeliger and R. Metzger. </author> <title> Developing an interprocedural optimizing compiler. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(4) </volume> <pages> 41-48, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Today, many commercial compilers include some interprocedural optimizations. In 1991, the convex Application Compiler was the first on the market <ref> [58] </ref>; it includes several complex analyses, such as interprocedural pointer tracking or array section analysis. The acknowledged origins for this compiler have to be found in several research projects: ptran [72], ParaScope [20], Parafrase-2 [68], and pat [74]. <p> Not long ago, even flow insensitive array region analyses were considered too time and space consuming. They are now implemented in all leading research parallelizer projects, and can be found in some commercial compilers <ref> [58] </ref>. 10 2 An Example: The PIPS Compiler Workbench It is more and more widely acknowledged that not only program optimizations for parallel and sequential target machines, but also program maintenance and reverse-engineering benefit from interprocedural techniques.
Reference: [59] <author> T. Marlowe and B. Ryder. </author> <title> An efficient hybrid algorithm for incremental data flow analysis. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 184-196, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The rationale of these approaches is to avoid computing irrelevant intermediate results, while still performing a global analysis. Reducing the cost of interprocedural data flow analyses can also be achieved by demand-driven techniques, such as those presented in [33] and [69]. Incremental analysis <ref> [15, 59, 75] </ref> addresses the problem of avoiding unnecessary recomputations of data flow solutions in case of local program changes; but it requires an initial exhaustive solution.
Reference: [60] <author> T. Marlowe, B. Ryder, and M. Burke. </author> <title> Defining flow sensitivity in data flow problems. </title> <type> Research Technical Report lcsr-tr-249, </type> <institution> Rutgers University, Laboratory of Computer Science, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: Two main criteria have been proposed to classify interprocedural problems: Flow sensitivity and context sensitivity (also called path specificity ) <ref> [8, 64, 60] </ref>. An interprocedural problem is said to be flow sensitive if it cannot be solved precisely without considering the internal control flows of procedures. The above mentioned may be modified problem is flow insensitive, while the must be modified problem is flow sensitive. <p> Most research compiler prototypes take interprocedural issues into account and some classification effort has now been undertaken to better understand the relationship between the many interprocedural analyses published since 1974 <ref> [60] </ref>. But this effort must be pursued, in particular for interdependent analyses which may override the existing classification. The current interprocedural frameworks for scientific programming in Fortran are surveyed in this paper.
Reference: [61] <author> D. Maydan. </author> <title> Accurate Analysis of Array References. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> September </month> <year> 1992. </year> <month> 31 </month>
Reference-contexts: The main advantage is that usual intraprocedural analyses 3 and transformations can be applied on the resulting code, hopefully with the same precision. However, there are several drawbacks <ref> [41, 61] </ref>: 1. Inlining is not always possible, due to array reshaping 1 , or recursive procedures in languages other than fortran 77. 2. The resulting code is usually much larger than the original 2 .
Reference: [62] <author> H. Mayer and M. Wolfe. </author> <title> Interprocedural alias analysis: Implementation and empirical results. </title> <journal> Software : Practice and Experience, </journal> <volume> 23(11) </volume> <pages> 1201-1233, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: At first, the main purpose was therefore to analyze interprocedural dependences. And alias analysis <ref> [16, 29, 72, 62] </ref> as well as summary data flow information or inter-procedural side-effects (sdfi) [28] on scalar variables were among the main analyses.
Reference: [63] <author> E. Morel and C. </author> <title> Renvoise. Interprocedural elimination of partial redundancies. </title> <editor> In S. Muchnick and N. Jones, editors, </editor> <title> Program Flow Analysis. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1981. </year>
Reference-contexts: To enhance the results of these analyses, and to enable other useful code transformations 6 Recursivity and formal procedures are not often used in scientific programs. 9 such as partial redundancy elimination <ref> [63] </ref>, many other interprocedural scalar analyses have also been introduced. They range from constant propagation [18, 36, 10, 40, 23], to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation.
Reference: [64] <author> E. Myers. </author> <title> A precise inter-procedural data-flow algorithm. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 219-230, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: For instance it is easier to precisely compute the set of variables that may be modified by a procedure call, than the set of variables that are always modified <ref> [64] </ref> because in the last case the internal control flow of the called procedures must be taken into account. Two main criteria have been proposed to classify interprocedural problems: Flow sensitivity and context sensitivity (also called path specificity ) [8, 64, 60]. <p> Two main criteria have been proposed to classify interprocedural problems: Flow sensitivity and context sensitivity (also called path specificity ) <ref> [8, 64, 60] </ref>. An interprocedural problem is said to be flow sensitive if it cannot be solved precisely without considering the internal control flows of procedures. The above mentioned may be modified problem is flow insensitive, while the must be modified problem is flow sensitive. <p> However there are only two possible paths, namely E-R and E'-R'. Of course, these two criteria must be taken into account when chosing or implementing an interprocedural analysis framework. 1.3 Interprocedural Analysis Frameworks Many interprocedural analysis frameworks have been defined (some examples can be found in <ref> [64, 21, 46, 79] </ref>). They differ in several ways: The degree of summarization and the accuracy of the translation across procedure calls are two important issues, and directly depend on the specific semantic problem and the representation of its solutions. <p> Several solutions have been proposed for this problem, the most common approach being to tag solutions with path history or path specific information <ref> [64, 73, 55] </ref>. However, this may result in an exponential growth of the number of solutions at each node, and thus in overwhelming execution times and memory requirements. Selective cloning [41, 27] also appears as a partial solution, since it reduces the number of unrealizable paths taken into account.
Reference: [65] <author> T. Nguyen, J. Gu, and Z. Li. </author> <title> An interprocedural parallelizing compiler and its support for memory hierarchy research. </title> <booktitle> In Languages and Compilers for Parallel Computing, Lecture Notes in Computer Science, </booktitle> <pages> pages 96-110. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: and that their potential has not yet been measured. 2.5 PIPS and Other Research Tools Many research Fortran optimizing tools include some kind of interprocedural analyses, but the closest ones to pips certainly are fiat/suif [46, 47], ParaScope [26] and the D system [44], Polaris [11], Parafrase-2 [68], and Panorama <ref> [65, 37, 38] </ref>. FIAT/SUIF suif is an intraprocedural compiler for parallel machines developed at Stanford.
Reference: [66] <author> P. M. Petersen. </author> <title> Evaluation of Programs and Parallelizing Compilers using Dynamic Analysis Techniques. </title> <type> PhD thesis, </type> <institution> CSRD, University of Illinois, </institution> <month> January </month> <year> 1993. </year> <note> CSRD report No. 1273. </note>
Reference-contexts: Introduction Real life applications are split into several procedures to factorize code as much as possible and to improve readability. Traditional compilers only provide separate compilation, and procedure boundaries prevent many analyses and optimizations, such as constant propagation for instance, although they are of paramount importance in par-allelizing compilers <ref> [12, 66] </ref> to achieve reasonable performances. They are also more and more necessary for sequential machines. Interprocedural techniques have been introduced to cope with this problem. To our knowledge, the oldest article about interprocedural analyses was written more than twenty years ago by Allen [2].
Reference: [67] <author> A. Platonoff. </author> <title> Automatic data distribution for massively parallel computers. </title> <booktitle> In Fifth International Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 555-570, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Other analyses include interprocedural symbolic complexity and reduction detection, or intraprocedu-ral dependence and array data flow analyses <ref> [67] </ref>. pips also includes an impressive set of intraprocedural program transformations, including source code generation, which heavily rely on the previously described interprocedural analyses.
Reference: [68] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C. Lee, B. Leung, and D. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitionning, synchronizing and scheduling programs on multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: In 1991, the convex Application Compiler was the first on the market [58]; it includes several complex analyses, such as interprocedural pointer tracking or array section analysis. The acknowledged origins for this compiler have to be found in several research projects: ptran [72], ParaScope [20], Parafrase-2 <ref> [68] </ref>, and pat [74]. Other commercial products, such as the product line developed by Applied Parallel Research, the xlf compiler from ibm (option -qipa), or the sgi, foresys, kap, and vast compilers, have since included various interprocedural analyses. But most interprocedural techniques are still in the research domain. <p> the accuracy/speed space and that their potential has not yet been measured. 2.5 PIPS and Other Research Tools Many research Fortran optimizing tools include some kind of interprocedural analyses, but the closest ones to pips certainly are fiat/suif [46, 47], ParaScope [26] and the D system [44], Polaris [11], Parafrase-2 <ref> [68] </ref>, and Panorama [65, 37, 38]. FIAT/SUIF suif is an intraprocedural compiler for parallel machines developed at Stanford.
Reference: [69] <author> T. Reps. </author> <title> Solving demand versions of interprocedural analysis problems. </title> <booktitle> In International Conference on Compiler Construction, </booktitle> <pages> pages 389-403, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The rationale of these approaches is to avoid computing irrelevant intermediate results, while still performing a global analysis. Reducing the cost of interprocedural data flow analyses can also be achieved by demand-driven techniques, such as those presented in [33] and <ref> [69] </ref>. Incremental analysis [15, 59, 75] addresses the problem of avoiding unnecessary recomputations of data flow solutions in case of local program changes; but it requires an initial exhaustive solution.
Reference: [70] <author> T. Reps, S. Horwitz, and M. Sagiv. </author> <title> Precise interprocedural dataflow analysis via graph reachability. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 49-61, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: When the representation of the program interprocedural control flow is approximative, and when the problem is context sensitive, the ability to avoid taking into account unrealizable paths <ref> [55, 70] </ref> is an issue. Several solutions have been proposed for this problem, the most common approach being to tag solutions with path history or path specific information [64, 73, 55].
Reference: [71] <author> B. Ryder. </author> <title> Constructing the call graph of a program. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(3):216-225, </volume> <month> May </month> <year> 1979. </year> <month> 32 </month>
Reference-contexts: Each edge is labeled with the actual parameters associated to the corresponding call site. 4 Mathematically, this is a multigraph, since two nodes can be connected by several edges. 7 Call graphs can be constructed very efficiently <ref> [71, 45] </ref>, and can provide sufficient information for many program optimizations, including parallelization. However, they do not allow flow sensitive interprocedural analyses, because they do not take into account the intraprocedural control flow of individual procedures.
Reference: [72] <author> V. Sarkar. </author> <title> PTRAN | the IBM parallel translation system. </title> <booktitle> In Parallel Functional Programming Languages and Compilers, </booktitle> <pages> pages 309-391. </pages> <publisher> ACM Press Frontier Series, </publisher> <year> 1991. </year>
Reference-contexts: In 1991, the convex Application Compiler was the first on the market [58]; it includes several complex analyses, such as interprocedural pointer tracking or array section analysis. The acknowledged origins for this compiler have to be found in several research projects: ptran <ref> [72] </ref>, ParaScope [20], Parafrase-2 [68], and pat [74]. Other commercial products, such as the product line developed by Applied Parallel Research, the xlf compiler from ibm (option -qipa), or the sgi, foresys, kap, and vast compilers, have since included various interprocedural analyses. <p> At first, the main purpose was therefore to analyze interprocedural dependences. And alias analysis <ref> [16, 29, 72, 62] </ref> as well as summary data flow information or inter-procedural side-effects (sdfi) [28] on scalar variables were among the main analyses. <p> This transformation spots global arrays used as temporary variables in procedures and reallocates them as stack-allocated local variables. It is useful for the maintenance and parallelization of dusty deck programs. 9 in regions are similar to upward exposed read regions computed by others <ref> [72, 42, 37, 78] </ref>. 10 out regions are different from downward exposed regions, because they also depend on the future of the computation. 17 2.4 Recent and Planned Developments We have only described some of the many analyses available in pips.
Reference: [73] <author> M. Sharir and A. Pnuelli. </author> <title> Two approaches to interprocedural data flow analysis. </title> <editor> In S. Muchnick and N. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 189-233. </pages> <publisher> Prentice-Hall, Inc., </publisher> <year> 1981. </year>
Reference-contexts: Several solutions have been proposed for this problem, the most common approach being to tag solutions with path history or path specific information <ref> [64, 73, 55] </ref>. However, this may result in an exponential growth of the number of solutions at each node, and thus in overwhelming execution times and memory requirements. Selective cloning [41, 27] also appears as a partial solution, since it reduces the number of unrealizable paths taken into account.
Reference: [74] <author> K. Smith and W. Appelbe. </author> <title> PAT an interactive Fortran parallelizing assistant tool. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 58-62, </pages> <year> 1988. </year>
Reference-contexts: In 1991, the convex Application Compiler was the first on the market [58]; it includes several complex analyses, such as interprocedural pointer tracking or array section analysis. The acknowledged origins for this compiler have to be found in several research projects: ptran [72], ParaScope [20], Parafrase-2 [68], and pat <ref> [74] </ref>. Other commercial products, such as the product line developed by Applied Parallel Research, the xlf compiler from ibm (option -qipa), or the sgi, foresys, kap, and vast compilers, have since included various interprocedural analyses. But most interprocedural techniques are still in the research domain.
Reference: [75] <author> V. Sreedhar, G. Gao, and Y.-F. Lee. </author> <title> A new framework for exhaustive and incremental analysis using DJ graphs. </title> <booktitle> In International Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 278-290, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The rationale of these approaches is to avoid computing irrelevant intermediate results, while still performing a global analysis. Reducing the cost of interprocedural data flow analyses can also be achieved by demand-driven techniques, such as those presented in [33] and [69]. Incremental analysis <ref> [15, 59, 75] </ref> addresses the problem of avoiding unnecessary recomputations of data flow solutions in case of local program changes; but it requires an initial exhaustive solution.
Reference: [76] <author> E. Stoltz, M. Gelerk, and M. Wolfe. </author> <title> Extended SSA with factored use-def chains to support optimization and parallelism. </title> <booktitle> In International Conference on System Sciences, </booktitle> <pages> pages 43-52, </pages> <year> 1994. </year>
Reference-contexts: In this last case, the size of the representation of each procedure is reduced. Several sparse interprocedural representations have been designed for particular classes of problems, such as the program summary graph [19], the system dependence graph [34, 10], : : : Intraprocedural sparse representations include the ssa form <ref> [3, 32, 76] </ref> and the sparse data flow evaluation graph [24]. The unified interprocedural graph [48] provides a demand-driven unified representation which combines the advantages of the sparse representations without restricting the scope of the possible analyses.
Reference: [77] <author> R. Triolet, P. Feautrier, and F. Irigoin. </author> <title> Direct parallelization of call statements. </title> <booktitle> In ACM SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 176-185, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: They range from constant propagation [18, 36, 10, 40, 23], to subexpression availability and variable values [47], ranges [13], or preconditions [53, 52] propagation. To handle arrays more accurately than sdfi, flow insensitive array region analysis was introduced by Triolet <ref> [77] </ref>, followed by many others [21, 7, 50, 57]. Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler.
Reference: [78] <author> P. Tu. </author> <title> Automatic Array Privatization and Demand-Driven Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1995. </year>
Reference-contexts: Today, many commercial products include some interprocedural flow insensitive analyses, as complex as array region analyses, or pointer tracking in the most advanced tools such as the convex Application Compiler. But research prototypes are still ahead, in particular for symbolic analyses [40, 54] and flow sensitive array region analyses <ref> [37, 42, 78, 31] </ref> which are mainly used for array privatization in parallelizing tools. Also, the compilation of fortran dialects such as hpf raises new interproce-dural problems [1, 44, 9]. 1.5 Conclusion A wide variety of interprocedural frameworks exists. They are more or less adapted to a specific interprocedural problem. <p> This transformation spots global arrays used as temporary variables in procedures and reallocates them as stack-allocated local variables. It is useful for the maintenance and parallelization of dusty deck programs. 9 in regions are similar to upward exposed read regions computed by others <ref> [72, 42, 37, 78] </ref>. 10 out regions are different from downward exposed regions, because they also depend on the future of the computation. 17 2.4 Recent and Planned Developments We have only described some of the many analyses available in pips.
Reference: [79] <author> Kwangkeun Yi and Williams Ludwell Harrison III. </author> <title> Automatic generation and management of interprocedural program analyses. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 246-259, </pages> <month> January </month> <year> 1993. </year> <month> 33 </month>
Reference-contexts: However there are only two possible paths, namely E-R and E'-R'. Of course, these two criteria must be taken into account when chosing or implementing an interprocedural analysis framework. 1.3 Interprocedural Analysis Frameworks Many interprocedural analysis frameworks have been defined (some examples can be found in <ref> [64, 21, 46, 79] </ref>). They differ in several ways: The degree of summarization and the accuracy of the translation across procedure calls are two important issues, and directly depend on the specific semantic problem and the representation of its solutions.
References-found: 79

