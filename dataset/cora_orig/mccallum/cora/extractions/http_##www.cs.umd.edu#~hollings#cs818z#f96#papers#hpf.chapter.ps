URL: http://www.cs.umd.edu/~hollings/cs818z/f96/papers/hpf.chapter.ps
Refering-URL: http://www.cs.umd.edu/~hollings/cs818z/f96/readingList.htm
Root-URL: 
Email: haupt, rankag@npac.syr.edu  
Title: COMPILING HPF FOR DISTRIBUTED MEMORY MIMD COMPUTERS  
Author: Zeki Bozkus, Alok Choudhary*, Geoffrey Fox, Tomasz Haupt and Sanjay Ranka** 
Address: Syracuse, NY, 13244-4100  fzbozkus, choudhar, gcf,  
Affiliation: Northeast Parallel Architectures Center Syracuse University,  Computer Engineering Dept. Syracuse University Computer Science Dept. Syracuse University  
Abstract: This paper describes the design of a High Performance Fortran (HPF/Fortran 90D) compiler, a source-to-source translator for distributed memory systems. HPF is a data parallel language with compiler directives to enable users to specify data alignment and distributions. A systematic methodology to process distribution directives of HPF is presented. Furthermore, techniques for data and computation partitioning, communication detection and generation, and the run-time support for the compiler are discussed. Finally, initial performance results for the compiler are presented which show that the code produced by the compiler is portable, yet efficient. We believe that the methodology to process data distribution, computation partitioning, communication system design and the overall compiler design can be used by the other HPF compiler implementors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> American National Standards Institue. </author> <title> Fortran 90: X3j3 internal document s8.118. Summitted as Text for ISO/IEC 1539:1991, </title> <month> May </month> <year> 1991. </year>
Reference: [2] <author> G. C. Fox, S. Hiranadani, K. Kenndy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D Language Specification. </title> <type> Technical report, </type> <institution> Rice and Syracuse University, </institution> <year> 1992. </year>
Reference-contexts: Users who traditionally require tremendous amount of computing power still prefer conventional supercomputers, recognizing that parallel computing is still a high risk technology, which does not protect software investment. To overcome this deficiency, we have designed Fortran D language <ref> [2] </ref> with our colleagues at Rice University. Fortran D is a version of Fortran enhanced with a rich set of data decomposition specifications to provide a simple machine-indepentent programming model for most data-parallel computations.
Reference: [3] <author> High Performance Fortran Forum. </author> <title> High performance fortran language specification version 1.0. </title> <type> Draft, </type> <note> Also available as technical report CRPC 26 Chapter 1 TR92225 from the Center for Research on Parallel Computation, </note> <institution> Rice University., </institution> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Recently, the High Performance Fortran Forum, an informal group of people from academia, industry and national labs, led by Ken Kennedy, developed a language called HPF (High Performance Fortran) <ref> [3] </ref> based on Fortran D. Companies that have already committed to developing compilers and/or supporting HPF include Intel, TMC, PGI, DEC, IBM, and others.
Reference: [4] <institution> The Thinking Machine Corporation. </institution> <note> CM Fortran User's Guide version 0.7-f, </note> <month> July </month> <year> 1990. </year>
Reference: [5] <institution> Maspar Computer Corporation. </institution> <note> MasPar Fortran User Guide version 1.1, </note> <month> Aug. </month> <year> 1991. </year>
Reference: [6] <author> G. Fox. </author> <title> The architecture of problems and portable parallel software systems. </title> <type> Technical Report SCCS-78b, </type> <institution> Syracuse University, </institution> <year> 1991. </year>
Reference-contexts: This is performed by the sequentialization module. Array operations and forall statements in the original program are transferred into loops or nested loops. The communication module detects communication requirements and inserts appropriate communication primitives. Finally, the code generator produces loosely synchronous <ref> [6] </ref> SPMD code. The generated code is structured as alternating phases of local computation and global communication. Local computations consist of operations by each processor on the data in its own memory.
Reference: [7] <author> Z. Bozkus et al. </author> <title> Compiling the FORALL statement on MIMD parallel computers. </title> <type> Technical Report SCCS-389, </type> <institution> Northeast Parallel Architectures Center, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: The front-end to parse Fortran 90 for the compiler was obtained from ParaSoft Corporation. In this module, our compiler also transforms each array assignment statement and where statement into equivalent forall statement with no loss of information <ref> [7] </ref>. In this way, the subsequent steps need only deal with forall statements. The partitioning module processes the data distribution directives; namely, template, distribute and align. Using these directives, it partitions data and computation among processors. <p> The DIST parameter gives the distribution attribute such as block or cyclic. The set BOUND primitive computes and returns the local computation range in local lower bound, local upper bound and local stride for each processor. The algorithm to implement this primitive can be found in <ref> [7] </ref>. 10 Chapter 1 In summary, our computation and data distributions have two implications. The processor that is assigned an iteration is responsible for computing the rhs expression of the assignment statement. <p> We use pattern matching techniques similar to those proposed by Chen [23]. Further, we extend the above tests to include unstructured communication. Table 1 shows the patterns of communication primitives used in our compiler. The detail of communication detection algorithm can be found in <ref> [7] </ref>.
Reference: [8] <author> D. Padua B. Leasure D. Kuck, R. Kuhn and M. Wolf. </author> <title> Dependence graph and compiler optimizations. </title> <booktitle> Proc. of 8th ACM Symp. Principles on Programming Lang., </booktitle> <month> September </month> <year> 1981. </year>
Reference: [9] <author> Z. Bozkus et al. </author> <title> Compiling Distribution Directives in a Fortran 90D Compiler. </title> <type> Technical Report SCCS-388, </type> <institution> Northeast Parallel Architectures Center, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: The original indices can be calculated by f 1 if they are required. The algorithm to compile align directive can be found in <ref> [9] </ref>. Stage 2 : Each dimension of the template is mapped onto the logical processor grid, based on the DISTRIBUTE directive attributes. Block divides the template into contiguous chunks. Cyclic specifies a round-robin division of the template. <p> This enhances portability across a large number of architectures. By performing the above three stage mapping, the compiler is decoupled from the specifics of a given machine or configuration. Compilation of distribution directives is discussed in detail in <ref> [9] </ref>. Computation Partitioning Once the data is distributed, there are several alternatives to assign computations to processing elements (PEs) for each instance of a forall statement. One of the most common methods is to use the owner computes rule.
Reference: [10] <author> S. Chatterjee, J.R. Gilbert, R. Schreiber, and S.H Tseng. </author> <title> Automatic Array Alignment in Data-Parallel Programs. </title> <booktitle> Twentieth Annual ACM SIGACT/SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Improved performance estimation of communication costs. Our compiler takes the data distribution for the source arrays from the user as compiler directives. However, any future compiler will require a capability to perform automatic data distribution and alignments <ref> [18, 19, 10] </ref>. Such techniques usually require computing trade-offs between exploitable parallelism and the communication costs. The costs of collective communication routines can be determined more precisely, thereby enabling the compiler to generate better distributions automatically.
Reference: [11] <author> I. Ahmad, R. Bordawekar, Z. Bozkus, A. Choudhary, G. Fox, K. Para-suram, R. Ponnusamy, S. Ranka, and R. Thakur. </author> <title> Fortran 90D Intrinsic Functions on Distributed Memory Machines: Implementation and Scalability. </title> <type> Technical Report SCCS-256, </type> <institution> Northeast Parallel Architectures Center, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Table 3 presents a sample of performance numbers for a subset of the intrinsic functions on iPSC/860. A detailed performance study is presented in <ref> [11] </ref>. The times in the table include both the computation and communication times for each function. For most of the functions we were able to obtain almost linear speedups. <p> All this information is stored into a structure which is called distributed array descriptor (DAD) <ref> [11] </ref>. In summary, parallel intrinsic functions, communication routines, dynamic data redistribution primitives and others are part of the run-time support system. 20 Chapter 1 7 OPTIMIZATIONS Several types of communication and computation optimization can be performed to generate a more efficient code.
Reference: [12] <author> H. Zima, H. Bast, and M. Gerndt. </author> <title> Superb: A tool for semi Automatic SIMD/MIMD Parallelization. </title> <booktitle> Parallel Computing, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: In the owner computes rule, the computation is assigned to the PE owning the lhs data element. This rule is simple to implement and performs well in a large number of cases. Most of the current implementations of parallelizing compilers uses the owner computes rule <ref> [12, 13] </ref>. However, it may not be possible to apply the owner computes rule for every case without extensive overhead. <p> Matrix size is 1023x1024 and it is column distributed.(Intel iPSC/860, time in seconds). 9 SUMMARY OF RELATED WORK The compilation technique of Fortran 77 for distributed memory systems has been addressed by Callahan and Kennedy [13]. Currently, a Fortran 77D compiler is being developed at Rice [28, 34]. Superb <ref> [12] </ref> compiles a Fortran 77 Compiling HPF 23 1 2 3 4 5 6 Speedup Processors Gaussian Elimination Compiler generated 3 3 3 3 Hand written + + + + the hand-written code and Fortran 90D compiler generated code for Gaussian Elimination). program into a semantically equivalent parallel SUPRENUM multiprocessor.
Reference: [13] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for Distributed Memory Multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <pages> pages 171-207, </pages> <year> 1988. </year>
Reference-contexts: In the owner computes rule, the computation is assigned to the PE owning the lhs data element. This rule is simple to implement and performs well in a large number of cases. Most of the current implementations of parallelizing compilers uses the owner computes rule <ref> [12, 13] </ref>. However, it may not be possible to apply the owner computes rule for every case without extensive overhead. <p> Matrix size is 1023x1024 and it is column distributed.(Intel iPSC/860, time in seconds). 9 SUMMARY OF RELATED WORK The compilation technique of Fortran 77 for distributed memory systems has been addressed by Callahan and Kennedy <ref> [13] </ref>. Currently, a Fortran 77D compiler is being developed at Rice [28, 34].
Reference: [14] <author> M. Wu and G. Fox et al. </author> <title> Compiling Fortran 90 programs for distributed memory MIMD paralelel computers. </title> <type> Technical Report SCCS-88, </type> <institution> Northeast Parallel Architectures Center, </institution> <month> May </month> <year> 1991. </year> <title> Compiling HPF 27 </title>
Reference-contexts: The generated code will be in the following order. Communications ! some global communication primitives to read Computation ! local computation Communication ! a communication primitive to write For reasonably simple expressions, the compiler can transform such index expressions into the canonical form by performing some symbolic expression operations <ref> [14] </ref>. However, it may not always be possible to perform such transformations for complex expressions.
Reference: [15] <author> R. Allen. </author> <title> Dependency analysis for Subscripted Variables and its Application to Program Transformation. </title> <type> Technical Report PhD thesis, </type> <institution> Rice University, </institution> <year> 1983. </year>
Reference: [16] <author> J. Ng, V. Sarkar, and J.F. Shaw. </author> <title> Optimized execution of Fortran 90 array constructs on supercomputer architectures. </title> <address> Supercomputing'91, </address> <year> 1991. </year>
Reference: [17] <author> G. C. Fox, M.A. Johnson, G.A. Lyzenga, S. W. Otto, J.K. Salmon, and D. W. Walker. </author> <title> In Solving Problems on Concurent Processors, volume 1-2. </title> <publisher> Prentice Hall, </publisher> <month> May </month> <year> 1988. </year>
Reference-contexts: The processor that owns an array element (lhs or rhs) must communicate the value of that element to the processors performing the computation. 5 COMMUNICATION Our HPF compiler produces calls to collective communication routines <ref> [17] </ref> instead of generating individual processor send and receive calls inside the compiled code. There are three main reasons for using collective communication to support interprocessor communication in the HPF compiler. 1. Improved performance of HPF programs. To achieve good performance, interprocessor communication must be minimized. <p> The third category uses multiple broadcast trees to spread data. The fourth category is implemented using unstructured communication patterns. The fifth category is implemented using existing research on parallel matrix algorithms <ref> [17] </ref>. Some of the intrinsic functions can be further optimized for the underlying hardware architecture.
Reference: [18] <author> K. Knobe, J. D. Lukas, and G. L. Steele. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 102-118, </pages> <month> Feb </month> <year> 1990. </year>
Reference-contexts: Improved performance estimation of communication costs. Our compiler takes the data distribution for the source arrays from the user as compiler directives. However, any future compiler will require a capability to perform automatic data distribution and alignments <ref> [18, 19, 10] </ref>. Such techniques usually require computing trade-offs between exploitable parallelism and the communication costs. The costs of collective communication routines can be determined more precisely, thereby enabling the compiler to generate better distributions automatically.
Reference: [19] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 213-221, </pages> <month> Oct </month> <year> 1991. </year>
Reference-contexts: Improved performance estimation of communication costs. Our compiler takes the data distribution for the source arrays from the user as compiler directives. However, any future compiler will require a capability to perform automatic data distribution and alignments <ref> [18, 19, 10] </ref>. Such techniques usually require computing trade-offs between exploitable parallelism and the communication costs. The costs of collective communication routines can be determined more precisely, thereby enabling the compiler to generate better distributions automatically.
Reference: [20] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Con-currency: Practice and Experience, </journal> <month> September </month> <year> 1990. </year>
Reference-contexts: This is particularly useful when the shift amount is known at compile time. This primitive uses that fact to avoid intra processor copying of data and directly stores data in the overlap areas <ref> [20] </ref>. temporary shift: This is similar to overlap shift except that the data is shifted into a temporary array. This is useful when the shift amount is not a compile time constant.
Reference: [21] <author> C. Koelbel and P. Mehrotra. </author> <title> Supporting Compiling Global Name-Space Parallel Loops for Distributed Execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> October </month> <year> 1991. </year>
Reference-contexts: We have implemented two sets of unstructured communication primitives: One, to support cases where the communicating processors can determine the send and receive lists based only on local information, and hence, only require preprocessing that involves local computations <ref> [21] </ref>, and the other, where to determine the send and receive lists preprocessing itself requires communication among the processors [22]. <p> Koelbel and Mehrotra <ref> [26, 21] </ref> puts a great deal of effort on run-time analysis for optimizing message passing in implementation of Kali. Quinn et al. [35, 36] use a data parallel approach for compiling C* for hypercube machines. The ADAPT system [37] compiles Fortran 90 for execution on MIMD distributed memory architectures.
Reference: [22] <author> H. Berryman J. Saltz, J. Wu and S. Hiranandani. </author> <title> Distributed Memory Compiler Design for Sparse Problems. </title> <type> Interim Report ICASE, </type> <institution> NASA Langley Research Center, </institution> <year> 1991. </year>
Reference-contexts: communication primitives: One, to support cases where the communicating processors can determine the send and receive lists based only on local information, and hence, only require preprocessing that involves local computations [21], and the other, where to determine the send and receive lists preprocessing itself requires communication among the processors <ref> [22] </ref>. <p> Chen [23, 39] describes general compiler optimization techniques that reduce communication overhead for Fortran-90 implementation on massivelly parallel machines. Many techniques especially unstructured communication of our compiler are adapted from Saltz et al. <ref> [40, 29, 22] </ref>. Gupta et al. [24, 41] use collective communication on automatic data partitioning on distributed memory machines.
Reference: [23] <author> J. Li and M. Chen. </author> <title> Compiling Communication -Efficient Programs for Massively Parallel Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Specifically, this involves a number of tests on the relationship among subscripts of various arrays in a forall statement. These tests should also include information about array alignments and distributions. We use pattern matching techniques similar to those proposed by Chen <ref> [23] </ref>. Further, we extend the above tests to include unstructured communication. Table 1 shows the patterns of communication primitives used in our compiler. The detail of communication detection algorithm can be found in [7]. <p> Since in the original data parallel constructs such as forall statement, there is no data dependency between different loop iteration, vectorization can be performed easily by the node compiler. Our compiler performs several optimizations to reduce the total cost of communication. Some of communication optimizations <ref> [23, 28, 29] </ref> are as follows. Vectorized communication. One of the important considerations for message passing on distributed memory machines is the setup time required for sending a message. Typically, this cost is equivalent to the sending cost of hundreds of bytes. <p> The ADAPT system [37] compiles Fortran 90 for execution on MIMD distributed memory architectures. The ADAPTOR [38] is a tool that transform data parallel programs written in Fortran with array extension and layout directives to explicit message passing. Chen <ref> [23, 39] </ref> describes general compiler optimization techniques that reduce communication overhead for Fortran-90 implementation on massivelly parallel machines. Many techniques especially unstructured communication of our compiler are adapted from Saltz et al. [40, 29, 22].
Reference: [24] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers. </title> <booktitle> IEEE: Transaction on Parallel and Distributed Systems, </booktitle> <pages> pages 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Chen [23, 39] describes general compiler optimization techniques that reduce communication overhead for Fortran-90 implementation on massivelly parallel machines. Many techniques especially unstructured communication of our compiler are adapted from Saltz et al. [40, 29, 22]. Gupta et al. <ref> [24, 41] </ref> use collective communication on automatic data partitioning on distributed memory machines. Due to space limitations, we do not elaborate on various other related projects. 24 Chapter 1 10 SUMMARY AND CONCLUSIONS HPF are languages that incorporate parallel constructs and allow users to specify data distributions.
Reference: [25] <author> R. Das, J. Saltz, and H. Berryman. </author> <title> A Manual For PARTI Runtime Primitives. </title> <type> NASA,ICASE Interim Report 17, </type> <month> May </month> <year> 1991. </year>
Reference-contexts: For unstructured communication, this optimization can be achieved by performing the entire preprocessing loop before communication so that the schedule routine can combine the messages to the maximum extent. The preprocessing loop is also called the "inspector" loop <ref> [25, 26] </ref>. <p> These two primitives are available in PARTI (Parallel Automatic Runtime Toolkit at ICASE) <ref> [25] </ref> designed to efficiently support irregular patterns of distributed array accesses. The PARTI and other communication primitives and intrinsic functions form the run-time support system of our Fortran 90D compiler. 6 RUN-TIME SUPPORT SYSTEM The Fortran 90D/HPF compiler relies on a very powerful run-time support system.
Reference: [26] <author> C. Koelbel, P. Mehrotra, and J. V. Rosendale. </author> <title> Supporting Shared Data Structures on Distributed Memory Architectures. </title> <booktitle> PPoPP, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: For unstructured communication, this optimization can be achieved by performing the entire preprocessing loop before communication so that the schedule routine can combine the messages to the maximum extent. The preprocessing loop is also called the "inspector" loop <ref> [25, 26] </ref>. <p> Koelbel and Mehrotra <ref> [26, 21] </ref> puts a great deal of effort on run-time analysis for optimizing message passing in implementation of Kali. Quinn et al. [35, 36] use a data parallel approach for compiling C* for hypercube machines. The ADAPT system [37] compiles Fortran 90 for execution on MIMD distributed memory architectures.
Reference: [27] <author> A.V. Aho, R. Sethi, and J.D Ullman. </author> <booktitle> Compilers Principles, Techniques and Tools. </booktitle> <month> March </month> <year> 1988. </year> <note> 28 Chapter 1 </note>
Reference-contexts: Hence, if the compiler recognizes that the same schedule can be reused, it does not generate code for scheduling but it passes a pointer to the already existing schedule. Furthermore, the preprocessing computation can be moved up as much as possible by analyzing definition-use chains <ref> [27] </ref>. Reduction in communication overhead can be significant if the scheduling code can be moved 16 Chapter 1 out of one or more nested loops by this analysis. In the above example, local list (line 6) is used to store the index of one-dimensional array. <p> Code movement. The compiler can utilize the information that the run-time support routines do not have procedural side effects. For example, Compiling HPF 21 the preprocessing loop or communication routines can be moved up as much as possible by analyzing definition-use chains <ref> [27] </ref>. This may lead to moving of the scheduling code out of one or more nested loops which may reduce the amount of communication required significantly. We are incrementally incorporating many more optimizations in the compiler. 8 EXPERIMENTAL RESULTS A prototype compiler is complete (it was demonstrated at Supercomputing'92).
Reference: [28] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimization for Fortran D on MIMD distributed-memory machines. </title> <booktitle> Proc. </booktitle> <address> Supercomputing'91, </address> <month> Nov </month> <year> 1991. </year>
Reference-contexts: Since in the original data parallel constructs such as forall statement, there is no data dependency between different loop iteration, vectorization can be performed easily by the node compiler. Our compiler performs several optimizations to reduce the total cost of communication. Some of communication optimizations <ref> [23, 28, 29] </ref> are as follows. Vectorized communication. One of the important considerations for message passing on distributed memory machines is the setup time required for sending a message. Typically, this cost is equivalent to the sending cost of hundreds of bytes. <p> Matrix size is 1023x1024 and it is column distributed.(Intel iPSC/860, time in seconds). 9 SUMMARY OF RELATED WORK The compilation technique of Fortran 77 for distributed memory systems has been addressed by Callahan and Kennedy [13]. Currently, a Fortran 77D compiler is being developed at Rice <ref> [28, 34] </ref>.
Reference: [29] <author> R. Mirchandaney J. Saltz, K. Crowley and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> December </month> <year> 1991. </year>
Reference-contexts: Since in the original data parallel constructs such as forall statement, there is no data dependency between different loop iteration, vectorization can be performed easily by the node compiler. Our compiler performs several optimizations to reduce the total cost of communication. Some of communication optimizations <ref> [23, 28, 29] </ref> are as follows. Vectorized communication. One of the important considerations for message passing on distributed memory machines is the setup time required for sending a message. Typically, this cost is equivalent to the sending cost of hundreds of bytes. <p> Chen [23, 39] describes general compiler optimization techniques that reduce communication overhead for Fortran-90 implementation on massivelly parallel machines. Many techniques especially unstructured communication of our compiler are adapted from Saltz et al. <ref> [40, 29, 22] </ref>. Gupta et al. [24, 41] use collective communication on automatic data partitioning on distributed memory machines.
Reference: [30] <institution> ParaSoft Corp. </institution> <note> Express Fortran refernce guide Version 3.0, </note> <year> 1990. </year>
Reference-contexts: In this section, we describe our experience in using the compiler. One of the principal requirements of the users of distributed memory MIMD systems is some "guarantee" of the portability for their code. Express parallel programming environment <ref> [30] </ref> guarantees this the portability on various platforms including, Intel iPSC/860, nCUBE/2, networks of workstations etc. We should emphasize that we have implemented a collective communication library which is currently built on the top of Express message passing primitives.
Reference: [31] <author> A. Beguelin, J. Dongarra, A. Geist, R. Mancheck, and V. </author> <note> Sunderam. </note>
Reference-contexts: We should emphasize that we have implemented a collective communication library which is currently built on the top of Express message passing primitives. Hence, in order to change to any other message passing system such as PVM <ref> [31] </ref> (which also runs on several platforms), we only need to replace the calls to the communication primitives in our communication library (not the compiler).
References-found: 31

