URL: http://www.cs.berkeley.edu/~johnw/papers/microneuro93.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~johnw/publications.html
Root-URL: 
Title: Development of a Connectionist Network Supercomputer  
Author: Krste Asanovic, James Beck, Jerry Feldman, Nelson Morgan, and John Wawrzynek 
Date: December 12, 1992  
Address: Berkeley, California  
Affiliation: University of California and the International Computer Science Institute  
Abstract: This paper describes an effort at UC Berkeley and the International Computer Science Institute to develop a super-computer for artificial neural network applications. We describe our applications targets, machine goals, and the system architecture. 
Abstract-found: 1
Intro-found: 1
Reference: [AM91] <author> Krste Asanovic and Nelson Morgan. </author> <title> Experimental Determination of Precision Requirements for Back-Propagation Training of Artificial Neural Networks. </title> <booktitle> In Proceedings 2nd International Conference on Microelectronics for Neural Networks, </booktitle> <address> Munich, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: CNS-1 will supply orders of magnitude more capability in a form that we already know how to exploit. There are several features of connectionist computation that are exploited in the CNS-1 design. Experiments show that limited precision fixed point arithmetic suffices for almost all algorithms <ref> [AM91] </ref>. Many problems are highly regular and are well suited to parallel and pipelined execution. Connectionist networks are "embarassingly" parallel and map nicely to distributed memory machines. Communication is usually multi-cast and normally values only are sent, eliminating the read latency that plagues most distributed computing.
Reference: [FLR92] <author> Jerome A. Feldman, Chu-Cheow Lim, and Thomas Rauber. </author> <title> The Shared-Memory Langauge pSather on a Distributed-Memory Multiprocessor. </title> <booktitle> In Second Workshop on Languages, Compilers, and Run-Time Environments for Distributed-Memory Multiprocessors, </booktitle> <address> Boulder, Colorado, </address> <month> Sept 30 - Oct 2 </month> <year> 1992. </year>
Reference-contexts: This C compiler will then be used as the target for a port of Sather [Omo91]. Parallel language support for distributed memory machines is still very much a research issue. We expect to use experience gained in a pSather <ref> [FLR92] </ref> port to the CM-5 to port pSather to CNS-1. The CNS-1 hardware is simple, fast, and flexible, and should prove an interesting vehicle for further research into parallel languages.
Reference: [Ham90] <author> D. Hammerstrom. </author> <title> A VLSI architecture for High-Performance, Low-Cost, On-Chip Learning. </title> <booktitle> In Proc. International Joint Conference on Neural Networks, </booktitle> <pages> pages II-537-543, </pages> <year> 1990. </year>
Reference-contexts: Implementations of this machine consisted of 4 to 40 TI-TMS320C30 floating-point DSP chips connected via a ring of Xilinx programmable gate arrays, each implementing a simple two-register data pipeline. Several related efforts are underway to construct programmable digital neurocomputers, most notably the CNAPS chip from Adaptive Solutions <ref> [Ham90] </ref> and the MA-16 chip from Siemens [RBR + 91]. Adaptive Solutions provides a SIMD array with 64 processing elements per chip, in a system with four chips on a board controlled by a common microcode sequencer.
Reference: [IYM + 89] <author> A. Iwata, Y. Yoshida, S. Matsuda, Y. Sato, and N. Suzumura. </author> <title> An Artificial Neural Network Accelerator using General Purpose Floating Point Digital Signal Processors. </title> <booktitle> In Proceeding JCNN, </booktitle> <pages> pages II-171-175, </pages> <year> 1989. </year>
Reference-contexts: Many of these have used special-purpose architectures incorporating commercial DSP chips. Examples of this approach are the NeuroTurbo from Nagoya University <ref> [IYM + 89] </ref> and our own Ring Array Processor (RAP) machine [MBK + 92]. The RAP has been used since 1990 as an essential component in the development of connectionist algorithms for speech recognition.
Reference: [LWM + 92] <author> John Lazzaro, John Wawrzynek, Misha Mahowald, Massimo Sivilotti, and David Gillespie. </author> <title> Silicon Auditory Processor as Computer Peripherals. </title> <booktitle> In Advances in Neural Information Processings Systems, Proceedings of the 1992 Conference, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: In some cases we are interested in interfacing with specific analog interface systems that are part of our research <ref> [LWM + 92] </ref>. These requirements provide a strong impetus for users to want a significant say in the design of the computing system. In addition to the specific connectionist applications of CNS-1, we are very interested in the potential of CNS-1 as a general purpose connectionist accelerator.
Reference: [MBK + 92] <author> Nelson Morgan, James Beck, Phil Kohn, Jeff Bilmes, Eric Allman, and Jochaim Beer. </author> <title> The Ring Array Processor(RAP): A Multiprocessing Peripheral for Connectionist Applications. </title> <journal> Journal of Parallel and Distributed Computing, Special Issue on Neural Networks, </journal> <volume> 14 </volume> <pages> 248-259, </pages> <year> 1992. </year>
Reference-contexts: Recent work in our lab and elsewhere has shown the practicality of connectionist systems for a range of important problems, but has also revealed needs for computational resources far exceeding those available to investigators in the field. Our own earlier development of the RAP <ref> [MBK + 92] </ref> attached processor has played a crucial role in our research and is proving valuable to other groups who have acquired the system. CNS-1 will supply orders of magnitude more capability in a form that we already know how to exploit. <p> Many of these have used special-purpose architectures incorporating commercial DSP chips. Examples of this approach are the NeuroTurbo from Nagoya University [IYM + 89] and our own Ring Array Processor (RAP) machine <ref> [MBK + 92] </ref>. The RAP has been used since 1990 as an essential component in the development of connectionist algorithms for speech recognition.
Reference: [Omo91] <author> Stephen M. Omohundro. </author> <title> The Sather Language. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> Ca., </address> <year> 1991. </year>
Reference-contexts: Both C and C++ will be made available by a port of the GNU gcc compiler to the Torrent processor. This C compiler will then be used as the target for a port of Sather <ref> [Omo91] </ref>. Parallel language support for distributed memory machines is still very much a research issue. We expect to use experience gained in a pSather [FLR92] port to the CM-5 to port pSather to CNS-1.
Reference: [Ram92] <institution> Rambus Inc., 2465 Latham Street, Mountain View, California, USA. </institution> <note> Rambus Technology Guide, Preliminary Edition, </note> <month> May </month> <year> 1992. </year>
Reference-contexts: The MIPS CPU provides general scalar processing and supports the vector coprocessor by providing scalar operands, address generation, and loop control. To provide the high memory bandwidth needed to feed the vector unit, the memory interface implements four Rambus channels <ref> [Ram92] </ref> with an aggregate memory bandwidth of over 1GB/s. Each Torrent processor will have four 18Mb RDRAM chips per Rambus channel giving a per node total of 32MB of RDRAM.
Reference: [RBR + 91] <author> U. Ramacher, J. Beichter, W. Raab, J. Anlauf, N. Bruls, M. Hachmann, and M. </author> <title> Wesseling. Design of a 1st Generation Neurocomputer. In VLSI Design of Neural Networks. </title> <publisher> Kluwer Academic, </publisher> <year> 1991. </year>
Reference-contexts: Several related efforts are underway to construct programmable digital neurocomputers, most notably the CNAPS chip from Adaptive Solutions [Ham90] and the MA-16 chip from Siemens <ref> [RBR + 91] </ref>. Adaptive Solutions provides a SIMD array with 64 processing elements per chip, in a system with four chips on a board controlled by a common microcode sequencer. As with the CNS-1, processing elements are similar to general purpose DSPs with reduced precision multipliers.
Reference: [vECGS92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proc. Tenth International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year> <month> 9 </month>
Reference-contexts: The communication and management of continuous sensory data streams require rapid processor response to asynchronous external events. Torrent has a fast and simple network interface with messages sent and received directly from processor registers. The active message model <ref> [vECGS92] </ref> is directly supported, where an arriving message triggers execution of a local event handler. System design is simplified because each processing node comprises only a single chip processor and DRAM. No external logic is necessary to complete a node, increasing reliability and minimizing the board area required per processor.
References-found: 10

