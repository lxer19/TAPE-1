URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jjlien/www/puishop6.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jjlien/www/index.html
Root-URL: 
Title: Automatically Recognizing Facial Expressions in the Spatio-Temporal Domain  
Author: , James J. Lien, Takeo Kanade, Adena J. Zlochower, Jeffrey F. Cohn, and Ching-Chung Li 
Affiliation: VASC, The Robotics Institute, Carnegie Mellon University,  Dept. of Electrical Engineering, University of  Dept. of Psychology, University of  
Address: Banff, Alberta, Canada,  Pittsburgh, PA 15213  Pittsburgh, Pittsburgh, PA 15260  Pittsburgh, Pittsburgh, PA 15260  
Note: Workshop on Perceptual User Interfaces, pp. 94-97,  
Email: jjlien@cs.cmu.edu  
Phone: 2  3  
Date: October 19-21, 1997.  
Abstract: We developed a computer vision system that automatically recognizes facial action units (AUs) or AU combinations using Hidden Markov Models (HMMs). AUs are defined as visually discriminable muscle movements. The facial expressions are recognized in digitized image sequences of arbitrary length. In this paper, we use two approaches to extract the expression information: (1) facial feature point tracking, which is sensitive to subtle feature motion, in the mouth region, and (2) pixel-wise flow tracking, which includes more motion information, in the forehead and brow regions. In the latter approach, we use principal component analysis (PCA) to compress the data. We accurately recognize 93% of the lower face expressions and 91% of the upper face expressions. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M.S. Bartlett, P.A. Viola, T.J. Sejnowski, B.A. Golomb, J. Larsen, J.C. Hager and P. Ekman, </author> <title> "Classifying Facial Action," </title> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: Principal component analysis (PCA) has excellent properties for our purposes, including image data compression and maintenance of a strong correlation between two consecutive motion frames. Since our goal is to recognize expression rather than identifying individuals or objects <ref> [1, 9, 13] </ref>, we analyze facial motion using optical flow not the gray value to ignore differences across individual subjects. Before using PCA, the images are automatically normalized using affine transformation to ensure that the pixel-wise flows of each frame have exact geometric correspondence.
Reference: 2. <author> M.J. Black and Y. Yacoob, </author> <title> "Recognizing Facial Expressions under Rigid and Non-Rigid Facial Motions," </title> <booktitle> International Workshop on Automatic Face and Gesture Recognition, </booktitle> <address> Zurich, </address> <pages> pp. 12-17, </pages> <year> 1995. </year>
Reference-contexts: A number of automated facial expression recognition systems analyze six basic emotions (joy, fear, anger, disgust, sadness and surprise), and the associated expressions are classified into emotion categories rather than facial action <ref> [2, 10, 15] </ref>. In reality, humans are capable of producing thousands of expressions varying in complexity and meaning that are not fully captured with a limited number of expressions and emotion categories. Our goal is to recognize a variety of facial actions. <p> In an individual region, the flow direction is changed to conform to the flow plurality of the region <ref> [2, 10, 15] </ref> or averaged over an entire region [7, 8]. Black and colleagues [2, 3] also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. <p> In an individual region, the flow direction is changed to conform to the flow plurality of the region [2, 10, 15] or averaged over an entire region [7, 8]. Black and colleagues <ref> [2, 3] </ref> also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. As a result, the recognition ability and accuracy of the systems may be reduced.
Reference: 3. <author> M.J. Black, Y. Yacoob, A.D. Jepson, </author> <title> and D.J. Fleet, "Learning Parameterized Models of Image Motion," </title> <journal> Computer Vision and Pattern Recognition, </journal> <year> 1997. </year>
Reference-contexts: In an individual region, the flow direction is changed to conform to the flow plurality of the region [2, 10, 15] or averaged over an entire region [7, 8]. Black and colleagues <ref> [2, 3] </ref> also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. As a result, the recognition ability and accuracy of the systems may be reduced.
Reference: 4. <author> P. Ekman and W.V. Friesen, </author> <title> "The Facial Action Coding System," </title> <publisher> Consulting Psychologists Press, Inc., </publisher> <address> San Francisco, CA, </address> <year> 1978. </year>
Reference-contexts: As a result, the recognition ability and accuracy of the systems may be reduced. Our goal is to develop a system that recognizes both subtle feature motion and complex facial expressions. Our approach to facial expression analysis is based on the Facial Action Coding System (FACS) <ref> [4] </ref>. FACS separates expressions into upper and lower face action units (AUs), which are the smallest visibly discriminable muscle actions that combine to form expressions. We use optical flow to track facial feature points and pixel-wise facial motion.
Reference: 5. <author> I.A. Essa, </author> <title> "Analysis, Interpretation and Synthesis of Facial Expressions," </title> <type> Perceptual Computing Technical Report 303, </type> <institution> MIT Media Laboratory, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Our goal is to recognize a variety of facial actions. Automated recognition of individual motion sequences is a challenging task. Currently, most facial expression recognition systems use either complicated three-dimensional wireframe face models to recognize and synthesize facial expressions <ref> [5, 12] </ref> or use averaged optical flow within local regions (e.g., forehead, eyes, nose, mouth, cheek, and chin) for recognition. In an individual region, the flow direction is changed to conform to the flow plurality of the region [2, 10, 15] or averaged over an entire region [7, 8].
Reference: 6. <author> B.D. Lucas and T. Kanade, </author> <title> "An Iterative Image Registration Technique with an Application to Stereo Vision," </title> <booktitle> Proceedings of the 7th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1981. </year>
Reference-contexts: We use a computer mouse to manually select 10 facial feature points around the lip contour in the first frame of each image sequence. Each point is the center of a 13x13-flow window that includes the horizontal and vertical flows. By using the hierarchical optical flow tracking method <ref> [6] </ref>, the facial feature points are tracked automatically in the remaining frames of the image sequence. The displacement of each feature point is calculated by subtracting its normalized position in the first frame from its current normalized position.
Reference: 7. <author> K. Mase and A. Pentland, </author> <title> "Automatic Lipreading by Optical-Flow Analysis," </title> <journal> Systems and Computers in Japan, </journal> <volume> Vol. 22, No. 6, </volume> <year> 1991. </year>
Reference-contexts: In an individual region, the flow direction is changed to conform to the flow plurality of the region [2, 10, 15] or averaged over an entire region <ref> [7, 8] </ref>. Black and colleagues [2, 3] also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed. <p> Recognition Using Hidden Markov Models Mase and Pentland <ref> [7] </ref> use a similar flow-based PCA approach for lipreading recognition. They use a template matching method that analyzes the minimum value of the sumsquared-difference (SSD) between the projected flow curve of the test word and that of the word templates in the two-dimensional eigenspace.
Reference: 8. <author> K. Mase, </author> <title> "Recognition of Facial Expression from Optical Flow," </title> <journal> IEICE Transactions, </journal> <volume> Vol. E74, </volume> <pages> pp. 3474-3483, </pages> <year> 1991. </year>
Reference-contexts: In an individual region, the flow direction is changed to conform to the flow plurality of the region [2, 10, 15] or averaged over an entire region <ref> [7, 8] </ref>. Black and colleagues [2, 3] also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed.
Reference: 9. <author> H. Murase and S.K. Nayar, </author> <title> "Visual Learning and Recognition of 3-D Objects from Appearance," </title> <journal> International Journal of Computer Vision, </journal> <volume> 14, </volume> <pages> pp. 5-24, </pages> <year> 1995. </year>
Reference-contexts: Principal component analysis (PCA) has excellent properties for our purposes, including image data compression and maintenance of a strong correlation between two consecutive motion frames. Since our goal is to recognize expression rather than identifying individuals or objects <ref> [1, 9, 13] </ref>, we analyze facial motion using optical flow not the gray value to ignore differences across individual subjects. Before using PCA, the images are automatically normalized using affine transformation to ensure that the pixel-wise flows of each frame have exact geometric correspondence.
Reference: 10. <author> M. Rosenblum, Y. Yacoob and L.S. Davis, </author> <title> "Human Emotion Recognition from Motion Using a Radial Basis Function Network Architecture," </title> <booktitle> Proceedings of the Workshop on Motion of Non-rigid and Articulated Objects, </booktitle> <address> Austin, TX, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: A number of automated facial expression recognition systems analyze six basic emotions (joy, fear, anger, disgust, sadness and surprise), and the associated expressions are classified into emotion categories rather than facial action <ref> [2, 10, 15] </ref>. In reality, humans are capable of producing thousands of expressions varying in complexity and meaning that are not fully captured with a limited number of expressions and emotion categories. Our goal is to recognize a variety of facial actions. <p> In an individual region, the flow direction is changed to conform to the flow plurality of the region <ref> [2, 10, 15] </ref> or averaged over an entire region [7, 8]. Black and colleagues [2, 3] also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed.
Reference: 11. <author> L.R. Rabiner, </author> <title> "An Introduction to Hidden Markov Models," </title> <journal> IEEE ASSP Magazine, </journal> <pages> pp. 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: This approach is impractical for our purposes because the length of our image sequences is arbitrary (it varies between 9 and 44 frames) and the projected flow curve is in a higher dimensional eigenspace. We employ Hidden Markov Models (HMMs) <ref> [11] </ref> for facial expression recognition because they perform well in the spatio-temporal domain and are analogous to human performance (e.g., for speech and gesture recognition).
Reference: 12. <author> D. Terzopoulos and K. Waters, </author> <title> "Analysis of Facial Images Using Physical and Anatomical Models," </title> <booktitle> IEEE International Conference on Computer Vision, </booktitle> <pages> pp. 727-732, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Our goal is to recognize a variety of facial actions. Automated recognition of individual motion sequences is a challenging task. Currently, most facial expression recognition systems use either complicated three-dimensional wireframe face models to recognize and synthesize facial expressions <ref> [5, 12] </ref> or use averaged optical flow within local regions (e.g., forehead, eyes, nose, mouth, cheek, and chin) for recognition. In an individual region, the flow direction is changed to conform to the flow plurality of the region [2, 10, 15] or averaged over an entire region [7, 8].
Reference: 13. <author> M. Turk and A. Pentland, </author> <title> "Eigenfaces for Recognition," </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> Vol. 3, No. 1, </volume> <pages> pp. 71-86, </pages> <year> 1991. </year>
Reference-contexts: Principal component analysis (PCA) has excellent properties for our purposes, including image data compression and maintenance of a strong correlation between two consecutive motion frames. Since our goal is to recognize expression rather than identifying individuals or objects <ref> [1, 9, 13] </ref>, we analyze facial motion using optical flow not the gray value to ignore differences across individual subjects. Before using PCA, the images are automatically normalized using affine transformation to ensure that the pixel-wise flows of each frame have exact geometric correspondence.
Reference: 14. <author> Y.T. Wu, T. Kanade, J. F. Cohn, and C.C. Li, </author> <title> Optical Flow Estimation Using Wavelet Motion Model, </title> <address> ICCV, </address> <year> 1998. </year>
Reference-contexts: Pixel-wise Tracking and Principal Component Analysis To recognize pixel-wise motion in the upper face, we use Wus pixel-wise optical flow algorithm <ref> [14] </ref> to track the entire face image (417 x 385 = row x column pixels). Currently, the following upper face expressions are recognized in the forehead and brow regions: AUs 4 (brows lowered), 1+4 (inner part of the brow raised and drawn together), and 1+2 (entire brow raised).
Reference: 15. <author> J. Yacoob and L. Davis, </author> <title> "Computing Spatio-Temporal Representations of Human Faces," </title> <booktitle> In Proc. Computer Vision and Pattern Recognition, CVPR-94, </booktitle> <pages> pp. 70-75, </pages> <address> Seattle, WA, </address> <month> June </month> <year> 1994. </year> <title> Table 1: Lower face expression recognition based on 43 test sequences. The average recognition rate is 93%. Table 2: Upper face expression recognition based on 47 test sequences. The average recognition rate is 91%. </title>
Reference-contexts: A number of automated facial expression recognition systems analyze six basic emotions (joy, fear, anger, disgust, sadness and surprise), and the associated expressions are classified into emotion categories rather than facial action <ref> [2, 10, 15] </ref>. In reality, humans are capable of producing thousands of expressions varying in complexity and meaning that are not fully captured with a limited number of expressions and emotion categories. Our goal is to recognize a variety of facial actions. <p> In an individual region, the flow direction is changed to conform to the flow plurality of the region <ref> [2, 10, 15] </ref> or averaged over an entire region [7, 8]. Black and colleagues [2, 3] also assign parameter thresholds to their classification paradigm. These methods are relatively insensitive to subtle motion because information about small deviations is lost when their flow pattern is removed or thresholds are imposed.
References-found: 15

