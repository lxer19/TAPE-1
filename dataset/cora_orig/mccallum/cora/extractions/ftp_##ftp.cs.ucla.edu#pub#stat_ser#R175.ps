URL: ftp://ftp.cs.ucla.edu/pub/stat_ser/R175.ps
Refering-URL: http://singapore.cs.ucla.edu/csl_papers.html
Root-URL: http://www.cs.ucla.edu
Email: judea@cs.ucla.edu  
Title: Belief Networks Revisited  
Author: Judea Pearl 
Date: January 6, 1994  
Address: Los Angeles, CA 90024  
Affiliation: Cognitive Systems Laboratory Computer Science Department University of California,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Charniak, </author> <title> Bayesian networks without tears, </title> <note> AI Magazine 12 (4) (1991) 50-63. </note>
Reference-contexts: the development of belief networks, both before and after the publication of Fusion, although space fl This work was supported in part by NSF grant IRI-9157936 and by State of California MICRO grants 91-124 and 91-125. 1 Some of these applications are described in a recent tutorial article by Charniak <ref> [1] </ref>. 1 permits but a sketchy account of the wealth of recent developments in this area. 2 2 Origins The idea of studying distributed probabilistic computations on graphical models began brewing in my mind in the late 1970s, after I read Rumelhart's paper on reading comprehension [23].
Reference: [2] <author> G.F. Cooper, </author> <title> Computational complexity of probabilistic inference using Bayesian belief networks (research note), </title> <booktitle> Artificial Intelligence 42 (2) (1990) 393-405. </booktitle>
Reference-contexts: While the task of computing probabilities in general networks is 5 Specific aspects of these problems (e.g., arc reversals [24]) had been worked out in the literature on influence diagrams, but the general problems remained unsettled until quite recently [25]. 6 NP-hard [22] <ref> [2] </ref>, the complexity of the first two methods is exponential in the size of the largest clique found in some triangulation of the network. The third method might yield a higher complexity in some networks, but it is convenient in networks with a few long loops.
Reference: [3] <author> G. F. Cooper and E. Herskovits, </author> <title> A Bayesian method for constructing Bayesian belief networks from databases, </title> <booktitle> in: Proceedings of the Conference on Uncertainty in AI (Morgan Kaufmann, </booktitle> <address> San Mateo, CA) (1990) 86-94. </address> <month> 7 Structurally, </month> <title> the two algorithms are essentially the same; the one described in Fusion propagates messages in a tree of singletons, whereas Lauritzen and Spiegelhalter's algorithm propagates these messages in a tree of compound variables known as a join tree ([18], pp. 111-113) or junction tree [11]. </title> <type> 8 </type>
Reference-contexts: A fast algorithm for recovering sparse networks is described by Spirtes and Glymour [27], and Bayesian methods of computing the "probability that X is a cause for Y" were developed by Cooper and Herskovits <ref> [3] </ref>. Causal reorganization of categorical databases is studied in [4].
Reference: [4] <author> R. Dechter and J. Pearl, </author> <title> Directed constraint networks: A relational framework for casual modeling, </title> <booktitle> in: Proceedings of the 12th International Joint Conference of Artificial Intelligence (IJCAI-91) Sydney, </booktitle> <publisher> Australia (Morgan Kaufmann, </publisher> <address> San Mateo, CA) (1991) 1164-1170. </address>
Reference-contexts: A fast algorithm for recovering sparse networks is described by Spirtes and Glymour [27], and Bayesian methods of computing the "probability that X is a cause for Y" were developed by Cooper and Herskovits [3]. Causal reorganization of categorical databases is studied in <ref> [4] </ref>.
Reference: [5] <author> D. Geiger, Graphoids: </author> <title> A qualitative framework for probabilistic inference, </title> <type> Ph.D. dissertation, </type> <institution> University of California, </institution> <address> Los Angeles, CA (1990). </address>
Reference-contexts: Verma [28] has proved the soundness of the d-separation criterion using the semi-graphoid axioms [20], thus rendering the criterion valid for a wide class of informational dependencies, including probabilistic, graphical, correlational, and database dependencies. Geiger <ref> [5] </ref> has shown that the criterion cannot be improved; namely, d-separation reveals all the independencies that can be inferred from the information provided by the network builder. <p> Representations using undirected graphs (also known as Markov fields) are discussed in [18], Chapter 3, and <ref> [5] </ref>; representations using multi-graphs and annotated graphs have been developed by Geva and Paz ([7]. Network Updating Techniques. Since the publication of Fusion, many techniques for updating belief networks have been developed and refined.
Reference: [6] <author> D. Geiger, T.S. Verma and J. Pearl, </author> <title> Identifying independence in Bayesian networks, </title> <booktitle> Networks 20 (5) (1990) 507-534. </booktitle>
Reference-contexts: Geiger [5] has shown that the criterion cannot be improved; namely, d-separation reveals all the independencies that can be inferred from the information provided by the network builder. A more comprehensive separation criterion, applicable to networks containing deterministic nodes, was developed by Geiger, Verma, and Pearl <ref> [6] </ref> and has been shown to be testable in time proportional to the number of edges in the network.
Reference: [7] <author> R.Y. Geva and A. Paz, </author> <title> Towards complete representation of graphoids in graphs, </title> <booktitle> in: Proceedings of the 15th International Workshop on Graph Theoretic Concepts in Computer Sciences, </booktitle> <publisher> Rodluc (Springer-Verlag, </publisher> <address> New York) (1989) 41-62. </address>
Reference: [8] <author> D. Heckerman, </author> <title> Probabilistic similarity networks, Networks 20 (5) (year) 607-636. </title> <publisher> Also MIT Press (1991). </publisher>
Reference-contexts: It provides, therefore, the semantics needed for defining and characterizing belief networks. Technically, the d-separation criterion has facilitated immediate solutions to three practical problems (see [19], Section 4 A further reduction has been achieved by Heckerman's similarity networks <ref> [8] </ref>. 5 4.4.): 5 (1) How to characterize precisely the set of graphical transformations (e.g., arc reversals, node removals, node collapsing) that can legitimately be performed on a network, (2) how to test whether one network is entailed by or is equivalent to another, and (3) how to delineate the minimum
Reference: [9] <author> M. Henrion, </author> <title> Propagation of uncertainty by probabilistic logic sampling in Bayes' networks, </title> <editor> in: J.F. Lemmer and L.N. Kanal, eds., </editor> <booktitle> Uncertainty in Artificial Intelligence 2 (Elsevier Science Publishers/North-Holland, </booktitle> <address> Amsterdam, Netherlands, </address> <year> 1988) </year> <month> 149-164. </month>
Reference: [10] <author> R.A. Howard and J.E. Matheson, </author> <title> Influence diagrams, </title> <editor> in: R.A. Howard and J.E. Matheson, eds., </editor> <booktitle> The Principles and Applications of Decision Analysis Vol. 2 (Strategic Decisions Group, </booktitle> <address> Menlo Park, CA, </address> <year> 1984) </year> <month> 721-762. </month>
Reference-contexts: I therefore decided to investigate systematically how directed and undirected graphs could be used as a language for encoding, decoding, and reasoning with such independencies. At about the same time, Howard and Matheson were studying the properties of influence diagrams <ref> [10] </ref> and were asking similar questions about graphs and dependencies, albeit from a somewhat different perspective: the links in the diagrams were treated 2 A more complete account and an updated bibliography are provided in the revised second printing of my book Probabilistic Reasoning in Intelligence Systems [18]. 2 as traces
Reference: [11] <author> F.V. Jensen, K.G. Olsen and S.K. Andersen, </author> <title> An algebra of Bayesian belief universes for knowledge-based systems, </title> <booktitle> Networks 20 (5) (1990) 637-660. </booktitle>
Reference-contexts: I speculated that the loop-cut conditioning method would be more ef-ficient than the one I labeled "compounding," that is, forming clusters of compound variables that are tree structured and applying the tree-propagation algorithm to the resulting tree. Lauritzen and Spiegelhalter [13], and later Jensen et al. <ref> [11] </ref>, have perfected this tree-clustering method to the point that it is now the most widely used algorithm in practical applications.
Reference: [12] <author> J.H. Kim and J. Pearl, </author> <title> A computational model for combined causal and diagnostic reasoning in inference systems, </title> <booktitle> in: Proceedings IJCAI-83, </booktitle> <address> Karlsruhe, Germany (1983) 190-193. </address> <month> 9 </month>
Reference-contexts: This gave rise to the tree-propagation algorithm reported in [15] and, a year later, the Kim-Pearl algorithm <ref> [12] </ref>, which supported not only bi-directional inferences but also intercausal interactions, such as "explaining-away." These two algorithms were described in Section 2 of Fusion.
Reference: [13] <author> S.L. Lauritzen and D.J. Spiegelhalter, </author> <title> Local computations with prob-abilities on graphical structures and their application to expert systems (with discussion), </title> <journal> Journal Royal Statistical Society, </journal> <volume> Series B 50 (2) (1988) 157-224. </volume>
Reference-contexts: Network Updating Techniques. Since the publication of Fusion, many techniques for updating belief networks have been developed and refined. Among the most popular are Shachter's method of node elimination [24], Lauritzen and Spiegelhalter's method of graph-triangulation and clique-tree propagation <ref> [13] </ref>, and the method of loop-cut conditioning (Fusion Section 2.4. <p> I speculated that the loop-cut conditioning method would be more ef-ficient than the one I labeled "compounding," that is, forming clusters of compound variables that are tree structured and applying the tree-propagation algorithm to the resulting tree. Lauritzen and Spiegelhalter <ref> [13] </ref>, and later Jensen et al. [11], have perfected this tree-clustering method to the point that it is now the most widely used algorithm in practical applications.
Reference: [14] <author> R.M. Oliver and J.Q. Smith, eds., </author> <title> Influence Diagrams, Belief Nets and Decision Analysis (John Wiley, </title> <address> Rexdale, Ontario, Canada, </address> <year> 1990). </year>
Reference-contexts: The consistent agreement between plausible reasoning and probability calculus could not be coincidental, but strongly suggests that human intuition invokes some crude form of probabilistic computation. 3 Fusion has been criticized for "substituting mathematics for clarity" (e.g., R. E. Bar-low, in <ref> [14] </ref>, page 117). In my judgment, it was precisely this conversion of networks and diagrams to mathematically defined objects that led to their current acceptance in practical reasoning systems. 3 3.
Reference: [15] <author> J. Pearl, </author> <title> Reverend Bayes on inference engines: A distributed hierarchical approach, </title> <booktitle> in: Proceedings AAAI National Conference on AI, </booktitle> <address> Pittsburgh, PA (1982) 133-136. </address>
Reference-contexts: This gave rise to the tree-propagation algorithm reported in <ref> [15] </ref> and, a year later, the Kim-Pearl algorithm [12], which supported not only bi-directional inferences but also intercausal interactions, such as "explaining-away." These two algorithms were described in Section 2 of Fusion.
Reference: [16] <author> J. Pearl, </author> <title> How to do with probabilities what people say you can't, </title> <booktitle> in: Proceedings 2nd IEEE Conference on AI Applications, </booktitle> <address> Miami, FL (1985) 6-12. </address>
Reference: [17] <author> J. Pearl, </author> <title> Evidential reasoning using stochastic simulation of causal models, </title> <booktitle> Artificial Intelligence 32 (2) (1987) 245-258. </booktitle>
Reference: [18] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems, </title> <publisher> Morgan Kauf-mann, </publisher> <address> Palo Alto, CA, </address> <year> 1988). </year> <note> Revised second printing (1991). </note>
Reference-contexts: of influence diagrams [10] and were asking similar questions about graphs and dependencies, albeit from a somewhat different perspective: the links in the diagrams were treated 2 A more complete account and an updated bibliography are provided in the revised second printing of my book Probabilistic Reasoning in Intelligence Systems <ref> [18] </ref>. 2 as traces of the information that a person finds convenient to consult while assessing probabilities. The inconclusive results of Howard and Matheson's report jolted me into trying a different approach, in which the links are designated specifically to causal associations. <p> The relation of conditional independence has received an axiomatic characterization using the theory of graphoids [20] (see also <ref> [18] </ref>, Chapter 3), which provides symbolic machinery for deciding whether one independency follows from others and whether we can capture such independencies by graphs. Representations using undirected graphs (also known as Markov fields) are discussed in [18], Chapter 3, and [5]; representations using multi-graphs and annotated graphs have been developed by <p> independence has received an axiomatic characterization using the theory of graphoids [20] (see also <ref> [18] </ref>, Chapter 3), which provides symbolic machinery for deciding whether one independency follows from others and whether we can capture such independencies by graphs. Representations using undirected graphs (also known as Markov fields) are discussed in [18], Chapter 3, and [5]; representations using multi-graphs and annotated graphs have been developed by Geva and Paz ([7]. Network Updating Techniques. Since the publication of Fusion, many techniques for updating belief networks have been developed and refined.
Reference: [19] <author> J. Pearl, D. Geiger and T.S. Verma, </author> <title> The logic of influence diagrams, </title> <booktitle> in: </booktitle> <pages> [14] 67-88. </pages>
Reference-contexts: It provides, therefore, the semantics needed for defining and characterizing belief networks. Technically, the d-separation criterion has facilitated immediate solutions to three practical problems (see <ref> [19] </ref>, Section 4 A further reduction has been achieved by Heckerman's similarity networks [8]. 5 4.4.): 5 (1) How to characterize precisely the set of graphical transformations (e.g., arc reversals, node removals, node collapsing) that can legitimately be performed on a network, (2) how to test whether one network is entailed
Reference: [20] <author> J. Pearl and A. Paz, </author> <year> 1989. </year> <title> On the logic of representing dependencies by graphs, </title> <booktitle> in: Proceedings 1986 Canadian AI Conference, </booktitle> <address> Montreal, Ontario, Canada (1986) 94-98. </address>
Reference-contexts: These patterns have since been used to define causation and to uncover causal relationships in data [21] (see also last paragraph in this section). Verma [28] has proved the soundness of the d-separation criterion using the semi-graphoid axioms <ref> [20] </ref>, thus rendering the criterion valid for a wide class of informational dependencies, including probabilistic, graphical, correlational, and database dependencies. Geiger [5] has shown that the criterion cannot be improved; namely, d-separation reveals all the independencies that can be inferred from the information provided by the network builder. <p> The relation of conditional independence has received an axiomatic characterization using the theory of graphoids <ref> [20] </ref> (see also [18], Chapter 3), which provides symbolic machinery for deciding whether one independency follows from others and whether we can capture such independencies by graphs.
Reference: [21] <author> J. Pearl and T. Verma, </author> <title> A theory of inferred causation, </title> <editor> in: J.A. Allen, R. Fikes and E. Sandewall, eds., </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceeding of the Second International Conference Morgan Kaufmann, </booktitle> <address> San Mateo, CA) (1991) 441-452. </address>
Reference-contexts: On the conceptual side, by identifying the independencies embedded in directed acyclic graphs, the d-separation criterion has also identified special patterns of independencies that are characteristics of causal organizations. These patterns have since been used to define causation and to uncover causal relationships in data <ref> [21] </ref> (see also last paragraph in this section). Verma [28] has proved the soundness of the d-separation criterion using the semi-graphoid axioms [20], thus rendering the criterion valid for a wide class of informational dependencies, including probabilistic, graphical, correlational, and database dependencies. <p> Technically, the probabilistic semantics that belief networks attribute to the links and their orientations has rendered this prospect feasible, and several systems have been developed for this purpose. Pearl and Verma <ref> [21] </ref> have developed a probabilistic account of causation based on minimal-model semantics, 6 This theory provides criteria for identifying genuine and spurious causes, with and without temporal information, and yields algorithms for recovering causal networks with hidden variables from statistical data.
Reference: [22] <author> A. Rosenthal, </author> <title> A computer scientist looks at reliability computations, </title> <editor> in: Barlow et al., eds., </editor> <title> Reliability and Fault Tree Analysis (SIAM, </title> <address> Philadel-phia, </address> <year> 1975) </year> <month> 133-152. </month>
Reference-contexts: While the task of computing probabilities in general networks is 5 Specific aspects of these problems (e.g., arc reversals [24]) had been worked out in the literature on influence diagrams, but the general problems remained unsettled until quite recently [25]. 6 NP-hard <ref> [22] </ref> [2], the complexity of the first two methods is exponential in the size of the largest clique found in some triangulation of the network. The third method might yield a higher complexity in some networks, but it is convenient in networks with a few long loops.
Reference: [23] <author> D.E. Rumelhart, </author> <title> Toward an interactive model of reading, </title> <type> Tech. Report #CHIP-56, </type> <institution> University of California, La Jolla, </institution> <address> CA (1976). </address> <month> 10 </month>
Reference-contexts: tutorial article by Charniak [1]. 1 permits but a sketchy account of the wealth of recent developments in this area. 2 2 Origins The idea of studying distributed probabilistic computations on graphical models began brewing in my mind in the late 1970s, after I read Rumelhart's paper on reading comprehension <ref> [23] </ref>. In this paper, Rumelhart presented compelling evidence that text comprehension must be a distributed process that combines both top-down and bottom-up inferences.
Reference: [24] <author> R.D. Shachter, </author> <title> Evaluating influence diagrams, </title> <note> Operations Research 34 (6) (1986) 871-882. </note>
Reference-contexts: Network Updating Techniques. Since the publication of Fusion, many techniques for updating belief networks have been developed and refined. Among the most popular are Shachter's method of node elimination <ref> [24] </ref>, Lauritzen and Spiegelhalter's method of graph-triangulation and clique-tree propagation [13], and the method of loop-cut conditioning (Fusion Section 2.4. While the task of computing probabilities in general networks is 5 Specific aspects of these problems (e.g., arc reversals [24]) had been worked out in the literature on influence diagrams, but <p> Among the most popular are Shachter's method of node elimination <ref> [24] </ref>, Lauritzen and Spiegelhalter's method of graph-triangulation and clique-tree propagation [13], and the method of loop-cut conditioning (Fusion Section 2.4. While the task of computing probabilities in general networks is 5 Specific aspects of these problems (e.g., arc reversals [24]) had been worked out in the literature on influence diagrams, but the general problems remained unsettled until quite recently [25]. 6 NP-hard [22] [2], the complexity of the first two methods is exponential in the size of the largest clique found in some triangulation of the network.
Reference: [25] <author> J.Q. Smith, </author> <title> Influence diagrams for statistical modeling, </title> <note> Annals of Statistics 17 (2) (1989) 564-572. </note>
Reference-contexts: While the task of computing probabilities in general networks is 5 Specific aspects of these problems (e.g., arc reversals [24]) had been worked out in the literature on influence diagrams, but the general problems remained unsettled until quite recently <ref> [25] </ref>. 6 NP-hard [22] [2], the complexity of the first two methods is exponential in the size of the largest clique found in some triangulation of the network. The third method might yield a higher complexity in some networks, but it is convenient in networks with a few long loops.
Reference: [26] <author> D.J. Spiegelhalter and S.L. Lauritzen, </author> <title> Sequential updating of conditional probabilities on directed graphical structures, </title> <booktitle> Networks 20 (5) (1990) 579-605. </booktitle>
Reference-contexts: Statistical techniques have also been developed for systematic updating of the conditional probabilities annotating the network so as to achieve a better match with past empirical data <ref> [26] </ref>. The preprocessing method of tree-decomposition with hidden variables (Fusion, Section 3) is still not well developed. Causal Discovery. One of the most exciting prospects in recent years has been the possibility of using belief networks to discover causal relationships in raw statistical data.
Reference: [27] <author> P. Spirtes and C. Glymour, </author> <title> An algorithm for fast recovery of sparse causal graphs, </title> <note> Social Science Computer Review 9 (1) (1991) 62-72. </note>
Reference-contexts: A fast algorithm for recovering sparse networks is described by Spirtes and Glymour <ref> [27] </ref>, and Bayesian methods of computing the "probability that X is a cause for Y" were developed by Cooper and Herskovits [3]. Causal reorganization of categorical databases is studied in [4].
Reference: [28] <author> T. Verma, </author> <title> Causal networks: semantics and expressiveness, </title> <type> Tech. Report #R-65, </type> <institution> Cognitive Systems Laboratory, University of California, </institution> <address> Los Angeles, CA (1986). </address> <booktitle> Also in: Proceedings of the 4th Workshop on Uncertainty in Artificial Intelligence, </booktitle> <institution> Minneapolis, MN (Advanced Decision Systems, Mountain View, </institution> <address> CA) (1988) 352-359. </address> <month> 11 </month>
Reference-contexts: These patterns have since been used to define causation and to uncover causal relationships in data [21] (see also last paragraph in this section). Verma <ref> [28] </ref> has proved the soundness of the d-separation criterion using the semi-graphoid axioms [20], thus rendering the criterion valid for a wide class of informational dependencies, including probabilistic, graphical, correlational, and database dependencies.
References-found: 28

