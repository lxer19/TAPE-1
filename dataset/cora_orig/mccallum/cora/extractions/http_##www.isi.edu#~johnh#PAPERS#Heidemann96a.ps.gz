URL: http://www.isi.edu/~johnh/PAPERS/Heidemann96a.ps.gz
Refering-URL: http://www.isi.edu/~johnh/PAPERS/Heidemann96a.html
Root-URL: http://www.isi.edu
Email: @isi.edu.  
Title: Over Several Transport Protocols  
Author: John Heidemann Katia Obraczka Joe Touch 
Address: Admiralty Way, Marina del Rey, CA, 90292-6695,  
Date: June 6, 1997  
Web: HTTP  
Note: Modeling the Performance of  DRAFT  This research is supported by the Defense Advanced Research Projects Agency (DARPA) through FBI contracts #J-FBI-95-185 entitled "Cities Online", and #J-FBI-95-204, "Global Operating Systems Technologies". The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Department of the Army, DARPA, or the U.S. Government. The authors can be contacted at 4676  or by electronic mail to johnh, katia or touch, each  
Abstract: This paper is a draft that will appear in IEEE/ACM Transactions on Networking. Final editing is still expected. Please replace it with the final version when published. Abstract This paper considers the interaction of HTTP with several transport protocols, including TCP, Transaction TCP, a UDP-based request-response protocol, and HTTP with persistent TCP connections. We present an analytic model for each of these protocols and use that model to evaluate network overhead carrying HTTP traffic across a variety of network characteristics. This model includes an analysis of the transient effects of TCP slow-start. We validate this model by comparing it to network packet traces measured with two protocols (HTTP and persistent HTTP) over local and wide-area networks. We show that the model is accurate within 5% of measured performance for wide-area networks, but can underestimate latency when the bandwidth is high and delay is low. We use the model to compare the connection-setup costs of these protocols, bounding the possible performance improvement. We evaluate these costs for a range of network characteristics, finding that setup optimizations are relatively unimportant for current modem, ISDN, and LAN users but can pro This paper is copyright c fl1997 by the Institute of Electrical and Electronics Engineers. vide moderate to substantial performance improvement over high-speed WANs. We also use the model to predict performance over future network characteristics. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Berners-Lee, R. Cailliae, A. Luotonen, H. F. Nielsen, and A. </author> <title> Secret, "The World-Wide Web," </title> <journal> Communications of the ACM, </journal> <volume> vol. 37, </volume> <pages> pp. 76-82, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The World Wide Web <ref> [1] </ref> has rapidly become one of the most popular Internet services [2]. The popularity of the web has resulted in a corresponding popularity for HTTP, the standard Hyper-Text Transport Protocol [3, 4]. HTTP is layered over TCP. The strengths of TCP are well known.
Reference: [2] <author> V. Paxson, </author> <title> "Empirically-derived analytic models of wide-area TCP connections," </title> <journal> ACM/IEEE Transactions on Networking, </journal> <volume> vol. 2, </volume> <pages> pp. 316-336, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The World Wide Web [1] has rapidly become one of the most popular Internet services <ref> [2] </ref>. The popularity of the web has resulted in a corresponding popularity for HTTP, the standard Hyper-Text Transport Protocol [3, 4]. HTTP is layered over TCP. The strengths of TCP are well known. TCP is a well understood protocol with carefully tuned flow control and congestion avoidance algorithms [5].
Reference: [3] <author> T. Berners-Lee, R. Fielding, and H. Frystyk, </author> <title> "Hypertext transfer protocol|HTTP/1.0," RFC 1945, Internet Request For Comments, </title> <month> May </month> <year> 1995. </year>
Reference-contexts: 1 Introduction The World Wide Web [1] has rapidly become one of the most popular Internet services [2]. The popularity of the web has resulted in a corresponding popularity for HTTP, the standard Hyper-Text Transport Protocol <ref> [3, 4] </ref>. HTTP is layered over TCP. The strengths of TCP are well known. TCP is a well understood protocol with carefully tuned flow control and congestion avoidance algorithms [5]. These characteristics make TCP an excellent protocol for bulk data transport in congested networks.
Reference: [4] <author> R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee, </author> <title> "Hypertext transfer protocol| HTTP/1.1," RFC 2068, Internet Request For Comments, </title> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: 1 Introduction The World Wide Web [1] has rapidly become one of the most popular Internet services [2]. The popularity of the web has resulted in a corresponding popularity for HTTP, the standard Hyper-Text Transport Protocol <ref> [3, 4] </ref>. HTTP is layered over TCP. The strengths of TCP are well known. TCP is a well understood protocol with carefully tuned flow control and congestion avoidance algorithms [5]. These characteristics make TCP an excellent protocol for bulk data transport in congested networks. <p> By requiring fewer TCP connections than HTTP, P-HTTP also conserves server and network resources. 2 Padmanabhan and Mogul's results have been corroborated by Simon Spero in an unpublished study [13]. A version of P-HTTP is part of the specification of HTTP/1.1 <ref> [4] </ref>. Both Padmanabhan and Mogul's and Spero's analyses of HTTP overhead were founded on measurements between relatively well-connected Internet hosts (bandwidth about 1Mb/s, round-trip time 70ms). We show that our analytic model of performance allows us to extend these results to other networks. <p> When either bandwidth is low or delay is very low, TCP performs much better. 4.4 HTTP over TCP with connection caching P-HTTP [10] has been proposed as a solution to several problems resulting from running HTTP over TCP, and persistent connections are part of the HTTP/1.1 standard <ref> [4] </ref>. P-HTTP reuses a single TCP connection to amortize connection setup and congestion avoidance costs across several HTTP requests. When a transaction is completed, the TCP connection is left open for subsequent transactions. <p> The client made HTTP/1.0-style requests. For HTTP over caching TCP protocols we used the fourth beta version of Apache 1.1 with some modifications. This server implements "keep-alive" HTTP connections, an experimental implementation of persistent connection HTTP (abbreviated HTTP/1.0+KA) similar in character to persistent connections recently standardized as HTTP/1.1 <ref> [4] </ref>. This server was slightly modified to avoid two interactions between P-HTTP and TCP which substantially reduce performance [21].
Reference: [5] <author> V. Jacobson, </author> <title> "Congestion avoidance and control," </title> <booktitle> in Proceedings of the SIGCOMM '88, </booktitle> <pages> pp. 314-329, </pages> <publisher> ACM, </publisher> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: The popularity of the web has resulted in a corresponding popularity for HTTP, the standard Hyper-Text Transport Protocol [3, 4]. HTTP is layered over TCP. The strengths of TCP are well known. TCP is a well understood protocol with carefully tuned flow control and congestion avoidance algorithms <ref> [5] </ref>. These characteristics make TCP an excellent protocol for bulk data transport in congested networks. Web traffic is not ideally matched to TCP, however. In practice, web access is request-response oriented with bursts of numerous requests and small, unidirectional responses. <p> We next consider the effects of TCP's slow-start algorithm. 4.3.1 TCP slow-start TCP's slow-start algorithm limits transmission by a congestion window (cwnd) which is initialized to one segment and increases each time an ACK is received <ref> [5] </ref>. The size of increase changes: initially cwnd grows in one segment increments (the slow-start phase), then later by 1=cwnd (congestion avoidance phase). TCP is thus "clocked" by the flow of acknowledgments.
Reference: [6] <author> C. Cunha, A. Bestavros, and M. Crovella, </author> <title> "Characteristics of WWW client-based traces," </title> <type> Tech. Rep. 95-010, </type> <institution> Boston University, </institution> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Retrieval of a complete web page requires separate requests for text and each embedded image, thus making traffic inherently bursty. Responses are small to keep transmission times down; several studies have documented typical response size as less than 1KB <ref> [6] </ref>, 6KB [7], or 21KB [8]. Finally, web users often bounce rapidly from site to site, as verified by both client [6] and server side traces [8]. 1 1 For example, from the 1995-96 Boston University survey [6] we can deduce that an upper bound on the mean number of unique <p> Responses are small to keep transmission times down; several studies have documented typical response size as less than 1KB <ref> [6] </ref>, 6KB [7], or 21KB [8]. Finally, web users often bounce rapidly from site to site, as verified by both client [6] and server side traces [8]. 1 1 For example, from the 1995-96 Boston University survey [6] we can deduce that an upper bound on the mean number of unique URLs read from each site is 15.5. <p> times down; several studies have documented typical response size as less than 1KB <ref> [6] </ref>, 6KB [7], or 21KB [8]. Finally, web users often bounce rapidly from site to site, as verified by both client [6] and server side traces [8]. 1 1 For example, from the 1995-96 Boston University survey [6] we can deduce that an upper bound on the mean number of unique URLs read from each site is 15.5. The NCSA server-side traces suggest that clients read a mean of 2.92 text pages at their site per-browsing session. Unfortunately, TCP is poorly suited to frequent, short, request-response-style traffic.
Reference: [7] <author> J. </author> <title> Touch, "Defining `high speed' protocols : Five challenges and an example that survives the challenges," </title> <journal> IEEE Journal of Selected Areas in Communication, </journal> <volume> vol. 13, </volume> <pages> pp. 828-835, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Retrieval of a complete web page requires separate requests for text and each embedded image, thus making traffic inherently bursty. Responses are small to keep transmission times down; several studies have documented typical response size as less than 1KB [6], 6KB <ref> [7] </ref>, or 21KB [8].
Reference: [8] <author> M. F. Arlitt and C. L. Williamson, </author> <title> "Web server workload characterization: The search for invariants," </title> <booktitle> in Proceedings of the ACM SIGMETRICS, </booktitle> <pages> pp. 126-137, </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: Retrieval of a complete web page requires separate requests for text and each embedded image, thus making traffic inherently bursty. Responses are small to keep transmission times down; several studies have documented typical response size as less than 1KB [6], 6KB [7], or 21KB <ref> [8] </ref>. Finally, web users often bounce rapidly from site to site, as verified by both client [6] and server side traces [8]. 1 1 For example, from the 1995-96 Boston University survey [6] we can deduce that an upper bound on the mean number of unique URLs read from each site <p> Responses are small to keep transmission times down; several studies have documented typical response size as less than 1KB [6], 6KB [7], or 21KB <ref> [8] </ref>. Finally, web users often bounce rapidly from site to site, as verified by both client [6] and server side traces [8]. 1 1 For example, from the 1995-96 Boston University survey [6] we can deduce that an upper bound on the mean number of unique URLs read from each site is 15.5.
Reference: [9] <author> R. Braden, </author> <title> "Extending TCP for transactions| concepts," RFC 1379, Internet Request For Comments, </title> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: The NCSA server-side traces suggest that clients read a mean of 2.92 text pages at their site per-browsing session. Unfortunately, TCP is poorly suited to frequent, short, request-response-style traffic. Frequent connection setup and tear-down costs burden servers with many connections left in TIME WAIT state <ref> [9, 10] </ref>. Short connections can interact poorly with TCP's slow-start algorithm for congestion avoidance [10]. Finally, TCP's initial three-way handshake adds latency to each transaction [10]. These mismatches between the needs of HTTP and the services provided by TCP contribute to increased latency for most web users. <p> Pipelining reduces the number of packets transmitted and supports request independence (as discussed in Section 4.1). We discuss the performance implications of HTTP/1.1 with pipelining in Section 5.5. 2.2 Transaction TCP Transaction TCP, or T/TCP <ref> [9, 15] </ref>, was proposed to bridge the gap between the services provided by UDP and TCP for request-response applications. 2 T/TCP improves TCP performance by caching per-host information sufficient to bypass the TCP's three-way handshake and avoid slow start. 3 T/TCP also shortens TCP's TIME WAIT period from 240 to 12 <p> This connection is reused for subsequent requests; it will be closed if server or client demand is high, or when idle for a given length of time. Stevens has suggested the use of T/TCP for HTTP traffic [16]. T/TCP <ref> [9] </ref> is an extension of TCP enhanced to support transactions. T/TCP caches TCP connection setup information so that subsequent TCP connections avoid the three-way handshake and reduce slow-start cost.
Reference: [10] <author> V. N. Padmanabhan and J. C. Mogul, </author> <title> "Improving HTTP latency," </title> <booktitle> in Proceedings of the Second International World Wide Web Conference, </booktitle> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: The NCSA server-side traces suggest that clients read a mean of 2.92 text pages at their site per-browsing session. Unfortunately, TCP is poorly suited to frequent, short, request-response-style traffic. Frequent connection setup and tear-down costs burden servers with many connections left in TIME WAIT state <ref> [9, 10] </ref>. Short connections can interact poorly with TCP's slow-start algorithm for congestion avoidance [10]. Finally, TCP's initial three-way handshake adds latency to each transaction [10]. These mismatches between the needs of HTTP and the services provided by TCP contribute to increased latency for most web users. <p> Unfortunately, TCP is poorly suited to frequent, short, request-response-style traffic. Frequent connection setup and tear-down costs burden servers with many connections left in TIME WAIT state [9, 10]. Short connections can interact poorly with TCP's slow-start algorithm for congestion avoidance <ref> [10] </ref>. Finally, TCP's initial three-way handshake adds latency to each transaction [10]. These mismatches between the needs of HTTP and the services provided by TCP contribute to increased latency for most web users. Fundamentally, TCP is optimized for large-scale bulk data transport, while HTTP often needs a light-weight, request-response protocol. <p> Frequent connection setup and tear-down costs burden servers with many connections left in TIME WAIT state [9, 10]. Short connections can interact poorly with TCP's slow-start algorithm for congestion avoidance <ref> [10] </ref>. Finally, TCP's initial three-way handshake adds latency to each transaction [10]. These mismatches between the needs of HTTP and the services provided by TCP contribute to increased latency for most web users. Fundamentally, TCP is optimized for large-scale bulk data transport, while HTTP often needs a light-weight, request-response protocol. <p> That paper focuses on comparison of HTTP with and without persistent connections; this paper more accurately models slow-start and work-loads and analyzes additional protocols in more detail. 2.1 Persistent-Connection HTTP Padmanabhan and Mogul conducted experiments to quantify the cost of using TCP as HTTP's transport mechanism <ref> [10] </ref>. Their examination of a typical HTTP request-response demonstrated throughputs for short responses as small as 10% of the throughput obtainable by bulk data transfers under similar network conditions. They attribute these costs to TCP's connection setup and slow-start mechanisms. <p> They attribute these costs to TCP's connection setup and slow-start mechanisms. To amortize TCP's connection overhead over multiple HTTP interactions, Padmanabhan and Mogul propose a "persistent-connection" HTTP, or P-HTTP, a variant of HTTP that uses one TCP connection to carry multiple HTTP requests <ref> [10] </ref>. Mogul also investigates trace-driven simulations of HTTP and P-HTTP, demonstrating that P-HTTP can avoid these setup costs and achieve significantly better performance than HTTP when there is temporal locality in web accesses [12]. <p> It is well known that TCP is not the best protocol for such situations. When either bandwidth is low or delay is very low, TCP performs much better. 4.4 HTTP over TCP with connection caching P-HTTP <ref> [10] </ref> has been proposed as a solution to several problems resulting from running HTTP over TCP, and persistent connections are part of the HTTP/1.1 standard [4]. P-HTTP reuses a single TCP connection to amortize connection setup and congestion avoidance costs across several HTTP requests.
Reference: [11] <author> J. Touch, J. Heidemann, and K. Obraczka, </author> <title> "Analysis of HTTP performance." </title> <note> Released as web page http://www.isi.edu/lsam/publications /http-perf/, Currently submitted for publication, </note> <month> June </month> <year> 1996. </year> <month> 22 </month>
Reference-contexts: A simplified version of the HTTP over TCP and caching TCP models of this paper can be found elsewhere <ref> [11] </ref>. That paper focuses on comparison of HTTP with and without persistent connections; this paper more accurately models slow-start and work-loads and analyzes additional protocols in more detail. 2.1 Persistent-Connection HTTP Padmanabhan and Mogul conducted experiments to quantify the cost of using TCP as HTTP's transport mechanism [10].
Reference: [12] <author> J. C. </author> <title> Mogul, </title> <booktitle> "The case for persistent-connection HTTP," in Proceedings of the SIGCOMM '95, </booktitle> <pages> pp. 299-313, </pages> <publisher> ACM, </publisher> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Mogul also investigates trace-driven simulations of HTTP and P-HTTP, demonstrating that P-HTTP can avoid these setup costs and achieve significantly better performance than HTTP when there is temporal locality in web accesses <ref> [12] </ref>. By requiring fewer TCP connections than HTTP, P-HTTP also conserves server and network resources. 2 Padmanabhan and Mogul's results have been corroborated by Simon Spero in an unpublished study [13]. A version of P-HTTP is part of the specification of HTTP/1.1 [4]. <p> The fast-Internet case corresponds roughly to characteristics present in Mogul's and Spero's studies <ref> [12, 13] </ref>. DirecPC also presents measured values from a system with satellite down-link and a modem back-channel [18]. (We assume that the back-channel is not a factor limiting performance.) For several other networks we have had to estimate these parameters. Modem and ISDN figures employ measured latencies and theoretical bandwidths.
Reference: [13] <author> S. E. Spero, </author> <title> "Analysis of HTTP performance problems." </title> <note> http://sunsite.unc.edu/mdma-release/http-prob.html, 1995. </note>
Reference-contexts: By requiring fewer TCP connections than HTTP, P-HTTP also conserves server and network resources. 2 Padmanabhan and Mogul's results have been corroborated by Simon Spero in an unpublished study <ref> [13] </ref>. A version of P-HTTP is part of the specification of HTTP/1.1 [4]. Both Padmanabhan and Mogul's and Spero's analyses of HTTP overhead were founded on measurements between relatively well-connected Internet hosts (bandwidth about 1Mb/s, round-trip time 70ms). <p> The fast-Internet case corresponds roughly to characteristics present in Mogul's and Spero's studies <ref> [12, 13] </ref>. DirecPC also presents measured values from a system with satellite down-link and a modem back-channel [18]. (We assume that the back-channel is not a factor limiting performance.) For several other networks we have had to estimate these parameters. Modem and ISDN figures employ measured latencies and theoretical bandwidths. <p> performance is dominated by response size. 9 The zero processing-time assumption is clearly incorrect; request-handling overhead depends strongly on server 7 The front page (http://www.yahoo.com) at Yahoo on May 1, 1996. 8 The front page (http://www.gnn.com) at GNN on May 1, 1996. 9 Some older browsers had substantially longer requests <ref> [13] </ref>. Performance concerns are one reason modern browsers keep requests short. hardware, software, and load. We remove this effect from our computations to focus instead on the network protocol aspects of HTTP instead of server implementation.
Reference: [14] <author> H. F. Nielsen, J. Gettys, A. Baird-Smith, E. Prud'hommeaux, H. W. Lie, and C. Lilley, </author> <title> "Network performance effects of HTTP/1.1, CSS1, </title> <journal> and PNG." </journal> <note> NOTE-pipelining-970207, available as web page http://www.w3.org/pub/WWW/- Protocols/HTTP/Performance/Pipeline.html, 7 February 1997. </note>
Reference-contexts: A recent technical note has suggested that use of pipelining is important to get good performance from the HTTP/1.1 implementation of P-HTTP <ref> [14] </ref>. Pipelining reduces the number of packets transmitted and supports request independence (as discussed in Section 4.1). <p> Since the models capture the essence of performance in such networks, comparisons between the models should correspond to comparisons between the protocols operating in actual networks. 5.5 Additional Validation A recent technical note by the World-Wide Web Consortium has suggested that pipelining substantially reduces packet counts for HTTP/1.1 <ref> [14] </ref>. We call the resulting protocol persistent-connection HTTP with pipelining, abbreviated HTTP/1.1+P. A comparison of their results with our model's predictions is particularly interesting both because their observations are made with different client and server software and because they have optimized the buffering of their system to improve performance. <p> Their server software was either Apache or Jigsaw. A complete description of their methodology can be found in their technical note <ref> [14] </ref>. Table 8 summarizes the results of their measurements and our predictions. The adjusted portion of the prediction corresponds to addition of a 3:7ms server processing time. The N-Ethernet and N-Fast-Internet cases show substantial discrepancy from our predicted values. <p> These experiments use the workload described in Section 5.5. Basic indicates our basic (unadjusted) model, adjusted is the model adjusted for processing time, measurement indicates performance as measured in <ref> [14] </ref> with two values for N-Fast-Internet as described in Section 5.5, ratio m:a shows the ratio of the measurement to prediction/adjusted. Ethernet case at this time, although, as described in Section 5.4, in LANs, per-packet processing (which is not considered in our model) can overwhelm connection startup costs. <p> The performance improvement always approaches a workload-dependent limit as the bandwidth-delay product rises; in this case the asymptote is 2, the ratio of 8:4 (non-caching:caching) round-trip delays. A recent technical note by W3C has suggested that pipelining substantially reduces packet counts for persistent-connection HTTP <ref> [14] </ref>. Although they substantially reduce packet counts, their measurements of elapsed times support the conclusion that HTTP over caching-TCP protocols offer comparatively modest performance improvements over low-bandwidth-delay connections today but can provide substantial improvement when conditions approach the Fast-Internet case.
Reference: [15] <author> R. Braden, </author> <title> "T/TCP|TCP extensions for transactions functional specification," RFC 1644, Internet Request For Comments, </title> <month> July </month> <year> 1994. </year>
Reference-contexts: Pipelining reduces the number of packets transmitted and supports request independence (as discussed in Section 4.1). We discuss the performance implications of HTTP/1.1 with pipelining in Section 5.5. 2.2 Transaction TCP Transaction TCP, or T/TCP <ref> [9, 15] </ref>, was proposed to bridge the gap between the services provided by UDP and TCP for request-response applications. 2 T/TCP improves TCP performance by caching per-host information sufficient to bypass the TCP's three-way handshake and avoid slow start. 3 T/TCP also shortens TCP's TIME WAIT period from 240 to 12
Reference: [16] <author> W. R. </author> <title> Stevens, </title> <journal> TCP/IP Illustrated, </journal> <volume> vol. 3. </volume> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: Stevens compares the time to complete a client-server transaction using TCP, UDP, and T/TCP for different sizes of the request and reply over Pentium-based hardware on a 10Mb/s Ethernet <ref> [16] </ref>. <p> This connection is reused for subsequent requests; it will be closed if server or client demand is high, or when idle for a given length of time. Stevens has suggested the use of T/TCP for HTTP traffic <ref> [16] </ref>. T/TCP [9] is an extension of TCP enhanced to support transactions. T/TCP caches TCP connection setup information so that subsequent TCP connections avoid the three-way handshake and reduce slow-start cost.
Reference: [17] <author> B. C. Neuman, </author> <title> The Virtual System Model: A Scalable Approach to Organizing Large Systems. </title> <type> Ph.D. dissertation, </type> <institution> University of Washington, </institution> <year> 1992. </year>
Reference-contexts: ARDP was proposed and implemented as the transport mechanism for the Prospero information discovery tool <ref> [17] </ref>. ARDP's main design goal is to provide a reliable yet light-weight communication mechanism to transport requests and responses between clients and servers. The current version of ARDP (in development) borrows TCP-style flow-control, congestion-avoidance, and retransmission algorithms. 4 ARDP avoids TCP's three-way handshake, instead randomly selecting connection identifiers.
Reference: [18] <author> D. DeLucia, </author> <title> "Direcpc performance." </title> <type> Personal communication., </type> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: The fast-Internet case corresponds roughly to characteristics present in Mogul's and Spero's studies [12, 13]. DirecPC also presents measured values from a system with satellite down-link and a modem back-channel <ref> [18] </ref>. (We assume that the back-channel is not a factor limiting performance.) For several other networks we have had to estimate these parameters. Modem and ISDN figures employ measured latencies and theoretical bandwidths. Fast-Ethernet and ADSL use theoretical bandwidths and latencies suggested by similar systems (10Mb/s Ethernet and ISDN, respectively).
Reference: [19] <author> R. Braden, </author> <title> "Requirements for Internet hosts| communication layers," RFC 1122, Internet Request For Comments, </title> <address> Oct. </address> <year> 1989. </year>
Reference-contexts: First, in BSD-derived TCP implementations the ACK of the SYN packet on the HTTP server opens the congestion window, so the cwnd for the reply begins at 2. Second, TCP's delayed-acknowledgment algorithm normally causes the client to ACK every other segment, not each segment <ref> [19] </ref>. Because the congestion window opens per ACK received rather than per segment acknowledged, the slow-start window opens much slower than is usually assumed.
Reference: [20] <author> J. Mogul and S. Deering, </author> <title> "Path MTU discovery," RFC 1191, Internet Request For Comments, </title> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Second, most LANs allow TCP segments much larger than are typical in WANs (1460 bytes instead of 512 or 536 bytes), although wide deployment of (S TCP =S min ), for a 512B segment size and the small-cluster workload. path-MTU discovery supports larger WAN segment size <ref> [20] </ref>. An interesting observation is that employing slow-start does not substantially affect performance over 10Mb/s Ethernet because large segment-size and low round-trip time result in a small muws.
Reference: [21] <author> J. Heidemann, </author> <title> "Performance interactions between P-HTTP and TCP implementations," </title> <journal> ACM Computer Communication Review, </journal> <volume> vol. 27, </volume> <pages> pp. 65-73, </pages> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: Finally, if we assume that the first transaction results in a cache-hit for a caching-TCP protocol (Equation 13), then caching-TCP has no overhead. Thus, when caches last long enough to encompass access of multiple clusters, caching protocols are very helpful. Implementation issues can limit this benefit, however <ref> [21] </ref>. 4.5 HTTP over Multiple, Concurrent TCP Connections Many web browsers open multiple concurrent connections to mitigate TCP start-up costs (HTTP over parallel connections). We can bound their performance by HTTP over TCP with and without connection caching. <p> This server implements "keep-alive" HTTP connections, an experimental implementation of persistent connection HTTP (abbreviated HTTP/1.0+KA) similar in character to persistent connections recently standardized as HTTP/1.1 [4]. This server was slightly modified to avoid two interactions between P-HTTP and TCP which substantially reduce performance <ref> [21] </ref>. <p> Another source of error in our model results from interactions between the application-level behavior and the underlying TCP implementation. In the course of validation we found two such interactions that crippled HTTP/1.0+KA performance <ref> [21] </ref>. In both cases, packets shorter than maximum-segment-size caused our TCP connection to wait for a delayed acknowledgment, stalling data transfer for up to 200ms. <p> We believe that this stall is due to an interaction between a short TCP segment and TCP silly-window avoidance [22] similar to the odd/short-final-segment problem we encountered in our experiments <ref> [21] </ref>. If so, this interaction can be avoided by appropriate buffering. We correct for it by subtracting 1 second from the measured times. With this correction our model is much closer to the measured values which are 1.15-1.49 times slower. <p> Finally, validation of our model has led to insight into request-response protocol design and suggested several areas for future work. Validation of these experiments have detected interactions between application- and kernel-level networking that substantially reduce performance <ref> [21] </ref>. A broader question is how to optimize TCP for brief, request-response-style traffic. We are currently exploring two approaches to this problem. <p> Given a large initial window, we are investigating how a rate-limited addition to slow-start can prevent overloading intermediate routers <ref> [21] </ref>. We have generalized our experiences with TCP to other transport protocols. We have also found that the performance of protocols that fail to cache congestion-control information suffers in high-bandwidth-delay conditions, and have modified our design for ARDP accordingly.
Reference: [22] <author> D. D. Clark, </author> <title> "Window and acknowlegement strategy in TCP," RFC 813, Internet Request For Comments, </title> <month> July </month> <year> 1982. </year>
Reference-contexts: Examination of their traces for this network configuration shows a consistent stall of about 1 second following the third segment of the reply. We believe that this stall is due to an interaction between a short TCP segment and TCP silly-window avoidance <ref> [22] </ref> similar to the odd/short-final-segment problem we encountered in our experiments [21]. If so, this interaction can be avoided by appropriate buffering. We correct for it by subtracting 1 second from the measured times.
Reference: [23] <author> J. </author> <title> Touch, "TCP control block interdependence," RFC 2140, Internet Request For Comments, </title> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: A broader question is how to optimize TCP for brief, request-response-style traffic. We are currently exploring two approaches to this problem. We are examining how TCP congestion-control information should be initialized for multiple connections separated by space or time <ref> [23] </ref>; this work investigates alternatives to divide bandwidth among existing and new connections and for reusing cached congestion information. Given a large initial window, we are investigating how a rate-limited addition to slow-start can prevent overloading intermediate routers [21]. We have generalized our experiences with TCP to other transport protocols.
Reference: [24] <author> S. Shenker, L. Zhang, and D. D. Clark, </author> <title> "Some observations on the dynamics of a congestion control algorithm," </title> <journal> ACM Computer Communication Review, </journal> <volume> vol. 20, </volume> <pages> pp. 30-39, </pages> <month> Oct. </month> <year> 1990. </year>
References-found: 24

