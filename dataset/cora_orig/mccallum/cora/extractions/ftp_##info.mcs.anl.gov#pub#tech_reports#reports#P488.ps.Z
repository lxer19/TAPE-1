URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P488.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Title: COMPUTING GRADIENTS IN LARGE-SCALE OPTIMIZATION USING AUTOMATIC DIFFERENTIATION  
Phone: 60439  
Author: Christian H. Bischof, Ali Bouaricha, Peyvand M. Khademi, Jorge J. More 
Note: (Final Revised Version)  
Date: January 1995  June 1996  
Address: 9700 South Cass Avenue Argonne, Illinois  Preprint MCS-P488-0195  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  
Abstract: Work supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38, by the National Aerospace Agency under Purchase Order L25935D, and by the National Science Foundation, through the Center for Research on Parallel Computation, under Cooperative Agreement No. CCR-9120008. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> AVERICK, B., CARTER, R., MOR E, J., and XUE, G.-L., </author> <year> 1992. </year> <title> The MINPACK-2 test problem collection, </title> <type> Technical Report ANL/MCS-TM-150, Revised, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory. </institution>
Reference-contexts: The selected problems are representative of large-scale optimization problems arising from applications in superconductivity, optimal design, combustion, and lubrication. We give only a brief description of two of these problems to illustrate the partially separable structure of these problems. For further information refer to <ref> [1] </ref>. The Ginzburg-Landau (GL2) problem is of the form (10), where v : IR 2 7! IR 4 .
Reference: [2] <author> AVERICK, B. and MOR E, J., </author> <year> 1994. </year> <title> Evaluation of large-scale optimization problems on vector and parallel architectures, </title> <journal> SIAM Journal on Optimization 4, </journal> <pages> 708-721. </pages>
Reference-contexts: The above ratio can be expected for well-coded gradient computations on scalar architectures but requires special techniques on vector and parallel architectures <ref> [2] </ref>. On vector architectures we can expect the ratio (14) to hold only if both the function and the gradient evaluation codes vectorize or if neither code vectorizes.
Reference: [3] <author> AVERICK, B., MOR E, J., BISCHOF, C., CARLE, A., and GRIEWANK, A., </author> <year> 1994. </year> <title> Computing large sparse Jacobian matrices using automatic differentiation, </title> <journal> SIAM Journal Scientific and Statistical Computing 15, </journal> <pages> 285-294. 19 </pages>
Reference-contexts: By exploiting the capability to compute directional derivatives (7), we can easily compute compressed Jacobian matrices via automatic differentiation (for additional details, see <ref> [3] </ref>): Given the seed matrix S, ADIFOR-generated code computes the compressed Jacobian matrix f 0 (x)S. In contrast to the approximation techniques based on the compressed Jaco-bian matrix approach [13, 15], all columns of the compressed Jacobian matrix are computed at once. <p> We do not elaborate further on this point because this contrast in accuracies between automatic differentiation and function differences shows consistency with previously published work <ref> [3] </ref> on the computation of sparse Jacobian matrices with automatic differentiation. 11 Table 1: Memory Requirements for GL2 (n = 160; 000, p = 8) Platform F FD FD/F AD AD/F Sparse AD Sparse AD/F SPARC / IBM 2.59 31.39 12.1 48.13 18.6 38.65 15.0 Cray C90 3.07 42.76 13.9 59.50
Reference: [4] <author> BISCHOF, C., CARLE, A., CORLISS, G., GRIEWANK, A., and HOVLAND, P., </author> <year> 1992. </year> <title> ADIFOR: Generating derivative codes from Fortran programs, </title> <booktitle> Scientific Programming 1(1), </booktitle> <pages> 11-29. </pages>
Reference-contexts: The interpretation overhead associated with using this trace for the purposes of automatic differentiation, as well as its potentially very large size, can be a serious computational bottleneck [30]. Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR <ref> [4, 6] </ref>, ADIC [10], AMC [16], and Odyssee [29] tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77.
Reference: [5] <author> BISCHOF, C., CARLE, A., and KHADEMI, P., </author> <year> 1994. </year> <title> Fortran 77 interface specification to the SparsLinC library, </title> <type> Technical Report ANL/MCS-TM-196, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory. </institution>
Reference-contexts: By default, this operation is implemented as a DO loop; and as long as p is of moderate size and the vectors 6 are dense, this is an efficient way of expressing a vector linear combination. The SparsLinC library <ref> [5, 6] </ref> addresses the situation where the seed matrix S is sparse and most of the vectors involved in the computation of f 0 (x)S are sparse.
Reference: [6] <author> BISCHOF, C., CARLE, A., KHADEMI, P., and MAUER, A., </author> <year> 1994. </year> <title> The ADIFOR 2.0 system for the automatic differentiation of Fortran 77 programs, </title> <type> Preprint MCS-P481-1194, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, and CRPC-TR94491, Center for Research on Parallel Computation, Rice University. Forthcoming in IEEE Computational Science & Engineering. </institution>
Reference-contexts: The interpretation overhead associated with using this trace for the purposes of automatic differentiation, as well as its potentially very large size, can be a serious computational bottleneck [30]. Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR <ref> [4, 6] </ref>, ADIC [10], AMC [16], and Odyssee [29] tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77. <p> By default, this operation is implemented as a DO loop; and as long as p is of moderate size and the vectors 6 are dense, this is an efficient way of expressing a vector linear combination. The SparsLinC library <ref> [5, 6] </ref> addresses the situation where the seed matrix S is sparse and most of the vectors involved in the computation of f 0 (x)S are sparse. <p> This situation arises, for example, in the computation of large sparse Jacobian matrices, since the sparsity of the final Jacobian matrix implies that, with great probability, all intermediate derivative computations involve sparse vectors as well. SparsLinC implements routines for executing the vector linear combination (9) using sparse data structures <ref> [6] </ref>. It is fully integrated into ADIFOR and ADIC and provides a mechanism for transparently exploiting sparsity in derivative computations. SparsLinC does not require knowledge of the sparsity structure of the Jacobian matrix; indeed, the sparsity structure of the Jacobian matrix is a byproduct of the derivative computation.
Reference: [7] <author> BISCHOF, C., GREEN, L., HAIGLER, K., and KNAUFF, T., </author> <year> 1994. </year> <title> Parallel calculation of sensitivity derivatives for aircraft design using automatic differentiation, </title> <booktitle> Proceedings of the 5th AIAA/NASA/USAF/ISSMO Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <pages> AIAA 94-4261, 73-84. </pages> <institution> American Institute of Aeronautics and Astronautics. </institution>
Reference-contexts: The results in Table 6 show that the AD approach outperforms the FD approach on scalar architectures. The performance of the various approaches on vector architectures is harder to predict as performance depends on the delicate interplay between the code and the compiler (for examples, see <ref> [7, 11] </ref>). Note that the results in Table 6 show that the performance of AD is comparable to that of FD on the Cray C90 for those problems (MSA and ODC) where the function evaluation code fails to vectorize.
Reference: [8] <author> BISCHOF, C., KHADEMI, P., BOUARICHA, A., and CARLE, A., </author> <year> 1996. </year> <title> Efficient computation of gradients and Jacobians by transparent exploitation of sparsity in automatic differentiation, </title> <booktitle> Optimization Methods and Software 7(1), </booktitle> <pages> 1-39. </pages>
Reference-contexts: Certainly, the memory needed for representing the sparse Jacobian matrix has a lower bound of nnz (f 0 (x)). Beyond this, SparsLinC requires additional memory for internal representations as explained in <ref> [8] </ref>. The first column in Tables 1-3 shows the memory required for running the original function. Memory requirements for the hand-coded MINPACK-2 gradient codes are not shown separately, but are always between a factor of 1.5-2 times the memory requirements of the corresponding function. <p> We also note the large variation in T for the Sparse AD results on the SPARC 10. This results from the way SparsLinC exploits the particular sparsity characteristics of each problem (this issue is explored in <ref> [8] </ref>). Finally, we note that the performance of Sparse AD degrades on vector computers, as a result of pervasive use of indirect addressing and lack of vector instructions, though this performance could be improved through the use of hardware-supported gather/scatter instructions.
Reference: [9] <author> BISCHOF, C. and EL-KHADIRI, M., </author> <year> 1992. </year> <title> Extending compile-time reverse mode and exploiting partial separability in ADIFOR, </title> <type> Technical Report ANL/MCS-TM-163, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory. </institution>
Reference: [10] <author> BISCHOF, C., JONES, W., MAUER, A., and SAMAREH, J., </author> <year> 1996. </year> <title> Experiences with the application of the ADIC automatic differentiation tool to the CSCMDO 3-D volume grid generation code, </title> <booktitle> Proceedings of the 34th AIAA Aerospace Sciences Meeting, </booktitle> <pages> pages AIAA 96-0716. </pages> <institution> American Institute of Aeronautics and Astronomics. </institution>
Reference-contexts: The interpretation overhead associated with using this trace for the purposes of automatic differentiation, as well as its potentially very large size, can be a serious computational bottleneck [30]. Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR [4, 6], ADIC <ref> [10] </ref>, AMC [16], and Odyssee [29] tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77. ADIFOR and ADIC mainly use the forward mode, with the reverse mode at the statement level, while AMC and Odyssee use the reverse mode.
Reference: [11] <author> BOUARICHA, A., and MOR E, J., </author> <year> 1995. </year> <title> Impact of partial separability on large-scale optimization. </title> <type> Preprint MCS-P487-0195, </type> <institution> Argonne National Laboratory, Argonne, Illinois. </institution> <note> Forthcoming in Computational Optimization and Applications. </note>
Reference-contexts: In many situations it is desirable to have a tool for the determination of f 0 (x) that does not require knowledge of the sparsity pattern of f 0 (x). This situation arises, for example, while developing interfaces for the solution of large-scale optimization problems <ref> [11] </ref>, where it is desirable to relieve the user of the error-prone task of providing the sparsity pattern. In these situations, a sparse implementation of automatic differentiation, such as provided by the ADIFOR/SparsLinC approach, is the only feasible approach. <p> The results in Table 6 show that the AD approach outperforms the FD approach on scalar architectures. The performance of the various approaches on vector architectures is harder to predict as performance depends on the delicate interplay between the code and the compiler (for examples, see <ref> [7, 11] </ref>). Note that the results in Table 6 show that the performance of AD is comparable to that of FD on the Cray C90 for those problems (MSA and ODC) where the function evaluation code fails to vectorize.
Reference: [12] <author> COLEMAN, T., GARBOW, B., and MOR E, J., </author> <year> 1984. </year> <title> Fortran subroutines for estimating sparse Jacobian matrices, </title> <journal> ACM Transactions on Mathematical Software 10,346-347. </journal>
Reference-contexts: For example, if a matrix is banded with bandwidth fi or if it can be permuted to a matrix with bandwidth fi, it can be shown [13] that p fi. In our experiments we employ the graph-coloring software described in <ref> [12] </ref> to determine an appropriate partition. In an optimization algorithm we invariably need to compute a sequence frf 0 (x k )g of 7 gradients for some sequence fx k g of iterates. This step requires the computation of a sequence of Jacobian matrices ff 0 (x k )g. <p> Another reason for the high relative cost of computing the graph coloring is that the algorithm we employ (subroutine DSM from Coleman, Garbow, and More <ref> [12] </ref>) is intended to produce graph colorings with a small p by employing several heuristics. The runtime of subroutine DSM could be reduced by a factor of two or more without a substantial increase in p by only using one of the heuristics.
Reference: [13] <author> COLEMAN, T. and MOR E, J., </author> <year> 1983. </year> <title> Estimation of sparse Jacobian matrices and graph coloring problems, </title> <journal> SIAM Journal on Numerical Analysis 20, </journal> <pages> 187-209. 20 </pages>
Reference-contexts: The compressed Jacobian matrix approach has long been used in connection with the determination of sparse Jacobian matrices by differences of function values; see, for example, <ref> [13, 15] </ref>. As we mentioned in Section 1, the compressed Jacobian matrix approach requires the determination of a partitioning of the columns of f 0 (x) into structurally orthogonal columns. <p> Because of the structural orthogonality property, we can uniquely extract all entries of the original Jacobian matrix from the compressed Jacobian. The partitioning problem can be considered as a graph-coloring problem <ref> [13] </ref>. Given a graph representation of the sparsity structure of f 0 (x), these algorithms produce a partitioning of the columns of f 0 (x) into p structurally orthogonal groups by graph-coloring algorithms for the column-intersection graph associated with f 0 (x). <p> For many sparsity patterns, p is small and independent of n. For example, if a matrix is banded with bandwidth fi or if it can be permuted to a matrix with bandwidth fi, it can be shown <ref> [13] </ref> that p fi. In our experiments we employ the graph-coloring software described in [12] to determine an appropriate partition. In an optimization algorithm we invariably need to compute a sequence frf 0 (x k )g of 7 gradients for some sequence fx k g of iterates. <p> In contrast to the approximation techniques based on the compressed Jaco-bian matrix approach <ref> [13, 15] </ref>, all columns of the compressed Jacobian matrix are computed at once. In many situations it is desirable to have a tool for the determination of f 0 (x) that does not require knowledge of the sparsity pattern of f 0 (x).
Reference: [14] <author> CONN, A.R., GOULD, N.I.M., and TOINT, </author> <title> P.L., </title> <booktitle> 1992. LANCELOT, Springer Series in Computational Mathematics, </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: [15] <author> CURTIS, A., POWELL, M., and REID, J., </author> <year> 1974. </year> <title> On the estimation of sparse Jacobian matrices, </title> <journal> J. Inst. Math. Appl. 13,117-119. </journal>
Reference-contexts: The compressed Jacobian matrix approach has long been used in connection with the determination of sparse Jacobian matrices by differences of function values; see, for example, <ref> [13, 15] </ref>. As we mentioned in Section 1, the compressed Jacobian matrix approach requires the determination of a partitioning of the columns of f 0 (x) into structurally orthogonal columns. <p> In contrast to the approximation techniques based on the compressed Jaco-bian matrix approach <ref> [13, 15] </ref>, all columns of the compressed Jacobian matrix are computed at once. In many situations it is desirable to have a tool for the determination of f 0 (x) that does not require knowledge of the sparsity pattern of f 0 (x).
Reference: [16] <author> GIERING, R., </author> <year> 1992. </year> <title> Adjoint model compiler, </title> <note> manual version 0.2, AMC version 2.04, Technical Report, </note> <institution> Max-Planck Institut fur Meteorologie. </institution>
Reference-contexts: The interpretation overhead associated with using this trace for the purposes of automatic differentiation, as well as its potentially very large size, can be a serious computational bottleneck [30]. Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR [4, 6], ADIC [10], AMC <ref> [16] </ref>, and Odyssee [29] tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77. ADIFOR and ADIC mainly use the forward mode, with the reverse mode at the statement level, while AMC and Odyssee use the reverse mode.
Reference: [17] <author> GRIEWANK, A. and TOINT, P.L., </author> <year> 1982. </year> <title> On the unconstrained optimization of partially separable functions, in Nonlinear Optimization 1981 (M. </title> <editor> J. D. Powell, ed.), </editor> <publisher> Academic Press. </publisher>
Reference: [18] <author> GRIEWANK, A. and TOINT, P.L., </author> <year> 1982. </year> <title> Partitioned variable metric updates for large structured optimization problems, </title> <journal> Numerische Mathematik 39, </journal> <pages> 119-137. </pages>
Reference: [19] <author> GRIEWANK, A. and TOINT, P.L., </author> <year> 1984. </year> <title> `Numerical experiments with partially separable optimization problems, in Numerical Analysis: </title> <booktitle> Proceedings Dundee 1983 (D. </booktitle> <editor> F. Griffiths, ed.), </editor> <booktitle> Lecture Notes in Mathematics 1066, </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: [20] <author> GRIEWANK, A., </author> <year> 1992. </year> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation, </title> <journal> Optimization Methods and Software, </journal> <volume> 1(1), </volume> <pages> 35-54. </pages>
Reference-contexts: The storage requirement of the reverse mode, however, can be a difficulty because of the possible dependence on L ff g+M ff g . Griewank <ref> [20] </ref> suggested a snapshot approach to circumvent this difficulty. There have been various implementations of automatic differentiation; an extensive survey can be found in [25]. In particular, we mention GRESS [24], and PADRE-2 [26] for Fortran programs and ADOL-C [23] for C programs.
Reference: [21] <author> GRIEWANK, A., </author> <year> 1993. </year> <title> Some bounds on the complexity of gradients, Jacobians, </title> <editor> and Hessians, in P.M. Pardalos, editor, </editor> <booktitle> Complexity in Nonlinear Optimization, </booktitle> <pages> 128-161. </pages> <publisher> World Scientific Publishers. </publisher>
Reference-contexts: by the computation of f (x), then an AD-generated code employing the forward mode requires L f 0 (x)S (2 + 3p) L ff g ; M f 0 (x)S (1 + p) M ff g : (8) floating-point operations and memory, respectively, to compute f 0 (x)S (see Griewank <ref> [21] </ref>). With the reverse mode, on the other hand, we can compute f 0 (x) T Q where, Q is a seed matrix with m rows and q columns.
Reference: [22] <author> GRIEWANK, A. and CORLISS, G., eds., </author> <year> 1991. </year> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <institution> Society for Industrial and Applied Mathematics. </institution>
Reference-contexts: 1 The ADIFOR Tool and the SparsLinC Library Automatic differentiation <ref> [22, 28] </ref> (AD) is a chain-rule-based technique for evaluating the derivatives of functions defined by computer programs. AD produces code that, in the absence of floating-point exceptions, computes the values of the analytical derivatives accurate to machine precision.
Reference: [23] <author> GRIEWANK, A., JUEDES, D., and UTKE, J., </author> <year> 1996. </year> <title> ADOL-C: A package for the automatic differentiation of algorithms written in C/C+, </title> <journal> ACM Transactions on Mathematical Software 22, </journal> <pages> 131-167. </pages>
Reference-contexts: Griewank [20] suggested a snapshot approach to circumvent this difficulty. There have been various implementations of automatic differentiation; an extensive survey can be found in [25]. In particular, we mention GRESS [24], and PADRE-2 [26] for Fortran programs and ADOL-C <ref> [23] </ref> for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse modes. In order to save control flow information and intermediate values, these tools generate a trace of the computation by recording the particulars of every operation performed in the code.
Reference: [24] <author> HORWEDEL, J., </author> <year> 1991. </year> <title> GRESS: A preprocessor for sensitivity studies on Fortran programs, </title> <booktitle> [22], </booktitle> <pages> 243-250. </pages>
Reference-contexts: Griewank [20] suggested a snapshot approach to circumvent this difficulty. There have been various implementations of automatic differentiation; an extensive survey can be found in [25]. In particular, we mention GRESS <ref> [24] </ref>, and PADRE-2 [26] for Fortran programs and ADOL-C [23] for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse modes.
Reference: [25] <author> JUEDES, D., </author> <year> 1991. </year> <title> A taxonomy of automatic differentiation tools, </title> <booktitle> [22], </booktitle> <pages> 315-330. </pages>
Reference-contexts: The storage requirement of the reverse mode, however, can be a difficulty because of the possible dependence on L ff g+M ff g . Griewank [20] suggested a snapshot approach to circumvent this difficulty. There have been various implementations of automatic differentiation; an extensive survey can be found in <ref> [25] </ref>. In particular, we mention GRESS [24], and PADRE-2 [26] for Fortran programs and ADOL-C [23] for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse modes.
Reference: [26] <author> KUBOTA, K., </author> <year> 1991. </year> <title> PADRE2, a FORTRAN precompiler yielding error estimates and second derivatives, </title> <booktitle> [22], </booktitle> <pages> 251-262. 21 </pages>
Reference-contexts: Griewank [20] suggested a snapshot approach to circumvent this difficulty. There have been various implementations of automatic differentiation; an extensive survey can be found in [25]. In particular, we mention GRESS [24], and PADRE-2 <ref> [26] </ref> for Fortran programs and ADOL-C [23] for C programs. GRESS, PADRE-2, and ADOL-C implement both the forward and reverse modes. In order to save control flow information and intermediate values, these tools generate a trace of the computation by recording the particulars of every operation performed in the code.
Reference: [27] <author> LESCRENIER, M., </author> <year> 1988. </year> <title> Partially separable optimization and parallel computing, </title> <journal> Annals Operations Research 14, </journal> <pages> 213-224. </pages>
Reference: [28] <author> RALL, L., </author> <year> 1981. </year> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: 1 The ADIFOR Tool and the SparsLinC Library Automatic differentiation <ref> [22, 28] </ref> (AD) is a chain-rule-based technique for evaluating the derivatives of functions defined by computer programs. AD produces code that, in the absence of floating-point exceptions, computes the values of the analytical derivatives accurate to machine precision.
Reference: [29] <author> ROSTAING, N., DALMAS, S., and GALLIGO, A., </author> <year> 1993. </year> <title> Automatic differentiation in Odyssee, </title> <journal> Tellus, </journal> <volume> 45a(5), </volume> <pages> 558-568. </pages>
Reference-contexts: Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR [4, 6], ADIC [10], AMC [16], and Odyssee <ref> [29] </ref> tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77. ADIFOR and ADIC mainly use the forward mode, with the reverse mode at the statement level, while AMC and Odyssee use the reverse mode.
Reference: [30] <author> SOULIE, E. </author> <title> User's experience with Fortran compilers for least squares problems, </title> <booktitle> [22], </booktitle> <pages> 297-306. </pages>
Reference-contexts: The interpretation overhead associated with using this trace for the purposes of automatic differentiation, as well as its potentially very large size, can be a serious computational bottleneck <ref> [30] </ref>. Recently, a source transformation approach to automatic differentiation has been explored in the ADIFOR [4, 6], ADIC [10], AMC [16], and Odyssee [29] tools. ADIFOR transforms Fortran 77 code, ADIC transforms ANSI-C code, and AMC and Odyssee transform a subset of Fortran 77.
Reference: [31] <author> TOINT, P.L., </author> <year> 1986. </year> <title> Numerical solution of large sets of algebraic nonlinear equations, </title> <booktitle> Mathematics of Computation 46, </booktitle> <pages> 175-189. </pages>
Reference: [32] <author> TOINT, P.L., </author> <year> 1987. </year> <title> On large scale nonlinear least squares calculations, </title> <journal> SIAM Journal Scientific and Statistical Computing 8, </journal> <pages> 416-435. </pages>
Reference: [33] <author> TOINT, P.L., and TUYTTENS, D., </author> <year> 1990. </year> <title> On large-scale nonlinear network optimiz ation, </title> <booktitle> Mathematical Programming 48, </booktitle> <pages> 125-159. </pages>
Reference: [34] <author> TOINT, P.L., and TUYTTENS, D., </author> <year> 1992. </year> <title> LSNNO: A Fortran subroutine for solving large-scale nonlinear network optimization problems, </title> <journal> ACM Transactions on Mathematical Software 18, </journal> <pages> 308-328. 22 </pages>
References-found: 34

