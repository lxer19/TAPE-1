URL: http://www.cs.helsinki.fi/~hahonen/ahonen_podp94.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~hahonen/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Generating grammars for SGML tagged texts lacking DTD  
Author: Helena Ahonen Heikki Mannila 
Keyword: structured documents, SGML, grammatical inference, inductive inference, context-free grammars  
Note: Research Centre for Domestic Languages  
Address: Helsinki  Erja Nikunen  
Affiliation: University of  University of Helsinki  
Abstract: We describe a technique for forming a context free grammar for a document that has some kind of tagging | structural or typographical | but no concise description of the structure is available. The technique is based on ideas from machine learning. It forms first a set of finite-state automata describing the document completely. These automata are modified by considering certain context conditions; the modifications correspond to generalizing the underlying languages. Finally, the automata are converted into regular expressions, which are then used to construct the grammar. An alternative representation, characteristic k-grams, is also introduced. Additionally, the paper describes some interactive operations necessary for generating a grammar for a large and complicated document. fl This work was partially supported by the Academy of Finland. Authors' addresses: Helena Ahonen, Heikki Mannila, Department of Computer Science, University of Helsinki, P.O. Box 26 (Teollisuuskatu 23), FIN-00014 University of Helsinki, Finland. Erja Nikunen, Research Centre for Domestic Languages, Sornaisten rantatie 25, FIN-00500 Helsinki, Finland. e-mail: fhahonen,mannilag@cs.Helsinki.FI, enikunen@domlang.FI 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Information Processing Text and Office Systems Standard Generalized Markup Language (SGML). </institution> <type> Technical Report ISO/IEC 8879, </type> <institution> International Organization for Standardization ISO/IEC, </institution> <address> Geneva/New York, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction There are a number of documents that have Standard Generalized Markup Language (SGML) <ref> [1] </ref> tags but no Document Type Definition (DTD), i.e., a grammar. There are also documents that have typographical tagging to be used in printing.
Reference: [2] <author> Jinhua Chen. </author> <title> Grammar generation and query processing for text databases. Research proposal, </title> <institution> University of Waterloo, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: The DTD can then be used to facilitate transformations and queries that have structural conditions. Additionally, most structure editors require a document to have a DTD. The structure also provides general knowledge of the document. The problem of DTD generation has also been studied in <ref> [2] </ref> and [3] but the approaches differ from the one presented here. Example 1 This example illustrates the conversion process described above. Part of a Finnish dictionary [4] containing Word Perfect commands for e.g. understriking and highlighting (in Finnish): &lt;A&gt;<L>kaame/a<a><l><Y>15&lt;y&gt; kammottava, kamala, kauhea, karmea, hirvea, pelottava. &lt;A&gt;Kaamea onnettomuus, verityo.
Reference: [3] <author> Peter Fankhauser and Yi Xu. </author> <title> Markitup! An incremental approach to document structure recognition. Electronic Publishing Origination, </title> <booktitle> Dissemination and Design 6(4) </booktitle> <pages> 447-456, </pages> <year> 1994. </year>
Reference-contexts: The DTD can then be used to facilitate transformations and queries that have structural conditions. Additionally, most structure editors require a document to have a DTD. The structure also provides general knowledge of the document. The problem of DTD generation has also been studied in [2] and <ref> [3] </ref> but the approaches differ from the one presented here. Example 1 This example illustrates the conversion process described above. Part of a Finnish dictionary [4] containing Word Perfect commands for e.g. understriking and highlighting (in Finnish): &lt;A&gt;<L>kaame/a<a><l><Y>15&lt;y&gt; kammottava, kamala, kauhea, karmea, hirvea, pelottava. &lt;A&gt;Kaamea onnettomuus, verityo.
Reference: [4] <editor> Suomen kielen perussanakirja. Ensimmainen osa (A-K). Valtion painatuskeskus, </editor> <address> Helsinki, </address> <year> 1990. </year>
Reference-contexts: The structure also provides general knowledge of the document. The problem of DTD generation has also been studied in [2] and [3] but the approaches differ from the one presented here. Example 1 This example illustrates the conversion process described above. Part of a Finnish dictionary <ref> [4] </ref> containing Word Perfect commands for e.g. understriking and highlighting (in Finnish): &lt;A&gt;<L>kaame/a<a><l><Y>15&lt;y&gt; kammottava, kamala, kauhea, karmea, hirvea, pelottava. &lt;A&gt;Kaamea onnettomuus, verityo. Tuliaseet tekivat kaameaa jalkea. Kertoa kaameita kummitusjuttuja.&lt;a&gt; &lt;L&gt;Ark.<l> &lt;A&gt;Kaamea hattu. <p> As a representation of data we have used sets of (2; 1)-contextual 2-grams. We have experimented with several different document types, and the results are encouraging. Most challenging of the documents was the part A - K of a Finnish dictionary <ref> [4] </ref>. We converted the typographical tags of the dictionary, which consists of about 16000 entries, to structural tags, and obtained a set of 468 distinct productions. Every production also received a frequency, i.e., the number of entries that the production covers.
Reference: [5] <author> Stephen Muggleton. </author> <title> Inductive Acquisition of Expert Knowl--edge. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: This de-facto grammar, however, is often quite large and, moreover, it is overly restrictive with respect to updates. Thus, one should be able to generalize the productions in some meaningful way. For the generalization, we use techniques from machine learning <ref> [5, 6] </ref> and formulate the problem as a grammatical inference problem (see Section 3). The method we have developed proceeds as follows. 1. The example productions are transformed to a set of finite automata, one for each nonterminal. <p> This property states roughly that in the structure of the document what can follow a certain component is completely determined by the k preceding components at the same level. Steps 1 and 2 are based on the ideas of synthesis of finite automata presented in <ref> [7, 5] </ref>. Specifically (k; h)-contextuality is a modification of k-reversibility [7] and k-contextuality [5]. 3. The resulting automata are transformed to regular expressions, which form the right-hand sides of the productions for the corresponding nonterminals. The rest of this paper is organized as follows. Section 2 gives the basic definitions. <p> Steps 1 and 2 are based on the ideas of synthesis of finite automata presented in [7, 5]. Specifically (k; h)-contextuality is a modification of k-reversibility [7] and k-contextuality <ref> [5] </ref>. 3. The resulting automata are transformed to regular expressions, which form the right-hand sides of the productions for the corresponding nonterminals. The rest of this paper is organized as follows. Section 2 gives the basic definitions. <p> Some methods [14] are heuristic in the sense that their result does not belong to any particular class of languages. Some methods <ref> [7, 5] </ref>, however, are characteristic: they guarantee that the result belongs to some specified subclass of regular languages. <p> Some methods [14] are heuristic in the sense that their result does not belong to any particular class of languages. Some methods [7, 5], however, are characteristic: they guarantee that the result belongs to some specified subclass of regular languages. In the following we consider two such subclasses: k-contextual <ref> [5] </ref> and (k; h)-contextual languages. 4.1 Prefix-tree automaton The right-hand sides of productions obtained from the user's examples are represented by an automaton called a prefix-tree au tomaton. To construct a prefix-tree automaton we first take the set of sample productions that have the same left-hand side. <p> If a sufficiently long se-quence of nonterminals occurs in two places in the examples, the components that can follow this sequence are independent of the position of the sequence in the document structure. A language satisfying this condition is called k-contextual <ref> [5] </ref>. The property of k-contextuality can be described simply in terms of automata.
Reference: [6] <author> Balas K. Natarajan. </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: This de-facto grammar, however, is often quite large and, moreover, it is overly restrictive with respect to updates. Thus, one should be able to generalize the productions in some meaningful way. For the generalization, we use techniques from machine learning <ref> [5, 6] </ref> and formulate the problem as a grammatical inference problem (see Section 3). The method we have developed proceeds as follows. 1. The example productions are transformed to a set of finite automata, one for each nonterminal.
Reference: [7] <author> Dana Angluin. </author> <title> Inference of reversible languages. </title> <journal> Journal of the ACM, </journal> <volume> 29(3) </volume> <pages> 741-765, </pages> <year> 1982. </year>
Reference-contexts: This property states roughly that in the structure of the document what can follow a certain component is completely determined by the k preceding components at the same level. Steps 1 and 2 are based on the ideas of synthesis of finite automata presented in <ref> [7, 5] </ref>. Specifically (k; h)-contextuality is a modification of k-reversibility [7] and k-contextuality [5]. 3. The resulting automata are transformed to regular expressions, which form the right-hand sides of the productions for the corresponding nonterminals. The rest of this paper is organized as follows. Section 2 gives the basic definitions. <p> Steps 1 and 2 are based on the ideas of synthesis of finite automata presented in [7, 5]. Specifically (k; h)-contextuality is a modification of k-reversibility <ref> [7] </ref> and k-contextuality [5]. 3. The resulting automata are transformed to regular expressions, which form the right-hand sides of the productions for the corresponding nonterminals. The rest of this paper is organized as follows. Section 2 gives the basic definitions. <p> Some methods [14] are heuristic in the sense that their result does not belong to any particular class of languages. Some methods <ref> [7, 5] </ref>, however, are characteristic: they guarantee that the result belongs to some specified subclass of regular languages.
Reference: [8] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference-contexts: Finally, Section 7 contains some concluding remarks. 2 Basic definitions Our method uses finite automata to represent and manipulate the collection of examples. We assume that the reader is familiar with finite-state automata, context-free grammars, and regular expressions (see, e.g., <ref> [8] </ref> for details), and just give the basic definitions for reference. <p> To obtain a useful description of the structure of the document, we still have to produce a grammar from these. An automaton can be converted into a regular expression by using standard dynamic programming methods <ref> [8] </ref>. One of our goals was to obtain a readable grammar. The regular expressions produced by the standard method are not always so short as they could be, and therefore they have to be simplified. The simplified regular expressions form the right-hand sides of the productions for the corresponding nonterminals.
Reference: [9] <author> Dana Angluin and Carl H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <year> 1983. </year>
Reference-contexts: S is a special nonterminal called the start symbol. 3 Grammar generation as a grammat ical inference problem A grammatical inference problem can be specified by giving the following items <ref> [9] </ref>: 1. the class of languages, 2. the hypothesis space, i.e., a set of representations for the languages, 3. for each language, the set of positive and negative examples, and the admissible sequences of examples, 4. the class of inference methods, 5. the criteria for a successful inference.
Reference: [10] <author> E. Mark Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10(5) </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: The most important criterion of success is identification in the limit which is defined as follows <ref> [10] </ref>: Definition 2 Method M identifies language L in the limit if, after a finite number of queries, M makes a correct guess and does not alter its guess thereafter. <p> In machine learning terms, the example productions of our application are all positive examples. That is, the user gives no examples of illegal document structures. This is natural for the user, but it can make the learning problem undecidable: Theorem 3 (Gold <ref> [10] </ref>) Any class of languages containing all the finite languages and at least one infinite language cannot be identified in the limit from only positive samples. From this theorem it follows that the class of context-free lan-guages and even the class of regular languages cannot be learned from positive samples.
Reference: [11] <author> Yasubumi Sakakibara. </author> <title> Efficient learning of context-free grammars from positive structural examples. </title> <journal> Information and Computation, </journal> <volume> 97(1) </volume> <pages> 23-60, </pages> <year> 1992. </year>
Reference-contexts: In fact, this is quite natural: a consistent generalization of a set of positive examples would be an automaton accepting all strings! Hence, to learn from positive examples, one needs some restrictions on the allowed result of the generalization, or some background knowledge. Methods presented in <ref> [11, 12] </ref> use, as extra knowledge, unlabelled derivation trees of sample data. Although SGML tags form derivation trees the methods of [11, 12] are inappropriate for us, because the terminals, i.e., the text of the sample document, do not generally determine the nonterminals. <p> Methods presented in <ref> [11, 12] </ref> use, as extra knowledge, unlabelled derivation trees of sample data. Although SGML tags form derivation trees the methods of [11, 12] are inappropriate for us, because the terminals, i.e., the text of the sample document, do not generally determine the nonterminals. The text varies too much to be used in the inference process.
Reference: [12] <author> Timo Knuutila. </author> <title> Inference of k-testable tree languages. </title> <editor> In H. Bunke, editor, </editor> <booktitle> Proceedings of the International Workshop on Structural and Syntactic Pattern Recognition, </booktitle> <pages> pages 109-120, </pages> <year> 1993. </year>
Reference-contexts: In fact, this is quite natural: a consistent generalization of a set of positive examples would be an automaton accepting all strings! Hence, to learn from positive examples, one needs some restrictions on the allowed result of the generalization, or some background knowledge. Methods presented in <ref> [11, 12] </ref> use, as extra knowledge, unlabelled derivation trees of sample data. Although SGML tags form derivation trees the methods of [11, 12] are inappropriate for us, because the terminals, i.e., the text of the sample document, do not generally determine the nonterminals. <p> Methods presented in <ref> [11, 12] </ref> use, as extra knowledge, unlabelled derivation trees of sample data. Although SGML tags form derivation trees the methods of [11, 12] are inappropriate for us, because the terminals, i.e., the text of the sample document, do not generally determine the nonterminals. The text varies too much to be used in the inference process.
Reference: [13] <author> Jerome A. Feldman, James Gips, James J. Horning, and Stephen Reder. </author> <title> Grammatical complexity and inference. </title> <type> Report TR CS 125/1969, </type> <institution> Stanford University, </institution> <year> 1969. </year>
Reference-contexts: For these reasons, our method reduces the inference of context-free grammars to the inference of regular expressions that can appear on the right-hand sides of productions. 4 Generalizing right-hand sides of pro ductions A number of inference methods for regular languages use the schema introduced by <ref> [13] </ref>: Construct first a prefix-tree automaton that accepts exactly the examples, and generalize the automaton by merging states. Some methods [14] are heuristic in the sense that their result does not belong to any particular class of languages.
Reference: [14] <author> A. W. Biermann and J. A. Feldman. </author> <title> On the synthesis of finite-state machines from samples of their behavior. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 21(6) </volume> <pages> 592-597, </pages> <year> 1972. </year>
Reference-contexts: Some methods <ref> [14] </ref> are heuristic in the sense that their result does not belong to any particular class of languages. Some methods [7, 5], however, are characteristic: they guarantee that the result belongs to some specified subclass of regular languages.
Reference: [15] <author> Helena Ahonen. </author> <title> Generating grammars for structured documents using grammatical inference methods. Ph. Lic. </title> <type> thesis, </type> <institution> University of Helsinki, Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: A k-gram of the form # k a, where a 2 , is called an initial k-gram. Respectively, a k-gram of the form u#, where u 2 k is called a final k-gram. Any (k; h)-contextual automaton can be converted into a set of characteristic k-grams <ref> [15] </ref>. Hence, the set of k-grams can be used as an efficient representation alternative to automata. Generalizing a set of examples to a k-contextual language can be done simply by adding all substrings of length k+1 to the set of k-grams.
References-found: 15

