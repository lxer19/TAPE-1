URL: http://www.isi.edu/~lindell/publications/optsoft95.ps
Refering-URL: http://www.isi.edu/~lindell/publications/index.html
Root-URL: http://www.isi.edu
Title: A Recursive Approach to Multivariate Automatic Differentiation  
Author: Dan Kalman Robert Lindell P. O. 
Date: April 16, 1997  
Address: Washington, D.C. 20016  Box 92957 Los Angeles, CA 90009-2957  
Affiliation: The American University  The Aerospace Corporation  
Abstract: In one approach to automatic differentiation, the range of a function is generalized from a single real value to an aggregate representing the values of the function and one or more derivatives. The operations and functions of elementary analysis are extended to these aggregates so as to preserve the validity of the derivatives. In this paper we develop a recursive approach to defining the necessary operations in the context of functions of several variables. Formally, the definitions are essentially the same as those needed in the single variable case. The resulting system provides automatic propagation of values of all partial derivatives up to a prespecified order for a function of several variables.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. I. Arnold. </author> <title> Geometrical Methods in the Theory of Ordinary Differential Equations, 2nd Ed., </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The connections between 6 these two approaches will be further elaborated below. Derivative structures have also been studied in the context of differential equations, referred to as k-jets (see <ref> [1] </ref>) or prolongations ([13]). Those developments refer to the algebraic and topological properties of derivative structures, but are not concerned with procedures for computation with the structures. The definition of DS (n; m) is explicitly recursive with respect to the level parameter n.
Reference: [2] <author> Christian Bischof, George Corliss, and Andreas Griewank. </author> <title> Structured Second- and Higher-Order Derivatives through Univariate Taylor Series. </title> <type> Preprint MCS-P296-0392, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: In particular, identities that show that the abstract operations preserve the meaning of the components as derivatives must be satisfied. The alternative is to adopt these identities as the definitions of the operations. Returning to the example of the exponential function, we could define e f <ref> [2] </ref> to [2] . In this approach it is known a priori that the operations perform automatic differentiation correctly. <p> In particular, identities that show that the abstract operations preserve the meaning of the components as derivatives must be satisfied. The alternative is to adopt these identities as the definitions of the operations. Returning to the example of the exponential function, we could define e f <ref> [2] </ref> to [2] . In this approach it is known a priori that the operations perform automatic differentiation correctly. <p> Moreover, there are ways to reconstruct the full set of partial derivatives of a function from univariate taylor expansions for the restrictions of the function to a set of lines <ref> [2] </ref>. That scheme requires storing more data than the set of partial derivatives, but the computational complexity of propagating a series of univariate expansions provides a vast savings over the convolutions inherent in the derivative structure product defined here.
Reference: [3] <author> Harley Flanders. </author> <title> Automatic Differentiation of Composite Functions. </title> <editor> in Andreas Griewank and George F. Corliss, Eds. </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <volume> 95 - 99, </volume> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: The final point of emphasis is the simplicity and elegance of a sample implementation of these algorithms. Mention has been made of the relation of our work to that of Neidinger. Other related work includes <ref> [3] </ref> and [8]. The first of these considers the differentiation of composite functions, and the second considers the differentiation of inverse functions. We will remark on these related papers in somewhat greater detail after developing our approach below. Here is an outline of the remainder of the paper. <p> While we are not 25 aware of previous work in which recursion is applied to the number of variables, our use of recursion on order is but a minor modification of the approach presented by Neidinger [11]. In contrast, Flanders <ref> [3] </ref> and Lawson [8] use explicit representations of the polynomials P j . These approaches require elaborate notation and an involved formulation. Moreover, they appear to impose limitations.
Reference: [4] <author> Andreas Griewank and George F. Corliss, Eds. </author> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year> <month> 36 </month>
Reference-contexts: 1 Introduction Automatic Differentiation refers to a family of techniques for automatically computing derivatives as a byproduct of function evaluation. A survey of different approaches can be found in <ref> [4] </ref>. In this paper we shall restrict our attention to what is called the forward mode of automatic differentiation, and in particular, the approach described in [17]. <p> Our development provides an alternate solution to that of Flanders in the composite function problem. It would be interesting to try applying our methods in the inverse function problem considered by Lawson. Other related problems where our approach might be applicable are considered in Chapters 15 and 16 of <ref> [4] </ref>. We have not emphasized issues related to implementation and performance, and a detailed consideration of these issues is beyond the scope of this work. Nevertheless, the issue of performance has obvious importance. The very notion of propagating all the partial derivatives throughout the computation is subject to examination. <p> One obvious way to improve the performance of the system is to store Taylor coefficients rather than partial derivatives, as this eliminates the need for binomial coefficients and saves a few operations in derivative structure multiplication. This approach has been discussed by several authors, see for instance <ref> [4, chapters 2 and 14] </ref>, [10]. Examination of our recursive definitions reveals that multiplication is of central importance, and is therefore a first target for 26 optimization. Thus, the use of Taylor coefficients seems highly desirable. <p> For example, this is a common feature of functional programming [5] languages. Indeed, a specialization of this idea to automatic differentiation is one of the features of an efficient approach called reverse mode <ref> [4, chapter 27 1] </ref>. Our interest here would be to take advantage of lazy evaluation as a service provided by the programming language, without special treatment by the programmer. Perhaps this will lead to computational savings without compromising the simplicity of the recursive formulation of automatic differentiation.
Reference: [5] <author> John Hughes. </author> <title> Why Functional Programming Matters. </title> <editor> in David A. Turner, Ed. </editor> <booktitle> Research Topics in Functional Programming, </booktitle> <volume> 17 - 942, </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: It seems likely that some modification of memoization might be able to improve efficiency for those computations as well. It might also be interesting to implement automatic differentiation in an environment that provides so called lazy evaluation. For example, this is a common feature of functional programming <ref> [5] </ref> languages. Indeed, a specialization of this idea to automatic differentiation is one of the features of an efficient approach called reverse mode [4, chapter 27 1].
Reference: [6] <author> Ronald E. Huss. </author> <title> An Ada library for automatic evaluation of derivatives. </title> <journal> Applied Mathematics and Computation, </journal> <volume> 35:103 - 123, </volume> <year> 1990. </year>
Reference: [7] <author> Dan Kalman and Robert Lindell. </author> <title> Automatic Differentiation in Astrodynamical Modeling. </title> <editor> in Andreas Griewank and George F. Corliss, Eds. </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <volume> 228 - 241, </volume> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: Vectors of this type are quite common in discussions of forward mode automatic differentiation, (see 8 for example <ref> [7, 9, 17] </ref>). It is clear that DS (n; m) is a generalization of this type of structure. As we shall see below, the operations we define on DS (n; m) generalize those defined on one variable derivative vectors in just the same way. <p> Evidently inspired by Leibniz' rule, Eq. (8) is formally identical to the definition of multiplication for derivative vectors used in the single variable case, 12 as in <ref> [7, 11] </ref>. There, the components of a and b are real numbers, and it is a simple matter to verify that f [1;m] fi g [1;m] = (f g) [1;m] : In the general case with n &gt; 1; the symbols have a slightly different meaning.
Reference: [8] <author> Charles L. Lawson. </author> <title> Automatic Differentiation of Inverse Functions. </title> <editor> in Andreas Griewank and George F. Corliss, Eds. </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <volume> 87 - 94, </volume> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: The final point of emphasis is the simplicity and elegance of a sample implementation of these algorithms. Mention has been made of the relation of our work to that of Neidinger. Other related work includes [3] and <ref> [8] </ref>. The first of these considers the differentiation of composite functions, and the second considers the differentiation of inverse functions. We will remark on these related papers in somewhat greater detail after developing our approach below. Here is an outline of the remainder of the paper. <p> While we are not 25 aware of previous work in which recursion is applied to the number of variables, our use of recursion on order is but a minor modification of the approach presented by Neidinger [11]. In contrast, Flanders [3] and Lawson <ref> [8] </ref> use explicit representations of the polynomials P j . These approaches require elaborate notation and an involved formulation. Moreover, they appear to impose limitations.
Reference: [9] <author> Richard D. Neidinger. </author> <title> Automatic differentiation and apl. </title> <journal> College Mathematics Journal, </journal> <volume> 20(3):238 - 251, </volume> <month> May </month> <year> 1989. </year>
Reference-contexts: Vectors of this type are quite common in discussions of forward mode automatic differentiation, (see 8 for example <ref> [7, 9, 17] </ref>). It is clear that DS (n; m) is a generalization of this type of structure. As we shall see below, the operations we define on DS (n; m) generalize those defined on one variable derivative vectors in just the same way.
Reference: [10] <author> Richard D. Neidinger. </author> <title> Computing Multivariate Taylor Series Coefficients to Arbitrary Order. Presentation: </title> <booktitle> International Congress for Industrial and Applied Mathematics, </booktitle> <address> Wash-ington, D.C., </address> <month> July 8, </month> <year> 1991. </year>
Reference-contexts: This approach has been discussed by several authors, see for instance [4, chapters 2 and 14], <ref> [10] </ref>. Examination of our recursive definitions reveals that multiplication is of central importance, and is therefore a first target for 26 optimization. Thus, the use of Taylor coefficients seems highly desirable.
Reference: [11] <author> Richard D. Neidinger. </author> <title> An efficient method for the numerical evaluation of partial derivatives of arbitrary order. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 18(2):159 - 173, </volume> <month> June </month> <year> 1992. </year>
Reference-contexts: By way of analogy, the construction sketched above for automatic differentation relative to a single variable is well known. We will show that essentially this same construction can be iterated to provide automatic generation of partial derivatives relative to several variables. Neidinger <ref> [11] </ref> develops an alternate approach to multivariate automatic differentiation. His structures are defined using multidimensional arrays and explicit subscript manipulation. One very attractive feature of this development is a recursion based on the differential order. <p> Viewed at the lowest level of detail, it is clear that our derivative structures are essentially the same as those described in <ref> [11] </ref>. There, operations are formulated in terms of the individual elements which are identified by subscript vectors. In our approach, operations at one level are defined in terms of the components in the preceding level, allowing us to conceptually manipulate objects of one dimension, rather than n. <p> Evidently inspired by Leibniz' rule, Eq. (8) is formally identical to the definition of multiplication for derivative vectors used in the single variable case, 12 as in <ref> [7, 11] </ref>. There, the components of a and b are real numbers, and it is a simple matter to verify that f [1;m] fi g [1;m] = (f g) [1;m] : In the general case with n &gt; 1; the symbols have a slightly different meaning. <p> A more flexible system would permit whatever order a computation specified, providing essentially arbitrary order. In <ref> [11] </ref> this is achieved by recursively defining higher order derivatives for the elementary functions. In this section we apply the approach of [11] to derivative structures. More specifically, we show how to compute the value of a function on a derivative structure using recursion on order. <p> A more flexible system would permit whatever order a computation specified, providing essentially arbitrary order. In <ref> [11] </ref> this is achieved by recursively defining higher order derivatives for the elementary functions. In this section we apply the approach of [11] to derivative structures. More specifically, we show how to compute the value of a function on a derivative structure using recursion on order. Theorem 4 provides a useful theoretical tool to verify that the algorithm is correct. <p> While we are not 25 aware of previous work in which recursion is applied to the number of variables, our use of recursion on order is but a minor modification of the approach presented by Neidinger <ref> [11] </ref>. In contrast, Flanders [3] and Lawson [8] use explicit representations of the polynomials P j . These approaches require elaborate notation and an involved formulation. Moreover, they appear to impose limitations.
Reference: [12] <author> Peter Norvig. </author> <title> Paradigms of Artificial Inteligence Programming: Case Studies in Common Lisp, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1992. </year>
Reference-contexts: In general, the projection 0 commutes with all the elementary functions: ( 0 a) = 0 (a): Therefore, a similar computational redundancy must occur in the level recursion for all the elementary functions. Some of the effects of this inefficiency might be offset using a memoization scheme <ref> [12] </ref>. Without modification, this technique could be used to eliminate redundant calls to a function that occur in the course of the recursion. However, the technique would not combine computations that are common to substructures in several different parts of the recursion.
Reference: [13] <author> Peter J. Olver. </author> <title> Applications of Lie Groups to Differential Equations, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference: [14] <author> Louis B. Rall. </author> <title> Applications of software for automatic differentiation in numerical computation. </title> <type> Technical Report 1976, </type> <institution> Mathematics Research Center, Madison, Wisconsin, </institution> <month> July </month> <year> 1979. </year>
Reference: [15] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications. </title> <booktitle> Lecture Notes in Computer Science No. </booktitle> <volume> 120. </volume> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference: [16] <author> Louis B. Rall. </author> <title> Differentiation in Pascal-SC: Type GRADIENT. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 10(2):161 - 184, </volume> <month> June </month> <year> 1984. </year>
Reference: [17] <author> Louis B. Rall. </author> <title> The arithmetic of differentiation. </title> <journal> Mathematics Magazine, </journal> <volume> 59(5):275 - 282, </volume> <month> December </month> <year> 1986. </year> <month> 37 </month>
Reference-contexts: A survey of different approaches can be found in [4]. In this paper we shall restrict our attention to what is called the forward mode of automatic differentiation, and in particular, the approach described in <ref> [17] </ref>. <p> Vectors of this type are quite common in discussions of forward mode automatic differentiation, (see 8 for example <ref> [7, 9, 17] </ref>). It is clear that DS (n; m) is a generalization of this type of structure. As we shall see below, the operations we define on DS (n; m) generalize those defined on one variable derivative vectors in just the same way.
References-found: 17

