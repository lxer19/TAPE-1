URL: http://www.iro.umontreal.ca/~bengioy/ift6266/svm/BurgesTutorial.ps
Refering-URL: http://www.iro.umontreal.ca/~bengioy/ift6266/svm/svm.html
Root-URL: http://www.iro.umontreal.ca
Email: burges@lucent.com  
Title: A Tutorial on Support Vector Machines for Pattern Recognition  
Author: Christopher J.C. Burges 
Note: Submitted to Data Mining and Knowledge Discovery  
Date: November 19, 1997  
Address: Room 3G-429, 101 Crawford's Corner Road Holmdel, NJ 07733-3030  
Affiliation: Bell Laboratories, Lucent Technologies  
Abstract: The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels; we then show how SVMs nevertheless provide a natural mechanism for implementing structural risk minimization, often resulting in good generalization performance. Finally, we discuss the various bounds on the generalization performance of SVMs. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> V. Vapnik. </author> <title> Statistical Learning Theory. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: 1 Introduction The purpose of this paper is to provide an introductory yet extensive tutorial on the basic ideas behind Support Vector Machines (SVMs). Many papers on SVMs offer some kind of overview, but are very limited by space constraints. The books <ref> [1, 2] </ref> contain excellent descriptions of SVMs, but they leave room for an account whose purpose from the start is to teach. <p> Neither can generalize well. The exploration and formalization of these concepts has resulted in one of the shining peaks of the theory of machine learning <ref> [3, 1, 2] </ref>. In the following, bold typeface will indicate vector or matrix quantities; normal typeface will be used for vector and matrix components and for scalars. We will indicate components of vectors and matrices with Greek indices, and label vectors and matrices themselves with Roman indices.
Reference: [3] <author> V. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data [in Russian]. </title> <publisher> Nauka, </publisher> <address> Moscow, </address> <year> 1979. </year> <title> (English translation: </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1982). </year>
Reference-contexts: The books [1, 2] contain excellent descriptions of SVMs, but they leave room for an account whose purpose from the start is to teach. Although the subject can be said to have started in the late seventies <ref> [3] </ref>, it is only now receiving increasing attention, and so the time appears suitable for an introductory review. This work grew out of several introductory talks on SVMs that I have given; from those talks it became clear that a more extensive account, with lots of examples, would be beneficial. <p> Neither can generalize well. The exploration and formalization of these concepts has resulted in one of the shining peaks of the theory of machine learning <ref> [3, 1, 2] </ref>. In the following, bold typeface will indicate vector or matrix quantities; normal typeface will be used for vector and matrix components and for scalars. We will indicate components of vectors and matrices with Greek indices, and label vectors and matrices themselves with Roman indices. <p> However, even though the bound is not valid, nearest neighbour 12 The derivation of the bound assumes that the empirical risk converges uniformly to the actual risk as the number of training observations increases <ref> [3] </ref>. A necessary and sufficient condition for this is that lim l!1 H (l)=l = 0, where l is the number of training samples and H (l) is the VC entropy of the set of decision functions [3, 1]. <p> A necessary and sufficient condition for this is that lim l!1 H (l)=l = 0, where l is the number of training samples and H (l) is the VC entropy of the set of decision functions <ref> [3, 1] </ref>. For any set of functions with infinite VC dimension, the VC entropy is l log 2: hence for these classifiers, the required uniform convergence does not hold, and so neither does the bound. DRAFT 8 classifiers can still perform well. <p> (2l=m) + 1 (1=m) ln (j=4) (8) which is certainly met for all j if f (z) = z which is true, since f (z) is monotonic increasing, and f (z = 1) = 0:236. 5 Structural Risk Minimization We can now summarize the principle of structural risk minimization (SRM) <ref> [3] </ref>. Note that the VC confidence term in Eq. (3) depends on the chosen class of functions, whereas the empirical risk and actual risk depend on the one particular function chosen by the training procedure.
Reference: [4] <author> V. Vapnik, S. Golowich, and A. Smola. </author> <title> Support vector method for function approximation, regression estimation, </title> <booktitle> and signal processing. Advances in Neural Information Processing Systems, </booktitle> <volume> 9 </volume> <pages> 281-287, </pages> <year> 1996. </year>
Reference-contexts: The tutorial 1 DRAFT 2 dwells entirely on the pattern recognition problem. Many of the ideas there carry directly over to the cases of regression estimation and linear operator inversion <ref> [1, 4] </ref>, but space constraints precluded the exploration of these topics here. The tutorial contains some new material. All of the proofs are my own versions, where I have placed a strong emphasis on their being both clear and self-contained, to make the material as accessible as possible. <p> DRAFT 25 will convince you that the corresponding can map two vectors that are linearly dependent in L onto two vectors that are linearly independent in H. So far we have considered cases where is done implicitly. One can equally well turn things around and start with <ref> [4] </ref>, and then construct the corresponding kernel. For example [4], if L = R 1 , then a Fourier expansion in the data x, cut off after N terms, has the form f (x) = a 0 + r=1 and this can be viewed as a dot product between a vector <p> So far we have considered cases where is done implicitly. One can equally well turn things around and start with <ref> [4] </ref>, and then construct the corresponding kernel. For example [4], if L = R 1 , then a Fourier expansion in the data x, cut off after N terms, has the form f (x) = a 0 + r=1 and this can be viewed as a dot product between a vector a = (a 0 ; a 11 ; :
Reference: [5] <author> Stuart Geman and Elie Bienenstock. </author> <title> Neural networks and the bias / variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: This was done at the expense of some elegance and generality: however generality is usually easily added once the basic ideas are clear. The longer proofs are collected in the Appendix. The problem which drove the initial development of SVMs occurs in several guises - the bias variance tradeoff <ref> [5, 6] </ref>, capacity control [7], overfitting [8, 9] but the basic idea is the same.
Reference: [6] <author> Thomas H. Wonnacott and Ronald J. Wonnacott. </author> <title> Introductory Statistics for Business and Economics. </title> <publisher> Wiley, </publisher> <address> 4th edition, </address> <year> 1990. </year>
Reference-contexts: This was done at the expense of some elegance and generality: however generality is usually easily added once the basic ideas are clear. The longer proofs are collected in the Appendix. The problem which drove the initial development of SVMs occurs in several guises - the bias variance tradeoff <ref> [5, 6] </ref>, capacity control [7], overfitting [8, 9] but the basic idea is the same.
Reference: [7] <author> I. Guyon, V. Vapnik, B. Boser, L. Bottou, and S.A. Solla. </author> <title> Structural risk minimization for character recognition. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 4 </volume> <pages> 471-479, </pages> <year> 1992. </year>
Reference-contexts: The longer proofs are collected in the Appendix. The problem which drove the initial development of SVMs occurs in several guises - the bias variance tradeoff [5, 6], capacity control <ref> [7] </ref>, overfitting [8, 9] but the basic idea is the same. <p> The orientation is shown in 7 The authors of <ref> [7] </ref> call it the "guaranteed risk", but this is something of a misnomer, since it is really a bound on a risk, not a risk, and it holds only with a certain probability, and so is not guaranteed. 8 This is the essential idea of structural risk minimization.
Reference: [8] <author> C.M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: The longer proofs are collected in the Appendix. The problem which drove the initial development of SVMs occurs in several guises - the bias variance tradeoff [5, 6], capacity control [7], overfitting <ref> [8, 9] </ref> but the basic idea is the same. <p> Suppose we are given l observations. Each observation consists of a pair: a vector x i 2 R n ; i = 1; : : : ; l and the associated 1 The reader in whom this elicits a sinking feeling is urged to study <ref> [10, 11, 8] </ref>.
Reference: [9] <author> D.C. Montgomery and E.A. Peck. </author> <title> Introduction to Linear Regression Analysis. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: The longer proofs are collected in the Appendix. The problem which drove the initial development of SVMs occurs in several guises - the bias variance tradeoff [5, 6], capacity control [7], overfitting <ref> [8, 9] </ref> but the basic idea is the same.
Reference: [10] <author> Gilbert Strang. </author> <title> Introduction to Applied Mathematics. </title> <publisher> Wellesley-Cambridge Press, </publisher> <year> 1986. </year>
Reference-contexts: Suppose we are given l observations. Each observation consists of a pair: a vector x i 2 R n ; i = 1; : : : ; l and the associated 1 The reader in whom this elicits a sinking feeling is urged to study <ref> [10, 11, 8] </ref>.
Reference: [11] <author> R. Fletcher. </author> <title> Practical Methods of Optimization. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> 2nd edition, </address> <year> 1987. </year>
Reference-contexts: Suppose we are given l observations. Each observation consists of a pair: a vector x i 2 R n ; i = 1; : : : ; l and the associated 1 The reader in whom this elicits a sinking feeling is urged to study <ref> [10, 11, 8] </ref>. <p> This particular dual formulation of the problem is called the Wolfe dual <ref> [11] </ref>. It has the property that the maximum of L P , subject to constraints C 2 , occurs at the same values of the w, b and ff, as the minimum of L P , subject to constraints C 1 . <p> Note that, while w is explicitly determined by the training procedure, the threshold b is not (although it is implicitly determined). However b is easily found, since one of the Karush Kuhn Tucker optimality conditions <ref> [11, 14] </ref>, which are certainly satisfied by the solution of any optimization problem with linear constraints 17 , is that for each Lagrange multiplier, the product of that multiplier with its corresponding constraint vanishes: ff i (y i (w x i + b) 1) = 0; i = 1; ; l: <p> solution of any constrained optimization problem (convex or not), with any kind of constraints, provided that the intersection of the set of feasible directions with the set of descent directions coincides with the intersection of the set of feasible directions for linearized constraints with the set of descent directions (see <ref> [11, 14] </ref>). <p> It turns out that every local solution is also global. This is a property of any convex programming problem <ref> [11] </ref>. Furthermore, the solution is guaranteed to be unique if 19 If possible, try using 16 or 24 bit color. <p> Note that for quadratic objective functions F , the Hessian is positive definite if and only if F is strictly convex; this is not true for non-quadratic F : there, a positive definite Hessian implies a strictly convex objective function, but not vice versa (consider F = x 4 ) <ref> [11] </ref>. 21 Choose an ff 0 which is in the null space of the Hessian H ij = y i y j x i x j , and require that ff 0 be orthogonal to the vector all of whose components are 1. <p> enough so that a Hessian of size N S by N S 24 Note we do not have to do line search since this can be computed analytically. 25 This holds for any convex programming problem which satisfies the general regularity condition mentioned before, which is always satisfied by SVMs <ref> [11] </ref>. DRAFT 21 will fit in memory.
Reference: [12] <author> A.N. Kolmogorov and S.V.Fomin. </author> <title> Introductory Real Analysis. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1970. </year>
Reference-contexts: Levin and J.S. Denker, which first appeared in [1]. Define the step 9 Such a set of m points (which span an m 1 dimensional subspace of a linear space) are said to be "in general position" <ref> [12] </ref>. The convex hull of a set of m points in general position defines an m 1 dimensional simplex, the vertices of which are the points themselves [12]. 10 A family of classifiers is said to have infinite VC dimension if it can shatter l points, no matter how large l. <p> a set of m points (which span an m 1 dimensional subspace of a linear space) are said to be "in general position" <ref> [12] </ref>. The convex hull of a set of m points in general position defines an m 1 dimensional simplex, the vertices of which are the points themselves [12]. 10 A family of classifiers is said to have infinite VC dimension if it can shatter l points, no matter how large l. DRAFT 6 function (x); x 2 R : f (x) = 1 8x &gt; 0; (x) = 1 8x 0g. <p> Specifically, it is any linear space, with an inner product defined, which is also complete with respect to the Euclidean norm (that is, any Cauchy sequence of points converges to a point in the space). Some authors (e.g. <ref> [12] </ref>) also require that it be separable (that is, it must have a countable subset whose closure is the space itself), and some (e.g. [26]) don't. It's interesting that the older mathematical literature (e.g. [12]) also required that Hilbert spaces be infinite dimensional, and that mathematicians (e.g. [27]) are quite happy <p> Some authors (e.g. <ref> [12] </ref>) also require that it be separable (that is, it must have a countable subset whose closure is the space itself), and some (e.g. [26]) don't. It's interesting that the older mathematical literature (e.g. [12]) also required that Hilbert spaces be infinite dimensional, and that mathematicians (e.g. [27]) are quite happy defining infinite dimensional Euclidean spaces. Research on Hilbert spaces centers on operators in those spaces, since the basic properties have long since been worked out.
Reference: [13] <author> M. Anthony and N. Biggs. </author> <title> Pac learning and neural networks. </title> <booktitle> In The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 694-697, </pages> <year> 1995. </year>
Reference-contexts: the points as origin, such that the position vectors of the remaining n points are linearly independent, but can never choose n + 2 such points (since no n + 1 vectors in R n can be linearly independent). x An alternative proof of the corollary can be found in <ref> [13] </ref>, and references therein. 3.2 The VC Dimension and the Number of Parameters The VC dimension thus gives concreteness to the notion of the capacity of a given set of functions.
Reference: [14] <author> Garth P. McCormick. </author> <title> Non Linear Programming: Theory, Algorithms and Applications. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1983. </year>
Reference-contexts: Note that, while w is explicitly determined by the training procedure, the threshold b is not (although it is implicitly determined). However b is easily found, since one of the Karush Kuhn Tucker optimality conditions <ref> [11, 14] </ref>, which are certainly satisfied by the solution of any optimization problem with linear constraints 17 , is that for each Lagrange multiplier, the product of that multiplier with its corresponding constraint vanishes: ff i (y i (w x i + b) 1) = 0; i = 1; ; l: <p> solution of any constrained optimization problem (convex or not), with any kind of constraints, provided that the intersection of the set of feasible directions with the set of descent directions coincides with the intersection of the set of feasible directions for linearized constraints with the set of descent directions (see <ref> [11, 14] </ref>).
Reference: [15] <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297, </pages> <year> 1995. </year>
Reference-contexts: This can be done by introducing positive slack variables i ; i = 1; ; l in the constraints <ref> [15] </ref>, which then become: x i w + b +1 i for y i = +1 (36) 17 In fact the KKT conditions are satisfied at the solution of any constrained optimization problem (convex or not), with any kind of constraints, provided that the intersection of the set of feasible directions
Reference: [16] <author> C.J.C. Burges, P.Knirsch, and R. Haratsch. </author> <title> Support vector web page: </title> <type> http://svm.research.bell-labs.com. Technical report, </type> <institution> Lucent Technologies, </institution> <year> 1996. </year>
Reference-contexts: The error in the non-separable case is identified with a cross. The reader is invited to use Lucent's SVM Applet <ref> [16] </ref> to experiment and create pictures like these 19 . 6.7 Global Solutions and Uniqueness When is a solution produced by support vector training global, and when is it unique? By "global", we mean that there exists no other point in the feasible region at which the objective function takes a
Reference: [17] <author> William H. Press, Brain P. Flannery, Saul A. Teukolsky, and William T. Vet-tering. </author> <title> Numerical recipes in C: </title> <booktitle> the art of scientific computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year> <note> DRAFT 39 </note>
Reference-contexts: Here we give summary descriptions, and a brief explanation of the technique we currently use. Below, a "face" means a set of points lying on the boundary of the feasible region. * Constrained conjugate gradient ascent: here standard conjugate gradient ascent <ref> [17] </ref> is used, but directions are projected into the subspace in which the equality constraint (15) holds.
Reference: [18] <author> Jorge J. More and Gerardo Toraldo. </author> <title> On the solution of large quadratic programming problems with bound constraints. </title> <journal> SIAM J. Optimization, </journal> <volume> 1(1) </volume> <pages> 93-113, </pages> <year> 1991. </year>
Reference-contexts: When a face is encountered (or left), the corresponding inequality constraint is made active (or inactive), and the process restarted. * Projection methods: The above method has the disadvantage of only allowing one constraint to become active or inactive at a time. Projection methods <ref> [18] </ref> attempt to solve this by using conjugate gradient to move around within a face, but projections to move between faces (for example, by finding a point which increases the objective function and which lies outside the feasible region, and then using line search and projection into the feasible region to
Reference: [19] <author> James R. Bunch and Linda Kaufman. </author> <title> Some stable methods for calculating inertia and solving symmetric linear systems. </title> <journal> Mathematics of computation, </journal> <volume> 31(137) </volume> <pages> 163-179, </pages> <year> 1977. </year>
Reference-contexts: Only that part of the Hessian that is needed is computed. A Bunch-Kaufman decomposition <ref> [19, 20] </ref> is used to solve a series of equality constrained problems. * Interior Point Methods: Interior point methods essentially rescale the problem so as to always remain inside the feasible region. An example is the "LOQO" algorithm of Vanderbei [21, 22], which is a primal-dual path following algorithm.
Reference: [20] <author> James R. Bunch and Linda Kaufman. </author> <title> A computational method for the indefinite quadratic programming problem. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 34 </volume> <pages> 341-370, </pages> <year> 1980. </year>
Reference-contexts: Only that part of the Hessian that is needed is computed. A Bunch-Kaufman decomposition <ref> [19, 20] </ref> is used to solve a series of equality constrained problems. * Interior Point Methods: Interior point methods essentially rescale the problem so as to always remain inside the feasible region. An example is the "LOQO" algorithm of Vanderbei [21, 22], which is a primal-dual path following algorithm.
Reference: [21] <author> R.J. Vanderbei. </author> <title> LOQO: An interior point code for quadratic programming. </title> <type> Technical report, </type> <institution> Program in Statistics & Operations Research, Princeton University, </institution> <year> 1994. </year>
Reference-contexts: A Bunch-Kaufman decomposition [19, 20] is used to solve a series of equality constrained problems. * Interior Point Methods: Interior point methods essentially rescale the problem so as to always remain inside the feasible region. An example is the "LOQO" algorithm of Vanderbei <ref> [21, 22] </ref>, which is a primal-dual path following algorithm. This method is likely to be useful for problems where the number of support vectors as a fraction of training sample size is expected to be large.
Reference: [22] <author> R. J. Vanderbei. </author> <title> Interior point methods : Algorithms and formulations. </title> <journal> ORSA J. Computing, </journal> <volume> 6(1) </volume> <pages> 32-34, </pages> <year> 1994. </year>
Reference-contexts: A Bunch-Kaufman decomposition [19, 20] is used to solve a series of equality constrained problems. * Interior Point Methods: Interior point methods essentially rescale the problem so as to always remain inside the feasible region. An example is the "LOQO" algorithm of Vanderbei <ref> [21, 22] </ref>, which is a primal-dual path following algorithm. This method is likely to be useful for problems where the number of support vectors as a fraction of training sample size is expected to be large.
Reference: [23] <author> B. E. Boser, I. M. Guyon, and V .Vapnik. </author> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> In Fifth Annual Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, 1992. </address> <publisher> ACM. </publisher>
Reference-contexts: What do you do when the number of training points is such that the resulting Hessian would not fit in memory? In one method the data is split into chunks <ref> [23] </ref>. Once a given chunk has been trained, the rest of the training data is tested on the result, and the results ordered so that the most egregious errors can be added to the next chunk. <p> An alternative algorithm has been proposed which claims to overcome this limitation [24]. 7 Non-Linear Support Vector Machines How can the above methods be generalized to the case where the decision function is not a linear function of the data? Boser, Guyon and Vapnik, in 1992 <ref> [23] </ref>, showed that a rather old trick [25] can be used to accomplish this in an astonishingly straightforward way. First notice that the only way in which the data appears in the training problem, Eqs. (39) - (41), is in the form of dot products, x i x j .
Reference: [24] <author> Edgar Osuna, Robert Freund, and Federico Girosi. </author> <title> An improved training algorithm for support vector machines. </title> <booktitle> In IEEE Neural Networks for Signal Processing, Proceedings, </booktitle> <pages> pages 24 - 26, </pages> <address> Amelia Island, </address> <year> 1997. </year>
Reference-contexts: DRAFT 21 will fit in memory. An alternative algorithm has been proposed which claims to overcome this limitation <ref> [24] </ref>. 7 Non-Linear Support Vector Machines How can the above methods be generalized to the case where the decision function is not a linear function of the data? Boser, Guyon and Vapnik, in 1992 [23], showed that a rather old trick [25] can be used to accomplish this in an astonishingly
Reference: [25] <author> M.A. Aizerman, </author> <title> E.M. Braverman, and L.I.Rozonoer. Theoretical foundations of the potential function method in pattern recognition learning. </title> <journal> Automation and Remote Control, </journal> <volume> 25 </volume> <pages> 821-837, </pages> <year> 1964. </year>
Reference-contexts: has been proposed which claims to overcome this limitation [24]. 7 Non-Linear Support Vector Machines How can the above methods be generalized to the case where the decision function is not a linear function of the data? Boser, Guyon and Vapnik, in 1992 [23], showed that a rather old trick <ref> [25] </ref> can be used to accomplish this in an astonishingly straightforward way. First notice that the only way in which the data appears in the training problem, Eqs. (39) - (41), is in the form of dot products, x i x j .
Reference: [26] <author> P.R. Halmos. </author> <title> A Hilbert Space Problem Book. </title> <address> D. </address> <publisher> Van Nostrand Company, Inc., </publisher> <year> 1967. </year>
Reference-contexts: Some authors (e.g. [12]) also require that it be separable (that is, it must have a countable subset whose closure is the space itself), and some (e.g. <ref> [26] </ref>) don't. It's interesting that the older mathematical literature (e.g. [12]) also required that Hilbert spaces be infinite dimensional, and that mathematicians (e.g. [27]) are quite happy defining infinite dimensional Euclidean spaces.
Reference: [27] <author> R.E. Greene. </author> <title> Isometric Embeddings of Riemannian and Pseudo-Riemannian Manifolds. </title> <publisher> American Mathematical Society, </publisher> <year> 1970. </year>
Reference-contexts: It's interesting that the older mathematical literature (e.g. [12]) also required that Hilbert spaces be infinite dimensional, and that mathematicians (e.g. <ref> [27] </ref>) are quite happy defining infinite dimensional Euclidean spaces. Research on Hilbert spaces centers on operators in those spaces, since the basic properties have long since been worked out. Since some people understandably blanch at the mention of Hilbert spaces, I decided to use the term Euclidean throughout this tutorial.
Reference: [28] <author> C.J.C. Burges. </author> <title> Simplified support vector decision rules. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 71-77, </pages> <address> Bari, Italy, 1996. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Despite this, ideas along these lines can be used to significantly speed up the test phase of SVMs <ref> [28] </ref>. Note also that it is easy to find kernels (for example, kernels which are functions of the dot products of the x i in L) such that the training algorithm and solution found are independent of the dimension of both L and H. <p> The idea is easy to implement and seems to work better than other methods for incorporating invariances proposed so far. The reduced set method <ref> [28, 36] </ref> was introduced to address the speed of support vector machines in test phase, and also starts with a trained SVM.
Reference: [29] <author> R. Courant and D. </author> <title> Hilbert. Methods of Mathematical Physics. </title> <publisher> Interscience, </publisher> <year> 1953. </year>
Reference-contexts: DRAFT 23 7.1 Mercer's Condition For which kernels does there exist a pair fH; g, with the properties described above, and for which does there not? The answer is given by Mercer's condition <ref> [1, 29] </ref>: There exists a mapping and an expansion K (x; y) = i if and only if, for any g (x) such that Z g (x) 2 dx is finite (58) then Z K (x; y)g (x)g (y)dxdy 0: (59) Note that for specific cases, it may not be easy
Reference: [30] <author> Alex J. Smola, Bernhard Scholkopf, and Klaus-Robert Muller. </author> <title> The connection between regularization operators and support vector kernels. to appear in Neural Networks, </title> <year> 1997. </year>
Reference-contexts: One trivial consequence is that any kernel which can be expressed as K (x; y) = P 1 p=0 c p (xy) p , where the c p are positive real coefficients and the series is convergent (and hence is absolutely convergent), satisfies Mercer's condition, a fact also noted in <ref> [30] </ref>. 7.2 Some Notes on and H Mercer's condition tells us whether or not a prospective kernel is actually a dot product in some space, but it does not tell us how to construct or even what H is.
Reference: [31] <author> B. Scholkopf, A. Smola, and K. Muller. </author> <title> Nonlinear (principal) component analysis ICANN, </title> <note> to appear, 1997. DRAFT 40 </note>
Reference-contexts: i ) (x j ) = K (x i ; x j ) = sin ((x i x j )=2) Finally, it is clear that the above mapping trick will work for any algorithm in which the data only appear as dot products, a fact exploited by the authors of <ref> [31] </ref>. 7.3 Some Examples of Non-Linear SVMs The first kernels investigated for the pattern recognition problem were polynomial classifiers, K (x; y) = (x y + 1) p , SVM radial basis function classifiers, K (x; y) = exp kxyk 2 =2oe 2 , and SVM two layer neural nets, K
Reference: [32] <author> B. Scholkopf, K. Sung, C. Burges, F. Girosi, P. Niyogi, T. Poggio, and V. Vap-nik. </author> <title> Comparing support vector machines with gaussian kernels to radial basis function classifiers. </title> <journal> IEEE Trans. Sign. Processing, </journal> <volume> 45:2758 - 2765, </volume> <year> 1997. </year>
Reference-contexts: For SVM RBFs, the number of centers (N S in Eq. (53)), the centers themselves (the s i ), the weights (ff i ), and the threshold (b) are all produced automatically by the SVM training and give excellent results compared to classical RBFs, for the case of Gaussian RBFs <ref> [32] </ref>. However, it turns out that the hyperbolic tangent kernel only satisfies Mercer's condition for certain values of the parameters and ffi 29 . <p> Even though their VC dimension is infinite, SVM RBFs can have excellent performance <ref> [32] </ref>.
Reference: [33] <author> John Shawe-Taylor, Peter L. Batrlett, Robert C. Williamson, and Martin An-thony. </author> <title> A framework for structural risk minimzation. </title> <booktitle> In Proceedings, 9th Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 68-76, </pages> <year> 1996. </year>
Reference-contexts: There we would simply have to include the empirical risk in the right hand side of Eq. (3). Also, there exist generalizations of the kind of SRM described here, which allow the hierarchies of functions themselves to be data-dependent <ref> [33, 34] </ref>. However, the above argument circumvents the need for such generalizations in this case.
Reference: [34] <author> John Shawe-Taylor, Peter L. Batrlett, Robert C. Williamson, and Martin An-thony. </author> <title> Structural risk minimization over data-dependent hierarchies. </title> <type> Technical report, NeuroCOLT Technical Report NC-TR-96-053, </type> <year> 1996. </year>
Reference-contexts: There we would simply have to include the empirical risk in the right hand side of Eq. (3). Also, there exist generalizations of the kind of SRM described here, which allow the hierarchies of functions themselves to be data-dependent <ref> [33, 34] </ref>. However, the above argument circumvents the need for such generalizations in this case.
Reference: [35] <author> B. Scholkopf, C. Burges, and V. Vapnik. </author> <title> Incorporating invariances in support vector learning machines. </title> <editor> In C. von der Malsburg, W. von Seelen, J. C. Vorbruggen, and B. Sendhoff, editors, </editor> <booktitle> Artificial Neural Networks | ICANN'96, </booktitle> <pages> pages 47 - 52, </pages> <address> Berlin, </address> <year> 1996. </year> <booktitle> Springer Lecture Notes in Computer Science, </booktitle> <volume> Vol. </volume> <pages> 1112. </pages>
Reference-contexts: DRAFT 34 9 Two Extensions We very briefly describe two of the simplest, and most effective, methods for improving the performance of SVMs. The virtual support vector method <ref> [35, 36] </ref>, attempts to incorporate known invari-ances of the problem (for example, translation invariance for the image recognition problem) by first training a system, and then creating new data by distorting the resulting support vectors (translating them, in the case mentioned), and finally training a new system on the distorted (and
Reference: [36] <author> C. J. C. Burges and B. Scholkopf. </author> <title> Improving the accuracy and speed of support vector learning machines. </title> <editor> In M. Mozer, M. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <pages> pages 375-381, </pages> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. </publisher>
Reference-contexts: DRAFT 34 9 Two Extensions We very briefly describe two of the simplest, and most effective, methods for improving the performance of SVMs. The virtual support vector method <ref> [35, 36] </ref>, attempts to incorporate known invari-ances of the problem (for example, translation invariance for the image recognition problem) by first training a system, and then creating new data by distorting the resulting support vectors (translating them, in the case mentioned), and finally training a new system on the distorted (and <p> The idea is easy to implement and seems to work better than other methods for incorporating invariances proposed so far. The reduced set method <ref> [28, 36] </ref> was introduced to address the speed of support vector machines in test phase, and also starts with a trained SVM. <p> The same technique could be used for SVM regression to find much more efficient function representations (which could be used, for example, in data compression). Combining these two methods gave a factor of 50 speedup with only a 10% loss in accuracy on the NIST digits <ref> [36] </ref>. 10 Conclusions SVMs provide a principled approach connecting the problem of pattern recognition 35 to fundamental underlying bounds which govern their generalization performance.
References-found: 35

