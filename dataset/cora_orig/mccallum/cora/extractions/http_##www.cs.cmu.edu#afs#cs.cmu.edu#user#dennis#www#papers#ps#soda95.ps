URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/dennis/www/papers/ps/soda95.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/dennis/www/resume.html
Root-URL: 
Title: Splay Trees for Data Compression  
Author: Dennis Grinberg (CMU) Sivaramakrishnan Rajagopalan (Boston U.) Ramarathnam Venkatesan (Bellcore) Victor K. Wei (Chinese U. Hong Kong) 
Abstract: We present applications of splay trees to two topics in data compression. First is a variant of the move-to-front (mtf) data compression (of Bentley,Sleator Tarjan and Wei) algorithm, where we introduce secondary list(s). This seems to capture higher-order correlations. An implementation of this algorithm with Sleator-Tarjan splay trees runs in time (provably) proportional to the entropy of the input sequence. When tested on some telephony data, compression ratio and run time showed significant improvements over original mtf-algorithm, making it competitive or better than popular programs. For stationary ergodic sources, we analyse the compression and output distribution of the original mtf-algorithm, which suggests why the secondary list is appropriate to introduce. We also derive analytical upper bounds on the average codeword length in terms of stochastic parameters of the source. Secondly, we consider the compression (or coding) of source sequences where the codewords are required to preserve the alphabetic order of the source symbols. We describe the use of the semisplay tree, the regular splay tree, and splay trees with depth three and four for this application, and derive upper bounds on their compression efficiency. For example, the average codeword length of the semisplay tree is is no more than twice the source entropy plus some minor terms. 
Abstract-found: 1
Intro-found: 1
Reference: [BAF] <institution> Bellcore Automatic Message Accounting Format (BAF) Requirements. </institution> <type> Technical Report TR-NWT-001100, </type> <institution> Bell Communications Research, </institution> <note> Issue 1, </note> <month> November </month> <year> 1990, </year> <title> Revision 7, </title> <month> March </month> <year> 1992. </year>
Reference-contexts: Therefore, this method does not yield improved bounds. However, this result does not preclude the possibility that better proof techniques can improve the bound. 3.5 Simulation Results. The preliminary results reported below are from tests on formatted telephony data (see <ref> [BAF, FCIF] </ref>) and some C-programs. The tested file sizes varied from a few kilobytes to 70Megabytes. Usually the front-end that breaks an input stream into words is trivial and a few lines of C-code.
Reference: [1] <author> J. L. Bentley, D. D. Sleator, R. E. Tarjan, and V. K. Wei, </author> <title> A locally adaptive data compression scheme, </title> <journal> Comm. of the ACM, </journal> <month> 29 </month> <year> (1986) </year> <month> 320-330. </month>
Reference-contexts: If the source distribution is known, Huffman's algorithm [10] generates an optimal set of codewords for compressing this source. First, we present a variant of the move-to-front algorithm of Bentley, et al. <ref> [1] </ref>. It maintains an additional list and has improved compression efficiency for certain sources with memory. This two-list variant of the move-to-front algorithm is implemented using splay trees for the lists and tested on a collection of telephony data. <p> Let x 1 ; x 2 ; :::; x N be the source sequence. Let `(j) be the length of Elias encoding of the positive integer j. Let N (j) denote the number of appearances of the integer j as a token. Using analysis similar to that in <ref> [1] </ref>, one can show THEOREM 1. The output length of the secondary list can be upper bounded by X N (j)`(j)+ j N +2 log (1+log N (j) It is an open problem to precisely characterize the class of sources for which the secondary list helps compression. <p> Clearly the length of the compressed file is P P j , plus the overhead for first time occurrences and for making the representations self-delimiting-which can be ignored. An analysis of this for indpendent identically distributed sources, given in <ref> [1, 3] </ref> shows that the expected inter arrival time of a letter a is 1=p (a). Hence the expected length of the output per symbol is 1=L a j j = a where H is the entropy of the source and L is the length of the sequence.
Reference: [2] <author> L. Breiman, </author> <title> Probability, </title> <publisher> SIAM. </publisher>
Reference: [3] <author> P. Elias, </author> <title> Interval and recency rank source coding: two on-line adaptive variable-length Splay Trees and Data Compression 9 schemes, </title> <journal> IEEE Trans. on Info. Theory, </journal> <month> 33 </month> <year> (1987) </year> <month> 3-10. </month>
Reference-contexts: Clearly the length of the compressed file is P P j , plus the overhead for first time occurrences and for making the representations self-delimiting-which can be ignored. An analysis of this for indpendent identically distributed sources, given in <ref> [1, 3] </ref> shows that the expected inter arrival time of a letter a is 1=p (a). Hence the expected length of the output per symbol is 1=L a j j = a where H is the entropy of the source and L is the length of the sequence.
Reference: [FCIF] <institution> FCIF|Flexible Computer Interface Format, BellCore (Technical Advisory) TA-STS-000873, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: Therefore, this method does not yield improved bounds. However, this result does not preclude the possibility that better proof techniques can improve the bound. 3.5 Simulation Results. The preliminary results reported below are from tests on formatted telephony data (see <ref> [BAF, FCIF] </ref>) and some C-programs. The tested file sizes varied from a few kilobytes to 70Megabytes. Usually the front-end that breaks an input stream into words is trivial and a few lines of C-code.
Reference: [4] <author> N. Faller, </author> <title> An adaptive system for data compression, </title> <booktitle> Record of the 7th Asilomar Conf. on Circuits, Systems, and Computers (1973) 593-597. </booktitle>
Reference: [5] <author> R. G. Gallager, </author> <title> Variations on a theme by Huffman, </title> <journal> IEEE Trans. on Information Theory, </journal> <month> 24 </month> <year> (1978) </year> <month> 668-674. </month>
Reference: [6] <author> E. N. Gilbert and E. F. Moore, </author> <title> Variable-length binary encodings, </title> <journal> Bell System Tech. J., </journal> <month> 38 </month> <year> (1959) </year> <month> 933-968. </month>
Reference: [7] <author> R. N. Horspool and G. V. Cormack, </author> <title> A general purpose data compression technique with practical applications, </title> <booktitle> Proceedings of the CIPS Session 84, </booktitle> <year> (1984) </year> <month> 138-141. </month>
Reference: [8] <author> T. C. Hu, </author> <title> A new proof of the T-C algorithm, </title> <journal> SIAM J. Appl. Math., </journal> <month> 25 </month> <year> (1973) </year> <month> 83-94. </month>
Reference: [9] <author> T. C. Hu and A. C. Tucker, </author> <title> Optimal computer search trees and variable length alphabetic codes, </title> <journal> SIAM J. Appl. Math., </journal> <month> 21 </month> <year> (1971) </year> <month> 514-532. </month>
Reference: [10] <author> D. A. Huffman, </author> <title> A method for the construction of minimum-redundancy codes, </title> <booktitle> Proc. of the IRE, </booktitle> <month> 40 </month> <year> (1959) </year> <month> 933-968. </month>
Reference-contexts: Huffman tree (or Huffman code) is a classic result in compression. It gives the best code for compressing a source file whose symbols are generated by an independent but identical distribution (i.i.d). If the source distribution is known, Huffman's algorithm <ref> [10] </ref> generates an optimal set of codewords for compressing this source. First, we present a variant of the move-to-front algorithm of Bentley, et al. [1]. It maintains an additional list and has improved compression efficiency for certain sources with memory.
Reference: [11] <author> D. W. Jones, </author> <title> Application of splay trees to data compression, </title> <journal> Comm. of the ACM, </journal> <month> 31 </month> <year> (1988) </year> <month> 996-1007. </month>
Reference-contexts: An example of semisplaying a tree at node x is shown in Figure 2. D. Jones <ref> [11] </ref> suggested the use of semisplaying for the purpose of data compression with an alphabetic order. He also studies variations of splaying for compression without an alphabetic order. In the following, we analyze the efficiency of semisplaying for compression with an alphabetic order.
Reference: [12] <author> D. E. Knuth, </author> <title> Optimum binary search trees, </title> <journal> Acta Informat., </journal> <month> 1 </month> <year> (1971) </year> <month> 14-25. </month>
Reference: [13] <author> D. E. Knuth, </author> <title> Dynamic Huffman coding, </title> <editor> J. </editor> <booktitle> Algorithms, </booktitle> <month> 6 </month> <year> (1985) </year> <month> 163-180. </month>
Reference: [14] <author> N. Nakatsu, </author> <title> Bounds on the redundancy of binary alphabetical codes, </title> <journal> IEEE Trans. Information Theory, </journal> <month> 37 </month> <year> (1991) </year> <month> 1225-1229. </month>
Reference: [15] <author> B. Y. Ryabko, </author> <title> Data compression by means of a `book stack', </title> <type> Prob. </type> <institution> Inf. Transm., </institution> <month> 16 </month> <year> (1980) </year> <month> 265-269. </month>
Reference: [16] <author> D. D. Sleator and R. E. Tarjan, </author> <title> Self-adjusting binary search trees, </title> <journal> Comm. of the ACM, </journal> <month> 32 </month> <year> (1985) </year> <month> 652-686. </month>
Reference-contexts: For example, the average codeword length of the semisplay tree is no more than twice the source entropy plus some minor terms. Subsequently, it was discovered that this result can be derived from a remark in <ref> [16] </ref>. Other splay tree versions we studied include the regular splay tree and splay trees with depth three and four. <p> The symbol "B" has probability close to one. The average codeword length is nearly two, and the entropy is nearly zero. Improvements to this bound based on properties of the source probability distribution exist, e.g. [19][14]. The proof is similar to that in Sleator and Tarjan <ref> [16] </ref> for the bound on the amortized cost of (regular) splaying. However, there are several specializations for compression studies. Each leaf node has a positive weight w (i). <p> Its efficiency is analyzed below. Theorem 2: For the splay tree, the average 6 Grinberg, Rajagopalan, Venkatesan, and Wei codeword length per source symbol is upper bounded by 3H (p) + 2 + (n=T ) log (1=p min ), where p is the empirical source probability distribution. In <ref> [16] </ref>, Lemma 1, the amortized access time of the splay tree is upper bounded by 3 (r (R 0 ) r (x)) + 1. The same analysis implies that the amortized compression cost of the splay tree is upper bounded by 3 (r (R 0 ) r (x)) + 2. <p> The same analysis implies that the amortized compression cost of the splay tree is upper bounded by 3 (r (R 0 ) r (x)) + 2. The additional bit comes from the branch from node x to node X; it was not counted in <ref> [16] </ref> but needs to be counted here. Then the proof of Theorem 2 is similar to that of Theorem 1. In the above analysis, the weights of leaf nodes remain constant throughout splayings. By varying the weights over time, we can obtain another result. <p> The output integer of the first module is sent directly to the third module, unless it is less than 512 in this case it is processed by the second module. We used splay tree to implement the search tree. We chose a version called simple splaying <ref> [16] </ref> where the second rotation in the zig-zag case is omitted. Making a tradeoff between speed and compression, we used a variable-length encoding of the integers but we required our codewords to always be an integral number of nibbles (4 bits).
Reference: [17] <author> J. S. Vitter, </author> <title> Design and analysis of dynamic Huffman coding, </title> <booktitle> Proc. 26th Symp. on Foundations of Computer Science, </booktitle> <year> (1985) </year> <month> 293-302. </month>
Reference: [18] <author> A. D. Wyner and J. Ziv, </author> <title> Some asymptotic properties of the entropy of a stationary ergodic data source with applications to data compression, </title> <journal> IEEE Trans. on Information Theory, </journal> <month> 35 </month> <year> (1989) </year> <month> 1250-1258. </month>
Reference: [19] <author> R. W. Yeung, </author> <title> Alphabetic codes revisited, </title> <journal> IEEE Trans. on Information Theory, </journal> <month> 37 </month> <year> (1991) </year> <month> 564-572. </month>
References-found: 21

