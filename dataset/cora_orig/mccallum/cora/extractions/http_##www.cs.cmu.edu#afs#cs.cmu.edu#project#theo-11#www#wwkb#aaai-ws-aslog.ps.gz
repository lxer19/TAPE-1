URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/aaai-ws-aslog.ps.gz
Refering-URL: http://www.cs.cmu.edu/~WebKB/
Root-URL: 
Email: juffi@cs.cmu.edu  mitchell+@cs.cmu.edu  riloff@cs.utah.edu  
Title: A Case Study in Using Linguistic Phrases for Text Categorization on the WWW  
Author: Johannes Furnkranz Tom Mitchell Ellen Riloff 
Address: Pittsburgh, PA 15213  Pittsburgh, PA 15213  Salt Lake City, UT 84112  
Affiliation: School of Computer Science Carnegie Mellon University  School of Computer Science Carnegie Mellon University  Department of Computer Science University of Utah  
Abstract: Most learning algorithms that are applied to text categorization problems rely on a bag-of-words document representation, i.e., each word occurring in the document is considered as a separate feature. In this paper, we investigate the use of linguistic phrases as input features for text categorization problems. These features are based on information extraction patterns that are generated and used by the AutoSlog-TS system. We present experimental results on using such features as background knowledge for two machine learning algorithms on a classification task on the WWW. The results show that phrasal features can improve the precision of learned theories at the expense of coverage. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cohen, W. W., and Singer, Y. </author> <year> 1996. </year> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-96), </booktitle> <pages> 307-315. </pages>
Reference-contexts: Thus, Ripper is a member of the family of separate-and-conquer (or covering) rule learning algorithms (Furnkranz 1998). What makes Ripper particularly well-suited for text categorization problems is its ability to use set-valued features <ref> (Cohen 1996) </ref>. For conventional machine learning algorithms, a document is typically represented as a set of binary features, each encoding the presence or absence of a particular word in that document. <p> an ideal choice for evaluating the utility of phrasal features, because significant parts of WWW pages do not contain natural language text. 9 Thus, we plan to further evaluate this technique on commonly used text categorization benchmarks, such as the 20 newsgroups dataset (Lang 1995) and the REUTERS newswire collection <ref> (Cohen & Singer 1996) </ref>. On the other hand, we are also working on further improving the classification accuracy on the 4 universities data set used in this case study. For example, all approaches used in this study performed very poorly on the Project class (precision was typically below 20%).
Reference: <author> Cohen, W. W. </author> <year> 1995. </year> <title> Fast effective rule induction. </title> <editor> In Prieditis, A., and Russell, S., eds., </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML-95), </booktitle> <pages> 115-123. </pages> <address> Lake Tahoe, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: This option was used in our experiments to obtain better confidence estimates for the predictions, which we used for generating recall/precision graphs. A more detailed description of this smoothing technique and of Rainbow in general can be found in (Craven et al. 1998a). Ripper Ripper 4 <ref> (Cohen 1995) </ref> is an efficient, noise-tolerant rule learning algorithm based on the incremental reduced error pruning algorithm (Furnkranz & Widmer 1994; Furnkranz 1997).
Reference: <author> Cohen, W. W. </author> <year> 1996. </year> <title> Learning trees and rules with set-valued features. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligene (AAAI-96), </booktitle> <pages> 709-716. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Thus, Ripper is a member of the family of separate-and-conquer (or covering) rule learning algorithms (Furnkranz 1998). What makes Ripper particularly well-suited for text categorization problems is its ability to use set-valued features <ref> (Cohen 1996) </ref>. For conventional machine learning algorithms, a document is typically represented as a set of binary features, each encoding the presence or absence of a particular word in that document. <p> an ideal choice for evaluating the utility of phrasal features, because significant parts of WWW pages do not contain natural language text. 9 Thus, we plan to further evaluate this technique on commonly used text categorization benchmarks, such as the 20 newsgroups dataset (Lang 1995) and the REUTERS newswire collection <ref> (Cohen & Singer 1996) </ref>. On the other hand, we are also working on further improving the classification accuracy on the 4 universities data set used in this case study. For example, all approaches used in this study performed very poorly on the Project class (precision was typically below 20%).
Reference: <author> Craven, M.; DiPasquio, D.; Freitag, D.; McCallum, A.; Mitchell, T.; Nigam, K.; and Slattery, S. </author> <year> 1998a. </year> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <type> Technical report, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: They are quite reliable features for the problem of discriminating between student pages and other departmental pages and, for this task, should not be removed by a stop list (see also <ref> (Craven et al. 1998a) </ref>). &lt; &gt; is student, student of &lt; &gt;, and student at &lt; &gt;. 2 All four of them are highly indicative of student pages and at least the last three of them are quite unlikely to appear on other types of pages. <p> This option was used in our experiments to obtain better confidence estimates for the predictions, which we used for generating recall/precision graphs. A more detailed description of this smoothing technique and of Rainbow in general can be found in <ref> (Craven et al. 1998a) </ref>. Ripper Ripper 4 (Cohen 1995) is an efficient, noise-tolerant rule learning algorithm based on the incremental reduced error pruning algorithm (Furnkranz & Widmer 1994; Furnkranz 1997).
Reference: <author> Craven, M.; DiPasquio, D.; Freitag, D.; McCallum, A.; Mitchell, T.; Nigam, K.; and Slattery, S. </author> <year> 1998b. </year> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <booktitle> In Proceedings of the 15th National Conference on Artificial Intelligence (AAAI-98). </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: For the remainder of this paper, we will continue to think of each word (or phrase) as a separate binary feature. The WebKB Project The goal of the WebKB Project <ref> (Craven et al. 1998b) </ref> is to extract a computer-understandable knowledge base from the WWW whose contents mirrors the contents of the WWW. Many applications could be imagined for such a knowledge base. <p> The algorithms were evaluated using the same procedure as in <ref> (Craven et al. 1998b) </ref>: Each experiment consists of four runs, in which the pages of one of the four universities are left out in turn. Thus, each training set consists of the 4,120 pages from miscellaneous universities plus the pages from three of the four universities.
Reference: <author> Furnkranz, J., and Widmer, G. </author> <year> 1994. </year> <title> Incremental Reduced Error Pruning. </title> <editor> In Cohen, W., and Hirsh, H., eds., </editor> <booktitle> Proceedings of the 11th International Conference on Machine Learning (ML-94), </booktitle> <pages> 70-77. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: used in (Lang 1995). 8 Two other interesting observations to make in figure 1 are the differences in maximum recall between 7 This is one of the differences between Ripper and the original incremental reduced error pruning algorithm, which | less efficiently | considers all conditions as candidates for pruning <ref> (Furnkranz & Widmer 1994) </ref>. 8 We have also investigated whether the Student class contains a higher percentage of natural language text, but we could not empirically confirm this hypothesis (using the crude measure number of phrases per class over number of words per class). and Ripper (bottom) using phrases+words versus bigrams+words.
Reference: <author> Furnkranz, J. </author> <year> 1997. </year> <title> Pruning algorithms for rule learning. </title> <booktitle> Machine Learning 27(2) </booktitle> <pages> 139-171. </pages>
Reference: <author> Furnkranz, J. </author> <year> 1998. </year> <title> Separate-and-conquer rule learning. </title> <journal> Artificial Intelligence Review. </journal> <note> In press. </note>
Reference-contexts: All examples covered by the resulting rule are then removed from the training set and a new rule is learned in the same way until all examples are covered by at least one rule. Thus, Ripper is a member of the family of separate-and-conquer (or covering) rule learning algorithms <ref> (Furnkranz 1998) </ref>. What makes Ripper particularly well-suited for text categorization problems is its ability to use set-valued features (Cohen 1996). For conventional machine learning algorithms, a document is typically represented as a set of binary features, each encoding the presence or absence of a particular word in that document.
Reference: <author> Lang, K. </author> <year> 1995. </year> <title> NewsWeeder: Learning to filter netnews. </title> <editor> In Prieditis, A., and Russell, S., eds., </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML-95), </booktitle> <pages> 331-339. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This may be partly due to the fact that there are too few training examples for some of these classes. We plan to further investigate this on a more balanced data set, like the 20 newsgroups data used in <ref> (Lang 1995) </ref>. 8 Two other interesting observations to make in figure 1 are the differences in maximum recall between 7 This is one of the differences between Ripper and the original incremental reduced error pruning algorithm, which | less efficiently | considers all conditions as candidates for pruning (Furnkranz & Widmer <p> our case study may not have been an ideal choice for evaluating the utility of phrasal features, because significant parts of WWW pages do not contain natural language text. 9 Thus, we plan to further evaluate this technique on commonly used text categorization benchmarks, such as the 20 newsgroups dataset <ref> (Lang 1995) </ref> and the REUTERS newswire collection (Cohen & Singer 1996). On the other hand, we are also working on further improving the classification accuracy on the 4 universities data set used in this case study.
Reference: <author> Mitchell, T. M. </author> <year> 1997. </year> <title> Machine Learning. </title> <publisher> McGraw Hill. </publisher>
Reference-contexts: The data set used for our experiments is the 4-universities dataset, which has been collected for the WebKB project at Carnegie Mellon University. Rainbow Rainbow is a Naive Bayes classifier for text classification tasks <ref> (Mitchell 1997) </ref>, developed by Andrew McCallum at CMU 3 . It estimates the probability that a document is a member of a certain class using the 3 Available from http://www.cs.cmu.edu/afs/cs/ project/theo-11/www/naive-bayes.html. probabilities of words occurring in documents of that class independent of their context.
Reference: <author> Quinlan, J. R. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 239-266. </pages>
Reference-contexts: Ripper Ripper 4 (Cohen 1995) is an efficient, noise-tolerant rule learning algorithm based on the incremental reduced error pruning algorithm (Furnkranz & Widmer 1994; Furnkranz 1997). It learns single rules by greedily adding one condition at a time (using Foil's information gain heuristic <ref> (Quinlan 1990) </ref>) until the rule no longer makes incorrect predictions on the growing set, a randomly chosen subset of the training set.
Reference: <author> Riloff, E., and Lorenzen, J. </author> <year> 1998. </year> <title> Extraction-based text categorization: Generating domain-specific role relationships automatically. </title> <editor> In Strzalkowski, T., ed., </editor> <booktitle> Natural Language Information Retrieval. </booktitle> <publisher> Kluwer Academic Publishers. forthcoming. </publisher>
Reference-contexts: Note that the last of these, student at &lt; &gt;, does not match a contiguous piece of text, but is based on prepositional attachment to the word student . In previous work (e.g., <ref> (Riloff & Lorenzen 1998) </ref>), promising results on the usefulness of such phrases for text categorization tasks were obtained using simple statistical thresholding methods to find the best classification terms. <p> A set of experiments demonstrated that the occurrence or absence of linguistic phrases of the above form can be successfully used for recognizing relevant documents of the terrorist domain of the 4th Message Understanding Conference (MUC-4) <ref> (Riloff & Lorenzen 1998) </ref>. In this paper, we explore the potential use of the extraction patterns generated by AutoSlog-TS as phrasal features for state-of-the-art learning algorithms.
Reference: <author> Riloff, E. </author> <year> 1995. </year> <title> Little words can make a big difference for text classification. </title> <booktitle> In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> 130-136. </pages>
Reference-contexts: It seems likely that the plural version students at &lt; &gt; occurs more frequently on departmental pages than on student home pages. That such small differences in the use of words can make a big difference for text classification was previously observed in <ref> (Riloff 1995) </ref>. A set of experiments demonstrated that the occurrence or absence of linguistic phrases of the above form can be successfully used for recognizing relevant documents of the terrorist domain of the 4th Message Understanding Conference (MUC-4) (Riloff & Lorenzen 1998).
Reference: <author> Riloff, E. </author> <year> 1996a. </year> <title> Automatically generating extraction patterns from untagged text. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> 1044-1049. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: AutoSlog-TS, originally conceived for information extraction <ref> (Riloff 1996a) </ref>, is able to provide a learner with features that capture some of the syntactic structure of natural language text. <p> The extracted patterns typically represent subject-verb or verb-direct-object relationships (e.g., &lt;subject&gt; teaches or teaches &lt;direct-object&gt;) as well as prepositional phrase attachments (e.g., teaches at &lt;noun-phrase&gt; or teacher at &lt;noun-phrase&gt;). See (Riloff 1996b) for a more detailed description of Au-toSlog. The key difference between AutoSlog and AutoSlog-TS <ref> (Riloff 1996a) </ref> is the removal of the need for an annotated training corpus. AutoSlog-TS simply generates extraction patterns for all noun phrases in the training corpus whose syntactic role matches one of the heuristics.
Reference: <author> Riloff, E. </author> <year> 1996b. </year> <title> An empirical study of automated dictionary construction for information extraction in three domains. </title> <booktitle> Artificial Intelligence 85 </booktitle> <pages> 101-134. </pages>
Reference-contexts: The extracted patterns typically represent subject-verb or verb-direct-object relationships (e.g., &lt;subject&gt; teaches or teaches &lt;direct-object&gt;) as well as prepositional phrase attachments (e.g., teaches at &lt;noun-phrase&gt; or teacher at &lt;noun-phrase&gt;). See <ref> (Riloff 1996b) </ref> for a more detailed description of Au-toSlog. The key difference between AutoSlog and AutoSlog-TS (Riloff 1996a) is the removal of the need for an annotated training corpus. AutoSlog-TS simply generates extraction patterns for all noun phrases in the training corpus whose syntactic role matches one of the heuristics.
Reference: <author> Witten, I. H., and Bell, T. C. </author> <year> 1991. </year> <title> The zero-frequence problem: Estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Transactions on Information Theory 37(4) </journal> <pages> 1085-1094. </pages>
References-found: 16

