URL: http://www.cs.purdue.edu/homes/sb/papers/concurrency_pe_may94.ps
Refering-URL: http://www.cs.purdue.edu/homes/sb/papers/LIST.html
Root-URL: http://www.cs.purdue.edu
Title: Data Management for a Class of Iterative Computations on Distributed Memory MIMD Systems  
Author: M.C. Cornea-Hasegan, Dan C. Marinescu, and Zhongyun Zhang 
Date: November 2, 1993  
Address: West Lafayette, IN 47907  
Affiliation: Computer Sciences Department Purdue University  
Abstract: This paper discusses data management techniques for mapping a large data space onto the memory hierarchy of a distributed memory MIMD system. Experimental results for structural biology computations using the Molecular Replacement Method are presented. fl Work supported in part by the NSF under grants CCR-9119388 and BIR-9301210.
Abstract-found: 1
Intro-found: 1
Reference: [Fox 87] <author> G.C. Fox, </author> <title> Domain Decomposition in Distributred and Shared Memory Environments, </title> <booktitle> Proceedings of Supercomputing | 1st International Conference, </booktitle> <month> June </month> <year> 1987, </year> <pages> pp. 1042-1074. </pages>
Reference-contexts: The data management problem is about how to store data at the different levels of this hierarchy and is responsible for the dynamic data distribution and work allocation. Related work is reported in [Nic 86], [Nic 87], <ref> [Fox 87] </ref>, [Nic 88], and others. The larger is the data space, the more data has to be stored at the lower levels of the memory hierarchy.
Reference: [Gil 89] <author> W.K. Giloi, C. Hastedt, F. Shoen and W. Schroeder-Preikschat, </author> <title> A Distributed Implementation of Shared Virtual Memory with Strong and Weak Coherence, </title> <booktitle> Lect. Notes in Comp. Sci., </booktitle> <volume> 487, </volume> <editor> ed. A. Bode, </editor> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: No data coherence problem exists in the present implementation, but a subsequent one will consider a case of weak data coherence. Mechanisms for implementing a shared virtual memory are discussed in <ref> [Gil 89] </ref>, [Li 89], and [Stu 90]. Some of the terminology from virtual memory management (e.g., working set, fault, etc.), are extended here, to cover the interaction between data allocation and work distribution in a DMIMD system.
Reference: [Int 90] <author> Intel Corporation, </author> <title> iPSC/2 and iPSC/860 Programmer's Reference Manual, </title> <year> 1990 </year>
Reference-contexts: 1,738 6,713 7 2,879 116,997 5 22, 262 2,160,386 Table 7: The effect of the amount of data memory upon the execution time. the DN scheme, is based on the interrupt driven messages supported by the iPSC/860 and the Touchstone Delta machines (see the hrecv and associated system calls in <ref> [Int 90] </ref>). The results depend upon the overhead for processing an interrupt. An alternative to using interrupts, is to use polling toperiodically check whether other nodes request data stored in one node's local memory, and to service the existing requests.
Reference: [Kup 92] <author> A. Kuppermann, </author> <title> Ab Initio Quantum Mechanical Calculation of the Cross Sections and Rates of Chemical Reactions, </title> <booktitle> Proc. First Delta Appl. Workshop, </booktitle> <pages> pp. 146-155. </pages>
Reference-contexts: Ab initio quantum mechanics calculations of the cross-section and of rates of chemical reactions require inversions and eigensolutions of dense symmetric real matrices of 10 4 fi 10 4 elements <ref> [Kup 92] </ref>, and so on. In this paper, a class of problems is considered, characterized by the following traits : (a) The size of the problem is so large, that it is impractical to solve them using sequential or vector supercomputers.
Reference: [Li 89] <author> K. Li and R. Schaefer, </author> <title> A Hypercube Shared Virtual Memory System, </title> <booktitle> in Proc. 1989 Int. Conf. on Parallel Processing. </booktitle>
Reference-contexts: No data coherence problem exists in the present implementation, but a subsequent one will consider a case of weak data coherence. Mechanisms for implementing a shared virtual memory are discussed in [Gil 89], <ref> [Li 89] </ref>, and [Stu 90]. Some of the terminology from virtual memory management (e.g., working set, fault, etc.), are extended here, to cover the interaction between data allocation and work distribution in a DMIMD system.
Reference: [Li 92] <author> P. Li and D. Curkendall, </author> <title> Parallel 3-D Perspective Rendering, </title> <booktitle> Proc. First Delta Appl. Workshop, </booktitle> <pages> pp. 51-59. </pages>
Reference-contexts: 1 Introduction Some of the applications generically labeled "Grand Challenge Problems" require a very large data space. For example, the 3-D perspective rendering of a complete Venus map has an input data space of about 240 Gbytes <ref> [Li 92] </ref>. The determination of the atomic structure of a virus at high resolution, requires the processing of 200 Mbytes of data or more [Mar 92].
Reference: [Mar 92] <author> D.C. Marinescu, J.R. Rice, M.A. Cornea-Hasegan, R.E. Lynch and M.G. Ross-mann, </author> <title> Macromolecular Electron Density Averaging on Distributed Memory and MIMD 31 Systems, Intel Technology Focus Conference, </title> <address> Timberline Lodge, </address> <month> April </month> <year> 1992, </year> <pages> pp 32-56, </pages> <note> also in Concurrency, Practice and Experience, 1993 (in print) </note>
Reference-contexts: For example, the 3-D perspective rendering of a complete Venus map has an input data space of about 240 Gbytes [Li 92]. The determination of the atomic structure of a virus at high resolution, requires the processing of 200 Mbytes of data or more <ref> [Mar 92] </ref>. Ab initio quantum mechanics calculations of the cross-section and of rates of chemical reactions require inversions and eigensolutions of dense symmetric real matrices of 10 4 fi 10 4 elements [Kup 92], and so on. <p> As the amount of memory available in each PE of a distributed memory MIMD system limits the actual speedup that can be achieved on a real system, a Shared Virtual Memory, SVM, was implemented for the Intel iPSC/860 hypercube and the Touchstone Delta System, that is extensively discussed in <ref> [Mar 92] </ref>. No data coherence problem exists in the present implementation, but a subsequent one will consider a case of weak data coherence. Mechanisms for implementing a shared virtual memory are discussed in [Gil 89], [Li 89], and [Stu 90]. <p> The points in B are those for which an intensive computation has to be carried out, while practically no computation takes place for the points in C <ref> [Mar 92] </ref>. To distribute the work evenly among processors, a count of all the points in B is recorded 10 Initialization k = 0, i = 0, DAU count = 0, N proc = d N DAU Set A to be the set of all DAUs. <p> The calculation of the average electron density for three problems, called in the following, problems A, B and C, is used to compare different data management strategies. The three problems differ in the size of the grid, the non-crystallographic redundancy <ref> [Mar 92] </ref>, and the ratio of the number of points in the protein and of the number of points outside the protein. As shown before, the larger is the number m of transformations, the larger is the number of grid points to be averaged.
Reference: [Mar 93] <author> D.C. Marinescu, J.R. Rice, </author> <title> Speedup, Communication Complexity and Blocking | A la Recherche du Temps Perdu, </title> <booktitle> Seventh International Parallel Processing Symposium, </booktitle> <address> Newport Beach, CA, </address> <month> April 13-16, </month> <year> 1993. </year>
Reference-contexts: From these measurements, it can be concluded that 16 fi 16 fi 16 is a good choice, and therefore all the measurements reported in the following sections are using this size of a DAU. The relative speedup. The relative speedup <ref> [Mar 93] </ref> is a measure of performance for a parallel computation. It is denoted by S P;Q = T P =T Q with T P the execution time with P processing elements and T Q the one with Q &gt; P PEs.
Reference: [Nic 86] <author> D.M. Nicol, P.F. Reynolds, </author> <title> Dynamic Remapping Decisions in Multi-Phase Parallel Computations, </title> <type> ICASE report 86-58, </type> <month> September </month> <year> 1986. </year>
Reference-contexts: The data management problem is about how to store data at the different levels of this hierarchy and is responsible for the dynamic data distribution and work allocation. Related work is reported in <ref> [Nic 86] </ref>, [Nic 87], [Fox 87], [Nic 88], and others. The larger is the data space, the more data has to be stored at the lower levels of the memory hierarchy.
Reference: [Nic 87] <author> D.M. Nicol, J.H. Saltz, </author> <title> Schedules for Mapping Irregular Parallel Computations, </title> <type> ICASE report 87-52, </type> <month> September </month> <year> 1987. </year>
Reference-contexts: The data management problem is about how to store data at the different levels of this hierarchy and is responsible for the dynamic data distribution and work allocation. Related work is reported in [Nic 86], <ref> [Nic 87] </ref>, [Fox 87], [Nic 88], and others. The larger is the data space, the more data has to be stored at the lower levels of the memory hierarchy.
Reference: [Nic 88] <author> D.M. Nicol, </author> <title> Parallel Algorithms for Mapping Pipelined and Parallel Computations, </title> <type> ICASE report 88-2, </type> <month> April </month> <year> 1988. </year>
Reference-contexts: The data management problem is about how to store data at the different levels of this hierarchy and is responsible for the dynamic data distribution and work allocation. Related work is reported in [Nic 86], [Nic 87], [Fox 87], <ref> [Nic 88] </ref>, and others. The larger is the data space, the more data has to be stored at the lower levels of the memory hierarchy.
Reference: [Ros 72] <author> M.G. Rossmann, </author> <title> editor, The Molecular Replacement Method A Collection of Papers on the Use of the Non-Crystallographic Symmetry, </title> <publisher> Gordon and Breach Science Publishers, </publisher> <address> NY, London, Paris, </address> <year> 1972 </year>
Reference-contexts: the electron density averaging, part of the Molecular Replacement Method. 4 The Molecular Replacement Method The data management schemes discussed in this paper are related to one phase in the process of determining the structure of biological molecules by the Molecular Replacement Method, proposed by Rossmann and Blow in 1972 <ref> [Ros 72] </ref>. This method uses data gathered by X-ray crystallography, and is applicable in determining the structure of macromolecules which exhibit a high degree of symmetry and have in the order of a million of non-hydrogen atoms.
Reference: [Stu 90] <author> M. Stumm and S. Zhou, </author> <title> Algorithms Implementing Distributed Shared Memory, </title> <booktitle> Computer 23 (1990), </booktitle> <volume> 54. </volume> <pages> 32 </pages>
Reference-contexts: No data coherence problem exists in the present implementation, but a subsequent one will consider a case of weak data coherence. Mechanisms for implementing a shared virtual memory are discussed in [Gil 89], [Li 89], and <ref> [Stu 90] </ref>. Some of the terminology from virtual memory management (e.g., working set, fault, etc.), are extended here, to cover the interaction between data allocation and work distribution in a DMIMD system. A new lookahead technique is presented, which reduces the execution time for a certain size of the problem.
References-found: 13

