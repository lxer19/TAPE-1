URL: http://www.cs.cornell.edu/Info/People/amaldi/final_pap_2-12-97.ps
Refering-URL: http://www.cs.cornell.edu/Info/Projects/ccop/reports.html
Root-URL: 
Email: amaldi@cs.cornell.edu  mattavelli@de.epfl.ch  
Title: A combinatorial optimization approach to extract piecewise linear structure from nonlinear data and an application
Author: Edoardo Amaldix and Marco Mattavelliy 
Note: 1 Manuscript received: E.A. was partially funded by a Postdoctoral fellowship of the Swiss National Science Foundation. M.M. is currently with the Integrated System Center (C3I),  
Date: December 9, 1997  
Address: Ithaca, NY 14853, USA  CH-1015 Lausanne, Switzerland  CH-1015 Lausanne, Switzerland.  
Affiliation: xSchool of Operations Research and Theory Center Cornell University,  ySignal Processing Laboratory, Swiss Federal Institute of Technology,  Swiss Federal Institute of Technology,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Bertero, T. Poggio, and V. Torre. </author> <title> Ill-posed problems in early vision. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76(8) </volume> <pages> 869-889, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Since these tasks involve recovering properties of 3-D scenes from 2-D images, they can be regarded as ill-posed inverse optics problems <ref> [1] </ref>. In particular, the solution may not exist, may not be unique or may not depend continuously on the data due to the ambiguities arising from the 2-D projection and the noise or errors in the data. Regularization theory (see [1]) has been proposed as a unified and standard way of <p> images, they can be regarded as ill-posed inverse optics problems <ref> [1] </ref>. In particular, the solution may not exist, may not be unique or may not depend continuously on the data due to the ambiguities arising from the 2-D projection and the noise or errors in the data. Regularization theory (see [1]) has been proposed as a unified and standard way of finding approximate solutions to ill-posed problems. <p> The global smoothness constraint is a typical example of widely used a priori assumption. A variety of computational vision problems such as, for instance, edge detection, surface reconstruction and optical flow estimation, have first been tackled using quadratic functionals and Tikhonov-like stabilizers <ref> [1, 3] </ref>. Since discontinuities frequently convey crucial information, classes of nonlinear and spatially non-invariant regularization methods [4] as well as a probabilistic framework based on Markov random fields (MRF) [2] have been proposed. The aim is to deal with more realistic piecewise smoothness constraints.
Reference: [2] <author> J. Marroquin. </author> <title> Regularization theory and low-level vision. </title> <editor> In M. Arbib, editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 800-804, </pages> <address> Cambridge, MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The main idea is to introduce a priori knowledge by using variational principles that impose constraints on the admissible solutions or by making statistical assumptions on the solution space (see <ref> [2] </ref> and the included references). The global smoothness constraint is a typical example of widely used a priori assumption. A variety of computational vision problems such as, for instance, edge detection, surface reconstruction and optical flow estimation, have first been tackled using quadratic functionals and Tikhonov-like stabilizers [1, 3]. <p> Since discontinuities frequently convey crucial information, classes of nonlinear and spatially non-invariant regularization methods [4] as well as a probabilistic framework based on Markov random fields (MRF) <ref> [2] </ref> have been proposed. The aim is to deal with more realistic piecewise smoothness constraints. However, the introduction of discontinuities 2 within this type of approaches (using explicit line processes or assuming prior distributions) turns out to be a delicate issue. <p> reader is referred to [10] for a more detailed discussion of discrete linear inverse problems with discontinuities and piecewise linear models. 4 regularization approaches where piecewise smooth random fields are obtained by imposing a set of constraints on their local properties and, in particular, on the shape of the discontinuities <ref> [2] </ref>. Such local constraints try, for instance, to favor discontinuities arranged in piecewise smooth lines and to prevent the formation of smooth patches (regions of the image corresponding to the same submodel) that are too small. <p> Not surprisingly, the number of distinct submodels in the mixture depends on . Markov random fields <ref> [2, 45] </ref>, which take into account discontinuities, are also confronted with the problem of estimating the number of submodels. Moreover, extensions of MRFs to 2-D piecewise smooth models implicitly require that regions, assigned to the same submodel, be contiguous since they only integrate local information [45].
Reference: [3] <author> T. Poggio, V. Torre, and C. Koch. </author> <title> Computational vision and regularization theory. </title> <journal> Nature, </journal> <volume> 317 </volume> <pages> 314-319, </pages> <month> September </month> <year> 1985. </year>
Reference-contexts: The global smoothness constraint is a typical example of widely used a priori assumption. A variety of computational vision problems such as, for instance, edge detection, surface reconstruction and optical flow estimation, have first been tackled using quadratic functionals and Tikhonov-like stabilizers <ref> [1, 3] </ref>. Since discontinuities frequently convey crucial information, classes of nonlinear and spatially non-invariant regularization methods [4] as well as a probabilistic framework based on Markov random fields (MRF) [2] have been proposed. The aim is to deal with more realistic piecewise smoothness constraints.
Reference: [4] <author> D. Terzopoulos. </author> <title> Regularization of inverse visual problems involving discontinuities. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-8(4):413-424, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: A variety of computational vision problems such as, for instance, edge detection, surface reconstruction and optical flow estimation, have first been tackled using quadratic functionals and Tikhonov-like stabilizers [1, 3]. Since discontinuities frequently convey crucial information, classes of nonlinear and spatially non-invariant regularization methods <ref> [4] </ref> as well as a probabilistic framework based on Markov random fields (MRF) [2] have been proposed. The aim is to deal with more realistic piecewise smoothness constraints.
Reference: [5] <author> P.J. Rousseeuw L. Kaufman. </author> <title> Finding groups in data. </title> <publisher> John Wiley & Sons, Inc, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Piecewise linear models are often attractive since they enable approximation of complex nonlinear phenomena but are still simple enough due to local linearity. Unfortunately, piecewise decomposition and parameter estimation appears as a difficult problem. Conventional approaches in which one first partitions the data using some clustering methods <ref> [5] </ref> and then estimates the parameters associated with each submodel using robust regression techniques [6] have major drawbacks. In particular, the number of linear submodels needs to be guessed in advance and, even more importantly, clustering methods do not take into account the type of submodel used. <p> First, one partitions the data into a certain number of components using clustering methods <ref> [5] </ref> and then one estimates each linear submodel using robust regression techniques. The data is usually subdivided into disjoint or fuzzy sets by minimizing some distance measure between the data points and the cluster centroids. <p> If the number of candidate centroids is arbitrarily limited to a certain value c using some thresholding criteria, the time complexity is still O (p c! (ck)!k! ), where k is the number of clusters, p is as above and obviously c &gt; k <ref> [5] </ref>. An even more important limitation of such two-phase approaches is that clustering methods may yield meaningless partitions in terms of piecewise linear models since they do not take into account the type of submodels (linear) used. <p> Note that no robust regression algorithm can deal with such data unless a preliminary and, in most cases, very problematic clustering stage is performed before the regressions. Figure 5 gives typical examples of the clusters obtained with the classical k-medoid algorithm <ref> [5] </ref> which is based on the minimization of the error distances between the vectors. Note that, although the correct number of components -which is usually unknown- has been used, the resulting clusters differ considerably from the actual motion components. <p> The three reported segmentations correspond to different threshold values that have been used to define the set of candidate centroids based on the histogram of the vector components <ref> [5] </ref>. Thus, even if the correct number of components is known and the thresholds are tuned to have reasonable computational requirements, the clustering stage can yield a misleading decomposition.
Reference: [6] <author> A. M. Leroy P.J. Rousseeuw. </author> <title> Robust regression and outlier detection. </title> <publisher> John Wiley & Sons, Inc, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Unfortunately, piecewise decomposition and parameter estimation appears as a difficult problem. Conventional approaches in which one first partitions the data using some clustering methods [5] and then estimates the parameters associated with each submodel using robust regression techniques <ref> [6] </ref> have major drawbacks. In particular, the number of linear submodels needs to be guessed in advance and, even more importantly, clustering methods do not take into account the type of submodel used. <p> In other words, one is confronted with over-determined systems (p is typically much larger than n) for which classical least-mean-square error solutions are not appropriate. Also robust regression techniques <ref> [6] </ref>, which assume that the underlying linear estimation problem admits a solution and that (1) is inconsistent because of spurious or noisy data, have a limited range of application. <p> Robust regression techniques <ref> [6] </ref> have been devised to deal with outliers and have been applied to a number of signal and image processing problems [26]. The breakdown point of such a method is the smallest fraction of outliers contaminating the data that can make the estimation error unbounded. <p> The breakdown point of such a method is the smallest fraction of outliers contaminating the data that can make the estimation error unbounded. Among the robust estimators that have been proposed in the literature, the repeated-median and the least-median-of-squares ones have the highest breakdown point of 0:5 <ref> [6, 26] </ref>. Unfortunately, they have very high computational requirements, namely an O (p n log n p) and respectively an O (p (n+1) log p) time complexity, where p is the number of data points and n is the number of parameters to be estimated.
Reference: [7] <author> J. Illingworth and J. Kittler. </author> <title> A survey of the Hough Transform. </title> <journal> Comput. Vison Graphics Image Process., </journal> <volume> 44 </volume> <pages> 87-116, </pages> <year> 1988. </year>
Reference-contexts: In particular, the number of linear submodels needs to be guessed in advance and, even more importantly, clustering methods do not take into account the type of submodel used. In principle, the Hough Transform (HT) and its variants <ref> [7, 8] </ref> can solve a wide range of model identification problems without requiring any a priori assumption. However, their computational requirements to guarantee a reasonable accuracy can be prohibitive even for small problems. <p> An illustrative example arising in our 2-D motion analysis application will be presented in Section 6.3. Clearly, the MIN PCS-based algorithm does not have these serious shortcomings. 5.3 Hough transform The Hough Transform is a general class of techniques for linear and nonlinear model identification <ref> [7, 8] </ref>. Local information is used to accumulate evidence for some particular sets of parameter values of the model under consideration. Being relatively insensitive to noise and partially incorrect data, HT has been proposed, for instance, to determine shape, motion and geometric transformation parameters [27, 28, 29]. <p> However, even if the storage requirements are reduced, the order of complexity of the HT remains very high for models with more than a few parameters. Moreover, variants with refined search strategies (see <ref> [7, 8] </ref>) that in some cases considerably reduce the computational requirements may not be appropriate when more than a single model have to be simultaneously identified (e.g., the submodels of a piecewise linear one).
Reference: [8] <author> V. F. Leavers. </author> <title> Which Hough Transform? CVGIP: </title> <journal> Image Understading, </journal> <volume> 58(2) </volume> <pages> 250-264, </pages> <year> 1993. </year> <month> 22 </month>
Reference-contexts: In particular, the number of linear submodels needs to be guessed in advance and, even more importantly, clustering methods do not take into account the type of submodel used. In principle, the Hough Transform (HT) and its variants <ref> [7, 8] </ref> can solve a wide range of model identification problems without requiring any a priori assumption. However, their computational requirements to guarantee a reasonable accuracy can be prohibitive even for small problems. <p> An illustrative example arising in our 2-D motion analysis application will be presented in Section 6.3. Clearly, the MIN PCS-based algorithm does not have these serious shortcomings. 5.3 Hough transform The Hough Transform is a general class of techniques for linear and nonlinear model identification <ref> [7, 8] </ref>. Local information is used to accumulate evidence for some particular sets of parameter values of the model under consideration. Being relatively insensitive to noise and partially incorrect data, HT has been proposed, for instance, to determine shape, motion and geometric transformation parameters [27, 28, 29]. <p> However, even if the storage requirements are reduced, the order of complexity of the HT remains very high for models with more than a few parameters. Moreover, variants with refined search strategies (see <ref> [7, 8] </ref>) that in some cases considerably reduce the computational requirements may not be appropriate when more than a single model have to be simultaneously identified (e.g., the submodels of a piecewise linear one).
Reference: [9] <author> G. Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by multiple moving objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-7(4):384-401, </volume> <month> July </month> <year> 1985. </year>
Reference-contexts: In particular, we focus here on the segmentation of the optical flow. 2-D motion estimation has attracted a considerable attention because it is an intermediate step towards 3-D motion estimation <ref> [9] </ref> and it is useful in a number of settings such as, for instance, object tracking, robot guidance and in new video compression standards like MPEG-4 (see [10] and included references). <p> In practice, however, the high dimensionality and the fine resolution needed to guarantee a reasonable accuracy require very large amounts of memory and computation time even for small problems, see for instance <ref> [9] </ref>. In some cases, intelligent iterative and adaptive coarse to fine search strategies [28, 29] can be applied. In others, deterministically or randomly selected subsets of the data are used in order to make some hypotheses or eliminate outliers in conjunction with other heuristics [9, 30]. <p> In some cases, intelligent iterative and adaptive coarse to fine search strategies [28, 29] can be applied. In others, deterministically or randomly selected subsets of the data are used in order to make some hypotheses or eliminate outliers in conjunction with other heuristics <ref> [9, 30] </ref>. However, even if the storage requirements are reduced, the order of complexity of the HT remains very high for models with more than a few parameters. <p> While the former methods involve first the computation of the optical flow and then its interpretation <ref> [9, 31, 32, 33, 34, 35, 36] </ref>, the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities [37, 38, 39, 40, 41, 42, 43]. <p> Under reasonable assumptions (see for instance <ref> [9] </ref>), the 2-D apparent motion of an object can be approximated by an affine transformation given, for every image point (x; y), by: V x (x; y) = w 1 + w 2 x + w 3 y where w 1 ; : : : ; w 6 are the parameters.
Reference: [10] <author> M. Mattavelli. </author> <title> Motion analysis and estimation: from ill-posed discrete inverse linear problems to MPEG-2 coding. </title> <type> PhD thesis, </type> <institution> Communication Systems Division, Swiss Federal Institute of Technology, </institution> <type> Thesis No. 1597, </type> <institution> Lausanne, </institution> <year> 1997. </year>
Reference-contexts: segmentation of the optical flow. 2-D motion estimation has attracted a considerable attention because it is an intermediate step towards 3-D motion estimation [9] and it is useful in a number of settings such as, for instance, object tracking, robot guidance and in new video compression standards like MPEG-4 (see <ref> [10] </ref> and included references). <p> In other words, it enables one to enforce piecewise smoothness constraints on the solutions and to deal with discontinuities without making any particular assumption on their geometry 1 . This is in contrast with probabilistic 1 The reader is referred to <ref> [10] </ref> for a more detailed discussion of discrete linear inverse problems with discontinuities and piecewise linear models. 4 regularization approaches where piecewise smooth random fields are obtained by imposing a set of constraints on their local properties and, in particular, on the shape of the discontinuities [2]. <p> In spite of the considerable research effort, estimating motion from image sequences containing multiple moving objects remains a challenging problem. As discussed in <ref> [10] </ref>, the methods proposed in the literature may be subdivided into two wide classes: those based on the optical flow, i.e., the 2-D field of instantaneous velocities of brightness patterns, and those based on motion models. <p> In practice, x 0 is randomly generated, typical values of ff and fi are respectively 2 and 3=2, and fl is linearly decreased from T 0 to 0 over the C cycles (fl = 1 c C ). See <ref> [10] </ref> for more details.
Reference: [11] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: MIN PCS: Given a possibly inconsistent linear system Ax = b with A 2 R pfin and b 2 R p , find a Partition of this system into a MINimum number of Consistent Subsystems. According to the well-known Occam razor principle <ref> [11] </ref>, we look for the "simplest" piecewise linear model consistent with the data, which is most likely to be the correct one. Here complexity is measured in terms of the number of linear submodels.
Reference: [12] <author> P. Crescenzi and V. Kann. </author> <title> A compendium of np-optimization problems. </title> <note> Available at: http:// www.nada.kth.se /~viggo /problemlist /compendium.html, </note> <year> 1995. </year>
Reference-contexts: can every instance of MIN PCS be solved in a number of operations that grows polynomially with its size (n, p and the magnitude of the coefficients) or is it at least as hard to solve in the worst-case as many important problems such as the famous traveling salesman problem <ref> [12] </ref>? The theory of NP-completeness [13, 14] provides a framework to address this question. Theorem 1 MIN PCS is NP-hard, i.e., it cannot be solved in polynomial time unless P = NP. The proof is given in the Appendix. <p> Not surprisingly, the number of clusters, which is usually unknown and needs to be guessed in advance, has a strong impact on the results. But, since even the simplest clustering problems are NP-hard <ref> [12] </ref>, the high computational requirements rule out a trial-and-error procedure.
Reference: [13] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: a guide to the theory of NP completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: PCS be solved in a number of operations that grows polynomially with its size (n, p and the magnitude of the coefficients) or is it at least as hard to solve in the worst-case as many important problems such as the famous traveling salesman problem [12]? The theory of NP-completeness <ref> [13, 14] </ref> provides a framework to address this question. Theorem 1 MIN PCS is NP-hard, i.e., it cannot be solved in polynomial time unless P = NP. The proof is given in the Appendix. <p> NP-completeness is established by giving a polynomial time reduction from the classical NP-complete PARTITION problem that is defined as follows <ref> [13] </ref>.
Reference: [14] <author> C. H. Papadimitriou and K. Steiglitz. </author> <title> Combinatorial optimization algorithms and complexity. </title> <publisher> Prentice Hall, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: PCS be solved in a number of operations that grows polynomially with its size (n, p and the magnitude of the coefficients) or is it at least as hard to solve in the worst-case as many important problems such as the famous traveling salesman problem [12]? The theory of NP-completeness <ref> [13, 14] </ref> provides a framework to address this question. Theorem 1 MIN PCS is NP-hard, i.e., it cannot be solved in polynomial time unless P = NP. The proof is given in the Appendix.
Reference: [15] <author> J. Ryan. </author> <title> Transversals of IIS-hypergraphs. </title> <journal> Congressus Numerantium, </journal> <volume> 81 </volume> <pages> 17-22, </pages> <year> 1991. </year>
Reference-contexts: It is noteworthy that MIN PCS is trivial for inconsistent systems of inequalities since any such system can be partitioned into two consistent subsystems. This fact is obvious for homogeneous systems and is easily verified for inhomogeneous ones <ref> [15] </ref>.
Reference: [16] <author> E. Amaldi. </author> <title> From finding maximum feasible subsystems of linear systems to feedforward neural network design. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, Swiss Federal Institute of Technology, Lausanne, </institution> <year> 1994. </year>
Reference-contexts: Several variants of the general problem of finding MAXimum Consistent Subsystems of linear systems, referred to as MAX CS, arise in other fields such as machine learning and operations research (see <ref> [16] </ref> 7 and the included references). In [17] we investigated the computational complexity of MAX CS and in [16, 18] some variants of the so-called perceptron method [19] studied in the machine learning literature. <p> Several variants of the general problem of finding MAXimum Consistent Subsystems of linear systems, referred to as MAX CS, arise in other fields such as machine learning and operations research (see [16] 7 and the included references). In [17] we investigated the computational complexity of MAX CS and in <ref> [16, 18] </ref> some variants of the so-called perceptron method [19] studied in the machine learning literature. Although MAX CS is NP-hard to approximate within some constant factors [17], those thermal 3 perceptron variants [20, 16, 18] provide good solutions in a short amount of time. <p> In [17] we investigated the computational complexity of MAX CS and in [16, 18] some variants of the so-called perceptron method [19] studied in the machine learning literature. Although MAX CS is NP-hard to approximate within some constant factors [17], those thermal 3 perceptron variants <ref> [20, 16, 18] </ref> provide good solutions in a short amount of time. In this subsection we show that a thermal version of the Agmon-Motzkin-Schoenberg relaxation method (AMS) for solving consistent systems of linear inequalities [21, 22, 23] can be used to find large consistent subsystems of (2). <p> Now a minimum partition includes only two consistent subsystems, for instance equations (12) and (14) on one side and equations (11) and (13) on the other side. 21 Annealing schedule As discussed in <ref> [20, 16, 18] </ref> for the thermal perceptron procedure, the choice of the initial temperature T 0 and the annealing schedule can have a significant impact on the quality of the solutions.
Reference: [17] <author> E. Amaldi and V. Kann. </author> <title> The complexity and approximability of finding maximum feasible sub systems of linear relations. </title> <booktitle> Theoretical Computer Science, </booktitle> <address> 147(1-2):181-210, </address> <year> 1995. </year>
Reference-contexts: Several variants of the general problem of finding MAXimum Consistent Subsystems of linear systems, referred to as MAX CS, arise in other fields such as machine learning and operations research (see [16] 7 and the included references). In <ref> [17] </ref> we investigated the computational complexity of MAX CS and in [16, 18] some variants of the so-called perceptron method [19] studied in the machine learning literature. Although MAX CS is NP-hard to approximate within some constant factors [17], those thermal 3 perceptron variants [20, 16, 18] provide good solutions in <p> In <ref> [17] </ref> we investigated the computational complexity of MAX CS and in [16, 18] some variants of the so-called perceptron method [19] studied in the machine learning literature. Although MAX CS is NP-hard to approximate within some constant factors [17], those thermal 3 perceptron variants [20, 16, 18] provide good solutions in a short amount of time.
Reference: [18] <author> E. Amaldi, C. Diderich, and R. Hauser. </author> <title> On the probabilistic and thermal perceptron train ing algorithms. </title> <type> Technical report, </type> <year> 1997. </year> <note> Manuscript available at http://www.cs.cornell.edu/ Info/People/amaldi/amaldi.html. </note>
Reference-contexts: Several variants of the general problem of finding MAXimum Consistent Subsystems of linear systems, referred to as MAX CS, arise in other fields such as machine learning and operations research (see [16] 7 and the included references). In [17] we investigated the computational complexity of MAX CS and in <ref> [16, 18] </ref> some variants of the so-called perceptron method [19] studied in the machine learning literature. Although MAX CS is NP-hard to approximate within some constant factors [17], those thermal 3 perceptron variants [20, 16, 18] provide good solutions in a short amount of time. <p> In [17] we investigated the computational complexity of MAX CS and in [16, 18] some variants of the so-called perceptron method [19] studied in the machine learning literature. Although MAX CS is NP-hard to approximate within some constant factors [17], those thermal 3 perceptron variants <ref> [20, 16, 18] </ref> provide good solutions in a short amount of time. In this subsection we show that a thermal version of the Agmon-Motzkin-Schoenberg relaxation method (AMS) for solving consistent systems of linear inequalities [21, 22, 23] can be used to find large consistent subsystems of (2). <p> Therefore it suffices to check whether each new estimate x i+1 satisfies a larger number of pairs of inequalities (2) than the best one generated so far. See <ref> [18] </ref> for the asymptotic convergence properties of closely related variants of the thermal perceptron procedure. <p> Now a minimum partition includes only two consistent subsystems, for instance equations (12) and (14) on one side and equations (11) and (13) on the other side. 21 Annealing schedule As discussed in <ref> [20, 16, 18] </ref> for the thermal perceptron procedure, the choice of the initial temperature T 0 and the annealing schedule can have a significant impact on the quality of the solutions.
Reference: [19] <author> M. L. Minsky and S. Papert. </author> <title> Perceptrons: An introduction to computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year> <note> Expanded edition. </note>
Reference-contexts: In [17] we investigated the computational complexity of MAX CS and in [16, 18] some variants of the so-called perceptron method <ref> [19] </ref> studied in the machine learning literature. Although MAX CS is NP-hard to approximate within some constant factors [17], those thermal 3 perceptron variants [20, 16, 18] provide good solutions in a short amount of time.
Reference: [20] <author> M. Frean. </author> <title> A "thermal" perceptron learning rule. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 946-957, </pages> <year> 1992. </year>
Reference-contexts: In [17] we investigated the computational complexity of MAX CS and in [16, 18] some variants of the so-called perceptron method [19] studied in the machine learning literature. Although MAX CS is NP-hard to approximate within some constant factors [17], those thermal 3 perceptron variants <ref> [20, 16, 18] </ref> provide good solutions in a short amount of time. In this subsection we show that a thermal version of the Agmon-Motzkin-Schoenberg relaxation method (AMS) for solving consistent systems of linear inequalities [21, 22, 23] can be used to find large consistent subsystems of (2). <p> As in <ref> [20] </ref>, it is appropriate to pay decreasing attention to unsatisfied inequalities with large violations, namely, to perform at the beginning all updates prescribed by the standard procedure and then only those which aim at correcting unsatisfied inequalities with progressively smaller and smaller v k i . <p> Now a minimum partition includes only two consistent subsystems, for instance equations (12) and (14) on one side and equations (11) and (13) on the other side. 21 Annealing schedule As discussed in <ref> [20, 16, 18] </ref> for the thermal perceptron procedure, the choice of the initial temperature T 0 and the annealing schedule can have a significant impact on the quality of the solutions.
Reference: [21] <author> S. Agmon. </author> <title> The relaxation method for linear inequalities. </title> <journal> Canadian Journal of Mathematics, </journal> <volume> 6 </volume> <pages> 382-392, </pages> <year> 1954. </year>
Reference-contexts: In this subsection we show that a thermal version of the Agmon-Motzkin-Schoenberg relaxation method (AMS) for solving consistent systems of linear inequalities <ref> [21, 22, 23] </ref> can be used to find large consistent subsystems of (2). The AMS method is a simple iterative procedure that generates a sequence of estimates.
Reference: [22] <author> T. S. Motzkin and I. J. </author> <title> Schoenberg. The relaxation method for linear inequalities. </title> <journal> Canadian Journal of Mathematics, </journal> <volume> 6 </volume> <pages> 393-404, </pages> <year> 1954. </year>
Reference-contexts: In this subsection we show that a thermal version of the Agmon-Motzkin-Schoenberg relaxation method (AMS) for solving consistent systems of linear inequalities <ref> [21, 22, 23] </ref> can be used to find large consistent subsystems of (2). The AMS method is a simple iterative procedure that generates a sequence of estimates.
Reference: [23] <author> Y. Censor. </author> <title> Row-action methods for huge and sparse systems and their applications. </title> <journal> SIAM Reviews, </journal> <volume> 23 </volume> <pages> 444-466, </pages> <year> 1981. </year>
Reference-contexts: In this subsection we show that a thermal version of the Agmon-Motzkin-Schoenberg relaxation method (AMS) for solving consistent systems of linear inequalities <ref> [21, 22, 23] </ref> can be used to find large consistent subsystems of (2). The AMS method is a simple iterative procedure that generates a sequence of estimates.
Reference: [24] <author> J. Telgen. </author> <title> On relaxation methods for systems of linear inequalities. </title> <journal> European Journal of Operational Research, </journal> <volume> 9 </volume> <pages> 184-189, </pages> <year> 1982. </year>
Reference-contexts: For other values, x i+1 lies on the line connecting x i and its orthogonal reflexion with respect to H k . For any consistent system, this method is guaranteed to yield a solution in a finite but in the worst-case exponential number of iterations <ref> [24] </ref>. Although more sophisticated stopping criteria have been proposed [24], in practice one usually stops after a predefined maximum number of cycles C through the p inequalities. <p> For any consistent system, this method is guaranteed to yield a solution in a finite but in the worst-case exponential number of iterations <ref> [24] </ref>. Although more sophisticated stopping criteria have been proposed [24], in practice one usually stops after a predefined maximum number of cycles C through the p inequalities. We now describe a thermal variant of the AMS relaxation method which provides close-to-maximum consistent subsystems of (2) in a short amount of time.
Reference: [25] <author> R. </author> <title> Greer. Trees and Hills: Methodology for Maximizing Functions of Systems of Linear Relations, </title> <booktitle> volume 22 of Annals of Discrete Mathematics. </booktitle> <publisher> Elsevier science publishers B.V., </publisher> <address> Amsterdam, </address> <year> 1984. </year>
Reference-contexts: See [18] for the asymptotic convergence properties of closely related variants of the thermal perceptron procedure. Finally, it is worth noting that implicit enumeration methods, like the general technique for maximizing functions of linear relations presented in <ref> [25] </ref>, can be adapted to find optimal solutions in a finite but in the worst-case exponential amount of time.
Reference: [26] <author> P. Meer, D. Mintz, A. Rosenfeld, and D. Kim. </author> <title> Robust regression methods for computer vision: a review. </title> <journal> International journal of computer vision, </journal> <volume> 6 </volume> <pages> 59-70, </pages> <year> 1991. </year>
Reference-contexts: Robust regression techniques [6] have been devised to deal with outliers and have been applied to a number of signal and image processing problems <ref> [26] </ref>. The breakdown point of such a method is the smallest fraction of outliers contaminating the data that can make the estimation error unbounded. Among the robust estimators that have been proposed in the literature, the repeated-median and the least-median-of-squares ones have the highest breakdown point of 0:5 [6, 26]. <p> The breakdown point of such a method is the smallest fraction of outliers contaminating the data that can make the estimation error unbounded. Among the robust estimators that have been proposed in the literature, the repeated-median and the least-median-of-squares ones have the highest breakdown point of 0:5 <ref> [6, 26] </ref>. Unfortunately, they have very high computational requirements, namely an O (p n log n p) and respectively an O (p (n+1) log p) time complexity, where p is the number of data points and n is the number of parameters to be estimated. <p> Variants of the least-median-of-squares method using Monte Carlo-like techniques have a lower complexity but they may fail with some nonzero (even though low) probability <ref> [26] </ref>. In practice, when the discontinuities are not known a priori, the definition of outliers is too restrictive.
Reference: [27] <author> H. Maitre. </author> <title> The adaptive Hough Transform. </title> <booktitle> Traitement du signal, </booktitle> <volume> 2, </volume> <year> 1985. </year>
Reference-contexts: Local information is used to accumulate evidence for some particular sets of parameter values of the model under consideration. Being relatively insensitive to noise and partially incorrect data, HT has been proposed, for instance, to determine shape, motion and geometric transformation parameters <ref> [27, 28, 29] </ref>. In the classical HT, the first step is to map each feature in the data space onto a multidimensional surface in the parameter space corresponding to all combinations of the parameter values consistent with it (e.g., hyperplanes for linear models).
Reference: [28] <author> J. Illingworth and J. Kittler. </author> <title> The adaptive Hough Transform. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-9(5):690-698, </volume> <month> September </month> <year> 1987. </year>
Reference-contexts: Local information is used to accumulate evidence for some particular sets of parameter values of the model under consideration. Being relatively insensitive to noise and partially incorrect data, HT has been proposed, for instance, to determine shape, motion and geometric transformation parameters <ref> [27, 28, 29] </ref>. In the classical HT, the first step is to map each feature in the data space onto a multidimensional surface in the parameter space corresponding to all combinations of the parameter values consistent with it (e.g., hyperplanes for linear models). <p> In practice, however, the high dimensionality and the fine resolution needed to guarantee a reasonable accuracy require very large amounts of memory and computation time even for small problems, see for instance [9]. In some cases, intelligent iterative and adaptive coarse to fine search strategies <ref> [28, 29] </ref> can be applied. In others, deterministically or randomly selected subsets of the data are used in order to make some hypotheses or eliminate outliers in conjunction with other heuristics [9, 30]. <p> In the Randomized HT [30], for instance, one may have to sample a much larger number of subsets of data points in order to achieve the ad-hoc threshold indicating the presence of a probable peak. In coarse to fine strategies such as the adaptive HT <ref> [28] </ref>, the initial stages where a coarse quantization is considered may fail to detect some of the peaks corresponding to the submodels to be identified. Indeed, relatively small peaks may be hidden in a noisy background.
Reference: [29] <author> R.A. Samy and C.A. Bozzo. </author> <title> Moving object recognition using motion enhanced Hough Transform. </title> <editor> In V. Cappellini and A.G. Costantinides, editors, </editor> <booktitle> Digital Signal Processing-84, </booktitle> <pages> pages 770-775, </pages> <address> Amsterdam, The Netherlands, </address> <year> 1984. </year>
Reference-contexts: Local information is used to accumulate evidence for some particular sets of parameter values of the model under consideration. Being relatively insensitive to noise and partially incorrect data, HT has been proposed, for instance, to determine shape, motion and geometric transformation parameters <ref> [27, 28, 29] </ref>. In the classical HT, the first step is to map each feature in the data space onto a multidimensional surface in the parameter space corresponding to all combinations of the parameter values consistent with it (e.g., hyperplanes for linear models). <p> In practice, however, the high dimensionality and the fine resolution needed to guarantee a reasonable accuracy require very large amounts of memory and computation time even for small problems, see for instance [9]. In some cases, intelligent iterative and adaptive coarse to fine search strategies <ref> [28, 29] </ref> can be applied. In others, deterministically or randomly selected subsets of the data are used in order to make some hypotheses or eliminate outliers in conjunction with other heuristics [9, 30].
Reference: [30] <author> L. Xu, E. Oja, and P. Kultanen. </author> <title> A new curve detection method: Randomized Hough Transform. </title> <journal> Pattern Recognition Letter, </journal> <volume> 11 </volume> (5):334-344, 1990. 
Reference-contexts: In some cases, intelligent iterative and adaptive coarse to fine search strategies [28, 29] can be applied. In others, deterministically or randomly selected subsets of the data are used in order to make some hypotheses or eliminate outliers in conjunction with other heuristics <ref> [9, 30] </ref>. However, even if the storage requirements are reduced, the order of complexity of the HT remains very high for models with more than a few parameters. <p> Moreover, variants with refined search strategies (see [7, 8]) that in some cases considerably reduce the computational requirements may not be appropriate when more than a single model have to be simultaneously identified (e.g., the submodels of a piecewise linear one). In the Randomized HT <ref> [30] </ref>, for instance, one may have to sample a much larger number of subsets of data points in order to achieve the ad-hoc threshold indicating the presence of a probable peak.
Reference: [31] <author> J. L. Barron, D. J. Fleet, and S. S. Beauchemin. </author> <title> Performances of optical flow techniques. </title> <type> Technical Report Report no. 299, </type> <institution> Dept. of Computer Science University of Western Ontario, </institution> <address> London, Ontario, Canada, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation <ref> [9, 31, 32, 33, 34, 35, 36] </ref>, the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities [37, 38, 39, 40, 41, 42, 43]. <p> Let us assume that the optical flow has been estimated using a standard technique <ref> [31] </ref>, i.e., we have a vector (V x (x k ; y k ); V y (x k ; y k )) for p image points (x k ; y k ) with 1 k p.
Reference: [32] <author> A. Rognone, M. Campani, and A. Verri. </author> <title> Identifying multiple motions from optical flow. </title> <editor> In G. San dini editor, editor, </editor> <booktitle> Proceeding of the second European conference on computer vision, </booktitle> <pages> pages 258-266, </pages> <address> S. Margherita Ligure, Italy, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation <ref> [9, 31, 32, 33, 34, 35, 36] </ref>, the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities [37, 38, 39, 40, 41, 42, 43]. <p> However, frameworks in which outliers are detected and rejected do not allow simultaneous estimation of different motion components. Several algorithms that identify multiple motions from pre-computed optical flow fields rely on clustering techniques <ref> [32, 40] </ref>. As previously mentioned, these algorithms have two main drawbacks. First, the number of moving components has to be estimated. Second, working only with optical flow vector components and neglecting the dependence of those amplitudes on the reference coordinates, may not suffice to recover the actual motion components. <p> First, the number of moving components has to be estimated. Second, working only with optical flow vector components and neglecting the dependence of those amplitudes on the reference coordinates, may not suffice to recover the actual motion components. Not surprisingly, clustering-based approaches such as 14 in <ref> [32, 40] </ref> present a high degree of arbitrariness. Model-based methods face the same type of fundamental difficulty [40, 43]. The major issue is to partition the images into layers corresponding to different coherently moving components. In some approaches, one tries to identify and extract dominant components iteratively.
Reference: [33] <author> T. M. Chin, , M. R. Luettgen, W. C. Karl, and A. S. Willsky. </author> <title> An estimation theoretic prespective on image processing and the calculation of optical flow. </title> <editor> In I. Sezan and R. L. Lagendijk, editors, </editor> <title> Motion Analysis and Image Sequence Processing, chapter 2. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, USA, </address> <month> January </month> <year> 1993. </year> <month> 24 </month>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation <ref> [9, 31, 32, 33, 34, 35, 36] </ref>, the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities [37, 38, 39, 40, 41, 42, 43].
Reference: [34] <author> E. Dubois and J. Konrad. </author> <title> Estimation of 2-D motion fields from image sequences with application to motion compensated processing. </title> <editor> In I. Sezan and R. L. Lagendijk, editors, </editor> <title> Motion Analysis and Image Sequence Processing, chapter 3. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, USA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation <ref> [9, 31, 32, 33, 34, 35, 36] </ref>, the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities [37, 38, 39, 40, 41, 42, 43].
Reference: [35] <author> M. Pertrou, M. Bober, and J. Kittler. </author> <title> Multiresolution motion segmentation. </title> <publisher> In IEEE Com puter Society Press, </publisher> <editor> editor, </editor> <booktitle> Proceedings of the 12th international conference on pattern recognition, </booktitle> <volume> volume 1, </volume> <pages> pages 743-746, </pages> <address> Jerusalem, Israel, </address> <month> October 9-13 </month> <year> 1994. </year>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation <ref> [9, 31, 32, 33, 34, 35, 36] </ref>, the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities [37, 38, 39, 40, 41, 42, 43].
Reference: [36] <author> Y. Weiss. </author> <title> A unified mixture framework for motion segmentation: incorporating spatial coherence and estimating the number of models. </title> <booktitle> In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 321-326, </pages> <year> 1996. </year>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation <ref> [9, 31, 32, 33, 34, 35, 36] </ref>, the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities [37, 38, 39, 40, 41, 42, 43]. <p> Note that the threshold on the percentage of the data needed to be considered as dominant (e.g., at least 50% for robust regression techniques) imposes further restrictions. A class of methods based on mixture models has recently been investigated (see for instance <ref> [42, 43, 36, 44] </ref>). <p> Moreover, the number of submodels in the mixture (or at least an upper bound on it) needs to be guessed in advance [42, 43]. Indeed, the automatic way of estimating the number of submodels mentioned in <ref> [36, 44] </ref> refers to the fact that, if the algorithm is initialized with a large enough number of submodels, then redundant submodels will have the same parameter values. Clearly, selecting a reasonable upper bound is a delicate problem. Larger bounds are safer but they increase the computational requirements further. <p> By avoiding the necessary initial guesses or trial and error procedures, this can reduce very significantly the severe computational load which is the main drawback of this class of methods. Finally, note that spatial constraints based on static intensity cues similar to the ones discussed in <ref> [36] </ref>, can be easily incorporated into the MIN PCS-based framework. If there is strong evidence that nearby flow vectors must be assigned to the same submodel, we can just consider the corresponding blocks of 4 inequalities (7) as larger blocks.
Reference: [37] <author> J. B. Burt, R. Hingorani, and R. J. Kolczynski. </author> <title> Mechanism for isolating component patterns in the sequential analysis of multiple motion. </title> <booktitle> In Proc. of the IEEE Workshop on visual motion, </booktitle> <pages> pages 187-193, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation [9, 31, 32, 33, 34, 35, 36], the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities <ref> [37, 38, 39, 40, 41, 42, 43] </ref>.
Reference: [38] <author> J. Bergen, P. Anadan, K. Hanna, and R. Hingorani. </author> <title> Hierarchical model based motion estimation. </title> <editor> In G. Sandini editor, editor, </editor> <booktitle> Proceeding of the second European conference on computer vision, </booktitle> <pages> pages 237-252, </pages> <address> S. Margherita Ligure, Italy, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation [9, 31, 32, 33, 34, 35, 36], the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities <ref> [37, 38, 39, 40, 41, 42, 43] </ref>.
Reference: [39] <author> N. Irani, B. Rousso, and S. Peleg. </author> <title> detecting and tracking multiple moving objects using temporal integration. </title> <editor> In G. Sandini editor, editor, </editor> <booktitle> Proceeding of the second European conference on computer vision, </booktitle> <pages> pages 282-287, </pages> <address> S. Margherita Ligure, Italy, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation [9, 31, 32, 33, 34, 35, 36], the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities <ref> [37, 38, 39, 40, 41, 42, 43] </ref>.
Reference: [40] <author> J. Wang and E. Adelson. </author> <title> Layered representation for motion analysis. </title> <booktitle> In Proc. of the IEEE Int. conference on computer vision and pattern recognition, </booktitle> <pages> pages 361-366, </pages> <address> New York, USA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation [9, 31, 32, 33, 34, 35, 36], the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities <ref> [37, 38, 39, 40, 41, 42, 43] </ref>. <p> However, frameworks in which outliers are detected and rejected do not allow simultaneous estimation of different motion components. Several algorithms that identify multiple motions from pre-computed optical flow fields rely on clustering techniques <ref> [32, 40] </ref>. As previously mentioned, these algorithms have two main drawbacks. First, the number of moving components has to be estimated. Second, working only with optical flow vector components and neglecting the dependence of those amplitudes on the reference coordinates, may not suffice to recover the actual motion components. <p> First, the number of moving components has to be estimated. Second, working only with optical flow vector components and neglecting the dependence of those amplitudes on the reference coordinates, may not suffice to recover the actual motion components. Not surprisingly, clustering-based approaches such as 14 in <ref> [32, 40] </ref> present a high degree of arbitrariness. Model-based methods face the same type of fundamental difficulty [40, 43]. The major issue is to partition the images into layers corresponding to different coherently moving components. In some approaches, one tries to identify and extract dominant components iteratively. <p> Not surprisingly, clustering-based approaches such as 14 in [32, 40] present a high degree of arbitrariness. Model-based methods face the same type of fundamental difficulty <ref> [40, 43] </ref>. The major issue is to partition the images into layers corresponding to different coherently moving components. In some approaches, one tries to identify and extract dominant components iteratively. <p> All experiments we have carried out indicate that our approach yields reliable high level segmentations at a very low computational cost. More accurate segmentations can then be obtained by using model 18 based approaches and a layered motion representation scheme such as in <ref> [40, 41] </ref>. Indeed, the spatial segmentation and the corresponding motion parameter values obtained with our method provide a very good starting point for a layered representation.
Reference: [41] <author> S. Hsu, P. Anadan, and S. Peleg. </author> <title> Accurate computation of optical flow by using layered motion representation. </title> <publisher> In IEEE Computer Society Press, </publisher> <editor> editor, </editor> <booktitle> Proceedings of the 12th international conference on pattern recognition, </booktitle> <volume> volume 1, </volume> <pages> pages 743-746, </pages> <address> Jerusalem, Israel, </address> <month> October 9-13 </month> <year> 1994. </year>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation [9, 31, 32, 33, 34, 35, 36], the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities <ref> [37, 38, 39, 40, 41, 42, 43] </ref>. <p> All experiments we have carried out indicate that our approach yields reliable high level segmentations at a very low computational cost. More accurate segmentations can then be obtained by using model 18 based approaches and a layered motion representation scheme such as in <ref> [40, 41] </ref>. Indeed, the spatial segmentation and the corresponding motion parameter values obtained with our method provide a very good starting point for a layered representation.
Reference: [42] <author> Y. Weiss and E. H. Adelson. </author> <title> Motion estimation and segmentation using a recurrent mixture of experts architecture. </title> <editor> In F. Girosi et al., editor, </editor> <title> Neural Network for Signal Processing V, </title> <booktitle> Proceedings of the 1995 IEEE Workshop, Cambridge,Massachussetts, </booktitle> <pages> pages 293-302, </pages> <address> New York, NY, </address> <month> August 31, September 2 </month> <year> 1995. </year> <booktitle> IEEE Signal Processing Society. </booktitle>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation [9, 31, 32, 33, 34, 35, 36], the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities <ref> [37, 38, 39, 40, 41, 42, 43] </ref>. <p> Note that the threshold on the percentage of the data needed to be considered as dominant (e.g., at least 50% for robust regression techniques) imposes further restrictions. A class of methods based on mixture models has recently been investigated (see for instance <ref> [42, 43, 36, 44] </ref>). <p> Moreover, the number of submodels in the mixture (or at least an upper bound on it) needs to be guessed in advance <ref> [42, 43] </ref>. Indeed, the automatic way of estimating the number of submodels mentioned in [36, 44] refers to the fact that, if the algorithm is initialized with a large enough number of submodels, then redundant submodels will have the same parameter values.
Reference: [43] <author> S. Ayer. </author> <title> Sequential and competitive methods for the estimation of multiple motion. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Swiss Federal Institute of Technology, </institution> <type> Thesis No. 1405, </type> <year> 1996. </year> <month> 25 </month>
Reference-contexts: While the former methods involve first the computation of the optical flow and then its interpretation [9, 31, 32, 33, 34, 35, 36], the latter ones, known as parametric methods, try to extract the motion parameters directly from the image intensities <ref> [37, 38, 39, 40, 41, 42, 43] </ref>. <p> Not surprisingly, clustering-based approaches such as 14 in [32, 40] present a high degree of arbitrariness. Model-based methods face the same type of fundamental difficulty <ref> [40, 43] </ref>. The major issue is to partition the images into layers corresponding to different coherently moving components. In some approaches, one tries to identify and extract dominant components iteratively. <p> Note that the threshold on the percentage of the data needed to be considered as dominant (e.g., at least 50% for robust regression techniques) imposes further restrictions. A class of methods based on mixture models has recently been investigated (see for instance <ref> [42, 43, 36, 44] </ref>). <p> Moreover, the number of submodels in the mixture (or at least an upper bound on it) needs to be guessed in advance <ref> [42, 43] </ref>. Indeed, the automatic way of estimating the number of submodels mentioned in [36, 44] refers to the fact that, if the algorithm is initialized with a large enough number of submodels, then redundant submodels will have the same parameter values.
Reference: [44] <author> Y. Weiss. </author> <title> Smoothness in layers: motion segmentation using nonparametric mixture estimation. </title> <booktitle> In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 520-527, </pages> <year> 1997. </year>
Reference-contexts: Note that the threshold on the percentage of the data needed to be considered as dominant (e.g., at least 50% for robust regression techniques) imposes further restrictions. A class of methods based on mixture models has recently been investigated (see for instance <ref> [42, 43, 36, 44] </ref>). <p> Moreover, the number of submodels in the mixture (or at least an upper bound on it) needs to be guessed in advance [42, 43]. Indeed, the automatic way of estimating the number of submodels mentioned in <ref> [36, 44] </ref> refers to the fact that, if the algorithm is initialized with a large enough number of submodels, then redundant submodels will have the same parameter values. Clearly, selecting a reasonable upper bound is a delicate problem. Larger bounds are safer but they increase the computational requirements further.
Reference: [45] <author> P. Bouthemy and E. Francois. </author> <title> Motion segmentation and qualitative dynamic scene analysis from an image sequence. </title> <journal> International journal of computer vision, </journal> <volume> 10(2) </volume> <pages> 157-182, </pages> <year> 1993. </year>
Reference-contexts: Not surprisingly, the number of distinct submodels in the mixture depends on . Markov random fields <ref> [2, 45] </ref>, which take into account discontinuities, are also confronted with the problem of estimating the number of submodels. Moreover, extensions of MRFs to 2-D piecewise smooth models implicitly require that regions, assigned to the same submodel, be contiguous since they only integrate local information [45]. <p> Markov random fields [2, 45], which take into account discontinuities, are also confronted with the problem of estimating the number of submodels. Moreover, extensions of MRFs to 2-D piecewise smooth models implicitly require that regions, assigned to the same submodel, be contiguous since they only integrate local information <ref> [45] </ref>.
Reference: [46] <author> M. Mattavelli and E. Amaldi. </author> <title> Using perceptron-like algorithms for the analysis and parame terization of object motion. </title> <editor> In F. Girosi et al., editor, </editor> <title> Neural Network for Signal Processing V, </title> <booktitle> Proceedings of the 1995 IEEE Workshop, </booktitle> <pages> pages 303-312, </pages> <address> Cambridge,Massachussetts, Aug. 31, </address> <month> Sept. 2 </month> <year> 1995. </year> <booktitle> IEEE Signal Processing Society. </booktitle>
Reference-contexts: Let us consider an image sequence that evolves in time with the apparent motion given 4 A preliminary version of the formulation was presented in <ref> [46] </ref>. 15 by equation: I (x; y; t) = I (x V x (x; y); y V y (x; y); t 1); (5) where x and y are the image coordinates, I () is the image intensity and V x (x; y) and V y (x; y) denote the image displacements

References-found: 46

