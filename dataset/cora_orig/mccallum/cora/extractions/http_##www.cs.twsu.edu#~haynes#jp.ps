URL: http://www.cs.twsu.edu/~haynes/jp.ps
Refering-URL: http://adept.cs.twsu.edu/~thomas/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: [haynes,sandip,dschoen,rogerw]@euler.mcs.utulsa.edu  
Title: Evolving Multiagent Coordination Strategies with Genetic Programming  
Author: Thomas Haynes, Sandip Sen, Dale Schoenefeld Roger Wainwright 
Keyword: Multiagent Coordination, Behavioral Strategies, Evolution of Behavior, Genetic Programming  
Address: Tulsa  
Affiliation: Department of Mathematical Computer Sciences, The University of  
Abstract: The design and development of behavioral strategies to coordinate the actions of multiple agents is a central issue in multiagent systems research. We propose a novel approach of evolving, rather than handcrafting, behavioral strategies. The evolution scheme used is a variant of the Genetic Programming (GP) paradigm. As a proof of principle, we evolve behavioral strategies in the predator-prey domain that has been studied widely in the Distributed Artificial Intelligence community. We use the GP to evolve behavioral strategies for individual agents, as prior literature claims that communication between predators is not necessary for successfully capturing the prey. The evolved strategy, when used by each predator, performs better than all but one of the handcrafted strategies mentioned in literature. We analyze the shortcomings of each of these strategies. The next set of experiments involve co-evolving predators and prey. To our surprise, a simple prey strategy evolves that consistently evades all of the predator strategies. We analyze the implications of the relative successes of evolution in the two sets of experiments and comment on the nature of domains for which GP based evolution is a viable mechanism for generating coordination strategies. We conclude with our design for concurrent evolution of multiple agent strategies in domains where agents need to communicate with each other to successfully solve a common problem.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Peter J. Angeline. </author> <title> Genetic programming and emergent intelligence. </title> <editor> In Kenneth E. Kinnear, Jr., editor, </editor> <booktitle> Advances in Genetic Programming, </booktitle> <pages> pages 75-97. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: A drawback to the GP paradigm is that without better identification and encapsulation of reusable components, strategies which are complex and encapsulate a large amount of knowledge are unlikely to be evolved. The GP community is actively researching this issue <ref> [1, 16, 19] </ref>. Acknowledgments We must thank Larry Stephens for providing us with the thirty test cases he and Matthias Merx used in their work [29, 30]. It provided us with an opportunity to compare our work with that of previous researchers.
Reference: [2] <author> Peter J. Angeline and Jordan B. Pollack. </author> <title> Competitive environments evolve better solutions for complex tasks. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 264-278. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: We are examining the rise of cooperation strategies without implicit communication [9]. This is achieved by having each predator agent being controlled by its own program. Such a system solves a cooperative co-evolution problem as opposed to a competitive co-evolution problem as described in <ref> [2, 8, 25] </ref>. We believe that cooperative co-evolution provides opportunities to produce solutions to problems that can not be solved with implicit communication. 7 Conclusions Strongly typed genetic programming is able to generate effective individual behavioral strategies for predator agents trying to capture a prey agent.
Reference: [3] <author> M. Benda, V. Jagannathan, and R. Dodhiawalla. </author> <title> On optimal cooperation of knowledge sources. </title> <type> Technical Report BCS-G2010-28, </type> <institution> Boeing AI Center, Boeing Computer Services, Bellevue, WA, </institution> <month> August </month> <year> 1985. </year>
Reference-contexts: Populations of such structures are evolved to produce increasingly efficient coordination strategies. We have used both single and multiagent domains domains to evaluate the evolution of behavioral strategies by STGP [11, 12]. Of particular relevance to this special issue is the predator-prey pursuit game <ref> [3] </ref>. We have used this domain to test our hypothesis that useful coordination strategies can be evolved using the STGP paradigm for non-trivial problems. This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. <p> Section 7 presents our conclusions about the utility of GP for evolving behavioral strategies. 2 The Pursuit Problem The original version of the predator-prey pursuit problem was introduced by Benda, et al. <ref> [3] </ref> and consisted of four blue (predator) agents trying to capture a red (prey) agent by surrounding it from four directions on a grid-world. Agent movements were limited to either a horizontal or a vertical step per time unit. The movement of the prey agent was random.
Reference: [4] <editor> Alan H. Bond and Les Gasser. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: In effect, we want to evolve behavioral strategies that guide the actions of agents in a given domain. The identification, design, and implementation of strategies for coordination is a central research issue in the field of Distributed Artificial Intelligence (DAI) <ref> [4] </ref>. Current research techniques in developing coordination strategies are mostly off-line mechanisms that use extensive domain knowledge to design from scratch the most appropriate cooperation strategy. It is nearly impossible to identify or even prove the existence of the best coordination strategy.
Reference: [5] <editor> Lawrence Davis, editor. </editor> <booktitle> Handbook of genetic algorithms. </booktitle> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: GAs are not guaranteed to find optimal solutions (unlike Simulated Annealing algorithms), they still possess some nice provable properties (optimal allocation of trials to substrings, evaluating exponential number of schemas with linear number of string evaluations, etc.), and have been found to be useful in a number of practical applications <ref> [5] </ref>. Koza's work on Genetic Programming [18] was motivated by the representational constraint in traditional GAs.
Reference: [6] <author> Edmund H. Durfee, Victor R. Lesser, and Daniel D. Corkill. </author> <title> Trends in cooperative distributed problem solving. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(1) </volume> <pages> 63-83, </pages> <month> March </month> <year> 1989. </year> <month> 15 </month>
Reference: [7] <author> Les Gasser, Nicolas Rouquette, Randall W. Hill, and John Lieb. </author> <title> Representing and using organiza-tional knowledge in DAI systems. </title> <editor> In Les Gasser and Michael N. Huhns, editors, </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence, </booktitle> <pages> pages 55-78. </pages> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [7, 17, 20, 29, 30] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We find that STGP evolved coordination strategies perform competitively with the best available manually generated strategies in this domain. <p> The goal of this problem was to show the effectiveness of nine organizational structures, with varying degrees of agent cooperation and control, on the efficiency with which the predator agents could capture the prey. The approach undertaken by Gasser et al. <ref> [7] </ref> allowed for the predators to occupy and maintain a Lieb configuration (each predator occupying a different quadrant, where a quadrant is defined by diagonals intersecting at the location of the prey) while homing in on the prey.
Reference: [8] <author> Thomas Haynes and Sandip Sen. </author> <title> Evolving behavioral strategies in predators and prey. </title> <booktitle> In IJCAI-95 Workshop on Adaptation and Learning in Multiagent Systems, </booktitle> <year> 1995. </year> <note> (in press). </note>
Reference-contexts: The initial results from the prey learning experiment also prompted us to conduct an experiment in which predators were trained against a prey which moves in a straight line. The best evolved strategy is referred to as Linear STGP. In <ref> [8] </ref> we found it to be evident that the Linear STGP strategy is not very general, and only has significant performance when pitted against a Linear prey. <p> We are examining the rise of cooperation strategies without implicit communication [9]. This is achieved by having each predator agent being controlled by its own program. Such a system solves a cooperative co-evolution problem as opposed to a competitive co-evolution problem as described in <ref> [2, 8, 25] </ref>. We believe that cooperative co-evolution provides opportunities to produce solutions to problems that can not be solved with implicit communication. 7 Conclusions Strongly typed genetic programming is able to generate effective individual behavioral strategies for predator agents trying to capture a prey agent.
Reference: [9] <author> Thomas Haynes, Sandip Sen, Dale Schoenefeld, and Roger Wainwright. </author> <title> Evolving a team. </title> <booktitle> In AAAI Fall Symposium on Genetic Programming, </booktitle> <year> 1995. </year> <note> (in press). </note>
Reference-contexts: The developed strategies in this research had implicit communication in that the same program was used to control the four predator agents. We are examining the rise of cooperation strategies without implicit communication <ref> [9] </ref>. This is achieved by having each predator agent being controlled by its own program. Such a system solves a cooperative co-evolution problem as opposed to a competitive co-evolution problem as described in [2, 8, 25].
Reference: [10] <author> Thomas Haynes, Roger Wainwright, and Sandip Sen. </author> <title> Evolving cooperation strategies. </title> <type> Technical Report UTULSA-MCS-94-10, </type> <institution> The University of Tulsa, </institution> <month> December 16, </month> <year> 1994. </year>
Reference-contexts: To compare the algorithms we utilized 30 test cases from Stephens [29], averaged over 26 different initial random seeds. 1 In our previous work <ref> [10, 11] </ref> we had allowed the simulation to run for 200 time steps. <p> We found the evolved strategies still ignored other predator locations. In order to test the hypothesis, that the prey was learning to escape the predators, we ran experiments where the prey was pitted against our version of Manhattan distance (MD) algorithm <ref> [10] </ref>. The prey was very successful in evading the predators. This was particularly surprising because the algorithm developed by the prey was simple: pick a random direction and move in a straight line in that direction. <p> Why is the Linear prey so effective compared to the other deterministic prey algorithms? 2. Is the encoding of domain responsible for our failure to capture the prey? The Linear prey is so effective because it avoids locality of movement. The greedy strategies <ref> [10, 17] </ref> fare well against the preys staying in a small neighborhood. Locality allows the predators both to surround the prey and to escape any deadlock situations, such as that in Figure 7 (a). The deadlock is that predator 2 and predator 3 are both vying for the same cell.
Reference: [11] <author> Thomas Haynes, Roger Wainwright, Sandip Sen, and Dale Schoenefeld. </author> <title> Strongly typed genetic programming in evolving cooperation strategies. </title> <booktitle> In Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <year> 1995. </year> <note> (in press). </note>
Reference-contexts: It is nearly impossible to identify or even prove the existence of the best coordination strategy. In most cases a coordination strategy is chosen if it is reasonably good. In <ref> [11] </ref>, we presented a new approach for developing coordination strategies for multiagent problem solving situations, which is different from most of the existing techniques for constructing coordination strategies in two ways: * Strategies for coordination are incrementally constructed by repeatedly solving problems in the domain, i.e., on-line. 1 * We rely <p> We believe that evolution can provide a workable alternative in generating coordination strategies in domains where the handcrafting of such strategies is either prohibitively time consuming or difficult. The approach proposed in <ref> [11] </ref> for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed genetic programming (STGP) paradigm [23], which is an extension of genetic programming (GP) [18]. <p> We can then measure their efficiency and effectiveness by some criteria relevant to the domain. Populations of such structures are evolved to produce increasingly efficient coordination strategies. We have used both single and multiagent domains domains to evaluate the evolution of behavioral strategies by STGP <ref> [11, 12] </ref>. Of particular relevance to this special issue is the predator-prey pursuit game [3]. We have used this domain to test our hypothesis that useful coordination strategies can be evolved using the STGP paradigm for non-trivial problems. <p> Furthermore, the predators do not possess any explicit communication skills; two predators cannot communicate to resolve conflicts or negotiate a capture strategy. We decided to utilize STGP rather than GP because STGP reduces the solution space to be searched <ref> [11, 23] </ref>. The GP system, GPengine, used in this research is an extension of that used in [13] and is written in C. Furthermore, it allows for strong typing, as described by [23]. <p> To compare the algorithms we utilized 30 test cases from Stephens [29], averaged over 26 different initial random seeds. 1 In our previous work <ref> [10, 11] </ref> we had allowed the simulation to run for 200 time steps.
Reference: [12] <author> Thomas D. Haynes. </author> <title> A simulation of adaptive agents in a hostile environment. </title> <type> Master's thesis, </type> <institution> University of Tulsa, Tulsa, OK., </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: We can then measure their efficiency and effectiveness by some criteria relevant to the domain. Populations of such structures are evolved to produce increasingly efficient coordination strategies. We have used both single and multiagent domains domains to evaluate the evolution of behavioral strategies by STGP <ref> [11, 12] </ref>. Of particular relevance to this special issue is the predator-prey pursuit game [3]. We have used this domain to test our hypothesis that useful coordination strategies can be evolved using the STGP paradigm for non-trivial problems.
Reference: [13] <author> Thomas D. Haynes and Roger L. Wainwright. </author> <title> A simulation of adaptive agents in a hostile environment. </title> <booktitle> In Proceedings of the 1995 ACM Symposium on Applied Computing, </booktitle> <pages> pages 318-323. </pages> <publisher> ACM Press, </publisher> <year> 1995. </year>
Reference-contexts: We decided to utilize STGP rather than GP because STGP reduces the solution space to be searched [11, 23]. The GP system, GPengine, used in this research is an extension of that used in <ref> [13] </ref> and is written in C. Furthermore, it allows for strong typing, as described by [23].
Reference: [14] <author> John H. Holland. </author> <booktitle> Adpatation in natural and artificial systems. </booktitle> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1975. </year>
Reference-contexts: invalid in our domain where the grid world is toroidal. 3 Evolving Coordination Strategies In the following subsections we briefly introduce the genetic programming paradigm, along with its strongly typed variant, and explain how we have used it to evolve coordination strategies. 3.1 Genetic Programming Holland's work on adaptive systems <ref> [14] </ref> produced a class of biologically inspired algorithms known as genetic algorithms (GAs) that can manipulate and develop solutions to optimization, learning, and other types of problems.
Reference: [15] <editor> Kenneth E. Kinnear, Jr., editor. </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Although GPs do not possess the nice theoretical properties of traditional GAs, they have attracted a tremendous number of researchers because of the wide range of applicability of this paradigm, and the easily interpretable form of the solutions that are produced by these algorithms <ref> [15, 18, 19] </ref>. A GP algorithm can be described as follows: 1. Randomly generate a population of N programs made up of functions and terminals in the problem. 3 2.
Reference: [16] <author> Kenneth E. Kinnear, Jr. </author> <title> Alternatives in automatic function definition: A comparision of performance. </title> <editor> In Kenneth E. Kinnear, Jr., editor, </editor> <booktitle> Advances in Genetic Programming, </booktitle> <pages> pages 119-141. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: A drawback to the GP paradigm is that without better identification and encapsulation of reusable components, strategies which are complex and encapsulate a large amount of knowledge are unlikely to be evolved. The GP community is actively researching this issue <ref> [1, 16, 19] </ref>. Acknowledgments We must thank Larry Stephens for providing us with the thirty test cases he and Matthias Merx used in their work [29, 30]. It provided us with an opportunity to compare our work with that of previous researchers.
Reference: [17] <author> Richard E. Korf. </author> <title> A simple solution to pursuit games. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 183-194, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [7, 17, 20, 29, 30] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We find that STGP evolved coordination strategies perform competitively with the best available manually generated strategies in this domain. <p> In their research, the predator and prey agents took turns in making their moves. We believe this is not very realistic. A more realistic scenario is for all agents to choose their actions concurrently. This will introduce significant uncertainty and complexity into the problem. Korf <ref> [17] </ref> claims in his research that a discretization of the continuous world that allows only horizontal and vertical movements is a poor approximation. He calls this the orthogonal game. Korf developed several greedy solutions to problems where eight predators are allowed to move orthogonally as well as diagonally. <p> The fitness measure becomes an average of the training cases. These training cases can be either the same throughout all generations or randomly generated for each generation. In our experiments, we used random training cases per generation. 4 Evolution of Individual Greedy Strategies Korf's <ref> [17] </ref> basic claim is that predators need not jointly decide on a strategy, but can choose locally optimal moves and still be able to capture the prey consistently. We used this philosophy as the basis for our research in this domain. <p> Furthermore, it allows for strong typing, as described by [23]. A graphical reporting system was created for X-Windows using the Tcl and Tk toolkit [24], with the Blt extension; this system is a modification of that by Martin [22]. 4.1 Deterministic Predator Algorithms Korf <ref> [17] </ref> used two greedy heuristics in his work: Manhattan distance and max norm. The Manhattan distance algorithm determines the best move to make based on the sum of the differences of the x and y coordinates of a predator and the prey. <p> In Korf's <ref> [17] </ref> formulation of the randomly moving prey, the prey's possible moves are limited to those surrounding cells which are unoccupied by the predators. Since we chose to let all agents, prey and predators, move simultaneously, the randomly moving prey is allowed to consider all directions. <p> Why is the Linear prey so effective compared to the other deterministic prey algorithms? 2. Is the encoding of domain responsible for our failure to capture the prey? The Linear prey is so effective because it avoids locality of movement. The greedy strategies <ref> [10, 17] </ref> fare well against the preys staying in a small neighborhood. Locality allows the predators both to surround the prey and to escape any deadlock situations, such as that in Figure 7 (a). The deadlock is that predator 2 and predator 3 are both vying for the same cell.
Reference: [18] <author> John R. Koza. </author> <title> Genetic Programming, On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: The approach proposed in [11] for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed genetic programming (STGP) paradigm [23], which is an extension of genetic programming (GP) <ref> [18] </ref>. To use the STGP approach for evolving coordination strategies, the strategies are encoded as symbolic expressions (S-expressions) and an evaluation criterion is chosen for evaluating arbitrary S-expressions. <p> Koza's work on Genetic Programming <ref> [18] </ref> was motivated by the representational constraint in traditional GAs. <p> Although GPs do not possess the nice theoretical properties of traditional GAs, they have attracted a tremendous number of researchers because of the wide range of applicability of this paradigm, and the easily interpretable form of the solutions that are produced by these algorithms <ref> [15, 18, 19] </ref>. A GP algorithm can be described as follows: 1. Randomly generate a population of N programs made up of functions and terminals in the problem. 3 2.
Reference: [19] <author> John R. Koza. </author> <title> Genetic Programming II, Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Although GPs do not possess the nice theoretical properties of traditional GAs, they have attracted a tremendous number of researchers because of the wide range of applicability of this paradigm, and the easily interpretable form of the solutions that are produced by these algorithms <ref> [15, 18, 19] </ref>. A GP algorithm can be described as follows: 1. Randomly generate a population of N programs made up of functions and terminals in the problem. 3 2. <p> A drawback to the GP paradigm is that without better identification and encapsulation of reusable components, strategies which are complex and encapsulate a large amount of knowledge are unlikely to be evolved. The GP community is actively researching this issue <ref> [1, 16, 19] </ref>. Acknowledgments We must thank Larry Stephens for providing us with the thirty test cases he and Matthias Merx used in their work [29, 30]. It provided us with an opportunity to compare our work with that of previous researchers.
Reference: [20] <author> Ran Levy and Jeffrey S. Rosenschein. </author> <title> A game theoretic approach to the pursuit problem. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 195-213, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [7, 17, 20, 29, 30] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We find that STGP evolved coordination strategies perform competitively with the best available manually generated strategies in this domain.
Reference: [21] <author> Mauro Manela and J. A. Campbell. </author> <title> Designing good pursuit problems as testbeds for Distributed AI: a novel application of Genetic Algorithms. </title> <booktitle> In Fifth European Workshop on Modelling Autonomous Agents in a Multi-Agent World, </booktitle> <address> Neuch^atel, Switzerland, </address> <month> August 24-27 </month> <year> 1993. </year>
Reference-contexts: Any ties are broken randomly. He claims this addition to the prey movements makes the problem considerably more difficult. Manela and Campbell investigated the utility of N fi M (predators fi prey) pursuit games as a testbed for DAI research. <ref> [21] </ref> They utilized Genetic Algorithms to evolve parameters for decision modules. A difference between their domain and the others is that the grid is bounded, and not toroidal. They found that the 4 fi 1 game was not interesting for DAI research.
Reference: [22] <author> Martin C. Martin. graphs.blt. </author> <note> GP FTP Archives, </note> <year> 1994. </year>
Reference-contexts: Furthermore, it allows for strong typing, as described by [23]. A graphical reporting system was created for X-Windows using the Tcl and Tk toolkit [24], with the Blt extension; this system is a modification of that by Martin <ref> [22] </ref>. 4.1 Deterministic Predator Algorithms Korf [17] used two greedy heuristics in his work: Manhattan distance and max norm. The Manhattan distance algorithm determines the best move to make based on the sum of the differences of the x and y coordinates of a predator and the prey.
Reference: [23] <author> David J. Montana. </author> <title> Strongly typed genetic programming. </title> <type> Technical Report 7866, </type> <institution> Bolt Beranek and Newman, Inc., </institution> <month> March 25, </month> <year> 1994. </year>
Reference-contexts: The approach proposed in [11] for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed genetic programming (STGP) paradigm <ref> [23] </ref>, which is an extension of genetic programming (GP) [18]. To use the STGP approach for evolving coordination strategies, the strategies are encoded as symbolic expressions (S-expressions) and an evaluation criterion is chosen for evaluating arbitrary S-expressions. <p> The set of all terminals is called the terminal set, and the set of all functions is called the function set. In traditional GP, all of the terminal and function set members must be of the same type. Montana <ref> [23] </ref> introduced STGP, in which the variables, constants, arguments, and returned values can be of any type. <p> Furthermore, the predators do not possess any explicit communication skills; two predators cannot communicate to resolve conflicts or negotiate a capture strategy. We decided to utilize STGP rather than GP because STGP reduces the solution space to be searched <ref> [11, 23] </ref>. The GP system, GPengine, used in this research is an extension of that used in [13] and is written in C. Furthermore, it allows for strong typing, as described by [23]. <p> We decided to utilize STGP rather than GP because STGP reduces the solution space to be searched [11, 23]. The GP system, GPengine, used in this research is an extension of that used in [13] and is written in C. Furthermore, it allows for strong typing, as described by <ref> [23] </ref>. A graphical reporting system was created for X-Windows using the Tcl and Tk toolkit [24], with the Blt extension; this system is a modification of that by Martin [22]. 4.1 Deterministic Predator Algorithms Korf [17] used two greedy heuristics in his work: Manhattan distance and max norm.
Reference: [24] <author> John K. Ousterhout. </author> <title> Tcl and the Tk Toolkit. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1994. </year>
Reference-contexts: The GP system, GPengine, used in this research is an extension of that used in [13] and is written in C. Furthermore, it allows for strong typing, as described by [23]. A graphical reporting system was created for X-Windows using the Tcl and Tk toolkit <ref> [24] </ref>, with the Blt extension; this system is a modification of that by Martin [22]. 4.1 Deterministic Predator Algorithms Korf [17] used two greedy heuristics in his work: Manhattan distance and max norm.
Reference: [25] <author> Craig W. Reynolds. </author> <title> Competition, coevolution and the game of tag. </title> <booktitle> In Artificial Life IV. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: We expect that the populations will see-saw between being better on the average. This has been shown in Reynold's work on co-evolution in the game of tag <ref> [25] </ref>. In his work, the two opposing agents, from the same population, take turns being 10 the predator and the prey. <p> We are examining the rise of cooperation strategies without implicit communication [9]. This is achieved by having each predator agent being controlled by its own program. Such a system solves a cooperative co-evolution problem as opposed to a competitive co-evolution problem as described in <ref> [2, 8, 25] </ref>. We believe that cooperative co-evolution provides opportunities to produce solutions to problems that can not be solved with implicit communication. 7 Conclusions Strongly typed genetic programming is able to generate effective individual behavioral strategies for predator agents trying to capture a prey agent.
Reference: [26] <author> L. Sachs. </author> <title> Applied Statistics: A Handbook of Techniques. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year> <month> 16 </month>
Reference-contexts: We 1 This apparently uncommon number was used for averaging so that we can use a formula for the Wilcoxon matched pair signed-rank test <ref> [26] </ref>, which is used to test the significance of the results. 7 IFTE ( &lt;( IFTE ( T, MD ( CellOf ( Prey, H ), CellOf ( Bi,E )), MD ( CellOf ( Prey, N), CellOf ( Bi, H ))), MD ( CellOf ( Prey, N ), CellOf ( Bi, W
Reference: [27] <author> Munindar P. Singh. </author> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Working Papers of the 10th International Workshop on Distributed Artificial Intelligence, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: This study, as well as the study by Singh <ref> [27] </ref> on using group intentions for agent coordination, lacks any experimental results that allow comparison with other work on this problem. Stephens and Merx [29, 30] performed a series of experiments to demonstrate the relative effectiveness of three different control strategies.
Reference: [28] <author> Reid G. Smith. </author> <title> The contract net protocol: High-level communication and control in a distributed problem solver. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(12):1104-1113, </volume> <month> December </month> <year> 1980. </year>
Reference-contexts: In general, we believe that the predator agents must employ either some explicit communication or further domain knowledge to capture the Linear, 1Ply, and Still prey algorithms. A contract net <ref> [28] </ref> based predator algorithm, where agents get assigned different sides of the prey to occupy, should fare well against these prey algorithms.
Reference: [29] <author> Larry M. Stephens and Matthias B. Merx. </author> <title> Agent organization as an effector of DAI system performance. </title> <booktitle> In Working Papers of the 9th International Workshop on Distributed Artificial Intelligence, </booktitle> <month> September </month> <year> 1989. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [7, 17, 20, 29, 30] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We find that STGP evolved coordination strategies perform competitively with the best available manually generated strategies in this domain. <p> This study, as well as the study by Singh [27] on using group intentions for agent coordination, lacks any experimental results that allow comparison with other work on this problem. Stephens and Merx <ref> [29, 30] </ref> performed a series of experiments to demonstrate the relative effectiveness of three different control strategies. They defined the local control strategy where a predator broadcasts its position to other predators when it occupies a neighboring location to the prey. <p> To compare the algorithms we utilized 30 test cases from Stephens <ref> [29] </ref>, averaged over 26 different initial random seeds. 1 In our previous work [10, 11] we had allowed the simulation to run for 200 time steps. <p> The GP community is actively researching this issue [1, 16, 19]. Acknowledgments We must thank Larry Stephens for providing us with the thirty test cases he and Matthias Merx used in their work <ref> [29, 30] </ref>. It provided us with an opportunity to compare our work with that of previous researchers. This research was partially supported by OCAST Grant AR2-004, NSF Research Initiative Award IRI-9410180 and Sun Microsystems, Inc.
Reference: [30] <author> Larry M. Stephens and Matthias B. Merx. </author> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Proceedings of the 1990 Distributed AI Workshop, </booktitle> <month> October </month> <year> 1990. </year> <month> 17 </month>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [7, 17, 20, 29, 30] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We find that STGP evolved coordination strategies perform competitively with the best available manually generated strategies in this domain. <p> This study, as well as the study by Singh [27] on using group intentions for agent coordination, lacks any experimental results that allow comparison with other work on this problem. Stephens and Merx <ref> [29, 30] </ref> performed a series of experiments to demonstrate the relative effectiveness of three different control strategies. They defined the local control strategy where a predator broadcasts its position to other predators when it occupies a neighboring location to the prey. <p> The GP community is actively researching this issue [1, 16, 19]. Acknowledgments We must thank Larry Stephens for providing us with the thirty test cases he and Matthias Merx used in their work <ref> [29, 30] </ref>. It provided us with an opportunity to compare our work with that of previous researchers. This research was partially supported by OCAST Grant AR2-004, NSF Research Initiative Award IRI-9410180 and Sun Microsystems, Inc.
References-found: 30

