URL: http://www-csag.cs.uiuc.edu/papers/iccd97.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Email: fzhang,dasdan,achieng@cs.uiuc.edu  schulzm@informatik.tu-muenchen.de  rgupta@ics.uci.edu  
Title: Architectural Adaptation for Application-Specific Locality Optimizations  
Author: Xingbin Zhang Ali Dasdan Martin Schulz Rajesh K. Gupta Andrew A. Chien 
Date: (1997).  
Note: In Proceedings of the IEEE International Conference on Computer Design  
Address: Irvine  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  Institut fur Informatik Technische Universitat Munchen  Information and Computer Science, University of California at  
Abstract: We propose a machine architecture that integrates programmable logic into key components of the system with the goal of customizing architectural mechanisms and policies to match an application. This approach presents an improvement over traditional approach of exploiting programmable logic as a separate co-processor by preserving machine usability through software and over traditional computer architecture by providing application-specific hardware assists. We present two case studies of architectural customization to enhance latency tolerance and efficiently utilize network bisection on multiprocessors for sparse matrix computations. We demonstrate that application-specific hardware assists and policies can provide substantial improvements in performance on a per application basis. Based on these preliminary results, we propose that an application-driven machine customization provides a promising approach to achieve high performance and combat performance fragility. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Seminconductor technology workshop conclusions. Roadmap, Semiconductor Industry Association, </institution> <year> 1993. </year>
Reference-contexts: 1 Introduction Technology projections for the coming decade <ref> [1] </ref> point out that system performance is going to be increasingly dominated by intra-chip interconnect delay. This presents a unique opportunity for programmable logic as the interconnect dominance reduces the contribution of per stage logic complexity on performance and the marginal costs of adding switching logic in the interconnect. <p> In particular, the difference in scaling of switching logic speed and interconnect delays points out increasing opportunities for programmable logic circuits in the coming decade. Projections by the Semiconductor Industry Association (SIA) <ref> [1] </ref> show that on-chip system performance is going to be increasingly dominated by interconnect delays. Due to these interconnect delays, the on-chip clock periods will be limited to about 1 nanosecond, which is well above the projections based on channel length scaling [1]. <p> Projections by the Semiconductor Industry Association (SIA) <ref> [1] </ref> show that on-chip system performance is going to be increasingly dominated by interconnect delays. Due to these interconnect delays, the on-chip clock periods will be limited to about 1 nanosecond, which is well above the projections based on channel length scaling [1]. Meanwhile, the unit gate delay (inverter with fanout of two) scales down to 20 pico-seconds. Thus, modern day control logic consisting of 7-8 logic stages per cycle would form less than 20% of the total cycle time.
Reference: [2] <author> BORKAR, S., COHN, R., COX, G., GLEASON, S., GROSS, T., KUNG, H. T., LAM, M., MOORE, B., PETERSON, C., PIEPER, J., RANKIN, L., TSENG, P. S., SUTTON, J., URBANSKI, J., AND WEBB, J. </author> <title> iWarp: An integrated solution to high-speed parallel computing. </title> <booktitle> In Proc. IEEE Supercomputing '88 (1988), </booktitle> <pages> pp. 330341. </pages> <address> Orlando, Florida. </address>
Reference-contexts: The application developer with the help of compilation tools selects appropriate hardware assists to customize the machine to match the application without having to repartition the system functionality or rewrite the application. There are other approaches for locality optimizations. Researchers have proposed processor-in-memory (PIM) <ref> [2, 3, 7, 8, 21] </ref> as a solution for solving the latency and bandwidth limitations of the memory hierarchy.
Reference: [3] <author> BORKAR, S., COHN, R., COX, G., GROSS, T., KUNG, H. T., LAM, M., LEVINE, M., MOORE, B., MOORE, W., PETERSON, C., SUS-MAN, J., SUTTON, J., URBANSKI, J., AND WEBB, J. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proc. 17th IEEE Int. Symp. on Comp. Arch. </booktitle> <year> (1990), </year> <pages> pp. 7081. </pages>
Reference-contexts: The application developer with the help of compilation tools selects appropriate hardware assists to customize the machine to match the application without having to repartition the system functionality or rewrite the application. There are other approaches for locality optimizations. Researchers have proposed processor-in-memory (PIM) <ref> [2, 3, 7, 8, 21] </ref> as a solution for solving the latency and bandwidth limitations of the memory hierarchy.
Reference: [4] <author> CHIEN, A., DASDAN, A., GUPTA, R., AND ZHANG, B. </author> <title> Rapid Architectural Design and Validation Using Program-Driven Simulations. </title> <booktitle> In Symp. on High-level Design, Validation and Test (HLDVT) (1996). </booktitle>
Reference-contexts: The details of our simulation environment are presented in <ref> [4] </ref>. Table 1 shows the simulation parameters used. We report our empirical results for current day computer technologies and then use derivative met-rics (such as miss rate) to extrapolate potential benefits for future computer systems which will exhibit much higher clock rates and memory sizes.
Reference: [5] <author> CHIEN, A. A., AND GUPTA, R. K. </author> <title> MORPH: A system architecture for robust high performance using customization. </title> <booktitle> In Frontiers of Massive-Parallelism (1996). Annapolis, </booktitle> <address> Maryland. </address>
Reference-contexts: Because the interconnect delays scale down much more slowly than transistor switching delays, in the year 2007, the delay of the average length inter-connect (assuming an average interconnect length of 1000X the pitch) would correspond to approximately three gate delays (see <ref> [5] </ref> for a detailed analysis). This is in contrast to less than half the gate delay of the average interconnect in current process technology. This implies that due to purely electrical reasons, it would be preferred to include at least one inter-connect buffer in a cycle time. <p> We believe that the advances in design technology will address the increase of logic complexity. 3.1 Project Context Our study is in the context of the MORPH <ref> [5] </ref> project, a NSF point design study for Petaflops architectures in the year 2007 technology window. The key elements of the MORPH (MultiprocessOr with Reconfigurable Parallel Hardware) architecture consists of processing and memory elements embedded in a scalable interconnect. <p> Since no single machine organization fits all applications, the delivered performance is often only a small fraction of the peak ma schemes. (Total size of non-zeros, 1.35 MB) chine performance (frequently less than a tenth <ref> [5] </ref>). Therefore, we believe that there are significant opportunities for application-specific architectural adaptation. In this paper, we have demonstrated mechanisms for latency hiding and required bandwidth reduction that leverage small hardware support as well as do not change the programming model.
Reference: [6] <author> CHOI, L., AND CHIEN, A. A. </author> <title> The design and performance evaluation of the DI-multicomputer. </title> <journal> J. Parallel and Distributed Comput, </journal> <volume> 36:119143, </volume> <year> 1996. </year>
Reference-contexts: We base our design on the premise that communication is already critical and getting increasingly so [17], and flexible interconnects can be used to replace static wires at competitive performance <ref> [6, 9, 20] </ref>. Our approach presents an improvement over co-processing by preserving machine usability through software and over traditional computer architecture by providing application-specific hardware assists.
Reference: [7] <author> DALLY, W., CHIEN, A., FISKE, J., FYLER, G., HORWAT, W., KEEN, J., LETHIN, R., NOAKES, M., NUTH, P., AND WILLS, D. </author> <title> The message driven processor: an integrated multicomputer processing element. </title> <booktitle> In Proc. IEEE Int. Conf. on Comp. Design: VLSI in Computers and Processors (1992), </booktitle> <pages> pp. 41619. </pages>
Reference-contexts: The application developer with the help of compilation tools selects appropriate hardware assists to customize the machine to match the application without having to repartition the system functionality or rewrite the application. There are other approaches for locality optimizations. Researchers have proposed processor-in-memory (PIM) <ref> [2, 3, 7, 8, 21] </ref> as a solution for solving the latency and bandwidth limitations of the memory hierarchy.
Reference: [8] <author> DALLY, W. J., FISKE, J. A. S., KEEN, J. S., LETHIN, R. A., NOAKES, M. D., NUTH, P. R., DAVISON, R. E., AND FYLER, G. A. </author> <title> The message-driven processor. </title> <booktitle> IEEE Micro (April 1992), </booktitle> <pages> pp. 2339. </pages>
Reference-contexts: The application developer with the help of compilation tools selects appropriate hardware assists to customize the machine to match the application without having to repartition the system functionality or rewrite the application. There are other approaches for locality optimizations. Researchers have proposed processor-in-memory (PIM) <ref> [2, 3, 7, 8, 21] </ref> as a solution for solving the latency and bandwidth limitations of the memory hierarchy.
Reference: [9] <author> DALLY, W. J., AND SONG, P. </author> <title> Design of a self-timed VLSI mul-ticomputer communication controller. </title> <booktitle> In Proc. IEEE Int. Conf. on Comp. Design (1987), </booktitle> <pages> pp. 2304. </pages>
Reference-contexts: We base our design on the premise that communication is already critical and getting increasingly so [17], and flexible interconnects can be used to replace static wires at competitive performance <ref> [6, 9, 20] </ref>. Our approach presents an improvement over co-processing by preserving machine usability through software and over traditional computer architecture by providing application-specific hardware assists.
Reference: [10] <author> ERNST, R., HENKEL, J., AND BENNER, T. </author> <title> Hardware-Software Cosynthesis for Microcontrollers. IEEE Design & Test of Computers (Dec. </title> <booktitle> 1993), </booktitle> <pages> pp. 6475. </pages>
Reference-contexts: The hardware assists are built using programmable circuit blocks for easy interpretation with the predesigned CPU. Figure 1 shows the schematic of a co-processing architecture, where the co-processing hardware may be operated under direct control of the processor which stalls while the dedicated hardware is operational <ref> [10] </ref>, or the co-processing may be done concurrently with software [13]. However, a system generated using this approach typically can not be retargeted to another application without repartitioning hardware and software functionality and reimplementing the co-processing hardware even if the macro-level organization of the system components remains unaffected.
Reference: [11] <author> GAJSKI, D., DUTT, N., WU, C. H., AND LIN, Y. L. </author> <title> High-level Synthesis: Introduction to chip and system design. </title> <publisher> Kluwer, </publisher> <year> 1992. </year>
Reference-contexts: In view of these technology trends and advances in circuit modeling using hardware description languages (HDLs) such as Verilog and VHDL, the process of hardware design is increasingly a language-level activity, supported by compilation and synthesis tools <ref> [11, 12] </ref>.
Reference: [12] <author> GUPTA, R. K. </author> <title> Co-Synthesis of Hardware and Software for Digital Embedded Systems. </title> <publisher> Kluwer, </publisher> <year> 1995. </year>
Reference-contexts: In view of these technology trends and advances in circuit modeling using hardware description languages (HDLs) such as Verilog and VHDL, the process of hardware design is increasingly a language-level activity, supported by compilation and synthesis tools <ref> [11, 12] </ref>.
Reference: [13] <author> GUPTA, R. K., AND MICHELI, G. D. </author> <title> Hardware-Software Cosyn-thesis for Digital Systems. </title> <booktitle> IEEE Design & Test of Computers (Sept. </booktitle> <year> 1993), </year> <pages> pp. 2941. </pages>
Reference-contexts: Figure 1 shows the schematic of a co-processing architecture, where the co-processing hardware may be operated under direct control of the processor which stalls while the dedicated hardware is operational [10], or the co-processing may be done concurrently with software <ref> [13] </ref>. However, a system generated using this approach typically can not be retargeted to another application without repartitioning hardware and software functionality and reimplementing the co-processing hardware even if the macro-level organization of the system components remains unaffected.
Reference: [14] <author> HENNESSY, J. L., AND JOUPPI, N. P. </author> <title> Computer technology and architecture: An evolving interaction. </title> <booktitle> IEEE Computer (Sep 1991), </booktitle> <pages> pp. 1829. </pages>
Reference-contexts: Thus, modern day control logic consisting of 7-8 logic stages per cycle would form less than 20% of the total cycle time. This clearly challenges the fundamental design trade-off today that tries to simplify the amount of logic per stage in the interest of reducing the cycle time <ref> [14] </ref>. In addition, this points to a sharply reduced marginal cost of per stage logic complexity on the circuit-level performance. The decreasing delay penalty for (re)programmable logic blocks compared to interconnect delays also makes the incorporation of small programmable logic blocks attractive even in custom data paths.
Reference: [15] <author> HOROWITZ, M., MARTONOSI, M., MOWRY, T. C., AND SMITH, M. D. </author> <title> Informing memory operations: Providing memory performance feedback in modern processors. </title> <booktitle> In Proc. 23rd IEEE Int. Symp. on Comp. Arch. </booktitle> <year> (1996). </year>
Reference-contexts: In addition, by adding a small amount of programmable logic to the memory units, we can yield some benefits of having computational elements within the memory. Researchers have also proposed various specific architectural mechanisms for locality optimizations, for instance, group prefetching [23] 6 and informing memory operations <ref> [15] </ref>, with mechanism--specific implementations.
Reference: [16] <author> JOUPPI, N. P. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proc. the Int. Symp. on Comp. Arch. </booktitle> <year> (1990), </year> <pages> pp. 36473. </pages>
Reference-contexts: Among other examples that applications can benefit from are mechanisms for recognition of working set size for a given application that can be used to alter cache update policies or even use a synthesized small victim cache <ref> [16] </ref>, and mechanisms for monitoring access patterns and conflicts in the caches or memory banks and reconfiguring the assists according to these patterns and conflicts.
Reference: [17] <author> KOGGE, P. M. </author> <title> Summary of the architecture group findings. </title> <booktitle> In PetaFlops Architecture Workshop (PAWS) (Apr. </booktitle> <year> 1996). </year>
Reference-contexts: We propose a machine architecture that integrates programmable logic into key components of the system with the goal of customizing architectural mechanisms and policies to match an application. We base our design on the premise that communication is already critical and getting increasingly so <ref> [17] </ref>, and flexible interconnects can be used to replace static wires at competitive performance [6, 9, 20]. Our approach presents an improvement over co-processing by preserving machine usability through software and over traditional computer architecture by providing application-specific hardware assists.
Reference: [18] <author> KU, D., AND MICHELI, G. D. </author> <title> HardwareC A Language for Hardware Design (version 2.0). </title> <type> CSL Tech. Rep. </type> <institution> CSL-TR-90-419, Stan-ford Univ., </institution> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: We also manually translated the C routines modeling the customizable logic blocks into HardwareC <ref> [18] </ref> to evaluate their hardware cost in terms of space and cycle delays. (However, our recent work is focused on automatic translation of these routines to synthe-sizable blocks [19].) L1 Cache L2 Cache Line Size 32B or 64B 32B or 64B Associativity 1 2 Cache Size 32KB 512KB Write Write back
Reference: [19] <author> LIAO, S. Y., TJIANG, S., AND GUPTA, R. K. </author> <title> An Efficient Implementation of Reactivity in Modeling Hardware in the CSYN Synthesis and Simulation Environment. </title> <booktitle> In Proc. Design Automation Conf. </booktitle> <year> (1997). </year>
Reference-contexts: We also manually translated the C routines modeling the customizable logic blocks into HardwareC [18] to evaluate their hardware cost in terms of space and cycle delays. (However, our recent work is focused on automatic translation of these routines to synthe-sizable blocks <ref> [19] </ref>.) L1 Cache L2 Cache Line Size 32B or 64B 32B or 64B Associativity 1 2 Cache Size 32KB 512KB Write Write back + Write back + Policy Write allocate Write allocate Replacement Policy Random Random Transfer (L1-L2) (L2-Mem) Rate 16B/5 cycles 8B/15 cycles Table 1.
Reference: [20] <author> SEITZ, C. L. </author> <title> Let's route packets instead of wires. </title> <booktitle> In Proc. 6th MIT Conf. on Advanced Research in VLSI (1990), </booktitle> <editor> W. J. Dally, Ed., </editor> <publisher> MIT Press, </publisher> <pages> pp. 13337. </pages>
Reference-contexts: We base our design on the premise that communication is already critical and getting increasingly so [17], and flexible interconnects can be used to replace static wires at competitive performance <ref> [6, 9, 20] </ref>. Our approach presents an improvement over co-processing by preserving machine usability through software and over traditional computer architecture by providing application-specific hardware assists.
Reference: [21] <author> SPERTUS, E., GOLDSTEIN, S. C., SCHAUSER, K. E., VON EICKEN, T., CULLER, D. E., AND DALLY, W. J. </author> <title> Evaluation of mechanisms for fine-grained parallel programs in the J-Machine and the CM-5. </title> <booktitle> In Proc. 20th IEEE Int. Symp. on Comp. Arch. </booktitle> <year> (1993), </year> <pages> pp. 302313. </pages>
Reference-contexts: The application developer with the help of compilation tools selects appropriate hardware assists to customize the machine to match the application without having to repartition the system functionality or rewrite the application. There are other approaches for locality optimizations. Researchers have proposed processor-in-memory (PIM) <ref> [2, 3, 7, 8, 21] </ref> as a solution for solving the latency and bandwidth limitations of the memory hierarchy.
Reference: [22] <author> VEENSTRA, J. E., AND FOWLER, R. J. MINT: </author> <title> A front end for efficient simulation of shared-memoy multiprocessors. </title> <booktitle> In Proc. 2nd Int. Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of Comp. and Telecommunication Syst. </institution> <month> (MASCOTS) (Jan. </month> <year> 1994), </year> <pages> pp. </pages> <year> 201207. </year>
Reference-contexts: Space for elements, which is 40 bytes per matrix element, are allocated dynamically in blocks of elements for efficiency. There are also several one dimensional arrays for storing the root pointers for row and column lists. 3 We perform cycle-based system-level simulation using a program-driven simulator based on MINT <ref> [22] </ref> that interprets program binaries and models configurable logic blocks behaviorly. The details of our simulation environment are presented in [4]. Table 1 shows the simulation parameters used.
Reference: [23] <author> ZHANG, Z., AND TORRELLAS, J. </author> <title> Speeding up irregular applications in shared-memorymultiprocessors: Memory binding and group prefetching. </title> <booktitle> In Proc. 22th IEEE Int. Symp. on Comp. Arch. </booktitle> <year> (1995). </year> <month> 7 </month>
Reference-contexts: if (startBlock &lt;= vAddr && vAddr &lt; endBlock) - /* Get row pointer from returned cache line */ ptrLoc = data [24]; /* row ptr offset = 24 */ &lt;Initiate transfer of elt at ptrLoc to L1 cache&gt; -- Our prefetching examples are similar to the prefetching schemes proposed in <ref> [23] </ref>, where they are shown to benefit various irregular applications. However, unlike [23], using architectural customization enables more flexible prefetch-ing policies, e.g., multiple level prefetch, according to the application access pattern. 4.3 Architectural Adaptation for Bandwidth Re duction Our second case study uses a sparse matrix-matrix multiply routine as an example <p> pointer from returned cache line */ ptrLoc = data [24]; /* row ptr offset = 24 */ &lt;Initiate transfer of elt at ptrLoc to L1 cache&gt; -- Our prefetching examples are similar to the prefetching schemes proposed in <ref> [23] </ref>, where they are shown to benefit various irregular applications. However, unlike [23], using architectural customization enables more flexible prefetch-ing policies, e.g., multiple level prefetch, according to the application access pattern. 4.3 Architectural Adaptation for Bandwidth Re duction Our second case study uses a sparse matrix-matrix multiply routine as an example to show architectural adaptation to improve data reuse and reduce data traffic <p> The latter part has an application-specific management policy, and can be distinguished by mapping it to a reserved 1 As pointed in <ref> [23] </ref>, the implementation of this prefetching scheme is complicated by the need to translate the virtual pointer address to physical address. We assume that the prefetcher logic can also access the TLB structure. <p> We assume that the prefetcher logic can also access the TLB structure. An alternative implementation is to place the prefetcher logic in memory and forward the data of the next record to the upper memory hierarchy. This requires an additional group translation table <ref> [23] </ref> for address translation. address space. <p> In addition, by adding a small amount of programmable logic to the memory units, we can yield some benefits of having computational elements within the memory. Researchers have also proposed various specific architectural mechanisms for locality optimizations, for instance, group prefetching <ref> [23] </ref> 6 and informing memory operations [15], with mechanism--specific implementations.
References-found: 23

