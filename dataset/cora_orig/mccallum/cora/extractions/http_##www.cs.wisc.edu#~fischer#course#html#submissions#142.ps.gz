URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/142.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/
Root-URL: http://www.cs.wisc.edu
Email: ken@rice.edu sethi@rice.edu  
Title: A Constraint-based Code Placement Framework and its Application to Communication Placement  
Author: Ken Kennedy Ajay Sethi 
Address: 6100 S.Main MS 41, Houston, TX 77005  
Affiliation: Department of Computer Science, Rice University  
Note: Center for Research on Parallel Computation  Extended Abstract  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Section 1 enumerates the significant differences between our framework and the previous techniques. The problem of data-flow based communication generation and placement has been addressed by several researchers. Amarasinghe and Lam use last write tree to optimize communication for single loop nests <ref> [1] </ref>. Moreover, they don't allow loops within conditionals. Granston and Veidenbaum combine PRE and dependence analysis to eliminate redundant monolithic global-memory accesses across loop nests in the presence of conditionals [4].
Reference: [2] <author> J. Cocke and R. Miller. </author> <title> Some analysis techniques for optimizing computer programs. </title> <booktitle> In Proceedings of the 2nd Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 143-146, </pages> <year> 1969. </year>
Reference-contexts: Note that a node nested in multiple loops is a member of the Tarjan interval of the header of each enclosing loop. * G is reducible; that is, each loop has a unique header node. The classical node splitting transforma tion <ref> [2] </ref> can be used to obtain a reducible graph. * There are no critical edges, which connect a node with multiple successors to nodes with multiple predecessors [10].
Reference: [3] <author> C. Gong, R. Gupta, and R. Melhem. </author> <title> Compilation techniques for optimizing communication on distributed-memory systems. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: in Section 9 by summarizing our framework and providing directions for future research. 2 Communication Placement: Motivating Example The similarities between the classical expression availability and availability of non-local data due to communication can be exploited to use traditional code motion and placement techniques for communication placement in distributed-memory compilers <ref> [4, 3, 5] </ref>. Since interprocessor communication is typically orders of magnitude more expensive than accessing local data, it is important to hide the latency of non-local accesses by separating Begin and End primitives (Send and Recv in this context). <p> They eliminate reads of not-owned variables, in parallelized and vectorized codes, if these variables have already been read or written locally. Gong, Gupta, and Melhem present a data-flow framework to separate sends and receives by placing sends at "the earliest point at which the communication can be performed" <ref> [3] </ref>. However, their technique does not eliminate partially redundant communication, handles only singly-nested loops and one-dimensional arrays, and does not provide balanced communication placement. 12 The unified communication optimization framework developed by Gupta, Schonberg, and Srinivasan adapts PRE for communication placement [5].
Reference: [4] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: in Section 9 by summarizing our framework and providing directions for future research. 2 Communication Placement: Motivating Example The similarities between the classical expression availability and availability of non-local data due to communication can be exploited to use traditional code motion and placement techniques for communication placement in distributed-memory compilers <ref> [4, 3, 5] </ref>. Since interprocessor communication is typically orders of magnitude more expensive than accessing local data, it is important to hide the latency of non-local accesses by separating Begin and End primitives (Send and Recv in this context). <p> Amarasinghe and Lam use last write tree to optimize communication for single loop nests [1]. Moreover, they don't allow loops within conditionals. Granston and Veidenbaum combine PRE and dependence analysis to eliminate redundant monolithic global-memory accesses across loop nests in the presence of conditionals <ref> [4] </ref>. They eliminate reads of not-owned variables, in parallelized and vectorized codes, if these variables have already been read or written locally. Gong, Gupta, and Melhem present a data-flow framework to separate sends and receives by placing sends at "the earliest point at which the communication can be performed" [3].
Reference: [5] <author> M. Gupta, E. Schonberg, and H. Srinivasan. </author> <title> A unified data-flow framework for optimizing communication. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: in Section 9 by summarizing our framework and providing directions for future research. 2 Communication Placement: Motivating Example The similarities between the classical expression availability and availability of non-local data due to communication can be exploited to use traditional code motion and placement techniques for communication placement in distributed-memory compilers <ref> [4, 3, 5] </ref>. Since interprocessor communication is typically orders of magnitude more expensive than accessing local data, it is important to hide the latency of non-local accesses by separating Begin and End primitives (Send and Recv in this context). <p> However, their technique does not eliminate partially redundant communication, handles only singly-nested loops and one-dimensional arrays, and does not provide balanced communication placement. 12 The unified communication optimization framework developed by Gupta, Schonberg, and Srinivasan adapts PRE for communication placement <ref> [5] </ref>. It initiates non-blocking receives immediately after the sends and uses a wait primitive before each reference to block the statement from executing till the data is received.
Reference: [6] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Give-N-Take | A balanced code placement framework. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Unlike the above two techniques, the Give-N-Take framework developed by Kennedy and von Hanxleden allows splitting computations and provides non-atomic placement region (instead of a single location) for code placement candidates <ref> [6] </ref>. It uses complex interval analysis to determine the earliest placement of Begin and the latest placement of End primitives. Moreover, it ensures that the two are balanced; that is, every program execution path contains matching Begin and End pairs. <p> if the loop body is not executed, it introduces a new value on an execution path of the program. (In the context of communication placement, the traditional safety criteria requires that everything communicated is used.) However, hoisting communication out of a loop (that is, message vectorization) can only cause overcommunication <ref> [6] </ref>. Due to the importance of message vectorization in distributed-memory compilation [8] we relax the safety criteria to hoist communication out of potentially zero-trip loops. <p> A jump edge corresponds to a jump out of a loop while all the remaining edges are classified as forward edges (see <ref> [6] </ref> for details). * For every non-empty interval T (h), there exists an unique g 2 T (h) such that (g; h) 2 E; that is, there is only one back edge out of T (h). This can be achieved by adding a post body node to T (h) [6]. 3.2 <p> (see <ref> [6] </ref> for details). * For every non-empty interval T (h), there exists an unique g 2 T (h) such that (g; h) 2 E; that is, there is only one back edge out of T (h). This can be achieved by adding a post body node to T (h) [6]. 3.2 Definitions Let Comm be the set of data communicated among the processors. <p> Succs F (n) and Succs E (n) then correspond to the forward and entry edge successors of n. Additionally, the edges induce the following traversal orders over G <ref> [6] </ref>: given a forward edge (m; n), a Forward order visits m before n, and a Backward order visits m after n. <p> Each of the three steps require a simple uni-directional analysis, where each equation inspects only a subset of incoming/outgoing edges. Therefore, the complexity of our algorithm is O (E). Under the assumption that O (E) is the same as the order of the program size <ref> [6] </ref>, our algorithm has linear time complexity. A separate phase for constraint analysis facilitates taking different constraints into account by initializing the SAFE variables appropriately. Furthermore, the equations for safety, earliest, latest and other placement criteria can be modified depending on the communication primitives and machine characteristics.
Reference: [7] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: : : ) enddo enddo Recv uu (1:n,block size-1:block size,1:n) 3fl Recv uu (1:n,block start:block start+1,1:n) 4fl do k = 1, n do i = 1, n uu (i,j+1,k), uu (i,j+2,k)) enddo enddo enddo end (b) The constraint-independent communication placement. and the constraint-independent communication placement corresponding to the owner-computes rule <ref> [7] </ref>. For the sake of clarity, we have neither reduced/localized loop bounds nor inserted guards around the loops and communication primitives.
Reference: [8] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Due to the importance of message vectorization in distributed-memory compilation <ref> [8] </ref> we relax the safety criteria to hoist communication out of potentially zero-trip loops.
Reference: [9] <author> K. Kennedy and N. Nedeljkovic. </author> <title> Combining dependence and data-flow analyses to optimize communication. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: It was shown by Kennedy and Nedeljkovic that for Livermore 18 explicit hydrodynamics kernel and Shallow weather prediction program, the total execution time can be reduced by up to 20% by various communication optimizations, including latency hiding, that were enabled by data-flow based communication placement <ref> [9] </ref>. We describe our experience with two benchmark programs: Erlebacher, described in Section 2, and Disper. Disper is a highly data-parallel, 1000 line stencil computation from the UTCOMP reservoir simulator [11]. Fortran D compiler was used to translate the programs into SPMD node codes.
Reference: [10] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Optimal code motion: </title> <journal> Theory and practice. ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(4) </volume> <pages> 1117-1155, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The Lazy Code Motion (LCM) technique proposed by Knoop, Ruthing, and Steffen shows how to decompose the bi-directional structure of the PRE algorithm into a sequence of uni-directional analyses <ref> [10] </ref>. In addition, the "lazyness" of their algorithm allows them to avoid unnecessary code motion inherent in the PRE algorithm, where unnecessary code motion is defined as the code motion that does not shorten any program execution path. <p> The classical node splitting transforma tion [2] can be used to obtain a reducible graph. * There are no critical edges, which connect a node with multiple successors to nodes with multiple predecessors <ref> [10] </ref>. Critical edges are eliminated by splitting edges as follows: every edge leading to a node with more than one predecessor is split by inserting a synthetic node. * An entry (back) edge corresponds to the edge from (to) an interval header to (from) a node within the interval. <p> We now present the equations that provide the safe placement for Send (d), for d 2 Comm. The solution of the following equation, which requires a Backward and Upward traversal of G, gives the set of safe nodes for Send placement <ref> [10] </ref>: SAFE (n; d) = SAFE (n; d) " [Used (n; d) [ Transp (n; d) " " s2Succs F (n) SAFE (s; d)] where Used , Transp, and Succs F are as defined in Section 3.2. <p> If we initialize EARLIEST (n; d) = &gt;, for all n 2 N , d 2 Comm, the following equations give the set of nodes that satisfy the earliest property <ref> [10] </ref>: EARLIEST (n; d) = 8 : p2Preds F (n) [EARLIEST (p; d) " :Transp (p; d) [ :Safe (p; d)] otherwise For loop header node h, communication set d, and n = Succs E (h), n is earliest if Send (d) is not hoisted out of the loop: EARLIEST (n;
Reference: [11] <author> U. Kremer and Marcelo Rame. </author> <title> Compositional oil reservoir simulation in Fortran D: A feasibility study on Intel iPSC/860. </title> <type> Technical Report TR93-209, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: We describe our experience with two benchmark programs: Erlebacher, described in Section 2, and Disper. Disper is a highly data-parallel, 1000 line stencil computation from the UTCOMP reservoir simulator <ref> [11] </ref>. Fortran D compiler was used to translate the programs into SPMD node codes. The node codes were hand-instrumented to correspond to two versions of communication placement: with and without latency 11 # milliseconds % Program Size Proc.
Reference: [12] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: 1 Introduction Classical code motion and placement techniques improve the efficiency of programs by moving computations to less frequently executed locations and by eliminating redundant recomputation of values. The Partial Redundancy Elimination (PRE) algorithm proposed by Morel and Renvoise combines loop invariant code motion and common subexpression elimination <ref> [12] </ref>. By moving computation of expressions to earlier locations, it eliminates partially redundant computations computations that are redundant along some, but not all, paths reaching a particular node.
Reference: [13] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 62-73, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: In case the loop bounds are unknown, the loop bounds can be assumed to be either really small (and all the messages fits in the buffer) or really large (such that no message fits in the buffer) <ref> [13] </ref>. 9 6.1.2 Send Placement If Loop (n) &gt; total buffer for header node n then (a) not all messages can be hoisted out of the loop and (b) no message can be moved across node n.
Reference: [14] <author> R. E. Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year> <month> 14 </month>
Reference-contexts: Let s and e be the unique start and end nodes of G. Each edge in E can be classified as an entry, back, forward, or jump edge. The important properties of G are as follows: * G is based on Tarjan intervals <ref> [14] </ref>, where a Tarjan interval T (h) is a set of control-flow nodes that correspond to a loop in the program text, entered through a unique header node h, where h 62 T (h).
References-found: 14

