URL: http://www.neci.nj.nec.com/homepages/giles/papers/IEEE.TNN.long.term.dependencies.NARX.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Phone: 2  3  4  
Title: Learning long-term dependencies in NARX recurrent neural networks  
Author: Tsungnan Lin ; Bill G. Horne Peter Tino ; and C. Lee Giles ; 
Keyword: Recurrent neural networks, tapped-delay lines, long-term dependencies, architecture, automata, memory, temporal sequences, gradient descent training, latch ing, NARX networks, auto-regressive, IIR filters.  
Note: Published in IEEE Transactions on Neural Networks, vol. 7, no. 6, p. 1329, 1996. Copyright IEEE.  
Address: 4 Independence Way, Princeton, NJ 08540  Princeton, NJ 08540  Ilkovicova 3, 812 19 Bratislava, Slovakia  College Park, MD 20742  
Affiliation: 1 NEC Research Institute,  Department of Electrical Engineering, Princeton University,  Dept. of Computer Science and Engineering, Slovak Technical University,  UMIACS, University of Maryland,  
Abstract: It has recently been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show tht the long-term dependencies problem is lessened for a class of architectures called NARX recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have "hidden states" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumption regarding what it means to latch infor mation robustly and suggest possible ways to loosen these assumptions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.D. Back and A.C. Tsoi. </author> <title> FIR and IIR synapses, a new neural network architecture for time series modeling. </title> <journal> Neural Computation, </journal> <volume> 3(3) </volume> <pages> 375-385, </pages> <year> 1991. </year>
Reference-contexts: We also presented two experimental problems which show that NARX networks can 17 outperform networks with single delays on some simple problems involving long-term dependencies. We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback <ref> [1, 18] </ref> would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in [6, 9, 24, 37]. Acknowledgments We would like to acknowledge Andrew Back, Yoshua Bengio and Juergen Schmidhuber for useful discussions and suggestions.
Reference: [2] <author> Y. Bengio, P. Simard, and P. Frasconi. </author> <title> Learning long-term dependencies with gradient is difficult. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 157-166, </pages> <year> 1994. </year>
Reference-contexts: This was noted by Mozer who reported that RNNs were able to learn short term musical structure using gradient based methods [21], but had difficulty capturing global behavior. These ideas were recently formalized by Bengio et al. <ref> [2] </ref>, who showed that if a system is to latch information robustly, then the fraction of the gradient due to information n time steps in the past approaches zero as n becomes large. Several approaches have been suggested to circumvent the problem of vanishing gradients. <p> Several approaches have been suggested to circumvent the problem of vanishing gradients. For example, gradient-based methods can be abandoned completely in favor of alternative optimization methods <ref> [2, 25] </ref>. However, the algorithms investigated so far either perform just as poorly on problems involving long-term dependencies, or, when they are better, require far more computational resources [2]. <p> For example, gradient-based methods can be abandoned completely in favor of alternative optimization methods [2, 25]. However, the algorithms investigated so far either perform just as poorly on problems involving long-term dependencies, or, when they are better, require far more computational resources <ref> [2] </ref>. Another possibility is to modify conventional gradient-descent by more heavily weighing the fraction of the gradient due to information far in the past, but there is no guarantee that such a modified algorithm would converge to a minimum of the error surface being searched [2]. <p> require far more computational resources <ref> [2] </ref>. Another possibility is to modify conventional gradient-descent by more heavily weighing the fraction of the gradient due to information far in the past, but there is no guarantee that such a modified algorithm would converge to a minimum of the error surface being searched [2]. As an alternative to using different learning algorithms, one suggestion has been to alter the input data so that it represents a reduced description that makes global features more explicit and more readily detectable [21, 29, 28]. <p> Typically, these networks converge much faster and generalize better than other networks. The results in this paper show the reason why gradient-descent learning is better in NARX networks. 2 Vanishing gradients and long-term dependencies Bengio et al. <ref> [2] </ref> have analytically explained why learning problems with long-term dependencies is difficult. They argue that for many practical applications the goal of the network must be to robustly latch information, i.e. the network must be able to store information for a long period of time in the presence of noise. <p> In the Appendix, we discuss this definition of robustness in more detail and describe how some of the assumptions associated with it might be loosened. In this section we briefly describe some of the key aspects of the results in <ref> [2] </ref>. A recurrent neural 3 network can be described in the form x (t + 1) = f (x (t); u (t); w) (1) where x, u, y and w are column vectors representing the states, inputs, outputs and weights of the network respectively. <p> Bengio et al. <ref> [2] </ref> showed that if the network satisfies their definition of robustly latching information, i.e. if the Jacobian at each time step has all of its eigenvalues inside the unit circle, then J x (T; n) is an exponentially decreasing function of n, so that lim n!1 J x (T; n) = <p> It is possible to derive analytical results for some simple toy problems to show that NARX networks are indeed less sensitive to long-term dependencies. Here we give one such example, which is based upon the latching problem described in <ref> [2] </ref>. 8 Consider the simple one node autonomous recurrent network described by, x (t) = tanh (wx (t 1)) (14) where w = 1:25, which has two stable fixed points at 0:710 and one unstable fixed point at zero. <p> We tried two different problems: the latching problem and a grammatical inference problem . 5.1 The latching problem We explored a slight modification on the latching problem described in <ref> [2] </ref>. This problem is a minimal task designed as a test that must necessarily be passed in order for a network to latch information robustly. Bengio et al. describe the task as one in which the input values are to be learned. <p> We would also like to thank the anonymous reviewers for helpful suggestions that greatly improved this manuscript. Appendix: A closer look at robust information latching In this section we make a critical examination of the definition of robust latching given by Bengio et al. <ref> [2] </ref>. Specifically, they assume that if a network is to be robust to noise, then the states must always be in the reduced attracting set of the hyperbolic attractor. While such a condition is sufficient to latch information robustly, it is not necessary.
Reference: [3] <author> S. Chen, S.A. Billings, and P.M. Grant. </author> <title> Non-linear system identification using neural networks. </title> <journal> International Journal of Control, </journal> <volume> 51(6) </volume> <pages> 1191-1214, </pages> <year> 1990. </year>
Reference-contexts: In this paper, we also propose an architectural approach to deal with long-term dependencies. 2 We focus on a class of architectures based upon Nonlinear AutoRegressive models with eXogenous inputs (NARX models), and are therefore called NARX recurrent neural networks <ref> [3, 22] </ref>. (However, there is no reason that this method cannot be extended to other recurrent architectures.) This is a powerful class of models which has recently been shown to be computationally equivalent to Turing machines [31]. <p> It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers <ref> [3] </ref>, waste water treatment plants [34, 35], catalytic reforming systems in a petroleum refinery [35], nonlinear oscillations associated with multi-legged locomotion in biological systems [36], time series [4], and various artificial nonlinear systems [3, 22, 26]. <p> It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [34, 35], catalytic reforming systems in a petroleum refinery [35], nonlinear oscillations associated with multi-legged locomotion in biological systems [36], time series [4], and various artificial nonlinear systems <ref> [3, 22, 26] </ref>. Furthermore, we have previously reported that gradient-descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification [11, 15]. <p> are not sufficiently powerful to discover a relationship between target outputs and inputs that occur at a much earlier time, which they term the problem of long-term dependencies. 5 3 NARX networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXoge nous inputs (NARX) model 2 <ref> [3, 19, 20, 34, 35] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (9) where u (t) and y (t) represent input and output of the network at <p> When the function f can be approximated by a Multilayer Perceptron, the resulting system is called a NARX recurrent neural network <ref> [3, 22] </ref>. In this paper we shall consider NARX networks with zero input order and a one dimensional output, i.e. those networks which have feedback from the output only. However there is no reason why our results could not be extended to networks with higher input orders.
Reference: [4] <author> J. Connor, L.E. Atlas, and D.R. Martin. </author> <title> Recurrent networks and NARMA modeling. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 301-308, </pages> <year> 1992. </year>
Reference-contexts: It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [34, 35], catalytic reforming systems in a petroleum refinery [35], nonlinear oscillations associated with multi-legged locomotion in biological systems [36], time series <ref> [4] </ref>, and various artificial nonlinear systems [3, 22, 26]. Furthermore, we have previously reported that gradient-descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification [11, 15].
Reference: [5] <author> E. Coven, I. Kan, and J.Yorke. </author> <title> Pseudo-orbit shadowing in the family of tent maps. </title> <journal> Transactions AMS, </journal> <volume> 308 </volume> <pages> 227-241, </pages> <year> 1988. </year>
Reference-contexts: A useful formalization of this idea in dynamical systems' theory is stated in terms of the shadowing lemma <ref> [5, 10] </ref>. Given a number b &gt; 0, a b-pseudo-orbit of the system S A is a sequence f ~ x (t)g such that kM ( ~ x (t)) ~ x (t + 1)k &lt; b, for all t 0.
Reference: [6] <author> B. de Vries and J.C. Principe. </author> <title> The gamma model | A new neural model for temporal processing. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 565-576, </pages> <year> 1992. </year>
Reference-contexts: We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback [1, 18] would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in <ref> [6, 9, 24, 37] </ref>. Acknowledgments We would like to acknowledge Andrew Back, Yoshua Bengio and Juergen Schmidhuber for useful discussions and suggestions. We would also like to thank the anonymous reviewers for helpful suggestions that greatly improved this manuscript.
Reference: [7] <author> J.L. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference-contexts: By increasing the length of the strings to be learned, we will be able to create a problem with long term dependencies, in which the output will depend on input values far in the past. In this experiment we compared Elman's Simple Recurrent Network <ref> [7] </ref> against NARX networks. Each network had six hidden nodes. Since the output if each hidden node in an Elman network is fed back, there were six delay elements (states) in the network. The NARX network had six feedback delays from the output node.
Reference: [8] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda. Unified integration of explicit rules and learning by example in recurrent networks. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 7(2) </volume> <pages> 340-346, </pages> <year> 1995. </year>
Reference-contexts: In our simulation, we fixed the recurrent feedback weight to w = 1:25, which gives the autonomous network two stable fixed points at 0:710 and one unstable fixed point at zero, as described in Section 4. It can be shown <ref> [8] </ref> that the network is robust to perturbations in the range [0:155; 0:155]. Thus, the uniform noise in e (t) was restricted to this range.
Reference: [9] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda. Local feedback multilayered networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 120-130, </pages> <year> 1992. </year>
Reference-contexts: This effect is called the problem of vanishing gradient, or forgetting behavior <ref> [9] </ref>. <p> We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback [1, 18] would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in <ref> [6, 9, 24, 37] </ref>. Acknowledgments We would like to acknowledge Andrew Back, Yoshua Bengio and Juergen Schmidhuber for useful discussions and suggestions. We would also like to thank the anonymous reviewers for helpful suggestions that greatly improved this manuscript.
Reference: [10] <author> M. Garzon and F. Botelho. </author> <title> Observability of neural network behavior. </title> <editor> In J.D. Cowen, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 455-462. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year> <month> 21 </month>
Reference-contexts: A useful formalization of this idea in dynamical systems' theory is stated in terms of the shadowing lemma <ref> [5, 10] </ref>. Given a number b &gt; 0, a b-pseudo-orbit of the system S A is a sequence f ~ x (t)g such that kM ( ~ x (t)) ~ x (t + 1)k &lt; b, for all t 0. <p> It is proved in <ref> [10] </ref> that except possibly for small exceptional sets, discrete-time analog neural networks do have the shadowing property. In particular, they show that that the shadowing property holds for networks with sigmoidal (i.e. strictly increasing, bounded form above and below, and continuously differentiable) activation functions.
Reference: [11] <author> C.L. Giles and B.G. </author> <title> Horne. Representation and learning in recurrent neural network architec-tures. </title> <booktitle> In Proceedings of the Eigth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 128-134, </pages> <year> 1994. </year>
Reference-contexts: Furthermore, we have previously reported that gradient-descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification <ref> [11, 15] </ref>. Typically, these networks converge much faster and generalize better than other networks. <p> This has been observed previously, in the sense that gradient-descent learning appeared to be more effective in NARX networks than in recurrent neural network architectures that have "hidden states" on problems including grammatical inference and nonlinear system identification <ref> [11, 15] </ref>. The intuitive explanation for this behavior is that the output delays are manifested as jump-ahead connections in the unfolded network that is often used to describe algorithms like Backpropagation Through Time.
Reference: [12] <author> M. Gori, M. Maggini, and G. </author> <title> Soda. Scheduling of modular architectures for inductive inference of regular grammars. </title> <booktitle> In ECAI'94 Workshop on Combining Symbolic and Connectionist Processing, Amsterdam, </booktitle> <pages> pages 78-87. </pages> <publisher> Wiley, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: However, this approach may fail if short term dependencies are equally as important. Hochreiter also proposes a specific architectural approach which utilizes high order units. [14]. Finally, it has been suggested that a network architecture that operates on multiple time scales might be useful for tackling this problem <ref> [12, 13] </ref>.
Reference: [13] <author> S. El Hihi and Y. Bengio. </author> <title> Hierarchical recurrent neural networks for long-term dependencies. </title> <booktitle> In Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: However, this approach may fail if short term dependencies are equally as important. Hochreiter also proposes a specific architectural approach which utilizes high order units. [14]. Finally, it has been suggested that a network architecture that operates on multiple time scales might be useful for tackling this problem <ref> [12, 13] </ref>.
Reference: [14] <author> Sepp Hochreiter and Jurgen Schmidhuber. </author> <title> Long short term memory. </title> <institution> Forschungsberichte Kunstliche Intelligenz FKI-207-95, Institut fur Informatik, Technische Universitat Munchen, </institution> <year> 1995. </year>
Reference-contexts: However, this approach may fail if short term dependencies are equally as important. Hochreiter also proposes a specific architectural approach which utilizes high order units. <ref> [14] </ref>. Finally, it has been suggested that a network architecture that operates on multiple time scales might be useful for tackling this problem [12, 13].
Reference: [15] <author> B.G. Horne and C.L. Giles. </author> <title> An experimental comparison of recurrent neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 697-704. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Furthermore, we have previously reported that gradient-descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification <ref> [11, 15] </ref>. Typically, these networks converge much faster and generalize better than other networks. <p> This has been observed previously, in the sense that gradient-descent learning appeared to be more effective in NARX networks than in recurrent neural network architectures that have "hidden states" on problems including grammatical inference and nonlinear system identification <ref> [11, 15] </ref>. The intuitive explanation for this behavior is that the output delays are manifested as jump-ahead connections in the unfolded network that is often used to describe algorithms like Backpropagation Through Time.
Reference: [16] <author> T. Kailath. </author> <title> Linear Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: The weight links in the figure can be adjusted or fixed; it depends on the application. From a system perspective, it is preferrable to put equations into a state space form <ref> [16] </ref>. In this form the Jacobian can be examined and derived [17].
Reference: [17] <author> Hassan K. Khalil. </author> <title> Nonlinear Systems. </title> <publisher> Macmillan Publishing Company, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The weight links in the figure can be adjusted or fixed; it depends on the application. From a system perspective, it is preferrable to put equations into a state space form [16]. In this form the Jacobian can be examined and derived <ref> [17] </ref>.
Reference: [18] <author> R.R. Leighton and B.C. Conrath. </author> <title> The autoregressive backpropagation algorithm. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 369-377, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: We also presented two experimental problems which show that NARX networks can 17 outperform networks with single delays on some simple problems involving long-term dependencies. We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback <ref> [1, 18] </ref> would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in [6, 9, 24, 37]. Acknowledgments We would like to acknowledge Andrew Back, Yoshua Bengio and Juergen Schmidhuber for useful discussions and suggestions.
Reference: [19] <author> I.J. Leontaritis and S.A. Billings. </author> <title> Input-output parametric models for non-linear systems: Part I: deterministic non-linear systems. </title> <journal> International Journal of Control, </journal> <volume> 41(2) </volume> <pages> 303-328, </pages> <year> 1985. </year>
Reference-contexts: are not sufficiently powerful to discover a relationship between target outputs and inputs that occur at a much earlier time, which they term the problem of long-term dependencies. 5 3 NARX networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXoge nous inputs (NARX) model 2 <ref> [3, 19, 20, 34, 35] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (9) where u (t) and y (t) represent input and output of the network at
Reference: [20] <author> L. Ljung. </author> <title> System identification : Theory for the user. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: are not sufficiently powerful to discover a relationship between target outputs and inputs that occur at a much earlier time, which they term the problem of long-term dependencies. 5 3 NARX networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXoge nous inputs (NARX) model 2 <ref> [3, 19, 20, 34, 35] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (9) where u (t) and y (t) represent input and output of the network at
Reference: [21] <author> M. C. Mozer. </author> <title> Induction of multiscale temporal structure. </title> <editor> In J.E. Moody, S. J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Neural Information Processing Systems 4, </booktitle> <pages> pages 275-282. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: This was noted by Mozer who reported that RNNs were able to learn short term musical structure using gradient based methods <ref> [21] </ref>, but had difficulty capturing global behavior. These ideas were recently formalized by Bengio et al. [2], who showed that if a system is to latch information robustly, then the fraction of the gradient due to information n time steps in the past approaches zero as n becomes large. <p> As an alternative to using different learning algorithms, one suggestion has been to alter the input data so that it represents a reduced description that makes global features more explicit and more readily detectable <ref> [21, 29, 28] </ref>. However, this approach may fail if short term dependencies are equally as important. Hochreiter also proposes a specific architectural approach which utilizes high order units. [14].
Reference: [22] <author> K.S. Narendra and K. Parthasarathy. </author> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1 </volume> <pages> 4-27, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In this paper, we also propose an architectural approach to deal with long-term dependencies. 2 We focus on a class of architectures based upon Nonlinear AutoRegressive models with eXogenous inputs (NARX models), and are therefore called NARX recurrent neural networks <ref> [3, 22] </ref>. (However, there is no reason that this method cannot be extended to other recurrent architectures.) This is a powerful class of models which has recently been shown to be computationally equivalent to Turing machines [31]. <p> It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [34, 35], catalytic reforming systems in a petroleum refinery [35], nonlinear oscillations associated with multi-legged locomotion in biological systems [36], time series [4], and various artificial nonlinear systems <ref> [3, 22, 26] </ref>. Furthermore, we have previously reported that gradient-descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification [11, 15]. <p> When the function f can be approximated by a Multilayer Perceptron, the resulting system is called a NARX recurrent neural network <ref> [3, 22] </ref>. In this paper we shall consider NARX networks with zero input order and a one dimensional output, i.e. those networks which have feedback from the output only. However there is no reason why our results could not be extended to networks with higher input orders.
Reference: [23] <author> O. Nerrand, P. Roussel-Ragot, L. Personnaz, G. Dreyfus, and S. Marcos. </author> <title> Neural networks and nonlinear adaptive filtering: Unifying concepts and new algorithms. </title> <journal> Neural Computation, </journal> <volume> 5(2) </volume> <pages> 165-199, </pages> <year> 1993. </year>
Reference-contexts: Almost any recurrent neural network architecture can be expressed in this form <ref> [23] </ref>, where f and g depend on the specific architecture. For example, in simple first-order recurrent neural networks f would be a sigmoid of a weighted sum of the values x (t) and u (t) and g would simply select one of the states as output.
Reference: [24] <author> P. Poddar and K.P. Unnikrishnan. </author> <title> Non-linear prediction of speech signals using memory neuron networks. In B.H. Juang, S.Y. Kung, and C.A. Kamm, editors, Neural Networks for Signal Processing: </title> <booktitle> Proceedings of the 1991 IEEE Workshop, </booktitle> <pages> pages 1-10. </pages> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference-contexts: We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback [1, 18] would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in <ref> [6, 9, 24, 37] </ref>. Acknowledgments We would like to acknowledge Andrew Back, Yoshua Bengio and Juergen Schmidhuber for useful discussions and suggestions. We would also like to thank the anonymous reviewers for helpful suggestions that greatly improved this manuscript.
Reference: [25] <author> G.V. Puskorius and L.A. Feldkamp. </author> <title> Neurocontrol of nonlinear dynamical systems with kalman filter-trained recurrent networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 279-297, </pages> <year> 1994. </year>
Reference-contexts: Several approaches have been suggested to circumvent the problem of vanishing gradients. For example, gradient-based methods can be abandoned completely in favor of alternative optimization methods <ref> [2, 25] </ref>. However, the algorithms investigated so far either perform just as poorly on problems involving long-term dependencies, or, when they are better, require far more computational resources [2].
Reference: [26] <author> S.-Z. Qin, H.-T. Su, and T.J. McAvoy. </author> <title> Comparison of four neural net learning methods for dynamic system identification. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(1) </volume> <pages> 122-130, </pages> <year> 1992. </year> <month> 22 </month>
Reference-contexts: It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [34, 35], catalytic reforming systems in a petroleum refinery [35], nonlinear oscillations associated with multi-legged locomotion in biological systems [36], time series [4], and various artificial nonlinear systems <ref> [3, 22, 26] </ref>. Furthermore, we have previously reported that gradient-descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification [11, 15].
Reference: [27] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: gradient can be expanded r w C = p 0 We can expand this further by assuming that the weights at different time indices are independent and computing the partial gradient with respect to these weights, which is the methodology used to derive algorithms such as Backpropagation Through Time (BPTT) <ref> [27, 38] </ref>. The total gradient is then equal to the sum of these partial gradients.
Reference: [28] <author> J. Schmidhuber. </author> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242, </pages> <year> 1992. </year>
Reference-contexts: As an alternative to using different learning algorithms, one suggestion has been to alter the input data so that it represents a reduced description that makes global features more explicit and more readily detectable <ref> [21, 29, 28] </ref>. However, this approach may fail if short term dependencies are equally as important. Hochreiter also proposes a specific architectural approach which utilizes high order units. [14].
Reference: [29] <author> J. Schmidhuber. </author> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 291-298. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: As an alternative to using different learning algorithms, one suggestion has been to alter the input data so that it represents a reduced description that makes global features more explicit and more readily detectable <ref> [21, 29, 28] </ref>. However, this approach may fail if short term dependencies are equally as important. Hochreiter also proposes a specific architectural approach which utilizes high order units. [14].
Reference: [30] <author> D.R. Seidl and D. Lorenz. </author> <title> A structure by which a recurrent neural network can approximate a nonlinear dynamic system. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 709-714, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical systems <ref> [30, 32, 33] </ref>. However, learning simple behavior can be quite difficult using gradient descent. For example, even though these systems are Turing equivalent, it has been difficult to get them to successfully learn small finite state machines from example strings encoded as temporal sequences.
Reference: [31] <author> H.T. Siegelmann, B.G. Horne, and C.L. Giles. </author> <title> Computational capabilities of recurrent narx neural networks. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics. </journal> <note> Accepted. Also, Technical report UMIACS-TR-95-78 and CS-TR-3500, </note> <institution> University of Maryland, College Park, Md. </institution>
Reference-contexts: models with eXogenous inputs (NARX models), and are therefore called NARX recurrent neural networks [3, 22]. (However, there is no reason that this method cannot be extended to other recurrent architectures.) This is a powerful class of models which has recently been shown to be computationally equivalent to Turing machines <ref> [31] </ref>.
Reference: [32] <author> H.T. Siegelmann and E.D. Sontag. </author> <title> On the computational power of neural networks. </title> <journal> Journal of Computer and System Science, </journal> <volume> 50(1) </volume> <pages> 132-150, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical systems <ref> [30, 32, 33] </ref>. However, learning simple behavior can be quite difficult using gradient descent. For example, even though these systems are Turing equivalent, it has been difficult to get them to successfully learn small finite state machines from example strings encoded as temporal sequences.
Reference: [33] <author> E.D. Sontag. </author> <title> Systems combining linearity and saturations and relations to neural networks. </title> <type> Technical Report SYCON-92-01, </type> <institution> Rutgers Center for Systems and Control, </institution> <year> 1992. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical systems <ref> [30, 32, 33] </ref>. However, learning simple behavior can be quite difficult using gradient descent. For example, even though these systems are Turing equivalent, it has been difficult to get them to successfully learn small finite state machines from example strings encoded as temporal sequences.
Reference: [34] <author> H.-T. Su and T.J. McAvoy. </author> <title> Identification of chemical processes using recurrent networks. </title> <booktitle> In Proceedings of the American Controls Conference, </booktitle> <volume> volume 3, </volume> <pages> pages 2314-2319, </pages> <year> 1991. </year>
Reference-contexts: It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants <ref> [34, 35] </ref>, catalytic reforming systems in a petroleum refinery [35], nonlinear oscillations associated with multi-legged locomotion in biological systems [36], time series [4], and various artificial nonlinear systems [3, 22, 26]. <p> are not sufficiently powerful to discover a relationship between target outputs and inputs that occur at a much earlier time, which they term the problem of long-term dependencies. 5 3 NARX networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXoge nous inputs (NARX) model 2 <ref> [3, 19, 20, 34, 35] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (9) where u (t) and y (t) represent input and output of the network at
Reference: [35] <author> H.-T. Su, T.J. McAvoy, and P. Werbos. </author> <title> Long-term predictions of chemical processes using recurrent neural networks: A parallel training approach. </title> <journal> Industrial Engineering and Chemical Research, </journal> <volume> 31 </volume> <pages> 1338-1352, </pages> <year> 1992. </year>
Reference-contexts: It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants <ref> [34, 35] </ref>, catalytic reforming systems in a petroleum refinery [35], nonlinear oscillations associated with multi-legged locomotion in biological systems [36], time series [4], and various artificial nonlinear systems [3, 22, 26]. <p> It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [34, 35], catalytic reforming systems in a petroleum refinery <ref> [35] </ref>, nonlinear oscillations associated with multi-legged locomotion in biological systems [36], time series [4], and various artificial nonlinear systems [3, 22, 26]. <p> are not sufficiently powerful to discover a relationship between target outputs and inputs that occur at a much earlier time, which they term the problem of long-term dependencies. 5 3 NARX networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXoge nous inputs (NARX) model 2 <ref> [3, 19, 20, 34, 35] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (9) where u (t) and y (t) represent input and output of the network at
Reference: [36] <author> S.T. Venkataraman. </author> <title> On encoding nonlinear oscillations in neural networks for locomotion. </title> <booktitle> In Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 14-20, </pages> <year> 1994. </year>
Reference-contexts: It has been demonstrated that they are well suited for modeling nonlinear systems such as heat exchangers [3], waste water treatment plants [34, 35], catalytic reforming systems in a petroleum refinery [35], nonlinear oscillations associated with multi-legged locomotion in biological systems <ref> [36] </ref>, time series [4], and various artificial nonlinear systems [3, 22, 26]. Furthermore, we have previously reported that gradient-descent learning is more effective in NARX networks than in recurrent neural network architectures with "hidden states" when applied to problems including grammatical inference and nonlinear system identification [11, 15].
Reference: [37] <author> E.A. Wan. </author> <title> Time series prediction by using a connectionist network with internal delay lines. In A.S. </title> <editor> Weigend and N.A. Gershenfeld, editors, </editor> <booktitle> Time Series Prediction, </booktitle> <pages> pages 195-217. </pages> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: We speculate that similar results could be obtained for other networks. In particular we hypothesize that any network that uses tapped delay feedback [1, 18] would demonstrate improved performance on problems involving long-term dependencies. It may also be possible to obtain similar results for the architectures proposed in <ref> [6, 9, 24, 37] </ref>. Acknowledgments We would like to acknowledge Andrew Back, Yoshua Bengio and Juergen Schmidhuber for useful discussions and suggestions. We would also like to thank the anonymous reviewers for helpful suggestions that greatly improved this manuscript.
Reference: [38] <author> R.J. Williams and D. Zipser. </author> <title> Gradient-based learning algorithms for recurrent networks and their computational complexity. </title> <editor> In Y. Chauvin and D. E. Rumelhart, editors, Back-propagation: </editor> <booktitle> Theory, Architectures and Applications, chapter 13, </booktitle> <pages> pages 433-486. </pages> <publisher> Lawrence Erlbaum Publishers, </publisher> <address> Hillsdale, N.J., </address> <year> 1995. </year> <month> 23 </month>
Reference-contexts: gradient can be expanded r w C = p 0 We can expand this further by assuming that the weights at different time indices are independent and computing the partial gradient with respect to these weights, which is the methodology used to derive algorithms such as Backpropagation Through Time (BPTT) <ref> [27, 38] </ref>. The total gradient is then equal to the sum of these partial gradients.
References-found: 38

