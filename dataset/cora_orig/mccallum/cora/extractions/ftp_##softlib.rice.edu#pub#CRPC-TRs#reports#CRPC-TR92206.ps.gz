URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92206.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: PDS: DIRECT SEARCH METHODS FOR UNCONSTRAINED OPTIMIZATION ON EITHER SEQUENTIAL OR PARALLEL MACHINES  
Author: VIRGINIA TORCZON 
Keyword: Key words. nonlinear optimization, unconstrained optimization, multidirectional search, parallel direct search, parallel computing  
Affiliation: RICE UNIVERSITY  
Abstract: PDS is a collection of Fortran subroutines for solving unconstrained nonlinear optimization problems using direct search methods. The software is written so that execution on sequential machines is straightforward while execution on Intel distributed memory machines, such as the iPSC/2, the iPSC/860 or the Touchstone Delta, can be accomplished simply by including a few well-defined routines containing calls to Intel-specific Fortran libraries. Those interested in using the methods on other distributed memory machines, even something as basic as a network of workstations or personal computers, need only modify these few subroutines to handle the global communication requirements. Furthermore, since the parallelism is clearly defined at the "do-loop" level, it is a simple matter to insert compiler directives that allow for execution on shared memory parallel machines. Included here is an example of such directives, contained in comment statements, for execution on a Sequent Symmetry S81. PDS encompasses an entire class of general-purpose optimization methods which require only that the user provide a subroutine to evaluate the function. These methods require even less of the problem to be solved since direct search methods presume only that the function is continuous. Thus, parallel direct search methods are particularly effective on parameter estimation problems involving a relatively small number of parameters. They are also very interesting as parallel algorithms because they are perfectly scalable: they can use any number of processors regardless of the dimension of the problem to be solved and, in fact, tend to perform better as more processors are added. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. E. Dennis, Jr. and V. Torczon, </author> <title> Direct search methods on parallel machines, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. 448-474. </pages>
Reference-contexts: The Parallel Direct Search Algorithms. A detailed development and description of the parallel direct search algorithms can be found in <ref> [1] </ref>. Convergence results for the multidirectional search algorithm, the direct search method upon which these parallel schemes are based, can be found in [8]. We briefly summarize the parallel direct search schemes here. <p> The derivation of the template, as well as the algorithm actually used to construct it, are described in some detail in <ref> [1] </ref>. 3. Usage. Use of the subroutines and drivers for the parallel direct search algorithms will be detailed in this section. We begin with a minimal description of what is necessary to run the parallel direct search schemes using the drivers included here. The user must provide: 1. <p> This is the example given in <ref> [1] </ref>. 2 The entries for the file INPUT can be seen in Table 10. When we run CREATE on either a Sun SPARCstation 1 or a Sequent Symmetry 2 For the numbers reported in [1], we used a different stopping test so that the results could be compared for different choices <p> This is the example given in <ref> [1] </ref>. 2 The entries for the file INPUT can be seen in Table 10. When we run CREATE on either a Sun SPARCstation 1 or a Sequent Symmetry 2 For the numbers reported in [1], we used a different stopping test so that the results could be compared for different choices of sss.
Reference: [2] <author> S. L. S. Jacoby, J. S. Kowalik, and J. T. Pizzo, </author> <title> Iterative Methods for Nonlinear Optimization Problems, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1972. </year>
Reference-contexts: Note that for a right-angled simplex, the n edges 7 adjacent to v 0 are of length SCALE. For a regular simplex, every edge in the simplex has the same length, SCALE. The algorithm for constructing a regular simplex is taken from <ref> [2] </ref> but appears also in [6].
Reference: [3] <author> C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh, </author> <title> Basic linear algebra subprograms for fortran usage, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5 (1979), </volume> <pages> pp. 308-323. </pages>
Reference-contexts: The call graph for the optimization. The subroutines DONE and MAXLEN make use of DNRM2, one of the basic linear algebra subprograms (BLAS) <ref> [3] </ref>, to compute the Euclidean vector norm. A version has been included with this code, but users can certainly link to a local library instead.
Reference: [4] <author> J. J. Mor e, B. S. Garbow, and K. E. Hillstrom, </author> <title> Testing unconstrained optimization software, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7 (1981), </volume> <pages> pp. 17-41. </pages>
Reference-contexts: Testing. Included for testing purposes is a sample driver, CREATE, to create the search scheme, a sample driver, OPTIM, to handle the optimization, a function evaluation routine FCN that evaluates the extended Rosenbrock function <ref> [4] </ref>, [5], and an input file INPUT to test the two-dimensional Rosenbrock function. This is the example given in [1]. 2 The entries for the file INPUT can be seen in Table 10.
Reference: [5] <author> H. H. </author> <title> Rosenbrock, An automatic method for finding the greatest or least value of a function, </title> <journal> The Computer Journal, </journal> <volume> 3 (1960), </volume> <pages> pp. 175-184. </pages>
Reference-contexts: Testing. Included for testing purposes is a sample driver, CREATE, to create the search scheme, a sample driver, OPTIM, to handle the optimization, a function evaluation routine FCN that evaluates the extended Rosenbrock function [4], <ref> [5] </ref>, and an input file INPUT to test the two-dimensional Rosenbrock function. This is the example given in [1]. 2 The entries for the file INPUT can be seen in Table 10.
Reference: [6] <author> W. Spendley, G. R. Hext, and F. R. Himsworth, </author> <title> Sequential application of simplex designs in optimisation and evolutionary operation, </title> <journal> Technometrics, </journal> <volume> 4 (1962), </volume> <pages> pp. 441-461. </pages>
Reference-contexts: The rest of the input variables are described below. Line Variable Fortran Type # of Entries 1 required N integer 1 2 required STEPTOL floating point 1 3 required MAXITR integer 1 4 required V0 floating point n 5 required TYPE integer 1 <ref> [6] </ref> [depends on TYPE] SCALE floating point 1 [6 + n 1] [depends on TYPE] SIMPLEX floating point n 2 (n/line) 7 [+n] required DEBUG integer 1 8 [+n] required SSS integer 1 Table 3 Format of the input. 3.3.1. The stopping conditions. <p> Note that for a right-angled simplex, the n edges 7 adjacent to v 0 are of length SCALE. For a regular simplex, every edge in the simplex has the same length, SCALE. The algorithm for constructing a regular simplex is taken from [2] but appears also in <ref> [6] </ref>.
Reference: [7] <author> V. Torczon, </author> <title> Multi-Directional Search: A Direct Search Algorithm for Parallel Machines, </title> <type> Ph.D. thesis, </type> <institution> Department of Mathematical Sciences, Rice University, Houston, TX, </institution> <note> 1989; also available as Tech. Report 90-7, </note> <institution> Department of Mathematical Sciences, Rice University, Houston, </institution> <month> TX 77251-1892. </month> <title> [8] , On the convergence of the multidirectional search algorithm, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. 123-145. </pages>
Reference-contexts: The Search Scheme. To divorce the number of processors that can be used from the dimension of the problem to be solved, we rely on search schemes that take the basic multidirectional search algorithm introduced in <ref> [7] </ref> and look ahead to subsequent iterations of the algorithm. Because the multidirectional search algorithm is essentially a sampling method, the moves allowed are limited but predictable. As a consequence, "look-aheads" are possible and unlimited. This simple strategy leads to extremely flexible, scalable algorithms ideally suited to parallel computation.
Reference: [9] <author> D. J. Woods, </author> <title> An Interactive Approach for Solving Multi-Objective Optimization Problems, </title> <type> Ph.D. thesis, </type> <institution> Department of Mathematical Sciences, Rice University, Houston, TX, </institution> <note> 1985; also available as Tech. Report 85-5, </note> <institution> Department of Mathematical Sciences, Rice University, </institution> <address> Houston, TX 77251-1892. </address>
Reference-contexts: This test is a slight modification of a test proposed in <ref> [9] </ref>. Its main advantage in the parallel setting is that each processor can test for convergence independently without any need for further synchronization.
References-found: 8

