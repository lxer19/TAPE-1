URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/maclin.tr94.ps.Z
Refering-URL: http://www.cs.wisc.edu/~maclin/abstract-ml94-robot-learning.html
Root-URL: 
Email: Email: fmaclin,shavlikg@cs.wisc.edu  
Phone: Phone: 608-263-0475  
Title: Incorporating Advice into Agents that Learn from Reinforcements  
Author: Richard Maclin Jude W. Shavlik 
Address: 1210 West Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department  
Abstract: Learning from reinforcements is a promising approach for creating intelligent agents. However, reinforcement learning usually requires a large number of training episodes. We present a system called ratle that addresses this shortcoming by allowing a connectionist Q-learner to accept advice given, at any time and in a natural manner, by an external observer. In ratle, the advice-giver watches the learner and occasionally makes suggestions, expressed as instructions in a simple programming language. Based on techniques from knowledge-based neural networks, ratle inserts these programs directly into the agent's utility function. Subsequent reinforcement learning further integrates and refines the advice. We present empirical evidence that shows our approach leads to statistically-significant gains in expected reward. Importantly, the advice improves the expected reward regardless of the stage of training at which it is given. A shorter version of this paper appears in the Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), AAAI Press. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. & Chapman, D. </author> <year> (1987). </year> <title> Pengi: An implementation of a theory of activity. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 268-272), </pages> <address> Seattle, WA. </address>
Reference: <author> Barto, A., Sutton, R., & Watkins, C. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M. & Moore, J., editors, </editor> <title> Learning and Computational Neuroscience. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Berenji, H. & Khedkar, P. </author> <year> (1992). </year> <title> Learning and tuning fuzzy logic controllers through reinforcements. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3 </volume> <pages> 724-740. </pages>
Reference: <author> Brooks, R. </author> <year> (1990). </year> <title> The behavior language; user's guide. </title> <type> AI Memo 1227, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference: <author> Chapman, D. </author> <year> (1991). </year> <title> Vision, instruction, and action. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> 25 Clouse, </note> <author> J. & Utgoff, P. </author> <year> (1992). </year> <title> A teaching method for reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Workshop, </booktitle> <pages> (pp. 92-101), </pages> <address> Aberdeen, Scot-land. </address>
Reference: <author> Cohen, P. & Feigenbaum, E. </author> <year> (1982). </year> <booktitle> The Handbook of Artificial Intelligence (volume 3). </booktitle> <publisher> William Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference: <author> Diederich, J. </author> <year> (1989). </year> <title> "Learning by instruction" in connectionist systems. </title> <booktitle> In Machine Learning: Proceedings of the Sixth International Workshop, </booktitle> <pages> (pp. 66-68), </pages> <address> Ithaca, NY. </address>
Reference: <author> Dietterich, T. </author> <year> (1991). </year> <title> Knowledge compilation: Bridging the gap between specification and implementation. </title> <journal> IEEE Expert, </journal> <volume> 6 </volume> <pages> 80-82. </pages>
Reference-contexts: Ratle then parses the advice, using traditional methods from programming-language compilers. Step 3. Convert the advice into a usable form. After the advice has been parsed, ratle transforms the general advice into terms that can be directly understood by the agent. Using techniques from knowledge compilation <ref> (Dietterich, 1991) </ref>, a learner can convert ("operationalize") high-level advice into a (usually larger) collection of directly interpretable statements (see Gordon and Subramanian, 1994).
Reference: <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211. </pages>
Reference: <author> Fu, L. M. </author> <year> (1989). </year> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 325-340. </pages>
Reference: <author> Gat, E. </author> <year> (1991). </year> <title> ALFA: A language for programming reactive robotic control systems. </title> <booktitle> In IEEE International Conference on Robotics and Automation (volume 2), </booktitle> <pages> (pp. 1116-1121). </pages>
Reference: <author> Gordon, D. & Subramanian, D. </author> <year> (1994). </year> <title> A multistrategy learning scheme for agent knowledge acquisition. </title> <journal> Informatica, </journal> <volume> 17 </volume> <pages> 331-346. </pages>
Reference-contexts: After the advice has been parsed, ratle transforms the general advice into terms that can be directly understood by the agent. Using techniques from knowledge compilation (Dietterich, 1991), a learner can convert ("operationalize") high-level advice into a (usually larger) collection of directly interpretable statements <ref> (see Gordon and Subramanian, 1994) </ref>.
Reference: <author> Gruau, F. </author> <year> (1994). </year> <title> Neural Network Synthesis using Cellular Encoding and the Genetic Algorithm. </title> <type> PhD thesis, </type> <institution> Ecole Normale Superieure de Lyon, France. </institution>
Reference: <author> Hayes-Roth, F., Klahr, P., & Mostow, D. J. </author> <year> (1981). </year> <title> Advice-taking and knowledge refinement: An iterative view of skill acquisition. </title> <editor> In Anderson, J., editor, </editor> <title> Cognitive Skills and their Acquisition. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Huffman, S. & Laird, J. </author> <year> (1993). </year> <title> Learning procedures from interactive natural language instructions. </title> <booktitle> In Machine Learning: Proceedings on the Tenth International Conference, </booktitle> <pages> (pp. 143-150), </pages> <address> Amherst, MA. </address>
Reference: <author> Jordan, M. </author> <year> (1989). </year> <title> Serial order: A parallel, distributed processing approach. </title> <editor> In Elman, J. & Rumelhart, D., editors, </editor> <booktitle> Advances in Connectionist Theory: Speech. </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Kaelbling, L. </author> <year> (1987). </year> <title> REX: A symbolic language for the design and parallel implementation of embedded systems. </title> <booktitle> In Proceedings of the AIAA Conference on Computers in Aerospace, </booktitle> <address> Wakefield, MA. </address>
Reference: <author> Laird, J., Hucka, M., Yager, E., & Tuck, C. </author> <year> (1990). </year> <title> Correcting and extending domain knowledge using outside guidance. </title> <booktitle> In Machine Learning: Proceedings of the Seventh International Workshop, </booktitle> <pages> (pp. 235-243), </pages> <address> Austin, TX. </address>
Reference: <author> Laird, J., Newell, A., & Rosenbloom, P. </author> <year> (1987). </year> <title> SOAR: an architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 1-64. </pages> <note> 26 Lin, </note> <author> L. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> 8:293-321. 
Reference-contexts: Also, we do not assume the advice is correct; instead we use learning to refine and evaluate the advice. More recently, Laird et al. (1990) created an advice-taking system called Robo-Soar based on the Soar architecture <ref> (Laird et al., 1987) </ref>. Soar uses its knowledge to select operators to achieve goals. If it is unable to select from a set of operators or no operator is available an impasse arises. In this case Soar creates a subgoal and applies itself to the subgoal.
Reference: <author> Lin, L. </author> <year> (1993). </year> <title> Scaling up reinforcement learning for robot control. </title> <booktitle> In Machine Learning: Proceedings on the Tenth International Conference, </booktitle> <pages> (pp. 182-189), </pages> <address> Amherst, MA. </address>
Reference-contexts: Our work extends knowledge-based neural 22 networks to a new task and shows that "domain theories" can be supplied incremen-tally (as opposed to providing the domain theory at the start of the learning task). Our work on ratle is similar to our earlier work with the fskbann system <ref> (Maclin & Shavlik, 1993) </ref>. Fskbann uses a type of recurrent neural network introduced by Jordan (1989) and Elman (1990) that maintains information from previous activations using the recurrent network links. Fskbann extends kbann to deal with state units, but it does not create new state units.
Reference: <author> Maclin, R. & Shavlik, J. </author> <year> (1993). </year> <title> Using knowledge-based neural networks to improve algorithms: Refining the Chou-Fasman algorithm for protein folding. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 195-215. </pages>
Reference-contexts: Our work extends knowledge-based neural 22 networks to a new task and shows that "domain theories" can be supplied incremen-tally (as opposed to providing the domain theory at the start of the learning task). Our work on ratle is similar to our earlier work with the fskbann system <ref> (Maclin & Shavlik, 1993) </ref>. Fskbann uses a type of recurrent neural network introduced by Jordan (1989) and Elman (1990) that maintains information from previous activations using the recurrent network links. Fskbann extends kbann to deal with state units, but it does not create new state units.
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1992). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 311-365. </pages>
Reference: <author> McCarthy, J. </author> <year> (1958). </year> <title> Programs with common sense. </title> <booktitle> In Proceedings of the Symposium on the Mechanization of Thought Processes (volume I), </booktitle> <pages> (pp. 77-84). </pages> <note> (Reprinted in M. </note> <editor> Minsky, editor, </editor> <booktitle> 1968, Semantic Information Processing. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, 403-409.). </publisher>
Reference: <author> Mitchell, T., Keller, R., & Kedar-Cabelli, S. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 47-80. </pages>
Reference-contexts: Huffman and Laird (1993) developed another Soar-based system called Instructo-Soar that allows an agent to interpret simple imperative statements such as "Pick up the red block" and "Move down." InstructoSoar examines these instructions in the context of its current problem solving situation and uses explanation-based learning <ref> (Mitchell et al., 1986) </ref> to try to generate a general rule based on the advice that may be used in similar situations. Ratle differs from InstructoSoar in that we provide a language for entering general advice rather than attempting to generalize specific advice.
Reference: <author> Mostow, D. J. </author> <year> (1982). </year> <title> Transforming declarative advice into effective procedures: A heuristic search example. </title> <editor> In Michalski, R., Carbonell, J., & Mitchell, T., editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (volume 1). </booktitle> <publisher> Tioga Press, </publisher> <address> Palo Alto. </address>
Reference: <author> Nilsson, N. </author> <year> (1994). </year> <title> Teleo-reactive programs for agent control. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 139-158. </pages>
Reference: <author> Nowlan, S. & Hinton, G. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 473-493. </pages>
Reference-contexts: Our agent currently suffers from the limitation that it must learn the value of actions when facing each of the four directions East, North, West, and South. We plan to examine the use of soft weight-sharing <ref> (Nowlan & Hinton, 1992) </ref> to foster transfer of learning across the corresponding rules for each direction. We also plan to evaluate our approach in other domains. One especially interesting area of interest is the training of agents in a cooperative environment, where tasks are performed by multiple agents working together.
Reference: <author> Omlin, C. & Giles, C. </author> <year> (1992). </year> <title> Training second-order recurrent neural networks using hints. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Workshop, </booktitle> <pages> (pp. 361-366), </pages> <address> Aberdeen, Scotland. </address>
Reference: <author> Sacerdoti, E. </author> <year> (1974). </year> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 5 </volume> <pages> 115-135. </pages>
Reference-contexts: Providing advice to a problem solver An early example of a system that makes use of advice is the abstrips planning system <ref> (Sacerdoti, 1974) </ref>. In abstrips, a human user assigns initial "criticalities" to preconditions to cause the planner to focus on making key planning steps.
Reference: <author> Siegelmann, H. </author> <year> (1994). </year> <booktitle> Neural programming language. In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA. </address>
Reference: <author> Suppes, P. </author> <year> (1991). </year> <title> Language for Humans and Robots. </title> <publisher> Blackwell, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. </author> <year> (1991). </year> <title> Reinforcement learning architectures for animats. </title> <editor> In Meyer, J. & Wilson, S., editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> 27 Tesauro, </note> <author> G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277. </pages>
Reference-contexts: Step 5. Judge the value of the advice. The final step of the advice-taking process is to evaluate the advice. One can also envision that in some circumstances such as a game-learner that can play against itself (Tesauro, 1992) or when an agent builds an internal world model <ref> (Sutton, 1991) </ref> it would be straightforward to empirically evaluate the new advice.
Reference: <author> Thrun, S. </author> <year> (1994). </year> <type> Personal communication. </type>
Reference-contexts: This process is analogous to how kbann processes multiple rules with the same consequent. We do not use this process with output units because we do not necessarily want output units activity to be near 1 (max utility) since overpredictions of utility can cause problems for connectionist Q-learning <ref> (Thrun, 1994) </ref>. Step 5. Judge the value of the advice. Once ratle inserts the advice into the RL agent, the agent returns to exploring its environment. In this way the agent can evaluate the advice empirically and refine the advice based on its further experience.
Reference: <author> Thrun, S. & Mitchell, T. </author> <year> (1993). </year> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of the Thirteenth Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 930-936), </pages> <address> Chambery, France. </address>
Reference: <author> Towell, G. & Shavlik, J. </author> <title> (in press). </title> <booktitle> Knowledge-based artificial neural networks. Artificial Intelligence. </booktitle>
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 861-866), </pages> <address> Boston, MA. </address>
Reference: <author> Utgoff, P. & Clouse, J. </author> <year> (1991). </year> <title> Two kinds of training information for evaluation function learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 596-600), </pages> <address> Anaheim, CA. </address>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, Cam-bridge. </institution>
Reference-contexts: In our augmentation, an observer watches the learner and periodically provides advice, which ratle incorporates into the action-choosing module of the RL agent. In Q-learning <ref> (Watkins, 1989) </ref> the action-choosing module is a utility function that maps states and actions to a numeric value. The utility value of a particular state and action is the predicted future (discounted) reward that will be achieved if that action is taken by the agent in that state.
Reference: <author> Whitehead, S. </author> <year> (1991). </year> <title> A complexity analysis of cooperative mechanisms in reinforcement learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 607-613), </pages> <address> Anaheim, CA. </address> <month> 28 </month>
References-found: 40

