URL: ftp://ftp.cis.upenn.edu/pub/msingh/ijar95.ps.Z
Refering-URL: http://www.cis.upenn.edu/~msingh/frames/papers_list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Construction of Bayesian Network Structures from Data: a Brief Survey and an Efficient Algorithm  
Author: Moninder Singh and Marco Valtorta 
Keyword: Bayesian Networks, Probabilistic Networks, Probabilistic Model Construction, Conditional Independence.  
Address: SC 29208, U.S.A  
Affiliation: Dept. of Computer Science, University of South Carolina Columbia,  
Abstract: Previous algorithms for the recovery of Bayesianbelief network structures from data have been either highly dependent on conditional independence (CI) tests, or have required an ordering on the nodes to be supplied by the user. We present an algorithm that integrates these two approaches - CI tests are used to generate an ordering on the nodes from the database which is then used to recover the underlying Bayesian network structure using a non CI test based method. Results of the evaluation of the algorithm on a number of databases (e.g. ALARM, LED and SOYBEAN) are presented. We also discuss some algorithm performance issues and open problems. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Singh, M., and Valtorta, M., </author> <title> An algorithm for the construction of Bayesian network structures from data, </title> <booktitle> Proceedings of the 9th Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Washington D.C., </address> <publisher> Morgan Kaufmann, </publisher> <pages> 259-265, </pages> <year> 1993. </year>
Reference-contexts: 1. INTRODUCTION In very general terms, different methods of learning probabilistic network structures from data can be classified into three groups. Some of these fl A preliminary version of this paper was presented in <ref> [1] </ref>. y Current address is the Department of Computer and Information Science, University of Pennsylvania, 200 S 33rd St., Philadelphia, PA 19104 z E-mail: msingh@gradient.cis.upenn.edu or mgv@usceast.cs.scarolina.edu International Journal of Approximate Reasoning 1995 12:111-131 c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111
Reference: 2. <author> Glymour, C., Scheines, R., Spirtes, P., and Kelly, K., </author> <title> Discovering Causal Structure, </title> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1987. </year>
Reference-contexts: Information Science, University of Pennsylvania, 200 S 33rd St., Philadelphia, PA 19104 z E-mail: msingh@gradient.cis.upenn.edu or mgv@usceast.cs.scarolina.edu International Journal of Approximate Reasoning 1995 12:111-131 c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions <ref> [2, 3] </ref>; others are more general but require extensive testing of independence relations [4, 5, 6, 7, 8]; others yet take a Bayesian approach [9, 10, 11, 12]. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions.
Reference: 3. <author> Pearl, J., and Wermuth, N., </author> <title> When can association graphs admit a causal interpretation? (first report), </title> <booktitle> Preliminary Papers of the 4th International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, 141-150, </address> <month> January 3-6 </month> <year> 1993. </year>
Reference-contexts: Information Science, University of Pennsylvania, 200 S 33rd St., Philadelphia, PA 19104 z E-mail: msingh@gradient.cis.upenn.edu or mgv@usceast.cs.scarolina.edu International Journal of Approximate Reasoning 1995 12:111-131 c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions <ref> [2, 3] </ref>; others are more general but require extensive testing of independence relations [4, 5, 6, 7, 8]; others yet take a Bayesian approach [9, 10, 11, 12]. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions.
Reference: 4. <author> Fung, R. M., and Crawford, S. L., </author> <title> Constructor: A system for the induction of probabilistic models, </title> <booktitle> Proceedings of AAAI, </booktitle> <address> Boston, MA, </address> <publisher> MIT Press, </publisher> <pages> 762-769, </pages> <year> 1990. </year>
Reference-contexts: E-mail: msingh@gradient.cis.upenn.edu or mgv@usceast.cs.scarolina.edu International Journal of Approximate Reasoning 1995 12:111-131 c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions [2, 3]; others are more general but require extensive testing of independence relations <ref> [4, 5, 6, 7, 8] </ref>; others yet take a Bayesian approach [9, 10, 11, 12]. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions. Our work concentrates on CI test based methods and Bayesian methods. <p> RESULTS The CB algorithm was implemented in C on a DEC Station 5000, and was used to reconstruct the underlying Bayesian network structure from four databases. For two of the databases, namely the ALARM [21, 9, 10] and the LED <ref> [4] </ref> databases, the Bayesian network structure which had been used to generate the database was known. <p> However, the metric used by K2 ranked the earlier network structure (Figure 1 (b)) to be more probable. The time taken was reduced to under 7 minutes. 4.2. The LED Database The LED database was used by Fung and Crawford <ref> [4] </ref> for the evaluation of their Markov network generating program called Constructor. <p> Secondly, we have used a fixed ff level for the 2 test. This will almost certainly introduce dependencies that are purely the result of chance. It is possible to use the technique of Cross Validation for tuning this parameter. Fung and Crawford <ref> [4] </ref> discuss the tuning of the alpha level in performing belief-network learning.
Reference: 5. <author> Verma, T., and Pearl, J., </author> <title> An algorithm for deciding if a set of observed independencies has a causal explanation, </title> <booktitle> Proceedings of the 8th Conference on Uncertainty in Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> 323-330, </pages> <year> 1992. </year>
Reference-contexts: E-mail: msingh@gradient.cis.upenn.edu or mgv@usceast.cs.scarolina.edu International Journal of Approximate Reasoning 1995 12:111-131 c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions [2, 3]; others are more general but require extensive testing of independence relations <ref> [4, 5, 6, 7, 8] </ref>; others yet take a Bayesian approach [9, 10, 11, 12]. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions. Our work concentrates on CI test based methods and Bayesian methods. <p> However, there are two major drawbacks of such CI test based algorithms. Firstly, the CI test requires determining independence relations of order n 2, in the worst case. "Such tests may be unreliable, unless the volume of data is enormous" [10, page 332]. Also, as Verma and Pearl <ref> [5, pages 326-327] </ref> have noted, "in general, the set of all independence statements which hold for a given domain will grow exponentially as the number of variables grow". As such, CI test based approaches become rapidly computationally infeasible as the number of vertices increases. <p> The two phases are executed iteratively first for 0th order CI relations, then for 1st order CI relations, and so on until the termination criteria is met. Steps 1 to 4 of the algorithm are based on the algorithms given by Verma and Pearl <ref> [5] </ref> and Spirtes and Glymour [6]. We have allowed edges to be oriented in both directions because at any given stage, since CI tests of all orders have not been performed, all CI relations have not been discovered and there will be a number of extra edges. <p> For each pair of non adjacent variables a; b in G, if there is a node c that is not in S ab and is adjacent to both a and b, then orient the edges from a ! c and b ! c (see <ref> [5, 6] </ref>) unless such an orientation leads to the introduction of a directed cycle in the graph. If an edge has already been oriented in the reverse direction, make that edge bidirected. 4. <p> If an edge has already been oriented in the reverse direction, make that edge bidirected. 4. Try to assign directions to the yet undirected edges in G by applying the following four rules <ref> [5, 18] </ref> if this can be done without introducing directed cycles in the graph: Rule 1: If a ! b and b c and a and c are not adjacent, then direct b ! c. <p> Each of steps 3 through 9 have only polynomial complexity in the number of variables, by arguments that are either simple or described in <ref> [5, 10] </ref>. In step 2, the number of independence tests carried out is exponential in the size of the order of the independence relations to be tested, which is bounded by the maximum of jA G abj.
Reference: 6. <author> Spirtes, P., and Glymour, C., </author> <title> An algorithm for fast recovery of sparse causal graphs, </title> <journal> Social Science Computing Review, </journal> <volume> 9(1), </volume> <pages> 62-72, </pages> <year> 1991. </year>
Reference-contexts: E-mail: msingh@gradient.cis.upenn.edu or mgv@usceast.cs.scarolina.edu International Journal of Approximate Reasoning 1995 12:111-131 c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions [2, 3]; others are more general but require extensive testing of independence relations <ref> [4, 5, 6, 7, 8] </ref>; others yet take a Bayesian approach [9, 10, 11, 12]. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions. Our work concentrates on CI test based methods and Bayesian methods. <p> As such, CI test based approaches become rapidly computationally infeasible as the number of vertices increases. Spirtes and Glymour <ref> [6, page 62] </ref> have presented "an asymptotically correct algorithm whose complexity for fixed graph connectivity increases polynomially in the number of vertices, and may in practice recover sparse graphs with several hundred variables"; but for dense graphs with limited data, the algorithm might be unreliable [10]. <p> The two phases are executed iteratively first for 0th order CI relations, then for 1st order CI relations, and so on until the termination criteria is met. Steps 1 to 4 of the algorithm are based on the algorithms given by Verma and Pearl [5] and Spirtes and Glymour <ref> [6] </ref>. We have allowed edges to be oriented in both directions because at any given stage, since CI tests of all orders have not been performed, all CI relations have not been discovered and there will be a number of extra edges. <p> Let i be the set of parents of node i; 1 i n. 1. Start with the complete graph G 1 on the set of vertices Z. ord 0. 2. (based on <ref> [6] </ref>) Modify G 1 as follows : For each pair of vertices a; b that are adjacent in G 1 , if A G 1 ab has a cardinality greater than or equal to ord, and I (a; S ab ; b) 5 where S ab A G 1 ab of <p> For each pair of non adjacent variables a; b in G, if there is a node c that is not in S ab and is adjacent to both a and b, then orient the edges from a ! c and b ! c (see <ref> [5, 6] </ref>) unless such an orientation leads to the introduction of a directed cycle in the graph. If an edge has already been oriented in the reverse direction, make that edge bidirected. 4. <p> They applied the PC algorithm <ref> [6] </ref> to the ALARM database split into two parts of 10000 cases each. The algorithm did not make any linearity assumption. <p> The number of times that steps 2 through 9 of the CB algorithm are executed is bound by the sum of the largest two degrees in the undirected graph constructed at the end of step 2, by an argument almost identical to that of <ref> [6, page 68] </ref>. Each of steps 3 through 9 have only polynomial complexity in the number of variables, by arguments that are either simple or described in [5, 10].
Reference: 7. <author> Pearl, J., and Verma, T., </author> <title> A theory of inferred causation, </title> <booktitle> Proceedings of the 2nd International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> 441-452, </pages> <year> 1991. </year>
Reference-contexts: E-mail: msingh@gradient.cis.upenn.edu or mgv@usceast.cs.scarolina.edu International Journal of Approximate Reasoning 1995 12:111-131 c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions [2, 3]; others are more general but require extensive testing of independence relations <ref> [4, 5, 6, 7, 8] </ref>; others yet take a Bayesian approach [9, 10, 11, 12]. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions. Our work concentrates on CI test based methods and Bayesian methods.
Reference: 8. <author> Spirtes, P., Glymour, C., and Scheines, R., </author> <title> Causality from Probability, </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1990, </year> <pages> 181-199. </pages>
Reference-contexts: E-mail: msingh@gradient.cis.upenn.edu or mgv@usceast.cs.scarolina.edu International Journal of Approximate Reasoning 1995 12:111-131 c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions [2, 3]; others are more general but require extensive testing of independence relations <ref> [4, 5, 6, 7, 8] </ref>; others yet take a Bayesian approach [9, 10, 11, 12]. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions. Our work concentrates on CI test based methods and Bayesian methods.
Reference: 9. <author> Herskovits, E. H., </author> <title> Computer-based probabilistic-network construction, </title> <type> PhD thesis, </type> <institution> Medical Information Sciences, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1991. </year>
Reference-contexts: c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions [2, 3]; others are more general but require extensive testing of independence relations [4, 5, 6, 7, 8]; others yet take a Bayesian approach <ref> [9, 10, 11, 12] </ref>. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions. Our work concentrates on CI test based methods and Bayesian methods. A number of algorithms have been designed which are based on CI tests. <p> However, since the number of possible structures grow exponentially as a function of the number of variables, it is computationally infeasible to find the most probable belief network structure, given the data, by exhaustively enumerating all possible belief network structures. Herskovits and Cooper <ref> [10, 9] </ref> proposed a greedy algorithm, called the K2 algorithm, to maximize P (B S ; D) by finding the parent set of each variable that maximizes the function g (i; i ). <p> In a small setting, grouping variables into generic classes, such as symptoms and diseases may be sufficient to limit 1 Herskovits <ref> [9] </ref> suggested the use of the metric (on which K2 is based) with a CI test based method to do away with the requirement for an order of nodes. 2 In this paper, no distinction is made between the nodes of a Bayesian network and the variables they represent. 3 Whereas <p> He showed that the metric on which K2 is based is minimized, as the number of cases increases, without limit, on "those [Bayesian] network structures that, for a given node order, most parsimoniously capture all the independencies manifested in the data" <ref> [9, chapter 6] </ref>. More precisely, he showed that the K2 metric will always favor, as the number of cases in the database increase without limit, a minimal I-map consistent with the given ordering (see [13, chapter 3] for the definition of minimal I-map). <p> RESULTS The CB algorithm was implemented in C on a DEC Station 5000, and was used to reconstruct the underlying Bayesian network structure from four databases. For two of the databases, namely the ALARM <ref> [21, 9, 10] </ref> and the LED [4] databases, the Bayesian network structure which had been used to generate the database was known. <p> In these cases, since the true underlying Bayesian network is unknown, we used the shell HUGIN [25] to test the predictive accuracy of the constructed networks. Once the network structure had been constructed, the required conditional probabilities were calculated from the database using the following relation <ref> [9] </ref>: P (x i = v ik j i = w ij ) = N ij + r i where the various symbols have the same meanings as in Theorem 1. <p> We used the CB algorithm to reconstruct the ALARM network (Figure 1 (a)) by using 10,000 cases of a (20,000 case) database generated by Herskovits <ref> [9, 10] </ref> from the original network by using a Monte Carlo technique called probabilistic logic sampling [26]. We used a bound of 15 on the maximum degree of the undirected graph generated in step 2. <p> Herskovits <ref> [9] </ref> had achieved a maximum predictive accuracy of about 84% with the K2 algorithm, and a predictive accuracy of 86% with K2-multiscore (an extension of the K2 algorithm in which the `n' most probable networks corresponding to a particular ordering are generated, and are collectively used for prediction with the network <p> Herskovits <ref> [9] </ref> also discusses in detail the possible reasons why the network generated by the K2 algorithm had a predictive accuracy much lower than the 98% accuracy achieved by [24]. <p> However, this problem is highly complicated by the extremely large number of possible network structures for any given set of variables. The CB algorithm can be easily extended on the lines of Herskovits' K2-multiscore algorithm <ref> [9] </ref> to construct the `n' most probable models corresponding to a particular ordering of the nodes.
Reference: 10. <author> Cooper, G. F., and Herskovits, E. H., </author> <title> A Bayesian method for the induction of probabilistic networks from data, </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions [2, 3]; others are more general but require extensive testing of independence relations [4, 5, 6, 7, 8]; others yet take a Bayesian approach <ref> [9, 10, 11, 12] </ref>. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions. Our work concentrates on CI test based methods and Bayesian methods. A number of algorithms have been designed which are based on CI tests. <p> However, there are two major drawbacks of such CI test based algorithms. Firstly, the CI test requires determining independence relations of order n 2, in the worst case. "Such tests may be unreliable, unless the volume of data is enormous" <ref> [10, page 332] </ref>. Also, as Verma and Pearl [5, pages 326-327] have noted, "in general, the set of all independence statements which hold for a given domain will grow exponentially as the number of variables grow". <p> Spirtes and Glymour [6, page 62] have presented "an asymptotically correct algorithm whose complexity for fixed graph connectivity increases polynomially in the number of vertices, and may in practice recover sparse graphs with several hundred variables"; but for dense graphs with limited data, the algorithm might be unreliable <ref> [10] </ref>. On the other hand, Cooper and Herskovits [10] have given a Bayesian non-CI test based method, which they call the BLN (Bayesian learning of belief networks) method. <p> have presented "an asymptotically correct algorithm whose complexity for fixed graph connectivity increases polynomially in the number of vertices, and may in practice recover sparse graphs with several hundred variables"; but for dense graphs with limited data, the algorithm might be unreliable <ref> [10] </ref>. On the other hand, Cooper and Herskovits [10] have given a Bayesian non-CI test based method, which they call the BLN (Bayesian learning of belief networks) method. <p> On the other hand, Cooper and Herskovits [10] have given a Bayesian non-CI test based method, which they call the BLN (Bayesian learning of belief networks) method. Given that a set of four assumptions hold <ref> [10, page 338] </ref>, namely, (i) The database variables are discrete, (ii) Cases occur independently, given a belief network model, (iii) All variables are instantiated to some value in every case, and finally (iv) Before observing the database, we are indifferent regarding the numerical probabilities to place on the belief network structure, <p> (i) The database variables are discrete, (ii) Cases occur independently, given a belief network model, (iii) All variables are instantiated to some value in every case, and finally (iv) Before observing the database, we are indifferent regarding the numerical probabilities to place on the belief network structure, Cooper and Herskovits <ref> [10] </ref> have shown the following result: Theorem 1. (due to Cooper and Herskovits [10]). Consider a set Z of n discrete variables. Each variable x i 2 Z has r i possible value assignments: (v i1 ; : : : ; v ir i ). <p> network model, (iii) All variables are instantiated to some value in every case, and finally (iv) Before observing the database, we are indifferent regarding the numerical probabilities to place on the belief network structure, Cooper and Herskovits <ref> [10] </ref> have shown the following result: Theorem 1. (due to Cooper and Herskovits [10]). Consider a set Z of n discrete variables. Each variable x i 2 Z has r i possible value assignments: (v i1 ; : : : ; v ir i ). <p> However, since the number of possible structures grow exponentially as a function of the number of variables, it is computationally infeasible to find the most probable belief network structure, given the data, by exhaustively enumerating all possible belief network structures. Herskovits and Cooper <ref> [10, 9] </ref> proposed a greedy algorithm, called the K2 algorithm, to maximize P (B S ; D) by finding the parent set of each variable that maximizes the function g (i; i ). <p> Although, theoretically, equation 1 can be used to find the probability P ( i ! j j D) (and P ( i j j D)) from the data <ref> [10, page 318] </ref> which can then be used to orient an edge i j (on the basis of which orientation is more probable), it is computationally infeasible do so because of the sheer number of network structures which have that edge. Hence the use of a heuristic. <p> RESULTS The CB algorithm was implemented in C on a DEC Station 5000, and was used to reconstruct the underlying Bayesian network structure from four databases. For two of the databases, namely the ALARM <ref> [21, 9, 10] </ref> and the LED [4] databases, the Bayesian network structure which had been used to generate the database was known. <p> We used the CB algorithm to reconstruct the ALARM network (Figure 1 (a)) by using 10,000 cases of a (20,000 case) database generated by Herskovits <ref> [9, 10] </ref> from the original network by using a Monte Carlo technique called probabilistic logic sampling [26]. We used a bound of 15 on the maximum degree of the undirected graph generated in step 2. <p> Due to the bound, it did not generate a network for CI relations of order 0. Out of 46 edges, it recovered 45 edges (Figure 1 (b)). The only missing edge was the edge 12 ! 32 (an edge which is not strongly supported by the data <ref> [10] </ref>. Two of the edges recovered were incorrectly oriented. However, the algorithm also recovered 14 extra edges. This is probably due to the incorrectly oriented edges, and to some extent, due to the greedy nature of K2. One of the incorrectly oriented edge was between the variables 34 and 33. <p> The remaining extra edge was between nodes 15 and 34 which is recovered, once again, due to the greedy nature of K2. The total time taken was under 13 minutes. Cooper and Herskovits <ref> [10] </ref> reported that K2, when given a total ordering consistent with the partial order of the nodes as specified by ALARM, recovered the complete network with the exception of one missing edge (between nodes 12 and 32) and one extra arc (from node 15 to 34). [27] reported similar results with <p> Each of steps 3 through 9 have only polynomial complexity in the number of variables, by arguments that are either simple or described in <ref> [5, 10] </ref>. In step 2, the number of independence tests carried out is exponential in the size of the order of the independence relations to be tested, which is bounded by the maximum of jA G abj.
Reference: 11. <author> Lauritzen, S. L., Thiesson, B., and Spiegelhalter, D., </author> <title> Diagnostic systems created by model selection methods-a case study, </title> <booktitle> Preliminary Papers of the 4th International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, 93-105, </address> <month> January 3-6 </month> <year> 1993. </year> <month> 130 </month>
Reference-contexts: c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions [2, 3]; others are more general but require extensive testing of independence relations [4, 5, 6, 7, 8]; others yet take a Bayesian approach <ref> [9, 10, 11, 12] </ref>. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions. Our work concentrates on CI test based methods and Bayesian methods. A number of algorithms have been designed which are based on CI tests. <p> We ignore Markov nets [13, chapter 3], chain graphs [14, 15], and other graphical representations (e.g., [16, 17]). 115 the number of orderings to be searched without having to use dramatically greedy heuristics. This was shown to be adequate for a medical application with 10 nodes in <ref> [11] </ref>, where variables were divided in "blocks." In some applications, however, it may be impossible to divide variables into classes, or the classes may be too large to impose sufficient structure on the space of candidate orderings. <p> The initial configuration (k = 0) at which the annealing algorithm starts consists of the parent sets, i k ; 1 i n, obtained at the end of Step 6 See <ref> [11] </ref> for a discussion of other similar situations. 120 6 of the CB algorithm. The algorithm randomly selects two nodes, and determines which of the two occurs earlier in the total ordering.
Reference: 12. <author> Lam, W., and Bacchus, F., </author> <title> Using causal information and local measures to learn Bayesian networks, </title> <booktitle> Proceedings of the 9th Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Washington D.C., </address> <publisher> Morgan Kaufmann, </publisher> <pages> 243-250, </pages> <year> 1993. </year>
Reference-contexts: c fl 1995 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/95/$7.00 111 112 methods are based on linearity and normality assumptions [2, 3]; others are more general but require extensive testing of independence relations [4, 5, 6, 7, 8]; others yet take a Bayesian approach <ref> [9, 10, 11, 12] </ref>. In this paper, we do not consider methods of the first kind, namely, those that make linearity and normality assumptions. Our work concentrates on CI test based methods and Bayesian methods. A number of algorithms have been designed which are based on CI tests.
Reference: 13. <author> Pearl, J., </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: We ignore Markov nets <ref> [13, chapter 3] </ref>, chain graphs [14, 15], and other graphical representations (e.g., [16, 17]). 115 the number of orderings to be searched without having to use dramatically greedy heuristics. <p> More precisely, he showed that the K2 metric will always favor, as the number of cases in the database increase without limit, a minimal I-map consistent with the given ordering (see <ref> [13, chapter 3] </ref> for the definition of minimal I-map). Despite the convergence result, it is still important to provide K2 with a good node order, since there are too many orderings (n! for n nodes) to search blindly among them, unless drastically greedy (myopic) search regimens are used.
Reference: 14. <author> Lauritzen, S. L., and Wermuth, N., </author> <title> Graphical models for associations between variables, some of which are qualitative and some quantitative, </title> <journal> Annals of Statistics, </journal> <volume> 17, </volume> <pages> 31-57, </pages> <year> 1989. </year>
Reference-contexts: We ignore Markov nets [13, chapter 3], chain graphs <ref> [14, 15] </ref>, and other graphical representations (e.g., [16, 17]). 115 the number of orderings to be searched without having to use dramatically greedy heuristics.
Reference: 15. <author> Lauritzen, S. L., and Wermuth, N., </author> <title> Graphical models for associations between variables, some of which are qualitative and some quantitative: Correction note, </title> <journal> Annals of Statistics, </journal> <volume> 17, </volume> <year> 1916, 1989. </year>
Reference-contexts: We ignore Markov nets [13, chapter 3], chain graphs <ref> [14, 15] </ref>, and other graphical representations (e.g., [16, 17]). 115 the number of orderings to be searched without having to use dramatically greedy heuristics.
Reference: 16. <author> Shachter, R. D., </author> <title> A graph-based inference method for conditional independence, </title> <booktitle> Proceedings of the 7th Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Los Angeles, </address> <publisher> Morgan Kaufmann, </publisher> <pages> 353-360, </pages> <year> 1991. </year>
Reference-contexts: We ignore Markov nets [13, chapter 3], chain graphs [14, 15], and other graphical representations (e.g., <ref> [16, 17] </ref>). 115 the number of orderings to be searched without having to use dramatically greedy heuristics.
Reference: 17. <author> Geiger, D., and Heckerman, D., </author> <booktitle> Advances in probabilistic reasoning, Proceedings of the 7th Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Los Angeles, </address> <publisher> Morgan Kaufmann, </publisher> <pages> 118-126, </pages> <year> 1991. </year>
Reference-contexts: We ignore Markov nets [13, chapter 3], chain graphs [14, 15], and other graphical representations (e.g., <ref> [16, 17] </ref>). 115 the number of orderings to be searched without having to use dramatically greedy heuristics.
Reference: 18. <author> Dor, D., and Tarsi, M., </author> <title> A simple algorithm to construct a consistent expression of a partially oriented graph, </title> <type> Technical Report R-185, </type> <institution> Cognitive Systems Laboratory, Department of Computer Science, Univ. of California at Los Angeles, </institution> <year> 1992. </year>
Reference-contexts: If an edge has already been oriented in the reverse direction, make that edge bidirected. 4. Try to assign directions to the yet undirected edges in G by applying the following four rules <ref> [5, 18] </ref> if this can be done without introducing directed cycles in the graph: Rule 1: If a ! b and b c and a and c are not adjacent, then direct b ! c.
Reference: 19. <author> Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P., </author> <title> Optimization by simulated annealing, </title> <journal> Science, </journal> <volume> 220, </volume> <pages> 671-680, </pages> <year> 1983. </year>
Reference-contexts: Although, this has the advantage of being computationally efficient, it does not ensure optimality even though the metric used by K2 is exact. Hence, we also explored the use of another search strategy, namely Simulated Annealing <ref> [19] </ref> though this has the disadvantage of being very computationally expensive. Simulated annealing [19] is a stochastic optimization technique used to find the maximum probability (minimum cost) configuration of some cost function corresponding to combinatorial problems with cost functions having many local minima. <p> Although, this has the advantage of being computationally efficient, it does not ensure optimality even though the metric used by K2 is exact. Hence, we also explored the use of another search strategy, namely Simulated Annealing <ref> [19] </ref> though this has the disadvantage of being very computationally expensive. Simulated annealing [19] is a stochastic optimization technique used to find the maximum probability (minimum cost) configuration of some cost function corresponding to combinatorial problems with cost functions having many local minima.
Reference: 20. <author> Aarts, E., and Korst, J., </author> <title> Simulated Annealing and Boltzmann Machines: a stochastic approach to combinatorial optimization and neural computing, </title> <publisher> John Wiley and Sons, </publisher> <year> 1989. </year>
Reference-contexts: For our experiments, we used a polynomial-time cooling schedule (which however does not give any guarantee for the deviation in cost between the final solution obtained by the algorithm and the optimal cost) described in <ref> [20] </ref>. 4. RESULTS The CB algorithm was implemented in C on a DEC Station 5000, and was used to reconstruct the underlying Bayesian network structure from four databases.
Reference: 21. <author> Beinlich, I. A., Suermondt, H. J., Chavez, R. M., and Cooper, G. F., </author> <title> The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks, </title> <booktitle> Proceedings of the 2nd European Conference on Artificial Intelligence in Medicine, </booktitle> <address> London, England, 247-256, </address> <year> 1989. </year>
Reference-contexts: RESULTS The CB algorithm was implemented in C on a DEC Station 5000, and was used to reconstruct the underlying Bayesian network structure from four databases. For two of the databases, namely the ALARM <ref> [21, 9, 10] </ref> and the LED [4] databases, the Bayesian network structure which had been used to generate the database was known. <p> For all tests, we used the 2 test with a fixed ff level (set to 0.1) for testing Conditional Independence. The results of the tests are described in the following sections: 4.1. The ALARM Database The ALARM network <ref> [21] </ref> was constructed as a prototype to model potential anesthesia problems that could arise in the operating room. The network contains a total of 37 nodes and 46 arcs representing 8 diagnostic problems, 16 findings and 13 intermediate variables that relate diagnostic problems to findings.
Reference: 22. <author> Murphy, P. M., and Aha, D. W., </author> <title> UCI repository of machine learning databases, </title> <institution> machine-readable data repository Department of Information and Computer Science, University of California, Irvine. </institution>
Reference-contexts: For this purpose, we selected two databases from the University of California (Irvine) Repository of Machine Learning Databases <ref> [22] </ref>, namely the LETTER RECOGNITION Database [23] and the SOYBEAN Database [24]. In these cases, since the true underlying Bayesian network is unknown, we used the shell HUGIN [25] to test the predictive accuracy of the constructed networks.
Reference: 23. <author> Frey, P. W., and Slate, D. J., </author> <title> Letter recognition using holland-style adaptive classifiers, </title> <journal> Machine Learning, </journal> <volume> 6(2), </volume> <year> 1991. </year>
Reference-contexts: For this purpose, we selected two databases from the University of California (Irvine) Repository of Machine Learning Databases [22], namely the LETTER RECOGNITION Database <ref> [23] </ref> and the SOYBEAN Database [24]. In these cases, since the true underlying Bayesian network is unknown, we used the shell HUGIN [25] to test the predictive accuracy of the constructed networks. <p> The algorithm recovered the original LED network (Figure 2 (a)) in this case using, once again, CI tests up to order 1. 4.3. The LETTER RECOGNITION Database The LETTER RECOGNITION database <ref> [23] </ref> was used to investigate the ability of several variations of Holland-style adaptive classifier systems 125 to learn to correctly identify each of a large number of black-and-white rectangular pixel displays (presented as 16-attribute vectors) as one of the 26 capital letters in the English alphabet. <p> However, while the network formed using greedy search was too large for inference, the network formed using annealing had a predictive accuracy of about 82.5%. The best predictive accuracy achieved by <ref> [23] </ref> (using 16000 cases for learning and 4000 cases for testing) was similar. On forcing the class variable to be the root of the resultant Bayesian network, both the probability and the predictive accuracy dropped (though they were the same for the two search strategies)(Figure 3 (a)).
Reference: 24. <author> Michalski, R. S., and Chilausky, R. L., </author> <title> Learning by being told and learning from examples: An experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis, </title> <journal> International Journal of Policy Analysis and Information Systems, </journal> <volume> 4(2), </volume> <year> 1980. </year>
Reference-contexts: For this purpose, we selected two databases from the University of California (Irvine) Repository of Machine Learning Databases [22], namely the LETTER RECOGNITION Database [23] and the SOYBEAN Database <ref> [24] </ref>. In these cases, since the true underlying Bayesian network is unknown, we used the shell HUGIN [25] to test the predictive accuracy of the constructed networks. <p> The predictive accuracy obtained was around 80.5%. 4.4. The SOYBEAN Database The SOYBEAN database is one the most widely used databases in the domain of machine learning. Created by Michalski and Chilausky <ref> [24] </ref>, the 36 variable database consists of a 307 case training set and a 376 case evaluation set in the domain of soybean-plant-disease diagnosis. There are 19 classes, only the first 15 of which have been used in prior work. <p> Herskovits [9] also discusses in detail the possible reasons why the network generated by the K2 algorithm had a predictive accuracy much lower than the 98% accuracy achieved by <ref> [24] </ref>. Similar arguments would explain why the CB algorithm generated networks with much lower predictive accuracy, since both algorithms are based on the same metric. 5.
Reference: 25. <author> Anderson, S. K., Olesen, K. G., Jensen, F. V., and Jensen, F., </author> <title> HUGIN A shell for building Bayesian belief universes for expert systems, </title> <booktitle> Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1080-1085, </pages> <year> 1989. </year>
Reference-contexts: For this purpose, we selected two databases from the University of California (Irvine) Repository of Machine Learning Databases [22], namely the LETTER RECOGNITION Database [23] and the SOYBEAN Database [24]. In these cases, since the true underlying Bayesian network is unknown, we used the shell HUGIN <ref> [25] </ref> to test the predictive accuracy of the constructed networks.
Reference: 26. <author> Henrion, M., </author> <title> Propagating Uncertainty in Bayesian networks by probabilistic logic sampling, </title> <booktitle> in Uncertainty in Artificial Intelligence 2 (J. </booktitle> <editor> F. Lemmer, 131 L. N. Kanal, Eds.), </editor> <publisher> Elsevier Science, North-Holland, </publisher> <pages> 149-163, </pages> <year> 1988. </year>
Reference-contexts: We used the CB algorithm to reconstruct the ALARM network (Figure 1 (a)) by using 10,000 cases of a (20,000 case) database generated by Herskovits [9, 10] from the original network by using a Monte Carlo technique called probabilistic logic sampling <ref> [26] </ref>. We used a bound of 15 on the maximum degree of the undirected graph generated in step 2. The algorithm recovered the network shown in Figure 1 (b) using CI tests up to only order 2.
Reference: 27. <author> Spirtes, P., </author> <type> Personal communication. </type>
Reference-contexts: Cooper and Herskovits [10] reported that K2, when given a total ordering consistent with the partial order of the nodes as specified by ALARM, recovered the complete network with the exception of one missing edge (between nodes 12 and 32) and one extra arc (from node 15 to 34). <ref> [27] </ref> reported similar results with the PC algorithm. They applied the PC algorithm [6] to the ALARM database split into two parts of 10000 cases each. The algorithm did not make any linearity assumption.
Reference: 28. <author> Buntine, W., </author> <title> Theory refinement on Bayesian networks, </title> <booktitle> Proceedings of the 7th Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Los Angeles, </address> <publisher> Morgan Kaufmann, </publisher> <pages> 52-60, </pages> <year> 1991. </year>
Reference-contexts: The results are shown in Figure 4. 7 Other methods of treating missing values are described in <ref> [28, 29] </ref> 126 When no prior information was used, the networks generated (with greedy search as well as with simulated annealing) were ranked as much more probable than the networks generated with the corresponding search strategy by forcing the class variable to be the root of the resultant network structure.
Reference: 29. <author> Dempster, A., Laird, N., and Rubin, D., </author> <title> Maximum likelihood from incomplete data via the EM algorithm, </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 39, </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The results are shown in Figure 4. 7 Other methods of treating missing values are described in <ref> [28, 29] </ref> 126 When no prior information was used, the networks generated (with greedy search as well as with simulated annealing) were ranked as much more probable than the networks generated with the corresponding search strategy by forcing the class variable to be the root of the resultant network structure.
Reference: 30. <author> Draper, D., </author> <title> Assessment and propagation of model uncertainty, </title> <booktitle> Preliminary Papers of the 4th International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, 497-509, </address> <month> January 3-6 </month> <year> 1993. </year>
Reference-contexts: This would be reasonable only if the most probable network given the database of cases has a much higher probability as compared to the next most probable network given the data. However, this assumption may not be valid in many practical situations. Many authors (e.g., <ref> [30, 31] </ref>) have stressed the importance of (and have offered possible solutions) accounting for model uncertainty. <p> However, since we cannot possibly find the set of all possible models and average over them, additional work is needed to develop methods of finding a tractable number of models which would give a reasonable approximation to the actual solution. Solutions proposed by Madigan and Raftery [31] and Draper <ref> [30] </ref> though feasible for small networks might be computationally very expensive for networks having a large number of variables. Secondly, we have used a fixed ff level for the 2 test. This will almost certainly introduce dependencies that are purely the result of chance.
Reference: 31. <author> Madigan, D., and Raftery, A. E., </author> <title> Model selection and accounting for model uncertainty in graphical models using occam's window, </title> <type> Technical Report 213 (revised), </type> <institution> Dept. of Statistics, Univ. of Washington, </institution> <year> 1993. </year>
Reference-contexts: This would be reasonable only if the most probable network given the database of cases has a much higher probability as compared to the next most probable network given the data. However, this assumption may not be valid in many practical situations. Many authors (e.g., <ref> [30, 31] </ref>) have stressed the importance of (and have offered possible solutions) accounting for model uncertainty. <p> However, this assumption may not be valid in many practical situations. Many authors (e.g., [30, 31]) have stressed the importance of (and have offered possible solutions) accounting for model uncertainty. As Madigan and Raftery <ref> [31] </ref> state, "a panacea is provided by the standard Bayesian formalism which averages the posterior distribution of the quantity of interest under each of the models, weighted by their posterior model probabilities". <p> However, since we cannot possibly find the set of all possible models and average over them, additional work is needed to develop methods of finding a tractable number of models which would give a reasonable approximation to the actual solution. Solutions proposed by Madigan and Raftery <ref> [31] </ref> and Draper [30] though feasible for small networks might be computationally very expensive for networks having a large number of variables. Secondly, we have used a fixed ff level for the 2 test. This will almost certainly introduce dependencies that are purely the result of chance.
References-found: 31

