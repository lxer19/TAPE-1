URL: http://www.cs.uoregon.edu/~jens/workload.ps
Refering-URL: http://www.cs.uoregon.edu/~jens/research.html
Root-URL: http://www.cs.uoregon.edu
Email: jens, kurtwg@cs.uoregon.edu  
Title: A Comparative Study of Real Workload Traces and Synthetic Workload Models for Parallel Job Scheduling  
Author: Virginia Lo, Jens Mache and Kurt Windisch 
Keyword: workload models, workload characterization, trace-driven simulation, performance evaluation, parallel job scheduling.  
Note: This research was sponsored by NSF grant MIP-9108528.  
Address: OR 97403  
Affiliation: Department of Computer and Information Science University of Oregon, Eugene,  
Abstract: Two basic approaches are taken when modeling workloads in simulation-based performance evaluation of parallel job scheduling algorithms: (1) a carefully reconstructed trace from a real supercomputer can provide a very realistic job stream, or (2) a flexible synthetic model that attempts to capture the behavior of observed workloads can be devised. Both approaches require that accurate statistical observations be made and that the researcher be aware of the applicability of a given trace for his or her experimental goals. In this paper, we compare a number of real workload traces and synthetic workload models currently used to evaluate job scheduling and allocation strategies. Our results indicate that the choice of workload model alone real workload trace versus synthetic workload models did not significantly affect the relative performance of the algorithms in this study (two scheduling algorithms and three static processor allocation algorithms). Almost all traces and models gave the same ranking of algorithms from best to worst. However, two specific workload characteristics were found to significantly affect algorithm performance: (a) proportion of power-of-two job sizes and (b) degree of correlation between job size and job runtime. When used in the experimental evaluation of resource management algorithms, workloads differing in these two characteristics may lead to discrepant conclusions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. H. Arpaci, A. C. Dusseau, A. M. Vahdat, L. T. Liu, T. E. Anderson, and D. A. Patterson. </author> <title> The interaction of parallel and sequential workloads on a network of workstations. </title> <booktitle> In Proceedings SIGMETRICS'95, </booktitle> <year> 1995. </year>
Reference-contexts: This project is distinguished from our previous work in that we evaluate the experimental method, not the scheduling techniques themselves. Some of the first researchers to use real traces from production machines to drive their simulations include <ref> [18, 1, 17] </ref>. At the same time, analysis of this emerging body of trace data was conducted by Feitelson and Downey in the development of realistic synthetic workload models.
Reference: [2] <author> S. H. Chiang, R. K. Mansharamini, and M. K. Vernon. </author> <title> Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies. </title> <booktitle> In Proceedings SIGMETRICS'94, </booktitle> <year> 1994. </year>
Reference-contexts: As we shall see, our work further explains some of the phenomena he observed. The only recent study that we know of that has focused on the experimental methodology itself, i.e., the effect choice of workload has on scheduling performance results, is that of Chiang et. al. <ref> [2] </ref>. They compare the performance of several scheduling strategies over a wide range of workload parameters and conclude that the discrepancies among various studies are due to differences in the (synthetic) workloads used for performance evaluation.
Reference: [3] <author> P. J. Chuang and N. F. Tzeng. </author> <title> An efficient submesh allocation strategy for mesh computer systems. </title> <booktitle> In Proceedings IEEE International Conference on Distributed Computer Systems, </booktitle> <pages> pages 256-263, </pages> <year> 1991. </year>
Reference-contexts: Included were two scheduling algorithms: First Come First Served and ScanUp [15], a multi-level queueing algorithm, and three static allocation strategies: First Fit [23], Frame Sliding <ref> [3] </ref> , and Paging [16]. The real traces were captured from four production machines in use for scientific computing at research labs and supercomputer sites around the world (two IBM SP-2s, an Intel Paragon, and a Cray T3E). <p> ScanUp was shown to outperform a wide range of scheduling algorithms. Allocation Strategies Static allocation strategies can be classified as contiguous or non-contiguous, based on whether or not the set of allocated processors are directly connected by communication links in the interconnection network. * Frame Sliding <ref> [3] </ref> searches for a rectangular block of processors by sliding a window of the desired size across the mesh in horizontal and vertical strides based on the width and height of the requested rectangle.
Reference: [4] <author> Intel Corp. </author> <title> Paragon Network Queuing System manual. </title> <month> October </month> <year> 1993. </year>
Reference-contexts: The job scheduling strategy involves the decision about which of many queued jobs is next to be allocated resources. Job scheduling policies range from the classic First-Come First Served algorithm to complex, multi-level queue models such as those implemented by scheduling systems such as NQS, Load Leveler, or EASY <ref> [4, 13, 12] </ref>. The job allocation strategy selects the set of processors to be allocated to the job based on its jobsize request.
Reference: [5] <author> A. B. Downey. </author> <title> A parallel workload model and its implications for processor allocation. </title> <booktitle> In Proceedings HPDC'97, </booktitle> <year> 1997. </year>
Reference-contexts: The real traces were captured from four production machines in use for scientific computing at research labs and supercomputer sites around the world (two IBM SP-2s, an Intel Paragon, and a Cray T3E). The synthetic models include "naive" models and those developed by Downey <ref> [6, 5] </ref> and Feitelson [7] based on their careful analyses of traces from production machines. <p> Feitelson's model combines observations of five different parallel supercomputers in the evaluation of gang scheduling strategies [7], and Downey's model is based on detailed analysis of the SDSC Paragon, utilized in experiments evaluating adaptive job scheduling algorithms <ref> [6, 5] </ref>. The synthetic workload models developed by Downey and Feitelson are used in our experiments and are discussed in more detail in Section 4. Several studies statistically analyzed workload traces from production use of real parallel supercomputers. <p> We note that Feitelson chose to use an exponential distribution to model interarrival times; our own studies of workload data have shown this assumption to be strongly justified [20]. Allen Downey <ref> [6, 5] </ref> focused his analysis on two machines, the SDSC Paragon and the CTC SP-2. He proposed a uniform-log distribution for modeling job lifetimes (approximated by the product of runtime and number of processors).
Reference: [6] <author> A. B. Downey. </author> <title> Predicting queue times on space-sharing parallel computers. </title> <booktitle> In Proceedings of the 3rd Workshop on Job Scheduling Strategies for Parallel Processing, IPPS '97, </booktitle> <year> 1997. </year>
Reference-contexts: The real traces were captured from four production machines in use for scientific computing at research labs and supercomputer sites around the world (two IBM SP-2s, an Intel Paragon, and a Cray T3E). The synthetic models include "naive" models and those developed by Downey <ref> [6, 5] </ref> and Feitelson [7] based on their careful analyses of traces from production machines. <p> Feitelson's model combines observations of five different parallel supercomputers in the evaluation of gang scheduling strategies [7], and Downey's model is based on detailed analysis of the SDSC Paragon, utilized in experiments evaluating adaptive job scheduling algorithms <ref> [6, 5] </ref>. The synthetic workload models developed by Downey and Feitelson are used in our experiments and are discussed in more detail in Section 4. Several studies statistically analyzed workload traces from production use of real parallel supercomputers. <p> We note that Feitelson chose to use an exponential distribution to model interarrival times; our own studies of workload data have shown this assumption to be strongly justified [20]. Allen Downey <ref> [6, 5] </ref> focused his analysis on two machines, the SDSC Paragon and the CTC SP-2. He proposed a uniform-log distribution for modeling job lifetimes (approximated by the product of runtime and number of processors).
Reference: [7] <author> D. Feitelson. </author> <title> Packing schemes for gang scheduling. </title> <booktitle> In Proceedings of the 2nd Workshop on Job Scheduling Strategies for Parallel Processing, IPPS '96, </booktitle> <year> 1996. </year>
Reference-contexts: The real traces were captured from four production machines in use for scientific computing at research labs and supercomputer sites around the world (two IBM SP-2s, an Intel Paragon, and a Cray T3E). The synthetic models include "naive" models and those developed by Downey [6, 5] and Feitelson <ref> [7] </ref> based on their careful analyses of traces from production machines. <p> At the same time, analysis of this emerging body of trace data was conducted by Feitelson and Downey in the development of realistic synthetic workload models. Feitelson's model combines observations of five different parallel supercomputers in the evaluation of gang scheduling strategies <ref> [7] </ref>, and Downey's model is based on detailed analysis of the SDSC Paragon, utilized in experiments evaluating adaptive job scheduling algorithms [6, 5]. The synthetic workload models developed by Downey and Feitelson are used in our experiments and are discussed in more detail in Section 4. <p> For the most part, performance evaluation studies have relied on "naive" synthetic models, using classic probability distributions such as exponential and uniform. Dror Feitelson <ref> [7] </ref> analyzed trace data from five of the above machines, specifically the NASA iPSC/860, ANL IBM SP-1, SDSC Paragon, LLNL Butterfly, and ETH Paragon. His goal was to derive probabilistic models for use in his experiments with gang scheduling algorithms. <p> We used Pearson's r, which presumes a linear relationship between the variables, to compute the correlations. In reality, this relationship does not necessarily hold. Feitelson <ref> [7] </ref> noted a strong positive correlation for the NAS iPSC/860 trace but found much weaker relationships for other traces.
Reference: [8] <author> D. G. Feitelson. </author> <title> A survey of scheduling in multipro-grammed parallel systems. </title> <type> Technical Report RC 19790 (87657), </type> <institution> IBM Research Division, T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Research in job scheduling and processor allocation is thoroughly surveyed in <ref> [8] </ref>; more recent work in this area includes that reported in [10] as well as our own [16, 21, 17]. This project is distinguished from our previous work in that we evaluate the experimental method, not the scheduling techniques themselves.
Reference: [9] <author> D. G. Feitelson and B. Nitzberg. </author> <title> Job characteristics of a production parallel scientific workload on the NASA Ames iPSC/860. </title> <booktitle> In Proceedings of the 1st. Workshop on Job Scheduling Strategies for Parallel Processing, IPPS '95, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Several studies statistically analyzed workload traces from production use of real parallel supercomputers. Feitelson and Nitzberg analyzed the workload from an Intel iPSC/860 located at NASA Ames, providing the first widely available workload measurements from a real system <ref> [9] </ref>.
Reference: [10] <author> D. G. Feitelson and L. Rudolph, </author> <title> editors. Job Scheduling Strategies for Parallel Processing. </title> <booktitle> Springer Lecture Notes in Computer Science, </booktitle> <pages> 1995-1997. </pages>
Reference-contexts: Research in job scheduling and processor allocation is thoroughly surveyed in [8]; more recent work in this area includes that reported in <ref> [10] </ref> as well as our own [16, 21, 17]. This project is distinguished from our previous work in that we evaluate the experimental method, not the scheduling techniques themselves. Some of the first researchers to use real traces from production machines to drive their simulations include [18, 1, 17].
Reference: [11] <author> S. Hotovy. </author> <title> Workload evolution on the Cornell Theory Cen--ter IBM SP2. </title> <booktitle> In Proceedings of the 2nd Workshop on Job Scheduling Strategies for Parallel Processing, IPPS '96, </booktitle> <year> 1996. </year> <title> [12] http://www.llnl.gov/liv comp/dpcs/. LLNL distributed production control system. [13] http://www.tc.cornell.edu/Papers/abdullah.jul96/ index.html. Extensible Argonne scheduler system (EASY). </title>
Reference-contexts: Overall, the profiles of the two workloads were surprisingly similar. Hotovy analyzed the evolution of the workload on an IBM SP-2 at the Cornell Theory Center (CTC), concluding that workloads change in significant ways over time, requiring adaptations in the scheduling mechanisms for efficient operation <ref> [11] </ref>. A few of the many studies of job scheduling and processor allocation algorithms have offered insights into the effects of workload characteristics on those algorithms. Of special interest to our study is Krueger's work on scheduling and allocation performance under workloads exhibiting negative correlations between jobsize and runtimes [15]. <p> The traces come from two periods: July 1994-March 1995 with scheduling managed by IBM's LoadLeveller and July 1995-Feb 1996 under LoadLeveller and Easy <ref> [11] </ref>. * NASA Ames IBM SP-2: The NASA Ames IBM SP-2 machine has 160 nodes connected by a high performance switch. The traces cover two years, from August 1995 to August 1997.
Reference: [14] <author> J. Jones. </author> <title> NASA Ames NAS, </title> <type> Personal communication, </type> <year> 1997. </year>
Reference-contexts: The traces cover two years, from August 1995 to August 1997. The scheduling policies are implemented through the Portable Batch System (PBS) <ref> [14] </ref>. * KFA Cray T3E: The Forschungszentrum Julich has a 512 node CRAY T3E. Jobs are submitted through NQS. Job queues differ in maximum (power-of-two) jobsize and maximum run-time. The (static) allocation algorithm is contiguous.
Reference: [15] <author> P. Krueger, T. Lai, and V. A. Dixit-Radiya. </author> <title> Job scheduling is more important than processor allocation for hypercube computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(5) </volume> <pages> 488-497, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Included were two scheduling algorithms: First Come First Served and ScanUp <ref> [15] </ref>, a multi-level queueing algorithm, and three static allocation strategies: First Fit [23], Frame Sliding [3] , and Paging [16]. <p> A few of the many studies of job scheduling and processor allocation algorithms have offered insights into the effects of workload characteristics on those algorithms. Of special interest to our study is Krueger's work on scheduling and allocation performance under workloads exhibiting negative correlations between jobsize and runtimes <ref> [15] </ref>. As we shall see, our work further explains some of the phenomena he observed. The only recent study that we know of that has focused on the experimental methodology itself, i.e., the effect choice of workload has on scheduling performance results, is that of Chiang et. al. [2]. <p> Job Scheduling Strategies * FCFS is the classic First Come First Served Scheduling Algorithm. FCFS is a simple, single queue algorithm commonly used as a standard of comparison or as a default algorithm. * ScanUp <ref> [15] </ref> is a multi-queue job scheduling strategy in which jobs are queued by jobsize. The scheduler services one queue at a time, from smallest to largest, serving only those jobs that arrived in a given queue before it selected that queue (thereby avoiding starvation). <p> Another interesting phenomenon is the fact that with a 100% power-of-two job mix, it is the scheduling strategy (not allocation strategy) that determines performance. ScanUp outperforms First Come First Serve, regardless of allocation strategy. This result is consistent with that of <ref> [15] </ref> in their experiments with scheduling and allocation algorithms for the hypercube. 5.5.2 Effects of degree of correlation between jobsize and runtime Degree of correlation between jobsize and runtime is of interest in the scheduling community because it reflects certain assumptions about the work model and the type of scheduling algorithms
Reference: [16] <author> V. M. Lo, K. Windisch, W. Liu, and B. Nitzberg. </author> <title> Non-contiguous processor allocation algorithms for mesh-connected multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 8(7) </volume> <pages> 712-726, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: Included were two scheduling algorithms: First Come First Served and ScanUp [15], a multi-level queueing algorithm, and three static allocation strategies: First Fit [23], Frame Sliding [3] , and Paging <ref> [16] </ref>. The real traces were captured from four production machines in use for scientific computing at research labs and supercomputer sites around the world (two IBM SP-2s, an Intel Paragon, and a Cray T3E). <p> Research in job scheduling and processor allocation is thoroughly surveyed in [8]; more recent work in this area includes that reported in [10] as well as our own <ref> [16, 21, 17] </ref>. This project is distinguished from our previous work in that we evaluate the experimental method, not the scheduling techniques themselves. Some of the first researchers to use real traces from production machines to drive their simulations include [18, 1, 17]. <p> Frame Sliding is a contigu ous strategy with marginal performance. * First Fit [23] searches for a contiguous block of processors starting at a reference point (e.g. lower left hand corner of the mesh) First Fit was shown to have the best performance among all contigu ous strategies in <ref> [16] </ref>. * Paging allocates processors by scanning the free list of processors in a fixed order and allocating them to the job without regard to their contiguity. Paging was shown to have the best performance among all non-contiguous allocation strategies in [16] 4 . <p> the best performance among all contigu ous strategies in <ref> [16] </ref>. * Paging allocates processors by scanning the free list of processors in a fixed order and allocating them to the job without regard to their contiguity. Paging was shown to have the best performance among all non-contiguous allocation strategies in [16] 4 .
Reference: [17] <author> J. Mache and V. Lo. </author> <title> Minimizing message-passing contention in fragmentation-free processor allocation. </title> <booktitle> In Proceedings International Conference on Parallel and Distributed Computing Systems, </booktitle> <year> 1997. </year>
Reference-contexts: Research in job scheduling and processor allocation is thoroughly surveyed in [8]; more recent work in this area includes that reported in [10] as well as our own <ref> [16, 21, 17] </ref>. This project is distinguished from our previous work in that we evaluate the experimental method, not the scheduling techniques themselves. Some of the first researchers to use real traces from production machines to drive their simulations include [18, 1, 17]. <p> This project is distinguished from our previous work in that we evaluate the experimental method, not the scheduling techniques themselves. Some of the first researchers to use real traces from production machines to drive their simulations include <ref> [18, 1, 17] </ref>. At the same time, analysis of this emerging body of trace data was conducted by Feitelson and Downey in the development of realistic synthetic workload models. <p> of processors allocated to jobs at any given time, averaged over the entire workload. * average response time: the elapsed time from when a job arrives for scheduling to when it completes execution, averaged over the entire work 4 Since that time we have developed a superior algorithm called MC <ref> [17] </ref> that is contiguous when a contiguous block ex ists, but non-contiguous otherwise.
Reference: [18] <author> J. Subhlok, T. Gross, and T. Suzuoka. </author> <title> Impacts of job mix on optimizations for space sharing schedulers. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <year> 1996. </year>
Reference-contexts: This project is distinguished from our previous work in that we evaluate the experimental method, not the scheduling techniques themselves. Some of the first researchers to use real traces from production machines to drive their simulations include <ref> [18, 1, 17] </ref>. At the same time, analysis of this emerging body of trace data was conducted by Feitelson and Downey in the development of realistic synthetic workload models.
Reference: [19] <author> M. Wan, R. Moore, G. Kremenek, and K. Steube. </author> <title> A batch scheduler for the Intel Paragon MPP system with a noncontiguous node allocation algorithm. </title> <booktitle> In Proceedings of the 2nd Workshop on Job Scheduling Strategies for Parallel Processing, IPPS '96, </booktitle> <year> 1996. </year>
Reference-contexts: The (static) allocation algorithm is a block-based noncontiguous strategy. The SDSC traces were taken from 1995-1996, in three-month groups. <ref> [20, 19] </ref> * CTC IBM SP-2: The Cornell Theory Center IBM SP-2 machine has 512 nodes connected by a high performance switch.
Reference: [20] <author> K. Windisch, V. Lo, D. Feitelson, B. Nitzberg, and R. Moore. </author> <title> A comparison of workload traces from two production parallel machines. </title> <booktitle> In Proceedings Fifth Symposium of Massively Parallel Computation, </booktitle> <year> 1996. </year>
Reference-contexts: Several studies statistically analyzed workload traces from production use of real parallel supercomputers. Feitelson and Nitzberg analyzed the workload from an Intel iPSC/860 located at NASA Ames, providing the first widely available workload measurements from a real system [9]. Windisch et. al. <ref> [20] </ref> continued this effort by analyzing traces from the In 1 Adaptive allocation strategies for moldable jobs allocate a number of processors that is a function of the number of free processors in the system and of characteristics of the job. tel Paragon at the San Diego Supercomputer Center (SDSC), and <p> The (static) allocation algorithm is a block-based noncontiguous strategy. The SDSC traces were taken from 1995-1996, in three-month groups. <ref> [20, 19] </ref> * CTC IBM SP-2: The Cornell Theory Center IBM SP-2 machine has 512 nodes connected by a high performance switch. <p> We note that Feitelson chose to use an exponential distribution to model interarrival times; our own studies of workload data have shown this assumption to be strongly justified <ref> [20] </ref>. Allen Downey [6, 5] focused his analysis on two machines, the SDSC Paragon and the CTC SP-2. He proposed a uniform-log distribution for modeling job lifetimes (approximated by the product of runtime and number of processors).
Reference: [21] <author> K. Windisch, V. M. Lo, and B. Bose. </author> <title> Contiguous and non-contiguous processor allocation algorithms for k-ary n-cubes. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: Research in job scheduling and processor allocation is thoroughly surveyed in [8]; more recent work in this area includes that reported in [10] as well as our own <ref> [16, 21, 17] </ref>. This project is distinguished from our previous work in that we evaluate the experimental method, not the scheduling techniques themselves. Some of the first researchers to use real traces from production machines to drive their simulations include [18, 1, 17].
Reference: [22] <author> K. Windisch, J. V. Miller, and V. M. Lo. Procsimity: </author> <title> an experimental tool for processor allocation and scheduling in highly parallel systems. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 414-421, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: For studies involving the CTC workload, we used a 16 fi 27 mesh to match the size (but not the topology) of the CTC SP-2 machine. For the experiment investigating power of two jobsizes, we used a 32 fi 32 mesh. All experiments were conducted using ProcSimity <ref> [22] </ref>, a simulation tool we developed for evaluating job scheduling and processor allocation algorithms for distributed memory parallel machines. ProcSimity models a variety of network topologies and several current flow control and routing technologies. ProcSimity supports both synthetic job streams and trace-driven simulation.
Reference: [23] <author> Y. Zhu. </author> <title> Efficient processor allocation strategies for mesh-connected parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 328-337, </pages> <year> 1992. </year>
Reference-contexts: Included were two scheduling algorithms: First Come First Served and ScanUp [15], a multi-level queueing algorithm, and three static allocation strategies: First Fit <ref> [23] </ref>, Frame Sliding [3] , and Paging [16]. The real traces were captured from four production machines in use for scientific computing at research labs and supercomputer sites around the world (two IBM SP-2s, an Intel Paragon, and a Cray T3E). <p> Frame Sliding is a contigu ous strategy with marginal performance. * First Fit <ref> [23] </ref> searches for a contiguous block of processors starting at a reference point (e.g. lower left hand corner of the mesh) First Fit was shown to have the best performance among all contigu ous strategies in [16]. * Paging allocates processors by scanning the free list of processors in a fixed
References-found: 21

