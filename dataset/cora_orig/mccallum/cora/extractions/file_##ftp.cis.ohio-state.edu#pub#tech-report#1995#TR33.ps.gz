URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1995/TR33.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Title: Necessary and Sufficient Conditions on Information for Causal Message Ordering and Their Optimal Implementation  
Author: Ajay D. Kshemkalyani P. O. Mukesh Singhal 
Keyword: Key Words: Causal message ordering, distributed systems, optimal, synchronization, concurrency.  
Address: Box 12195  Triangle Park NC 27709  2015 Neil Avenue Columbus, OH 43210  
Affiliation: IBM Corporation  Research  Department of Computer and Information Science The Ohio State University  
Abstract: This paper formulates invariants that represent necessary and sufficient conditions on the information required for enforcing causal ordering. The paper then presents an optimal algorithm for enforcing causal message ordering. The algorithm works with non-FIFO channels and allows a process to multicast to arbitrary and dynamically changing process groups. We show that the algorithm satisfies the invariants and is optimal both in space complexity of message overheads and in space complexity of message logs. In general, the space complexity of causal message ordering is shown to be O(n 2 ) where n is the number of nodes in the system. The algorithm achieves optimality by transmitting the bare minimum causal dependency information specified by the necessity condition, and using an encoding scheme to represent and transmit this information. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Alagar, S. Venkatesan, </author> <title> An Optimal Algorithm for Distributed Snapshots with Causal Message Ordering, </title> <journal> Information Processing Letters, </journal> <volume> 50, </volume> <pages> 311-316, </pages> <year> 1994. </year>
Reference-contexts: Note that causal ordering property is strictly stronger than the FIFO property. The concept of causal ordering is of considerable interest to the design of distributed systems and finds applications in several domains such as updates of replicated data [5, 8], global state collection <ref> [1] </ref>, distributed shared memory [3], teleconferencing [14], multimedia systems [2], and fair resource allocation [10]. Causal message ordering requires appending some control information with each message to enforce the causal order.
Reference: [2] <author> F. Adelstein, M. Singhal, </author> <title> Real-Time Causal Message Ordering in Multimedia Systems, </title> <booktitle> Proc. 15th Intl. Conf. on Distributed Computing Systems, May-June 1995. </booktitle> <pages> 42 </pages>
Reference-contexts: The concept of causal ordering is of considerable interest to the design of distributed systems and finds applications in several domains such as updates of replicated data [5, 8], global state collection [1], distributed shared memory [3], teleconferencing [14], multimedia systems <ref> [2] </ref>, and fair resource allocation [10]. Causal message ordering requires appending some control information with each message to enforce the causal order. <p> Thus, at most n (sender, timestamp) pairs are required. Enforcing CO for paths of length two Corollary 3 Under general message communication patterns, CO over causal dependency chains of length two can be enforced with a message overhead of O (n) <ref> [2] </ref>. Proof: When sending a message from i to j, i sends a message identifier for (1) the latest message it has sent to each other node (n identifiers) and for (2) the latest message from each other node to j that it is aware of (n identifiers). <p> node k (including i to k), k can enforce CO for paths of length two. 2 38 Enforcing CO for paths of length &gt; 2 Corollary 4 Under general message communication patterns, enforcing CO over causal dependency chains of length greater than two requires a message overhead of n 2 <ref> [2] </ref>. Proof: Theorem 8 used a construction that shows this result. 2 9 FIFO-Channels Case When the communication channels are FIFO, the following optimization must be performed for optimality.
Reference: [3] <author> M. Ahamad, P. Hutto, R. John, </author> <title> Implementing and Programming Causal Distributed Memory, </title> <booktitle> Proceedings of the 11th Intl. Conf. on Distributed Computing Systems, </booktitle> <pages> 274-281, </pages> <year> 1991. </year>
Reference-contexts: Note that causal ordering property is strictly stronger than the FIFO property. The concept of causal ordering is of considerable interest to the design of distributed systems and finds applications in several domains such as updates of replicated data [5, 8], global state collection [1], distributed shared memory <ref> [3] </ref>, teleconferencing [14], multimedia systems [2], and fair resource allocation [10]. Causal message ordering requires appending some control information with each message to enforce the causal order.
Reference: [4] <author> K. Birman, T. Joseph, </author> <title> Reliable Communication in Presence of Failures, </title> <journal> ACM Trans actions on Computer Systems, </journal> <volume> 5(1), </volume> <pages> 47-76, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: V Asynchronous execution of processes and unpredictable communication delays create nondeterminism in distributed systems that complicates the design, verification, and analysis of distributed programs. To simplify the design and development of distributed applications, the idea of "causal message ordering" was introduced by Joseph and Birman <ref> [4] </ref>. Definition 2 (Causal ordering (CO)): If for any two messages M and M 0 , Send (M ) ! Send (M 0 ) and M and M 0 have the same destination, then causal ordering ensures that Delivery (M ) ! Delivery (M 0 ). <p> The recipient process of a message uses this information to determine if there are undelivered messages that causally precede this message and delays the delivery of this message until all such messages have been delivered. Previous Work In the first ISIS system implementation of CO <ref> [4] </ref>, a message carries a history of all the messages that causally precede it. Due to redundant information, this scheme is resilient to processor crashes; however, a complex mechanism is required to prevent unbounded growth of the control information. In any case, the volume of control information can be huge. <p> Due to redundant information, this scheme is resilient to processor crashes; however, a complex mechanism is required to prevent unbounded growth of the control information. In any case, the volume of control information can be huge. The CO algorithm in [13] is similar to <ref> [4] </ref> but carries message-ids rather than entire messages in the control information. Furthermore, unnecessary control information is not sent if the sending host had sent it before. The control information in the Schiper-Eggli-Sandoz causal ordering algorithm [18] consists of n vectors of length upto n each.
Reference: [5] <author> K. Birman, T. Joseph, </author> <title> Exploiting Replication in Distributed Systems, </title> <editor> in S. Mullender, ed., </editor> <booktitle> Distributed Systems, (ACM, </booktitle> <address> New York, </address> <year> 1990). </year>
Reference-contexts: Note that causal ordering property is strictly stronger than the FIFO property. The concept of causal ordering is of considerable interest to the design of distributed systems and finds applications in several domains such as updates of replicated data <ref> [5, 8] </ref>, global state collection [1], distributed shared memory [3], teleconferencing [14], multimedia systems [2], and fair resource allocation [10]. Causal message ordering requires appending some control information with each message to enforce the causal order.
Reference: [6] <author> K. Birman, A. Schiper, P. Stephenson, </author> <title> Lightweight Causal and Atomic Group Multi cast, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(3), </volume> <year> 1991, </year> <pages> 272-314. </pages>
Reference-contexts: The implementation of these nice properties results in the exchange of additional messages and typically incurs additional delays in the delivery of messages. In the causal multicast in overlapping groups implementation of ISIS <ref> [6] </ref>, every process maintains a vector for every group whether it belongs to that group or not. A vector for a group informs the process of the number of messages multicast by the various members of the group.
Reference: [7] <author> C. A. Fidge, </author> <title> Timestamps in Message-Passing Systems That Preserve Partial Ordering, </title> <journal> Australian Computer Science Communications, </journal> <volume> Vol. 10, No. 1, </volume> <pages> 56-66, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: The control information in the Schiper-Eggli-Sandoz causal ordering algorithm [18] consists of n vectors of length upto n each. This information represents messages sent in the causal past that are not known to be delivered. The receiving site uses vector time <ref> [7, 11] </ref> to determine whether messages represented in the control vectors need to be delivered before the current message is delivered. The causal ordering algorithm of Raynal-Schiper-Toueg [15] attaches a matrix SENT of size n fi n with every message. <p> Each node i maintains a counter, clock i , that is incremented on the occurrence of each local communication event. We do not require either Lamport's scalar clocks [10], or vector <ref> [7, 11] </ref> or matrix clocks [16] in the algorithm. Let (i; a) denote the event at local time a on node i. A message M sent by i at local time a is denoted as M i;a . The destinations of this multicast are denoted by M i;a :Dests. <p> A similar notion was previously used in [19] to efficiently implement vector clocks <ref> [7, 11] </ref>. The CO algorithm for FIFO channels is now given, with the changes to the original algorithm underlined.
Reference: [8] <author> T. Joseph, K. Birman, </author> <title> Low Cost Management of Replicated Data in Fault-Tolerant Distributed Systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(1), </volume> <pages> 54-70, </pages> <year> 1986. </year>
Reference-contexts: Note that causal ordering property is strictly stronger than the FIFO property. The concept of causal ordering is of considerable interest to the design of distributed systems and finds applications in several domains such as updates of replicated data <ref> [5, 8] </ref>, global state collection [1], distributed shared memory [3], teleconferencing [14], multimedia systems [2], and fair resource allocation [10]. Causal message ordering requires appending some control information with each message to enforce the causal order.
Reference: [9] <author> J. Kim, C. Kim, </author> <title> An Efficient Causal Ordering Protocol in Group Communications, </title> <type> Tech. Report, </type> <institution> Pohang University of Science and Technology, Korea, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Unlike [17], the algorithm proposed here does not require the knowledge of the topology of the underlying network; instead, it uses the dynamic communication pattern and structure of the computation to reduce the message overhead. The algorithm in <ref> [9] </ref> tracks direct predecessors of a message M , rather than all the predecessors of M. This results in a savings in the message overhead and makes it more efficient than previous ones. <p> However, this algorithm is designed for group communication (where groups are fixed a priori) and the reduction in message overhead is partly because all messages are sent to within the group. This implementation <ref> [9] </ref> tracks direct predecessors inefficiently and consequently tracks indirect predecessors also. All existing algorithms have higher message overhead and use more storage than is required for an optimal algorithm. <p> At the time M j;b is multicast, l i;a :Dests is updated to f4g and l j;b , where l j;b :Dests = f 2, 9, 11, 12 g, is inserted in LOG j . This notion was used in <ref> [9] </ref> which tracked direct dependencies in the context of a single group communication. 3. RCV (4): Let M arrive at j and let maxfx j o i;x 2 O M g = a.
Reference: [10] <author> L. Lamport, </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System, </title> <journal> Communications of the ACM, </journal> <volume> 558-565, 21(7), </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: An event at local time a on process i is denoted by (i; a). The cause and effect relationship is captured by Lamport's "happened before" relation (!) <ref> [10] </ref>, which defines a partial order on the events of a distributed execution. <p> The concept of causal ordering is of considerable interest to the design of distributed systems and finds applications in several domains such as updates of replicated data [5, 8], global state collection [1], distributed shared memory [3], teleconferencing [14], multimedia systems [2], and fair resource allocation <ref> [10] </ref>. Causal message ordering requires appending some control information with each message to enforce the causal order. <p> Each node i maintains a counter, clock i , that is incremented on the occurrence of each local communication event. We do not require either Lamport's scalar clocks <ref> [10] </ref>, or vector [7, 11] or matrix clocks [16] in the algorithm. Let (i; a) denote the event at local time a on node i. A message M sent by i at local time a is denoted as M i;a .
Reference: [11] <author> F. Mattern, </author> <title> Virtual Time and Global States of Distributed Systems, Parallel and Distributed Algorithms, </title> <publisher> North-Holland, </publisher> <pages> 215-226, </pages> <year> 1989. </year>
Reference-contexts: The control information in the Schiper-Eggli-Sandoz causal ordering algorithm [18] consists of n vectors of length upto n each. This information represents messages sent in the causal past that are not known to be delivered. The receiving site uses vector time <ref> [7, 11] </ref> to determine whether messages represented in the control vectors need to be delivered before the current message is delivered. The causal ordering algorithm of Raynal-Schiper-Toueg [15] attaches a matrix SENT of size n fi n with every message. <p> Each node i maintains a counter, clock i , that is incremented on the occurrence of each local communication event. We do not require either Lamport's scalar clocks [10], or vector <ref> [7, 11] </ref> or matrix clocks [16] in the algorithm. Let (i; a) denote the event at local time a on node i. A message M sent by i at local time a is denoted as M i;a . The destinations of this multicast are denoted by M i;a :Dests. <p> A similar notion was previously used in [19] to efficiently implement vector clocks <ref> [7, 11] </ref>. The CO algorithm for FIFO channels is now given, with the changes to the original algorithm underlined.
Reference: [12] <author> A. Mostefaoui, M. Raynal, </author> <title> Causal Multicasts in Overlapping Groups: Towards a Low Cost Approach, </title> <booktitle> IEEE Conf. on Future Trends of Distributed Computer Systems, </booktitle> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Clearly, this method can get expensive if there are several groups with large sizes the maximum number of groups is 2 n 1. 2 In the causal multicast in overlapping groups algorithm of Mostefaoui and Raynal <ref> [12] </ref>, a process keeps only one scalar for every group and appends only one vector (with the size equal to the number of groups) to every message; however, the algorithm assumes synchronous model of distributed execution. That is, the execution proceeds in synchronized phases and it requires additional resynchronization messages.
Reference: [13] <author> L. L. Peterson, N. C. Buchholz, R. D. Schlichting, </author> <title> Preserving and Using Context In formation in Interprocess Communication, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7, </volume> <pages> 217-246, </pages> <year> 1989. </year>
Reference-contexts: Due to redundant information, this scheme is resilient to processor crashes; however, a complex mechanism is required to prevent unbounded growth of the control information. In any case, the volume of control information can be huge. The CO algorithm in <ref> [13] </ref> is similar to [4] but carries message-ids rather than entire messages in the control information. Furthermore, unnecessary control information is not sent if the sending host had sent it before. The control information in the Schiper-Eggli-Sandoz causal ordering algorithm [18] consists of n vectors of length upto n each.
Reference: [14] <author> K. Ravindran, B. Prasad, </author> <title> Communication Structures and Paradigms for Distributed Conferencing Applications, </title> <booktitle> Proc. 12th Int'l. Conf. on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The concept of causal ordering is of considerable interest to the design of distributed systems and finds applications in several domains such as updates of replicated data [5, 8], global state collection [1], distributed shared memory [3], teleconferencing <ref> [14] </ref>, multimedia systems [2], and fair resource allocation [10]. Causal message ordering requires appending some control information with each message to enforce the causal order.
Reference: [15] <author> M. Raynal, A. Schiper, S. Toueg, </author> <title> The Causal Ordering Abstraction and a Simple Way to Implement It, </title> <journal> Information Processing Letters, </journal> <volume> 39(6), </volume> <pages> 343-350, </pages> <year> 1991. </year>
Reference-contexts: This information represents messages sent in the causal past that are not known to be delivered. The receiving site uses vector time [7, 11] to determine whether messages represented in the control vectors need to be delivered before the current message is delivered. The causal ordering algorithm of Raynal-Schiper-Toueg <ref> [15] </ref> attaches a matrix SENT of size n fi n with every message. SEN T [i; j] indicates the number of messages that are known to be sent by i to j. <p> The general notion of using a n fi n array for representing the timestamp for the latest message sent between a (source,destination) pair was used in <ref> [15, 18] </ref>; however, they stored this information explicitly, whereas we deduce and represent this information implicitly from the bare minimum information specified by the Propagation Constraints. Example (RCV 5): Let l i;18 :Dests = f 2,11, 13, 14 g and l i;20 :Dests = f 4,11, 14, 15, 18 g.
Reference: [16] <author> M. Raynal and M. Singhal, </author> <title> Logical Time: A Way to Capture Causality in Distributed Systems, </title> <note> submitted to IEEE Computers (Available as OSU or IRISA Tech Report). </note>
Reference-contexts: Each node i maintains a counter, clock i , that is incremented on the occurrence of each local communication event. We do not require either Lamport's scalar clocks [10], or vector [7, 11] or matrix clocks <ref> [16] </ref> in the algorithm. Let (i; a) denote the event at local time a on node i. A message M sent by i at local time a is denoted as M i;a . The destinations of this multicast are denoted by M i;a :Dests.
Reference: [17] <author> L. Rodrigues and P. Verissimo, </author> <title> Causal Separators for Large-Scale Multicast Commu nication, </title> <booktitle> Proc 15th IEEE Int'l. Conf. on Distributed Computing Systems, May-June 1995, </booktitle> <pages> pp. 83-91. </pages>
Reference-contexts: Nonetheless, this algorithm is desirable in situations where additional message traffic and delays can be tolerated for much reduced message overhead. Rodrigues and Verissimo <ref> [17] </ref> exploit the topology of the underlying communication network to reduce the size of control information transferred in messages. Unlike [17], the algorithm proposed here does not require the knowledge of the topology of the underlying network; instead, it uses the dynamic communication pattern and structure of the computation to reduce <p> Nonetheless, this algorithm is desirable in situations where additional message traffic and delays can be tolerated for much reduced message overhead. Rodrigues and Verissimo <ref> [17] </ref> exploit the topology of the underlying communication network to reduce the size of control information transferred in messages. Unlike [17], the algorithm proposed here does not require the knowledge of the topology of the underlying network; instead, it uses the dynamic communication pattern and structure of the computation to reduce the message overhead.
Reference: [18] <author> A. Schiper, J. Eggli, A. Sandoz, </author> <title> A New Algorithm to Implement Causal Ordering, </title> <booktitle> Proceedings of the 3rd International Workshop on Distributed Algorithms, </booktitle> <year> 1989, </year> <pages> 219-232, </pages> <booktitle> in Lecture Notes in Computer Science 392, </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The CO algorithm in [13] is similar to [4] but carries message-ids rather than entire messages in the control information. Furthermore, unnecessary control information is not sent if the sending host had sent it before. The control information in the Schiper-Eggli-Sandoz causal ordering algorithm <ref> [18] </ref> consists of n vectors of length upto n each. This information represents messages sent in the causal past that are not known to be delivered. <p> The general notion of using a n fi n array for representing the timestamp for the latest message sent between a (source,destination) pair was used in <ref> [15, 18] </ref>; however, they stored this information explicitly, whereas we deduce and represent this information implicitly from the bare minimum information specified by the Propagation Constraints. Example (RCV 5): Let l i;18 :Dests = f 2,11, 13, 14 g and l i;20 :Dests = f 4,11, 14, 15, 18 g.
Reference: [19] <author> M. Singhal, A. D. Kshemkalyani, </author> <title> An Efficient Implementation of Vector Clocks, </title> <journal> Information Processing Letters, </journal> <volume> 43, </volume> <pages> 47-53, </pages> <month> Aug. </month> <year> 1992. </year> <month> 43 </month>
Reference-contexts: A similar notion was previously used in <ref> [19] </ref> to efficiently implement vector clocks [7, 11]. The CO algorithm for FIFO channels is now given, with the changes to the original algorithm underlined.
References-found: 19

